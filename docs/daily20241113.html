<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241112.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and\n  200+ FPS", "author": "Zhiwen Fan and Kevin Wang and Kairun Wen and Zehao Zhu and Dejia Xu and Zhangyang Wang", "abstract": "  Recent advances in real-time neural rendering using point-based techniques\nhave enabled broader adoption of 3D representations. However, foundational\napproaches like 3D Gaussian Splatting impose substantial storage overhead, as\nStructure-from-Motion (SfM) points can grow to millions, often requiring\ngigabyte-level disk space for a single unbounded scene. This growth presents\nscalability challenges and hinders splatting efficiency. To address this, we\nintroduce LightGaussian, a method for transforming 3D Gaussians into a more\ncompact format. Inspired by Network Pruning, LightGaussian identifies Gaussians\nwith minimal global significance on scene reconstruction, and applies a pruning\nand recovery process to reduce redundancy while preserving visual quality.\nKnowledge distillation and pseudo-view augmentation then transfer spherical\nharmonic coefficients to a lower degree, yielding compact representations.\nGaussian Vector Quantization, based on each Gaussian's global significance,\nfurther lowers bitwidth with minimal accuracy loss. LightGaussian achieves an\naverage 15x compression rate while boosting FPS from 144 to 237 within the\n3D-GS framework, enabling efficient complex scene representation on the\nMip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach\nis also adaptable to other 3D representations (e.g., Scaffold-GS),\ndemonstrating strong generalization capabilities.\n", "link": "http://arxiv.org/abs/2311.17245v6", "date": "2024-11-12", "relevancy": 3.4489, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7161}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6825}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS&body=Title%3A%20LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS%0AAuthor%3A%20Zhiwen%20Fan%20and%20Kevin%20Wang%20and%20Kairun%20Wen%20and%20Zehao%20Zhu%20and%20Dejia%20Xu%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20real-time%20neural%20rendering%20using%20point-based%20techniques%0Ahave%20enabled%20broader%20adoption%20of%203D%20representations.%20However%2C%20foundational%0Aapproaches%20like%203D%20Gaussian%20Splatting%20impose%20substantial%20storage%20overhead%2C%20as%0AStructure-from-Motion%20%28SfM%29%20points%20can%20grow%20to%20millions%2C%20often%20requiring%0Agigabyte-level%20disk%20space%20for%20a%20single%20unbounded%20scene.%20This%20growth%20presents%0Ascalability%20challenges%20and%20hinders%20splatting%20efficiency.%20To%20address%20this%2C%20we%0Aintroduce%20LightGaussian%2C%20a%20method%20for%20transforming%203D%20Gaussians%20into%20a%20more%0Acompact%20format.%20Inspired%20by%20Network%20Pruning%2C%20LightGaussian%20identifies%20Gaussians%0Awith%20minimal%20global%20significance%20on%20scene%20reconstruction%2C%20and%20applies%20a%20pruning%0Aand%20recovery%20process%20to%20reduce%20redundancy%20while%20preserving%20visual%20quality.%0AKnowledge%20distillation%20and%20pseudo-view%20augmentation%20then%20transfer%20spherical%0Aharmonic%20coefficients%20to%20a%20lower%20degree%2C%20yielding%20compact%20representations.%0AGaussian%20Vector%20Quantization%2C%20based%20on%20each%20Gaussian%27s%20global%20significance%2C%0Afurther%20lowers%20bitwidth%20with%20minimal%20accuracy%20loss.%20LightGaussian%20achieves%20an%0Aaverage%2015x%20compression%20rate%20while%20boosting%20FPS%20from%20144%20to%20237%20within%20the%0A3D-GS%20framework%2C%20enabling%20efficient%20complex%20scene%20representation%20on%20the%0AMip-NeRF%20360%20and%20Tank%20%26%20Temple%20datasets.%20The%20proposed%20Gaussian%20pruning%20approach%0Ais%20also%20adaptable%20to%20other%203D%20representations%20%28e.g.%2C%20Scaffold-GS%29%2C%0Ademonstrating%20strong%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17245v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightGaussian%253A%2520Unbounded%25203D%2520Gaussian%2520Compression%2520with%252015x%2520Reduction%2520and%250A%2520%2520200%252B%2520FPS%26entry.906535625%3DZhiwen%2520Fan%2520and%2520Kevin%2520Wang%2520and%2520Kairun%2520Wen%2520and%2520Zehao%2520Zhu%2520and%2520Dejia%2520Xu%2520and%2520Zhangyang%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520real-time%2520neural%2520rendering%2520using%2520point-based%2520techniques%250Ahave%2520enabled%2520broader%2520adoption%2520of%25203D%2520representations.%2520However%252C%2520foundational%250Aapproaches%2520like%25203D%2520Gaussian%2520Splatting%2520impose%2520substantial%2520storage%2520overhead%252C%2520as%250AStructure-from-Motion%2520%2528SfM%2529%2520points%2520can%2520grow%2520to%2520millions%252C%2520often%2520requiring%250Agigabyte-level%2520disk%2520space%2520for%2520a%2520single%2520unbounded%2520scene.%2520This%2520growth%2520presents%250Ascalability%2520challenges%2520and%2520hinders%2520splatting%2520efficiency.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520LightGaussian%252C%2520a%2520method%2520for%2520transforming%25203D%2520Gaussians%2520into%2520a%2520more%250Acompact%2520format.%2520Inspired%2520by%2520Network%2520Pruning%252C%2520LightGaussian%2520identifies%2520Gaussians%250Awith%2520minimal%2520global%2520significance%2520on%2520scene%2520reconstruction%252C%2520and%2520applies%2520a%2520pruning%250Aand%2520recovery%2520process%2520to%2520reduce%2520redundancy%2520while%2520preserving%2520visual%2520quality.%250AKnowledge%2520distillation%2520and%2520pseudo-view%2520augmentation%2520then%2520transfer%2520spherical%250Aharmonic%2520coefficients%2520to%2520a%2520lower%2520degree%252C%2520yielding%2520compact%2520representations.%250AGaussian%2520Vector%2520Quantization%252C%2520based%2520on%2520each%2520Gaussian%2527s%2520global%2520significance%252C%250Afurther%2520lowers%2520bitwidth%2520with%2520minimal%2520accuracy%2520loss.%2520LightGaussian%2520achieves%2520an%250Aaverage%252015x%2520compression%2520rate%2520while%2520boosting%2520FPS%2520from%2520144%2520to%2520237%2520within%2520the%250A3D-GS%2520framework%252C%2520enabling%2520efficient%2520complex%2520scene%2520representation%2520on%2520the%250AMip-NeRF%2520360%2520and%2520Tank%2520%2526%2520Temple%2520datasets.%2520The%2520proposed%2520Gaussian%2520pruning%2520approach%250Ais%2520also%2520adaptable%2520to%2520other%25203D%2520representations%2520%2528e.g.%252C%2520Scaffold-GS%2529%252C%250Ademonstrating%2520strong%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17245v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightGaussian%3A%20Unbounded%203D%20Gaussian%20Compression%20with%2015x%20Reduction%20and%0A%20%20200%2B%20FPS&entry.906535625=Zhiwen%20Fan%20and%20Kevin%20Wang%20and%20Kairun%20Wen%20and%20Zehao%20Zhu%20and%20Dejia%20Xu%20and%20Zhangyang%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20real-time%20neural%20rendering%20using%20point-based%20techniques%0Ahave%20enabled%20broader%20adoption%20of%203D%20representations.%20However%2C%20foundational%0Aapproaches%20like%203D%20Gaussian%20Splatting%20impose%20substantial%20storage%20overhead%2C%20as%0AStructure-from-Motion%20%28SfM%29%20points%20can%20grow%20to%20millions%2C%20often%20requiring%0Agigabyte-level%20disk%20space%20for%20a%20single%20unbounded%20scene.%20This%20growth%20presents%0Ascalability%20challenges%20and%20hinders%20splatting%20efficiency.%20To%20address%20this%2C%20we%0Aintroduce%20LightGaussian%2C%20a%20method%20for%20transforming%203D%20Gaussians%20into%20a%20more%0Acompact%20format.%20Inspired%20by%20Network%20Pruning%2C%20LightGaussian%20identifies%20Gaussians%0Awith%20minimal%20global%20significance%20on%20scene%20reconstruction%2C%20and%20applies%20a%20pruning%0Aand%20recovery%20process%20to%20reduce%20redundancy%20while%20preserving%20visual%20quality.%0AKnowledge%20distillation%20and%20pseudo-view%20augmentation%20then%20transfer%20spherical%0Aharmonic%20coefficients%20to%20a%20lower%20degree%2C%20yielding%20compact%20representations.%0AGaussian%20Vector%20Quantization%2C%20based%20on%20each%20Gaussian%27s%20global%20significance%2C%0Afurther%20lowers%20bitwidth%20with%20minimal%20accuracy%20loss.%20LightGaussian%20achieves%20an%0Aaverage%2015x%20compression%20rate%20while%20boosting%20FPS%20from%20144%20to%20237%20within%20the%0A3D-GS%20framework%2C%20enabling%20efficient%20complex%20scene%20representation%20on%20the%0AMip-NeRF%20360%20and%20Tank%20%26%20Temple%20datasets.%20The%20proposed%20Gaussian%20pruning%20approach%0Ais%20also%20adaptable%20to%20other%203D%20representations%20%28e.g.%2C%20Scaffold-GS%29%2C%0Ademonstrating%20strong%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17245v6&entry.124074799=Read"},
{"title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D\n  Generation", "author": "Yushi Lan and Shangchen Zhou and Zhaoyang Lyu and Fangzhou Hong and Shuai Yang and Bo Dai and Xingang Pan and Chen Change Loy", "abstract": "  While 3D content generation has advanced significantly, existing methods\nstill face challenges with input formats, latent space design, and output\nrepresentations. This paper introduces a novel 3D generation framework that\naddresses these challenges, offering scalable, high-quality 3D generation with\nan interactive Point Cloud-structured Latent space. Our framework employs a\nVariational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)\nrenderings as input, using a unique latent space design that preserves 3D shape\ninformation, and incorporates a cascaded latent diffusion model for improved\nshape-texture disentanglement. The proposed method, GaussianAnything, supports\nmulti-modal conditional 3D generation, allowing for point cloud, caption, and\nsingle/multi-view image inputs. Notably, the newly proposed latent space\nnaturally enables geometry-texture disentanglement, thus allowing 3D-aware\nediting. Experimental results demonstrate the effectiveness of our approach on\nmultiple datasets, outperforming existing methods in both text- and\nimage-conditioned 3D generation.\n", "link": "http://arxiv.org/abs/2411.08033v1", "date": "2024-11-12", "relevancy": 3.3142, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6709}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianAnything%3A%20Interactive%20Point%20Cloud%20Latent%20Diffusion%20for%203D%0A%20%20Generation&body=Title%3A%20GaussianAnything%3A%20Interactive%20Point%20Cloud%20Latent%20Diffusion%20for%203D%0A%20%20Generation%0AAuthor%3A%20Yushi%20Lan%20and%20Shangchen%20Zhou%20and%20Zhaoyang%20Lyu%20and%20Fangzhou%20Hong%20and%20Shuai%20Yang%20and%20Bo%20Dai%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20While%203D%20content%20generation%20has%20advanced%20significantly%2C%20existing%20methods%0Astill%20face%20challenges%20with%20input%20formats%2C%20latent%20space%20design%2C%20and%20output%0Arepresentations.%20This%20paper%20introduces%20a%20novel%203D%20generation%20framework%20that%0Aaddresses%20these%20challenges%2C%20offering%20scalable%2C%20high-quality%203D%20generation%20with%0Aan%20interactive%20Point%20Cloud-structured%20Latent%20space.%20Our%20framework%20employs%20a%0AVariational%20Autoencoder%20%28VAE%29%20with%20multi-view%20posed%20RGB-D%28epth%29-N%28ormal%29%0Arenderings%20as%20input%2C%20using%20a%20unique%20latent%20space%20design%20that%20preserves%203D%20shape%0Ainformation%2C%20and%20incorporates%20a%20cascaded%20latent%20diffusion%20model%20for%20improved%0Ashape-texture%20disentanglement.%20The%20proposed%20method%2C%20GaussianAnything%2C%20supports%0Amulti-modal%20conditional%203D%20generation%2C%20allowing%20for%20point%20cloud%2C%20caption%2C%20and%0Asingle/multi-view%20image%20inputs.%20Notably%2C%20the%20newly%20proposed%20latent%20space%0Anaturally%20enables%20geometry-texture%20disentanglement%2C%20thus%20allowing%203D-aware%0Aediting.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Amultiple%20datasets%2C%20outperforming%20existing%20methods%20in%20both%20text-%20and%0Aimage-conditioned%203D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianAnything%253A%2520Interactive%2520Point%2520Cloud%2520Latent%2520Diffusion%2520for%25203D%250A%2520%2520Generation%26entry.906535625%3DYushi%2520Lan%2520and%2520Shangchen%2520Zhou%2520and%2520Zhaoyang%2520Lyu%2520and%2520Fangzhou%2520Hong%2520and%2520Shuai%2520Yang%2520and%2520Bo%2520Dai%2520and%2520Xingang%2520Pan%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520While%25203D%2520content%2520generation%2520has%2520advanced%2520significantly%252C%2520existing%2520methods%250Astill%2520face%2520challenges%2520with%2520input%2520formats%252C%2520latent%2520space%2520design%252C%2520and%2520output%250Arepresentations.%2520This%2520paper%2520introduces%2520a%2520novel%25203D%2520generation%2520framework%2520that%250Aaddresses%2520these%2520challenges%252C%2520offering%2520scalable%252C%2520high-quality%25203D%2520generation%2520with%250Aan%2520interactive%2520Point%2520Cloud-structured%2520Latent%2520space.%2520Our%2520framework%2520employs%2520a%250AVariational%2520Autoencoder%2520%2528VAE%2529%2520with%2520multi-view%2520posed%2520RGB-D%2528epth%2529-N%2528ormal%2529%250Arenderings%2520as%2520input%252C%2520using%2520a%2520unique%2520latent%2520space%2520design%2520that%2520preserves%25203D%2520shape%250Ainformation%252C%2520and%2520incorporates%2520a%2520cascaded%2520latent%2520diffusion%2520model%2520for%2520improved%250Ashape-texture%2520disentanglement.%2520The%2520proposed%2520method%252C%2520GaussianAnything%252C%2520supports%250Amulti-modal%2520conditional%25203D%2520generation%252C%2520allowing%2520for%2520point%2520cloud%252C%2520caption%252C%2520and%250Asingle/multi-view%2520image%2520inputs.%2520Notably%252C%2520the%2520newly%2520proposed%2520latent%2520space%250Anaturally%2520enables%2520geometry-texture%2520disentanglement%252C%2520thus%2520allowing%25203D-aware%250Aediting.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%250Amultiple%2520datasets%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520text-%2520and%250Aimage-conditioned%25203D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianAnything%3A%20Interactive%20Point%20Cloud%20Latent%20Diffusion%20for%203D%0A%20%20Generation&entry.906535625=Yushi%20Lan%20and%20Shangchen%20Zhou%20and%20Zhaoyang%20Lyu%20and%20Fangzhou%20Hong%20and%20Shuai%20Yang%20and%20Bo%20Dai%20and%20Xingang%20Pan%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20While%203D%20content%20generation%20has%20advanced%20significantly%2C%20existing%20methods%0Astill%20face%20challenges%20with%20input%20formats%2C%20latent%20space%20design%2C%20and%20output%0Arepresentations.%20This%20paper%20introduces%20a%20novel%203D%20generation%20framework%20that%0Aaddresses%20these%20challenges%2C%20offering%20scalable%2C%20high-quality%203D%20generation%20with%0Aan%20interactive%20Point%20Cloud-structured%20Latent%20space.%20Our%20framework%20employs%20a%0AVariational%20Autoencoder%20%28VAE%29%20with%20multi-view%20posed%20RGB-D%28epth%29-N%28ormal%29%0Arenderings%20as%20input%2C%20using%20a%20unique%20latent%20space%20design%20that%20preserves%203D%20shape%0Ainformation%2C%20and%20incorporates%20a%20cascaded%20latent%20diffusion%20model%20for%20improved%0Ashape-texture%20disentanglement.%20The%20proposed%20method%2C%20GaussianAnything%2C%20supports%0Amulti-modal%20conditional%203D%20generation%2C%20allowing%20for%20point%20cloud%2C%20caption%2C%20and%0Asingle/multi-view%20image%20inputs.%20Notably%2C%20the%20newly%20proposed%20latent%20space%0Anaturally%20enables%20geometry-texture%20disentanglement%2C%20thus%20allowing%203D-aware%0Aediting.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Amultiple%20datasets%2C%20outperforming%20existing%20methods%20in%20both%20text-%20and%0Aimage-conditioned%203D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08033v1&entry.124074799=Read"},
{"title": "LapGSR: Laplacian Reconstructive Network for Guided Thermal\n  Super-Resolution", "author": "Aditya Kasliwal and Ishaan Gakhar and Aryan Kamani and Pratinav Seth and Ujjwal Verma", "abstract": "  In the last few years, the fusion of multi-modal data has been widely studied\nfor various applications such as robotics, gesture recognition, and autonomous\nnavigation. Indeed, high-quality visual sensors are expensive, and\nconsumer-grade sensors produce low-resolution images. Researchers have\ndeveloped methods to combine RGB color images with non-visual data, such as\nthermal, to overcome this limitation to improve resolution. Fusing multiple\nmodalities to produce visually appealing, high-resolution images often requires\ndense models with millions of parameters and a heavy computational load, which\nis commonly attributed to the intricate architecture of the model.\n  We propose LapGSR, a multimodal, lightweight, generative model incorporating\nLaplacian image pyramids for guided thermal super-resolution. This approach\nuses a Laplacian Pyramid on RGB color images to extract vital edge information,\nwhich is then used to bypass heavy feature map computation in the higher layers\nof the model in tandem with a combined pixel and adversarial loss. LapGSR\npreserves the spatial and structural details of the image while also being\nefficient and compact. This results in a model with significantly fewer\nparameters than other SOTA models while demonstrating excellent results on two\ncross-domain datasets viz. ULB17-VT and VGTSR datasets.\n", "link": "http://arxiv.org/abs/2411.07750v1", "date": "2024-11-12", "relevancy": 2.9843, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6427}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LapGSR%3A%20Laplacian%20Reconstructive%20Network%20for%20Guided%20Thermal%0A%20%20Super-Resolution&body=Title%3A%20LapGSR%3A%20Laplacian%20Reconstructive%20Network%20for%20Guided%20Thermal%0A%20%20Super-Resolution%0AAuthor%3A%20Aditya%20Kasliwal%20and%20Ishaan%20Gakhar%20and%20Aryan%20Kamani%20and%20Pratinav%20Seth%20and%20Ujjwal%20Verma%0AAbstract%3A%20%20%20In%20the%20last%20few%20years%2C%20the%20fusion%20of%20multi-modal%20data%20has%20been%20widely%20studied%0Afor%20various%20applications%20such%20as%20robotics%2C%20gesture%20recognition%2C%20and%20autonomous%0Anavigation.%20Indeed%2C%20high-quality%20visual%20sensors%20are%20expensive%2C%20and%0Aconsumer-grade%20sensors%20produce%20low-resolution%20images.%20Researchers%20have%0Adeveloped%20methods%20to%20combine%20RGB%20color%20images%20with%20non-visual%20data%2C%20such%20as%0Athermal%2C%20to%20overcome%20this%20limitation%20to%20improve%20resolution.%20Fusing%20multiple%0Amodalities%20to%20produce%20visually%20appealing%2C%20high-resolution%20images%20often%20requires%0Adense%20models%20with%20millions%20of%20parameters%20and%20a%20heavy%20computational%20load%2C%20which%0Ais%20commonly%20attributed%20to%20the%20intricate%20architecture%20of%20the%20model.%0A%20%20We%20propose%20LapGSR%2C%20a%20multimodal%2C%20lightweight%2C%20generative%20model%20incorporating%0ALaplacian%20image%20pyramids%20for%20guided%20thermal%20super-resolution.%20This%20approach%0Auses%20a%20Laplacian%20Pyramid%20on%20RGB%20color%20images%20to%20extract%20vital%20edge%20information%2C%0Awhich%20is%20then%20used%20to%20bypass%20heavy%20feature%20map%20computation%20in%20the%20higher%20layers%0Aof%20the%20model%20in%20tandem%20with%20a%20combined%20pixel%20and%20adversarial%20loss.%20LapGSR%0Apreserves%20the%20spatial%20and%20structural%20details%20of%20the%20image%20while%20also%20being%0Aefficient%20and%20compact.%20This%20results%20in%20a%20model%20with%20significantly%20fewer%0Aparameters%20than%20other%20SOTA%20models%20while%20demonstrating%20excellent%20results%20on%20two%0Across-domain%20datasets%20viz.%20ULB17-VT%20and%20VGTSR%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLapGSR%253A%2520Laplacian%2520Reconstructive%2520Network%2520for%2520Guided%2520Thermal%250A%2520%2520Super-Resolution%26entry.906535625%3DAditya%2520Kasliwal%2520and%2520Ishaan%2520Gakhar%2520and%2520Aryan%2520Kamani%2520and%2520Pratinav%2520Seth%2520and%2520Ujjwal%2520Verma%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520few%2520years%252C%2520the%2520fusion%2520of%2520multi-modal%2520data%2520has%2520been%2520widely%2520studied%250Afor%2520various%2520applications%2520such%2520as%2520robotics%252C%2520gesture%2520recognition%252C%2520and%2520autonomous%250Anavigation.%2520Indeed%252C%2520high-quality%2520visual%2520sensors%2520are%2520expensive%252C%2520and%250Aconsumer-grade%2520sensors%2520produce%2520low-resolution%2520images.%2520Researchers%2520have%250Adeveloped%2520methods%2520to%2520combine%2520RGB%2520color%2520images%2520with%2520non-visual%2520data%252C%2520such%2520as%250Athermal%252C%2520to%2520overcome%2520this%2520limitation%2520to%2520improve%2520resolution.%2520Fusing%2520multiple%250Amodalities%2520to%2520produce%2520visually%2520appealing%252C%2520high-resolution%2520images%2520often%2520requires%250Adense%2520models%2520with%2520millions%2520of%2520parameters%2520and%2520a%2520heavy%2520computational%2520load%252C%2520which%250Ais%2520commonly%2520attributed%2520to%2520the%2520intricate%2520architecture%2520of%2520the%2520model.%250A%2520%2520We%2520propose%2520LapGSR%252C%2520a%2520multimodal%252C%2520lightweight%252C%2520generative%2520model%2520incorporating%250ALaplacian%2520image%2520pyramids%2520for%2520guided%2520thermal%2520super-resolution.%2520This%2520approach%250Auses%2520a%2520Laplacian%2520Pyramid%2520on%2520RGB%2520color%2520images%2520to%2520extract%2520vital%2520edge%2520information%252C%250Awhich%2520is%2520then%2520used%2520to%2520bypass%2520heavy%2520feature%2520map%2520computation%2520in%2520the%2520higher%2520layers%250Aof%2520the%2520model%2520in%2520tandem%2520with%2520a%2520combined%2520pixel%2520and%2520adversarial%2520loss.%2520LapGSR%250Apreserves%2520the%2520spatial%2520and%2520structural%2520details%2520of%2520the%2520image%2520while%2520also%2520being%250Aefficient%2520and%2520compact.%2520This%2520results%2520in%2520a%2520model%2520with%2520significantly%2520fewer%250Aparameters%2520than%2520other%2520SOTA%2520models%2520while%2520demonstrating%2520excellent%2520results%2520on%2520two%250Across-domain%2520datasets%2520viz.%2520ULB17-VT%2520and%2520VGTSR%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LapGSR%3A%20Laplacian%20Reconstructive%20Network%20for%20Guided%20Thermal%0A%20%20Super-Resolution&entry.906535625=Aditya%20Kasliwal%20and%20Ishaan%20Gakhar%20and%20Aryan%20Kamani%20and%20Pratinav%20Seth%20and%20Ujjwal%20Verma&entry.1292438233=%20%20In%20the%20last%20few%20years%2C%20the%20fusion%20of%20multi-modal%20data%20has%20been%20widely%20studied%0Afor%20various%20applications%20such%20as%20robotics%2C%20gesture%20recognition%2C%20and%20autonomous%0Anavigation.%20Indeed%2C%20high-quality%20visual%20sensors%20are%20expensive%2C%20and%0Aconsumer-grade%20sensors%20produce%20low-resolution%20images.%20Researchers%20have%0Adeveloped%20methods%20to%20combine%20RGB%20color%20images%20with%20non-visual%20data%2C%20such%20as%0Athermal%2C%20to%20overcome%20this%20limitation%20to%20improve%20resolution.%20Fusing%20multiple%0Amodalities%20to%20produce%20visually%20appealing%2C%20high-resolution%20images%20often%20requires%0Adense%20models%20with%20millions%20of%20parameters%20and%20a%20heavy%20computational%20load%2C%20which%0Ais%20commonly%20attributed%20to%20the%20intricate%20architecture%20of%20the%20model.%0A%20%20We%20propose%20LapGSR%2C%20a%20multimodal%2C%20lightweight%2C%20generative%20model%20incorporating%0ALaplacian%20image%20pyramids%20for%20guided%20thermal%20super-resolution.%20This%20approach%0Auses%20a%20Laplacian%20Pyramid%20on%20RGB%20color%20images%20to%20extract%20vital%20edge%20information%2C%0Awhich%20is%20then%20used%20to%20bypass%20heavy%20feature%20map%20computation%20in%20the%20higher%20layers%0Aof%20the%20model%20in%20tandem%20with%20a%20combined%20pixel%20and%20adversarial%20loss.%20LapGSR%0Apreserves%20the%20spatial%20and%20structural%20details%20of%20the%20image%20while%20also%20being%0Aefficient%20and%20compact.%20This%20results%20in%20a%20model%20with%20significantly%20fewer%0Aparameters%20than%20other%20SOTA%20models%20while%20demonstrating%20excellent%20results%20on%20two%0Across-domain%20datasets%20viz.%20ULB17-VT%20and%20VGTSR%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07750v1&entry.124074799=Read"},
{"title": "3D Focusing-and-Matching Network for Multi-Instance Point Cloud\n  Registration", "author": "Liyuan Zhang and Le Hui and Qi Liu and Bo Li and Yuchao Dai", "abstract": "  Multi-instance point cloud registration aims to estimate the pose of all\ninstances of a model point cloud in the whole scene. Existing methods all adopt\nthe strategy of first obtaining the global correspondence and then clustering\nto obtain the pose of each instance. However, due to the cluttered and occluded\nobjects in the scene, it is difficult to obtain an accurate correspondence\nbetween the model point cloud and all instances in the scene. To this end, we\npropose a simple yet powerful 3D focusing-and-matching network for\nmulti-instance point cloud registration by learning the multiple pair-wise\npoint cloud registration. Specifically, we first present a 3D multi-object\nfocusing module to locate the center of each object and generate object\nproposals. By using self-attention and cross-attention to associate the model\npoint cloud with structurally similar objects, we can locate potential matching\ninstances by regressing object centers. Then, we propose a 3D dual masking\ninstance matching module to estimate the pose between the model point cloud and\neach object proposal. It performs instance mask and overlap mask masks to\naccurately predict the pair-wise correspondence. Extensive experiments on two\npublic benchmarks, Scan2CAD and ROBI, show that our method achieves a new\nstate-of-the-art performance on the multi-instance point cloud registration\ntask. Code is available at https://github.com/zlynpu/3DFMNet.\n", "link": "http://arxiv.org/abs/2411.07740v1", "date": "2024-11-12", "relevancy": 2.9842, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration&body=Title%3A%203D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration%0AAuthor%3A%20Liyuan%20Zhang%20and%20Le%20Hui%20and%20Qi%20Liu%20and%20Bo%20Li%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20Multi-instance%20point%20cloud%20registration%20aims%20to%20estimate%20the%20pose%20of%20all%0Ainstances%20of%20a%20model%20point%20cloud%20in%20the%20whole%20scene.%20Existing%20methods%20all%20adopt%0Athe%20strategy%20of%20first%20obtaining%20the%20global%20correspondence%20and%20then%20clustering%0Ato%20obtain%20the%20pose%20of%20each%20instance.%20However%2C%20due%20to%20the%20cluttered%20and%20occluded%0Aobjects%20in%20the%20scene%2C%20it%20is%20difficult%20to%20obtain%20an%20accurate%20correspondence%0Abetween%20the%20model%20point%20cloud%20and%20all%20instances%20in%20the%20scene.%20To%20this%20end%2C%20we%0Apropose%20a%20simple%20yet%20powerful%203D%20focusing-and-matching%20network%20for%0Amulti-instance%20point%20cloud%20registration%20by%20learning%20the%20multiple%20pair-wise%0Apoint%20cloud%20registration.%20Specifically%2C%20we%20first%20present%20a%203D%20multi-object%0Afocusing%20module%20to%20locate%20the%20center%20of%20each%20object%20and%20generate%20object%0Aproposals.%20By%20using%20self-attention%20and%20cross-attention%20to%20associate%20the%20model%0Apoint%20cloud%20with%20structurally%20similar%20objects%2C%20we%20can%20locate%20potential%20matching%0Ainstances%20by%20regressing%20object%20centers.%20Then%2C%20we%20propose%20a%203D%20dual%20masking%0Ainstance%20matching%20module%20to%20estimate%20the%20pose%20between%20the%20model%20point%20cloud%20and%0Aeach%20object%20proposal.%20It%20performs%20instance%20mask%20and%20overlap%20mask%20masks%20to%0Aaccurately%20predict%20the%20pair-wise%20correspondence.%20Extensive%20experiments%20on%20two%0Apublic%20benchmarks%2C%20Scan2CAD%20and%20ROBI%2C%20show%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance%20on%20the%20multi-instance%20point%20cloud%20registration%0Atask.%20Code%20is%20available%20at%20https%3A//github.com/zlynpu/3DFMNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Focusing-and-Matching%2520Network%2520for%2520Multi-Instance%2520Point%2520Cloud%250A%2520%2520Registration%26entry.906535625%3DLiyuan%2520Zhang%2520and%2520Le%2520Hui%2520and%2520Qi%2520Liu%2520and%2520Bo%2520Li%2520and%2520Yuchao%2520Dai%26entry.1292438233%3D%2520%2520Multi-instance%2520point%2520cloud%2520registration%2520aims%2520to%2520estimate%2520the%2520pose%2520of%2520all%250Ainstances%2520of%2520a%2520model%2520point%2520cloud%2520in%2520the%2520whole%2520scene.%2520Existing%2520methods%2520all%2520adopt%250Athe%2520strategy%2520of%2520first%2520obtaining%2520the%2520global%2520correspondence%2520and%2520then%2520clustering%250Ato%2520obtain%2520the%2520pose%2520of%2520each%2520instance.%2520However%252C%2520due%2520to%2520the%2520cluttered%2520and%2520occluded%250Aobjects%2520in%2520the%2520scene%252C%2520it%2520is%2520difficult%2520to%2520obtain%2520an%2520accurate%2520correspondence%250Abetween%2520the%2520model%2520point%2520cloud%2520and%2520all%2520instances%2520in%2520the%2520scene.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520powerful%25203D%2520focusing-and-matching%2520network%2520for%250Amulti-instance%2520point%2520cloud%2520registration%2520by%2520learning%2520the%2520multiple%2520pair-wise%250Apoint%2520cloud%2520registration.%2520Specifically%252C%2520we%2520first%2520present%2520a%25203D%2520multi-object%250Afocusing%2520module%2520to%2520locate%2520the%2520center%2520of%2520each%2520object%2520and%2520generate%2520object%250Aproposals.%2520By%2520using%2520self-attention%2520and%2520cross-attention%2520to%2520associate%2520the%2520model%250Apoint%2520cloud%2520with%2520structurally%2520similar%2520objects%252C%2520we%2520can%2520locate%2520potential%2520matching%250Ainstances%2520by%2520regressing%2520object%2520centers.%2520Then%252C%2520we%2520propose%2520a%25203D%2520dual%2520masking%250Ainstance%2520matching%2520module%2520to%2520estimate%2520the%2520pose%2520between%2520the%2520model%2520point%2520cloud%2520and%250Aeach%2520object%2520proposal.%2520It%2520performs%2520instance%2520mask%2520and%2520overlap%2520mask%2520masks%2520to%250Aaccurately%2520predict%2520the%2520pair-wise%2520correspondence.%2520Extensive%2520experiments%2520on%2520two%250Apublic%2520benchmarks%252C%2520Scan2CAD%2520and%2520ROBI%252C%2520show%2520that%2520our%2520method%2520achieves%2520a%2520new%250Astate-of-the-art%2520performance%2520on%2520the%2520multi-instance%2520point%2520cloud%2520registration%250Atask.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/zlynpu/3DFMNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Focusing-and-Matching%20Network%20for%20Multi-Instance%20Point%20Cloud%0A%20%20Registration&entry.906535625=Liyuan%20Zhang%20and%20Le%20Hui%20and%20Qi%20Liu%20and%20Bo%20Li%20and%20Yuchao%20Dai&entry.1292438233=%20%20Multi-instance%20point%20cloud%20registration%20aims%20to%20estimate%20the%20pose%20of%20all%0Ainstances%20of%20a%20model%20point%20cloud%20in%20the%20whole%20scene.%20Existing%20methods%20all%20adopt%0Athe%20strategy%20of%20first%20obtaining%20the%20global%20correspondence%20and%20then%20clustering%0Ato%20obtain%20the%20pose%20of%20each%20instance.%20However%2C%20due%20to%20the%20cluttered%20and%20occluded%0Aobjects%20in%20the%20scene%2C%20it%20is%20difficult%20to%20obtain%20an%20accurate%20correspondence%0Abetween%20the%20model%20point%20cloud%20and%20all%20instances%20in%20the%20scene.%20To%20this%20end%2C%20we%0Apropose%20a%20simple%20yet%20powerful%203D%20focusing-and-matching%20network%20for%0Amulti-instance%20point%20cloud%20registration%20by%20learning%20the%20multiple%20pair-wise%0Apoint%20cloud%20registration.%20Specifically%2C%20we%20first%20present%20a%203D%20multi-object%0Afocusing%20module%20to%20locate%20the%20center%20of%20each%20object%20and%20generate%20object%0Aproposals.%20By%20using%20self-attention%20and%20cross-attention%20to%20associate%20the%20model%0Apoint%20cloud%20with%20structurally%20similar%20objects%2C%20we%20can%20locate%20potential%20matching%0Ainstances%20by%20regressing%20object%20centers.%20Then%2C%20we%20propose%20a%203D%20dual%20masking%0Ainstance%20matching%20module%20to%20estimate%20the%20pose%20between%20the%20model%20point%20cloud%20and%0Aeach%20object%20proposal.%20It%20performs%20instance%20mask%20and%20overlap%20mask%20masks%20to%0Aaccurately%20predict%20the%20pair-wise%20correspondence.%20Extensive%20experiments%20on%20two%0Apublic%20benchmarks%2C%20Scan2CAD%20and%20ROBI%2C%20show%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20performance%20on%20the%20multi-instance%20point%20cloud%20registration%0Atask.%20Code%20is%20available%20at%20https%3A//github.com/zlynpu/3DFMNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07740v1&entry.124074799=Read"},
{"title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding\n  with Task Divide-and-Conquer", "author": "Lu Zhang and Tiancheng Zhao and Heting Ying and Yibo Ma and Kyusong Lee", "abstract": "  Recent advancements in Large Language Models (LLMs) have expanded their\ncapabilities to multimodal contexts, including comprehensive video\nunderstanding. However, processing extensive videos such as 24-hour CCTV\nfootage or full-length films presents significant challenges due to the vast\ndata and processing demands. Traditional methods, like extracting key frames or\nconverting frames to text, often result in substantial information loss. To\naddress these shortcomings, we develop OmAgent, efficiently stores and\nretrieves relevant video frames for specific queries, preserving the detailed\ncontent of videos. Additionally, it features an Divide-and-Conquer Loop capable\nof autonomous reasoning, dynamically invoking APIs and tools to enhance query\nprocessing and accuracy. This approach ensures robust video understanding,\nsignificantly reducing information loss. Experimental results affirm OmAgent's\nefficacy in handling various types of videos and complex tasks. Moreover, we\nhave endowed it with greater autonomy and a robust tool-calling system,\nenabling it to accomplish even more intricate tasks.\n", "link": "http://arxiv.org/abs/2406.16620v3", "date": "2024-11-12", "relevancy": 2.9596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmAgent%3A%20A%20Multi-modal%20Agent%20Framework%20for%20Complex%20Video%20Understanding%0A%20%20with%20Task%20Divide-and-Conquer&body=Title%3A%20OmAgent%3A%20A%20Multi-modal%20Agent%20Framework%20for%20Complex%20Video%20Understanding%0A%20%20with%20Task%20Divide-and-Conquer%0AAuthor%3A%20Lu%20Zhang%20and%20Tiancheng%20Zhao%20and%20Heting%20Ying%20and%20Yibo%20Ma%20and%20Kyusong%20Lee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20expanded%20their%0Acapabilities%20to%20multimodal%20contexts%2C%20including%20comprehensive%20video%0Aunderstanding.%20However%2C%20processing%20extensive%20videos%20such%20as%2024-hour%20CCTV%0Afootage%20or%20full-length%20films%20presents%20significant%20challenges%20due%20to%20the%20vast%0Adata%20and%20processing%20demands.%20Traditional%20methods%2C%20like%20extracting%20key%20frames%20or%0Aconverting%20frames%20to%20text%2C%20often%20result%20in%20substantial%20information%20loss.%20To%0Aaddress%20these%20shortcomings%2C%20we%20develop%20OmAgent%2C%20efficiently%20stores%20and%0Aretrieves%20relevant%20video%20frames%20for%20specific%20queries%2C%20preserving%20the%20detailed%0Acontent%20of%20videos.%20Additionally%2C%20it%20features%20an%20Divide-and-Conquer%20Loop%20capable%0Aof%20autonomous%20reasoning%2C%20dynamically%20invoking%20APIs%20and%20tools%20to%20enhance%20query%0Aprocessing%20and%20accuracy.%20This%20approach%20ensures%20robust%20video%20understanding%2C%0Asignificantly%20reducing%20information%20loss.%20Experimental%20results%20affirm%20OmAgent%27s%0Aefficacy%20in%20handling%20various%20types%20of%20videos%20and%20complex%20tasks.%20Moreover%2C%20we%0Ahave%20endowed%20it%20with%20greater%20autonomy%20and%20a%20robust%20tool-calling%20system%2C%0Aenabling%20it%20to%20accomplish%20even%20more%20intricate%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16620v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmAgent%253A%2520A%2520Multi-modal%2520Agent%2520Framework%2520for%2520Complex%2520Video%2520Understanding%250A%2520%2520with%2520Task%2520Divide-and-Conquer%26entry.906535625%3DLu%2520Zhang%2520and%2520Tiancheng%2520Zhao%2520and%2520Heting%2520Ying%2520and%2520Yibo%2520Ma%2520and%2520Kyusong%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520expanded%2520their%250Acapabilities%2520to%2520multimodal%2520contexts%252C%2520including%2520comprehensive%2520video%250Aunderstanding.%2520However%252C%2520processing%2520extensive%2520videos%2520such%2520as%252024-hour%2520CCTV%250Afootage%2520or%2520full-length%2520films%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520vast%250Adata%2520and%2520processing%2520demands.%2520Traditional%2520methods%252C%2520like%2520extracting%2520key%2520frames%2520or%250Aconverting%2520frames%2520to%2520text%252C%2520often%2520result%2520in%2520substantial%2520information%2520loss.%2520To%250Aaddress%2520these%2520shortcomings%252C%2520we%2520develop%2520OmAgent%252C%2520efficiently%2520stores%2520and%250Aretrieves%2520relevant%2520video%2520frames%2520for%2520specific%2520queries%252C%2520preserving%2520the%2520detailed%250Acontent%2520of%2520videos.%2520Additionally%252C%2520it%2520features%2520an%2520Divide-and-Conquer%2520Loop%2520capable%250Aof%2520autonomous%2520reasoning%252C%2520dynamically%2520invoking%2520APIs%2520and%2520tools%2520to%2520enhance%2520query%250Aprocessing%2520and%2520accuracy.%2520This%2520approach%2520ensures%2520robust%2520video%2520understanding%252C%250Asignificantly%2520reducing%2520information%2520loss.%2520Experimental%2520results%2520affirm%2520OmAgent%2527s%250Aefficacy%2520in%2520handling%2520various%2520types%2520of%2520videos%2520and%2520complex%2520tasks.%2520Moreover%252C%2520we%250Ahave%2520endowed%2520it%2520with%2520greater%2520autonomy%2520and%2520a%2520robust%2520tool-calling%2520system%252C%250Aenabling%2520it%2520to%2520accomplish%2520even%2520more%2520intricate%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16620v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmAgent%3A%20A%20Multi-modal%20Agent%20Framework%20for%20Complex%20Video%20Understanding%0A%20%20with%20Task%20Divide-and-Conquer&entry.906535625=Lu%20Zhang%20and%20Tiancheng%20Zhao%20and%20Heting%20Ying%20and%20Yibo%20Ma%20and%20Kyusong%20Lee&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20expanded%20their%0Acapabilities%20to%20multimodal%20contexts%2C%20including%20comprehensive%20video%0Aunderstanding.%20However%2C%20processing%20extensive%20videos%20such%20as%2024-hour%20CCTV%0Afootage%20or%20full-length%20films%20presents%20significant%20challenges%20due%20to%20the%20vast%0Adata%20and%20processing%20demands.%20Traditional%20methods%2C%20like%20extracting%20key%20frames%20or%0Aconverting%20frames%20to%20text%2C%20often%20result%20in%20substantial%20information%20loss.%20To%0Aaddress%20these%20shortcomings%2C%20we%20develop%20OmAgent%2C%20efficiently%20stores%20and%0Aretrieves%20relevant%20video%20frames%20for%20specific%20queries%2C%20preserving%20the%20detailed%0Acontent%20of%20videos.%20Additionally%2C%20it%20features%20an%20Divide-and-Conquer%20Loop%20capable%0Aof%20autonomous%20reasoning%2C%20dynamically%20invoking%20APIs%20and%20tools%20to%20enhance%20query%0Aprocessing%20and%20accuracy.%20This%20approach%20ensures%20robust%20video%20understanding%2C%0Asignificantly%20reducing%20information%20loss.%20Experimental%20results%20affirm%20OmAgent%27s%0Aefficacy%20in%20handling%20various%20types%20of%20videos%20and%20complex%20tasks.%20Moreover%2C%20we%0Ahave%20endowed%20it%20with%20greater%20autonomy%20and%20a%20robust%20tool-calling%20system%2C%0Aenabling%20it%20to%20accomplish%20even%20more%20intricate%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16620v3&entry.124074799=Read"},
{"title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning", "author": "Dake Bu and Wei Huang and Andi Han and Atsushi Nitanda and Taiji Suzuki and Qingfu Zhang and Hau-San Wong", "abstract": "  Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.\n", "link": "http://arxiv.org/abs/2411.02199v4", "date": "2024-11-12", "relevancy": 2.9519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&body=Title%3A%20Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning%0AAuthor%3A%20Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02199v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520Transformers%2520Harness%2520Multi-Concept%2520Word%2520Semantics%2520for%2520Efficient%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DDake%2520Bu%2520and%2520Wei%2520Huang%2520and%2520Andi%2520Han%2520and%2520Atsushi%2520Nitanda%2520and%2520Taiji%2520Suzuki%2520and%2520Qingfu%2520Zhang%2520and%2520Hau-San%2520Wong%26entry.1292438233%3D%2520%2520Transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520displayed%2520remarkable%250Acreative%2520prowess%2520and%2520emergence%2520capabilities.%2520Existing%2520empirical%2520studies%2520have%250Arevealed%2520a%2520strong%2520connection%2520between%2520these%2520LLMs%2527%2520impressive%2520emergence%2520abilities%250Aand%2520their%2520in-context%2520learning%2520%2528ICL%2529%2520capacity%252C%2520allowing%2520them%2520to%2520solve%2520new%2520tasks%250Ausing%2520only%2520task-specific%2520prompts%2520without%2520further%2520fine-tuning.%2520On%2520the%2520other%250Ahand%252C%2520existing%2520empirical%2520and%2520theoretical%2520studies%2520also%2520show%2520that%2520there%2520is%2520a%250Alinear%2520regularity%2520of%2520the%2520multi-concept%2520encoded%2520semantic%2520representation%2520behind%250Atransformer-based%2520LLMs.%2520However%252C%2520existing%2520theoretical%2520work%2520fail%2520to%2520build%2520up%2520an%250Aunderstanding%2520of%2520the%2520connection%2520between%2520this%2520regularity%2520and%2520the%2520innovative%250Apower%2520of%2520ICL.%2520Additionally%252C%2520prior%2520work%2520often%2520focuses%2520on%2520simplified%252C%2520unrealistic%250Ascenarios%2520involving%2520linear%2520transformers%2520or%2520unrealistic%2520loss%2520functions%252C%2520and%2520they%250Aachieve%2520only%2520linear%2520or%2520sub-linear%2520convergence%2520rates.%2520In%2520contrast%252C%2520this%2520work%250Aprovides%2520a%2520fine-grained%2520mathematical%2520analysis%2520to%2520show%2520how%2520transformers%2520leverage%250Athe%2520multi-concept%2520semantics%2520of%2520words%2520to%2520enable%2520powerful%2520ICL%2520and%2520excellent%250Aout-of-distribution%2520ICL%2520abilities%252C%2520offering%2520insights%2520into%2520how%2520transformers%250Ainnovate%2520solutions%2520for%2520certain%2520unseen%2520tasks%2520encoded%2520with%2520multiple%2520cross-concept%250Asemantics.%2520Inspired%2520by%2520empirical%2520studies%2520on%2520the%2520linear%2520latent%2520geometry%2520of%2520LLMs%252C%250Athe%2520analysis%2520is%2520based%2520on%2520a%2520concept-based%2520low-noise%2520sparse%2520coding%2520prompt%2520model.%250ALeveraging%2520advanced%2520techniques%252C%2520this%2520work%2520showcases%2520the%2520exponential%25200-1%2520loss%250Aconvergence%2520over%2520the%2520highly%2520non-convex%2520training%2520dynamics%252C%2520which%2520pioneeringly%250Aincorporates%2520the%2520challenges%2520of%2520softmax%2520self-attention%252C%2520ReLU-activated%2520MLPs%252C%2520and%250Across-entropy%2520loss.%2520Empirical%2520simulations%2520corroborate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02199v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20Transformers%20Harness%20Multi-Concept%20Word%20Semantics%20for%20Efficient%0A%20%20In-Context%20Learning&entry.906535625=Dake%20Bu%20and%20Wei%20Huang%20and%20Andi%20Han%20and%20Atsushi%20Nitanda%20and%20Taiji%20Suzuki%20and%20Qingfu%20Zhang%20and%20Hau-San%20Wong&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20have%20displayed%20remarkable%0Acreative%20prowess%20and%20emergence%20capabilities.%20Existing%20empirical%20studies%20have%0Arevealed%20a%20strong%20connection%20between%20these%20LLMs%27%20impressive%20emergence%20abilities%0Aand%20their%20in-context%20learning%20%28ICL%29%20capacity%2C%20allowing%20them%20to%20solve%20new%20tasks%0Ausing%20only%20task-specific%20prompts%20without%20further%20fine-tuning.%20On%20the%20other%0Ahand%2C%20existing%20empirical%20and%20theoretical%20studies%20also%20show%20that%20there%20is%20a%0Alinear%20regularity%20of%20the%20multi-concept%20encoded%20semantic%20representation%20behind%0Atransformer-based%20LLMs.%20However%2C%20existing%20theoretical%20work%20fail%20to%20build%20up%20an%0Aunderstanding%20of%20the%20connection%20between%20this%20regularity%20and%20the%20innovative%0Apower%20of%20ICL.%20Additionally%2C%20prior%20work%20often%20focuses%20on%20simplified%2C%20unrealistic%0Ascenarios%20involving%20linear%20transformers%20or%20unrealistic%20loss%20functions%2C%20and%20they%0Aachieve%20only%20linear%20or%20sub-linear%20convergence%20rates.%20In%20contrast%2C%20this%20work%0Aprovides%20a%20fine-grained%20mathematical%20analysis%20to%20show%20how%20transformers%20leverage%0Athe%20multi-concept%20semantics%20of%20words%20to%20enable%20powerful%20ICL%20and%20excellent%0Aout-of-distribution%20ICL%20abilities%2C%20offering%20insights%20into%20how%20transformers%0Ainnovate%20solutions%20for%20certain%20unseen%20tasks%20encoded%20with%20multiple%20cross-concept%0Asemantics.%20Inspired%20by%20empirical%20studies%20on%20the%20linear%20latent%20geometry%20of%20LLMs%2C%0Athe%20analysis%20is%20based%20on%20a%20concept-based%20low-noise%20sparse%20coding%20prompt%20model.%0ALeveraging%20advanced%20techniques%2C%20this%20work%20showcases%20the%20exponential%200-1%20loss%0Aconvergence%20over%20the%20highly%20non-convex%20training%20dynamics%2C%20which%20pioneeringly%0Aincorporates%20the%20challenges%20of%20softmax%20self-attention%2C%20ReLU-activated%20MLPs%2C%20and%0Across-entropy%20loss.%20Empirical%20simulations%20corroborate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02199v4&entry.124074799=Read"},
{"title": "Revisiting the Adversarial Robustness of Vision Language Models: a\n  Multimodal Perspective", "author": "Wanqi Zhou and Shuanghao Bai and Danilo P. Mandic and Qibin Zhao and Badong Chen", "abstract": "  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional\ngeneralization across diverse downstream tasks. While recent studies reveal\ntheir vulnerability to adversarial attacks, research to date has primarily\nfocused on enhancing the robustness of image encoders against image-based\nattacks, with defenses against text-based and multimodal attacks remaining\nlargely unexplored. To this end, this work presents the first comprehensive\nstudy on improving the adversarial robustness of VLMs against attacks targeting\nimage, text, and multimodal inputs. This is achieved by proposing multimodal\ncontrastive adversarial training (MMCoA). Such an approach strengthens the\nrobustness of both image and text encoders by aligning the clean text\nembeddings with adversarial image embeddings, and adversarial text embeddings\nwith clean image embeddings. The robustness of the proposed MMCoA is examined\nagainst existing defense methods over image, text, and multimodal attacks on\nthe CLIP model. Extensive experiments on 15 datasets across two tasks reveal\nthe characteristics of different adversarial defense methods under distinct\ndistribution shifts and dataset complexities across the three attack types.\nThis paves the way for a unified framework of adversarial robustness against\ndifferent modality attacks, opening up new possibilities for securing VLMs\nagainst multimodal attacks. The code is available at\nhttps://github.com/ElleZWQ/MMCoA.git.\n", "link": "http://arxiv.org/abs/2404.19287v3", "date": "2024-11-12", "relevancy": 2.9259, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Adversarial%20Robustness%20of%20Vision%20Language%20Models%3A%20a%0A%20%20Multimodal%20Perspective&body=Title%3A%20Revisiting%20the%20Adversarial%20Robustness%20of%20Vision%20Language%20Models%3A%20a%0A%20%20Multimodal%20Perspective%0AAuthor%3A%20Wanqi%20Zhou%20and%20Shuanghao%20Bai%20and%20Danilo%20P.%20Mandic%20and%20Qibin%20Zhao%20and%20Badong%20Chen%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20exhibit%20exceptional%0Ageneralization%20across%20diverse%20downstream%20tasks.%20While%20recent%20studies%20reveal%0Atheir%20vulnerability%20to%20adversarial%20attacks%2C%20research%20to%20date%20has%20primarily%0Afocused%20on%20enhancing%20the%20robustness%20of%20image%20encoders%20against%20image-based%0Aattacks%2C%20with%20defenses%20against%20text-based%20and%20multimodal%20attacks%20remaining%0Alargely%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%20the%20first%20comprehensive%0Astudy%20on%20improving%20the%20adversarial%20robustness%20of%20VLMs%20against%20attacks%20targeting%0Aimage%2C%20text%2C%20and%20multimodal%20inputs.%20This%20is%20achieved%20by%20proposing%20multimodal%0Acontrastive%20adversarial%20training%20%28MMCoA%29.%20Such%20an%20approach%20strengthens%20the%0Arobustness%20of%20both%20image%20and%20text%20encoders%20by%20aligning%20the%20clean%20text%0Aembeddings%20with%20adversarial%20image%20embeddings%2C%20and%20adversarial%20text%20embeddings%0Awith%20clean%20image%20embeddings.%20The%20robustness%20of%20the%20proposed%20MMCoA%20is%20examined%0Aagainst%20existing%20defense%20methods%20over%20image%2C%20text%2C%20and%20multimodal%20attacks%20on%0Athe%20CLIP%20model.%20Extensive%20experiments%20on%2015%20datasets%20across%20two%20tasks%20reveal%0Athe%20characteristics%20of%20different%20adversarial%20defense%20methods%20under%20distinct%0Adistribution%20shifts%20and%20dataset%20complexities%20across%20the%20three%20attack%20types.%0AThis%20paves%20the%20way%20for%20a%20unified%20framework%20of%20adversarial%20robustness%20against%0Adifferent%20modality%20attacks%2C%20opening%20up%20new%20possibilities%20for%20securing%20VLMs%0Aagainst%20multimodal%20attacks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ElleZWQ/MMCoA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Adversarial%2520Robustness%2520of%2520Vision%2520Language%2520Models%253A%2520a%250A%2520%2520Multimodal%2520Perspective%26entry.906535625%3DWanqi%2520Zhou%2520and%2520Shuanghao%2520Bai%2520and%2520Danilo%2520P.%2520Mandic%2520and%2520Qibin%2520Zhao%2520and%2520Badong%2520Chen%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520exhibit%2520exceptional%250Ageneralization%2520across%2520diverse%2520downstream%2520tasks.%2520While%2520recent%2520studies%2520reveal%250Atheir%2520vulnerability%2520to%2520adversarial%2520attacks%252C%2520research%2520to%2520date%2520has%2520primarily%250Afocused%2520on%2520enhancing%2520the%2520robustness%2520of%2520image%2520encoders%2520against%2520image-based%250Aattacks%252C%2520with%2520defenses%2520against%2520text-based%2520and%2520multimodal%2520attacks%2520remaining%250Alargely%2520unexplored.%2520To%2520this%2520end%252C%2520this%2520work%2520presents%2520the%2520first%2520comprehensive%250Astudy%2520on%2520improving%2520the%2520adversarial%2520robustness%2520of%2520VLMs%2520against%2520attacks%2520targeting%250Aimage%252C%2520text%252C%2520and%2520multimodal%2520inputs.%2520This%2520is%2520achieved%2520by%2520proposing%2520multimodal%250Acontrastive%2520adversarial%2520training%2520%2528MMCoA%2529.%2520Such%2520an%2520approach%2520strengthens%2520the%250Arobustness%2520of%2520both%2520image%2520and%2520text%2520encoders%2520by%2520aligning%2520the%2520clean%2520text%250Aembeddings%2520with%2520adversarial%2520image%2520embeddings%252C%2520and%2520adversarial%2520text%2520embeddings%250Awith%2520clean%2520image%2520embeddings.%2520The%2520robustness%2520of%2520the%2520proposed%2520MMCoA%2520is%2520examined%250Aagainst%2520existing%2520defense%2520methods%2520over%2520image%252C%2520text%252C%2520and%2520multimodal%2520attacks%2520on%250Athe%2520CLIP%2520model.%2520Extensive%2520experiments%2520on%252015%2520datasets%2520across%2520two%2520tasks%2520reveal%250Athe%2520characteristics%2520of%2520different%2520adversarial%2520defense%2520methods%2520under%2520distinct%250Adistribution%2520shifts%2520and%2520dataset%2520complexities%2520across%2520the%2520three%2520attack%2520types.%250AThis%2520paves%2520the%2520way%2520for%2520a%2520unified%2520framework%2520of%2520adversarial%2520robustness%2520against%250Adifferent%2520modality%2520attacks%252C%2520opening%2520up%2520new%2520possibilities%2520for%2520securing%2520VLMs%250Aagainst%2520multimodal%2520attacks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ElleZWQ/MMCoA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Adversarial%20Robustness%20of%20Vision%20Language%20Models%3A%20a%0A%20%20Multimodal%20Perspective&entry.906535625=Wanqi%20Zhou%20and%20Shuanghao%20Bai%20and%20Danilo%20P.%20Mandic%20and%20Qibin%20Zhao%20and%20Badong%20Chen&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20exhibit%20exceptional%0Ageneralization%20across%20diverse%20downstream%20tasks.%20While%20recent%20studies%20reveal%0Atheir%20vulnerability%20to%20adversarial%20attacks%2C%20research%20to%20date%20has%20primarily%0Afocused%20on%20enhancing%20the%20robustness%20of%20image%20encoders%20against%20image-based%0Aattacks%2C%20with%20defenses%20against%20text-based%20and%20multimodal%20attacks%20remaining%0Alargely%20unexplored.%20To%20this%20end%2C%20this%20work%20presents%20the%20first%20comprehensive%0Astudy%20on%20improving%20the%20adversarial%20robustness%20of%20VLMs%20against%20attacks%20targeting%0Aimage%2C%20text%2C%20and%20multimodal%20inputs.%20This%20is%20achieved%20by%20proposing%20multimodal%0Acontrastive%20adversarial%20training%20%28MMCoA%29.%20Such%20an%20approach%20strengthens%20the%0Arobustness%20of%20both%20image%20and%20text%20encoders%20by%20aligning%20the%20clean%20text%0Aembeddings%20with%20adversarial%20image%20embeddings%2C%20and%20adversarial%20text%20embeddings%0Awith%20clean%20image%20embeddings.%20The%20robustness%20of%20the%20proposed%20MMCoA%20is%20examined%0Aagainst%20existing%20defense%20methods%20over%20image%2C%20text%2C%20and%20multimodal%20attacks%20on%0Athe%20CLIP%20model.%20Extensive%20experiments%20on%2015%20datasets%20across%20two%20tasks%20reveal%0Athe%20characteristics%20of%20different%20adversarial%20defense%20methods%20under%20distinct%0Adistribution%20shifts%20and%20dataset%20complexities%20across%20the%20three%20attack%20types.%0AThis%20paves%20the%20way%20for%20a%20unified%20framework%20of%20adversarial%20robustness%20against%0Adifferent%20modality%20attacks%2C%20opening%20up%20new%20possibilities%20for%20securing%20VLMs%0Aagainst%20multimodal%20attacks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ElleZWQ/MMCoA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19287v3&entry.124074799=Read"},
{"title": "Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial\n  Pruning", "author": "Jianhao Li and Tianyu Sun and Xueqian Zhang and Zhongdao Wang and Bailan Feng and Hengshuang Zhao", "abstract": "  This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive LiDAR sweeps, resulting in a remarkable improvement in\nperception accuracy. However, the computation cost also increases, hindering\nprevious approaches from utilizing a large number of LiDAR sweeps. To tackle\nthis challenge, we find that a considerable portion of points in the\naccumulated point cloud is redundant, and discarding these points has minimal\nimpact on perception accuracy. We introduce a simple yet effective Gumbel\nSpatial Pruning (GSP) layer that dynamically prunes points based on a learned\nend-to-end sampling. The GSP layer is decoupled from other network components\nand thus can be seamlessly integrated into existing point cloud network\narchitectures. Without incurring additional computational overhead, we increase\nthe number of LiDAR sweeps from 10, a common practice, to as many as 40.\nConsequently, there is a significant enhancement in perception performance. For\ninstance, in nuScenes 3D object detection and BEV map segmentation tasks, our\npruning strategy improves the vanilla TransL baseline and other baseline\nmethods.\n", "link": "http://arxiv.org/abs/2411.07742v1", "date": "2024-11-12", "relevancy": 2.9095, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6072}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5704}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Perception%20on%20Multi-Sweep%20Point%20Cloud%20with%20Gumbel%20Spatial%0A%20%20Pruning&body=Title%3A%20Efficient%203D%20Perception%20on%20Multi-Sweep%20Point%20Cloud%20with%20Gumbel%20Spatial%0A%20%20Pruning%0AAuthor%3A%20Jianhao%20Li%20and%20Tianyu%20Sun%20and%20Xueqian%20Zhang%20and%20Zhongdao%20Wang%20and%20Bailan%20Feng%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20This%20paper%20studies%20point%20cloud%20perception%20within%20outdoor%20environments.%0AExisting%20methods%20face%20limitations%20in%20recognizing%20objects%20located%20at%20a%20distance%0Aor%20occluded%2C%20due%20to%20the%20sparse%20nature%20of%20outdoor%20point%20clouds.%20In%20this%20work%2C%20we%0Aobserve%20a%20significant%20mitigation%20of%20this%20problem%20by%20accumulating%20multiple%0Atemporally%20consecutive%20LiDAR%20sweeps%2C%20resulting%20in%20a%20remarkable%20improvement%20in%0Aperception%20accuracy.%20However%2C%20the%20computation%20cost%20also%20increases%2C%20hindering%0Aprevious%20approaches%20from%20utilizing%20a%20large%20number%20of%20LiDAR%20sweeps.%20To%20tackle%0Athis%20challenge%2C%20we%20find%20that%20a%20considerable%20portion%20of%20points%20in%20the%0Aaccumulated%20point%20cloud%20is%20redundant%2C%20and%20discarding%20these%20points%20has%20minimal%0Aimpact%20on%20perception%20accuracy.%20We%20introduce%20a%20simple%20yet%20effective%20Gumbel%0ASpatial%20Pruning%20%28GSP%29%20layer%20that%20dynamically%20prunes%20points%20based%20on%20a%20learned%0Aend-to-end%20sampling.%20The%20GSP%20layer%20is%20decoupled%20from%20other%20network%20components%0Aand%20thus%20can%20be%20seamlessly%20integrated%20into%20existing%20point%20cloud%20network%0Aarchitectures.%20Without%20incurring%20additional%20computational%20overhead%2C%20we%20increase%0Athe%20number%20of%20LiDAR%20sweeps%20from%2010%2C%20a%20common%20practice%2C%20to%20as%20many%20as%2040.%0AConsequently%2C%20there%20is%20a%20significant%20enhancement%20in%20perception%20performance.%20For%0Ainstance%2C%20in%20nuScenes%203D%20object%20detection%20and%20BEV%20map%20segmentation%20tasks%2C%20our%0Apruning%20strategy%20improves%20the%20vanilla%20TransL%20baseline%20and%20other%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Perception%2520on%2520Multi-Sweep%2520Point%2520Cloud%2520with%2520Gumbel%2520Spatial%250A%2520%2520Pruning%26entry.906535625%3DJianhao%2520Li%2520and%2520Tianyu%2520Sun%2520and%2520Xueqian%2520Zhang%2520and%2520Zhongdao%2520Wang%2520and%2520Bailan%2520Feng%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520point%2520cloud%2520perception%2520within%2520outdoor%2520environments.%250AExisting%2520methods%2520face%2520limitations%2520in%2520recognizing%2520objects%2520located%2520at%2520a%2520distance%250Aor%2520occluded%252C%2520due%2520to%2520the%2520sparse%2520nature%2520of%2520outdoor%2520point%2520clouds.%2520In%2520this%2520work%252C%2520we%250Aobserve%2520a%2520significant%2520mitigation%2520of%2520this%2520problem%2520by%2520accumulating%2520multiple%250Atemporally%2520consecutive%2520LiDAR%2520sweeps%252C%2520resulting%2520in%2520a%2520remarkable%2520improvement%2520in%250Aperception%2520accuracy.%2520However%252C%2520the%2520computation%2520cost%2520also%2520increases%252C%2520hindering%250Aprevious%2520approaches%2520from%2520utilizing%2520a%2520large%2520number%2520of%2520LiDAR%2520sweeps.%2520To%2520tackle%250Athis%2520challenge%252C%2520we%2520find%2520that%2520a%2520considerable%2520portion%2520of%2520points%2520in%2520the%250Aaccumulated%2520point%2520cloud%2520is%2520redundant%252C%2520and%2520discarding%2520these%2520points%2520has%2520minimal%250Aimpact%2520on%2520perception%2520accuracy.%2520We%2520introduce%2520a%2520simple%2520yet%2520effective%2520Gumbel%250ASpatial%2520Pruning%2520%2528GSP%2529%2520layer%2520that%2520dynamically%2520prunes%2520points%2520based%2520on%2520a%2520learned%250Aend-to-end%2520sampling.%2520The%2520GSP%2520layer%2520is%2520decoupled%2520from%2520other%2520network%2520components%250Aand%2520thus%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520point%2520cloud%2520network%250Aarchitectures.%2520Without%2520incurring%2520additional%2520computational%2520overhead%252C%2520we%2520increase%250Athe%2520number%2520of%2520LiDAR%2520sweeps%2520from%252010%252C%2520a%2520common%2520practice%252C%2520to%2520as%2520many%2520as%252040.%250AConsequently%252C%2520there%2520is%2520a%2520significant%2520enhancement%2520in%2520perception%2520performance.%2520For%250Ainstance%252C%2520in%2520nuScenes%25203D%2520object%2520detection%2520and%2520BEV%2520map%2520segmentation%2520tasks%252C%2520our%250Apruning%2520strategy%2520improves%2520the%2520vanilla%2520TransL%2520baseline%2520and%2520other%2520baseline%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Perception%20on%20Multi-Sweep%20Point%20Cloud%20with%20Gumbel%20Spatial%0A%20%20Pruning&entry.906535625=Jianhao%20Li%20and%20Tianyu%20Sun%20and%20Xueqian%20Zhang%20and%20Zhongdao%20Wang%20and%20Bailan%20Feng%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20This%20paper%20studies%20point%20cloud%20perception%20within%20outdoor%20environments.%0AExisting%20methods%20face%20limitations%20in%20recognizing%20objects%20located%20at%20a%20distance%0Aor%20occluded%2C%20due%20to%20the%20sparse%20nature%20of%20outdoor%20point%20clouds.%20In%20this%20work%2C%20we%0Aobserve%20a%20significant%20mitigation%20of%20this%20problem%20by%20accumulating%20multiple%0Atemporally%20consecutive%20LiDAR%20sweeps%2C%20resulting%20in%20a%20remarkable%20improvement%20in%0Aperception%20accuracy.%20However%2C%20the%20computation%20cost%20also%20increases%2C%20hindering%0Aprevious%20approaches%20from%20utilizing%20a%20large%20number%20of%20LiDAR%20sweeps.%20To%20tackle%0Athis%20challenge%2C%20we%20find%20that%20a%20considerable%20portion%20of%20points%20in%20the%0Aaccumulated%20point%20cloud%20is%20redundant%2C%20and%20discarding%20these%20points%20has%20minimal%0Aimpact%20on%20perception%20accuracy.%20We%20introduce%20a%20simple%20yet%20effective%20Gumbel%0ASpatial%20Pruning%20%28GSP%29%20layer%20that%20dynamically%20prunes%20points%20based%20on%20a%20learned%0Aend-to-end%20sampling.%20The%20GSP%20layer%20is%20decoupled%20from%20other%20network%20components%0Aand%20thus%20can%20be%20seamlessly%20integrated%20into%20existing%20point%20cloud%20network%0Aarchitectures.%20Without%20incurring%20additional%20computational%20overhead%2C%20we%20increase%0Athe%20number%20of%20LiDAR%20sweeps%20from%2010%2C%20a%20common%20practice%2C%20to%20as%20many%20as%2040.%0AConsequently%2C%20there%20is%20a%20significant%20enhancement%20in%20perception%20performance.%20For%0Ainstance%2C%20in%20nuScenes%203D%20object%20detection%20and%20BEV%20map%20segmentation%20tasks%2C%20our%0Apruning%20strategy%20improves%20the%20vanilla%20TransL%20baseline%20and%20other%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07742v1&entry.124074799=Read"},
{"title": "DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with\n  Generative Adversarial Networks", "author": "Zhaoxi Zhang and Yueliang Ying", "abstract": "  Computed tomography (CT) provides highly detailed three-dimensional (3D)\nmedical images but is costly, time-consuming, and often inaccessible in\nintraoperative settings (Organization et al. 2011). Recent advancements have\nexplored reconstructing 3D chest volumes from sparse 2D X-rays, such as\nsingle-view or orthogonal double-view images. However, current models tend to\nprocess 2D images in a planar manner, prioritizing visual realism over\nstructural accuracy. In this work, we introduce DuoLift Generative Adversarial\nNetworks (DuoLift-GAN), a novel architecture with dual branches that\nindependently elevate 2D images and their features into 3D representations.\nThese 3D outputs are merged into a unified 3D feature map and decoded into a\ncomplete 3D chest volume, enabling richer 3D information capture. We also\npresent a masked loss function that directs reconstruction towards critical\nanatomical regions, improving structural accuracy and visual quality. This\npaper demonstrates that DuoLift-GAN significantly enhances reconstruction\naccuracy while achieving superior visual realism compared to existing methods.\n", "link": "http://arxiv.org/abs/2411.07941v1", "date": "2024-11-12", "relevancy": 2.9046, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks&body=Title%3A%20DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Zhaoxi%20Zhang%20and%20Yueliang%20Ying%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20provides%20highly%20detailed%20three-dimensional%20%283D%29%0Amedical%20images%20but%20is%20costly%2C%20time-consuming%2C%20and%20often%20inaccessible%20in%0Aintraoperative%20settings%20%28Organization%20et%20al.%202011%29.%20Recent%20advancements%20have%0Aexplored%20reconstructing%203D%20chest%20volumes%20from%20sparse%202D%20X-rays%2C%20such%20as%0Asingle-view%20or%20orthogonal%20double-view%20images.%20However%2C%20current%20models%20tend%20to%0Aprocess%202D%20images%20in%20a%20planar%20manner%2C%20prioritizing%20visual%20realism%20over%0Astructural%20accuracy.%20In%20this%20work%2C%20we%20introduce%20DuoLift%20Generative%20Adversarial%0ANetworks%20%28DuoLift-GAN%29%2C%20a%20novel%20architecture%20with%20dual%20branches%20that%0Aindependently%20elevate%202D%20images%20and%20their%20features%20into%203D%20representations.%0AThese%203D%20outputs%20are%20merged%20into%20a%20unified%203D%20feature%20map%20and%20decoded%20into%20a%0Acomplete%203D%20chest%20volume%2C%20enabling%20richer%203D%20information%20capture.%20We%20also%0Apresent%20a%20masked%20loss%20function%20that%20directs%20reconstruction%20towards%20critical%0Aanatomical%20regions%2C%20improving%20structural%20accuracy%20and%20visual%20quality.%20This%0Apaper%20demonstrates%20that%20DuoLift-GAN%20significantly%20enhances%20reconstruction%0Aaccuracy%20while%20achieving%20superior%20visual%20realism%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuoLift-GAN%253AReconstructing%2520CT%2520from%2520Single-view%2520and%2520Biplanar%2520X-Rays%2520with%250A%2520%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DZhaoxi%2520Zhang%2520and%2520Yueliang%2520Ying%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520provides%2520highly%2520detailed%2520three-dimensional%2520%25283D%2529%250Amedical%2520images%2520but%2520is%2520costly%252C%2520time-consuming%252C%2520and%2520often%2520inaccessible%2520in%250Aintraoperative%2520settings%2520%2528Organization%2520et%2520al.%25202011%2529.%2520Recent%2520advancements%2520have%250Aexplored%2520reconstructing%25203D%2520chest%2520volumes%2520from%2520sparse%25202D%2520X-rays%252C%2520such%2520as%250Asingle-view%2520or%2520orthogonal%2520double-view%2520images.%2520However%252C%2520current%2520models%2520tend%2520to%250Aprocess%25202D%2520images%2520in%2520a%2520planar%2520manner%252C%2520prioritizing%2520visual%2520realism%2520over%250Astructural%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DuoLift%2520Generative%2520Adversarial%250ANetworks%2520%2528DuoLift-GAN%2529%252C%2520a%2520novel%2520architecture%2520with%2520dual%2520branches%2520that%250Aindependently%2520elevate%25202D%2520images%2520and%2520their%2520features%2520into%25203D%2520representations.%250AThese%25203D%2520outputs%2520are%2520merged%2520into%2520a%2520unified%25203D%2520feature%2520map%2520and%2520decoded%2520into%2520a%250Acomplete%25203D%2520chest%2520volume%252C%2520enabling%2520richer%25203D%2520information%2520capture.%2520We%2520also%250Apresent%2520a%2520masked%2520loss%2520function%2520that%2520directs%2520reconstruction%2520towards%2520critical%250Aanatomical%2520regions%252C%2520improving%2520structural%2520accuracy%2520and%2520visual%2520quality.%2520This%250Apaper%2520demonstrates%2520that%2520DuoLift-GAN%2520significantly%2520enhances%2520reconstruction%250Aaccuracy%2520while%2520achieving%2520superior%2520visual%2520realism%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks&entry.906535625=Zhaoxi%20Zhang%20and%20Yueliang%20Ying&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20provides%20highly%20detailed%20three-dimensional%20%283D%29%0Amedical%20images%20but%20is%20costly%2C%20time-consuming%2C%20and%20often%20inaccessible%20in%0Aintraoperative%20settings%20%28Organization%20et%20al.%202011%29.%20Recent%20advancements%20have%0Aexplored%20reconstructing%203D%20chest%20volumes%20from%20sparse%202D%20X-rays%2C%20such%20as%0Asingle-view%20or%20orthogonal%20double-view%20images.%20However%2C%20current%20models%20tend%20to%0Aprocess%202D%20images%20in%20a%20planar%20manner%2C%20prioritizing%20visual%20realism%20over%0Astructural%20accuracy.%20In%20this%20work%2C%20we%20introduce%20DuoLift%20Generative%20Adversarial%0ANetworks%20%28DuoLift-GAN%29%2C%20a%20novel%20architecture%20with%20dual%20branches%20that%0Aindependently%20elevate%202D%20images%20and%20their%20features%20into%203D%20representations.%0AThese%203D%20outputs%20are%20merged%20into%20a%20unified%203D%20feature%20map%20and%20decoded%20into%20a%0Acomplete%203D%20chest%20volume%2C%20enabling%20richer%203D%20information%20capture.%20We%20also%0Apresent%20a%20masked%20loss%20function%20that%20directs%20reconstruction%20towards%20critical%0Aanatomical%20regions%2C%20improving%20structural%20accuracy%20and%20visual%20quality.%20This%0Apaper%20demonstrates%20that%20DuoLift-GAN%20significantly%20enhances%20reconstruction%0Aaccuracy%20while%20achieving%20superior%20visual%20realism%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07941v1&entry.124074799=Read"},
{"title": "Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification", "author": "Zhaorui Tan and Xi Yang and Qiufeng Wang and Anh Nguyen and Kaizhu Huang", "abstract": "  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n", "link": "http://arxiv.org/abs/2410.04492v4", "date": "2024-11-12", "relevancy": 2.7781, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification&body=Title%3A%20Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification%0AAuthor%3A%20Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20Vision%20models%20excel%20in%20image%20classification%20but%20struggle%20to%20generalize%20to%0Aunseen%20data%2C%20such%20as%20classifying%20images%20from%20unseen%20domains%20or%20discovering%0Anovel%20categories.%20In%20this%20paper%2C%20we%20explore%20the%20relationship%20between%20logical%0Areasoning%20and%20deep%20learning%20generalization%20in%20visual%20classification.%20A%20logical%0Aregularization%20termed%20L-Reg%20is%20derived%20which%20bridges%20a%20logical%20analysis%0Aframework%20to%20image%20classification.%20Our%20work%20reveals%20that%20L-Reg%20reduces%20the%0Acomplexity%20of%20the%20model%20in%20terms%20of%20the%20feature%20distribution%20and%20classifier%0Aweights.%20Specifically%2C%20we%20unveil%20the%20interpretability%20brought%20by%20L-Reg%2C%20as%20it%0Aenables%20the%20model%20to%20extract%20the%20salient%20features%2C%20such%20as%20faces%20to%20persons%2C%0Afor%20classification.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20L-Reg%0Aenhances%20generalization%20across%20various%20scenarios%2C%20including%20multi-domain%0Ageneralization%20and%20generalized%20category%20discovery.%20In%20complex%20real-world%0Ascenarios%20where%20images%20span%20unknown%20classes%20and%20unseen%20domains%2C%20L-Reg%0Aconsistently%20improves%20generalization%2C%20highlighting%20its%20practical%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04492v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpret%2520Your%2520Decision%253A%2520Logical%2520Reasoning%2520Regularization%2520for%250A%2520%2520Generalization%2520in%2520Visual%2520Classification%26entry.906535625%3DZhaorui%2520Tan%2520and%2520Xi%2520Yang%2520and%2520Qiufeng%2520Wang%2520and%2520Anh%2520Nguyen%2520and%2520Kaizhu%2520Huang%26entry.1292438233%3D%2520%2520Vision%2520models%2520excel%2520in%2520image%2520classification%2520but%2520struggle%2520to%2520generalize%2520to%250Aunseen%2520data%252C%2520such%2520as%2520classifying%2520images%2520from%2520unseen%2520domains%2520or%2520discovering%250Anovel%2520categories.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520relationship%2520between%2520logical%250Areasoning%2520and%2520deep%2520learning%2520generalization%2520in%2520visual%2520classification.%2520A%2520logical%250Aregularization%2520termed%2520L-Reg%2520is%2520derived%2520which%2520bridges%2520a%2520logical%2520analysis%250Aframework%2520to%2520image%2520classification.%2520Our%2520work%2520reveals%2520that%2520L-Reg%2520reduces%2520the%250Acomplexity%2520of%2520the%2520model%2520in%2520terms%2520of%2520the%2520feature%2520distribution%2520and%2520classifier%250Aweights.%2520Specifically%252C%2520we%2520unveil%2520the%2520interpretability%2520brought%2520by%2520L-Reg%252C%2520as%2520it%250Aenables%2520the%2520model%2520to%2520extract%2520the%2520salient%2520features%252C%2520such%2520as%2520faces%2520to%2520persons%252C%250Afor%2520classification.%2520Theoretical%2520analysis%2520and%2520experiments%2520demonstrate%2520that%2520L-Reg%250Aenhances%2520generalization%2520across%2520various%2520scenarios%252C%2520including%2520multi-domain%250Ageneralization%2520and%2520generalized%2520category%2520discovery.%2520In%2520complex%2520real-world%250Ascenarios%2520where%2520images%2520span%2520unknown%2520classes%2520and%2520unseen%2520domains%252C%2520L-Reg%250Aconsistently%2520improves%2520generalization%252C%2520highlighting%2520its%2520practical%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04492v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification&entry.906535625=Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Kaizhu%20Huang&entry.1292438233=%20%20Vision%20models%20excel%20in%20image%20classification%20but%20struggle%20to%20generalize%20to%0Aunseen%20data%2C%20such%20as%20classifying%20images%20from%20unseen%20domains%20or%20discovering%0Anovel%20categories.%20In%20this%20paper%2C%20we%20explore%20the%20relationship%20between%20logical%0Areasoning%20and%20deep%20learning%20generalization%20in%20visual%20classification.%20A%20logical%0Aregularization%20termed%20L-Reg%20is%20derived%20which%20bridges%20a%20logical%20analysis%0Aframework%20to%20image%20classification.%20Our%20work%20reveals%20that%20L-Reg%20reduces%20the%0Acomplexity%20of%20the%20model%20in%20terms%20of%20the%20feature%20distribution%20and%20classifier%0Aweights.%20Specifically%2C%20we%20unveil%20the%20interpretability%20brought%20by%20L-Reg%2C%20as%20it%0Aenables%20the%20model%20to%20extract%20the%20salient%20features%2C%20such%20as%20faces%20to%20persons%2C%0Afor%20classification.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20L-Reg%0Aenhances%20generalization%20across%20various%20scenarios%2C%20including%20multi-domain%0Ageneralization%20and%20generalized%20category%20discovery.%20In%20complex%20real-world%0Ascenarios%20where%20images%20span%20unknown%20classes%20and%20unseen%20domains%2C%20L-Reg%0Aconsistently%20improves%20generalization%2C%20highlighting%20its%20practical%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04492v4&entry.124074799=Read"},
{"title": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI", "author": "Bruno Viti and Franz Thaler and Kathrin Lisa Kapper and Martin Urschler and Martin Holler and Elias Karabelas", "abstract": "  Segmentation of cardiac magnetic resonance images (MRI) is crucial for the\nanalysis and assessment of cardiac function, helping to diagnose and treat\nvarious cardiovascular diseases. Most recent techniques rely on deep learning\nand usually require an extensive amount of labeled data. To overcome this\nproblem, few-shot learning has the capability of reducing data dependency on\nlabeled data. In this work, we introduce a new method that merges few-shot\nlearning with a U-Net architecture and Gaussian Process Emulators (GPEs),\nenhancing data integration from a support set for improved performance. GPEs\nare trained to learn the relation between the support images and the\ncorresponding masks in latent space, facilitating the segmentation of unseen\nquery images given only a small labeled support set at inference. We test our\nmodel with the M&Ms-2 public dataset to assess its ability to segment the heart\nin cardiac magnetic resonance imaging from different orientations, and compare\nit with state-of-the-art unsupervised and few-shot methods. Our architecture\nshows higher DICE coefficients compared to these methods, especially in the\nmore challenging setups where the size of the support set is considerably\nsmall.\n", "link": "http://arxiv.org/abs/2411.06911v2", "date": "2024-11-12", "relevancy": 2.7671, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5739}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5495}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI&body=Title%3A%20Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI%0AAuthor%3A%20Bruno%20Viti%20and%20Franz%20Thaler%20and%20Kathrin%20Lisa%20Kapper%20and%20Martin%20Urschler%20and%20Martin%20Holler%20and%20Elias%20Karabelas%0AAbstract%3A%20%20%20Segmentation%20of%20cardiac%20magnetic%20resonance%20images%20%28MRI%29%20is%20crucial%20for%20the%0Aanalysis%20and%20assessment%20of%20cardiac%20function%2C%20helping%20to%20diagnose%20and%20treat%0Avarious%20cardiovascular%20diseases.%20Most%20recent%20techniques%20rely%20on%20deep%20learning%0Aand%20usually%20require%20an%20extensive%20amount%20of%20labeled%20data.%20To%20overcome%20this%0Aproblem%2C%20few-shot%20learning%20has%20the%20capability%20of%20reducing%20data%20dependency%20on%0Alabeled%20data.%20In%20this%20work%2C%20we%20introduce%20a%20new%20method%20that%20merges%20few-shot%0Alearning%20with%20a%20U-Net%20architecture%20and%20Gaussian%20Process%20Emulators%20%28GPEs%29%2C%0Aenhancing%20data%20integration%20from%20a%20support%20set%20for%20improved%20performance.%20GPEs%0Aare%20trained%20to%20learn%20the%20relation%20between%20the%20support%20images%20and%20the%0Acorresponding%20masks%20in%20latent%20space%2C%20facilitating%20the%20segmentation%20of%20unseen%0Aquery%20images%20given%20only%20a%20small%20labeled%20support%20set%20at%20inference.%20We%20test%20our%0Amodel%20with%20the%20M%26Ms-2%20public%20dataset%20to%20assess%20its%20ability%20to%20segment%20the%20heart%0Ain%20cardiac%20magnetic%20resonance%20imaging%20from%20different%20orientations%2C%20and%20compare%0Ait%20with%20state-of-the-art%20unsupervised%20and%20few-shot%20methods.%20Our%20architecture%0Ashows%20higher%20DICE%20coefficients%20compared%20to%20these%20methods%2C%20especially%20in%20the%0Amore%20challenging%20setups%20where%20the%20size%20of%20the%20support%20set%20is%20considerably%0Asmall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Emulators%2520for%2520Few-Shot%2520Segmentation%2520in%2520Cardiac%2520MRI%26entry.906535625%3DBruno%2520Viti%2520and%2520Franz%2520Thaler%2520and%2520Kathrin%2520Lisa%2520Kapper%2520and%2520Martin%2520Urschler%2520and%2520Martin%2520Holler%2520and%2520Elias%2520Karabelas%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520cardiac%2520magnetic%2520resonance%2520images%2520%2528MRI%2529%2520is%2520crucial%2520for%2520the%250Aanalysis%2520and%2520assessment%2520of%2520cardiac%2520function%252C%2520helping%2520to%2520diagnose%2520and%2520treat%250Avarious%2520cardiovascular%2520diseases.%2520Most%2520recent%2520techniques%2520rely%2520on%2520deep%2520learning%250Aand%2520usually%2520require%2520an%2520extensive%2520amount%2520of%2520labeled%2520data.%2520To%2520overcome%2520this%250Aproblem%252C%2520few-shot%2520learning%2520has%2520the%2520capability%2520of%2520reducing%2520data%2520dependency%2520on%250Alabeled%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520method%2520that%2520merges%2520few-shot%250Alearning%2520with%2520a%2520U-Net%2520architecture%2520and%2520Gaussian%2520Process%2520Emulators%2520%2528GPEs%2529%252C%250Aenhancing%2520data%2520integration%2520from%2520a%2520support%2520set%2520for%2520improved%2520performance.%2520GPEs%250Aare%2520trained%2520to%2520learn%2520the%2520relation%2520between%2520the%2520support%2520images%2520and%2520the%250Acorresponding%2520masks%2520in%2520latent%2520space%252C%2520facilitating%2520the%2520segmentation%2520of%2520unseen%250Aquery%2520images%2520given%2520only%2520a%2520small%2520labeled%2520support%2520set%2520at%2520inference.%2520We%2520test%2520our%250Amodel%2520with%2520the%2520M%2526Ms-2%2520public%2520dataset%2520to%2520assess%2520its%2520ability%2520to%2520segment%2520the%2520heart%250Ain%2520cardiac%2520magnetic%2520resonance%2520imaging%2520from%2520different%2520orientations%252C%2520and%2520compare%250Ait%2520with%2520state-of-the-art%2520unsupervised%2520and%2520few-shot%2520methods.%2520Our%2520architecture%250Ashows%2520higher%2520DICE%2520coefficients%2520compared%2520to%2520these%2520methods%252C%2520especially%2520in%2520the%250Amore%2520challenging%2520setups%2520where%2520the%2520size%2520of%2520the%2520support%2520set%2520is%2520considerably%250Asmall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Emulators%20for%20Few-Shot%20Segmentation%20in%20Cardiac%20MRI&entry.906535625=Bruno%20Viti%20and%20Franz%20Thaler%20and%20Kathrin%20Lisa%20Kapper%20and%20Martin%20Urschler%20and%20Martin%20Holler%20and%20Elias%20Karabelas&entry.1292438233=%20%20Segmentation%20of%20cardiac%20magnetic%20resonance%20images%20%28MRI%29%20is%20crucial%20for%20the%0Aanalysis%20and%20assessment%20of%20cardiac%20function%2C%20helping%20to%20diagnose%20and%20treat%0Avarious%20cardiovascular%20diseases.%20Most%20recent%20techniques%20rely%20on%20deep%20learning%0Aand%20usually%20require%20an%20extensive%20amount%20of%20labeled%20data.%20To%20overcome%20this%0Aproblem%2C%20few-shot%20learning%20has%20the%20capability%20of%20reducing%20data%20dependency%20on%0Alabeled%20data.%20In%20this%20work%2C%20we%20introduce%20a%20new%20method%20that%20merges%20few-shot%0Alearning%20with%20a%20U-Net%20architecture%20and%20Gaussian%20Process%20Emulators%20%28GPEs%29%2C%0Aenhancing%20data%20integration%20from%20a%20support%20set%20for%20improved%20performance.%20GPEs%0Aare%20trained%20to%20learn%20the%20relation%20between%20the%20support%20images%20and%20the%0Acorresponding%20masks%20in%20latent%20space%2C%20facilitating%20the%20segmentation%20of%20unseen%0Aquery%20images%20given%20only%20a%20small%20labeled%20support%20set%20at%20inference.%20We%20test%20our%0Amodel%20with%20the%20M%26Ms-2%20public%20dataset%20to%20assess%20its%20ability%20to%20segment%20the%20heart%0Ain%20cardiac%20magnetic%20resonance%20imaging%20from%20different%20orientations%2C%20and%20compare%0Ait%20with%20state-of-the-art%20unsupervised%20and%20few-shot%20methods.%20Our%20architecture%0Ashows%20higher%20DICE%20coefficients%20compared%20to%20these%20methods%2C%20especially%20in%20the%0Amore%20challenging%20setups%20where%20the%20size%20of%20the%20support%20set%20is%20considerably%0Asmall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06911v2&entry.124074799=Read"},
{"title": "SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State\n  Space Model", "author": "Xinyuan Qian and Jiaran Gao and Yaodan Zhang and Qiquan Zhang and Hexin Liu and Leibny Paola Garcia and Haizhou Li", "abstract": "  Speech enhancement plays an essential role in various applications, and the\nintegration of visual information has been demonstrated to bring substantial\nadvantages. However, the majority of current research concentrates on the\nexamination of facial and lip movements, which can be compromised or entirely\ninaccessible in scenarios where occlusions occur or when the camera view is\ndistant. Whereas contextual visual cues from the surrounding environment have\nbeen overlooked: for example, when we see a dog bark, our brain has the innate\nability to discern and filter out the barking noise. To this end, in this\npaper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is\nthe first proposal to use rich contextual information from synchronized video\nas auxiliary cues to indicate the type of noise, which eventually improves the\nspeech enhancement performance. Specifically, we propose the VC-S$^2$E method,\nwhich incorporates the Conformer and Mamba modules for their complementary\nstrengths. Extensive experiments are conducted on public MUSIC, AVSpeech and\nAudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E\nover other competitive methods. We will make the source code publicly\navailable. Project demo page: https://AVSEPage.github.io/\n", "link": "http://arxiv.org/abs/2411.07751v1", "date": "2024-11-12", "relevancy": 2.7516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAV-SE%3A%20Scene-aware%20Audio-Visual%20Speech%20Enhancement%20with%20Selective%20State%0A%20%20Space%20Model&body=Title%3A%20SAV-SE%3A%20Scene-aware%20Audio-Visual%20Speech%20Enhancement%20with%20Selective%20State%0A%20%20Space%20Model%0AAuthor%3A%20Xinyuan%20Qian%20and%20Jiaran%20Gao%20and%20Yaodan%20Zhang%20and%20Qiquan%20Zhang%20and%20Hexin%20Liu%20and%20Leibny%20Paola%20Garcia%20and%20Haizhou%20Li%0AAbstract%3A%20%20%20Speech%20enhancement%20plays%20an%20essential%20role%20in%20various%20applications%2C%20and%20the%0Aintegration%20of%20visual%20information%20has%20been%20demonstrated%20to%20bring%20substantial%0Aadvantages.%20However%2C%20the%20majority%20of%20current%20research%20concentrates%20on%20the%0Aexamination%20of%20facial%20and%20lip%20movements%2C%20which%20can%20be%20compromised%20or%20entirely%0Ainaccessible%20in%20scenarios%20where%20occlusions%20occur%20or%20when%20the%20camera%20view%20is%0Adistant.%20Whereas%20contextual%20visual%20cues%20from%20the%20surrounding%20environment%20have%0Abeen%20overlooked%3A%20for%20example%2C%20when%20we%20see%20a%20dog%20bark%2C%20our%20brain%20has%20the%20innate%0Aability%20to%20discern%20and%20filter%20out%20the%20barking%20noise.%20To%20this%20end%2C%20in%20this%0Apaper%2C%20we%20introduce%20a%20novel%20task%2C%20i.e.%20SAV-SE.%20To%20our%20best%20knowledge%2C%20this%20is%0Athe%20first%20proposal%20to%20use%20rich%20contextual%20information%20from%20synchronized%20video%0Aas%20auxiliary%20cues%20to%20indicate%20the%20type%20of%20noise%2C%20which%20eventually%20improves%20the%0Aspeech%20enhancement%20performance.%20Specifically%2C%20we%20propose%20the%20VC-S%24%5E2%24E%20method%2C%0Awhich%20incorporates%20the%20Conformer%20and%20Mamba%20modules%20for%20their%20complementary%0Astrengths.%20Extensive%20experiments%20are%20conducted%20on%20public%20MUSIC%2C%20AVSpeech%20and%0AAudioSet%20datasets%2C%20where%20the%20results%20demonstrate%20the%20superiority%20of%20VC-S%24%5E2%24E%0Aover%20other%20competitive%20methods.%20We%20will%20make%20the%20source%20code%20publicly%0Aavailable.%20Project%20demo%20page%3A%20https%3A//AVSEPage.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAV-SE%253A%2520Scene-aware%2520Audio-Visual%2520Speech%2520Enhancement%2520with%2520Selective%2520State%250A%2520%2520Space%2520Model%26entry.906535625%3DXinyuan%2520Qian%2520and%2520Jiaran%2520Gao%2520and%2520Yaodan%2520Zhang%2520and%2520Qiquan%2520Zhang%2520and%2520Hexin%2520Liu%2520and%2520Leibny%2520Paola%2520Garcia%2520and%2520Haizhou%2520Li%26entry.1292438233%3D%2520%2520Speech%2520enhancement%2520plays%2520an%2520essential%2520role%2520in%2520various%2520applications%252C%2520and%2520the%250Aintegration%2520of%2520visual%2520information%2520has%2520been%2520demonstrated%2520to%2520bring%2520substantial%250Aadvantages.%2520However%252C%2520the%2520majority%2520of%2520current%2520research%2520concentrates%2520on%2520the%250Aexamination%2520of%2520facial%2520and%2520lip%2520movements%252C%2520which%2520can%2520be%2520compromised%2520or%2520entirely%250Ainaccessible%2520in%2520scenarios%2520where%2520occlusions%2520occur%2520or%2520when%2520the%2520camera%2520view%2520is%250Adistant.%2520Whereas%2520contextual%2520visual%2520cues%2520from%2520the%2520surrounding%2520environment%2520have%250Abeen%2520overlooked%253A%2520for%2520example%252C%2520when%2520we%2520see%2520a%2520dog%2520bark%252C%2520our%2520brain%2520has%2520the%2520innate%250Aability%2520to%2520discern%2520and%2520filter%2520out%2520the%2520barking%2520noise.%2520To%2520this%2520end%252C%2520in%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520task%252C%2520i.e.%2520SAV-SE.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520proposal%2520to%2520use%2520rich%2520contextual%2520information%2520from%2520synchronized%2520video%250Aas%2520auxiliary%2520cues%2520to%2520indicate%2520the%2520type%2520of%2520noise%252C%2520which%2520eventually%2520improves%2520the%250Aspeech%2520enhancement%2520performance.%2520Specifically%252C%2520we%2520propose%2520the%2520VC-S%2524%255E2%2524E%2520method%252C%250Awhich%2520incorporates%2520the%2520Conformer%2520and%2520Mamba%2520modules%2520for%2520their%2520complementary%250Astrengths.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520public%2520MUSIC%252C%2520AVSpeech%2520and%250AAudioSet%2520datasets%252C%2520where%2520the%2520results%2520demonstrate%2520the%2520superiority%2520of%2520VC-S%2524%255E2%2524E%250Aover%2520other%2520competitive%2520methods.%2520We%2520will%2520make%2520the%2520source%2520code%2520publicly%250Aavailable.%2520Project%2520demo%2520page%253A%2520https%253A//AVSEPage.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAV-SE%3A%20Scene-aware%20Audio-Visual%20Speech%20Enhancement%20with%20Selective%20State%0A%20%20Space%20Model&entry.906535625=Xinyuan%20Qian%20and%20Jiaran%20Gao%20and%20Yaodan%20Zhang%20and%20Qiquan%20Zhang%20and%20Hexin%20Liu%20and%20Leibny%20Paola%20Garcia%20and%20Haizhou%20Li&entry.1292438233=%20%20Speech%20enhancement%20plays%20an%20essential%20role%20in%20various%20applications%2C%20and%20the%0Aintegration%20of%20visual%20information%20has%20been%20demonstrated%20to%20bring%20substantial%0Aadvantages.%20However%2C%20the%20majority%20of%20current%20research%20concentrates%20on%20the%0Aexamination%20of%20facial%20and%20lip%20movements%2C%20which%20can%20be%20compromised%20or%20entirely%0Ainaccessible%20in%20scenarios%20where%20occlusions%20occur%20or%20when%20the%20camera%20view%20is%0Adistant.%20Whereas%20contextual%20visual%20cues%20from%20the%20surrounding%20environment%20have%0Abeen%20overlooked%3A%20for%20example%2C%20when%20we%20see%20a%20dog%20bark%2C%20our%20brain%20has%20the%20innate%0Aability%20to%20discern%20and%20filter%20out%20the%20barking%20noise.%20To%20this%20end%2C%20in%20this%0Apaper%2C%20we%20introduce%20a%20novel%20task%2C%20i.e.%20SAV-SE.%20To%20our%20best%20knowledge%2C%20this%20is%0Athe%20first%20proposal%20to%20use%20rich%20contextual%20information%20from%20synchronized%20video%0Aas%20auxiliary%20cues%20to%20indicate%20the%20type%20of%20noise%2C%20which%20eventually%20improves%20the%0Aspeech%20enhancement%20performance.%20Specifically%2C%20we%20propose%20the%20VC-S%24%5E2%24E%20method%2C%0Awhich%20incorporates%20the%20Conformer%20and%20Mamba%20modules%20for%20their%20complementary%0Astrengths.%20Extensive%20experiments%20are%20conducted%20on%20public%20MUSIC%2C%20AVSpeech%20and%0AAudioSet%20datasets%2C%20where%20the%20results%20demonstrate%20the%20superiority%20of%20VC-S%24%5E2%24E%0Aover%20other%20competitive%20methods.%20We%20will%20make%20the%20source%20code%20publicly%0Aavailable.%20Project%20demo%20page%3A%20https%3A//AVSEPage.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07751v1&entry.124074799=Read"},
{"title": "Fast Disentangled Slim Tensor Learning for Multi-view Clustering", "author": "Deng Xu and Chao Zhang and Zechao Li and Chunlin Chen and Huaxiong Li", "abstract": "  Tensor-based multi-view clustering has recently received significant\nattention due to its exceptional ability to explore cross-view high-order\ncorrelations. However, most existing methods still encounter some limitations.\n(1) Most of them explore the correlations among different affinity matrices,\nmaking them unscalable to large-scale data. (2) Although some methods address\nit by introducing bipartite graphs, they may result in sub-optimal solutions\ncaused by an unstable anchor selection process. (3) They generally ignore the\nnegative impact of latent semantic-unrelated information in each view. To\ntackle these issues, we propose a new approach termed fast Disentangled Slim\nTensor Learning (DSTL) for multi-view clustering . Instead of focusing on the\nmulti-view graph structures, DSTL directly explores the high-order correlations\namong multi-view latent semantic representations based on matrix factorization.\nTo alleviate the negative influence of feature redundancy, inspired by robust\nPCA, DSTL disentangles the latent low-dimensional representation into a\nsemantic-unrelated part and a semantic-related part for each view.\nSubsequently, two slim tensors are constructed with tensor-based\nregularization. To further enhance the quality of feature disentanglement, the\nsemantic-related representations are aligned across views through a consensus\nalignment indicator. Our proposed model is computationally efficient and can be\nsolved effectively. Extensive experiments demonstrate the superiority and\nefficiency of DSTL over state-of-the-art approaches. The code of DSTL is\navailable at https://github.com/dengxu-nju/DSTL.\n", "link": "http://arxiv.org/abs/2411.07685v1", "date": "2024-11-12", "relevancy": 2.7316, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5214}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Disentangled%20Slim%20Tensor%20Learning%20for%20Multi-view%20Clustering&body=Title%3A%20Fast%20Disentangled%20Slim%20Tensor%20Learning%20for%20Multi-view%20Clustering%0AAuthor%3A%20Deng%20Xu%20and%20Chao%20Zhang%20and%20Zechao%20Li%20and%20Chunlin%20Chen%20and%20Huaxiong%20Li%0AAbstract%3A%20%20%20Tensor-based%20multi-view%20clustering%20has%20recently%20received%20significant%0Aattention%20due%20to%20its%20exceptional%20ability%20to%20explore%20cross-view%20high-order%0Acorrelations.%20However%2C%20most%20existing%20methods%20still%20encounter%20some%20limitations.%0A%281%29%20Most%20of%20them%20explore%20the%20correlations%20among%20different%20affinity%20matrices%2C%0Amaking%20them%20unscalable%20to%20large-scale%20data.%20%282%29%20Although%20some%20methods%20address%0Ait%20by%20introducing%20bipartite%20graphs%2C%20they%20may%20result%20in%20sub-optimal%20solutions%0Acaused%20by%20an%20unstable%20anchor%20selection%20process.%20%283%29%20They%20generally%20ignore%20the%0Anegative%20impact%20of%20latent%20semantic-unrelated%20information%20in%20each%20view.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20new%20approach%20termed%20fast%20Disentangled%20Slim%0ATensor%20Learning%20%28DSTL%29%20for%20multi-view%20clustering%20.%20Instead%20of%20focusing%20on%20the%0Amulti-view%20graph%20structures%2C%20DSTL%20directly%20explores%20the%20high-order%20correlations%0Aamong%20multi-view%20latent%20semantic%20representations%20based%20on%20matrix%20factorization.%0ATo%20alleviate%20the%20negative%20influence%20of%20feature%20redundancy%2C%20inspired%20by%20robust%0APCA%2C%20DSTL%20disentangles%20the%20latent%20low-dimensional%20representation%20into%20a%0Asemantic-unrelated%20part%20and%20a%20semantic-related%20part%20for%20each%20view.%0ASubsequently%2C%20two%20slim%20tensors%20are%20constructed%20with%20tensor-based%0Aregularization.%20To%20further%20enhance%20the%20quality%20of%20feature%20disentanglement%2C%20the%0Asemantic-related%20representations%20are%20aligned%20across%20views%20through%20a%20consensus%0Aalignment%20indicator.%20Our%20proposed%20model%20is%20computationally%20efficient%20and%20can%20be%0Asolved%20effectively.%20Extensive%20experiments%20demonstrate%20the%20superiority%20and%0Aefficiency%20of%20DSTL%20over%20state-of-the-art%20approaches.%20The%20code%20of%20DSTL%20is%0Aavailable%20at%20https%3A//github.com/dengxu-nju/DSTL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Disentangled%2520Slim%2520Tensor%2520Learning%2520for%2520Multi-view%2520Clustering%26entry.906535625%3DDeng%2520Xu%2520and%2520Chao%2520Zhang%2520and%2520Zechao%2520Li%2520and%2520Chunlin%2520Chen%2520and%2520Huaxiong%2520Li%26entry.1292438233%3D%2520%2520Tensor-based%2520multi-view%2520clustering%2520has%2520recently%2520received%2520significant%250Aattention%2520due%2520to%2520its%2520exceptional%2520ability%2520to%2520explore%2520cross-view%2520high-order%250Acorrelations.%2520However%252C%2520most%2520existing%2520methods%2520still%2520encounter%2520some%2520limitations.%250A%25281%2529%2520Most%2520of%2520them%2520explore%2520the%2520correlations%2520among%2520different%2520affinity%2520matrices%252C%250Amaking%2520them%2520unscalable%2520to%2520large-scale%2520data.%2520%25282%2529%2520Although%2520some%2520methods%2520address%250Ait%2520by%2520introducing%2520bipartite%2520graphs%252C%2520they%2520may%2520result%2520in%2520sub-optimal%2520solutions%250Acaused%2520by%2520an%2520unstable%2520anchor%2520selection%2520process.%2520%25283%2529%2520They%2520generally%2520ignore%2520the%250Anegative%2520impact%2520of%2520latent%2520semantic-unrelated%2520information%2520in%2520each%2520view.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520new%2520approach%2520termed%2520fast%2520Disentangled%2520Slim%250ATensor%2520Learning%2520%2528DSTL%2529%2520for%2520multi-view%2520clustering%2520.%2520Instead%2520of%2520focusing%2520on%2520the%250Amulti-view%2520graph%2520structures%252C%2520DSTL%2520directly%2520explores%2520the%2520high-order%2520correlations%250Aamong%2520multi-view%2520latent%2520semantic%2520representations%2520based%2520on%2520matrix%2520factorization.%250ATo%2520alleviate%2520the%2520negative%2520influence%2520of%2520feature%2520redundancy%252C%2520inspired%2520by%2520robust%250APCA%252C%2520DSTL%2520disentangles%2520the%2520latent%2520low-dimensional%2520representation%2520into%2520a%250Asemantic-unrelated%2520part%2520and%2520a%2520semantic-related%2520part%2520for%2520each%2520view.%250ASubsequently%252C%2520two%2520slim%2520tensors%2520are%2520constructed%2520with%2520tensor-based%250Aregularization.%2520To%2520further%2520enhance%2520the%2520quality%2520of%2520feature%2520disentanglement%252C%2520the%250Asemantic-related%2520representations%2520are%2520aligned%2520across%2520views%2520through%2520a%2520consensus%250Aalignment%2520indicator.%2520Our%2520proposed%2520model%2520is%2520computationally%2520efficient%2520and%2520can%2520be%250Asolved%2520effectively.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520and%250Aefficiency%2520of%2520DSTL%2520over%2520state-of-the-art%2520approaches.%2520The%2520code%2520of%2520DSTL%2520is%250Aavailable%2520at%2520https%253A//github.com/dengxu-nju/DSTL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Disentangled%20Slim%20Tensor%20Learning%20for%20Multi-view%20Clustering&entry.906535625=Deng%20Xu%20and%20Chao%20Zhang%20and%20Zechao%20Li%20and%20Chunlin%20Chen%20and%20Huaxiong%20Li&entry.1292438233=%20%20Tensor-based%20multi-view%20clustering%20has%20recently%20received%20significant%0Aattention%20due%20to%20its%20exceptional%20ability%20to%20explore%20cross-view%20high-order%0Acorrelations.%20However%2C%20most%20existing%20methods%20still%20encounter%20some%20limitations.%0A%281%29%20Most%20of%20them%20explore%20the%20correlations%20among%20different%20affinity%20matrices%2C%0Amaking%20them%20unscalable%20to%20large-scale%20data.%20%282%29%20Although%20some%20methods%20address%0Ait%20by%20introducing%20bipartite%20graphs%2C%20they%20may%20result%20in%20sub-optimal%20solutions%0Acaused%20by%20an%20unstable%20anchor%20selection%20process.%20%283%29%20They%20generally%20ignore%20the%0Anegative%20impact%20of%20latent%20semantic-unrelated%20information%20in%20each%20view.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20new%20approach%20termed%20fast%20Disentangled%20Slim%0ATensor%20Learning%20%28DSTL%29%20for%20multi-view%20clustering%20.%20Instead%20of%20focusing%20on%20the%0Amulti-view%20graph%20structures%2C%20DSTL%20directly%20explores%20the%20high-order%20correlations%0Aamong%20multi-view%20latent%20semantic%20representations%20based%20on%20matrix%20factorization.%0ATo%20alleviate%20the%20negative%20influence%20of%20feature%20redundancy%2C%20inspired%20by%20robust%0APCA%2C%20DSTL%20disentangles%20the%20latent%20low-dimensional%20representation%20into%20a%0Asemantic-unrelated%20part%20and%20a%20semantic-related%20part%20for%20each%20view.%0ASubsequently%2C%20two%20slim%20tensors%20are%20constructed%20with%20tensor-based%0Aregularization.%20To%20further%20enhance%20the%20quality%20of%20feature%20disentanglement%2C%20the%0Asemantic-related%20representations%20are%20aligned%20across%20views%20through%20a%20consensus%0Aalignment%20indicator.%20Our%20proposed%20model%20is%20computationally%20efficient%20and%20can%20be%0Asolved%20effectively.%20Extensive%20experiments%20demonstrate%20the%20superiority%20and%0Aefficiency%20of%20DSTL%20over%20state-of-the-art%20approaches.%20The%20code%20of%20DSTL%20is%0Aavailable%20at%20https%3A//github.com/dengxu-nju/DSTL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07685v1&entry.124074799=Read"},
{"title": "REVEX: A Unified Framework for Removal-Based Explainable Artificial\n  Intelligence in Video", "author": "F. Xavier Gaya-Morey and Jose M. Buades-Rubio and I. Scott MacKenzie and Cristina Manresa-Yee", "abstract": "  We developed REVEX, a removal-based video explanations framework. This work\nextends fine-grained explanation frameworks for computer vision data and adapts\nsix existing techniques to video by adding temporal information and local\nexplanations. The adapted methods were evaluated across networks, datasets,\nimage classes, and evaluation metrics. By decomposing explanation into steps,\nstrengths and weaknesses were revealed in the studied methods, for example, on\npixel clustering and perturbations in the input. Video LIME outperformed other\nmethods with deletion values up to 31\\% lower and insertion up to 30\\% higher,\ndepending on method and network. Video RISE achieved superior performance in\nthe average drop metric, with values 10\\% lower. In contrast,\nlocalization-based metrics revealed low performance across all methods, with\nsignificant variation depending on network. Pointing game accuracy reached\n53\\%, and IoU-based metrics remained below 20\\%. Drawing on the findings across\nXAI methods, we further examine the limitations of the employed XAI evaluation\nmetrics and highlight their suitability in different applications.\n", "link": "http://arxiv.org/abs/2401.11796v2", "date": "2024-11-12", "relevancy": 2.7211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REVEX%3A%20A%20Unified%20Framework%20for%20Removal-Based%20Explainable%20Artificial%0A%20%20Intelligence%20in%20Video&body=Title%3A%20REVEX%3A%20A%20Unified%20Framework%20for%20Removal-Based%20Explainable%20Artificial%0A%20%20Intelligence%20in%20Video%0AAuthor%3A%20F.%20Xavier%20Gaya-Morey%20and%20Jose%20M.%20Buades-Rubio%20and%20I.%20Scott%20MacKenzie%20and%20Cristina%20Manresa-Yee%0AAbstract%3A%20%20%20We%20developed%20REVEX%2C%20a%20removal-based%20video%20explanations%20framework.%20This%20work%0Aextends%20fine-grained%20explanation%20frameworks%20for%20computer%20vision%20data%20and%20adapts%0Asix%20existing%20techniques%20to%20video%20by%20adding%20temporal%20information%20and%20local%0Aexplanations.%20The%20adapted%20methods%20were%20evaluated%20across%20networks%2C%20datasets%2C%0Aimage%20classes%2C%20and%20evaluation%20metrics.%20By%20decomposing%20explanation%20into%20steps%2C%0Astrengths%20and%20weaknesses%20were%20revealed%20in%20the%20studied%20methods%2C%20for%20example%2C%20on%0Apixel%20clustering%20and%20perturbations%20in%20the%20input.%20Video%20LIME%20outperformed%20other%0Amethods%20with%20deletion%20values%20up%20to%2031%5C%25%20lower%20and%20insertion%20up%20to%2030%5C%25%20higher%2C%0Adepending%20on%20method%20and%20network.%20Video%20RISE%20achieved%20superior%20performance%20in%0Athe%20average%20drop%20metric%2C%20with%20values%2010%5C%25%20lower.%20In%20contrast%2C%0Alocalization-based%20metrics%20revealed%20low%20performance%20across%20all%20methods%2C%20with%0Asignificant%20variation%20depending%20on%20network.%20Pointing%20game%20accuracy%20reached%0A53%5C%25%2C%20and%20IoU-based%20metrics%20remained%20below%2020%5C%25.%20Drawing%20on%20the%20findings%20across%0AXAI%20methods%2C%20we%20further%20examine%20the%20limitations%20of%20the%20employed%20XAI%20evaluation%0Ametrics%20and%20highlight%20their%20suitability%20in%20different%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREVEX%253A%2520A%2520Unified%2520Framework%2520for%2520Removal-Based%2520Explainable%2520Artificial%250A%2520%2520Intelligence%2520in%2520Video%26entry.906535625%3DF.%2520Xavier%2520Gaya-Morey%2520and%2520Jose%2520M.%2520Buades-Rubio%2520and%2520I.%2520Scott%2520MacKenzie%2520and%2520Cristina%2520Manresa-Yee%26entry.1292438233%3D%2520%2520We%2520developed%2520REVEX%252C%2520a%2520removal-based%2520video%2520explanations%2520framework.%2520This%2520work%250Aextends%2520fine-grained%2520explanation%2520frameworks%2520for%2520computer%2520vision%2520data%2520and%2520adapts%250Asix%2520existing%2520techniques%2520to%2520video%2520by%2520adding%2520temporal%2520information%2520and%2520local%250Aexplanations.%2520The%2520adapted%2520methods%2520were%2520evaluated%2520across%2520networks%252C%2520datasets%252C%250Aimage%2520classes%252C%2520and%2520evaluation%2520metrics.%2520By%2520decomposing%2520explanation%2520into%2520steps%252C%250Astrengths%2520and%2520weaknesses%2520were%2520revealed%2520in%2520the%2520studied%2520methods%252C%2520for%2520example%252C%2520on%250Apixel%2520clustering%2520and%2520perturbations%2520in%2520the%2520input.%2520Video%2520LIME%2520outperformed%2520other%250Amethods%2520with%2520deletion%2520values%2520up%2520to%252031%255C%2525%2520lower%2520and%2520insertion%2520up%2520to%252030%255C%2525%2520higher%252C%250Adepending%2520on%2520method%2520and%2520network.%2520Video%2520RISE%2520achieved%2520superior%2520performance%2520in%250Athe%2520average%2520drop%2520metric%252C%2520with%2520values%252010%255C%2525%2520lower.%2520In%2520contrast%252C%250Alocalization-based%2520metrics%2520revealed%2520low%2520performance%2520across%2520all%2520methods%252C%2520with%250Asignificant%2520variation%2520depending%2520on%2520network.%2520Pointing%2520game%2520accuracy%2520reached%250A53%255C%2525%252C%2520and%2520IoU-based%2520metrics%2520remained%2520below%252020%255C%2525.%2520Drawing%2520on%2520the%2520findings%2520across%250AXAI%2520methods%252C%2520we%2520further%2520examine%2520the%2520limitations%2520of%2520the%2520employed%2520XAI%2520evaluation%250Ametrics%2520and%2520highlight%2520their%2520suitability%2520in%2520different%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REVEX%3A%20A%20Unified%20Framework%20for%20Removal-Based%20Explainable%20Artificial%0A%20%20Intelligence%20in%20Video&entry.906535625=F.%20Xavier%20Gaya-Morey%20and%20Jose%20M.%20Buades-Rubio%20and%20I.%20Scott%20MacKenzie%20and%20Cristina%20Manresa-Yee&entry.1292438233=%20%20We%20developed%20REVEX%2C%20a%20removal-based%20video%20explanations%20framework.%20This%20work%0Aextends%20fine-grained%20explanation%20frameworks%20for%20computer%20vision%20data%20and%20adapts%0Asix%20existing%20techniques%20to%20video%20by%20adding%20temporal%20information%20and%20local%0Aexplanations.%20The%20adapted%20methods%20were%20evaluated%20across%20networks%2C%20datasets%2C%0Aimage%20classes%2C%20and%20evaluation%20metrics.%20By%20decomposing%20explanation%20into%20steps%2C%0Astrengths%20and%20weaknesses%20were%20revealed%20in%20the%20studied%20methods%2C%20for%20example%2C%20on%0Apixel%20clustering%20and%20perturbations%20in%20the%20input.%20Video%20LIME%20outperformed%20other%0Amethods%20with%20deletion%20values%20up%20to%2031%5C%25%20lower%20and%20insertion%20up%20to%2030%5C%25%20higher%2C%0Adepending%20on%20method%20and%20network.%20Video%20RISE%20achieved%20superior%20performance%20in%0Athe%20average%20drop%20metric%2C%20with%20values%2010%5C%25%20lower.%20In%20contrast%2C%0Alocalization-based%20metrics%20revealed%20low%20performance%20across%20all%20methods%2C%20with%0Asignificant%20variation%20depending%20on%20network.%20Pointing%20game%20accuracy%20reached%0A53%5C%25%2C%20and%20IoU-based%20metrics%20remained%20below%2020%5C%25.%20Drawing%20on%20the%20findings%20across%0AXAI%20methods%2C%20we%20further%20examine%20the%20limitations%20of%20the%20employed%20XAI%20evaluation%0Ametrics%20and%20highlight%20their%20suitability%20in%20different%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11796v2&entry.124074799=Read"},
{"title": "INTRABENCH: Interactive Radiological Benchmark", "author": "Constantin Ulrich and Tassilo Wald and Emily Tempus and Maximilian Rokuss and Paul F. Jaeger and Klaus Maier-Hein", "abstract": "  Current interactive segmentation approaches, inspired by the success of\nMETA's Segment Anything model, have achieved notable advancements, however,\nthey come with substantial limitations that hinder their practical application\nin real clinical scenarios. These include unrealistic human interaction\nrequirements, such as slice-by-slice operations for 2D models on 3D data, a\nlack of iterative refinement, and insufficient evaluation experiments. These\nshortcomings prevent accurate assessment of model performance and lead to\ninconsistent outcomes across studies. IntRaBench overcomes these challenges by\noffering a comprehensive and reproducible framework for evaluating interactive\nsegmentation methods in realistic, clinically relevant scenarios. It includes\ndiverse datasets, target structures, and segmentation models, and provides a\nflexible codebase that allows seamless integration of new models and prompting\nstrategies. Additionally, we introduce advanced techniques to minimize\nclinician interaction, ensuring fair comparisons between 2D and 3D models. By\nopen-sourcing IntRaBench, we invite the research community to integrate their\nmodels and prompting techniques, ensuring continuous and transparent evaluation\nof interactive segmentation models in 3D medical imaging.\n", "link": "http://arxiv.org/abs/2411.07885v1", "date": "2024-11-12", "relevancy": 2.6864, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5465}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INTRABENCH%3A%20Interactive%20Radiological%20Benchmark&body=Title%3A%20INTRABENCH%3A%20Interactive%20Radiological%20Benchmark%0AAuthor%3A%20Constantin%20Ulrich%20and%20Tassilo%20Wald%20and%20Emily%20Tempus%20and%20Maximilian%20Rokuss%20and%20Paul%20F.%20Jaeger%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Current%20interactive%20segmentation%20approaches%2C%20inspired%20by%20the%20success%20of%0AMETA%27s%20Segment%20Anything%20model%2C%20have%20achieved%20notable%20advancements%2C%20however%2C%0Athey%20come%20with%20substantial%20limitations%20that%20hinder%20their%20practical%20application%0Ain%20real%20clinical%20scenarios.%20These%20include%20unrealistic%20human%20interaction%0Arequirements%2C%20such%20as%20slice-by-slice%20operations%20for%202D%20models%20on%203D%20data%2C%20a%0Alack%20of%20iterative%20refinement%2C%20and%20insufficient%20evaluation%20experiments.%20These%0Ashortcomings%20prevent%20accurate%20assessment%20of%20model%20performance%20and%20lead%20to%0Ainconsistent%20outcomes%20across%20studies.%20IntRaBench%20overcomes%20these%20challenges%20by%0Aoffering%20a%20comprehensive%20and%20reproducible%20framework%20for%20evaluating%20interactive%0Asegmentation%20methods%20in%20realistic%2C%20clinically%20relevant%20scenarios.%20It%20includes%0Adiverse%20datasets%2C%20target%20structures%2C%20and%20segmentation%20models%2C%20and%20provides%20a%0Aflexible%20codebase%20that%20allows%20seamless%20integration%20of%20new%20models%20and%20prompting%0Astrategies.%20Additionally%2C%20we%20introduce%20advanced%20techniques%20to%20minimize%0Aclinician%20interaction%2C%20ensuring%20fair%20comparisons%20between%202D%20and%203D%20models.%20By%0Aopen-sourcing%20IntRaBench%2C%20we%20invite%20the%20research%20community%20to%20integrate%20their%0Amodels%20and%20prompting%20techniques%2C%20ensuring%20continuous%20and%20transparent%20evaluation%0Aof%20interactive%20segmentation%20models%20in%203D%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINTRABENCH%253A%2520Interactive%2520Radiological%2520Benchmark%26entry.906535625%3DConstantin%2520Ulrich%2520and%2520Tassilo%2520Wald%2520and%2520Emily%2520Tempus%2520and%2520Maximilian%2520Rokuss%2520and%2520Paul%2520F.%2520Jaeger%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520Current%2520interactive%2520segmentation%2520approaches%252C%2520inspired%2520by%2520the%2520success%2520of%250AMETA%2527s%2520Segment%2520Anything%2520model%252C%2520have%2520achieved%2520notable%2520advancements%252C%2520however%252C%250Athey%2520come%2520with%2520substantial%2520limitations%2520that%2520hinder%2520their%2520practical%2520application%250Ain%2520real%2520clinical%2520scenarios.%2520These%2520include%2520unrealistic%2520human%2520interaction%250Arequirements%252C%2520such%2520as%2520slice-by-slice%2520operations%2520for%25202D%2520models%2520on%25203D%2520data%252C%2520a%250Alack%2520of%2520iterative%2520refinement%252C%2520and%2520insufficient%2520evaluation%2520experiments.%2520These%250Ashortcomings%2520prevent%2520accurate%2520assessment%2520of%2520model%2520performance%2520and%2520lead%2520to%250Ainconsistent%2520outcomes%2520across%2520studies.%2520IntRaBench%2520overcomes%2520these%2520challenges%2520by%250Aoffering%2520a%2520comprehensive%2520and%2520reproducible%2520framework%2520for%2520evaluating%2520interactive%250Asegmentation%2520methods%2520in%2520realistic%252C%2520clinically%2520relevant%2520scenarios.%2520It%2520includes%250Adiverse%2520datasets%252C%2520target%2520structures%252C%2520and%2520segmentation%2520models%252C%2520and%2520provides%2520a%250Aflexible%2520codebase%2520that%2520allows%2520seamless%2520integration%2520of%2520new%2520models%2520and%2520prompting%250Astrategies.%2520Additionally%252C%2520we%2520introduce%2520advanced%2520techniques%2520to%2520minimize%250Aclinician%2520interaction%252C%2520ensuring%2520fair%2520comparisons%2520between%25202D%2520and%25203D%2520models.%2520By%250Aopen-sourcing%2520IntRaBench%252C%2520we%2520invite%2520the%2520research%2520community%2520to%2520integrate%2520their%250Amodels%2520and%2520prompting%2520techniques%252C%2520ensuring%2520continuous%2520and%2520transparent%2520evaluation%250Aof%2520interactive%2520segmentation%2520models%2520in%25203D%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INTRABENCH%3A%20Interactive%20Radiological%20Benchmark&entry.906535625=Constantin%20Ulrich%20and%20Tassilo%20Wald%20and%20Emily%20Tempus%20and%20Maximilian%20Rokuss%20and%20Paul%20F.%20Jaeger%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Current%20interactive%20segmentation%20approaches%2C%20inspired%20by%20the%20success%20of%0AMETA%27s%20Segment%20Anything%20model%2C%20have%20achieved%20notable%20advancements%2C%20however%2C%0Athey%20come%20with%20substantial%20limitations%20that%20hinder%20their%20practical%20application%0Ain%20real%20clinical%20scenarios.%20These%20include%20unrealistic%20human%20interaction%0Arequirements%2C%20such%20as%20slice-by-slice%20operations%20for%202D%20models%20on%203D%20data%2C%20a%0Alack%20of%20iterative%20refinement%2C%20and%20insufficient%20evaluation%20experiments.%20These%0Ashortcomings%20prevent%20accurate%20assessment%20of%20model%20performance%20and%20lead%20to%0Ainconsistent%20outcomes%20across%20studies.%20IntRaBench%20overcomes%20these%20challenges%20by%0Aoffering%20a%20comprehensive%20and%20reproducible%20framework%20for%20evaluating%20interactive%0Asegmentation%20methods%20in%20realistic%2C%20clinically%20relevant%20scenarios.%20It%20includes%0Adiverse%20datasets%2C%20target%20structures%2C%20and%20segmentation%20models%2C%20and%20provides%20a%0Aflexible%20codebase%20that%20allows%20seamless%20integration%20of%20new%20models%20and%20prompting%0Astrategies.%20Additionally%2C%20we%20introduce%20advanced%20techniques%20to%20minimize%0Aclinician%20interaction%2C%20ensuring%20fair%20comparisons%20between%202D%20and%203D%20models.%20By%0Aopen-sourcing%20IntRaBench%2C%20we%20invite%20the%20research%20community%20to%20integrate%20their%0Amodels%20and%20prompting%20techniques%2C%20ensuring%20continuous%20and%20transparent%20evaluation%0Aof%20interactive%20segmentation%20models%20in%203D%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07885v1&entry.124074799=Read"},
{"title": "No-Reference Point Cloud Quality Assessment via Graph Convolutional\n  Network", "author": "Wu Chen and Qiuping Jiang and Wei Zhou and Feng Shao and Guangtao Zhai and Weisi Lin", "abstract": "  Three-dimensional (3D) point cloud, as an emerging visual media format, is\nincreasingly favored by consumers as it can provide more realistic visual\ninformation than two-dimensional (2D) data. Similar to 2D plane images and\nvideos, point clouds inevitably suffer from quality degradation and information\nloss through multimedia communication systems. Therefore, automatic point cloud\nquality assessment (PCQA) is of critical importance. In this work, we propose a\nnovel no-reference PCQA method by using a graph convolutional network (GCN) to\ncharacterize the mutual dependencies of multi-view 2D projected image contents.\nThe proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e.,\nmulti-view projection, graph construction, and GCN-based quality prediction.\nFirst, multi-view projection is performed on the test point cloud to obtain a\nset of horizontally and vertically projected images. Then, a\nperception-consistent graph is constructed based on the spatial relations among\ndifferent projected images. Finally, reasoning on the constructed graph is\nperformed by GCN to characterize the mutual dependencies and interactions\nbetween different projected images, and aggregate feature information of\nmulti-view projected images for final quality prediction. Experimental results\non two publicly available benchmark databases show that our proposed GC-PCQA\ncan achieve superior performance than state-of-the-art quality assessment\nmetrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.\n", "link": "http://arxiv.org/abs/2411.07728v1", "date": "2024-11-12", "relevancy": 2.6842, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5343}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20via%20Graph%20Convolutional%0A%20%20Network&body=Title%3A%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20via%20Graph%20Convolutional%0A%20%20Network%0AAuthor%3A%20Wu%20Chen%20and%20Qiuping%20Jiang%20and%20Wei%20Zhou%20and%20Feng%20Shao%20and%20Guangtao%20Zhai%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Three-dimensional%20%283D%29%20point%20cloud%2C%20as%20an%20emerging%20visual%20media%20format%2C%20is%0Aincreasingly%20favored%20by%20consumers%20as%20it%20can%20provide%20more%20realistic%20visual%0Ainformation%20than%20two-dimensional%20%282D%29%20data.%20Similar%20to%202D%20plane%20images%20and%0Avideos%2C%20point%20clouds%20inevitably%20suffer%20from%20quality%20degradation%20and%20information%0Aloss%20through%20multimedia%20communication%20systems.%20Therefore%2C%20automatic%20point%20cloud%0Aquality%20assessment%20%28PCQA%29%20is%20of%20critical%20importance.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20no-reference%20PCQA%20method%20by%20using%20a%20graph%20convolutional%20network%20%28GCN%29%20to%0Acharacterize%20the%20mutual%20dependencies%20of%20multi-view%202D%20projected%20image%20contents.%0AThe%20proposed%20GCN-based%20PCQA%20%28GC-PCQA%29%20method%20contains%20three%20modules%2C%20i.e.%2C%0Amulti-view%20projection%2C%20graph%20construction%2C%20and%20GCN-based%20quality%20prediction.%0AFirst%2C%20multi-view%20projection%20is%20performed%20on%20the%20test%20point%20cloud%20to%20obtain%20a%0Aset%20of%20horizontally%20and%20vertically%20projected%20images.%20Then%2C%20a%0Aperception-consistent%20graph%20is%20constructed%20based%20on%20the%20spatial%20relations%20among%0Adifferent%20projected%20images.%20Finally%2C%20reasoning%20on%20the%20constructed%20graph%20is%0Aperformed%20by%20GCN%20to%20characterize%20the%20mutual%20dependencies%20and%20interactions%0Abetween%20different%20projected%20images%2C%20and%20aggregate%20feature%20information%20of%0Amulti-view%20projected%20images%20for%20final%20quality%20prediction.%20Experimental%20results%0Aon%20two%20publicly%20available%20benchmark%20databases%20show%20that%20our%20proposed%20GC-PCQA%0Acan%20achieve%20superior%20performance%20than%20state-of-the-art%20quality%20assessment%0Ametrics.%20The%20code%20will%20be%20available%20at%3A%20https%3A//github.com/chenwuwq/GC-PCQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo-Reference%2520Point%2520Cloud%2520Quality%2520Assessment%2520via%2520Graph%2520Convolutional%250A%2520%2520Network%26entry.906535625%3DWu%2520Chen%2520and%2520Qiuping%2520Jiang%2520and%2520Wei%2520Zhou%2520and%2520Feng%2520Shao%2520and%2520Guangtao%2520Zhai%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Three-dimensional%2520%25283D%2529%2520point%2520cloud%252C%2520as%2520an%2520emerging%2520visual%2520media%2520format%252C%2520is%250Aincreasingly%2520favored%2520by%2520consumers%2520as%2520it%2520can%2520provide%2520more%2520realistic%2520visual%250Ainformation%2520than%2520two-dimensional%2520%25282D%2529%2520data.%2520Similar%2520to%25202D%2520plane%2520images%2520and%250Avideos%252C%2520point%2520clouds%2520inevitably%2520suffer%2520from%2520quality%2520degradation%2520and%2520information%250Aloss%2520through%2520multimedia%2520communication%2520systems.%2520Therefore%252C%2520automatic%2520point%2520cloud%250Aquality%2520assessment%2520%2528PCQA%2529%2520is%2520of%2520critical%2520importance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520no-reference%2520PCQA%2520method%2520by%2520using%2520a%2520graph%2520convolutional%2520network%2520%2528GCN%2529%2520to%250Acharacterize%2520the%2520mutual%2520dependencies%2520of%2520multi-view%25202D%2520projected%2520image%2520contents.%250AThe%2520proposed%2520GCN-based%2520PCQA%2520%2528GC-PCQA%2529%2520method%2520contains%2520three%2520modules%252C%2520i.e.%252C%250Amulti-view%2520projection%252C%2520graph%2520construction%252C%2520and%2520GCN-based%2520quality%2520prediction.%250AFirst%252C%2520multi-view%2520projection%2520is%2520performed%2520on%2520the%2520test%2520point%2520cloud%2520to%2520obtain%2520a%250Aset%2520of%2520horizontally%2520and%2520vertically%2520projected%2520images.%2520Then%252C%2520a%250Aperception-consistent%2520graph%2520is%2520constructed%2520based%2520on%2520the%2520spatial%2520relations%2520among%250Adifferent%2520projected%2520images.%2520Finally%252C%2520reasoning%2520on%2520the%2520constructed%2520graph%2520is%250Aperformed%2520by%2520GCN%2520to%2520characterize%2520the%2520mutual%2520dependencies%2520and%2520interactions%250Abetween%2520different%2520projected%2520images%252C%2520and%2520aggregate%2520feature%2520information%2520of%250Amulti-view%2520projected%2520images%2520for%2520final%2520quality%2520prediction.%2520Experimental%2520results%250Aon%2520two%2520publicly%2520available%2520benchmark%2520databases%2520show%2520that%2520our%2520proposed%2520GC-PCQA%250Acan%2520achieve%2520superior%2520performance%2520than%2520state-of-the-art%2520quality%2520assessment%250Ametrics.%2520The%2520code%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/chenwuwq/GC-PCQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No-Reference%20Point%20Cloud%20Quality%20Assessment%20via%20Graph%20Convolutional%0A%20%20Network&entry.906535625=Wu%20Chen%20and%20Qiuping%20Jiang%20and%20Wei%20Zhou%20and%20Feng%20Shao%20and%20Guangtao%20Zhai%20and%20Weisi%20Lin&entry.1292438233=%20%20Three-dimensional%20%283D%29%20point%20cloud%2C%20as%20an%20emerging%20visual%20media%20format%2C%20is%0Aincreasingly%20favored%20by%20consumers%20as%20it%20can%20provide%20more%20realistic%20visual%0Ainformation%20than%20two-dimensional%20%282D%29%20data.%20Similar%20to%202D%20plane%20images%20and%0Avideos%2C%20point%20clouds%20inevitably%20suffer%20from%20quality%20degradation%20and%20information%0Aloss%20through%20multimedia%20communication%20systems.%20Therefore%2C%20automatic%20point%20cloud%0Aquality%20assessment%20%28PCQA%29%20is%20of%20critical%20importance.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20no-reference%20PCQA%20method%20by%20using%20a%20graph%20convolutional%20network%20%28GCN%29%20to%0Acharacterize%20the%20mutual%20dependencies%20of%20multi-view%202D%20projected%20image%20contents.%0AThe%20proposed%20GCN-based%20PCQA%20%28GC-PCQA%29%20method%20contains%20three%20modules%2C%20i.e.%2C%0Amulti-view%20projection%2C%20graph%20construction%2C%20and%20GCN-based%20quality%20prediction.%0AFirst%2C%20multi-view%20projection%20is%20performed%20on%20the%20test%20point%20cloud%20to%20obtain%20a%0Aset%20of%20horizontally%20and%20vertically%20projected%20images.%20Then%2C%20a%0Aperception-consistent%20graph%20is%20constructed%20based%20on%20the%20spatial%20relations%20among%0Adifferent%20projected%20images.%20Finally%2C%20reasoning%20on%20the%20constructed%20graph%20is%0Aperformed%20by%20GCN%20to%20characterize%20the%20mutual%20dependencies%20and%20interactions%0Abetween%20different%20projected%20images%2C%20and%20aggregate%20feature%20information%20of%0Amulti-view%20projected%20images%20for%20final%20quality%20prediction.%20Experimental%20results%0Aon%20two%20publicly%20available%20benchmark%20databases%20show%20that%20our%20proposed%20GC-PCQA%0Acan%20achieve%20superior%20performance%20than%20state-of-the-art%20quality%20assessment%0Ametrics.%20The%20code%20will%20be%20available%20at%3A%20https%3A//github.com/chenwuwq/GC-PCQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07728v1&entry.124074799=Read"},
{"title": "Pseudo-triplet Guided Few-shot Composed Image Retrieval", "author": "Bohan Hou and Haoqiang Lin and Haokun Wen and Meng Liu and Mingzhu Xu and Xuemeng Song", "abstract": "  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve\nthe target image with a multimodal query, i.e., a reference image, and its\ncomplementary modification text. As previous supervised or zero-shot learning\nparadigms all fail to strike a good trade-off between the model's\ngeneralization ability and retrieval performance, recent researchers have\nintroduced the task of few-shot CIR (FS-CIR) and proposed a textual\ninversion-based network based on pretrained CLIP model to realize it. Despite\nits promising performance, the approach encounters two key limitations: simply\nrelying on the few annotated samples for CIR model training and\nindiscriminately selecting training triplets for CIR model fine-tuning. To\naddress these two limitations, we propose a novel two-stage pseudo triplet\nguided few-shot CIR scheme, dubbed PTG-FSCIR. In the first stage, we propose an\nattentive masking and captioning-based pseudo triplet generation method, to\nconstruct pseudo triplets from pure image data and use them to fulfill the\nCIR-task specific pertaining. In the second stage, we propose a challenging\ntriplet-based CIR fine-tuning method, where we design a pseudo modification\ntext-based sample challenging score estimation strategy and a robust top\nrange-based random sampling strategy for sampling robust challenging triplets\nto promote the model fine-tuning. Notably, our scheme is plug-and-play and\ncompatible with any existing supervised CIR models. We test our scheme across\ntwo backbones on three public datasets (i.e., FashionIQ, CIRR, and\nBirds-to-Words), achieving maximum improvements of 13.3%, 22.2%, and 17.4%\nrespectively, demonstrating our scheme's efficacy.\n", "link": "http://arxiv.org/abs/2407.06001v2", "date": "2024-11-12", "relevancy": 2.6823, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5664}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5221}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval&body=Title%3A%20Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval%0AAuthor%3A%20Bohan%20Hou%20and%20Haoqiang%20Lin%20and%20Haokun%20Wen%20and%20Meng%20Liu%20and%20Mingzhu%20Xu%20and%20Xuemeng%20Song%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20challenging%20task%20that%20aims%20to%20retrieve%0Athe%20target%20image%20with%20a%20multimodal%20query%2C%20i.e.%2C%20a%20reference%20image%2C%20and%20its%0Acomplementary%20modification%20text.%20As%20previous%20supervised%20or%20zero-shot%20learning%0Aparadigms%20all%20fail%20to%20strike%20a%20good%20trade-off%20between%20the%20model%27s%0Ageneralization%20ability%20and%20retrieval%20performance%2C%20recent%20researchers%20have%0Aintroduced%20the%20task%20of%20few-shot%20CIR%20%28FS-CIR%29%20and%20proposed%20a%20textual%0Ainversion-based%20network%20based%20on%20pretrained%20CLIP%20model%20to%20realize%20it.%20Despite%0Aits%20promising%20performance%2C%20the%20approach%20encounters%20two%20key%20limitations%3A%20simply%0Arelying%20on%20the%20few%20annotated%20samples%20for%20CIR%20model%20training%20and%0Aindiscriminately%20selecting%20training%20triplets%20for%20CIR%20model%20fine-tuning.%20To%0Aaddress%20these%20two%20limitations%2C%20we%20propose%20a%20novel%20two-stage%20pseudo%20triplet%0Aguided%20few-shot%20CIR%20scheme%2C%20dubbed%20PTG-FSCIR.%20In%20the%20first%20stage%2C%20we%20propose%20an%0Aattentive%20masking%20and%20captioning-based%20pseudo%20triplet%20generation%20method%2C%20to%0Aconstruct%20pseudo%20triplets%20from%20pure%20image%20data%20and%20use%20them%20to%20fulfill%20the%0ACIR-task%20specific%20pertaining.%20In%20the%20second%20stage%2C%20we%20propose%20a%20challenging%0Atriplet-based%20CIR%20fine-tuning%20method%2C%20where%20we%20design%20a%20pseudo%20modification%0Atext-based%20sample%20challenging%20score%20estimation%20strategy%20and%20a%20robust%20top%0Arange-based%20random%20sampling%20strategy%20for%20sampling%20robust%20challenging%20triplets%0Ato%20promote%20the%20model%20fine-tuning.%20Notably%2C%20our%20scheme%20is%20plug-and-play%20and%0Acompatible%20with%20any%20existing%20supervised%20CIR%20models.%20We%20test%20our%20scheme%20across%0Atwo%20backbones%20on%20three%20public%20datasets%20%28i.e.%2C%20FashionIQ%2C%20CIRR%2C%20and%0ABirds-to-Words%29%2C%20achieving%20maximum%20improvements%20of%2013.3%25%2C%2022.2%25%2C%20and%2017.4%25%0Arespectively%2C%20demonstrating%20our%20scheme%27s%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06001v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-triplet%2520Guided%2520Few-shot%2520Composed%2520Image%2520Retrieval%26entry.906535625%3DBohan%2520Hou%2520and%2520Haoqiang%2520Lin%2520and%2520Haokun%2520Wen%2520and%2520Meng%2520Liu%2520and%2520Mingzhu%2520Xu%2520and%2520Xuemeng%2520Song%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CIR%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520retrieve%250Athe%2520target%2520image%2520with%2520a%2520multimodal%2520query%252C%2520i.e.%252C%2520a%2520reference%2520image%252C%2520and%2520its%250Acomplementary%2520modification%2520text.%2520As%2520previous%2520supervised%2520or%2520zero-shot%2520learning%250Aparadigms%2520all%2520fail%2520to%2520strike%2520a%2520good%2520trade-off%2520between%2520the%2520model%2527s%250Ageneralization%2520ability%2520and%2520retrieval%2520performance%252C%2520recent%2520researchers%2520have%250Aintroduced%2520the%2520task%2520of%2520few-shot%2520CIR%2520%2528FS-CIR%2529%2520and%2520proposed%2520a%2520textual%250Ainversion-based%2520network%2520based%2520on%2520pretrained%2520CLIP%2520model%2520to%2520realize%2520it.%2520Despite%250Aits%2520promising%2520performance%252C%2520the%2520approach%2520encounters%2520two%2520key%2520limitations%253A%2520simply%250Arelying%2520on%2520the%2520few%2520annotated%2520samples%2520for%2520CIR%2520model%2520training%2520and%250Aindiscriminately%2520selecting%2520training%2520triplets%2520for%2520CIR%2520model%2520fine-tuning.%2520To%250Aaddress%2520these%2520two%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520two-stage%2520pseudo%2520triplet%250Aguided%2520few-shot%2520CIR%2520scheme%252C%2520dubbed%2520PTG-FSCIR.%2520In%2520the%2520first%2520stage%252C%2520we%2520propose%2520an%250Aattentive%2520masking%2520and%2520captioning-based%2520pseudo%2520triplet%2520generation%2520method%252C%2520to%250Aconstruct%2520pseudo%2520triplets%2520from%2520pure%2520image%2520data%2520and%2520use%2520them%2520to%2520fulfill%2520the%250ACIR-task%2520specific%2520pertaining.%2520In%2520the%2520second%2520stage%252C%2520we%2520propose%2520a%2520challenging%250Atriplet-based%2520CIR%2520fine-tuning%2520method%252C%2520where%2520we%2520design%2520a%2520pseudo%2520modification%250Atext-based%2520sample%2520challenging%2520score%2520estimation%2520strategy%2520and%2520a%2520robust%2520top%250Arange-based%2520random%2520sampling%2520strategy%2520for%2520sampling%2520robust%2520challenging%2520triplets%250Ato%2520promote%2520the%2520model%2520fine-tuning.%2520Notably%252C%2520our%2520scheme%2520is%2520plug-and-play%2520and%250Acompatible%2520with%2520any%2520existing%2520supervised%2520CIR%2520models.%2520We%2520test%2520our%2520scheme%2520across%250Atwo%2520backbones%2520on%2520three%2520public%2520datasets%2520%2528i.e.%252C%2520FashionIQ%252C%2520CIRR%252C%2520and%250ABirds-to-Words%2529%252C%2520achieving%2520maximum%2520improvements%2520of%252013.3%2525%252C%252022.2%2525%252C%2520and%252017.4%2525%250Arespectively%252C%2520demonstrating%2520our%2520scheme%2527s%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06001v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-triplet%20Guided%20Few-shot%20Composed%20Image%20Retrieval&entry.906535625=Bohan%20Hou%20and%20Haoqiang%20Lin%20and%20Haokun%20Wen%20and%20Meng%20Liu%20and%20Mingzhu%20Xu%20and%20Xuemeng%20Song&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20challenging%20task%20that%20aims%20to%20retrieve%0Athe%20target%20image%20with%20a%20multimodal%20query%2C%20i.e.%2C%20a%20reference%20image%2C%20and%20its%0Acomplementary%20modification%20text.%20As%20previous%20supervised%20or%20zero-shot%20learning%0Aparadigms%20all%20fail%20to%20strike%20a%20good%20trade-off%20between%20the%20model%27s%0Ageneralization%20ability%20and%20retrieval%20performance%2C%20recent%20researchers%20have%0Aintroduced%20the%20task%20of%20few-shot%20CIR%20%28FS-CIR%29%20and%20proposed%20a%20textual%0Ainversion-based%20network%20based%20on%20pretrained%20CLIP%20model%20to%20realize%20it.%20Despite%0Aits%20promising%20performance%2C%20the%20approach%20encounters%20two%20key%20limitations%3A%20simply%0Arelying%20on%20the%20few%20annotated%20samples%20for%20CIR%20model%20training%20and%0Aindiscriminately%20selecting%20training%20triplets%20for%20CIR%20model%20fine-tuning.%20To%0Aaddress%20these%20two%20limitations%2C%20we%20propose%20a%20novel%20two-stage%20pseudo%20triplet%0Aguided%20few-shot%20CIR%20scheme%2C%20dubbed%20PTG-FSCIR.%20In%20the%20first%20stage%2C%20we%20propose%20an%0Aattentive%20masking%20and%20captioning-based%20pseudo%20triplet%20generation%20method%2C%20to%0Aconstruct%20pseudo%20triplets%20from%20pure%20image%20data%20and%20use%20them%20to%20fulfill%20the%0ACIR-task%20specific%20pertaining.%20In%20the%20second%20stage%2C%20we%20propose%20a%20challenging%0Atriplet-based%20CIR%20fine-tuning%20method%2C%20where%20we%20design%20a%20pseudo%20modification%0Atext-based%20sample%20challenging%20score%20estimation%20strategy%20and%20a%20robust%20top%0Arange-based%20random%20sampling%20strategy%20for%20sampling%20robust%20challenging%20triplets%0Ato%20promote%20the%20model%20fine-tuning.%20Notably%2C%20our%20scheme%20is%20plug-and-play%20and%0Acompatible%20with%20any%20existing%20supervised%20CIR%20models.%20We%20test%20our%20scheme%20across%0Atwo%20backbones%20on%20three%20public%20datasets%20%28i.e.%2C%20FashionIQ%2C%20CIRR%2C%20and%0ABirds-to-Words%29%2C%20achieving%20maximum%20improvements%20of%2013.3%25%2C%2022.2%25%2C%20and%2017.4%25%0Arespectively%2C%20demonstrating%20our%20scheme%27s%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06001v2&entry.124074799=Read"},
{"title": "Extreme Rotation Estimation in the Wild", "author": "Hana Bezalel and Dotan Ankri and Ruojin Cai and Hadar Averbuch-Elor", "abstract": "  We present a technique and benchmark dataset for estimating the relative 3D\norientation between a pair of Internet images captured in an extreme setting,\nwhere the images have limited or non-overlapping field of views. Prior work\ntargeting extreme rotation estimation assume constrained 3D environments and\nemulate perspective images by cropping regions from panoramic views. However,\nreal images captured in the wild are highly diverse, exhibiting variation in\nboth appearance and camera intrinsics. In this work, we propose a\nTransformer-based method for estimating relative rotations in extreme\nreal-world settings, and contribute the ExtremeLandmarkPairs dataset, assembled\nfrom scene-level Internet photo collections. Our evaluation demonstrates that\nour approach succeeds in estimating the relative rotations in a wide variety of\nextreme-view Internet image pairs, outperforming various baselines, including\ndedicated rotation estimation techniques and contemporary 3D reconstruction\nmethods.\n", "link": "http://arxiv.org/abs/2411.07096v2", "date": "2024-11-12", "relevancy": 2.6812, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extreme%20Rotation%20Estimation%20in%20the%20Wild&body=Title%3A%20Extreme%20Rotation%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextreme-view%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtreme%2520Rotation%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DHana%2520Bezalel%2520and%2520Dotan%2520Ankri%2520and%2520Ruojin%2520Cai%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520technique%2520and%2520benchmark%2520dataset%2520for%2520estimating%2520the%2520relative%25203D%250Aorientation%2520between%2520a%2520pair%2520of%2520Internet%2520images%2520captured%2520in%2520an%2520extreme%2520setting%252C%250Awhere%2520the%2520images%2520have%2520limited%2520or%2520non-overlapping%2520field%2520of%2520views.%2520Prior%2520work%250Atargeting%2520extreme%2520rotation%2520estimation%2520assume%2520constrained%25203D%2520environments%2520and%250Aemulate%2520perspective%2520images%2520by%2520cropping%2520regions%2520from%2520panoramic%2520views.%2520However%252C%250Areal%2520images%2520captured%2520in%2520the%2520wild%2520are%2520highly%2520diverse%252C%2520exhibiting%2520variation%2520in%250Aboth%2520appearance%2520and%2520camera%2520intrinsics.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250ATransformer-based%2520method%2520for%2520estimating%2520relative%2520rotations%2520in%2520extreme%250Areal-world%2520settings%252C%2520and%2520contribute%2520the%2520ExtremeLandmarkPairs%2520dataset%252C%2520assembled%250Afrom%2520scene-level%2520Internet%2520photo%2520collections.%2520Our%2520evaluation%2520demonstrates%2520that%250Aour%2520approach%2520succeeds%2520in%2520estimating%2520the%2520relative%2520rotations%2520in%2520a%2520wide%2520variety%2520of%250Aextreme-view%2520Internet%2520image%2520pairs%252C%2520outperforming%2520various%2520baselines%252C%2520including%250Adedicated%2520rotation%2520estimation%2520techniques%2520and%2520contemporary%25203D%2520reconstruction%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20Rotation%20Estimation%20in%20the%20Wild&entry.906535625=Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextreme-view%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07096v2&entry.124074799=Read"},
{"title": "Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in\n  Alzheimer's Disease", "author": "Francesco Chiumento and Mingming Liu", "abstract": "  The rapid advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have shown great potential in medical diagnostics, particularly\nin radiology, where datasets such as X-rays are paired with human-generated\ndiagnostic reports. However, a significant research gap exists in the\nneuroimaging field, especially for conditions such as Alzheimer's disease, due\nto the lack of comprehensive diagnostic reports that can be utilized for model\nfine-tuning. This paper addresses this gap by generating synthetic diagnostic\nreports using GPT-4o-mini on structured data from the OASIS-4 dataset, which\ncomprises 663 patients. Using the synthetic reports as ground truth for\ntraining and validation, we then generated neurological reports directly from\nthe images in the dataset leveraging the pre-trained BiomedCLIP and T5 models.\nOur proposed method achieved a BLEU-4 score of 0.1827, ROUGE-L score of 0.3719,\nand METEOR score of 0.4163, revealing its potential in generating clinically\nrelevant and accurate diagnostic reports.\n", "link": "http://arxiv.org/abs/2411.07871v1", "date": "2024-11-12", "relevancy": 2.6404, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Multimodal%20Models%20for%20Enhanced%20Neuroimaging%20Diagnostics%20in%0A%20%20Alzheimer%27s%20Disease&body=Title%3A%20Leveraging%20Multimodal%20Models%20for%20Enhanced%20Neuroimaging%20Diagnostics%20in%0A%20%20Alzheimer%27s%20Disease%0AAuthor%3A%20Francesco%20Chiumento%20and%20Mingming%20Liu%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%0AModels%20%28VLMs%29%20have%20shown%20great%20potential%20in%20medical%20diagnostics%2C%20particularly%0Ain%20radiology%2C%20where%20datasets%20such%20as%20X-rays%20are%20paired%20with%20human-generated%0Adiagnostic%20reports.%20However%2C%20a%20significant%20research%20gap%20exists%20in%20the%0Aneuroimaging%20field%2C%20especially%20for%20conditions%20such%20as%20Alzheimer%27s%20disease%2C%20due%0Ato%20the%20lack%20of%20comprehensive%20diagnostic%20reports%20that%20can%20be%20utilized%20for%20model%0Afine-tuning.%20This%20paper%20addresses%20this%20gap%20by%20generating%20synthetic%20diagnostic%0Areports%20using%20GPT-4o-mini%20on%20structured%20data%20from%20the%20OASIS-4%20dataset%2C%20which%0Acomprises%20663%20patients.%20Using%20the%20synthetic%20reports%20as%20ground%20truth%20for%0Atraining%20and%20validation%2C%20we%20then%20generated%20neurological%20reports%20directly%20from%0Athe%20images%20in%20the%20dataset%20leveraging%20the%20pre-trained%20BiomedCLIP%20and%20T5%20models.%0AOur%20proposed%20method%20achieved%20a%20BLEU-4%20score%20of%200.1827%2C%20ROUGE-L%20score%20of%200.3719%2C%0Aand%20METEOR%20score%20of%200.4163%2C%20revealing%20its%20potential%20in%20generating%20clinically%0Arelevant%20and%20accurate%20diagnostic%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Multimodal%2520Models%2520for%2520Enhanced%2520Neuroimaging%2520Diagnostics%2520in%250A%2520%2520Alzheimer%2527s%2520Disease%26entry.906535625%3DFrancesco%2520Chiumento%2520and%2520Mingming%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520have%2520shown%2520great%2520potential%2520in%2520medical%2520diagnostics%252C%2520particularly%250Ain%2520radiology%252C%2520where%2520datasets%2520such%2520as%2520X-rays%2520are%2520paired%2520with%2520human-generated%250Adiagnostic%2520reports.%2520However%252C%2520a%2520significant%2520research%2520gap%2520exists%2520in%2520the%250Aneuroimaging%2520field%252C%2520especially%2520for%2520conditions%2520such%2520as%2520Alzheimer%2527s%2520disease%252C%2520due%250Ato%2520the%2520lack%2520of%2520comprehensive%2520diagnostic%2520reports%2520that%2520can%2520be%2520utilized%2520for%2520model%250Afine-tuning.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520generating%2520synthetic%2520diagnostic%250Areports%2520using%2520GPT-4o-mini%2520on%2520structured%2520data%2520from%2520the%2520OASIS-4%2520dataset%252C%2520which%250Acomprises%2520663%2520patients.%2520Using%2520the%2520synthetic%2520reports%2520as%2520ground%2520truth%2520for%250Atraining%2520and%2520validation%252C%2520we%2520then%2520generated%2520neurological%2520reports%2520directly%2520from%250Athe%2520images%2520in%2520the%2520dataset%2520leveraging%2520the%2520pre-trained%2520BiomedCLIP%2520and%2520T5%2520models.%250AOur%2520proposed%2520method%2520achieved%2520a%2520BLEU-4%2520score%2520of%25200.1827%252C%2520ROUGE-L%2520score%2520of%25200.3719%252C%250Aand%2520METEOR%2520score%2520of%25200.4163%252C%2520revealing%2520its%2520potential%2520in%2520generating%2520clinically%250Arelevant%2520and%2520accurate%2520diagnostic%2520reports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Multimodal%20Models%20for%20Enhanced%20Neuroimaging%20Diagnostics%20in%0A%20%20Alzheimer%27s%20Disease&entry.906535625=Francesco%20Chiumento%20and%20Mingming%20Liu&entry.1292438233=%20%20The%20rapid%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%0AModels%20%28VLMs%29%20have%20shown%20great%20potential%20in%20medical%20diagnostics%2C%20particularly%0Ain%20radiology%2C%20where%20datasets%20such%20as%20X-rays%20are%20paired%20with%20human-generated%0Adiagnostic%20reports.%20However%2C%20a%20significant%20research%20gap%20exists%20in%20the%0Aneuroimaging%20field%2C%20especially%20for%20conditions%20such%20as%20Alzheimer%27s%20disease%2C%20due%0Ato%20the%20lack%20of%20comprehensive%20diagnostic%20reports%20that%20can%20be%20utilized%20for%20model%0Afine-tuning.%20This%20paper%20addresses%20this%20gap%20by%20generating%20synthetic%20diagnostic%0Areports%20using%20GPT-4o-mini%20on%20structured%20data%20from%20the%20OASIS-4%20dataset%2C%20which%0Acomprises%20663%20patients.%20Using%20the%20synthetic%20reports%20as%20ground%20truth%20for%0Atraining%20and%20validation%2C%20we%20then%20generated%20neurological%20reports%20directly%20from%0Athe%20images%20in%20the%20dataset%20leveraging%20the%20pre-trained%20BiomedCLIP%20and%20T5%20models.%0AOur%20proposed%20method%20achieved%20a%20BLEU-4%20score%20of%200.1827%2C%20ROUGE-L%20score%20of%200.3719%2C%0Aand%20METEOR%20score%20of%200.4163%2C%20revealing%20its%20potential%20in%20generating%20clinically%0Arelevant%20and%20accurate%20diagnostic%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07871v1&entry.124074799=Read"},
{"title": "Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and\n  Re-Identification using Point Clouds", "author": "Daniel Fusaro and Federico Magistri and Jens Behley and Alberto Pretto and Cyrill Stachniss", "abstract": "  Robotic fruit monitoring is a key step toward automated agricultural\nproduction systems. Robots can significantly enhance plant and temporal fruit\nmonitoring by providing precise, high-throughput assessments that overcome the\nlimitations of traditional manual methods. Fruit monitoring is a challenging\ntask due to the significant variation in size, shape, orientation, and\nocclusion of fruits. Also, fruits may be harvested or newly grown between\nrecording sessions. Most methods are 2D image-based and they lack the 3D\nstructure, depth, and spatial information, which represent key aspects of fruit\nmonitoring. 3D colored point clouds, instead, can offer this information but\nthey introduce challenges such as their sparsity and irregularity. In this\npaper, we present a novel approach for temporal fruit monitoring that addresses\npoint clouds collected in a greenhouse over time. Our method segments fruits\nusing a learning-based instance segmentation approach directly on the point\ncloud. Each segmented fruit is processed by a 3D sparse convolutional neural\nnetwork to extract descriptors, which are used in an attention-based matching\nnetwork to associate fruits with their instances from previous data\ncollections. Experimental results on a real dataset of strawberries demonstrate\nthat our approach outperforms other methods for fruits re-identification over\ntime, allowing for precise temporal fruit monitoring in real and complex\nscenarios.\n", "link": "http://arxiv.org/abs/2411.07799v1", "date": "2024-11-12", "relevancy": 2.6161, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5314}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5191}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Point%20Clouds&body=Title%3A%20Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Point%20Clouds%0AAuthor%3A%20Daniel%20Fusaro%20and%20Federico%20Magistri%20and%20Jens%20Behley%20and%20Alberto%20Pretto%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Robotic%20fruit%20monitoring%20is%20a%20key%20step%20toward%20automated%20agricultural%0Aproduction%20systems.%20Robots%20can%20significantly%20enhance%20plant%20and%20temporal%20fruit%0Amonitoring%20by%20providing%20precise%2C%20high-throughput%20assessments%20that%20overcome%20the%0Alimitations%20of%20traditional%20manual%20methods.%20Fruit%20monitoring%20is%20a%20challenging%0Atask%20due%20to%20the%20significant%20variation%20in%20size%2C%20shape%2C%20orientation%2C%20and%0Aocclusion%20of%20fruits.%20Also%2C%20fruits%20may%20be%20harvested%20or%20newly%20grown%20between%0Arecording%20sessions.%20Most%20methods%20are%202D%20image-based%20and%20they%20lack%20the%203D%0Astructure%2C%20depth%2C%20and%20spatial%20information%2C%20which%20represent%20key%20aspects%20of%20fruit%0Amonitoring.%203D%20colored%20point%20clouds%2C%20instead%2C%20can%20offer%20this%20information%20but%0Athey%20introduce%20challenges%20such%20as%20their%20sparsity%20and%20irregularity.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20for%20temporal%20fruit%20monitoring%20that%20addresses%0Apoint%20clouds%20collected%20in%20a%20greenhouse%20over%20time.%20Our%20method%20segments%20fruits%0Ausing%20a%20learning-based%20instance%20segmentation%20approach%20directly%20on%20the%20point%0Acloud.%20Each%20segmented%20fruit%20is%20processed%20by%20a%203D%20sparse%20convolutional%20neural%0Anetwork%20to%20extract%20descriptors%2C%20which%20are%20used%20in%20an%20attention-based%20matching%0Anetwork%20to%20associate%20fruits%20with%20their%20instances%20from%20previous%20data%0Acollections.%20Experimental%20results%20on%20a%20real%20dataset%20of%20strawberries%20demonstrate%0Athat%20our%20approach%20outperforms%20other%20methods%20for%20fruits%20re-identification%20over%0Atime%2C%20allowing%20for%20precise%20temporal%20fruit%20monitoring%20in%20real%20and%20complex%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHorticultural%2520Temporal%2520Fruit%2520Monitoring%2520via%25203D%2520Instance%2520Segmentation%2520and%250A%2520%2520Re-Identification%2520using%2520Point%2520Clouds%26entry.906535625%3DDaniel%2520Fusaro%2520and%2520Federico%2520Magistri%2520and%2520Jens%2520Behley%2520and%2520Alberto%2520Pretto%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Robotic%2520fruit%2520monitoring%2520is%2520a%2520key%2520step%2520toward%2520automated%2520agricultural%250Aproduction%2520systems.%2520Robots%2520can%2520significantly%2520enhance%2520plant%2520and%2520temporal%2520fruit%250Amonitoring%2520by%2520providing%2520precise%252C%2520high-throughput%2520assessments%2520that%2520overcome%2520the%250Alimitations%2520of%2520traditional%2520manual%2520methods.%2520Fruit%2520monitoring%2520is%2520a%2520challenging%250Atask%2520due%2520to%2520the%2520significant%2520variation%2520in%2520size%252C%2520shape%252C%2520orientation%252C%2520and%250Aocclusion%2520of%2520fruits.%2520Also%252C%2520fruits%2520may%2520be%2520harvested%2520or%2520newly%2520grown%2520between%250Arecording%2520sessions.%2520Most%2520methods%2520are%25202D%2520image-based%2520and%2520they%2520lack%2520the%25203D%250Astructure%252C%2520depth%252C%2520and%2520spatial%2520information%252C%2520which%2520represent%2520key%2520aspects%2520of%2520fruit%250Amonitoring.%25203D%2520colored%2520point%2520clouds%252C%2520instead%252C%2520can%2520offer%2520this%2520information%2520but%250Athey%2520introduce%2520challenges%2520such%2520as%2520their%2520sparsity%2520and%2520irregularity.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520temporal%2520fruit%2520monitoring%2520that%2520addresses%250Apoint%2520clouds%2520collected%2520in%2520a%2520greenhouse%2520over%2520time.%2520Our%2520method%2520segments%2520fruits%250Ausing%2520a%2520learning-based%2520instance%2520segmentation%2520approach%2520directly%2520on%2520the%2520point%250Acloud.%2520Each%2520segmented%2520fruit%2520is%2520processed%2520by%2520a%25203D%2520sparse%2520convolutional%2520neural%250Anetwork%2520to%2520extract%2520descriptors%252C%2520which%2520are%2520used%2520in%2520an%2520attention-based%2520matching%250Anetwork%2520to%2520associate%2520fruits%2520with%2520their%2520instances%2520from%2520previous%2520data%250Acollections.%2520Experimental%2520results%2520on%2520a%2520real%2520dataset%2520of%2520strawberries%2520demonstrate%250Athat%2520our%2520approach%2520outperforms%2520other%2520methods%2520for%2520fruits%2520re-identification%2520over%250Atime%252C%2520allowing%2520for%2520precise%2520temporal%2520fruit%2520monitoring%2520in%2520real%2520and%2520complex%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Point%20Clouds&entry.906535625=Daniel%20Fusaro%20and%20Federico%20Magistri%20and%20Jens%20Behley%20and%20Alberto%20Pretto%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Robotic%20fruit%20monitoring%20is%20a%20key%20step%20toward%20automated%20agricultural%0Aproduction%20systems.%20Robots%20can%20significantly%20enhance%20plant%20and%20temporal%20fruit%0Amonitoring%20by%20providing%20precise%2C%20high-throughput%20assessments%20that%20overcome%20the%0Alimitations%20of%20traditional%20manual%20methods.%20Fruit%20monitoring%20is%20a%20challenging%0Atask%20due%20to%20the%20significant%20variation%20in%20size%2C%20shape%2C%20orientation%2C%20and%0Aocclusion%20of%20fruits.%20Also%2C%20fruits%20may%20be%20harvested%20or%20newly%20grown%20between%0Arecording%20sessions.%20Most%20methods%20are%202D%20image-based%20and%20they%20lack%20the%203D%0Astructure%2C%20depth%2C%20and%20spatial%20information%2C%20which%20represent%20key%20aspects%20of%20fruit%0Amonitoring.%203D%20colored%20point%20clouds%2C%20instead%2C%20can%20offer%20this%20information%20but%0Athey%20introduce%20challenges%20such%20as%20their%20sparsity%20and%20irregularity.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20approach%20for%20temporal%20fruit%20monitoring%20that%20addresses%0Apoint%20clouds%20collected%20in%20a%20greenhouse%20over%20time.%20Our%20method%20segments%20fruits%0Ausing%20a%20learning-based%20instance%20segmentation%20approach%20directly%20on%20the%20point%0Acloud.%20Each%20segmented%20fruit%20is%20processed%20by%20a%203D%20sparse%20convolutional%20neural%0Anetwork%20to%20extract%20descriptors%2C%20which%20are%20used%20in%20an%20attention-based%20matching%0Anetwork%20to%20associate%20fruits%20with%20their%20instances%20from%20previous%20data%0Acollections.%20Experimental%20results%20on%20a%20real%20dataset%20of%20strawberries%20demonstrate%0Athat%20our%20approach%20outperforms%20other%20methods%20for%20fruits%20re-identification%20over%0Atime%2C%20allowing%20for%20precise%20temporal%20fruit%20monitoring%20in%20real%20and%20complex%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07799v1&entry.124074799=Read"},
{"title": "When Geoscience Meets Foundation Models: Towards General Geoscience\n  Artificial Intelligence System", "author": "Hao Zhang and Jin-Jian Xu and Hong-Wei Cui and Lin Li and Yaowen Yang and Chao-Sheng Tang and Niklas Boers", "abstract": "  Artificial intelligence (AI) has significantly advanced Earth sciences, yet\nits full potential in to comprehensively modeling Earth's complex dynamics\nremains unrealized. Geoscience foundation models (GFMs) emerge as a\nparadigm-shifting solution, integrating extensive cross-disciplinary data to\nenhance the simulation and understanding of Earth system dynamics. These\ndata-centric AI models extract insights from petabytes of structured and\nunstructured data, effectively addressing the complexities of Earth systems\nthat traditional models struggle to capture. The unique strengths of GFMs\ninclude flexible task specification, diverse input-output capabilities, and\nmulti-modal knowledge representation, enabling analyses that surpass those of\nindividual data sources or traditional AI methods. This review not only\nhighlights the key advantages of GFMs, but also presents essential techniques\nfor their construction, with a focus on transformers, pre-training, and\nadaptation strategies. Subsequently, we examine recent advancements in GFMs,\nincluding large language models, vision models, and vision-language models,\nparticularly emphasizing the potential applications in remote sensing.\nAdditionally, the review concludes with a comprehensive analysis of the\nchallenges and future trends in GFMs, addressing five critical aspects: data\nintegration, model complexity, uncertainty quantification, interdisciplinary\ncollaboration, and concerns related to privacy, trust, and security. This\nreview offers a comprehensive overview of emerging geoscientific research\nparadigms, emphasizing the untapped opportunities at the intersection of\nadvanced AI techniques and geoscience. It examines major methodologies,\nshowcases advances in large-scale models, and discusses the challenges and\nprospects that will shape the future landscape of GFMs.\n", "link": "http://arxiv.org/abs/2309.06799v5", "date": "2024-11-12", "relevancy": 2.6138, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Geoscience%20Meets%20Foundation%20Models%3A%20Towards%20General%20Geoscience%0A%20%20Artificial%20Intelligence%20System&body=Title%3A%20When%20Geoscience%20Meets%20Foundation%20Models%3A%20Towards%20General%20Geoscience%0A%20%20Artificial%20Intelligence%20System%0AAuthor%3A%20Hao%20Zhang%20and%20Jin-Jian%20Xu%20and%20Hong-Wei%20Cui%20and%20Lin%20Li%20and%20Yaowen%20Yang%20and%20Chao-Sheng%20Tang%20and%20Niklas%20Boers%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20significantly%20advanced%20Earth%20sciences%2C%20yet%0Aits%20full%20potential%20in%20to%20comprehensively%20modeling%20Earth%27s%20complex%20dynamics%0Aremains%20unrealized.%20Geoscience%20foundation%20models%20%28GFMs%29%20emerge%20as%20a%0Aparadigm-shifting%20solution%2C%20integrating%20extensive%20cross-disciplinary%20data%20to%0Aenhance%20the%20simulation%20and%20understanding%20of%20Earth%20system%20dynamics.%20These%0Adata-centric%20AI%20models%20extract%20insights%20from%20petabytes%20of%20structured%20and%0Aunstructured%20data%2C%20effectively%20addressing%20the%20complexities%20of%20Earth%20systems%0Athat%20traditional%20models%20struggle%20to%20capture.%20The%20unique%20strengths%20of%20GFMs%0Ainclude%20flexible%20task%20specification%2C%20diverse%20input-output%20capabilities%2C%20and%0Amulti-modal%20knowledge%20representation%2C%20enabling%20analyses%20that%20surpass%20those%20of%0Aindividual%20data%20sources%20or%20traditional%20AI%20methods.%20This%20review%20not%20only%0Ahighlights%20the%20key%20advantages%20of%20GFMs%2C%20but%20also%20presents%20essential%20techniques%0Afor%20their%20construction%2C%20with%20a%20focus%20on%20transformers%2C%20pre-training%2C%20and%0Aadaptation%20strategies.%20Subsequently%2C%20we%20examine%20recent%20advancements%20in%20GFMs%2C%0Aincluding%20large%20language%20models%2C%20vision%20models%2C%20and%20vision-language%20models%2C%0Aparticularly%20emphasizing%20the%20potential%20applications%20in%20remote%20sensing.%0AAdditionally%2C%20the%20review%20concludes%20with%20a%20comprehensive%20analysis%20of%20the%0Achallenges%20and%20future%20trends%20in%20GFMs%2C%20addressing%20five%20critical%20aspects%3A%20data%0Aintegration%2C%20model%20complexity%2C%20uncertainty%20quantification%2C%20interdisciplinary%0Acollaboration%2C%20and%20concerns%20related%20to%20privacy%2C%20trust%2C%20and%20security.%20This%0Areview%20offers%20a%20comprehensive%20overview%20of%20emerging%20geoscientific%20research%0Aparadigms%2C%20emphasizing%20the%20untapped%20opportunities%20at%20the%20intersection%20of%0Aadvanced%20AI%20techniques%20and%20geoscience.%20It%20examines%20major%20methodologies%2C%0Ashowcases%20advances%20in%20large-scale%20models%2C%20and%20discusses%20the%20challenges%20and%0Aprospects%20that%20will%20shape%20the%20future%20landscape%20of%20GFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06799v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Geoscience%2520Meets%2520Foundation%2520Models%253A%2520Towards%2520General%2520Geoscience%250A%2520%2520Artificial%2520Intelligence%2520System%26entry.906535625%3DHao%2520Zhang%2520and%2520Jin-Jian%2520Xu%2520and%2520Hong-Wei%2520Cui%2520and%2520Lin%2520Li%2520and%2520Yaowen%2520Yang%2520and%2520Chao-Sheng%2520Tang%2520and%2520Niklas%2520Boers%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520significantly%2520advanced%2520Earth%2520sciences%252C%2520yet%250Aits%2520full%2520potential%2520in%2520to%2520comprehensively%2520modeling%2520Earth%2527s%2520complex%2520dynamics%250Aremains%2520unrealized.%2520Geoscience%2520foundation%2520models%2520%2528GFMs%2529%2520emerge%2520as%2520a%250Aparadigm-shifting%2520solution%252C%2520integrating%2520extensive%2520cross-disciplinary%2520data%2520to%250Aenhance%2520the%2520simulation%2520and%2520understanding%2520of%2520Earth%2520system%2520dynamics.%2520These%250Adata-centric%2520AI%2520models%2520extract%2520insights%2520from%2520petabytes%2520of%2520structured%2520and%250Aunstructured%2520data%252C%2520effectively%2520addressing%2520the%2520complexities%2520of%2520Earth%2520systems%250Athat%2520traditional%2520models%2520struggle%2520to%2520capture.%2520The%2520unique%2520strengths%2520of%2520GFMs%250Ainclude%2520flexible%2520task%2520specification%252C%2520diverse%2520input-output%2520capabilities%252C%2520and%250Amulti-modal%2520knowledge%2520representation%252C%2520enabling%2520analyses%2520that%2520surpass%2520those%2520of%250Aindividual%2520data%2520sources%2520or%2520traditional%2520AI%2520methods.%2520This%2520review%2520not%2520only%250Ahighlights%2520the%2520key%2520advantages%2520of%2520GFMs%252C%2520but%2520also%2520presents%2520essential%2520techniques%250Afor%2520their%2520construction%252C%2520with%2520a%2520focus%2520on%2520transformers%252C%2520pre-training%252C%2520and%250Aadaptation%2520strategies.%2520Subsequently%252C%2520we%2520examine%2520recent%2520advancements%2520in%2520GFMs%252C%250Aincluding%2520large%2520language%2520models%252C%2520vision%2520models%252C%2520and%2520vision-language%2520models%252C%250Aparticularly%2520emphasizing%2520the%2520potential%2520applications%2520in%2520remote%2520sensing.%250AAdditionally%252C%2520the%2520review%2520concludes%2520with%2520a%2520comprehensive%2520analysis%2520of%2520the%250Achallenges%2520and%2520future%2520trends%2520in%2520GFMs%252C%2520addressing%2520five%2520critical%2520aspects%253A%2520data%250Aintegration%252C%2520model%2520complexity%252C%2520uncertainty%2520quantification%252C%2520interdisciplinary%250Acollaboration%252C%2520and%2520concerns%2520related%2520to%2520privacy%252C%2520trust%252C%2520and%2520security.%2520This%250Areview%2520offers%2520a%2520comprehensive%2520overview%2520of%2520emerging%2520geoscientific%2520research%250Aparadigms%252C%2520emphasizing%2520the%2520untapped%2520opportunities%2520at%2520the%2520intersection%2520of%250Aadvanced%2520AI%2520techniques%2520and%2520geoscience.%2520It%2520examines%2520major%2520methodologies%252C%250Ashowcases%2520advances%2520in%2520large-scale%2520models%252C%2520and%2520discusses%2520the%2520challenges%2520and%250Aprospects%2520that%2520will%2520shape%2520the%2520future%2520landscape%2520of%2520GFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06799v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Geoscience%20Meets%20Foundation%20Models%3A%20Towards%20General%20Geoscience%0A%20%20Artificial%20Intelligence%20System&entry.906535625=Hao%20Zhang%20and%20Jin-Jian%20Xu%20and%20Hong-Wei%20Cui%20and%20Lin%20Li%20and%20Yaowen%20Yang%20and%20Chao-Sheng%20Tang%20and%20Niklas%20Boers&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20significantly%20advanced%20Earth%20sciences%2C%20yet%0Aits%20full%20potential%20in%20to%20comprehensively%20modeling%20Earth%27s%20complex%20dynamics%0Aremains%20unrealized.%20Geoscience%20foundation%20models%20%28GFMs%29%20emerge%20as%20a%0Aparadigm-shifting%20solution%2C%20integrating%20extensive%20cross-disciplinary%20data%20to%0Aenhance%20the%20simulation%20and%20understanding%20of%20Earth%20system%20dynamics.%20These%0Adata-centric%20AI%20models%20extract%20insights%20from%20petabytes%20of%20structured%20and%0Aunstructured%20data%2C%20effectively%20addressing%20the%20complexities%20of%20Earth%20systems%0Athat%20traditional%20models%20struggle%20to%20capture.%20The%20unique%20strengths%20of%20GFMs%0Ainclude%20flexible%20task%20specification%2C%20diverse%20input-output%20capabilities%2C%20and%0Amulti-modal%20knowledge%20representation%2C%20enabling%20analyses%20that%20surpass%20those%20of%0Aindividual%20data%20sources%20or%20traditional%20AI%20methods.%20This%20review%20not%20only%0Ahighlights%20the%20key%20advantages%20of%20GFMs%2C%20but%20also%20presents%20essential%20techniques%0Afor%20their%20construction%2C%20with%20a%20focus%20on%20transformers%2C%20pre-training%2C%20and%0Aadaptation%20strategies.%20Subsequently%2C%20we%20examine%20recent%20advancements%20in%20GFMs%2C%0Aincluding%20large%20language%20models%2C%20vision%20models%2C%20and%20vision-language%20models%2C%0Aparticularly%20emphasizing%20the%20potential%20applications%20in%20remote%20sensing.%0AAdditionally%2C%20the%20review%20concludes%20with%20a%20comprehensive%20analysis%20of%20the%0Achallenges%20and%20future%20trends%20in%20GFMs%2C%20addressing%20five%20critical%20aspects%3A%20data%0Aintegration%2C%20model%20complexity%2C%20uncertainty%20quantification%2C%20interdisciplinary%0Acollaboration%2C%20and%20concerns%20related%20to%20privacy%2C%20trust%2C%20and%20security.%20This%0Areview%20offers%20a%20comprehensive%20overview%20of%20emerging%20geoscientific%20research%0Aparadigms%2C%20emphasizing%20the%20untapped%20opportunities%20at%20the%20intersection%20of%0Aadvanced%20AI%20techniques%20and%20geoscience.%20It%20examines%20major%20methodologies%2C%0Ashowcases%20advances%20in%20large-scale%20models%2C%20and%20discusses%20the%20challenges%20and%0Aprospects%20that%20will%20shape%20the%20future%20landscape%20of%20GFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06799v5&entry.124074799=Read"},
{"title": "Constraint Learning for Parametric Point Cloud", "author": "Xi Cheng and Ruiqi Lei and Di Huang and Zhichao Liao and Fengyuan Piao and Yan Chen and Pingfa Feng and Long Zeng", "abstract": "  Parametric point clouds are sampled from CAD shapes, have become increasingly\nprevalent in industrial manufacturing. However, most existing point cloud\nlearning methods focus on the geometric features, such as local and global\nfeatures or developing efficient convolution operations, overlooking the\nimportant attribute of constraints inherent in CAD shapes, which limits these\nmethods' ability to fully comprehend CAD shapes. To address this issue, we\nanalyzed the effect of constraints, and proposed its deep learning-friendly\nrepresentation, after that, the Constraint Feature Learning Network (CstNet) is\ndeveloped to extract and leverage constraints. Our CstNet includes two stages.\nThe Stage 1 extracts constraints from B-Rep data or point cloud. The Stage 2\nleverages coordinates and constraints to enhance the comprehend of CAD shapes.\nAdditionally, we built up the Parametric 20,000 Multi-modal Dataset for the\nscarcity of labeled B-Rep datasets. Experiments demonstrate that our CstNet\nachieved state-of-the-art performance on both public and proposed CAD shapes\ndatasets. To the best of our knowledge, CstNet is the first constraint-based\nlearning method tailored for CAD shapes analysis.\n", "link": "http://arxiv.org/abs/2411.07747v1", "date": "2024-11-12", "relevancy": 2.5773, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5143}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraint%20Learning%20for%20Parametric%20Point%20Cloud&body=Title%3A%20Constraint%20Learning%20for%20Parametric%20Point%20Cloud%0AAuthor%3A%20Xi%20Cheng%20and%20Ruiqi%20Lei%20and%20Di%20Huang%20and%20Zhichao%20Liao%20and%20Fengyuan%20Piao%20and%20Yan%20Chen%20and%20Pingfa%20Feng%20and%20Long%20Zeng%0AAbstract%3A%20%20%20Parametric%20point%20clouds%20are%20sampled%20from%20CAD%20shapes%2C%20have%20become%20increasingly%0Aprevalent%20in%20industrial%20manufacturing.%20However%2C%20most%20existing%20point%20cloud%0Alearning%20methods%20focus%20on%20the%20geometric%20features%2C%20such%20as%20local%20and%20global%0Afeatures%20or%20developing%20efficient%20convolution%20operations%2C%20overlooking%20the%0Aimportant%20attribute%20of%20constraints%20inherent%20in%20CAD%20shapes%2C%20which%20limits%20these%0Amethods%27%20ability%20to%20fully%20comprehend%20CAD%20shapes.%20To%20address%20this%20issue%2C%20we%0Aanalyzed%20the%20effect%20of%20constraints%2C%20and%20proposed%20its%20deep%20learning-friendly%0Arepresentation%2C%20after%20that%2C%20the%20Constraint%20Feature%20Learning%20Network%20%28CstNet%29%20is%0Adeveloped%20to%20extract%20and%20leverage%20constraints.%20Our%20CstNet%20includes%20two%20stages.%0AThe%20Stage%201%20extracts%20constraints%20from%20B-Rep%20data%20or%20point%20cloud.%20The%20Stage%202%0Aleverages%20coordinates%20and%20constraints%20to%20enhance%20the%20comprehend%20of%20CAD%20shapes.%0AAdditionally%2C%20we%20built%20up%20the%20Parametric%2020%2C000%20Multi-modal%20Dataset%20for%20the%0Ascarcity%20of%20labeled%20B-Rep%20datasets.%20Experiments%20demonstrate%20that%20our%20CstNet%0Aachieved%20state-of-the-art%20performance%20on%20both%20public%20and%20proposed%20CAD%20shapes%0Adatasets.%20To%20the%20best%20of%20our%20knowledge%2C%20CstNet%20is%20the%20first%20constraint-based%0Alearning%20method%20tailored%20for%20CAD%20shapes%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraint%2520Learning%2520for%2520Parametric%2520Point%2520Cloud%26entry.906535625%3DXi%2520Cheng%2520and%2520Ruiqi%2520Lei%2520and%2520Di%2520Huang%2520and%2520Zhichao%2520Liao%2520and%2520Fengyuan%2520Piao%2520and%2520Yan%2520Chen%2520and%2520Pingfa%2520Feng%2520and%2520Long%2520Zeng%26entry.1292438233%3D%2520%2520Parametric%2520point%2520clouds%2520are%2520sampled%2520from%2520CAD%2520shapes%252C%2520have%2520become%2520increasingly%250Aprevalent%2520in%2520industrial%2520manufacturing.%2520However%252C%2520most%2520existing%2520point%2520cloud%250Alearning%2520methods%2520focus%2520on%2520the%2520geometric%2520features%252C%2520such%2520as%2520local%2520and%2520global%250Afeatures%2520or%2520developing%2520efficient%2520convolution%2520operations%252C%2520overlooking%2520the%250Aimportant%2520attribute%2520of%2520constraints%2520inherent%2520in%2520CAD%2520shapes%252C%2520which%2520limits%2520these%250Amethods%2527%2520ability%2520to%2520fully%2520comprehend%2520CAD%2520shapes.%2520To%2520address%2520this%2520issue%252C%2520we%250Aanalyzed%2520the%2520effect%2520of%2520constraints%252C%2520and%2520proposed%2520its%2520deep%2520learning-friendly%250Arepresentation%252C%2520after%2520that%252C%2520the%2520Constraint%2520Feature%2520Learning%2520Network%2520%2528CstNet%2529%2520is%250Adeveloped%2520to%2520extract%2520and%2520leverage%2520constraints.%2520Our%2520CstNet%2520includes%2520two%2520stages.%250AThe%2520Stage%25201%2520extracts%2520constraints%2520from%2520B-Rep%2520data%2520or%2520point%2520cloud.%2520The%2520Stage%25202%250Aleverages%2520coordinates%2520and%2520constraints%2520to%2520enhance%2520the%2520comprehend%2520of%2520CAD%2520shapes.%250AAdditionally%252C%2520we%2520built%2520up%2520the%2520Parametric%252020%252C000%2520Multi-modal%2520Dataset%2520for%2520the%250Ascarcity%2520of%2520labeled%2520B-Rep%2520datasets.%2520Experiments%2520demonstrate%2520that%2520our%2520CstNet%250Aachieved%2520state-of-the-art%2520performance%2520on%2520both%2520public%2520and%2520proposed%2520CAD%2520shapes%250Adatasets.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520CstNet%2520is%2520the%2520first%2520constraint-based%250Alearning%2520method%2520tailored%2520for%2520CAD%2520shapes%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraint%20Learning%20for%20Parametric%20Point%20Cloud&entry.906535625=Xi%20Cheng%20and%20Ruiqi%20Lei%20and%20Di%20Huang%20and%20Zhichao%20Liao%20and%20Fengyuan%20Piao%20and%20Yan%20Chen%20and%20Pingfa%20Feng%20and%20Long%20Zeng&entry.1292438233=%20%20Parametric%20point%20clouds%20are%20sampled%20from%20CAD%20shapes%2C%20have%20become%20increasingly%0Aprevalent%20in%20industrial%20manufacturing.%20However%2C%20most%20existing%20point%20cloud%0Alearning%20methods%20focus%20on%20the%20geometric%20features%2C%20such%20as%20local%20and%20global%0Afeatures%20or%20developing%20efficient%20convolution%20operations%2C%20overlooking%20the%0Aimportant%20attribute%20of%20constraints%20inherent%20in%20CAD%20shapes%2C%20which%20limits%20these%0Amethods%27%20ability%20to%20fully%20comprehend%20CAD%20shapes.%20To%20address%20this%20issue%2C%20we%0Aanalyzed%20the%20effect%20of%20constraints%2C%20and%20proposed%20its%20deep%20learning-friendly%0Arepresentation%2C%20after%20that%2C%20the%20Constraint%20Feature%20Learning%20Network%20%28CstNet%29%20is%0Adeveloped%20to%20extract%20and%20leverage%20constraints.%20Our%20CstNet%20includes%20two%20stages.%0AThe%20Stage%201%20extracts%20constraints%20from%20B-Rep%20data%20or%20point%20cloud.%20The%20Stage%202%0Aleverages%20coordinates%20and%20constraints%20to%20enhance%20the%20comprehend%20of%20CAD%20shapes.%0AAdditionally%2C%20we%20built%20up%20the%20Parametric%2020%2C000%20Multi-modal%20Dataset%20for%20the%0Ascarcity%20of%20labeled%20B-Rep%20datasets.%20Experiments%20demonstrate%20that%20our%20CstNet%0Aachieved%20state-of-the-art%20performance%20on%20both%20public%20and%20proposed%20CAD%20shapes%0Adatasets.%20To%20the%20best%20of%20our%20knowledge%2C%20CstNet%20is%20the%20first%20constraint-based%0Alearning%20method%20tailored%20for%20CAD%20shapes%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07747v1&entry.124074799=Read"},
{"title": "Isometric Transformations for Image Augmentation in Mueller Matrix\n  Polarimetry", "author": "Christopher Hahne and Omar Rodriguez-Nunez and \u00c9l\u00e9a Gros and Th\u00e9otim Lucas and Ekkehard Hewer and Tatiana Novikova and Theoni Maragkou and Philippe Schucht and Richard McKinley", "abstract": "  Mueller matrix polarimetry captures essential information about polarized\nlight interactions with a sample, presenting unique challenges for data\naugmentation in deep learning due to its distinct structure. While\naugmentations are an effective and affordable way to enhance dataset diversity\nand reduce overfitting, standard transformations like rotations and flips do\nnot preserve the polarization properties in Mueller matrix images. To this end,\nwe introduce a versatile simulation framework that applies physically\nconsistent rotations and flips to Mueller matrices, tailored to maintain\npolarization fidelity. Our experimental results across multiple datasets reveal\nthat conventional augmentations can lead to misleading results when applied to\npolarimetric data, underscoring the necessity of our physics-based approach. In\nour experiments, we first compare our polarization-specific augmentations\nagainst real-world captures to validate their physical consistency. We then\napply these augmentations in a semantic segmentation task, achieving\nsubstantial improvements in model generalization and performance. This study\nunderscores the necessity of physics-informed data augmentation for\npolarimetric imaging in deep learning (DL), paving the way for broader adoption\nand more robust applications across diverse research in the field. In\nparticular, our framework unlocks the potential of DL models for polarimetric\ndatasets with limited sample sizes. Our code implementation is available at\ngithub.com/hahnec/polar_augment.\n", "link": "http://arxiv.org/abs/2411.07918v1", "date": "2024-11-12", "relevancy": 2.5688, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.52}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Isometric%20Transformations%20for%20Image%20Augmentation%20in%20Mueller%20Matrix%0A%20%20Polarimetry&body=Title%3A%20Isometric%20Transformations%20for%20Image%20Augmentation%20in%20Mueller%20Matrix%0A%20%20Polarimetry%0AAuthor%3A%20Christopher%20Hahne%20and%20Omar%20Rodriguez-Nunez%20and%20%C3%89l%C3%A9a%20Gros%20and%20Th%C3%A9otim%20Lucas%20and%20Ekkehard%20Hewer%20and%20Tatiana%20Novikova%20and%20Theoni%20Maragkou%20and%20Philippe%20Schucht%20and%20Richard%20McKinley%0AAbstract%3A%20%20%20Mueller%20matrix%20polarimetry%20captures%20essential%20information%20about%20polarized%0Alight%20interactions%20with%20a%20sample%2C%20presenting%20unique%20challenges%20for%20data%0Aaugmentation%20in%20deep%20learning%20due%20to%20its%20distinct%20structure.%20While%0Aaugmentations%20are%20an%20effective%20and%20affordable%20way%20to%20enhance%20dataset%20diversity%0Aand%20reduce%20overfitting%2C%20standard%20transformations%20like%20rotations%20and%20flips%20do%0Anot%20preserve%20the%20polarization%20properties%20in%20Mueller%20matrix%20images.%20To%20this%20end%2C%0Awe%20introduce%20a%20versatile%20simulation%20framework%20that%20applies%20physically%0Aconsistent%20rotations%20and%20flips%20to%20Mueller%20matrices%2C%20tailored%20to%20maintain%0Apolarization%20fidelity.%20Our%20experimental%20results%20across%20multiple%20datasets%20reveal%0Athat%20conventional%20augmentations%20can%20lead%20to%20misleading%20results%20when%20applied%20to%0Apolarimetric%20data%2C%20underscoring%20the%20necessity%20of%20our%20physics-based%20approach.%20In%0Aour%20experiments%2C%20we%20first%20compare%20our%20polarization-specific%20augmentations%0Aagainst%20real-world%20captures%20to%20validate%20their%20physical%20consistency.%20We%20then%0Aapply%20these%20augmentations%20in%20a%20semantic%20segmentation%20task%2C%20achieving%0Asubstantial%20improvements%20in%20model%20generalization%20and%20performance.%20This%20study%0Aunderscores%20the%20necessity%20of%20physics-informed%20data%20augmentation%20for%0Apolarimetric%20imaging%20in%20deep%20learning%20%28DL%29%2C%20paving%20the%20way%20for%20broader%20adoption%0Aand%20more%20robust%20applications%20across%20diverse%20research%20in%20the%20field.%20In%0Aparticular%2C%20our%20framework%20unlocks%20the%20potential%20of%20DL%20models%20for%20polarimetric%0Adatasets%20with%20limited%20sample%20sizes.%20Our%20code%20implementation%20is%20available%20at%0Agithub.com/hahnec/polar_augment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIsometric%2520Transformations%2520for%2520Image%2520Augmentation%2520in%2520Mueller%2520Matrix%250A%2520%2520Polarimetry%26entry.906535625%3DChristopher%2520Hahne%2520and%2520Omar%2520Rodriguez-Nunez%2520and%2520%25C3%2589l%25C3%25A9a%2520Gros%2520and%2520Th%25C3%25A9otim%2520Lucas%2520and%2520Ekkehard%2520Hewer%2520and%2520Tatiana%2520Novikova%2520and%2520Theoni%2520Maragkou%2520and%2520Philippe%2520Schucht%2520and%2520Richard%2520McKinley%26entry.1292438233%3D%2520%2520Mueller%2520matrix%2520polarimetry%2520captures%2520essential%2520information%2520about%2520polarized%250Alight%2520interactions%2520with%2520a%2520sample%252C%2520presenting%2520unique%2520challenges%2520for%2520data%250Aaugmentation%2520in%2520deep%2520learning%2520due%2520to%2520its%2520distinct%2520structure.%2520While%250Aaugmentations%2520are%2520an%2520effective%2520and%2520affordable%2520way%2520to%2520enhance%2520dataset%2520diversity%250Aand%2520reduce%2520overfitting%252C%2520standard%2520transformations%2520like%2520rotations%2520and%2520flips%2520do%250Anot%2520preserve%2520the%2520polarization%2520properties%2520in%2520Mueller%2520matrix%2520images.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520a%2520versatile%2520simulation%2520framework%2520that%2520applies%2520physically%250Aconsistent%2520rotations%2520and%2520flips%2520to%2520Mueller%2520matrices%252C%2520tailored%2520to%2520maintain%250Apolarization%2520fidelity.%2520Our%2520experimental%2520results%2520across%2520multiple%2520datasets%2520reveal%250Athat%2520conventional%2520augmentations%2520can%2520lead%2520to%2520misleading%2520results%2520when%2520applied%2520to%250Apolarimetric%2520data%252C%2520underscoring%2520the%2520necessity%2520of%2520our%2520physics-based%2520approach.%2520In%250Aour%2520experiments%252C%2520we%2520first%2520compare%2520our%2520polarization-specific%2520augmentations%250Aagainst%2520real-world%2520captures%2520to%2520validate%2520their%2520physical%2520consistency.%2520We%2520then%250Aapply%2520these%2520augmentations%2520in%2520a%2520semantic%2520segmentation%2520task%252C%2520achieving%250Asubstantial%2520improvements%2520in%2520model%2520generalization%2520and%2520performance.%2520This%2520study%250Aunderscores%2520the%2520necessity%2520of%2520physics-informed%2520data%2520augmentation%2520for%250Apolarimetric%2520imaging%2520in%2520deep%2520learning%2520%2528DL%2529%252C%2520paving%2520the%2520way%2520for%2520broader%2520adoption%250Aand%2520more%2520robust%2520applications%2520across%2520diverse%2520research%2520in%2520the%2520field.%2520In%250Aparticular%252C%2520our%2520framework%2520unlocks%2520the%2520potential%2520of%2520DL%2520models%2520for%2520polarimetric%250Adatasets%2520with%2520limited%2520sample%2520sizes.%2520Our%2520code%2520implementation%2520is%2520available%2520at%250Agithub.com/hahnec/polar_augment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Isometric%20Transformations%20for%20Image%20Augmentation%20in%20Mueller%20Matrix%0A%20%20Polarimetry&entry.906535625=Christopher%20Hahne%20and%20Omar%20Rodriguez-Nunez%20and%20%C3%89l%C3%A9a%20Gros%20and%20Th%C3%A9otim%20Lucas%20and%20Ekkehard%20Hewer%20and%20Tatiana%20Novikova%20and%20Theoni%20Maragkou%20and%20Philippe%20Schucht%20and%20Richard%20McKinley&entry.1292438233=%20%20Mueller%20matrix%20polarimetry%20captures%20essential%20information%20about%20polarized%0Alight%20interactions%20with%20a%20sample%2C%20presenting%20unique%20challenges%20for%20data%0Aaugmentation%20in%20deep%20learning%20due%20to%20its%20distinct%20structure.%20While%0Aaugmentations%20are%20an%20effective%20and%20affordable%20way%20to%20enhance%20dataset%20diversity%0Aand%20reduce%20overfitting%2C%20standard%20transformations%20like%20rotations%20and%20flips%20do%0Anot%20preserve%20the%20polarization%20properties%20in%20Mueller%20matrix%20images.%20To%20this%20end%2C%0Awe%20introduce%20a%20versatile%20simulation%20framework%20that%20applies%20physically%0Aconsistent%20rotations%20and%20flips%20to%20Mueller%20matrices%2C%20tailored%20to%20maintain%0Apolarization%20fidelity.%20Our%20experimental%20results%20across%20multiple%20datasets%20reveal%0Athat%20conventional%20augmentations%20can%20lead%20to%20misleading%20results%20when%20applied%20to%0Apolarimetric%20data%2C%20underscoring%20the%20necessity%20of%20our%20physics-based%20approach.%20In%0Aour%20experiments%2C%20we%20first%20compare%20our%20polarization-specific%20augmentations%0Aagainst%20real-world%20captures%20to%20validate%20their%20physical%20consistency.%20We%20then%0Aapply%20these%20augmentations%20in%20a%20semantic%20segmentation%20task%2C%20achieving%0Asubstantial%20improvements%20in%20model%20generalization%20and%20performance.%20This%20study%0Aunderscores%20the%20necessity%20of%20physics-informed%20data%20augmentation%20for%0Apolarimetric%20imaging%20in%20deep%20learning%20%28DL%29%2C%20paving%20the%20way%20for%20broader%20adoption%0Aand%20more%20robust%20applications%20across%20diverse%20research%20in%20the%20field.%20In%0Aparticular%2C%20our%20framework%20unlocks%20the%20potential%20of%20DL%20models%20for%20polarimetric%0Adatasets%20with%20limited%20sample%20sizes.%20Our%20code%20implementation%20is%20available%20at%0Agithub.com/hahnec/polar_augment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07918v1&entry.124074799=Read"},
{"title": "SimBase: A Simple Baseline for Temporal Video Grounding", "author": "Peijun Bao and Alex C. Kot", "abstract": "  This paper presents SimBase, a simple yet effective baseline for temporal\nvideo grounding. While recent advances in temporal grounding have led to\nimpressive performance, they have also driven network architectures toward\ngreater complexity, with a range of methods to (1) capture temporal\nrelationships and (2) achieve effective multimodal fusion. In contrast, this\npaper explores the question: How effective can a simplified approach be? To\ninvestigate, we design SimBase, a network that leverages lightweight,\none-dimensional temporal convolutional layers instead of complex temporal\nstructures. For cross-modal interaction, SimBase only employs an element-wise\nproduct instead of intricate multimodal fusion. Remarkably, SimBase achieves\nstate-of-the-art results on two large-scale datasets. As a simple yet powerful\nbaseline, we hope SimBase will spark new ideas and streamline future\nevaluations in temporal video grounding.\n", "link": "http://arxiv.org/abs/2411.07945v1", "date": "2024-11-12", "relevancy": 2.5333, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5149}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimBase%3A%20A%20Simple%20Baseline%20for%20Temporal%20Video%20Grounding&body=Title%3A%20SimBase%3A%20A%20Simple%20Baseline%20for%20Temporal%20Video%20Grounding%0AAuthor%3A%20Peijun%20Bao%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20This%20paper%20presents%20SimBase%2C%20a%20simple%20yet%20effective%20baseline%20for%20temporal%0Avideo%20grounding.%20While%20recent%20advances%20in%20temporal%20grounding%20have%20led%20to%0Aimpressive%20performance%2C%20they%20have%20also%20driven%20network%20architectures%20toward%0Agreater%20complexity%2C%20with%20a%20range%20of%20methods%20to%20%281%29%20capture%20temporal%0Arelationships%20and%20%282%29%20achieve%20effective%20multimodal%20fusion.%20In%20contrast%2C%20this%0Apaper%20explores%20the%20question%3A%20How%20effective%20can%20a%20simplified%20approach%20be%3F%20To%0Ainvestigate%2C%20we%20design%20SimBase%2C%20a%20network%20that%20leverages%20lightweight%2C%0Aone-dimensional%20temporal%20convolutional%20layers%20instead%20of%20complex%20temporal%0Astructures.%20For%20cross-modal%20interaction%2C%20SimBase%20only%20employs%20an%20element-wise%0Aproduct%20instead%20of%20intricate%20multimodal%20fusion.%20Remarkably%2C%20SimBase%20achieves%0Astate-of-the-art%20results%20on%20two%20large-scale%20datasets.%20As%20a%20simple%20yet%20powerful%0Abaseline%2C%20we%20hope%20SimBase%20will%20spark%20new%20ideas%20and%20streamline%20future%0Aevaluations%20in%20temporal%20video%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimBase%253A%2520A%2520Simple%2520Baseline%2520for%2520Temporal%2520Video%2520Grounding%26entry.906535625%3DPeijun%2520Bao%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520SimBase%252C%2520a%2520simple%2520yet%2520effective%2520baseline%2520for%2520temporal%250Avideo%2520grounding.%2520While%2520recent%2520advances%2520in%2520temporal%2520grounding%2520have%2520led%2520to%250Aimpressive%2520performance%252C%2520they%2520have%2520also%2520driven%2520network%2520architectures%2520toward%250Agreater%2520complexity%252C%2520with%2520a%2520range%2520of%2520methods%2520to%2520%25281%2529%2520capture%2520temporal%250Arelationships%2520and%2520%25282%2529%2520achieve%2520effective%2520multimodal%2520fusion.%2520In%2520contrast%252C%2520this%250Apaper%2520explores%2520the%2520question%253A%2520How%2520effective%2520can%2520a%2520simplified%2520approach%2520be%253F%2520To%250Ainvestigate%252C%2520we%2520design%2520SimBase%252C%2520a%2520network%2520that%2520leverages%2520lightweight%252C%250Aone-dimensional%2520temporal%2520convolutional%2520layers%2520instead%2520of%2520complex%2520temporal%250Astructures.%2520For%2520cross-modal%2520interaction%252C%2520SimBase%2520only%2520employs%2520an%2520element-wise%250Aproduct%2520instead%2520of%2520intricate%2520multimodal%2520fusion.%2520Remarkably%252C%2520SimBase%2520achieves%250Astate-of-the-art%2520results%2520on%2520two%2520large-scale%2520datasets.%2520As%2520a%2520simple%2520yet%2520powerful%250Abaseline%252C%2520we%2520hope%2520SimBase%2520will%2520spark%2520new%2520ideas%2520and%2520streamline%2520future%250Aevaluations%2520in%2520temporal%2520video%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimBase%3A%20A%20Simple%20Baseline%20for%20Temporal%20Video%20Grounding&entry.906535625=Peijun%20Bao%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20This%20paper%20presents%20SimBase%2C%20a%20simple%20yet%20effective%20baseline%20for%20temporal%0Avideo%20grounding.%20While%20recent%20advances%20in%20temporal%20grounding%20have%20led%20to%0Aimpressive%20performance%2C%20they%20have%20also%20driven%20network%20architectures%20toward%0Agreater%20complexity%2C%20with%20a%20range%20of%20methods%20to%20%281%29%20capture%20temporal%0Arelationships%20and%20%282%29%20achieve%20effective%20multimodal%20fusion.%20In%20contrast%2C%20this%0Apaper%20explores%20the%20question%3A%20How%20effective%20can%20a%20simplified%20approach%20be%3F%20To%0Ainvestigate%2C%20we%20design%20SimBase%2C%20a%20network%20that%20leverages%20lightweight%2C%0Aone-dimensional%20temporal%20convolutional%20layers%20instead%20of%20complex%20temporal%0Astructures.%20For%20cross-modal%20interaction%2C%20SimBase%20only%20employs%20an%20element-wise%0Aproduct%20instead%20of%20intricate%20multimodal%20fusion.%20Remarkably%2C%20SimBase%20achieves%0Astate-of-the-art%20results%20on%20two%20large-scale%20datasets.%20As%20a%20simple%20yet%20powerful%0Abaseline%2C%20we%20hope%20SimBase%20will%20spark%20new%20ideas%20and%20streamline%20future%0Aevaluations%20in%20temporal%20video%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07945v1&entry.124074799=Read"},
{"title": "What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?", "author": "Katie Kang and Amrith Setlur and Dibya Ghosh and Jacob Steinhardt and Claire Tomlin and Sergey Levine and Aviral Kumar", "abstract": "  Despite the remarkable capabilities of modern large language models (LLMs),\nthe mechanisms behind their problem-solving abilities remain elusive. In this\nwork, we aim to better understand how the learning dynamics of LLM finetuning\nshapes downstream generalization. Our analysis focuses on reasoning tasks,\nwhose problem structure allows us to distinguish between memorization (the\nexact replication of reasoning steps from the training data) and performance\n(the correctness of the final solution). We find that a model's generalization\nbehavior can be effectively characterized by a training metric we call\npre-memorization train accuracy: the accuracy of model samples on training\nqueries before they begin to copy the exact reasoning steps from the training\nset. On the dataset level, this metric is able to reliably predict test\naccuracy, achieving $R^2$ of around or exceeding 0.9 across various models\n(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On\na per-example level, this metric is also indicative of whether individual model\npredictions are robust to perturbations in the training query. By connecting a\nmodel's learning behavior to its generalization, pre-memorization train\naccuracy can guide targeted improvements to training strategies. We focus on\ndata curation as an example, and show that prioritizing examples with low\npre-memorization accuracy leads to 1.5-2x improvements in data efficiency\ncompared to i.i.d. data scaling, and outperforms other standard data curation\ntechniques.\n", "link": "http://arxiv.org/abs/2411.07681v1", "date": "2024-11-12", "relevancy": 2.5199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F&body=Title%3A%20What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F%0AAuthor%3A%20Katie%20Kang%20and%20Amrith%20Setlur%20and%20Dibya%20Ghosh%20and%20Jacob%20Steinhardt%20and%20Claire%20Tomlin%20and%20Sergey%20Levine%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%0Athe%20mechanisms%20behind%20their%20problem-solving%20abilities%20remain%20elusive.%20In%20this%0Awork%2C%20we%20aim%20to%20better%20understand%20how%20the%20learning%20dynamics%20of%20LLM%20finetuning%0Ashapes%20downstream%20generalization.%20Our%20analysis%20focuses%20on%20reasoning%20tasks%2C%0Awhose%20problem%20structure%20allows%20us%20to%20distinguish%20between%20memorization%20%28the%0Aexact%20replication%20of%20reasoning%20steps%20from%20the%20training%20data%29%20and%20performance%0A%28the%20correctness%20of%20the%20final%20solution%29.%20We%20find%20that%20a%20model%27s%20generalization%0Abehavior%20can%20be%20effectively%20characterized%20by%20a%20training%20metric%20we%20call%0Apre-memorization%20train%20accuracy%3A%20the%20accuracy%20of%20model%20samples%20on%20training%0Aqueries%20before%20they%20begin%20to%20copy%20the%20exact%20reasoning%20steps%20from%20the%20training%0Aset.%20On%20the%20dataset%20level%2C%20this%20metric%20is%20able%20to%20reliably%20predict%20test%0Aaccuracy%2C%20achieving%20%24R%5E2%24%20of%20around%20or%20exceeding%200.9%20across%20various%20models%0A%28Llama3%208%2C%20Gemma2%209B%29%2C%20datasets%20%28GSM8k%2C%20MATH%29%2C%20and%20training%20configurations.%20On%0Aa%20per-example%20level%2C%20this%20metric%20is%20also%20indicative%20of%20whether%20individual%20model%0Apredictions%20are%20robust%20to%20perturbations%20in%20the%20training%20query.%20By%20connecting%20a%0Amodel%27s%20learning%20behavior%20to%20its%20generalization%2C%20pre-memorization%20train%0Aaccuracy%20can%20guide%20targeted%20improvements%20to%20training%20strategies.%20We%20focus%20on%0Adata%20curation%20as%20an%20example%2C%20and%20show%20that%20prioritizing%20examples%20with%20low%0Apre-memorization%20accuracy%20leads%20to%201.5-2x%20improvements%20in%20data%20efficiency%0Acompared%20to%20i.i.d.%20data%20scaling%2C%20and%20outperforms%20other%20standard%20data%20curation%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520Learning%2520Dynamics%2520Reveal%2520About%2520Generalization%2520in%2520LLM%2520Reasoning%253F%26entry.906535625%3DKatie%2520Kang%2520and%2520Amrith%2520Setlur%2520and%2520Dibya%2520Ghosh%2520and%2520Jacob%2520Steinhardt%2520and%2520Claire%2520Tomlin%2520and%2520Sergey%2520Levine%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520capabilities%2520of%2520modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Athe%2520mechanisms%2520behind%2520their%2520problem-solving%2520abilities%2520remain%2520elusive.%2520In%2520this%250Awork%252C%2520we%2520aim%2520to%2520better%2520understand%2520how%2520the%2520learning%2520dynamics%2520of%2520LLM%2520finetuning%250Ashapes%2520downstream%2520generalization.%2520Our%2520analysis%2520focuses%2520on%2520reasoning%2520tasks%252C%250Awhose%2520problem%2520structure%2520allows%2520us%2520to%2520distinguish%2520between%2520memorization%2520%2528the%250Aexact%2520replication%2520of%2520reasoning%2520steps%2520from%2520the%2520training%2520data%2529%2520and%2520performance%250A%2528the%2520correctness%2520of%2520the%2520final%2520solution%2529.%2520We%2520find%2520that%2520a%2520model%2527s%2520generalization%250Abehavior%2520can%2520be%2520effectively%2520characterized%2520by%2520a%2520training%2520metric%2520we%2520call%250Apre-memorization%2520train%2520accuracy%253A%2520the%2520accuracy%2520of%2520model%2520samples%2520on%2520training%250Aqueries%2520before%2520they%2520begin%2520to%2520copy%2520the%2520exact%2520reasoning%2520steps%2520from%2520the%2520training%250Aset.%2520On%2520the%2520dataset%2520level%252C%2520this%2520metric%2520is%2520able%2520to%2520reliably%2520predict%2520test%250Aaccuracy%252C%2520achieving%2520%2524R%255E2%2524%2520of%2520around%2520or%2520exceeding%25200.9%2520across%2520various%2520models%250A%2528Llama3%25208%252C%2520Gemma2%25209B%2529%252C%2520datasets%2520%2528GSM8k%252C%2520MATH%2529%252C%2520and%2520training%2520configurations.%2520On%250Aa%2520per-example%2520level%252C%2520this%2520metric%2520is%2520also%2520indicative%2520of%2520whether%2520individual%2520model%250Apredictions%2520are%2520robust%2520to%2520perturbations%2520in%2520the%2520training%2520query.%2520By%2520connecting%2520a%250Amodel%2527s%2520learning%2520behavior%2520to%2520its%2520generalization%252C%2520pre-memorization%2520train%250Aaccuracy%2520can%2520guide%2520targeted%2520improvements%2520to%2520training%2520strategies.%2520We%2520focus%2520on%250Adata%2520curation%2520as%2520an%2520example%252C%2520and%2520show%2520that%2520prioritizing%2520examples%2520with%2520low%250Apre-memorization%2520accuracy%2520leads%2520to%25201.5-2x%2520improvements%2520in%2520data%2520efficiency%250Acompared%2520to%2520i.i.d.%2520data%2520scaling%252C%2520and%2520outperforms%2520other%2520standard%2520data%2520curation%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20Learning%20Dynamics%20Reveal%20About%20Generalization%20in%20LLM%20Reasoning%3F&entry.906535625=Katie%20Kang%20and%20Amrith%20Setlur%20and%20Dibya%20Ghosh%20and%20Jacob%20Steinhardt%20and%20Claire%20Tomlin%20and%20Sergey%20Levine%20and%20Aviral%20Kumar&entry.1292438233=%20%20Despite%20the%20remarkable%20capabilities%20of%20modern%20large%20language%20models%20%28LLMs%29%2C%0Athe%20mechanisms%20behind%20their%20problem-solving%20abilities%20remain%20elusive.%20In%20this%0Awork%2C%20we%20aim%20to%20better%20understand%20how%20the%20learning%20dynamics%20of%20LLM%20finetuning%0Ashapes%20downstream%20generalization.%20Our%20analysis%20focuses%20on%20reasoning%20tasks%2C%0Awhose%20problem%20structure%20allows%20us%20to%20distinguish%20between%20memorization%20%28the%0Aexact%20replication%20of%20reasoning%20steps%20from%20the%20training%20data%29%20and%20performance%0A%28the%20correctness%20of%20the%20final%20solution%29.%20We%20find%20that%20a%20model%27s%20generalization%0Abehavior%20can%20be%20effectively%20characterized%20by%20a%20training%20metric%20we%20call%0Apre-memorization%20train%20accuracy%3A%20the%20accuracy%20of%20model%20samples%20on%20training%0Aqueries%20before%20they%20begin%20to%20copy%20the%20exact%20reasoning%20steps%20from%20the%20training%0Aset.%20On%20the%20dataset%20level%2C%20this%20metric%20is%20able%20to%20reliably%20predict%20test%0Aaccuracy%2C%20achieving%20%24R%5E2%24%20of%20around%20or%20exceeding%200.9%20across%20various%20models%0A%28Llama3%208%2C%20Gemma2%209B%29%2C%20datasets%20%28GSM8k%2C%20MATH%29%2C%20and%20training%20configurations.%20On%0Aa%20per-example%20level%2C%20this%20metric%20is%20also%20indicative%20of%20whether%20individual%20model%0Apredictions%20are%20robust%20to%20perturbations%20in%20the%20training%20query.%20By%20connecting%20a%0Amodel%27s%20learning%20behavior%20to%20its%20generalization%2C%20pre-memorization%20train%0Aaccuracy%20can%20guide%20targeted%20improvements%20to%20training%20strategies.%20We%20focus%20on%0Adata%20curation%20as%20an%20example%2C%20and%20show%20that%20prioritizing%20examples%20with%20low%0Apre-memorization%20accuracy%20leads%20to%201.5-2x%20improvements%20in%20data%20efficiency%0Acompared%20to%20i.i.d.%20data%20scaling%2C%20and%20outperforms%20other%20standard%20data%20curation%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07681v1&entry.124074799=Read"},
{"title": "Scalar Function Topology Divergence: Comparing Topology of 3D Objects", "author": "Ilya Trofimov and Daria Voronkova and Eduard Tulchinskii and Evgeny Burnaev and Serguei Barannikov", "abstract": "  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. The minimization of SFTD\nensures that the corresponding topological features of scalar functions are\nlocated in the same places. The proposed tool provides useful visualizations\ndepicting areas where functions have topological dissimilarities. We provide\napplications of the proposed method to 3D computer vision. In particular,\nexperiments demonstrate that SFTD as an additional loss improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation. Additionally, we\nshow that SFTD outperforms Betti matching loss in 2D segmentation problems.\n", "link": "http://arxiv.org/abs/2407.08364v3", "date": "2024-11-12", "relevancy": 2.4832, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5014}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects&body=Title%3A%20Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects%0AAuthor%3A%20Ilya%20Trofimov%20and%20Daria%20Voronkova%20and%20Eduard%20Tulchinskii%20and%20Evgeny%20Burnaev%20and%20Serguei%20Barannikov%0AAbstract%3A%20%20%20We%20propose%20a%20new%20topological%20tool%20for%20computer%20vision%20-%20Scalar%20Function%0ATopology%20Divergence%20%28SFTD%29%2C%20which%20measures%20the%20dissimilarity%20of%20multi-scale%0Atopology%20between%20sublevel%20sets%20of%20two%20functions%20having%20a%20common%20domain.%0AFunctions%20can%20be%20defined%20on%20an%20undirected%20graph%20or%20Euclidean%20space%20of%20any%0Adimensionality.%20Most%20of%20the%20existing%20methods%20for%20comparing%20topology%20are%20based%0Aon%20Wasserstein%20distance%20between%20persistence%20barcodes%20and%20they%20don%27t%20take%20into%0Aaccount%20the%20localization%20of%20topological%20features.%20The%20minimization%20of%20SFTD%0Aensures%20that%20the%20corresponding%20topological%20features%20of%20scalar%20functions%20are%0Alocated%20in%20the%20same%20places.%20The%20proposed%20tool%20provides%20useful%20visualizations%0Adepicting%20areas%20where%20functions%20have%20topological%20dissimilarities.%20We%20provide%0Aapplications%20of%20the%20proposed%20method%20to%203D%20computer%20vision.%20In%20particular%2C%0Aexperiments%20demonstrate%20that%20SFTD%20as%20an%20additional%20loss%20improves%20the%0Areconstruction%20of%20cellular%203D%20shapes%20from%202D%20fluorescence%20microscopy%20images%2C%0Aand%20helps%20to%20identify%20topological%20errors%20in%203D%20segmentation.%20Additionally%2C%20we%0Ashow%20that%20SFTD%20outperforms%20Betti%20matching%20loss%20in%202D%20segmentation%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalar%2520Function%2520Topology%2520Divergence%253A%2520Comparing%2520Topology%2520of%25203D%2520Objects%26entry.906535625%3DIlya%2520Trofimov%2520and%2520Daria%2520Voronkova%2520and%2520Eduard%2520Tulchinskii%2520and%2520Evgeny%2520Burnaev%2520and%2520Serguei%2520Barannikov%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520topological%2520tool%2520for%2520computer%2520vision%2520-%2520Scalar%2520Function%250ATopology%2520Divergence%2520%2528SFTD%2529%252C%2520which%2520measures%2520the%2520dissimilarity%2520of%2520multi-scale%250Atopology%2520between%2520sublevel%2520sets%2520of%2520two%2520functions%2520having%2520a%2520common%2520domain.%250AFunctions%2520can%2520be%2520defined%2520on%2520an%2520undirected%2520graph%2520or%2520Euclidean%2520space%2520of%2520any%250Adimensionality.%2520Most%2520of%2520the%2520existing%2520methods%2520for%2520comparing%2520topology%2520are%2520based%250Aon%2520Wasserstein%2520distance%2520between%2520persistence%2520barcodes%2520and%2520they%2520don%2527t%2520take%2520into%250Aaccount%2520the%2520localization%2520of%2520topological%2520features.%2520The%2520minimization%2520of%2520SFTD%250Aensures%2520that%2520the%2520corresponding%2520topological%2520features%2520of%2520scalar%2520functions%2520are%250Alocated%2520in%2520the%2520same%2520places.%2520The%2520proposed%2520tool%2520provides%2520useful%2520visualizations%250Adepicting%2520areas%2520where%2520functions%2520have%2520topological%2520dissimilarities.%2520We%2520provide%250Aapplications%2520of%2520the%2520proposed%2520method%2520to%25203D%2520computer%2520vision.%2520In%2520particular%252C%250Aexperiments%2520demonstrate%2520that%2520SFTD%2520as%2520an%2520additional%2520loss%2520improves%2520the%250Areconstruction%2520of%2520cellular%25203D%2520shapes%2520from%25202D%2520fluorescence%2520microscopy%2520images%252C%250Aand%2520helps%2520to%2520identify%2520topological%2520errors%2520in%25203D%2520segmentation.%2520Additionally%252C%2520we%250Ashow%2520that%2520SFTD%2520outperforms%2520Betti%2520matching%2520loss%2520in%25202D%2520segmentation%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalar%20Function%20Topology%20Divergence%3A%20Comparing%20Topology%20of%203D%20Objects&entry.906535625=Ilya%20Trofimov%20and%20Daria%20Voronkova%20and%20Eduard%20Tulchinskii%20and%20Evgeny%20Burnaev%20and%20Serguei%20Barannikov&entry.1292438233=%20%20We%20propose%20a%20new%20topological%20tool%20for%20computer%20vision%20-%20Scalar%20Function%0ATopology%20Divergence%20%28SFTD%29%2C%20which%20measures%20the%20dissimilarity%20of%20multi-scale%0Atopology%20between%20sublevel%20sets%20of%20two%20functions%20having%20a%20common%20domain.%0AFunctions%20can%20be%20defined%20on%20an%20undirected%20graph%20or%20Euclidean%20space%20of%20any%0Adimensionality.%20Most%20of%20the%20existing%20methods%20for%20comparing%20topology%20are%20based%0Aon%20Wasserstein%20distance%20between%20persistence%20barcodes%20and%20they%20don%27t%20take%20into%0Aaccount%20the%20localization%20of%20topological%20features.%20The%20minimization%20of%20SFTD%0Aensures%20that%20the%20corresponding%20topological%20features%20of%20scalar%20functions%20are%0Alocated%20in%20the%20same%20places.%20The%20proposed%20tool%20provides%20useful%20visualizations%0Adepicting%20areas%20where%20functions%20have%20topological%20dissimilarities.%20We%20provide%0Aapplications%20of%20the%20proposed%20method%20to%203D%20computer%20vision.%20In%20particular%2C%0Aexperiments%20demonstrate%20that%20SFTD%20as%20an%20additional%20loss%20improves%20the%0Areconstruction%20of%20cellular%203D%20shapes%20from%202D%20fluorescence%20microscopy%20images%2C%0Aand%20helps%20to%20identify%20topological%20errors%20in%203D%20segmentation.%20Additionally%2C%20we%0Ashow%20that%20SFTD%20outperforms%20Betti%20matching%20loss%20in%202D%20segmentation%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08364v3&entry.124074799=Read"},
{"title": "Exploring Advanced Large Language Models with LLMsuite", "author": "Giorgio Roffo", "abstract": "  This tutorial explores the advancements and challenges in the development of\nLarge Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent\nlimitations like temporal knowledge cutoffs, mathematical inaccuracies, and the\ngeneration of incorrect information, proposing solutions like Retrieval\nAugmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks\nsuch as ReAct and LangChain. The integration of these techniques enhances LLM\nperformance and reliability, especially in multi-step reasoning and complex\ntask execution. The paper also covers fine-tuning strategies, including\ninstruction fine-tuning, parameter-efficient methods like LoRA, and\nReinforcement Learning from Human Feedback (RLHF) as well as Reinforced\nSelf-Training (ReST). Additionally, it provides a comprehensive survey of\ntransformer architectures and training techniques for LLMs. The source code can\nbe accessed by contacting the author via email for a request.\n", "link": "http://arxiv.org/abs/2407.12036v2", "date": "2024-11-12", "relevancy": 2.4619, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Advanced%20Large%20Language%20Models%20with%20LLMsuite&body=Title%3A%20Exploring%20Advanced%20Large%20Language%20Models%20with%20LLMsuite%0AAuthor%3A%20Giorgio%20Roffo%0AAbstract%3A%20%20%20This%20tutorial%20explores%20the%20advancements%20and%20challenges%20in%20the%20development%20of%0ALarge%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Gemini.%20It%20addresses%20inherent%0Alimitations%20like%20temporal%20knowledge%20cutoffs%2C%20mathematical%20inaccuracies%2C%20and%20the%0Ageneration%20of%20incorrect%20information%2C%20proposing%20solutions%20like%20Retrieval%0AAugmented%20Generation%20%28RAG%29%2C%20Program-Aided%20Language%20Models%20%28PAL%29%2C%20and%20frameworks%0Asuch%20as%20ReAct%20and%20LangChain.%20The%20integration%20of%20these%20techniques%20enhances%20LLM%0Aperformance%20and%20reliability%2C%20especially%20in%20multi-step%20reasoning%20and%20complex%0Atask%20execution.%20The%20paper%20also%20covers%20fine-tuning%20strategies%2C%20including%0Ainstruction%20fine-tuning%2C%20parameter-efficient%20methods%20like%20LoRA%2C%20and%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20as%20well%20as%20Reinforced%0ASelf-Training%20%28ReST%29.%20Additionally%2C%20it%20provides%20a%20comprehensive%20survey%20of%0Atransformer%20architectures%20and%20training%20techniques%20for%20LLMs.%20The%20source%20code%20can%0Abe%20accessed%20by%20contacting%20the%20author%20via%20email%20for%20a%20request.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Advanced%2520Large%2520Language%2520Models%2520with%2520LLMsuite%26entry.906535625%3DGiorgio%2520Roffo%26entry.1292438233%3D%2520%2520This%2520tutorial%2520explores%2520the%2520advancements%2520and%2520challenges%2520in%2520the%2520development%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520ChatGPT%2520and%2520Gemini.%2520It%2520addresses%2520inherent%250Alimitations%2520like%2520temporal%2520knowledge%2520cutoffs%252C%2520mathematical%2520inaccuracies%252C%2520and%2520the%250Ageneration%2520of%2520incorrect%2520information%252C%2520proposing%2520solutions%2520like%2520Retrieval%250AAugmented%2520Generation%2520%2528RAG%2529%252C%2520Program-Aided%2520Language%2520Models%2520%2528PAL%2529%252C%2520and%2520frameworks%250Asuch%2520as%2520ReAct%2520and%2520LangChain.%2520The%2520integration%2520of%2520these%2520techniques%2520enhances%2520LLM%250Aperformance%2520and%2520reliability%252C%2520especially%2520in%2520multi-step%2520reasoning%2520and%2520complex%250Atask%2520execution.%2520The%2520paper%2520also%2520covers%2520fine-tuning%2520strategies%252C%2520including%250Ainstruction%2520fine-tuning%252C%2520parameter-efficient%2520methods%2520like%2520LoRA%252C%2520and%250AReinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520as%2520well%2520as%2520Reinforced%250ASelf-Training%2520%2528ReST%2529.%2520Additionally%252C%2520it%2520provides%2520a%2520comprehensive%2520survey%2520of%250Atransformer%2520architectures%2520and%2520training%2520techniques%2520for%2520LLMs.%2520The%2520source%2520code%2520can%250Abe%2520accessed%2520by%2520contacting%2520the%2520author%2520via%2520email%2520for%2520a%2520request.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Advanced%20Large%20Language%20Models%20with%20LLMsuite&entry.906535625=Giorgio%20Roffo&entry.1292438233=%20%20This%20tutorial%20explores%20the%20advancements%20and%20challenges%20in%20the%20development%20of%0ALarge%20Language%20Models%20%28LLMs%29%20such%20as%20ChatGPT%20and%20Gemini.%20It%20addresses%20inherent%0Alimitations%20like%20temporal%20knowledge%20cutoffs%2C%20mathematical%20inaccuracies%2C%20and%20the%0Ageneration%20of%20incorrect%20information%2C%20proposing%20solutions%20like%20Retrieval%0AAugmented%20Generation%20%28RAG%29%2C%20Program-Aided%20Language%20Models%20%28PAL%29%2C%20and%20frameworks%0Asuch%20as%20ReAct%20and%20LangChain.%20The%20integration%20of%20these%20techniques%20enhances%20LLM%0Aperformance%20and%20reliability%2C%20especially%20in%20multi-step%20reasoning%20and%20complex%0Atask%20execution.%20The%20paper%20also%20covers%20fine-tuning%20strategies%2C%20including%0Ainstruction%20fine-tuning%2C%20parameter-efficient%20methods%20like%20LoRA%2C%20and%0AReinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20as%20well%20as%20Reinforced%0ASelf-Training%20%28ReST%29.%20Additionally%2C%20it%20provides%20a%20comprehensive%20survey%20of%0Atransformer%20architectures%20and%20training%20techniques%20for%20LLMs.%20The%20source%20code%20can%0Abe%20accessed%20by%20contacting%20the%20author%20via%20email%20for%20a%20request.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12036v2&entry.124074799=Read"},
{"title": "Derivational Morphology Reveals Analogical Generalization in Large\n  Language Models", "author": "Valentin Hofmann and Leonie Weissweiler and David Mortensen and Hinrich Sch\u00fctze and Janet Pierrehumbert", "abstract": "  What mechanisms underlie linguistic generalization in large language models\n(LLMs)? This question has attracted considerable attention, with most studies\nanalyzing the extent to which the language skills of LLMs resemble rules. As of\nyet, it is not known whether linguistic generalization in LLMs could equally\nwell be explained as the result of analogical processes, which can be\nformalized as similarity operations on stored exemplars. A key shortcoming of\nprior research is its focus on linguistic phenomena with a high degree of\nregularity, for which rule-based and analogical approaches make the same\npredictions. Here, we instead examine derivational morphology, specifically\nEnglish adjective nominalization, which displays notable variability. We\nintroduce a new method for investigating linguistic generalization in LLMs:\nfocusing on GPT-J, we fit cognitive models that instantiate rule-based and\nanalogical learning to the LLM training data and compare their predictions on a\nset of nonce adjectives with those of the LLM, allowing us to draw direct\nconclusions regarding underlying mechanisms. As expected, rule-based and\nanalogical models explain the predictions of GPT-J equally well for adjectives\nwith regular nominalization patterns. However, for adjectives with variable\nnominalization patterns, the analogical model provides a much better match.\nFurthermore, GPT-J's behavior is sensitive to the individual word frequencies,\neven for regular forms, a behavior that is consistent with an analogical\naccount of regular forms but not a rule-based one. These findings refute the\nhypothesis that GPT-J's linguistic generalization on adjective nominalization\ninvolves rules, suggesting similarity operations on stored exemplars as the\nunderlying mechanism. Overall, our study suggests that analogical processes\nplay a bigger role in the linguistic generalization of LLMs than previously\nthought.\n", "link": "http://arxiv.org/abs/2411.07990v1", "date": "2024-11-12", "relevancy": 2.4548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Derivational%20Morphology%20Reveals%20Analogical%20Generalization%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Derivational%20Morphology%20Reveals%20Analogical%20Generalization%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Valentin%20Hofmann%20and%20Leonie%20Weissweiler%20and%20David%20Mortensen%20and%20Hinrich%20Sch%C3%BCtze%20and%20Janet%20Pierrehumbert%0AAbstract%3A%20%20%20What%20mechanisms%20underlie%20linguistic%20generalization%20in%20large%20language%20models%0A%28LLMs%29%3F%20This%20question%20has%20attracted%20considerable%20attention%2C%20with%20most%20studies%0Aanalyzing%20the%20extent%20to%20which%20the%20language%20skills%20of%20LLMs%20resemble%20rules.%20As%20of%0Ayet%2C%20it%20is%20not%20known%20whether%20linguistic%20generalization%20in%20LLMs%20could%20equally%0Awell%20be%20explained%20as%20the%20result%20of%20analogical%20processes%2C%20which%20can%20be%0Aformalized%20as%20similarity%20operations%20on%20stored%20exemplars.%20A%20key%20shortcoming%20of%0Aprior%20research%20is%20its%20focus%20on%20linguistic%20phenomena%20with%20a%20high%20degree%20of%0Aregularity%2C%20for%20which%20rule-based%20and%20analogical%20approaches%20make%20the%20same%0Apredictions.%20Here%2C%20we%20instead%20examine%20derivational%20morphology%2C%20specifically%0AEnglish%20adjective%20nominalization%2C%20which%20displays%20notable%20variability.%20We%0Aintroduce%20a%20new%20method%20for%20investigating%20linguistic%20generalization%20in%20LLMs%3A%0Afocusing%20on%20GPT-J%2C%20we%20fit%20cognitive%20models%20that%20instantiate%20rule-based%20and%0Aanalogical%20learning%20to%20the%20LLM%20training%20data%20and%20compare%20their%20predictions%20on%20a%0Aset%20of%20nonce%20adjectives%20with%20those%20of%20the%20LLM%2C%20allowing%20us%20to%20draw%20direct%0Aconclusions%20regarding%20underlying%20mechanisms.%20As%20expected%2C%20rule-based%20and%0Aanalogical%20models%20explain%20the%20predictions%20of%20GPT-J%20equally%20well%20for%20adjectives%0Awith%20regular%20nominalization%20patterns.%20However%2C%20for%20adjectives%20with%20variable%0Anominalization%20patterns%2C%20the%20analogical%20model%20provides%20a%20much%20better%20match.%0AFurthermore%2C%20GPT-J%27s%20behavior%20is%20sensitive%20to%20the%20individual%20word%20frequencies%2C%0Aeven%20for%20regular%20forms%2C%20a%20behavior%20that%20is%20consistent%20with%20an%20analogical%0Aaccount%20of%20regular%20forms%20but%20not%20a%20rule-based%20one.%20These%20findings%20refute%20the%0Ahypothesis%20that%20GPT-J%27s%20linguistic%20generalization%20on%20adjective%20nominalization%0Ainvolves%20rules%2C%20suggesting%20similarity%20operations%20on%20stored%20exemplars%20as%20the%0Aunderlying%20mechanism.%20Overall%2C%20our%20study%20suggests%20that%20analogical%20processes%0Aplay%20a%20bigger%20role%20in%20the%20linguistic%20generalization%20of%20LLMs%20than%20previously%0Athought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDerivational%2520Morphology%2520Reveals%2520Analogical%2520Generalization%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DValentin%2520Hofmann%2520and%2520Leonie%2520Weissweiler%2520and%2520David%2520Mortensen%2520and%2520Hinrich%2520Sch%25C3%25BCtze%2520and%2520Janet%2520Pierrehumbert%26entry.1292438233%3D%2520%2520What%2520mechanisms%2520underlie%2520linguistic%2520generalization%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%253F%2520This%2520question%2520has%2520attracted%2520considerable%2520attention%252C%2520with%2520most%2520studies%250Aanalyzing%2520the%2520extent%2520to%2520which%2520the%2520language%2520skills%2520of%2520LLMs%2520resemble%2520rules.%2520As%2520of%250Ayet%252C%2520it%2520is%2520not%2520known%2520whether%2520linguistic%2520generalization%2520in%2520LLMs%2520could%2520equally%250Awell%2520be%2520explained%2520as%2520the%2520result%2520of%2520analogical%2520processes%252C%2520which%2520can%2520be%250Aformalized%2520as%2520similarity%2520operations%2520on%2520stored%2520exemplars.%2520A%2520key%2520shortcoming%2520of%250Aprior%2520research%2520is%2520its%2520focus%2520on%2520linguistic%2520phenomena%2520with%2520a%2520high%2520degree%2520of%250Aregularity%252C%2520for%2520which%2520rule-based%2520and%2520analogical%2520approaches%2520make%2520the%2520same%250Apredictions.%2520Here%252C%2520we%2520instead%2520examine%2520derivational%2520morphology%252C%2520specifically%250AEnglish%2520adjective%2520nominalization%252C%2520which%2520displays%2520notable%2520variability.%2520We%250Aintroduce%2520a%2520new%2520method%2520for%2520investigating%2520linguistic%2520generalization%2520in%2520LLMs%253A%250Afocusing%2520on%2520GPT-J%252C%2520we%2520fit%2520cognitive%2520models%2520that%2520instantiate%2520rule-based%2520and%250Aanalogical%2520learning%2520to%2520the%2520LLM%2520training%2520data%2520and%2520compare%2520their%2520predictions%2520on%2520a%250Aset%2520of%2520nonce%2520adjectives%2520with%2520those%2520of%2520the%2520LLM%252C%2520allowing%2520us%2520to%2520draw%2520direct%250Aconclusions%2520regarding%2520underlying%2520mechanisms.%2520As%2520expected%252C%2520rule-based%2520and%250Aanalogical%2520models%2520explain%2520the%2520predictions%2520of%2520GPT-J%2520equally%2520well%2520for%2520adjectives%250Awith%2520regular%2520nominalization%2520patterns.%2520However%252C%2520for%2520adjectives%2520with%2520variable%250Anominalization%2520patterns%252C%2520the%2520analogical%2520model%2520provides%2520a%2520much%2520better%2520match.%250AFurthermore%252C%2520GPT-J%2527s%2520behavior%2520is%2520sensitive%2520to%2520the%2520individual%2520word%2520frequencies%252C%250Aeven%2520for%2520regular%2520forms%252C%2520a%2520behavior%2520that%2520is%2520consistent%2520with%2520an%2520analogical%250Aaccount%2520of%2520regular%2520forms%2520but%2520not%2520a%2520rule-based%2520one.%2520These%2520findings%2520refute%2520the%250Ahypothesis%2520that%2520GPT-J%2527s%2520linguistic%2520generalization%2520on%2520adjective%2520nominalization%250Ainvolves%2520rules%252C%2520suggesting%2520similarity%2520operations%2520on%2520stored%2520exemplars%2520as%2520the%250Aunderlying%2520mechanism.%2520Overall%252C%2520our%2520study%2520suggests%2520that%2520analogical%2520processes%250Aplay%2520a%2520bigger%2520role%2520in%2520the%2520linguistic%2520generalization%2520of%2520LLMs%2520than%2520previously%250Athought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Derivational%20Morphology%20Reveals%20Analogical%20Generalization%20in%20Large%0A%20%20Language%20Models&entry.906535625=Valentin%20Hofmann%20and%20Leonie%20Weissweiler%20and%20David%20Mortensen%20and%20Hinrich%20Sch%C3%BCtze%20and%20Janet%20Pierrehumbert&entry.1292438233=%20%20What%20mechanisms%20underlie%20linguistic%20generalization%20in%20large%20language%20models%0A%28LLMs%29%3F%20This%20question%20has%20attracted%20considerable%20attention%2C%20with%20most%20studies%0Aanalyzing%20the%20extent%20to%20which%20the%20language%20skills%20of%20LLMs%20resemble%20rules.%20As%20of%0Ayet%2C%20it%20is%20not%20known%20whether%20linguistic%20generalization%20in%20LLMs%20could%20equally%0Awell%20be%20explained%20as%20the%20result%20of%20analogical%20processes%2C%20which%20can%20be%0Aformalized%20as%20similarity%20operations%20on%20stored%20exemplars.%20A%20key%20shortcoming%20of%0Aprior%20research%20is%20its%20focus%20on%20linguistic%20phenomena%20with%20a%20high%20degree%20of%0Aregularity%2C%20for%20which%20rule-based%20and%20analogical%20approaches%20make%20the%20same%0Apredictions.%20Here%2C%20we%20instead%20examine%20derivational%20morphology%2C%20specifically%0AEnglish%20adjective%20nominalization%2C%20which%20displays%20notable%20variability.%20We%0Aintroduce%20a%20new%20method%20for%20investigating%20linguistic%20generalization%20in%20LLMs%3A%0Afocusing%20on%20GPT-J%2C%20we%20fit%20cognitive%20models%20that%20instantiate%20rule-based%20and%0Aanalogical%20learning%20to%20the%20LLM%20training%20data%20and%20compare%20their%20predictions%20on%20a%0Aset%20of%20nonce%20adjectives%20with%20those%20of%20the%20LLM%2C%20allowing%20us%20to%20draw%20direct%0Aconclusions%20regarding%20underlying%20mechanisms.%20As%20expected%2C%20rule-based%20and%0Aanalogical%20models%20explain%20the%20predictions%20of%20GPT-J%20equally%20well%20for%20adjectives%0Awith%20regular%20nominalization%20patterns.%20However%2C%20for%20adjectives%20with%20variable%0Anominalization%20patterns%2C%20the%20analogical%20model%20provides%20a%20much%20better%20match.%0AFurthermore%2C%20GPT-J%27s%20behavior%20is%20sensitive%20to%20the%20individual%20word%20frequencies%2C%0Aeven%20for%20regular%20forms%2C%20a%20behavior%20that%20is%20consistent%20with%20an%20analogical%0Aaccount%20of%20regular%20forms%20but%20not%20a%20rule-based%20one.%20These%20findings%20refute%20the%0Ahypothesis%20that%20GPT-J%27s%20linguistic%20generalization%20on%20adjective%20nominalization%0Ainvolves%20rules%2C%20suggesting%20similarity%20operations%20on%20stored%20exemplars%20as%20the%0Aunderlying%20mechanism.%20Overall%2C%20our%20study%20suggests%20that%20analogical%20processes%0Aplay%20a%20bigger%20role%20in%20the%20linguistic%20generalization%20of%20LLMs%20than%20previously%0Athought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07990v1&entry.124074799=Read"},
{"title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on\n  Supervised Regression (Preprint)", "author": "Yansel Gonzalez Tejeda and Helmut A. Mayer", "abstract": "  In this tutorial, we present a compact and holistic discussion of Deep\nLearning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression. While there are numerous books and articles on the individual\ntopics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts but\nalso provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an optimal\nresource for students, professors, and anyone interested in understanding the\nfoundations of Deep Learning. Upon acceptance we will provide an accompanying\nrepository under\n\\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}\n  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine\nLearning.\n", "link": "http://arxiv.org/abs/2408.12308v3", "date": "2024-11-12", "relevancy": 2.4431, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&body=Title%3A%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29%0AAuthor%3A%20Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer%0AAbstract%3A%20%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12308v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520with%2520CNNs%253A%2520A%2520Compact%2520Holistic%2520Tutorial%2520with%2520Focus%2520on%250A%2520%2520Supervised%2520Regression%2520%2528Preprint%2529%26entry.906535625%3DYansel%2520Gonzalez%2520Tejeda%2520and%2520Helmut%2520A.%2520Mayer%26entry.1292438233%3D%2520%2520In%2520this%2520tutorial%252C%2520we%2520present%2520a%2520compact%2520and%2520holistic%2520discussion%2520of%2520Deep%250ALearning%2520with%2520a%2520focus%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520supervised%250Aregression.%2520While%2520there%2520are%2520numerous%2520books%2520and%2520articles%2520on%2520the%2520individual%250Atopics%2520we%2520cover%252C%2520comprehensive%2520and%2520detailed%2520tutorials%2520that%2520address%2520Deep%250ALearning%2520from%2520a%2520foundational%2520yet%2520rigorous%2520and%2520accessible%2520perspective%2520are%2520rare.%250AMost%2520resources%2520on%2520CNNs%2520are%2520either%2520too%2520advanced%252C%2520focusing%2520on%2520cutting-edge%250Aarchitectures%252C%2520or%2520too%2520narrow%252C%2520addressing%2520only%2520specific%2520applications%2520like%2520image%250Aclassification.This%2520tutorial%2520not%2520only%2520summarizes%2520the%2520most%2520relevant%2520concepts%2520but%250Aalso%2520provides%2520an%2520in-depth%2520exploration%2520of%2520each%252C%2520offering%2520a%2520complete%2520yet%2520agile%250Aset%2520of%2520ideas.%2520Moreover%252C%2520we%2520highlight%2520the%2520powerful%2520synergy%2520between%2520learning%250Atheory%252C%2520statistic%252C%2520and%2520machine%2520learning%252C%2520which%2520together%2520underpin%2520the%2520Deep%250ALearning%2520and%2520CNN%2520frameworks.%2520We%2520aim%2520for%2520this%2520tutorial%2520to%2520serve%2520as%2520an%2520optimal%250Aresource%2520for%2520students%252C%2520professors%252C%2520and%2520anyone%2520interested%2520in%2520understanding%2520the%250Afoundations%2520of%2520Deep%2520Learning.%2520Upon%2520acceptance%2520we%2520will%2520provide%2520an%2520accompanying%250Arepository%2520under%250A%255Chref%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%250A%2520%2520Keywords%253A%2520Tutorial%252C%2520Deep%2520Learning%252C%2520Convolutional%2520Neural%2520Networks%252C%2520Machine%250ALearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12308v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&entry.906535625=Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer&entry.1292438233=%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12308v3&entry.124074799=Read"},
{"title": "Automatic Album Sequencing", "author": "Vincent Herrmann and Dylan R. Ashley and J\u00fcrgen Schmidhuber", "abstract": "  Album sequencing is a critical part of the album production process.\nRecently, a data-driven approach was proposed that sequences general\ncollections of independent media by extracting the narrative essence of the\nitems in the collections. While this approach implies an album sequencing\ntechnique, it is not widely accessible to a less technical audience, requiring\nadvanced knowledge of machine learning techniques to use. To address this, we\nintroduce a new user-friendly web-based tool that allows a less technical\naudience to upload music tracks, execute this technique in one click, and\nsubsequently presents the result in a clean visualization to the user. To both\nincrease the number of templates available to the user and address shortcomings\nof previous work, we also introduce a new direct transformer-based album\nsequencing method. We find that our more direct method outperforms a random\nbaseline but does not reach the same performance as the narrative essence\napproach. Both methods are included in our web-based user interface, and this\n-- alongside a full copy of our implementation -- is publicly available at\nhttps://github.com/dylanashley/automatic-album-sequencing\n", "link": "http://arxiv.org/abs/2411.07772v1", "date": "2024-11-12", "relevancy": 2.4367, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Album%20Sequencing&body=Title%3A%20Automatic%20Album%20Sequencing%0AAuthor%3A%20Vincent%20Herrmann%20and%20Dylan%20R.%20Ashley%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Album%20sequencing%20is%20a%20critical%20part%20of%20the%20album%20production%20process.%0ARecently%2C%20a%20data-driven%20approach%20was%20proposed%20that%20sequences%20general%0Acollections%20of%20independent%20media%20by%20extracting%20the%20narrative%20essence%20of%20the%0Aitems%20in%20the%20collections.%20While%20this%20approach%20implies%20an%20album%20sequencing%0Atechnique%2C%20it%20is%20not%20widely%20accessible%20to%20a%20less%20technical%20audience%2C%20requiring%0Aadvanced%20knowledge%20of%20machine%20learning%20techniques%20to%20use.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20user-friendly%20web-based%20tool%20that%20allows%20a%20less%20technical%0Aaudience%20to%20upload%20music%20tracks%2C%20execute%20this%20technique%20in%20one%20click%2C%20and%0Asubsequently%20presents%20the%20result%20in%20a%20clean%20visualization%20to%20the%20user.%20To%20both%0Aincrease%20the%20number%20of%20templates%20available%20to%20the%20user%20and%20address%20shortcomings%0Aof%20previous%20work%2C%20we%20also%20introduce%20a%20new%20direct%20transformer-based%20album%0Asequencing%20method.%20We%20find%20that%20our%20more%20direct%20method%20outperforms%20a%20random%0Abaseline%20but%20does%20not%20reach%20the%20same%20performance%20as%20the%20narrative%20essence%0Aapproach.%20Both%20methods%20are%20included%20in%20our%20web-based%20user%20interface%2C%20and%20this%0A--%20alongside%20a%20full%20copy%20of%20our%20implementation%20--%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dylanashley/automatic-album-sequencing%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Album%2520Sequencing%26entry.906535625%3DVincent%2520Herrmann%2520and%2520Dylan%2520R.%2520Ashley%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Album%2520sequencing%2520is%2520a%2520critical%2520part%2520of%2520the%2520album%2520production%2520process.%250ARecently%252C%2520a%2520data-driven%2520approach%2520was%2520proposed%2520that%2520sequences%2520general%250Acollections%2520of%2520independent%2520media%2520by%2520extracting%2520the%2520narrative%2520essence%2520of%2520the%250Aitems%2520in%2520the%2520collections.%2520While%2520this%2520approach%2520implies%2520an%2520album%2520sequencing%250Atechnique%252C%2520it%2520is%2520not%2520widely%2520accessible%2520to%2520a%2520less%2520technical%2520audience%252C%2520requiring%250Aadvanced%2520knowledge%2520of%2520machine%2520learning%2520techniques%2520to%2520use.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520new%2520user-friendly%2520web-based%2520tool%2520that%2520allows%2520a%2520less%2520technical%250Aaudience%2520to%2520upload%2520music%2520tracks%252C%2520execute%2520this%2520technique%2520in%2520one%2520click%252C%2520and%250Asubsequently%2520presents%2520the%2520result%2520in%2520a%2520clean%2520visualization%2520to%2520the%2520user.%2520To%2520both%250Aincrease%2520the%2520number%2520of%2520templates%2520available%2520to%2520the%2520user%2520and%2520address%2520shortcomings%250Aof%2520previous%2520work%252C%2520we%2520also%2520introduce%2520a%2520new%2520direct%2520transformer-based%2520album%250Asequencing%2520method.%2520We%2520find%2520that%2520our%2520more%2520direct%2520method%2520outperforms%2520a%2520random%250Abaseline%2520but%2520does%2520not%2520reach%2520the%2520same%2520performance%2520as%2520the%2520narrative%2520essence%250Aapproach.%2520Both%2520methods%2520are%2520included%2520in%2520our%2520web-based%2520user%2520interface%252C%2520and%2520this%250A--%2520alongside%2520a%2520full%2520copy%2520of%2520our%2520implementation%2520--%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/dylanashley/automatic-album-sequencing%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Album%20Sequencing&entry.906535625=Vincent%20Herrmann%20and%20Dylan%20R.%20Ashley%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Album%20sequencing%20is%20a%20critical%20part%20of%20the%20album%20production%20process.%0ARecently%2C%20a%20data-driven%20approach%20was%20proposed%20that%20sequences%20general%0Acollections%20of%20independent%20media%20by%20extracting%20the%20narrative%20essence%20of%20the%0Aitems%20in%20the%20collections.%20While%20this%20approach%20implies%20an%20album%20sequencing%0Atechnique%2C%20it%20is%20not%20widely%20accessible%20to%20a%20less%20technical%20audience%2C%20requiring%0Aadvanced%20knowledge%20of%20machine%20learning%20techniques%20to%20use.%20To%20address%20this%2C%20we%0Aintroduce%20a%20new%20user-friendly%20web-based%20tool%20that%20allows%20a%20less%20technical%0Aaudience%20to%20upload%20music%20tracks%2C%20execute%20this%20technique%20in%20one%20click%2C%20and%0Asubsequently%20presents%20the%20result%20in%20a%20clean%20visualization%20to%20the%20user.%20To%20both%0Aincrease%20the%20number%20of%20templates%20available%20to%20the%20user%20and%20address%20shortcomings%0Aof%20previous%20work%2C%20we%20also%20introduce%20a%20new%20direct%20transformer-based%20album%0Asequencing%20method.%20We%20find%20that%20our%20more%20direct%20method%20outperforms%20a%20random%0Abaseline%20but%20does%20not%20reach%20the%20same%20performance%20as%20the%20narrative%20essence%0Aapproach.%20Both%20methods%20are%20included%20in%20our%20web-based%20user%20interface%2C%20and%20this%0A--%20alongside%20a%20full%20copy%20of%20our%20implementation%20--%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dylanashley/automatic-album-sequencing%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07772v1&entry.124074799=Read"},
{"title": "Exploring the loss landscape of regularized neural networks via convex\n  duality", "author": "Sungyoon Kim and Aaron Mishkin and Mert Pilanci", "abstract": "  We discuss several aspects of the loss landscape of regularized neural\nnetworks: the structure of stationary points, connectivity of optimal\nsolutions, path with nonincreasing loss to arbitrary global optimum, and the\nnonuniqueness of optimal solutions, by casting the problem into an equivalent\nconvex problem and considering its dual. Starting from two-layer neural\nnetworks with scalar output, we first characterize the solution set of the\nconvex problem using its dual and further characterize all stationary points.\nWith the characterization, we show that the topology of the global optima goes\nthrough a phase transition as the width of the network changes, and construct\ncounterexamples where the problem may have a continuum of optimal solutions.\nFinally, we show that the solution set characterization and connectivity\nresults can be extended to different architectures, including two-layer\nvector-valued neural networks and parallel three-layer neural networks.\n", "link": "http://arxiv.org/abs/2411.07729v1", "date": "2024-11-12", "relevancy": 2.3678, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20loss%20landscape%20of%20regularized%20neural%20networks%20via%20convex%0A%20%20duality&body=Title%3A%20Exploring%20the%20loss%20landscape%20of%20regularized%20neural%20networks%20via%20convex%0A%20%20duality%0AAuthor%3A%20Sungyoon%20Kim%20and%20Aaron%20Mishkin%20and%20Mert%20Pilanci%0AAbstract%3A%20%20%20We%20discuss%20several%20aspects%20of%20the%20loss%20landscape%20of%20regularized%20neural%0Anetworks%3A%20the%20structure%20of%20stationary%20points%2C%20connectivity%20of%20optimal%0Asolutions%2C%20path%20with%20nonincreasing%20loss%20to%20arbitrary%20global%20optimum%2C%20and%20the%0Anonuniqueness%20of%20optimal%20solutions%2C%20by%20casting%20the%20problem%20into%20an%20equivalent%0Aconvex%20problem%20and%20considering%20its%20dual.%20Starting%20from%20two-layer%20neural%0Anetworks%20with%20scalar%20output%2C%20we%20first%20characterize%20the%20solution%20set%20of%20the%0Aconvex%20problem%20using%20its%20dual%20and%20further%20characterize%20all%20stationary%20points.%0AWith%20the%20characterization%2C%20we%20show%20that%20the%20topology%20of%20the%20global%20optima%20goes%0Athrough%20a%20phase%20transition%20as%20the%20width%20of%20the%20network%20changes%2C%20and%20construct%0Acounterexamples%20where%20the%20problem%20may%20have%20a%20continuum%20of%20optimal%20solutions.%0AFinally%2C%20we%20show%20that%20the%20solution%20set%20characterization%20and%20connectivity%0Aresults%20can%20be%20extended%20to%20different%20architectures%2C%20including%20two-layer%0Avector-valued%20neural%20networks%20and%20parallel%20three-layer%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520loss%2520landscape%2520of%2520regularized%2520neural%2520networks%2520via%2520convex%250A%2520%2520duality%26entry.906535625%3DSungyoon%2520Kim%2520and%2520Aaron%2520Mishkin%2520and%2520Mert%2520Pilanci%26entry.1292438233%3D%2520%2520We%2520discuss%2520several%2520aspects%2520of%2520the%2520loss%2520landscape%2520of%2520regularized%2520neural%250Anetworks%253A%2520the%2520structure%2520of%2520stationary%2520points%252C%2520connectivity%2520of%2520optimal%250Asolutions%252C%2520path%2520with%2520nonincreasing%2520loss%2520to%2520arbitrary%2520global%2520optimum%252C%2520and%2520the%250Anonuniqueness%2520of%2520optimal%2520solutions%252C%2520by%2520casting%2520the%2520problem%2520into%2520an%2520equivalent%250Aconvex%2520problem%2520and%2520considering%2520its%2520dual.%2520Starting%2520from%2520two-layer%2520neural%250Anetworks%2520with%2520scalar%2520output%252C%2520we%2520first%2520characterize%2520the%2520solution%2520set%2520of%2520the%250Aconvex%2520problem%2520using%2520its%2520dual%2520and%2520further%2520characterize%2520all%2520stationary%2520points.%250AWith%2520the%2520characterization%252C%2520we%2520show%2520that%2520the%2520topology%2520of%2520the%2520global%2520optima%2520goes%250Athrough%2520a%2520phase%2520transition%2520as%2520the%2520width%2520of%2520the%2520network%2520changes%252C%2520and%2520construct%250Acounterexamples%2520where%2520the%2520problem%2520may%2520have%2520a%2520continuum%2520of%2520optimal%2520solutions.%250AFinally%252C%2520we%2520show%2520that%2520the%2520solution%2520set%2520characterization%2520and%2520connectivity%250Aresults%2520can%2520be%2520extended%2520to%2520different%2520architectures%252C%2520including%2520two-layer%250Avector-valued%2520neural%2520networks%2520and%2520parallel%2520three-layer%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20loss%20landscape%20of%20regularized%20neural%20networks%20via%20convex%0A%20%20duality&entry.906535625=Sungyoon%20Kim%20and%20Aaron%20Mishkin%20and%20Mert%20Pilanci&entry.1292438233=%20%20We%20discuss%20several%20aspects%20of%20the%20loss%20landscape%20of%20regularized%20neural%0Anetworks%3A%20the%20structure%20of%20stationary%20points%2C%20connectivity%20of%20optimal%0Asolutions%2C%20path%20with%20nonincreasing%20loss%20to%20arbitrary%20global%20optimum%2C%20and%20the%0Anonuniqueness%20of%20optimal%20solutions%2C%20by%20casting%20the%20problem%20into%20an%20equivalent%0Aconvex%20problem%20and%20considering%20its%20dual.%20Starting%20from%20two-layer%20neural%0Anetworks%20with%20scalar%20output%2C%20we%20first%20characterize%20the%20solution%20set%20of%20the%0Aconvex%20problem%20using%20its%20dual%20and%20further%20characterize%20all%20stationary%20points.%0AWith%20the%20characterization%2C%20we%20show%20that%20the%20topology%20of%20the%20global%20optima%20goes%0Athrough%20a%20phase%20transition%20as%20the%20width%20of%20the%20network%20changes%2C%20and%20construct%0Acounterexamples%20where%20the%20problem%20may%20have%20a%20continuum%20of%20optimal%20solutions.%0AFinally%2C%20we%20show%20that%20the%20solution%20set%20characterization%20and%20connectivity%0Aresults%20can%20be%20extended%20to%20different%20architectures%2C%20including%20two-layer%0Avector-valued%20neural%20networks%20and%20parallel%20three-layer%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07729v1&entry.124074799=Read"},
{"title": "LEO: Generative Latent Image Animator for Human Video Synthesis", "author": "Yaohui Wang and Xin Ma and Xinyuan Chen and Cunjian Chen and Antitza Dantcheva and Bo Dai and Yu Qiao", "abstract": "  Spatio-temporal coherency is a major challenge in synthesizing high quality\nvideos, particularly in synthesizing human videos that contain rich global and\nlocal deformations. To resolve this challenge, previous approaches have\nresorted to different features in the generation process aimed at representing\nappearance and motion. However, in the absence of strict mechanisms to\nguarantee such disentanglement, a separation of motion from appearance has\nremained challenging, resulting in spatial distortions and temporal jittering\nthat break the spatio-temporal coherency. Motivated by this, we here propose\nLEO, a novel framework for human video synthesis, placing emphasis on\nspatio-temporal coherency. Our key idea is to represent motion as a sequence of\nflow maps in the generation process, which inherently isolate motion from\nappearance. We implement this idea via a flow-based image animator and a Latent\nMotion Diffusion Model (LMDM). The former bridges a space of motion codes with\nthe space of flow maps, and synthesizes video frames in a warp-and-inpaint\nmanner. LMDM learns to capture motion prior in the training data by\nsynthesizing sequences of motion codes. Extensive quantitative and qualitative\nanalysis suggests that LEO significantly improves coherent synthesis of human\nvideos over previous methods on the datasets TaichiHD, FaceForensics and\nCelebV-HQ. In addition, the effective disentanglement of appearance and motion\nin LEO allows for two additional tasks, namely infinite-length human video\nsynthesis, as well as content-preserving video editing.\n", "link": "http://arxiv.org/abs/2305.03989v3", "date": "2024-11-12", "relevancy": 2.3614, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6109}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5922}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEO%3A%20Generative%20Latent%20Image%20Animator%20for%20Human%20Video%20Synthesis&body=Title%3A%20LEO%3A%20Generative%20Latent%20Image%20Animator%20for%20Human%20Video%20Synthesis%0AAuthor%3A%20Yaohui%20Wang%20and%20Xin%20Ma%20and%20Xinyuan%20Chen%20and%20Cunjian%20Chen%20and%20Antitza%20Dantcheva%20and%20Bo%20Dai%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Spatio-temporal%20coherency%20is%20a%20major%20challenge%20in%20synthesizing%20high%20quality%0Avideos%2C%20particularly%20in%20synthesizing%20human%20videos%20that%20contain%20rich%20global%20and%0Alocal%20deformations.%20To%20resolve%20this%20challenge%2C%20previous%20approaches%20have%0Aresorted%20to%20different%20features%20in%20the%20generation%20process%20aimed%20at%20representing%0Aappearance%20and%20motion.%20However%2C%20in%20the%20absence%20of%20strict%20mechanisms%20to%0Aguarantee%20such%20disentanglement%2C%20a%20separation%20of%20motion%20from%20appearance%20has%0Aremained%20challenging%2C%20resulting%20in%20spatial%20distortions%20and%20temporal%20jittering%0Athat%20break%20the%20spatio-temporal%20coherency.%20Motivated%20by%20this%2C%20we%20here%20propose%0ALEO%2C%20a%20novel%20framework%20for%20human%20video%20synthesis%2C%20placing%20emphasis%20on%0Aspatio-temporal%20coherency.%20Our%20key%20idea%20is%20to%20represent%20motion%20as%20a%20sequence%20of%0Aflow%20maps%20in%20the%20generation%20process%2C%20which%20inherently%20isolate%20motion%20from%0Aappearance.%20We%20implement%20this%20idea%20via%20a%20flow-based%20image%20animator%20and%20a%20Latent%0AMotion%20Diffusion%20Model%20%28LMDM%29.%20The%20former%20bridges%20a%20space%20of%20motion%20codes%20with%0Athe%20space%20of%20flow%20maps%2C%20and%20synthesizes%20video%20frames%20in%20a%20warp-and-inpaint%0Amanner.%20LMDM%20learns%20to%20capture%20motion%20prior%20in%20the%20training%20data%20by%0Asynthesizing%20sequences%20of%20motion%20codes.%20Extensive%20quantitative%20and%20qualitative%0Aanalysis%20suggests%20that%20LEO%20significantly%20improves%20coherent%20synthesis%20of%20human%0Avideos%20over%20previous%20methods%20on%20the%20datasets%20TaichiHD%2C%20FaceForensics%20and%0ACelebV-HQ.%20In%20addition%2C%20the%20effective%20disentanglement%20of%20appearance%20and%20motion%0Ain%20LEO%20allows%20for%20two%20additional%20tasks%2C%20namely%20infinite-length%20human%20video%0Asynthesis%2C%20as%20well%20as%20content-preserving%20video%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.03989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEO%253A%2520Generative%2520Latent%2520Image%2520Animator%2520for%2520Human%2520Video%2520Synthesis%26entry.906535625%3DYaohui%2520Wang%2520and%2520Xin%2520Ma%2520and%2520Xinyuan%2520Chen%2520and%2520Cunjian%2520Chen%2520and%2520Antitza%2520Dantcheva%2520and%2520Bo%2520Dai%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Spatio-temporal%2520coherency%2520is%2520a%2520major%2520challenge%2520in%2520synthesizing%2520high%2520quality%250Avideos%252C%2520particularly%2520in%2520synthesizing%2520human%2520videos%2520that%2520contain%2520rich%2520global%2520and%250Alocal%2520deformations.%2520To%2520resolve%2520this%2520challenge%252C%2520previous%2520approaches%2520have%250Aresorted%2520to%2520different%2520features%2520in%2520the%2520generation%2520process%2520aimed%2520at%2520representing%250Aappearance%2520and%2520motion.%2520However%252C%2520in%2520the%2520absence%2520of%2520strict%2520mechanisms%2520to%250Aguarantee%2520such%2520disentanglement%252C%2520a%2520separation%2520of%2520motion%2520from%2520appearance%2520has%250Aremained%2520challenging%252C%2520resulting%2520in%2520spatial%2520distortions%2520and%2520temporal%2520jittering%250Athat%2520break%2520the%2520spatio-temporal%2520coherency.%2520Motivated%2520by%2520this%252C%2520we%2520here%2520propose%250ALEO%252C%2520a%2520novel%2520framework%2520for%2520human%2520video%2520synthesis%252C%2520placing%2520emphasis%2520on%250Aspatio-temporal%2520coherency.%2520Our%2520key%2520idea%2520is%2520to%2520represent%2520motion%2520as%2520a%2520sequence%2520of%250Aflow%2520maps%2520in%2520the%2520generation%2520process%252C%2520which%2520inherently%2520isolate%2520motion%2520from%250Aappearance.%2520We%2520implement%2520this%2520idea%2520via%2520a%2520flow-based%2520image%2520animator%2520and%2520a%2520Latent%250AMotion%2520Diffusion%2520Model%2520%2528LMDM%2529.%2520The%2520former%2520bridges%2520a%2520space%2520of%2520motion%2520codes%2520with%250Athe%2520space%2520of%2520flow%2520maps%252C%2520and%2520synthesizes%2520video%2520frames%2520in%2520a%2520warp-and-inpaint%250Amanner.%2520LMDM%2520learns%2520to%2520capture%2520motion%2520prior%2520in%2520the%2520training%2520data%2520by%250Asynthesizing%2520sequences%2520of%2520motion%2520codes.%2520Extensive%2520quantitative%2520and%2520qualitative%250Aanalysis%2520suggests%2520that%2520LEO%2520significantly%2520improves%2520coherent%2520synthesis%2520of%2520human%250Avideos%2520over%2520previous%2520methods%2520on%2520the%2520datasets%2520TaichiHD%252C%2520FaceForensics%2520and%250ACelebV-HQ.%2520In%2520addition%252C%2520the%2520effective%2520disentanglement%2520of%2520appearance%2520and%2520motion%250Ain%2520LEO%2520allows%2520for%2520two%2520additional%2520tasks%252C%2520namely%2520infinite-length%2520human%2520video%250Asynthesis%252C%2520as%2520well%2520as%2520content-preserving%2520video%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.03989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEO%3A%20Generative%20Latent%20Image%20Animator%20for%20Human%20Video%20Synthesis&entry.906535625=Yaohui%20Wang%20and%20Xin%20Ma%20and%20Xinyuan%20Chen%20and%20Cunjian%20Chen%20and%20Antitza%20Dantcheva%20and%20Bo%20Dai%20and%20Yu%20Qiao&entry.1292438233=%20%20Spatio-temporal%20coherency%20is%20a%20major%20challenge%20in%20synthesizing%20high%20quality%0Avideos%2C%20particularly%20in%20synthesizing%20human%20videos%20that%20contain%20rich%20global%20and%0Alocal%20deformations.%20To%20resolve%20this%20challenge%2C%20previous%20approaches%20have%0Aresorted%20to%20different%20features%20in%20the%20generation%20process%20aimed%20at%20representing%0Aappearance%20and%20motion.%20However%2C%20in%20the%20absence%20of%20strict%20mechanisms%20to%0Aguarantee%20such%20disentanglement%2C%20a%20separation%20of%20motion%20from%20appearance%20has%0Aremained%20challenging%2C%20resulting%20in%20spatial%20distortions%20and%20temporal%20jittering%0Athat%20break%20the%20spatio-temporal%20coherency.%20Motivated%20by%20this%2C%20we%20here%20propose%0ALEO%2C%20a%20novel%20framework%20for%20human%20video%20synthesis%2C%20placing%20emphasis%20on%0Aspatio-temporal%20coherency.%20Our%20key%20idea%20is%20to%20represent%20motion%20as%20a%20sequence%20of%0Aflow%20maps%20in%20the%20generation%20process%2C%20which%20inherently%20isolate%20motion%20from%0Aappearance.%20We%20implement%20this%20idea%20via%20a%20flow-based%20image%20animator%20and%20a%20Latent%0AMotion%20Diffusion%20Model%20%28LMDM%29.%20The%20former%20bridges%20a%20space%20of%20motion%20codes%20with%0Athe%20space%20of%20flow%20maps%2C%20and%20synthesizes%20video%20frames%20in%20a%20warp-and-inpaint%0Amanner.%20LMDM%20learns%20to%20capture%20motion%20prior%20in%20the%20training%20data%20by%0Asynthesizing%20sequences%20of%20motion%20codes.%20Extensive%20quantitative%20and%20qualitative%0Aanalysis%20suggests%20that%20LEO%20significantly%20improves%20coherent%20synthesis%20of%20human%0Avideos%20over%20previous%20methods%20on%20the%20datasets%20TaichiHD%2C%20FaceForensics%20and%0ACelebV-HQ.%20In%20addition%2C%20the%20effective%20disentanglement%20of%20appearance%20and%20motion%0Ain%20LEO%20allows%20for%20two%20additional%20tasks%2C%20namely%20infinite-length%20human%20video%0Asynthesis%2C%20as%20well%20as%20content-preserving%20video%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03989v3&entry.124074799=Read"},
{"title": "Emotion Classification of Children Expressions", "author": "Sanchayan Vivekananthan", "abstract": "  This paper proposes a process for a classification model for the facial\nexpressions. The proposed process would aid in specific categorisation of\nchildren's emotions from 2 emotions namely 'Happy' and 'Sad'. Since the\nexisting emotion recognition systems algorithms primarily train on adult faces,\nthe model developed is achieved by using advanced concepts of models with\nSqueeze-andExcitation blocks, Convolutional Block Attention modules, and robust\ndata augmentation. Stable Diffusion image synthesis was used for expanding and\ndiversifying the data set generating realistic and various training samples.\nThe model designed using Batch Normalisation, Dropout, and SE Attention\nmechanisms for the classification of children's emotions achieved an accuracy\nrate of 89\\% due to these methods improving the precision of emotion\nrecognition in children. The relative importance of this issue is raised in\nthis study with an emphasis on the call for a more specific model in emotion\ndetection systems for the young generation with specific direction on how the\nyoung people can be assisted to manage emotions while online.\n", "link": "http://arxiv.org/abs/2411.07708v1", "date": "2024-11-12", "relevancy": 2.3478, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4827}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.463}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion%20Classification%20of%20Children%20Expressions&body=Title%3A%20Emotion%20Classification%20of%20Children%20Expressions%0AAuthor%3A%20Sanchayan%20Vivekananthan%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20process%20for%20a%20classification%20model%20for%20the%20facial%0Aexpressions.%20The%20proposed%20process%20would%20aid%20in%20specific%20categorisation%20of%0Achildren%27s%20emotions%20from%202%20emotions%20namely%20%27Happy%27%20and%20%27Sad%27.%20Since%20the%0Aexisting%20emotion%20recognition%20systems%20algorithms%20primarily%20train%20on%20adult%20faces%2C%0Athe%20model%20developed%20is%20achieved%20by%20using%20advanced%20concepts%20of%20models%20with%0ASqueeze-andExcitation%20blocks%2C%20Convolutional%20Block%20Attention%20modules%2C%20and%20robust%0Adata%20augmentation.%20Stable%20Diffusion%20image%20synthesis%20was%20used%20for%20expanding%20and%0Adiversifying%20the%20data%20set%20generating%20realistic%20and%20various%20training%20samples.%0AThe%20model%20designed%20using%20Batch%20Normalisation%2C%20Dropout%2C%20and%20SE%20Attention%0Amechanisms%20for%20the%20classification%20of%20children%27s%20emotions%20achieved%20an%20accuracy%0Arate%20of%2089%5C%25%20due%20to%20these%20methods%20improving%20the%20precision%20of%20emotion%0Arecognition%20in%20children.%20The%20relative%20importance%20of%20this%20issue%20is%20raised%20in%0Athis%20study%20with%20an%20emphasis%20on%20the%20call%20for%20a%20more%20specific%20model%20in%20emotion%0Adetection%20systems%20for%20the%20young%20generation%20with%20specific%20direction%20on%20how%20the%0Ayoung%20people%20can%20be%20assisted%20to%20manage%20emotions%20while%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion%2520Classification%2520of%2520Children%2520Expressions%26entry.906535625%3DSanchayan%2520Vivekananthan%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520process%2520for%2520a%2520classification%2520model%2520for%2520the%2520facial%250Aexpressions.%2520The%2520proposed%2520process%2520would%2520aid%2520in%2520specific%2520categorisation%2520of%250Achildren%2527s%2520emotions%2520from%25202%2520emotions%2520namely%2520%2527Happy%2527%2520and%2520%2527Sad%2527.%2520Since%2520the%250Aexisting%2520emotion%2520recognition%2520systems%2520algorithms%2520primarily%2520train%2520on%2520adult%2520faces%252C%250Athe%2520model%2520developed%2520is%2520achieved%2520by%2520using%2520advanced%2520concepts%2520of%2520models%2520with%250ASqueeze-andExcitation%2520blocks%252C%2520Convolutional%2520Block%2520Attention%2520modules%252C%2520and%2520robust%250Adata%2520augmentation.%2520Stable%2520Diffusion%2520image%2520synthesis%2520was%2520used%2520for%2520expanding%2520and%250Adiversifying%2520the%2520data%2520set%2520generating%2520realistic%2520and%2520various%2520training%2520samples.%250AThe%2520model%2520designed%2520using%2520Batch%2520Normalisation%252C%2520Dropout%252C%2520and%2520SE%2520Attention%250Amechanisms%2520for%2520the%2520classification%2520of%2520children%2527s%2520emotions%2520achieved%2520an%2520accuracy%250Arate%2520of%252089%255C%2525%2520due%2520to%2520these%2520methods%2520improving%2520the%2520precision%2520of%2520emotion%250Arecognition%2520in%2520children.%2520The%2520relative%2520importance%2520of%2520this%2520issue%2520is%2520raised%2520in%250Athis%2520study%2520with%2520an%2520emphasis%2520on%2520the%2520call%2520for%2520a%2520more%2520specific%2520model%2520in%2520emotion%250Adetection%2520systems%2520for%2520the%2520young%2520generation%2520with%2520specific%2520direction%2520on%2520how%2520the%250Ayoung%2520people%2520can%2520be%2520assisted%2520to%2520manage%2520emotions%2520while%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion%20Classification%20of%20Children%20Expressions&entry.906535625=Sanchayan%20Vivekananthan&entry.1292438233=%20%20This%20paper%20proposes%20a%20process%20for%20a%20classification%20model%20for%20the%20facial%0Aexpressions.%20The%20proposed%20process%20would%20aid%20in%20specific%20categorisation%20of%0Achildren%27s%20emotions%20from%202%20emotions%20namely%20%27Happy%27%20and%20%27Sad%27.%20Since%20the%0Aexisting%20emotion%20recognition%20systems%20algorithms%20primarily%20train%20on%20adult%20faces%2C%0Athe%20model%20developed%20is%20achieved%20by%20using%20advanced%20concepts%20of%20models%20with%0ASqueeze-andExcitation%20blocks%2C%20Convolutional%20Block%20Attention%20modules%2C%20and%20robust%0Adata%20augmentation.%20Stable%20Diffusion%20image%20synthesis%20was%20used%20for%20expanding%20and%0Adiversifying%20the%20data%20set%20generating%20realistic%20and%20various%20training%20samples.%0AThe%20model%20designed%20using%20Batch%20Normalisation%2C%20Dropout%2C%20and%20SE%20Attention%0Amechanisms%20for%20the%20classification%20of%20children%27s%20emotions%20achieved%20an%20accuracy%0Arate%20of%2089%5C%25%20due%20to%20these%20methods%20improving%20the%20precision%20of%20emotion%0Arecognition%20in%20children.%20The%20relative%20importance%20of%20this%20issue%20is%20raised%20in%0Athis%20study%20with%20an%20emphasis%20on%20the%20call%20for%20a%20more%20specific%20model%20in%20emotion%0Adetection%20systems%20for%20the%20young%20generation%20with%20specific%20direction%20on%20how%20the%0Ayoung%20people%20can%20be%20assisted%20to%20manage%20emotions%20while%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07708v1&entry.124074799=Read"},
{"title": "PatchCTG: Patch Cardiotocography Transformer for Antepartum Fetal Health\n  Monitoring", "author": "M. Jaleed Khan and Manu Vatish and Gabriel Davis Jones", "abstract": "  Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but\ntraditional methods like the Dawes-Redman system are often limited by high\ninter-observer variability, leading to inconsistent interpretations and\npotential misdiagnoses. This paper introduces PatchCTG, a transformer-based\nmodel specifically designed for CTG analysis, employing patch-based\ntokenisation, instance normalisation and channel-independent processing to\ncapture essential local and global temporal dependencies within CTG signals.\nPatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over\n20,000 CTG traces across diverse clinical outcomes after applying the inclusion\nand exclusion criteria. With extensive hyperparameter optimisation, PatchCTG\nachieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at\nYouden's index threshold, demonstrating adaptability to various clinical needs.\nTesting across varying temporal thresholds showed robust predictive\nperformance, particularly with finetuning on data closer to delivery, achieving\na sensitivity of 52% and specificity of 88% for near-delivery cases. These\nfindings suggest the potential of PatchCTG to enhance clinical decision-making\nin antepartum care by providing a reliable, objective tool for fetal health\nassessment. The source code is available at\nhttps://github.com/jaleedkhan/PatchCTG.\n", "link": "http://arxiv.org/abs/2411.07796v1", "date": "2024-11-12", "relevancy": 2.3423, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4778}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4737}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchCTG%3A%20Patch%20Cardiotocography%20Transformer%20for%20Antepartum%20Fetal%20Health%0A%20%20Monitoring&body=Title%3A%20PatchCTG%3A%20Patch%20Cardiotocography%20Transformer%20for%20Antepartum%20Fetal%20Health%0A%20%20Monitoring%0AAuthor%3A%20M.%20Jaleed%20Khan%20and%20Manu%20Vatish%20and%20Gabriel%20Davis%20Jones%0AAbstract%3A%20%20%20Antepartum%20Cardiotocography%20%28CTG%29%20is%20vital%20for%20fetal%20health%20monitoring%2C%20but%0Atraditional%20methods%20like%20the%20Dawes-Redman%20system%20are%20often%20limited%20by%20high%0Ainter-observer%20variability%2C%20leading%20to%20inconsistent%20interpretations%20and%0Apotential%20misdiagnoses.%20This%20paper%20introduces%20PatchCTG%2C%20a%20transformer-based%0Amodel%20specifically%20designed%20for%20CTG%20analysis%2C%20employing%20patch-based%0Atokenisation%2C%20instance%20normalisation%20and%20channel-independent%20processing%20to%0Acapture%20essential%20local%20and%20global%20temporal%20dependencies%20within%20CTG%20signals.%0APatchCTG%20was%20evaluated%20on%20the%20Oxford%20Maternity%20%28OXMAT%29%20dataset%2C%20comprising%20over%0A20%2C000%20CTG%20traces%20across%20diverse%20clinical%20outcomes%20after%20applying%20the%20inclusion%0Aand%20exclusion%20criteria.%20With%20extensive%20hyperparameter%20optimisation%2C%20PatchCTG%0Aachieved%20an%20AUC%20of%2077%25%2C%20with%20specificity%20of%2088%25%20and%20sensitivity%20of%2057%25%20at%0AYouden%27s%20index%20threshold%2C%20demonstrating%20adaptability%20to%20various%20clinical%20needs.%0ATesting%20across%20varying%20temporal%20thresholds%20showed%20robust%20predictive%0Aperformance%2C%20particularly%20with%20finetuning%20on%20data%20closer%20to%20delivery%2C%20achieving%0Aa%20sensitivity%20of%2052%25%20and%20specificity%20of%2088%25%20for%20near-delivery%20cases.%20These%0Afindings%20suggest%20the%20potential%20of%20PatchCTG%20to%20enhance%20clinical%20decision-making%0Ain%20antepartum%20care%20by%20providing%20a%20reliable%2C%20objective%20tool%20for%20fetal%20health%0Aassessment.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/jaleedkhan/PatchCTG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchCTG%253A%2520Patch%2520Cardiotocography%2520Transformer%2520for%2520Antepartum%2520Fetal%2520Health%250A%2520%2520Monitoring%26entry.906535625%3DM.%2520Jaleed%2520Khan%2520and%2520Manu%2520Vatish%2520and%2520Gabriel%2520Davis%2520Jones%26entry.1292438233%3D%2520%2520Antepartum%2520Cardiotocography%2520%2528CTG%2529%2520is%2520vital%2520for%2520fetal%2520health%2520monitoring%252C%2520but%250Atraditional%2520methods%2520like%2520the%2520Dawes-Redman%2520system%2520are%2520often%2520limited%2520by%2520high%250Ainter-observer%2520variability%252C%2520leading%2520to%2520inconsistent%2520interpretations%2520and%250Apotential%2520misdiagnoses.%2520This%2520paper%2520introduces%2520PatchCTG%252C%2520a%2520transformer-based%250Amodel%2520specifically%2520designed%2520for%2520CTG%2520analysis%252C%2520employing%2520patch-based%250Atokenisation%252C%2520instance%2520normalisation%2520and%2520channel-independent%2520processing%2520to%250Acapture%2520essential%2520local%2520and%2520global%2520temporal%2520dependencies%2520within%2520CTG%2520signals.%250APatchCTG%2520was%2520evaluated%2520on%2520the%2520Oxford%2520Maternity%2520%2528OXMAT%2529%2520dataset%252C%2520comprising%2520over%250A20%252C000%2520CTG%2520traces%2520across%2520diverse%2520clinical%2520outcomes%2520after%2520applying%2520the%2520inclusion%250Aand%2520exclusion%2520criteria.%2520With%2520extensive%2520hyperparameter%2520optimisation%252C%2520PatchCTG%250Aachieved%2520an%2520AUC%2520of%252077%2525%252C%2520with%2520specificity%2520of%252088%2525%2520and%2520sensitivity%2520of%252057%2525%2520at%250AYouden%2527s%2520index%2520threshold%252C%2520demonstrating%2520adaptability%2520to%2520various%2520clinical%2520needs.%250ATesting%2520across%2520varying%2520temporal%2520thresholds%2520showed%2520robust%2520predictive%250Aperformance%252C%2520particularly%2520with%2520finetuning%2520on%2520data%2520closer%2520to%2520delivery%252C%2520achieving%250Aa%2520sensitivity%2520of%252052%2525%2520and%2520specificity%2520of%252088%2525%2520for%2520near-delivery%2520cases.%2520These%250Afindings%2520suggest%2520the%2520potential%2520of%2520PatchCTG%2520to%2520enhance%2520clinical%2520decision-making%250Ain%2520antepartum%2520care%2520by%2520providing%2520a%2520reliable%252C%2520objective%2520tool%2520for%2520fetal%2520health%250Aassessment.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jaleedkhan/PatchCTG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchCTG%3A%20Patch%20Cardiotocography%20Transformer%20for%20Antepartum%20Fetal%20Health%0A%20%20Monitoring&entry.906535625=M.%20Jaleed%20Khan%20and%20Manu%20Vatish%20and%20Gabriel%20Davis%20Jones&entry.1292438233=%20%20Antepartum%20Cardiotocography%20%28CTG%29%20is%20vital%20for%20fetal%20health%20monitoring%2C%20but%0Atraditional%20methods%20like%20the%20Dawes-Redman%20system%20are%20often%20limited%20by%20high%0Ainter-observer%20variability%2C%20leading%20to%20inconsistent%20interpretations%20and%0Apotential%20misdiagnoses.%20This%20paper%20introduces%20PatchCTG%2C%20a%20transformer-based%0Amodel%20specifically%20designed%20for%20CTG%20analysis%2C%20employing%20patch-based%0Atokenisation%2C%20instance%20normalisation%20and%20channel-independent%20processing%20to%0Acapture%20essential%20local%20and%20global%20temporal%20dependencies%20within%20CTG%20signals.%0APatchCTG%20was%20evaluated%20on%20the%20Oxford%20Maternity%20%28OXMAT%29%20dataset%2C%20comprising%20over%0A20%2C000%20CTG%20traces%20across%20diverse%20clinical%20outcomes%20after%20applying%20the%20inclusion%0Aand%20exclusion%20criteria.%20With%20extensive%20hyperparameter%20optimisation%2C%20PatchCTG%0Aachieved%20an%20AUC%20of%2077%25%2C%20with%20specificity%20of%2088%25%20and%20sensitivity%20of%2057%25%20at%0AYouden%27s%20index%20threshold%2C%20demonstrating%20adaptability%20to%20various%20clinical%20needs.%0ATesting%20across%20varying%20temporal%20thresholds%20showed%20robust%20predictive%0Aperformance%2C%20particularly%20with%20finetuning%20on%20data%20closer%20to%20delivery%2C%20achieving%0Aa%20sensitivity%20of%2052%25%20and%20specificity%20of%2088%25%20for%20near-delivery%20cases.%20These%0Afindings%20suggest%20the%20potential%20of%20PatchCTG%20to%20enhance%20clinical%20decision-making%0Ain%20antepartum%20care%20by%20providing%20a%20reliable%2C%20objective%20tool%20for%20fetal%20health%0Aassessment.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/jaleedkhan/PatchCTG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07796v1&entry.124074799=Read"},
{"title": "Learning Disentangled Representations for Perceptual Point Cloud Quality\n  Assessment via Mutual Information Minimization", "author": "Ziyu Shan and Yujie Zhang and Yipeng Liu and Yiling Xu", "abstract": "  No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively\nassess the human perceptual quality of point clouds without relying on\npristine-quality point clouds for reference. It is becoming increasingly\nsignificant with the rapid advancement of immersive media applications such as\nvirtual reality (VR) and augmented reality (AR). However, current NR-PCQA\nmodels attempt to indiscriminately learn point cloud content and distortion\nrepresentations within a single network, overlooking their distinct\ncontributions to quality information. To address this issue, we propose DisPA,\na novel disentangled representation learning framework for NR-PCQA. The\nframework trains a dual-branch disentanglement network to minimize mutual\ninformation (MI) between representations of point cloud content and distortion.\nSpecifically, to fully disentangle representations, the two branches adopt\ndifferent philosophies: the content-aware encoder is pretrained by a masked\nauto-encoding strategy, which can allow the encoder to capture semantic\ninformation from rendered images of distorted point clouds; the\ndistortion-aware encoder takes a mini-patch map as input, which forces the\nencoder to focus on low-level distortion patterns. Furthermore, we utilize an\nMI estimator to estimate the tight upper bound of the actual MI and further\nminimize it to achieve explicit representation disentanglement. Extensive\nexperimental results demonstrate that DisPA outperforms state-of-the-art\nmethods on multiple PCQA datasets.\n", "link": "http://arxiv.org/abs/2411.07936v1", "date": "2024-11-12", "relevancy": 2.3278, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6063}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5709}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Disentangled%20Representations%20for%20Perceptual%20Point%20Cloud%20Quality%0A%20%20Assessment%20via%20Mutual%20Information%20Minimization&body=Title%3A%20Learning%20Disentangled%20Representations%20for%20Perceptual%20Point%20Cloud%20Quality%0A%20%20Assessment%20via%20Mutual%20Information%20Minimization%0AAuthor%3A%20Ziyu%20Shan%20and%20Yujie%20Zhang%20and%20Yipeng%20Liu%20and%20Yiling%20Xu%0AAbstract%3A%20%20%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20%28NR-PCQA%29%20aims%20to%20objectively%0Aassess%20the%20human%20perceptual%20quality%20of%20point%20clouds%20without%20relying%20on%0Apristine-quality%20point%20clouds%20for%20reference.%20It%20is%20becoming%20increasingly%0Asignificant%20with%20the%20rapid%20advancement%20of%20immersive%20media%20applications%20such%20as%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29.%20However%2C%20current%20NR-PCQA%0Amodels%20attempt%20to%20indiscriminately%20learn%20point%20cloud%20content%20and%20distortion%0Arepresentations%20within%20a%20single%20network%2C%20overlooking%20their%20distinct%0Acontributions%20to%20quality%20information.%20To%20address%20this%20issue%2C%20we%20propose%20DisPA%2C%0Aa%20novel%20disentangled%20representation%20learning%20framework%20for%20NR-PCQA.%20The%0Aframework%20trains%20a%20dual-branch%20disentanglement%20network%20to%20minimize%20mutual%0Ainformation%20%28MI%29%20between%20representations%20of%20point%20cloud%20content%20and%20distortion.%0ASpecifically%2C%20to%20fully%20disentangle%20representations%2C%20the%20two%20branches%20adopt%0Adifferent%20philosophies%3A%20the%20content-aware%20encoder%20is%20pretrained%20by%20a%20masked%0Aauto-encoding%20strategy%2C%20which%20can%20allow%20the%20encoder%20to%20capture%20semantic%0Ainformation%20from%20rendered%20images%20of%20distorted%20point%20clouds%3B%20the%0Adistortion-aware%20encoder%20takes%20a%20mini-patch%20map%20as%20input%2C%20which%20forces%20the%0Aencoder%20to%20focus%20on%20low-level%20distortion%20patterns.%20Furthermore%2C%20we%20utilize%20an%0AMI%20estimator%20to%20estimate%20the%20tight%20upper%20bound%20of%20the%20actual%20MI%20and%20further%0Aminimize%20it%20to%20achieve%20explicit%20representation%20disentanglement.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20DisPA%20outperforms%20state-of-the-art%0Amethods%20on%20multiple%20PCQA%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Disentangled%2520Representations%2520for%2520Perceptual%2520Point%2520Cloud%2520Quality%250A%2520%2520Assessment%2520via%2520Mutual%2520Information%2520Minimization%26entry.906535625%3DZiyu%2520Shan%2520and%2520Yujie%2520Zhang%2520and%2520Yipeng%2520Liu%2520and%2520Yiling%2520Xu%26entry.1292438233%3D%2520%2520No-Reference%2520Point%2520Cloud%2520Quality%2520Assessment%2520%2528NR-PCQA%2529%2520aims%2520to%2520objectively%250Aassess%2520the%2520human%2520perceptual%2520quality%2520of%2520point%2520clouds%2520without%2520relying%2520on%250Apristine-quality%2520point%2520clouds%2520for%2520reference.%2520It%2520is%2520becoming%2520increasingly%250Asignificant%2520with%2520the%2520rapid%2520advancement%2520of%2520immersive%2520media%2520applications%2520such%2520as%250Avirtual%2520reality%2520%2528VR%2529%2520and%2520augmented%2520reality%2520%2528AR%2529.%2520However%252C%2520current%2520NR-PCQA%250Amodels%2520attempt%2520to%2520indiscriminately%2520learn%2520point%2520cloud%2520content%2520and%2520distortion%250Arepresentations%2520within%2520a%2520single%2520network%252C%2520overlooking%2520their%2520distinct%250Acontributions%2520to%2520quality%2520information.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520DisPA%252C%250Aa%2520novel%2520disentangled%2520representation%2520learning%2520framework%2520for%2520NR-PCQA.%2520The%250Aframework%2520trains%2520a%2520dual-branch%2520disentanglement%2520network%2520to%2520minimize%2520mutual%250Ainformation%2520%2528MI%2529%2520between%2520representations%2520of%2520point%2520cloud%2520content%2520and%2520distortion.%250ASpecifically%252C%2520to%2520fully%2520disentangle%2520representations%252C%2520the%2520two%2520branches%2520adopt%250Adifferent%2520philosophies%253A%2520the%2520content-aware%2520encoder%2520is%2520pretrained%2520by%2520a%2520masked%250Aauto-encoding%2520strategy%252C%2520which%2520can%2520allow%2520the%2520encoder%2520to%2520capture%2520semantic%250Ainformation%2520from%2520rendered%2520images%2520of%2520distorted%2520point%2520clouds%253B%2520the%250Adistortion-aware%2520encoder%2520takes%2520a%2520mini-patch%2520map%2520as%2520input%252C%2520which%2520forces%2520the%250Aencoder%2520to%2520focus%2520on%2520low-level%2520distortion%2520patterns.%2520Furthermore%252C%2520we%2520utilize%2520an%250AMI%2520estimator%2520to%2520estimate%2520the%2520tight%2520upper%2520bound%2520of%2520the%2520actual%2520MI%2520and%2520further%250Aminimize%2520it%2520to%2520achieve%2520explicit%2520representation%2520disentanglement.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520DisPA%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520multiple%2520PCQA%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Disentangled%20Representations%20for%20Perceptual%20Point%20Cloud%20Quality%0A%20%20Assessment%20via%20Mutual%20Information%20Minimization&entry.906535625=Ziyu%20Shan%20and%20Yujie%20Zhang%20and%20Yipeng%20Liu%20and%20Yiling%20Xu&entry.1292438233=%20%20No-Reference%20Point%20Cloud%20Quality%20Assessment%20%28NR-PCQA%29%20aims%20to%20objectively%0Aassess%20the%20human%20perceptual%20quality%20of%20point%20clouds%20without%20relying%20on%0Apristine-quality%20point%20clouds%20for%20reference.%20It%20is%20becoming%20increasingly%0Asignificant%20with%20the%20rapid%20advancement%20of%20immersive%20media%20applications%20such%20as%0Avirtual%20reality%20%28VR%29%20and%20augmented%20reality%20%28AR%29.%20However%2C%20current%20NR-PCQA%0Amodels%20attempt%20to%20indiscriminately%20learn%20point%20cloud%20content%20and%20distortion%0Arepresentations%20within%20a%20single%20network%2C%20overlooking%20their%20distinct%0Acontributions%20to%20quality%20information.%20To%20address%20this%20issue%2C%20we%20propose%20DisPA%2C%0Aa%20novel%20disentangled%20representation%20learning%20framework%20for%20NR-PCQA.%20The%0Aframework%20trains%20a%20dual-branch%20disentanglement%20network%20to%20minimize%20mutual%0Ainformation%20%28MI%29%20between%20representations%20of%20point%20cloud%20content%20and%20distortion.%0ASpecifically%2C%20to%20fully%20disentangle%20representations%2C%20the%20two%20branches%20adopt%0Adifferent%20philosophies%3A%20the%20content-aware%20encoder%20is%20pretrained%20by%20a%20masked%0Aauto-encoding%20strategy%2C%20which%20can%20allow%20the%20encoder%20to%20capture%20semantic%0Ainformation%20from%20rendered%20images%20of%20distorted%20point%20clouds%3B%20the%0Adistortion-aware%20encoder%20takes%20a%20mini-patch%20map%20as%20input%2C%20which%20forces%20the%0Aencoder%20to%20focus%20on%20low-level%20distortion%20patterns.%20Furthermore%2C%20we%20utilize%20an%0AMI%20estimator%20to%20estimate%20the%20tight%20upper%20bound%20of%20the%20actual%20MI%20and%20further%0Aminimize%20it%20to%20achieve%20explicit%20representation%20disentanglement.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20DisPA%20outperforms%20state-of-the-art%0Amethods%20on%20multiple%20PCQA%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07936v1&entry.124074799=Read"},
{"title": "Act in Collusion: A Persistent Distributed Multi-Target Backdoor in\n  Federated Learning", "author": "Tao Liu and Wu Yang and Chen Xu and Jiguang Lv and Huanran Wang and Yuhang Zhang and Shuchun Xu and Dapeng Man", "abstract": "  Federated learning, a novel paradigm designed to protect data privacy, is\nvulnerable to backdoor attacks due to its distributed nature. Current research\noften designs attacks based on a single attacker with a single backdoor,\noverlooking more realistic and complex threats in federated learning. We\npropose a more practical threat model for federated learning: the distributed\nmulti-target backdoor. In this model, multiple attackers control different\nclients, embedding various triggers and targeting different classes,\ncollaboratively implanting backdoors into the global model via central\naggregation. Empirical validation shows that existing methods struggle to\nmaintain the effectiveness of multiple backdoors in the global model. Our key\ninsight is that similar backdoor triggers cause parameter conflicts and\ninjecting new backdoors disrupts gradient directions, significantly weakening\nsome backdoors performance. To solve this, we propose a Distributed\nMulti-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of\nbackdoors from different malicious clients. To avoid parameter conflicts, we\ndesign a multi-channel dispersed frequency trigger strategy to maximize trigger\ndifferences. To mitigate gradient interference, we introduce backdoor replay in\nlocal training to neutralize conflicting gradients. Extensive validation shows\nthat 30 rounds after the attack, Attack Success Rates of three different\nbackdoors from various clients remain above 93%. The code will be made publicly\navailable after the review period.\n", "link": "http://arxiv.org/abs/2411.03926v2", "date": "2024-11-12", "relevancy": 2.3086, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4633}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning&body=Title%3A%20Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning%0AAuthor%3A%20Tao%20Liu%20and%20Wu%20Yang%20and%20Chen%20Xu%20and%20Jiguang%20Lv%20and%20Huanran%20Wang%20and%20Yuhang%20Zhang%20and%20Shuchun%20Xu%20and%20Dapeng%20Man%0AAbstract%3A%20%20%20Federated%20learning%2C%20a%20novel%20paradigm%20designed%20to%20protect%20data%20privacy%2C%20is%0Avulnerable%20to%20backdoor%20attacks%20due%20to%20its%20distributed%20nature.%20Current%20research%0Aoften%20designs%20attacks%20based%20on%20a%20single%20attacker%20with%20a%20single%20backdoor%2C%0Aoverlooking%20more%20realistic%20and%20complex%20threats%20in%20federated%20learning.%20We%0Apropose%20a%20more%20practical%20threat%20model%20for%20federated%20learning%3A%20the%20distributed%0Amulti-target%20backdoor.%20In%20this%20model%2C%20multiple%20attackers%20control%20different%0Aclients%2C%20embedding%20various%20triggers%20and%20targeting%20different%20classes%2C%0Acollaboratively%20implanting%20backdoors%20into%20the%20global%20model%20via%20central%0Aaggregation.%20Empirical%20validation%20shows%20that%20existing%20methods%20struggle%20to%0Amaintain%20the%20effectiveness%20of%20multiple%20backdoors%20in%20the%20global%20model.%20Our%20key%0Ainsight%20is%20that%20similar%20backdoor%20triggers%20cause%20parameter%20conflicts%20and%0Ainjecting%20new%20backdoors%20disrupts%20gradient%20directions%2C%20significantly%20weakening%0Asome%20backdoors%20performance.%20To%20solve%20this%2C%20we%20propose%20a%20Distributed%0AMulti-Target%20Backdoor%20Attack%20%28DMBA%29%2C%20ensuring%20efficiency%20and%20persistence%20of%0Abackdoors%20from%20different%20malicious%20clients.%20To%20avoid%20parameter%20conflicts%2C%20we%0Adesign%20a%20multi-channel%20dispersed%20frequency%20trigger%20strategy%20to%20maximize%20trigger%0Adifferences.%20To%20mitigate%20gradient%20interference%2C%20we%20introduce%20backdoor%20replay%20in%0Alocal%20training%20to%20neutralize%20conflicting%20gradients.%20Extensive%20validation%20shows%0Athat%2030%20rounds%20after%20the%20attack%2C%20Attack%20Success%20Rates%20of%20three%20different%0Abackdoors%20from%20various%20clients%20remain%20above%2093%25.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20period.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAct%2520in%2520Collusion%253A%2520A%2520Persistent%2520Distributed%2520Multi-Target%2520Backdoor%2520in%250A%2520%2520Federated%2520Learning%26entry.906535625%3DTao%2520Liu%2520and%2520Wu%2520Yang%2520and%2520Chen%2520Xu%2520and%2520Jiguang%2520Lv%2520and%2520Huanran%2520Wang%2520and%2520Yuhang%2520Zhang%2520and%2520Shuchun%2520Xu%2520and%2520Dapeng%2520Man%26entry.1292438233%3D%2520%2520Federated%2520learning%252C%2520a%2520novel%2520paradigm%2520designed%2520to%2520protect%2520data%2520privacy%252C%2520is%250Avulnerable%2520to%2520backdoor%2520attacks%2520due%2520to%2520its%2520distributed%2520nature.%2520Current%2520research%250Aoften%2520designs%2520attacks%2520based%2520on%2520a%2520single%2520attacker%2520with%2520a%2520single%2520backdoor%252C%250Aoverlooking%2520more%2520realistic%2520and%2520complex%2520threats%2520in%2520federated%2520learning.%2520We%250Apropose%2520a%2520more%2520practical%2520threat%2520model%2520for%2520federated%2520learning%253A%2520the%2520distributed%250Amulti-target%2520backdoor.%2520In%2520this%2520model%252C%2520multiple%2520attackers%2520control%2520different%250Aclients%252C%2520embedding%2520various%2520triggers%2520and%2520targeting%2520different%2520classes%252C%250Acollaboratively%2520implanting%2520backdoors%2520into%2520the%2520global%2520model%2520via%2520central%250Aaggregation.%2520Empirical%2520validation%2520shows%2520that%2520existing%2520methods%2520struggle%2520to%250Amaintain%2520the%2520effectiveness%2520of%2520multiple%2520backdoors%2520in%2520the%2520global%2520model.%2520Our%2520key%250Ainsight%2520is%2520that%2520similar%2520backdoor%2520triggers%2520cause%2520parameter%2520conflicts%2520and%250Ainjecting%2520new%2520backdoors%2520disrupts%2520gradient%2520directions%252C%2520significantly%2520weakening%250Asome%2520backdoors%2520performance.%2520To%2520solve%2520this%252C%2520we%2520propose%2520a%2520Distributed%250AMulti-Target%2520Backdoor%2520Attack%2520%2528DMBA%2529%252C%2520ensuring%2520efficiency%2520and%2520persistence%2520of%250Abackdoors%2520from%2520different%2520malicious%2520clients.%2520To%2520avoid%2520parameter%2520conflicts%252C%2520we%250Adesign%2520a%2520multi-channel%2520dispersed%2520frequency%2520trigger%2520strategy%2520to%2520maximize%2520trigger%250Adifferences.%2520To%2520mitigate%2520gradient%2520interference%252C%2520we%2520introduce%2520backdoor%2520replay%2520in%250Alocal%2520training%2520to%2520neutralize%2520conflicting%2520gradients.%2520Extensive%2520validation%2520shows%250Athat%252030%2520rounds%2520after%2520the%2520attack%252C%2520Attack%2520Success%2520Rates%2520of%2520three%2520different%250Abackdoors%2520from%2520various%2520clients%2520remain%2520above%252093%2525.%2520The%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520after%2520the%2520review%2520period.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Act%20in%20Collusion%3A%20A%20Persistent%20Distributed%20Multi-Target%20Backdoor%20in%0A%20%20Federated%20Learning&entry.906535625=Tao%20Liu%20and%20Wu%20Yang%20and%20Chen%20Xu%20and%20Jiguang%20Lv%20and%20Huanran%20Wang%20and%20Yuhang%20Zhang%20and%20Shuchun%20Xu%20and%20Dapeng%20Man&entry.1292438233=%20%20Federated%20learning%2C%20a%20novel%20paradigm%20designed%20to%20protect%20data%20privacy%2C%20is%0Avulnerable%20to%20backdoor%20attacks%20due%20to%20its%20distributed%20nature.%20Current%20research%0Aoften%20designs%20attacks%20based%20on%20a%20single%20attacker%20with%20a%20single%20backdoor%2C%0Aoverlooking%20more%20realistic%20and%20complex%20threats%20in%20federated%20learning.%20We%0Apropose%20a%20more%20practical%20threat%20model%20for%20federated%20learning%3A%20the%20distributed%0Amulti-target%20backdoor.%20In%20this%20model%2C%20multiple%20attackers%20control%20different%0Aclients%2C%20embedding%20various%20triggers%20and%20targeting%20different%20classes%2C%0Acollaboratively%20implanting%20backdoors%20into%20the%20global%20model%20via%20central%0Aaggregation.%20Empirical%20validation%20shows%20that%20existing%20methods%20struggle%20to%0Amaintain%20the%20effectiveness%20of%20multiple%20backdoors%20in%20the%20global%20model.%20Our%20key%0Ainsight%20is%20that%20similar%20backdoor%20triggers%20cause%20parameter%20conflicts%20and%0Ainjecting%20new%20backdoors%20disrupts%20gradient%20directions%2C%20significantly%20weakening%0Asome%20backdoors%20performance.%20To%20solve%20this%2C%20we%20propose%20a%20Distributed%0AMulti-Target%20Backdoor%20Attack%20%28DMBA%29%2C%20ensuring%20efficiency%20and%20persistence%20of%0Abackdoors%20from%20different%20malicious%20clients.%20To%20avoid%20parameter%20conflicts%2C%20we%0Adesign%20a%20multi-channel%20dispersed%20frequency%20trigger%20strategy%20to%20maximize%20trigger%0Adifferences.%20To%20mitigate%20gradient%20interference%2C%20we%20introduce%20backdoor%20replay%20in%0Alocal%20training%20to%20neutralize%20conflicting%20gradients.%20Extensive%20validation%20shows%0Athat%2030%20rounds%20after%20the%20attack%2C%20Attack%20Success%20Rates%20of%20three%20different%0Abackdoors%20from%20various%20clients%20remain%20above%2093%25.%20The%20code%20will%20be%20made%20publicly%0Aavailable%20after%20the%20review%20period.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03926v2&entry.124074799=Read"},
{"title": "Adapting Segment Anything Model to Multi-modal Salient Object Detection\n  with Semantic Feature Fusion Guidance", "author": "Kunpeng Wang and Danying Lin and Chenglong Li and Zhengzheng Tu and Bin Luo", "abstract": "  Although most existing multi-modal salient object detection (SOD) methods\ndemonstrate effectiveness through training models from scratch, the limited\nmulti-modal data hinders these methods from reaching optimality. In this paper,\nwe propose a novel framework to explore and exploit the powerful feature\nrepresentation and zero-shot generalization ability of the pre-trained Segment\nAnything Model (SAM) for multi-modal SOD. Despite serving as a recent vision\nfundamental model, driving the class-agnostic SAM to comprehend and detect\nsalient objects accurately is non-trivial, especially in challenging scenes. To\nthis end, we develop \\underline{SAM} with se\\underline{m}antic\nf\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which\nincorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to\nmulti-modal SOD tasks. However, it is difficult for SAM trained on single-modal\ndata to directly mine the complementary benefits of multi-modal inputs and\ncomprehensively utilize them to achieve accurate saliency prediction. To\naddress these issues, we first design a multi-modal complementary fusion module\nto extract robust multi-modal semantic features by integrating information from\nvisible and thermal or depth image pairs. Then, we feed the extracted\nmulti-modal semantic features into both the SAM image encoder and mask decoder\nfor fine-tuning and prompting, respectively. Specifically, in the image\nencoder, a multi-modal adapter is proposed to adapt the single-modal SAM to\nmulti-modal information. In the mask decoder, a semantic-geometric prompt\ngeneration strategy is proposed to produce corresponding embeddings with\nvarious saliency cues. Extensive experiments on both RGB-D and RGB-T SOD\nbenchmarks show the effectiveness of the proposed framework. The code will be\navailable at \\url{https://github.com/Angknpng/Sammese}.\n", "link": "http://arxiv.org/abs/2408.15063v4", "date": "2024-11-12", "relevancy": 2.2961, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5998}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Segment%20Anything%20Model%20to%20Multi-modal%20Salient%20Object%20Detection%0A%20%20with%20Semantic%20Feature%20Fusion%20Guidance&body=Title%3A%20Adapting%20Segment%20Anything%20Model%20to%20Multi-modal%20Salient%20Object%20Detection%0A%20%20with%20Semantic%20Feature%20Fusion%20Guidance%0AAuthor%3A%20Kunpeng%20Wang%20and%20Danying%20Lin%20and%20Chenglong%20Li%20and%20Zhengzheng%20Tu%20and%20Bin%20Luo%0AAbstract%3A%20%20%20Although%20most%20existing%20multi-modal%20salient%20object%20detection%20%28SOD%29%20methods%0Ademonstrate%20effectiveness%20through%20training%20models%20from%20scratch%2C%20the%20limited%0Amulti-modal%20data%20hinders%20these%20methods%20from%20reaching%20optimality.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20framework%20to%20explore%20and%20exploit%20the%20powerful%20feature%0Arepresentation%20and%20zero-shot%20generalization%20ability%20of%20the%20pre-trained%20Segment%0AAnything%20Model%20%28SAM%29%20for%20multi-modal%20SOD.%20Despite%20serving%20as%20a%20recent%20vision%0Afundamental%20model%2C%20driving%20the%20class-agnostic%20SAM%20to%20comprehend%20and%20detect%0Asalient%20objects%20accurately%20is%20non-trivial%2C%20especially%20in%20challenging%20scenes.%20To%0Athis%20end%2C%20we%20develop%20%5Cunderline%7BSAM%7D%20with%20se%5Cunderline%7Bm%7Dantic%0Af%5Cunderline%7Be%7Dature%20fu%5Cunderline%7Bs%7Dion%20guidanc%5Cunderline%7Be%7D%20%28Sammese%29%2C%20which%0Aincorporates%20multi-modal%20saliency-specific%20knowledge%20into%20SAM%20to%20adapt%20SAM%20to%0Amulti-modal%20SOD%20tasks.%20However%2C%20it%20is%20difficult%20for%20SAM%20trained%20on%20single-modal%0Adata%20to%20directly%20mine%20the%20complementary%20benefits%20of%20multi-modal%20inputs%20and%0Acomprehensively%20utilize%20them%20to%20achieve%20accurate%20saliency%20prediction.%20To%0Aaddress%20these%20issues%2C%20we%20first%20design%20a%20multi-modal%20complementary%20fusion%20module%0Ato%20extract%20robust%20multi-modal%20semantic%20features%20by%20integrating%20information%20from%0Avisible%20and%20thermal%20or%20depth%20image%20pairs.%20Then%2C%20we%20feed%20the%20extracted%0Amulti-modal%20semantic%20features%20into%20both%20the%20SAM%20image%20encoder%20and%20mask%20decoder%0Afor%20fine-tuning%20and%20prompting%2C%20respectively.%20Specifically%2C%20in%20the%20image%0Aencoder%2C%20a%20multi-modal%20adapter%20is%20proposed%20to%20adapt%20the%20single-modal%20SAM%20to%0Amulti-modal%20information.%20In%20the%20mask%20decoder%2C%20a%20semantic-geometric%20prompt%0Ageneration%20strategy%20is%20proposed%20to%20produce%20corresponding%20embeddings%20with%0Avarious%20saliency%20cues.%20Extensive%20experiments%20on%20both%20RGB-D%20and%20RGB-T%20SOD%0Abenchmarks%20show%20the%20effectiveness%20of%20the%20proposed%20framework.%20The%20code%20will%20be%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Angknpng/Sammese%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15063v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Segment%2520Anything%2520Model%2520to%2520Multi-modal%2520Salient%2520Object%2520Detection%250A%2520%2520with%2520Semantic%2520Feature%2520Fusion%2520Guidance%26entry.906535625%3DKunpeng%2520Wang%2520and%2520Danying%2520Lin%2520and%2520Chenglong%2520Li%2520and%2520Zhengzheng%2520Tu%2520and%2520Bin%2520Luo%26entry.1292438233%3D%2520%2520Although%2520most%2520existing%2520multi-modal%2520salient%2520object%2520detection%2520%2528SOD%2529%2520methods%250Ademonstrate%2520effectiveness%2520through%2520training%2520models%2520from%2520scratch%252C%2520the%2520limited%250Amulti-modal%2520data%2520hinders%2520these%2520methods%2520from%2520reaching%2520optimality.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520framework%2520to%2520explore%2520and%2520exploit%2520the%2520powerful%2520feature%250Arepresentation%2520and%2520zero-shot%2520generalization%2520ability%2520of%2520the%2520pre-trained%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520for%2520multi-modal%2520SOD.%2520Despite%2520serving%2520as%2520a%2520recent%2520vision%250Afundamental%2520model%252C%2520driving%2520the%2520class-agnostic%2520SAM%2520to%2520comprehend%2520and%2520detect%250Asalient%2520objects%2520accurately%2520is%2520non-trivial%252C%2520especially%2520in%2520challenging%2520scenes.%2520To%250Athis%2520end%252C%2520we%2520develop%2520%255Cunderline%257BSAM%257D%2520with%2520se%255Cunderline%257Bm%257Dantic%250Af%255Cunderline%257Be%257Dature%2520fu%255Cunderline%257Bs%257Dion%2520guidanc%255Cunderline%257Be%257D%2520%2528Sammese%2529%252C%2520which%250Aincorporates%2520multi-modal%2520saliency-specific%2520knowledge%2520into%2520SAM%2520to%2520adapt%2520SAM%2520to%250Amulti-modal%2520SOD%2520tasks.%2520However%252C%2520it%2520is%2520difficult%2520for%2520SAM%2520trained%2520on%2520single-modal%250Adata%2520to%2520directly%2520mine%2520the%2520complementary%2520benefits%2520of%2520multi-modal%2520inputs%2520and%250Acomprehensively%2520utilize%2520them%2520to%2520achieve%2520accurate%2520saliency%2520prediction.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520first%2520design%2520a%2520multi-modal%2520complementary%2520fusion%2520module%250Ato%2520extract%2520robust%2520multi-modal%2520semantic%2520features%2520by%2520integrating%2520information%2520from%250Avisible%2520and%2520thermal%2520or%2520depth%2520image%2520pairs.%2520Then%252C%2520we%2520feed%2520the%2520extracted%250Amulti-modal%2520semantic%2520features%2520into%2520both%2520the%2520SAM%2520image%2520encoder%2520and%2520mask%2520decoder%250Afor%2520fine-tuning%2520and%2520prompting%252C%2520respectively.%2520Specifically%252C%2520in%2520the%2520image%250Aencoder%252C%2520a%2520multi-modal%2520adapter%2520is%2520proposed%2520to%2520adapt%2520the%2520single-modal%2520SAM%2520to%250Amulti-modal%2520information.%2520In%2520the%2520mask%2520decoder%252C%2520a%2520semantic-geometric%2520prompt%250Ageneration%2520strategy%2520is%2520proposed%2520to%2520produce%2520corresponding%2520embeddings%2520with%250Avarious%2520saliency%2520cues.%2520Extensive%2520experiments%2520on%2520both%2520RGB-D%2520and%2520RGB-T%2520SOD%250Abenchmarks%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework.%2520The%2520code%2520will%2520be%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/Angknpng/Sammese%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15063v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Segment%20Anything%20Model%20to%20Multi-modal%20Salient%20Object%20Detection%0A%20%20with%20Semantic%20Feature%20Fusion%20Guidance&entry.906535625=Kunpeng%20Wang%20and%20Danying%20Lin%20and%20Chenglong%20Li%20and%20Zhengzheng%20Tu%20and%20Bin%20Luo&entry.1292438233=%20%20Although%20most%20existing%20multi-modal%20salient%20object%20detection%20%28SOD%29%20methods%0Ademonstrate%20effectiveness%20through%20training%20models%20from%20scratch%2C%20the%20limited%0Amulti-modal%20data%20hinders%20these%20methods%20from%20reaching%20optimality.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20framework%20to%20explore%20and%20exploit%20the%20powerful%20feature%0Arepresentation%20and%20zero-shot%20generalization%20ability%20of%20the%20pre-trained%20Segment%0AAnything%20Model%20%28SAM%29%20for%20multi-modal%20SOD.%20Despite%20serving%20as%20a%20recent%20vision%0Afundamental%20model%2C%20driving%20the%20class-agnostic%20SAM%20to%20comprehend%20and%20detect%0Asalient%20objects%20accurately%20is%20non-trivial%2C%20especially%20in%20challenging%20scenes.%20To%0Athis%20end%2C%20we%20develop%20%5Cunderline%7BSAM%7D%20with%20se%5Cunderline%7Bm%7Dantic%0Af%5Cunderline%7Be%7Dature%20fu%5Cunderline%7Bs%7Dion%20guidanc%5Cunderline%7Be%7D%20%28Sammese%29%2C%20which%0Aincorporates%20multi-modal%20saliency-specific%20knowledge%20into%20SAM%20to%20adapt%20SAM%20to%0Amulti-modal%20SOD%20tasks.%20However%2C%20it%20is%20difficult%20for%20SAM%20trained%20on%20single-modal%0Adata%20to%20directly%20mine%20the%20complementary%20benefits%20of%20multi-modal%20inputs%20and%0Acomprehensively%20utilize%20them%20to%20achieve%20accurate%20saliency%20prediction.%20To%0Aaddress%20these%20issues%2C%20we%20first%20design%20a%20multi-modal%20complementary%20fusion%20module%0Ato%20extract%20robust%20multi-modal%20semantic%20features%20by%20integrating%20information%20from%0Avisible%20and%20thermal%20or%20depth%20image%20pairs.%20Then%2C%20we%20feed%20the%20extracted%0Amulti-modal%20semantic%20features%20into%20both%20the%20SAM%20image%20encoder%20and%20mask%20decoder%0Afor%20fine-tuning%20and%20prompting%2C%20respectively.%20Specifically%2C%20in%20the%20image%0Aencoder%2C%20a%20multi-modal%20adapter%20is%20proposed%20to%20adapt%20the%20single-modal%20SAM%20to%0Amulti-modal%20information.%20In%20the%20mask%20decoder%2C%20a%20semantic-geometric%20prompt%0Ageneration%20strategy%20is%20proposed%20to%20produce%20corresponding%20embeddings%20with%0Avarious%20saliency%20cues.%20Extensive%20experiments%20on%20both%20RGB-D%20and%20RGB-T%20SOD%0Abenchmarks%20show%20the%20effectiveness%20of%20the%20proposed%20framework.%20The%20code%20will%20be%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Angknpng/Sammese%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15063v4&entry.124074799=Read"},
{"title": "Chain Association-based Attacking and Shielding Natural Language\n  Processing Systems", "author": "Jiacheng Huang and Long Chen", "abstract": "  Association as a gift enables people do not have to mention something in\ncompletely straightforward words and allows others to understand what they\nintend to refer to. In this paper, we propose a chain association-based\nadversarial attack against natural language processing systems, utilizing the\ncomprehension gap between humans and machines. We first generate a chain\nassociation graph for Chinese characters based on the association paradigm for\nbuilding search space of potential adversarial examples. Then, we introduce an\ndiscrete particle swarm optimization algorithm to search for the optimal\nadversarial examples. We conduct comprehensive experiments and show that\nadvanced natural language processing models and applications, including large\nlanguage models, are vulnerable to our attack, while humans appear good at\nunderstanding the perturbed text. We also explore two methods, including\nadversarial training and associative graph-based recovery, to shield systems\nfrom chain association-based attack. Since a few examples that use some\nderogatory terms, this paper contains materials that may be offensive or\nupsetting to some people.\n", "link": "http://arxiv.org/abs/2411.07843v1", "date": "2024-11-12", "relevancy": 2.2901, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain%20Association-based%20Attacking%20and%20Shielding%20Natural%20Language%0A%20%20Processing%20Systems&body=Title%3A%20Chain%20Association-based%20Attacking%20and%20Shielding%20Natural%20Language%0A%20%20Processing%20Systems%0AAuthor%3A%20Jiacheng%20Huang%20and%20Long%20Chen%0AAbstract%3A%20%20%20Association%20as%20a%20gift%20enables%20people%20do%20not%20have%20to%20mention%20something%20in%0Acompletely%20straightforward%20words%20and%20allows%20others%20to%20understand%20what%20they%0Aintend%20to%20refer%20to.%20In%20this%20paper%2C%20we%20propose%20a%20chain%20association-based%0Aadversarial%20attack%20against%20natural%20language%20processing%20systems%2C%20utilizing%20the%0Acomprehension%20gap%20between%20humans%20and%20machines.%20We%20first%20generate%20a%20chain%0Aassociation%20graph%20for%20Chinese%20characters%20based%20on%20the%20association%20paradigm%20for%0Abuilding%20search%20space%20of%20potential%20adversarial%20examples.%20Then%2C%20we%20introduce%20an%0Adiscrete%20particle%20swarm%20optimization%20algorithm%20to%20search%20for%20the%20optimal%0Aadversarial%20examples.%20We%20conduct%20comprehensive%20experiments%20and%20show%20that%0Aadvanced%20natural%20language%20processing%20models%20and%20applications%2C%20including%20large%0Alanguage%20models%2C%20are%20vulnerable%20to%20our%20attack%2C%20while%20humans%20appear%20good%20at%0Aunderstanding%20the%20perturbed%20text.%20We%20also%20explore%20two%20methods%2C%20including%0Aadversarial%20training%20and%20associative%20graph-based%20recovery%2C%20to%20shield%20systems%0Afrom%20chain%20association-based%20attack.%20Since%20a%20few%20examples%20that%20use%20some%0Aderogatory%20terms%2C%20this%20paper%20contains%20materials%20that%20may%20be%20offensive%20or%0Aupsetting%20to%20some%20people.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain%2520Association-based%2520Attacking%2520and%2520Shielding%2520Natural%2520Language%250A%2520%2520Processing%2520Systems%26entry.906535625%3DJiacheng%2520Huang%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Association%2520as%2520a%2520gift%2520enables%2520people%2520do%2520not%2520have%2520to%2520mention%2520something%2520in%250Acompletely%2520straightforward%2520words%2520and%2520allows%2520others%2520to%2520understand%2520what%2520they%250Aintend%2520to%2520refer%2520to.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520chain%2520association-based%250Aadversarial%2520attack%2520against%2520natural%2520language%2520processing%2520systems%252C%2520utilizing%2520the%250Acomprehension%2520gap%2520between%2520humans%2520and%2520machines.%2520We%2520first%2520generate%2520a%2520chain%250Aassociation%2520graph%2520for%2520Chinese%2520characters%2520based%2520on%2520the%2520association%2520paradigm%2520for%250Abuilding%2520search%2520space%2520of%2520potential%2520adversarial%2520examples.%2520Then%252C%2520we%2520introduce%2520an%250Adiscrete%2520particle%2520swarm%2520optimization%2520algorithm%2520to%2520search%2520for%2520the%2520optimal%250Aadversarial%2520examples.%2520We%2520conduct%2520comprehensive%2520experiments%2520and%2520show%2520that%250Aadvanced%2520natural%2520language%2520processing%2520models%2520and%2520applications%252C%2520including%2520large%250Alanguage%2520models%252C%2520are%2520vulnerable%2520to%2520our%2520attack%252C%2520while%2520humans%2520appear%2520good%2520at%250Aunderstanding%2520the%2520perturbed%2520text.%2520We%2520also%2520explore%2520two%2520methods%252C%2520including%250Aadversarial%2520training%2520and%2520associative%2520graph-based%2520recovery%252C%2520to%2520shield%2520systems%250Afrom%2520chain%2520association-based%2520attack.%2520Since%2520a%2520few%2520examples%2520that%2520use%2520some%250Aderogatory%2520terms%252C%2520this%2520paper%2520contains%2520materials%2520that%2520may%2520be%2520offensive%2520or%250Aupsetting%2520to%2520some%2520people.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain%20Association-based%20Attacking%20and%20Shielding%20Natural%20Language%0A%20%20Processing%20Systems&entry.906535625=Jiacheng%20Huang%20and%20Long%20Chen&entry.1292438233=%20%20Association%20as%20a%20gift%20enables%20people%20do%20not%20have%20to%20mention%20something%20in%0Acompletely%20straightforward%20words%20and%20allows%20others%20to%20understand%20what%20they%0Aintend%20to%20refer%20to.%20In%20this%20paper%2C%20we%20propose%20a%20chain%20association-based%0Aadversarial%20attack%20against%20natural%20language%20processing%20systems%2C%20utilizing%20the%0Acomprehension%20gap%20between%20humans%20and%20machines.%20We%20first%20generate%20a%20chain%0Aassociation%20graph%20for%20Chinese%20characters%20based%20on%20the%20association%20paradigm%20for%0Abuilding%20search%20space%20of%20potential%20adversarial%20examples.%20Then%2C%20we%20introduce%20an%0Adiscrete%20particle%20swarm%20optimization%20algorithm%20to%20search%20for%20the%20optimal%0Aadversarial%20examples.%20We%20conduct%20comprehensive%20experiments%20and%20show%20that%0Aadvanced%20natural%20language%20processing%20models%20and%20applications%2C%20including%20large%0Alanguage%20models%2C%20are%20vulnerable%20to%20our%20attack%2C%20while%20humans%20appear%20good%20at%0Aunderstanding%20the%20perturbed%20text.%20We%20also%20explore%20two%20methods%2C%20including%0Aadversarial%20training%20and%20associative%20graph-based%20recovery%2C%20to%20shield%20systems%0Afrom%20chain%20association-based%20attack.%20Since%20a%20few%20examples%20that%20use%20some%0Aderogatory%20terms%2C%20this%20paper%20contains%20materials%20that%20may%20be%20offensive%20or%0Aupsetting%20to%20some%20people.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07843v1&entry.124074799=Read"},
{"title": "Overview frequency principle/spectral bias in deep learning", "author": "Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo", "abstract": "  Understanding deep learning is increasingly emergent as it penetrates more\nand more into industry and science. In recent years, a research line from\nFourier analysis sheds lights on this magical \"black box\" by showing a\nFrequency Principle (F-Principle or spectral bias) of the training behavior of\ndeep neural networks (DNNs) -- DNNs often fit functions from low to high\nfrequency during the training. The F-Principle is first demonstrated by\nonedimensional synthetic data followed by the verification in high-dimensional\nreal datasets. A series of works subsequently enhance the validity of the\nF-Principle. This low-frequency implicit bias reveals the strength of neural\nnetwork in learning low-frequency functions as well as its deficiency in\nlearning high-frequency functions. Such understanding inspires the design of\nDNN-based algorithms in practical problems, explains experimental phenomena\nemerging in various scenarios, and further advances the study of deep learning\nfrom the frequency perspective. Although incomplete, we provide an overview of\nF-Principle and propose some open problems for future research.\n", "link": "http://arxiv.org/abs/2201.07395v4", "date": "2024-11-12", "relevancy": 2.2827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overview%20frequency%20principle/spectral%20bias%20in%20deep%20learning&body=Title%3A%20Overview%20frequency%20principle/spectral%20bias%20in%20deep%20learning%0AAuthor%3A%20Zhi-Qin%20John%20Xu%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo%0AAbstract%3A%20%20%20Understanding%20deep%20learning%20is%20increasingly%20emergent%20as%20it%20penetrates%20more%0Aand%20more%20into%20industry%20and%20science.%20In%20recent%20years%2C%20a%20research%20line%20from%0AFourier%20analysis%20sheds%20lights%20on%20this%20magical%20%22black%20box%22%20by%20showing%20a%0AFrequency%20Principle%20%28F-Principle%20or%20spectral%20bias%29%20of%20the%20training%20behavior%20of%0Adeep%20neural%20networks%20%28DNNs%29%20--%20DNNs%20often%20fit%20functions%20from%20low%20to%20high%0Afrequency%20during%20the%20training.%20The%20F-Principle%20is%20first%20demonstrated%20by%0Aonedimensional%20synthetic%20data%20followed%20by%20the%20verification%20in%20high-dimensional%0Areal%20datasets.%20A%20series%20of%20works%20subsequently%20enhance%20the%20validity%20of%20the%0AF-Principle.%20This%20low-frequency%20implicit%20bias%20reveals%20the%20strength%20of%20neural%0Anetwork%20in%20learning%20low-frequency%20functions%20as%20well%20as%20its%20deficiency%20in%0Alearning%20high-frequency%20functions.%20Such%20understanding%20inspires%20the%20design%20of%0ADNN-based%20algorithms%20in%20practical%20problems%2C%20explains%20experimental%20phenomena%0Aemerging%20in%20various%20scenarios%2C%20and%20further%20advances%20the%20study%20of%20deep%20learning%0Afrom%20the%20frequency%20perspective.%20Although%20incomplete%2C%20we%20provide%20an%20overview%20of%0AF-Principle%20and%20propose%20some%20open%20problems%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.07395v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverview%2520frequency%2520principle/spectral%2520bias%2520in%2520deep%2520learning%26entry.906535625%3DZhi-Qin%2520John%2520Xu%2520and%2520Yaoyu%2520Zhang%2520and%2520Tao%2520Luo%26entry.1292438233%3D%2520%2520Understanding%2520deep%2520learning%2520is%2520increasingly%2520emergent%2520as%2520it%2520penetrates%2520more%250Aand%2520more%2520into%2520industry%2520and%2520science.%2520In%2520recent%2520years%252C%2520a%2520research%2520line%2520from%250AFourier%2520analysis%2520sheds%2520lights%2520on%2520this%2520magical%2520%2522black%2520box%2522%2520by%2520showing%2520a%250AFrequency%2520Principle%2520%2528F-Principle%2520or%2520spectral%2520bias%2529%2520of%2520the%2520training%2520behavior%2520of%250Adeep%2520neural%2520networks%2520%2528DNNs%2529%2520--%2520DNNs%2520often%2520fit%2520functions%2520from%2520low%2520to%2520high%250Afrequency%2520during%2520the%2520training.%2520The%2520F-Principle%2520is%2520first%2520demonstrated%2520by%250Aonedimensional%2520synthetic%2520data%2520followed%2520by%2520the%2520verification%2520in%2520high-dimensional%250Areal%2520datasets.%2520A%2520series%2520of%2520works%2520subsequently%2520enhance%2520the%2520validity%2520of%2520the%250AF-Principle.%2520This%2520low-frequency%2520implicit%2520bias%2520reveals%2520the%2520strength%2520of%2520neural%250Anetwork%2520in%2520learning%2520low-frequency%2520functions%2520as%2520well%2520as%2520its%2520deficiency%2520in%250Alearning%2520high-frequency%2520functions.%2520Such%2520understanding%2520inspires%2520the%2520design%2520of%250ADNN-based%2520algorithms%2520in%2520practical%2520problems%252C%2520explains%2520experimental%2520phenomena%250Aemerging%2520in%2520various%2520scenarios%252C%2520and%2520further%2520advances%2520the%2520study%2520of%2520deep%2520learning%250Afrom%2520the%2520frequency%2520perspective.%2520Although%2520incomplete%252C%2520we%2520provide%2520an%2520overview%2520of%250AF-Principle%2520and%2520propose%2520some%2520open%2520problems%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.07395v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overview%20frequency%20principle/spectral%20bias%20in%20deep%20learning&entry.906535625=Zhi-Qin%20John%20Xu%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo&entry.1292438233=%20%20Understanding%20deep%20learning%20is%20increasingly%20emergent%20as%20it%20penetrates%20more%0Aand%20more%20into%20industry%20and%20science.%20In%20recent%20years%2C%20a%20research%20line%20from%0AFourier%20analysis%20sheds%20lights%20on%20this%20magical%20%22black%20box%22%20by%20showing%20a%0AFrequency%20Principle%20%28F-Principle%20or%20spectral%20bias%29%20of%20the%20training%20behavior%20of%0Adeep%20neural%20networks%20%28DNNs%29%20--%20DNNs%20often%20fit%20functions%20from%20low%20to%20high%0Afrequency%20during%20the%20training.%20The%20F-Principle%20is%20first%20demonstrated%20by%0Aonedimensional%20synthetic%20data%20followed%20by%20the%20verification%20in%20high-dimensional%0Areal%20datasets.%20A%20series%20of%20works%20subsequently%20enhance%20the%20validity%20of%20the%0AF-Principle.%20This%20low-frequency%20implicit%20bias%20reveals%20the%20strength%20of%20neural%0Anetwork%20in%20learning%20low-frequency%20functions%20as%20well%20as%20its%20deficiency%20in%0Alearning%20high-frequency%20functions.%20Such%20understanding%20inspires%20the%20design%20of%0ADNN-based%20algorithms%20in%20practical%20problems%2C%20explains%20experimental%20phenomena%0Aemerging%20in%20various%20scenarios%2C%20and%20further%20advances%20the%20study%20of%20deep%20learning%0Afrom%20the%20frequency%20perspective.%20Although%20incomplete%2C%20we%20provide%20an%20overview%20of%0AF-Principle%20and%20propose%20some%20open%20problems%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.07395v4&entry.124074799=Read"},
{"title": "Tukey g-and-h neural network regression for non-Gaussian data", "author": "Arthur P. Guillaumin and Natalia Efremova", "abstract": "  This paper addresses non-Gaussian regression with neural networks via the use\nof the Tukey g-and-h distribution.The Tukey g-and-h transform is a flexible\nparametric transform with two parameters $g$ and $h$ which, when applied to a\nstandard normal random variable, introduces both skewness and kurtosis,\nresulting in a distribution commonly called the Tukey g-and-h distribution.\nSpecific values of $g$ and $h$ produce good approximations to other families of\ndistributions, such as the Cauchy and student-t distributions. The flexibility\nof the Tukey g-and-h distribution has driven its popularity in the statistical\ncommunity, in applied sciences and finance. In this work we consider the\ntraining of a neural network to predict the parameters of a Tukey g-and-h\ndistribution in a regression framework via the minimization of the\ncorresponding negative log-likelihood, despite the latter having no closed-form\nexpression. We demonstrate the efficiency of our procedure in simulated\nexamples and apply our method to a real-world dataset of global crop yield for\nseveral types of crops. Finally, we show how we can carry out a goodness-of-fit\nanalysis between the predicted distributions and the test data. A Pytorch\nimplementation is made available on Github and as a Pypi package.\n", "link": "http://arxiv.org/abs/2411.07957v1", "date": "2024-11-12", "relevancy": 2.2798, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4795}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4632}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tukey%20g-and-h%20neural%20network%20regression%20for%20non-Gaussian%20data&body=Title%3A%20Tukey%20g-and-h%20neural%20network%20regression%20for%20non-Gaussian%20data%0AAuthor%3A%20Arthur%20P.%20Guillaumin%20and%20Natalia%20Efremova%0AAbstract%3A%20%20%20This%20paper%20addresses%20non-Gaussian%20regression%20with%20neural%20networks%20via%20the%20use%0Aof%20the%20Tukey%20g-and-h%20distribution.The%20Tukey%20g-and-h%20transform%20is%20a%20flexible%0Aparametric%20transform%20with%20two%20parameters%20%24g%24%20and%20%24h%24%20which%2C%20when%20applied%20to%20a%0Astandard%20normal%20random%20variable%2C%20introduces%20both%20skewness%20and%20kurtosis%2C%0Aresulting%20in%20a%20distribution%20commonly%20called%20the%20Tukey%20g-and-h%20distribution.%0ASpecific%20values%20of%20%24g%24%20and%20%24h%24%20produce%20good%20approximations%20to%20other%20families%20of%0Adistributions%2C%20such%20as%20the%20Cauchy%20and%20student-t%20distributions.%20The%20flexibility%0Aof%20the%20Tukey%20g-and-h%20distribution%20has%20driven%20its%20popularity%20in%20the%20statistical%0Acommunity%2C%20in%20applied%20sciences%20and%20finance.%20In%20this%20work%20we%20consider%20the%0Atraining%20of%20a%20neural%20network%20to%20predict%20the%20parameters%20of%20a%20Tukey%20g-and-h%0Adistribution%20in%20a%20regression%20framework%20via%20the%20minimization%20of%20the%0Acorresponding%20negative%20log-likelihood%2C%20despite%20the%20latter%20having%20no%20closed-form%0Aexpression.%20We%20demonstrate%20the%20efficiency%20of%20our%20procedure%20in%20simulated%0Aexamples%20and%20apply%20our%20method%20to%20a%20real-world%20dataset%20of%20global%20crop%20yield%20for%0Aseveral%20types%20of%20crops.%20Finally%2C%20we%20show%20how%20we%20can%20carry%20out%20a%20goodness-of-fit%0Aanalysis%20between%20the%20predicted%20distributions%20and%20the%20test%20data.%20A%20Pytorch%0Aimplementation%20is%20made%20available%20on%20Github%20and%20as%20a%20Pypi%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTukey%2520g-and-h%2520neural%2520network%2520regression%2520for%2520non-Gaussian%2520data%26entry.906535625%3DArthur%2520P.%2520Guillaumin%2520and%2520Natalia%2520Efremova%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520non-Gaussian%2520regression%2520with%2520neural%2520networks%2520via%2520the%2520use%250Aof%2520the%2520Tukey%2520g-and-h%2520distribution.The%2520Tukey%2520g-and-h%2520transform%2520is%2520a%2520flexible%250Aparametric%2520transform%2520with%2520two%2520parameters%2520%2524g%2524%2520and%2520%2524h%2524%2520which%252C%2520when%2520applied%2520to%2520a%250Astandard%2520normal%2520random%2520variable%252C%2520introduces%2520both%2520skewness%2520and%2520kurtosis%252C%250Aresulting%2520in%2520a%2520distribution%2520commonly%2520called%2520the%2520Tukey%2520g-and-h%2520distribution.%250ASpecific%2520values%2520of%2520%2524g%2524%2520and%2520%2524h%2524%2520produce%2520good%2520approximations%2520to%2520other%2520families%2520of%250Adistributions%252C%2520such%2520as%2520the%2520Cauchy%2520and%2520student-t%2520distributions.%2520The%2520flexibility%250Aof%2520the%2520Tukey%2520g-and-h%2520distribution%2520has%2520driven%2520its%2520popularity%2520in%2520the%2520statistical%250Acommunity%252C%2520in%2520applied%2520sciences%2520and%2520finance.%2520In%2520this%2520work%2520we%2520consider%2520the%250Atraining%2520of%2520a%2520neural%2520network%2520to%2520predict%2520the%2520parameters%2520of%2520a%2520Tukey%2520g-and-h%250Adistribution%2520in%2520a%2520regression%2520framework%2520via%2520the%2520minimization%2520of%2520the%250Acorresponding%2520negative%2520log-likelihood%252C%2520despite%2520the%2520latter%2520having%2520no%2520closed-form%250Aexpression.%2520We%2520demonstrate%2520the%2520efficiency%2520of%2520our%2520procedure%2520in%2520simulated%250Aexamples%2520and%2520apply%2520our%2520method%2520to%2520a%2520real-world%2520dataset%2520of%2520global%2520crop%2520yield%2520for%250Aseveral%2520types%2520of%2520crops.%2520Finally%252C%2520we%2520show%2520how%2520we%2520can%2520carry%2520out%2520a%2520goodness-of-fit%250Aanalysis%2520between%2520the%2520predicted%2520distributions%2520and%2520the%2520test%2520data.%2520A%2520Pytorch%250Aimplementation%2520is%2520made%2520available%2520on%2520Github%2520and%2520as%2520a%2520Pypi%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tukey%20g-and-h%20neural%20network%20regression%20for%20non-Gaussian%20data&entry.906535625=Arthur%20P.%20Guillaumin%20and%20Natalia%20Efremova&entry.1292438233=%20%20This%20paper%20addresses%20non-Gaussian%20regression%20with%20neural%20networks%20via%20the%20use%0Aof%20the%20Tukey%20g-and-h%20distribution.The%20Tukey%20g-and-h%20transform%20is%20a%20flexible%0Aparametric%20transform%20with%20two%20parameters%20%24g%24%20and%20%24h%24%20which%2C%20when%20applied%20to%20a%0Astandard%20normal%20random%20variable%2C%20introduces%20both%20skewness%20and%20kurtosis%2C%0Aresulting%20in%20a%20distribution%20commonly%20called%20the%20Tukey%20g-and-h%20distribution.%0ASpecific%20values%20of%20%24g%24%20and%20%24h%24%20produce%20good%20approximations%20to%20other%20families%20of%0Adistributions%2C%20such%20as%20the%20Cauchy%20and%20student-t%20distributions.%20The%20flexibility%0Aof%20the%20Tukey%20g-and-h%20distribution%20has%20driven%20its%20popularity%20in%20the%20statistical%0Acommunity%2C%20in%20applied%20sciences%20and%20finance.%20In%20this%20work%20we%20consider%20the%0Atraining%20of%20a%20neural%20network%20to%20predict%20the%20parameters%20of%20a%20Tukey%20g-and-h%0Adistribution%20in%20a%20regression%20framework%20via%20the%20minimization%20of%20the%0Acorresponding%20negative%20log-likelihood%2C%20despite%20the%20latter%20having%20no%20closed-form%0Aexpression.%20We%20demonstrate%20the%20efficiency%20of%20our%20procedure%20in%20simulated%0Aexamples%20and%20apply%20our%20method%20to%20a%20real-world%20dataset%20of%20global%20crop%20yield%20for%0Aseveral%20types%20of%20crops.%20Finally%2C%20we%20show%20how%20we%20can%20carry%20out%20a%20goodness-of-fit%0Aanalysis%20between%20the%20predicted%20distributions%20and%20the%20test%20data.%20A%20Pytorch%0Aimplementation%20is%20made%20available%20on%20Github%20and%20as%20a%20Pypi%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07957v1&entry.124074799=Read"},
{"title": "Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse\n  Tensor-based Transformer", "author": "Xiao Huo and Junhui Ho and Shuai Wan and Fuzheng Yang", "abstract": "  The evolution of 3D visualization techniques has fundamentally transformed\nhow we interact with digital content. At the forefront of this change is point\ncloud technology, offering an immersive experience that surpasses traditional\n2D representations. However, the massive data size of point clouds presents\nsignificant challenges in data compression. Current methods for lossy point\ncloud attribute compression (PCAC) generally focus on reconstructing the\noriginal point clouds with minimal error. However, for point cloud\nvisualization scenarios, the reconstructed point clouds with distortion still\nneed to undergo a complex rendering process, which affects the final\nuser-perceived quality. In this paper, we propose an end-to-end deep learning\nframework that seamlessly integrates PCAC with differentiable rendering,\ndenoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of\nrendered multiview images for viewing. In a differentiable manner, the impact\nof the rendering process on the reconstructed point clouds is taken into\naccount. Moreover, we characterize point clouds as sparse tensors and propose a\nsparse tensor-based transformer, called SP-Trans. By aligning with the local\ndensity of the point cloud and utilizing an enhanced local attention mechanism,\nSP-Trans captures the intricate relationships within the point cloud, further\nimproving feature analysis and synthesis within the framework. Extensive\nexperiments demonstrate that the proposed RO-PCAC achieves state-of-the-art\ncompression performance, compared to existing reconstruction-oriented methods,\nincluding traditional, learning-based, and hybrid methods.\n", "link": "http://arxiv.org/abs/2411.07899v1", "date": "2024-11-12", "relevancy": 2.2771, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5889}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer&body=Title%3A%20Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer%0AAuthor%3A%20Xiao%20Huo%20and%20Junhui%20Ho%20and%20Shuai%20Wan%20and%20Fuzheng%20Yang%0AAbstract%3A%20%20%20The%20evolution%20of%203D%20visualization%20techniques%20has%20fundamentally%20transformed%0Ahow%20we%20interact%20with%20digital%20content.%20At%20the%20forefront%20of%20this%20change%20is%20point%0Acloud%20technology%2C%20offering%20an%20immersive%20experience%20that%20surpasses%20traditional%0A2D%20representations.%20However%2C%20the%20massive%20data%20size%20of%20point%20clouds%20presents%0Asignificant%20challenges%20in%20data%20compression.%20Current%20methods%20for%20lossy%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20generally%20focus%20on%20reconstructing%20the%0Aoriginal%20point%20clouds%20with%20minimal%20error.%20However%2C%20for%20point%20cloud%0Avisualization%20scenarios%2C%20the%20reconstructed%20point%20clouds%20with%20distortion%20still%0Aneed%20to%20undergo%20a%20complex%20rendering%20process%2C%20which%20affects%20the%20final%0Auser-perceived%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20deep%20learning%0Aframework%20that%20seamlessly%20integrates%20PCAC%20with%20differentiable%20rendering%2C%0Adenoted%20as%20rendering-oriented%20PCAC%20%28RO-PCAC%29%2C%20directly%20targeting%20the%20quality%20of%0Arendered%20multiview%20images%20for%20viewing.%20In%20a%20differentiable%20manner%2C%20the%20impact%0Aof%20the%20rendering%20process%20on%20the%20reconstructed%20point%20clouds%20is%20taken%20into%0Aaccount.%20Moreover%2C%20we%20characterize%20point%20clouds%20as%20sparse%20tensors%20and%20propose%20a%0Asparse%20tensor-based%20transformer%2C%20called%20SP-Trans.%20By%20aligning%20with%20the%20local%0Adensity%20of%20the%20point%20cloud%20and%20utilizing%20an%20enhanced%20local%20attention%20mechanism%2C%0ASP-Trans%20captures%20the%20intricate%20relationships%20within%20the%20point%20cloud%2C%20further%0Aimproving%20feature%20analysis%20and%20synthesis%20within%20the%20framework.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20RO-PCAC%20achieves%20state-of-the-art%0Acompression%20performance%2C%20compared%20to%20existing%20reconstruction-oriented%20methods%2C%0Aincluding%20traditional%2C%20learning-based%2C%20and%20hybrid%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRendering-Oriented%25203D%2520Point%2520Cloud%2520Attribute%2520Compression%2520using%2520Sparse%250A%2520%2520Tensor-based%2520Transformer%26entry.906535625%3DXiao%2520Huo%2520and%2520Junhui%2520Ho%2520and%2520Shuai%2520Wan%2520and%2520Fuzheng%2520Yang%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%25203D%2520visualization%2520techniques%2520has%2520fundamentally%2520transformed%250Ahow%2520we%2520interact%2520with%2520digital%2520content.%2520At%2520the%2520forefront%2520of%2520this%2520change%2520is%2520point%250Acloud%2520technology%252C%2520offering%2520an%2520immersive%2520experience%2520that%2520surpasses%2520traditional%250A2D%2520representations.%2520However%252C%2520the%2520massive%2520data%2520size%2520of%2520point%2520clouds%2520presents%250Asignificant%2520challenges%2520in%2520data%2520compression.%2520Current%2520methods%2520for%2520lossy%2520point%250Acloud%2520attribute%2520compression%2520%2528PCAC%2529%2520generally%2520focus%2520on%2520reconstructing%2520the%250Aoriginal%2520point%2520clouds%2520with%2520minimal%2520error.%2520However%252C%2520for%2520point%2520cloud%250Avisualization%2520scenarios%252C%2520the%2520reconstructed%2520point%2520clouds%2520with%2520distortion%2520still%250Aneed%2520to%2520undergo%2520a%2520complex%2520rendering%2520process%252C%2520which%2520affects%2520the%2520final%250Auser-perceived%2520quality.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%2520deep%2520learning%250Aframework%2520that%2520seamlessly%2520integrates%2520PCAC%2520with%2520differentiable%2520rendering%252C%250Adenoted%2520as%2520rendering-oriented%2520PCAC%2520%2528RO-PCAC%2529%252C%2520directly%2520targeting%2520the%2520quality%2520of%250Arendered%2520multiview%2520images%2520for%2520viewing.%2520In%2520a%2520differentiable%2520manner%252C%2520the%2520impact%250Aof%2520the%2520rendering%2520process%2520on%2520the%2520reconstructed%2520point%2520clouds%2520is%2520taken%2520into%250Aaccount.%2520Moreover%252C%2520we%2520characterize%2520point%2520clouds%2520as%2520sparse%2520tensors%2520and%2520propose%2520a%250Asparse%2520tensor-based%2520transformer%252C%2520called%2520SP-Trans.%2520By%2520aligning%2520with%2520the%2520local%250Adensity%2520of%2520the%2520point%2520cloud%2520and%2520utilizing%2520an%2520enhanced%2520local%2520attention%2520mechanism%252C%250ASP-Trans%2520captures%2520the%2520intricate%2520relationships%2520within%2520the%2520point%2520cloud%252C%2520further%250Aimproving%2520feature%2520analysis%2520and%2520synthesis%2520within%2520the%2520framework.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520RO-PCAC%2520achieves%2520state-of-the-art%250Acompression%2520performance%252C%2520compared%2520to%2520existing%2520reconstruction-oriented%2520methods%252C%250Aincluding%2520traditional%252C%2520learning-based%252C%2520and%2520hybrid%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rendering-Oriented%203D%20Point%20Cloud%20Attribute%20Compression%20using%20Sparse%0A%20%20Tensor-based%20Transformer&entry.906535625=Xiao%20Huo%20and%20Junhui%20Ho%20and%20Shuai%20Wan%20and%20Fuzheng%20Yang&entry.1292438233=%20%20The%20evolution%20of%203D%20visualization%20techniques%20has%20fundamentally%20transformed%0Ahow%20we%20interact%20with%20digital%20content.%20At%20the%20forefront%20of%20this%20change%20is%20point%0Acloud%20technology%2C%20offering%20an%20immersive%20experience%20that%20surpasses%20traditional%0A2D%20representations.%20However%2C%20the%20massive%20data%20size%20of%20point%20clouds%20presents%0Asignificant%20challenges%20in%20data%20compression.%20Current%20methods%20for%20lossy%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20generally%20focus%20on%20reconstructing%20the%0Aoriginal%20point%20clouds%20with%20minimal%20error.%20However%2C%20for%20point%20cloud%0Avisualization%20scenarios%2C%20the%20reconstructed%20point%20clouds%20with%20distortion%20still%0Aneed%20to%20undergo%20a%20complex%20rendering%20process%2C%20which%20affects%20the%20final%0Auser-perceived%20quality.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20deep%20learning%0Aframework%20that%20seamlessly%20integrates%20PCAC%20with%20differentiable%20rendering%2C%0Adenoted%20as%20rendering-oriented%20PCAC%20%28RO-PCAC%29%2C%20directly%20targeting%20the%20quality%20of%0Arendered%20multiview%20images%20for%20viewing.%20In%20a%20differentiable%20manner%2C%20the%20impact%0Aof%20the%20rendering%20process%20on%20the%20reconstructed%20point%20clouds%20is%20taken%20into%0Aaccount.%20Moreover%2C%20we%20characterize%20point%20clouds%20as%20sparse%20tensors%20and%20propose%20a%0Asparse%20tensor-based%20transformer%2C%20called%20SP-Trans.%20By%20aligning%20with%20the%20local%0Adensity%20of%20the%20point%20cloud%20and%20utilizing%20an%20enhanced%20local%20attention%20mechanism%2C%0ASP-Trans%20captures%20the%20intricate%20relationships%20within%20the%20point%20cloud%2C%20further%0Aimproving%20feature%20analysis%20and%20synthesis%20within%20the%20framework.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20RO-PCAC%20achieves%20state-of-the-art%0Acompression%20performance%2C%20compared%20to%20existing%20reconstruction-oriented%20methods%2C%0Aincluding%20traditional%2C%20learning-based%2C%20and%20hybrid%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07899v1&entry.124074799=Read"},
{"title": "Spatially Regularized Graph Attention Autoencoder Framework for\n  Detecting Rainfall Extremes", "author": "Mihir Agarwal and Progyan Das and Udit Bhatia", "abstract": "  We introduce a novel Graph Attention Autoencoder (GAE) with spatial\nregularization to address the challenge of scalable anomaly detection in\nspatiotemporal rainfall data across India from 1990 to 2015. Our model\nleverages a Graph Attention Network (GAT) to capture spatial dependencies and\ntemporal dynamics in the data, further enhanced by a spatial regularization\nterm ensuring geographic coherence. We construct two graph datasets employing\nrainfall, pressure, and temperature attributes from the Indian Meteorological\nDepartment and ERA5 Reanalysis on Single Levels, respectively. Our network\noperates on graph representations of the data, where nodes represent geographic\nlocations, and edges, inferred through event synchronization, denote\nsignificant co-occurrences of rainfall events. Through extensive experiments,\nwe demonstrate that our GAE effectively identifies anomalous rainfall patterns\nacross the Indian landscape. Our work paves the way for sophisticated\nspatiotemporal anomaly detection methodologies in climate science, contributing\nto better climate change preparedness and response strategies.\n", "link": "http://arxiv.org/abs/2411.07753v1", "date": "2024-11-12", "relevancy": 2.2768, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4631}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4517}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially%20Regularized%20Graph%20Attention%20Autoencoder%20Framework%20for%0A%20%20Detecting%20Rainfall%20Extremes&body=Title%3A%20Spatially%20Regularized%20Graph%20Attention%20Autoencoder%20Framework%20for%0A%20%20Detecting%20Rainfall%20Extremes%0AAuthor%3A%20Mihir%20Agarwal%20and%20Progyan%20Das%20and%20Udit%20Bhatia%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20Graph%20Attention%20Autoencoder%20%28GAE%29%20with%20spatial%0Aregularization%20to%20address%20the%20challenge%20of%20scalable%20anomaly%20detection%20in%0Aspatiotemporal%20rainfall%20data%20across%20India%20from%201990%20to%202015.%20Our%20model%0Aleverages%20a%20Graph%20Attention%20Network%20%28GAT%29%20to%20capture%20spatial%20dependencies%20and%0Atemporal%20dynamics%20in%20the%20data%2C%20further%20enhanced%20by%20a%20spatial%20regularization%0Aterm%20ensuring%20geographic%20coherence.%20We%20construct%20two%20graph%20datasets%20employing%0Arainfall%2C%20pressure%2C%20and%20temperature%20attributes%20from%20the%20Indian%20Meteorological%0ADepartment%20and%20ERA5%20Reanalysis%20on%20Single%20Levels%2C%20respectively.%20Our%20network%0Aoperates%20on%20graph%20representations%20of%20the%20data%2C%20where%20nodes%20represent%20geographic%0Alocations%2C%20and%20edges%2C%20inferred%20through%20event%20synchronization%2C%20denote%0Asignificant%20co-occurrences%20of%20rainfall%20events.%20Through%20extensive%20experiments%2C%0Awe%20demonstrate%20that%20our%20GAE%20effectively%20identifies%20anomalous%20rainfall%20patterns%0Aacross%20the%20Indian%20landscape.%20Our%20work%20paves%20the%20way%20for%20sophisticated%0Aspatiotemporal%20anomaly%20detection%20methodologies%20in%20climate%20science%2C%20contributing%0Ato%20better%20climate%20change%20preparedness%20and%20response%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially%2520Regularized%2520Graph%2520Attention%2520Autoencoder%2520Framework%2520for%250A%2520%2520Detecting%2520Rainfall%2520Extremes%26entry.906535625%3DMihir%2520Agarwal%2520and%2520Progyan%2520Das%2520and%2520Udit%2520Bhatia%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520Graph%2520Attention%2520Autoencoder%2520%2528GAE%2529%2520with%2520spatial%250Aregularization%2520to%2520address%2520the%2520challenge%2520of%2520scalable%2520anomaly%2520detection%2520in%250Aspatiotemporal%2520rainfall%2520data%2520across%2520India%2520from%25201990%2520to%25202015.%2520Our%2520model%250Aleverages%2520a%2520Graph%2520Attention%2520Network%2520%2528GAT%2529%2520to%2520capture%2520spatial%2520dependencies%2520and%250Atemporal%2520dynamics%2520in%2520the%2520data%252C%2520further%2520enhanced%2520by%2520a%2520spatial%2520regularization%250Aterm%2520ensuring%2520geographic%2520coherence.%2520We%2520construct%2520two%2520graph%2520datasets%2520employing%250Arainfall%252C%2520pressure%252C%2520and%2520temperature%2520attributes%2520from%2520the%2520Indian%2520Meteorological%250ADepartment%2520and%2520ERA5%2520Reanalysis%2520on%2520Single%2520Levels%252C%2520respectively.%2520Our%2520network%250Aoperates%2520on%2520graph%2520representations%2520of%2520the%2520data%252C%2520where%2520nodes%2520represent%2520geographic%250Alocations%252C%2520and%2520edges%252C%2520inferred%2520through%2520event%2520synchronization%252C%2520denote%250Asignificant%2520co-occurrences%2520of%2520rainfall%2520events.%2520Through%2520extensive%2520experiments%252C%250Awe%2520demonstrate%2520that%2520our%2520GAE%2520effectively%2520identifies%2520anomalous%2520rainfall%2520patterns%250Aacross%2520the%2520Indian%2520landscape.%2520Our%2520work%2520paves%2520the%2520way%2520for%2520sophisticated%250Aspatiotemporal%2520anomaly%2520detection%2520methodologies%2520in%2520climate%2520science%252C%2520contributing%250Ato%2520better%2520climate%2520change%2520preparedness%2520and%2520response%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially%20Regularized%20Graph%20Attention%20Autoencoder%20Framework%20for%0A%20%20Detecting%20Rainfall%20Extremes&entry.906535625=Mihir%20Agarwal%20and%20Progyan%20Das%20and%20Udit%20Bhatia&entry.1292438233=%20%20We%20introduce%20a%20novel%20Graph%20Attention%20Autoencoder%20%28GAE%29%20with%20spatial%0Aregularization%20to%20address%20the%20challenge%20of%20scalable%20anomaly%20detection%20in%0Aspatiotemporal%20rainfall%20data%20across%20India%20from%201990%20to%202015.%20Our%20model%0Aleverages%20a%20Graph%20Attention%20Network%20%28GAT%29%20to%20capture%20spatial%20dependencies%20and%0Atemporal%20dynamics%20in%20the%20data%2C%20further%20enhanced%20by%20a%20spatial%20regularization%0Aterm%20ensuring%20geographic%20coherence.%20We%20construct%20two%20graph%20datasets%20employing%0Arainfall%2C%20pressure%2C%20and%20temperature%20attributes%20from%20the%20Indian%20Meteorological%0ADepartment%20and%20ERA5%20Reanalysis%20on%20Single%20Levels%2C%20respectively.%20Our%20network%0Aoperates%20on%20graph%20representations%20of%20the%20data%2C%20where%20nodes%20represent%20geographic%0Alocations%2C%20and%20edges%2C%20inferred%20through%20event%20synchronization%2C%20denote%0Asignificant%20co-occurrences%20of%20rainfall%20events.%20Through%20extensive%20experiments%2C%0Awe%20demonstrate%20that%20our%20GAE%20effectively%20identifies%20anomalous%20rainfall%20patterns%0Aacross%20the%20Indian%20landscape.%20Our%20work%20paves%20the%20way%20for%20sophisticated%0Aspatiotemporal%20anomaly%20detection%20methodologies%20in%20climate%20science%2C%20contributing%0Ato%20better%20climate%20change%20preparedness%20and%20response%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07753v1&entry.124074799=Read"},
{"title": "LLMs Can Evolve Continually on Modality for X-Modal Reasoning", "author": "Jiazuo Yu and Haomiao Xiong and Lu Zhang and Haiwen Diao and Yunzhi Zhuge and Lanqing Hong and Dong Wang and Huchuan Lu and You He and Long Chen", "abstract": "  Multimodal Large Language Models (MLLMs) have gained significant attention\ndue to their impressive capabilities in multimodal understanding. However,\nexisting methods rely heavily on extensive modal-specific pretraining and\njoint-modal tuning, leading to significant computational burdens when expanding\nto new modalities. In this paper, we propose PathWeave, a flexible and scalable\nframework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs\nto continually EVolve on modalities for $\\mathbb{X}$-modal reasoning. We\nleverage the concept of Continual Learning and develop an incremental training\nstrategy atop pre-trained MLLMs, enabling their expansion to new modalities\nusing uni-modal data, without executing joint-modal pretraining. In detail, a\nnovel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and\ncross-modal adapters are seamlessly integrated to facilitate efficient modality\nalignment and collaboration. Additionally, an MoE-based gating module is\napplied between two types of adapters to further enhance the multimodal\ninteraction. To investigate the proposed method, we establish a challenging\nbenchmark called Continual Learning of Modality (MCL), which consists of\nhigh-quality QA data from five distinct modalities: image, video, audio, depth\nand point cloud. Extensive experiments demonstrate the effectiveness of the\nproposed AnA framework on learning plasticity and memory stability during\ncontinual learning. Furthermore, PathWeave performs comparably to\nstate-of-the-art MLLMs while concurrently reducing parameter training burdens\nby 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave\n", "link": "http://arxiv.org/abs/2410.20178v2", "date": "2024-11-12", "relevancy": 2.2729, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Can%20Evolve%20Continually%20on%20Modality%20for%20X-Modal%20Reasoning&body=Title%3A%20LLMs%20Can%20Evolve%20Continually%20on%20Modality%20for%20X-Modal%20Reasoning%0AAuthor%3A%20Jiazuo%20Yu%20and%20Haomiao%20Xiong%20and%20Lu%20Zhang%20and%20Haiwen%20Diao%20and%20Yunzhi%20Zhuge%20and%20Lanqing%20Hong%20and%20Dong%20Wang%20and%20Huchuan%20Lu%20and%20You%20He%20and%20Long%20Chen%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20gained%20significant%20attention%0Adue%20to%20their%20impressive%20capabilities%20in%20multimodal%20understanding.%20However%2C%0Aexisting%20methods%20rely%20heavily%20on%20extensive%20modal-specific%20pretraining%20and%0Ajoint-modal%20tuning%2C%20leading%20to%20significant%20computational%20burdens%20when%20expanding%0Ato%20new%20modalities.%20In%20this%20paper%2C%20we%20propose%20PathWeave%2C%20a%20flexible%20and%20scalable%0Aframework%20with%20modal-Path%20sWitching%20and%20ExpAnsion%20abilities%20that%20enables%20MLLMs%0Ato%20continually%20EVolve%20on%20modalities%20for%20%24%5Cmathbb%7BX%7D%24-modal%20reasoning.%20We%0Aleverage%20the%20concept%20of%20Continual%20Learning%20and%20develop%20an%20incremental%20training%0Astrategy%20atop%20pre-trained%20MLLMs%2C%20enabling%20their%20expansion%20to%20new%20modalities%0Ausing%20uni-modal%20data%2C%20without%20executing%20joint-modal%20pretraining.%20In%20detail%2C%20a%0Anovel%20Adapter-in-Adapter%20%28AnA%29%20framework%20is%20introduced%2C%20in%20which%20uni-modal%20and%0Across-modal%20adapters%20are%20seamlessly%20integrated%20to%20facilitate%20efficient%20modality%0Aalignment%20and%20collaboration.%20Additionally%2C%20an%20MoE-based%20gating%20module%20is%0Aapplied%20between%20two%20types%20of%20adapters%20to%20further%20enhance%20the%20multimodal%0Ainteraction.%20To%20investigate%20the%20proposed%20method%2C%20we%20establish%20a%20challenging%0Abenchmark%20called%20Continual%20Learning%20of%20Modality%20%28MCL%29%2C%20which%20consists%20of%0Ahigh-quality%20QA%20data%20from%20five%20distinct%20modalities%3A%20image%2C%20video%2C%20audio%2C%20depth%0Aand%20point%20cloud.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20AnA%20framework%20on%20learning%20plasticity%20and%20memory%20stability%20during%0Acontinual%20learning.%20Furthermore%2C%20PathWeave%20performs%20comparably%20to%0Astate-of-the-art%20MLLMs%20while%20concurrently%20reducing%20parameter%20training%20burdens%0Aby%2098.73%25.%20Our%20code%20locates%20at%20https%3A//github.com/JiazuoYu/PathWeave%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Can%2520Evolve%2520Continually%2520on%2520Modality%2520for%2520X-Modal%2520Reasoning%26entry.906535625%3DJiazuo%2520Yu%2520and%2520Haomiao%2520Xiong%2520and%2520Lu%2520Zhang%2520and%2520Haiwen%2520Diao%2520and%2520Yunzhi%2520Zhuge%2520and%2520Lanqing%2520Hong%2520and%2520Dong%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520You%2520He%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520gained%2520significant%2520attention%250Adue%2520to%2520their%2520impressive%2520capabilities%2520in%2520multimodal%2520understanding.%2520However%252C%250Aexisting%2520methods%2520rely%2520heavily%2520on%2520extensive%2520modal-specific%2520pretraining%2520and%250Ajoint-modal%2520tuning%252C%2520leading%2520to%2520significant%2520computational%2520burdens%2520when%2520expanding%250Ato%2520new%2520modalities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PathWeave%252C%2520a%2520flexible%2520and%2520scalable%250Aframework%2520with%2520modal-Path%2520sWitching%2520and%2520ExpAnsion%2520abilities%2520that%2520enables%2520MLLMs%250Ato%2520continually%2520EVolve%2520on%2520modalities%2520for%2520%2524%255Cmathbb%257BX%257D%2524-modal%2520reasoning.%2520We%250Aleverage%2520the%2520concept%2520of%2520Continual%2520Learning%2520and%2520develop%2520an%2520incremental%2520training%250Astrategy%2520atop%2520pre-trained%2520MLLMs%252C%2520enabling%2520their%2520expansion%2520to%2520new%2520modalities%250Ausing%2520uni-modal%2520data%252C%2520without%2520executing%2520joint-modal%2520pretraining.%2520In%2520detail%252C%2520a%250Anovel%2520Adapter-in-Adapter%2520%2528AnA%2529%2520framework%2520is%2520introduced%252C%2520in%2520which%2520uni-modal%2520and%250Across-modal%2520adapters%2520are%2520seamlessly%2520integrated%2520to%2520facilitate%2520efficient%2520modality%250Aalignment%2520and%2520collaboration.%2520Additionally%252C%2520an%2520MoE-based%2520gating%2520module%2520is%250Aapplied%2520between%2520two%2520types%2520of%2520adapters%2520to%2520further%2520enhance%2520the%2520multimodal%250Ainteraction.%2520To%2520investigate%2520the%2520proposed%2520method%252C%2520we%2520establish%2520a%2520challenging%250Abenchmark%2520called%2520Continual%2520Learning%2520of%2520Modality%2520%2528MCL%2529%252C%2520which%2520consists%2520of%250Ahigh-quality%2520QA%2520data%2520from%2520five%2520distinct%2520modalities%253A%2520image%252C%2520video%252C%2520audio%252C%2520depth%250Aand%2520point%2520cloud.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520AnA%2520framework%2520on%2520learning%2520plasticity%2520and%2520memory%2520stability%2520during%250Acontinual%2520learning.%2520Furthermore%252C%2520PathWeave%2520performs%2520comparably%2520to%250Astate-of-the-art%2520MLLMs%2520while%2520concurrently%2520reducing%2520parameter%2520training%2520burdens%250Aby%252098.73%2525.%2520Our%2520code%2520locates%2520at%2520https%253A//github.com/JiazuoYu/PathWeave%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Can%20Evolve%20Continually%20on%20Modality%20for%20X-Modal%20Reasoning&entry.906535625=Jiazuo%20Yu%20and%20Haomiao%20Xiong%20and%20Lu%20Zhang%20and%20Haiwen%20Diao%20and%20Yunzhi%20Zhuge%20and%20Lanqing%20Hong%20and%20Dong%20Wang%20and%20Huchuan%20Lu%20and%20You%20He%20and%20Long%20Chen&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20gained%20significant%20attention%0Adue%20to%20their%20impressive%20capabilities%20in%20multimodal%20understanding.%20However%2C%0Aexisting%20methods%20rely%20heavily%20on%20extensive%20modal-specific%20pretraining%20and%0Ajoint-modal%20tuning%2C%20leading%20to%20significant%20computational%20burdens%20when%20expanding%0Ato%20new%20modalities.%20In%20this%20paper%2C%20we%20propose%20PathWeave%2C%20a%20flexible%20and%20scalable%0Aframework%20with%20modal-Path%20sWitching%20and%20ExpAnsion%20abilities%20that%20enables%20MLLMs%0Ato%20continually%20EVolve%20on%20modalities%20for%20%24%5Cmathbb%7BX%7D%24-modal%20reasoning.%20We%0Aleverage%20the%20concept%20of%20Continual%20Learning%20and%20develop%20an%20incremental%20training%0Astrategy%20atop%20pre-trained%20MLLMs%2C%20enabling%20their%20expansion%20to%20new%20modalities%0Ausing%20uni-modal%20data%2C%20without%20executing%20joint-modal%20pretraining.%20In%20detail%2C%20a%0Anovel%20Adapter-in-Adapter%20%28AnA%29%20framework%20is%20introduced%2C%20in%20which%20uni-modal%20and%0Across-modal%20adapters%20are%20seamlessly%20integrated%20to%20facilitate%20efficient%20modality%0Aalignment%20and%20collaboration.%20Additionally%2C%20an%20MoE-based%20gating%20module%20is%0Aapplied%20between%20two%20types%20of%20adapters%20to%20further%20enhance%20the%20multimodal%0Ainteraction.%20To%20investigate%20the%20proposed%20method%2C%20we%20establish%20a%20challenging%0Abenchmark%20called%20Continual%20Learning%20of%20Modality%20%28MCL%29%2C%20which%20consists%20of%0Ahigh-quality%20QA%20data%20from%20five%20distinct%20modalities%3A%20image%2C%20video%2C%20audio%2C%20depth%0Aand%20point%20cloud.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20AnA%20framework%20on%20learning%20plasticity%20and%20memory%20stability%20during%0Acontinual%20learning.%20Furthermore%2C%20PathWeave%20performs%20comparably%20to%0Astate-of-the-art%20MLLMs%20while%20concurrently%20reducing%20parameter%20training%20burdens%0Aby%2098.73%25.%20Our%20code%20locates%20at%20https%3A//github.com/JiazuoYu/PathWeave%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20178v2&entry.124074799=Read"},
{"title": "Provable Compositional Generalization for Object-Centric Learning", "author": "Thadd\u00e4us Wiedemer and Jack Brady and Alexander Panfilov and Attila Juhos and Matthias Bethge and Wieland Brendel", "abstract": "  Learning representations that generalize to novel compositions of known\nconcepts is crucial for bridging the gap between human and machine perception.\nOne prominent effort is learning object-centric representations, which are\nwidely conjectured to enable compositional generalization. Yet, it remains\nunclear when this conjecture will be true, as a principled theoretical or\nempirical understanding of compositional generalization is lacking. In this\nwork, we investigate when compositional generalization is guaranteed for\nobject-centric representations through the lens of identifiability theory. We\nshow that autoencoders that satisfy structural assumptions on the decoder and\nenforce encoder-decoder consistency will learn object-centric representations\nthat provably generalize compositionally. We validate our theoretical result\nand highlight the practical relevance of our assumptions through experiments on\nsynthetic image data.\n", "link": "http://arxiv.org/abs/2310.05327v2", "date": "2024-11-12", "relevancy": 2.2516, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6002}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Compositional%20Generalization%20for%20Object-Centric%20Learning&body=Title%3A%20Provable%20Compositional%20Generalization%20for%20Object-Centric%20Learning%0AAuthor%3A%20Thadd%C3%A4us%20Wiedemer%20and%20Jack%20Brady%20and%20Alexander%20Panfilov%20and%20Attila%20Juhos%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20Learning%20representations%20that%20generalize%20to%20novel%20compositions%20of%20known%0Aconcepts%20is%20crucial%20for%20bridging%20the%20gap%20between%20human%20and%20machine%20perception.%0AOne%20prominent%20effort%20is%20learning%20object-centric%20representations%2C%20which%20are%0Awidely%20conjectured%20to%20enable%20compositional%20generalization.%20Yet%2C%20it%20remains%0Aunclear%20when%20this%20conjecture%20will%20be%20true%2C%20as%20a%20principled%20theoretical%20or%0Aempirical%20understanding%20of%20compositional%20generalization%20is%20lacking.%20In%20this%0Awork%2C%20we%20investigate%20when%20compositional%20generalization%20is%20guaranteed%20for%0Aobject-centric%20representations%20through%20the%20lens%20of%20identifiability%20theory.%20We%0Ashow%20that%20autoencoders%20that%20satisfy%20structural%20assumptions%20on%20the%20decoder%20and%0Aenforce%20encoder-decoder%20consistency%20will%20learn%20object-centric%20representations%0Athat%20provably%20generalize%20compositionally.%20We%20validate%20our%20theoretical%20result%0Aand%20highlight%20the%20practical%20relevance%20of%20our%20assumptions%20through%20experiments%20on%0Asynthetic%20image%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Compositional%2520Generalization%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DThadd%25C3%25A4us%2520Wiedemer%2520and%2520Jack%2520Brady%2520and%2520Alexander%2520Panfilov%2520and%2520Attila%2520Juhos%2520and%2520Matthias%2520Bethge%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520Learning%2520representations%2520that%2520generalize%2520to%2520novel%2520compositions%2520of%2520known%250Aconcepts%2520is%2520crucial%2520for%2520bridging%2520the%2520gap%2520between%2520human%2520and%2520machine%2520perception.%250AOne%2520prominent%2520effort%2520is%2520learning%2520object-centric%2520representations%252C%2520which%2520are%250Awidely%2520conjectured%2520to%2520enable%2520compositional%2520generalization.%2520Yet%252C%2520it%2520remains%250Aunclear%2520when%2520this%2520conjecture%2520will%2520be%2520true%252C%2520as%2520a%2520principled%2520theoretical%2520or%250Aempirical%2520understanding%2520of%2520compositional%2520generalization%2520is%2520lacking.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520when%2520compositional%2520generalization%2520is%2520guaranteed%2520for%250Aobject-centric%2520representations%2520through%2520the%2520lens%2520of%2520identifiability%2520theory.%2520We%250Ashow%2520that%2520autoencoders%2520that%2520satisfy%2520structural%2520assumptions%2520on%2520the%2520decoder%2520and%250Aenforce%2520encoder-decoder%2520consistency%2520will%2520learn%2520object-centric%2520representations%250Athat%2520provably%2520generalize%2520compositionally.%2520We%2520validate%2520our%2520theoretical%2520result%250Aand%2520highlight%2520the%2520practical%2520relevance%2520of%2520our%2520assumptions%2520through%2520experiments%2520on%250Asynthetic%2520image%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Compositional%20Generalization%20for%20Object-Centric%20Learning&entry.906535625=Thadd%C3%A4us%20Wiedemer%20and%20Jack%20Brady%20and%20Alexander%20Panfilov%20and%20Attila%20Juhos%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel&entry.1292438233=%20%20Learning%20representations%20that%20generalize%20to%20novel%20compositions%20of%20known%0Aconcepts%20is%20crucial%20for%20bridging%20the%20gap%20between%20human%20and%20machine%20perception.%0AOne%20prominent%20effort%20is%20learning%20object-centric%20representations%2C%20which%20are%0Awidely%20conjectured%20to%20enable%20compositional%20generalization.%20Yet%2C%20it%20remains%0Aunclear%20when%20this%20conjecture%20will%20be%20true%2C%20as%20a%20principled%20theoretical%20or%0Aempirical%20understanding%20of%20compositional%20generalization%20is%20lacking.%20In%20this%0Awork%2C%20we%20investigate%20when%20compositional%20generalization%20is%20guaranteed%20for%0Aobject-centric%20representations%20through%20the%20lens%20of%20identifiability%20theory.%20We%0Ashow%20that%20autoencoders%20that%20satisfy%20structural%20assumptions%20on%20the%20decoder%20and%0Aenforce%20encoder-decoder%20consistency%20will%20learn%20object-centric%20representations%0Athat%20provably%20generalize%20compositionally.%20We%20validate%20our%20theoretical%20result%0Aand%20highlight%20the%20practical%20relevance%20of%20our%20assumptions%20through%20experiments%20on%0Asynthetic%20image%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05327v2&entry.124074799=Read"},
{"title": "Exploiting Activation Sparsity with Dense to Dynamic-k\n  Mixture-of-Experts Conversion", "author": "Filip Szatkowski and Bartosz W\u00f3jcik and Miko\u0142aj Pi\u00f3rczy\u0144ski and Simone Scardapane", "abstract": "  Transformer models can face practical limitations due to their high\ncomputational requirements. At the same time, such models exhibit significant\nactivation sparsity, which can be leveraged to reduce the inference cost by\nconverting parts of the network into equivalent Mixture-of-Experts (MoE)\nlayers. Despite the crucial role played by activation sparsity, its impact on\nthis process remains unexplored. We demonstrate that the efficiency of the\nconversion can be significantly enhanced by a proper regularization of the\nactivation sparsity of the base model. Moreover, motivated by the high variance\nof the number of activated neurons for different inputs, we introduce a more\neffective dynamic-$k$ expert selection rule that adjusts the number of executed\nexperts on a per-token basis. To achieve further savings, we extend this\napproach to multi-head attention projections. Finally, we develop an efficient\nimplementation that translates these computational savings into actual\nwall-clock speedup. The proposed method, Dense to Dynamic-$k$\nMixture-of-Experts (D2DMoE), outperforms existing approaches on common NLP and\nvision tasks, reducing inference cost by up to 60% without significantly\nimpacting performance.\n", "link": "http://arxiv.org/abs/2310.04361v4", "date": "2024-11-12", "relevancy": 2.2412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5671}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5633}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion&body=Title%3A%20Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion%0AAuthor%3A%20Filip%20Szatkowski%20and%20Bartosz%20W%C3%B3jcik%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20Transformer%20models%20can%20face%20practical%20limitations%20due%20to%20their%20high%0Acomputational%20requirements.%20At%20the%20same%20time%2C%20such%20models%20exhibit%20significant%0Aactivation%20sparsity%2C%20which%20can%20be%20leveraged%20to%20reduce%20the%20inference%20cost%20by%0Aconverting%20parts%20of%20the%20network%20into%20equivalent%20Mixture-of-Experts%20%28MoE%29%0Alayers.%20Despite%20the%20crucial%20role%20played%20by%20activation%20sparsity%2C%20its%20impact%20on%0Athis%20process%20remains%20unexplored.%20We%20demonstrate%20that%20the%20efficiency%20of%20the%0Aconversion%20can%20be%20significantly%20enhanced%20by%20a%20proper%20regularization%20of%20the%0Aactivation%20sparsity%20of%20the%20base%20model.%20Moreover%2C%20motivated%20by%20the%20high%20variance%0Aof%20the%20number%20of%20activated%20neurons%20for%20different%20inputs%2C%20we%20introduce%20a%20more%0Aeffective%20dynamic-%24k%24%20expert%20selection%20rule%20that%20adjusts%20the%20number%20of%20executed%0Aexperts%20on%20a%20per-token%20basis.%20To%20achieve%20further%20savings%2C%20we%20extend%20this%0Aapproach%20to%20multi-head%20attention%20projections.%20Finally%2C%20we%20develop%20an%20efficient%0Aimplementation%20that%20translates%20these%20computational%20savings%20into%20actual%0Awall-clock%20speedup.%20The%20proposed%20method%2C%20Dense%20to%20Dynamic-%24k%24%0AMixture-of-Experts%20%28D2DMoE%29%2C%20outperforms%20existing%20approaches%20on%20common%20NLP%20and%0Avision%20tasks%2C%20reducing%20inference%20cost%20by%20up%20to%2060%25%20without%20significantly%0Aimpacting%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04361v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Activation%2520Sparsity%2520with%2520Dense%2520to%2520Dynamic-k%250A%2520%2520Mixture-of-Experts%2520Conversion%26entry.906535625%3DFilip%2520Szatkowski%2520and%2520Bartosz%2520W%25C3%25B3jcik%2520and%2520Miko%25C5%2582aj%2520Pi%25C3%25B3rczy%25C5%2584ski%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520Transformer%2520models%2520can%2520face%2520practical%2520limitations%2520due%2520to%2520their%2520high%250Acomputational%2520requirements.%2520At%2520the%2520same%2520time%252C%2520such%2520models%2520exhibit%2520significant%250Aactivation%2520sparsity%252C%2520which%2520can%2520be%2520leveraged%2520to%2520reduce%2520the%2520inference%2520cost%2520by%250Aconverting%2520parts%2520of%2520the%2520network%2520into%2520equivalent%2520Mixture-of-Experts%2520%2528MoE%2529%250Alayers.%2520Despite%2520the%2520crucial%2520role%2520played%2520by%2520activation%2520sparsity%252C%2520its%2520impact%2520on%250Athis%2520process%2520remains%2520unexplored.%2520We%2520demonstrate%2520that%2520the%2520efficiency%2520of%2520the%250Aconversion%2520can%2520be%2520significantly%2520enhanced%2520by%2520a%2520proper%2520regularization%2520of%2520the%250Aactivation%2520sparsity%2520of%2520the%2520base%2520model.%2520Moreover%252C%2520motivated%2520by%2520the%2520high%2520variance%250Aof%2520the%2520number%2520of%2520activated%2520neurons%2520for%2520different%2520inputs%252C%2520we%2520introduce%2520a%2520more%250Aeffective%2520dynamic-%2524k%2524%2520expert%2520selection%2520rule%2520that%2520adjusts%2520the%2520number%2520of%2520executed%250Aexperts%2520on%2520a%2520per-token%2520basis.%2520To%2520achieve%2520further%2520savings%252C%2520we%2520extend%2520this%250Aapproach%2520to%2520multi-head%2520attention%2520projections.%2520Finally%252C%2520we%2520develop%2520an%2520efficient%250Aimplementation%2520that%2520translates%2520these%2520computational%2520savings%2520into%2520actual%250Awall-clock%2520speedup.%2520The%2520proposed%2520method%252C%2520Dense%2520to%2520Dynamic-%2524k%2524%250AMixture-of-Experts%2520%2528D2DMoE%2529%252C%2520outperforms%2520existing%2520approaches%2520on%2520common%2520NLP%2520and%250Avision%2520tasks%252C%2520reducing%2520inference%2520cost%2520by%2520up%2520to%252060%2525%2520without%2520significantly%250Aimpacting%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04361v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Activation%20Sparsity%20with%20Dense%20to%20Dynamic-k%0A%20%20Mixture-of-Experts%20Conversion&entry.906535625=Filip%20Szatkowski%20and%20Bartosz%20W%C3%B3jcik%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane&entry.1292438233=%20%20Transformer%20models%20can%20face%20practical%20limitations%20due%20to%20their%20high%0Acomputational%20requirements.%20At%20the%20same%20time%2C%20such%20models%20exhibit%20significant%0Aactivation%20sparsity%2C%20which%20can%20be%20leveraged%20to%20reduce%20the%20inference%20cost%20by%0Aconverting%20parts%20of%20the%20network%20into%20equivalent%20Mixture-of-Experts%20%28MoE%29%0Alayers.%20Despite%20the%20crucial%20role%20played%20by%20activation%20sparsity%2C%20its%20impact%20on%0Athis%20process%20remains%20unexplored.%20We%20demonstrate%20that%20the%20efficiency%20of%20the%0Aconversion%20can%20be%20significantly%20enhanced%20by%20a%20proper%20regularization%20of%20the%0Aactivation%20sparsity%20of%20the%20base%20model.%20Moreover%2C%20motivated%20by%20the%20high%20variance%0Aof%20the%20number%20of%20activated%20neurons%20for%20different%20inputs%2C%20we%20introduce%20a%20more%0Aeffective%20dynamic-%24k%24%20expert%20selection%20rule%20that%20adjusts%20the%20number%20of%20executed%0Aexperts%20on%20a%20per-token%20basis.%20To%20achieve%20further%20savings%2C%20we%20extend%20this%0Aapproach%20to%20multi-head%20attention%20projections.%20Finally%2C%20we%20develop%20an%20efficient%0Aimplementation%20that%20translates%20these%20computational%20savings%20into%20actual%0Awall-clock%20speedup.%20The%20proposed%20method%2C%20Dense%20to%20Dynamic-%24k%24%0AMixture-of-Experts%20%28D2DMoE%29%2C%20outperforms%20existing%20approaches%20on%20common%20NLP%20and%0Avision%20tasks%2C%20reducing%20inference%20cost%20by%20up%20to%2060%25%20without%20significantly%0Aimpacting%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04361v4&entry.124074799=Read"},
{"title": "LiCS: Navigation using Learned-imitation on Cluttered Space", "author": "Joshua Julian Damanik and Jae-Won Jung and Chala Adane Deresa and Han-Lim Choi", "abstract": "  In this letter, we propose a robust and fast navigation system in a narrow\nindoor environment for UGV (Unmanned Ground Vehicle) using 2D LiDAR and\nodometry. We used behavior cloning with Transformer neural network to learn the\noptimization-based baseline algorithm. We inject Gaussian noise during expert\ndemonstration to increase the robustness of learned policy. We evaluate the\nperformance of LiCS using both simulation and hardware experiments. It\noutperforms all other baselines in terms of navigation performance and can\nmaintain its robust performance even on highly cluttered environments. During\nthe hardware experiments, LiCS can maintain safe navigation at maximum speed of\n$1.5\\ m/s$.\n", "link": "http://arxiv.org/abs/2406.14947v2", "date": "2024-11-12", "relevancy": 2.2322, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5645}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5592}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiCS%3A%20Navigation%20using%20Learned-imitation%20on%20Cluttered%20Space&body=Title%3A%20LiCS%3A%20Navigation%20using%20Learned-imitation%20on%20Cluttered%20Space%0AAuthor%3A%20Joshua%20Julian%20Damanik%20and%20Jae-Won%20Jung%20and%20Chala%20Adane%20Deresa%20and%20Han-Lim%20Choi%0AAbstract%3A%20%20%20In%20this%20letter%2C%20we%20propose%20a%20robust%20and%20fast%20navigation%20system%20in%20a%20narrow%0Aindoor%20environment%20for%20UGV%20%28Unmanned%20Ground%20Vehicle%29%20using%202D%20LiDAR%20and%0Aodometry.%20We%20used%20behavior%20cloning%20with%20Transformer%20neural%20network%20to%20learn%20the%0Aoptimization-based%20baseline%20algorithm.%20We%20inject%20Gaussian%20noise%20during%20expert%0Ademonstration%20to%20increase%20the%20robustness%20of%20learned%20policy.%20We%20evaluate%20the%0Aperformance%20of%20LiCS%20using%20both%20simulation%20and%20hardware%20experiments.%20It%0Aoutperforms%20all%20other%20baselines%20in%20terms%20of%20navigation%20performance%20and%20can%0Amaintain%20its%20robust%20performance%20even%20on%20highly%20cluttered%20environments.%20During%0Athe%20hardware%20experiments%2C%20LiCS%20can%20maintain%20safe%20navigation%20at%20maximum%20speed%20of%0A%241.5%5C%20m/s%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiCS%253A%2520Navigation%2520using%2520Learned-imitation%2520on%2520Cluttered%2520Space%26entry.906535625%3DJoshua%2520Julian%2520Damanik%2520and%2520Jae-Won%2520Jung%2520and%2520Chala%2520Adane%2520Deresa%2520and%2520Han-Lim%2520Choi%26entry.1292438233%3D%2520%2520In%2520this%2520letter%252C%2520we%2520propose%2520a%2520robust%2520and%2520fast%2520navigation%2520system%2520in%2520a%2520narrow%250Aindoor%2520environment%2520for%2520UGV%2520%2528Unmanned%2520Ground%2520Vehicle%2529%2520using%25202D%2520LiDAR%2520and%250Aodometry.%2520We%2520used%2520behavior%2520cloning%2520with%2520Transformer%2520neural%2520network%2520to%2520learn%2520the%250Aoptimization-based%2520baseline%2520algorithm.%2520We%2520inject%2520Gaussian%2520noise%2520during%2520expert%250Ademonstration%2520to%2520increase%2520the%2520robustness%2520of%2520learned%2520policy.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520LiCS%2520using%2520both%2520simulation%2520and%2520hardware%2520experiments.%2520It%250Aoutperforms%2520all%2520other%2520baselines%2520in%2520terms%2520of%2520navigation%2520performance%2520and%2520can%250Amaintain%2520its%2520robust%2520performance%2520even%2520on%2520highly%2520cluttered%2520environments.%2520During%250Athe%2520hardware%2520experiments%252C%2520LiCS%2520can%2520maintain%2520safe%2520navigation%2520at%2520maximum%2520speed%2520of%250A%25241.5%255C%2520m/s%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiCS%3A%20Navigation%20using%20Learned-imitation%20on%20Cluttered%20Space&entry.906535625=Joshua%20Julian%20Damanik%20and%20Jae-Won%20Jung%20and%20Chala%20Adane%20Deresa%20and%20Han-Lim%20Choi&entry.1292438233=%20%20In%20this%20letter%2C%20we%20propose%20a%20robust%20and%20fast%20navigation%20system%20in%20a%20narrow%0Aindoor%20environment%20for%20UGV%20%28Unmanned%20Ground%20Vehicle%29%20using%202D%20LiDAR%20and%0Aodometry.%20We%20used%20behavior%20cloning%20with%20Transformer%20neural%20network%20to%20learn%20the%0Aoptimization-based%20baseline%20algorithm.%20We%20inject%20Gaussian%20noise%20during%20expert%0Ademonstration%20to%20increase%20the%20robustness%20of%20learned%20policy.%20We%20evaluate%20the%0Aperformance%20of%20LiCS%20using%20both%20simulation%20and%20hardware%20experiments.%20It%0Aoutperforms%20all%20other%20baselines%20in%20terms%20of%20navigation%20performance%20and%20can%0Amaintain%20its%20robust%20performance%20even%20on%20highly%20cluttered%20environments.%20During%0Athe%20hardware%20experiments%2C%20LiCS%20can%20maintain%20safe%20navigation%20at%20maximum%20speed%20of%0A%241.5%5C%20m/s%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14947v2&entry.124074799=Read"},
{"title": "RINO: Accurate, Robust Radar-Inertial Odometry with Non-Iterative\n  Estimation", "author": "Shuocheng Yang and Yueming Cao and Shengbo Li and Jianqiang Wang and Shaobing Xu", "abstract": "  Precise localization and mapping are critical for achieving autonomous\nnavigation in self-driving vehicles. However, ego-motion estimation still faces\nsignificant challenges, particularly when GNSS failures occur or under extreme\nweather conditions (e.g., fog, rain, and snow). In recent years, scanning radar\nhas emerged as an effective solution due to its strong penetration\ncapabilities. Nevertheless, scanning radar data inherently contains high levels\nof noise, necessitating hundreds to thousands of iterations of optimization to\nestimate a reliable transformation from the noisy data. Such iterative solving\nis time-consuming, unstable, and prone to failure. To address these challenges,\nwe propose an accurate and robust Radar-Inertial Odometry system, RINO, which\nemploys a non-iterative solving approach. Our method decouples rotation and\ntranslation estimation and applies an adaptive voting scheme for 2D rotation\nestimation, enhancing efficiency while ensuring consistent solving time.\nAdditionally, the approach implements a loosely coupled system between the\nscanning radar and an inertial measurement unit (IMU), leveraging Error-State\nKalman Filtering (ESKF). Notably, we successfully estimated the uncertainty of\nthe pose estimation from the scanning radar, incorporating this into the\nfilter's Maximum A Posteriori estimation, a consideration that has been\npreviously overlooked. Validation on publicly available datasets demonstrates\nthat RINO outperforms state-of-the-art methods and baselines in both accuracy\nand robustness. Our code is available at https://github.com/yangsc4063/rino.\n", "link": "http://arxiv.org/abs/2411.07699v1", "date": "2024-11-12", "relevancy": 2.2308, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5402}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RINO%3A%20Accurate%2C%20Robust%20Radar-Inertial%20Odometry%20with%20Non-Iterative%0A%20%20Estimation&body=Title%3A%20RINO%3A%20Accurate%2C%20Robust%20Radar-Inertial%20Odometry%20with%20Non-Iterative%0A%20%20Estimation%0AAuthor%3A%20Shuocheng%20Yang%20and%20Yueming%20Cao%20and%20Shengbo%20Li%20and%20Jianqiang%20Wang%20and%20Shaobing%20Xu%0AAbstract%3A%20%20%20Precise%20localization%20and%20mapping%20are%20critical%20for%20achieving%20autonomous%0Anavigation%20in%20self-driving%20vehicles.%20However%2C%20ego-motion%20estimation%20still%20faces%0Asignificant%20challenges%2C%20particularly%20when%20GNSS%20failures%20occur%20or%20under%20extreme%0Aweather%20conditions%20%28e.g.%2C%20fog%2C%20rain%2C%20and%20snow%29.%20In%20recent%20years%2C%20scanning%20radar%0Ahas%20emerged%20as%20an%20effective%20solution%20due%20to%20its%20strong%20penetration%0Acapabilities.%20Nevertheless%2C%20scanning%20radar%20data%20inherently%20contains%20high%20levels%0Aof%20noise%2C%20necessitating%20hundreds%20to%20thousands%20of%20iterations%20of%20optimization%20to%0Aestimate%20a%20reliable%20transformation%20from%20the%20noisy%20data.%20Such%20iterative%20solving%0Ais%20time-consuming%2C%20unstable%2C%20and%20prone%20to%20failure.%20To%20address%20these%20challenges%2C%0Awe%20propose%20an%20accurate%20and%20robust%20Radar-Inertial%20Odometry%20system%2C%20RINO%2C%20which%0Aemploys%20a%20non-iterative%20solving%20approach.%20Our%20method%20decouples%20rotation%20and%0Atranslation%20estimation%20and%20applies%20an%20adaptive%20voting%20scheme%20for%202D%20rotation%0Aestimation%2C%20enhancing%20efficiency%20while%20ensuring%20consistent%20solving%20time.%0AAdditionally%2C%20the%20approach%20implements%20a%20loosely%20coupled%20system%20between%20the%0Ascanning%20radar%20and%20an%20inertial%20measurement%20unit%20%28IMU%29%2C%20leveraging%20Error-State%0AKalman%20Filtering%20%28ESKF%29.%20Notably%2C%20we%20successfully%20estimated%20the%20uncertainty%20of%0Athe%20pose%20estimation%20from%20the%20scanning%20radar%2C%20incorporating%20this%20into%20the%0Afilter%27s%20Maximum%20A%20Posteriori%20estimation%2C%20a%20consideration%20that%20has%20been%0Apreviously%20overlooked.%20Validation%20on%20publicly%20available%20datasets%20demonstrates%0Athat%20RINO%20outperforms%20state-of-the-art%20methods%20and%20baselines%20in%20both%20accuracy%0Aand%20robustness.%20Our%20code%20is%20available%20at%20https%3A//github.com/yangsc4063/rino.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRINO%253A%2520Accurate%252C%2520Robust%2520Radar-Inertial%2520Odometry%2520with%2520Non-Iterative%250A%2520%2520Estimation%26entry.906535625%3DShuocheng%2520Yang%2520and%2520Yueming%2520Cao%2520and%2520Shengbo%2520Li%2520and%2520Jianqiang%2520Wang%2520and%2520Shaobing%2520Xu%26entry.1292438233%3D%2520%2520Precise%2520localization%2520and%2520mapping%2520are%2520critical%2520for%2520achieving%2520autonomous%250Anavigation%2520in%2520self-driving%2520vehicles.%2520However%252C%2520ego-motion%2520estimation%2520still%2520faces%250Asignificant%2520challenges%252C%2520particularly%2520when%2520GNSS%2520failures%2520occur%2520or%2520under%2520extreme%250Aweather%2520conditions%2520%2528e.g.%252C%2520fog%252C%2520rain%252C%2520and%2520snow%2529.%2520In%2520recent%2520years%252C%2520scanning%2520radar%250Ahas%2520emerged%2520as%2520an%2520effective%2520solution%2520due%2520to%2520its%2520strong%2520penetration%250Acapabilities.%2520Nevertheless%252C%2520scanning%2520radar%2520data%2520inherently%2520contains%2520high%2520levels%250Aof%2520noise%252C%2520necessitating%2520hundreds%2520to%2520thousands%2520of%2520iterations%2520of%2520optimization%2520to%250Aestimate%2520a%2520reliable%2520transformation%2520from%2520the%2520noisy%2520data.%2520Such%2520iterative%2520solving%250Ais%2520time-consuming%252C%2520unstable%252C%2520and%2520prone%2520to%2520failure.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520an%2520accurate%2520and%2520robust%2520Radar-Inertial%2520Odometry%2520system%252C%2520RINO%252C%2520which%250Aemploys%2520a%2520non-iterative%2520solving%2520approach.%2520Our%2520method%2520decouples%2520rotation%2520and%250Atranslation%2520estimation%2520and%2520applies%2520an%2520adaptive%2520voting%2520scheme%2520for%25202D%2520rotation%250Aestimation%252C%2520enhancing%2520efficiency%2520while%2520ensuring%2520consistent%2520solving%2520time.%250AAdditionally%252C%2520the%2520approach%2520implements%2520a%2520loosely%2520coupled%2520system%2520between%2520the%250Ascanning%2520radar%2520and%2520an%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%252C%2520leveraging%2520Error-State%250AKalman%2520Filtering%2520%2528ESKF%2529.%2520Notably%252C%2520we%2520successfully%2520estimated%2520the%2520uncertainty%2520of%250Athe%2520pose%2520estimation%2520from%2520the%2520scanning%2520radar%252C%2520incorporating%2520this%2520into%2520the%250Afilter%2527s%2520Maximum%2520A%2520Posteriori%2520estimation%252C%2520a%2520consideration%2520that%2520has%2520been%250Apreviously%2520overlooked.%2520Validation%2520on%2520publicly%2520available%2520datasets%2520demonstrates%250Athat%2520RINO%2520outperforms%2520state-of-the-art%2520methods%2520and%2520baselines%2520in%2520both%2520accuracy%250Aand%2520robustness.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yangsc4063/rino.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RINO%3A%20Accurate%2C%20Robust%20Radar-Inertial%20Odometry%20with%20Non-Iterative%0A%20%20Estimation&entry.906535625=Shuocheng%20Yang%20and%20Yueming%20Cao%20and%20Shengbo%20Li%20and%20Jianqiang%20Wang%20and%20Shaobing%20Xu&entry.1292438233=%20%20Precise%20localization%20and%20mapping%20are%20critical%20for%20achieving%20autonomous%0Anavigation%20in%20self-driving%20vehicles.%20However%2C%20ego-motion%20estimation%20still%20faces%0Asignificant%20challenges%2C%20particularly%20when%20GNSS%20failures%20occur%20or%20under%20extreme%0Aweather%20conditions%20%28e.g.%2C%20fog%2C%20rain%2C%20and%20snow%29.%20In%20recent%20years%2C%20scanning%20radar%0Ahas%20emerged%20as%20an%20effective%20solution%20due%20to%20its%20strong%20penetration%0Acapabilities.%20Nevertheless%2C%20scanning%20radar%20data%20inherently%20contains%20high%20levels%0Aof%20noise%2C%20necessitating%20hundreds%20to%20thousands%20of%20iterations%20of%20optimization%20to%0Aestimate%20a%20reliable%20transformation%20from%20the%20noisy%20data.%20Such%20iterative%20solving%0Ais%20time-consuming%2C%20unstable%2C%20and%20prone%20to%20failure.%20To%20address%20these%20challenges%2C%0Awe%20propose%20an%20accurate%20and%20robust%20Radar-Inertial%20Odometry%20system%2C%20RINO%2C%20which%0Aemploys%20a%20non-iterative%20solving%20approach.%20Our%20method%20decouples%20rotation%20and%0Atranslation%20estimation%20and%20applies%20an%20adaptive%20voting%20scheme%20for%202D%20rotation%0Aestimation%2C%20enhancing%20efficiency%20while%20ensuring%20consistent%20solving%20time.%0AAdditionally%2C%20the%20approach%20implements%20a%20loosely%20coupled%20system%20between%20the%0Ascanning%20radar%20and%20an%20inertial%20measurement%20unit%20%28IMU%29%2C%20leveraging%20Error-State%0AKalman%20Filtering%20%28ESKF%29.%20Notably%2C%20we%20successfully%20estimated%20the%20uncertainty%20of%0Athe%20pose%20estimation%20from%20the%20scanning%20radar%2C%20incorporating%20this%20into%20the%0Afilter%27s%20Maximum%20A%20Posteriori%20estimation%2C%20a%20consideration%20that%20has%20been%0Apreviously%20overlooked.%20Validation%20on%20publicly%20available%20datasets%20demonstrates%0Athat%20RINO%20outperforms%20state-of-the-art%20methods%20and%20baselines%20in%20both%20accuracy%0Aand%20robustness.%20Our%20code%20is%20available%20at%20https%3A//github.com/yangsc4063/rino.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07699v1&entry.124074799=Read"},
{"title": "ALOcc: Adaptive Lifting-based 3D Semantic Occupancy and Cost\n  Volume-based Flow Prediction", "author": "Dubing Chen and Jin Fang and Wencheng Han and Xinjing Cheng and Junbo Yin and Chenzhong Xu and Fahad Shahbaz Khan and Jianbing Shen", "abstract": "  Vision-based semantic occupancy and flow prediction plays a crucial role in\nproviding spatiotemporal cues for real-world tasks, such as autonomous driving.\nExisting methods prioritize higher accuracy to cater to the demands of these\ntasks. In this work, we strive to improve performance by introducing a series\nof targeted improvements for 3D semantic occupancy prediction and flow\nestimation. First, we introduce an occlusion-aware adaptive lifting mechanism\nwith a depth denoising technique to improve the robustness of 2D-to-3D feature\ntransformation and reduce the reliance on depth priors. Second, we strengthen\nthe semantic consistency between 3D features and their original 2D modalities\nby utilizing shared semantic prototypes to jointly constrain both 2D and 3D\nfeatures. This is complemented by confidence- and category-based sampling\nstrategies to tackle long-tail challenges in 3D space. To alleviate the feature\nencoding burden in the joint prediction of semantics and flow, we propose a BEV\ncost volume-based prediction method that links flow and semantic features\nthrough a cost volume and employs a classification-regression supervision\nscheme to address the varying flow scales in dynamic scenes. Our purely\nconvolutional architecture framework, named ALOcc, achieves an optimal tradeoff\nbetween speed and accuracy achieving state-of-the-art results on multiple\nbenchmarks. On Occ3D and training without the camera visible mask, our ALOcc\nachieves an absolute gain of 2.5\\% in terms of RayIoU while operating at a\ncomparable speed compared to the state-of-the-art, using the same input size\n(256$\\times$704) and ResNet-50 backbone. Our method also achieves 2nd place in\nthe CVPR24 Occupancy and Flow Prediction Competition.\n", "link": "http://arxiv.org/abs/2411.07725v1", "date": "2024-11-12", "relevancy": 2.224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5954}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALOcc%3A%20Adaptive%20Lifting-based%203D%20Semantic%20Occupancy%20and%20Cost%0A%20%20Volume-based%20Flow%20Prediction&body=Title%3A%20ALOcc%3A%20Adaptive%20Lifting-based%203D%20Semantic%20Occupancy%20and%20Cost%0A%20%20Volume-based%20Flow%20Prediction%0AAuthor%3A%20Dubing%20Chen%20and%20Jin%20Fang%20and%20Wencheng%20Han%20and%20Xinjing%20Cheng%20and%20Junbo%20Yin%20and%20Chenzhong%20Xu%20and%20Fahad%20Shahbaz%20Khan%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20Vision-based%20semantic%20occupancy%20and%20flow%20prediction%20plays%20a%20crucial%20role%20in%0Aproviding%20spatiotemporal%20cues%20for%20real-world%20tasks%2C%20such%20as%20autonomous%20driving.%0AExisting%20methods%20prioritize%20higher%20accuracy%20to%20cater%20to%20the%20demands%20of%20these%0Atasks.%20In%20this%20work%2C%20we%20strive%20to%20improve%20performance%20by%20introducing%20a%20series%0Aof%20targeted%20improvements%20for%203D%20semantic%20occupancy%20prediction%20and%20flow%0Aestimation.%20First%2C%20we%20introduce%20an%20occlusion-aware%20adaptive%20lifting%20mechanism%0Awith%20a%20depth%20denoising%20technique%20to%20improve%20the%20robustness%20of%202D-to-3D%20feature%0Atransformation%20and%20reduce%20the%20reliance%20on%20depth%20priors.%20Second%2C%20we%20strengthen%0Athe%20semantic%20consistency%20between%203D%20features%20and%20their%20original%202D%20modalities%0Aby%20utilizing%20shared%20semantic%20prototypes%20to%20jointly%20constrain%20both%202D%20and%203D%0Afeatures.%20This%20is%20complemented%20by%20confidence-%20and%20category-based%20sampling%0Astrategies%20to%20tackle%20long-tail%20challenges%20in%203D%20space.%20To%20alleviate%20the%20feature%0Aencoding%20burden%20in%20the%20joint%20prediction%20of%20semantics%20and%20flow%2C%20we%20propose%20a%20BEV%0Acost%20volume-based%20prediction%20method%20that%20links%20flow%20and%20semantic%20features%0Athrough%20a%20cost%20volume%20and%20employs%20a%20classification-regression%20supervision%0Ascheme%20to%20address%20the%20varying%20flow%20scales%20in%20dynamic%20scenes.%20Our%20purely%0Aconvolutional%20architecture%20framework%2C%20named%20ALOcc%2C%20achieves%20an%20optimal%20tradeoff%0Abetween%20speed%20and%20accuracy%20achieving%20state-of-the-art%20results%20on%20multiple%0Abenchmarks.%20On%20Occ3D%20and%20training%20without%20the%20camera%20visible%20mask%2C%20our%20ALOcc%0Aachieves%20an%20absolute%20gain%20of%202.5%5C%25%20in%20terms%20of%20RayIoU%20while%20operating%20at%20a%0Acomparable%20speed%20compared%20to%20the%20state-of-the-art%2C%20using%20the%20same%20input%20size%0A%28256%24%5Ctimes%24704%29%20and%20ResNet-50%20backbone.%20Our%20method%20also%20achieves%202nd%20place%20in%0Athe%20CVPR24%20Occupancy%20and%20Flow%20Prediction%20Competition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALOcc%253A%2520Adaptive%2520Lifting-based%25203D%2520Semantic%2520Occupancy%2520and%2520Cost%250A%2520%2520Volume-based%2520Flow%2520Prediction%26entry.906535625%3DDubing%2520Chen%2520and%2520Jin%2520Fang%2520and%2520Wencheng%2520Han%2520and%2520Xinjing%2520Cheng%2520and%2520Junbo%2520Yin%2520and%2520Chenzhong%2520Xu%2520and%2520Fahad%2520Shahbaz%2520Khan%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520Vision-based%2520semantic%2520occupancy%2520and%2520flow%2520prediction%2520plays%2520a%2520crucial%2520role%2520in%250Aproviding%2520spatiotemporal%2520cues%2520for%2520real-world%2520tasks%252C%2520such%2520as%2520autonomous%2520driving.%250AExisting%2520methods%2520prioritize%2520higher%2520accuracy%2520to%2520cater%2520to%2520the%2520demands%2520of%2520these%250Atasks.%2520In%2520this%2520work%252C%2520we%2520strive%2520to%2520improve%2520performance%2520by%2520introducing%2520a%2520series%250Aof%2520targeted%2520improvements%2520for%25203D%2520semantic%2520occupancy%2520prediction%2520and%2520flow%250Aestimation.%2520First%252C%2520we%2520introduce%2520an%2520occlusion-aware%2520adaptive%2520lifting%2520mechanism%250Awith%2520a%2520depth%2520denoising%2520technique%2520to%2520improve%2520the%2520robustness%2520of%25202D-to-3D%2520feature%250Atransformation%2520and%2520reduce%2520the%2520reliance%2520on%2520depth%2520priors.%2520Second%252C%2520we%2520strengthen%250Athe%2520semantic%2520consistency%2520between%25203D%2520features%2520and%2520their%2520original%25202D%2520modalities%250Aby%2520utilizing%2520shared%2520semantic%2520prototypes%2520to%2520jointly%2520constrain%2520both%25202D%2520and%25203D%250Afeatures.%2520This%2520is%2520complemented%2520by%2520confidence-%2520and%2520category-based%2520sampling%250Astrategies%2520to%2520tackle%2520long-tail%2520challenges%2520in%25203D%2520space.%2520To%2520alleviate%2520the%2520feature%250Aencoding%2520burden%2520in%2520the%2520joint%2520prediction%2520of%2520semantics%2520and%2520flow%252C%2520we%2520propose%2520a%2520BEV%250Acost%2520volume-based%2520prediction%2520method%2520that%2520links%2520flow%2520and%2520semantic%2520features%250Athrough%2520a%2520cost%2520volume%2520and%2520employs%2520a%2520classification-regression%2520supervision%250Ascheme%2520to%2520address%2520the%2520varying%2520flow%2520scales%2520in%2520dynamic%2520scenes.%2520Our%2520purely%250Aconvolutional%2520architecture%2520framework%252C%2520named%2520ALOcc%252C%2520achieves%2520an%2520optimal%2520tradeoff%250Abetween%2520speed%2520and%2520accuracy%2520achieving%2520state-of-the-art%2520results%2520on%2520multiple%250Abenchmarks.%2520On%2520Occ3D%2520and%2520training%2520without%2520the%2520camera%2520visible%2520mask%252C%2520our%2520ALOcc%250Aachieves%2520an%2520absolute%2520gain%2520of%25202.5%255C%2525%2520in%2520terms%2520of%2520RayIoU%2520while%2520operating%2520at%2520a%250Acomparable%2520speed%2520compared%2520to%2520the%2520state-of-the-art%252C%2520using%2520the%2520same%2520input%2520size%250A%2528256%2524%255Ctimes%2524704%2529%2520and%2520ResNet-50%2520backbone.%2520Our%2520method%2520also%2520achieves%25202nd%2520place%2520in%250Athe%2520CVPR24%2520Occupancy%2520and%2520Flow%2520Prediction%2520Competition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALOcc%3A%20Adaptive%20Lifting-based%203D%20Semantic%20Occupancy%20and%20Cost%0A%20%20Volume-based%20Flow%20Prediction&entry.906535625=Dubing%20Chen%20and%20Jin%20Fang%20and%20Wencheng%20Han%20and%20Xinjing%20Cheng%20and%20Junbo%20Yin%20and%20Chenzhong%20Xu%20and%20Fahad%20Shahbaz%20Khan%20and%20Jianbing%20Shen&entry.1292438233=%20%20Vision-based%20semantic%20occupancy%20and%20flow%20prediction%20plays%20a%20crucial%20role%20in%0Aproviding%20spatiotemporal%20cues%20for%20real-world%20tasks%2C%20such%20as%20autonomous%20driving.%0AExisting%20methods%20prioritize%20higher%20accuracy%20to%20cater%20to%20the%20demands%20of%20these%0Atasks.%20In%20this%20work%2C%20we%20strive%20to%20improve%20performance%20by%20introducing%20a%20series%0Aof%20targeted%20improvements%20for%203D%20semantic%20occupancy%20prediction%20and%20flow%0Aestimation.%20First%2C%20we%20introduce%20an%20occlusion-aware%20adaptive%20lifting%20mechanism%0Awith%20a%20depth%20denoising%20technique%20to%20improve%20the%20robustness%20of%202D-to-3D%20feature%0Atransformation%20and%20reduce%20the%20reliance%20on%20depth%20priors.%20Second%2C%20we%20strengthen%0Athe%20semantic%20consistency%20between%203D%20features%20and%20their%20original%202D%20modalities%0Aby%20utilizing%20shared%20semantic%20prototypes%20to%20jointly%20constrain%20both%202D%20and%203D%0Afeatures.%20This%20is%20complemented%20by%20confidence-%20and%20category-based%20sampling%0Astrategies%20to%20tackle%20long-tail%20challenges%20in%203D%20space.%20To%20alleviate%20the%20feature%0Aencoding%20burden%20in%20the%20joint%20prediction%20of%20semantics%20and%20flow%2C%20we%20propose%20a%20BEV%0Acost%20volume-based%20prediction%20method%20that%20links%20flow%20and%20semantic%20features%0Athrough%20a%20cost%20volume%20and%20employs%20a%20classification-regression%20supervision%0Ascheme%20to%20address%20the%20varying%20flow%20scales%20in%20dynamic%20scenes.%20Our%20purely%0Aconvolutional%20architecture%20framework%2C%20named%20ALOcc%2C%20achieves%20an%20optimal%20tradeoff%0Abetween%20speed%20and%20accuracy%20achieving%20state-of-the-art%20results%20on%20multiple%0Abenchmarks.%20On%20Occ3D%20and%20training%20without%20the%20camera%20visible%20mask%2C%20our%20ALOcc%0Aachieves%20an%20absolute%20gain%20of%202.5%5C%25%20in%20terms%20of%20RayIoU%20while%20operating%20at%20a%0Acomparable%20speed%20compared%20to%20the%20state-of-the-art%2C%20using%20the%20same%20input%20size%0A%28256%24%5Ctimes%24704%29%20and%20ResNet-50%20backbone.%20Our%20method%20also%20achieves%202nd%20place%20in%0Athe%20CVPR24%20Occupancy%20and%20Flow%20Prediction%20Competition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07725v1&entry.124074799=Read"},
{"title": "Language Models as Causal Effect Generators", "author": "Lucius E. J. Bynum and Kyunghyun Cho", "abstract": "  We present a framework for large language model (LLM) based data generation\nwith controllable causal structure. In particular, we define a procedure for\nturning any language model and any directed acyclic graph (DAG) into a\nsequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM\nis a causal model with user-defined structure and LLM-defined structural\nequations. We characterize how an SD-SCM allows sampling from observational,\ninterventional, and counterfactual distributions according to the desired\ncausal structure. We then leverage this procedure to propose a new type of\nbenchmark for causal inference methods, generating individual-level\ncounterfactual data without needing to manually specify functional\nrelationships between variables. We create an example benchmark consisting of\nthousands of datasets, and test a suite of popular estimation methods on these\ndatasets for average, conditional average, and individual treatment effect\nestimation, both with and without hidden confounding. Apart from generating\ndata, the same procedure also allows us to test for the presence of a causal\neffect that might be encoded in an LLM. This procedure can underpin auditing\nLLMs for misinformation, discrimination, or otherwise undesirable behavior. We\nbelieve SD-SCMs can serve as a useful tool in any application that would\nbenefit from sequential data with controllable causal structure.\n", "link": "http://arxiv.org/abs/2411.08019v1", "date": "2024-11-12", "relevancy": 2.2025, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.44}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20as%20Causal%20Effect%20Generators&body=Title%3A%20Language%20Models%20as%20Causal%20Effect%20Generators%0AAuthor%3A%20Lucius%20E.%20J.%20Bynum%20and%20Kyunghyun%20Cho%0AAbstract%3A%20%20%20We%20present%20a%20framework%20for%20large%20language%20model%20%28LLM%29%20based%20data%20generation%0Awith%20controllable%20causal%20structure.%20In%20particular%2C%20we%20define%20a%20procedure%20for%0Aturning%20any%20language%20model%20and%20any%20directed%20acyclic%20graph%20%28DAG%29%20into%20a%0Asequence-driven%20structural%20causal%20model%20%28SD-SCM%29.%20Broadly%20speaking%2C%20an%20SD-SCM%0Ais%20a%20causal%20model%20with%20user-defined%20structure%20and%20LLM-defined%20structural%0Aequations.%20We%20characterize%20how%20an%20SD-SCM%20allows%20sampling%20from%20observational%2C%0Ainterventional%2C%20and%20counterfactual%20distributions%20according%20to%20the%20desired%0Acausal%20structure.%20We%20then%20leverage%20this%20procedure%20to%20propose%20a%20new%20type%20of%0Abenchmark%20for%20causal%20inference%20methods%2C%20generating%20individual-level%0Acounterfactual%20data%20without%20needing%20to%20manually%20specify%20functional%0Arelationships%20between%20variables.%20We%20create%20an%20example%20benchmark%20consisting%20of%0Athousands%20of%20datasets%2C%20and%20test%20a%20suite%20of%20popular%20estimation%20methods%20on%20these%0Adatasets%20for%20average%2C%20conditional%20average%2C%20and%20individual%20treatment%20effect%0Aestimation%2C%20both%20with%20and%20without%20hidden%20confounding.%20Apart%20from%20generating%0Adata%2C%20the%20same%20procedure%20also%20allows%20us%20to%20test%20for%20the%20presence%20of%20a%20causal%0Aeffect%20that%20might%20be%20encoded%20in%20an%20LLM.%20This%20procedure%20can%20underpin%20auditing%0ALLMs%20for%20misinformation%2C%20discrimination%2C%20or%20otherwise%20undesirable%20behavior.%20We%0Abelieve%20SD-SCMs%20can%20serve%20as%20a%20useful%20tool%20in%20any%20application%20that%20would%0Abenefit%20from%20sequential%20data%20with%20controllable%20causal%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520as%2520Causal%2520Effect%2520Generators%26entry.906535625%3DLucius%2520E.%2520J.%2520Bynum%2520and%2520Kyunghyun%2520Cho%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520framework%2520for%2520large%2520language%2520model%2520%2528LLM%2529%2520based%2520data%2520generation%250Awith%2520controllable%2520causal%2520structure.%2520In%2520particular%252C%2520we%2520define%2520a%2520procedure%2520for%250Aturning%2520any%2520language%2520model%2520and%2520any%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%2520into%2520a%250Asequence-driven%2520structural%2520causal%2520model%2520%2528SD-SCM%2529.%2520Broadly%2520speaking%252C%2520an%2520SD-SCM%250Ais%2520a%2520causal%2520model%2520with%2520user-defined%2520structure%2520and%2520LLM-defined%2520structural%250Aequations.%2520We%2520characterize%2520how%2520an%2520SD-SCM%2520allows%2520sampling%2520from%2520observational%252C%250Ainterventional%252C%2520and%2520counterfactual%2520distributions%2520according%2520to%2520the%2520desired%250Acausal%2520structure.%2520We%2520then%2520leverage%2520this%2520procedure%2520to%2520propose%2520a%2520new%2520type%2520of%250Abenchmark%2520for%2520causal%2520inference%2520methods%252C%2520generating%2520individual-level%250Acounterfactual%2520data%2520without%2520needing%2520to%2520manually%2520specify%2520functional%250Arelationships%2520between%2520variables.%2520We%2520create%2520an%2520example%2520benchmark%2520consisting%2520of%250Athousands%2520of%2520datasets%252C%2520and%2520test%2520a%2520suite%2520of%2520popular%2520estimation%2520methods%2520on%2520these%250Adatasets%2520for%2520average%252C%2520conditional%2520average%252C%2520and%2520individual%2520treatment%2520effect%250Aestimation%252C%2520both%2520with%2520and%2520without%2520hidden%2520confounding.%2520Apart%2520from%2520generating%250Adata%252C%2520the%2520same%2520procedure%2520also%2520allows%2520us%2520to%2520test%2520for%2520the%2520presence%2520of%2520a%2520causal%250Aeffect%2520that%2520might%2520be%2520encoded%2520in%2520an%2520LLM.%2520This%2520procedure%2520can%2520underpin%2520auditing%250ALLMs%2520for%2520misinformation%252C%2520discrimination%252C%2520or%2520otherwise%2520undesirable%2520behavior.%2520We%250Abelieve%2520SD-SCMs%2520can%2520serve%2520as%2520a%2520useful%2520tool%2520in%2520any%2520application%2520that%2520would%250Abenefit%2520from%2520sequential%2520data%2520with%2520controllable%2520causal%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20as%20Causal%20Effect%20Generators&entry.906535625=Lucius%20E.%20J.%20Bynum%20and%20Kyunghyun%20Cho&entry.1292438233=%20%20We%20present%20a%20framework%20for%20large%20language%20model%20%28LLM%29%20based%20data%20generation%0Awith%20controllable%20causal%20structure.%20In%20particular%2C%20we%20define%20a%20procedure%20for%0Aturning%20any%20language%20model%20and%20any%20directed%20acyclic%20graph%20%28DAG%29%20into%20a%0Asequence-driven%20structural%20causal%20model%20%28SD-SCM%29.%20Broadly%20speaking%2C%20an%20SD-SCM%0Ais%20a%20causal%20model%20with%20user-defined%20structure%20and%20LLM-defined%20structural%0Aequations.%20We%20characterize%20how%20an%20SD-SCM%20allows%20sampling%20from%20observational%2C%0Ainterventional%2C%20and%20counterfactual%20distributions%20according%20to%20the%20desired%0Acausal%20structure.%20We%20then%20leverage%20this%20procedure%20to%20propose%20a%20new%20type%20of%0Abenchmark%20for%20causal%20inference%20methods%2C%20generating%20individual-level%0Acounterfactual%20data%20without%20needing%20to%20manually%20specify%20functional%0Arelationships%20between%20variables.%20We%20create%20an%20example%20benchmark%20consisting%20of%0Athousands%20of%20datasets%2C%20and%20test%20a%20suite%20of%20popular%20estimation%20methods%20on%20these%0Adatasets%20for%20average%2C%20conditional%20average%2C%20and%20individual%20treatment%20effect%0Aestimation%2C%20both%20with%20and%20without%20hidden%20confounding.%20Apart%20from%20generating%0Adata%2C%20the%20same%20procedure%20also%20allows%20us%20to%20test%20for%20the%20presence%20of%20a%20causal%0Aeffect%20that%20might%20be%20encoded%20in%20an%20LLM.%20This%20procedure%20can%20underpin%20auditing%0ALLMs%20for%20misinformation%2C%20discrimination%2C%20or%20otherwise%20undesirable%20behavior.%20We%0Abelieve%20SD-SCMs%20can%20serve%20as%20a%20useful%20tool%20in%20any%20application%20that%20would%0Abenefit%20from%20sequential%20data%20with%20controllable%20causal%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08019v1&entry.124074799=Read"},
{"title": "CDXFormer: Boosting Remote Sensing Change Detection with Extended Long\n  Short-Term Memory", "author": "Zhenkai Wu and Xiaowen Ma and Rongrong Lian and Zhentao Lin and Wei Zhang", "abstract": "  In complex scenes and varied conditions, effectively integrating\nspatial-temporal context is crucial for accurately identifying changes.\nHowever, current RS-CD methods lack a balanced consideration of performance and\nefficiency. CNNs lack global context, Transformers have quadratic computational\ncomplexity, and Mambas are restricted by CUDA acceleration. In this paper, we\npropose CDXFormer, with a core component that is a powerful XLSTM-based feature\nenhancement layer, integrating the advantages of linear computational\ncomplexity, global context perception, and strong interpret-ability.\nSpecifically, we introduce a scale-specific Feature Enhancer layer,\nincorporating a Cross-Temporal Global Perceptron customized for\nsemantic-accurate deep features, and a Cross-Temporal Spatial Refiner\ncustomized for detail-rich shallow features. Additionally, we propose a\nCross-Scale Interactive Fusion module to progressively interact global change\nrepresentations with spatial responses. Extensive experimental results\ndemonstrate that CDXFormer achieves state-of-the-art performance across three\nbenchmark datasets, offering a compelling balance between efficiency and\naccuracy. Code is available at https://github.com/xwmaxwma/rschange.\n", "link": "http://arxiv.org/abs/2411.07863v1", "date": "2024-11-12", "relevancy": 2.1984, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDXFormer%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory&body=Title%3A%20CDXFormer%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory%0AAuthor%3A%20Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhentao%20Lin%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20In%20complex%20scenes%20and%20varied%20conditions%2C%20effectively%20integrating%0Aspatial-temporal%20context%20is%20crucial%20for%20accurately%20identifying%20changes.%0AHowever%2C%20current%20RS-CD%20methods%20lack%20a%20balanced%20consideration%20of%20performance%20and%0Aefficiency.%20CNNs%20lack%20global%20context%2C%20Transformers%20have%20quadratic%20computational%0Acomplexity%2C%20and%20Mambas%20are%20restricted%20by%20CUDA%20acceleration.%20In%20this%20paper%2C%20we%0Apropose%20CDXFormer%2C%20with%20a%20core%20component%20that%20is%20a%20powerful%20XLSTM-based%20feature%0Aenhancement%20layer%2C%20integrating%20the%20advantages%20of%20linear%20computational%0Acomplexity%2C%20global%20context%20perception%2C%20and%20strong%20interpret-ability.%0ASpecifically%2C%20we%20introduce%20a%20scale-specific%20Feature%20Enhancer%20layer%2C%0Aincorporating%20a%20Cross-Temporal%20Global%20Perceptron%20customized%20for%0Asemantic-accurate%20deep%20features%2C%20and%20a%20Cross-Temporal%20Spatial%20Refiner%0Acustomized%20for%20detail-rich%20shallow%20features.%20Additionally%2C%20we%20propose%20a%0ACross-Scale%20Interactive%20Fusion%20module%20to%20progressively%20interact%20global%20change%0Arepresentations%20with%20spatial%20responses.%20Extensive%20experimental%20results%0Ademonstrate%20that%20CDXFormer%20achieves%20state-of-the-art%20performance%20across%20three%0Abenchmark%20datasets%2C%20offering%20a%20compelling%20balance%20between%20efficiency%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rschange.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDXFormer%253A%2520Boosting%2520Remote%2520Sensing%2520Change%2520Detection%2520with%2520Extended%2520Long%250A%2520%2520Short-Term%2520Memory%26entry.906535625%3DZhenkai%2520Wu%2520and%2520Xiaowen%2520Ma%2520and%2520Rongrong%2520Lian%2520and%2520Zhentao%2520Lin%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520In%2520complex%2520scenes%2520and%2520varied%2520conditions%252C%2520effectively%2520integrating%250Aspatial-temporal%2520context%2520is%2520crucial%2520for%2520accurately%2520identifying%2520changes.%250AHowever%252C%2520current%2520RS-CD%2520methods%2520lack%2520a%2520balanced%2520consideration%2520of%2520performance%2520and%250Aefficiency.%2520CNNs%2520lack%2520global%2520context%252C%2520Transformers%2520have%2520quadratic%2520computational%250Acomplexity%252C%2520and%2520Mambas%2520are%2520restricted%2520by%2520CUDA%2520acceleration.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520CDXFormer%252C%2520with%2520a%2520core%2520component%2520that%2520is%2520a%2520powerful%2520XLSTM-based%2520feature%250Aenhancement%2520layer%252C%2520integrating%2520the%2520advantages%2520of%2520linear%2520computational%250Acomplexity%252C%2520global%2520context%2520perception%252C%2520and%2520strong%2520interpret-ability.%250ASpecifically%252C%2520we%2520introduce%2520a%2520scale-specific%2520Feature%2520Enhancer%2520layer%252C%250Aincorporating%2520a%2520Cross-Temporal%2520Global%2520Perceptron%2520customized%2520for%250Asemantic-accurate%2520deep%2520features%252C%2520and%2520a%2520Cross-Temporal%2520Spatial%2520Refiner%250Acustomized%2520for%2520detail-rich%2520shallow%2520features.%2520Additionally%252C%2520we%2520propose%2520a%250ACross-Scale%2520Interactive%2520Fusion%2520module%2520to%2520progressively%2520interact%2520global%2520change%250Arepresentations%2520with%2520spatial%2520responses.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520CDXFormer%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%250Abenchmark%2520datasets%252C%2520offering%2520a%2520compelling%2520balance%2520between%2520efficiency%2520and%250Aaccuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xwmaxwma/rschange.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDXFormer%3A%20Boosting%20Remote%20Sensing%20Change%20Detection%20with%20Extended%20Long%0A%20%20Short-Term%20Memory&entry.906535625=Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhentao%20Lin%20and%20Wei%20Zhang&entry.1292438233=%20%20In%20complex%20scenes%20and%20varied%20conditions%2C%20effectively%20integrating%0Aspatial-temporal%20context%20is%20crucial%20for%20accurately%20identifying%20changes.%0AHowever%2C%20current%20RS-CD%20methods%20lack%20a%20balanced%20consideration%20of%20performance%20and%0Aefficiency.%20CNNs%20lack%20global%20context%2C%20Transformers%20have%20quadratic%20computational%0Acomplexity%2C%20and%20Mambas%20are%20restricted%20by%20CUDA%20acceleration.%20In%20this%20paper%2C%20we%0Apropose%20CDXFormer%2C%20with%20a%20core%20component%20that%20is%20a%20powerful%20XLSTM-based%20feature%0Aenhancement%20layer%2C%20integrating%20the%20advantages%20of%20linear%20computational%0Acomplexity%2C%20global%20context%20perception%2C%20and%20strong%20interpret-ability.%0ASpecifically%2C%20we%20introduce%20a%20scale-specific%20Feature%20Enhancer%20layer%2C%0Aincorporating%20a%20Cross-Temporal%20Global%20Perceptron%20customized%20for%0Asemantic-accurate%20deep%20features%2C%20and%20a%20Cross-Temporal%20Spatial%20Refiner%0Acustomized%20for%20detail-rich%20shallow%20features.%20Additionally%2C%20we%20propose%20a%0ACross-Scale%20Interactive%20Fusion%20module%20to%20progressively%20interact%20global%20change%0Arepresentations%20with%20spatial%20responses.%20Extensive%20experimental%20results%0Ademonstrate%20that%20CDXFormer%20achieves%20state-of-the-art%20performance%20across%20three%0Abenchmark%20datasets%2C%20offering%20a%20compelling%20balance%20between%20efficiency%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rschange.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07863v1&entry.124074799=Read"},
{"title": "Piecewise Linearity of Min-Norm Solution Map of a Nonconvexly\n  Regularized Convex Sparse Model", "author": "Yi Zhang and Isao Yamada", "abstract": "  It is well known that the minimum $\\ell_2$-norm solution of the convex LASSO\nmodel, say $\\mathbf{x}_{\\star}$, is a continuous piecewise linear function of\nthe regularization parameter $\\lambda$, and its signed sparsity pattern is\nconstant within each linear piece. The current study is an extension of this\nclassic result, proving that the aforementioned properties extend to the\nmin-norm solution map $\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$, where\n$\\mathbf{y}$ is the observed signal, for a generalization of LASSO termed the\nscaled generalized minimax concave (sGMC) model. The sGMC model adopts a\nnonconvex debiased variant of the $\\ell_1$-norm as sparse regularizer, but its\nobjective function is overall-convex. Based on the geometric properties of\n$\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$, we propose an extension of the least\nangle regression (LARS) algorithm, which iteratively computes the closed-form\nexpression of $\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$ in each linear zone.\nUnder suitable conditions, the proposed algorithm provably obtains the whole\nsolution map $\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$ within finite iterations.\nNotably, our proof techniques for establishing continuity and piecewise\nlinearity of $\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$ are novel, and they lead\nto two side contributions: (a) our proofs establish continuity of the sGMC\nsolution set as a set-valued mapping of $(\\mathbf{y},\\lambda)$; (b) to prove\npiecewise linearity and piecewise constant sparsity pattern of\n$\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$, we do not require any assumption that\nprevious work relies on (whereas to prove some additional properties of\n$\\mathbf{x}_{\\star}(\\mathbf{y},\\lambda)$, we use a different set of assumptions\nfrom previous work).\n", "link": "http://arxiv.org/abs/2311.18438v3", "date": "2024-11-12", "relevancy": 2.1888, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4449}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4414}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Piecewise%20Linearity%20of%20Min-Norm%20Solution%20Map%20of%20a%20Nonconvexly%0A%20%20Regularized%20Convex%20Sparse%20Model&body=Title%3A%20Piecewise%20Linearity%20of%20Min-Norm%20Solution%20Map%20of%20a%20Nonconvexly%0A%20%20Regularized%20Convex%20Sparse%20Model%0AAuthor%3A%20Yi%20Zhang%20and%20Isao%20Yamada%0AAbstract%3A%20%20%20It%20is%20well%20known%20that%20the%20minimum%20%24%5Cell_2%24-norm%20solution%20of%20the%20convex%20LASSO%0Amodel%2C%20say%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%24%2C%20is%20a%20continuous%20piecewise%20linear%20function%20of%0Athe%20regularization%20parameter%20%24%5Clambda%24%2C%20and%20its%20signed%20sparsity%20pattern%20is%0Aconstant%20within%20each%20linear%20piece.%20The%20current%20study%20is%20an%20extension%20of%20this%0Aclassic%20result%2C%20proving%20that%20the%20aforementioned%20properties%20extend%20to%20the%0Amin-norm%20solution%20map%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20where%0A%24%5Cmathbf%7By%7D%24%20is%20the%20observed%20signal%2C%20for%20a%20generalization%20of%20LASSO%20termed%20the%0Ascaled%20generalized%20minimax%20concave%20%28sGMC%29%20model.%20The%20sGMC%20model%20adopts%20a%0Anonconvex%20debiased%20variant%20of%20the%20%24%5Cell_1%24-norm%20as%20sparse%20regularizer%2C%20but%20its%0Aobjective%20function%20is%20overall-convex.%20Based%20on%20the%20geometric%20properties%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20propose%20an%20extension%20of%20the%20least%0Aangle%20regression%20%28LARS%29%20algorithm%2C%20which%20iteratively%20computes%20the%20closed-form%0Aexpression%20of%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20in%20each%20linear%20zone.%0AUnder%20suitable%20conditions%2C%20the%20proposed%20algorithm%20provably%20obtains%20the%20whole%0Asolution%20map%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20within%20finite%20iterations.%0ANotably%2C%20our%20proof%20techniques%20for%20establishing%20continuity%20and%20piecewise%0Alinearity%20of%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20are%20novel%2C%20and%20they%20lead%0Ato%20two%20side%20contributions%3A%20%28a%29%20our%20proofs%20establish%20continuity%20of%20the%20sGMC%0Asolution%20set%20as%20a%20set-valued%20mapping%20of%20%24%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%3B%20%28b%29%20to%20prove%0Apiecewise%20linearity%20and%20piecewise%20constant%20sparsity%20pattern%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20do%20not%20require%20any%20assumption%20that%0Aprevious%20work%20relies%20on%20%28whereas%20to%20prove%20some%20additional%20properties%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20use%20a%20different%20set%20of%20assumptions%0Afrom%20previous%20work%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18438v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiecewise%2520Linearity%2520of%2520Min-Norm%2520Solution%2520Map%2520of%2520a%2520Nonconvexly%250A%2520%2520Regularized%2520Convex%2520Sparse%2520Model%26entry.906535625%3DYi%2520Zhang%2520and%2520Isao%2520Yamada%26entry.1292438233%3D%2520%2520It%2520is%2520well%2520known%2520that%2520the%2520minimum%2520%2524%255Cell_2%2524-norm%2520solution%2520of%2520the%2520convex%2520LASSO%250Amodel%252C%2520say%2520%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2524%252C%2520is%2520a%2520continuous%2520piecewise%2520linear%2520function%2520of%250Athe%2520regularization%2520parameter%2520%2524%255Clambda%2524%252C%2520and%2520its%2520signed%2520sparsity%2520pattern%2520is%250Aconstant%2520within%2520each%2520linear%2520piece.%2520The%2520current%2520study%2520is%2520an%2520extension%2520of%2520this%250Aclassic%2520result%252C%2520proving%2520that%2520the%2520aforementioned%2520properties%2520extend%2520to%2520the%250Amin-norm%2520solution%2520map%2520%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%252C%2520where%250A%2524%255Cmathbf%257By%257D%2524%2520is%2520the%2520observed%2520signal%252C%2520for%2520a%2520generalization%2520of%2520LASSO%2520termed%2520the%250Ascaled%2520generalized%2520minimax%2520concave%2520%2528sGMC%2529%2520model.%2520The%2520sGMC%2520model%2520adopts%2520a%250Anonconvex%2520debiased%2520variant%2520of%2520the%2520%2524%255Cell_1%2524-norm%2520as%2520sparse%2520regularizer%252C%2520but%2520its%250Aobjective%2520function%2520is%2520overall-convex.%2520Based%2520on%2520the%2520geometric%2520properties%2520of%250A%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%252C%2520we%2520propose%2520an%2520extension%2520of%2520the%2520least%250Aangle%2520regression%2520%2528LARS%2529%2520algorithm%252C%2520which%2520iteratively%2520computes%2520the%2520closed-form%250Aexpression%2520of%2520%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%2520in%2520each%2520linear%2520zone.%250AUnder%2520suitable%2520conditions%252C%2520the%2520proposed%2520algorithm%2520provably%2520obtains%2520the%2520whole%250Asolution%2520map%2520%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%2520within%2520finite%2520iterations.%250ANotably%252C%2520our%2520proof%2520techniques%2520for%2520establishing%2520continuity%2520and%2520piecewise%250Alinearity%2520of%2520%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%2520are%2520novel%252C%2520and%2520they%2520lead%250Ato%2520two%2520side%2520contributions%253A%2520%2528a%2529%2520our%2520proofs%2520establish%2520continuity%2520of%2520the%2520sGMC%250Asolution%2520set%2520as%2520a%2520set-valued%2520mapping%2520of%2520%2524%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%253B%2520%2528b%2529%2520to%2520prove%250Apiecewise%2520linearity%2520and%2520piecewise%2520constant%2520sparsity%2520pattern%2520of%250A%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%252C%2520we%2520do%2520not%2520require%2520any%2520assumption%2520that%250Aprevious%2520work%2520relies%2520on%2520%2528whereas%2520to%2520prove%2520some%2520additional%2520properties%2520of%250A%2524%255Cmathbf%257Bx%257D_%257B%255Cstar%257D%2528%255Cmathbf%257By%257D%252C%255Clambda%2529%2524%252C%2520we%2520use%2520a%2520different%2520set%2520of%2520assumptions%250Afrom%2520previous%2520work%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18438v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Piecewise%20Linearity%20of%20Min-Norm%20Solution%20Map%20of%20a%20Nonconvexly%0A%20%20Regularized%20Convex%20Sparse%20Model&entry.906535625=Yi%20Zhang%20and%20Isao%20Yamada&entry.1292438233=%20%20It%20is%20well%20known%20that%20the%20minimum%20%24%5Cell_2%24-norm%20solution%20of%20the%20convex%20LASSO%0Amodel%2C%20say%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%24%2C%20is%20a%20continuous%20piecewise%20linear%20function%20of%0Athe%20regularization%20parameter%20%24%5Clambda%24%2C%20and%20its%20signed%20sparsity%20pattern%20is%0Aconstant%20within%20each%20linear%20piece.%20The%20current%20study%20is%20an%20extension%20of%20this%0Aclassic%20result%2C%20proving%20that%20the%20aforementioned%20properties%20extend%20to%20the%0Amin-norm%20solution%20map%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20where%0A%24%5Cmathbf%7By%7D%24%20is%20the%20observed%20signal%2C%20for%20a%20generalization%20of%20LASSO%20termed%20the%0Ascaled%20generalized%20minimax%20concave%20%28sGMC%29%20model.%20The%20sGMC%20model%20adopts%20a%0Anonconvex%20debiased%20variant%20of%20the%20%24%5Cell_1%24-norm%20as%20sparse%20regularizer%2C%20but%20its%0Aobjective%20function%20is%20overall-convex.%20Based%20on%20the%20geometric%20properties%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20propose%20an%20extension%20of%20the%20least%0Aangle%20regression%20%28LARS%29%20algorithm%2C%20which%20iteratively%20computes%20the%20closed-form%0Aexpression%20of%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20in%20each%20linear%20zone.%0AUnder%20suitable%20conditions%2C%20the%20proposed%20algorithm%20provably%20obtains%20the%20whole%0Asolution%20map%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20within%20finite%20iterations.%0ANotably%2C%20our%20proof%20techniques%20for%20establishing%20continuity%20and%20piecewise%0Alinearity%20of%20%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%20are%20novel%2C%20and%20they%20lead%0Ato%20two%20side%20contributions%3A%20%28a%29%20our%20proofs%20establish%20continuity%20of%20the%20sGMC%0Asolution%20set%20as%20a%20set-valued%20mapping%20of%20%24%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%3B%20%28b%29%20to%20prove%0Apiecewise%20linearity%20and%20piecewise%20constant%20sparsity%20pattern%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20do%20not%20require%20any%20assumption%20that%0Aprevious%20work%20relies%20on%20%28whereas%20to%20prove%20some%20additional%20properties%20of%0A%24%5Cmathbf%7Bx%7D_%7B%5Cstar%7D%28%5Cmathbf%7By%7D%2C%5Clambda%29%24%2C%20we%20use%20a%20different%20set%20of%20assumptions%0Afrom%20previous%20work%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18438v3&entry.124074799=Read"},
{"title": "Robust Adaptive Safe Robotic Grasping with Tactile Sensing", "author": "Yitaek Kim and Jeeseop Kim and Albert H. Li and Aaron D. Ames and Christoffer Sloth", "abstract": "  Robotic grasping requires safe force interaction to prevent a grasped object\nfrom being damaged or slipping out of the hand. In this vein, this paper\nproposes an integrated framework for grasping with formal safety guarantees\nbased on Control Barrier Functions. We first design contact force and force\nclosure constraints, which are enforced by a safety filter to accomplish safe\ngrasping with finger force control. For sensory feedback, we develop a\ntechnique to estimate contact point, force, and torque from tactile sensors at\neach finger. We verify the framework with various safety filters in a numerical\nsimulation under a two-finger grasping scenario. We then experimentally\nvalidate the framework by grasping multiple objects, including fragile lab\nglassware, in a real robotic setup, showing that safe grasping can be\nsuccessfully achieved in the real world. We evaluate the performance of each\nsafety filter in the context of safety violation and conservatism, and find\nthat disturbance observer-based control barrier functions provide superior\nperformance for safety guarantees with minimum conservatism. The demonstration\nvideo is available at https://youtu.be/Cuj47mkXRdg.\n", "link": "http://arxiv.org/abs/2411.07833v1", "date": "2024-11-12", "relevancy": 2.1856, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing&body=Title%3A%20Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing%0AAuthor%3A%20Yitaek%20Kim%20and%20Jeeseop%20Kim%20and%20Albert%20H.%20Li%20and%20Aaron%20D.%20Ames%20and%20Christoffer%20Sloth%0AAbstract%3A%20%20%20Robotic%20grasping%20requires%20safe%20force%20interaction%20to%20prevent%20a%20grasped%20object%0Afrom%20being%20damaged%20or%20slipping%20out%20of%20the%20hand.%20In%20this%20vein%2C%20this%20paper%0Aproposes%20an%20integrated%20framework%20for%20grasping%20with%20formal%20safety%20guarantees%0Abased%20on%20Control%20Barrier%20Functions.%20We%20first%20design%20contact%20force%20and%20force%0Aclosure%20constraints%2C%20which%20are%20enforced%20by%20a%20safety%20filter%20to%20accomplish%20safe%0Agrasping%20with%20finger%20force%20control.%20For%20sensory%20feedback%2C%20we%20develop%20a%0Atechnique%20to%20estimate%20contact%20point%2C%20force%2C%20and%20torque%20from%20tactile%20sensors%20at%0Aeach%20finger.%20We%20verify%20the%20framework%20with%20various%20safety%20filters%20in%20a%20numerical%0Asimulation%20under%20a%20two-finger%20grasping%20scenario.%20We%20then%20experimentally%0Avalidate%20the%20framework%20by%20grasping%20multiple%20objects%2C%20including%20fragile%20lab%0Aglassware%2C%20in%20a%20real%20robotic%20setup%2C%20showing%20that%20safe%20grasping%20can%20be%0Asuccessfully%20achieved%20in%20the%20real%20world.%20We%20evaluate%20the%20performance%20of%20each%0Asafety%20filter%20in%20the%20context%20of%20safety%20violation%20and%20conservatism%2C%20and%20find%0Athat%20disturbance%20observer-based%20control%20barrier%20functions%20provide%20superior%0Aperformance%20for%20safety%20guarantees%20with%20minimum%20conservatism.%20The%20demonstration%0Avideo%20is%20available%20at%20https%3A//youtu.be/Cuj47mkXRdg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Adaptive%2520Safe%2520Robotic%2520Grasping%2520with%2520Tactile%2520Sensing%26entry.906535625%3DYitaek%2520Kim%2520and%2520Jeeseop%2520Kim%2520and%2520Albert%2520H.%2520Li%2520and%2520Aaron%2520D.%2520Ames%2520and%2520Christoffer%2520Sloth%26entry.1292438233%3D%2520%2520Robotic%2520grasping%2520requires%2520safe%2520force%2520interaction%2520to%2520prevent%2520a%2520grasped%2520object%250Afrom%2520being%2520damaged%2520or%2520slipping%2520out%2520of%2520the%2520hand.%2520In%2520this%2520vein%252C%2520this%2520paper%250Aproposes%2520an%2520integrated%2520framework%2520for%2520grasping%2520with%2520formal%2520safety%2520guarantees%250Abased%2520on%2520Control%2520Barrier%2520Functions.%2520We%2520first%2520design%2520contact%2520force%2520and%2520force%250Aclosure%2520constraints%252C%2520which%2520are%2520enforced%2520by%2520a%2520safety%2520filter%2520to%2520accomplish%2520safe%250Agrasping%2520with%2520finger%2520force%2520control.%2520For%2520sensory%2520feedback%252C%2520we%2520develop%2520a%250Atechnique%2520to%2520estimate%2520contact%2520point%252C%2520force%252C%2520and%2520torque%2520from%2520tactile%2520sensors%2520at%250Aeach%2520finger.%2520We%2520verify%2520the%2520framework%2520with%2520various%2520safety%2520filters%2520in%2520a%2520numerical%250Asimulation%2520under%2520a%2520two-finger%2520grasping%2520scenario.%2520We%2520then%2520experimentally%250Avalidate%2520the%2520framework%2520by%2520grasping%2520multiple%2520objects%252C%2520including%2520fragile%2520lab%250Aglassware%252C%2520in%2520a%2520real%2520robotic%2520setup%252C%2520showing%2520that%2520safe%2520grasping%2520can%2520be%250Asuccessfully%2520achieved%2520in%2520the%2520real%2520world.%2520We%2520evaluate%2520the%2520performance%2520of%2520each%250Asafety%2520filter%2520in%2520the%2520context%2520of%2520safety%2520violation%2520and%2520conservatism%252C%2520and%2520find%250Athat%2520disturbance%2520observer-based%2520control%2520barrier%2520functions%2520provide%2520superior%250Aperformance%2520for%2520safety%2520guarantees%2520with%2520minimum%2520conservatism.%2520The%2520demonstration%250Avideo%2520is%2520available%2520at%2520https%253A//youtu.be/Cuj47mkXRdg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Adaptive%20Safe%20Robotic%20Grasping%20with%20Tactile%20Sensing&entry.906535625=Yitaek%20Kim%20and%20Jeeseop%20Kim%20and%20Albert%20H.%20Li%20and%20Aaron%20D.%20Ames%20and%20Christoffer%20Sloth&entry.1292438233=%20%20Robotic%20grasping%20requires%20safe%20force%20interaction%20to%20prevent%20a%20grasped%20object%0Afrom%20being%20damaged%20or%20slipping%20out%20of%20the%20hand.%20In%20this%20vein%2C%20this%20paper%0Aproposes%20an%20integrated%20framework%20for%20grasping%20with%20formal%20safety%20guarantees%0Abased%20on%20Control%20Barrier%20Functions.%20We%20first%20design%20contact%20force%20and%20force%0Aclosure%20constraints%2C%20which%20are%20enforced%20by%20a%20safety%20filter%20to%20accomplish%20safe%0Agrasping%20with%20finger%20force%20control.%20For%20sensory%20feedback%2C%20we%20develop%20a%0Atechnique%20to%20estimate%20contact%20point%2C%20force%2C%20and%20torque%20from%20tactile%20sensors%20at%0Aeach%20finger.%20We%20verify%20the%20framework%20with%20various%20safety%20filters%20in%20a%20numerical%0Asimulation%20under%20a%20two-finger%20grasping%20scenario.%20We%20then%20experimentally%0Avalidate%20the%20framework%20by%20grasping%20multiple%20objects%2C%20including%20fragile%20lab%0Aglassware%2C%20in%20a%20real%20robotic%20setup%2C%20showing%20that%20safe%20grasping%20can%20be%0Asuccessfully%20achieved%20in%20the%20real%20world.%20We%20evaluate%20the%20performance%20of%20each%0Asafety%20filter%20in%20the%20context%20of%20safety%20violation%20and%20conservatism%2C%20and%20find%0Athat%20disturbance%20observer-based%20control%20barrier%20functions%20provide%20superior%0Aperformance%20for%20safety%20guarantees%20with%20minimum%20conservatism.%20The%20demonstration%0Avideo%20is%20available%20at%20https%3A//youtu.be/Cuj47mkXRdg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07833v1&entry.124074799=Read"},
{"title": "Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation", "author": "Wuzheng Dong", "abstract": "  This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA.\n", "link": "http://arxiv.org/abs/2411.07802v1", "date": "2024-11-12", "relevancy": 2.1852, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.555}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation&body=Title%3A%20Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation%0AAuthor%3A%20Wuzheng%20Dong%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20method%20for%20object%20recognition%20and%20automatic%20labeling%20in%0Alarge-area%20remote%20sensing%20images%20called%20LRSAA.%20The%20method%20integrates%20YOLOv11%0Aand%20MobileNetV3-SSD%20object%20detection%20algorithms%20through%20ensemble%20learning%20to%0Aenhance%20model%20performance.%20Furthermore%2C%20it%20employs%20Poisson%20disk%20sampling%0Asegmentation%20techniques%20and%20the%20EIOU%20metric%20to%20optimize%20the%20training%20and%0Ainference%20processes%20of%20segmented%20images%2C%20followed%20by%20the%20integration%20of%0Aresults.%20This%20approach%20not%20only%20reduces%20the%20demand%20for%20computational%20resources%0Abut%20also%20achieves%20a%20good%20balance%20between%20accuracy%20and%20speed.%20The%20source%20code%0Afor%20this%20project%20has%20been%20made%20publicly%20available%20on%0Ahttps%3A//github.com/anaerovane/LRSAA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Remote%2520Sensing%2520Image%2520Target%2520Recognition%2520and%2520Automatic%250A%2520%2520Annotation%26entry.906535625%3DWuzheng%2520Dong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520method%2520for%2520object%2520recognition%2520and%2520automatic%2520labeling%2520in%250Alarge-area%2520remote%2520sensing%2520images%2520called%2520LRSAA.%2520The%2520method%2520integrates%2520YOLOv11%250Aand%2520MobileNetV3-SSD%2520object%2520detection%2520algorithms%2520through%2520ensemble%2520learning%2520to%250Aenhance%2520model%2520performance.%2520Furthermore%252C%2520it%2520employs%2520Poisson%2520disk%2520sampling%250Asegmentation%2520techniques%2520and%2520the%2520EIOU%2520metric%2520to%2520optimize%2520the%2520training%2520and%250Ainference%2520processes%2520of%2520segmented%2520images%252C%2520followed%2520by%2520the%2520integration%2520of%250Aresults.%2520This%2520approach%2520not%2520only%2520reduces%2520the%2520demand%2520for%2520computational%2520resources%250Abut%2520also%2520achieves%2520a%2520good%2520balance%2520between%2520accuracy%2520and%2520speed.%2520The%2520source%2520code%250Afor%2520this%2520project%2520has%2520been%2520made%2520publicly%2520available%2520on%250Ahttps%253A//github.com/anaerovane/LRSAA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Remote%20Sensing%20Image%20Target%20Recognition%20and%20Automatic%0A%20%20Annotation&entry.906535625=Wuzheng%20Dong&entry.1292438233=%20%20This%20paper%20presents%20a%20method%20for%20object%20recognition%20and%20automatic%20labeling%20in%0Alarge-area%20remote%20sensing%20images%20called%20LRSAA.%20The%20method%20integrates%20YOLOv11%0Aand%20MobileNetV3-SSD%20object%20detection%20algorithms%20through%20ensemble%20learning%20to%0Aenhance%20model%20performance.%20Furthermore%2C%20it%20employs%20Poisson%20disk%20sampling%0Asegmentation%20techniques%20and%20the%20EIOU%20metric%20to%20optimize%20the%20training%20and%0Ainference%20processes%20of%20segmented%20images%2C%20followed%20by%20the%20integration%20of%0Aresults.%20This%20approach%20not%20only%20reduces%20the%20demand%20for%20computational%20resources%0Abut%20also%20achieves%20a%20good%20balance%20between%20accuracy%20and%20speed.%20The%20source%20code%0Afor%20this%20project%20has%20been%20made%20publicly%20available%20on%0Ahttps%3A//github.com/anaerovane/LRSAA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07802v1&entry.124074799=Read"},
{"title": "Is Cognition consistent with Perception? Assessing and Mitigating\n  Multimodal Knowledge Conflicts in Document Understanding", "author": "Zirui Shao and Chuwei Luo and Zhaoqing Zhu and Hangdi Xing and Zhi Yu and Qi Zheng and Jiajun Bu", "abstract": "  Multimodal large language models (MLLMs) have shown impressive capabilities\nin document understanding, a rapidly growing research area with significant\nindustrial demand in recent years. As a multimodal task, document understanding\nrequires models to possess both perceptual and cognitive abilities. However,\ncurrent MLLMs often face conflicts between perception and cognition. Taking a\ndocument VQA task (cognition) as an example, an MLLM might generate answers\nthat do not match the corresponding visual content identified by its OCR\n(perception). This conflict suggests that the MLLM might struggle to establish\nan intrinsic connection between the information it \"sees\" and what it\n\"understands.\" Such conflicts challenge the intuitive notion that cognition is\nconsistent with perception, hindering the performance and explainability of\nMLLMs. In this paper, we define the conflicts between cognition and perception\nas Cognition and Perception (C&P) knowledge conflicts, a form of multimodal\nknowledge conflicts, and systematically assess them with a focus on document\nunderstanding. Our analysis reveals that even GPT-4o, a leading MLLM, achieves\nonly 68.6% C&P consistency. To mitigate the C&P knowledge conflicts, we propose\na novel method called Multimodal Knowledge Consistency Fine-tuning. This method\nfirst ensures task-specific consistency and then connects the cognitive and\nperceptual knowledge. Our method significantly reduces C&P knowledge conflicts\nacross all tested MLLMs and enhances their performance in both cognitive and\nperceptual tasks in most scenarios.\n", "link": "http://arxiv.org/abs/2411.07722v1", "date": "2024-11-12", "relevancy": 2.1778, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Cognition%20consistent%20with%20Perception%3F%20Assessing%20and%20Mitigating%0A%20%20Multimodal%20Knowledge%20Conflicts%20in%20Document%20Understanding&body=Title%3A%20Is%20Cognition%20consistent%20with%20Perception%3F%20Assessing%20and%20Mitigating%0A%20%20Multimodal%20Knowledge%20Conflicts%20in%20Document%20Understanding%0AAuthor%3A%20Zirui%20Shao%20and%20Chuwei%20Luo%20and%20Zhaoqing%20Zhu%20and%20Hangdi%20Xing%20and%20Zhi%20Yu%20and%20Qi%20Zheng%20and%20Jiajun%20Bu%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20impressive%20capabilities%0Ain%20document%20understanding%2C%20a%20rapidly%20growing%20research%20area%20with%20significant%0Aindustrial%20demand%20in%20recent%20years.%20As%20a%20multimodal%20task%2C%20document%20understanding%0Arequires%20models%20to%20possess%20both%20perceptual%20and%20cognitive%20abilities.%20However%2C%0Acurrent%20MLLMs%20often%20face%20conflicts%20between%20perception%20and%20cognition.%20Taking%20a%0Adocument%20VQA%20task%20%28cognition%29%20as%20an%20example%2C%20an%20MLLM%20might%20generate%20answers%0Athat%20do%20not%20match%20the%20corresponding%20visual%20content%20identified%20by%20its%20OCR%0A%28perception%29.%20This%20conflict%20suggests%20that%20the%20MLLM%20might%20struggle%20to%20establish%0Aan%20intrinsic%20connection%20between%20the%20information%20it%20%22sees%22%20and%20what%20it%0A%22understands.%22%20Such%20conflicts%20challenge%20the%20intuitive%20notion%20that%20cognition%20is%0Aconsistent%20with%20perception%2C%20hindering%20the%20performance%20and%20explainability%20of%0AMLLMs.%20In%20this%20paper%2C%20we%20define%20the%20conflicts%20between%20cognition%20and%20perception%0Aas%20Cognition%20and%20Perception%20%28C%26P%29%20knowledge%20conflicts%2C%20a%20form%20of%20multimodal%0Aknowledge%20conflicts%2C%20and%20systematically%20assess%20them%20with%20a%20focus%20on%20document%0Aunderstanding.%20Our%20analysis%20reveals%20that%20even%20GPT-4o%2C%20a%20leading%20MLLM%2C%20achieves%0Aonly%2068.6%25%20C%26P%20consistency.%20To%20mitigate%20the%20C%26P%20knowledge%20conflicts%2C%20we%20propose%0Aa%20novel%20method%20called%20Multimodal%20Knowledge%20Consistency%20Fine-tuning.%20This%20method%0Afirst%20ensures%20task-specific%20consistency%20and%20then%20connects%20the%20cognitive%20and%0Aperceptual%20knowledge.%20Our%20method%20significantly%20reduces%20C%26P%20knowledge%20conflicts%0Aacross%20all%20tested%20MLLMs%20and%20enhances%20their%20performance%20in%20both%20cognitive%20and%0Aperceptual%20tasks%20in%20most%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Cognition%2520consistent%2520with%2520Perception%253F%2520Assessing%2520and%2520Mitigating%250A%2520%2520Multimodal%2520Knowledge%2520Conflicts%2520in%2520Document%2520Understanding%26entry.906535625%3DZirui%2520Shao%2520and%2520Chuwei%2520Luo%2520and%2520Zhaoqing%2520Zhu%2520and%2520Hangdi%2520Xing%2520and%2520Zhi%2520Yu%2520and%2520Qi%2520Zheng%2520and%2520Jiajun%2520Bu%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520impressive%2520capabilities%250Ain%2520document%2520understanding%252C%2520a%2520rapidly%2520growing%2520research%2520area%2520with%2520significant%250Aindustrial%2520demand%2520in%2520recent%2520years.%2520As%2520a%2520multimodal%2520task%252C%2520document%2520understanding%250Arequires%2520models%2520to%2520possess%2520both%2520perceptual%2520and%2520cognitive%2520abilities.%2520However%252C%250Acurrent%2520MLLMs%2520often%2520face%2520conflicts%2520between%2520perception%2520and%2520cognition.%2520Taking%2520a%250Adocument%2520VQA%2520task%2520%2528cognition%2529%2520as%2520an%2520example%252C%2520an%2520MLLM%2520might%2520generate%2520answers%250Athat%2520do%2520not%2520match%2520the%2520corresponding%2520visual%2520content%2520identified%2520by%2520its%2520OCR%250A%2528perception%2529.%2520This%2520conflict%2520suggests%2520that%2520the%2520MLLM%2520might%2520struggle%2520to%2520establish%250Aan%2520intrinsic%2520connection%2520between%2520the%2520information%2520it%2520%2522sees%2522%2520and%2520what%2520it%250A%2522understands.%2522%2520Such%2520conflicts%2520challenge%2520the%2520intuitive%2520notion%2520that%2520cognition%2520is%250Aconsistent%2520with%2520perception%252C%2520hindering%2520the%2520performance%2520and%2520explainability%2520of%250AMLLMs.%2520In%2520this%2520paper%252C%2520we%2520define%2520the%2520conflicts%2520between%2520cognition%2520and%2520perception%250Aas%2520Cognition%2520and%2520Perception%2520%2528C%2526P%2529%2520knowledge%2520conflicts%252C%2520a%2520form%2520of%2520multimodal%250Aknowledge%2520conflicts%252C%2520and%2520systematically%2520assess%2520them%2520with%2520a%2520focus%2520on%2520document%250Aunderstanding.%2520Our%2520analysis%2520reveals%2520that%2520even%2520GPT-4o%252C%2520a%2520leading%2520MLLM%252C%2520achieves%250Aonly%252068.6%2525%2520C%2526P%2520consistency.%2520To%2520mitigate%2520the%2520C%2526P%2520knowledge%2520conflicts%252C%2520we%2520propose%250Aa%2520novel%2520method%2520called%2520Multimodal%2520Knowledge%2520Consistency%2520Fine-tuning.%2520This%2520method%250Afirst%2520ensures%2520task-specific%2520consistency%2520and%2520then%2520connects%2520the%2520cognitive%2520and%250Aperceptual%2520knowledge.%2520Our%2520method%2520significantly%2520reduces%2520C%2526P%2520knowledge%2520conflicts%250Aacross%2520all%2520tested%2520MLLMs%2520and%2520enhances%2520their%2520performance%2520in%2520both%2520cognitive%2520and%250Aperceptual%2520tasks%2520in%2520most%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Cognition%20consistent%20with%20Perception%3F%20Assessing%20and%20Mitigating%0A%20%20Multimodal%20Knowledge%20Conflicts%20in%20Document%20Understanding&entry.906535625=Zirui%20Shao%20and%20Chuwei%20Luo%20and%20Zhaoqing%20Zhu%20and%20Hangdi%20Xing%20and%20Zhi%20Yu%20and%20Qi%20Zheng%20and%20Jiajun%20Bu&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20impressive%20capabilities%0Ain%20document%20understanding%2C%20a%20rapidly%20growing%20research%20area%20with%20significant%0Aindustrial%20demand%20in%20recent%20years.%20As%20a%20multimodal%20task%2C%20document%20understanding%0Arequires%20models%20to%20possess%20both%20perceptual%20and%20cognitive%20abilities.%20However%2C%0Acurrent%20MLLMs%20often%20face%20conflicts%20between%20perception%20and%20cognition.%20Taking%20a%0Adocument%20VQA%20task%20%28cognition%29%20as%20an%20example%2C%20an%20MLLM%20might%20generate%20answers%0Athat%20do%20not%20match%20the%20corresponding%20visual%20content%20identified%20by%20its%20OCR%0A%28perception%29.%20This%20conflict%20suggests%20that%20the%20MLLM%20might%20struggle%20to%20establish%0Aan%20intrinsic%20connection%20between%20the%20information%20it%20%22sees%22%20and%20what%20it%0A%22understands.%22%20Such%20conflicts%20challenge%20the%20intuitive%20notion%20that%20cognition%20is%0Aconsistent%20with%20perception%2C%20hindering%20the%20performance%20and%20explainability%20of%0AMLLMs.%20In%20this%20paper%2C%20we%20define%20the%20conflicts%20between%20cognition%20and%20perception%0Aas%20Cognition%20and%20Perception%20%28C%26P%29%20knowledge%20conflicts%2C%20a%20form%20of%20multimodal%0Aknowledge%20conflicts%2C%20and%20systematically%20assess%20them%20with%20a%20focus%20on%20document%0Aunderstanding.%20Our%20analysis%20reveals%20that%20even%20GPT-4o%2C%20a%20leading%20MLLM%2C%20achieves%0Aonly%2068.6%25%20C%26P%20consistency.%20To%20mitigate%20the%20C%26P%20knowledge%20conflicts%2C%20we%20propose%0Aa%20novel%20method%20called%20Multimodal%20Knowledge%20Consistency%20Fine-tuning.%20This%20method%0Afirst%20ensures%20task-specific%20consistency%20and%20then%20connects%20the%20cognitive%20and%0Aperceptual%20knowledge.%20Our%20method%20significantly%20reduces%20C%26P%20knowledge%20conflicts%0Aacross%20all%20tested%20MLLMs%20and%20enhances%20their%20performance%20in%20both%20cognitive%20and%0Aperceptual%20tasks%20in%20most%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07722v1&entry.124074799=Read"},
{"title": "Convolutional and Deep Learning based techniques for Time Series Ordinal\n  Classification", "author": "Rafael Ayll\u00f3n-Gavil\u00e1n and David Guijo-Rubio and Pedro Antonio Guti\u00e9rrez and Anthony Bagnall and C\u00e9sar Herv\u00e1s-Mart\u00ednez", "abstract": "  Time Series Classification (TSC) covers the supervised learning problem where\ninput data is provided in the form of series of values observed through\nrepeated measurements over time, and whose objective is to predict the category\nto which they belong. When the class values are ordinal, classifiers that take\nthis into account can perform better than nominal classifiers. Time Series\nOrdinal Classification (TSOC) is the field covering this gap, yet unexplored in\nthe literature. There are a wide range of time series problems showing an\nordered label structure, and TSC techniques that ignore the order relationship\ndiscard useful information. Hence, this paper presents a first benchmarking of\nTSOC methodologies, exploiting the ordering of the target labels to boost the\nperformance of current TSC state-of-the-art. Both convolutional- and deep\nlearning-based methodologies (among the best performing alternatives for\nnominal TSC) are adapted for TSOC. For the experiments, a selection of 29\nordinal problems from two well-known archives has been made. In this way, this\npaper contributes to the establishment of the state-of-the-art in TSOC. The\nresults obtained by ordinal versions are found to be significantly better than\ncurrent nominal TSC techniques in terms of ordinal performance metrics,\noutlining the importance of considering the ordering of the labels when dealing\nwith this kind of problems.\n", "link": "http://arxiv.org/abs/2306.10084v3", "date": "2024-11-12", "relevancy": 2.1746, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4399}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4384}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20and%20Deep%20Learning%20based%20techniques%20for%20Time%20Series%20Ordinal%0A%20%20Classification&body=Title%3A%20Convolutional%20and%20Deep%20Learning%20based%20techniques%20for%20Time%20Series%20Ordinal%0A%20%20Classification%0AAuthor%3A%20Rafael%20Ayll%C3%B3n-Gavil%C3%A1n%20and%20David%20Guijo-Rubio%20and%20Pedro%20Antonio%20Guti%C3%A9rrez%20and%20Anthony%20Bagnall%20and%20C%C3%A9sar%20Herv%C3%A1s-Mart%C3%ADnez%0AAbstract%3A%20%20%20Time%20Series%20Classification%20%28TSC%29%20covers%20the%20supervised%20learning%20problem%20where%0Ainput%20data%20is%20provided%20in%20the%20form%20of%20series%20of%20values%20observed%20through%0Arepeated%20measurements%20over%20time%2C%20and%20whose%20objective%20is%20to%20predict%20the%20category%0Ato%20which%20they%20belong.%20When%20the%20class%20values%20are%20ordinal%2C%20classifiers%20that%20take%0Athis%20into%20account%20can%20perform%20better%20than%20nominal%20classifiers.%20Time%20Series%0AOrdinal%20Classification%20%28TSOC%29%20is%20the%20field%20covering%20this%20gap%2C%20yet%20unexplored%20in%0Athe%20literature.%20There%20are%20a%20wide%20range%20of%20time%20series%20problems%20showing%20an%0Aordered%20label%20structure%2C%20and%20TSC%20techniques%20that%20ignore%20the%20order%20relationship%0Adiscard%20useful%20information.%20Hence%2C%20this%20paper%20presents%20a%20first%20benchmarking%20of%0ATSOC%20methodologies%2C%20exploiting%20the%20ordering%20of%20the%20target%20labels%20to%20boost%20the%0Aperformance%20of%20current%20TSC%20state-of-the-art.%20Both%20convolutional-%20and%20deep%0Alearning-based%20methodologies%20%28among%20the%20best%20performing%20alternatives%20for%0Anominal%20TSC%29%20are%20adapted%20for%20TSOC.%20For%20the%20experiments%2C%20a%20selection%20of%2029%0Aordinal%20problems%20from%20two%20well-known%20archives%20has%20been%20made.%20In%20this%20way%2C%20this%0Apaper%20contributes%20to%20the%20establishment%20of%20the%20state-of-the-art%20in%20TSOC.%20The%0Aresults%20obtained%20by%20ordinal%20versions%20are%20found%20to%20be%20significantly%20better%20than%0Acurrent%20nominal%20TSC%20techniques%20in%20terms%20of%20ordinal%20performance%20metrics%2C%0Aoutlining%20the%20importance%20of%20considering%20the%20ordering%20of%20the%20labels%20when%20dealing%0Awith%20this%20kind%20of%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10084v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520and%2520Deep%2520Learning%2520based%2520techniques%2520for%2520Time%2520Series%2520Ordinal%250A%2520%2520Classification%26entry.906535625%3DRafael%2520Ayll%25C3%25B3n-Gavil%25C3%25A1n%2520and%2520David%2520Guijo-Rubio%2520and%2520Pedro%2520Antonio%2520Guti%25C3%25A9rrez%2520and%2520Anthony%2520Bagnall%2520and%2520C%25C3%25A9sar%2520Herv%25C3%25A1s-Mart%25C3%25ADnez%26entry.1292438233%3D%2520%2520Time%2520Series%2520Classification%2520%2528TSC%2529%2520covers%2520the%2520supervised%2520learning%2520problem%2520where%250Ainput%2520data%2520is%2520provided%2520in%2520the%2520form%2520of%2520series%2520of%2520values%2520observed%2520through%250Arepeated%2520measurements%2520over%2520time%252C%2520and%2520whose%2520objective%2520is%2520to%2520predict%2520the%2520category%250Ato%2520which%2520they%2520belong.%2520When%2520the%2520class%2520values%2520are%2520ordinal%252C%2520classifiers%2520that%2520take%250Athis%2520into%2520account%2520can%2520perform%2520better%2520than%2520nominal%2520classifiers.%2520Time%2520Series%250AOrdinal%2520Classification%2520%2528TSOC%2529%2520is%2520the%2520field%2520covering%2520this%2520gap%252C%2520yet%2520unexplored%2520in%250Athe%2520literature.%2520There%2520are%2520a%2520wide%2520range%2520of%2520time%2520series%2520problems%2520showing%2520an%250Aordered%2520label%2520structure%252C%2520and%2520TSC%2520techniques%2520that%2520ignore%2520the%2520order%2520relationship%250Adiscard%2520useful%2520information.%2520Hence%252C%2520this%2520paper%2520presents%2520a%2520first%2520benchmarking%2520of%250ATSOC%2520methodologies%252C%2520exploiting%2520the%2520ordering%2520of%2520the%2520target%2520labels%2520to%2520boost%2520the%250Aperformance%2520of%2520current%2520TSC%2520state-of-the-art.%2520Both%2520convolutional-%2520and%2520deep%250Alearning-based%2520methodologies%2520%2528among%2520the%2520best%2520performing%2520alternatives%2520for%250Anominal%2520TSC%2529%2520are%2520adapted%2520for%2520TSOC.%2520For%2520the%2520experiments%252C%2520a%2520selection%2520of%252029%250Aordinal%2520problems%2520from%2520two%2520well-known%2520archives%2520has%2520been%2520made.%2520In%2520this%2520way%252C%2520this%250Apaper%2520contributes%2520to%2520the%2520establishment%2520of%2520the%2520state-of-the-art%2520in%2520TSOC.%2520The%250Aresults%2520obtained%2520by%2520ordinal%2520versions%2520are%2520found%2520to%2520be%2520significantly%2520better%2520than%250Acurrent%2520nominal%2520TSC%2520techniques%2520in%2520terms%2520of%2520ordinal%2520performance%2520metrics%252C%250Aoutlining%2520the%2520importance%2520of%2520considering%2520the%2520ordering%2520of%2520the%2520labels%2520when%2520dealing%250Awith%2520this%2520kind%2520of%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10084v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20and%20Deep%20Learning%20based%20techniques%20for%20Time%20Series%20Ordinal%0A%20%20Classification&entry.906535625=Rafael%20Ayll%C3%B3n-Gavil%C3%A1n%20and%20David%20Guijo-Rubio%20and%20Pedro%20Antonio%20Guti%C3%A9rrez%20and%20Anthony%20Bagnall%20and%20C%C3%A9sar%20Herv%C3%A1s-Mart%C3%ADnez&entry.1292438233=%20%20Time%20Series%20Classification%20%28TSC%29%20covers%20the%20supervised%20learning%20problem%20where%0Ainput%20data%20is%20provided%20in%20the%20form%20of%20series%20of%20values%20observed%20through%0Arepeated%20measurements%20over%20time%2C%20and%20whose%20objective%20is%20to%20predict%20the%20category%0Ato%20which%20they%20belong.%20When%20the%20class%20values%20are%20ordinal%2C%20classifiers%20that%20take%0Athis%20into%20account%20can%20perform%20better%20than%20nominal%20classifiers.%20Time%20Series%0AOrdinal%20Classification%20%28TSOC%29%20is%20the%20field%20covering%20this%20gap%2C%20yet%20unexplored%20in%0Athe%20literature.%20There%20are%20a%20wide%20range%20of%20time%20series%20problems%20showing%20an%0Aordered%20label%20structure%2C%20and%20TSC%20techniques%20that%20ignore%20the%20order%20relationship%0Adiscard%20useful%20information.%20Hence%2C%20this%20paper%20presents%20a%20first%20benchmarking%20of%0ATSOC%20methodologies%2C%20exploiting%20the%20ordering%20of%20the%20target%20labels%20to%20boost%20the%0Aperformance%20of%20current%20TSC%20state-of-the-art.%20Both%20convolutional-%20and%20deep%0Alearning-based%20methodologies%20%28among%20the%20best%20performing%20alternatives%20for%0Anominal%20TSC%29%20are%20adapted%20for%20TSOC.%20For%20the%20experiments%2C%20a%20selection%20of%2029%0Aordinal%20problems%20from%20two%20well-known%20archives%20has%20been%20made.%20In%20this%20way%2C%20this%0Apaper%20contributes%20to%20the%20establishment%20of%20the%20state-of-the-art%20in%20TSOC.%20The%0Aresults%20obtained%20by%20ordinal%20versions%20are%20found%20to%20be%20significantly%20better%20than%0Acurrent%20nominal%20TSC%20techniques%20in%20terms%20of%20ordinal%20performance%20metrics%2C%0Aoutlining%20the%20importance%20of%20considering%20the%20ordering%20of%20the%20labels%20when%20dealing%0Awith%20this%20kind%20of%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10084v3&entry.124074799=Read"},
{"title": "Towards Generalist Robot Learning from Internet Video: A Survey", "author": "Robert McCarthy and Daniel C. H. Tan and Dominik Schmidt and Fernando Acero and Nathan Herr and Yilun Du and Thomas G. Thuruthel and Zhibin Li", "abstract": "  Scaling deep learning to massive, diverse internet data has yielded\nremarkably general capabilities in visual and natural language understanding\nand generation. However, data has remained scarce and challenging to collect in\nrobotics, seeing robot learning struggle to obtain similarly general\ncapabilities. Promising Learning from Videos (LfV) methods aim to address the\nrobotics data bottleneck by augmenting traditional robot data with large-scale\ninternet video data. This video data offers broad foundational information\nregarding physical behaviour and the underlying physics of the world, and thus\ncan be highly informative for a generalist robot.\n  In this survey, we present a thorough overview of the emerging field of LfV.\nWe outline fundamental concepts, including the benefits and challenges of LfV.\nWe provide a comprehensive review of current methods for extracting knowledge\nfrom large-scale internet video, addressing key challenges in LfV, and boosting\ndownstream robot and reinforcement learning via the use of video data. The\nsurvey concludes with a critical discussion of challenges and opportunities in\nLfV. Here, we advocate for scalable foundation model approaches that can\nleverage the full range of available internet video to improve the learning of\nrobot policies and dynamics models. We hope this survey can inform and catalyse\nfurther LfV research, driving progress towards the development of\ngeneral-purpose robots.\n", "link": "http://arxiv.org/abs/2404.19664v4", "date": "2024-11-12", "relevancy": 2.1695, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&body=Title%3A%20Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey%0AAuthor%3A%20Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Scaling%20deep%20learning%20to%20massive%2C%20diverse%20internet%20data%20has%20yielded%0Aremarkably%20general%20capabilities%20in%20visual%20and%20natural%20language%20understanding%0Aand%20generation.%20However%2C%20data%20has%20remained%20scarce%20and%20challenging%20to%20collect%20in%0Arobotics%2C%20seeing%20robot%20learning%20struggle%20to%20obtain%20similarly%20general%0Acapabilities.%20Promising%20Learning%20from%20Videos%20%28LfV%29%20methods%20aim%20to%20address%20the%0Arobotics%20data%20bottleneck%20by%20augmenting%20traditional%20robot%20data%20with%20large-scale%0Ainternet%20video%20data.%20This%20video%20data%20offers%20broad%20foundational%20information%0Aregarding%20physical%20behaviour%20and%20the%20underlying%20physics%20of%20the%20world%2C%20and%20thus%0Acan%20be%20highly%20informative%20for%20a%20generalist%20robot.%0A%20%20In%20this%20survey%2C%20we%20present%20a%20thorough%20overview%20of%20the%20emerging%20field%20of%20LfV.%0AWe%20outline%20fundamental%20concepts%2C%20including%20the%20benefits%20and%20challenges%20of%20LfV.%0AWe%20provide%20a%20comprehensive%20review%20of%20current%20methods%20for%20extracting%20knowledge%0Afrom%20large-scale%20internet%20video%2C%20addressing%20key%20challenges%20in%20LfV%2C%20and%20boosting%0Adownstream%20robot%20and%20reinforcement%20learning%20via%20the%20use%20of%20video%20data.%20The%0Asurvey%20concludes%20with%20a%20critical%20discussion%20of%20challenges%20and%20opportunities%20in%0ALfV.%20Here%2C%20we%20advocate%20for%20scalable%20foundation%20model%20approaches%20that%20can%0Aleverage%20the%20full%20range%20of%20available%20internet%20video%20to%20improve%20the%20learning%20of%0Arobot%20policies%20and%20dynamics%20models.%20We%20hope%20this%20survey%20can%20inform%20and%20catalyse%0Afurther%20LfV%20research%2C%20driving%20progress%20towards%20the%20development%20of%0Ageneral-purpose%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19664v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Robot%2520Learning%2520from%2520Internet%2520Video%253A%2520A%2520Survey%26entry.906535625%3DRobert%2520McCarthy%2520and%2520Daniel%2520C.%2520H.%2520Tan%2520and%2520Dominik%2520Schmidt%2520and%2520Fernando%2520Acero%2520and%2520Nathan%2520Herr%2520and%2520Yilun%2520Du%2520and%2520Thomas%2520G.%2520Thuruthel%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Scaling%2520deep%2520learning%2520to%2520massive%252C%2520diverse%2520internet%2520data%2520has%2520yielded%250Aremarkably%2520general%2520capabilities%2520in%2520visual%2520and%2520natural%2520language%2520understanding%250Aand%2520generation.%2520However%252C%2520data%2520has%2520remained%2520scarce%2520and%2520challenging%2520to%2520collect%2520in%250Arobotics%252C%2520seeing%2520robot%2520learning%2520struggle%2520to%2520obtain%2520similarly%2520general%250Acapabilities.%2520Promising%2520Learning%2520from%2520Videos%2520%2528LfV%2529%2520methods%2520aim%2520to%2520address%2520the%250Arobotics%2520data%2520bottleneck%2520by%2520augmenting%2520traditional%2520robot%2520data%2520with%2520large-scale%250Ainternet%2520video%2520data.%2520This%2520video%2520data%2520offers%2520broad%2520foundational%2520information%250Aregarding%2520physical%2520behaviour%2520and%2520the%2520underlying%2520physics%2520of%2520the%2520world%252C%2520and%2520thus%250Acan%2520be%2520highly%2520informative%2520for%2520a%2520generalist%2520robot.%250A%2520%2520In%2520this%2520survey%252C%2520we%2520present%2520a%2520thorough%2520overview%2520of%2520the%2520emerging%2520field%2520of%2520LfV.%250AWe%2520outline%2520fundamental%2520concepts%252C%2520including%2520the%2520benefits%2520and%2520challenges%2520of%2520LfV.%250AWe%2520provide%2520a%2520comprehensive%2520review%2520of%2520current%2520methods%2520for%2520extracting%2520knowledge%250Afrom%2520large-scale%2520internet%2520video%252C%2520addressing%2520key%2520challenges%2520in%2520LfV%252C%2520and%2520boosting%250Adownstream%2520robot%2520and%2520reinforcement%2520learning%2520via%2520the%2520use%2520of%2520video%2520data.%2520The%250Asurvey%2520concludes%2520with%2520a%2520critical%2520discussion%2520of%2520challenges%2520and%2520opportunities%2520in%250ALfV.%2520Here%252C%2520we%2520advocate%2520for%2520scalable%2520foundation%2520model%2520approaches%2520that%2520can%250Aleverage%2520the%2520full%2520range%2520of%2520available%2520internet%2520video%2520to%2520improve%2520the%2520learning%2520of%250Arobot%2520policies%2520and%2520dynamics%2520models.%2520We%2520hope%2520this%2520survey%2520can%2520inform%2520and%2520catalyse%250Afurther%2520LfV%2520research%252C%2520driving%2520progress%2520towards%2520the%2520development%2520of%250Ageneral-purpose%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19664v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Learning%20from%20Internet%20Video%3A%20A%20Survey&entry.906535625=Robert%20McCarthy%20and%20Daniel%20C.%20H.%20Tan%20and%20Dominik%20Schmidt%20and%20Fernando%20Acero%20and%20Nathan%20Herr%20and%20Yilun%20Du%20and%20Thomas%20G.%20Thuruthel%20and%20Zhibin%20Li&entry.1292438233=%20%20Scaling%20deep%20learning%20to%20massive%2C%20diverse%20internet%20data%20has%20yielded%0Aremarkably%20general%20capabilities%20in%20visual%20and%20natural%20language%20understanding%0Aand%20generation.%20However%2C%20data%20has%20remained%20scarce%20and%20challenging%20to%20collect%20in%0Arobotics%2C%20seeing%20robot%20learning%20struggle%20to%20obtain%20similarly%20general%0Acapabilities.%20Promising%20Learning%20from%20Videos%20%28LfV%29%20methods%20aim%20to%20address%20the%0Arobotics%20data%20bottleneck%20by%20augmenting%20traditional%20robot%20data%20with%20large-scale%0Ainternet%20video%20data.%20This%20video%20data%20offers%20broad%20foundational%20information%0Aregarding%20physical%20behaviour%20and%20the%20underlying%20physics%20of%20the%20world%2C%20and%20thus%0Acan%20be%20highly%20informative%20for%20a%20generalist%20robot.%0A%20%20In%20this%20survey%2C%20we%20present%20a%20thorough%20overview%20of%20the%20emerging%20field%20of%20LfV.%0AWe%20outline%20fundamental%20concepts%2C%20including%20the%20benefits%20and%20challenges%20of%20LfV.%0AWe%20provide%20a%20comprehensive%20review%20of%20current%20methods%20for%20extracting%20knowledge%0Afrom%20large-scale%20internet%20video%2C%20addressing%20key%20challenges%20in%20LfV%2C%20and%20boosting%0Adownstream%20robot%20and%20reinforcement%20learning%20via%20the%20use%20of%20video%20data.%20The%0Asurvey%20concludes%20with%20a%20critical%20discussion%20of%20challenges%20and%20opportunities%20in%0ALfV.%20Here%2C%20we%20advocate%20for%20scalable%20foundation%20model%20approaches%20that%20can%0Aleverage%20the%20full%20range%20of%20available%20internet%20video%20to%20improve%20the%20learning%20of%0Arobot%20policies%20and%20dynamics%20models.%20We%20hope%20this%20survey%20can%20inform%20and%20catalyse%0Afurther%20LfV%20research%2C%20driving%20progress%20towards%20the%20development%20of%0Ageneral-purpose%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19664v4&entry.124074799=Read"},
{"title": "Efficient Hamiltonian, structure and trace distance learning of Gaussian\n  states", "author": "Marco Fanizza and Cambyse Rouz\u00e9 and Daniel Stilck Fran\u00e7a", "abstract": "  In this work, we initiate the study of Hamiltonian learning for positive\ntemperature bosonic Gaussian states, the quantum generalization of the widely\nstudied problem of learning Gaussian graphical models. We obtain efficient\nprotocols, both in sample and computational complexity, for the task of\ninferring the parameters of their underlying quadratic Hamiltonian under the\nassumption of bounded temperature, squeezing, displacement and maximal degree\nof the interaction graph. Our protocol only requires heterodyne measurements,\nwhich are often experimentally feasible, and has a sample complexity that\nscales logarithmically with the number of modes. Furthermore, we show that it\nis possible to learn the underlying interaction graph in a similar setting and\nsample complexity. Taken together, our results put the status of the quantum\nHamiltonian learning problem for continuous variable systems in a much more\nadvanced state when compared to spins, where state-of-the-art results are\neither unavailable or quantitatively inferior to ours. In addition, we use our\ntechniques to obtain the first results on learning Gaussian states in trace\ndistance with a quadratic scaling in precision and polynomial in the number of\nmodes, albeit imposing certain restrictions on the Gaussian states. Our main\ntechnical innovations are several continuity bounds for the covariance and\nHamiltonian matrix of a Gaussian state, which are of independent interest,\ncombined with what we call the local inversion technique. In essence, the local\ninversion technique allows us to reliably infer the Hamiltonian of a Gaussian\nstate by only estimating in parallel submatrices of the covariance matrix whose\nsize scales with the desired precision, but not the number of modes. This way\nwe bypass the need to obtain precise global estimates of the covariance matrix,\ncontrolling the sample complexity.\n", "link": "http://arxiv.org/abs/2411.03163v2", "date": "2024-11-12", "relevancy": 2.166, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4546}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4227}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states&body=Title%3A%20Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states%0AAuthor%3A%20Marco%20Fanizza%20and%20Cambyse%20Rouz%C3%A9%20and%20Daniel%20Stilck%20Fran%C3%A7a%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20initiate%20the%20study%20of%20Hamiltonian%20learning%20for%20positive%0Atemperature%20bosonic%20Gaussian%20states%2C%20the%20quantum%20generalization%20of%20the%20widely%0Astudied%20problem%20of%20learning%20Gaussian%20graphical%20models.%20We%20obtain%20efficient%0Aprotocols%2C%20both%20in%20sample%20and%20computational%20complexity%2C%20for%20the%20task%20of%0Ainferring%20the%20parameters%20of%20their%20underlying%20quadratic%20Hamiltonian%20under%20the%0Aassumption%20of%20bounded%20temperature%2C%20squeezing%2C%20displacement%20and%20maximal%20degree%0Aof%20the%20interaction%20graph.%20Our%20protocol%20only%20requires%20heterodyne%20measurements%2C%0Awhich%20are%20often%20experimentally%20feasible%2C%20and%20has%20a%20sample%20complexity%20that%0Ascales%20logarithmically%20with%20the%20number%20of%20modes.%20Furthermore%2C%20we%20show%20that%20it%0Ais%20possible%20to%20learn%20the%20underlying%20interaction%20graph%20in%20a%20similar%20setting%20and%0Asample%20complexity.%20Taken%20together%2C%20our%20results%20put%20the%20status%20of%20the%20quantum%0AHamiltonian%20learning%20problem%20for%20continuous%20variable%20systems%20in%20a%20much%20more%0Aadvanced%20state%20when%20compared%20to%20spins%2C%20where%20state-of-the-art%20results%20are%0Aeither%20unavailable%20or%20quantitatively%20inferior%20to%20ours.%20In%20addition%2C%20we%20use%20our%0Atechniques%20to%20obtain%20the%20first%20results%20on%20learning%20Gaussian%20states%20in%20trace%0Adistance%20with%20a%20quadratic%20scaling%20in%20precision%20and%20polynomial%20in%20the%20number%20of%0Amodes%2C%20albeit%20imposing%20certain%20restrictions%20on%20the%20Gaussian%20states.%20Our%20main%0Atechnical%20innovations%20are%20several%20continuity%20bounds%20for%20the%20covariance%20and%0AHamiltonian%20matrix%20of%20a%20Gaussian%20state%2C%20which%20are%20of%20independent%20interest%2C%0Acombined%20with%20what%20we%20call%20the%20local%20inversion%20technique.%20In%20essence%2C%20the%20local%0Ainversion%20technique%20allows%20us%20to%20reliably%20infer%20the%20Hamiltonian%20of%20a%20Gaussian%0Astate%20by%20only%20estimating%20in%20parallel%20submatrices%20of%20the%20covariance%20matrix%20whose%0Asize%20scales%20with%20the%20desired%20precision%2C%20but%20not%20the%20number%20of%20modes.%20This%20way%0Awe%20bypass%20the%20need%20to%20obtain%20precise%20global%20estimates%20of%20the%20covariance%20matrix%2C%0Acontrolling%20the%20sample%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Hamiltonian%252C%2520structure%2520and%2520trace%2520distance%2520learning%2520of%2520Gaussian%250A%2520%2520states%26entry.906535625%3DMarco%2520Fanizza%2520and%2520Cambyse%2520Rouz%25C3%25A9%2520and%2520Daniel%2520Stilck%2520Fran%25C3%25A7a%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520initiate%2520the%2520study%2520of%2520Hamiltonian%2520learning%2520for%2520positive%250Atemperature%2520bosonic%2520Gaussian%2520states%252C%2520the%2520quantum%2520generalization%2520of%2520the%2520widely%250Astudied%2520problem%2520of%2520learning%2520Gaussian%2520graphical%2520models.%2520We%2520obtain%2520efficient%250Aprotocols%252C%2520both%2520in%2520sample%2520and%2520computational%2520complexity%252C%2520for%2520the%2520task%2520of%250Ainferring%2520the%2520parameters%2520of%2520their%2520underlying%2520quadratic%2520Hamiltonian%2520under%2520the%250Aassumption%2520of%2520bounded%2520temperature%252C%2520squeezing%252C%2520displacement%2520and%2520maximal%2520degree%250Aof%2520the%2520interaction%2520graph.%2520Our%2520protocol%2520only%2520requires%2520heterodyne%2520measurements%252C%250Awhich%2520are%2520often%2520experimentally%2520feasible%252C%2520and%2520has%2520a%2520sample%2520complexity%2520that%250Ascales%2520logarithmically%2520with%2520the%2520number%2520of%2520modes.%2520Furthermore%252C%2520we%2520show%2520that%2520it%250Ais%2520possible%2520to%2520learn%2520the%2520underlying%2520interaction%2520graph%2520in%2520a%2520similar%2520setting%2520and%250Asample%2520complexity.%2520Taken%2520together%252C%2520our%2520results%2520put%2520the%2520status%2520of%2520the%2520quantum%250AHamiltonian%2520learning%2520problem%2520for%2520continuous%2520variable%2520systems%2520in%2520a%2520much%2520more%250Aadvanced%2520state%2520when%2520compared%2520to%2520spins%252C%2520where%2520state-of-the-art%2520results%2520are%250Aeither%2520unavailable%2520or%2520quantitatively%2520inferior%2520to%2520ours.%2520In%2520addition%252C%2520we%2520use%2520our%250Atechniques%2520to%2520obtain%2520the%2520first%2520results%2520on%2520learning%2520Gaussian%2520states%2520in%2520trace%250Adistance%2520with%2520a%2520quadratic%2520scaling%2520in%2520precision%2520and%2520polynomial%2520in%2520the%2520number%2520of%250Amodes%252C%2520albeit%2520imposing%2520certain%2520restrictions%2520on%2520the%2520Gaussian%2520states.%2520Our%2520main%250Atechnical%2520innovations%2520are%2520several%2520continuity%2520bounds%2520for%2520the%2520covariance%2520and%250AHamiltonian%2520matrix%2520of%2520a%2520Gaussian%2520state%252C%2520which%2520are%2520of%2520independent%2520interest%252C%250Acombined%2520with%2520what%2520we%2520call%2520the%2520local%2520inversion%2520technique.%2520In%2520essence%252C%2520the%2520local%250Ainversion%2520technique%2520allows%2520us%2520to%2520reliably%2520infer%2520the%2520Hamiltonian%2520of%2520a%2520Gaussian%250Astate%2520by%2520only%2520estimating%2520in%2520parallel%2520submatrices%2520of%2520the%2520covariance%2520matrix%2520whose%250Asize%2520scales%2520with%2520the%2520desired%2520precision%252C%2520but%2520not%2520the%2520number%2520of%2520modes.%2520This%2520way%250Awe%2520bypass%2520the%2520need%2520to%2520obtain%2520precise%2520global%2520estimates%2520of%2520the%2520covariance%2520matrix%252C%250Acontrolling%2520the%2520sample%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Hamiltonian%2C%20structure%20and%20trace%20distance%20learning%20of%20Gaussian%0A%20%20states&entry.906535625=Marco%20Fanizza%20and%20Cambyse%20Rouz%C3%A9%20and%20Daniel%20Stilck%20Fran%C3%A7a&entry.1292438233=%20%20In%20this%20work%2C%20we%20initiate%20the%20study%20of%20Hamiltonian%20learning%20for%20positive%0Atemperature%20bosonic%20Gaussian%20states%2C%20the%20quantum%20generalization%20of%20the%20widely%0Astudied%20problem%20of%20learning%20Gaussian%20graphical%20models.%20We%20obtain%20efficient%0Aprotocols%2C%20both%20in%20sample%20and%20computational%20complexity%2C%20for%20the%20task%20of%0Ainferring%20the%20parameters%20of%20their%20underlying%20quadratic%20Hamiltonian%20under%20the%0Aassumption%20of%20bounded%20temperature%2C%20squeezing%2C%20displacement%20and%20maximal%20degree%0Aof%20the%20interaction%20graph.%20Our%20protocol%20only%20requires%20heterodyne%20measurements%2C%0Awhich%20are%20often%20experimentally%20feasible%2C%20and%20has%20a%20sample%20complexity%20that%0Ascales%20logarithmically%20with%20the%20number%20of%20modes.%20Furthermore%2C%20we%20show%20that%20it%0Ais%20possible%20to%20learn%20the%20underlying%20interaction%20graph%20in%20a%20similar%20setting%20and%0Asample%20complexity.%20Taken%20together%2C%20our%20results%20put%20the%20status%20of%20the%20quantum%0AHamiltonian%20learning%20problem%20for%20continuous%20variable%20systems%20in%20a%20much%20more%0Aadvanced%20state%20when%20compared%20to%20spins%2C%20where%20state-of-the-art%20results%20are%0Aeither%20unavailable%20or%20quantitatively%20inferior%20to%20ours.%20In%20addition%2C%20we%20use%20our%0Atechniques%20to%20obtain%20the%20first%20results%20on%20learning%20Gaussian%20states%20in%20trace%0Adistance%20with%20a%20quadratic%20scaling%20in%20precision%20and%20polynomial%20in%20the%20number%20of%0Amodes%2C%20albeit%20imposing%20certain%20restrictions%20on%20the%20Gaussian%20states.%20Our%20main%0Atechnical%20innovations%20are%20several%20continuity%20bounds%20for%20the%20covariance%20and%0AHamiltonian%20matrix%20of%20a%20Gaussian%20state%2C%20which%20are%20of%20independent%20interest%2C%0Acombined%20with%20what%20we%20call%20the%20local%20inversion%20technique.%20In%20essence%2C%20the%20local%0Ainversion%20technique%20allows%20us%20to%20reliably%20infer%20the%20Hamiltonian%20of%20a%20Gaussian%0Astate%20by%20only%20estimating%20in%20parallel%20submatrices%20of%20the%20covariance%20matrix%20whose%0Asize%20scales%20with%20the%20desired%20precision%2C%20but%20not%20the%20number%20of%20modes.%20This%20way%0Awe%20bypass%20the%20need%20to%20obtain%20precise%20global%20estimates%20of%20the%20covariance%20matrix%2C%0Acontrolling%20the%20sample%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03163v2&entry.124074799=Read"},
{"title": "Efficient Federated Finetuning of Tiny Transformers with\n  Resource-Constrained Devices", "author": "Kilian Pfeiffer and Mohamed Aboelenien Ahmed and Ramin Khalili and J\u00f6rg Henkel", "abstract": "  In recent years, Large Language Models (LLMs) through Transformer structures\nhave dominated many machine learning tasks, especially text processing.\nHowever, these models require massive amounts of data for training and induce\nhigh resource requirements, particularly in terms of the large number of\nFloating Point Operations (FLOPs) and the high amounts of memory needed. To\nfine-tune such a model in a parameter-efficient way, techniques like Adapter or\nLoRA have been developed. However, we observe that the application of LoRA,\nwhen used in federated learning (FL), while still being parameter-efficient, is\nmemory and FLOP inefficient. Based on that observation, we develop a novel\nlayer finetuning scheme that allows devices in cross-device FL to make use of\npretrained neural networks (NNs) while adhering to given resource constraints.\nWe show that our presented scheme outperforms the current state of the art when\ndealing with homogeneous or heterogeneous computation and memory constraints\nand is on par with LoRA regarding limited communication, thereby achieving\nsignificantly higher accuracies in FL training.\n", "link": "http://arxiv.org/abs/2411.07826v1", "date": "2024-11-12", "relevancy": 2.1282, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5475}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5325}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Federated%20Finetuning%20of%20Tiny%20Transformers%20with%0A%20%20Resource-Constrained%20Devices&body=Title%3A%20Efficient%20Federated%20Finetuning%20of%20Tiny%20Transformers%20with%0A%20%20Resource-Constrained%20Devices%0AAuthor%3A%20Kilian%20Pfeiffer%20and%20Mohamed%20Aboelenien%20Ahmed%20and%20Ramin%20Khalili%20and%20J%C3%B6rg%20Henkel%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20through%20Transformer%20structures%0Ahave%20dominated%20many%20machine%20learning%20tasks%2C%20especially%20text%20processing.%0AHowever%2C%20these%20models%20require%20massive%20amounts%20of%20data%20for%20training%20and%20induce%0Ahigh%20resource%20requirements%2C%20particularly%20in%20terms%20of%20the%20large%20number%20of%0AFloating%20Point%20Operations%20%28FLOPs%29%20and%20the%20high%20amounts%20of%20memory%20needed.%20To%0Afine-tune%20such%20a%20model%20in%20a%20parameter-efficient%20way%2C%20techniques%20like%20Adapter%20or%0ALoRA%20have%20been%20developed.%20However%2C%20we%20observe%20that%20the%20application%20of%20LoRA%2C%0Awhen%20used%20in%20federated%20learning%20%28FL%29%2C%20while%20still%20being%20parameter-efficient%2C%20is%0Amemory%20and%20FLOP%20inefficient.%20Based%20on%20that%20observation%2C%20we%20develop%20a%20novel%0Alayer%20finetuning%20scheme%20that%20allows%20devices%20in%20cross-device%20FL%20to%20make%20use%20of%0Apretrained%20neural%20networks%20%28NNs%29%20while%20adhering%20to%20given%20resource%20constraints.%0AWe%20show%20that%20our%20presented%20scheme%20outperforms%20the%20current%20state%20of%20the%20art%20when%0Adealing%20with%20homogeneous%20or%20heterogeneous%20computation%20and%20memory%20constraints%0Aand%20is%20on%20par%20with%20LoRA%20regarding%20limited%20communication%2C%20thereby%20achieving%0Asignificantly%20higher%20accuracies%20in%20FL%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Federated%2520Finetuning%2520of%2520Tiny%2520Transformers%2520with%250A%2520%2520Resource-Constrained%2520Devices%26entry.906535625%3DKilian%2520Pfeiffer%2520and%2520Mohamed%2520Aboelenien%2520Ahmed%2520and%2520Ramin%2520Khalili%2520and%2520J%25C3%25B6rg%2520Henkel%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520through%2520Transformer%2520structures%250Ahave%2520dominated%2520many%2520machine%2520learning%2520tasks%252C%2520especially%2520text%2520processing.%250AHowever%252C%2520these%2520models%2520require%2520massive%2520amounts%2520of%2520data%2520for%2520training%2520and%2520induce%250Ahigh%2520resource%2520requirements%252C%2520particularly%2520in%2520terms%2520of%2520the%2520large%2520number%2520of%250AFloating%2520Point%2520Operations%2520%2528FLOPs%2529%2520and%2520the%2520high%2520amounts%2520of%2520memory%2520needed.%2520To%250Afine-tune%2520such%2520a%2520model%2520in%2520a%2520parameter-efficient%2520way%252C%2520techniques%2520like%2520Adapter%2520or%250ALoRA%2520have%2520been%2520developed.%2520However%252C%2520we%2520observe%2520that%2520the%2520application%2520of%2520LoRA%252C%250Awhen%2520used%2520in%2520federated%2520learning%2520%2528FL%2529%252C%2520while%2520still%2520being%2520parameter-efficient%252C%2520is%250Amemory%2520and%2520FLOP%2520inefficient.%2520Based%2520on%2520that%2520observation%252C%2520we%2520develop%2520a%2520novel%250Alayer%2520finetuning%2520scheme%2520that%2520allows%2520devices%2520in%2520cross-device%2520FL%2520to%2520make%2520use%2520of%250Apretrained%2520neural%2520networks%2520%2528NNs%2529%2520while%2520adhering%2520to%2520given%2520resource%2520constraints.%250AWe%2520show%2520that%2520our%2520presented%2520scheme%2520outperforms%2520the%2520current%2520state%2520of%2520the%2520art%2520when%250Adealing%2520with%2520homogeneous%2520or%2520heterogeneous%2520computation%2520and%2520memory%2520constraints%250Aand%2520is%2520on%2520par%2520with%2520LoRA%2520regarding%2520limited%2520communication%252C%2520thereby%2520achieving%250Asignificantly%2520higher%2520accuracies%2520in%2520FL%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Federated%20Finetuning%20of%20Tiny%20Transformers%20with%0A%20%20Resource-Constrained%20Devices&entry.906535625=Kilian%20Pfeiffer%20and%20Mohamed%20Aboelenien%20Ahmed%20and%20Ramin%20Khalili%20and%20J%C3%B6rg%20Henkel&entry.1292438233=%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20through%20Transformer%20structures%0Ahave%20dominated%20many%20machine%20learning%20tasks%2C%20especially%20text%20processing.%0AHowever%2C%20these%20models%20require%20massive%20amounts%20of%20data%20for%20training%20and%20induce%0Ahigh%20resource%20requirements%2C%20particularly%20in%20terms%20of%20the%20large%20number%20of%0AFloating%20Point%20Operations%20%28FLOPs%29%20and%20the%20high%20amounts%20of%20memory%20needed.%20To%0Afine-tune%20such%20a%20model%20in%20a%20parameter-efficient%20way%2C%20techniques%20like%20Adapter%20or%0ALoRA%20have%20been%20developed.%20However%2C%20we%20observe%20that%20the%20application%20of%20LoRA%2C%0Awhen%20used%20in%20federated%20learning%20%28FL%29%2C%20while%20still%20being%20parameter-efficient%2C%20is%0Amemory%20and%20FLOP%20inefficient.%20Based%20on%20that%20observation%2C%20we%20develop%20a%20novel%0Alayer%20finetuning%20scheme%20that%20allows%20devices%20in%20cross-device%20FL%20to%20make%20use%20of%0Apretrained%20neural%20networks%20%28NNs%29%20while%20adhering%20to%20given%20resource%20constraints.%0AWe%20show%20that%20our%20presented%20scheme%20outperforms%20the%20current%20state%20of%20the%20art%20when%0Adealing%20with%20homogeneous%20or%20heterogeneous%20computation%20and%20memory%20constraints%0Aand%20is%20on%20par%20with%20LoRA%20regarding%20limited%20communication%2C%20thereby%20achieving%0Asignificantly%20higher%20accuracies%20in%20FL%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07826v1&entry.124074799=Read"},
{"title": "TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical\n  Phase Recognition", "author": "Isabel Funke and Dominik Rivoir and Stefanie Krell and Stefanie Speidel", "abstract": "  To enable context-aware computer assistance in the operating room of the\nfuture, cognitive systems need to understand automatically which surgical phase\nis being performed by the medical team. The primary source of information for\nsurgical phase recognition is typically video, which presents two challenges:\nextracting meaningful features from the video stream and effectively modeling\ntemporal information in the sequence of visual features. For temporal modeling,\nattention mechanisms have gained popularity due to their ability to capture\nlong-range dependencies. In this paper, we explore design choices for attention\nin existing temporal models for surgical phase recognition and propose a novel\napproach that uses attention more effectively and does not require hand-crafted\nconstraints: TUNeS, an efficient and simple temporal model that incorporates\nself-attention at the core of a convolutional U-Net structure. In addition, we\npropose to train the feature extractor, a standard CNN, together with an LSTM\non preferably long video segments, i.e., with long temporal context. In our\nexperiments, almost all temporal models performed better on top of feature\nextractors that were trained with longer temporal context. On these\ncontextualized features, TUNeS achieves state-of-the-art results on the\nCholec80 dataset. This study offers new insights on how to use attention\nmechanisms to build accurate and efficient temporal models for surgical phase\nrecognition. Implementing automatic surgical phase recognition is essential to\nautomate the analysis and optimization of surgical workflows and to enable\ncontext-aware computer assistance during surgery, thus ultimately improving\npatient care.\n", "link": "http://arxiv.org/abs/2307.09997v5", "date": "2024-11-12", "relevancy": 2.1282, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition&body=Title%3A%20TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition%0AAuthor%3A%20Isabel%20Funke%20and%20Dominik%20Rivoir%20and%20Stefanie%20Krell%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20To%20enable%20context-aware%20computer%20assistance%20in%20the%20operating%20room%20of%20the%0Afuture%2C%20cognitive%20systems%20need%20to%20understand%20automatically%20which%20surgical%20phase%0Ais%20being%20performed%20by%20the%20medical%20team.%20The%20primary%20source%20of%20information%20for%0Asurgical%20phase%20recognition%20is%20typically%20video%2C%20which%20presents%20two%20challenges%3A%0Aextracting%20meaningful%20features%20from%20the%20video%20stream%20and%20effectively%20modeling%0Atemporal%20information%20in%20the%20sequence%20of%20visual%20features.%20For%20temporal%20modeling%2C%0Aattention%20mechanisms%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20capture%0Along-range%20dependencies.%20In%20this%20paper%2C%20we%20explore%20design%20choices%20for%20attention%0Ain%20existing%20temporal%20models%20for%20surgical%20phase%20recognition%20and%20propose%20a%20novel%0Aapproach%20that%20uses%20attention%20more%20effectively%20and%20does%20not%20require%20hand-crafted%0Aconstraints%3A%20TUNeS%2C%20an%20efficient%20and%20simple%20temporal%20model%20that%20incorporates%0Aself-attention%20at%20the%20core%20of%20a%20convolutional%20U-Net%20structure.%20In%20addition%2C%20we%0Apropose%20to%20train%20the%20feature%20extractor%2C%20a%20standard%20CNN%2C%20together%20with%20an%20LSTM%0Aon%20preferably%20long%20video%20segments%2C%20i.e.%2C%20with%20long%20temporal%20context.%20In%20our%0Aexperiments%2C%20almost%20all%20temporal%20models%20performed%20better%20on%20top%20of%20feature%0Aextractors%20that%20were%20trained%20with%20longer%20temporal%20context.%20On%20these%0Acontextualized%20features%2C%20TUNeS%20achieves%20state-of-the-art%20results%20on%20the%0ACholec80%20dataset.%20This%20study%20offers%20new%20insights%20on%20how%20to%20use%20attention%0Amechanisms%20to%20build%20accurate%20and%20efficient%20temporal%20models%20for%20surgical%20phase%0Arecognition.%20Implementing%20automatic%20surgical%20phase%20recognition%20is%20essential%20to%0Aautomate%20the%20analysis%20and%20optimization%20of%20surgical%20workflows%20and%20to%20enable%0Acontext-aware%20computer%20assistance%20during%20surgery%2C%20thus%20ultimately%20improving%0Apatient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09997v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUNeS%253A%2520A%2520Temporal%2520U-Net%2520with%2520Self-Attention%2520for%2520Video-based%2520Surgical%250A%2520%2520Phase%2520Recognition%26entry.906535625%3DIsabel%2520Funke%2520and%2520Dominik%2520Rivoir%2520and%2520Stefanie%2520Krell%2520and%2520Stefanie%2520Speidel%26entry.1292438233%3D%2520%2520To%2520enable%2520context-aware%2520computer%2520assistance%2520in%2520the%2520operating%2520room%2520of%2520the%250Afuture%252C%2520cognitive%2520systems%2520need%2520to%2520understand%2520automatically%2520which%2520surgical%2520phase%250Ais%2520being%2520performed%2520by%2520the%2520medical%2520team.%2520The%2520primary%2520source%2520of%2520information%2520for%250Asurgical%2520phase%2520recognition%2520is%2520typically%2520video%252C%2520which%2520presents%2520two%2520challenges%253A%250Aextracting%2520meaningful%2520features%2520from%2520the%2520video%2520stream%2520and%2520effectively%2520modeling%250Atemporal%2520information%2520in%2520the%2520sequence%2520of%2520visual%2520features.%2520For%2520temporal%2520modeling%252C%250Aattention%2520mechanisms%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520ability%2520to%2520capture%250Along-range%2520dependencies.%2520In%2520this%2520paper%252C%2520we%2520explore%2520design%2520choices%2520for%2520attention%250Ain%2520existing%2520temporal%2520models%2520for%2520surgical%2520phase%2520recognition%2520and%2520propose%2520a%2520novel%250Aapproach%2520that%2520uses%2520attention%2520more%2520effectively%2520and%2520does%2520not%2520require%2520hand-crafted%250Aconstraints%253A%2520TUNeS%252C%2520an%2520efficient%2520and%2520simple%2520temporal%2520model%2520that%2520incorporates%250Aself-attention%2520at%2520the%2520core%2520of%2520a%2520convolutional%2520U-Net%2520structure.%2520In%2520addition%252C%2520we%250Apropose%2520to%2520train%2520the%2520feature%2520extractor%252C%2520a%2520standard%2520CNN%252C%2520together%2520with%2520an%2520LSTM%250Aon%2520preferably%2520long%2520video%2520segments%252C%2520i.e.%252C%2520with%2520long%2520temporal%2520context.%2520In%2520our%250Aexperiments%252C%2520almost%2520all%2520temporal%2520models%2520performed%2520better%2520on%2520top%2520of%2520feature%250Aextractors%2520that%2520were%2520trained%2520with%2520longer%2520temporal%2520context.%2520On%2520these%250Acontextualized%2520features%252C%2520TUNeS%2520achieves%2520state-of-the-art%2520results%2520on%2520the%250ACholec80%2520dataset.%2520This%2520study%2520offers%2520new%2520insights%2520on%2520how%2520to%2520use%2520attention%250Amechanisms%2520to%2520build%2520accurate%2520and%2520efficient%2520temporal%2520models%2520for%2520surgical%2520phase%250Arecognition.%2520Implementing%2520automatic%2520surgical%2520phase%2520recognition%2520is%2520essential%2520to%250Aautomate%2520the%2520analysis%2520and%2520optimization%2520of%2520surgical%2520workflows%2520and%2520to%2520enable%250Acontext-aware%2520computer%2520assistance%2520during%2520surgery%252C%2520thus%2520ultimately%2520improving%250Apatient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09997v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition&entry.906535625=Isabel%20Funke%20and%20Dominik%20Rivoir%20and%20Stefanie%20Krell%20and%20Stefanie%20Speidel&entry.1292438233=%20%20To%20enable%20context-aware%20computer%20assistance%20in%20the%20operating%20room%20of%20the%0Afuture%2C%20cognitive%20systems%20need%20to%20understand%20automatically%20which%20surgical%20phase%0Ais%20being%20performed%20by%20the%20medical%20team.%20The%20primary%20source%20of%20information%20for%0Asurgical%20phase%20recognition%20is%20typically%20video%2C%20which%20presents%20two%20challenges%3A%0Aextracting%20meaningful%20features%20from%20the%20video%20stream%20and%20effectively%20modeling%0Atemporal%20information%20in%20the%20sequence%20of%20visual%20features.%20For%20temporal%20modeling%2C%0Aattention%20mechanisms%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20capture%0Along-range%20dependencies.%20In%20this%20paper%2C%20we%20explore%20design%20choices%20for%20attention%0Ain%20existing%20temporal%20models%20for%20surgical%20phase%20recognition%20and%20propose%20a%20novel%0Aapproach%20that%20uses%20attention%20more%20effectively%20and%20does%20not%20require%20hand-crafted%0Aconstraints%3A%20TUNeS%2C%20an%20efficient%20and%20simple%20temporal%20model%20that%20incorporates%0Aself-attention%20at%20the%20core%20of%20a%20convolutional%20U-Net%20structure.%20In%20addition%2C%20we%0Apropose%20to%20train%20the%20feature%20extractor%2C%20a%20standard%20CNN%2C%20together%20with%20an%20LSTM%0Aon%20preferably%20long%20video%20segments%2C%20i.e.%2C%20with%20long%20temporal%20context.%20In%20our%0Aexperiments%2C%20almost%20all%20temporal%20models%20performed%20better%20on%20top%20of%20feature%0Aextractors%20that%20were%20trained%20with%20longer%20temporal%20context.%20On%20these%0Acontextualized%20features%2C%20TUNeS%20achieves%20state-of-the-art%20results%20on%20the%0ACholec80%20dataset.%20This%20study%20offers%20new%20insights%20on%20how%20to%20use%20attention%0Amechanisms%20to%20build%20accurate%20and%20efficient%20temporal%20models%20for%20surgical%20phase%0Arecognition.%20Implementing%20automatic%20surgical%20phase%20recognition%20is%20essential%20to%0Aautomate%20the%20analysis%20and%20optimization%20of%20surgical%20workflows%20and%20to%20enable%0Acontext-aware%20computer%20assistance%20during%20surgery%2C%20thus%20ultimately%20improving%0Apatient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09997v5&entry.124074799=Read"},
{"title": "Levin Tree Search with Context Models", "author": "Laurent Orseau and Marcus Hutter and Levi H. S. Lelis", "abstract": "  Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a\nprobability distribution over actions) and comes with a theoretical guarantee\non the number of expansions before reaching a goal node, depending on the\nquality of the policy. This guarantee can be used as a loss function, which we\ncall the LTS loss, to optimize neural networks representing the policy\n(LTS+NN). In this work we show that the neural network can be substituted with\nparameterized context models originating from the online compression literature\n(LTS+CM). We show that the LTS loss is convex under this new model, which\nallows for using standard convex optimization tools, and obtain convergence\nguarantees to the optimal parameters in an online setting for a given set of\nsolution trajectories -- guarantees that cannot be provided for neural\nnetworks. The new LTS+CM algorithm compares favorably against LTS+NN on several\nbenchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle\n(STP). The difference is particularly large on STP, where LTS+NN fails to solve\nmost of the test instances while LTS+CM solves each test instance in a fraction\nof a second. Furthermore, we show that LTS+CM is able to learn a policy that\nsolves the Rubik's cube in only a few hundred expansions, which considerably\nimproves upon previous machine learning techniques.\n", "link": "http://arxiv.org/abs/2305.16945v3", "date": "2024-11-12", "relevancy": 2.1244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Levin%20Tree%20Search%20with%20Context%20Models&body=Title%3A%20Levin%20Tree%20Search%20with%20Context%20Models%0AAuthor%3A%20Laurent%20Orseau%20and%20Marcus%20Hutter%20and%20Levi%20H.%20S.%20Lelis%0AAbstract%3A%20%20%20Levin%20Tree%20Search%20%28LTS%29%20is%20a%20search%20algorithm%20that%20makes%20use%20of%20a%20policy%20%28a%0Aprobability%20distribution%20over%20actions%29%20and%20comes%20with%20a%20theoretical%20guarantee%0Aon%20the%20number%20of%20expansions%20before%20reaching%20a%20goal%20node%2C%20depending%20on%20the%0Aquality%20of%20the%20policy.%20This%20guarantee%20can%20be%20used%20as%20a%20loss%20function%2C%20which%20we%0Acall%20the%20LTS%20loss%2C%20to%20optimize%20neural%20networks%20representing%20the%20policy%0A%28LTS%2BNN%29.%20In%20this%20work%20we%20show%20that%20the%20neural%20network%20can%20be%20substituted%20with%0Aparameterized%20context%20models%20originating%20from%20the%20online%20compression%20literature%0A%28LTS%2BCM%29.%20We%20show%20that%20the%20LTS%20loss%20is%20convex%20under%20this%20new%20model%2C%20which%0Aallows%20for%20using%20standard%20convex%20optimization%20tools%2C%20and%20obtain%20convergence%0Aguarantees%20to%20the%20optimal%20parameters%20in%20an%20online%20setting%20for%20a%20given%20set%20of%0Asolution%20trajectories%20--%20guarantees%20that%20cannot%20be%20provided%20for%20neural%0Anetworks.%20The%20new%20LTS%2BCM%20algorithm%20compares%20favorably%20against%20LTS%2BNN%20on%20several%0Abenchmarks%3A%20Sokoban%20%28Boxoban%29%2C%20The%20Witness%2C%20and%20the%2024-Sliding%20Tile%20puzzle%0A%28STP%29.%20The%20difference%20is%20particularly%20large%20on%20STP%2C%20where%20LTS%2BNN%20fails%20to%20solve%0Amost%20of%20the%20test%20instances%20while%20LTS%2BCM%20solves%20each%20test%20instance%20in%20a%20fraction%0Aof%20a%20second.%20Furthermore%2C%20we%20show%20that%20LTS%2BCM%20is%20able%20to%20learn%20a%20policy%20that%0Asolves%20the%20Rubik%27s%20cube%20in%20only%20a%20few%20hundred%20expansions%2C%20which%20considerably%0Aimproves%20upon%20previous%20machine%20learning%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.16945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLevin%2520Tree%2520Search%2520with%2520Context%2520Models%26entry.906535625%3DLaurent%2520Orseau%2520and%2520Marcus%2520Hutter%2520and%2520Levi%2520H.%2520S.%2520Lelis%26entry.1292438233%3D%2520%2520Levin%2520Tree%2520Search%2520%2528LTS%2529%2520is%2520a%2520search%2520algorithm%2520that%2520makes%2520use%2520of%2520a%2520policy%2520%2528a%250Aprobability%2520distribution%2520over%2520actions%2529%2520and%2520comes%2520with%2520a%2520theoretical%2520guarantee%250Aon%2520the%2520number%2520of%2520expansions%2520before%2520reaching%2520a%2520goal%2520node%252C%2520depending%2520on%2520the%250Aquality%2520of%2520the%2520policy.%2520This%2520guarantee%2520can%2520be%2520used%2520as%2520a%2520loss%2520function%252C%2520which%2520we%250Acall%2520the%2520LTS%2520loss%252C%2520to%2520optimize%2520neural%2520networks%2520representing%2520the%2520policy%250A%2528LTS%252BNN%2529.%2520In%2520this%2520work%2520we%2520show%2520that%2520the%2520neural%2520network%2520can%2520be%2520substituted%2520with%250Aparameterized%2520context%2520models%2520originating%2520from%2520the%2520online%2520compression%2520literature%250A%2528LTS%252BCM%2529.%2520We%2520show%2520that%2520the%2520LTS%2520loss%2520is%2520convex%2520under%2520this%2520new%2520model%252C%2520which%250Aallows%2520for%2520using%2520standard%2520convex%2520optimization%2520tools%252C%2520and%2520obtain%2520convergence%250Aguarantees%2520to%2520the%2520optimal%2520parameters%2520in%2520an%2520online%2520setting%2520for%2520a%2520given%2520set%2520of%250Asolution%2520trajectories%2520--%2520guarantees%2520that%2520cannot%2520be%2520provided%2520for%2520neural%250Anetworks.%2520The%2520new%2520LTS%252BCM%2520algorithm%2520compares%2520favorably%2520against%2520LTS%252BNN%2520on%2520several%250Abenchmarks%253A%2520Sokoban%2520%2528Boxoban%2529%252C%2520The%2520Witness%252C%2520and%2520the%252024-Sliding%2520Tile%2520puzzle%250A%2528STP%2529.%2520The%2520difference%2520is%2520particularly%2520large%2520on%2520STP%252C%2520where%2520LTS%252BNN%2520fails%2520to%2520solve%250Amost%2520of%2520the%2520test%2520instances%2520while%2520LTS%252BCM%2520solves%2520each%2520test%2520instance%2520in%2520a%2520fraction%250Aof%2520a%2520second.%2520Furthermore%252C%2520we%2520show%2520that%2520LTS%252BCM%2520is%2520able%2520to%2520learn%2520a%2520policy%2520that%250Asolves%2520the%2520Rubik%2527s%2520cube%2520in%2520only%2520a%2520few%2520hundred%2520expansions%252C%2520which%2520considerably%250Aimproves%2520upon%2520previous%2520machine%2520learning%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.16945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Levin%20Tree%20Search%20with%20Context%20Models&entry.906535625=Laurent%20Orseau%20and%20Marcus%20Hutter%20and%20Levi%20H.%20S.%20Lelis&entry.1292438233=%20%20Levin%20Tree%20Search%20%28LTS%29%20is%20a%20search%20algorithm%20that%20makes%20use%20of%20a%20policy%20%28a%0Aprobability%20distribution%20over%20actions%29%20and%20comes%20with%20a%20theoretical%20guarantee%0Aon%20the%20number%20of%20expansions%20before%20reaching%20a%20goal%20node%2C%20depending%20on%20the%0Aquality%20of%20the%20policy.%20This%20guarantee%20can%20be%20used%20as%20a%20loss%20function%2C%20which%20we%0Acall%20the%20LTS%20loss%2C%20to%20optimize%20neural%20networks%20representing%20the%20policy%0A%28LTS%2BNN%29.%20In%20this%20work%20we%20show%20that%20the%20neural%20network%20can%20be%20substituted%20with%0Aparameterized%20context%20models%20originating%20from%20the%20online%20compression%20literature%0A%28LTS%2BCM%29.%20We%20show%20that%20the%20LTS%20loss%20is%20convex%20under%20this%20new%20model%2C%20which%0Aallows%20for%20using%20standard%20convex%20optimization%20tools%2C%20and%20obtain%20convergence%0Aguarantees%20to%20the%20optimal%20parameters%20in%20an%20online%20setting%20for%20a%20given%20set%20of%0Asolution%20trajectories%20--%20guarantees%20that%20cannot%20be%20provided%20for%20neural%0Anetworks.%20The%20new%20LTS%2BCM%20algorithm%20compares%20favorably%20against%20LTS%2BNN%20on%20several%0Abenchmarks%3A%20Sokoban%20%28Boxoban%29%2C%20The%20Witness%2C%20and%20the%2024-Sliding%20Tile%20puzzle%0A%28STP%29.%20The%20difference%20is%20particularly%20large%20on%20STP%2C%20where%20LTS%2BNN%20fails%20to%20solve%0Amost%20of%20the%20test%20instances%20while%20LTS%2BCM%20solves%20each%20test%20instance%20in%20a%20fraction%0Aof%20a%20second.%20Furthermore%2C%20we%20show%20that%20LTS%2BCM%20is%20able%20to%20learn%20a%20policy%20that%0Asolves%20the%20Rubik%27s%20cube%20in%20only%20a%20few%20hundred%20expansions%2C%20which%20considerably%0Aimproves%20upon%20previous%20machine%20learning%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.16945v3&entry.124074799=Read"},
{"title": "Suite-IN: Aggregating Motion Features from Apple Suite for Robust\n  Inertial Navigation", "author": "Lan Sun and Songpengcheng Xia and Junyuan Deng and Jiarui Yang and Zengyuan Lai and Qi Wu and Ling Pei", "abstract": "  With the rapid development of wearable technology, devices like smartphones,\nsmartwatches, and headphones equipped with IMUs have become essential for\napplications such as pedestrian positioning. However, traditional pedestrian\ndead reckoning (PDR) methods struggle with diverse motion patterns, while\nrecent data-driven approaches, though improving accuracy, often lack robustness\ndue to reliance on a single device.In our work, we attempt to enhance the\npositioning performance using the low-cost commodity IMUs embedded in the\nwearable devices. We propose a multi-device deep learning framework named\nSuite-IN, aggregating motion data from Apple Suite for inertial navigation.\nMotion data captured by sensors on different body parts contains both local and\nglobal motion information, making it essential to reduce the negative effects\nof localized movements and extract global motion representations from multiple\ndevices.\n", "link": "http://arxiv.org/abs/2411.07828v1", "date": "2024-11-12", "relevancy": 2.1196, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Suite-IN%3A%20Aggregating%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%0A%20%20Inertial%20Navigation&body=Title%3A%20Suite-IN%3A%20Aggregating%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%0A%20%20Inertial%20Navigation%0AAuthor%3A%20Lan%20Sun%20and%20Songpengcheng%20Xia%20and%20Junyuan%20Deng%20and%20Jiarui%20Yang%20and%20Zengyuan%20Lai%20and%20Qi%20Wu%20and%20Ling%20Pei%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20wearable%20technology%2C%20devices%20like%20smartphones%2C%0Asmartwatches%2C%20and%20headphones%20equipped%20with%20IMUs%20have%20become%20essential%20for%0Aapplications%20such%20as%20pedestrian%20positioning.%20However%2C%20traditional%20pedestrian%0Adead%20reckoning%20%28PDR%29%20methods%20struggle%20with%20diverse%20motion%20patterns%2C%20while%0Arecent%20data-driven%20approaches%2C%20though%20improving%20accuracy%2C%20often%20lack%20robustness%0Adue%20to%20reliance%20on%20a%20single%20device.In%20our%20work%2C%20we%20attempt%20to%20enhance%20the%0Apositioning%20performance%20using%20the%20low-cost%20commodity%20IMUs%20embedded%20in%20the%0Awearable%20devices.%20We%20propose%20a%20multi-device%20deep%20learning%20framework%20named%0ASuite-IN%2C%20aggregating%20motion%20data%20from%20Apple%20Suite%20for%20inertial%20navigation.%0AMotion%20data%20captured%20by%20sensors%20on%20different%20body%20parts%20contains%20both%20local%20and%0Aglobal%20motion%20information%2C%20making%20it%20essential%20to%20reduce%20the%20negative%20effects%0Aof%20localized%20movements%20and%20extract%20global%20motion%20representations%20from%20multiple%0Adevices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuite-IN%253A%2520Aggregating%2520Motion%2520Features%2520from%2520Apple%2520Suite%2520for%2520Robust%250A%2520%2520Inertial%2520Navigation%26entry.906535625%3DLan%2520Sun%2520and%2520Songpengcheng%2520Xia%2520and%2520Junyuan%2520Deng%2520and%2520Jiarui%2520Yang%2520and%2520Zengyuan%2520Lai%2520and%2520Qi%2520Wu%2520and%2520Ling%2520Pei%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520wearable%2520technology%252C%2520devices%2520like%2520smartphones%252C%250Asmartwatches%252C%2520and%2520headphones%2520equipped%2520with%2520IMUs%2520have%2520become%2520essential%2520for%250Aapplications%2520such%2520as%2520pedestrian%2520positioning.%2520However%252C%2520traditional%2520pedestrian%250Adead%2520reckoning%2520%2528PDR%2529%2520methods%2520struggle%2520with%2520diverse%2520motion%2520patterns%252C%2520while%250Arecent%2520data-driven%2520approaches%252C%2520though%2520improving%2520accuracy%252C%2520often%2520lack%2520robustness%250Adue%2520to%2520reliance%2520on%2520a%2520single%2520device.In%2520our%2520work%252C%2520we%2520attempt%2520to%2520enhance%2520the%250Apositioning%2520performance%2520using%2520the%2520low-cost%2520commodity%2520IMUs%2520embedded%2520in%2520the%250Awearable%2520devices.%2520We%2520propose%2520a%2520multi-device%2520deep%2520learning%2520framework%2520named%250ASuite-IN%252C%2520aggregating%2520motion%2520data%2520from%2520Apple%2520Suite%2520for%2520inertial%2520navigation.%250AMotion%2520data%2520captured%2520by%2520sensors%2520on%2520different%2520body%2520parts%2520contains%2520both%2520local%2520and%250Aglobal%2520motion%2520information%252C%2520making%2520it%2520essential%2520to%2520reduce%2520the%2520negative%2520effects%250Aof%2520localized%2520movements%2520and%2520extract%2520global%2520motion%2520representations%2520from%2520multiple%250Adevices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Suite-IN%3A%20Aggregating%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%0A%20%20Inertial%20Navigation&entry.906535625=Lan%20Sun%20and%20Songpengcheng%20Xia%20and%20Junyuan%20Deng%20and%20Jiarui%20Yang%20and%20Zengyuan%20Lai%20and%20Qi%20Wu%20and%20Ling%20Pei&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20wearable%20technology%2C%20devices%20like%20smartphones%2C%0Asmartwatches%2C%20and%20headphones%20equipped%20with%20IMUs%20have%20become%20essential%20for%0Aapplications%20such%20as%20pedestrian%20positioning.%20However%2C%20traditional%20pedestrian%0Adead%20reckoning%20%28PDR%29%20methods%20struggle%20with%20diverse%20motion%20patterns%2C%20while%0Arecent%20data-driven%20approaches%2C%20though%20improving%20accuracy%2C%20often%20lack%20robustness%0Adue%20to%20reliance%20on%20a%20single%20device.In%20our%20work%2C%20we%20attempt%20to%20enhance%20the%0Apositioning%20performance%20using%20the%20low-cost%20commodity%20IMUs%20embedded%20in%20the%0Awearable%20devices.%20We%20propose%20a%20multi-device%20deep%20learning%20framework%20named%0ASuite-IN%2C%20aggregating%20motion%20data%20from%20Apple%20Suite%20for%20inertial%20navigation.%0AMotion%20data%20captured%20by%20sensors%20on%20different%20body%20parts%20contains%20both%20local%20and%0Aglobal%20motion%20information%2C%20making%20it%20essential%20to%20reduce%20the%20negative%20effects%0Aof%20localized%20movements%20and%20extract%20global%20motion%20representations%20from%20multiple%0Adevices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07828v1&entry.124074799=Read"},
{"title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with\n  ImageRAG", "author": "Zilun Zhang and Haozhan Shen and Tiancheng Zhao and Yuhao Wang and Bin Chen and Yuxiang Cai and Yongheng Shang and Jianwei Yin", "abstract": "  Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000\n$\\times$ 100,000 pixels or more) poses a significant challenge for current\nRemote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize\nthe UHR image to standard input image size, the extensive spatial and\ncontextual information that UHR images contain will be neglected. Otherwise,\nthe original size of these images often exceeds the token limits of standard\nRSMLLMs, making it difficult to process the entire image and capture long-range\ndependencies to answer the query based on the abundant visual context. In this\npaper, we introduce ImageRAG for RS, a training-free framework to address the\ncomplexities of analyzing UHR remote sensing imagery. By transforming UHR\nremote sensing image analysis task to image's long context selection task, we\ndesign an innovative image contextual retrieval mechanism based on the\nRetrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's\ncore innovation lies in its ability to selectively retrieve and focus on the\nmost relevant portions of the UHR image as visual contexts that pertain to a\ngiven query. Fast path and slow path are proposed in this framework to handle\nthis task efficiently and effectively. ImageRAG allows RSMLLMs to manage\nextensive context and spatial information from UHR RSI, ensuring the analysis\nis both accurate and efficient.\n", "link": "http://arxiv.org/abs/2411.07688v1", "date": "2024-11-12", "relevancy": 2.1181, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.549}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Ultra%20High%20Resolution%20Remote%20Sensing%20Imagery%20Analysis%20with%0A%20%20ImageRAG&body=Title%3A%20Enhancing%20Ultra%20High%20Resolution%20Remote%20Sensing%20Imagery%20Analysis%20with%0A%20%20ImageRAG%0AAuthor%3A%20Zilun%20Zhang%20and%20Haozhan%20Shen%20and%20Tiancheng%20Zhao%20and%20Yuhao%20Wang%20and%20Bin%20Chen%20and%20Yuxiang%20Cai%20and%20Yongheng%20Shang%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20Ultra%20High%20Resolution%20%28UHR%29%20remote%20sensing%20imagery%20%28RSI%29%20%28e.g.%20100%2C000%0A%24%5Ctimes%24%20100%2C000%20pixels%20or%20more%29%20poses%20a%20significant%20challenge%20for%20current%0ARemote%20Sensing%20Multimodal%20Large%20Language%20Models%20%28RSMLLMs%29.%20If%20choose%20to%20resize%0Athe%20UHR%20image%20to%20standard%20input%20image%20size%2C%20the%20extensive%20spatial%20and%0Acontextual%20information%20that%20UHR%20images%20contain%20will%20be%20neglected.%20Otherwise%2C%0Athe%20original%20size%20of%20these%20images%20often%20exceeds%20the%20token%20limits%20of%20standard%0ARSMLLMs%2C%20making%20it%20difficult%20to%20process%20the%20entire%20image%20and%20capture%20long-range%0Adependencies%20to%20answer%20the%20query%20based%20on%20the%20abundant%20visual%20context.%20In%20this%0Apaper%2C%20we%20introduce%20ImageRAG%20for%20RS%2C%20a%20training-free%20framework%20to%20address%20the%0Acomplexities%20of%20analyzing%20UHR%20remote%20sensing%20imagery.%20By%20transforming%20UHR%0Aremote%20sensing%20image%20analysis%20task%20to%20image%27s%20long%20context%20selection%20task%2C%20we%0Adesign%20an%20innovative%20image%20contextual%20retrieval%20mechanism%20based%20on%20the%0ARetrieval-Augmented%20Generation%20%28RAG%29%20technique%2C%20denoted%20as%20ImageRAG.%20ImageRAG%27s%0Acore%20innovation%20lies%20in%20its%20ability%20to%20selectively%20retrieve%20and%20focus%20on%20the%0Amost%20relevant%20portions%20of%20the%20UHR%20image%20as%20visual%20contexts%20that%20pertain%20to%20a%0Agiven%20query.%20Fast%20path%20and%20slow%20path%20are%20proposed%20in%20this%20framework%20to%20handle%0Athis%20task%20efficiently%20and%20effectively.%20ImageRAG%20allows%20RSMLLMs%20to%20manage%0Aextensive%20context%20and%20spatial%20information%20from%20UHR%20RSI%2C%20ensuring%20the%20analysis%0Ais%20both%20accurate%20and%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Ultra%2520High%2520Resolution%2520Remote%2520Sensing%2520Imagery%2520Analysis%2520with%250A%2520%2520ImageRAG%26entry.906535625%3DZilun%2520Zhang%2520and%2520Haozhan%2520Shen%2520and%2520Tiancheng%2520Zhao%2520and%2520Yuhao%2520Wang%2520and%2520Bin%2520Chen%2520and%2520Yuxiang%2520Cai%2520and%2520Yongheng%2520Shang%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520Ultra%2520High%2520Resolution%2520%2528UHR%2529%2520remote%2520sensing%2520imagery%2520%2528RSI%2529%2520%2528e.g.%2520100%252C000%250A%2524%255Ctimes%2524%2520100%252C000%2520pixels%2520or%2520more%2529%2520poses%2520a%2520significant%2520challenge%2520for%2520current%250ARemote%2520Sensing%2520Multimodal%2520Large%2520Language%2520Models%2520%2528RSMLLMs%2529.%2520If%2520choose%2520to%2520resize%250Athe%2520UHR%2520image%2520to%2520standard%2520input%2520image%2520size%252C%2520the%2520extensive%2520spatial%2520and%250Acontextual%2520information%2520that%2520UHR%2520images%2520contain%2520will%2520be%2520neglected.%2520Otherwise%252C%250Athe%2520original%2520size%2520of%2520these%2520images%2520often%2520exceeds%2520the%2520token%2520limits%2520of%2520standard%250ARSMLLMs%252C%2520making%2520it%2520difficult%2520to%2520process%2520the%2520entire%2520image%2520and%2520capture%2520long-range%250Adependencies%2520to%2520answer%2520the%2520query%2520based%2520on%2520the%2520abundant%2520visual%2520context.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520ImageRAG%2520for%2520RS%252C%2520a%2520training-free%2520framework%2520to%2520address%2520the%250Acomplexities%2520of%2520analyzing%2520UHR%2520remote%2520sensing%2520imagery.%2520By%2520transforming%2520UHR%250Aremote%2520sensing%2520image%2520analysis%2520task%2520to%2520image%2527s%2520long%2520context%2520selection%2520task%252C%2520we%250Adesign%2520an%2520innovative%2520image%2520contextual%2520retrieval%2520mechanism%2520based%2520on%2520the%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520technique%252C%2520denoted%2520as%2520ImageRAG.%2520ImageRAG%2527s%250Acore%2520innovation%2520lies%2520in%2520its%2520ability%2520to%2520selectively%2520retrieve%2520and%2520focus%2520on%2520the%250Amost%2520relevant%2520portions%2520of%2520the%2520UHR%2520image%2520as%2520visual%2520contexts%2520that%2520pertain%2520to%2520a%250Agiven%2520query.%2520Fast%2520path%2520and%2520slow%2520path%2520are%2520proposed%2520in%2520this%2520framework%2520to%2520handle%250Athis%2520task%2520efficiently%2520and%2520effectively.%2520ImageRAG%2520allows%2520RSMLLMs%2520to%2520manage%250Aextensive%2520context%2520and%2520spatial%2520information%2520from%2520UHR%2520RSI%252C%2520ensuring%2520the%2520analysis%250Ais%2520both%2520accurate%2520and%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Ultra%20High%20Resolution%20Remote%20Sensing%20Imagery%20Analysis%20with%0A%20%20ImageRAG&entry.906535625=Zilun%20Zhang%20and%20Haozhan%20Shen%20and%20Tiancheng%20Zhao%20and%20Yuhao%20Wang%20and%20Bin%20Chen%20and%20Yuxiang%20Cai%20and%20Yongheng%20Shang%20and%20Jianwei%20Yin&entry.1292438233=%20%20Ultra%20High%20Resolution%20%28UHR%29%20remote%20sensing%20imagery%20%28RSI%29%20%28e.g.%20100%2C000%0A%24%5Ctimes%24%20100%2C000%20pixels%20or%20more%29%20poses%20a%20significant%20challenge%20for%20current%0ARemote%20Sensing%20Multimodal%20Large%20Language%20Models%20%28RSMLLMs%29.%20If%20choose%20to%20resize%0Athe%20UHR%20image%20to%20standard%20input%20image%20size%2C%20the%20extensive%20spatial%20and%0Acontextual%20information%20that%20UHR%20images%20contain%20will%20be%20neglected.%20Otherwise%2C%0Athe%20original%20size%20of%20these%20images%20often%20exceeds%20the%20token%20limits%20of%20standard%0ARSMLLMs%2C%20making%20it%20difficult%20to%20process%20the%20entire%20image%20and%20capture%20long-range%0Adependencies%20to%20answer%20the%20query%20based%20on%20the%20abundant%20visual%20context.%20In%20this%0Apaper%2C%20we%20introduce%20ImageRAG%20for%20RS%2C%20a%20training-free%20framework%20to%20address%20the%0Acomplexities%20of%20analyzing%20UHR%20remote%20sensing%20imagery.%20By%20transforming%20UHR%0Aremote%20sensing%20image%20analysis%20task%20to%20image%27s%20long%20context%20selection%20task%2C%20we%0Adesign%20an%20innovative%20image%20contextual%20retrieval%20mechanism%20based%20on%20the%0ARetrieval-Augmented%20Generation%20%28RAG%29%20technique%2C%20denoted%20as%20ImageRAG.%20ImageRAG%27s%0Acore%20innovation%20lies%20in%20its%20ability%20to%20selectively%20retrieve%20and%20focus%20on%20the%0Amost%20relevant%20portions%20of%20the%20UHR%20image%20as%20visual%20contexts%20that%20pertain%20to%20a%0Agiven%20query.%20Fast%20path%20and%20slow%20path%20are%20proposed%20in%20this%20framework%20to%20handle%0Athis%20task%20efficiently%20and%20effectively.%20ImageRAG%20allows%20RSMLLMs%20to%20manage%0Aextensive%20context%20and%20spatial%20information%20from%20UHR%20RSI%2C%20ensuring%20the%20analysis%0Ais%20both%20accurate%20and%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07688v1&entry.124074799=Read"},
{"title": "Fast and Functional Structured Data Generators Rooted in\n  Out-of-Equilibrium Physics", "author": "Alessandra Carbone and Aur\u00e9lien Decelle and Lorenzo Rosset and Beatriz Seoane", "abstract": "  In this study, we address the challenge of using energy-based models to\nproduce high-quality, label-specific data in complex structured datasets, such\nas population genetics, RNA or protein sequences data. Traditional training\nmethods encounter difficulties due to inefficient Markov chain Monte Carlo\nmixing, which affects the diversity of synthetic data and increases generation\ntimes. To address these issues, we use a novel training algorithm that exploits\nnon-equilibrium effects. This approach, applied on the Restricted Boltzmann\nMachine, improves the model's ability to correctly classify samples and\ngenerate high-quality synthetic data in only a few sampling steps. The\neffectiveness of this method is demonstrated by its successful application to\nfour different types of data: handwritten digits, mutations of human genomes\nclassified by continental origin, functionally characterized sequences of an\nenzyme protein family, and homologous RNA sequences from specific taxonomies.\n", "link": "http://arxiv.org/abs/2307.06797v2", "date": "2024-11-12", "relevancy": 2.1177, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.541}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5213}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Functional%20Structured%20Data%20Generators%20Rooted%20in%0A%20%20Out-of-Equilibrium%20Physics&body=Title%3A%20Fast%20and%20Functional%20Structured%20Data%20Generators%20Rooted%20in%0A%20%20Out-of-Equilibrium%20Physics%0AAuthor%3A%20Alessandra%20Carbone%20and%20Aur%C3%A9lien%20Decelle%20and%20Lorenzo%20Rosset%20and%20Beatriz%20Seoane%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20using%20energy-based%20models%20to%0Aproduce%20high-quality%2C%20label-specific%20data%20in%20complex%20structured%20datasets%2C%20such%0Aas%20population%20genetics%2C%20RNA%20or%20protein%20sequences%20data.%20Traditional%20training%0Amethods%20encounter%20difficulties%20due%20to%20inefficient%20Markov%20chain%20Monte%20Carlo%0Amixing%2C%20which%20affects%20the%20diversity%20of%20synthetic%20data%20and%20increases%20generation%0Atimes.%20To%20address%20these%20issues%2C%20we%20use%20a%20novel%20training%20algorithm%20that%20exploits%0Anon-equilibrium%20effects.%20This%20approach%2C%20applied%20on%20the%20Restricted%20Boltzmann%0AMachine%2C%20improves%20the%20model%27s%20ability%20to%20correctly%20classify%20samples%20and%0Agenerate%20high-quality%20synthetic%20data%20in%20only%20a%20few%20sampling%20steps.%20The%0Aeffectiveness%20of%20this%20method%20is%20demonstrated%20by%20its%20successful%20application%20to%0Afour%20different%20types%20of%20data%3A%20handwritten%20digits%2C%20mutations%20of%20human%20genomes%0Aclassified%20by%20continental%20origin%2C%20functionally%20characterized%20sequences%20of%20an%0Aenzyme%20protein%20family%2C%20and%20homologous%20RNA%20sequences%20from%20specific%20taxonomies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.06797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Functional%2520Structured%2520Data%2520Generators%2520Rooted%2520in%250A%2520%2520Out-of-Equilibrium%2520Physics%26entry.906535625%3DAlessandra%2520Carbone%2520and%2520Aur%25C3%25A9lien%2520Decelle%2520and%2520Lorenzo%2520Rosset%2520and%2520Beatriz%2520Seoane%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520challenge%2520of%2520using%2520energy-based%2520models%2520to%250Aproduce%2520high-quality%252C%2520label-specific%2520data%2520in%2520complex%2520structured%2520datasets%252C%2520such%250Aas%2520population%2520genetics%252C%2520RNA%2520or%2520protein%2520sequences%2520data.%2520Traditional%2520training%250Amethods%2520encounter%2520difficulties%2520due%2520to%2520inefficient%2520Markov%2520chain%2520Monte%2520Carlo%250Amixing%252C%2520which%2520affects%2520the%2520diversity%2520of%2520synthetic%2520data%2520and%2520increases%2520generation%250Atimes.%2520To%2520address%2520these%2520issues%252C%2520we%2520use%2520a%2520novel%2520training%2520algorithm%2520that%2520exploits%250Anon-equilibrium%2520effects.%2520This%2520approach%252C%2520applied%2520on%2520the%2520Restricted%2520Boltzmann%250AMachine%252C%2520improves%2520the%2520model%2527s%2520ability%2520to%2520correctly%2520classify%2520samples%2520and%250Agenerate%2520high-quality%2520synthetic%2520data%2520in%2520only%2520a%2520few%2520sampling%2520steps.%2520The%250Aeffectiveness%2520of%2520this%2520method%2520is%2520demonstrated%2520by%2520its%2520successful%2520application%2520to%250Afour%2520different%2520types%2520of%2520data%253A%2520handwritten%2520digits%252C%2520mutations%2520of%2520human%2520genomes%250Aclassified%2520by%2520continental%2520origin%252C%2520functionally%2520characterized%2520sequences%2520of%2520an%250Aenzyme%2520protein%2520family%252C%2520and%2520homologous%2520RNA%2520sequences%2520from%2520specific%2520taxonomies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.06797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Functional%20Structured%20Data%20Generators%20Rooted%20in%0A%20%20Out-of-Equilibrium%20Physics&entry.906535625=Alessandra%20Carbone%20and%20Aur%C3%A9lien%20Decelle%20and%20Lorenzo%20Rosset%20and%20Beatriz%20Seoane&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20the%20challenge%20of%20using%20energy-based%20models%20to%0Aproduce%20high-quality%2C%20label-specific%20data%20in%20complex%20structured%20datasets%2C%20such%0Aas%20population%20genetics%2C%20RNA%20or%20protein%20sequences%20data.%20Traditional%20training%0Amethods%20encounter%20difficulties%20due%20to%20inefficient%20Markov%20chain%20Monte%20Carlo%0Amixing%2C%20which%20affects%20the%20diversity%20of%20synthetic%20data%20and%20increases%20generation%0Atimes.%20To%20address%20these%20issues%2C%20we%20use%20a%20novel%20training%20algorithm%20that%20exploits%0Anon-equilibrium%20effects.%20This%20approach%2C%20applied%20on%20the%20Restricted%20Boltzmann%0AMachine%2C%20improves%20the%20model%27s%20ability%20to%20correctly%20classify%20samples%20and%0Agenerate%20high-quality%20synthetic%20data%20in%20only%20a%20few%20sampling%20steps.%20The%0Aeffectiveness%20of%20this%20method%20is%20demonstrated%20by%20its%20successful%20application%20to%0Afour%20different%20types%20of%20data%3A%20handwritten%20digits%2C%20mutations%20of%20human%20genomes%0Aclassified%20by%20continental%20origin%2C%20functionally%20characterized%20sequences%20of%20an%0Aenzyme%20protein%20family%2C%20and%20homologous%20RNA%20sequences%20from%20specific%20taxonomies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.06797v2&entry.124074799=Read"},
{"title": "UniTE: A Survey and Unified Pipeline for Pre-training Spatiotemporal\n  Trajectory Embeddings", "author": "Yan Lin and Zeyu Zhou and Yicheng Liu and Haochen Lv and Haomin Wen and Tianyi Li and Yushuai Li and Christian S. Jensen and Shengnan Guo and Youfang Lin and Huaiyu Wan", "abstract": "  Spatiotemporal trajectories are sequences of timestamped locations, which\nenable a variety of analyses that in turn enable important real-world\napplications. It is common to map trajectories to vectors, called embeddings,\nbefore subsequent analyses. Thus, the qualities of embeddings are very\nimportant. Methods for pre-training embeddings, which leverage unlabeled\ntrajectories for training universal embeddings, have shown promising\napplicability across different tasks, thus attracting considerable interest.\nHowever, research progress on this topic faces two key challenges: a lack of a\ncomprehensive overview of existing methods, resulting in several related\nmethods not being well-recognized, and the absence of a unified pipeline,\ncomplicating the development of new methods and the analysis of methods.\n  We present UniTE, a survey and a unified pipeline for this domain. In doing\nso, we present a comprehensive list of existing methods for pre-training\ntrajectory embeddings, which includes methods that either explicitly or\nimplicitly employ pre-training techniques. Further, we present a unified and\nmodular pipeline with publicly available underlying code, simplifying the\nprocess of constructing and evaluating methods for pre-training trajectory\nembeddings. Additionally, we contribute a selection of experimental results\nusing the proposed pipeline on real-world datasets. Implementation of the\npipeline is publicly available at https://github.com/Logan-Lin/UniTE.\n", "link": "http://arxiv.org/abs/2407.12550v2", "date": "2024-11-12", "relevancy": 2.1154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20Spatiotemporal%0A%20%20Trajectory%20Embeddings&body=Title%3A%20UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20Spatiotemporal%0A%20%20Trajectory%20Embeddings%0AAuthor%3A%20Yan%20Lin%20and%20Zeyu%20Zhou%20and%20Yicheng%20Liu%20and%20Haochen%20Lv%20and%20Haomin%20Wen%20and%20Tianyi%20Li%20and%20Yushuai%20Li%20and%20Christian%20S.%20Jensen%20and%20Shengnan%20Guo%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20Spatiotemporal%20trajectories%20are%20sequences%20of%20timestamped%20locations%2C%20which%0Aenable%20a%20variety%20of%20analyses%20that%20in%20turn%20enable%20important%20real-world%0Aapplications.%20It%20is%20common%20to%20map%20trajectories%20to%20vectors%2C%20called%20embeddings%2C%0Abefore%20subsequent%20analyses.%20Thus%2C%20the%20qualities%20of%20embeddings%20are%20very%0Aimportant.%20Methods%20for%20pre-training%20embeddings%2C%20which%20leverage%20unlabeled%0Atrajectories%20for%20training%20universal%20embeddings%2C%20have%20shown%20promising%0Aapplicability%20across%20different%20tasks%2C%20thus%20attracting%20considerable%20interest.%0AHowever%2C%20research%20progress%20on%20this%20topic%20faces%20two%20key%20challenges%3A%20a%20lack%20of%20a%0Acomprehensive%20overview%20of%20existing%20methods%2C%20resulting%20in%20several%20related%0Amethods%20not%20being%20well-recognized%2C%20and%20the%20absence%20of%20a%20unified%20pipeline%2C%0Acomplicating%20the%20development%20of%20new%20methods%20and%20the%20analysis%20of%20methods.%0A%20%20We%20present%20UniTE%2C%20a%20survey%20and%20a%20unified%20pipeline%20for%20this%20domain.%20In%20doing%0Aso%2C%20we%20present%20a%20comprehensive%20list%20of%20existing%20methods%20for%20pre-training%0Atrajectory%20embeddings%2C%20which%20includes%20methods%20that%20either%20explicitly%20or%0Aimplicitly%20employ%20pre-training%20techniques.%20Further%2C%20we%20present%20a%20unified%20and%0Amodular%20pipeline%20with%20publicly%20available%20underlying%20code%2C%20simplifying%20the%0Aprocess%20of%20constructing%20and%20evaluating%20methods%20for%20pre-training%20trajectory%0Aembeddings.%20Additionally%2C%20we%20contribute%20a%20selection%20of%20experimental%20results%0Ausing%20the%20proposed%20pipeline%20on%20real-world%20datasets.%20Implementation%20of%20the%0Apipeline%20is%20publicly%20available%20at%20https%3A//github.com/Logan-Lin/UniTE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTE%253A%2520A%2520Survey%2520and%2520Unified%2520Pipeline%2520for%2520Pre-training%2520Spatiotemporal%250A%2520%2520Trajectory%2520Embeddings%26entry.906535625%3DYan%2520Lin%2520and%2520Zeyu%2520Zhou%2520and%2520Yicheng%2520Liu%2520and%2520Haochen%2520Lv%2520and%2520Haomin%2520Wen%2520and%2520Tianyi%2520Li%2520and%2520Yushuai%2520Li%2520and%2520Christian%2520S.%2520Jensen%2520and%2520Shengnan%2520Guo%2520and%2520Youfang%2520Lin%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520Spatiotemporal%2520trajectories%2520are%2520sequences%2520of%2520timestamped%2520locations%252C%2520which%250Aenable%2520a%2520variety%2520of%2520analyses%2520that%2520in%2520turn%2520enable%2520important%2520real-world%250Aapplications.%2520It%2520is%2520common%2520to%2520map%2520trajectories%2520to%2520vectors%252C%2520called%2520embeddings%252C%250Abefore%2520subsequent%2520analyses.%2520Thus%252C%2520the%2520qualities%2520of%2520embeddings%2520are%2520very%250Aimportant.%2520Methods%2520for%2520pre-training%2520embeddings%252C%2520which%2520leverage%2520unlabeled%250Atrajectories%2520for%2520training%2520universal%2520embeddings%252C%2520have%2520shown%2520promising%250Aapplicability%2520across%2520different%2520tasks%252C%2520thus%2520attracting%2520considerable%2520interest.%250AHowever%252C%2520research%2520progress%2520on%2520this%2520topic%2520faces%2520two%2520key%2520challenges%253A%2520a%2520lack%2520of%2520a%250Acomprehensive%2520overview%2520of%2520existing%2520methods%252C%2520resulting%2520in%2520several%2520related%250Amethods%2520not%2520being%2520well-recognized%252C%2520and%2520the%2520absence%2520of%2520a%2520unified%2520pipeline%252C%250Acomplicating%2520the%2520development%2520of%2520new%2520methods%2520and%2520the%2520analysis%2520of%2520methods.%250A%2520%2520We%2520present%2520UniTE%252C%2520a%2520survey%2520and%2520a%2520unified%2520pipeline%2520for%2520this%2520domain.%2520In%2520doing%250Aso%252C%2520we%2520present%2520a%2520comprehensive%2520list%2520of%2520existing%2520methods%2520for%2520pre-training%250Atrajectory%2520embeddings%252C%2520which%2520includes%2520methods%2520that%2520either%2520explicitly%2520or%250Aimplicitly%2520employ%2520pre-training%2520techniques.%2520Further%252C%2520we%2520present%2520a%2520unified%2520and%250Amodular%2520pipeline%2520with%2520publicly%2520available%2520underlying%2520code%252C%2520simplifying%2520the%250Aprocess%2520of%2520constructing%2520and%2520evaluating%2520methods%2520for%2520pre-training%2520trajectory%250Aembeddings.%2520Additionally%252C%2520we%2520contribute%2520a%2520selection%2520of%2520experimental%2520results%250Ausing%2520the%2520proposed%2520pipeline%2520on%2520real-world%2520datasets.%2520Implementation%2520of%2520the%250Apipeline%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Logan-Lin/UniTE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTE%3A%20A%20Survey%20and%20Unified%20Pipeline%20for%20Pre-training%20Spatiotemporal%0A%20%20Trajectory%20Embeddings&entry.906535625=Yan%20Lin%20and%20Zeyu%20Zhou%20and%20Yicheng%20Liu%20and%20Haochen%20Lv%20and%20Haomin%20Wen%20and%20Tianyi%20Li%20and%20Yushuai%20Li%20and%20Christian%20S.%20Jensen%20and%20Shengnan%20Guo%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan&entry.1292438233=%20%20Spatiotemporal%20trajectories%20are%20sequences%20of%20timestamped%20locations%2C%20which%0Aenable%20a%20variety%20of%20analyses%20that%20in%20turn%20enable%20important%20real-world%0Aapplications.%20It%20is%20common%20to%20map%20trajectories%20to%20vectors%2C%20called%20embeddings%2C%0Abefore%20subsequent%20analyses.%20Thus%2C%20the%20qualities%20of%20embeddings%20are%20very%0Aimportant.%20Methods%20for%20pre-training%20embeddings%2C%20which%20leverage%20unlabeled%0Atrajectories%20for%20training%20universal%20embeddings%2C%20have%20shown%20promising%0Aapplicability%20across%20different%20tasks%2C%20thus%20attracting%20considerable%20interest.%0AHowever%2C%20research%20progress%20on%20this%20topic%20faces%20two%20key%20challenges%3A%20a%20lack%20of%20a%0Acomprehensive%20overview%20of%20existing%20methods%2C%20resulting%20in%20several%20related%0Amethods%20not%20being%20well-recognized%2C%20and%20the%20absence%20of%20a%20unified%20pipeline%2C%0Acomplicating%20the%20development%20of%20new%20methods%20and%20the%20analysis%20of%20methods.%0A%20%20We%20present%20UniTE%2C%20a%20survey%20and%20a%20unified%20pipeline%20for%20this%20domain.%20In%20doing%0Aso%2C%20we%20present%20a%20comprehensive%20list%20of%20existing%20methods%20for%20pre-training%0Atrajectory%20embeddings%2C%20which%20includes%20methods%20that%20either%20explicitly%20or%0Aimplicitly%20employ%20pre-training%20techniques.%20Further%2C%20we%20present%20a%20unified%20and%0Amodular%20pipeline%20with%20publicly%20available%20underlying%20code%2C%20simplifying%20the%0Aprocess%20of%20constructing%20and%20evaluating%20methods%20for%20pre-training%20trajectory%0Aembeddings.%20Additionally%2C%20we%20contribute%20a%20selection%20of%20experimental%20results%0Ausing%20the%20proposed%20pipeline%20on%20real-world%20datasets.%20Implementation%20of%20the%0Apipeline%20is%20publicly%20available%20at%20https%3A//github.com/Logan-Lin/UniTE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12550v2&entry.124074799=Read"},
{"title": "OWLed: Outlier-weighed Layerwise Pruning for Efficient Autonomous\n  Driving Framework", "author": "Jiaxi Li and Lu Yin and Xilu Wang", "abstract": "  The integration of Large Language Models (LLMs) into autonomous driving\nsystems offers promising enhancements in environmental understanding and\ndecision-making. However, the substantial computational demands of deploying\nLLMs locally on vehicles render this approach unfeasible for real-world\nautomotive applications. To address this challenge, we introduce OWLed, the\nOutlier-Weighed Layerwise Pruning for Efficient Autonomous Driving Framework\nthat leverages outlier-weighted layerwise sparsity for model compression. Our\nmethod assigns non-uniform sparsity ratios to different layers based on the\ndistribution of outlier features, significantly reducing the model size without\nthe need for fine-tuning. To ensure the compressed model adapts well to\nautonomous driving tasks, we incorporate driving environment data into both the\ncalibration and pruning processes. Our empirical studies reveal that the\nencoder component is more sensitive to pruning than the LLM, highlighting its\ncritical role in the system. Experimental results demonstrate that OWLed\noutperforms existing methods in perception, action prediction, and language\nunderstanding while substantially lowering computational requirements. These\nfindings underscore the potential of combining advanced pruning techniques with\nLLMs to develop efficient and robust autonomous driving systems capable of\nhandling complex scenarios. Code will be made publicly available.\n", "link": "http://arxiv.org/abs/2411.07711v1", "date": "2024-11-12", "relevancy": 2.1095, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5381}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OWLed%3A%20Outlier-weighed%20Layerwise%20Pruning%20for%20Efficient%20Autonomous%0A%20%20Driving%20Framework&body=Title%3A%20OWLed%3A%20Outlier-weighed%20Layerwise%20Pruning%20for%20Efficient%20Autonomous%0A%20%20Driving%20Framework%0AAuthor%3A%20Jiaxi%20Li%20and%20Lu%20Yin%20and%20Xilu%20Wang%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20autonomous%20driving%0Asystems%20offers%20promising%20enhancements%20in%20environmental%20understanding%20and%0Adecision-making.%20However%2C%20the%20substantial%20computational%20demands%20of%20deploying%0ALLMs%20locally%20on%20vehicles%20render%20this%20approach%20unfeasible%20for%20real-world%0Aautomotive%20applications.%20To%20address%20this%20challenge%2C%20we%20introduce%20OWLed%2C%20the%0AOutlier-Weighed%20Layerwise%20Pruning%20for%20Efficient%20Autonomous%20Driving%20Framework%0Athat%20leverages%20outlier-weighted%20layerwise%20sparsity%20for%20model%20compression.%20Our%0Amethod%20assigns%20non-uniform%20sparsity%20ratios%20to%20different%20layers%20based%20on%20the%0Adistribution%20of%20outlier%20features%2C%20significantly%20reducing%20the%20model%20size%20without%0Athe%20need%20for%20fine-tuning.%20To%20ensure%20the%20compressed%20model%20adapts%20well%20to%0Aautonomous%20driving%20tasks%2C%20we%20incorporate%20driving%20environment%20data%20into%20both%20the%0Acalibration%20and%20pruning%20processes.%20Our%20empirical%20studies%20reveal%20that%20the%0Aencoder%20component%20is%20more%20sensitive%20to%20pruning%20than%20the%20LLM%2C%20highlighting%20its%0Acritical%20role%20in%20the%20system.%20Experimental%20results%20demonstrate%20that%20OWLed%0Aoutperforms%20existing%20methods%20in%20perception%2C%20action%20prediction%2C%20and%20language%0Aunderstanding%20while%20substantially%20lowering%20computational%20requirements.%20These%0Afindings%20underscore%20the%20potential%20of%20combining%20advanced%20pruning%20techniques%20with%0ALLMs%20to%20develop%20efficient%20and%20robust%20autonomous%20driving%20systems%20capable%20of%0Ahandling%20complex%20scenarios.%20Code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOWLed%253A%2520Outlier-weighed%2520Layerwise%2520Pruning%2520for%2520Efficient%2520Autonomous%250A%2520%2520Driving%2520Framework%26entry.906535625%3DJiaxi%2520Li%2520and%2520Lu%2520Yin%2520and%2520Xilu%2520Wang%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520autonomous%2520driving%250Asystems%2520offers%2520promising%2520enhancements%2520in%2520environmental%2520understanding%2520and%250Adecision-making.%2520However%252C%2520the%2520substantial%2520computational%2520demands%2520of%2520deploying%250ALLMs%2520locally%2520on%2520vehicles%2520render%2520this%2520approach%2520unfeasible%2520for%2520real-world%250Aautomotive%2520applications.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520OWLed%252C%2520the%250AOutlier-Weighed%2520Layerwise%2520Pruning%2520for%2520Efficient%2520Autonomous%2520Driving%2520Framework%250Athat%2520leverages%2520outlier-weighted%2520layerwise%2520sparsity%2520for%2520model%2520compression.%2520Our%250Amethod%2520assigns%2520non-uniform%2520sparsity%2520ratios%2520to%2520different%2520layers%2520based%2520on%2520the%250Adistribution%2520of%2520outlier%2520features%252C%2520significantly%2520reducing%2520the%2520model%2520size%2520without%250Athe%2520need%2520for%2520fine-tuning.%2520To%2520ensure%2520the%2520compressed%2520model%2520adapts%2520well%2520to%250Aautonomous%2520driving%2520tasks%252C%2520we%2520incorporate%2520driving%2520environment%2520data%2520into%2520both%2520the%250Acalibration%2520and%2520pruning%2520processes.%2520Our%2520empirical%2520studies%2520reveal%2520that%2520the%250Aencoder%2520component%2520is%2520more%2520sensitive%2520to%2520pruning%2520than%2520the%2520LLM%252C%2520highlighting%2520its%250Acritical%2520role%2520in%2520the%2520system.%2520Experimental%2520results%2520demonstrate%2520that%2520OWLed%250Aoutperforms%2520existing%2520methods%2520in%2520perception%252C%2520action%2520prediction%252C%2520and%2520language%250Aunderstanding%2520while%2520substantially%2520lowering%2520computational%2520requirements.%2520These%250Afindings%2520underscore%2520the%2520potential%2520of%2520combining%2520advanced%2520pruning%2520techniques%2520with%250ALLMs%2520to%2520develop%2520efficient%2520and%2520robust%2520autonomous%2520driving%2520systems%2520capable%2520of%250Ahandling%2520complex%2520scenarios.%2520Code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OWLed%3A%20Outlier-weighed%20Layerwise%20Pruning%20for%20Efficient%20Autonomous%0A%20%20Driving%20Framework&entry.906535625=Jiaxi%20Li%20and%20Lu%20Yin%20and%20Xilu%20Wang&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20autonomous%20driving%0Asystems%20offers%20promising%20enhancements%20in%20environmental%20understanding%20and%0Adecision-making.%20However%2C%20the%20substantial%20computational%20demands%20of%20deploying%0ALLMs%20locally%20on%20vehicles%20render%20this%20approach%20unfeasible%20for%20real-world%0Aautomotive%20applications.%20To%20address%20this%20challenge%2C%20we%20introduce%20OWLed%2C%20the%0AOutlier-Weighed%20Layerwise%20Pruning%20for%20Efficient%20Autonomous%20Driving%20Framework%0Athat%20leverages%20outlier-weighted%20layerwise%20sparsity%20for%20model%20compression.%20Our%0Amethod%20assigns%20non-uniform%20sparsity%20ratios%20to%20different%20layers%20based%20on%20the%0Adistribution%20of%20outlier%20features%2C%20significantly%20reducing%20the%20model%20size%20without%0Athe%20need%20for%20fine-tuning.%20To%20ensure%20the%20compressed%20model%20adapts%20well%20to%0Aautonomous%20driving%20tasks%2C%20we%20incorporate%20driving%20environment%20data%20into%20both%20the%0Acalibration%20and%20pruning%20processes.%20Our%20empirical%20studies%20reveal%20that%20the%0Aencoder%20component%20is%20more%20sensitive%20to%20pruning%20than%20the%20LLM%2C%20highlighting%20its%0Acritical%20role%20in%20the%20system.%20Experimental%20results%20demonstrate%20that%20OWLed%0Aoutperforms%20existing%20methods%20in%20perception%2C%20action%20prediction%2C%20and%20language%0Aunderstanding%20while%20substantially%20lowering%20computational%20requirements.%20These%0Afindings%20underscore%20the%20potential%20of%20combining%20advanced%20pruning%20techniques%20with%0ALLMs%20to%20develop%20efficient%20and%20robust%20autonomous%20driving%20systems%20capable%20of%0Ahandling%20complex%20scenarios.%20Code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07711v1&entry.124074799=Read"},
{"title": "L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection", "author": "Xun Huang and Ziyu Xu and Hai Wu and Jinlong Wang and Qiming Xia and Yan Xia and Jonathan Li and Kyle Gao and Chenglu Wen and Cheng Wang", "abstract": "  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n", "link": "http://arxiv.org/abs/2408.03677v4", "date": "2024-11-12", "relevancy": 2.0988, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.526}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&body=Title%3A%20L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection%0AAuthor%3A%20Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2020.0%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03677v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL4DR%253A%2520LiDAR-4DRadar%2520Fusion%2520for%2520Weather-Robust%25203D%2520Object%2520Detection%26entry.906535625%3DXun%2520Huang%2520and%2520Ziyu%2520Xu%2520and%2520Hai%2520Wu%2520and%2520Jinlong%2520Wang%2520and%2520Qiming%2520Xia%2520and%2520Yan%2520Xia%2520and%2520Jonathan%2520Li%2520and%2520Kyle%2520Gao%2520and%2520Chenglu%2520Wen%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520LiDAR-based%2520vision%2520systems%2520are%2520integral%2520for%25203D%2520object%2520detection%252C%2520which%2520is%250Acrucial%2520for%2520autonomous%2520navigation.%2520However%252C%2520they%2520suffer%2520from%2520performance%250Adegradation%2520in%2520adverse%2520weather%2520conditions%2520due%2520to%2520the%2520quality%2520deterioration%2520of%250ALiDAR%2520point%2520clouds.%2520Fusing%2520LiDAR%2520with%2520the%2520weather-robust%25204D%2520radar%2520sensor%2520is%250Aexpected%2520to%2520solve%2520this%2520problem.%2520However%252C%2520the%2520fusion%2520of%2520LiDAR%2520and%25204D%2520radar%2520is%250Achallenging%2520because%2520they%2520differ%2520significantly%2520in%2520terms%2520of%2520data%2520quality%2520and%2520the%250Adegree%2520of%2520degradation%2520in%2520adverse%2520weather.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250AL4DR%252C%2520a%2520weather-robust%25203D%2520object%2520detection%2520method%2520that%2520effectively%2520achieves%250ALiDAR%2520and%25204D%2520Radar%2520fusion.%2520Our%2520L4DR%2520includes%2520Multi-Modal%2520Encoding%2520%2528MME%2529%2520and%250AForeground-Aware%2520Denoising%2520%2528FAD%2529%2520technique%2520to%2520reconcile%2520sensor%2520gaps%252C%2520which%2520is%250Athe%2520first%2520exploration%2520of%2520the%2520complementarity%2520of%2520early%2520fusion%2520between%2520LiDAR%2520and%250A4D%2520radar.%2520Additionally%252C%2520we%2520design%2520an%2520Inter-Modal%2520and%2520Intra-Modal%2520%2528%257BIM%257D2%2520%2529%250Aparallel%2520feature%2520extraction%2520backbone%2520coupled%2520with%2520a%2520Multi-Scale%2520Gated%2520Fusion%250A%2528MSGF%2529%2520module%2520to%2520counteract%2520the%2520varying%2520degrees%2520of%2520sensor%2520degradation%2520under%250Aadverse%2520weather%2520conditions.%2520Experimental%2520evaluation%2520on%2520a%2520VoD%2520dataset%2520with%250Asimulated%2520fog%2520proves%2520that%2520L4DR%2520is%2520more%2520adaptable%2520to%2520changing%2520weather%250Aconditions.%2520It%2520delivers%2520a%2520significant%2520performance%2520increase%2520under%2520different%2520fog%250Alevels%252C%2520improving%2520the%25203D%2520mAP%2520by%2520up%2520to%252020.0%2525%2520over%2520the%2520traditional%2520LiDAR-only%250Aapproach.%2520Moreover%252C%2520the%2520results%2520on%2520the%2520K-Radar%2520dataset%2520validate%2520the%2520consistent%250Aperformance%2520improvement%2520of%2520L4DR%2520in%2520real-world%2520adverse%2520weather%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03677v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L4DR%3A%20LiDAR-4DRadar%20Fusion%20for%20Weather-Robust%203D%20Object%20Detection&entry.906535625=Xun%20Huang%20and%20Ziyu%20Xu%20and%20Hai%20Wu%20and%20Jinlong%20Wang%20and%20Qiming%20Xia%20and%20Yan%20Xia%20and%20Jonathan%20Li%20and%20Kyle%20Gao%20and%20Chenglu%20Wen%20and%20Cheng%20Wang&entry.1292438233=%20%20LiDAR-based%20vision%20systems%20are%20integral%20for%203D%20object%20detection%2C%20which%20is%0Acrucial%20for%20autonomous%20navigation.%20However%2C%20they%20suffer%20from%20performance%0Adegradation%20in%20adverse%20weather%20conditions%20due%20to%20the%20quality%20deterioration%20of%0ALiDAR%20point%20clouds.%20Fusing%20LiDAR%20with%20the%20weather-robust%204D%20radar%20sensor%20is%0Aexpected%20to%20solve%20this%20problem.%20However%2C%20the%20fusion%20of%20LiDAR%20and%204D%20radar%20is%0Achallenging%20because%20they%20differ%20significantly%20in%20terms%20of%20data%20quality%20and%20the%0Adegree%20of%20degradation%20in%20adverse%20weather.%20To%20address%20these%20issues%2C%20we%20introduce%0AL4DR%2C%20a%20weather-robust%203D%20object%20detection%20method%20that%20effectively%20achieves%0ALiDAR%20and%204D%20Radar%20fusion.%20Our%20L4DR%20includes%20Multi-Modal%20Encoding%20%28MME%29%20and%0AForeground-Aware%20Denoising%20%28FAD%29%20technique%20to%20reconcile%20sensor%20gaps%2C%20which%20is%0Athe%20first%20exploration%20of%20the%20complementarity%20of%20early%20fusion%20between%20LiDAR%20and%0A4D%20radar.%20Additionally%2C%20we%20design%20an%20Inter-Modal%20and%20Intra-Modal%20%28%7BIM%7D2%20%29%0Aparallel%20feature%20extraction%20backbone%20coupled%20with%20a%20Multi-Scale%20Gated%20Fusion%0A%28MSGF%29%20module%20to%20counteract%20the%20varying%20degrees%20of%20sensor%20degradation%20under%0Aadverse%20weather%20conditions.%20Experimental%20evaluation%20on%20a%20VoD%20dataset%20with%0Asimulated%20fog%20proves%20that%20L4DR%20is%20more%20adaptable%20to%20changing%20weather%0Aconditions.%20It%20delivers%20a%20significant%20performance%20increase%20under%20different%20fog%0Alevels%2C%20improving%20the%203D%20mAP%20by%20up%20to%2020.0%25%20over%20the%20traditional%20LiDAR-only%0Aapproach.%20Moreover%2C%20the%20results%20on%20the%20K-Radar%20dataset%20validate%20the%20consistent%0Aperformance%20improvement%20of%20L4DR%20in%20real-world%20adverse%20weather%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03677v4&entry.124074799=Read"},
{"title": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible\n  Architectures Reveal Poor Generalization", "author": "Davide Buffelli and Jamie McGowan and Wangkun Xu and Alexandru Cioba and Da-shan Shiu and Guillaume Hennequin and Alberto Bernacchia", "abstract": "  Second-order optimization has been shown to accelerate the training of deep\nneural networks in many applications, often yielding faster progress per\niteration on the training loss compared to first-order optimizers.However, the\ngeneralization properties of second-order methods are still being debated.\nTheoretical investigations have proved difficult to carry out outside the\ntractable settings of heavily simplified model classes -- thus, the relevance\nof existing theories to practical deep learning applications remains unclear.\nSimilarly, empirical studies in large-scale models and real datasets are\nsignificantly confounded by the necessity to approximate second-order updates\nin practice. It is often unclear whether the observed generalization behaviour\narises specifically from the second-order nature of the parameter updates, or\ninstead reflects the specific structured (e.g.\\ Kronecker) approximations used\nor any damping-based interpolation towards first-order updates. Here, we show\nfor the first time that exact Gauss-Newton (GN) updates take on a tractable\nform in a class of deep reversible architectures that are sufficiently\nexpressive to be meaningfully applied to common benchmark datasets. We exploit\nthis novel setting to study the training and generalization properties of the\nGN optimizer. We find that exact GN generalizes poorly. In the mini-batch\ntraining setting, this manifests as rapidly saturating progress even on the\n\\emph{training} loss, with parameter updates found to overfit each\nmini-batchatch without producing the features that would support generalization\nto other mini-batches. We show that our experiments run in the ``lazy'' regime,\nin which the neural tangent kernel (NTK) changes very little during the course\nof training. This behaviour is associated with having no significant changes in\nneural representations, explaining the lack of generalization.\n", "link": "http://arxiv.org/abs/2411.07979v1", "date": "2024-11-12", "relevancy": 2.0966, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5219}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization&body=Title%3A%20Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization%0AAuthor%3A%20Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Wangkun%20Xu%20and%20Alexandru%20Cioba%20and%20Da-shan%20Shiu%20and%20Guillaume%20Hennequin%20and%20Alberto%20Bernacchia%0AAbstract%3A%20%20%20Second-order%20optimization%20has%20been%20shown%20to%20accelerate%20the%20training%20of%20deep%0Aneural%20networks%20in%20many%20applications%2C%20often%20yielding%20faster%20progress%20per%0Aiteration%20on%20the%20training%20loss%20compared%20to%20first-order%20optimizers.However%2C%20the%0Ageneralization%20properties%20of%20second-order%20methods%20are%20still%20being%20debated.%0ATheoretical%20investigations%20have%20proved%20difficult%20to%20carry%20out%20outside%20the%0Atractable%20settings%20of%20heavily%20simplified%20model%20classes%20--%20thus%2C%20the%20relevance%0Aof%20existing%20theories%20to%20practical%20deep%20learning%20applications%20remains%20unclear.%0ASimilarly%2C%20empirical%20studies%20in%20large-scale%20models%20and%20real%20datasets%20are%0Asignificantly%20confounded%20by%20the%20necessity%20to%20approximate%20second-order%20updates%0Ain%20practice.%20It%20is%20often%20unclear%20whether%20the%20observed%20generalization%20behaviour%0Aarises%20specifically%20from%20the%20second-order%20nature%20of%20the%20parameter%20updates%2C%20or%0Ainstead%20reflects%20the%20specific%20structured%20%28e.g.%5C%20Kronecker%29%20approximations%20used%0Aor%20any%20damping-based%20interpolation%20towards%20first-order%20updates.%20Here%2C%20we%20show%0Afor%20the%20first%20time%20that%20exact%20Gauss-Newton%20%28GN%29%20updates%20take%20on%20a%20tractable%0Aform%20in%20a%20class%20of%20deep%20reversible%20architectures%20that%20are%20sufficiently%0Aexpressive%20to%20be%20meaningfully%20applied%20to%20common%20benchmark%20datasets.%20We%20exploit%0Athis%20novel%20setting%20to%20study%20the%20training%20and%20generalization%20properties%20of%20the%0AGN%20optimizer.%20We%20find%20that%20exact%20GN%20generalizes%20poorly.%20In%20the%20mini-batch%0Atraining%20setting%2C%20this%20manifests%20as%20rapidly%20saturating%20progress%20even%20on%20the%0A%5Cemph%7Btraining%7D%20loss%2C%20with%20parameter%20updates%20found%20to%20overfit%20each%0Amini-batchatch%20without%20producing%20the%20features%20that%20would%20support%20generalization%0Ato%20other%20mini-batches.%20We%20show%20that%20our%20experiments%20run%20in%20the%20%60%60lazy%27%27%20regime%2C%0Ain%20which%20the%20neural%20tangent%20kernel%20%28NTK%29%20changes%20very%20little%20during%20the%20course%0Aof%20training.%20This%20behaviour%20is%20associated%20with%20having%20no%20significant%20changes%20in%0Aneural%20representations%2C%20explaining%20the%20lack%20of%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%252C%2520Tractable%2520Gauss-Newton%2520Optimization%2520in%2520Deep%2520Reversible%250A%2520%2520Architectures%2520Reveal%2520Poor%2520Generalization%26entry.906535625%3DDavide%2520Buffelli%2520and%2520Jamie%2520McGowan%2520and%2520Wangkun%2520Xu%2520and%2520Alexandru%2520Cioba%2520and%2520Da-shan%2520Shiu%2520and%2520Guillaume%2520Hennequin%2520and%2520Alberto%2520Bernacchia%26entry.1292438233%3D%2520%2520Second-order%2520optimization%2520has%2520been%2520shown%2520to%2520accelerate%2520the%2520training%2520of%2520deep%250Aneural%2520networks%2520in%2520many%2520applications%252C%2520often%2520yielding%2520faster%2520progress%2520per%250Aiteration%2520on%2520the%2520training%2520loss%2520compared%2520to%2520first-order%2520optimizers.However%252C%2520the%250Ageneralization%2520properties%2520of%2520second-order%2520methods%2520are%2520still%2520being%2520debated.%250ATheoretical%2520investigations%2520have%2520proved%2520difficult%2520to%2520carry%2520out%2520outside%2520the%250Atractable%2520settings%2520of%2520heavily%2520simplified%2520model%2520classes%2520--%2520thus%252C%2520the%2520relevance%250Aof%2520existing%2520theories%2520to%2520practical%2520deep%2520learning%2520applications%2520remains%2520unclear.%250ASimilarly%252C%2520empirical%2520studies%2520in%2520large-scale%2520models%2520and%2520real%2520datasets%2520are%250Asignificantly%2520confounded%2520by%2520the%2520necessity%2520to%2520approximate%2520second-order%2520updates%250Ain%2520practice.%2520It%2520is%2520often%2520unclear%2520whether%2520the%2520observed%2520generalization%2520behaviour%250Aarises%2520specifically%2520from%2520the%2520second-order%2520nature%2520of%2520the%2520parameter%2520updates%252C%2520or%250Ainstead%2520reflects%2520the%2520specific%2520structured%2520%2528e.g.%255C%2520Kronecker%2529%2520approximations%2520used%250Aor%2520any%2520damping-based%2520interpolation%2520towards%2520first-order%2520updates.%2520Here%252C%2520we%2520show%250Afor%2520the%2520first%2520time%2520that%2520exact%2520Gauss-Newton%2520%2528GN%2529%2520updates%2520take%2520on%2520a%2520tractable%250Aform%2520in%2520a%2520class%2520of%2520deep%2520reversible%2520architectures%2520that%2520are%2520sufficiently%250Aexpressive%2520to%2520be%2520meaningfully%2520applied%2520to%2520common%2520benchmark%2520datasets.%2520We%2520exploit%250Athis%2520novel%2520setting%2520to%2520study%2520the%2520training%2520and%2520generalization%2520properties%2520of%2520the%250AGN%2520optimizer.%2520We%2520find%2520that%2520exact%2520GN%2520generalizes%2520poorly.%2520In%2520the%2520mini-batch%250Atraining%2520setting%252C%2520this%2520manifests%2520as%2520rapidly%2520saturating%2520progress%2520even%2520on%2520the%250A%255Cemph%257Btraining%257D%2520loss%252C%2520with%2520parameter%2520updates%2520found%2520to%2520overfit%2520each%250Amini-batchatch%2520without%2520producing%2520the%2520features%2520that%2520would%2520support%2520generalization%250Ato%2520other%2520mini-batches.%2520We%2520show%2520that%2520our%2520experiments%2520run%2520in%2520the%2520%2560%2560lazy%2527%2527%2520regime%252C%250Ain%2520which%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%2520changes%2520very%2520little%2520during%2520the%2520course%250Aof%2520training.%2520This%2520behaviour%2520is%2520associated%2520with%2520having%2520no%2520significant%2520changes%2520in%250Aneural%2520representations%252C%2520explaining%2520the%2520lack%2520of%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization&entry.906535625=Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Wangkun%20Xu%20and%20Alexandru%20Cioba%20and%20Da-shan%20Shiu%20and%20Guillaume%20Hennequin%20and%20Alberto%20Bernacchia&entry.1292438233=%20%20Second-order%20optimization%20has%20been%20shown%20to%20accelerate%20the%20training%20of%20deep%0Aneural%20networks%20in%20many%20applications%2C%20often%20yielding%20faster%20progress%20per%0Aiteration%20on%20the%20training%20loss%20compared%20to%20first-order%20optimizers.However%2C%20the%0Ageneralization%20properties%20of%20second-order%20methods%20are%20still%20being%20debated.%0ATheoretical%20investigations%20have%20proved%20difficult%20to%20carry%20out%20outside%20the%0Atractable%20settings%20of%20heavily%20simplified%20model%20classes%20--%20thus%2C%20the%20relevance%0Aof%20existing%20theories%20to%20practical%20deep%20learning%20applications%20remains%20unclear.%0ASimilarly%2C%20empirical%20studies%20in%20large-scale%20models%20and%20real%20datasets%20are%0Asignificantly%20confounded%20by%20the%20necessity%20to%20approximate%20second-order%20updates%0Ain%20practice.%20It%20is%20often%20unclear%20whether%20the%20observed%20generalization%20behaviour%0Aarises%20specifically%20from%20the%20second-order%20nature%20of%20the%20parameter%20updates%2C%20or%0Ainstead%20reflects%20the%20specific%20structured%20%28e.g.%5C%20Kronecker%29%20approximations%20used%0Aor%20any%20damping-based%20interpolation%20towards%20first-order%20updates.%20Here%2C%20we%20show%0Afor%20the%20first%20time%20that%20exact%20Gauss-Newton%20%28GN%29%20updates%20take%20on%20a%20tractable%0Aform%20in%20a%20class%20of%20deep%20reversible%20architectures%20that%20are%20sufficiently%0Aexpressive%20to%20be%20meaningfully%20applied%20to%20common%20benchmark%20datasets.%20We%20exploit%0Athis%20novel%20setting%20to%20study%20the%20training%20and%20generalization%20properties%20of%20the%0AGN%20optimizer.%20We%20find%20that%20exact%20GN%20generalizes%20poorly.%20In%20the%20mini-batch%0Atraining%20setting%2C%20this%20manifests%20as%20rapidly%20saturating%20progress%20even%20on%20the%0A%5Cemph%7Btraining%7D%20loss%2C%20with%20parameter%20updates%20found%20to%20overfit%20each%0Amini-batchatch%20without%20producing%20the%20features%20that%20would%20support%20generalization%0Ato%20other%20mini-batches.%20We%20show%20that%20our%20experiments%20run%20in%20the%20%60%60lazy%27%27%20regime%2C%0Ain%20which%20the%20neural%20tangent%20kernel%20%28NTK%29%20changes%20very%20little%20during%20the%20course%0Aof%20training.%20This%20behaviour%20is%20associated%20with%20having%20no%20significant%20changes%20in%0Aneural%20representations%2C%20explaining%20the%20lack%20of%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07979v1&entry.124074799=Read"},
{"title": "Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain\n  Adaptation using a Gaussian Mixture Model", "author": "Pascal Schlachter and Simon Wagner and Bin Yang", "abstract": "  In practice, domain shifts are likely to occur between training and test\ndata, necessitating domain adaptation (DA) to adjust the pre-trained source\nmodel to the target domain. Recently, universal domain adaptation (UniDA) has\ngained attention for addressing the possibility of an additional category\n(label) shift between the source and target domain. This means new classes can\nappear in the target data, some source classes may no longer be present, or\nboth at the same time. For practical applicability, UniDA methods must handle\nboth source-free and online scenarios, enabling adaptation without access to\nthe source data and performing batch-wise updates in parallel with prediction.\nIn an online setting, preserving knowledge across batches is crucial. However,\nexisting methods often require substantial memory, which is impractical because\nmemory is limited and valuable, in particular on embedded systems. Therefore,\nwe consider memory-efficiency as an additional constraint. To achieve\nmemory-efficient online source-free universal domain adaptation (SF-UniDA), we\npropose a novel method that continuously captures the distribution of known\nclasses in the feature space using a Gaussian mixture model (GMM). This\napproach, combined with entropy-based out-of-distribution detection, allows for\nthe generation of reliable pseudo-labels. Finally, we combine a contrastive\nloss with a KL divergence loss to perform the adaptation. Our approach not only\nachieves state-of-the-art results in all experiments on the DomainNet and\nOffice-Home datasets but also significantly outperforms the existing methods on\nthe challenging VisDA-C dataset, setting a new benchmark for online SF-UniDA.\nOur code is available at https://github.com/pascalschlachter/GMM.\n", "link": "http://arxiv.org/abs/2407.14208v2", "date": "2024-11-12", "relevancy": 2.0946, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5318}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%20Pseudo-Labeling%20for%20Online%20Source-Free%20Universal%20Domain%0A%20%20Adaptation%20using%20a%20Gaussian%20Mixture%20Model&body=Title%3A%20Memory-Efficient%20Pseudo-Labeling%20for%20Online%20Source-Free%20Universal%20Domain%0A%20%20Adaptation%20using%20a%20Gaussian%20Mixture%20Model%0AAuthor%3A%20Pascal%20Schlachter%20and%20Simon%20Wagner%20and%20Bin%20Yang%0AAbstract%3A%20%20%20In%20practice%2C%20domain%20shifts%20are%20likely%20to%20occur%20between%20training%20and%20test%0Adata%2C%20necessitating%20domain%20adaptation%20%28DA%29%20to%20adjust%20the%20pre-trained%20source%0Amodel%20to%20the%20target%20domain.%20Recently%2C%20universal%20domain%20adaptation%20%28UniDA%29%20has%0Agained%20attention%20for%20addressing%20the%20possibility%20of%20an%20additional%20category%0A%28label%29%20shift%20between%20the%20source%20and%20target%20domain.%20This%20means%20new%20classes%20can%0Aappear%20in%20the%20target%20data%2C%20some%20source%20classes%20may%20no%20longer%20be%20present%2C%20or%0Aboth%20at%20the%20same%20time.%20For%20practical%20applicability%2C%20UniDA%20methods%20must%20handle%0Aboth%20source-free%20and%20online%20scenarios%2C%20enabling%20adaptation%20without%20access%20to%0Athe%20source%20data%20and%20performing%20batch-wise%20updates%20in%20parallel%20with%20prediction.%0AIn%20an%20online%20setting%2C%20preserving%20knowledge%20across%20batches%20is%20crucial.%20However%2C%0Aexisting%20methods%20often%20require%20substantial%20memory%2C%20which%20is%20impractical%20because%0Amemory%20is%20limited%20and%20valuable%2C%20in%20particular%20on%20embedded%20systems.%20Therefore%2C%0Awe%20consider%20memory-efficiency%20as%20an%20additional%20constraint.%20To%20achieve%0Amemory-efficient%20online%20source-free%20universal%20domain%20adaptation%20%28SF-UniDA%29%2C%20we%0Apropose%20a%20novel%20method%20that%20continuously%20captures%20the%20distribution%20of%20known%0Aclasses%20in%20the%20feature%20space%20using%20a%20Gaussian%20mixture%20model%20%28GMM%29.%20This%0Aapproach%2C%20combined%20with%20entropy-based%20out-of-distribution%20detection%2C%20allows%20for%0Athe%20generation%20of%20reliable%20pseudo-labels.%20Finally%2C%20we%20combine%20a%20contrastive%0Aloss%20with%20a%20KL%20divergence%20loss%20to%20perform%20the%20adaptation.%20Our%20approach%20not%20only%0Aachieves%20state-of-the-art%20results%20in%20all%20experiments%20on%20the%20DomainNet%20and%0AOffice-Home%20datasets%20but%20also%20significantly%20outperforms%20the%20existing%20methods%20on%0Athe%20challenging%20VisDA-C%20dataset%2C%20setting%20a%20new%20benchmark%20for%20online%20SF-UniDA.%0AOur%20code%20is%20available%20at%20https%3A//github.com/pascalschlachter/GMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%2520Pseudo-Labeling%2520for%2520Online%2520Source-Free%2520Universal%2520Domain%250A%2520%2520Adaptation%2520using%2520a%2520Gaussian%2520Mixture%2520Model%26entry.906535625%3DPascal%2520Schlachter%2520and%2520Simon%2520Wagner%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520In%2520practice%252C%2520domain%2520shifts%2520are%2520likely%2520to%2520occur%2520between%2520training%2520and%2520test%250Adata%252C%2520necessitating%2520domain%2520adaptation%2520%2528DA%2529%2520to%2520adjust%2520the%2520pre-trained%2520source%250Amodel%2520to%2520the%2520target%2520domain.%2520Recently%252C%2520universal%2520domain%2520adaptation%2520%2528UniDA%2529%2520has%250Agained%2520attention%2520for%2520addressing%2520the%2520possibility%2520of%2520an%2520additional%2520category%250A%2528label%2529%2520shift%2520between%2520the%2520source%2520and%2520target%2520domain.%2520This%2520means%2520new%2520classes%2520can%250Aappear%2520in%2520the%2520target%2520data%252C%2520some%2520source%2520classes%2520may%2520no%2520longer%2520be%2520present%252C%2520or%250Aboth%2520at%2520the%2520same%2520time.%2520For%2520practical%2520applicability%252C%2520UniDA%2520methods%2520must%2520handle%250Aboth%2520source-free%2520and%2520online%2520scenarios%252C%2520enabling%2520adaptation%2520without%2520access%2520to%250Athe%2520source%2520data%2520and%2520performing%2520batch-wise%2520updates%2520in%2520parallel%2520with%2520prediction.%250AIn%2520an%2520online%2520setting%252C%2520preserving%2520knowledge%2520across%2520batches%2520is%2520crucial.%2520However%252C%250Aexisting%2520methods%2520often%2520require%2520substantial%2520memory%252C%2520which%2520is%2520impractical%2520because%250Amemory%2520is%2520limited%2520and%2520valuable%252C%2520in%2520particular%2520on%2520embedded%2520systems.%2520Therefore%252C%250Awe%2520consider%2520memory-efficiency%2520as%2520an%2520additional%2520constraint.%2520To%2520achieve%250Amemory-efficient%2520online%2520source-free%2520universal%2520domain%2520adaptation%2520%2528SF-UniDA%2529%252C%2520we%250Apropose%2520a%2520novel%2520method%2520that%2520continuously%2520captures%2520the%2520distribution%2520of%2520known%250Aclasses%2520in%2520the%2520feature%2520space%2520using%2520a%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529.%2520This%250Aapproach%252C%2520combined%2520with%2520entropy-based%2520out-of-distribution%2520detection%252C%2520allows%2520for%250Athe%2520generation%2520of%2520reliable%2520pseudo-labels.%2520Finally%252C%2520we%2520combine%2520a%2520contrastive%250Aloss%2520with%2520a%2520KL%2520divergence%2520loss%2520to%2520perform%2520the%2520adaptation.%2520Our%2520approach%2520not%2520only%250Aachieves%2520state-of-the-art%2520results%2520in%2520all%2520experiments%2520on%2520the%2520DomainNet%2520and%250AOffice-Home%2520datasets%2520but%2520also%2520significantly%2520outperforms%2520the%2520existing%2520methods%2520on%250Athe%2520challenging%2520VisDA-C%2520dataset%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520online%2520SF-UniDA.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/pascalschlachter/GMM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%20Pseudo-Labeling%20for%20Online%20Source-Free%20Universal%20Domain%0A%20%20Adaptation%20using%20a%20Gaussian%20Mixture%20Model&entry.906535625=Pascal%20Schlachter%20and%20Simon%20Wagner%20and%20Bin%20Yang&entry.1292438233=%20%20In%20practice%2C%20domain%20shifts%20are%20likely%20to%20occur%20between%20training%20and%20test%0Adata%2C%20necessitating%20domain%20adaptation%20%28DA%29%20to%20adjust%20the%20pre-trained%20source%0Amodel%20to%20the%20target%20domain.%20Recently%2C%20universal%20domain%20adaptation%20%28UniDA%29%20has%0Agained%20attention%20for%20addressing%20the%20possibility%20of%20an%20additional%20category%0A%28label%29%20shift%20between%20the%20source%20and%20target%20domain.%20This%20means%20new%20classes%20can%0Aappear%20in%20the%20target%20data%2C%20some%20source%20classes%20may%20no%20longer%20be%20present%2C%20or%0Aboth%20at%20the%20same%20time.%20For%20practical%20applicability%2C%20UniDA%20methods%20must%20handle%0Aboth%20source-free%20and%20online%20scenarios%2C%20enabling%20adaptation%20without%20access%20to%0Athe%20source%20data%20and%20performing%20batch-wise%20updates%20in%20parallel%20with%20prediction.%0AIn%20an%20online%20setting%2C%20preserving%20knowledge%20across%20batches%20is%20crucial.%20However%2C%0Aexisting%20methods%20often%20require%20substantial%20memory%2C%20which%20is%20impractical%20because%0Amemory%20is%20limited%20and%20valuable%2C%20in%20particular%20on%20embedded%20systems.%20Therefore%2C%0Awe%20consider%20memory-efficiency%20as%20an%20additional%20constraint.%20To%20achieve%0Amemory-efficient%20online%20source-free%20universal%20domain%20adaptation%20%28SF-UniDA%29%2C%20we%0Apropose%20a%20novel%20method%20that%20continuously%20captures%20the%20distribution%20of%20known%0Aclasses%20in%20the%20feature%20space%20using%20a%20Gaussian%20mixture%20model%20%28GMM%29.%20This%0Aapproach%2C%20combined%20with%20entropy-based%20out-of-distribution%20detection%2C%20allows%20for%0Athe%20generation%20of%20reliable%20pseudo-labels.%20Finally%2C%20we%20combine%20a%20contrastive%0Aloss%20with%20a%20KL%20divergence%20loss%20to%20perform%20the%20adaptation.%20Our%20approach%20not%20only%0Aachieves%20state-of-the-art%20results%20in%20all%20experiments%20on%20the%20DomainNet%20and%0AOffice-Home%20datasets%20but%20also%20significantly%20outperforms%20the%20existing%20methods%20on%0Athe%20challenging%20VisDA-C%20dataset%2C%20setting%20a%20new%20benchmark%20for%20online%20SF-UniDA.%0AOur%20code%20is%20available%20at%20https%3A//github.com/pascalschlachter/GMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14208v2&entry.124074799=Read"},
{"title": "DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring", "author": "Mahmut S. Gokmen and Cody Bumgardner and Caner Ozcan", "abstract": "  Coronary artery disease (CAD), one of the most common cause of mortality in\nthe world. Coronary artery calcium (CAC) scoring using computed tomography (CT)\nis key for risk assessment to prevent coronary disease. Previous studies on\nrisk assessment and calcification detection in CT scans primarily use\napproaches based on UNET architecture, frequently implemented on pre-built\nmodels. However, these models are limited by the availability of annotated CT\nscans containing CAC and suffering from imbalanced dataset, decreasing\nperformance of CAC segmentation and scoring. In this study, we extend this\napproach by incorporating the self-supervised learning (SSL) technique of DINO\n(self-distillation with no labels) to eliminate limitations of scarce annotated\ndata in CT scans. The DINO model's ability to train without requiring CAC area\nannotations enhances its robustness in generating distinct features. The DINO\nmodel is trained on to focus specifically on calcified areas by using labels,\naiming to generate features that effectively capture and highlight key\ncharacteristics. The label-guided DINO (DINO-LG) enhances classification by\ndistinguishing CT slices that contain calcification from those that do not,\nperforming 57% better than the standard DINO model in this task. CAC scoring\nand segmentation tasks are performed by a basic U-NET architecture, fed\nspecifically with CT slices containing calcified areas as identified by the\nDINO-LG model. This targeted identification performed by DINO-LG model improves\nCAC segmentation performance by approximately 10% and significant increase in\nCAC scoring accuracy.\n", "link": "http://arxiv.org/abs/2411.07976v1", "date": "2024-11-12", "relevancy": 2.0912, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring&body=Title%3A%20DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring%0AAuthor%3A%20Mahmut%20S.%20Gokmen%20and%20Cody%20Bumgardner%20and%20Caner%20Ozcan%0AAbstract%3A%20%20%20Coronary%20artery%20disease%20%28CAD%29%2C%20one%20of%20the%20most%20common%20cause%20of%20mortality%20in%0Athe%20world.%20Coronary%20artery%20calcium%20%28CAC%29%20scoring%20using%20computed%20tomography%20%28CT%29%0Ais%20key%20for%20risk%20assessment%20to%20prevent%20coronary%20disease.%20Previous%20studies%20on%0Arisk%20assessment%20and%20calcification%20detection%20in%20CT%20scans%20primarily%20use%0Aapproaches%20based%20on%20UNET%20architecture%2C%20frequently%20implemented%20on%20pre-built%0Amodels.%20However%2C%20these%20models%20are%20limited%20by%20the%20availability%20of%20annotated%20CT%0Ascans%20containing%20CAC%20and%20suffering%20from%20imbalanced%20dataset%2C%20decreasing%0Aperformance%20of%20CAC%20segmentation%20and%20scoring.%20In%20this%20study%2C%20we%20extend%20this%0Aapproach%20by%20incorporating%20the%20self-supervised%20learning%20%28SSL%29%20technique%20of%20DINO%0A%28self-distillation%20with%20no%20labels%29%20to%20eliminate%20limitations%20of%20scarce%20annotated%0Adata%20in%20CT%20scans.%20The%20DINO%20model%27s%20ability%20to%20train%20without%20requiring%20CAC%20area%0Aannotations%20enhances%20its%20robustness%20in%20generating%20distinct%20features.%20The%20DINO%0Amodel%20is%20trained%20on%20to%20focus%20specifically%20on%20calcified%20areas%20by%20using%20labels%2C%0Aaiming%20to%20generate%20features%20that%20effectively%20capture%20and%20highlight%20key%0Acharacteristics.%20The%20label-guided%20DINO%20%28DINO-LG%29%20enhances%20classification%20by%0Adistinguishing%20CT%20slices%20that%20contain%20calcification%20from%20those%20that%20do%20not%2C%0Aperforming%2057%25%20better%20than%20the%20standard%20DINO%20model%20in%20this%20task.%20CAC%20scoring%0Aand%20segmentation%20tasks%20are%20performed%20by%20a%20basic%20U-NET%20architecture%2C%20fed%0Aspecifically%20with%20CT%20slices%20containing%20calcified%20areas%20as%20identified%20by%20the%0ADINO-LG%20model.%20This%20targeted%20identification%20performed%20by%20DINO-LG%20model%20improves%0ACAC%20segmentation%20performance%20by%20approximately%2010%25%20and%20significant%20increase%20in%0ACAC%20scoring%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINO-LG%253A%2520A%2520Task-Specific%2520DINO%2520Model%2520for%2520Coronary%2520Calcium%2520Scoring%26entry.906535625%3DMahmut%2520S.%2520Gokmen%2520and%2520Cody%2520Bumgardner%2520and%2520Caner%2520Ozcan%26entry.1292438233%3D%2520%2520Coronary%2520artery%2520disease%2520%2528CAD%2529%252C%2520one%2520of%2520the%2520most%2520common%2520cause%2520of%2520mortality%2520in%250Athe%2520world.%2520Coronary%2520artery%2520calcium%2520%2528CAC%2529%2520scoring%2520using%2520computed%2520tomography%2520%2528CT%2529%250Ais%2520key%2520for%2520risk%2520assessment%2520to%2520prevent%2520coronary%2520disease.%2520Previous%2520studies%2520on%250Arisk%2520assessment%2520and%2520calcification%2520detection%2520in%2520CT%2520scans%2520primarily%2520use%250Aapproaches%2520based%2520on%2520UNET%2520architecture%252C%2520frequently%2520implemented%2520on%2520pre-built%250Amodels.%2520However%252C%2520these%2520models%2520are%2520limited%2520by%2520the%2520availability%2520of%2520annotated%2520CT%250Ascans%2520containing%2520CAC%2520and%2520suffering%2520from%2520imbalanced%2520dataset%252C%2520decreasing%250Aperformance%2520of%2520CAC%2520segmentation%2520and%2520scoring.%2520In%2520this%2520study%252C%2520we%2520extend%2520this%250Aapproach%2520by%2520incorporating%2520the%2520self-supervised%2520learning%2520%2528SSL%2529%2520technique%2520of%2520DINO%250A%2528self-distillation%2520with%2520no%2520labels%2529%2520to%2520eliminate%2520limitations%2520of%2520scarce%2520annotated%250Adata%2520in%2520CT%2520scans.%2520The%2520DINO%2520model%2527s%2520ability%2520to%2520train%2520without%2520requiring%2520CAC%2520area%250Aannotations%2520enhances%2520its%2520robustness%2520in%2520generating%2520distinct%2520features.%2520The%2520DINO%250Amodel%2520is%2520trained%2520on%2520to%2520focus%2520specifically%2520on%2520calcified%2520areas%2520by%2520using%2520labels%252C%250Aaiming%2520to%2520generate%2520features%2520that%2520effectively%2520capture%2520and%2520highlight%2520key%250Acharacteristics.%2520The%2520label-guided%2520DINO%2520%2528DINO-LG%2529%2520enhances%2520classification%2520by%250Adistinguishing%2520CT%2520slices%2520that%2520contain%2520calcification%2520from%2520those%2520that%2520do%2520not%252C%250Aperforming%252057%2525%2520better%2520than%2520the%2520standard%2520DINO%2520model%2520in%2520this%2520task.%2520CAC%2520scoring%250Aand%2520segmentation%2520tasks%2520are%2520performed%2520by%2520a%2520basic%2520U-NET%2520architecture%252C%2520fed%250Aspecifically%2520with%2520CT%2520slices%2520containing%2520calcified%2520areas%2520as%2520identified%2520by%2520the%250ADINO-LG%2520model.%2520This%2520targeted%2520identification%2520performed%2520by%2520DINO-LG%2520model%2520improves%250ACAC%2520segmentation%2520performance%2520by%2520approximately%252010%2525%2520and%2520significant%2520increase%2520in%250ACAC%2520scoring%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINO-LG%3A%20A%20Task-Specific%20DINO%20Model%20for%20Coronary%20Calcium%20Scoring&entry.906535625=Mahmut%20S.%20Gokmen%20and%20Cody%20Bumgardner%20and%20Caner%20Ozcan&entry.1292438233=%20%20Coronary%20artery%20disease%20%28CAD%29%2C%20one%20of%20the%20most%20common%20cause%20of%20mortality%20in%0Athe%20world.%20Coronary%20artery%20calcium%20%28CAC%29%20scoring%20using%20computed%20tomography%20%28CT%29%0Ais%20key%20for%20risk%20assessment%20to%20prevent%20coronary%20disease.%20Previous%20studies%20on%0Arisk%20assessment%20and%20calcification%20detection%20in%20CT%20scans%20primarily%20use%0Aapproaches%20based%20on%20UNET%20architecture%2C%20frequently%20implemented%20on%20pre-built%0Amodels.%20However%2C%20these%20models%20are%20limited%20by%20the%20availability%20of%20annotated%20CT%0Ascans%20containing%20CAC%20and%20suffering%20from%20imbalanced%20dataset%2C%20decreasing%0Aperformance%20of%20CAC%20segmentation%20and%20scoring.%20In%20this%20study%2C%20we%20extend%20this%0Aapproach%20by%20incorporating%20the%20self-supervised%20learning%20%28SSL%29%20technique%20of%20DINO%0A%28self-distillation%20with%20no%20labels%29%20to%20eliminate%20limitations%20of%20scarce%20annotated%0Adata%20in%20CT%20scans.%20The%20DINO%20model%27s%20ability%20to%20train%20without%20requiring%20CAC%20area%0Aannotations%20enhances%20its%20robustness%20in%20generating%20distinct%20features.%20The%20DINO%0Amodel%20is%20trained%20on%20to%20focus%20specifically%20on%20calcified%20areas%20by%20using%20labels%2C%0Aaiming%20to%20generate%20features%20that%20effectively%20capture%20and%20highlight%20key%0Acharacteristics.%20The%20label-guided%20DINO%20%28DINO-LG%29%20enhances%20classification%20by%0Adistinguishing%20CT%20slices%20that%20contain%20calcification%20from%20those%20that%20do%20not%2C%0Aperforming%2057%25%20better%20than%20the%20standard%20DINO%20model%20in%20this%20task.%20CAC%20scoring%0Aand%20segmentation%20tasks%20are%20performed%20by%20a%20basic%20U-NET%20architecture%2C%20fed%0Aspecifically%20with%20CT%20slices%20containing%20calcified%20areas%20as%20identified%20by%20the%0ADINO-LG%20model.%20This%20targeted%20identification%20performed%20by%20DINO-LG%20model%20improves%0ACAC%20segmentation%20performance%20by%20approximately%2010%25%20and%20significant%20increase%20in%0ACAC%20scoring%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07976v1&entry.124074799=Read"},
{"title": "Bootstrapping Reinforcement Learning with Imitation for Vision-Based\n  Agile Flight", "author": "Jiaxu Xing and Angel Romero and Leonard Bauersfeld and Davide Scaramuzza", "abstract": "  Learning visuomotor policies for agile quadrotor flight presents significant\ndifficulties, primarily from inefficient policy exploration caused by\nhigh-dimensional visual inputs and the need for precise and low-latency\ncontrol. To address these challenges, we propose a novel approach that combines\nthe performance of Reinforcement Learning (RL) and the sample efficiency of\nImitation Learning (IL) in the task of vision-based autonomous drone racing.\nWhile RL provides a framework for learning high-performance controllers through\ntrial and error, it faces challenges with sample efficiency and computational\ndemands due to the high dimensionality of visual inputs. Conversely, IL\nefficiently learns from visual expert demonstrations, but it remains limited by\nthe expert's performance and state distribution. To overcome these limitations,\nour policy learning framework integrates the strengths of both approaches. Our\nframework contains three phases: training a teacher policy using RL with\nprivileged state information, distilling it into a student policy via IL, and\nadaptive fine-tuning via RL. Testing in both simulated and real-world scenarios\nshows our approach can not only learn in scenarios where RL from scratch fails\nbut also outperforms existing IL methods in both robustness and performance,\nsuccessfully navigating a quadrotor through a race course using only visual\ninformation. Videos of the experiments are available at\nhttps://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.\n", "link": "http://arxiv.org/abs/2403.12203v3", "date": "2024-11-12", "relevancy": 2.0804, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5146}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight&body=Title%3A%20Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight%0AAuthor%3A%20Jiaxu%20Xing%20and%20Angel%20Romero%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Learning%20visuomotor%20policies%20for%20agile%20quadrotor%20flight%20presents%20significant%0Adifficulties%2C%20primarily%20from%20inefficient%20policy%20exploration%20caused%20by%0Ahigh-dimensional%20visual%20inputs%20and%20the%20need%20for%20precise%20and%20low-latency%0Acontrol.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Athe%20performance%20of%20Reinforcement%20Learning%20%28RL%29%20and%20the%20sample%20efficiency%20of%0AImitation%20Learning%20%28IL%29%20in%20the%20task%20of%20vision-based%20autonomous%20drone%20racing.%0AWhile%20RL%20provides%20a%20framework%20for%20learning%20high-performance%20controllers%20through%0Atrial%20and%20error%2C%20it%20faces%20challenges%20with%20sample%20efficiency%20and%20computational%0Ademands%20due%20to%20the%20high%20dimensionality%20of%20visual%20inputs.%20Conversely%2C%20IL%0Aefficiently%20learns%20from%20visual%20expert%20demonstrations%2C%20but%20it%20remains%20limited%20by%0Athe%20expert%27s%20performance%20and%20state%20distribution.%20To%20overcome%20these%20limitations%2C%0Aour%20policy%20learning%20framework%20integrates%20the%20strengths%20of%20both%20approaches.%20Our%0Aframework%20contains%20three%20phases%3A%20training%20a%20teacher%20policy%20using%20RL%20with%0Aprivileged%20state%20information%2C%20distilling%20it%20into%20a%20student%20policy%20via%20IL%2C%20and%0Aadaptive%20fine-tuning%20via%20RL.%20Testing%20in%20both%20simulated%20and%20real-world%20scenarios%0Ashows%20our%20approach%20can%20not%20only%20learn%20in%20scenarios%20where%20RL%20from%20scratch%20fails%0Abut%20also%20outperforms%20existing%20IL%20methods%20in%20both%20robustness%20and%20performance%2C%0Asuccessfully%20navigating%20a%20quadrotor%20through%20a%20race%20course%20using%20only%20visual%0Ainformation.%20Videos%20of%20the%20experiments%20are%20available%20at%0Ahttps%3A//rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12203v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Reinforcement%2520Learning%2520with%2520Imitation%2520for%2520Vision-Based%250A%2520%2520Agile%2520Flight%26entry.906535625%3DJiaxu%2520Xing%2520and%2520Angel%2520Romero%2520and%2520Leonard%2520Bauersfeld%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Learning%2520visuomotor%2520policies%2520for%2520agile%2520quadrotor%2520flight%2520presents%2520significant%250Adifficulties%252C%2520primarily%2520from%2520inefficient%2520policy%2520exploration%2520caused%2520by%250Ahigh-dimensional%2520visual%2520inputs%2520and%2520the%2520need%2520for%2520precise%2520and%2520low-latency%250Acontrol.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520combines%250Athe%2520performance%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%2520and%2520the%2520sample%2520efficiency%2520of%250AImitation%2520Learning%2520%2528IL%2529%2520in%2520the%2520task%2520of%2520vision-based%2520autonomous%2520drone%2520racing.%250AWhile%2520RL%2520provides%2520a%2520framework%2520for%2520learning%2520high-performance%2520controllers%2520through%250Atrial%2520and%2520error%252C%2520it%2520faces%2520challenges%2520with%2520sample%2520efficiency%2520and%2520computational%250Ademands%2520due%2520to%2520the%2520high%2520dimensionality%2520of%2520visual%2520inputs.%2520Conversely%252C%2520IL%250Aefficiently%2520learns%2520from%2520visual%2520expert%2520demonstrations%252C%2520but%2520it%2520remains%2520limited%2520by%250Athe%2520expert%2527s%2520performance%2520and%2520state%2520distribution.%2520To%2520overcome%2520these%2520limitations%252C%250Aour%2520policy%2520learning%2520framework%2520integrates%2520the%2520strengths%2520of%2520both%2520approaches.%2520Our%250Aframework%2520contains%2520three%2520phases%253A%2520training%2520a%2520teacher%2520policy%2520using%2520RL%2520with%250Aprivileged%2520state%2520information%252C%2520distilling%2520it%2520into%2520a%2520student%2520policy%2520via%2520IL%252C%2520and%250Aadaptive%2520fine-tuning%2520via%2520RL.%2520Testing%2520in%2520both%2520simulated%2520and%2520real-world%2520scenarios%250Ashows%2520our%2520approach%2520can%2520not%2520only%2520learn%2520in%2520scenarios%2520where%2520RL%2520from%2520scratch%2520fails%250Abut%2520also%2520outperforms%2520existing%2520IL%2520methods%2520in%2520both%2520robustness%2520and%2520performance%252C%250Asuccessfully%2520navigating%2520a%2520quadrotor%2520through%2520a%2520race%2520course%2520using%2520only%2520visual%250Ainformation.%2520Videos%2520of%2520the%2520experiments%2520are%2520available%2520at%250Ahttps%253A//rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12203v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Reinforcement%20Learning%20with%20Imitation%20for%20Vision-Based%0A%20%20Agile%20Flight&entry.906535625=Jiaxu%20Xing%20and%20Angel%20Romero%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Learning%20visuomotor%20policies%20for%20agile%20quadrotor%20flight%20presents%20significant%0Adifficulties%2C%20primarily%20from%20inefficient%20policy%20exploration%20caused%20by%0Ahigh-dimensional%20visual%20inputs%20and%20the%20need%20for%20precise%20and%20low-latency%0Acontrol.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%20combines%0Athe%20performance%20of%20Reinforcement%20Learning%20%28RL%29%20and%20the%20sample%20efficiency%20of%0AImitation%20Learning%20%28IL%29%20in%20the%20task%20of%20vision-based%20autonomous%20drone%20racing.%0AWhile%20RL%20provides%20a%20framework%20for%20learning%20high-performance%20controllers%20through%0Atrial%20and%20error%2C%20it%20faces%20challenges%20with%20sample%20efficiency%20and%20computational%0Ademands%20due%20to%20the%20high%20dimensionality%20of%20visual%20inputs.%20Conversely%2C%20IL%0Aefficiently%20learns%20from%20visual%20expert%20demonstrations%2C%20but%20it%20remains%20limited%20by%0Athe%20expert%27s%20performance%20and%20state%20distribution.%20To%20overcome%20these%20limitations%2C%0Aour%20policy%20learning%20framework%20integrates%20the%20strengths%20of%20both%20approaches.%20Our%0Aframework%20contains%20three%20phases%3A%20training%20a%20teacher%20policy%20using%20RL%20with%0Aprivileged%20state%20information%2C%20distilling%20it%20into%20a%20student%20policy%20via%20IL%2C%20and%0Aadaptive%20fine-tuning%20via%20RL.%20Testing%20in%20both%20simulated%20and%20real-world%20scenarios%0Ashows%20our%20approach%20can%20not%20only%20learn%20in%20scenarios%20where%20RL%20from%20scratch%20fails%0Abut%20also%20outperforms%20existing%20IL%20methods%20in%20both%20robustness%20and%20performance%2C%0Asuccessfully%20navigating%20a%20quadrotor%20through%20a%20race%20course%20using%20only%20visual%0Ainformation.%20Videos%20of%20the%20experiments%20are%20available%20at%0Ahttps%3A//rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12203v3&entry.124074799=Read"},
{"title": "FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for\n  Scalable Training", "author": "Philip Zmushko and Aleksandr Beznosikov and Martin Tak\u00e1\u010d and Samuel Horv\u00e1th", "abstract": "  With the increase in the number of parameters in large language models, the\nprocess of pre-training and fine-tuning increasingly demands larger volumes of\nGPU memory. A significant portion of this memory is typically consumed by the\noptimizer state. To overcome this challenge, recent approaches such as low-rank\nadaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao\net al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been\nproposed. However, in all these algorithms, the $\\textit{effective rank of the\nweight updates remains low-rank}$, which can lead to a substantial loss of\ninformation from the gradient. This loss can be critically important,\nespecially during the pre-training stage. In this paper, we introduce\n$\\texttt{FRUGAL}$ ($\\textbf{F}$ull-$\\textbf{R}$ank $\\textbf{U}$pdates with\n$\\textbf{G}$r$\\textbf{A}$dient sp$\\textbf{L}$itting), a new memory-efficient\noptimization framework. $\\texttt{FRUGAL}$ leverages gradient splitting to\nperform low-dimensional updates using advanced algorithms (such as Adam), while\nupdates along the remaining directions are executed via state-free methods like\nSGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with\nvarious low-rank update selection techniques, including GaLore and BAdam. We\nprovide theoretical convergence guarantees for our framework when using SGDM\nfor low-dimensional updates and SGD for state-free updates. Additionally, our\nmethod consistently outperforms concurrent approaches across various fixed\nmemory budgets, achieving state-of-the-art results in pre-training and\nfine-tuning tasks while balancing memory efficiency and performance metrics.\n", "link": "http://arxiv.org/abs/2411.07837v1", "date": "2024-11-12", "relevancy": 2.0691, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5197}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training&body=Title%3A%20FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training%0AAuthor%3A%20Philip%20Zmushko%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Samuel%20Horv%C3%A1th%0AAbstract%3A%20%20%20With%20the%20increase%20in%20the%20number%20of%20parameters%20in%20large%20language%20models%2C%20the%0Aprocess%20of%20pre-training%20and%20fine-tuning%20increasingly%20demands%20larger%20volumes%20of%0AGPU%20memory.%20A%20significant%20portion%20of%20this%20memory%20is%20typically%20consumed%20by%20the%0Aoptimizer%20state.%20To%20overcome%20this%20challenge%2C%20recent%20approaches%20such%20as%20low-rank%0Aadaptation%20%28LoRA%20%28Hu%20et%20al.%2C%202021%29%29%2C%20low-rank%20gradient%20projection%20%28GaLore%20%28Zhao%0Aet%20al.%2C%202024%29%29%2C%20and%20blockwise%20optimization%20%28BAdam%20%28Luo%20et%20al.%2C%202024%29%29%20have%20been%0Aproposed.%20However%2C%20in%20all%20these%20algorithms%2C%20the%20%24%5Ctextit%7Beffective%20rank%20of%20the%0Aweight%20updates%20remains%20low-rank%7D%24%2C%20which%20can%20lead%20to%20a%20substantial%20loss%20of%0Ainformation%20from%20the%20gradient.%20This%20loss%20can%20be%20critically%20important%2C%0Aespecially%20during%20the%20pre-training%20stage.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctexttt%7BFRUGAL%7D%24%20%28%24%5Ctextbf%7BF%7D%24ull-%24%5Ctextbf%7BR%7D%24ank%20%24%5Ctextbf%7BU%7D%24pdates%20with%0A%24%5Ctextbf%7BG%7D%24r%24%5Ctextbf%7BA%7D%24dient%20sp%24%5Ctextbf%7BL%7D%24itting%29%2C%20a%20new%20memory-efficient%0Aoptimization%20framework.%20%24%5Ctexttt%7BFRUGAL%7D%24%20leverages%20gradient%20splitting%20to%0Aperform%20low-dimensional%20updates%20using%20advanced%20algorithms%20%28such%20as%20Adam%29%2C%20while%0Aupdates%20along%20the%20remaining%20directions%20are%20executed%20via%20state-free%20methods%20like%0ASGD%20or%20signSGD%20%28Bernstein%20et%20al.%2C%202018%29.%20Our%20framework%20can%20be%20integrated%20with%0Avarious%20low-rank%20update%20selection%20techniques%2C%20including%20GaLore%20and%20BAdam.%20We%0Aprovide%20theoretical%20convergence%20guarantees%20for%20our%20framework%20when%20using%20SGDM%0Afor%20low-dimensional%20updates%20and%20SGD%20for%20state-free%20updates.%20Additionally%2C%20our%0Amethod%20consistently%20outperforms%20concurrent%20approaches%20across%20various%20fixed%0Amemory%20budgets%2C%20achieving%20state-of-the-art%20results%20in%20pre-training%20and%0Afine-tuning%20tasks%20while%20balancing%20memory%20efficiency%20and%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRUGAL%253A%2520Memory-Efficient%2520Optimization%2520by%2520Reducing%2520State%2520Overhead%2520for%250A%2520%2520Scalable%2520Training%26entry.906535625%3DPhilip%2520Zmushko%2520and%2520Aleksandr%2520Beznosikov%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%2520and%2520Samuel%2520Horv%25C3%25A1th%26entry.1292438233%3D%2520%2520With%2520the%2520increase%2520in%2520the%2520number%2520of%2520parameters%2520in%2520large%2520language%2520models%252C%2520the%250Aprocess%2520of%2520pre-training%2520and%2520fine-tuning%2520increasingly%2520demands%2520larger%2520volumes%2520of%250AGPU%2520memory.%2520A%2520significant%2520portion%2520of%2520this%2520memory%2520is%2520typically%2520consumed%2520by%2520the%250Aoptimizer%2520state.%2520To%2520overcome%2520this%2520challenge%252C%2520recent%2520approaches%2520such%2520as%2520low-rank%250Aadaptation%2520%2528LoRA%2520%2528Hu%2520et%2520al.%252C%25202021%2529%2529%252C%2520low-rank%2520gradient%2520projection%2520%2528GaLore%2520%2528Zhao%250Aet%2520al.%252C%25202024%2529%2529%252C%2520and%2520blockwise%2520optimization%2520%2528BAdam%2520%2528Luo%2520et%2520al.%252C%25202024%2529%2529%2520have%2520been%250Aproposed.%2520However%252C%2520in%2520all%2520these%2520algorithms%252C%2520the%2520%2524%255Ctextit%257Beffective%2520rank%2520of%2520the%250Aweight%2520updates%2520remains%2520low-rank%257D%2524%252C%2520which%2520can%2520lead%2520to%2520a%2520substantial%2520loss%2520of%250Ainformation%2520from%2520the%2520gradient.%2520This%2520loss%2520can%2520be%2520critically%2520important%252C%250Aespecially%2520during%2520the%2520pre-training%2520stage.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A%2524%255Ctexttt%257BFRUGAL%257D%2524%2520%2528%2524%255Ctextbf%257BF%257D%2524ull-%2524%255Ctextbf%257BR%257D%2524ank%2520%2524%255Ctextbf%257BU%257D%2524pdates%2520with%250A%2524%255Ctextbf%257BG%257D%2524r%2524%255Ctextbf%257BA%257D%2524dient%2520sp%2524%255Ctextbf%257BL%257D%2524itting%2529%252C%2520a%2520new%2520memory-efficient%250Aoptimization%2520framework.%2520%2524%255Ctexttt%257BFRUGAL%257D%2524%2520leverages%2520gradient%2520splitting%2520to%250Aperform%2520low-dimensional%2520updates%2520using%2520advanced%2520algorithms%2520%2528such%2520as%2520Adam%2529%252C%2520while%250Aupdates%2520along%2520the%2520remaining%2520directions%2520are%2520executed%2520via%2520state-free%2520methods%2520like%250ASGD%2520or%2520signSGD%2520%2528Bernstein%2520et%2520al.%252C%25202018%2529.%2520Our%2520framework%2520can%2520be%2520integrated%2520with%250Avarious%2520low-rank%2520update%2520selection%2520techniques%252C%2520including%2520GaLore%2520and%2520BAdam.%2520We%250Aprovide%2520theoretical%2520convergence%2520guarantees%2520for%2520our%2520framework%2520when%2520using%2520SGDM%250Afor%2520low-dimensional%2520updates%2520and%2520SGD%2520for%2520state-free%2520updates.%2520Additionally%252C%2520our%250Amethod%2520consistently%2520outperforms%2520concurrent%2520approaches%2520across%2520various%2520fixed%250Amemory%2520budgets%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520pre-training%2520and%250Afine-tuning%2520tasks%2520while%2520balancing%2520memory%2520efficiency%2520and%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRUGAL%3A%20Memory-Efficient%20Optimization%20by%20Reducing%20State%20Overhead%20for%0A%20%20Scalable%20Training&entry.906535625=Philip%20Zmushko%20and%20Aleksandr%20Beznosikov%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Samuel%20Horv%C3%A1th&entry.1292438233=%20%20With%20the%20increase%20in%20the%20number%20of%20parameters%20in%20large%20language%20models%2C%20the%0Aprocess%20of%20pre-training%20and%20fine-tuning%20increasingly%20demands%20larger%20volumes%20of%0AGPU%20memory.%20A%20significant%20portion%20of%20this%20memory%20is%20typically%20consumed%20by%20the%0Aoptimizer%20state.%20To%20overcome%20this%20challenge%2C%20recent%20approaches%20such%20as%20low-rank%0Aadaptation%20%28LoRA%20%28Hu%20et%20al.%2C%202021%29%29%2C%20low-rank%20gradient%20projection%20%28GaLore%20%28Zhao%0Aet%20al.%2C%202024%29%29%2C%20and%20blockwise%20optimization%20%28BAdam%20%28Luo%20et%20al.%2C%202024%29%29%20have%20been%0Aproposed.%20However%2C%20in%20all%20these%20algorithms%2C%20the%20%24%5Ctextit%7Beffective%20rank%20of%20the%0Aweight%20updates%20remains%20low-rank%7D%24%2C%20which%20can%20lead%20to%20a%20substantial%20loss%20of%0Ainformation%20from%20the%20gradient.%20This%20loss%20can%20be%20critically%20important%2C%0Aespecially%20during%20the%20pre-training%20stage.%20In%20this%20paper%2C%20we%20introduce%0A%24%5Ctexttt%7BFRUGAL%7D%24%20%28%24%5Ctextbf%7BF%7D%24ull-%24%5Ctextbf%7BR%7D%24ank%20%24%5Ctextbf%7BU%7D%24pdates%20with%0A%24%5Ctextbf%7BG%7D%24r%24%5Ctextbf%7BA%7D%24dient%20sp%24%5Ctextbf%7BL%7D%24itting%29%2C%20a%20new%20memory-efficient%0Aoptimization%20framework.%20%24%5Ctexttt%7BFRUGAL%7D%24%20leverages%20gradient%20splitting%20to%0Aperform%20low-dimensional%20updates%20using%20advanced%20algorithms%20%28such%20as%20Adam%29%2C%20while%0Aupdates%20along%20the%20remaining%20directions%20are%20executed%20via%20state-free%20methods%20like%0ASGD%20or%20signSGD%20%28Bernstein%20et%20al.%2C%202018%29.%20Our%20framework%20can%20be%20integrated%20with%0Avarious%20low-rank%20update%20selection%20techniques%2C%20including%20GaLore%20and%20BAdam.%20We%0Aprovide%20theoretical%20convergence%20guarantees%20for%20our%20framework%20when%20using%20SGDM%0Afor%20low-dimensional%20updates%20and%20SGD%20for%20state-free%20updates.%20Additionally%2C%20our%0Amethod%20consistently%20outperforms%20concurrent%20approaches%20across%20various%20fixed%0Amemory%20budgets%2C%20achieving%20state-of-the-art%20results%20in%20pre-training%20and%0Afine-tuning%20tasks%20while%20balancing%20memory%20efficiency%20and%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07837v1&entry.124074799=Read"},
{"title": "AdaSemiCD: An Adaptive Semi-Supervised Change Detection Method Based on\n  Pseudo-Label Evaluation", "author": "Ran Lingyan and Wen Dongcheng and Zhuo Tao and Zhang Shizhou and Zhang Xiuwei and Zhang Yanning", "abstract": "  Change Detection (CD) is an essential field in remote sensing, with a primary\nfocus on identifying areas of change in bi-temporal image pairs captured at\nvarying intervals of the same region by a satellite. The data annotation\nprocess for the CD task is both time-consuming and labor-intensive. To make\nbetter use of the scarce labeled data and abundant unlabeled data, we present\nan adaptive dynamic semi-supervised learning method, AdaSemiCD, to improve the\nuse of pseudo-labels and optimize the training process. Initially, due to the\nextreme class imbalance inherent in CD, the model is more inclined to focus on\nthe background class, and it is easy to confuse the boundary of the target\nobject. Considering these two points, we develop a measurable evaluation metric\nfor pseudo-labels that enhances the representation of information entropy by\nclass rebalancing and amplification of confusing areas to give a larger weight\nto prospects change objects. Subsequently, to enhance the reliability of\nsample-wise pseudo-labels, we introduce the AdaFusion module, which is capable\nof dynamically identifying the most uncertain region and substituting it with\nmore trustworthy content. Lastly, to ensure better training stability, we\nintroduce the AdaEMA module, which updates the teacher model using only batches\nof trusted samples. Experimental results from LEVIR-CD, WHU-CD, and CDD\ndatasets validate the efficacy and universality of our proposed adaptive\ntraining framework.\n", "link": "http://arxiv.org/abs/2411.07758v1", "date": "2024-11-12", "relevancy": 2.0478, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5395}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaSemiCD%3A%20An%20Adaptive%20Semi-Supervised%20Change%20Detection%20Method%20Based%20on%0A%20%20Pseudo-Label%20Evaluation&body=Title%3A%20AdaSemiCD%3A%20An%20Adaptive%20Semi-Supervised%20Change%20Detection%20Method%20Based%20on%0A%20%20Pseudo-Label%20Evaluation%0AAuthor%3A%20Ran%20Lingyan%20and%20Wen%20Dongcheng%20and%20Zhuo%20Tao%20and%20Zhang%20Shizhou%20and%20Zhang%20Xiuwei%20and%20Zhang%20Yanning%0AAbstract%3A%20%20%20Change%20Detection%20%28CD%29%20is%20an%20essential%20field%20in%20remote%20sensing%2C%20with%20a%20primary%0Afocus%20on%20identifying%20areas%20of%20change%20in%20bi-temporal%20image%20pairs%20captured%20at%0Avarying%20intervals%20of%20the%20same%20region%20by%20a%20satellite.%20The%20data%20annotation%0Aprocess%20for%20the%20CD%20task%20is%20both%20time-consuming%20and%20labor-intensive.%20To%20make%0Abetter%20use%20of%20the%20scarce%20labeled%20data%20and%20abundant%20unlabeled%20data%2C%20we%20present%0Aan%20adaptive%20dynamic%20semi-supervised%20learning%20method%2C%20AdaSemiCD%2C%20to%20improve%20the%0Ause%20of%20pseudo-labels%20and%20optimize%20the%20training%20process.%20Initially%2C%20due%20to%20the%0Aextreme%20class%20imbalance%20inherent%20in%20CD%2C%20the%20model%20is%20more%20inclined%20to%20focus%20on%0Athe%20background%20class%2C%20and%20it%20is%20easy%20to%20confuse%20the%20boundary%20of%20the%20target%0Aobject.%20Considering%20these%20two%20points%2C%20we%20develop%20a%20measurable%20evaluation%20metric%0Afor%20pseudo-labels%20that%20enhances%20the%20representation%20of%20information%20entropy%20by%0Aclass%20rebalancing%20and%20amplification%20of%20confusing%20areas%20to%20give%20a%20larger%20weight%0Ato%20prospects%20change%20objects.%20Subsequently%2C%20to%20enhance%20the%20reliability%20of%0Asample-wise%20pseudo-labels%2C%20we%20introduce%20the%20AdaFusion%20module%2C%20which%20is%20capable%0Aof%20dynamically%20identifying%20the%20most%20uncertain%20region%20and%20substituting%20it%20with%0Amore%20trustworthy%20content.%20Lastly%2C%20to%20ensure%20better%20training%20stability%2C%20we%0Aintroduce%20the%20AdaEMA%20module%2C%20which%20updates%20the%20teacher%20model%20using%20only%20batches%0Aof%20trusted%20samples.%20Experimental%20results%20from%20LEVIR-CD%2C%20WHU-CD%2C%20and%20CDD%0Adatasets%20validate%20the%20efficacy%20and%20universality%20of%20our%20proposed%20adaptive%0Atraining%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaSemiCD%253A%2520An%2520Adaptive%2520Semi-Supervised%2520Change%2520Detection%2520Method%2520Based%2520on%250A%2520%2520Pseudo-Label%2520Evaluation%26entry.906535625%3DRan%2520Lingyan%2520and%2520Wen%2520Dongcheng%2520and%2520Zhuo%2520Tao%2520and%2520Zhang%2520Shizhou%2520and%2520Zhang%2520Xiuwei%2520and%2520Zhang%2520Yanning%26entry.1292438233%3D%2520%2520Change%2520Detection%2520%2528CD%2529%2520is%2520an%2520essential%2520field%2520in%2520remote%2520sensing%252C%2520with%2520a%2520primary%250Afocus%2520on%2520identifying%2520areas%2520of%2520change%2520in%2520bi-temporal%2520image%2520pairs%2520captured%2520at%250Avarying%2520intervals%2520of%2520the%2520same%2520region%2520by%2520a%2520satellite.%2520The%2520data%2520annotation%250Aprocess%2520for%2520the%2520CD%2520task%2520is%2520both%2520time-consuming%2520and%2520labor-intensive.%2520To%2520make%250Abetter%2520use%2520of%2520the%2520scarce%2520labeled%2520data%2520and%2520abundant%2520unlabeled%2520data%252C%2520we%2520present%250Aan%2520adaptive%2520dynamic%2520semi-supervised%2520learning%2520method%252C%2520AdaSemiCD%252C%2520to%2520improve%2520the%250Ause%2520of%2520pseudo-labels%2520and%2520optimize%2520the%2520training%2520process.%2520Initially%252C%2520due%2520to%2520the%250Aextreme%2520class%2520imbalance%2520inherent%2520in%2520CD%252C%2520the%2520model%2520is%2520more%2520inclined%2520to%2520focus%2520on%250Athe%2520background%2520class%252C%2520and%2520it%2520is%2520easy%2520to%2520confuse%2520the%2520boundary%2520of%2520the%2520target%250Aobject.%2520Considering%2520these%2520two%2520points%252C%2520we%2520develop%2520a%2520measurable%2520evaluation%2520metric%250Afor%2520pseudo-labels%2520that%2520enhances%2520the%2520representation%2520of%2520information%2520entropy%2520by%250Aclass%2520rebalancing%2520and%2520amplification%2520of%2520confusing%2520areas%2520to%2520give%2520a%2520larger%2520weight%250Ato%2520prospects%2520change%2520objects.%2520Subsequently%252C%2520to%2520enhance%2520the%2520reliability%2520of%250Asample-wise%2520pseudo-labels%252C%2520we%2520introduce%2520the%2520AdaFusion%2520module%252C%2520which%2520is%2520capable%250Aof%2520dynamically%2520identifying%2520the%2520most%2520uncertain%2520region%2520and%2520substituting%2520it%2520with%250Amore%2520trustworthy%2520content.%2520Lastly%252C%2520to%2520ensure%2520better%2520training%2520stability%252C%2520we%250Aintroduce%2520the%2520AdaEMA%2520module%252C%2520which%2520updates%2520the%2520teacher%2520model%2520using%2520only%2520batches%250Aof%2520trusted%2520samples.%2520Experimental%2520results%2520from%2520LEVIR-CD%252C%2520WHU-CD%252C%2520and%2520CDD%250Adatasets%2520validate%2520the%2520efficacy%2520and%2520universality%2520of%2520our%2520proposed%2520adaptive%250Atraining%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaSemiCD%3A%20An%20Adaptive%20Semi-Supervised%20Change%20Detection%20Method%20Based%20on%0A%20%20Pseudo-Label%20Evaluation&entry.906535625=Ran%20Lingyan%20and%20Wen%20Dongcheng%20and%20Zhuo%20Tao%20and%20Zhang%20Shizhou%20and%20Zhang%20Xiuwei%20and%20Zhang%20Yanning&entry.1292438233=%20%20Change%20Detection%20%28CD%29%20is%20an%20essential%20field%20in%20remote%20sensing%2C%20with%20a%20primary%0Afocus%20on%20identifying%20areas%20of%20change%20in%20bi-temporal%20image%20pairs%20captured%20at%0Avarying%20intervals%20of%20the%20same%20region%20by%20a%20satellite.%20The%20data%20annotation%0Aprocess%20for%20the%20CD%20task%20is%20both%20time-consuming%20and%20labor-intensive.%20To%20make%0Abetter%20use%20of%20the%20scarce%20labeled%20data%20and%20abundant%20unlabeled%20data%2C%20we%20present%0Aan%20adaptive%20dynamic%20semi-supervised%20learning%20method%2C%20AdaSemiCD%2C%20to%20improve%20the%0Ause%20of%20pseudo-labels%20and%20optimize%20the%20training%20process.%20Initially%2C%20due%20to%20the%0Aextreme%20class%20imbalance%20inherent%20in%20CD%2C%20the%20model%20is%20more%20inclined%20to%20focus%20on%0Athe%20background%20class%2C%20and%20it%20is%20easy%20to%20confuse%20the%20boundary%20of%20the%20target%0Aobject.%20Considering%20these%20two%20points%2C%20we%20develop%20a%20measurable%20evaluation%20metric%0Afor%20pseudo-labels%20that%20enhances%20the%20representation%20of%20information%20entropy%20by%0Aclass%20rebalancing%20and%20amplification%20of%20confusing%20areas%20to%20give%20a%20larger%20weight%0Ato%20prospects%20change%20objects.%20Subsequently%2C%20to%20enhance%20the%20reliability%20of%0Asample-wise%20pseudo-labels%2C%20we%20introduce%20the%20AdaFusion%20module%2C%20which%20is%20capable%0Aof%20dynamically%20identifying%20the%20most%20uncertain%20region%20and%20substituting%20it%20with%0Amore%20trustworthy%20content.%20Lastly%2C%20to%20ensure%20better%20training%20stability%2C%20we%0Aintroduce%20the%20AdaEMA%20module%2C%20which%20updates%20the%20teacher%20model%20using%20only%20batches%0Aof%20trusted%20samples.%20Experimental%20results%20from%20LEVIR-CD%2C%20WHU-CD%2C%20and%20CDD%0Adatasets%20validate%20the%20efficacy%20and%20universality%20of%20our%20proposed%20adaptive%0Atraining%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07758v1&entry.124074799=Read"},
{"title": "Robust Clustering on High-Dimensional Data with Stochastic Quantization", "author": "Anton Kozyriev and Vladimir Norkin", "abstract": "  This paper addresses the limitations of conventional vector quantization\nalgorithms, particularly K-Means and its variant K-Means++, and investigates\nthe Stochastic Quantization (SQ) algorithm as a scalable alternative for\nhigh-dimensional unsupervised and semi-supervised learning tasks. Traditional\nclustering algorithms often suffer from inefficient memory utilization during\ncomputation, necessitating the loading of all data samples into memory, which\nbecomes impractical for large-scale datasets. While variants such as Mini-Batch\nK-Means partially mitigate this issue by reducing memory usage, they lack\nrobust theoretical convergence guarantees due to the non-convex nature of\nclustering problems. In contrast, the Stochastic Quantization algorithm\nprovides strong theoretical convergence guarantees, making it a robust\nalternative for clustering tasks. We demonstrate the computational efficiency\nand rapid convergence of the algorithm on an image classification problem with\npartially labeled data, comparing model accuracy across various ratios of\nlabeled to unlabeled data. To address the challenge of high dimensionality, we\nemploy a Triplet Network to encode images into low-dimensional representations\nin a latent space, which serve as a basis for comparing the efficiency of both\nthe Stochastic Quantization algorithm and traditional quantization algorithms.\nFurthermore, we enhance the algorithm's convergence speed by introducing\nmodifications with an adaptive learning rate.\n", "link": "http://arxiv.org/abs/2409.02066v4", "date": "2024-11-12", "relevancy": 2.0199, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.516}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization&body=Title%3A%20Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization%0AAuthor%3A%20Anton%20Kozyriev%20and%20Vladimir%20Norkin%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20limitations%20of%20conventional%20vector%20quantization%0Aalgorithms%2C%20particularly%20K-Means%20and%20its%20variant%20K-Means%2B%2B%2C%20and%20investigates%0Athe%20Stochastic%20Quantization%20%28SQ%29%20algorithm%20as%20a%20scalable%20alternative%20for%0Ahigh-dimensional%20unsupervised%20and%20semi-supervised%20learning%20tasks.%20Traditional%0Aclustering%20algorithms%20often%20suffer%20from%20inefficient%20memory%20utilization%20during%0Acomputation%2C%20necessitating%20the%20loading%20of%20all%20data%20samples%20into%20memory%2C%20which%0Abecomes%20impractical%20for%20large-scale%20datasets.%20While%20variants%20such%20as%20Mini-Batch%0AK-Means%20partially%20mitigate%20this%20issue%20by%20reducing%20memory%20usage%2C%20they%20lack%0Arobust%20theoretical%20convergence%20guarantees%20due%20to%20the%20non-convex%20nature%20of%0Aclustering%20problems.%20In%20contrast%2C%20the%20Stochastic%20Quantization%20algorithm%0Aprovides%20strong%20theoretical%20convergence%20guarantees%2C%20making%20it%20a%20robust%0Aalternative%20for%20clustering%20tasks.%20We%20demonstrate%20the%20computational%20efficiency%0Aand%20rapid%20convergence%20of%20the%20algorithm%20on%20an%20image%20classification%20problem%20with%0Apartially%20labeled%20data%2C%20comparing%20model%20accuracy%20across%20various%20ratios%20of%0Alabeled%20to%20unlabeled%20data.%20To%20address%20the%20challenge%20of%20high%20dimensionality%2C%20we%0Aemploy%20a%20Triplet%20Network%20to%20encode%20images%20into%20low-dimensional%20representations%0Ain%20a%20latent%20space%2C%20which%20serve%20as%20a%20basis%20for%20comparing%20the%20efficiency%20of%20both%0Athe%20Stochastic%20Quantization%20algorithm%20and%20traditional%20quantization%20algorithms.%0AFurthermore%2C%20we%20enhance%20the%20algorithm%27s%20convergence%20speed%20by%20introducing%0Amodifications%20with%20an%20adaptive%20learning%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02066v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Clustering%2520on%2520High-Dimensional%2520Data%2520with%2520Stochastic%2520Quantization%26entry.906535625%3DAnton%2520Kozyriev%2520and%2520Vladimir%2520Norkin%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520limitations%2520of%2520conventional%2520vector%2520quantization%250Aalgorithms%252C%2520particularly%2520K-Means%2520and%2520its%2520variant%2520K-Means%252B%252B%252C%2520and%2520investigates%250Athe%2520Stochastic%2520Quantization%2520%2528SQ%2529%2520algorithm%2520as%2520a%2520scalable%2520alternative%2520for%250Ahigh-dimensional%2520unsupervised%2520and%2520semi-supervised%2520learning%2520tasks.%2520Traditional%250Aclustering%2520algorithms%2520often%2520suffer%2520from%2520inefficient%2520memory%2520utilization%2520during%250Acomputation%252C%2520necessitating%2520the%2520loading%2520of%2520all%2520data%2520samples%2520into%2520memory%252C%2520which%250Abecomes%2520impractical%2520for%2520large-scale%2520datasets.%2520While%2520variants%2520such%2520as%2520Mini-Batch%250AK-Means%2520partially%2520mitigate%2520this%2520issue%2520by%2520reducing%2520memory%2520usage%252C%2520they%2520lack%250Arobust%2520theoretical%2520convergence%2520guarantees%2520due%2520to%2520the%2520non-convex%2520nature%2520of%250Aclustering%2520problems.%2520In%2520contrast%252C%2520the%2520Stochastic%2520Quantization%2520algorithm%250Aprovides%2520strong%2520theoretical%2520convergence%2520guarantees%252C%2520making%2520it%2520a%2520robust%250Aalternative%2520for%2520clustering%2520tasks.%2520We%2520demonstrate%2520the%2520computational%2520efficiency%250Aand%2520rapid%2520convergence%2520of%2520the%2520algorithm%2520on%2520an%2520image%2520classification%2520problem%2520with%250Apartially%2520labeled%2520data%252C%2520comparing%2520model%2520accuracy%2520across%2520various%2520ratios%2520of%250Alabeled%2520to%2520unlabeled%2520data.%2520To%2520address%2520the%2520challenge%2520of%2520high%2520dimensionality%252C%2520we%250Aemploy%2520a%2520Triplet%2520Network%2520to%2520encode%2520images%2520into%2520low-dimensional%2520representations%250Ain%2520a%2520latent%2520space%252C%2520which%2520serve%2520as%2520a%2520basis%2520for%2520comparing%2520the%2520efficiency%2520of%2520both%250Athe%2520Stochastic%2520Quantization%2520algorithm%2520and%2520traditional%2520quantization%2520algorithms.%250AFurthermore%252C%2520we%2520enhance%2520the%2520algorithm%2527s%2520convergence%2520speed%2520by%2520introducing%250Amodifications%2520with%2520an%2520adaptive%2520learning%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02066v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Clustering%20on%20High-Dimensional%20Data%20with%20Stochastic%20Quantization&entry.906535625=Anton%20Kozyriev%20and%20Vladimir%20Norkin&entry.1292438233=%20%20This%20paper%20addresses%20the%20limitations%20of%20conventional%20vector%20quantization%0Aalgorithms%2C%20particularly%20K-Means%20and%20its%20variant%20K-Means%2B%2B%2C%20and%20investigates%0Athe%20Stochastic%20Quantization%20%28SQ%29%20algorithm%20as%20a%20scalable%20alternative%20for%0Ahigh-dimensional%20unsupervised%20and%20semi-supervised%20learning%20tasks.%20Traditional%0Aclustering%20algorithms%20often%20suffer%20from%20inefficient%20memory%20utilization%20during%0Acomputation%2C%20necessitating%20the%20loading%20of%20all%20data%20samples%20into%20memory%2C%20which%0Abecomes%20impractical%20for%20large-scale%20datasets.%20While%20variants%20such%20as%20Mini-Batch%0AK-Means%20partially%20mitigate%20this%20issue%20by%20reducing%20memory%20usage%2C%20they%20lack%0Arobust%20theoretical%20convergence%20guarantees%20due%20to%20the%20non-convex%20nature%20of%0Aclustering%20problems.%20In%20contrast%2C%20the%20Stochastic%20Quantization%20algorithm%0Aprovides%20strong%20theoretical%20convergence%20guarantees%2C%20making%20it%20a%20robust%0Aalternative%20for%20clustering%20tasks.%20We%20demonstrate%20the%20computational%20efficiency%0Aand%20rapid%20convergence%20of%20the%20algorithm%20on%20an%20image%20classification%20problem%20with%0Apartially%20labeled%20data%2C%20comparing%20model%20accuracy%20across%20various%20ratios%20of%0Alabeled%20to%20unlabeled%20data.%20To%20address%20the%20challenge%20of%20high%20dimensionality%2C%20we%0Aemploy%20a%20Triplet%20Network%20to%20encode%20images%20into%20low-dimensional%20representations%0Ain%20a%20latent%20space%2C%20which%20serve%20as%20a%20basis%20for%20comparing%20the%20efficiency%20of%20both%0Athe%20Stochastic%20Quantization%20algorithm%20and%20traditional%20quantization%20algorithms.%0AFurthermore%2C%20we%20enhance%20the%20algorithm%27s%20convergence%20speed%20by%20introducing%0Amodifications%20with%20an%20adaptive%20learning%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02066v4&entry.124074799=Read"},
{"title": "Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit\n  Q-Learning", "author": "Alexi Canesse and Mathieu Petitbois and Ludovic Denoyer and Sylvain Lamprier and R\u00e9my Portelas", "abstract": "  Offline Reinforcement Learning (RL) has emerged as a powerful alternative to\nimitation learning for behavior modeling in various domains, particularly in\ncomplex navigation tasks. An existing challenge with Offline RL is the\nsignal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to\nerrors in value estimates. Towards this, multiple works have demonstrated the\nadvantage of hierarchical offline RL methods, which decouples high-level path\nplanning from low-level path following. In this work, we present a novel\nhierarchical transformer-based approach leveraging a learned quantizer of the\nspace. This quantization enables the training of a simpler zone-conditioned\nlow-level policy and simplifies planning, which is reduced to discrete\nautoregressive prediction. Among other benefits, zone-level reasoning in\nplanning enables explicit trajectory stitching rather than implicit stitching\nbased on noisy value function estimates. By combining this transformer-based\nplanner with recent advancements in offline RL, our proposed approach achieves\nstate-of-the-art results in complex long-distance navigation environments.\n", "link": "http://arxiv.org/abs/2411.07760v1", "date": "2024-11-12", "relevancy": 2.0016, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5215}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5047}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigation%20with%20QPHIL%3A%20Quantizing%20Planner%20for%20Hierarchical%20Implicit%0A%20%20Q-Learning&body=Title%3A%20Navigation%20with%20QPHIL%3A%20Quantizing%20Planner%20for%20Hierarchical%20Implicit%0A%20%20Q-Learning%0AAuthor%3A%20Alexi%20Canesse%20and%20Mathieu%20Petitbois%20and%20Ludovic%20Denoyer%20and%20Sylvain%20Lamprier%20and%20R%C3%A9my%20Portelas%0AAbstract%3A%20%20%20Offline%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20alternative%20to%0Aimitation%20learning%20for%20behavior%20modeling%20in%20various%20domains%2C%20particularly%20in%0Acomplex%20navigation%20tasks.%20An%20existing%20challenge%20with%20Offline%20RL%20is%20the%0Asignal-to-noise%20ratio%2C%20i.e.%20how%20to%20mitigate%20incorrect%20policy%20updates%20due%20to%0Aerrors%20in%20value%20estimates.%20Towards%20this%2C%20multiple%20works%20have%20demonstrated%20the%0Aadvantage%20of%20hierarchical%20offline%20RL%20methods%2C%20which%20decouples%20high-level%20path%0Aplanning%20from%20low-level%20path%20following.%20In%20this%20work%2C%20we%20present%20a%20novel%0Ahierarchical%20transformer-based%20approach%20leveraging%20a%20learned%20quantizer%20of%20the%0Aspace.%20This%20quantization%20enables%20the%20training%20of%20a%20simpler%20zone-conditioned%0Alow-level%20policy%20and%20simplifies%20planning%2C%20which%20is%20reduced%20to%20discrete%0Aautoregressive%20prediction.%20Among%20other%20benefits%2C%20zone-level%20reasoning%20in%0Aplanning%20enables%20explicit%20trajectory%20stitching%20rather%20than%20implicit%20stitching%0Abased%20on%20noisy%20value%20function%20estimates.%20By%20combining%20this%20transformer-based%0Aplanner%20with%20recent%20advancements%20in%20offline%20RL%2C%20our%20proposed%20approach%20achieves%0Astate-of-the-art%20results%20in%20complex%20long-distance%20navigation%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigation%2520with%2520QPHIL%253A%2520Quantizing%2520Planner%2520for%2520Hierarchical%2520Implicit%250A%2520%2520Q-Learning%26entry.906535625%3DAlexi%2520Canesse%2520and%2520Mathieu%2520Petitbois%2520and%2520Ludovic%2520Denoyer%2520and%2520Sylvain%2520Lamprier%2520and%2520R%25C3%25A9my%2520Portelas%26entry.1292438233%3D%2520%2520Offline%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520alternative%2520to%250Aimitation%2520learning%2520for%2520behavior%2520modeling%2520in%2520various%2520domains%252C%2520particularly%2520in%250Acomplex%2520navigation%2520tasks.%2520An%2520existing%2520challenge%2520with%2520Offline%2520RL%2520is%2520the%250Asignal-to-noise%2520ratio%252C%2520i.e.%2520how%2520to%2520mitigate%2520incorrect%2520policy%2520updates%2520due%2520to%250Aerrors%2520in%2520value%2520estimates.%2520Towards%2520this%252C%2520multiple%2520works%2520have%2520demonstrated%2520the%250Aadvantage%2520of%2520hierarchical%2520offline%2520RL%2520methods%252C%2520which%2520decouples%2520high-level%2520path%250Aplanning%2520from%2520low-level%2520path%2520following.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%250Ahierarchical%2520transformer-based%2520approach%2520leveraging%2520a%2520learned%2520quantizer%2520of%2520the%250Aspace.%2520This%2520quantization%2520enables%2520the%2520training%2520of%2520a%2520simpler%2520zone-conditioned%250Alow-level%2520policy%2520and%2520simplifies%2520planning%252C%2520which%2520is%2520reduced%2520to%2520discrete%250Aautoregressive%2520prediction.%2520Among%2520other%2520benefits%252C%2520zone-level%2520reasoning%2520in%250Aplanning%2520enables%2520explicit%2520trajectory%2520stitching%2520rather%2520than%2520implicit%2520stitching%250Abased%2520on%2520noisy%2520value%2520function%2520estimates.%2520By%2520combining%2520this%2520transformer-based%250Aplanner%2520with%2520recent%2520advancements%2520in%2520offline%2520RL%252C%2520our%2520proposed%2520approach%2520achieves%250Astate-of-the-art%2520results%2520in%2520complex%2520long-distance%2520navigation%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigation%20with%20QPHIL%3A%20Quantizing%20Planner%20for%20Hierarchical%20Implicit%0A%20%20Q-Learning&entry.906535625=Alexi%20Canesse%20and%20Mathieu%20Petitbois%20and%20Ludovic%20Denoyer%20and%20Sylvain%20Lamprier%20and%20R%C3%A9my%20Portelas&entry.1292438233=%20%20Offline%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20alternative%20to%0Aimitation%20learning%20for%20behavior%20modeling%20in%20various%20domains%2C%20particularly%20in%0Acomplex%20navigation%20tasks.%20An%20existing%20challenge%20with%20Offline%20RL%20is%20the%0Asignal-to-noise%20ratio%2C%20i.e.%20how%20to%20mitigate%20incorrect%20policy%20updates%20due%20to%0Aerrors%20in%20value%20estimates.%20Towards%20this%2C%20multiple%20works%20have%20demonstrated%20the%0Aadvantage%20of%20hierarchical%20offline%20RL%20methods%2C%20which%20decouples%20high-level%20path%0Aplanning%20from%20low-level%20path%20following.%20In%20this%20work%2C%20we%20present%20a%20novel%0Ahierarchical%20transformer-based%20approach%20leveraging%20a%20learned%20quantizer%20of%20the%0Aspace.%20This%20quantization%20enables%20the%20training%20of%20a%20simpler%20zone-conditioned%0Alow-level%20policy%20and%20simplifies%20planning%2C%20which%20is%20reduced%20to%20discrete%0Aautoregressive%20prediction.%20Among%20other%20benefits%2C%20zone-level%20reasoning%20in%0Aplanning%20enables%20explicit%20trajectory%20stitching%20rather%20than%20implicit%20stitching%0Abased%20on%20noisy%20value%20function%20estimates.%20By%20combining%20this%20transformer-based%0Aplanner%20with%20recent%20advancements%20in%20offline%20RL%2C%20our%20proposed%20approach%20achieves%0Astate-of-the-art%20results%20in%20complex%20long-distance%20navigation%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07760v1&entry.124074799=Read"},
{"title": "Odd-One-Out: Anomaly Detection by Comparing with Neighbors", "author": "Ankan Bhunia and Changjian Li and Hakan Bilen", "abstract": "  This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances in a given\nscene. In contrast to the traditional AD benchmarks, anomalies in our task are\nscene-specific, defined by the regular instances that make up the majority.\nSince object instances may be only partly visible from a single viewpoint, our\nsetting employs multiple views of each scene as input. To provide a testbed for\nfuture research in this task, we introduce two benchmarks, ToysAD-8K and\nPartsAD-15K. We propose a novel method that constructs 3D object-centric\nrepresentations from multiple 2D views for each instance and detects the\nanomalous ones through a cross-instance comparison. We rigorously analyze our\nmethod quantitatively and qualitatively on the presented benchmarks.\n", "link": "http://arxiv.org/abs/2406.20099v2", "date": "2024-11-12", "relevancy": 2.0, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5135}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors&body=Title%3A%20Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors%0AAuthor%3A%20Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20%28AD%29%20problem%20that%20focuses%20on%0Aidentifying%20%60odd-looking%27%20objects%20relative%20to%20the%20other%20instances%20in%20a%20given%0Ascene.%20In%20contrast%20to%20the%20traditional%20AD%20benchmarks%2C%20anomalies%20in%20our%20task%20are%0Ascene-specific%2C%20defined%20by%20the%20regular%20instances%20that%20make%20up%20the%20majority.%0ASince%20object%20instances%20may%20be%20only%20partly%20visible%20from%20a%20single%20viewpoint%2C%20our%0Asetting%20employs%20multiple%20views%20of%20each%20scene%20as%20input.%20To%20provide%20a%20testbed%20for%0Afuture%20research%20in%20this%20task%2C%20we%20introduce%20two%20benchmarks%2C%20ToysAD-8K%20and%0APartsAD-15K.%20We%20propose%20a%20novel%20method%20that%20constructs%203D%20object-centric%0Arepresentations%20from%20multiple%202D%20views%20for%20each%20instance%20and%20detects%20the%0Aanomalous%20ones%20through%20a%20cross-instance%20comparison.%20We%20rigorously%20analyze%20our%0Amethod%20quantitatively%20and%20qualitatively%20on%20the%20presented%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOdd-One-Out%253A%2520Anomaly%2520Detection%2520by%2520Comparing%2520with%2520Neighbors%26entry.906535625%3DAnkan%2520Bhunia%2520and%2520Changjian%2520Li%2520and%2520Hakan%2520Bilen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520anomaly%2520detection%2520%2528AD%2529%2520problem%2520that%2520focuses%2520on%250Aidentifying%2520%2560odd-looking%2527%2520objects%2520relative%2520to%2520the%2520other%2520instances%2520in%2520a%2520given%250Ascene.%2520In%2520contrast%2520to%2520the%2520traditional%2520AD%2520benchmarks%252C%2520anomalies%2520in%2520our%2520task%2520are%250Ascene-specific%252C%2520defined%2520by%2520the%2520regular%2520instances%2520that%2520make%2520up%2520the%2520majority.%250ASince%2520object%2520instances%2520may%2520be%2520only%2520partly%2520visible%2520from%2520a%2520single%2520viewpoint%252C%2520our%250Asetting%2520employs%2520multiple%2520views%2520of%2520each%2520scene%2520as%2520input.%2520To%2520provide%2520a%2520testbed%2520for%250Afuture%2520research%2520in%2520this%2520task%252C%2520we%2520introduce%2520two%2520benchmarks%252C%2520ToysAD-8K%2520and%250APartsAD-15K.%2520We%2520propose%2520a%2520novel%2520method%2520that%2520constructs%25203D%2520object-centric%250Arepresentations%2520from%2520multiple%25202D%2520views%2520for%2520each%2520instance%2520and%2520detects%2520the%250Aanomalous%2520ones%2520through%2520a%2520cross-instance%2520comparison.%2520We%2520rigorously%2520analyze%2520our%250Amethod%2520quantitatively%2520and%2520qualitatively%2520on%2520the%2520presented%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Odd-One-Out%3A%20Anomaly%20Detection%20by%20Comparing%20with%20Neighbors&entry.906535625=Ankan%20Bhunia%20and%20Changjian%20Li%20and%20Hakan%20Bilen&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20%28AD%29%20problem%20that%20focuses%20on%0Aidentifying%20%60odd-looking%27%20objects%20relative%20to%20the%20other%20instances%20in%20a%20given%0Ascene.%20In%20contrast%20to%20the%20traditional%20AD%20benchmarks%2C%20anomalies%20in%20our%20task%20are%0Ascene-specific%2C%20defined%20by%20the%20regular%20instances%20that%20make%20up%20the%20majority.%0ASince%20object%20instances%20may%20be%20only%20partly%20visible%20from%20a%20single%20viewpoint%2C%20our%0Asetting%20employs%20multiple%20views%20of%20each%20scene%20as%20input.%20To%20provide%20a%20testbed%20for%0Afuture%20research%20in%20this%20task%2C%20we%20introduce%20two%20benchmarks%2C%20ToysAD-8K%20and%0APartsAD-15K.%20We%20propose%20a%20novel%20method%20that%20constructs%203D%20object-centric%0Arepresentations%20from%20multiple%202D%20views%20for%20each%20instance%20and%20detects%20the%0Aanomalous%20ones%20through%20a%20cross-instance%20comparison.%20We%20rigorously%20analyze%20our%0Amethod%20quantitatively%20and%20qualitatively%20on%20the%20presented%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20099v2&entry.124074799=Read"},
{"title": "Iterative Learning Control with Mismatch Compensation for Residual\n  Vibration Suppression in Delta Robots", "author": "Mingkun Wu and Alisa Rupenyan and Burkhard Corves", "abstract": "  Unwanted vibrations stemming from the energy-optimized design of Delta robots\npose a challenge in their operation, especially with respect to precise\nreference tracking. To improve tracking accuracy, this paper proposes an\nadaptive mismatch-compensated iterative learning controller based on input\nshaping techniques. We establish a dynamic model considering the\nelectromechanical rigid-flexible coupling of the Delta robot, which integrates\nthe permanent magnet synchronous motor. Using this model, we design an\noptimization-based input shaper, considering the natural frequency of the\nrobot, which varies with the configuration. We proposed an iterative learning\ncontroller for the delta robot to improve tracking accuracy. Our iterative\nlearning controller incorporates model mismatch where the mismatch approximated\nby a fuzzy logic structure. The convergence property of the proposed controller\nis proved using a Barrier Composite Energy Function, providing a guarantee that\nthe tracking errors along the iteration axis converge to zero. Moreover,\nadaptive parameter update laws are designed to ensure convergence. Finally, we\nperform a series of high-fidelity simulations of the Delta robot using Simscape\nto demonstrate the effectiveness of the proposed control strategy.\n", "link": "http://arxiv.org/abs/2411.07862v1", "date": "2024-11-12", "relevancy": 1.9942, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5008}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Learning%20Control%20with%20Mismatch%20Compensation%20for%20Residual%0A%20%20Vibration%20Suppression%20in%20Delta%20Robots&body=Title%3A%20Iterative%20Learning%20Control%20with%20Mismatch%20Compensation%20for%20Residual%0A%20%20Vibration%20Suppression%20in%20Delta%20Robots%0AAuthor%3A%20Mingkun%20Wu%20and%20Alisa%20Rupenyan%20and%20Burkhard%20Corves%0AAbstract%3A%20%20%20Unwanted%20vibrations%20stemming%20from%20the%20energy-optimized%20design%20of%20Delta%20robots%0Apose%20a%20challenge%20in%20their%20operation%2C%20especially%20with%20respect%20to%20precise%0Areference%20tracking.%20To%20improve%20tracking%20accuracy%2C%20this%20paper%20proposes%20an%0Aadaptive%20mismatch-compensated%20iterative%20learning%20controller%20based%20on%20input%0Ashaping%20techniques.%20We%20establish%20a%20dynamic%20model%20considering%20the%0Aelectromechanical%20rigid-flexible%20coupling%20of%20the%20Delta%20robot%2C%20which%20integrates%0Athe%20permanent%20magnet%20synchronous%20motor.%20Using%20this%20model%2C%20we%20design%20an%0Aoptimization-based%20input%20shaper%2C%20considering%20the%20natural%20frequency%20of%20the%0Arobot%2C%20which%20varies%20with%20the%20configuration.%20We%20proposed%20an%20iterative%20learning%0Acontroller%20for%20the%20delta%20robot%20to%20improve%20tracking%20accuracy.%20Our%20iterative%0Alearning%20controller%20incorporates%20model%20mismatch%20where%20the%20mismatch%20approximated%0Aby%20a%20fuzzy%20logic%20structure.%20The%20convergence%20property%20of%20the%20proposed%20controller%0Ais%20proved%20using%20a%20Barrier%20Composite%20Energy%20Function%2C%20providing%20a%20guarantee%20that%0Athe%20tracking%20errors%20along%20the%20iteration%20axis%20converge%20to%20zero.%20Moreover%2C%0Aadaptive%20parameter%20update%20laws%20are%20designed%20to%20ensure%20convergence.%20Finally%2C%20we%0Aperform%20a%20series%20of%20high-fidelity%20simulations%20of%20the%20Delta%20robot%20using%20Simscape%0Ato%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20control%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Learning%2520Control%2520with%2520Mismatch%2520Compensation%2520for%2520Residual%250A%2520%2520Vibration%2520Suppression%2520in%2520Delta%2520Robots%26entry.906535625%3DMingkun%2520Wu%2520and%2520Alisa%2520Rupenyan%2520and%2520Burkhard%2520Corves%26entry.1292438233%3D%2520%2520Unwanted%2520vibrations%2520stemming%2520from%2520the%2520energy-optimized%2520design%2520of%2520Delta%2520robots%250Apose%2520a%2520challenge%2520in%2520their%2520operation%252C%2520especially%2520with%2520respect%2520to%2520precise%250Areference%2520tracking.%2520To%2520improve%2520tracking%2520accuracy%252C%2520this%2520paper%2520proposes%2520an%250Aadaptive%2520mismatch-compensated%2520iterative%2520learning%2520controller%2520based%2520on%2520input%250Ashaping%2520techniques.%2520We%2520establish%2520a%2520dynamic%2520model%2520considering%2520the%250Aelectromechanical%2520rigid-flexible%2520coupling%2520of%2520the%2520Delta%2520robot%252C%2520which%2520integrates%250Athe%2520permanent%2520magnet%2520synchronous%2520motor.%2520Using%2520this%2520model%252C%2520we%2520design%2520an%250Aoptimization-based%2520input%2520shaper%252C%2520considering%2520the%2520natural%2520frequency%2520of%2520the%250Arobot%252C%2520which%2520varies%2520with%2520the%2520configuration.%2520We%2520proposed%2520an%2520iterative%2520learning%250Acontroller%2520for%2520the%2520delta%2520robot%2520to%2520improve%2520tracking%2520accuracy.%2520Our%2520iterative%250Alearning%2520controller%2520incorporates%2520model%2520mismatch%2520where%2520the%2520mismatch%2520approximated%250Aby%2520a%2520fuzzy%2520logic%2520structure.%2520The%2520convergence%2520property%2520of%2520the%2520proposed%2520controller%250Ais%2520proved%2520using%2520a%2520Barrier%2520Composite%2520Energy%2520Function%252C%2520providing%2520a%2520guarantee%2520that%250Athe%2520tracking%2520errors%2520along%2520the%2520iteration%2520axis%2520converge%2520to%2520zero.%2520Moreover%252C%250Aadaptive%2520parameter%2520update%2520laws%2520are%2520designed%2520to%2520ensure%2520convergence.%2520Finally%252C%2520we%250Aperform%2520a%2520series%2520of%2520high-fidelity%2520simulations%2520of%2520the%2520Delta%2520robot%2520using%2520Simscape%250Ato%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520control%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Learning%20Control%20with%20Mismatch%20Compensation%20for%20Residual%0A%20%20Vibration%20Suppression%20in%20Delta%20Robots&entry.906535625=Mingkun%20Wu%20and%20Alisa%20Rupenyan%20and%20Burkhard%20Corves&entry.1292438233=%20%20Unwanted%20vibrations%20stemming%20from%20the%20energy-optimized%20design%20of%20Delta%20robots%0Apose%20a%20challenge%20in%20their%20operation%2C%20especially%20with%20respect%20to%20precise%0Areference%20tracking.%20To%20improve%20tracking%20accuracy%2C%20this%20paper%20proposes%20an%0Aadaptive%20mismatch-compensated%20iterative%20learning%20controller%20based%20on%20input%0Ashaping%20techniques.%20We%20establish%20a%20dynamic%20model%20considering%20the%0Aelectromechanical%20rigid-flexible%20coupling%20of%20the%20Delta%20robot%2C%20which%20integrates%0Athe%20permanent%20magnet%20synchronous%20motor.%20Using%20this%20model%2C%20we%20design%20an%0Aoptimization-based%20input%20shaper%2C%20considering%20the%20natural%20frequency%20of%20the%0Arobot%2C%20which%20varies%20with%20the%20configuration.%20We%20proposed%20an%20iterative%20learning%0Acontroller%20for%20the%20delta%20robot%20to%20improve%20tracking%20accuracy.%20Our%20iterative%0Alearning%20controller%20incorporates%20model%20mismatch%20where%20the%20mismatch%20approximated%0Aby%20a%20fuzzy%20logic%20structure.%20The%20convergence%20property%20of%20the%20proposed%20controller%0Ais%20proved%20using%20a%20Barrier%20Composite%20Energy%20Function%2C%20providing%20a%20guarantee%20that%0Athe%20tracking%20errors%20along%20the%20iteration%20axis%20converge%20to%20zero.%20Moreover%2C%0Aadaptive%20parameter%20update%20laws%20are%20designed%20to%20ensure%20convergence.%20Finally%2C%20we%0Aperform%20a%20series%20of%20high-fidelity%20simulations%20of%20the%20Delta%20robot%20using%20Simscape%0Ato%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20control%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07862v1&entry.124074799=Read"},
{"title": "Artistic Neural Style Transfer Algorithms with Activation Smoothing", "author": "Xiangtian Li and Han Cao and Zhaoyang Zhang and Jiacheng Hu and Yuhui Jin and Zihao Zhao", "abstract": "  The works of Gatys et al. demonstrated the capability of Convolutional Neural\nNetworks (CNNs) in creating artistic style images. This process of transferring\ncontent images in different styles is called Neural Style Transfer (NST). In\nthis paper, we re-implement image-based NST, fast NST, and arbitrary NST. We\nalso explore to utilize ResNet with activation smoothing in NST. Extensive\nexperimental results demonstrate that smoothing transformation can greatly\nimprove the quality of stylization results.\n", "link": "http://arxiv.org/abs/2411.08014v1", "date": "2024-11-12", "relevancy": 1.9885, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5213}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4947}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artistic%20Neural%20Style%20Transfer%20Algorithms%20with%20Activation%20Smoothing&body=Title%3A%20Artistic%20Neural%20Style%20Transfer%20Algorithms%20with%20Activation%20Smoothing%0AAuthor%3A%20Xiangtian%20Li%20and%20Han%20Cao%20and%20Zhaoyang%20Zhang%20and%20Jiacheng%20Hu%20and%20Yuhui%20Jin%20and%20Zihao%20Zhao%0AAbstract%3A%20%20%20The%20works%20of%20Gatys%20et%20al.%20demonstrated%20the%20capability%20of%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20in%20creating%20artistic%20style%20images.%20This%20process%20of%20transferring%0Acontent%20images%20in%20different%20styles%20is%20called%20Neural%20Style%20Transfer%20%28NST%29.%20In%0Athis%20paper%2C%20we%20re-implement%20image-based%20NST%2C%20fast%20NST%2C%20and%20arbitrary%20NST.%20We%0Aalso%20explore%20to%20utilize%20ResNet%20with%20activation%20smoothing%20in%20NST.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20smoothing%20transformation%20can%20greatly%0Aimprove%20the%20quality%20of%20stylization%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtistic%2520Neural%2520Style%2520Transfer%2520Algorithms%2520with%2520Activation%2520Smoothing%26entry.906535625%3DXiangtian%2520Li%2520and%2520Han%2520Cao%2520and%2520Zhaoyang%2520Zhang%2520and%2520Jiacheng%2520Hu%2520and%2520Yuhui%2520Jin%2520and%2520Zihao%2520Zhao%26entry.1292438233%3D%2520%2520The%2520works%2520of%2520Gatys%2520et%2520al.%2520demonstrated%2520the%2520capability%2520of%2520Convolutional%2520Neural%250ANetworks%2520%2528CNNs%2529%2520in%2520creating%2520artistic%2520style%2520images.%2520This%2520process%2520of%2520transferring%250Acontent%2520images%2520in%2520different%2520styles%2520is%2520called%2520Neural%2520Style%2520Transfer%2520%2528NST%2529.%2520In%250Athis%2520paper%252C%2520we%2520re-implement%2520image-based%2520NST%252C%2520fast%2520NST%252C%2520and%2520arbitrary%2520NST.%2520We%250Aalso%2520explore%2520to%2520utilize%2520ResNet%2520with%2520activation%2520smoothing%2520in%2520NST.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520smoothing%2520transformation%2520can%2520greatly%250Aimprove%2520the%2520quality%2520of%2520stylization%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artistic%20Neural%20Style%20Transfer%20Algorithms%20with%20Activation%20Smoothing&entry.906535625=Xiangtian%20Li%20and%20Han%20Cao%20and%20Zhaoyang%20Zhang%20and%20Jiacheng%20Hu%20and%20Yuhui%20Jin%20and%20Zihao%20Zhao&entry.1292438233=%20%20The%20works%20of%20Gatys%20et%20al.%20demonstrated%20the%20capability%20of%20Convolutional%20Neural%0ANetworks%20%28CNNs%29%20in%20creating%20artistic%20style%20images.%20This%20process%20of%20transferring%0Acontent%20images%20in%20different%20styles%20is%20called%20Neural%20Style%20Transfer%20%28NST%29.%20In%0Athis%20paper%2C%20we%20re-implement%20image-based%20NST%2C%20fast%20NST%2C%20and%20arbitrary%20NST.%20We%0Aalso%20explore%20to%20utilize%20ResNet%20with%20activation%20smoothing%20in%20NST.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20smoothing%20transformation%20can%20greatly%0Aimprove%20the%20quality%20of%20stylization%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08014v1&entry.124074799=Read"},
{"title": "On the Utilization of Unique Node Identifiers in Graph Neural Networks", "author": "Maya Bechler-Speicher and Moshe Eliasof and Carola-Bibiane Sch\u00f6nlieb and Ran Gilad-Bachrach and Amir Globerson", "abstract": "  Graph Neural Networks have inherent representational limitations due to their\nmessage-passing structure. Recent work has suggested that these limitations can\nbe overcome by using unique node identifiers (UIDs). Here we argue that despite\nthe advantages of UIDs, one of their disadvantages is that they lose the\ndesirable property of permutation-equivariance. We thus propose to focus on UID\nmodels that are permutation-equivariant, and present theoretical arguments for\ntheir advantages. Motivated by this, we propose a method to regularize UID\nmodels towards permutation equivariance, via a contrastive loss. We empirically\ndemonstrate that our approach improves generalization and extrapolation\nabilities while providing faster training convergence. On the recent BREC\nexpressiveness benchmark, our proposed method achieves state-of-the-art\nperformance compared to other random-based approaches.\n", "link": "http://arxiv.org/abs/2411.02271v2", "date": "2024-11-12", "relevancy": 1.983, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5111}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4885}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Utilization%20of%20Unique%20Node%20Identifiers%20in%20Graph%20Neural%20Networks&body=Title%3A%20On%20the%20Utilization%20of%20Unique%20Node%20Identifiers%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Maya%20Bechler-Speicher%20and%20Moshe%20Eliasof%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Ran%20Gilad-Bachrach%20and%20Amir%20Globerson%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20have%20inherent%20representational%20limitations%20due%20to%20their%0Amessage-passing%20structure.%20Recent%20work%20has%20suggested%20that%20these%20limitations%20can%0Abe%20overcome%20by%20using%20unique%20node%20identifiers%20%28UIDs%29.%20Here%20we%20argue%20that%20despite%0Athe%20advantages%20of%20UIDs%2C%20one%20of%20their%20disadvantages%20is%20that%20they%20lose%20the%0Adesirable%20property%20of%20permutation-equivariance.%20We%20thus%20propose%20to%20focus%20on%20UID%0Amodels%20that%20are%20permutation-equivariant%2C%20and%20present%20theoretical%20arguments%20for%0Atheir%20advantages.%20Motivated%20by%20this%2C%20we%20propose%20a%20method%20to%20regularize%20UID%0Amodels%20towards%20permutation%20equivariance%2C%20via%20a%20contrastive%20loss.%20We%20empirically%0Ademonstrate%20that%20our%20approach%20improves%20generalization%20and%20extrapolation%0Aabilities%20while%20providing%20faster%20training%20convergence.%20On%20the%20recent%20BREC%0Aexpressiveness%20benchmark%2C%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20other%20random-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Utilization%2520of%2520Unique%2520Node%2520Identifiers%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMaya%2520Bechler-Speicher%2520and%2520Moshe%2520Eliasof%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Ran%2520Gilad-Bachrach%2520and%2520Amir%2520Globerson%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520have%2520inherent%2520representational%2520limitations%2520due%2520to%2520their%250Amessage-passing%2520structure.%2520Recent%2520work%2520has%2520suggested%2520that%2520these%2520limitations%2520can%250Abe%2520overcome%2520by%2520using%2520unique%2520node%2520identifiers%2520%2528UIDs%2529.%2520Here%2520we%2520argue%2520that%2520despite%250Athe%2520advantages%2520of%2520UIDs%252C%2520one%2520of%2520their%2520disadvantages%2520is%2520that%2520they%2520lose%2520the%250Adesirable%2520property%2520of%2520permutation-equivariance.%2520We%2520thus%2520propose%2520to%2520focus%2520on%2520UID%250Amodels%2520that%2520are%2520permutation-equivariant%252C%2520and%2520present%2520theoretical%2520arguments%2520for%250Atheir%2520advantages.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520method%2520to%2520regularize%2520UID%250Amodels%2520towards%2520permutation%2520equivariance%252C%2520via%2520a%2520contrastive%2520loss.%2520We%2520empirically%250Ademonstrate%2520that%2520our%2520approach%2520improves%2520generalization%2520and%2520extrapolation%250Aabilities%2520while%2520providing%2520faster%2520training%2520convergence.%2520On%2520the%2520recent%2520BREC%250Aexpressiveness%2520benchmark%252C%2520our%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520compared%2520to%2520other%2520random-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Utilization%20of%20Unique%20Node%20Identifiers%20in%20Graph%20Neural%20Networks&entry.906535625=Maya%20Bechler-Speicher%20and%20Moshe%20Eliasof%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Ran%20Gilad-Bachrach%20and%20Amir%20Globerson&entry.1292438233=%20%20Graph%20Neural%20Networks%20have%20inherent%20representational%20limitations%20due%20to%20their%0Amessage-passing%20structure.%20Recent%20work%20has%20suggested%20that%20these%20limitations%20can%0Abe%20overcome%20by%20using%20unique%20node%20identifiers%20%28UIDs%29.%20Here%20we%20argue%20that%20despite%0Athe%20advantages%20of%20UIDs%2C%20one%20of%20their%20disadvantages%20is%20that%20they%20lose%20the%0Adesirable%20property%20of%20permutation-equivariance.%20We%20thus%20propose%20to%20focus%20on%20UID%0Amodels%20that%20are%20permutation-equivariant%2C%20and%20present%20theoretical%20arguments%20for%0Atheir%20advantages.%20Motivated%20by%20this%2C%20we%20propose%20a%20method%20to%20regularize%20UID%0Amodels%20towards%20permutation%20equivariance%2C%20via%20a%20contrastive%20loss.%20We%20empirically%0Ademonstrate%20that%20our%20approach%20improves%20generalization%20and%20extrapolation%0Aabilities%20while%20providing%20faster%20training%20convergence.%20On%20the%20recent%20BREC%0Aexpressiveness%20benchmark%2C%20our%20proposed%20method%20achieves%20state-of-the-art%0Aperformance%20compared%20to%20other%20random-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02271v2&entry.124074799=Read"},
{"title": "ExpressivityArena: Can LLMs Express Information Implicitly?", "author": "Joshua Tint and Som Sagar and Aditya Taparia and Kelly Raines and Bimsara Pathiraja and Caleb Liu and Ransalu Senanayake", "abstract": "  While Large Language Models (LLMs) have demonstrated remarkable performance\nin certain dimensions, their ability to express implicit language cues that\nhuman use for effective communication remains unclear. This paper presents\nExpressivityArena, a Python library for measuring the implicit communication\nabilities of LLMs. We provide a comprehensive framework to evaluate\nexpressivity of arbitrary LLMs and explore its practical implications. To this\nend, we refine the definition and measurements of ``expressivity,'' and use our\nframework in a set of small experiments. These experiments test LLMs in\ncreative and logical tasks such as poetry, coding, and emotion-based responses.\nThey are then evaluated by an automated grader, through ExpressivityArena,\nwhich we verify to be the most pragmatic for testing expressivity. Building on\nthese experiments, we deepen our understanding of the expressivity of LLMs by\nassessing their ability to remain expressive in conversations. Our findings\nindicate that LLMs are capable of generating and understanding expressive\ncontent, however, with some limitations. These insights will inform the future\ndevelopment and deployment of expressive LLMs. We provide the code for\nExpressivityArena alongside our paper.\n", "link": "http://arxiv.org/abs/2411.08010v1", "date": "2024-11-12", "relevancy": 1.9822, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpressivityArena%3A%20Can%20LLMs%20Express%20Information%20Implicitly%3F&body=Title%3A%20ExpressivityArena%3A%20Can%20LLMs%20Express%20Information%20Implicitly%3F%0AAuthor%3A%20Joshua%20Tint%20and%20Som%20Sagar%20and%20Aditya%20Taparia%20and%20Kelly%20Raines%20and%20Bimsara%20Pathiraja%20and%20Caleb%20Liu%20and%20Ransalu%20Senanayake%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%0Ain%20certain%20dimensions%2C%20their%20ability%20to%20express%20implicit%20language%20cues%20that%0Ahuman%20use%20for%20effective%20communication%20remains%20unclear.%20This%20paper%20presents%0AExpressivityArena%2C%20a%20Python%20library%20for%20measuring%20the%20implicit%20communication%0Aabilities%20of%20LLMs.%20We%20provide%20a%20comprehensive%20framework%20to%20evaluate%0Aexpressivity%20of%20arbitrary%20LLMs%20and%20explore%20its%20practical%20implications.%20To%20this%0Aend%2C%20we%20refine%20the%20definition%20and%20measurements%20of%20%60%60expressivity%2C%27%27%20and%20use%20our%0Aframework%20in%20a%20set%20of%20small%20experiments.%20These%20experiments%20test%20LLMs%20in%0Acreative%20and%20logical%20tasks%20such%20as%20poetry%2C%20coding%2C%20and%20emotion-based%20responses.%0AThey%20are%20then%20evaluated%20by%20an%20automated%20grader%2C%20through%20ExpressivityArena%2C%0Awhich%20we%20verify%20to%20be%20the%20most%20pragmatic%20for%20testing%20expressivity.%20Building%20on%0Athese%20experiments%2C%20we%20deepen%20our%20understanding%20of%20the%20expressivity%20of%20LLMs%20by%0Aassessing%20their%20ability%20to%20remain%20expressive%20in%20conversations.%20Our%20findings%0Aindicate%20that%20LLMs%20are%20capable%20of%20generating%20and%20understanding%20expressive%0Acontent%2C%20however%2C%20with%20some%20limitations.%20These%20insights%20will%20inform%20the%20future%0Adevelopment%20and%20deployment%20of%20expressive%20LLMs.%20We%20provide%20the%20code%20for%0AExpressivityArena%20alongside%20our%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpressivityArena%253A%2520Can%2520LLMs%2520Express%2520Information%2520Implicitly%253F%26entry.906535625%3DJoshua%2520Tint%2520and%2520Som%2520Sagar%2520and%2520Aditya%2520Taparia%2520and%2520Kelly%2520Raines%2520and%2520Bimsara%2520Pathiraja%2520and%2520Caleb%2520Liu%2520and%2520Ransalu%2520Senanayake%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%250Ain%2520certain%2520dimensions%252C%2520their%2520ability%2520to%2520express%2520implicit%2520language%2520cues%2520that%250Ahuman%2520use%2520for%2520effective%2520communication%2520remains%2520unclear.%2520This%2520paper%2520presents%250AExpressivityArena%252C%2520a%2520Python%2520library%2520for%2520measuring%2520the%2520implicit%2520communication%250Aabilities%2520of%2520LLMs.%2520We%2520provide%2520a%2520comprehensive%2520framework%2520to%2520evaluate%250Aexpressivity%2520of%2520arbitrary%2520LLMs%2520and%2520explore%2520its%2520practical%2520implications.%2520To%2520this%250Aend%252C%2520we%2520refine%2520the%2520definition%2520and%2520measurements%2520of%2520%2560%2560expressivity%252C%2527%2527%2520and%2520use%2520our%250Aframework%2520in%2520a%2520set%2520of%2520small%2520experiments.%2520These%2520experiments%2520test%2520LLMs%2520in%250Acreative%2520and%2520logical%2520tasks%2520such%2520as%2520poetry%252C%2520coding%252C%2520and%2520emotion-based%2520responses.%250AThey%2520are%2520then%2520evaluated%2520by%2520an%2520automated%2520grader%252C%2520through%2520ExpressivityArena%252C%250Awhich%2520we%2520verify%2520to%2520be%2520the%2520most%2520pragmatic%2520for%2520testing%2520expressivity.%2520Building%2520on%250Athese%2520experiments%252C%2520we%2520deepen%2520our%2520understanding%2520of%2520the%2520expressivity%2520of%2520LLMs%2520by%250Aassessing%2520their%2520ability%2520to%2520remain%2520expressive%2520in%2520conversations.%2520Our%2520findings%250Aindicate%2520that%2520LLMs%2520are%2520capable%2520of%2520generating%2520and%2520understanding%2520expressive%250Acontent%252C%2520however%252C%2520with%2520some%2520limitations.%2520These%2520insights%2520will%2520inform%2520the%2520future%250Adevelopment%2520and%2520deployment%2520of%2520expressive%2520LLMs.%2520We%2520provide%2520the%2520code%2520for%250AExpressivityArena%2520alongside%2520our%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpressivityArena%3A%20Can%20LLMs%20Express%20Information%20Implicitly%3F&entry.906535625=Joshua%20Tint%20and%20Som%20Sagar%20and%20Aditya%20Taparia%20and%20Kelly%20Raines%20and%20Bimsara%20Pathiraja%20and%20Caleb%20Liu%20and%20Ransalu%20Senanayake&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20performance%0Ain%20certain%20dimensions%2C%20their%20ability%20to%20express%20implicit%20language%20cues%20that%0Ahuman%20use%20for%20effective%20communication%20remains%20unclear.%20This%20paper%20presents%0AExpressivityArena%2C%20a%20Python%20library%20for%20measuring%20the%20implicit%20communication%0Aabilities%20of%20LLMs.%20We%20provide%20a%20comprehensive%20framework%20to%20evaluate%0Aexpressivity%20of%20arbitrary%20LLMs%20and%20explore%20its%20practical%20implications.%20To%20this%0Aend%2C%20we%20refine%20the%20definition%20and%20measurements%20of%20%60%60expressivity%2C%27%27%20and%20use%20our%0Aframework%20in%20a%20set%20of%20small%20experiments.%20These%20experiments%20test%20LLMs%20in%0Acreative%20and%20logical%20tasks%20such%20as%20poetry%2C%20coding%2C%20and%20emotion-based%20responses.%0AThey%20are%20then%20evaluated%20by%20an%20automated%20grader%2C%20through%20ExpressivityArena%2C%0Awhich%20we%20verify%20to%20be%20the%20most%20pragmatic%20for%20testing%20expressivity.%20Building%20on%0Athese%20experiments%2C%20we%20deepen%20our%20understanding%20of%20the%20expressivity%20of%20LLMs%20by%0Aassessing%20their%20ability%20to%20remain%20expressive%20in%20conversations.%20Our%20findings%0Aindicate%20that%20LLMs%20are%20capable%20of%20generating%20and%20understanding%20expressive%0Acontent%2C%20however%2C%20with%20some%20limitations.%20These%20insights%20will%20inform%20the%20future%0Adevelopment%20and%20deployment%20of%20expressive%20LLMs.%20We%20provide%20the%20code%20for%0AExpressivityArena%20alongside%20our%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08010v1&entry.124074799=Read"},
{"title": "Explicit and Implicit Semantic Ranking Framework", "author": "Xiaofeng Zhu and Thomas Lin and Vishal Anand and Matthew Calderwood and Eric Clausen-Brown and Gord Lueck and Wen-wai Yim and Cheng Wu", "abstract": "  The core challenge in numerous real-world applications is to match an inquiry\nto the best document from a mutable and finite set of candidates. Existing\nindustry solutions, especially latency-constrained services, often rely on\nsimilarity algorithms that sacrifice quality for speed. In this paper we\nintroduce a generic semantic learning-to-rank framework, Self-training Semantic\nCross-attention Ranking (sRank). This transformer-based framework uses linear\npairwise loss with mutable training batch sizes and achieves quality gains and\nhigh efficiency, and has been applied effectively to show gains on two industry\ntasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and\nAmbient Clinical Intelligence (ACI). In Smart Reply, sRank assists live\ncustomers with technical support by selecting the best reply from predefined\nsolutions based on consumer and support agent messages. It achieves 11.7% gain\nin offline top-one accuracy on the SR task over the previous system, and has\nenabled 38.7% time reduction in composing messages in telemetry recorded since\nits general release in January 2021. In the ACI task, sRank selects relevant\nhistorical physician templates that serve as guidance for a text summarization\nmodel to generate higher quality medical notes. It achieves 35.5% top-one\naccuracy gain, along with 46% relative ROUGE-L gain in generated medical notes.\n", "link": "http://arxiv.org/abs/2304.04918v2", "date": "2024-11-12", "relevancy": 1.9791, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20and%20Implicit%20Semantic%20Ranking%20Framework&body=Title%3A%20Explicit%20and%20Implicit%20Semantic%20Ranking%20Framework%0AAuthor%3A%20Xiaofeng%20Zhu%20and%20Thomas%20Lin%20and%20Vishal%20Anand%20and%20Matthew%20Calderwood%20and%20Eric%20Clausen-Brown%20and%20Gord%20Lueck%20and%20Wen-wai%20Yim%20and%20Cheng%20Wu%0AAbstract%3A%20%20%20The%20core%20challenge%20in%20numerous%20real-world%20applications%20is%20to%20match%20an%20inquiry%0Ato%20the%20best%20document%20from%20a%20mutable%20and%20finite%20set%20of%20candidates.%20Existing%0Aindustry%20solutions%2C%20especially%20latency-constrained%20services%2C%20often%20rely%20on%0Asimilarity%20algorithms%20that%20sacrifice%20quality%20for%20speed.%20In%20this%20paper%20we%0Aintroduce%20a%20generic%20semantic%20learning-to-rank%20framework%2C%20Self-training%20Semantic%0ACross-attention%20Ranking%20%28sRank%29.%20This%20transformer-based%20framework%20uses%20linear%0Apairwise%20loss%20with%20mutable%20training%20batch%20sizes%20and%20achieves%20quality%20gains%20and%0Ahigh%20efficiency%2C%20and%20has%20been%20applied%20effectively%20to%20show%20gains%20on%20two%20industry%0Atasks%20at%20Microsoft%20over%20real-world%20large-scale%20data%20sets%3A%20Smart%20Reply%20%28SR%29%20and%0AAmbient%20Clinical%20Intelligence%20%28ACI%29.%20In%20Smart%20Reply%2C%20sRank%20assists%20live%0Acustomers%20with%20technical%20support%20by%20selecting%20the%20best%20reply%20from%20predefined%0Asolutions%20based%20on%20consumer%20and%20support%20agent%20messages.%20It%20achieves%2011.7%25%20gain%0Ain%20offline%20top-one%20accuracy%20on%20the%20SR%20task%20over%20the%20previous%20system%2C%20and%20has%0Aenabled%2038.7%25%20time%20reduction%20in%20composing%20messages%20in%20telemetry%20recorded%20since%0Aits%20general%20release%20in%20January%202021.%20In%20the%20ACI%20task%2C%20sRank%20selects%20relevant%0Ahistorical%20physician%20templates%20that%20serve%20as%20guidance%20for%20a%20text%20summarization%0Amodel%20to%20generate%20higher%20quality%20medical%20notes.%20It%20achieves%2035.5%25%20top-one%0Aaccuracy%20gain%2C%20along%20with%2046%25%20relative%20ROUGE-L%20gain%20in%20generated%20medical%20notes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.04918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520and%2520Implicit%2520Semantic%2520Ranking%2520Framework%26entry.906535625%3DXiaofeng%2520Zhu%2520and%2520Thomas%2520Lin%2520and%2520Vishal%2520Anand%2520and%2520Matthew%2520Calderwood%2520and%2520Eric%2520Clausen-Brown%2520and%2520Gord%2520Lueck%2520and%2520Wen-wai%2520Yim%2520and%2520Cheng%2520Wu%26entry.1292438233%3D%2520%2520The%2520core%2520challenge%2520in%2520numerous%2520real-world%2520applications%2520is%2520to%2520match%2520an%2520inquiry%250Ato%2520the%2520best%2520document%2520from%2520a%2520mutable%2520and%2520finite%2520set%2520of%2520candidates.%2520Existing%250Aindustry%2520solutions%252C%2520especially%2520latency-constrained%2520services%252C%2520often%2520rely%2520on%250Asimilarity%2520algorithms%2520that%2520sacrifice%2520quality%2520for%2520speed.%2520In%2520this%2520paper%2520we%250Aintroduce%2520a%2520generic%2520semantic%2520learning-to-rank%2520framework%252C%2520Self-training%2520Semantic%250ACross-attention%2520Ranking%2520%2528sRank%2529.%2520This%2520transformer-based%2520framework%2520uses%2520linear%250Apairwise%2520loss%2520with%2520mutable%2520training%2520batch%2520sizes%2520and%2520achieves%2520quality%2520gains%2520and%250Ahigh%2520efficiency%252C%2520and%2520has%2520been%2520applied%2520effectively%2520to%2520show%2520gains%2520on%2520two%2520industry%250Atasks%2520at%2520Microsoft%2520over%2520real-world%2520large-scale%2520data%2520sets%253A%2520Smart%2520Reply%2520%2528SR%2529%2520and%250AAmbient%2520Clinical%2520Intelligence%2520%2528ACI%2529.%2520In%2520Smart%2520Reply%252C%2520sRank%2520assists%2520live%250Acustomers%2520with%2520technical%2520support%2520by%2520selecting%2520the%2520best%2520reply%2520from%2520predefined%250Asolutions%2520based%2520on%2520consumer%2520and%2520support%2520agent%2520messages.%2520It%2520achieves%252011.7%2525%2520gain%250Ain%2520offline%2520top-one%2520accuracy%2520on%2520the%2520SR%2520task%2520over%2520the%2520previous%2520system%252C%2520and%2520has%250Aenabled%252038.7%2525%2520time%2520reduction%2520in%2520composing%2520messages%2520in%2520telemetry%2520recorded%2520since%250Aits%2520general%2520release%2520in%2520January%25202021.%2520In%2520the%2520ACI%2520task%252C%2520sRank%2520selects%2520relevant%250Ahistorical%2520physician%2520templates%2520that%2520serve%2520as%2520guidance%2520for%2520a%2520text%2520summarization%250Amodel%2520to%2520generate%2520higher%2520quality%2520medical%2520notes.%2520It%2520achieves%252035.5%2525%2520top-one%250Aaccuracy%2520gain%252C%2520along%2520with%252046%2525%2520relative%2520ROUGE-L%2520gain%2520in%2520generated%2520medical%2520notes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.04918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20and%20Implicit%20Semantic%20Ranking%20Framework&entry.906535625=Xiaofeng%20Zhu%20and%20Thomas%20Lin%20and%20Vishal%20Anand%20and%20Matthew%20Calderwood%20and%20Eric%20Clausen-Brown%20and%20Gord%20Lueck%20and%20Wen-wai%20Yim%20and%20Cheng%20Wu&entry.1292438233=%20%20The%20core%20challenge%20in%20numerous%20real-world%20applications%20is%20to%20match%20an%20inquiry%0Ato%20the%20best%20document%20from%20a%20mutable%20and%20finite%20set%20of%20candidates.%20Existing%0Aindustry%20solutions%2C%20especially%20latency-constrained%20services%2C%20often%20rely%20on%0Asimilarity%20algorithms%20that%20sacrifice%20quality%20for%20speed.%20In%20this%20paper%20we%0Aintroduce%20a%20generic%20semantic%20learning-to-rank%20framework%2C%20Self-training%20Semantic%0ACross-attention%20Ranking%20%28sRank%29.%20This%20transformer-based%20framework%20uses%20linear%0Apairwise%20loss%20with%20mutable%20training%20batch%20sizes%20and%20achieves%20quality%20gains%20and%0Ahigh%20efficiency%2C%20and%20has%20been%20applied%20effectively%20to%20show%20gains%20on%20two%20industry%0Atasks%20at%20Microsoft%20over%20real-world%20large-scale%20data%20sets%3A%20Smart%20Reply%20%28SR%29%20and%0AAmbient%20Clinical%20Intelligence%20%28ACI%29.%20In%20Smart%20Reply%2C%20sRank%20assists%20live%0Acustomers%20with%20technical%20support%20by%20selecting%20the%20best%20reply%20from%20predefined%0Asolutions%20based%20on%20consumer%20and%20support%20agent%20messages.%20It%20achieves%2011.7%25%20gain%0Ain%20offline%20top-one%20accuracy%20on%20the%20SR%20task%20over%20the%20previous%20system%2C%20and%20has%0Aenabled%2038.7%25%20time%20reduction%20in%20composing%20messages%20in%20telemetry%20recorded%20since%0Aits%20general%20release%20in%20January%202021.%20In%20the%20ACI%20task%2C%20sRank%20selects%20relevant%0Ahistorical%20physician%20templates%20that%20serve%20as%20guidance%20for%20a%20text%20summarization%0Amodel%20to%20generate%20higher%20quality%20medical%20notes.%20It%20achieves%2035.5%25%20top-one%0Aaccuracy%20gain%2C%20along%20with%2046%25%20relative%20ROUGE-L%20gain%20in%20generated%20medical%20notes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.04918v2&entry.124074799=Read"},
{"title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive\n  Study", "author": "Van Bach Nguyen and Paul Youssef and Christin Seifert and J\u00f6rg Schl\u00f6tterer", "abstract": "  As NLP models become more complex, understanding their decisions becomes more\ncrucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's\nprediction, offer a way to explain these models. While Large Language Models\n(LLMs) have shown remarkable performance in NLP tasks, their efficacy in\ngenerating high-quality CFs remains uncertain. This work fills this gap by\ninvestigating how well LLMs generate CFs for two NLU tasks. We conduct a\ncomprehensive comparison of several common LLMs, and evaluate their CFs,\nassessing both intrinsic metrics, and the impact of these CFs on data\naugmentation. Moreover, we analyze differences between human and LLM-generated\nCFs, providing insights for future research directions. Our results show that\nLLMs generate fluent CFs, but struggle to keep the induced changes minimal.\nGenerating CFs for Sentiment Analysis (SA) is less challenging than NLI where\nLLMs show weaknesses in generating CFs that flip the original label. This also\nreflects on the data augmentation performance, where we observe a large gap\nbetween augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs'\nability to assess CFs in a mislabelled data setting, and show that they have a\nstrong bias towards agreeing with the provided labels. GPT4 is more robust\nagainst this bias and its scores correlate well with automatic metrics. Our\nfindings reveal several limitations and point to potential future work\ndirections.\n", "link": "http://arxiv.org/abs/2405.00722v2", "date": "2024-11-12", "relevancy": 1.977, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20for%20Generating%20and%20Evaluating%20Counterfactuals%3A%20A%20Comprehensive%0A%20%20Study&body=Title%3A%20LLMs%20for%20Generating%20and%20Evaluating%20Counterfactuals%3A%20A%20Comprehensive%0A%20%20Study%0AAuthor%3A%20Van%20Bach%20Nguyen%20and%20Paul%20Youssef%20and%20Christin%20Seifert%20and%20J%C3%B6rg%20Schl%C3%B6tterer%0AAbstract%3A%20%20%20As%20NLP%20models%20become%20more%20complex%2C%20understanding%20their%20decisions%20becomes%20more%0Acrucial.%20Counterfactuals%20%28CFs%29%2C%20where%20minimal%20changes%20to%20inputs%20flip%20a%20model%27s%0Aprediction%2C%20offer%20a%20way%20to%20explain%20these%20models.%20While%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20NLP%20tasks%2C%20their%20efficacy%20in%0Agenerating%20high-quality%20CFs%20remains%20uncertain.%20This%20work%20fills%20this%20gap%20by%0Ainvestigating%20how%20well%20LLMs%20generate%20CFs%20for%20two%20NLU%20tasks.%20We%20conduct%20a%0Acomprehensive%20comparison%20of%20several%20common%20LLMs%2C%20and%20evaluate%20their%20CFs%2C%0Aassessing%20both%20intrinsic%20metrics%2C%20and%20the%20impact%20of%20these%20CFs%20on%20data%0Aaugmentation.%20Moreover%2C%20we%20analyze%20differences%20between%20human%20and%20LLM-generated%0ACFs%2C%20providing%20insights%20for%20future%20research%20directions.%20Our%20results%20show%20that%0ALLMs%20generate%20fluent%20CFs%2C%20but%20struggle%20to%20keep%20the%20induced%20changes%20minimal.%0AGenerating%20CFs%20for%20Sentiment%20Analysis%20%28SA%29%20is%20less%20challenging%20than%20NLI%20where%0ALLMs%20show%20weaknesses%20in%20generating%20CFs%20that%20flip%20the%20original%20label.%20This%20also%0Areflects%20on%20the%20data%20augmentation%20performance%2C%20where%20we%20observe%20a%20large%20gap%0Abetween%20augmenting%20with%20human%20and%20LLMs%20CFs.%20Furthermore%2C%20we%20evaluate%20LLMs%27%0Aability%20to%20assess%20CFs%20in%20a%20mislabelled%20data%20setting%2C%20and%20show%20that%20they%20have%20a%0Astrong%20bias%20towards%20agreeing%20with%20the%20provided%20labels.%20GPT4%20is%20more%20robust%0Aagainst%20this%20bias%20and%20its%20scores%20correlate%20well%20with%20automatic%20metrics.%20Our%0Afindings%20reveal%20several%20limitations%20and%20point%20to%20potential%20future%20work%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520for%2520Generating%2520and%2520Evaluating%2520Counterfactuals%253A%2520A%2520Comprehensive%250A%2520%2520Study%26entry.906535625%3DVan%2520Bach%2520Nguyen%2520and%2520Paul%2520Youssef%2520and%2520Christin%2520Seifert%2520and%2520J%25C3%25B6rg%2520Schl%25C3%25B6tterer%26entry.1292438233%3D%2520%2520As%2520NLP%2520models%2520become%2520more%2520complex%252C%2520understanding%2520their%2520decisions%2520becomes%2520more%250Acrucial.%2520Counterfactuals%2520%2528CFs%2529%252C%2520where%2520minimal%2520changes%2520to%2520inputs%2520flip%2520a%2520model%2527s%250Aprediction%252C%2520offer%2520a%2520way%2520to%2520explain%2520these%2520models.%2520While%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520NLP%2520tasks%252C%2520their%2520efficacy%2520in%250Agenerating%2520high-quality%2520CFs%2520remains%2520uncertain.%2520This%2520work%2520fills%2520this%2520gap%2520by%250Ainvestigating%2520how%2520well%2520LLMs%2520generate%2520CFs%2520for%2520two%2520NLU%2520tasks.%2520We%2520conduct%2520a%250Acomprehensive%2520comparison%2520of%2520several%2520common%2520LLMs%252C%2520and%2520evaluate%2520their%2520CFs%252C%250Aassessing%2520both%2520intrinsic%2520metrics%252C%2520and%2520the%2520impact%2520of%2520these%2520CFs%2520on%2520data%250Aaugmentation.%2520Moreover%252C%2520we%2520analyze%2520differences%2520between%2520human%2520and%2520LLM-generated%250ACFs%252C%2520providing%2520insights%2520for%2520future%2520research%2520directions.%2520Our%2520results%2520show%2520that%250ALLMs%2520generate%2520fluent%2520CFs%252C%2520but%2520struggle%2520to%2520keep%2520the%2520induced%2520changes%2520minimal.%250AGenerating%2520CFs%2520for%2520Sentiment%2520Analysis%2520%2528SA%2529%2520is%2520less%2520challenging%2520than%2520NLI%2520where%250ALLMs%2520show%2520weaknesses%2520in%2520generating%2520CFs%2520that%2520flip%2520the%2520original%2520label.%2520This%2520also%250Areflects%2520on%2520the%2520data%2520augmentation%2520performance%252C%2520where%2520we%2520observe%2520a%2520large%2520gap%250Abetween%2520augmenting%2520with%2520human%2520and%2520LLMs%2520CFs.%2520Furthermore%252C%2520we%2520evaluate%2520LLMs%2527%250Aability%2520to%2520assess%2520CFs%2520in%2520a%2520mislabelled%2520data%2520setting%252C%2520and%2520show%2520that%2520they%2520have%2520a%250Astrong%2520bias%2520towards%2520agreeing%2520with%2520the%2520provided%2520labels.%2520GPT4%2520is%2520more%2520robust%250Aagainst%2520this%2520bias%2520and%2520its%2520scores%2520correlate%2520well%2520with%2520automatic%2520metrics.%2520Our%250Afindings%2520reveal%2520several%2520limitations%2520and%2520point%2520to%2520potential%2520future%2520work%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20for%20Generating%20and%20Evaluating%20Counterfactuals%3A%20A%20Comprehensive%0A%20%20Study&entry.906535625=Van%20Bach%20Nguyen%20and%20Paul%20Youssef%20and%20Christin%20Seifert%20and%20J%C3%B6rg%20Schl%C3%B6tterer&entry.1292438233=%20%20As%20NLP%20models%20become%20more%20complex%2C%20understanding%20their%20decisions%20becomes%20more%0Acrucial.%20Counterfactuals%20%28CFs%29%2C%20where%20minimal%20changes%20to%20inputs%20flip%20a%20model%27s%0Aprediction%2C%20offer%20a%20way%20to%20explain%20these%20models.%20While%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20remarkable%20performance%20in%20NLP%20tasks%2C%20their%20efficacy%20in%0Agenerating%20high-quality%20CFs%20remains%20uncertain.%20This%20work%20fills%20this%20gap%20by%0Ainvestigating%20how%20well%20LLMs%20generate%20CFs%20for%20two%20NLU%20tasks.%20We%20conduct%20a%0Acomprehensive%20comparison%20of%20several%20common%20LLMs%2C%20and%20evaluate%20their%20CFs%2C%0Aassessing%20both%20intrinsic%20metrics%2C%20and%20the%20impact%20of%20these%20CFs%20on%20data%0Aaugmentation.%20Moreover%2C%20we%20analyze%20differences%20between%20human%20and%20LLM-generated%0ACFs%2C%20providing%20insights%20for%20future%20research%20directions.%20Our%20results%20show%20that%0ALLMs%20generate%20fluent%20CFs%2C%20but%20struggle%20to%20keep%20the%20induced%20changes%20minimal.%0AGenerating%20CFs%20for%20Sentiment%20Analysis%20%28SA%29%20is%20less%20challenging%20than%20NLI%20where%0ALLMs%20show%20weaknesses%20in%20generating%20CFs%20that%20flip%20the%20original%20label.%20This%20also%0Areflects%20on%20the%20data%20augmentation%20performance%2C%20where%20we%20observe%20a%20large%20gap%0Abetween%20augmenting%20with%20human%20and%20LLMs%20CFs.%20Furthermore%2C%20we%20evaluate%20LLMs%27%0Aability%20to%20assess%20CFs%20in%20a%20mislabelled%20data%20setting%2C%20and%20show%20that%20they%20have%20a%0Astrong%20bias%20towards%20agreeing%20with%20the%20provided%20labels.%20GPT4%20is%20more%20robust%0Aagainst%20this%20bias%20and%20its%20scores%20correlate%20well%20with%20automatic%20metrics.%20Our%0Afindings%20reveal%20several%20limitations%20and%20point%20to%20potential%20future%20work%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00722v2&entry.124074799=Read"},
{"title": "Federated Low-Rank Adaptation with Differential Privacy over Wireless\n  Networks", "author": "Tianqu Kang and Zixin Wang and Hengtao He and Jun Zhang and Shenghui Song and Khaled B. Letaief", "abstract": "  Fine-tuning large pre-trained foundation models (FMs) on distributed edge\ndevices presents considerable computational and privacy challenges. Federated\nfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative\nmodel training without the need to share raw data. To lessen the computational\nburden on resource-limited devices, combining low-rank adaptation (LoRA) with\nfederated learning enables parameter-efficient fine-tuning. Additionally, the\nsplit FedFT architecture partitions an FM between edge devices and a central\nserver, reducing the necessity for complete model deployment on individual\ndevices. However, the risk of privacy eavesdropping attacks in FedFT remains a\nconcern, particularly in sensitive areas such as healthcare and finance. In\nthis paper, we propose a split FedFT framework with differential privacy (DP)\nover wireless networks, where the inherent wireless channel noise in the uplink\ntransmission is utilized to achieve DP guarantees without adding an extra\nartificial noise. We shall investigate the impact of the wireless noise on\nconvergence performance of the proposed framework. We will also show that by\nupdating only one of the low-rank matrices in the split FedFT with DP, the\nproposed method can mitigate the noise amplification effect. Simulation results\nwill demonstrate that the proposed framework achieves higher accuracy under\nstrict privacy budgets compared to baseline methods.\n", "link": "http://arxiv.org/abs/2411.07806v1", "date": "2024-11-12", "relevancy": 1.9751, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4905}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Low-Rank%20Adaptation%20with%20Differential%20Privacy%20over%20Wireless%0A%20%20Networks&body=Title%3A%20Federated%20Low-Rank%20Adaptation%20with%20Differential%20Privacy%20over%20Wireless%0A%20%20Networks%0AAuthor%3A%20Tianqu%20Kang%20and%20Zixin%20Wang%20and%20Hengtao%20He%20and%20Jun%20Zhang%20and%20Shenghui%20Song%20and%20Khaled%20B.%20Letaief%0AAbstract%3A%20%20%20Fine-tuning%20large%20pre-trained%20foundation%20models%20%28FMs%29%20on%20distributed%20edge%0Adevices%20presents%20considerable%20computational%20and%20privacy%20challenges.%20Federated%0Afine-tuning%20%28FedFT%29%20mitigates%20some%20privacy%20issues%20by%20facilitating%20collaborative%0Amodel%20training%20without%20the%20need%20to%20share%20raw%20data.%20To%20lessen%20the%20computational%0Aburden%20on%20resource-limited%20devices%2C%20combining%20low-rank%20adaptation%20%28LoRA%29%20with%0Afederated%20learning%20enables%20parameter-efficient%20fine-tuning.%20Additionally%2C%20the%0Asplit%20FedFT%20architecture%20partitions%20an%20FM%20between%20edge%20devices%20and%20a%20central%0Aserver%2C%20reducing%20the%20necessity%20for%20complete%20model%20deployment%20on%20individual%0Adevices.%20However%2C%20the%20risk%20of%20privacy%20eavesdropping%20attacks%20in%20FedFT%20remains%20a%0Aconcern%2C%20particularly%20in%20sensitive%20areas%20such%20as%20healthcare%20and%20finance.%20In%0Athis%20paper%2C%20we%20propose%20a%20split%20FedFT%20framework%20with%20differential%20privacy%20%28DP%29%0Aover%20wireless%20networks%2C%20where%20the%20inherent%20wireless%20channel%20noise%20in%20the%20uplink%0Atransmission%20is%20utilized%20to%20achieve%20DP%20guarantees%20without%20adding%20an%20extra%0Aartificial%20noise.%20We%20shall%20investigate%20the%20impact%20of%20the%20wireless%20noise%20on%0Aconvergence%20performance%20of%20the%20proposed%20framework.%20We%20will%20also%20show%20that%20by%0Aupdating%20only%20one%20of%20the%20low-rank%20matrices%20in%20the%20split%20FedFT%20with%20DP%2C%20the%0Aproposed%20method%20can%20mitigate%20the%20noise%20amplification%20effect.%20Simulation%20results%0Awill%20demonstrate%20that%20the%20proposed%20framework%20achieves%20higher%20accuracy%20under%0Astrict%20privacy%20budgets%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Low-Rank%2520Adaptation%2520with%2520Differential%2520Privacy%2520over%2520Wireless%250A%2520%2520Networks%26entry.906535625%3DTianqu%2520Kang%2520and%2520Zixin%2520Wang%2520and%2520Hengtao%2520He%2520and%2520Jun%2520Zhang%2520and%2520Shenghui%2520Song%2520and%2520Khaled%2520B.%2520Letaief%26entry.1292438233%3D%2520%2520Fine-tuning%2520large%2520pre-trained%2520foundation%2520models%2520%2528FMs%2529%2520on%2520distributed%2520edge%250Adevices%2520presents%2520considerable%2520computational%2520and%2520privacy%2520challenges.%2520Federated%250Afine-tuning%2520%2528FedFT%2529%2520mitigates%2520some%2520privacy%2520issues%2520by%2520facilitating%2520collaborative%250Amodel%2520training%2520without%2520the%2520need%2520to%2520share%2520raw%2520data.%2520To%2520lessen%2520the%2520computational%250Aburden%2520on%2520resource-limited%2520devices%252C%2520combining%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520with%250Afederated%2520learning%2520enables%2520parameter-efficient%2520fine-tuning.%2520Additionally%252C%2520the%250Asplit%2520FedFT%2520architecture%2520partitions%2520an%2520FM%2520between%2520edge%2520devices%2520and%2520a%2520central%250Aserver%252C%2520reducing%2520the%2520necessity%2520for%2520complete%2520model%2520deployment%2520on%2520individual%250Adevices.%2520However%252C%2520the%2520risk%2520of%2520privacy%2520eavesdropping%2520attacks%2520in%2520FedFT%2520remains%2520a%250Aconcern%252C%2520particularly%2520in%2520sensitive%2520areas%2520such%2520as%2520healthcare%2520and%2520finance.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520split%2520FedFT%2520framework%2520with%2520differential%2520privacy%2520%2528DP%2529%250Aover%2520wireless%2520networks%252C%2520where%2520the%2520inherent%2520wireless%2520channel%2520noise%2520in%2520the%2520uplink%250Atransmission%2520is%2520utilized%2520to%2520achieve%2520DP%2520guarantees%2520without%2520adding%2520an%2520extra%250Aartificial%2520noise.%2520We%2520shall%2520investigate%2520the%2520impact%2520of%2520the%2520wireless%2520noise%2520on%250Aconvergence%2520performance%2520of%2520the%2520proposed%2520framework.%2520We%2520will%2520also%2520show%2520that%2520by%250Aupdating%2520only%2520one%2520of%2520the%2520low-rank%2520matrices%2520in%2520the%2520split%2520FedFT%2520with%2520DP%252C%2520the%250Aproposed%2520method%2520can%2520mitigate%2520the%2520noise%2520amplification%2520effect.%2520Simulation%2520results%250Awill%2520demonstrate%2520that%2520the%2520proposed%2520framework%2520achieves%2520higher%2520accuracy%2520under%250Astrict%2520privacy%2520budgets%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Low-Rank%20Adaptation%20with%20Differential%20Privacy%20over%20Wireless%0A%20%20Networks&entry.906535625=Tianqu%20Kang%20and%20Zixin%20Wang%20and%20Hengtao%20He%20and%20Jun%20Zhang%20and%20Shenghui%20Song%20and%20Khaled%20B.%20Letaief&entry.1292438233=%20%20Fine-tuning%20large%20pre-trained%20foundation%20models%20%28FMs%29%20on%20distributed%20edge%0Adevices%20presents%20considerable%20computational%20and%20privacy%20challenges.%20Federated%0Afine-tuning%20%28FedFT%29%20mitigates%20some%20privacy%20issues%20by%20facilitating%20collaborative%0Amodel%20training%20without%20the%20need%20to%20share%20raw%20data.%20To%20lessen%20the%20computational%0Aburden%20on%20resource-limited%20devices%2C%20combining%20low-rank%20adaptation%20%28LoRA%29%20with%0Afederated%20learning%20enables%20parameter-efficient%20fine-tuning.%20Additionally%2C%20the%0Asplit%20FedFT%20architecture%20partitions%20an%20FM%20between%20edge%20devices%20and%20a%20central%0Aserver%2C%20reducing%20the%20necessity%20for%20complete%20model%20deployment%20on%20individual%0Adevices.%20However%2C%20the%20risk%20of%20privacy%20eavesdropping%20attacks%20in%20FedFT%20remains%20a%0Aconcern%2C%20particularly%20in%20sensitive%20areas%20such%20as%20healthcare%20and%20finance.%20In%0Athis%20paper%2C%20we%20propose%20a%20split%20FedFT%20framework%20with%20differential%20privacy%20%28DP%29%0Aover%20wireless%20networks%2C%20where%20the%20inherent%20wireless%20channel%20noise%20in%20the%20uplink%0Atransmission%20is%20utilized%20to%20achieve%20DP%20guarantees%20without%20adding%20an%20extra%0Aartificial%20noise.%20We%20shall%20investigate%20the%20impact%20of%20the%20wireless%20noise%20on%0Aconvergence%20performance%20of%20the%20proposed%20framework.%20We%20will%20also%20show%20that%20by%0Aupdating%20only%20one%20of%20the%20low-rank%20matrices%20in%20the%20split%20FedFT%20with%20DP%2C%20the%0Aproposed%20method%20can%20mitigate%20the%20noise%20amplification%20effect.%20Simulation%20results%0Awill%20demonstrate%20that%20the%20proposed%20framework%20achieves%20higher%20accuracy%20under%0Astrict%20privacy%20budgets%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07806v1&entry.124074799=Read"},
{"title": "Learning Memory Mechanisms for Decision Making through Demonstrations", "author": "William Yue and Bo Liu and Peter Stone", "abstract": "  In Partially Observable Markov Decision Processes, integrating an agent's\nhistory into memory poses a significant challenge for decision-making.\nTraditional imitation learning, relying on observation-action pairs for expert\ndemonstrations, fails to capture the expert's memory mechanisms used in\ndecision-making. To capture memory processes as demonstrations, we introduce\nthe concept of \\textbf{memory dependency pairs} $(p, q)$ indicating that events\nat time $p$ are recalled for decision-making at time $q$. We introduce\n\\textbf{AttentionTuner} to leverage memory dependency pairs in Transformers and\nfind significant improvements across several tasks compared to standard\nTransformers when evaluated on Memory Gym and the Long-term Memory Benchmark.\nCode is available at https://github.com/WilliamYue37/AttentionTuner .\n", "link": "http://arxiv.org/abs/2411.07954v1", "date": "2024-11-12", "relevancy": 1.9692, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4945}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Memory%20Mechanisms%20for%20Decision%20Making%20through%20Demonstrations&body=Title%3A%20Learning%20Memory%20Mechanisms%20for%20Decision%20Making%20through%20Demonstrations%0AAuthor%3A%20William%20Yue%20and%20Bo%20Liu%20and%20Peter%20Stone%0AAbstract%3A%20%20%20In%20Partially%20Observable%20Markov%20Decision%20Processes%2C%20integrating%20an%20agent%27s%0Ahistory%20into%20memory%20poses%20a%20significant%20challenge%20for%20decision-making.%0ATraditional%20imitation%20learning%2C%20relying%20on%20observation-action%20pairs%20for%20expert%0Ademonstrations%2C%20fails%20to%20capture%20the%20expert%27s%20memory%20mechanisms%20used%20in%0Adecision-making.%20To%20capture%20memory%20processes%20as%20demonstrations%2C%20we%20introduce%0Athe%20concept%20of%20%5Ctextbf%7Bmemory%20dependency%20pairs%7D%20%24%28p%2C%20q%29%24%20indicating%20that%20events%0Aat%20time%20%24p%24%20are%20recalled%20for%20decision-making%20at%20time%20%24q%24.%20We%20introduce%0A%5Ctextbf%7BAttentionTuner%7D%20to%20leverage%20memory%20dependency%20pairs%20in%20Transformers%20and%0Afind%20significant%20improvements%20across%20several%20tasks%20compared%20to%20standard%0ATransformers%20when%20evaluated%20on%20Memory%20Gym%20and%20the%20Long-term%20Memory%20Benchmark.%0ACode%20is%20available%20at%20https%3A//github.com/WilliamYue37/AttentionTuner%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Memory%2520Mechanisms%2520for%2520Decision%2520Making%2520through%2520Demonstrations%26entry.906535625%3DWilliam%2520Yue%2520and%2520Bo%2520Liu%2520and%2520Peter%2520Stone%26entry.1292438233%3D%2520%2520In%2520Partially%2520Observable%2520Markov%2520Decision%2520Processes%252C%2520integrating%2520an%2520agent%2527s%250Ahistory%2520into%2520memory%2520poses%2520a%2520significant%2520challenge%2520for%2520decision-making.%250ATraditional%2520imitation%2520learning%252C%2520relying%2520on%2520observation-action%2520pairs%2520for%2520expert%250Ademonstrations%252C%2520fails%2520to%2520capture%2520the%2520expert%2527s%2520memory%2520mechanisms%2520used%2520in%250Adecision-making.%2520To%2520capture%2520memory%2520processes%2520as%2520demonstrations%252C%2520we%2520introduce%250Athe%2520concept%2520of%2520%255Ctextbf%257Bmemory%2520dependency%2520pairs%257D%2520%2524%2528p%252C%2520q%2529%2524%2520indicating%2520that%2520events%250Aat%2520time%2520%2524p%2524%2520are%2520recalled%2520for%2520decision-making%2520at%2520time%2520%2524q%2524.%2520We%2520introduce%250A%255Ctextbf%257BAttentionTuner%257D%2520to%2520leverage%2520memory%2520dependency%2520pairs%2520in%2520Transformers%2520and%250Afind%2520significant%2520improvements%2520across%2520several%2520tasks%2520compared%2520to%2520standard%250ATransformers%2520when%2520evaluated%2520on%2520Memory%2520Gym%2520and%2520the%2520Long-term%2520Memory%2520Benchmark.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/WilliamYue37/AttentionTuner%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Memory%20Mechanisms%20for%20Decision%20Making%20through%20Demonstrations&entry.906535625=William%20Yue%20and%20Bo%20Liu%20and%20Peter%20Stone&entry.1292438233=%20%20In%20Partially%20Observable%20Markov%20Decision%20Processes%2C%20integrating%20an%20agent%27s%0Ahistory%20into%20memory%20poses%20a%20significant%20challenge%20for%20decision-making.%0ATraditional%20imitation%20learning%2C%20relying%20on%20observation-action%20pairs%20for%20expert%0Ademonstrations%2C%20fails%20to%20capture%20the%20expert%27s%20memory%20mechanisms%20used%20in%0Adecision-making.%20To%20capture%20memory%20processes%20as%20demonstrations%2C%20we%20introduce%0Athe%20concept%20of%20%5Ctextbf%7Bmemory%20dependency%20pairs%7D%20%24%28p%2C%20q%29%24%20indicating%20that%20events%0Aat%20time%20%24p%24%20are%20recalled%20for%20decision-making%20at%20time%20%24q%24.%20We%20introduce%0A%5Ctextbf%7BAttentionTuner%7D%20to%20leverage%20memory%20dependency%20pairs%20in%20Transformers%20and%0Afind%20significant%20improvements%20across%20several%20tasks%20compared%20to%20standard%0ATransformers%20when%20evaluated%20on%20Memory%20Gym%20and%20the%20Long-term%20Memory%20Benchmark.%0ACode%20is%20available%20at%20https%3A//github.com/WilliamYue37/AttentionTuner%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07954v1&entry.124074799=Read"},
{"title": "How To Discover Short, Shorter, and the Shortest Proofs of\n  Unsatisfiability: A Branch-and-Bound Approach for Resolution Proof Length\n  Minimization", "author": "Konstantin Sidorov and Koos van der Linden and Gon\u00e7alo Homem de Almeida Correia and Mathijs de Weerdt and Emir Demirovi\u0107", "abstract": "  Modern software for propositional satisfiability problems gives a powerful\nautomated reasoning toolkit, capable of outputting not only a\nsatisfiable/unsatisfiable signal but also a justification of unsatisfiability\nin the form of resolution proof (or a more expressive proof), which is commonly\nused for verification purposes. Empirically, modern SAT solvers produce\nrelatively short proofs, however, there are no inherent guarantees that these\nproofs cannot be significantly reduced. This paper proposes a novel\nbranch-and-bound algorithm for finding the shortest resolution proofs; to this\nend, we introduce a layer list representation of proofs that groups clauses by\ntheir level of indirection. As we show, this representation breaks all\npermutational symmetries, thereby improving upon the state-of-the-art\nsymmetry-breaking and informing the design of a novel workflow for proof\nminimization. In addition to that, we design pruning procedures that reason on\nproof length lower bound, clause subsumption, and dominance. Our experiments\nsuggest that the proofs from state-of-the-art solvers could be shortened by\n30-60% on the instances from SAT Competition 2002 and by 25-50% on small\nsynthetic formulas. When treated as an algorithm for finding the shortest\nproof, our approach solves twice as many instances as the previous work based\non SAT solving and reduces the time to optimality by orders of magnitude for\nthe instances solved by both approaches.\n", "link": "http://arxiv.org/abs/2411.07955v1", "date": "2024-11-12", "relevancy": 1.9674, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3952}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20To%20Discover%20Short%2C%20Shorter%2C%20and%20the%20Shortest%20Proofs%20of%0A%20%20Unsatisfiability%3A%20A%20Branch-and-Bound%20Approach%20for%20Resolution%20Proof%20Length%0A%20%20Minimization&body=Title%3A%20How%20To%20Discover%20Short%2C%20Shorter%2C%20and%20the%20Shortest%20Proofs%20of%0A%20%20Unsatisfiability%3A%20A%20Branch-and-Bound%20Approach%20for%20Resolution%20Proof%20Length%0A%20%20Minimization%0AAuthor%3A%20Konstantin%20Sidorov%20and%20Koos%20van%20der%20Linden%20and%20Gon%C3%A7alo%20Homem%20de%20Almeida%20Correia%20and%20Mathijs%20de%20Weerdt%20and%20Emir%20Demirovi%C4%87%0AAbstract%3A%20%20%20Modern%20software%20for%20propositional%20satisfiability%20problems%20gives%20a%20powerful%0Aautomated%20reasoning%20toolkit%2C%20capable%20of%20outputting%20not%20only%20a%0Asatisfiable/unsatisfiable%20signal%20but%20also%20a%20justification%20of%20unsatisfiability%0Ain%20the%20form%20of%20resolution%20proof%20%28or%20a%20more%20expressive%20proof%29%2C%20which%20is%20commonly%0Aused%20for%20verification%20purposes.%20Empirically%2C%20modern%20SAT%20solvers%20produce%0Arelatively%20short%20proofs%2C%20however%2C%20there%20are%20no%20inherent%20guarantees%20that%20these%0Aproofs%20cannot%20be%20significantly%20reduced.%20This%20paper%20proposes%20a%20novel%0Abranch-and-bound%20algorithm%20for%20finding%20the%20shortest%20resolution%20proofs%3B%20to%20this%0Aend%2C%20we%20introduce%20a%20layer%20list%20representation%20of%20proofs%20that%20groups%20clauses%20by%0Atheir%20level%20of%20indirection.%20As%20we%20show%2C%20this%20representation%20breaks%20all%0Apermutational%20symmetries%2C%20thereby%20improving%20upon%20the%20state-of-the-art%0Asymmetry-breaking%20and%20informing%20the%20design%20of%20a%20novel%20workflow%20for%20proof%0Aminimization.%20In%20addition%20to%20that%2C%20we%20design%20pruning%20procedures%20that%20reason%20on%0Aproof%20length%20lower%20bound%2C%20clause%20subsumption%2C%20and%20dominance.%20Our%20experiments%0Asuggest%20that%20the%20proofs%20from%20state-of-the-art%20solvers%20could%20be%20shortened%20by%0A30-60%25%20on%20the%20instances%20from%20SAT%20Competition%202002%20and%20by%2025-50%25%20on%20small%0Asynthetic%20formulas.%20When%20treated%20as%20an%20algorithm%20for%20finding%20the%20shortest%0Aproof%2C%20our%20approach%20solves%20twice%20as%20many%20instances%20as%20the%20previous%20work%20based%0Aon%20SAT%20solving%20and%20reduces%20the%20time%20to%20optimality%20by%20orders%20of%20magnitude%20for%0Athe%20instances%20solved%20by%20both%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520To%2520Discover%2520Short%252C%2520Shorter%252C%2520and%2520the%2520Shortest%2520Proofs%2520of%250A%2520%2520Unsatisfiability%253A%2520A%2520Branch-and-Bound%2520Approach%2520for%2520Resolution%2520Proof%2520Length%250A%2520%2520Minimization%26entry.906535625%3DKonstantin%2520Sidorov%2520and%2520Koos%2520van%2520der%2520Linden%2520and%2520Gon%25C3%25A7alo%2520Homem%2520de%2520Almeida%2520Correia%2520and%2520Mathijs%2520de%2520Weerdt%2520and%2520Emir%2520Demirovi%25C4%2587%26entry.1292438233%3D%2520%2520Modern%2520software%2520for%2520propositional%2520satisfiability%2520problems%2520gives%2520a%2520powerful%250Aautomated%2520reasoning%2520toolkit%252C%2520capable%2520of%2520outputting%2520not%2520only%2520a%250Asatisfiable/unsatisfiable%2520signal%2520but%2520also%2520a%2520justification%2520of%2520unsatisfiability%250Ain%2520the%2520form%2520of%2520resolution%2520proof%2520%2528or%2520a%2520more%2520expressive%2520proof%2529%252C%2520which%2520is%2520commonly%250Aused%2520for%2520verification%2520purposes.%2520Empirically%252C%2520modern%2520SAT%2520solvers%2520produce%250Arelatively%2520short%2520proofs%252C%2520however%252C%2520there%2520are%2520no%2520inherent%2520guarantees%2520that%2520these%250Aproofs%2520cannot%2520be%2520significantly%2520reduced.%2520This%2520paper%2520proposes%2520a%2520novel%250Abranch-and-bound%2520algorithm%2520for%2520finding%2520the%2520shortest%2520resolution%2520proofs%253B%2520to%2520this%250Aend%252C%2520we%2520introduce%2520a%2520layer%2520list%2520representation%2520of%2520proofs%2520that%2520groups%2520clauses%2520by%250Atheir%2520level%2520of%2520indirection.%2520As%2520we%2520show%252C%2520this%2520representation%2520breaks%2520all%250Apermutational%2520symmetries%252C%2520thereby%2520improving%2520upon%2520the%2520state-of-the-art%250Asymmetry-breaking%2520and%2520informing%2520the%2520design%2520of%2520a%2520novel%2520workflow%2520for%2520proof%250Aminimization.%2520In%2520addition%2520to%2520that%252C%2520we%2520design%2520pruning%2520procedures%2520that%2520reason%2520on%250Aproof%2520length%2520lower%2520bound%252C%2520clause%2520subsumption%252C%2520and%2520dominance.%2520Our%2520experiments%250Asuggest%2520that%2520the%2520proofs%2520from%2520state-of-the-art%2520solvers%2520could%2520be%2520shortened%2520by%250A30-60%2525%2520on%2520the%2520instances%2520from%2520SAT%2520Competition%25202002%2520and%2520by%252025-50%2525%2520on%2520small%250Asynthetic%2520formulas.%2520When%2520treated%2520as%2520an%2520algorithm%2520for%2520finding%2520the%2520shortest%250Aproof%252C%2520our%2520approach%2520solves%2520twice%2520as%2520many%2520instances%2520as%2520the%2520previous%2520work%2520based%250Aon%2520SAT%2520solving%2520and%2520reduces%2520the%2520time%2520to%2520optimality%2520by%2520orders%2520of%2520magnitude%2520for%250Athe%2520instances%2520solved%2520by%2520both%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20To%20Discover%20Short%2C%20Shorter%2C%20and%20the%20Shortest%20Proofs%20of%0A%20%20Unsatisfiability%3A%20A%20Branch-and-Bound%20Approach%20for%20Resolution%20Proof%20Length%0A%20%20Minimization&entry.906535625=Konstantin%20Sidorov%20and%20Koos%20van%20der%20Linden%20and%20Gon%C3%A7alo%20Homem%20de%20Almeida%20Correia%20and%20Mathijs%20de%20Weerdt%20and%20Emir%20Demirovi%C4%87&entry.1292438233=%20%20Modern%20software%20for%20propositional%20satisfiability%20problems%20gives%20a%20powerful%0Aautomated%20reasoning%20toolkit%2C%20capable%20of%20outputting%20not%20only%20a%0Asatisfiable/unsatisfiable%20signal%20but%20also%20a%20justification%20of%20unsatisfiability%0Ain%20the%20form%20of%20resolution%20proof%20%28or%20a%20more%20expressive%20proof%29%2C%20which%20is%20commonly%0Aused%20for%20verification%20purposes.%20Empirically%2C%20modern%20SAT%20solvers%20produce%0Arelatively%20short%20proofs%2C%20however%2C%20there%20are%20no%20inherent%20guarantees%20that%20these%0Aproofs%20cannot%20be%20significantly%20reduced.%20This%20paper%20proposes%20a%20novel%0Abranch-and-bound%20algorithm%20for%20finding%20the%20shortest%20resolution%20proofs%3B%20to%20this%0Aend%2C%20we%20introduce%20a%20layer%20list%20representation%20of%20proofs%20that%20groups%20clauses%20by%0Atheir%20level%20of%20indirection.%20As%20we%20show%2C%20this%20representation%20breaks%20all%0Apermutational%20symmetries%2C%20thereby%20improving%20upon%20the%20state-of-the-art%0Asymmetry-breaking%20and%20informing%20the%20design%20of%20a%20novel%20workflow%20for%20proof%0Aminimization.%20In%20addition%20to%20that%2C%20we%20design%20pruning%20procedures%20that%20reason%20on%0Aproof%20length%20lower%20bound%2C%20clause%20subsumption%2C%20and%20dominance.%20Our%20experiments%0Asuggest%20that%20the%20proofs%20from%20state-of-the-art%20solvers%20could%20be%20shortened%20by%0A30-60%25%20on%20the%20instances%20from%20SAT%20Competition%202002%20and%20by%2025-50%25%20on%20small%0Asynthetic%20formulas.%20When%20treated%20as%20an%20algorithm%20for%20finding%20the%20shortest%0Aproof%2C%20our%20approach%20solves%20twice%20as%20many%20instances%20as%20the%20previous%20work%20based%0Aon%20SAT%20solving%20and%20reduces%20the%20time%20to%20optimality%20by%20orders%20of%20magnitude%20for%0Athe%20instances%20solved%20by%20both%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07955v1&entry.124074799=Read"},
{"title": "Prediction of Acoustic Communication Performance for AUVs using Gaussian\n  Process Classification", "author": "Yifei Gao and Harun Yetkin and McMahon James and Daniel J. Stilwell", "abstract": "  Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic\ncommunication to coordinate their actions effectively. However, the reliability\nof underwater acoustic communication decreases as the communication range\nbetween vehicles increases. Consequently, teams of cooperating AUVs typically\nmake conservative assumptions about the maximum range at which they can\ncommunicate reliably. To address this limitation, we propose a novel approach\nthat involves learning a map representing the probability of successful\ncommunication based on the locations of the transmitting and receiving\nvehicles. This probabilistic communication map accounts for factors such as the\nrange between vehicles, environmental noise, and multi-path effects at a given\nlocation. In pursuit of this goal, we investigate the application of Gaussian\nprocess binary classification to generate the desired communication map. We\nspecialize existing results to this specific binary classification problem and\nexplore methods to incorporate uncertainty in vehicle location into the mapping\nprocess. Furthermore, we compare the prediction performance of the probability\ncommunication map generated using binary classification with that of a\nsignal-to-noise ratio (SNR) communication map generated using Gaussian process\nregression. Our approach is experimentally validated using communication and\nnavigation data collected during trials with a pair of Virginia Tech 690 AUVs.\n", "link": "http://arxiv.org/abs/2411.07933v1", "date": "2024-11-12", "relevancy": 1.9656, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20of%20Acoustic%20Communication%20Performance%20for%20AUVs%20using%20Gaussian%0A%20%20Process%20Classification&body=Title%3A%20Prediction%20of%20Acoustic%20Communication%20Performance%20for%20AUVs%20using%20Gaussian%0A%20%20Process%20Classification%0AAuthor%3A%20Yifei%20Gao%20and%20Harun%20Yetkin%20and%20McMahon%20James%20and%20Daniel%20J.%20Stilwell%0AAbstract%3A%20%20%20Cooperating%20autonomous%20underwater%20vehicles%20%28AUVs%29%20often%20rely%20on%20acoustic%0Acommunication%20to%20coordinate%20their%20actions%20effectively.%20However%2C%20the%20reliability%0Aof%20underwater%20acoustic%20communication%20decreases%20as%20the%20communication%20range%0Abetween%20vehicles%20increases.%20Consequently%2C%20teams%20of%20cooperating%20AUVs%20typically%0Amake%20conservative%20assumptions%20about%20the%20maximum%20range%20at%20which%20they%20can%0Acommunicate%20reliably.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20involves%20learning%20a%20map%20representing%20the%20probability%20of%20successful%0Acommunication%20based%20on%20the%20locations%20of%20the%20transmitting%20and%20receiving%0Avehicles.%20This%20probabilistic%20communication%20map%20accounts%20for%20factors%20such%20as%20the%0Arange%20between%20vehicles%2C%20environmental%20noise%2C%20and%20multi-path%20effects%20at%20a%20given%0Alocation.%20In%20pursuit%20of%20this%20goal%2C%20we%20investigate%20the%20application%20of%20Gaussian%0Aprocess%20binary%20classification%20to%20generate%20the%20desired%20communication%20map.%20We%0Aspecialize%20existing%20results%20to%20this%20specific%20binary%20classification%20problem%20and%0Aexplore%20methods%20to%20incorporate%20uncertainty%20in%20vehicle%20location%20into%20the%20mapping%0Aprocess.%20Furthermore%2C%20we%20compare%20the%20prediction%20performance%20of%20the%20probability%0Acommunication%20map%20generated%20using%20binary%20classification%20with%20that%20of%20a%0Asignal-to-noise%20ratio%20%28SNR%29%20communication%20map%20generated%20using%20Gaussian%20process%0Aregression.%20Our%20approach%20is%20experimentally%20validated%20using%20communication%20and%0Anavigation%20data%20collected%20during%20trials%20with%20a%20pair%20of%20Virginia%20Tech%20690%20AUVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520of%2520Acoustic%2520Communication%2520Performance%2520for%2520AUVs%2520using%2520Gaussian%250A%2520%2520Process%2520Classification%26entry.906535625%3DYifei%2520Gao%2520and%2520Harun%2520Yetkin%2520and%2520McMahon%2520James%2520and%2520Daniel%2520J.%2520Stilwell%26entry.1292438233%3D%2520%2520Cooperating%2520autonomous%2520underwater%2520vehicles%2520%2528AUVs%2529%2520often%2520rely%2520on%2520acoustic%250Acommunication%2520to%2520coordinate%2520their%2520actions%2520effectively.%2520However%252C%2520the%2520reliability%250Aof%2520underwater%2520acoustic%2520communication%2520decreases%2520as%2520the%2520communication%2520range%250Abetween%2520vehicles%2520increases.%2520Consequently%252C%2520teams%2520of%2520cooperating%2520AUVs%2520typically%250Amake%2520conservative%2520assumptions%2520about%2520the%2520maximum%2520range%2520at%2520which%2520they%2520can%250Acommunicate%2520reliably.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520approach%250Athat%2520involves%2520learning%2520a%2520map%2520representing%2520the%2520probability%2520of%2520successful%250Acommunication%2520based%2520on%2520the%2520locations%2520of%2520the%2520transmitting%2520and%2520receiving%250Avehicles.%2520This%2520probabilistic%2520communication%2520map%2520accounts%2520for%2520factors%2520such%2520as%2520the%250Arange%2520between%2520vehicles%252C%2520environmental%2520noise%252C%2520and%2520multi-path%2520effects%2520at%2520a%2520given%250Alocation.%2520In%2520pursuit%2520of%2520this%2520goal%252C%2520we%2520investigate%2520the%2520application%2520of%2520Gaussian%250Aprocess%2520binary%2520classification%2520to%2520generate%2520the%2520desired%2520communication%2520map.%2520We%250Aspecialize%2520existing%2520results%2520to%2520this%2520specific%2520binary%2520classification%2520problem%2520and%250Aexplore%2520methods%2520to%2520incorporate%2520uncertainty%2520in%2520vehicle%2520location%2520into%2520the%2520mapping%250Aprocess.%2520Furthermore%252C%2520we%2520compare%2520the%2520prediction%2520performance%2520of%2520the%2520probability%250Acommunication%2520map%2520generated%2520using%2520binary%2520classification%2520with%2520that%2520of%2520a%250Asignal-to-noise%2520ratio%2520%2528SNR%2529%2520communication%2520map%2520generated%2520using%2520Gaussian%2520process%250Aregression.%2520Our%2520approach%2520is%2520experimentally%2520validated%2520using%2520communication%2520and%250Anavigation%2520data%2520collected%2520during%2520trials%2520with%2520a%2520pair%2520of%2520Virginia%2520Tech%2520690%2520AUVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20Acoustic%20Communication%20Performance%20for%20AUVs%20using%20Gaussian%0A%20%20Process%20Classification&entry.906535625=Yifei%20Gao%20and%20Harun%20Yetkin%20and%20McMahon%20James%20and%20Daniel%20J.%20Stilwell&entry.1292438233=%20%20Cooperating%20autonomous%20underwater%20vehicles%20%28AUVs%29%20often%20rely%20on%20acoustic%0Acommunication%20to%20coordinate%20their%20actions%20effectively.%20However%2C%20the%20reliability%0Aof%20underwater%20acoustic%20communication%20decreases%20as%20the%20communication%20range%0Abetween%20vehicles%20increases.%20Consequently%2C%20teams%20of%20cooperating%20AUVs%20typically%0Amake%20conservative%20assumptions%20about%20the%20maximum%20range%20at%20which%20they%20can%0Acommunicate%20reliably.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%20approach%0Athat%20involves%20learning%20a%20map%20representing%20the%20probability%20of%20successful%0Acommunication%20based%20on%20the%20locations%20of%20the%20transmitting%20and%20receiving%0Avehicles.%20This%20probabilistic%20communication%20map%20accounts%20for%20factors%20such%20as%20the%0Arange%20between%20vehicles%2C%20environmental%20noise%2C%20and%20multi-path%20effects%20at%20a%20given%0Alocation.%20In%20pursuit%20of%20this%20goal%2C%20we%20investigate%20the%20application%20of%20Gaussian%0Aprocess%20binary%20classification%20to%20generate%20the%20desired%20communication%20map.%20We%0Aspecialize%20existing%20results%20to%20this%20specific%20binary%20classification%20problem%20and%0Aexplore%20methods%20to%20incorporate%20uncertainty%20in%20vehicle%20location%20into%20the%20mapping%0Aprocess.%20Furthermore%2C%20we%20compare%20the%20prediction%20performance%20of%20the%20probability%0Acommunication%20map%20generated%20using%20binary%20classification%20with%20that%20of%20a%0Asignal-to-noise%20ratio%20%28SNR%29%20communication%20map%20generated%20using%20Gaussian%20process%0Aregression.%20Our%20approach%20is%20experimentally%20validated%20using%20communication%20and%0Anavigation%20data%20collected%20during%20trials%20with%20a%20pair%20of%20Virginia%20Tech%20690%20AUVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07933v1&entry.124074799=Read"},
{"title": "Commissioning An All-Sky Infrared Camera Array for Detection Of Airborne\n  Objects", "author": "Laura Domin\u00e9 and Ankit Biswas and Richard Cloete and Alex Delacroix and Andriy Fedorenko and Lucas Jacaruso and Ezra Kelderman and Eric Keto and Sarah Little and Abraham Loeb and Eric Masson and Mike Prior and Forrest Schultz and Matthew Szenher and Wes Watters and Abby White", "abstract": "  To date there is little publicly available scientific data on Unidentified\nAerial Phenomena (UAP) whose properties and kinematics purportedly reside\noutside the performance envelope of known phenomena. To address this\ndeficiency, the Galileo Project is designing, building, and commissioning a\nmulti-modal ground-based observatory to continuously monitor the sky and\nconduct a rigorous long-term aerial census of all aerial phenomena, including\nnatural and human-made. One of the key instruments is an all-sky infrared\ncamera array using eight uncooled long-wave infrared FLIR Boson 640 cameras.\nTheir calibration includes a novel extrinsic calibration method using airplane\npositions from Automatic Dependent Surveillance-Broadcast (ADS-B) data. We\nestablish a first baseline for the system performance over five months of field\noperation, using a real-world dataset derived from ADS-B data, synthetic 3-D\ntrajectories, and a hand-labelled real-world dataset. We report acceptance\nrates (e.g. viewable airplanes that are recorded) and detection efficiencies\n(e.g. recorded airplanes which are successfully detected) for a variety of\nweather conditions, range and aircraft size. We reconstruct $\\sim$500,000\ntrajectories of aerial objects from this commissioning period. A toy outlier\nsearch focused on large sinuosity of the 2-D reconstructed trajectories flags\nabout 16% of trajectories as outliers. After manual review, 144 trajectories\nremain ambiguous: they are likely mundane objects but cannot be elucidated at\nthis stage of development without distance and kinematics estimation or other\nsensor modalities. Our observed count of ambiguous outliers combined with\nsystematic uncertainties yields an upper limit of 18,271 outliers count for the\nfive-month interval at a 95% confidence level. This likelihood-based method to\nevaluate significance is applicable to all of our future outlier searches.\n", "link": "http://arxiv.org/abs/2411.07956v1", "date": "2024-11-12", "relevancy": 1.9624, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4557}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commissioning%20An%20All-Sky%20Infrared%20Camera%20Array%20for%20Detection%20Of%20Airborne%0A%20%20Objects&body=Title%3A%20Commissioning%20An%20All-Sky%20Infrared%20Camera%20Array%20for%20Detection%20Of%20Airborne%0A%20%20Objects%0AAuthor%3A%20Laura%20Domin%C3%A9%20and%20Ankit%20Biswas%20and%20Richard%20Cloete%20and%20Alex%20Delacroix%20and%20Andriy%20Fedorenko%20and%20Lucas%20Jacaruso%20and%20Ezra%20Kelderman%20and%20Eric%20Keto%20and%20Sarah%20Little%20and%20Abraham%20Loeb%20and%20Eric%20Masson%20and%20Mike%20Prior%20and%20Forrest%20Schultz%20and%20Matthew%20Szenher%20and%20Wes%20Watters%20and%20Abby%20White%0AAbstract%3A%20%20%20To%20date%20there%20is%20little%20publicly%20available%20scientific%20data%20on%20Unidentified%0AAerial%20Phenomena%20%28UAP%29%20whose%20properties%20and%20kinematics%20purportedly%20reside%0Aoutside%20the%20performance%20envelope%20of%20known%20phenomena.%20To%20address%20this%0Adeficiency%2C%20the%20Galileo%20Project%20is%20designing%2C%20building%2C%20and%20commissioning%20a%0Amulti-modal%20ground-based%20observatory%20to%20continuously%20monitor%20the%20sky%20and%0Aconduct%20a%20rigorous%20long-term%20aerial%20census%20of%20all%20aerial%20phenomena%2C%20including%0Anatural%20and%20human-made.%20One%20of%20the%20key%20instruments%20is%20an%20all-sky%20infrared%0Acamera%20array%20using%20eight%20uncooled%20long-wave%20infrared%20FLIR%20Boson%20640%20cameras.%0ATheir%20calibration%20includes%20a%20novel%20extrinsic%20calibration%20method%20using%20airplane%0Apositions%20from%20Automatic%20Dependent%20Surveillance-Broadcast%20%28ADS-B%29%20data.%20We%0Aestablish%20a%20first%20baseline%20for%20the%20system%20performance%20over%20five%20months%20of%20field%0Aoperation%2C%20using%20a%20real-world%20dataset%20derived%20from%20ADS-B%20data%2C%20synthetic%203-D%0Atrajectories%2C%20and%20a%20hand-labelled%20real-world%20dataset.%20We%20report%20acceptance%0Arates%20%28e.g.%20viewable%20airplanes%20that%20are%20recorded%29%20and%20detection%20efficiencies%0A%28e.g.%20recorded%20airplanes%20which%20are%20successfully%20detected%29%20for%20a%20variety%20of%0Aweather%20conditions%2C%20range%20and%20aircraft%20size.%20We%20reconstruct%20%24%5Csim%24500%2C000%0Atrajectories%20of%20aerial%20objects%20from%20this%20commissioning%20period.%20A%20toy%20outlier%0Asearch%20focused%20on%20large%20sinuosity%20of%20the%202-D%20reconstructed%20trajectories%20flags%0Aabout%2016%25%20of%20trajectories%20as%20outliers.%20After%20manual%20review%2C%20144%20trajectories%0Aremain%20ambiguous%3A%20they%20are%20likely%20mundane%20objects%20but%20cannot%20be%20elucidated%20at%0Athis%20stage%20of%20development%20without%20distance%20and%20kinematics%20estimation%20or%20other%0Asensor%20modalities.%20Our%20observed%20count%20of%20ambiguous%20outliers%20combined%20with%0Asystematic%20uncertainties%20yields%20an%20upper%20limit%20of%2018%2C271%20outliers%20count%20for%20the%0Afive-month%20interval%20at%20a%2095%25%20confidence%20level.%20This%20likelihood-based%20method%20to%0Aevaluate%20significance%20is%20applicable%20to%20all%20of%20our%20future%20outlier%20searches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommissioning%2520An%2520All-Sky%2520Infrared%2520Camera%2520Array%2520for%2520Detection%2520Of%2520Airborne%250A%2520%2520Objects%26entry.906535625%3DLaura%2520Domin%25C3%25A9%2520and%2520Ankit%2520Biswas%2520and%2520Richard%2520Cloete%2520and%2520Alex%2520Delacroix%2520and%2520Andriy%2520Fedorenko%2520and%2520Lucas%2520Jacaruso%2520and%2520Ezra%2520Kelderman%2520and%2520Eric%2520Keto%2520and%2520Sarah%2520Little%2520and%2520Abraham%2520Loeb%2520and%2520Eric%2520Masson%2520and%2520Mike%2520Prior%2520and%2520Forrest%2520Schultz%2520and%2520Matthew%2520Szenher%2520and%2520Wes%2520Watters%2520and%2520Abby%2520White%26entry.1292438233%3D%2520%2520To%2520date%2520there%2520is%2520little%2520publicly%2520available%2520scientific%2520data%2520on%2520Unidentified%250AAerial%2520Phenomena%2520%2528UAP%2529%2520whose%2520properties%2520and%2520kinematics%2520purportedly%2520reside%250Aoutside%2520the%2520performance%2520envelope%2520of%2520known%2520phenomena.%2520To%2520address%2520this%250Adeficiency%252C%2520the%2520Galileo%2520Project%2520is%2520designing%252C%2520building%252C%2520and%2520commissioning%2520a%250Amulti-modal%2520ground-based%2520observatory%2520to%2520continuously%2520monitor%2520the%2520sky%2520and%250Aconduct%2520a%2520rigorous%2520long-term%2520aerial%2520census%2520of%2520all%2520aerial%2520phenomena%252C%2520including%250Anatural%2520and%2520human-made.%2520One%2520of%2520the%2520key%2520instruments%2520is%2520an%2520all-sky%2520infrared%250Acamera%2520array%2520using%2520eight%2520uncooled%2520long-wave%2520infrared%2520FLIR%2520Boson%2520640%2520cameras.%250ATheir%2520calibration%2520includes%2520a%2520novel%2520extrinsic%2520calibration%2520method%2520using%2520airplane%250Apositions%2520from%2520Automatic%2520Dependent%2520Surveillance-Broadcast%2520%2528ADS-B%2529%2520data.%2520We%250Aestablish%2520a%2520first%2520baseline%2520for%2520the%2520system%2520performance%2520over%2520five%2520months%2520of%2520field%250Aoperation%252C%2520using%2520a%2520real-world%2520dataset%2520derived%2520from%2520ADS-B%2520data%252C%2520synthetic%25203-D%250Atrajectories%252C%2520and%2520a%2520hand-labelled%2520real-world%2520dataset.%2520We%2520report%2520acceptance%250Arates%2520%2528e.g.%2520viewable%2520airplanes%2520that%2520are%2520recorded%2529%2520and%2520detection%2520efficiencies%250A%2528e.g.%2520recorded%2520airplanes%2520which%2520are%2520successfully%2520detected%2529%2520for%2520a%2520variety%2520of%250Aweather%2520conditions%252C%2520range%2520and%2520aircraft%2520size.%2520We%2520reconstruct%2520%2524%255Csim%2524500%252C000%250Atrajectories%2520of%2520aerial%2520objects%2520from%2520this%2520commissioning%2520period.%2520A%2520toy%2520outlier%250Asearch%2520focused%2520on%2520large%2520sinuosity%2520of%2520the%25202-D%2520reconstructed%2520trajectories%2520flags%250Aabout%252016%2525%2520of%2520trajectories%2520as%2520outliers.%2520After%2520manual%2520review%252C%2520144%2520trajectories%250Aremain%2520ambiguous%253A%2520they%2520are%2520likely%2520mundane%2520objects%2520but%2520cannot%2520be%2520elucidated%2520at%250Athis%2520stage%2520of%2520development%2520without%2520distance%2520and%2520kinematics%2520estimation%2520or%2520other%250Asensor%2520modalities.%2520Our%2520observed%2520count%2520of%2520ambiguous%2520outliers%2520combined%2520with%250Asystematic%2520uncertainties%2520yields%2520an%2520upper%2520limit%2520of%252018%252C271%2520outliers%2520count%2520for%2520the%250Afive-month%2520interval%2520at%2520a%252095%2525%2520confidence%2520level.%2520This%2520likelihood-based%2520method%2520to%250Aevaluate%2520significance%2520is%2520applicable%2520to%2520all%2520of%2520our%2520future%2520outlier%2520searches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commissioning%20An%20All-Sky%20Infrared%20Camera%20Array%20for%20Detection%20Of%20Airborne%0A%20%20Objects&entry.906535625=Laura%20Domin%C3%A9%20and%20Ankit%20Biswas%20and%20Richard%20Cloete%20and%20Alex%20Delacroix%20and%20Andriy%20Fedorenko%20and%20Lucas%20Jacaruso%20and%20Ezra%20Kelderman%20and%20Eric%20Keto%20and%20Sarah%20Little%20and%20Abraham%20Loeb%20and%20Eric%20Masson%20and%20Mike%20Prior%20and%20Forrest%20Schultz%20and%20Matthew%20Szenher%20and%20Wes%20Watters%20and%20Abby%20White&entry.1292438233=%20%20To%20date%20there%20is%20little%20publicly%20available%20scientific%20data%20on%20Unidentified%0AAerial%20Phenomena%20%28UAP%29%20whose%20properties%20and%20kinematics%20purportedly%20reside%0Aoutside%20the%20performance%20envelope%20of%20known%20phenomena.%20To%20address%20this%0Adeficiency%2C%20the%20Galileo%20Project%20is%20designing%2C%20building%2C%20and%20commissioning%20a%0Amulti-modal%20ground-based%20observatory%20to%20continuously%20monitor%20the%20sky%20and%0Aconduct%20a%20rigorous%20long-term%20aerial%20census%20of%20all%20aerial%20phenomena%2C%20including%0Anatural%20and%20human-made.%20One%20of%20the%20key%20instruments%20is%20an%20all-sky%20infrared%0Acamera%20array%20using%20eight%20uncooled%20long-wave%20infrared%20FLIR%20Boson%20640%20cameras.%0ATheir%20calibration%20includes%20a%20novel%20extrinsic%20calibration%20method%20using%20airplane%0Apositions%20from%20Automatic%20Dependent%20Surveillance-Broadcast%20%28ADS-B%29%20data.%20We%0Aestablish%20a%20first%20baseline%20for%20the%20system%20performance%20over%20five%20months%20of%20field%0Aoperation%2C%20using%20a%20real-world%20dataset%20derived%20from%20ADS-B%20data%2C%20synthetic%203-D%0Atrajectories%2C%20and%20a%20hand-labelled%20real-world%20dataset.%20We%20report%20acceptance%0Arates%20%28e.g.%20viewable%20airplanes%20that%20are%20recorded%29%20and%20detection%20efficiencies%0A%28e.g.%20recorded%20airplanes%20which%20are%20successfully%20detected%29%20for%20a%20variety%20of%0Aweather%20conditions%2C%20range%20and%20aircraft%20size.%20We%20reconstruct%20%24%5Csim%24500%2C000%0Atrajectories%20of%20aerial%20objects%20from%20this%20commissioning%20period.%20A%20toy%20outlier%0Asearch%20focused%20on%20large%20sinuosity%20of%20the%202-D%20reconstructed%20trajectories%20flags%0Aabout%2016%25%20of%20trajectories%20as%20outliers.%20After%20manual%20review%2C%20144%20trajectories%0Aremain%20ambiguous%3A%20they%20are%20likely%20mundane%20objects%20but%20cannot%20be%20elucidated%20at%0Athis%20stage%20of%20development%20without%20distance%20and%20kinematics%20estimation%20or%20other%0Asensor%20modalities.%20Our%20observed%20count%20of%20ambiguous%20outliers%20combined%20with%0Asystematic%20uncertainties%20yields%20an%20upper%20limit%20of%2018%2C271%20outliers%20count%20for%20the%0Afive-month%20interval%20at%20a%2095%25%20confidence%20level.%20This%20likelihood-based%20method%20to%0Aevaluate%20significance%20is%20applicable%20to%20all%20of%20our%20future%20outlier%20searches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07956v1&entry.124074799=Read"},
{"title": "Investigating the Effectiveness of Explainability Methods in Parkinson's\n  Detection from Speech", "author": "Eleonora Mancini and Francesco Paissan and Paolo Torroni and Cem Subakan and Mirco Ravanelli", "abstract": "  Speech impairments in Parkinson's disease (PD) provide significant early\nindicators for diagnosis. While models for speech-based PD detection have shown\nstrong performance, their interpretability remains underexplored. This study\nsystematically evaluates several explainability methods to identify PD-specific\nspeech features, aiming to support the development of accurate, interpretable\nmodels for clinical decision-making in PD diagnosis and monitoring. Our\nmethodology involves (i) obtaining attributions and saliency maps using\nmainstream interpretability techniques, (ii) quantitatively evaluating the\nfaithfulness of these maps and their combinations obtained via union and\nintersection through a range of established metrics, and (iii) assessing the\ninformation conveyed by the saliency maps for PD detection from an auxiliary\nclassifier. Our results reveal that, while explanations are aligned with the\nclassifier, they often fail to provide valuable information for domain experts.\n", "link": "http://arxiv.org/abs/2411.08013v1", "date": "2024-11-12", "relevancy": 1.9611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech&body=Title%3A%20Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech%0AAuthor%3A%20Eleonora%20Mancini%20and%20Francesco%20Paissan%20and%20Paolo%20Torroni%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli%0AAbstract%3A%20%20%20Speech%20impairments%20in%20Parkinson%27s%20disease%20%28PD%29%20provide%20significant%20early%0Aindicators%20for%20diagnosis.%20While%20models%20for%20speech-based%20PD%20detection%20have%20shown%0Astrong%20performance%2C%20their%20interpretability%20remains%20underexplored.%20This%20study%0Asystematically%20evaluates%20several%20explainability%20methods%20to%20identify%20PD-specific%0Aspeech%20features%2C%20aiming%20to%20support%20the%20development%20of%20accurate%2C%20interpretable%0Amodels%20for%20clinical%20decision-making%20in%20PD%20diagnosis%20and%20monitoring.%20Our%0Amethodology%20involves%20%28i%29%20obtaining%20attributions%20and%20saliency%20maps%20using%0Amainstream%20interpretability%20techniques%2C%20%28ii%29%20quantitatively%20evaluating%20the%0Afaithfulness%20of%20these%20maps%20and%20their%20combinations%20obtained%20via%20union%20and%0Aintersection%20through%20a%20range%20of%20established%20metrics%2C%20and%20%28iii%29%20assessing%20the%0Ainformation%20conveyed%20by%20the%20saliency%20maps%20for%20PD%20detection%20from%20an%20auxiliary%0Aclassifier.%20Our%20results%20reveal%20that%2C%20while%20explanations%20are%20aligned%20with%20the%0Aclassifier%2C%20they%20often%20fail%20to%20provide%20valuable%20information%20for%20domain%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Effectiveness%2520of%2520Explainability%2520Methods%2520in%2520Parkinson%2527s%250A%2520%2520Detection%2520from%2520Speech%26entry.906535625%3DEleonora%2520Mancini%2520and%2520Francesco%2520Paissan%2520and%2520Paolo%2520Torroni%2520and%2520Cem%2520Subakan%2520and%2520Mirco%2520Ravanelli%26entry.1292438233%3D%2520%2520Speech%2520impairments%2520in%2520Parkinson%2527s%2520disease%2520%2528PD%2529%2520provide%2520significant%2520early%250Aindicators%2520for%2520diagnosis.%2520While%2520models%2520for%2520speech-based%2520PD%2520detection%2520have%2520shown%250Astrong%2520performance%252C%2520their%2520interpretability%2520remains%2520underexplored.%2520This%2520study%250Asystematically%2520evaluates%2520several%2520explainability%2520methods%2520to%2520identify%2520PD-specific%250Aspeech%2520features%252C%2520aiming%2520to%2520support%2520the%2520development%2520of%2520accurate%252C%2520interpretable%250Amodels%2520for%2520clinical%2520decision-making%2520in%2520PD%2520diagnosis%2520and%2520monitoring.%2520Our%250Amethodology%2520involves%2520%2528i%2529%2520obtaining%2520attributions%2520and%2520saliency%2520maps%2520using%250Amainstream%2520interpretability%2520techniques%252C%2520%2528ii%2529%2520quantitatively%2520evaluating%2520the%250Afaithfulness%2520of%2520these%2520maps%2520and%2520their%2520combinations%2520obtained%2520via%2520union%2520and%250Aintersection%2520through%2520a%2520range%2520of%2520established%2520metrics%252C%2520and%2520%2528iii%2529%2520assessing%2520the%250Ainformation%2520conveyed%2520by%2520the%2520saliency%2520maps%2520for%2520PD%2520detection%2520from%2520an%2520auxiliary%250Aclassifier.%2520Our%2520results%2520reveal%2520that%252C%2520while%2520explanations%2520are%2520aligned%2520with%2520the%250Aclassifier%252C%2520they%2520often%2520fail%2520to%2520provide%2520valuable%2520information%2520for%2520domain%2520experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech&entry.906535625=Eleonora%20Mancini%20and%20Francesco%20Paissan%20and%20Paolo%20Torroni%20and%20Cem%20Subakan%20and%20Mirco%20Ravanelli&entry.1292438233=%20%20Speech%20impairments%20in%20Parkinson%27s%20disease%20%28PD%29%20provide%20significant%20early%0Aindicators%20for%20diagnosis.%20While%20models%20for%20speech-based%20PD%20detection%20have%20shown%0Astrong%20performance%2C%20their%20interpretability%20remains%20underexplored.%20This%20study%0Asystematically%20evaluates%20several%20explainability%20methods%20to%20identify%20PD-specific%0Aspeech%20features%2C%20aiming%20to%20support%20the%20development%20of%20accurate%2C%20interpretable%0Amodels%20for%20clinical%20decision-making%20in%20PD%20diagnosis%20and%20monitoring.%20Our%0Amethodology%20involves%20%28i%29%20obtaining%20attributions%20and%20saliency%20maps%20using%0Amainstream%20interpretability%20techniques%2C%20%28ii%29%20quantitatively%20evaluating%20the%0Afaithfulness%20of%20these%20maps%20and%20their%20combinations%20obtained%20via%20union%20and%0Aintersection%20through%20a%20range%20of%20established%20metrics%2C%20and%20%28iii%29%20assessing%20the%0Ainformation%20conveyed%20by%20the%20saliency%20maps%20for%20PD%20detection%20from%20an%20auxiliary%0Aclassifier.%20Our%20results%20reveal%20that%2C%20while%20explanations%20are%20aligned%20with%20the%0Aclassifier%2C%20they%20often%20fail%20to%20provide%20valuable%20information%20for%20domain%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08013v1&entry.124074799=Read"},
{"title": "Basis-to-Basis Operator Learning Using Function Encoders", "author": "Tyler Ingebrand and Adam J. Thorpe and Somdatta Goswami and Krishna Kumar and Ufuk Topcu", "abstract": "  We present Basis-to-Basis (B2B) operator learning, a novel approach for\nlearning operators on Hilbert spaces of functions based on the foundational\nideas of function encoders. We decompose the task of learning operators into\ntwo parts: learning sets of basis functions for both the input and output\nspaces and learning a potentially nonlinear mapping between the coefficients of\nthe basis functions. B2B operator learning circumvents many challenges of prior\nworks, such as requiring data to be at fixed locations, by leveraging classic\ntechniques such as least squares to compute the coefficients. It is especially\npotent for linear operators, where we compute a mapping between bases as a\nsingle matrix transformation with a closed-form solution. Furthermore, with\nminimal modifications and using the deep theoretical connections between\nfunction encoders and functional analysis, we derive operator learning\nalgorithms that are directly analogous to eigen-decomposition and singular\nvalue decomposition. We empirically validate B2B operator learning on seven\nbenchmark operator learning tasks and show that it demonstrates a\ntwo-orders-of-magnitude improvement in accuracy over existing approaches on\nseveral benchmark tasks.\n", "link": "http://arxiv.org/abs/2410.00171v2", "date": "2024-11-12", "relevancy": 1.9608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Basis-to-Basis%20Operator%20Learning%20Using%20Function%20Encoders&body=Title%3A%20Basis-to-Basis%20Operator%20Learning%20Using%20Function%20Encoders%0AAuthor%3A%20Tyler%20Ingebrand%20and%20Adam%20J.%20Thorpe%20and%20Somdatta%20Goswami%20and%20Krishna%20Kumar%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20We%20present%20Basis-to-Basis%20%28B2B%29%20operator%20learning%2C%20a%20novel%20approach%20for%0Alearning%20operators%20on%20Hilbert%20spaces%20of%20functions%20based%20on%20the%20foundational%0Aideas%20of%20function%20encoders.%20We%20decompose%20the%20task%20of%20learning%20operators%20into%0Atwo%20parts%3A%20learning%20sets%20of%20basis%20functions%20for%20both%20the%20input%20and%20output%0Aspaces%20and%20learning%20a%20potentially%20nonlinear%20mapping%20between%20the%20coefficients%20of%0Athe%20basis%20functions.%20B2B%20operator%20learning%20circumvents%20many%20challenges%20of%20prior%0Aworks%2C%20such%20as%20requiring%20data%20to%20be%20at%20fixed%20locations%2C%20by%20leveraging%20classic%0Atechniques%20such%20as%20least%20squares%20to%20compute%20the%20coefficients.%20It%20is%20especially%0Apotent%20for%20linear%20operators%2C%20where%20we%20compute%20a%20mapping%20between%20bases%20as%20a%0Asingle%20matrix%20transformation%20with%20a%20closed-form%20solution.%20Furthermore%2C%20with%0Aminimal%20modifications%20and%20using%20the%20deep%20theoretical%20connections%20between%0Afunction%20encoders%20and%20functional%20analysis%2C%20we%20derive%20operator%20learning%0Aalgorithms%20that%20are%20directly%20analogous%20to%20eigen-decomposition%20and%20singular%0Avalue%20decomposition.%20We%20empirically%20validate%20B2B%20operator%20learning%20on%20seven%0Abenchmark%20operator%20learning%20tasks%20and%20show%20that%20it%20demonstrates%20a%0Atwo-orders-of-magnitude%20improvement%20in%20accuracy%20over%20existing%20approaches%20on%0Aseveral%20benchmark%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBasis-to-Basis%2520Operator%2520Learning%2520Using%2520Function%2520Encoders%26entry.906535625%3DTyler%2520Ingebrand%2520and%2520Adam%2520J.%2520Thorpe%2520and%2520Somdatta%2520Goswami%2520and%2520Krishna%2520Kumar%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520We%2520present%2520Basis-to-Basis%2520%2528B2B%2529%2520operator%2520learning%252C%2520a%2520novel%2520approach%2520for%250Alearning%2520operators%2520on%2520Hilbert%2520spaces%2520of%2520functions%2520based%2520on%2520the%2520foundational%250Aideas%2520of%2520function%2520encoders.%2520We%2520decompose%2520the%2520task%2520of%2520learning%2520operators%2520into%250Atwo%2520parts%253A%2520learning%2520sets%2520of%2520basis%2520functions%2520for%2520both%2520the%2520input%2520and%2520output%250Aspaces%2520and%2520learning%2520a%2520potentially%2520nonlinear%2520mapping%2520between%2520the%2520coefficients%2520of%250Athe%2520basis%2520functions.%2520B2B%2520operator%2520learning%2520circumvents%2520many%2520challenges%2520of%2520prior%250Aworks%252C%2520such%2520as%2520requiring%2520data%2520to%2520be%2520at%2520fixed%2520locations%252C%2520by%2520leveraging%2520classic%250Atechniques%2520such%2520as%2520least%2520squares%2520to%2520compute%2520the%2520coefficients.%2520It%2520is%2520especially%250Apotent%2520for%2520linear%2520operators%252C%2520where%2520we%2520compute%2520a%2520mapping%2520between%2520bases%2520as%2520a%250Asingle%2520matrix%2520transformation%2520with%2520a%2520closed-form%2520solution.%2520Furthermore%252C%2520with%250Aminimal%2520modifications%2520and%2520using%2520the%2520deep%2520theoretical%2520connections%2520between%250Afunction%2520encoders%2520and%2520functional%2520analysis%252C%2520we%2520derive%2520operator%2520learning%250Aalgorithms%2520that%2520are%2520directly%2520analogous%2520to%2520eigen-decomposition%2520and%2520singular%250Avalue%2520decomposition.%2520We%2520empirically%2520validate%2520B2B%2520operator%2520learning%2520on%2520seven%250Abenchmark%2520operator%2520learning%2520tasks%2520and%2520show%2520that%2520it%2520demonstrates%2520a%250Atwo-orders-of-magnitude%2520improvement%2520in%2520accuracy%2520over%2520existing%2520approaches%2520on%250Aseveral%2520benchmark%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Basis-to-Basis%20Operator%20Learning%20Using%20Function%20Encoders&entry.906535625=Tyler%20Ingebrand%20and%20Adam%20J.%20Thorpe%20and%20Somdatta%20Goswami%20and%20Krishna%20Kumar%20and%20Ufuk%20Topcu&entry.1292438233=%20%20We%20present%20Basis-to-Basis%20%28B2B%29%20operator%20learning%2C%20a%20novel%20approach%20for%0Alearning%20operators%20on%20Hilbert%20spaces%20of%20functions%20based%20on%20the%20foundational%0Aideas%20of%20function%20encoders.%20We%20decompose%20the%20task%20of%20learning%20operators%20into%0Atwo%20parts%3A%20learning%20sets%20of%20basis%20functions%20for%20both%20the%20input%20and%20output%0Aspaces%20and%20learning%20a%20potentially%20nonlinear%20mapping%20between%20the%20coefficients%20of%0Athe%20basis%20functions.%20B2B%20operator%20learning%20circumvents%20many%20challenges%20of%20prior%0Aworks%2C%20such%20as%20requiring%20data%20to%20be%20at%20fixed%20locations%2C%20by%20leveraging%20classic%0Atechniques%20such%20as%20least%20squares%20to%20compute%20the%20coefficients.%20It%20is%20especially%0Apotent%20for%20linear%20operators%2C%20where%20we%20compute%20a%20mapping%20between%20bases%20as%20a%0Asingle%20matrix%20transformation%20with%20a%20closed-form%20solution.%20Furthermore%2C%20with%0Aminimal%20modifications%20and%20using%20the%20deep%20theoretical%20connections%20between%0Afunction%20encoders%20and%20functional%20analysis%2C%20we%20derive%20operator%20learning%0Aalgorithms%20that%20are%20directly%20analogous%20to%20eigen-decomposition%20and%20singular%0Avalue%20decomposition.%20We%20empirically%20validate%20B2B%20operator%20learning%20on%20seven%0Abenchmark%20operator%20learning%20tasks%20and%20show%20that%20it%20demonstrates%20a%0Atwo-orders-of-magnitude%20improvement%20in%20accuracy%20over%20existing%20approaches%20on%0Aseveral%20benchmark%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00171v2&entry.124074799=Read"},
{"title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise\n  Text-to-SQL Workflows", "author": "Fangyu Lei and Jixuan Chen and Yuxiao Ye and Ruisheng Cao and Dongchan Shin and Hongjin Su and Zhaoqing Suo and Hongcheng Gao and Wenjing Hu and Pengcheng Yin and Victor Zhong and Caiming Xiong and Ruoxi Sun and Qian Liu and Sida Wang and Tao Yu", "abstract": "  Real-world enterprise text-to-SQL workflows often involve complex cloud or\nlocal data across various database systems, multiple SQL queries in various\ndialects, and diverse operations from data transformation to analytics. We\nintroduce Spider 2.0, an evaluation framework comprising 632 real-world\ntext-to-SQL workflow problems derived from enterprise-level database use cases.\nThe databases in Spider 2.0 are sourced from real data applications, often\ncontaining over 1,000 columns and stored in local or cloud database systems\nsuch as BigQuery and Snowflake. We show that solving problems in Spider 2.0\nfrequently requires understanding and searching through database metadata,\ndialect documentation, and even project-level codebases. This challenge calls\nfor models to interact with complex SQL workflow environments, process\nextremely long contexts, perform intricate reasoning, and generate multiple SQL\nqueries with diverse operations, often exceeding 100 lines, which goes far\nbeyond traditional text-to-SQL challenges. Our evaluations indicate that based\non o1-preview, our code agent framework successfully solves only 17.0% of the\ntasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on\nSpider 2.0 show that while language models have demonstrated remarkable\nperformance in code generation -- especially in prior text-to-SQL benchmarks --\nthey require significant improvement in order to achieve adequate performance\nfor real-world enterprise usage. Progress on Spider 2.0 represents crucial\nsteps towards developing intelligent, autonomous, code agents for real-world\nenterprise settings. Our code, baseline models, and data are available at\nhttps://spider2-sql.github.io.\n", "link": "http://arxiv.org/abs/2411.07763v1", "date": "2024-11-12", "relevancy": 1.9578, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spider%202.0%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Enterprise%0A%20%20Text-to-SQL%20Workflows&body=Title%3A%20Spider%202.0%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Enterprise%0A%20%20Text-to-SQL%20Workflows%0AAuthor%3A%20Fangyu%20Lei%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Ruisheng%20Cao%20and%20Dongchan%20Shin%20and%20Hongjin%20Su%20and%20Zhaoqing%20Suo%20and%20Hongcheng%20Gao%20and%20Wenjing%20Hu%20and%20Pengcheng%20Yin%20and%20Victor%20Zhong%20and%20Caiming%20Xiong%20and%20Ruoxi%20Sun%20and%20Qian%20Liu%20and%20Sida%20Wang%20and%20Tao%20Yu%0AAbstract%3A%20%20%20Real-world%20enterprise%20text-to-SQL%20workflows%20often%20involve%20complex%20cloud%20or%0Alocal%20data%20across%20various%20database%20systems%2C%20multiple%20SQL%20queries%20in%20various%0Adialects%2C%20and%20diverse%20operations%20from%20data%20transformation%20to%20analytics.%20We%0Aintroduce%20Spider%202.0%2C%20an%20evaluation%20framework%20comprising%20632%20real-world%0Atext-to-SQL%20workflow%20problems%20derived%20from%20enterprise-level%20database%20use%20cases.%0AThe%20databases%20in%20Spider%202.0%20are%20sourced%20from%20real%20data%20applications%2C%20often%0Acontaining%20over%201%2C000%20columns%20and%20stored%20in%20local%20or%20cloud%20database%20systems%0Asuch%20as%20BigQuery%20and%20Snowflake.%20We%20show%20that%20solving%20problems%20in%20Spider%202.0%0Afrequently%20requires%20understanding%20and%20searching%20through%20database%20metadata%2C%0Adialect%20documentation%2C%20and%20even%20project-level%20codebases.%20This%20challenge%20calls%0Afor%20models%20to%20interact%20with%20complex%20SQL%20workflow%20environments%2C%20process%0Aextremely%20long%20contexts%2C%20perform%20intricate%20reasoning%2C%20and%20generate%20multiple%20SQL%0Aqueries%20with%20diverse%20operations%2C%20often%20exceeding%20100%20lines%2C%20which%20goes%20far%0Abeyond%20traditional%20text-to-SQL%20challenges.%20Our%20evaluations%20indicate%20that%20based%0Aon%20o1-preview%2C%20our%20code%20agent%20framework%20successfully%20solves%20only%2017.0%25%20of%20the%0Atasks%2C%20compared%20with%2091.2%25%20on%20Spider%201.0%20and%2073.0%25%20on%20BIRD.%20Our%20results%20on%0ASpider%202.0%20show%20that%20while%20language%20models%20have%20demonstrated%20remarkable%0Aperformance%20in%20code%20generation%20--%20especially%20in%20prior%20text-to-SQL%20benchmarks%20--%0Athey%20require%20significant%20improvement%20in%20order%20to%20achieve%20adequate%20performance%0Afor%20real-world%20enterprise%20usage.%20Progress%20on%20Spider%202.0%20represents%20crucial%0Asteps%20towards%20developing%20intelligent%2C%20autonomous%2C%20code%20agents%20for%20real-world%0Aenterprise%20settings.%20Our%20code%2C%20baseline%20models%2C%20and%20data%20are%20available%20at%0Ahttps%3A//spider2-sql.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpider%25202.0%253A%2520Evaluating%2520Language%2520Models%2520on%2520Real-World%2520Enterprise%250A%2520%2520Text-to-SQL%2520Workflows%26entry.906535625%3DFangyu%2520Lei%2520and%2520Jixuan%2520Chen%2520and%2520Yuxiao%2520Ye%2520and%2520Ruisheng%2520Cao%2520and%2520Dongchan%2520Shin%2520and%2520Hongjin%2520Su%2520and%2520Zhaoqing%2520Suo%2520and%2520Hongcheng%2520Gao%2520and%2520Wenjing%2520Hu%2520and%2520Pengcheng%2520Yin%2520and%2520Victor%2520Zhong%2520and%2520Caiming%2520Xiong%2520and%2520Ruoxi%2520Sun%2520and%2520Qian%2520Liu%2520and%2520Sida%2520Wang%2520and%2520Tao%2520Yu%26entry.1292438233%3D%2520%2520Real-world%2520enterprise%2520text-to-SQL%2520workflows%2520often%2520involve%2520complex%2520cloud%2520or%250Alocal%2520data%2520across%2520various%2520database%2520systems%252C%2520multiple%2520SQL%2520queries%2520in%2520various%250Adialects%252C%2520and%2520diverse%2520operations%2520from%2520data%2520transformation%2520to%2520analytics.%2520We%250Aintroduce%2520Spider%25202.0%252C%2520an%2520evaluation%2520framework%2520comprising%2520632%2520real-world%250Atext-to-SQL%2520workflow%2520problems%2520derived%2520from%2520enterprise-level%2520database%2520use%2520cases.%250AThe%2520databases%2520in%2520Spider%25202.0%2520are%2520sourced%2520from%2520real%2520data%2520applications%252C%2520often%250Acontaining%2520over%25201%252C000%2520columns%2520and%2520stored%2520in%2520local%2520or%2520cloud%2520database%2520systems%250Asuch%2520as%2520BigQuery%2520and%2520Snowflake.%2520We%2520show%2520that%2520solving%2520problems%2520in%2520Spider%25202.0%250Afrequently%2520requires%2520understanding%2520and%2520searching%2520through%2520database%2520metadata%252C%250Adialect%2520documentation%252C%2520and%2520even%2520project-level%2520codebases.%2520This%2520challenge%2520calls%250Afor%2520models%2520to%2520interact%2520with%2520complex%2520SQL%2520workflow%2520environments%252C%2520process%250Aextremely%2520long%2520contexts%252C%2520perform%2520intricate%2520reasoning%252C%2520and%2520generate%2520multiple%2520SQL%250Aqueries%2520with%2520diverse%2520operations%252C%2520often%2520exceeding%2520100%2520lines%252C%2520which%2520goes%2520far%250Abeyond%2520traditional%2520text-to-SQL%2520challenges.%2520Our%2520evaluations%2520indicate%2520that%2520based%250Aon%2520o1-preview%252C%2520our%2520code%2520agent%2520framework%2520successfully%2520solves%2520only%252017.0%2525%2520of%2520the%250Atasks%252C%2520compared%2520with%252091.2%2525%2520on%2520Spider%25201.0%2520and%252073.0%2525%2520on%2520BIRD.%2520Our%2520results%2520on%250ASpider%25202.0%2520show%2520that%2520while%2520language%2520models%2520have%2520demonstrated%2520remarkable%250Aperformance%2520in%2520code%2520generation%2520--%2520especially%2520in%2520prior%2520text-to-SQL%2520benchmarks%2520--%250Athey%2520require%2520significant%2520improvement%2520in%2520order%2520to%2520achieve%2520adequate%2520performance%250Afor%2520real-world%2520enterprise%2520usage.%2520Progress%2520on%2520Spider%25202.0%2520represents%2520crucial%250Asteps%2520towards%2520developing%2520intelligent%252C%2520autonomous%252C%2520code%2520agents%2520for%2520real-world%250Aenterprise%2520settings.%2520Our%2520code%252C%2520baseline%2520models%252C%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//spider2-sql.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spider%202.0%3A%20Evaluating%20Language%20Models%20on%20Real-World%20Enterprise%0A%20%20Text-to-SQL%20Workflows&entry.906535625=Fangyu%20Lei%20and%20Jixuan%20Chen%20and%20Yuxiao%20Ye%20and%20Ruisheng%20Cao%20and%20Dongchan%20Shin%20and%20Hongjin%20Su%20and%20Zhaoqing%20Suo%20and%20Hongcheng%20Gao%20and%20Wenjing%20Hu%20and%20Pengcheng%20Yin%20and%20Victor%20Zhong%20and%20Caiming%20Xiong%20and%20Ruoxi%20Sun%20and%20Qian%20Liu%20and%20Sida%20Wang%20and%20Tao%20Yu&entry.1292438233=%20%20Real-world%20enterprise%20text-to-SQL%20workflows%20often%20involve%20complex%20cloud%20or%0Alocal%20data%20across%20various%20database%20systems%2C%20multiple%20SQL%20queries%20in%20various%0Adialects%2C%20and%20diverse%20operations%20from%20data%20transformation%20to%20analytics.%20We%0Aintroduce%20Spider%202.0%2C%20an%20evaluation%20framework%20comprising%20632%20real-world%0Atext-to-SQL%20workflow%20problems%20derived%20from%20enterprise-level%20database%20use%20cases.%0AThe%20databases%20in%20Spider%202.0%20are%20sourced%20from%20real%20data%20applications%2C%20often%0Acontaining%20over%201%2C000%20columns%20and%20stored%20in%20local%20or%20cloud%20database%20systems%0Asuch%20as%20BigQuery%20and%20Snowflake.%20We%20show%20that%20solving%20problems%20in%20Spider%202.0%0Afrequently%20requires%20understanding%20and%20searching%20through%20database%20metadata%2C%0Adialect%20documentation%2C%20and%20even%20project-level%20codebases.%20This%20challenge%20calls%0Afor%20models%20to%20interact%20with%20complex%20SQL%20workflow%20environments%2C%20process%0Aextremely%20long%20contexts%2C%20perform%20intricate%20reasoning%2C%20and%20generate%20multiple%20SQL%0Aqueries%20with%20diverse%20operations%2C%20often%20exceeding%20100%20lines%2C%20which%20goes%20far%0Abeyond%20traditional%20text-to-SQL%20challenges.%20Our%20evaluations%20indicate%20that%20based%0Aon%20o1-preview%2C%20our%20code%20agent%20framework%20successfully%20solves%20only%2017.0%25%20of%20the%0Atasks%2C%20compared%20with%2091.2%25%20on%20Spider%201.0%20and%2073.0%25%20on%20BIRD.%20Our%20results%20on%0ASpider%202.0%20show%20that%20while%20language%20models%20have%20demonstrated%20remarkable%0Aperformance%20in%20code%20generation%20--%20especially%20in%20prior%20text-to-SQL%20benchmarks%20--%0Athey%20require%20significant%20improvement%20in%20order%20to%20achieve%20adequate%20performance%0Afor%20real-world%20enterprise%20usage.%20Progress%20on%20Spider%202.0%20represents%20crucial%0Asteps%20towards%20developing%20intelligent%2C%20autonomous%2C%20code%20agents%20for%20real-world%0Aenterprise%20settings.%20Our%20code%2C%20baseline%20models%2C%20and%20data%20are%20available%20at%0Ahttps%3A//spider2-sql.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07763v1&entry.124074799=Read"},
{"title": "Learning with Less: Knowledge Distillation from Large Language Models\n  via Unlabeled Data", "author": "Juanhui Li and Sreyashi Nag and Hui Liu and Xianfeng Tang and Sheikh Sarwar and Limeng Cui and Hansu Gu and Suhang Wang and Qi He and Jiliang Tang", "abstract": "  In real-world NLP applications, Large Language Models (LLMs) offer promising\nsolutions due to their extensive training on vast datasets. However, the large\nsize and high computation demands of LLMs limit their practicality in many\napplications, especially when further fine-tuning is required. To address these\nlimitations, smaller models are typically preferred for deployment. However,\ntheir training is hindered by the scarcity of labeled data. In contrast,\nunlabeled data is often readily which can be leveraged by using LLMs to\ngenerate pseudo-labels for training smaller models. This enables the smaller\nmodels (student) to acquire knowledge from LLMs(teacher) while reducing\ncomputational costs. This process introduces challenges, such as potential\nnoisy pseudo-labels. Selecting high-quality and informative data is therefore\ncritical to enhance model performance while improving the efficiency of data\nutilization. To address this, we propose LLKD that enables Learning with Less\ncomputational resources and less data for Knowledge Distillation from LLMs.\nLLKD is an adaptive sample selection method that incorporates signals from both\nthe teacher and student. Specifically, it prioritizes samples where the teacher\ndemonstrates high confidence in its labeling, indicating reliable labels, and\nwhere the student exhibits a high information need, identifying challenging\nsamples that require further learning. Our comprehensive experiments show that\nLLKD achieves superior performance across various datasets with higher data\nefficiency.\n", "link": "http://arxiv.org/abs/2411.08028v1", "date": "2024-11-12", "relevancy": 1.9445, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5279}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4869}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Less%3A%20Knowledge%20Distillation%20from%20Large%20Language%20Models%0A%20%20via%20Unlabeled%20Data&body=Title%3A%20Learning%20with%20Less%3A%20Knowledge%20Distillation%20from%20Large%20Language%20Models%0A%20%20via%20Unlabeled%20Data%0AAuthor%3A%20Juanhui%20Li%20and%20Sreyashi%20Nag%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Sheikh%20Sarwar%20and%20Limeng%20Cui%20and%20Hansu%20Gu%20and%20Suhang%20Wang%20and%20Qi%20He%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20In%20real-world%20NLP%20applications%2C%20Large%20Language%20Models%20%28LLMs%29%20offer%20promising%0Asolutions%20due%20to%20their%20extensive%20training%20on%20vast%20datasets.%20However%2C%20the%20large%0Asize%20and%20high%20computation%20demands%20of%20LLMs%20limit%20their%20practicality%20in%20many%0Aapplications%2C%20especially%20when%20further%20fine-tuning%20is%20required.%20To%20address%20these%0Alimitations%2C%20smaller%20models%20are%20typically%20preferred%20for%20deployment.%20However%2C%0Atheir%20training%20is%20hindered%20by%20the%20scarcity%20of%20labeled%20data.%20In%20contrast%2C%0Aunlabeled%20data%20is%20often%20readily%20which%20can%20be%20leveraged%20by%20using%20LLMs%20to%0Agenerate%20pseudo-labels%20for%20training%20smaller%20models.%20This%20enables%20the%20smaller%0Amodels%20%28student%29%20to%20acquire%20knowledge%20from%20LLMs%28teacher%29%20while%20reducing%0Acomputational%20costs.%20This%20process%20introduces%20challenges%2C%20such%20as%20potential%0Anoisy%20pseudo-labels.%20Selecting%20high-quality%20and%20informative%20data%20is%20therefore%0Acritical%20to%20enhance%20model%20performance%20while%20improving%20the%20efficiency%20of%20data%0Autilization.%20To%20address%20this%2C%20we%20propose%20LLKD%20that%20enables%20Learning%20with%20Less%0Acomputational%20resources%20and%20less%20data%20for%20Knowledge%20Distillation%20from%20LLMs.%0ALLKD%20is%20an%20adaptive%20sample%20selection%20method%20that%20incorporates%20signals%20from%20both%0Athe%20teacher%20and%20student.%20Specifically%2C%20it%20prioritizes%20samples%20where%20the%20teacher%0Ademonstrates%20high%20confidence%20in%20its%20labeling%2C%20indicating%20reliable%20labels%2C%20and%0Awhere%20the%20student%20exhibits%20a%20high%20information%20need%2C%20identifying%20challenging%0Asamples%20that%20require%20further%20learning.%20Our%20comprehensive%20experiments%20show%20that%0ALLKD%20achieves%20superior%20performance%20across%20various%20datasets%20with%20higher%20data%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Less%253A%2520Knowledge%2520Distillation%2520from%2520Large%2520Language%2520Models%250A%2520%2520via%2520Unlabeled%2520Data%26entry.906535625%3DJuanhui%2520Li%2520and%2520Sreyashi%2520Nag%2520and%2520Hui%2520Liu%2520and%2520Xianfeng%2520Tang%2520and%2520Sheikh%2520Sarwar%2520and%2520Limeng%2520Cui%2520and%2520Hansu%2520Gu%2520and%2520Suhang%2520Wang%2520and%2520Qi%2520He%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520In%2520real-world%2520NLP%2520applications%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520promising%250Asolutions%2520due%2520to%2520their%2520extensive%2520training%2520on%2520vast%2520datasets.%2520However%252C%2520the%2520large%250Asize%2520and%2520high%2520computation%2520demands%2520of%2520LLMs%2520limit%2520their%2520practicality%2520in%2520many%250Aapplications%252C%2520especially%2520when%2520further%2520fine-tuning%2520is%2520required.%2520To%2520address%2520these%250Alimitations%252C%2520smaller%2520models%2520are%2520typically%2520preferred%2520for%2520deployment.%2520However%252C%250Atheir%2520training%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520labeled%2520data.%2520In%2520contrast%252C%250Aunlabeled%2520data%2520is%2520often%2520readily%2520which%2520can%2520be%2520leveraged%2520by%2520using%2520LLMs%2520to%250Agenerate%2520pseudo-labels%2520for%2520training%2520smaller%2520models.%2520This%2520enables%2520the%2520smaller%250Amodels%2520%2528student%2529%2520to%2520acquire%2520knowledge%2520from%2520LLMs%2528teacher%2529%2520while%2520reducing%250Acomputational%2520costs.%2520This%2520process%2520introduces%2520challenges%252C%2520such%2520as%2520potential%250Anoisy%2520pseudo-labels.%2520Selecting%2520high-quality%2520and%2520informative%2520data%2520is%2520therefore%250Acritical%2520to%2520enhance%2520model%2520performance%2520while%2520improving%2520the%2520efficiency%2520of%2520data%250Autilization.%2520To%2520address%2520this%252C%2520we%2520propose%2520LLKD%2520that%2520enables%2520Learning%2520with%2520Less%250Acomputational%2520resources%2520and%2520less%2520data%2520for%2520Knowledge%2520Distillation%2520from%2520LLMs.%250ALLKD%2520is%2520an%2520adaptive%2520sample%2520selection%2520method%2520that%2520incorporates%2520signals%2520from%2520both%250Athe%2520teacher%2520and%2520student.%2520Specifically%252C%2520it%2520prioritizes%2520samples%2520where%2520the%2520teacher%250Ademonstrates%2520high%2520confidence%2520in%2520its%2520labeling%252C%2520indicating%2520reliable%2520labels%252C%2520and%250Awhere%2520the%2520student%2520exhibits%2520a%2520high%2520information%2520need%252C%2520identifying%2520challenging%250Asamples%2520that%2520require%2520further%2520learning.%2520Our%2520comprehensive%2520experiments%2520show%2520that%250ALLKD%2520achieves%2520superior%2520performance%2520across%2520various%2520datasets%2520with%2520higher%2520data%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Less%3A%20Knowledge%20Distillation%20from%20Large%20Language%20Models%0A%20%20via%20Unlabeled%20Data&entry.906535625=Juanhui%20Li%20and%20Sreyashi%20Nag%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Sheikh%20Sarwar%20and%20Limeng%20Cui%20and%20Hansu%20Gu%20and%20Suhang%20Wang%20and%20Qi%20He%20and%20Jiliang%20Tang&entry.1292438233=%20%20In%20real-world%20NLP%20applications%2C%20Large%20Language%20Models%20%28LLMs%29%20offer%20promising%0Asolutions%20due%20to%20their%20extensive%20training%20on%20vast%20datasets.%20However%2C%20the%20large%0Asize%20and%20high%20computation%20demands%20of%20LLMs%20limit%20their%20practicality%20in%20many%0Aapplications%2C%20especially%20when%20further%20fine-tuning%20is%20required.%20To%20address%20these%0Alimitations%2C%20smaller%20models%20are%20typically%20preferred%20for%20deployment.%20However%2C%0Atheir%20training%20is%20hindered%20by%20the%20scarcity%20of%20labeled%20data.%20In%20contrast%2C%0Aunlabeled%20data%20is%20often%20readily%20which%20can%20be%20leveraged%20by%20using%20LLMs%20to%0Agenerate%20pseudo-labels%20for%20training%20smaller%20models.%20This%20enables%20the%20smaller%0Amodels%20%28student%29%20to%20acquire%20knowledge%20from%20LLMs%28teacher%29%20while%20reducing%0Acomputational%20costs.%20This%20process%20introduces%20challenges%2C%20such%20as%20potential%0Anoisy%20pseudo-labels.%20Selecting%20high-quality%20and%20informative%20data%20is%20therefore%0Acritical%20to%20enhance%20model%20performance%20while%20improving%20the%20efficiency%20of%20data%0Autilization.%20To%20address%20this%2C%20we%20propose%20LLKD%20that%20enables%20Learning%20with%20Less%0Acomputational%20resources%20and%20less%20data%20for%20Knowledge%20Distillation%20from%20LLMs.%0ALLKD%20is%20an%20adaptive%20sample%20selection%20method%20that%20incorporates%20signals%20from%20both%0Athe%20teacher%20and%20student.%20Specifically%2C%20it%20prioritizes%20samples%20where%20the%20teacher%0Ademonstrates%20high%20confidence%20in%20its%20labeling%2C%20indicating%20reliable%20labels%2C%20and%0Awhere%20the%20student%20exhibits%20a%20high%20information%20need%2C%20identifying%20challenging%0Asamples%20that%20require%20further%20learning.%20Our%20comprehensive%20experiments%20show%20that%0ALLKD%20achieves%20superior%20performance%20across%20various%20datasets%20with%20higher%20data%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08028v1&entry.124074799=Read"},
{"title": "LLMPhy: Complex Physical Reasoning Using Large Language Models and World\n  Models", "author": "Anoop Cherian and Radu Corcodel and Siddarth Jain and Diego Romeres", "abstract": "  Physical reasoning is an important skill needed for robotic agents when\noperating in the real world. However, solving such reasoning problems often\ninvolves hypothesizing and reflecting over complex multi-body interactions\nunder the effect of a multitude of physical forces and thus learning all such\ninteractions poses a significant hurdle for state-of-the-art machine learning\nframeworks, including large language models (LLMs). To study this problem, we\npropose a new physical reasoning task and a dataset, dubbed TraySim. Our task\ninvolves predicting the dynamics of several objects on a tray that is given an\nexternal impact -- the domino effect of the ensued object interactions and\ntheir dynamics thus offering a challenging yet controlled setup, with the goal\nof reasoning being to infer the stability of the objects after the impact. To\nsolve this complex physical reasoning task, we present LLMPhy, a zero-shot\nblack-box optimization framework that leverages the physics knowledge and\nprogram synthesis abilities of LLMs, and synergizes these abilities with the\nworld models built into modern physics engines. Specifically, LLMPhy uses an\nLLM to generate code to iteratively estimate the physical hyperparameters of\nthe system (friction, damping, layout, etc.) via an implicit\nanalysis-by-synthesis approach using a (non-differentiable) simulator in the\nloop and uses the inferred parameters to imagine the dynamics of the scene\ntowards solving the reasoning task. To show the effectiveness of LLMPhy, we\npresent experiments on our TraySim dataset to predict the steady-state poses of\nthe objects. Our results show that the combination of the LLM and the physics\nengine leads to state-of-the-art zero-shot physical reasoning performance,\nwhile demonstrating superior convergence against standard black-box\noptimization methods and better estimation of the physical parameters.\n", "link": "http://arxiv.org/abs/2411.08027v1", "date": "2024-11-12", "relevancy": 1.6754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6239}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5419}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMPhy%3A%20Complex%20Physical%20Reasoning%20Using%20Large%20Language%20Models%20and%20World%0A%20%20Models&body=Title%3A%20LLMPhy%3A%20Complex%20Physical%20Reasoning%20Using%20Large%20Language%20Models%20and%20World%0A%20%20Models%0AAuthor%3A%20Anoop%20Cherian%20and%20Radu%20Corcodel%20and%20Siddarth%20Jain%20and%20Diego%20Romeres%0AAbstract%3A%20%20%20Physical%20reasoning%20is%20an%20important%20skill%20needed%20for%20robotic%20agents%20when%0Aoperating%20in%20the%20real%20world.%20However%2C%20solving%20such%20reasoning%20problems%20often%0Ainvolves%20hypothesizing%20and%20reflecting%20over%20complex%20multi-body%20interactions%0Aunder%20the%20effect%20of%20a%20multitude%20of%20physical%20forces%20and%20thus%20learning%20all%20such%0Ainteractions%20poses%20a%20significant%20hurdle%20for%20state-of-the-art%20machine%20learning%0Aframeworks%2C%20including%20large%20language%20models%20%28LLMs%29.%20To%20study%20this%20problem%2C%20we%0Apropose%20a%20new%20physical%20reasoning%20task%20and%20a%20dataset%2C%20dubbed%20TraySim.%20Our%20task%0Ainvolves%20predicting%20the%20dynamics%20of%20several%20objects%20on%20a%20tray%20that%20is%20given%20an%0Aexternal%20impact%20--%20the%20domino%20effect%20of%20the%20ensued%20object%20interactions%20and%0Atheir%20dynamics%20thus%20offering%20a%20challenging%20yet%20controlled%20setup%2C%20with%20the%20goal%0Aof%20reasoning%20being%20to%20infer%20the%20stability%20of%20the%20objects%20after%20the%20impact.%20To%0Asolve%20this%20complex%20physical%20reasoning%20task%2C%20we%20present%20LLMPhy%2C%20a%20zero-shot%0Ablack-box%20optimization%20framework%20that%20leverages%20the%20physics%20knowledge%20and%0Aprogram%20synthesis%20abilities%20of%20LLMs%2C%20and%20synergizes%20these%20abilities%20with%20the%0Aworld%20models%20built%20into%20modern%20physics%20engines.%20Specifically%2C%20LLMPhy%20uses%20an%0ALLM%20to%20generate%20code%20to%20iteratively%20estimate%20the%20physical%20hyperparameters%20of%0Athe%20system%20%28friction%2C%20damping%2C%20layout%2C%20etc.%29%20via%20an%20implicit%0Aanalysis-by-synthesis%20approach%20using%20a%20%28non-differentiable%29%20simulator%20in%20the%0Aloop%20and%20uses%20the%20inferred%20parameters%20to%20imagine%20the%20dynamics%20of%20the%20scene%0Atowards%20solving%20the%20reasoning%20task.%20To%20show%20the%20effectiveness%20of%20LLMPhy%2C%20we%0Apresent%20experiments%20on%20our%20TraySim%20dataset%20to%20predict%20the%20steady-state%20poses%20of%0Athe%20objects.%20Our%20results%20show%20that%20the%20combination%20of%20the%20LLM%20and%20the%20physics%0Aengine%20leads%20to%20state-of-the-art%20zero-shot%20physical%20reasoning%20performance%2C%0Awhile%20demonstrating%20superior%20convergence%20against%20standard%20black-box%0Aoptimization%20methods%20and%20better%20estimation%20of%20the%20physical%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMPhy%253A%2520Complex%2520Physical%2520Reasoning%2520Using%2520Large%2520Language%2520Models%2520and%2520World%250A%2520%2520Models%26entry.906535625%3DAnoop%2520Cherian%2520and%2520Radu%2520Corcodel%2520and%2520Siddarth%2520Jain%2520and%2520Diego%2520Romeres%26entry.1292438233%3D%2520%2520Physical%2520reasoning%2520is%2520an%2520important%2520skill%2520needed%2520for%2520robotic%2520agents%2520when%250Aoperating%2520in%2520the%2520real%2520world.%2520However%252C%2520solving%2520such%2520reasoning%2520problems%2520often%250Ainvolves%2520hypothesizing%2520and%2520reflecting%2520over%2520complex%2520multi-body%2520interactions%250Aunder%2520the%2520effect%2520of%2520a%2520multitude%2520of%2520physical%2520forces%2520and%2520thus%2520learning%2520all%2520such%250Ainteractions%2520poses%2520a%2520significant%2520hurdle%2520for%2520state-of-the-art%2520machine%2520learning%250Aframeworks%252C%2520including%2520large%2520language%2520models%2520%2528LLMs%2529.%2520To%2520study%2520this%2520problem%252C%2520we%250Apropose%2520a%2520new%2520physical%2520reasoning%2520task%2520and%2520a%2520dataset%252C%2520dubbed%2520TraySim.%2520Our%2520task%250Ainvolves%2520predicting%2520the%2520dynamics%2520of%2520several%2520objects%2520on%2520a%2520tray%2520that%2520is%2520given%2520an%250Aexternal%2520impact%2520--%2520the%2520domino%2520effect%2520of%2520the%2520ensued%2520object%2520interactions%2520and%250Atheir%2520dynamics%2520thus%2520offering%2520a%2520challenging%2520yet%2520controlled%2520setup%252C%2520with%2520the%2520goal%250Aof%2520reasoning%2520being%2520to%2520infer%2520the%2520stability%2520of%2520the%2520objects%2520after%2520the%2520impact.%2520To%250Asolve%2520this%2520complex%2520physical%2520reasoning%2520task%252C%2520we%2520present%2520LLMPhy%252C%2520a%2520zero-shot%250Ablack-box%2520optimization%2520framework%2520that%2520leverages%2520the%2520physics%2520knowledge%2520and%250Aprogram%2520synthesis%2520abilities%2520of%2520LLMs%252C%2520and%2520synergizes%2520these%2520abilities%2520with%2520the%250Aworld%2520models%2520built%2520into%2520modern%2520physics%2520engines.%2520Specifically%252C%2520LLMPhy%2520uses%2520an%250ALLM%2520to%2520generate%2520code%2520to%2520iteratively%2520estimate%2520the%2520physical%2520hyperparameters%2520of%250Athe%2520system%2520%2528friction%252C%2520damping%252C%2520layout%252C%2520etc.%2529%2520via%2520an%2520implicit%250Aanalysis-by-synthesis%2520approach%2520using%2520a%2520%2528non-differentiable%2529%2520simulator%2520in%2520the%250Aloop%2520and%2520uses%2520the%2520inferred%2520parameters%2520to%2520imagine%2520the%2520dynamics%2520of%2520the%2520scene%250Atowards%2520solving%2520the%2520reasoning%2520task.%2520To%2520show%2520the%2520effectiveness%2520of%2520LLMPhy%252C%2520we%250Apresent%2520experiments%2520on%2520our%2520TraySim%2520dataset%2520to%2520predict%2520the%2520steady-state%2520poses%2520of%250Athe%2520objects.%2520Our%2520results%2520show%2520that%2520the%2520combination%2520of%2520the%2520LLM%2520and%2520the%2520physics%250Aengine%2520leads%2520to%2520state-of-the-art%2520zero-shot%2520physical%2520reasoning%2520performance%252C%250Awhile%2520demonstrating%2520superior%2520convergence%2520against%2520standard%2520black-box%250Aoptimization%2520methods%2520and%2520better%2520estimation%2520of%2520the%2520physical%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMPhy%3A%20Complex%20Physical%20Reasoning%20Using%20Large%20Language%20Models%20and%20World%0A%20%20Models&entry.906535625=Anoop%20Cherian%20and%20Radu%20Corcodel%20and%20Siddarth%20Jain%20and%20Diego%20Romeres&entry.1292438233=%20%20Physical%20reasoning%20is%20an%20important%20skill%20needed%20for%20robotic%20agents%20when%0Aoperating%20in%20the%20real%20world.%20However%2C%20solving%20such%20reasoning%20problems%20often%0Ainvolves%20hypothesizing%20and%20reflecting%20over%20complex%20multi-body%20interactions%0Aunder%20the%20effect%20of%20a%20multitude%20of%20physical%20forces%20and%20thus%20learning%20all%20such%0Ainteractions%20poses%20a%20significant%20hurdle%20for%20state-of-the-art%20machine%20learning%0Aframeworks%2C%20including%20large%20language%20models%20%28LLMs%29.%20To%20study%20this%20problem%2C%20we%0Apropose%20a%20new%20physical%20reasoning%20task%20and%20a%20dataset%2C%20dubbed%20TraySim.%20Our%20task%0Ainvolves%20predicting%20the%20dynamics%20of%20several%20objects%20on%20a%20tray%20that%20is%20given%20an%0Aexternal%20impact%20--%20the%20domino%20effect%20of%20the%20ensued%20object%20interactions%20and%0Atheir%20dynamics%20thus%20offering%20a%20challenging%20yet%20controlled%20setup%2C%20with%20the%20goal%0Aof%20reasoning%20being%20to%20infer%20the%20stability%20of%20the%20objects%20after%20the%20impact.%20To%0Asolve%20this%20complex%20physical%20reasoning%20task%2C%20we%20present%20LLMPhy%2C%20a%20zero-shot%0Ablack-box%20optimization%20framework%20that%20leverages%20the%20physics%20knowledge%20and%0Aprogram%20synthesis%20abilities%20of%20LLMs%2C%20and%20synergizes%20these%20abilities%20with%20the%0Aworld%20models%20built%20into%20modern%20physics%20engines.%20Specifically%2C%20LLMPhy%20uses%20an%0ALLM%20to%20generate%20code%20to%20iteratively%20estimate%20the%20physical%20hyperparameters%20of%0Athe%20system%20%28friction%2C%20damping%2C%20layout%2C%20etc.%29%20via%20an%20implicit%0Aanalysis-by-synthesis%20approach%20using%20a%20%28non-differentiable%29%20simulator%20in%20the%0Aloop%20and%20uses%20the%20inferred%20parameters%20to%20imagine%20the%20dynamics%20of%20the%20scene%0Atowards%20solving%20the%20reasoning%20task.%20To%20show%20the%20effectiveness%20of%20LLMPhy%2C%20we%0Apresent%20experiments%20on%20our%20TraySim%20dataset%20to%20predict%20the%20steady-state%20poses%20of%0Athe%20objects.%20Our%20results%20show%20that%20the%20combination%20of%20the%20LLM%20and%20the%20physics%0Aengine%20leads%20to%20state-of-the-art%20zero-shot%20physical%20reasoning%20performance%2C%0Awhile%20demonstrating%20superior%20convergence%20against%20standard%20black-box%0Aoptimization%20methods%20and%20better%20estimation%20of%20the%20physical%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08027v1&entry.124074799=Read"},
{"title": "On the Convergence of Continual Federated Learning Using Incrementally\n  Aggregated Gradients", "author": "Satish Kumar Keshri and Nazreen Shah and Ranjitha Prasad", "abstract": "  The holy grail of machine learning is to enable Continual Federated Learning\n(CFL) to enhance the efficiency, privacy, and scalability of AI systems while\nlearning from streaming data. The primary challenge of a CFL system is to\novercome global catastrophic forgetting, wherein the accuracy of the global\nmodel trained on new tasks declines on the old tasks. In this work, we propose\nContinual Federated Learning with Aggregated Gradients (C-FLAG), a novel\nreplay-memory based federated strategy consisting of edge-based gradient\nupdates on memory and aggregated gradients on the current data. We provide\nconvergence analysis of the C-FLAG approach which addresses forgetting and bias\nwhile converging at a rate of $O(1/\\sqrt{T})$ over $T$ communication rounds. We\nformulate an optimization sub-problem that minimizes catastrophic forgetting,\ntranslating CFL into an iterative algorithm with adaptive learning rates that\nensure seamless learning across tasks. We empirically show that C-FLAG\noutperforms several state-of-the-art baselines on both task and\nclass-incremental settings with respect to metrics such as accuracy and\nforgetting.\n", "link": "http://arxiv.org/abs/2411.07959v1", "date": "2024-11-12", "relevancy": 1.879, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4785}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Continual%20Federated%20Learning%20Using%20Incrementally%0A%20%20Aggregated%20Gradients&body=Title%3A%20On%20the%20Convergence%20of%20Continual%20Federated%20Learning%20Using%20Incrementally%0A%20%20Aggregated%20Gradients%0AAuthor%3A%20Satish%20Kumar%20Keshri%20and%20Nazreen%20Shah%20and%20Ranjitha%20Prasad%0AAbstract%3A%20%20%20The%20holy%20grail%20of%20machine%20learning%20is%20to%20enable%20Continual%20Federated%20Learning%0A%28CFL%29%20to%20enhance%20the%20efficiency%2C%20privacy%2C%20and%20scalability%20of%20AI%20systems%20while%0Alearning%20from%20streaming%20data.%20The%20primary%20challenge%20of%20a%20CFL%20system%20is%20to%0Aovercome%20global%20catastrophic%20forgetting%2C%20wherein%20the%20accuracy%20of%20the%20global%0Amodel%20trained%20on%20new%20tasks%20declines%20on%20the%20old%20tasks.%20In%20this%20work%2C%20we%20propose%0AContinual%20Federated%20Learning%20with%20Aggregated%20Gradients%20%28C-FLAG%29%2C%20a%20novel%0Areplay-memory%20based%20federated%20strategy%20consisting%20of%20edge-based%20gradient%0Aupdates%20on%20memory%20and%20aggregated%20gradients%20on%20the%20current%20data.%20We%20provide%0Aconvergence%20analysis%20of%20the%20C-FLAG%20approach%20which%20addresses%20forgetting%20and%20bias%0Awhile%20converging%20at%20a%20rate%20of%20%24O%281/%5Csqrt%7BT%7D%29%24%20over%20%24T%24%20communication%20rounds.%20We%0Aformulate%20an%20optimization%20sub-problem%20that%20minimizes%20catastrophic%20forgetting%2C%0Atranslating%20CFL%20into%20an%20iterative%20algorithm%20with%20adaptive%20learning%20rates%20that%0Aensure%20seamless%20learning%20across%20tasks.%20We%20empirically%20show%20that%20C-FLAG%0Aoutperforms%20several%20state-of-the-art%20baselines%20on%20both%20task%20and%0Aclass-incremental%20settings%20with%20respect%20to%20metrics%20such%20as%20accuracy%20and%0Aforgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520of%2520Continual%2520Federated%2520Learning%2520Using%2520Incrementally%250A%2520%2520Aggregated%2520Gradients%26entry.906535625%3DSatish%2520Kumar%2520Keshri%2520and%2520Nazreen%2520Shah%2520and%2520Ranjitha%2520Prasad%26entry.1292438233%3D%2520%2520The%2520holy%2520grail%2520of%2520machine%2520learning%2520is%2520to%2520enable%2520Continual%2520Federated%2520Learning%250A%2528CFL%2529%2520to%2520enhance%2520the%2520efficiency%252C%2520privacy%252C%2520and%2520scalability%2520of%2520AI%2520systems%2520while%250Alearning%2520from%2520streaming%2520data.%2520The%2520primary%2520challenge%2520of%2520a%2520CFL%2520system%2520is%2520to%250Aovercome%2520global%2520catastrophic%2520forgetting%252C%2520wherein%2520the%2520accuracy%2520of%2520the%2520global%250Amodel%2520trained%2520on%2520new%2520tasks%2520declines%2520on%2520the%2520old%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%250AContinual%2520Federated%2520Learning%2520with%2520Aggregated%2520Gradients%2520%2528C-FLAG%2529%252C%2520a%2520novel%250Areplay-memory%2520based%2520federated%2520strategy%2520consisting%2520of%2520edge-based%2520gradient%250Aupdates%2520on%2520memory%2520and%2520aggregated%2520gradients%2520on%2520the%2520current%2520data.%2520We%2520provide%250Aconvergence%2520analysis%2520of%2520the%2520C-FLAG%2520approach%2520which%2520addresses%2520forgetting%2520and%2520bias%250Awhile%2520converging%2520at%2520a%2520rate%2520of%2520%2524O%25281/%255Csqrt%257BT%257D%2529%2524%2520over%2520%2524T%2524%2520communication%2520rounds.%2520We%250Aformulate%2520an%2520optimization%2520sub-problem%2520that%2520minimizes%2520catastrophic%2520forgetting%252C%250Atranslating%2520CFL%2520into%2520an%2520iterative%2520algorithm%2520with%2520adaptive%2520learning%2520rates%2520that%250Aensure%2520seamless%2520learning%2520across%2520tasks.%2520We%2520empirically%2520show%2520that%2520C-FLAG%250Aoutperforms%2520several%2520state-of-the-art%2520baselines%2520on%2520both%2520task%2520and%250Aclass-incremental%2520settings%2520with%2520respect%2520to%2520metrics%2520such%2520as%2520accuracy%2520and%250Aforgetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Continual%20Federated%20Learning%20Using%20Incrementally%0A%20%20Aggregated%20Gradients&entry.906535625=Satish%20Kumar%20Keshri%20and%20Nazreen%20Shah%20and%20Ranjitha%20Prasad&entry.1292438233=%20%20The%20holy%20grail%20of%20machine%20learning%20is%20to%20enable%20Continual%20Federated%20Learning%0A%28CFL%29%20to%20enhance%20the%20efficiency%2C%20privacy%2C%20and%20scalability%20of%20AI%20systems%20while%0Alearning%20from%20streaming%20data.%20The%20primary%20challenge%20of%20a%20CFL%20system%20is%20to%0Aovercome%20global%20catastrophic%20forgetting%2C%20wherein%20the%20accuracy%20of%20the%20global%0Amodel%20trained%20on%20new%20tasks%20declines%20on%20the%20old%20tasks.%20In%20this%20work%2C%20we%20propose%0AContinual%20Federated%20Learning%20with%20Aggregated%20Gradients%20%28C-FLAG%29%2C%20a%20novel%0Areplay-memory%20based%20federated%20strategy%20consisting%20of%20edge-based%20gradient%0Aupdates%20on%20memory%20and%20aggregated%20gradients%20on%20the%20current%20data.%20We%20provide%0Aconvergence%20analysis%20of%20the%20C-FLAG%20approach%20which%20addresses%20forgetting%20and%20bias%0Awhile%20converging%20at%20a%20rate%20of%20%24O%281/%5Csqrt%7BT%7D%29%24%20over%20%24T%24%20communication%20rounds.%20We%0Aformulate%20an%20optimization%20sub-problem%20that%20minimizes%20catastrophic%20forgetting%2C%0Atranslating%20CFL%20into%20an%20iterative%20algorithm%20with%20adaptive%20learning%20rates%20that%0Aensure%20seamless%20learning%20across%20tasks.%20We%20empirically%20show%20that%20C-FLAG%0Aoutperforms%20several%20state-of-the-art%20baselines%20on%20both%20task%20and%0Aclass-incremental%20settings%20with%20respect%20to%20metrics%20such%20as%20accuracy%20and%0Aforgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07959v1&entry.124074799=Read"},
{"title": "Kernel-based retrieval models for hyperspectral image data optimized\n  with Kernel Flows", "author": "Zina-Sabrina Duma and Tuomas Sihvonen and Jouni Susiluoto and Otto Lamminp\u00e4\u00e4 and Heikki Haario and Satu-Pia Reinikainen", "abstract": "  Kernel-based statistical methods are efficient, but their performance depends\nheavily on the selection of kernel parameters. In literature, the optimization\nstudies on kernel-based chemometric methods is limited and often reduced to\ngrid searching. Previously, the authors introduced Kernel Flows (KF) to learn\nkernel parameters for Kernel Partial Least-Squares (K-PLS) regression. KF is\neasy to implement and helps minimize overfitting. In cases of high collinearity\nbetween spectra and biogeophysical quantities in spectroscopy, simpler methods\nlike Principal Component Regression (PCR) may be more suitable. In this study,\nwe propose a new KF-type approach to optimize Kernel Principal Component\nRegression (K-PCR) and test it alongside KF-PLS. Both methods are benchmarked\nagainst non-linear regression techniques using two hyperspectral remote sensing\ndatasets.\n", "link": "http://arxiv.org/abs/2411.07800v1", "date": "2024-11-12", "relevancy": 1.404, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4816}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel-based%20retrieval%20models%20for%20hyperspectral%20image%20data%20optimized%0A%20%20with%20Kernel%20Flows&body=Title%3A%20Kernel-based%20retrieval%20models%20for%20hyperspectral%20image%20data%20optimized%0A%20%20with%20Kernel%20Flows%0AAuthor%3A%20Zina-Sabrina%20Duma%20and%20Tuomas%20Sihvonen%20and%20Jouni%20Susiluoto%20and%20Otto%20Lamminp%C3%A4%C3%A4%20and%20Heikki%20Haario%20and%20Satu-Pia%20Reinikainen%0AAbstract%3A%20%20%20Kernel-based%20statistical%20methods%20are%20efficient%2C%20but%20their%20performance%20depends%0Aheavily%20on%20the%20selection%20of%20kernel%20parameters.%20In%20literature%2C%20the%20optimization%0Astudies%20on%20kernel-based%20chemometric%20methods%20is%20limited%20and%20often%20reduced%20to%0Agrid%20searching.%20Previously%2C%20the%20authors%20introduced%20Kernel%20Flows%20%28KF%29%20to%20learn%0Akernel%20parameters%20for%20Kernel%20Partial%20Least-Squares%20%28K-PLS%29%20regression.%20KF%20is%0Aeasy%20to%20implement%20and%20helps%20minimize%20overfitting.%20In%20cases%20of%20high%20collinearity%0Abetween%20spectra%20and%20biogeophysical%20quantities%20in%20spectroscopy%2C%20simpler%20methods%0Alike%20Principal%20Component%20Regression%20%28PCR%29%20may%20be%20more%20suitable.%20In%20this%20study%2C%0Awe%20propose%20a%20new%20KF-type%20approach%20to%20optimize%20Kernel%20Principal%20Component%0ARegression%20%28K-PCR%29%20and%20test%20it%20alongside%20KF-PLS.%20Both%20methods%20are%20benchmarked%0Aagainst%20non-linear%20regression%20techniques%20using%20two%20hyperspectral%20remote%20sensing%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel-based%2520retrieval%2520models%2520for%2520hyperspectral%2520image%2520data%2520optimized%250A%2520%2520with%2520Kernel%2520Flows%26entry.906535625%3DZina-Sabrina%2520Duma%2520and%2520Tuomas%2520Sihvonen%2520and%2520Jouni%2520Susiluoto%2520and%2520Otto%2520Lamminp%25C3%25A4%25C3%25A4%2520and%2520Heikki%2520Haario%2520and%2520Satu-Pia%2520Reinikainen%26entry.1292438233%3D%2520%2520Kernel-based%2520statistical%2520methods%2520are%2520efficient%252C%2520but%2520their%2520performance%2520depends%250Aheavily%2520on%2520the%2520selection%2520of%2520kernel%2520parameters.%2520In%2520literature%252C%2520the%2520optimization%250Astudies%2520on%2520kernel-based%2520chemometric%2520methods%2520is%2520limited%2520and%2520often%2520reduced%2520to%250Agrid%2520searching.%2520Previously%252C%2520the%2520authors%2520introduced%2520Kernel%2520Flows%2520%2528KF%2529%2520to%2520learn%250Akernel%2520parameters%2520for%2520Kernel%2520Partial%2520Least-Squares%2520%2528K-PLS%2529%2520regression.%2520KF%2520is%250Aeasy%2520to%2520implement%2520and%2520helps%2520minimize%2520overfitting.%2520In%2520cases%2520of%2520high%2520collinearity%250Abetween%2520spectra%2520and%2520biogeophysical%2520quantities%2520in%2520spectroscopy%252C%2520simpler%2520methods%250Alike%2520Principal%2520Component%2520Regression%2520%2528PCR%2529%2520may%2520be%2520more%2520suitable.%2520In%2520this%2520study%252C%250Awe%2520propose%2520a%2520new%2520KF-type%2520approach%2520to%2520optimize%2520Kernel%2520Principal%2520Component%250ARegression%2520%2528K-PCR%2529%2520and%2520test%2520it%2520alongside%2520KF-PLS.%2520Both%2520methods%2520are%2520benchmarked%250Aagainst%2520non-linear%2520regression%2520techniques%2520using%2520two%2520hyperspectral%2520remote%2520sensing%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel-based%20retrieval%20models%20for%20hyperspectral%20image%20data%20optimized%0A%20%20with%20Kernel%20Flows&entry.906535625=Zina-Sabrina%20Duma%20and%20Tuomas%20Sihvonen%20and%20Jouni%20Susiluoto%20and%20Otto%20Lamminp%C3%A4%C3%A4%20and%20Heikki%20Haario%20and%20Satu-Pia%20Reinikainen&entry.1292438233=%20%20Kernel-based%20statistical%20methods%20are%20efficient%2C%20but%20their%20performance%20depends%0Aheavily%20on%20the%20selection%20of%20kernel%20parameters.%20In%20literature%2C%20the%20optimization%0Astudies%20on%20kernel-based%20chemometric%20methods%20is%20limited%20and%20often%20reduced%20to%0Agrid%20searching.%20Previously%2C%20the%20authors%20introduced%20Kernel%20Flows%20%28KF%29%20to%20learn%0Akernel%20parameters%20for%20Kernel%20Partial%20Least-Squares%20%28K-PLS%29%20regression.%20KF%20is%0Aeasy%20to%20implement%20and%20helps%20minimize%20overfitting.%20In%20cases%20of%20high%20collinearity%0Abetween%20spectra%20and%20biogeophysical%20quantities%20in%20spectroscopy%2C%20simpler%20methods%0Alike%20Principal%20Component%20Regression%20%28PCR%29%20may%20be%20more%20suitable.%20In%20this%20study%2C%0Awe%20propose%20a%20new%20KF-type%20approach%20to%20optimize%20Kernel%20Principal%20Component%0ARegression%20%28K-PCR%29%20and%20test%20it%20alongside%20KF-PLS.%20Both%20methods%20are%20benchmarked%0Aagainst%20non-linear%20regression%20techniques%20using%20two%20hyperspectral%20remote%20sensing%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07800v1&entry.124074799=Read"},
{"title": "ASER: Activation Smoothing and Error Reconstruction for Large Language\n  Model Quantization", "author": "Weibo Zhao and Yubin Shi and Xinyu Lyu and Wanchen Sui and Shen Li and Yong Li", "abstract": "  Quantization stands as a pivotal technique for large language model (LLM)\nserving, yet it poses significant challenges particularly in achieving\neffective low-bit quantization. The limited numerical mapping makes the\nquantized model produce a non-trivial error, bringing out intolerable\nperformance degration. This paper is anchored in the basic idea of model\ncompression objectives, and delves into the layer-wise error distribution of\nLLMs during post-training quantization. Subsequently, we introduce ASER, an\nalgorithm consisting of (1) Error Reconstruction: low-rank compensation for\nquantization error with LoRA-style matrices constructed by whitening SVD; (2)\nActivation Smoothing: outlier extraction to gain smooth activation and better\nerror compensation. ASER is capable of quantizing typical LLMs to low-bit ones,\nparticularly preserving accuracy even in W4A8 per-channel setup. Experimental\nresults show that ASER is competitive among the state-of-the-art quantization\nalgorithms, showing potential to activation quantization, with minor overhead.\n", "link": "http://arxiv.org/abs/2411.07762v1", "date": "2024-11-12", "relevancy": 1.8402, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASER%3A%20Activation%20Smoothing%20and%20Error%20Reconstruction%20for%20Large%20Language%0A%20%20Model%20Quantization&body=Title%3A%20ASER%3A%20Activation%20Smoothing%20and%20Error%20Reconstruction%20for%20Large%20Language%0A%20%20Model%20Quantization%0AAuthor%3A%20Weibo%20Zhao%20and%20Yubin%20Shi%20and%20Xinyu%20Lyu%20and%20Wanchen%20Sui%20and%20Shen%20Li%20and%20Yong%20Li%0AAbstract%3A%20%20%20Quantization%20stands%20as%20a%20pivotal%20technique%20for%20large%20language%20model%20%28LLM%29%0Aserving%2C%20yet%20it%20poses%20significant%20challenges%20particularly%20in%20achieving%0Aeffective%20low-bit%20quantization.%20The%20limited%20numerical%20mapping%20makes%20the%0Aquantized%20model%20produce%20a%20non-trivial%20error%2C%20bringing%20out%20intolerable%0Aperformance%20degration.%20This%20paper%20is%20anchored%20in%20the%20basic%20idea%20of%20model%0Acompression%20objectives%2C%20and%20delves%20into%20the%20layer-wise%20error%20distribution%20of%0ALLMs%20during%20post-training%20quantization.%20Subsequently%2C%20we%20introduce%20ASER%2C%20an%0Aalgorithm%20consisting%20of%20%281%29%20Error%20Reconstruction%3A%20low-rank%20compensation%20for%0Aquantization%20error%20with%20LoRA-style%20matrices%20constructed%20by%20whitening%20SVD%3B%20%282%29%0AActivation%20Smoothing%3A%20outlier%20extraction%20to%20gain%20smooth%20activation%20and%20better%0Aerror%20compensation.%20ASER%20is%20capable%20of%20quantizing%20typical%20LLMs%20to%20low-bit%20ones%2C%0Aparticularly%20preserving%20accuracy%20even%20in%20W4A8%20per-channel%20setup.%20Experimental%0Aresults%20show%20that%20ASER%20is%20competitive%20among%20the%20state-of-the-art%20quantization%0Aalgorithms%2C%20showing%20potential%20to%20activation%20quantization%2C%20with%20minor%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASER%253A%2520Activation%2520Smoothing%2520and%2520Error%2520Reconstruction%2520for%2520Large%2520Language%250A%2520%2520Model%2520Quantization%26entry.906535625%3DWeibo%2520Zhao%2520and%2520Yubin%2520Shi%2520and%2520Xinyu%2520Lyu%2520and%2520Wanchen%2520Sui%2520and%2520Shen%2520Li%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Quantization%2520stands%2520as%2520a%2520pivotal%2520technique%2520for%2520large%2520language%2520model%2520%2528LLM%2529%250Aserving%252C%2520yet%2520it%2520poses%2520significant%2520challenges%2520particularly%2520in%2520achieving%250Aeffective%2520low-bit%2520quantization.%2520The%2520limited%2520numerical%2520mapping%2520makes%2520the%250Aquantized%2520model%2520produce%2520a%2520non-trivial%2520error%252C%2520bringing%2520out%2520intolerable%250Aperformance%2520degration.%2520This%2520paper%2520is%2520anchored%2520in%2520the%2520basic%2520idea%2520of%2520model%250Acompression%2520objectives%252C%2520and%2520delves%2520into%2520the%2520layer-wise%2520error%2520distribution%2520of%250ALLMs%2520during%2520post-training%2520quantization.%2520Subsequently%252C%2520we%2520introduce%2520ASER%252C%2520an%250Aalgorithm%2520consisting%2520of%2520%25281%2529%2520Error%2520Reconstruction%253A%2520low-rank%2520compensation%2520for%250Aquantization%2520error%2520with%2520LoRA-style%2520matrices%2520constructed%2520by%2520whitening%2520SVD%253B%2520%25282%2529%250AActivation%2520Smoothing%253A%2520outlier%2520extraction%2520to%2520gain%2520smooth%2520activation%2520and%2520better%250Aerror%2520compensation.%2520ASER%2520is%2520capable%2520of%2520quantizing%2520typical%2520LLMs%2520to%2520low-bit%2520ones%252C%250Aparticularly%2520preserving%2520accuracy%2520even%2520in%2520W4A8%2520per-channel%2520setup.%2520Experimental%250Aresults%2520show%2520that%2520ASER%2520is%2520competitive%2520among%2520the%2520state-of-the-art%2520quantization%250Aalgorithms%252C%2520showing%2520potential%2520to%2520activation%2520quantization%252C%2520with%2520minor%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASER%3A%20Activation%20Smoothing%20and%20Error%20Reconstruction%20for%20Large%20Language%0A%20%20Model%20Quantization&entry.906535625=Weibo%20Zhao%20and%20Yubin%20Shi%20and%20Xinyu%20Lyu%20and%20Wanchen%20Sui%20and%20Shen%20Li%20and%20Yong%20Li&entry.1292438233=%20%20Quantization%20stands%20as%20a%20pivotal%20technique%20for%20large%20language%20model%20%28LLM%29%0Aserving%2C%20yet%20it%20poses%20significant%20challenges%20particularly%20in%20achieving%0Aeffective%20low-bit%20quantization.%20The%20limited%20numerical%20mapping%20makes%20the%0Aquantized%20model%20produce%20a%20non-trivial%20error%2C%20bringing%20out%20intolerable%0Aperformance%20degration.%20This%20paper%20is%20anchored%20in%20the%20basic%20idea%20of%20model%0Acompression%20objectives%2C%20and%20delves%20into%20the%20layer-wise%20error%20distribution%20of%0ALLMs%20during%20post-training%20quantization.%20Subsequently%2C%20we%20introduce%20ASER%2C%20an%0Aalgorithm%20consisting%20of%20%281%29%20Error%20Reconstruction%3A%20low-rank%20compensation%20for%0Aquantization%20error%20with%20LoRA-style%20matrices%20constructed%20by%20whitening%20SVD%3B%20%282%29%0AActivation%20Smoothing%3A%20outlier%20extraction%20to%20gain%20smooth%20activation%20and%20better%0Aerror%20compensation.%20ASER%20is%20capable%20of%20quantizing%20typical%20LLMs%20to%20low-bit%20ones%2C%0Aparticularly%20preserving%20accuracy%20even%20in%20W4A8%20per-channel%20setup.%20Experimental%0Aresults%20show%20that%20ASER%20is%20competitive%20among%20the%20state-of-the-art%20quantization%0Aalgorithms%2C%20showing%20potential%20to%20activation%20quantization%2C%20with%20minor%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07762v1&entry.124074799=Read"},
{"title": "Bandits with Abstention under Expert Advice", "author": "Stephen Pasteris and Alberto Rumi and Maximilian Thiessen and Shota Saito and Atsushi Miyauchi and Fabio Vitale and Mark Herbster", "abstract": "  We study the classic problem of prediction with expert advice under bandit\nfeedback. Our model assumes that one action, corresponding to the learner's\nabstention from play, has no reward or loss on every trial. We propose the CBA\nalgorithm, which exploits this assumption to obtain reward bounds that can\nsignificantly improve those of the classical Exp4 algorithm. We can view our\nproblem as the aggregation of confidence-rated predictors when the learner has\nthe option of abstention from play. Importantly, we are the first to achieve\nbounds on the expected cumulative reward for general confidence-rated\npredictors. In the special case of specialists we achieve a novel reward bound,\nsignificantly improving previous bounds of SpecialistExp (treating abstention\nas another action). As an example application, we discuss learning unions of\nballs in a finite metric space. In this contextual setting, we devise an\nefficient implementation of CBA, reducing the runtime from quadratic to almost\nlinear in the number of contexts. Preliminary experiments show that CBA\nimproves over existing bandit algorithms.\n", "link": "http://arxiv.org/abs/2402.14585v2", "date": "2024-11-12", "relevancy": 1.85, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4683}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bandits%20with%20Abstention%20under%20Expert%20Advice&body=Title%3A%20Bandits%20with%20Abstention%20under%20Expert%20Advice%0AAuthor%3A%20Stephen%20Pasteris%20and%20Alberto%20Rumi%20and%20Maximilian%20Thiessen%20and%20Shota%20Saito%20and%20Atsushi%20Miyauchi%20and%20Fabio%20Vitale%20and%20Mark%20Herbster%0AAbstract%3A%20%20%20We%20study%20the%20classic%20problem%20of%20prediction%20with%20expert%20advice%20under%20bandit%0Afeedback.%20Our%20model%20assumes%20that%20one%20action%2C%20corresponding%20to%20the%20learner%27s%0Aabstention%20from%20play%2C%20has%20no%20reward%20or%20loss%20on%20every%20trial.%20We%20propose%20the%20CBA%0Aalgorithm%2C%20which%20exploits%20this%20assumption%20to%20obtain%20reward%20bounds%20that%20can%0Asignificantly%20improve%20those%20of%20the%20classical%20Exp4%20algorithm.%20We%20can%20view%20our%0Aproblem%20as%20the%20aggregation%20of%20confidence-rated%20predictors%20when%20the%20learner%20has%0Athe%20option%20of%20abstention%20from%20play.%20Importantly%2C%20we%20are%20the%20first%20to%20achieve%0Abounds%20on%20the%20expected%20cumulative%20reward%20for%20general%20confidence-rated%0Apredictors.%20In%20the%20special%20case%20of%20specialists%20we%20achieve%20a%20novel%20reward%20bound%2C%0Asignificantly%20improving%20previous%20bounds%20of%20SpecialistExp%20%28treating%20abstention%0Aas%20another%20action%29.%20As%20an%20example%20application%2C%20we%20discuss%20learning%20unions%20of%0Aballs%20in%20a%20finite%20metric%20space.%20In%20this%20contextual%20setting%2C%20we%20devise%20an%0Aefficient%20implementation%20of%20CBA%2C%20reducing%20the%20runtime%20from%20quadratic%20to%20almost%0Alinear%20in%20the%20number%20of%20contexts.%20Preliminary%20experiments%20show%20that%20CBA%0Aimproves%20over%20existing%20bandit%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBandits%2520with%2520Abstention%2520under%2520Expert%2520Advice%26entry.906535625%3DStephen%2520Pasteris%2520and%2520Alberto%2520Rumi%2520and%2520Maximilian%2520Thiessen%2520and%2520Shota%2520Saito%2520and%2520Atsushi%2520Miyauchi%2520and%2520Fabio%2520Vitale%2520and%2520Mark%2520Herbster%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520classic%2520problem%2520of%2520prediction%2520with%2520expert%2520advice%2520under%2520bandit%250Afeedback.%2520Our%2520model%2520assumes%2520that%2520one%2520action%252C%2520corresponding%2520to%2520the%2520learner%2527s%250Aabstention%2520from%2520play%252C%2520has%2520no%2520reward%2520or%2520loss%2520on%2520every%2520trial.%2520We%2520propose%2520the%2520CBA%250Aalgorithm%252C%2520which%2520exploits%2520this%2520assumption%2520to%2520obtain%2520reward%2520bounds%2520that%2520can%250Asignificantly%2520improve%2520those%2520of%2520the%2520classical%2520Exp4%2520algorithm.%2520We%2520can%2520view%2520our%250Aproblem%2520as%2520the%2520aggregation%2520of%2520confidence-rated%2520predictors%2520when%2520the%2520learner%2520has%250Athe%2520option%2520of%2520abstention%2520from%2520play.%2520Importantly%252C%2520we%2520are%2520the%2520first%2520to%2520achieve%250Abounds%2520on%2520the%2520expected%2520cumulative%2520reward%2520for%2520general%2520confidence-rated%250Apredictors.%2520In%2520the%2520special%2520case%2520of%2520specialists%2520we%2520achieve%2520a%2520novel%2520reward%2520bound%252C%250Asignificantly%2520improving%2520previous%2520bounds%2520of%2520SpecialistExp%2520%2528treating%2520abstention%250Aas%2520another%2520action%2529.%2520As%2520an%2520example%2520application%252C%2520we%2520discuss%2520learning%2520unions%2520of%250Aballs%2520in%2520a%2520finite%2520metric%2520space.%2520In%2520this%2520contextual%2520setting%252C%2520we%2520devise%2520an%250Aefficient%2520implementation%2520of%2520CBA%252C%2520reducing%2520the%2520runtime%2520from%2520quadratic%2520to%2520almost%250Alinear%2520in%2520the%2520number%2520of%2520contexts.%2520Preliminary%2520experiments%2520show%2520that%2520CBA%250Aimproves%2520over%2520existing%2520bandit%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bandits%20with%20Abstention%20under%20Expert%20Advice&entry.906535625=Stephen%20Pasteris%20and%20Alberto%20Rumi%20and%20Maximilian%20Thiessen%20and%20Shota%20Saito%20and%20Atsushi%20Miyauchi%20and%20Fabio%20Vitale%20and%20Mark%20Herbster&entry.1292438233=%20%20We%20study%20the%20classic%20problem%20of%20prediction%20with%20expert%20advice%20under%20bandit%0Afeedback.%20Our%20model%20assumes%20that%20one%20action%2C%20corresponding%20to%20the%20learner%27s%0Aabstention%20from%20play%2C%20has%20no%20reward%20or%20loss%20on%20every%20trial.%20We%20propose%20the%20CBA%0Aalgorithm%2C%20which%20exploits%20this%20assumption%20to%20obtain%20reward%20bounds%20that%20can%0Asignificantly%20improve%20those%20of%20the%20classical%20Exp4%20algorithm.%20We%20can%20view%20our%0Aproblem%20as%20the%20aggregation%20of%20confidence-rated%20predictors%20when%20the%20learner%20has%0Athe%20option%20of%20abstention%20from%20play.%20Importantly%2C%20we%20are%20the%20first%20to%20achieve%0Abounds%20on%20the%20expected%20cumulative%20reward%20for%20general%20confidence-rated%0Apredictors.%20In%20the%20special%20case%20of%20specialists%20we%20achieve%20a%20novel%20reward%20bound%2C%0Asignificantly%20improving%20previous%20bounds%20of%20SpecialistExp%20%28treating%20abstention%0Aas%20another%20action%29.%20As%20an%20example%20application%2C%20we%20discuss%20learning%20unions%20of%0Aballs%20in%20a%20finite%20metric%20space.%20In%20this%20contextual%20setting%2C%20we%20devise%20an%0Aefficient%20implementation%20of%20CBA%2C%20reducing%20the%20runtime%20from%20quadratic%20to%20almost%0Alinear%20in%20the%20number%20of%20contexts.%20Preliminary%20experiments%20show%20that%20CBA%0Aimproves%20over%20existing%20bandit%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14585v2&entry.124074799=Read"},
{"title": "Sleep Staging from Airflow Signals Using Fourier Approximations of\n  Persistence Curves", "author": "Shashank Manjunath and Hau-Tieng Wu and Aarti Sathyanarayana", "abstract": "  Sleep staging is a challenging task, typically manually performed by sleep\ntechnologists based on electroencephalogram and other biosignals of patients\ntaken during overnight sleep studies. Recent work aims to leverage automated\nalgorithms to perform sleep staging not based on electroencephalogram signals,\nbut rather based on the airflow signals of subjects. Prior work uses ideas from\ntopological data analysis (TDA), specifically Hermite function expansions of\npersistence curves (HEPC) to featurize airflow signals. However, finite order\nHEPC captures only partial information. In this work, we propose Fourier\napproximations of persistence curves (FAPC), and use this technique to perform\nsleep staging based on airflow signals. We analyze performance using an XGBoost\nmodel on 1155 pediatric sleep studies taken from the Nationwide Children's\nHospital Sleep DataBank (NCHSDB), and find that FAPC methods provide\ncomplimentary information to HEPC methods alone, leading to a 4.9% increase in\nperformance over baseline methods.\n", "link": "http://arxiv.org/abs/2411.07964v1", "date": "2024-11-12", "relevancy": 1.6725, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.445}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4132}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sleep%20Staging%20from%20Airflow%20Signals%20Using%20Fourier%20Approximations%20of%0A%20%20Persistence%20Curves&body=Title%3A%20Sleep%20Staging%20from%20Airflow%20Signals%20Using%20Fourier%20Approximations%20of%0A%20%20Persistence%20Curves%0AAuthor%3A%20Shashank%20Manjunath%20and%20Hau-Tieng%20Wu%20and%20Aarti%20Sathyanarayana%0AAbstract%3A%20%20%20Sleep%20staging%20is%20a%20challenging%20task%2C%20typically%20manually%20performed%20by%20sleep%0Atechnologists%20based%20on%20electroencephalogram%20and%20other%20biosignals%20of%20patients%0Ataken%20during%20overnight%20sleep%20studies.%20Recent%20work%20aims%20to%20leverage%20automated%0Aalgorithms%20to%20perform%20sleep%20staging%20not%20based%20on%20electroencephalogram%20signals%2C%0Abut%20rather%20based%20on%20the%20airflow%20signals%20of%20subjects.%20Prior%20work%20uses%20ideas%20from%0Atopological%20data%20analysis%20%28TDA%29%2C%20specifically%20Hermite%20function%20expansions%20of%0Apersistence%20curves%20%28HEPC%29%20to%20featurize%20airflow%20signals.%20However%2C%20finite%20order%0AHEPC%20captures%20only%20partial%20information.%20In%20this%20work%2C%20we%20propose%20Fourier%0Aapproximations%20of%20persistence%20curves%20%28FAPC%29%2C%20and%20use%20this%20technique%20to%20perform%0Asleep%20staging%20based%20on%20airflow%20signals.%20We%20analyze%20performance%20using%20an%20XGBoost%0Amodel%20on%201155%20pediatric%20sleep%20studies%20taken%20from%20the%20Nationwide%20Children%27s%0AHospital%20Sleep%20DataBank%20%28NCHSDB%29%2C%20and%20find%20that%20FAPC%20methods%20provide%0Acomplimentary%20information%20to%20HEPC%20methods%20alone%2C%20leading%20to%20a%204.9%25%20increase%20in%0Aperformance%20over%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSleep%2520Staging%2520from%2520Airflow%2520Signals%2520Using%2520Fourier%2520Approximations%2520of%250A%2520%2520Persistence%2520Curves%26entry.906535625%3DShashank%2520Manjunath%2520and%2520Hau-Tieng%2520Wu%2520and%2520Aarti%2520Sathyanarayana%26entry.1292438233%3D%2520%2520Sleep%2520staging%2520is%2520a%2520challenging%2520task%252C%2520typically%2520manually%2520performed%2520by%2520sleep%250Atechnologists%2520based%2520on%2520electroencephalogram%2520and%2520other%2520biosignals%2520of%2520patients%250Ataken%2520during%2520overnight%2520sleep%2520studies.%2520Recent%2520work%2520aims%2520to%2520leverage%2520automated%250Aalgorithms%2520to%2520perform%2520sleep%2520staging%2520not%2520based%2520on%2520electroencephalogram%2520signals%252C%250Abut%2520rather%2520based%2520on%2520the%2520airflow%2520signals%2520of%2520subjects.%2520Prior%2520work%2520uses%2520ideas%2520from%250Atopological%2520data%2520analysis%2520%2528TDA%2529%252C%2520specifically%2520Hermite%2520function%2520expansions%2520of%250Apersistence%2520curves%2520%2528HEPC%2529%2520to%2520featurize%2520airflow%2520signals.%2520However%252C%2520finite%2520order%250AHEPC%2520captures%2520only%2520partial%2520information.%2520In%2520this%2520work%252C%2520we%2520propose%2520Fourier%250Aapproximations%2520of%2520persistence%2520curves%2520%2528FAPC%2529%252C%2520and%2520use%2520this%2520technique%2520to%2520perform%250Asleep%2520staging%2520based%2520on%2520airflow%2520signals.%2520We%2520analyze%2520performance%2520using%2520an%2520XGBoost%250Amodel%2520on%25201155%2520pediatric%2520sleep%2520studies%2520taken%2520from%2520the%2520Nationwide%2520Children%2527s%250AHospital%2520Sleep%2520DataBank%2520%2528NCHSDB%2529%252C%2520and%2520find%2520that%2520FAPC%2520methods%2520provide%250Acomplimentary%2520information%2520to%2520HEPC%2520methods%2520alone%252C%2520leading%2520to%2520a%25204.9%2525%2520increase%2520in%250Aperformance%2520over%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sleep%20Staging%20from%20Airflow%20Signals%20Using%20Fourier%20Approximations%20of%0A%20%20Persistence%20Curves&entry.906535625=Shashank%20Manjunath%20and%20Hau-Tieng%20Wu%20and%20Aarti%20Sathyanarayana&entry.1292438233=%20%20Sleep%20staging%20is%20a%20challenging%20task%2C%20typically%20manually%20performed%20by%20sleep%0Atechnologists%20based%20on%20electroencephalogram%20and%20other%20biosignals%20of%20patients%0Ataken%20during%20overnight%20sleep%20studies.%20Recent%20work%20aims%20to%20leverage%20automated%0Aalgorithms%20to%20perform%20sleep%20staging%20not%20based%20on%20electroencephalogram%20signals%2C%0Abut%20rather%20based%20on%20the%20airflow%20signals%20of%20subjects.%20Prior%20work%20uses%20ideas%20from%0Atopological%20data%20analysis%20%28TDA%29%2C%20specifically%20Hermite%20function%20expansions%20of%0Apersistence%20curves%20%28HEPC%29%20to%20featurize%20airflow%20signals.%20However%2C%20finite%20order%0AHEPC%20captures%20only%20partial%20information.%20In%20this%20work%2C%20we%20propose%20Fourier%0Aapproximations%20of%20persistence%20curves%20%28FAPC%29%2C%20and%20use%20this%20technique%20to%20perform%0Asleep%20staging%20based%20on%20airflow%20signals.%20We%20analyze%20performance%20using%20an%20XGBoost%0Amodel%20on%201155%20pediatric%20sleep%20studies%20taken%20from%20the%20Nationwide%20Children%27s%0AHospital%20Sleep%20DataBank%20%28NCHSDB%29%2C%20and%20find%20that%20FAPC%20methods%20provide%0Acomplimentary%20information%20to%20HEPC%20methods%20alone%2C%20leading%20to%20a%204.9%25%20increase%20in%0Aperformance%20over%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07964v1&entry.124074799=Read"},
{"title": "Optimal Control of Mechanical Ventilators with Learned Respiratory\n  Dynamics", "author": "Isaac Ronald Ward and Dylan M. Asmar and Mansur Arief and Jana Krystofova Mike and Mykel J. Kochenderfer", "abstract": "  Deciding on appropriate mechanical ventilator management strategies\nsignificantly impacts the health outcomes for patients with respiratory\ndiseases. Acute Respiratory Distress Syndrome (ARDS) is one such disease that\nrequires careful ventilator operation to be effectively treated. In this work,\nwe frame the management of ventilators for patients with ARDS as a sequential\ndecision making problem using the Markov decision process framework. We\nimplement and compare controllers based on clinical guidelines contained in the\nARDSnet protocol, optimal control theory, and learned latent dynamics\nrepresented as neural networks. The Pulse Physiology Engine's respiratory\ndynamics simulator is used to establish a repeatable benchmark, gather\nsimulated data, and quantitatively compare these controllers. We score\nperformance in terms of measured improvement in established ARDS health markers\n(pertaining to improved respiratory rate, oxygenation, and vital signs). Our\nresults demonstrate that techniques leveraging neural networks and optimal\ncontrol can automatically discover effective ventilation management strategies\nwithout access to explicit ventilator management procedures or guidelines (such\nas those defined in the ARDSnet protocol).\n", "link": "http://arxiv.org/abs/2411.07971v1", "date": "2024-11-12", "relevancy": 1.7856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.462}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.448}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Control%20of%20Mechanical%20Ventilators%20with%20Learned%20Respiratory%0A%20%20Dynamics&body=Title%3A%20Optimal%20Control%20of%20Mechanical%20Ventilators%20with%20Learned%20Respiratory%0A%20%20Dynamics%0AAuthor%3A%20Isaac%20Ronald%20Ward%20and%20Dylan%20M.%20Asmar%20and%20Mansur%20Arief%20and%20Jana%20Krystofova%20Mike%20and%20Mykel%20J.%20Kochenderfer%0AAbstract%3A%20%20%20Deciding%20on%20appropriate%20mechanical%20ventilator%20management%20strategies%0Asignificantly%20impacts%20the%20health%20outcomes%20for%20patients%20with%20respiratory%0Adiseases.%20Acute%20Respiratory%20Distress%20Syndrome%20%28ARDS%29%20is%20one%20such%20disease%20that%0Arequires%20careful%20ventilator%20operation%20to%20be%20effectively%20treated.%20In%20this%20work%2C%0Awe%20frame%20the%20management%20of%20ventilators%20for%20patients%20with%20ARDS%20as%20a%20sequential%0Adecision%20making%20problem%20using%20the%20Markov%20decision%20process%20framework.%20We%0Aimplement%20and%20compare%20controllers%20based%20on%20clinical%20guidelines%20contained%20in%20the%0AARDSnet%20protocol%2C%20optimal%20control%20theory%2C%20and%20learned%20latent%20dynamics%0Arepresented%20as%20neural%20networks.%20The%20Pulse%20Physiology%20Engine%27s%20respiratory%0Adynamics%20simulator%20is%20used%20to%20establish%20a%20repeatable%20benchmark%2C%20gather%0Asimulated%20data%2C%20and%20quantitatively%20compare%20these%20controllers.%20We%20score%0Aperformance%20in%20terms%20of%20measured%20improvement%20in%20established%20ARDS%20health%20markers%0A%28pertaining%20to%20improved%20respiratory%20rate%2C%20oxygenation%2C%20and%20vital%20signs%29.%20Our%0Aresults%20demonstrate%20that%20techniques%20leveraging%20neural%20networks%20and%20optimal%0Acontrol%20can%20automatically%20discover%20effective%20ventilation%20management%20strategies%0Awithout%20access%20to%20explicit%20ventilator%20management%20procedures%20or%20guidelines%20%28such%0Aas%20those%20defined%20in%20the%20ARDSnet%20protocol%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Control%2520of%2520Mechanical%2520Ventilators%2520with%2520Learned%2520Respiratory%250A%2520%2520Dynamics%26entry.906535625%3DIsaac%2520Ronald%2520Ward%2520and%2520Dylan%2520M.%2520Asmar%2520and%2520Mansur%2520Arief%2520and%2520Jana%2520Krystofova%2520Mike%2520and%2520Mykel%2520J.%2520Kochenderfer%26entry.1292438233%3D%2520%2520Deciding%2520on%2520appropriate%2520mechanical%2520ventilator%2520management%2520strategies%250Asignificantly%2520impacts%2520the%2520health%2520outcomes%2520for%2520patients%2520with%2520respiratory%250Adiseases.%2520Acute%2520Respiratory%2520Distress%2520Syndrome%2520%2528ARDS%2529%2520is%2520one%2520such%2520disease%2520that%250Arequires%2520careful%2520ventilator%2520operation%2520to%2520be%2520effectively%2520treated.%2520In%2520this%2520work%252C%250Awe%2520frame%2520the%2520management%2520of%2520ventilators%2520for%2520patients%2520with%2520ARDS%2520as%2520a%2520sequential%250Adecision%2520making%2520problem%2520using%2520the%2520Markov%2520decision%2520process%2520framework.%2520We%250Aimplement%2520and%2520compare%2520controllers%2520based%2520on%2520clinical%2520guidelines%2520contained%2520in%2520the%250AARDSnet%2520protocol%252C%2520optimal%2520control%2520theory%252C%2520and%2520learned%2520latent%2520dynamics%250Arepresented%2520as%2520neural%2520networks.%2520The%2520Pulse%2520Physiology%2520Engine%2527s%2520respiratory%250Adynamics%2520simulator%2520is%2520used%2520to%2520establish%2520a%2520repeatable%2520benchmark%252C%2520gather%250Asimulated%2520data%252C%2520and%2520quantitatively%2520compare%2520these%2520controllers.%2520We%2520score%250Aperformance%2520in%2520terms%2520of%2520measured%2520improvement%2520in%2520established%2520ARDS%2520health%2520markers%250A%2528pertaining%2520to%2520improved%2520respiratory%2520rate%252C%2520oxygenation%252C%2520and%2520vital%2520signs%2529.%2520Our%250Aresults%2520demonstrate%2520that%2520techniques%2520leveraging%2520neural%2520networks%2520and%2520optimal%250Acontrol%2520can%2520automatically%2520discover%2520effective%2520ventilation%2520management%2520strategies%250Awithout%2520access%2520to%2520explicit%2520ventilator%2520management%2520procedures%2520or%2520guidelines%2520%2528such%250Aas%2520those%2520defined%2520in%2520the%2520ARDSnet%2520protocol%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Control%20of%20Mechanical%20Ventilators%20with%20Learned%20Respiratory%0A%20%20Dynamics&entry.906535625=Isaac%20Ronald%20Ward%20and%20Dylan%20M.%20Asmar%20and%20Mansur%20Arief%20and%20Jana%20Krystofova%20Mike%20and%20Mykel%20J.%20Kochenderfer&entry.1292438233=%20%20Deciding%20on%20appropriate%20mechanical%20ventilator%20management%20strategies%0Asignificantly%20impacts%20the%20health%20outcomes%20for%20patients%20with%20respiratory%0Adiseases.%20Acute%20Respiratory%20Distress%20Syndrome%20%28ARDS%29%20is%20one%20such%20disease%20that%0Arequires%20careful%20ventilator%20operation%20to%20be%20effectively%20treated.%20In%20this%20work%2C%0Awe%20frame%20the%20management%20of%20ventilators%20for%20patients%20with%20ARDS%20as%20a%20sequential%0Adecision%20making%20problem%20using%20the%20Markov%20decision%20process%20framework.%20We%0Aimplement%20and%20compare%20controllers%20based%20on%20clinical%20guidelines%20contained%20in%20the%0AARDSnet%20protocol%2C%20optimal%20control%20theory%2C%20and%20learned%20latent%20dynamics%0Arepresented%20as%20neural%20networks.%20The%20Pulse%20Physiology%20Engine%27s%20respiratory%0Adynamics%20simulator%20is%20used%20to%20establish%20a%20repeatable%20benchmark%2C%20gather%0Asimulated%20data%2C%20and%20quantitatively%20compare%20these%20controllers.%20We%20score%0Aperformance%20in%20terms%20of%20measured%20improvement%20in%20established%20ARDS%20health%20markers%0A%28pertaining%20to%20improved%20respiratory%20rate%2C%20oxygenation%2C%20and%20vital%20signs%29.%20Our%0Aresults%20demonstrate%20that%20techniques%20leveraging%20neural%20networks%20and%20optimal%0Acontrol%20can%20automatically%20discover%20effective%20ventilation%20management%20strategies%0Awithout%20access%20to%20explicit%20ventilator%20management%20procedures%20or%20guidelines%20%28such%0Aas%20those%20defined%20in%20the%20ARDSnet%20protocol%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07971v1&entry.124074799=Read"},
{"title": "Towards Vision Mixture of Experts for Wildlife Monitoring on the Edge", "author": "Emmanuel Azuh Mensah and Anderson Lee and Haoran Zhang and Yitong Shan and Kurtis Heimerl", "abstract": "  The explosion of IoT sensors in industrial, consumer and remote sensing use\ncases has come with unprecedented demand for computing infrastructure to\ntransmit and to analyze petabytes of data. Concurrently, the world is slowly\nshifting its focus towards more sustainable computing. For these reasons, there\nhas been a recent effort to reduce the footprint of related computing\ninfrastructure, especially by deep learning algorithms, for advanced insight\ngeneration. The `TinyML' community is actively proposing methods to save\ncommunication bandwidth and excessive cloud storage costs while reducing\nalgorithm inference latency and promoting data privacy. Such proposed\napproaches should ideally process multiple types of data, including time\nseries, audio, satellite images, and video, near the network edge as multiple\ndata streams has been shown to improve the discriminative ability of learning\nalgorithms, especially for generating fine grained results. Incidentally, there\nhas been recent work on data driven conditional computation of subnetworks that\nhas shown real progress in using a single model to share parameters among very\ndifferent types of inputs such as images and text, reducing the computation\nrequirement of multi-tower multimodal networks. Inspired by such line of work,\nwe explore similar per patch conditional computation for the first time for\nmobile vision transformers (vision only case), that will eventually be used for\nsingle-tower multimodal edge models. We evaluate the model on Cornell Sap\nSucker Woods 60, a fine grained bird species discrimination dataset. Our\ninitial experiments uses $4X$ fewer parameters compared to MobileViTV2-1.0 with\na $1$% accuracy drop on the iNaturalist '21 birds test data provided as part of\nthe SSW60 dataset.\n", "link": "http://arxiv.org/abs/2411.07834v1", "date": "2024-11-12", "relevancy": 1.6779, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5867}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Vision%20Mixture%20of%20Experts%20for%20Wildlife%20Monitoring%20on%20the%20Edge&body=Title%3A%20Towards%20Vision%20Mixture%20of%20Experts%20for%20Wildlife%20Monitoring%20on%20the%20Edge%0AAuthor%3A%20Emmanuel%20Azuh%20Mensah%20and%20Anderson%20Lee%20and%20Haoran%20Zhang%20and%20Yitong%20Shan%20and%20Kurtis%20Heimerl%0AAbstract%3A%20%20%20The%20explosion%20of%20IoT%20sensors%20in%20industrial%2C%20consumer%20and%20remote%20sensing%20use%0Acases%20has%20come%20with%20unprecedented%20demand%20for%20computing%20infrastructure%20to%0Atransmit%20and%20to%20analyze%20petabytes%20of%20data.%20Concurrently%2C%20the%20world%20is%20slowly%0Ashifting%20its%20focus%20towards%20more%20sustainable%20computing.%20For%20these%20reasons%2C%20there%0Ahas%20been%20a%20recent%20effort%20to%20reduce%20the%20footprint%20of%20related%20computing%0Ainfrastructure%2C%20especially%20by%20deep%20learning%20algorithms%2C%20for%20advanced%20insight%0Ageneration.%20The%20%60TinyML%27%20community%20is%20actively%20proposing%20methods%20to%20save%0Acommunication%20bandwidth%20and%20excessive%20cloud%20storage%20costs%20while%20reducing%0Aalgorithm%20inference%20latency%20and%20promoting%20data%20privacy.%20Such%20proposed%0Aapproaches%20should%20ideally%20process%20multiple%20types%20of%20data%2C%20including%20time%0Aseries%2C%20audio%2C%20satellite%20images%2C%20and%20video%2C%20near%20the%20network%20edge%20as%20multiple%0Adata%20streams%20has%20been%20shown%20to%20improve%20the%20discriminative%20ability%20of%20learning%0Aalgorithms%2C%20especially%20for%20generating%20fine%20grained%20results.%20Incidentally%2C%20there%0Ahas%20been%20recent%20work%20on%20data%20driven%20conditional%20computation%20of%20subnetworks%20that%0Ahas%20shown%20real%20progress%20in%20using%20a%20single%20model%20to%20share%20parameters%20among%20very%0Adifferent%20types%20of%20inputs%20such%20as%20images%20and%20text%2C%20reducing%20the%20computation%0Arequirement%20of%20multi-tower%20multimodal%20networks.%20Inspired%20by%20such%20line%20of%20work%2C%0Awe%20explore%20similar%20per%20patch%20conditional%20computation%20for%20the%20first%20time%20for%0Amobile%20vision%20transformers%20%28vision%20only%20case%29%2C%20that%20will%20eventually%20be%20used%20for%0Asingle-tower%20multimodal%20edge%20models.%20We%20evaluate%20the%20model%20on%20Cornell%20Sap%0ASucker%20Woods%2060%2C%20a%20fine%20grained%20bird%20species%20discrimination%20dataset.%20Our%0Ainitial%20experiments%20uses%20%244X%24%20fewer%20parameters%20compared%20to%20MobileViTV2-1.0%20with%0Aa%20%241%24%25%20accuracy%20drop%20on%20the%20iNaturalist%20%2721%20birds%20test%20data%20provided%20as%20part%20of%0Athe%20SSW60%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Vision%2520Mixture%2520of%2520Experts%2520for%2520Wildlife%2520Monitoring%2520on%2520the%2520Edge%26entry.906535625%3DEmmanuel%2520Azuh%2520Mensah%2520and%2520Anderson%2520Lee%2520and%2520Haoran%2520Zhang%2520and%2520Yitong%2520Shan%2520and%2520Kurtis%2520Heimerl%26entry.1292438233%3D%2520%2520The%2520explosion%2520of%2520IoT%2520sensors%2520in%2520industrial%252C%2520consumer%2520and%2520remote%2520sensing%2520use%250Acases%2520has%2520come%2520with%2520unprecedented%2520demand%2520for%2520computing%2520infrastructure%2520to%250Atransmit%2520and%2520to%2520analyze%2520petabytes%2520of%2520data.%2520Concurrently%252C%2520the%2520world%2520is%2520slowly%250Ashifting%2520its%2520focus%2520towards%2520more%2520sustainable%2520computing.%2520For%2520these%2520reasons%252C%2520there%250Ahas%2520been%2520a%2520recent%2520effort%2520to%2520reduce%2520the%2520footprint%2520of%2520related%2520computing%250Ainfrastructure%252C%2520especially%2520by%2520deep%2520learning%2520algorithms%252C%2520for%2520advanced%2520insight%250Ageneration.%2520The%2520%2560TinyML%2527%2520community%2520is%2520actively%2520proposing%2520methods%2520to%2520save%250Acommunication%2520bandwidth%2520and%2520excessive%2520cloud%2520storage%2520costs%2520while%2520reducing%250Aalgorithm%2520inference%2520latency%2520and%2520promoting%2520data%2520privacy.%2520Such%2520proposed%250Aapproaches%2520should%2520ideally%2520process%2520multiple%2520types%2520of%2520data%252C%2520including%2520time%250Aseries%252C%2520audio%252C%2520satellite%2520images%252C%2520and%2520video%252C%2520near%2520the%2520network%2520edge%2520as%2520multiple%250Adata%2520streams%2520has%2520been%2520shown%2520to%2520improve%2520the%2520discriminative%2520ability%2520of%2520learning%250Aalgorithms%252C%2520especially%2520for%2520generating%2520fine%2520grained%2520results.%2520Incidentally%252C%2520there%250Ahas%2520been%2520recent%2520work%2520on%2520data%2520driven%2520conditional%2520computation%2520of%2520subnetworks%2520that%250Ahas%2520shown%2520real%2520progress%2520in%2520using%2520a%2520single%2520model%2520to%2520share%2520parameters%2520among%2520very%250Adifferent%2520types%2520of%2520inputs%2520such%2520as%2520images%2520and%2520text%252C%2520reducing%2520the%2520computation%250Arequirement%2520of%2520multi-tower%2520multimodal%2520networks.%2520Inspired%2520by%2520such%2520line%2520of%2520work%252C%250Awe%2520explore%2520similar%2520per%2520patch%2520conditional%2520computation%2520for%2520the%2520first%2520time%2520for%250Amobile%2520vision%2520transformers%2520%2528vision%2520only%2520case%2529%252C%2520that%2520will%2520eventually%2520be%2520used%2520for%250Asingle-tower%2520multimodal%2520edge%2520models.%2520We%2520evaluate%2520the%2520model%2520on%2520Cornell%2520Sap%250ASucker%2520Woods%252060%252C%2520a%2520fine%2520grained%2520bird%2520species%2520discrimination%2520dataset.%2520Our%250Ainitial%2520experiments%2520uses%2520%25244X%2524%2520fewer%2520parameters%2520compared%2520to%2520MobileViTV2-1.0%2520with%250Aa%2520%25241%2524%2525%2520accuracy%2520drop%2520on%2520the%2520iNaturalist%2520%252721%2520birds%2520test%2520data%2520provided%2520as%2520part%2520of%250Athe%2520SSW60%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Vision%20Mixture%20of%20Experts%20for%20Wildlife%20Monitoring%20on%20the%20Edge&entry.906535625=Emmanuel%20Azuh%20Mensah%20and%20Anderson%20Lee%20and%20Haoran%20Zhang%20and%20Yitong%20Shan%20and%20Kurtis%20Heimerl&entry.1292438233=%20%20The%20explosion%20of%20IoT%20sensors%20in%20industrial%2C%20consumer%20and%20remote%20sensing%20use%0Acases%20has%20come%20with%20unprecedented%20demand%20for%20computing%20infrastructure%20to%0Atransmit%20and%20to%20analyze%20petabytes%20of%20data.%20Concurrently%2C%20the%20world%20is%20slowly%0Ashifting%20its%20focus%20towards%20more%20sustainable%20computing.%20For%20these%20reasons%2C%20there%0Ahas%20been%20a%20recent%20effort%20to%20reduce%20the%20footprint%20of%20related%20computing%0Ainfrastructure%2C%20especially%20by%20deep%20learning%20algorithms%2C%20for%20advanced%20insight%0Ageneration.%20The%20%60TinyML%27%20community%20is%20actively%20proposing%20methods%20to%20save%0Acommunication%20bandwidth%20and%20excessive%20cloud%20storage%20costs%20while%20reducing%0Aalgorithm%20inference%20latency%20and%20promoting%20data%20privacy.%20Such%20proposed%0Aapproaches%20should%20ideally%20process%20multiple%20types%20of%20data%2C%20including%20time%0Aseries%2C%20audio%2C%20satellite%20images%2C%20and%20video%2C%20near%20the%20network%20edge%20as%20multiple%0Adata%20streams%20has%20been%20shown%20to%20improve%20the%20discriminative%20ability%20of%20learning%0Aalgorithms%2C%20especially%20for%20generating%20fine%20grained%20results.%20Incidentally%2C%20there%0Ahas%20been%20recent%20work%20on%20data%20driven%20conditional%20computation%20of%20subnetworks%20that%0Ahas%20shown%20real%20progress%20in%20using%20a%20single%20model%20to%20share%20parameters%20among%20very%0Adifferent%20types%20of%20inputs%20such%20as%20images%20and%20text%2C%20reducing%20the%20computation%0Arequirement%20of%20multi-tower%20multimodal%20networks.%20Inspired%20by%20such%20line%20of%20work%2C%0Awe%20explore%20similar%20per%20patch%20conditional%20computation%20for%20the%20first%20time%20for%0Amobile%20vision%20transformers%20%28vision%20only%20case%29%2C%20that%20will%20eventually%20be%20used%20for%0Asingle-tower%20multimodal%20edge%20models.%20We%20evaluate%20the%20model%20on%20Cornell%20Sap%0ASucker%20Woods%2060%2C%20a%20fine%20grained%20bird%20species%20discrimination%20dataset.%20Our%0Ainitial%20experiments%20uses%20%244X%24%20fewer%20parameters%20compared%20to%20MobileViTV2-1.0%20with%0Aa%20%241%24%25%20accuracy%20drop%20on%20the%20iNaturalist%20%2721%20birds%20test%20data%20provided%20as%20part%20of%0Athe%20SSW60%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07834v1&entry.124074799=Read"},
{"title": "OCMDP: Observation-Constrained Markov Decision Process", "author": "Taiyi Wang and Jianheng Liu and Bryan Lee and Zhihao Wu and Yu Wu", "abstract": "  In many practical applications, decision-making processes must balance the\ncosts of acquiring information with the benefits it provides. Traditional\ncontrol systems often assume full observability, an unrealistic assumption when\nobservations are expensive. We tackle the challenge of simultaneously learning\nobservation and control strategies in such cost-sensitive environments by\nintroducing the Observation-Constrained Markov Decision Process (OCMDP), where\nthe policy influences the observability of the true state. To manage the\ncomplexity arising from the combined observation and control actions, we\ndevelop an iterative, model-free deep reinforcement learning algorithm that\nseparates the sensing and control components of the policy. This decomposition\nenables efficient learning in the expanded action space by focusing on when and\nwhat to observe, as well as determining optimal control actions, without\nrequiring knowledge of the environment's dynamics. We validate our approach on\na simulated diagnostic task and a realistic healthcare environment using\nHeartPole. Given both scenarios, the experimental results demonstrate that our\nmodel achieves a substantial reduction in observation costs on average,\nsignificantly outperforming baseline methods by a notable margin in efficiency.\n", "link": "http://arxiv.org/abs/2411.07087v2", "date": "2024-11-12", "relevancy": 1.4892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5301}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5106}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCMDP%3A%20Observation-Constrained%20Markov%20Decision%20Process&body=Title%3A%20OCMDP%3A%20Observation-Constrained%20Markov%20Decision%20Process%0AAuthor%3A%20Taiyi%20Wang%20and%20Jianheng%20Liu%20and%20Bryan%20Lee%20and%20Zhihao%20Wu%20and%20Yu%20Wu%0AAbstract%3A%20%20%20In%20many%20practical%20applications%2C%20decision-making%20processes%20must%20balance%20the%0Acosts%20of%20acquiring%20information%20with%20the%20benefits%20it%20provides.%20Traditional%0Acontrol%20systems%20often%20assume%20full%20observability%2C%20an%20unrealistic%20assumption%20when%0Aobservations%20are%20expensive.%20We%20tackle%20the%20challenge%20of%20simultaneously%20learning%0Aobservation%20and%20control%20strategies%20in%20such%20cost-sensitive%20environments%20by%0Aintroducing%20the%20Observation-Constrained%20Markov%20Decision%20Process%20%28OCMDP%29%2C%20where%0Athe%20policy%20influences%20the%20observability%20of%20the%20true%20state.%20To%20manage%20the%0Acomplexity%20arising%20from%20the%20combined%20observation%20and%20control%20actions%2C%20we%0Adevelop%20an%20iterative%2C%20model-free%20deep%20reinforcement%20learning%20algorithm%20that%0Aseparates%20the%20sensing%20and%20control%20components%20of%20the%20policy.%20This%20decomposition%0Aenables%20efficient%20learning%20in%20the%20expanded%20action%20space%20by%20focusing%20on%20when%20and%0Awhat%20to%20observe%2C%20as%20well%20as%20determining%20optimal%20control%20actions%2C%20without%0Arequiring%20knowledge%20of%20the%20environment%27s%20dynamics.%20We%20validate%20our%20approach%20on%0Aa%20simulated%20diagnostic%20task%20and%20a%20realistic%20healthcare%20environment%20using%0AHeartPole.%20Given%20both%20scenarios%2C%20the%20experimental%20results%20demonstrate%20that%20our%0Amodel%20achieves%20a%20substantial%20reduction%20in%20observation%20costs%20on%20average%2C%0Asignificantly%20outperforming%20baseline%20methods%20by%20a%20notable%20margin%20in%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07087v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCMDP%253A%2520Observation-Constrained%2520Markov%2520Decision%2520Process%26entry.906535625%3DTaiyi%2520Wang%2520and%2520Jianheng%2520Liu%2520and%2520Bryan%2520Lee%2520and%2520Zhihao%2520Wu%2520and%2520Yu%2520Wu%26entry.1292438233%3D%2520%2520In%2520many%2520practical%2520applications%252C%2520decision-making%2520processes%2520must%2520balance%2520the%250Acosts%2520of%2520acquiring%2520information%2520with%2520the%2520benefits%2520it%2520provides.%2520Traditional%250Acontrol%2520systems%2520often%2520assume%2520full%2520observability%252C%2520an%2520unrealistic%2520assumption%2520when%250Aobservations%2520are%2520expensive.%2520We%2520tackle%2520the%2520challenge%2520of%2520simultaneously%2520learning%250Aobservation%2520and%2520control%2520strategies%2520in%2520such%2520cost-sensitive%2520environments%2520by%250Aintroducing%2520the%2520Observation-Constrained%2520Markov%2520Decision%2520Process%2520%2528OCMDP%2529%252C%2520where%250Athe%2520policy%2520influences%2520the%2520observability%2520of%2520the%2520true%2520state.%2520To%2520manage%2520the%250Acomplexity%2520arising%2520from%2520the%2520combined%2520observation%2520and%2520control%2520actions%252C%2520we%250Adevelop%2520an%2520iterative%252C%2520model-free%2520deep%2520reinforcement%2520learning%2520algorithm%2520that%250Aseparates%2520the%2520sensing%2520and%2520control%2520components%2520of%2520the%2520policy.%2520This%2520decomposition%250Aenables%2520efficient%2520learning%2520in%2520the%2520expanded%2520action%2520space%2520by%2520focusing%2520on%2520when%2520and%250Awhat%2520to%2520observe%252C%2520as%2520well%2520as%2520determining%2520optimal%2520control%2520actions%252C%2520without%250Arequiring%2520knowledge%2520of%2520the%2520environment%2527s%2520dynamics.%2520We%2520validate%2520our%2520approach%2520on%250Aa%2520simulated%2520diagnostic%2520task%2520and%2520a%2520realistic%2520healthcare%2520environment%2520using%250AHeartPole.%2520Given%2520both%2520scenarios%252C%2520the%2520experimental%2520results%2520demonstrate%2520that%2520our%250Amodel%2520achieves%2520a%2520substantial%2520reduction%2520in%2520observation%2520costs%2520on%2520average%252C%250Asignificantly%2520outperforming%2520baseline%2520methods%2520by%2520a%2520notable%2520margin%2520in%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07087v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCMDP%3A%20Observation-Constrained%20Markov%20Decision%20Process&entry.906535625=Taiyi%20Wang%20and%20Jianheng%20Liu%20and%20Bryan%20Lee%20and%20Zhihao%20Wu%20and%20Yu%20Wu&entry.1292438233=%20%20In%20many%20practical%20applications%2C%20decision-making%20processes%20must%20balance%20the%0Acosts%20of%20acquiring%20information%20with%20the%20benefits%20it%20provides.%20Traditional%0Acontrol%20systems%20often%20assume%20full%20observability%2C%20an%20unrealistic%20assumption%20when%0Aobservations%20are%20expensive.%20We%20tackle%20the%20challenge%20of%20simultaneously%20learning%0Aobservation%20and%20control%20strategies%20in%20such%20cost-sensitive%20environments%20by%0Aintroducing%20the%20Observation-Constrained%20Markov%20Decision%20Process%20%28OCMDP%29%2C%20where%0Athe%20policy%20influences%20the%20observability%20of%20the%20true%20state.%20To%20manage%20the%0Acomplexity%20arising%20from%20the%20combined%20observation%20and%20control%20actions%2C%20we%0Adevelop%20an%20iterative%2C%20model-free%20deep%20reinforcement%20learning%20algorithm%20that%0Aseparates%20the%20sensing%20and%20control%20components%20of%20the%20policy.%20This%20decomposition%0Aenables%20efficient%20learning%20in%20the%20expanded%20action%20space%20by%20focusing%20on%20when%20and%0Awhat%20to%20observe%2C%20as%20well%20as%20determining%20optimal%20control%20actions%2C%20without%0Arequiring%20knowledge%20of%20the%20environment%27s%20dynamics.%20We%20validate%20our%20approach%20on%0Aa%20simulated%20diagnostic%20task%20and%20a%20realistic%20healthcare%20environment%20using%0AHeartPole.%20Given%20both%20scenarios%2C%20the%20experimental%20results%20demonstrate%20that%20our%0Amodel%20achieves%20a%20substantial%20reduction%20in%20observation%20costs%20on%20average%2C%0Asignificantly%20outperforming%20baseline%20methods%20by%20a%20notable%20margin%20in%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07087v2&entry.124074799=Read"},
{"title": "Likelihood as a Performance Gauge for Retrieval-Augmented Generation", "author": "Tianyu Liu and Jirui Qi and Paul He and Arianna Bisazza and Mrinmaya Sachan and Ryan Cotterell", "abstract": "  Recent work finds that retrieval-augmented generation with large language\nmodels is prone to be influenced by the order of retrieved documents in the\ncontext. However, the lack of in-depth analysis limits the use of this\nphenomenon for prompt engineering in practice. In this study, we posit that\nlikelihoods serve as an effective gauge for language model performance. Through\nexperiments on two question-answering datasets with a variety of\nstate-of-the-art language models, we reveal correlations between answer\naccuracy and the likelihood of the question at both the corpus level and the\ninstance level. In addition, we find that question likelihood can also indicate\nthe position of the task-relevant information in the context. Based on these\nfindings, we propose two methods that use question likelihood as a gauge for\nselecting and constructing prompts that lead to better performance. We\ndemonstrate their effectiveness with experiments. In addition, our\nlikelihood-based methods are efficient, as they only need to compute the\nlikelihood of the input, requiring much fewer language model passes than\nheuristic prompt engineering methods that require generating responses. Our\nanalysis deepens our understanding of how input prompts affect model\nperformance and provides a promising direction for efficient prompt\noptimization.\n", "link": "http://arxiv.org/abs/2411.07773v1", "date": "2024-11-12", "relevancy": 1.7636, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood%20as%20a%20Performance%20Gauge%20for%20Retrieval-Augmented%20Generation&body=Title%3A%20Likelihood%20as%20a%20Performance%20Gauge%20for%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Tianyu%20Liu%20and%20Jirui%20Qi%20and%20Paul%20He%20and%20Arianna%20Bisazza%20and%20Mrinmaya%20Sachan%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20Recent%20work%20finds%20that%20retrieval-augmented%20generation%20with%20large%20language%0Amodels%20is%20prone%20to%20be%20influenced%20by%20the%20order%20of%20retrieved%20documents%20in%20the%0Acontext.%20However%2C%20the%20lack%20of%20in-depth%20analysis%20limits%20the%20use%20of%20this%0Aphenomenon%20for%20prompt%20engineering%20in%20practice.%20In%20this%20study%2C%20we%20posit%20that%0Alikelihoods%20serve%20as%20an%20effective%20gauge%20for%20language%20model%20performance.%20Through%0Aexperiments%20on%20two%20question-answering%20datasets%20with%20a%20variety%20of%0Astate-of-the-art%20language%20models%2C%20we%20reveal%20correlations%20between%20answer%0Aaccuracy%20and%20the%20likelihood%20of%20the%20question%20at%20both%20the%20corpus%20level%20and%20the%0Ainstance%20level.%20In%20addition%2C%20we%20find%20that%20question%20likelihood%20can%20also%20indicate%0Athe%20position%20of%20the%20task-relevant%20information%20in%20the%20context.%20Based%20on%20these%0Afindings%2C%20we%20propose%20two%20methods%20that%20use%20question%20likelihood%20as%20a%20gauge%20for%0Aselecting%20and%20constructing%20prompts%20that%20lead%20to%20better%20performance.%20We%0Ademonstrate%20their%20effectiveness%20with%20experiments.%20In%20addition%2C%20our%0Alikelihood-based%20methods%20are%20efficient%2C%20as%20they%20only%20need%20to%20compute%20the%0Alikelihood%20of%20the%20input%2C%20requiring%20much%20fewer%20language%20model%20passes%20than%0Aheuristic%20prompt%20engineering%20methods%20that%20require%20generating%20responses.%20Our%0Aanalysis%20deepens%20our%20understanding%20of%20how%20input%20prompts%20affect%20model%0Aperformance%20and%20provides%20a%20promising%20direction%20for%20efficient%20prompt%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood%2520as%2520a%2520Performance%2520Gauge%2520for%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DTianyu%2520Liu%2520and%2520Jirui%2520Qi%2520and%2520Paul%2520He%2520and%2520Arianna%2520Bisazza%2520and%2520Mrinmaya%2520Sachan%2520and%2520Ryan%2520Cotterell%26entry.1292438233%3D%2520%2520Recent%2520work%2520finds%2520that%2520retrieval-augmented%2520generation%2520with%2520large%2520language%250Amodels%2520is%2520prone%2520to%2520be%2520influenced%2520by%2520the%2520order%2520of%2520retrieved%2520documents%2520in%2520the%250Acontext.%2520However%252C%2520the%2520lack%2520of%2520in-depth%2520analysis%2520limits%2520the%2520use%2520of%2520this%250Aphenomenon%2520for%2520prompt%2520engineering%2520in%2520practice.%2520In%2520this%2520study%252C%2520we%2520posit%2520that%250Alikelihoods%2520serve%2520as%2520an%2520effective%2520gauge%2520for%2520language%2520model%2520performance.%2520Through%250Aexperiments%2520on%2520two%2520question-answering%2520datasets%2520with%2520a%2520variety%2520of%250Astate-of-the-art%2520language%2520models%252C%2520we%2520reveal%2520correlations%2520between%2520answer%250Aaccuracy%2520and%2520the%2520likelihood%2520of%2520the%2520question%2520at%2520both%2520the%2520corpus%2520level%2520and%2520the%250Ainstance%2520level.%2520In%2520addition%252C%2520we%2520find%2520that%2520question%2520likelihood%2520can%2520also%2520indicate%250Athe%2520position%2520of%2520the%2520task-relevant%2520information%2520in%2520the%2520context.%2520Based%2520on%2520these%250Afindings%252C%2520we%2520propose%2520two%2520methods%2520that%2520use%2520question%2520likelihood%2520as%2520a%2520gauge%2520for%250Aselecting%2520and%2520constructing%2520prompts%2520that%2520lead%2520to%2520better%2520performance.%2520We%250Ademonstrate%2520their%2520effectiveness%2520with%2520experiments.%2520In%2520addition%252C%2520our%250Alikelihood-based%2520methods%2520are%2520efficient%252C%2520as%2520they%2520only%2520need%2520to%2520compute%2520the%250Alikelihood%2520of%2520the%2520input%252C%2520requiring%2520much%2520fewer%2520language%2520model%2520passes%2520than%250Aheuristic%2520prompt%2520engineering%2520methods%2520that%2520require%2520generating%2520responses.%2520Our%250Aanalysis%2520deepens%2520our%2520understanding%2520of%2520how%2520input%2520prompts%2520affect%2520model%250Aperformance%2520and%2520provides%2520a%2520promising%2520direction%2520for%2520efficient%2520prompt%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood%20as%20a%20Performance%20Gauge%20for%20Retrieval-Augmented%20Generation&entry.906535625=Tianyu%20Liu%20and%20Jirui%20Qi%20and%20Paul%20He%20and%20Arianna%20Bisazza%20and%20Mrinmaya%20Sachan%20and%20Ryan%20Cotterell&entry.1292438233=%20%20Recent%20work%20finds%20that%20retrieval-augmented%20generation%20with%20large%20language%0Amodels%20is%20prone%20to%20be%20influenced%20by%20the%20order%20of%20retrieved%20documents%20in%20the%0Acontext.%20However%2C%20the%20lack%20of%20in-depth%20analysis%20limits%20the%20use%20of%20this%0Aphenomenon%20for%20prompt%20engineering%20in%20practice.%20In%20this%20study%2C%20we%20posit%20that%0Alikelihoods%20serve%20as%20an%20effective%20gauge%20for%20language%20model%20performance.%20Through%0Aexperiments%20on%20two%20question-answering%20datasets%20with%20a%20variety%20of%0Astate-of-the-art%20language%20models%2C%20we%20reveal%20correlations%20between%20answer%0Aaccuracy%20and%20the%20likelihood%20of%20the%20question%20at%20both%20the%20corpus%20level%20and%20the%0Ainstance%20level.%20In%20addition%2C%20we%20find%20that%20question%20likelihood%20can%20also%20indicate%0Athe%20position%20of%20the%20task-relevant%20information%20in%20the%20context.%20Based%20on%20these%0Afindings%2C%20we%20propose%20two%20methods%20that%20use%20question%20likelihood%20as%20a%20gauge%20for%0Aselecting%20and%20constructing%20prompts%20that%20lead%20to%20better%20performance.%20We%0Ademonstrate%20their%20effectiveness%20with%20experiments.%20In%20addition%2C%20our%0Alikelihood-based%20methods%20are%20efficient%2C%20as%20they%20only%20need%20to%20compute%20the%0Alikelihood%20of%20the%20input%2C%20requiring%20much%20fewer%20language%20model%20passes%20than%0Aheuristic%20prompt%20engineering%20methods%20that%20require%20generating%20responses.%20Our%0Aanalysis%20deepens%20our%20understanding%20of%20how%20input%20prompts%20affect%20model%0Aperformance%20and%20provides%20a%20promising%20direction%20for%20efficient%20prompt%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07773v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


