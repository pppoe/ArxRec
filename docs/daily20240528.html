<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240527.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap", "author": "Mingrui Li and Jingwei Huang and Lei Sun and Aaron Xuxiang Tian and Tianchen Deng and Hongyu Wang", "abstract": "  SLAM systems based on Gaussian Splatting have garnered attention due to their\ncapabilities for rapid real-time rendering and high-fidelity mapping. However,\ncurrent Gaussian Splatting SLAM systems usually struggle with large scene\nrepresentation and lack effective loop closure detection. To address these\nissues, we introduce NGM-SLAM, the first 3DGS based SLAM system that utilizes\nneural radiance field submaps for progressive scene expression, effectively\nintegrating the strengths of neural radiance fields and 3D Gaussian Splatting.\nWe utilize neural radiance field submaps as supervision and achieve\nhigh-quality scene expression and online loop closure adjustments through\nGaussian rendering of fused submaps. Our results on multiple real-world scenes\nand large-scale scene datasets demonstrate that our method can achieve accurate\nhole filling and high-quality scene expression, supporting monocular, stereo,\nand RGB-D inputs, and achieving state-of-the-art scene reconstruction and\ntracking performance.\n", "link": "http://arxiv.org/abs/2405.05702v5", "date": "2024-05-27", "relevancy": 3.4108, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8163}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6478}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&body=Title%3A%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap%0AAuthor%3A%20Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang%0AAbstract%3A%20%20%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%20garnered%20attention%20due%20to%20their%0Acapabilities%20for%20rapid%20real-time%20rendering%20and%20high-fidelity%20mapping.%20However%2C%0Acurrent%20Gaussian%20Splatting%20SLAM%20systems%20usually%20struggle%20with%20large%20scene%0Arepresentation%20and%20lack%20effective%20loop%20closure%20detection.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NGM-SLAM%2C%20the%20first%203DGS%20based%20SLAM%20system%20that%20utilizes%0Aneural%20radiance%20field%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%0Aintegrating%20the%20strengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%0AWe%20utilize%20neural%20radiance%20field%20submaps%20as%20supervision%20and%20achieve%0Ahigh-quality%20scene%20expression%20and%20online%20loop%20closure%20adjustments%20through%0AGaussian%20rendering%20of%20fused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%0Aand%20large-scale%20scene%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%0Ahole%20filling%20and%20high-quality%20scene%20expression%2C%20supporting%20monocular%2C%20stereo%2C%0Aand%20RGB-D%20inputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%0Atracking%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05702v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNGM-SLAM%253A%2520Gaussian%2520Splatting%2520SLAM%2520with%2520Radiance%2520Field%2520Submap%26entry.906535625%3DMingrui%2520Li%2520and%2520Jingwei%2520Huang%2520and%2520Lei%2520Sun%2520and%2520Aaron%2520Xuxiang%2520Tian%2520and%2520Tianchen%2520Deng%2520and%2520Hongyu%2520Wang%26entry.1292438233%3D%2520%2520SLAM%2520systems%2520based%2520on%2520Gaussian%2520Splatting%2520have%2520garnered%2520attention%2520due%2520to%2520their%250Acapabilities%2520for%2520rapid%2520real-time%2520rendering%2520and%2520high-fidelity%2520mapping.%2520However%252C%250Acurrent%2520Gaussian%2520Splatting%2520SLAM%2520systems%2520usually%2520struggle%2520with%2520large%2520scene%250Arepresentation%2520and%2520lack%2520effective%2520loop%2520closure%2520detection.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520NGM-SLAM%252C%2520the%2520first%25203DGS%2520based%2520SLAM%2520system%2520that%2520utilizes%250Aneural%2520radiance%2520field%2520submaps%2520for%2520progressive%2520scene%2520expression%252C%2520effectively%250Aintegrating%2520the%2520strengths%2520of%2520neural%2520radiance%2520fields%2520and%25203D%2520Gaussian%2520Splatting.%250AWe%2520utilize%2520neural%2520radiance%2520field%2520submaps%2520as%2520supervision%2520and%2520achieve%250Ahigh-quality%2520scene%2520expression%2520and%2520online%2520loop%2520closure%2520adjustments%2520through%250AGaussian%2520rendering%2520of%2520fused%2520submaps.%2520Our%2520results%2520on%2520multiple%2520real-world%2520scenes%250Aand%2520large-scale%2520scene%2520datasets%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%2520accurate%250Ahole%2520filling%2520and%2520high-quality%2520scene%2520expression%252C%2520supporting%2520monocular%252C%2520stereo%252C%250Aand%2520RGB-D%2520inputs%252C%2520and%2520achieving%2520state-of-the-art%2520scene%2520reconstruction%2520and%250Atracking%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05702v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&entry.906535625=Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang&entry.1292438233=%20%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%20garnered%20attention%20due%20to%20their%0Acapabilities%20for%20rapid%20real-time%20rendering%20and%20high-fidelity%20mapping.%20However%2C%0Acurrent%20Gaussian%20Splatting%20SLAM%20systems%20usually%20struggle%20with%20large%20scene%0Arepresentation%20and%20lack%20effective%20loop%20closure%20detection.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NGM-SLAM%2C%20the%20first%203DGS%20based%20SLAM%20system%20that%20utilizes%0Aneural%20radiance%20field%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%0Aintegrating%20the%20strengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%0AWe%20utilize%20neural%20radiance%20field%20submaps%20as%20supervision%20and%20achieve%0Ahigh-quality%20scene%20expression%20and%20online%20loop%20closure%20adjustments%20through%0AGaussian%20rendering%20of%20fused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%0Aand%20large-scale%20scene%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%0Ahole%20filling%20and%20high-quality%20scene%20expression%2C%20supporting%20monocular%2C%20stereo%2C%0Aand%20RGB-D%20inputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%0Atracking%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05702v5&entry.124074799=Read"},
{"title": "Human4DiT: Free-view Human Video Generation with 4D Diffusion\n  Transformer", "author": "Ruizhi Shao and Youxin Pang and Zerong Zheng and Jingxiang Sun and Yebin Liu", "abstract": "  We present a novel approach for generating high-quality, spatio-temporally\ncoherent human videos from a single image under arbitrary viewpoints. Our\nframework combines the strengths of U-Nets for accurate condition injection and\ndiffusion transformers for capturing global correlations across viewpoints and\ntime. The core is a cascaded 4D transformer architecture that factorizes\nattention across views, time, and spatial dimensions, enabling efficient\nmodeling of the 4D space. Precise conditioning is achieved by injecting human\nidentity, camera parameters, and temporal signals into the respective\ntransformers. To train this model, we curate a multi-dimensional dataset\nspanning images, videos, multi-view data and 3D/4D scans, along with a\nmulti-dimensional training strategy. Our approach overcomes the limitations of\nprevious methods based on GAN or UNet-based diffusion models, which struggle\nwith complex motions and viewpoint changes. Through extensive experiments, we\ndemonstrate our method's ability to synthesize realistic, coherent and\nfree-view human videos, paving the way for advanced multimedia applications in\nareas such as virtual reality and animation. Our project website is\nhttps://human4dit.github.io.\n", "link": "http://arxiv.org/abs/2405.17405v1", "date": "2024-05-27", "relevancy": 3.3853, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.679}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.679}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human4DiT%3A%20Free-view%20Human%20Video%20Generation%20with%204D%20Diffusion%0A%20%20Transformer&body=Title%3A%20Human4DiT%3A%20Free-view%20Human%20Video%20Generation%20with%204D%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Ruizhi%20Shao%20and%20Youxin%20Pang%20and%20Zerong%20Zheng%20and%20Jingxiang%20Sun%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20generating%20high-quality%2C%20spatio-temporally%0Acoherent%20human%20videos%20from%20a%20single%20image%20under%20arbitrary%20viewpoints.%20Our%0Aframework%20combines%20the%20strengths%20of%20U-Nets%20for%20accurate%20condition%20injection%20and%0Adiffusion%20transformers%20for%20capturing%20global%20correlations%20across%20viewpoints%20and%0Atime.%20The%20core%20is%20a%20cascaded%204D%20transformer%20architecture%20that%20factorizes%0Aattention%20across%20views%2C%20time%2C%20and%20spatial%20dimensions%2C%20enabling%20efficient%0Amodeling%20of%20the%204D%20space.%20Precise%20conditioning%20is%20achieved%20by%20injecting%20human%0Aidentity%2C%20camera%20parameters%2C%20and%20temporal%20signals%20into%20the%20respective%0Atransformers.%20To%20train%20this%20model%2C%20we%20curate%20a%20multi-dimensional%20dataset%0Aspanning%20images%2C%20videos%2C%20multi-view%20data%20and%203D/4D%20scans%2C%20along%20with%20a%0Amulti-dimensional%20training%20strategy.%20Our%20approach%20overcomes%20the%20limitations%20of%0Aprevious%20methods%20based%20on%20GAN%20or%20UNet-based%20diffusion%20models%2C%20which%20struggle%0Awith%20complex%20motions%20and%20viewpoint%20changes.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20our%20method%27s%20ability%20to%20synthesize%20realistic%2C%20coherent%20and%0Afree-view%20human%20videos%2C%20paving%20the%20way%20for%20advanced%20multimedia%20applications%20in%0Aareas%20such%20as%20virtual%20reality%20and%20animation.%20Our%20project%20website%20is%0Ahttps%3A//human4dit.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman4DiT%253A%2520Free-view%2520Human%2520Video%2520Generation%2520with%25204D%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DRuizhi%2520Shao%2520and%2520Youxin%2520Pang%2520and%2520Zerong%2520Zheng%2520and%2520Jingxiang%2520Sun%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520generating%2520high-quality%252C%2520spatio-temporally%250Acoherent%2520human%2520videos%2520from%2520a%2520single%2520image%2520under%2520arbitrary%2520viewpoints.%2520Our%250Aframework%2520combines%2520the%2520strengths%2520of%2520U-Nets%2520for%2520accurate%2520condition%2520injection%2520and%250Adiffusion%2520transformers%2520for%2520capturing%2520global%2520correlations%2520across%2520viewpoints%2520and%250Atime.%2520The%2520core%2520is%2520a%2520cascaded%25204D%2520transformer%2520architecture%2520that%2520factorizes%250Aattention%2520across%2520views%252C%2520time%252C%2520and%2520spatial%2520dimensions%252C%2520enabling%2520efficient%250Amodeling%2520of%2520the%25204D%2520space.%2520Precise%2520conditioning%2520is%2520achieved%2520by%2520injecting%2520human%250Aidentity%252C%2520camera%2520parameters%252C%2520and%2520temporal%2520signals%2520into%2520the%2520respective%250Atransformers.%2520To%2520train%2520this%2520model%252C%2520we%2520curate%2520a%2520multi-dimensional%2520dataset%250Aspanning%2520images%252C%2520videos%252C%2520multi-view%2520data%2520and%25203D/4D%2520scans%252C%2520along%2520with%2520a%250Amulti-dimensional%2520training%2520strategy.%2520Our%2520approach%2520overcomes%2520the%2520limitations%2520of%250Aprevious%2520methods%2520based%2520on%2520GAN%2520or%2520UNet-based%2520diffusion%2520models%252C%2520which%2520struggle%250Awith%2520complex%2520motions%2520and%2520viewpoint%2520changes.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520our%2520method%2527s%2520ability%2520to%2520synthesize%2520realistic%252C%2520coherent%2520and%250Afree-view%2520human%2520videos%252C%2520paving%2520the%2520way%2520for%2520advanced%2520multimedia%2520applications%2520in%250Aareas%2520such%2520as%2520virtual%2520reality%2520and%2520animation.%2520Our%2520project%2520website%2520is%250Ahttps%253A//human4dit.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human4DiT%3A%20Free-view%20Human%20Video%20Generation%20with%204D%20Diffusion%0A%20%20Transformer&entry.906535625=Ruizhi%20Shao%20and%20Youxin%20Pang%20and%20Zerong%20Zheng%20and%20Jingxiang%20Sun%20and%20Yebin%20Liu&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20generating%20high-quality%2C%20spatio-temporally%0Acoherent%20human%20videos%20from%20a%20single%20image%20under%20arbitrary%20viewpoints.%20Our%0Aframework%20combines%20the%20strengths%20of%20U-Nets%20for%20accurate%20condition%20injection%20and%0Adiffusion%20transformers%20for%20capturing%20global%20correlations%20across%20viewpoints%20and%0Atime.%20The%20core%20is%20a%20cascaded%204D%20transformer%20architecture%20that%20factorizes%0Aattention%20across%20views%2C%20time%2C%20and%20spatial%20dimensions%2C%20enabling%20efficient%0Amodeling%20of%20the%204D%20space.%20Precise%20conditioning%20is%20achieved%20by%20injecting%20human%0Aidentity%2C%20camera%20parameters%2C%20and%20temporal%20signals%20into%20the%20respective%0Atransformers.%20To%20train%20this%20model%2C%20we%20curate%20a%20multi-dimensional%20dataset%0Aspanning%20images%2C%20videos%2C%20multi-view%20data%20and%203D/4D%20scans%2C%20along%20with%20a%0Amulti-dimensional%20training%20strategy.%20Our%20approach%20overcomes%20the%20limitations%20of%0Aprevious%20methods%20based%20on%20GAN%20or%20UNet-based%20diffusion%20models%2C%20which%20struggle%0Awith%20complex%20motions%20and%20viewpoint%20changes.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20our%20method%27s%20ability%20to%20synthesize%20realistic%2C%20coherent%20and%0Afree-view%20human%20videos%2C%20paving%20the%20way%20for%20advanced%20multimedia%20applications%20in%0Aareas%20such%20as%20virtual%20reality%20and%20animation.%20Our%20project%20website%20is%0Ahttps%3A//human4dit.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17405v1&entry.124074799=Read"},
{"title": "FlexiDreamer: Single Image-to-3D Generation with FlexiCubes", "author": "Ruowen Zhao and Zhengyi Wang and Yikai Wang and Zihan Zhou and Jun Zhu", "abstract": "  3D content generation has wide applications in various fields. One of its\ndominant paradigms is by sparse-view reconstruction using multi-view images\ngenerated by diffusion models. However, since directly reconstructing triangle\nmeshes from multi-view images is challenging, most methodologies opt to an\nimplicit representation (such as NeRF) during the sparse-view reconstruction\nand acquire the target mesh by a post-processing extraction. However, the\nimplicit representation takes extensive time to train and the post-extraction\nalso leads to undesirable visual artifacts. In this paper, we propose\nFlexiDreamer, a novel framework that directly reconstructs high-quality meshes\nfrom multi-view generated images. We utilize an advanced gradient-based mesh\noptimization, namely FlexiCubes, for multi-view mesh reconstruction, which\nenables us to generate 3D meshes in an end-to-end manner. To address the\nreconstruction artifacts owing to the inconsistencies from generated images, we\ndesign a hybrid positional encoding scheme to improve the reconstruction\ngeometry and an orientation-aware texture mapping to mitigate surface ghosting.\nTo further enhance the results, we respectively incorporate eikonal and smooth\nregularizations to reduce geometric holes and surface noise. Our approach can\ngenerate high-fidelity 3D meshes in the single image-to-3D downstream task with\napproximately 1 minute, significantly outperforming previous methods.\n", "link": "http://arxiv.org/abs/2404.00987v2", "date": "2024-05-27", "relevancy": 3.3152, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6842}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6842}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiDreamer%3A%20Single%20Image-to-3D%20Generation%20with%20FlexiCubes&body=Title%3A%20FlexiDreamer%3A%20Single%20Image-to-3D%20Generation%20with%20FlexiCubes%0AAuthor%3A%20Ruowen%20Zhao%20and%20Zhengyi%20Wang%20and%20Yikai%20Wang%20and%20Zihan%20Zhou%20and%20Jun%20Zhu%0AAbstract%3A%20%20%203D%20content%20generation%20has%20wide%20applications%20in%20various%20fields.%20One%20of%20its%0Adominant%20paradigms%20is%20by%20sparse-view%20reconstruction%20using%20multi-view%20images%0Agenerated%20by%20diffusion%20models.%20However%2C%20since%20directly%20reconstructing%20triangle%0Ameshes%20from%20multi-view%20images%20is%20challenging%2C%20most%20methodologies%20opt%20to%20an%0Aimplicit%20representation%20%28such%20as%20NeRF%29%20during%20the%20sparse-view%20reconstruction%0Aand%20acquire%20the%20target%20mesh%20by%20a%20post-processing%20extraction.%20However%2C%20the%0Aimplicit%20representation%20takes%20extensive%20time%20to%20train%20and%20the%20post-extraction%0Aalso%20leads%20to%20undesirable%20visual%20artifacts.%20In%20this%20paper%2C%20we%20propose%0AFlexiDreamer%2C%20a%20novel%20framework%20that%20directly%20reconstructs%20high-quality%20meshes%0Afrom%20multi-view%20generated%20images.%20We%20utilize%20an%20advanced%20gradient-based%20mesh%0Aoptimization%2C%20namely%20FlexiCubes%2C%20for%20multi-view%20mesh%20reconstruction%2C%20which%0Aenables%20us%20to%20generate%203D%20meshes%20in%20an%20end-to-end%20manner.%20To%20address%20the%0Areconstruction%20artifacts%20owing%20to%20the%20inconsistencies%20from%20generated%20images%2C%20we%0Adesign%20a%20hybrid%20positional%20encoding%20scheme%20to%20improve%20the%20reconstruction%0Ageometry%20and%20an%20orientation-aware%20texture%20mapping%20to%20mitigate%20surface%20ghosting.%0ATo%20further%20enhance%20the%20results%2C%20we%20respectively%20incorporate%20eikonal%20and%20smooth%0Aregularizations%20to%20reduce%20geometric%20holes%20and%20surface%20noise.%20Our%20approach%20can%0Agenerate%20high-fidelity%203D%20meshes%20in%20the%20single%20image-to-3D%20downstream%20task%20with%0Aapproximately%201%20minute%2C%20significantly%20outperforming%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiDreamer%253A%2520Single%2520Image-to-3D%2520Generation%2520with%2520FlexiCubes%26entry.906535625%3DRuowen%2520Zhao%2520and%2520Zhengyi%2520Wang%2520and%2520Yikai%2520Wang%2520and%2520Zihan%2520Zhou%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%25203D%2520content%2520generation%2520has%2520wide%2520applications%2520in%2520various%2520fields.%2520One%2520of%2520its%250Adominant%2520paradigms%2520is%2520by%2520sparse-view%2520reconstruction%2520using%2520multi-view%2520images%250Agenerated%2520by%2520diffusion%2520models.%2520However%252C%2520since%2520directly%2520reconstructing%2520triangle%250Ameshes%2520from%2520multi-view%2520images%2520is%2520challenging%252C%2520most%2520methodologies%2520opt%2520to%2520an%250Aimplicit%2520representation%2520%2528such%2520as%2520NeRF%2529%2520during%2520the%2520sparse-view%2520reconstruction%250Aand%2520acquire%2520the%2520target%2520mesh%2520by%2520a%2520post-processing%2520extraction.%2520However%252C%2520the%250Aimplicit%2520representation%2520takes%2520extensive%2520time%2520to%2520train%2520and%2520the%2520post-extraction%250Aalso%2520leads%2520to%2520undesirable%2520visual%2520artifacts.%2520In%2520this%2520paper%252C%2520we%2520propose%250AFlexiDreamer%252C%2520a%2520novel%2520framework%2520that%2520directly%2520reconstructs%2520high-quality%2520meshes%250Afrom%2520multi-view%2520generated%2520images.%2520We%2520utilize%2520an%2520advanced%2520gradient-based%2520mesh%250Aoptimization%252C%2520namely%2520FlexiCubes%252C%2520for%2520multi-view%2520mesh%2520reconstruction%252C%2520which%250Aenables%2520us%2520to%2520generate%25203D%2520meshes%2520in%2520an%2520end-to-end%2520manner.%2520To%2520address%2520the%250Areconstruction%2520artifacts%2520owing%2520to%2520the%2520inconsistencies%2520from%2520generated%2520images%252C%2520we%250Adesign%2520a%2520hybrid%2520positional%2520encoding%2520scheme%2520to%2520improve%2520the%2520reconstruction%250Ageometry%2520and%2520an%2520orientation-aware%2520texture%2520mapping%2520to%2520mitigate%2520surface%2520ghosting.%250ATo%2520further%2520enhance%2520the%2520results%252C%2520we%2520respectively%2520incorporate%2520eikonal%2520and%2520smooth%250Aregularizations%2520to%2520reduce%2520geometric%2520holes%2520and%2520surface%2520noise.%2520Our%2520approach%2520can%250Agenerate%2520high-fidelity%25203D%2520meshes%2520in%2520the%2520single%2520image-to-3D%2520downstream%2520task%2520with%250Aapproximately%25201%2520minute%252C%2520significantly%2520outperforming%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiDreamer%3A%20Single%20Image-to-3D%20Generation%20with%20FlexiCubes&entry.906535625=Ruowen%20Zhao%20and%20Zhengyi%20Wang%20and%20Yikai%20Wang%20and%20Zihan%20Zhou%20and%20Jun%20Zhu&entry.1292438233=%20%203D%20content%20generation%20has%20wide%20applications%20in%20various%20fields.%20One%20of%20its%0Adominant%20paradigms%20is%20by%20sparse-view%20reconstruction%20using%20multi-view%20images%0Agenerated%20by%20diffusion%20models.%20However%2C%20since%20directly%20reconstructing%20triangle%0Ameshes%20from%20multi-view%20images%20is%20challenging%2C%20most%20methodologies%20opt%20to%20an%0Aimplicit%20representation%20%28such%20as%20NeRF%29%20during%20the%20sparse-view%20reconstruction%0Aand%20acquire%20the%20target%20mesh%20by%20a%20post-processing%20extraction.%20However%2C%20the%0Aimplicit%20representation%20takes%20extensive%20time%20to%20train%20and%20the%20post-extraction%0Aalso%20leads%20to%20undesirable%20visual%20artifacts.%20In%20this%20paper%2C%20we%20propose%0AFlexiDreamer%2C%20a%20novel%20framework%20that%20directly%20reconstructs%20high-quality%20meshes%0Afrom%20multi-view%20generated%20images.%20We%20utilize%20an%20advanced%20gradient-based%20mesh%0Aoptimization%2C%20namely%20FlexiCubes%2C%20for%20multi-view%20mesh%20reconstruction%2C%20which%0Aenables%20us%20to%20generate%203D%20meshes%20in%20an%20end-to-end%20manner.%20To%20address%20the%0Areconstruction%20artifacts%20owing%20to%20the%20inconsistencies%20from%20generated%20images%2C%20we%0Adesign%20a%20hybrid%20positional%20encoding%20scheme%20to%20improve%20the%20reconstruction%0Ageometry%20and%20an%20orientation-aware%20texture%20mapping%20to%20mitigate%20surface%20ghosting.%0ATo%20further%20enhance%20the%20results%2C%20we%20respectively%20incorporate%20eikonal%20and%20smooth%0Aregularizations%20to%20reduce%20geometric%20holes%20and%20surface%20noise.%20Our%20approach%20can%0Agenerate%20high-fidelity%203D%20meshes%20in%20the%20single%20image-to-3D%20downstream%20task%20with%0Aapproximately%201%20minute%2C%20significantly%20outperforming%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00987v2&entry.124074799=Read"},
{"title": "GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud\n  SLAM", "author": "Ganlin Zhang and Erik Sandstr\u00f6m and Youmin Zhang and Manthan Patel and Luc Van Gool and Martin R. Oswald", "abstract": "  Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode is available at https://github.com/zhangganlin/GlOIRE-SLAM\n", "link": "http://arxiv.org/abs/2403.19549v3", "date": "2024-05-27", "relevancy": 3.2704, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6321}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM&body=Title%3A%20GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM%0AAuthor%3A%20Ganlin%20Zhang%20and%20Erik%20Sandstr%C3%B6m%20and%20Youmin%20Zhang%20and%20Manthan%20Patel%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Recent%20advancements%20in%20RGB-only%20dense%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20have%20predominantly%20utilized%20grid-based%20neural%20implicit%20encodings%20and/or%0Astruggle%20to%20efficiently%20realize%20global%20map%20and%20pose%20consistency.%20To%20this%20end%2C%0Awe%20propose%20an%20efficient%20RGB-only%20dense%20SLAM%20system%20using%20a%20flexible%20neural%0Apoint%20cloud%20scene%20representation%20that%20adapts%20to%20keyframe%20poses%20and%20depth%0Aupdates%2C%20without%20needing%20costly%20backpropagation.%20Another%20critical%20challenge%20of%0ARGB-only%20SLAM%20is%20the%20lack%20of%20geometric%20priors.%20To%20alleviate%20this%20issue%2C%20with%0Athe%20aid%20of%20a%20monocular%20depth%20estimator%2C%20we%20introduce%20a%20novel%20DSPO%20layer%20for%0Abundle%20adjustment%20which%20optimizes%20the%20pose%20and%20depth%20of%20keyframes%20along%20with%0Athe%20scale%20of%20the%20monocular%20depth.%20Finally%2C%20our%20system%20benefits%20from%20loop%0Aclosure%20and%20online%20global%20bundle%20adjustment%20and%20performs%20either%20better%20or%0Acompetitive%20to%20existing%20dense%20neural%20RGB%20SLAM%20methods%20in%20tracking%2C%20mapping%20and%0Arendering%20accuracy%20on%20the%20Replica%2C%20TUM-RGBD%20and%20ScanNet%20datasets.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/zhangganlin/GlOIRE-SLAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19549v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlORIE-SLAM%253A%2520Globally%2520Optimized%2520RGB-only%2520Implicit%2520Encoding%2520Point%2520Cloud%250A%2520%2520SLAM%26entry.906535625%3DGanlin%2520Zhang%2520and%2520Erik%2520Sandstr%25C3%25B6m%2520and%2520Youmin%2520Zhang%2520and%2520Manthan%2520Patel%2520and%2520Luc%2520Van%2520Gool%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520RGB-only%2520dense%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%2520have%2520predominantly%2520utilized%2520grid-based%2520neural%2520implicit%2520encodings%2520and/or%250Astruggle%2520to%2520efficiently%2520realize%2520global%2520map%2520and%2520pose%2520consistency.%2520To%2520this%2520end%252C%250Awe%2520propose%2520an%2520efficient%2520RGB-only%2520dense%2520SLAM%2520system%2520using%2520a%2520flexible%2520neural%250Apoint%2520cloud%2520scene%2520representation%2520that%2520adapts%2520to%2520keyframe%2520poses%2520and%2520depth%250Aupdates%252C%2520without%2520needing%2520costly%2520backpropagation.%2520Another%2520critical%2520challenge%2520of%250ARGB-only%2520SLAM%2520is%2520the%2520lack%2520of%2520geometric%2520priors.%2520To%2520alleviate%2520this%2520issue%252C%2520with%250Athe%2520aid%2520of%2520a%2520monocular%2520depth%2520estimator%252C%2520we%2520introduce%2520a%2520novel%2520DSPO%2520layer%2520for%250Abundle%2520adjustment%2520which%2520optimizes%2520the%2520pose%2520and%2520depth%2520of%2520keyframes%2520along%2520with%250Athe%2520scale%2520of%2520the%2520monocular%2520depth.%2520Finally%252C%2520our%2520system%2520benefits%2520from%2520loop%250Aclosure%2520and%2520online%2520global%2520bundle%2520adjustment%2520and%2520performs%2520either%2520better%2520or%250Acompetitive%2520to%2520existing%2520dense%2520neural%2520RGB%2520SLAM%2520methods%2520in%2520tracking%252C%2520mapping%2520and%250Arendering%2520accuracy%2520on%2520the%2520Replica%252C%2520TUM-RGBD%2520and%2520ScanNet%2520datasets.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/zhangganlin/GlOIRE-SLAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19549v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM&entry.906535625=Ganlin%20Zhang%20and%20Erik%20Sandstr%C3%B6m%20and%20Youmin%20Zhang%20and%20Manthan%20Patel%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Recent%20advancements%20in%20RGB-only%20dense%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20have%20predominantly%20utilized%20grid-based%20neural%20implicit%20encodings%20and/or%0Astruggle%20to%20efficiently%20realize%20global%20map%20and%20pose%20consistency.%20To%20this%20end%2C%0Awe%20propose%20an%20efficient%20RGB-only%20dense%20SLAM%20system%20using%20a%20flexible%20neural%0Apoint%20cloud%20scene%20representation%20that%20adapts%20to%20keyframe%20poses%20and%20depth%0Aupdates%2C%20without%20needing%20costly%20backpropagation.%20Another%20critical%20challenge%20of%0ARGB-only%20SLAM%20is%20the%20lack%20of%20geometric%20priors.%20To%20alleviate%20this%20issue%2C%20with%0Athe%20aid%20of%20a%20monocular%20depth%20estimator%2C%20we%20introduce%20a%20novel%20DSPO%20layer%20for%0Abundle%20adjustment%20which%20optimizes%20the%20pose%20and%20depth%20of%20keyframes%20along%20with%0Athe%20scale%20of%20the%20monocular%20depth.%20Finally%2C%20our%20system%20benefits%20from%20loop%0Aclosure%20and%20online%20global%20bundle%20adjustment%20and%20performs%20either%20better%20or%0Acompetitive%20to%20existing%20dense%20neural%20RGB%20SLAM%20methods%20in%20tracking%2C%20mapping%20and%0Arendering%20accuracy%20on%20the%20Replica%2C%20TUM-RGBD%20and%20ScanNet%20datasets.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/zhangganlin/GlOIRE-SLAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19549v3&entry.124074799=Read"},
{"title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion\n  Scaffolds", "author": "Jiahui Lei and Yijia Weng and Adam Harley and Leonidas Guibas and Kostas Daniilidis", "abstract": "  We introduce 4D Motion Scaffolds (MoSca), a neural information processing\nsystem designed to reconstruct and synthesize novel views of dynamic scenes\nfrom monocular videos captured casually in the wild. To address such a\nchallenging and ill-posed inverse problem, we leverage prior knowledge from\nfoundational vision models, lift the video data to a novel Motion Scaffold\n(MoSca) representation, which compactly and smoothly encodes the underlying\nmotions / deformations. The scene geometry and appearance are then disentangled\nfrom the deformation field, and are encoded by globally fusing the Gaussians\nanchored onto the MoSca and optimized via Gaussian Splatting. Additionally,\ncamera poses can be seamlessly initialized and refined during the dynamic\nrendering process, without the need for other pose estimation tools.\nExperiments demonstrate state-of-the-art performance on dynamic rendering\nbenchmarks.\n", "link": "http://arxiv.org/abs/2405.17421v1", "date": "2024-05-27", "relevancy": 3.1906, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7098}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6207}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds&body=Title%3A%20MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds%0AAuthor%3A%20Jiahui%20Lei%20and%20Yijia%20Weng%20and%20Adam%20Harley%20and%20Leonidas%20Guibas%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20We%20introduce%204D%20Motion%20Scaffolds%20%28MoSca%29%2C%20a%20neural%20information%20processing%0Asystem%20designed%20to%20reconstruct%20and%20synthesize%20novel%20views%20of%20dynamic%20scenes%0Afrom%20monocular%20videos%20captured%20casually%20in%20the%20wild.%20To%20address%20such%20a%0Achallenging%20and%20ill-posed%20inverse%20problem%2C%20we%20leverage%20prior%20knowledge%20from%0Afoundational%20vision%20models%2C%20lift%20the%20video%20data%20to%20a%20novel%20Motion%20Scaffold%0A%28MoSca%29%20representation%2C%20which%20compactly%20and%20smoothly%20encodes%20the%20underlying%0Amotions%20/%20deformations.%20The%20scene%20geometry%20and%20appearance%20are%20then%20disentangled%0Afrom%20the%20deformation%20field%2C%20and%20are%20encoded%20by%20globally%20fusing%20the%20Gaussians%0Aanchored%20onto%20the%20MoSca%20and%20optimized%20via%20Gaussian%20Splatting.%20Additionally%2C%0Acamera%20poses%20can%20be%20seamlessly%20initialized%20and%20refined%20during%20the%20dynamic%0Arendering%20process%2C%20without%20the%20need%20for%20other%20pose%20estimation%20tools.%0AExperiments%20demonstrate%20state-of-the-art%20performance%20on%20dynamic%20rendering%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSca%253A%2520Dynamic%2520Gaussian%2520Fusion%2520from%2520Casual%2520Videos%2520via%25204D%2520Motion%250A%2520%2520Scaffolds%26entry.906535625%3DJiahui%2520Lei%2520and%2520Yijia%2520Weng%2520and%2520Adam%2520Harley%2520and%2520Leonidas%2520Guibas%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520We%2520introduce%25204D%2520Motion%2520Scaffolds%2520%2528MoSca%2529%252C%2520a%2520neural%2520information%2520processing%250Asystem%2520designed%2520to%2520reconstruct%2520and%2520synthesize%2520novel%2520views%2520of%2520dynamic%2520scenes%250Afrom%2520monocular%2520videos%2520captured%2520casually%2520in%2520the%2520wild.%2520To%2520address%2520such%2520a%250Achallenging%2520and%2520ill-posed%2520inverse%2520problem%252C%2520we%2520leverage%2520prior%2520knowledge%2520from%250Afoundational%2520vision%2520models%252C%2520lift%2520the%2520video%2520data%2520to%2520a%2520novel%2520Motion%2520Scaffold%250A%2528MoSca%2529%2520representation%252C%2520which%2520compactly%2520and%2520smoothly%2520encodes%2520the%2520underlying%250Amotions%2520/%2520deformations.%2520The%2520scene%2520geometry%2520and%2520appearance%2520are%2520then%2520disentangled%250Afrom%2520the%2520deformation%2520field%252C%2520and%2520are%2520encoded%2520by%2520globally%2520fusing%2520the%2520Gaussians%250Aanchored%2520onto%2520the%2520MoSca%2520and%2520optimized%2520via%2520Gaussian%2520Splatting.%2520Additionally%252C%250Acamera%2520poses%2520can%2520be%2520seamlessly%2520initialized%2520and%2520refined%2520during%2520the%2520dynamic%250Arendering%2520process%252C%2520without%2520the%2520need%2520for%2520other%2520pose%2520estimation%2520tools.%250AExperiments%2520demonstrate%2520state-of-the-art%2520performance%2520on%2520dynamic%2520rendering%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds&entry.906535625=Jiahui%20Lei%20and%20Yijia%20Weng%20and%20Adam%20Harley%20and%20Leonidas%20Guibas%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20We%20introduce%204D%20Motion%20Scaffolds%20%28MoSca%29%2C%20a%20neural%20information%20processing%0Asystem%20designed%20to%20reconstruct%20and%20synthesize%20novel%20views%20of%20dynamic%20scenes%0Afrom%20monocular%20videos%20captured%20casually%20in%20the%20wild.%20To%20address%20such%20a%0Achallenging%20and%20ill-posed%20inverse%20problem%2C%20we%20leverage%20prior%20knowledge%20from%0Afoundational%20vision%20models%2C%20lift%20the%20video%20data%20to%20a%20novel%20Motion%20Scaffold%0A%28MoSca%29%20representation%2C%20which%20compactly%20and%20smoothly%20encodes%20the%20underlying%0Amotions%20/%20deformations.%20The%20scene%20geometry%20and%20appearance%20are%20then%20disentangled%0Afrom%20the%20deformation%20field%2C%20and%20are%20encoded%20by%20globally%20fusing%20the%20Gaussians%0Aanchored%20onto%20the%20MoSca%20and%20optimized%20via%20Gaussian%20Splatting.%20Additionally%2C%0Acamera%20poses%20can%20be%20seamlessly%20initialized%20and%20refined%20during%20the%20dynamic%0Arendering%20process%2C%20without%20the%20need%20for%20other%20pose%20estimation%20tools.%0AExperiments%20demonstrate%20state-of-the-art%20performance%20on%20dynamic%20rendering%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17421v1&entry.124074799=Read"},
{"title": "F-3DGS: Factorized Coordinates and Representations for 3D Gaussian\n  Splatting", "author": "Xiangyu Sun and Joo Chan Lee and Daniel Rho and Jong Hwan Ko and Usman Ali and Eunbyung Park", "abstract": "  The neural radiance field (NeRF) has made significant strides in representing\n3D scenes and synthesizing novel views. Despite its advancements, the high\ncomputational costs of NeRF have posed challenges for its deployment in\nresource-constrained environments and real-time applications. As an alternative\nto NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers\nrapid rendering speeds while maintaining excellent image quality. However, as\nit represents objects and scenes using a myriad of Gaussians, it requires\nsubstantial storage to achieve high-quality representation. To mitigate the\nstorage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel\napproach that drastically reduces storage requirements while preserving image\nquality. Inspired by classical matrix and tensor factorization techniques, our\nmethod represents and approximates dense clusters of Gaussians with\nsignificantly fewer Gaussians through efficient factorization. We aim to\nefficiently represent dense 3D Gaussians by approximating them with a limited\namount of information for each axis and their combinations. This method allows\nus to encode a substantially large number of Gaussians along with their\nessential attributes -- such as color, scale, and rotation -- necessary for\nrendering using a relatively small number of elements. Extensive experimental\nresults demonstrate that F-3DGS achieves a significant reduction in storage\ncosts while maintaining comparable quality in rendered images.\n", "link": "http://arxiv.org/abs/2405.17083v1", "date": "2024-05-27", "relevancy": 3.1854, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7073}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6228}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Xiangyu%20Sun%20and%20Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Jong%20Hwan%20Ko%20and%20Usman%20Ali%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20The%20neural%20radiance%20field%20%28NeRF%29%20has%20made%20significant%20strides%20in%20representing%0A3D%20scenes%20and%20synthesizing%20novel%20views.%20Despite%20its%20advancements%2C%20the%20high%0Acomputational%20costs%20of%20NeRF%20have%20posed%20challenges%20for%20its%20deployment%20in%0Aresource-constrained%20environments%20and%20real-time%20applications.%20As%20an%20alternative%0Ato%20NeRF-like%20neural%20rendering%20methods%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%0Arapid%20rendering%20speeds%20while%20maintaining%20excellent%20image%20quality.%20However%2C%20as%0Ait%20represents%20objects%20and%20scenes%20using%20a%20myriad%20of%20Gaussians%2C%20it%20requires%0Asubstantial%20storage%20to%20achieve%20high-quality%20representation.%20To%20mitigate%20the%0Astorage%20overhead%2C%20we%20propose%20Factorized%203D%20Gaussian%20Splatting%20%28F-3DGS%29%2C%20a%20novel%0Aapproach%20that%20drastically%20reduces%20storage%20requirements%20while%20preserving%20image%0Aquality.%20Inspired%20by%20classical%20matrix%20and%20tensor%20factorization%20techniques%2C%20our%0Amethod%20represents%20and%20approximates%20dense%20clusters%20of%20Gaussians%20with%0Asignificantly%20fewer%20Gaussians%20through%20efficient%20factorization.%20We%20aim%20to%0Aefficiently%20represent%20dense%203D%20Gaussians%20by%20approximating%20them%20with%20a%20limited%0Aamount%20of%20information%20for%20each%20axis%20and%20their%20combinations.%20This%20method%20allows%0Aus%20to%20encode%20a%20substantially%20large%20number%20of%20Gaussians%20along%20with%20their%0Aessential%20attributes%20--%20such%20as%20color%2C%20scale%2C%20and%20rotation%20--%20necessary%20for%0Arendering%20using%20a%20relatively%20small%20number%20of%20elements.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20F-3DGS%20achieves%20a%20significant%20reduction%20in%20storage%0Acosts%20while%20maintaining%20comparable%20quality%20in%20rendered%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DF-3DGS%253A%2520Factorized%2520Coordinates%2520and%2520Representations%2520for%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Joo%2520Chan%2520Lee%2520and%2520Daniel%2520Rho%2520and%2520Jong%2520Hwan%2520Ko%2520and%2520Usman%2520Ali%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520The%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520has%2520made%2520significant%2520strides%2520in%2520representing%250A3D%2520scenes%2520and%2520synthesizing%2520novel%2520views.%2520Despite%2520its%2520advancements%252C%2520the%2520high%250Acomputational%2520costs%2520of%2520NeRF%2520have%2520posed%2520challenges%2520for%2520its%2520deployment%2520in%250Aresource-constrained%2520environments%2520and%2520real-time%2520applications.%2520As%2520an%2520alternative%250Ato%2520NeRF-like%2520neural%2520rendering%2520methods%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520offers%250Arapid%2520rendering%2520speeds%2520while%2520maintaining%2520excellent%2520image%2520quality.%2520However%252C%2520as%250Ait%2520represents%2520objects%2520and%2520scenes%2520using%2520a%2520myriad%2520of%2520Gaussians%252C%2520it%2520requires%250Asubstantial%2520storage%2520to%2520achieve%2520high-quality%2520representation.%2520To%2520mitigate%2520the%250Astorage%2520overhead%252C%2520we%2520propose%2520Factorized%25203D%2520Gaussian%2520Splatting%2520%2528F-3DGS%2529%252C%2520a%2520novel%250Aapproach%2520that%2520drastically%2520reduces%2520storage%2520requirements%2520while%2520preserving%2520image%250Aquality.%2520Inspired%2520by%2520classical%2520matrix%2520and%2520tensor%2520factorization%2520techniques%252C%2520our%250Amethod%2520represents%2520and%2520approximates%2520dense%2520clusters%2520of%2520Gaussians%2520with%250Asignificantly%2520fewer%2520Gaussians%2520through%2520efficient%2520factorization.%2520We%2520aim%2520to%250Aefficiently%2520represent%2520dense%25203D%2520Gaussians%2520by%2520approximating%2520them%2520with%2520a%2520limited%250Aamount%2520of%2520information%2520for%2520each%2520axis%2520and%2520their%2520combinations.%2520This%2520method%2520allows%250Aus%2520to%2520encode%2520a%2520substantially%2520large%2520number%2520of%2520Gaussians%2520along%2520with%2520their%250Aessential%2520attributes%2520--%2520such%2520as%2520color%252C%2520scale%252C%2520and%2520rotation%2520--%2520necessary%2520for%250Arendering%2520using%2520a%2520relatively%2520small%2520number%2520of%2520elements.%2520Extensive%2520experimental%250Aresults%2520demonstrate%2520that%2520F-3DGS%2520achieves%2520a%2520significant%2520reduction%2520in%2520storage%250Acosts%2520while%2520maintaining%2520comparable%2520quality%2520in%2520rendered%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=F-3DGS%3A%20Factorized%20Coordinates%20and%20Representations%20for%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Xiangyu%20Sun%20and%20Joo%20Chan%20Lee%20and%20Daniel%20Rho%20and%20Jong%20Hwan%20Ko%20and%20Usman%20Ali%20and%20Eunbyung%20Park&entry.1292438233=%20%20The%20neural%20radiance%20field%20%28NeRF%29%20has%20made%20significant%20strides%20in%20representing%0A3D%20scenes%20and%20synthesizing%20novel%20views.%20Despite%20its%20advancements%2C%20the%20high%0Acomputational%20costs%20of%20NeRF%20have%20posed%20challenges%20for%20its%20deployment%20in%0Aresource-constrained%20environments%20and%20real-time%20applications.%20As%20an%20alternative%0Ato%20NeRF-like%20neural%20rendering%20methods%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20offers%0Arapid%20rendering%20speeds%20while%20maintaining%20excellent%20image%20quality.%20However%2C%20as%0Ait%20represents%20objects%20and%20scenes%20using%20a%20myriad%20of%20Gaussians%2C%20it%20requires%0Asubstantial%20storage%20to%20achieve%20high-quality%20representation.%20To%20mitigate%20the%0Astorage%20overhead%2C%20we%20propose%20Factorized%203D%20Gaussian%20Splatting%20%28F-3DGS%29%2C%20a%20novel%0Aapproach%20that%20drastically%20reduces%20storage%20requirements%20while%20preserving%20image%0Aquality.%20Inspired%20by%20classical%20matrix%20and%20tensor%20factorization%20techniques%2C%20our%0Amethod%20represents%20and%20approximates%20dense%20clusters%20of%20Gaussians%20with%0Asignificantly%20fewer%20Gaussians%20through%20efficient%20factorization.%20We%20aim%20to%0Aefficiently%20represent%20dense%203D%20Gaussians%20by%20approximating%20them%20with%20a%20limited%0Aamount%20of%20information%20for%20each%20axis%20and%20their%20combinations.%20This%20method%20allows%0Aus%20to%20encode%20a%20substantially%20large%20number%20of%20Gaussians%20along%20with%20their%0Aessential%20attributes%20--%20such%20as%20color%2C%20scale%2C%20and%20rotation%20--%20necessary%20for%0Arendering%20using%20a%20relatively%20small%20number%20of%20elements.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20F-3DGS%20achieves%20a%20significant%20reduction%20in%20storage%0Acosts%20while%20maintaining%20comparable%20quality%20in%20rendered%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17083v1&entry.124074799=Read"},
{"title": "Memorize What Matters: Emergent Scene Decomposition from Multitraverse", "author": "Yiming Li and Zehong Wang and Yue Wang and Zhiding Yu and Zan Gojcic and Marco Pavone and Chen Feng and Jose M. Alvarez", "abstract": "  Humans naturally retain memories of permanent elements, while ephemeral\nmoments often slip through the cracks of memory. This selective retention is\ncrucial for robotic perception, localization, and mapping. To endow robots with\nthis capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised,\ncamera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM\nconverts multitraverse RGB videos from the same region into a Gaussian-based\nenvironmental map while concurrently performing 2D ephemeral object\nsegmentation. Our key observation is that the environment remains consistent\nacross traversals, while objects frequently change. This allows us to exploit\nself-supervision from repeated traversals to achieve environment-object\ndecomposition. More specifically, 3DGM formulates multitraverse environmental\nmapping as a robust differentiable rendering problem, treating pixels of the\nenvironment and objects as inliers and outliers, respectively. Using robust\nfeature distillation, feature residuals mining, and robust optimization, 3DGM\njointly performs 3D mapping and 2D segmentation without human intervention. We\nbuild the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets,\nto evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and\nneural rendering. Extensive results verify the effectiveness and potential of\nour method for self-driving and robotics.\n", "link": "http://arxiv.org/abs/2405.17187v1", "date": "2024-05-27", "relevancy": 3.11, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6415}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6219}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorize%20What%20Matters%3A%20Emergent%20Scene%20Decomposition%20from%20Multitraverse&body=Title%3A%20Memorize%20What%20Matters%3A%20Emergent%20Scene%20Decomposition%20from%20Multitraverse%0AAuthor%3A%20Yiming%20Li%20and%20Zehong%20Wang%20and%20Yue%20Wang%20and%20Zhiding%20Yu%20and%20Zan%20Gojcic%20and%20Marco%20Pavone%20and%20Chen%20Feng%20and%20Jose%20M.%20Alvarez%0AAbstract%3A%20%20%20Humans%20naturally%20retain%20memories%20of%20permanent%20elements%2C%20while%20ephemeral%0Amoments%20often%20slip%20through%20the%20cracks%20of%20memory.%20This%20selective%20retention%20is%0Acrucial%20for%20robotic%20perception%2C%20localization%2C%20and%20mapping.%20To%20endow%20robots%20with%0Athis%20capability%2C%20we%20introduce%203D%20Gaussian%20Mapping%20%283DGM%29%2C%20a%20self-supervised%2C%0Acamera-only%20offline%20mapping%20framework%20grounded%20in%203D%20Gaussian%20Splatting.%203DGM%0Aconverts%20multitraverse%20RGB%20videos%20from%20the%20same%20region%20into%20a%20Gaussian-based%0Aenvironmental%20map%20while%20concurrently%20performing%202D%20ephemeral%20object%0Asegmentation.%20Our%20key%20observation%20is%20that%20the%20environment%20remains%20consistent%0Aacross%20traversals%2C%20while%20objects%20frequently%20change.%20This%20allows%20us%20to%20exploit%0Aself-supervision%20from%20repeated%20traversals%20to%20achieve%20environment-object%0Adecomposition.%20More%20specifically%2C%203DGM%20formulates%20multitraverse%20environmental%0Amapping%20as%20a%20robust%20differentiable%20rendering%20problem%2C%20treating%20pixels%20of%20the%0Aenvironment%20and%20objects%20as%20inliers%20and%20outliers%2C%20respectively.%20Using%20robust%0Afeature%20distillation%2C%20feature%20residuals%20mining%2C%20and%20robust%20optimization%2C%203DGM%0Ajointly%20performs%203D%20mapping%20and%202D%20segmentation%20without%20human%20intervention.%20We%0Abuild%20the%20Mapverse%20benchmark%2C%20sourced%20from%20the%20Ithaca365%20and%20nuPlan%20datasets%2C%0Ato%20evaluate%20our%20method%20in%20unsupervised%202D%20segmentation%2C%203D%20reconstruction%2C%20and%0Aneural%20rendering.%20Extensive%20results%20verify%20the%20effectiveness%20and%20potential%20of%0Aour%20method%20for%20self-driving%20and%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorize%2520What%2520Matters%253A%2520Emergent%2520Scene%2520Decomposition%2520from%2520Multitraverse%26entry.906535625%3DYiming%2520Li%2520and%2520Zehong%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Zhiding%2520Yu%2520and%2520Zan%2520Gojcic%2520and%2520Marco%2520Pavone%2520and%2520Chen%2520Feng%2520and%2520Jose%2520M.%2520Alvarez%26entry.1292438233%3D%2520%2520Humans%2520naturally%2520retain%2520memories%2520of%2520permanent%2520elements%252C%2520while%2520ephemeral%250Amoments%2520often%2520slip%2520through%2520the%2520cracks%2520of%2520memory.%2520This%2520selective%2520retention%2520is%250Acrucial%2520for%2520robotic%2520perception%252C%2520localization%252C%2520and%2520mapping.%2520To%2520endow%2520robots%2520with%250Athis%2520capability%252C%2520we%2520introduce%25203D%2520Gaussian%2520Mapping%2520%25283DGM%2529%252C%2520a%2520self-supervised%252C%250Acamera-only%2520offline%2520mapping%2520framework%2520grounded%2520in%25203D%2520Gaussian%2520Splatting.%25203DGM%250Aconverts%2520multitraverse%2520RGB%2520videos%2520from%2520the%2520same%2520region%2520into%2520a%2520Gaussian-based%250Aenvironmental%2520map%2520while%2520concurrently%2520performing%25202D%2520ephemeral%2520object%250Asegmentation.%2520Our%2520key%2520observation%2520is%2520that%2520the%2520environment%2520remains%2520consistent%250Aacross%2520traversals%252C%2520while%2520objects%2520frequently%2520change.%2520This%2520allows%2520us%2520to%2520exploit%250Aself-supervision%2520from%2520repeated%2520traversals%2520to%2520achieve%2520environment-object%250Adecomposition.%2520More%2520specifically%252C%25203DGM%2520formulates%2520multitraverse%2520environmental%250Amapping%2520as%2520a%2520robust%2520differentiable%2520rendering%2520problem%252C%2520treating%2520pixels%2520of%2520the%250Aenvironment%2520and%2520objects%2520as%2520inliers%2520and%2520outliers%252C%2520respectively.%2520Using%2520robust%250Afeature%2520distillation%252C%2520feature%2520residuals%2520mining%252C%2520and%2520robust%2520optimization%252C%25203DGM%250Ajointly%2520performs%25203D%2520mapping%2520and%25202D%2520segmentation%2520without%2520human%2520intervention.%2520We%250Abuild%2520the%2520Mapverse%2520benchmark%252C%2520sourced%2520from%2520the%2520Ithaca365%2520and%2520nuPlan%2520datasets%252C%250Ato%2520evaluate%2520our%2520method%2520in%2520unsupervised%25202D%2520segmentation%252C%25203D%2520reconstruction%252C%2520and%250Aneural%2520rendering.%2520Extensive%2520results%2520verify%2520the%2520effectiveness%2520and%2520potential%2520of%250Aour%2520method%2520for%2520self-driving%2520and%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorize%20What%20Matters%3A%20Emergent%20Scene%20Decomposition%20from%20Multitraverse&entry.906535625=Yiming%20Li%20and%20Zehong%20Wang%20and%20Yue%20Wang%20and%20Zhiding%20Yu%20and%20Zan%20Gojcic%20and%20Marco%20Pavone%20and%20Chen%20Feng%20and%20Jose%20M.%20Alvarez&entry.1292438233=%20%20Humans%20naturally%20retain%20memories%20of%20permanent%20elements%2C%20while%20ephemeral%0Amoments%20often%20slip%20through%20the%20cracks%20of%20memory.%20This%20selective%20retention%20is%0Acrucial%20for%20robotic%20perception%2C%20localization%2C%20and%20mapping.%20To%20endow%20robots%20with%0Athis%20capability%2C%20we%20introduce%203D%20Gaussian%20Mapping%20%283DGM%29%2C%20a%20self-supervised%2C%0Acamera-only%20offline%20mapping%20framework%20grounded%20in%203D%20Gaussian%20Splatting.%203DGM%0Aconverts%20multitraverse%20RGB%20videos%20from%20the%20same%20region%20into%20a%20Gaussian-based%0Aenvironmental%20map%20while%20concurrently%20performing%202D%20ephemeral%20object%0Asegmentation.%20Our%20key%20observation%20is%20that%20the%20environment%20remains%20consistent%0Aacross%20traversals%2C%20while%20objects%20frequently%20change.%20This%20allows%20us%20to%20exploit%0Aself-supervision%20from%20repeated%20traversals%20to%20achieve%20environment-object%0Adecomposition.%20More%20specifically%2C%203DGM%20formulates%20multitraverse%20environmental%0Amapping%20as%20a%20robust%20differentiable%20rendering%20problem%2C%20treating%20pixels%20of%20the%0Aenvironment%20and%20objects%20as%20inliers%20and%20outliers%2C%20respectively.%20Using%20robust%0Afeature%20distillation%2C%20feature%20residuals%20mining%2C%20and%20robust%20optimization%2C%203DGM%0Ajointly%20performs%203D%20mapping%20and%202D%20segmentation%20without%20human%20intervention.%20We%0Abuild%20the%20Mapverse%20benchmark%2C%20sourced%20from%20the%20Ithaca365%20and%20nuPlan%20datasets%2C%0Ato%20evaluate%20our%20method%20in%20unsupervised%202D%20segmentation%2C%203D%20reconstruction%2C%20and%0Aneural%20rendering.%20Extensive%20results%20verify%20the%20effectiveness%20and%20potential%20of%0Aour%20method%20for%20self-driving%20and%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17187v1&entry.124074799=Read"},
{"title": "DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for\n  Refocusing,Defocus Rendering and Blur Removal", "author": "Yujie Wang and Praneeth Chakravarthula and Baoquan Chen", "abstract": "  3D Gaussian Splatting-based techniques have recently advanced 3D scene\nreconstruction and novel view synthesis, achieving high-quality real-time\nrendering. However, these approaches are inherently limited by the underlying\npinhole camera assumption in modeling the images and hence only work for\nAll-in-Focus (AiF) sharp image inputs. This severely affects their\napplicability in real-world scenarios where images often exhibit defocus blur\ndue to the limited depth-of-field (DOF) of imaging devices. Additionally,\nexisting 3D Gaussian Splatting (3DGS) methods also do not support rendering of\nDOF effects.\n  To address these challenges, we introduce DOF-GS that allows for rendering\nadjustable DOF effects, removing defocus blur as well as refocusing of 3D\nscenes, all from multi-view images degraded by defocus blur. To this end, we\nre-imagine the traditional Gaussian Splatting pipeline by employing a finite\naperture camera model coupled with explicit, differentiable defocus rendering\nguided by the Circle-of-Confusion (CoC). The proposed framework provides for\ndynamic adjustment of DOF effects by changing the aperture and focal distance\nof the underlying camera model on-demand. It also enables rendering varying DOF\neffects of 3D scenes post-optimization, and generating AiF images from\ndefocused training images. Furthermore, we devise a joint optimization strategy\nto further enhance details in the reconstructed scenes by jointly optimizing\nrendered defocused and AiF images. Our experimental results indicate that\nDOF-GS produces high-quality sharp all-in-focus renderings conditioned on\ninputs compromised by defocus blur, with the training process incurring only a\nmodest increase in GPU memory consumption. We further demonstrate the\napplications of the proposed method for adjustable defocus rendering and\nrefocusing of the 3D scene from input images degraded by defocus blur.\n", "link": "http://arxiv.org/abs/2405.17351v1", "date": "2024-05-27", "relevancy": 3.0338, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6511}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOF-GS%3A%20Adjustable%20Depth-of-Field%203D%20Gaussian%20Splatting%20for%0A%20%20Refocusing%2CDefocus%20Rendering%20and%20Blur%20Removal&body=Title%3A%20DOF-GS%3A%20Adjustable%20Depth-of-Field%203D%20Gaussian%20Splatting%20for%0A%20%20Refocusing%2CDefocus%20Rendering%20and%20Blur%20Removal%0AAuthor%3A%20Yujie%20Wang%20and%20Praneeth%20Chakravarthula%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting-based%20techniques%20have%20recently%20advanced%203D%20scene%0Areconstruction%20and%20novel%20view%20synthesis%2C%20achieving%20high-quality%20real-time%0Arendering.%20However%2C%20these%20approaches%20are%20inherently%20limited%20by%20the%20underlying%0Apinhole%20camera%20assumption%20in%20modeling%20the%20images%20and%20hence%20only%20work%20for%0AAll-in-Focus%20%28AiF%29%20sharp%20image%20inputs.%20This%20severely%20affects%20their%0Aapplicability%20in%20real-world%20scenarios%20where%20images%20often%20exhibit%20defocus%20blur%0Adue%20to%20the%20limited%20depth-of-field%20%28DOF%29%20of%20imaging%20devices.%20Additionally%2C%0Aexisting%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%20also%20do%20not%20support%20rendering%20of%0ADOF%20effects.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20DOF-GS%20that%20allows%20for%20rendering%0Aadjustable%20DOF%20effects%2C%20removing%20defocus%20blur%20as%20well%20as%20refocusing%20of%203D%0Ascenes%2C%20all%20from%20multi-view%20images%20degraded%20by%20defocus%20blur.%20To%20this%20end%2C%20we%0Are-imagine%20the%20traditional%20Gaussian%20Splatting%20pipeline%20by%20employing%20a%20finite%0Aaperture%20camera%20model%20coupled%20with%20explicit%2C%20differentiable%20defocus%20rendering%0Aguided%20by%20the%20Circle-of-Confusion%20%28CoC%29.%20The%20proposed%20framework%20provides%20for%0Adynamic%20adjustment%20of%20DOF%20effects%20by%20changing%20the%20aperture%20and%20focal%20distance%0Aof%20the%20underlying%20camera%20model%20on-demand.%20It%20also%20enables%20rendering%20varying%20DOF%0Aeffects%20of%203D%20scenes%20post-optimization%2C%20and%20generating%20AiF%20images%20from%0Adefocused%20training%20images.%20Furthermore%2C%20we%20devise%20a%20joint%20optimization%20strategy%0Ato%20further%20enhance%20details%20in%20the%20reconstructed%20scenes%20by%20jointly%20optimizing%0Arendered%20defocused%20and%20AiF%20images.%20Our%20experimental%20results%20indicate%20that%0ADOF-GS%20produces%20high-quality%20sharp%20all-in-focus%20renderings%20conditioned%20on%0Ainputs%20compromised%20by%20defocus%20blur%2C%20with%20the%20training%20process%20incurring%20only%20a%0Amodest%20increase%20in%20GPU%20memory%20consumption.%20We%20further%20demonstrate%20the%0Aapplications%20of%20the%20proposed%20method%20for%20adjustable%20defocus%20rendering%20and%0Arefocusing%20of%20the%203D%20scene%20from%20input%20images%20degraded%20by%20defocus%20blur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17351v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOF-GS%253A%2520Adjustable%2520Depth-of-Field%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520Refocusing%252CDefocus%2520Rendering%2520and%2520Blur%2520Removal%26entry.906535625%3DYujie%2520Wang%2520and%2520Praneeth%2520Chakravarthula%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting-based%2520techniques%2520have%2520recently%2520advanced%25203D%2520scene%250Areconstruction%2520and%2520novel%2520view%2520synthesis%252C%2520achieving%2520high-quality%2520real-time%250Arendering.%2520However%252C%2520these%2520approaches%2520are%2520inherently%2520limited%2520by%2520the%2520underlying%250Apinhole%2520camera%2520assumption%2520in%2520modeling%2520the%2520images%2520and%2520hence%2520only%2520work%2520for%250AAll-in-Focus%2520%2528AiF%2529%2520sharp%2520image%2520inputs.%2520This%2520severely%2520affects%2520their%250Aapplicability%2520in%2520real-world%2520scenarios%2520where%2520images%2520often%2520exhibit%2520defocus%2520blur%250Adue%2520to%2520the%2520limited%2520depth-of-field%2520%2528DOF%2529%2520of%2520imaging%2520devices.%2520Additionally%252C%250Aexisting%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520methods%2520also%2520do%2520not%2520support%2520rendering%2520of%250ADOF%2520effects.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520DOF-GS%2520that%2520allows%2520for%2520rendering%250Aadjustable%2520DOF%2520effects%252C%2520removing%2520defocus%2520blur%2520as%2520well%2520as%2520refocusing%2520of%25203D%250Ascenes%252C%2520all%2520from%2520multi-view%2520images%2520degraded%2520by%2520defocus%2520blur.%2520To%2520this%2520end%252C%2520we%250Are-imagine%2520the%2520traditional%2520Gaussian%2520Splatting%2520pipeline%2520by%2520employing%2520a%2520finite%250Aaperture%2520camera%2520model%2520coupled%2520with%2520explicit%252C%2520differentiable%2520defocus%2520rendering%250Aguided%2520by%2520the%2520Circle-of-Confusion%2520%2528CoC%2529.%2520The%2520proposed%2520framework%2520provides%2520for%250Adynamic%2520adjustment%2520of%2520DOF%2520effects%2520by%2520changing%2520the%2520aperture%2520and%2520focal%2520distance%250Aof%2520the%2520underlying%2520camera%2520model%2520on-demand.%2520It%2520also%2520enables%2520rendering%2520varying%2520DOF%250Aeffects%2520of%25203D%2520scenes%2520post-optimization%252C%2520and%2520generating%2520AiF%2520images%2520from%250Adefocused%2520training%2520images.%2520Furthermore%252C%2520we%2520devise%2520a%2520joint%2520optimization%2520strategy%250Ato%2520further%2520enhance%2520details%2520in%2520the%2520reconstructed%2520scenes%2520by%2520jointly%2520optimizing%250Arendered%2520defocused%2520and%2520AiF%2520images.%2520Our%2520experimental%2520results%2520indicate%2520that%250ADOF-GS%2520produces%2520high-quality%2520sharp%2520all-in-focus%2520renderings%2520conditioned%2520on%250Ainputs%2520compromised%2520by%2520defocus%2520blur%252C%2520with%2520the%2520training%2520process%2520incurring%2520only%2520a%250Amodest%2520increase%2520in%2520GPU%2520memory%2520consumption.%2520We%2520further%2520demonstrate%2520the%250Aapplications%2520of%2520the%2520proposed%2520method%2520for%2520adjustable%2520defocus%2520rendering%2520and%250Arefocusing%2520of%2520the%25203D%2520scene%2520from%2520input%2520images%2520degraded%2520by%2520defocus%2520blur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17351v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOF-GS%3A%20Adjustable%20Depth-of-Field%203D%20Gaussian%20Splatting%20for%0A%20%20Refocusing%2CDefocus%20Rendering%20and%20Blur%20Removal&entry.906535625=Yujie%20Wang%20and%20Praneeth%20Chakravarthula%20and%20Baoquan%20Chen&entry.1292438233=%20%203D%20Gaussian%20Splatting-based%20techniques%20have%20recently%20advanced%203D%20scene%0Areconstruction%20and%20novel%20view%20synthesis%2C%20achieving%20high-quality%20real-time%0Arendering.%20However%2C%20these%20approaches%20are%20inherently%20limited%20by%20the%20underlying%0Apinhole%20camera%20assumption%20in%20modeling%20the%20images%20and%20hence%20only%20work%20for%0AAll-in-Focus%20%28AiF%29%20sharp%20image%20inputs.%20This%20severely%20affects%20their%0Aapplicability%20in%20real-world%20scenarios%20where%20images%20often%20exhibit%20defocus%20blur%0Adue%20to%20the%20limited%20depth-of-field%20%28DOF%29%20of%20imaging%20devices.%20Additionally%2C%0Aexisting%203D%20Gaussian%20Splatting%20%283DGS%29%20methods%20also%20do%20not%20support%20rendering%20of%0ADOF%20effects.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20DOF-GS%20that%20allows%20for%20rendering%0Aadjustable%20DOF%20effects%2C%20removing%20defocus%20blur%20as%20well%20as%20refocusing%20of%203D%0Ascenes%2C%20all%20from%20multi-view%20images%20degraded%20by%20defocus%20blur.%20To%20this%20end%2C%20we%0Are-imagine%20the%20traditional%20Gaussian%20Splatting%20pipeline%20by%20employing%20a%20finite%0Aaperture%20camera%20model%20coupled%20with%20explicit%2C%20differentiable%20defocus%20rendering%0Aguided%20by%20the%20Circle-of-Confusion%20%28CoC%29.%20The%20proposed%20framework%20provides%20for%0Adynamic%20adjustment%20of%20DOF%20effects%20by%20changing%20the%20aperture%20and%20focal%20distance%0Aof%20the%20underlying%20camera%20model%20on-demand.%20It%20also%20enables%20rendering%20varying%20DOF%0Aeffects%20of%203D%20scenes%20post-optimization%2C%20and%20generating%20AiF%20images%20from%0Adefocused%20training%20images.%20Furthermore%2C%20we%20devise%20a%20joint%20optimization%20strategy%0Ato%20further%20enhance%20details%20in%20the%20reconstructed%20scenes%20by%20jointly%20optimizing%0Arendered%20defocused%20and%20AiF%20images.%20Our%20experimental%20results%20indicate%20that%0ADOF-GS%20produces%20high-quality%20sharp%20all-in-focus%20renderings%20conditioned%20on%0Ainputs%20compromised%20by%20defocus%20blur%2C%20with%20the%20training%20process%20incurring%20only%20a%0Amodest%20increase%20in%20GPU%20memory%20consumption.%20We%20further%20demonstrate%20the%0Aapplications%20of%20the%20proposed%20method%20for%20adjustable%20defocus%20rendering%20and%0Arefocusing%20of%20the%203D%20scene%20from%20input%20images%20degraded%20by%20defocus%20blur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17351v1&entry.124074799=Read"},
{"title": "Segment Any 3D Gaussians", "author": "Jiazhong Cen and Jiemin Fang and Chen Yang and Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian", "abstract": "  This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D\npromptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D\nvisual prompts as input, SAGA can segment the corresponding 3D target\nrepresented by 3D Gaussians within 4 ms. This is achieved by attaching an\nscale-gated affinity feature to each 3D Gaussian to endow it a new property\ntowards multi-granularity segmentation. Specifically, a scale-aware contrastive\ntraining strategy is proposed for the scale-gated affinity feature learning. It\n1) distills the segmentation capability of the Segment Anything Model (SAM)\nfrom 2D masks into the affinity features and 2) employs a soft scale gate\nmechanism to deal with multi-granularity ambiguity in 3D segmentation through\nadjusting the magnitude of each feature channel according to a specified 3D\nphysical scale. Evaluations demonstrate that SAGA achieves real-time\nmulti-granularity segmentation with quality comparable to state-of-the-art\nmethods. As one of the first methods addressing promptable segmentation in\n3D-GS, the simplicity and effectiveness of SAGA pave the way for future\nadvancements in this field. Our code will be released.\n", "link": "http://arxiv.org/abs/2312.00860v2", "date": "2024-05-27", "relevancy": 2.9867, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5842}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%203D%20Gaussians&body=Title%3A%20Segment%20Any%203D%20Gaussians%0AAuthor%3A%20Jiazhong%20Cen%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian%0AAbstract%3A%20%20%20This%20paper%20presents%20SAGA%20%28Segment%20Any%203D%20GAussians%29%2C%20a%20highly%20efficient%203D%0Apromptable%20segmentation%20method%20based%20on%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Given%202D%0Avisual%20prompts%20as%20input%2C%20SAGA%20can%20segment%20the%20corresponding%203D%20target%0Arepresented%20by%203D%20Gaussians%20within%204%20ms.%20This%20is%20achieved%20by%20attaching%20an%0Ascale-gated%20affinity%20feature%20to%20each%203D%20Gaussian%20to%20endow%20it%20a%20new%20property%0Atowards%20multi-granularity%20segmentation.%20Specifically%2C%20a%20scale-aware%20contrastive%0Atraining%20strategy%20is%20proposed%20for%20the%20scale-gated%20affinity%20feature%20learning.%20It%0A1%29%20distills%20the%20segmentation%20capability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%0Afrom%202D%20masks%20into%20the%20affinity%20features%20and%202%29%20employs%20a%20soft%20scale%20gate%0Amechanism%20to%20deal%20with%20multi-granularity%20ambiguity%20in%203D%20segmentation%20through%0Aadjusting%20the%20magnitude%20of%20each%20feature%20channel%20according%20to%20a%20specified%203D%0Aphysical%20scale.%20Evaluations%20demonstrate%20that%20SAGA%20achieves%20real-time%0Amulti-granularity%20segmentation%20with%20quality%20comparable%20to%20state-of-the-art%0Amethods.%20As%20one%20of%20the%20first%20methods%20addressing%20promptable%20segmentation%20in%0A3D-GS%2C%20the%20simplicity%20and%20effectiveness%20of%20SAGA%20pave%20the%20way%20for%20future%0Aadvancements%20in%20this%20field.%20Our%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%25203D%2520Gaussians%26entry.906535625%3DJiazhong%2520Cen%2520and%2520Jiemin%2520Fang%2520and%2520Chen%2520Yang%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Shen%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520SAGA%2520%2528Segment%2520Any%25203D%2520GAussians%2529%252C%2520a%2520highly%2520efficient%25203D%250Apromptable%2520segmentation%2520method%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529.%2520Given%25202D%250Avisual%2520prompts%2520as%2520input%252C%2520SAGA%2520can%2520segment%2520the%2520corresponding%25203D%2520target%250Arepresented%2520by%25203D%2520Gaussians%2520within%25204%2520ms.%2520This%2520is%2520achieved%2520by%2520attaching%2520an%250Ascale-gated%2520affinity%2520feature%2520to%2520each%25203D%2520Gaussian%2520to%2520endow%2520it%2520a%2520new%2520property%250Atowards%2520multi-granularity%2520segmentation.%2520Specifically%252C%2520a%2520scale-aware%2520contrastive%250Atraining%2520strategy%2520is%2520proposed%2520for%2520the%2520scale-gated%2520affinity%2520feature%2520learning.%2520It%250A1%2529%2520distills%2520the%2520segmentation%2520capability%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%250Afrom%25202D%2520masks%2520into%2520the%2520affinity%2520features%2520and%25202%2529%2520employs%2520a%2520soft%2520scale%2520gate%250Amechanism%2520to%2520deal%2520with%2520multi-granularity%2520ambiguity%2520in%25203D%2520segmentation%2520through%250Aadjusting%2520the%2520magnitude%2520of%2520each%2520feature%2520channel%2520according%2520to%2520a%2520specified%25203D%250Aphysical%2520scale.%2520Evaluations%2520demonstrate%2520that%2520SAGA%2520achieves%2520real-time%250Amulti-granularity%2520segmentation%2520with%2520quality%2520comparable%2520to%2520state-of-the-art%250Amethods.%2520As%2520one%2520of%2520the%2520first%2520methods%2520addressing%2520promptable%2520segmentation%2520in%250A3D-GS%252C%2520the%2520simplicity%2520and%2520effectiveness%2520of%2520SAGA%2520pave%2520the%2520way%2520for%2520future%250Aadvancements%2520in%2520this%2520field.%2520Our%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%203D%20Gaussians&entry.906535625=Jiazhong%20Cen%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian&entry.1292438233=%20%20This%20paper%20presents%20SAGA%20%28Segment%20Any%203D%20GAussians%29%2C%20a%20highly%20efficient%203D%0Apromptable%20segmentation%20method%20based%20on%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Given%202D%0Avisual%20prompts%20as%20input%2C%20SAGA%20can%20segment%20the%20corresponding%203D%20target%0Arepresented%20by%203D%20Gaussians%20within%204%20ms.%20This%20is%20achieved%20by%20attaching%20an%0Ascale-gated%20affinity%20feature%20to%20each%203D%20Gaussian%20to%20endow%20it%20a%20new%20property%0Atowards%20multi-granularity%20segmentation.%20Specifically%2C%20a%20scale-aware%20contrastive%0Atraining%20strategy%20is%20proposed%20for%20the%20scale-gated%20affinity%20feature%20learning.%20It%0A1%29%20distills%20the%20segmentation%20capability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%0Afrom%202D%20masks%20into%20the%20affinity%20features%20and%202%29%20employs%20a%20soft%20scale%20gate%0Amechanism%20to%20deal%20with%20multi-granularity%20ambiguity%20in%203D%20segmentation%20through%0Aadjusting%20the%20magnitude%20of%20each%20feature%20channel%20according%20to%20a%20specified%203D%0Aphysical%20scale.%20Evaluations%20demonstrate%20that%20SAGA%20achieves%20real-time%0Amulti-granularity%20segmentation%20with%20quality%20comparable%20to%20state-of-the-art%0Amethods.%20As%20one%20of%20the%20first%20methods%20addressing%20promptable%20segmentation%20in%0A3D-GS%2C%20the%20simplicity%20and%20effectiveness%20of%20SAGA%20pave%20the%20way%20for%20future%0Aadvancements%20in%20this%20field.%20Our%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00860v2&entry.124074799=Read"},
{"title": "All-day Depth Completion", "author": "Vadim Ezhov and Hyoungseob Park and Zhaoyang Zhang and Rishi Upadhyay and Howard Zhang and Chethan Chinder Chandrappa and Achuta Kadambi and Yunhao Ba and Julie Dorsey and Alex Wong", "abstract": "  We propose a method for depth estimation under different illumination\nconditions, i.e., day and night time. As photometry is uninformative in regions\nunder low-illumination, we tackle the problem through a multi-sensor fusion\napproach, where we take as input an additional synchronized sparse point cloud\n(i.e., from a LiDAR) projected onto the image plane as a sparse depth map,\nalong with a camera image. The crux of our method lies in the use of the\nabundantly available synthetic data to first approximate the 3D scene structure\nby learning a mapping from sparse to (coarse) dense depth maps along with their\npredictive uncertainty - we term this, SpaDe. In poorly illuminated regions\nwhere photometric intensities do not afford the inference of local shape, the\ncoarse approximation of scene depth serves as a prior; the uncertainty map is\nthen used with the image to guide refinement through an uncertainty-driven\nresidual learning (URL) scheme. The resulting depth completion network\nleverages complementary strengths from both modalities - depth is sparse but\ninsensitive to illumination and in metric scale, and image is dense but\nsensitive with scale ambiguity. SpaDe can be used in a plug-and-play fashion,\nwhich allows for 25% improvement when augmented onto existing methods to\npreprocess sparse depth. We demonstrate URL on the nuScenes dataset where we\nimprove over all baselines by an average 11.65% in all-day scenarios, 11.23%\nwhen tested specifically for daytime, and 13.12% for nighttime scenes.\n", "link": "http://arxiv.org/abs/2405.17315v1", "date": "2024-05-27", "relevancy": 2.8621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5935}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.562}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-day%20Depth%20Completion&body=Title%3A%20All-day%20Depth%20Completion%0AAuthor%3A%20Vadim%20Ezhov%20and%20Hyoungseob%20Park%20and%20Zhaoyang%20Zhang%20and%20Rishi%20Upadhyay%20and%20Howard%20Zhang%20and%20Chethan%20Chinder%20Chandrappa%20and%20Achuta%20Kadambi%20and%20Yunhao%20Ba%20and%20Julie%20Dorsey%20and%20Alex%20Wong%0AAbstract%3A%20%20%20We%20propose%20a%20method%20for%20depth%20estimation%20under%20different%20illumination%0Aconditions%2C%20i.e.%2C%20day%20and%20night%20time.%20As%20photometry%20is%20uninformative%20in%20regions%0Aunder%20low-illumination%2C%20we%20tackle%20the%20problem%20through%20a%20multi-sensor%20fusion%0Aapproach%2C%20where%20we%20take%20as%20input%20an%20additional%20synchronized%20sparse%20point%20cloud%0A%28i.e.%2C%20from%20a%20LiDAR%29%20projected%20onto%20the%20image%20plane%20as%20a%20sparse%20depth%20map%2C%0Aalong%20with%20a%20camera%20image.%20The%20crux%20of%20our%20method%20lies%20in%20the%20use%20of%20the%0Aabundantly%20available%20synthetic%20data%20to%20first%20approximate%20the%203D%20scene%20structure%0Aby%20learning%20a%20mapping%20from%20sparse%20to%20%28coarse%29%20dense%20depth%20maps%20along%20with%20their%0Apredictive%20uncertainty%20-%20we%20term%20this%2C%20SpaDe.%20In%20poorly%20illuminated%20regions%0Awhere%20photometric%20intensities%20do%20not%20afford%20the%20inference%20of%20local%20shape%2C%20the%0Acoarse%20approximation%20of%20scene%20depth%20serves%20as%20a%20prior%3B%20the%20uncertainty%20map%20is%0Athen%20used%20with%20the%20image%20to%20guide%20refinement%20through%20an%20uncertainty-driven%0Aresidual%20learning%20%28URL%29%20scheme.%20The%20resulting%20depth%20completion%20network%0Aleverages%20complementary%20strengths%20from%20both%20modalities%20-%20depth%20is%20sparse%20but%0Ainsensitive%20to%20illumination%20and%20in%20metric%20scale%2C%20and%20image%20is%20dense%20but%0Asensitive%20with%20scale%20ambiguity.%20SpaDe%20can%20be%20used%20in%20a%20plug-and-play%20fashion%2C%0Awhich%20allows%20for%2025%25%20improvement%20when%20augmented%20onto%20existing%20methods%20to%0Apreprocess%20sparse%20depth.%20We%20demonstrate%20URL%20on%20the%20nuScenes%20dataset%20where%20we%0Aimprove%20over%20all%20baselines%20by%20an%20average%2011.65%25%20in%20all-day%20scenarios%2C%2011.23%25%0Awhen%20tested%20specifically%20for%20daytime%2C%20and%2013.12%25%20for%20nighttime%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-day%2520Depth%2520Completion%26entry.906535625%3DVadim%2520Ezhov%2520and%2520Hyoungseob%2520Park%2520and%2520Zhaoyang%2520Zhang%2520and%2520Rishi%2520Upadhyay%2520and%2520Howard%2520Zhang%2520and%2520Chethan%2520Chinder%2520Chandrappa%2520and%2520Achuta%2520Kadambi%2520and%2520Yunhao%2520Ba%2520and%2520Julie%2520Dorsey%2520and%2520Alex%2520Wong%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520for%2520depth%2520estimation%2520under%2520different%2520illumination%250Aconditions%252C%2520i.e.%252C%2520day%2520and%2520night%2520time.%2520As%2520photometry%2520is%2520uninformative%2520in%2520regions%250Aunder%2520low-illumination%252C%2520we%2520tackle%2520the%2520problem%2520through%2520a%2520multi-sensor%2520fusion%250Aapproach%252C%2520where%2520we%2520take%2520as%2520input%2520an%2520additional%2520synchronized%2520sparse%2520point%2520cloud%250A%2528i.e.%252C%2520from%2520a%2520LiDAR%2529%2520projected%2520onto%2520the%2520image%2520plane%2520as%2520a%2520sparse%2520depth%2520map%252C%250Aalong%2520with%2520a%2520camera%2520image.%2520The%2520crux%2520of%2520our%2520method%2520lies%2520in%2520the%2520use%2520of%2520the%250Aabundantly%2520available%2520synthetic%2520data%2520to%2520first%2520approximate%2520the%25203D%2520scene%2520structure%250Aby%2520learning%2520a%2520mapping%2520from%2520sparse%2520to%2520%2528coarse%2529%2520dense%2520depth%2520maps%2520along%2520with%2520their%250Apredictive%2520uncertainty%2520-%2520we%2520term%2520this%252C%2520SpaDe.%2520In%2520poorly%2520illuminated%2520regions%250Awhere%2520photometric%2520intensities%2520do%2520not%2520afford%2520the%2520inference%2520of%2520local%2520shape%252C%2520the%250Acoarse%2520approximation%2520of%2520scene%2520depth%2520serves%2520as%2520a%2520prior%253B%2520the%2520uncertainty%2520map%2520is%250Athen%2520used%2520with%2520the%2520image%2520to%2520guide%2520refinement%2520through%2520an%2520uncertainty-driven%250Aresidual%2520learning%2520%2528URL%2529%2520scheme.%2520The%2520resulting%2520depth%2520completion%2520network%250Aleverages%2520complementary%2520strengths%2520from%2520both%2520modalities%2520-%2520depth%2520is%2520sparse%2520but%250Ainsensitive%2520to%2520illumination%2520and%2520in%2520metric%2520scale%252C%2520and%2520image%2520is%2520dense%2520but%250Asensitive%2520with%2520scale%2520ambiguity.%2520SpaDe%2520can%2520be%2520used%2520in%2520a%2520plug-and-play%2520fashion%252C%250Awhich%2520allows%2520for%252025%2525%2520improvement%2520when%2520augmented%2520onto%2520existing%2520methods%2520to%250Apreprocess%2520sparse%2520depth.%2520We%2520demonstrate%2520URL%2520on%2520the%2520nuScenes%2520dataset%2520where%2520we%250Aimprove%2520over%2520all%2520baselines%2520by%2520an%2520average%252011.65%2525%2520in%2520all-day%2520scenarios%252C%252011.23%2525%250Awhen%2520tested%2520specifically%2520for%2520daytime%252C%2520and%252013.12%2525%2520for%2520nighttime%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-day%20Depth%20Completion&entry.906535625=Vadim%20Ezhov%20and%20Hyoungseob%20Park%20and%20Zhaoyang%20Zhang%20and%20Rishi%20Upadhyay%20and%20Howard%20Zhang%20and%20Chethan%20Chinder%20Chandrappa%20and%20Achuta%20Kadambi%20and%20Yunhao%20Ba%20and%20Julie%20Dorsey%20and%20Alex%20Wong&entry.1292438233=%20%20We%20propose%20a%20method%20for%20depth%20estimation%20under%20different%20illumination%0Aconditions%2C%20i.e.%2C%20day%20and%20night%20time.%20As%20photometry%20is%20uninformative%20in%20regions%0Aunder%20low-illumination%2C%20we%20tackle%20the%20problem%20through%20a%20multi-sensor%20fusion%0Aapproach%2C%20where%20we%20take%20as%20input%20an%20additional%20synchronized%20sparse%20point%20cloud%0A%28i.e.%2C%20from%20a%20LiDAR%29%20projected%20onto%20the%20image%20plane%20as%20a%20sparse%20depth%20map%2C%0Aalong%20with%20a%20camera%20image.%20The%20crux%20of%20our%20method%20lies%20in%20the%20use%20of%20the%0Aabundantly%20available%20synthetic%20data%20to%20first%20approximate%20the%203D%20scene%20structure%0Aby%20learning%20a%20mapping%20from%20sparse%20to%20%28coarse%29%20dense%20depth%20maps%20along%20with%20their%0Apredictive%20uncertainty%20-%20we%20term%20this%2C%20SpaDe.%20In%20poorly%20illuminated%20regions%0Awhere%20photometric%20intensities%20do%20not%20afford%20the%20inference%20of%20local%20shape%2C%20the%0Acoarse%20approximation%20of%20scene%20depth%20serves%20as%20a%20prior%3B%20the%20uncertainty%20map%20is%0Athen%20used%20with%20the%20image%20to%20guide%20refinement%20through%20an%20uncertainty-driven%0Aresidual%20learning%20%28URL%29%20scheme.%20The%20resulting%20depth%20completion%20network%0Aleverages%20complementary%20strengths%20from%20both%20modalities%20-%20depth%20is%20sparse%20but%0Ainsensitive%20to%20illumination%20and%20in%20metric%20scale%2C%20and%20image%20is%20dense%20but%0Asensitive%20with%20scale%20ambiguity.%20SpaDe%20can%20be%20used%20in%20a%20plug-and-play%20fashion%2C%0Awhich%20allows%20for%2025%25%20improvement%20when%20augmented%20onto%20existing%20methods%20to%0Apreprocess%20sparse%20depth.%20We%20demonstrate%20URL%20on%20the%20nuScenes%20dataset%20where%20we%0Aimprove%20over%20all%20baselines%20by%20an%20average%2011.65%25%20in%20all-day%20scenarios%2C%2011.23%25%0Awhen%20tested%20specifically%20for%20daytime%2C%20and%2013.12%25%20for%20nighttime%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17315v1&entry.124074799=Read"},
{"title": "GenWarp: Single Image to Novel Views with Semantic-Preserving Generative\n  Warping", "author": "Junyoung Seo and Kazumi Fukuda and Takashi Shibuya and Takuya Narihira and Naoki Murata and Shoukang Hu and Chieh-Hsin Lai and Seungryong Kim and Yuki Mitsufuji", "abstract": "  Generating novel views from a single image remains a challenging task due to\nthe complexity of 3D scenes and the limited diversity in the existing\nmulti-view datasets to train a model on. Recent research combining large-scale\ntext-to-image (T2I) models with monocular depth estimation (MDE) has shown\npromise in handling in-the-wild images. In these methods, an input view is\ngeometrically warped to novel views with estimated depth maps, then the warped\nimage is inpainted by T2I models. However, they struggle with noisy depth maps\nand loss of semantic details when warping an input view to novel viewpoints. In\nthis paper, we propose a novel approach for single-shot novel view synthesis, a\nsemantic-preserving generative warping framework that enables T2I generative\nmodels to learn where to warp and where to generate, through augmenting\ncross-view attention with self-attention. Our approach addresses the\nlimitations of existing methods by conditioning the generative model on source\nview images and incorporating geometric warping signals. Qualitative and\nquantitative evaluations demonstrate that our model outperforms existing\nmethods in both in-domain and out-of-domain scenarios. Project page is\navailable at https://GenWarp-NVS.github.io/.\n", "link": "http://arxiv.org/abs/2405.17251v1", "date": "2024-05-27", "relevancy": 2.841, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5809}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5809}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenWarp%3A%20Single%20Image%20to%20Novel%20Views%20with%20Semantic-Preserving%20Generative%0A%20%20Warping&body=Title%3A%20GenWarp%3A%20Single%20Image%20to%20Novel%20Views%20with%20Semantic-Preserving%20Generative%0A%20%20Warping%0AAuthor%3A%20Junyoung%20Seo%20and%20Kazumi%20Fukuda%20and%20Takashi%20Shibuya%20and%20Takuya%20Narihira%20and%20Naoki%20Murata%20and%20Shoukang%20Hu%20and%20Chieh-Hsin%20Lai%20and%20Seungryong%20Kim%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Generating%20novel%20views%20from%20a%20single%20image%20remains%20a%20challenging%20task%20due%20to%0Athe%20complexity%20of%203D%20scenes%20and%20the%20limited%20diversity%20in%20the%20existing%0Amulti-view%20datasets%20to%20train%20a%20model%20on.%20Recent%20research%20combining%20large-scale%0Atext-to-image%20%28T2I%29%20models%20with%20monocular%20depth%20estimation%20%28MDE%29%20has%20shown%0Apromise%20in%20handling%20in-the-wild%20images.%20In%20these%20methods%2C%20an%20input%20view%20is%0Ageometrically%20warped%20to%20novel%20views%20with%20estimated%20depth%20maps%2C%20then%20the%20warped%0Aimage%20is%20inpainted%20by%20T2I%20models.%20However%2C%20they%20struggle%20with%20noisy%20depth%20maps%0Aand%20loss%20of%20semantic%20details%20when%20warping%20an%20input%20view%20to%20novel%20viewpoints.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20single-shot%20novel%20view%20synthesis%2C%20a%0Asemantic-preserving%20generative%20warping%20framework%20that%20enables%20T2I%20generative%0Amodels%20to%20learn%20where%20to%20warp%20and%20where%20to%20generate%2C%20through%20augmenting%0Across-view%20attention%20with%20self-attention.%20Our%20approach%20addresses%20the%0Alimitations%20of%20existing%20methods%20by%20conditioning%20the%20generative%20model%20on%20source%0Aview%20images%20and%20incorporating%20geometric%20warping%20signals.%20Qualitative%20and%0Aquantitative%20evaluations%20demonstrate%20that%20our%20model%20outperforms%20existing%0Amethods%20in%20both%20in-domain%20and%20out-of-domain%20scenarios.%20Project%20page%20is%0Aavailable%20at%20https%3A//GenWarp-NVS.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenWarp%253A%2520Single%2520Image%2520to%2520Novel%2520Views%2520with%2520Semantic-Preserving%2520Generative%250A%2520%2520Warping%26entry.906535625%3DJunyoung%2520Seo%2520and%2520Kazumi%2520Fukuda%2520and%2520Takashi%2520Shibuya%2520and%2520Takuya%2520Narihira%2520and%2520Naoki%2520Murata%2520and%2520Shoukang%2520Hu%2520and%2520Chieh-Hsin%2520Lai%2520and%2520Seungryong%2520Kim%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Generating%2520novel%2520views%2520from%2520a%2520single%2520image%2520remains%2520a%2520challenging%2520task%2520due%2520to%250Athe%2520complexity%2520of%25203D%2520scenes%2520and%2520the%2520limited%2520diversity%2520in%2520the%2520existing%250Amulti-view%2520datasets%2520to%2520train%2520a%2520model%2520on.%2520Recent%2520research%2520combining%2520large-scale%250Atext-to-image%2520%2528T2I%2529%2520models%2520with%2520monocular%2520depth%2520estimation%2520%2528MDE%2529%2520has%2520shown%250Apromise%2520in%2520handling%2520in-the-wild%2520images.%2520In%2520these%2520methods%252C%2520an%2520input%2520view%2520is%250Ageometrically%2520warped%2520to%2520novel%2520views%2520with%2520estimated%2520depth%2520maps%252C%2520then%2520the%2520warped%250Aimage%2520is%2520inpainted%2520by%2520T2I%2520models.%2520However%252C%2520they%2520struggle%2520with%2520noisy%2520depth%2520maps%250Aand%2520loss%2520of%2520semantic%2520details%2520when%2520warping%2520an%2520input%2520view%2520to%2520novel%2520viewpoints.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520single-shot%2520novel%2520view%2520synthesis%252C%2520a%250Asemantic-preserving%2520generative%2520warping%2520framework%2520that%2520enables%2520T2I%2520generative%250Amodels%2520to%2520learn%2520where%2520to%2520warp%2520and%2520where%2520to%2520generate%252C%2520through%2520augmenting%250Across-view%2520attention%2520with%2520self-attention.%2520Our%2520approach%2520addresses%2520the%250Alimitations%2520of%2520existing%2520methods%2520by%2520conditioning%2520the%2520generative%2520model%2520on%2520source%250Aview%2520images%2520and%2520incorporating%2520geometric%2520warping%2520signals.%2520Qualitative%2520and%250Aquantitative%2520evaluations%2520demonstrate%2520that%2520our%2520model%2520outperforms%2520existing%250Amethods%2520in%2520both%2520in-domain%2520and%2520out-of-domain%2520scenarios.%2520Project%2520page%2520is%250Aavailable%2520at%2520https%253A//GenWarp-NVS.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenWarp%3A%20Single%20Image%20to%20Novel%20Views%20with%20Semantic-Preserving%20Generative%0A%20%20Warping&entry.906535625=Junyoung%20Seo%20and%20Kazumi%20Fukuda%20and%20Takashi%20Shibuya%20and%20Takuya%20Narihira%20and%20Naoki%20Murata%20and%20Shoukang%20Hu%20and%20Chieh-Hsin%20Lai%20and%20Seungryong%20Kim%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Generating%20novel%20views%20from%20a%20single%20image%20remains%20a%20challenging%20task%20due%20to%0Athe%20complexity%20of%203D%20scenes%20and%20the%20limited%20diversity%20in%20the%20existing%0Amulti-view%20datasets%20to%20train%20a%20model%20on.%20Recent%20research%20combining%20large-scale%0Atext-to-image%20%28T2I%29%20models%20with%20monocular%20depth%20estimation%20%28MDE%29%20has%20shown%0Apromise%20in%20handling%20in-the-wild%20images.%20In%20these%20methods%2C%20an%20input%20view%20is%0Ageometrically%20warped%20to%20novel%20views%20with%20estimated%20depth%20maps%2C%20then%20the%20warped%0Aimage%20is%20inpainted%20by%20T2I%20models.%20However%2C%20they%20struggle%20with%20noisy%20depth%20maps%0Aand%20loss%20of%20semantic%20details%20when%20warping%20an%20input%20view%20to%20novel%20viewpoints.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20single-shot%20novel%20view%20synthesis%2C%20a%0Asemantic-preserving%20generative%20warping%20framework%20that%20enables%20T2I%20generative%0Amodels%20to%20learn%20where%20to%20warp%20and%20where%20to%20generate%2C%20through%20augmenting%0Across-view%20attention%20with%20self-attention.%20Our%20approach%20addresses%20the%0Alimitations%20of%20existing%20methods%20by%20conditioning%20the%20generative%20model%20on%20source%0Aview%20images%20and%20incorporating%20geometric%20warping%20signals.%20Qualitative%20and%0Aquantitative%20evaluations%20demonstrate%20that%20our%20model%20outperforms%20existing%0Amethods%20in%20both%20in-domain%20and%20out-of-domain%20scenarios.%20Project%20page%20is%0Aavailable%20at%20https%3A//GenWarp-NVS.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17251v1&entry.124074799=Read"},
{"title": "Controllable Longer Image Animation with Diffusion Models", "author": "Qiang Wang and Minghua Liu and Junjun Hu and Fan Jiang and Mu Xu", "abstract": "  Generating realistic animated videos from static images is an important area\nof research in computer vision. Methods based on physical simulation and motion\nprediction have achieved notable advances, but they are often limited to\nspecific object textures and motion trajectories, failing to exhibit highly\ncomplex environments and physical dynamics. In this paper, we introduce an\nopen-domain controllable image animation method using motion priors with video\ndiffusion models. Our method achieves precise control over the direction and\nspeed of motion in the movable region by extracting the motion field\ninformation from videos and learning moving trajectories and strengths. Current\npretrained video generation models are typically limited to producing very\nshort videos, typically less than 30 frames. In contrast, we propose an\nefficient long-duration video generation method based on noise reschedule\nspecifically tailored for image animation tasks, facilitating the creation of\nvideos over 100 frames in length while maintaining consistency in content\nscenery and motion coordination. Specifically, we decompose the denoise process\ninto two distinct phases: the shaping of scene contours and the refining of\nmotion details. Then we reschedule the noise to control the generated frame\nsequences maintaining long-distance noise correlation. We conducted extensive\nexperiments with 10 baselines, encompassing both commercial tools and academic\nmethodologies, which demonstrate the superiority of our method. Our project\npage: \\url{https://wangqiang9.github.io/Controllable.github.io/}\n", "link": "http://arxiv.org/abs/2405.17306v1", "date": "2024-05-27", "relevancy": 2.786, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.7225}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6969}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Longer%20Image%20Animation%20with%20Diffusion%20Models&body=Title%3A%20Controllable%20Longer%20Image%20Animation%20with%20Diffusion%20Models%0AAuthor%3A%20Qiang%20Wang%20and%20Minghua%20Liu%20and%20Junjun%20Hu%20and%20Fan%20Jiang%20and%20Mu%20Xu%0AAbstract%3A%20%20%20Generating%20realistic%20animated%20videos%20from%20static%20images%20is%20an%20important%20area%0Aof%20research%20in%20computer%20vision.%20Methods%20based%20on%20physical%20simulation%20and%20motion%0Aprediction%20have%20achieved%20notable%20advances%2C%20but%20they%20are%20often%20limited%20to%0Aspecific%20object%20textures%20and%20motion%20trajectories%2C%20failing%20to%20exhibit%20highly%0Acomplex%20environments%20and%20physical%20dynamics.%20In%20this%20paper%2C%20we%20introduce%20an%0Aopen-domain%20controllable%20image%20animation%20method%20using%20motion%20priors%20with%20video%0Adiffusion%20models.%20Our%20method%20achieves%20precise%20control%20over%20the%20direction%20and%0Aspeed%20of%20motion%20in%20the%20movable%20region%20by%20extracting%20the%20motion%20field%0Ainformation%20from%20videos%20and%20learning%20moving%20trajectories%20and%20strengths.%20Current%0Apretrained%20video%20generation%20models%20are%20typically%20limited%20to%20producing%20very%0Ashort%20videos%2C%20typically%20less%20than%2030%20frames.%20In%20contrast%2C%20we%20propose%20an%0Aefficient%20long-duration%20video%20generation%20method%20based%20on%20noise%20reschedule%0Aspecifically%20tailored%20for%20image%20animation%20tasks%2C%20facilitating%20the%20creation%20of%0Avideos%20over%20100%20frames%20in%20length%20while%20maintaining%20consistency%20in%20content%0Ascenery%20and%20motion%20coordination.%20Specifically%2C%20we%20decompose%20the%20denoise%20process%0Ainto%20two%20distinct%20phases%3A%20the%20shaping%20of%20scene%20contours%20and%20the%20refining%20of%0Amotion%20details.%20Then%20we%20reschedule%20the%20noise%20to%20control%20the%20generated%20frame%0Asequences%20maintaining%20long-distance%20noise%20correlation.%20We%20conducted%20extensive%0Aexperiments%20with%2010%20baselines%2C%20encompassing%20both%20commercial%20tools%20and%20academic%0Amethodologies%2C%20which%20demonstrate%20the%20superiority%20of%20our%20method.%20Our%20project%0Apage%3A%20%5Curl%7Bhttps%3A//wangqiang9.github.io/Controllable.github.io/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Longer%2520Image%2520Animation%2520with%2520Diffusion%2520Models%26entry.906535625%3DQiang%2520Wang%2520and%2520Minghua%2520Liu%2520and%2520Junjun%2520Hu%2520and%2520Fan%2520Jiang%2520and%2520Mu%2520Xu%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520animated%2520videos%2520from%2520static%2520images%2520is%2520an%2520important%2520area%250Aof%2520research%2520in%2520computer%2520vision.%2520Methods%2520based%2520on%2520physical%2520simulation%2520and%2520motion%250Aprediction%2520have%2520achieved%2520notable%2520advances%252C%2520but%2520they%2520are%2520often%2520limited%2520to%250Aspecific%2520object%2520textures%2520and%2520motion%2520trajectories%252C%2520failing%2520to%2520exhibit%2520highly%250Acomplex%2520environments%2520and%2520physical%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%250Aopen-domain%2520controllable%2520image%2520animation%2520method%2520using%2520motion%2520priors%2520with%2520video%250Adiffusion%2520models.%2520Our%2520method%2520achieves%2520precise%2520control%2520over%2520the%2520direction%2520and%250Aspeed%2520of%2520motion%2520in%2520the%2520movable%2520region%2520by%2520extracting%2520the%2520motion%2520field%250Ainformation%2520from%2520videos%2520and%2520learning%2520moving%2520trajectories%2520and%2520strengths.%2520Current%250Apretrained%2520video%2520generation%2520models%2520are%2520typically%2520limited%2520to%2520producing%2520very%250Ashort%2520videos%252C%2520typically%2520less%2520than%252030%2520frames.%2520In%2520contrast%252C%2520we%2520propose%2520an%250Aefficient%2520long-duration%2520video%2520generation%2520method%2520based%2520on%2520noise%2520reschedule%250Aspecifically%2520tailored%2520for%2520image%2520animation%2520tasks%252C%2520facilitating%2520the%2520creation%2520of%250Avideos%2520over%2520100%2520frames%2520in%2520length%2520while%2520maintaining%2520consistency%2520in%2520content%250Ascenery%2520and%2520motion%2520coordination.%2520Specifically%252C%2520we%2520decompose%2520the%2520denoise%2520process%250Ainto%2520two%2520distinct%2520phases%253A%2520the%2520shaping%2520of%2520scene%2520contours%2520and%2520the%2520refining%2520of%250Amotion%2520details.%2520Then%2520we%2520reschedule%2520the%2520noise%2520to%2520control%2520the%2520generated%2520frame%250Asequences%2520maintaining%2520long-distance%2520noise%2520correlation.%2520We%2520conducted%2520extensive%250Aexperiments%2520with%252010%2520baselines%252C%2520encompassing%2520both%2520commercial%2520tools%2520and%2520academic%250Amethodologies%252C%2520which%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method.%2520Our%2520project%250Apage%253A%2520%255Curl%257Bhttps%253A//wangqiang9.github.io/Controllable.github.io/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Longer%20Image%20Animation%20with%20Diffusion%20Models&entry.906535625=Qiang%20Wang%20and%20Minghua%20Liu%20and%20Junjun%20Hu%20and%20Fan%20Jiang%20and%20Mu%20Xu&entry.1292438233=%20%20Generating%20realistic%20animated%20videos%20from%20static%20images%20is%20an%20important%20area%0Aof%20research%20in%20computer%20vision.%20Methods%20based%20on%20physical%20simulation%20and%20motion%0Aprediction%20have%20achieved%20notable%20advances%2C%20but%20they%20are%20often%20limited%20to%0Aspecific%20object%20textures%20and%20motion%20trajectories%2C%20failing%20to%20exhibit%20highly%0Acomplex%20environments%20and%20physical%20dynamics.%20In%20this%20paper%2C%20we%20introduce%20an%0Aopen-domain%20controllable%20image%20animation%20method%20using%20motion%20priors%20with%20video%0Adiffusion%20models.%20Our%20method%20achieves%20precise%20control%20over%20the%20direction%20and%0Aspeed%20of%20motion%20in%20the%20movable%20region%20by%20extracting%20the%20motion%20field%0Ainformation%20from%20videos%20and%20learning%20moving%20trajectories%20and%20strengths.%20Current%0Apretrained%20video%20generation%20models%20are%20typically%20limited%20to%20producing%20very%0Ashort%20videos%2C%20typically%20less%20than%2030%20frames.%20In%20contrast%2C%20we%20propose%20an%0Aefficient%20long-duration%20video%20generation%20method%20based%20on%20noise%20reschedule%0Aspecifically%20tailored%20for%20image%20animation%20tasks%2C%20facilitating%20the%20creation%20of%0Avideos%20over%20100%20frames%20in%20length%20while%20maintaining%20consistency%20in%20content%0Ascenery%20and%20motion%20coordination.%20Specifically%2C%20we%20decompose%20the%20denoise%20process%0Ainto%20two%20distinct%20phases%3A%20the%20shaping%20of%20scene%20contours%20and%20the%20refining%20of%0Amotion%20details.%20Then%20we%20reschedule%20the%20noise%20to%20control%20the%20generated%20frame%0Asequences%20maintaining%20long-distance%20noise%20correlation.%20We%20conducted%20extensive%0Aexperiments%20with%2010%20baselines%2C%20encompassing%20both%20commercial%20tools%20and%20academic%0Amethodologies%2C%20which%20demonstrate%20the%20superiority%20of%20our%20method.%20Our%20project%0Apage%3A%20%5Curl%7Bhttps%3A//wangqiang9.github.io/Controllable.github.io/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17306v1&entry.124074799=Read"},
{"title": "MCGAN: Enhancing GAN Training with Regression-Based Generator Loss", "author": "Baoren Xiao and Hao Ni and Weixin Yang", "abstract": "  Generative adversarial networks (GANs) have emerged as a powerful tool for\ngenerating high-fidelity data. However, the main bottleneck of existing\napproaches is the lack of supervision on the generator training, which often\nresults in undamped oscillation and unsatisfactory performance. To address this\nissue, we propose an algorithm called Monte Carlo GAN (MCGAN). This approach,\nutilizing an innovative generative loss function, termly the regression loss,\nreformulates the generator training as a regression task and enables the\ngenerator training by minimizing the mean squared error between the\ndiscriminator's output of real data and the expected discriminator of fake\ndata. We demonstrate the desirable analytic properties of the regression loss,\nincluding discriminability and optimality, and show that our method requires a\nweaker condition on the discriminator for effective generator training. These\nproperties justify the strength of this approach to improve the training\nstability while retaining the optimality of GAN by leveraging strong\nsupervision of the regression loss. Numerical results on CIFAR-10 and CIFAR-100\ndatasets demonstrate that the proposed MCGAN significantly and consistently\nimproves the existing state-of-the-art GAN models in terms of quality,\naccuracy, training stability, and learned latent space. Furthermore, the\nproposed algorithm exhibits great flexibility for integrating with a variety of\nbackbone models to generate spatial images, temporal time-series, and\nspatio-temporal video data.\n", "link": "http://arxiv.org/abs/2405.17191v1", "date": "2024-05-27", "relevancy": 2.7835, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5683}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCGAN%3A%20Enhancing%20GAN%20Training%20with%20Regression-Based%20Generator%20Loss&body=Title%3A%20MCGAN%3A%20Enhancing%20GAN%20Training%20with%20Regression-Based%20Generator%20Loss%0AAuthor%3A%20Baoren%20Xiao%20and%20Hao%20Ni%20and%20Weixin%20Yang%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Agenerating%20high-fidelity%20data.%20However%2C%20the%20main%20bottleneck%20of%20existing%0Aapproaches%20is%20the%20lack%20of%20supervision%20on%20the%20generator%20training%2C%20which%20often%0Aresults%20in%20undamped%20oscillation%20and%20unsatisfactory%20performance.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20algorithm%20called%20Monte%20Carlo%20GAN%20%28MCGAN%29.%20This%20approach%2C%0Autilizing%20an%20innovative%20generative%20loss%20function%2C%20termly%20the%20regression%20loss%2C%0Areformulates%20the%20generator%20training%20as%20a%20regression%20task%20and%20enables%20the%0Agenerator%20training%20by%20minimizing%20the%20mean%20squared%20error%20between%20the%0Adiscriminator%27s%20output%20of%20real%20data%20and%20the%20expected%20discriminator%20of%20fake%0Adata.%20We%20demonstrate%20the%20desirable%20analytic%20properties%20of%20the%20regression%20loss%2C%0Aincluding%20discriminability%20and%20optimality%2C%20and%20show%20that%20our%20method%20requires%20a%0Aweaker%20condition%20on%20the%20discriminator%20for%20effective%20generator%20training.%20These%0Aproperties%20justify%20the%20strength%20of%20this%20approach%20to%20improve%20the%20training%0Astability%20while%20retaining%20the%20optimality%20of%20GAN%20by%20leveraging%20strong%0Asupervision%20of%20the%20regression%20loss.%20Numerical%20results%20on%20CIFAR-10%20and%20CIFAR-100%0Adatasets%20demonstrate%20that%20the%20proposed%20MCGAN%20significantly%20and%20consistently%0Aimproves%20the%20existing%20state-of-the-art%20GAN%20models%20in%20terms%20of%20quality%2C%0Aaccuracy%2C%20training%20stability%2C%20and%20learned%20latent%20space.%20Furthermore%2C%20the%0Aproposed%20algorithm%20exhibits%20great%20flexibility%20for%20integrating%20with%20a%20variety%20of%0Abackbone%20models%20to%20generate%20spatial%20images%2C%20temporal%20time-series%2C%20and%0Aspatio-temporal%20video%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCGAN%253A%2520Enhancing%2520GAN%2520Training%2520with%2520Regression-Based%2520Generator%2520Loss%26entry.906535625%3DBaoren%2520Xiao%2520and%2520Hao%2520Ni%2520and%2520Weixin%2520Yang%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Agenerating%2520high-fidelity%2520data.%2520However%252C%2520the%2520main%2520bottleneck%2520of%2520existing%250Aapproaches%2520is%2520the%2520lack%2520of%2520supervision%2520on%2520the%2520generator%2520training%252C%2520which%2520often%250Aresults%2520in%2520undamped%2520oscillation%2520and%2520unsatisfactory%2520performance.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520an%2520algorithm%2520called%2520Monte%2520Carlo%2520GAN%2520%2528MCGAN%2529.%2520This%2520approach%252C%250Autilizing%2520an%2520innovative%2520generative%2520loss%2520function%252C%2520termly%2520the%2520regression%2520loss%252C%250Areformulates%2520the%2520generator%2520training%2520as%2520a%2520regression%2520task%2520and%2520enables%2520the%250Agenerator%2520training%2520by%2520minimizing%2520the%2520mean%2520squared%2520error%2520between%2520the%250Adiscriminator%2527s%2520output%2520of%2520real%2520data%2520and%2520the%2520expected%2520discriminator%2520of%2520fake%250Adata.%2520We%2520demonstrate%2520the%2520desirable%2520analytic%2520properties%2520of%2520the%2520regression%2520loss%252C%250Aincluding%2520discriminability%2520and%2520optimality%252C%2520and%2520show%2520that%2520our%2520method%2520requires%2520a%250Aweaker%2520condition%2520on%2520the%2520discriminator%2520for%2520effective%2520generator%2520training.%2520These%250Aproperties%2520justify%2520the%2520strength%2520of%2520this%2520approach%2520to%2520improve%2520the%2520training%250Astability%2520while%2520retaining%2520the%2520optimality%2520of%2520GAN%2520by%2520leveraging%2520strong%250Asupervision%2520of%2520the%2520regression%2520loss.%2520Numerical%2520results%2520on%2520CIFAR-10%2520and%2520CIFAR-100%250Adatasets%2520demonstrate%2520that%2520the%2520proposed%2520MCGAN%2520significantly%2520and%2520consistently%250Aimproves%2520the%2520existing%2520state-of-the-art%2520GAN%2520models%2520in%2520terms%2520of%2520quality%252C%250Aaccuracy%252C%2520training%2520stability%252C%2520and%2520learned%2520latent%2520space.%2520Furthermore%252C%2520the%250Aproposed%2520algorithm%2520exhibits%2520great%2520flexibility%2520for%2520integrating%2520with%2520a%2520variety%2520of%250Abackbone%2520models%2520to%2520generate%2520spatial%2520images%252C%2520temporal%2520time-series%252C%2520and%250Aspatio-temporal%2520video%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCGAN%3A%20Enhancing%20GAN%20Training%20with%20Regression-Based%20Generator%20Loss&entry.906535625=Baoren%20Xiao%20and%20Hao%20Ni%20and%20Weixin%20Yang&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%0Agenerating%20high-fidelity%20data.%20However%2C%20the%20main%20bottleneck%20of%20existing%0Aapproaches%20is%20the%20lack%20of%20supervision%20on%20the%20generator%20training%2C%20which%20often%0Aresults%20in%20undamped%20oscillation%20and%20unsatisfactory%20performance.%20To%20address%20this%0Aissue%2C%20we%20propose%20an%20algorithm%20called%20Monte%20Carlo%20GAN%20%28MCGAN%29.%20This%20approach%2C%0Autilizing%20an%20innovative%20generative%20loss%20function%2C%20termly%20the%20regression%20loss%2C%0Areformulates%20the%20generator%20training%20as%20a%20regression%20task%20and%20enables%20the%0Agenerator%20training%20by%20minimizing%20the%20mean%20squared%20error%20between%20the%0Adiscriminator%27s%20output%20of%20real%20data%20and%20the%20expected%20discriminator%20of%20fake%0Adata.%20We%20demonstrate%20the%20desirable%20analytic%20properties%20of%20the%20regression%20loss%2C%0Aincluding%20discriminability%20and%20optimality%2C%20and%20show%20that%20our%20method%20requires%20a%0Aweaker%20condition%20on%20the%20discriminator%20for%20effective%20generator%20training.%20These%0Aproperties%20justify%20the%20strength%20of%20this%20approach%20to%20improve%20the%20training%0Astability%20while%20retaining%20the%20optimality%20of%20GAN%20by%20leveraging%20strong%0Asupervision%20of%20the%20regression%20loss.%20Numerical%20results%20on%20CIFAR-10%20and%20CIFAR-100%0Adatasets%20demonstrate%20that%20the%20proposed%20MCGAN%20significantly%20and%20consistently%0Aimproves%20the%20existing%20state-of-the-art%20GAN%20models%20in%20terms%20of%20quality%2C%0Aaccuracy%2C%20training%20stability%2C%20and%20learned%20latent%20space.%20Furthermore%2C%20the%0Aproposed%20algorithm%20exhibits%20great%20flexibility%20for%20integrating%20with%20a%20variety%20of%0Abackbone%20models%20to%20generate%20spatial%20images%2C%20temporal%20time-series%2C%20and%0Aspatio-temporal%20video%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17191v1&entry.124074799=Read"},
{"title": "Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive\n  Backbone Ensembling", "author": "Cristian Rodriguez-Opazo and Ehsan Abbasnejad and Damien Teney and Edison Marrese-Taylor and Hamed Damirchi and Anton van den Hengel", "abstract": "  Contrastive Language-Image Pretraining (CLIP) stands out as a prominent\nmethod for image representation learning. Various architectures, from vision\ntransformers (ViTs) to convolutional networks (ResNets) have been trained with\nCLIP to serve as general solutions to diverse vision tasks. This paper explores\nthe differences across various CLIP-trained vision backbones. Despite using the\nsame data and training objective, we find that these architectures have notably\ndifferent representations, different classification performance across\ndatasets, and different robustness properties to certain types of image\nperturbations. Our findings indicate a remarkable possible synergy across\nbackbones by leveraging their respective strengths. In principle,\nclassification accuracy could be improved by over 40 percentage with an\ninformed selection of the optimal backbone per test example.Using this insight,\nwe develop a straightforward yet powerful approach to adaptively ensemble\nmultiple backbones. The approach uses as few as one labeled example per class\nto tune the adaptive combination of backbones. On a large collection of\ndatasets, the method achieves a remarkable increase in accuracy of up to 39.1%\nover the best single backbone, well beyond traditional ensembles\n", "link": "http://arxiv.org/abs/2405.17139v1", "date": "2024-05-27", "relevancy": 2.7188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5124}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergy%20and%20Diversity%20in%20CLIP%3A%20Enhancing%20Performance%20Through%20Adaptive%0A%20%20Backbone%20Ensembling&body=Title%3A%20Synergy%20and%20Diversity%20in%20CLIP%3A%20Enhancing%20Performance%20Through%20Adaptive%0A%20%20Backbone%20Ensembling%0AAuthor%3A%20Cristian%20Rodriguez-Opazo%20and%20Ehsan%20Abbasnejad%20and%20Damien%20Teney%20and%20Edison%20Marrese-Taylor%20and%20Hamed%20Damirchi%20and%20Anton%20van%20den%20Hengel%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20stands%20out%20as%20a%20prominent%0Amethod%20for%20image%20representation%20learning.%20Various%20architectures%2C%20from%20vision%0Atransformers%20%28ViTs%29%20to%20convolutional%20networks%20%28ResNets%29%20have%20been%20trained%20with%0ACLIP%20to%20serve%20as%20general%20solutions%20to%20diverse%20vision%20tasks.%20This%20paper%20explores%0Athe%20differences%20across%20various%20CLIP-trained%20vision%20backbones.%20Despite%20using%20the%0Asame%20data%20and%20training%20objective%2C%20we%20find%20that%20these%20architectures%20have%20notably%0Adifferent%20representations%2C%20different%20classification%20performance%20across%0Adatasets%2C%20and%20different%20robustness%20properties%20to%20certain%20types%20of%20image%0Aperturbations.%20Our%20findings%20indicate%20a%20remarkable%20possible%20synergy%20across%0Abackbones%20by%20leveraging%20their%20respective%20strengths.%20In%20principle%2C%0Aclassification%20accuracy%20could%20be%20improved%20by%20over%2040%20percentage%20with%20an%0Ainformed%20selection%20of%20the%20optimal%20backbone%20per%20test%20example.Using%20this%20insight%2C%0Awe%20develop%20a%20straightforward%20yet%20powerful%20approach%20to%20adaptively%20ensemble%0Amultiple%20backbones.%20The%20approach%20uses%20as%20few%20as%20one%20labeled%20example%20per%20class%0Ato%20tune%20the%20adaptive%20combination%20of%20backbones.%20On%20a%20large%20collection%20of%0Adatasets%2C%20the%20method%20achieves%20a%20remarkable%20increase%20in%20accuracy%20of%20up%20to%2039.1%25%0Aover%20the%20best%20single%20backbone%2C%20well%20beyond%20traditional%20ensembles%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergy%2520and%2520Diversity%2520in%2520CLIP%253A%2520Enhancing%2520Performance%2520Through%2520Adaptive%250A%2520%2520Backbone%2520Ensembling%26entry.906535625%3DCristian%2520Rodriguez-Opazo%2520and%2520Ehsan%2520Abbasnejad%2520and%2520Damien%2520Teney%2520and%2520Edison%2520Marrese-Taylor%2520and%2520Hamed%2520Damirchi%2520and%2520Anton%2520van%2520den%2520Hengel%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520stands%2520out%2520as%2520a%2520prominent%250Amethod%2520for%2520image%2520representation%2520learning.%2520Various%2520architectures%252C%2520from%2520vision%250Atransformers%2520%2528ViTs%2529%2520to%2520convolutional%2520networks%2520%2528ResNets%2529%2520have%2520been%2520trained%2520with%250ACLIP%2520to%2520serve%2520as%2520general%2520solutions%2520to%2520diverse%2520vision%2520tasks.%2520This%2520paper%2520explores%250Athe%2520differences%2520across%2520various%2520CLIP-trained%2520vision%2520backbones.%2520Despite%2520using%2520the%250Asame%2520data%2520and%2520training%2520objective%252C%2520we%2520find%2520that%2520these%2520architectures%2520have%2520notably%250Adifferent%2520representations%252C%2520different%2520classification%2520performance%2520across%250Adatasets%252C%2520and%2520different%2520robustness%2520properties%2520to%2520certain%2520types%2520of%2520image%250Aperturbations.%2520Our%2520findings%2520indicate%2520a%2520remarkable%2520possible%2520synergy%2520across%250Abackbones%2520by%2520leveraging%2520their%2520respective%2520strengths.%2520In%2520principle%252C%250Aclassification%2520accuracy%2520could%2520be%2520improved%2520by%2520over%252040%2520percentage%2520with%2520an%250Ainformed%2520selection%2520of%2520the%2520optimal%2520backbone%2520per%2520test%2520example.Using%2520this%2520insight%252C%250Awe%2520develop%2520a%2520straightforward%2520yet%2520powerful%2520approach%2520to%2520adaptively%2520ensemble%250Amultiple%2520backbones.%2520The%2520approach%2520uses%2520as%2520few%2520as%2520one%2520labeled%2520example%2520per%2520class%250Ato%2520tune%2520the%2520adaptive%2520combination%2520of%2520backbones.%2520On%2520a%2520large%2520collection%2520of%250Adatasets%252C%2520the%2520method%2520achieves%2520a%2520remarkable%2520increase%2520in%2520accuracy%2520of%2520up%2520to%252039.1%2525%250Aover%2520the%2520best%2520single%2520backbone%252C%2520well%2520beyond%2520traditional%2520ensembles%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergy%20and%20Diversity%20in%20CLIP%3A%20Enhancing%20Performance%20Through%20Adaptive%0A%20%20Backbone%20Ensembling&entry.906535625=Cristian%20Rodriguez-Opazo%20and%20Ehsan%20Abbasnejad%20and%20Damien%20Teney%20and%20Edison%20Marrese-Taylor%20and%20Hamed%20Damirchi%20and%20Anton%20van%20den%20Hengel&entry.1292438233=%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20stands%20out%20as%20a%20prominent%0Amethod%20for%20image%20representation%20learning.%20Various%20architectures%2C%20from%20vision%0Atransformers%20%28ViTs%29%20to%20convolutional%20networks%20%28ResNets%29%20have%20been%20trained%20with%0ACLIP%20to%20serve%20as%20general%20solutions%20to%20diverse%20vision%20tasks.%20This%20paper%20explores%0Athe%20differences%20across%20various%20CLIP-trained%20vision%20backbones.%20Despite%20using%20the%0Asame%20data%20and%20training%20objective%2C%20we%20find%20that%20these%20architectures%20have%20notably%0Adifferent%20representations%2C%20different%20classification%20performance%20across%0Adatasets%2C%20and%20different%20robustness%20properties%20to%20certain%20types%20of%20image%0Aperturbations.%20Our%20findings%20indicate%20a%20remarkable%20possible%20synergy%20across%0Abackbones%20by%20leveraging%20their%20respective%20strengths.%20In%20principle%2C%0Aclassification%20accuracy%20could%20be%20improved%20by%20over%2040%20percentage%20with%20an%0Ainformed%20selection%20of%20the%20optimal%20backbone%20per%20test%20example.Using%20this%20insight%2C%0Awe%20develop%20a%20straightforward%20yet%20powerful%20approach%20to%20adaptively%20ensemble%0Amultiple%20backbones.%20The%20approach%20uses%20as%20few%20as%20one%20labeled%20example%20per%20class%0Ato%20tune%20the%20adaptive%20combination%20of%20backbones.%20On%20a%20large%20collection%20of%0Adatasets%2C%20the%20method%20achieves%20a%20remarkable%20increase%20in%20accuracy%20of%20up%20to%2039.1%25%0Aover%20the%20best%20single%20backbone%2C%20well%20beyond%20traditional%20ensembles%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17139v1&entry.124074799=Read"},
{"title": "GesGPT: Speech Gesture Synthesis With Text Parsing from GPT", "author": "Nan Gao and Zeyu Zhao and Zhi Zeng and Shuwu Zhang and Dongdong Weng and Yihua Bao", "abstract": "  Gesture synthesis has gained significant attention as a critical research\nfield, aiming to produce contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. In this letter, we propose GesGPT, a novel approach to\ngesture generation that leverages the semantic analysis capabilities of large\nlanguage models , such as ChatGPT. By capitalizing on the strengths of LLMs for\ntext analysis, we adopt a controlled approach to generate and integrate\nprofessional gestures and base gestures through a text parsing script,\nresulting in diverse and meaningful gestures. Firstly, our approach involves\nthe development of prompt principles that transform gesture generation into an\nintention classification problem using ChatGPT. We also conduct further\nanalysis on emphasis words and semantic words to aid in gesture generation.\nSubsequently, we construct a specialized gesture lexicon with multiple semantic\nannotations, decoupling the synthesis of gestures into professional gestures\nand base gestures. Finally, we merge the professional gestures with base\ngestures. Experimental results demonstrate that GesGPT effectively generates\ncontextually appropriate and expressive gestures.\n", "link": "http://arxiv.org/abs/2303.13013v2", "date": "2024-05-27", "relevancy": 2.708, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5353}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GesGPT%3A%20Speech%20Gesture%20Synthesis%20With%20Text%20Parsing%20from%20GPT&body=Title%3A%20GesGPT%3A%20Speech%20Gesture%20Synthesis%20With%20Text%20Parsing%20from%20GPT%0AAuthor%3A%20Nan%20Gao%20and%20Zeyu%20Zhao%20and%20Zhi%20Zeng%20and%20Shuwu%20Zhang%20and%20Dongdong%20Weng%20and%20Yihua%20Bao%0AAbstract%3A%20%20%20Gesture%20synthesis%20has%20gained%20significant%20attention%20as%20a%20critical%20research%0Afield%2C%20aiming%20to%20produce%20contextually%20appropriate%20and%20natural%20gestures%0Acorresponding%20to%20speech%20or%20textual%20input.%20Although%20deep%20learning-based%0Aapproaches%20have%20achieved%20remarkable%20progress%2C%20they%20often%20overlook%20the%20rich%0Asemantic%20information%20present%20in%20the%20text%2C%20leading%20to%20less%20expressive%20and%0Ameaningful%20gestures.%20In%20this%20letter%2C%20we%20propose%20GesGPT%2C%20a%20novel%20approach%20to%0Agesture%20generation%20that%20leverages%20the%20semantic%20analysis%20capabilities%20of%20large%0Alanguage%20models%20%2C%20such%20as%20ChatGPT.%20By%20capitalizing%20on%20the%20strengths%20of%20LLMs%20for%0Atext%20analysis%2C%20we%20adopt%20a%20controlled%20approach%20to%20generate%20and%20integrate%0Aprofessional%20gestures%20and%20base%20gestures%20through%20a%20text%20parsing%20script%2C%0Aresulting%20in%20diverse%20and%20meaningful%20gestures.%20Firstly%2C%20our%20approach%20involves%0Athe%20development%20of%20prompt%20principles%20that%20transform%20gesture%20generation%20into%20an%0Aintention%20classification%20problem%20using%20ChatGPT.%20We%20also%20conduct%20further%0Aanalysis%20on%20emphasis%20words%20and%20semantic%20words%20to%20aid%20in%20gesture%20generation.%0ASubsequently%2C%20we%20construct%20a%20specialized%20gesture%20lexicon%20with%20multiple%20semantic%0Aannotations%2C%20decoupling%20the%20synthesis%20of%20gestures%20into%20professional%20gestures%0Aand%20base%20gestures.%20Finally%2C%20we%20merge%20the%20professional%20gestures%20with%20base%0Agestures.%20Experimental%20results%20demonstrate%20that%20GesGPT%20effectively%20generates%0Acontextually%20appropriate%20and%20expressive%20gestures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.13013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGesGPT%253A%2520Speech%2520Gesture%2520Synthesis%2520With%2520Text%2520Parsing%2520from%2520GPT%26entry.906535625%3DNan%2520Gao%2520and%2520Zeyu%2520Zhao%2520and%2520Zhi%2520Zeng%2520and%2520Shuwu%2520Zhang%2520and%2520Dongdong%2520Weng%2520and%2520Yihua%2520Bao%26entry.1292438233%3D%2520%2520Gesture%2520synthesis%2520has%2520gained%2520significant%2520attention%2520as%2520a%2520critical%2520research%250Afield%252C%2520aiming%2520to%2520produce%2520contextually%2520appropriate%2520and%2520natural%2520gestures%250Acorresponding%2520to%2520speech%2520or%2520textual%2520input.%2520Although%2520deep%2520learning-based%250Aapproaches%2520have%2520achieved%2520remarkable%2520progress%252C%2520they%2520often%2520overlook%2520the%2520rich%250Asemantic%2520information%2520present%2520in%2520the%2520text%252C%2520leading%2520to%2520less%2520expressive%2520and%250Ameaningful%2520gestures.%2520In%2520this%2520letter%252C%2520we%2520propose%2520GesGPT%252C%2520a%2520novel%2520approach%2520to%250Agesture%2520generation%2520that%2520leverages%2520the%2520semantic%2520analysis%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%252C%2520such%2520as%2520ChatGPT.%2520By%2520capitalizing%2520on%2520the%2520strengths%2520of%2520LLMs%2520for%250Atext%2520analysis%252C%2520we%2520adopt%2520a%2520controlled%2520approach%2520to%2520generate%2520and%2520integrate%250Aprofessional%2520gestures%2520and%2520base%2520gestures%2520through%2520a%2520text%2520parsing%2520script%252C%250Aresulting%2520in%2520diverse%2520and%2520meaningful%2520gestures.%2520Firstly%252C%2520our%2520approach%2520involves%250Athe%2520development%2520of%2520prompt%2520principles%2520that%2520transform%2520gesture%2520generation%2520into%2520an%250Aintention%2520classification%2520problem%2520using%2520ChatGPT.%2520We%2520also%2520conduct%2520further%250Aanalysis%2520on%2520emphasis%2520words%2520and%2520semantic%2520words%2520to%2520aid%2520in%2520gesture%2520generation.%250ASubsequently%252C%2520we%2520construct%2520a%2520specialized%2520gesture%2520lexicon%2520with%2520multiple%2520semantic%250Aannotations%252C%2520decoupling%2520the%2520synthesis%2520of%2520gestures%2520into%2520professional%2520gestures%250Aand%2520base%2520gestures.%2520Finally%252C%2520we%2520merge%2520the%2520professional%2520gestures%2520with%2520base%250Agestures.%2520Experimental%2520results%2520demonstrate%2520that%2520GesGPT%2520effectively%2520generates%250Acontextually%2520appropriate%2520and%2520expressive%2520gestures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.13013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GesGPT%3A%20Speech%20Gesture%20Synthesis%20With%20Text%20Parsing%20from%20GPT&entry.906535625=Nan%20Gao%20and%20Zeyu%20Zhao%20and%20Zhi%20Zeng%20and%20Shuwu%20Zhang%20and%20Dongdong%20Weng%20and%20Yihua%20Bao&entry.1292438233=%20%20Gesture%20synthesis%20has%20gained%20significant%20attention%20as%20a%20critical%20research%0Afield%2C%20aiming%20to%20produce%20contextually%20appropriate%20and%20natural%20gestures%0Acorresponding%20to%20speech%20or%20textual%20input.%20Although%20deep%20learning-based%0Aapproaches%20have%20achieved%20remarkable%20progress%2C%20they%20often%20overlook%20the%20rich%0Asemantic%20information%20present%20in%20the%20text%2C%20leading%20to%20less%20expressive%20and%0Ameaningful%20gestures.%20In%20this%20letter%2C%20we%20propose%20GesGPT%2C%20a%20novel%20approach%20to%0Agesture%20generation%20that%20leverages%20the%20semantic%20analysis%20capabilities%20of%20large%0Alanguage%20models%20%2C%20such%20as%20ChatGPT.%20By%20capitalizing%20on%20the%20strengths%20of%20LLMs%20for%0Atext%20analysis%2C%20we%20adopt%20a%20controlled%20approach%20to%20generate%20and%20integrate%0Aprofessional%20gestures%20and%20base%20gestures%20through%20a%20text%20parsing%20script%2C%0Aresulting%20in%20diverse%20and%20meaningful%20gestures.%20Firstly%2C%20our%20approach%20involves%0Athe%20development%20of%20prompt%20principles%20that%20transform%20gesture%20generation%20into%20an%0Aintention%20classification%20problem%20using%20ChatGPT.%20We%20also%20conduct%20further%0Aanalysis%20on%20emphasis%20words%20and%20semantic%20words%20to%20aid%20in%20gesture%20generation.%0ASubsequently%2C%20we%20construct%20a%20specialized%20gesture%20lexicon%20with%20multiple%20semantic%0Aannotations%2C%20decoupling%20the%20synthesis%20of%20gestures%20into%20professional%20gestures%0Aand%20base%20gestures.%20Finally%2C%20we%20merge%20the%20professional%20gestures%20with%20base%0Agestures.%20Experimental%20results%20demonstrate%20that%20GesGPT%20effectively%20generates%0Acontextually%20appropriate%20and%20expressive%20gestures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.13013v2&entry.124074799=Read"},
{"title": "NeurTV: Total Variation on the Neural Domain", "author": "Yisi Luo and Xile Zhao and Kai Ye and Deyu Meng", "abstract": "  Recently, we have witnessed the success of total variation (TV) for many\nimaging applications. However, traditional TV is defined on the original pixel\ndomain, which limits its potential. In this work, we suggest a new TV\nregularization defined on the neural domain. Concretely, the discrete data is\ncontinuously and implicitly represented by a deep neural network (DNN), and we\nuse the derivatives of DNN outputs w.r.t. input coordinates to capture local\ncorrelations of data. As compared with classical TV on the original domain, the\nproposed TV on the neural domain (termed NeurTV) enjoys two advantages. First,\nNeurTV is not limited to meshgrid but is suitable for both meshgrid and\nnon-meshgrid data. Second, NeurTV can more exactly capture local correlations\nacross data for any direction and any order of derivatives attributed to the\nimplicit and continuous nature of neural domain. We theoretically reinterpret\nNeurTV under the variational approximation framework, which allows us to build\nthe connection between classical TV and NeurTV and inspires us to develop\nvariants (e.g., NeurTV with arbitrary resolution and space-variant NeurTV).\nExtensive numerical experiments with meshgrid data (e.g., color and\nhyperspectral images) and non-meshgrid data (e.g., point clouds and spatial\ntranscriptomics) showcase the effectiveness of the proposed methods.\n", "link": "http://arxiv.org/abs/2405.17241v1", "date": "2024-05-27", "relevancy": 2.7077, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5571}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5544}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeurTV%3A%20Total%20Variation%20on%20the%20Neural%20Domain&body=Title%3A%20NeurTV%3A%20Total%20Variation%20on%20the%20Neural%20Domain%0AAuthor%3A%20Yisi%20Luo%20and%20Xile%20Zhao%20and%20Kai%20Ye%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Recently%2C%20we%20have%20witnessed%20the%20success%20of%20total%20variation%20%28TV%29%20for%20many%0Aimaging%20applications.%20However%2C%20traditional%20TV%20is%20defined%20on%20the%20original%20pixel%0Adomain%2C%20which%20limits%20its%20potential.%20In%20this%20work%2C%20we%20suggest%20a%20new%20TV%0Aregularization%20defined%20on%20the%20neural%20domain.%20Concretely%2C%20the%20discrete%20data%20is%0Acontinuously%20and%20implicitly%20represented%20by%20a%20deep%20neural%20network%20%28DNN%29%2C%20and%20we%0Ause%20the%20derivatives%20of%20DNN%20outputs%20w.r.t.%20input%20coordinates%20to%20capture%20local%0Acorrelations%20of%20data.%20As%20compared%20with%20classical%20TV%20on%20the%20original%20domain%2C%20the%0Aproposed%20TV%20on%20the%20neural%20domain%20%28termed%20NeurTV%29%20enjoys%20two%20advantages.%20First%2C%0ANeurTV%20is%20not%20limited%20to%20meshgrid%20but%20is%20suitable%20for%20both%20meshgrid%20and%0Anon-meshgrid%20data.%20Second%2C%20NeurTV%20can%20more%20exactly%20capture%20local%20correlations%0Aacross%20data%20for%20any%20direction%20and%20any%20order%20of%20derivatives%20attributed%20to%20the%0Aimplicit%20and%20continuous%20nature%20of%20neural%20domain.%20We%20theoretically%20reinterpret%0ANeurTV%20under%20the%20variational%20approximation%20framework%2C%20which%20allows%20us%20to%20build%0Athe%20connection%20between%20classical%20TV%20and%20NeurTV%20and%20inspires%20us%20to%20develop%0Avariants%20%28e.g.%2C%20NeurTV%20with%20arbitrary%20resolution%20and%20space-variant%20NeurTV%29.%0AExtensive%20numerical%20experiments%20with%20meshgrid%20data%20%28e.g.%2C%20color%20and%0Ahyperspectral%20images%29%20and%20non-meshgrid%20data%20%28e.g.%2C%20point%20clouds%20and%20spatial%0Atranscriptomics%29%20showcase%20the%20effectiveness%20of%20the%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurTV%253A%2520Total%2520Variation%2520on%2520the%2520Neural%2520Domain%26entry.906535625%3DYisi%2520Luo%2520and%2520Xile%2520Zhao%2520and%2520Kai%2520Ye%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Recently%252C%2520we%2520have%2520witnessed%2520the%2520success%2520of%2520total%2520variation%2520%2528TV%2529%2520for%2520many%250Aimaging%2520applications.%2520However%252C%2520traditional%2520TV%2520is%2520defined%2520on%2520the%2520original%2520pixel%250Adomain%252C%2520which%2520limits%2520its%2520potential.%2520In%2520this%2520work%252C%2520we%2520suggest%2520a%2520new%2520TV%250Aregularization%2520defined%2520on%2520the%2520neural%2520domain.%2520Concretely%252C%2520the%2520discrete%2520data%2520is%250Acontinuously%2520and%2520implicitly%2520represented%2520by%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%252C%2520and%2520we%250Ause%2520the%2520derivatives%2520of%2520DNN%2520outputs%2520w.r.t.%2520input%2520coordinates%2520to%2520capture%2520local%250Acorrelations%2520of%2520data.%2520As%2520compared%2520with%2520classical%2520TV%2520on%2520the%2520original%2520domain%252C%2520the%250Aproposed%2520TV%2520on%2520the%2520neural%2520domain%2520%2528termed%2520NeurTV%2529%2520enjoys%2520two%2520advantages.%2520First%252C%250ANeurTV%2520is%2520not%2520limited%2520to%2520meshgrid%2520but%2520is%2520suitable%2520for%2520both%2520meshgrid%2520and%250Anon-meshgrid%2520data.%2520Second%252C%2520NeurTV%2520can%2520more%2520exactly%2520capture%2520local%2520correlations%250Aacross%2520data%2520for%2520any%2520direction%2520and%2520any%2520order%2520of%2520derivatives%2520attributed%2520to%2520the%250Aimplicit%2520and%2520continuous%2520nature%2520of%2520neural%2520domain.%2520We%2520theoretically%2520reinterpret%250ANeurTV%2520under%2520the%2520variational%2520approximation%2520framework%252C%2520which%2520allows%2520us%2520to%2520build%250Athe%2520connection%2520between%2520classical%2520TV%2520and%2520NeurTV%2520and%2520inspires%2520us%2520to%2520develop%250Avariants%2520%2528e.g.%252C%2520NeurTV%2520with%2520arbitrary%2520resolution%2520and%2520space-variant%2520NeurTV%2529.%250AExtensive%2520numerical%2520experiments%2520with%2520meshgrid%2520data%2520%2528e.g.%252C%2520color%2520and%250Ahyperspectral%2520images%2529%2520and%2520non-meshgrid%2520data%2520%2528e.g.%252C%2520point%2520clouds%2520and%2520spatial%250Atranscriptomics%2529%2520showcase%2520the%2520effectiveness%2520of%2520the%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeurTV%3A%20Total%20Variation%20on%20the%20Neural%20Domain&entry.906535625=Yisi%20Luo%20and%20Xile%20Zhao%20and%20Kai%20Ye%20and%20Deyu%20Meng&entry.1292438233=%20%20Recently%2C%20we%20have%20witnessed%20the%20success%20of%20total%20variation%20%28TV%29%20for%20many%0Aimaging%20applications.%20However%2C%20traditional%20TV%20is%20defined%20on%20the%20original%20pixel%0Adomain%2C%20which%20limits%20its%20potential.%20In%20this%20work%2C%20we%20suggest%20a%20new%20TV%0Aregularization%20defined%20on%20the%20neural%20domain.%20Concretely%2C%20the%20discrete%20data%20is%0Acontinuously%20and%20implicitly%20represented%20by%20a%20deep%20neural%20network%20%28DNN%29%2C%20and%20we%0Ause%20the%20derivatives%20of%20DNN%20outputs%20w.r.t.%20input%20coordinates%20to%20capture%20local%0Acorrelations%20of%20data.%20As%20compared%20with%20classical%20TV%20on%20the%20original%20domain%2C%20the%0Aproposed%20TV%20on%20the%20neural%20domain%20%28termed%20NeurTV%29%20enjoys%20two%20advantages.%20First%2C%0ANeurTV%20is%20not%20limited%20to%20meshgrid%20but%20is%20suitable%20for%20both%20meshgrid%20and%0Anon-meshgrid%20data.%20Second%2C%20NeurTV%20can%20more%20exactly%20capture%20local%20correlations%0Aacross%20data%20for%20any%20direction%20and%20any%20order%20of%20derivatives%20attributed%20to%20the%0Aimplicit%20and%20continuous%20nature%20of%20neural%20domain.%20We%20theoretically%20reinterpret%0ANeurTV%20under%20the%20variational%20approximation%20framework%2C%20which%20allows%20us%20to%20build%0Athe%20connection%20between%20classical%20TV%20and%20NeurTV%20and%20inspires%20us%20to%20develop%0Avariants%20%28e.g.%2C%20NeurTV%20with%20arbitrary%20resolution%20and%20space-variant%20NeurTV%29.%0AExtensive%20numerical%20experiments%20with%20meshgrid%20data%20%28e.g.%2C%20color%20and%0Ahyperspectral%20images%29%20and%20non-meshgrid%20data%20%28e.g.%2C%20point%20clouds%20and%20spatial%0Atranscriptomics%29%20showcase%20the%20effectiveness%20of%20the%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17241v1&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Networks", "author": "Arturs Berzins and Andreas Radler and Sebastian Sanokowski and Sepp Hochreiter and Johannes Brandstetter", "abstract": "  Geometry is a ubiquitous language of computer graphics, design, and\nengineering. However, the lack of large shape datasets limits the application\nof state-of-the-art supervised learning methods and motivates the exploration\nof alternative learning strategies. To this end, we introduce geometry-informed\nneural networks (GINNs) to train shape generative models \\emph{without any\ndata}. GINNs combine (i) learning under constraints, (ii) neural fields as a\nsuitable representation, and (iii) generating diverse solutions to\nunder-determined problems. We apply GINNs to several two and three-dimensional\nproblems of increasing levels of complexity. Our results demonstrate the\nfeasibility of training shape generative models in a data-free setting. This\nnew paradigm opens several exciting research directions, expanding the\napplication of generative models into domains where data is sparse.\n", "link": "http://arxiv.org/abs/2402.14009v2", "date": "2024-05-27", "relevancy": 2.6765, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5401}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5365}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Networks&body=Title%3A%20Geometry-Informed%20Neural%20Networks%0AAuthor%3A%20Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Geometry%20is%20a%20ubiquitous%20language%20of%20computer%20graphics%2C%20design%2C%20and%0Aengineering.%20However%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%0Aof%20state-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%0Aof%20alternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20to%20train%20shape%20generative%20models%20%5Cemph%7Bwithout%20any%0Adata%7D.%20GINNs%20combine%20%28i%29%20learning%20under%20constraints%2C%20%28ii%29%20neural%20fields%20as%20a%0Asuitable%20representation%2C%20and%20%28iii%29%20generating%20diverse%20solutions%20to%0Aunder-determined%20problems.%20We%20apply%20GINNs%20to%20several%20two%20and%20three-dimensional%0Aproblems%20of%20increasing%20levels%20of%20complexity.%20Our%20results%20demonstrate%20the%0Afeasibility%20of%20training%20shape%20generative%20models%20in%20a%20data-free%20setting.%20This%0Anew%20paradigm%20opens%20several%20exciting%20research%20directions%2C%20expanding%20the%0Aapplication%20of%20generative%20models%20into%20domains%20where%20data%20is%20sparse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Networks%26entry.906535625%3DArturs%2520Berzins%2520and%2520Andreas%2520Radler%2520and%2520Sebastian%2520Sanokowski%2520and%2520Sepp%2520Hochreiter%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Geometry%2520is%2520a%2520ubiquitous%2520language%2520of%2520computer%2520graphics%252C%2520design%252C%2520and%250Aengineering.%2520However%252C%2520the%2520lack%2520of%2520large%2520shape%2520datasets%2520limits%2520the%2520application%250Aof%2520state-of-the-art%2520supervised%2520learning%2520methods%2520and%2520motivates%2520the%2520exploration%250Aof%2520alternative%2520learning%2520strategies.%2520To%2520this%2520end%252C%2520we%2520introduce%2520geometry-informed%250Aneural%2520networks%2520%2528GINNs%2529%2520to%2520train%2520shape%2520generative%2520models%2520%255Cemph%257Bwithout%2520any%250Adata%257D.%2520GINNs%2520combine%2520%2528i%2529%2520learning%2520under%2520constraints%252C%2520%2528ii%2529%2520neural%2520fields%2520as%2520a%250Asuitable%2520representation%252C%2520and%2520%2528iii%2529%2520generating%2520diverse%2520solutions%2520to%250Aunder-determined%2520problems.%2520We%2520apply%2520GINNs%2520to%2520several%2520two%2520and%2520three-dimensional%250Aproblems%2520of%2520increasing%2520levels%2520of%2520complexity.%2520Our%2520results%2520demonstrate%2520the%250Afeasibility%2520of%2520training%2520shape%2520generative%2520models%2520in%2520a%2520data-free%2520setting.%2520This%250Anew%2520paradigm%2520opens%2520several%2520exciting%2520research%2520directions%252C%2520expanding%2520the%250Aapplication%2520of%2520generative%2520models%2520into%2520domains%2520where%2520data%2520is%2520sparse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Networks&entry.906535625=Arturs%20Berzins%20and%20Andreas%20Radler%20and%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Geometry%20is%20a%20ubiquitous%20language%20of%20computer%20graphics%2C%20design%2C%20and%0Aengineering.%20However%2C%20the%20lack%20of%20large%20shape%20datasets%20limits%20the%20application%0Aof%20state-of-the-art%20supervised%20learning%20methods%20and%20motivates%20the%20exploration%0Aof%20alternative%20learning%20strategies.%20To%20this%20end%2C%20we%20introduce%20geometry-informed%0Aneural%20networks%20%28GINNs%29%20to%20train%20shape%20generative%20models%20%5Cemph%7Bwithout%20any%0Adata%7D.%20GINNs%20combine%20%28i%29%20learning%20under%20constraints%2C%20%28ii%29%20neural%20fields%20as%20a%0Asuitable%20representation%2C%20and%20%28iii%29%20generating%20diverse%20solutions%20to%0Aunder-determined%20problems.%20We%20apply%20GINNs%20to%20several%20two%20and%20three-dimensional%0Aproblems%20of%20increasing%20levels%20of%20complexity.%20Our%20results%20demonstrate%20the%0Afeasibility%20of%20training%20shape%20generative%20models%20in%20a%20data-free%20setting.%20This%0Anew%20paradigm%20opens%20several%20exciting%20research%20directions%2C%20expanding%20the%0Aapplication%20of%20generative%20models%20into%20domains%20where%20data%20is%20sparse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14009v2&entry.124074799=Read"},
{"title": "LCM: Locally Constrained Compact Point Cloud Model for Masked Point\n  Modeling", "author": "Yaohua Zha and Naiqi Li and Yanzi Wang and Tao Dai and Hang Guo and Bin Chen and Zhi Wang and Zhihao Ouyang and Shu-Tao Xia", "abstract": "  The pre-trained point cloud model based on Masked Point Modeling (MPM) has\nexhibited substantial improvements across various tasks. However, these models\nheavily rely on the Transformer, leading to quadratic complexity and limited\ndecoder, hindering their practice application. To address this limitation, we\nfirst conduct a comprehensive analysis of existing Transformer-based MPM,\nemphasizing the idea that redundancy reduction is crucial for point cloud\nanalysis. To this end, we propose a Locally constrained Compact point cloud\nModel (LCM) consisting of a locally constrained compact encoder and a locally\nconstrained Mamba-based decoder. Our encoder replaces self-attention with our\nlocal aggregation layers to achieve an elegant balance between performance and\nefficiency. Considering the varying information density between masked and\nunmasked patches in the decoder inputs of MPM, we introduce a locally\nconstrained Mamba-based decoder. This decoder ensures linear complexity while\nmaximizing the perception of point cloud geometry information from unmasked\npatches with higher information density. Extensive experimental results show\nthat our compact model significantly surpasses existing Transformer-based\nmodels in both performance and efficiency, especially our LCM-based Point-MAE\nmodel, compared to the Transformer-based model, achieved an improvement of\n2.24%, 0.87%, and 0.94% in performance on the three variants of ScanObjectNN\nwhile reducing parameters by 88% and computation by 73%.\n", "link": "http://arxiv.org/abs/2405.17149v1", "date": "2024-05-27", "relevancy": 2.6705, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5365}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCM%3A%20Locally%20Constrained%20Compact%20Point%20Cloud%20Model%20for%20Masked%20Point%0A%20%20Modeling&body=Title%3A%20LCM%3A%20Locally%20Constrained%20Compact%20Point%20Cloud%20Model%20for%20Masked%20Point%0A%20%20Modeling%0AAuthor%3A%20Yaohua%20Zha%20and%20Naiqi%20Li%20and%20Yanzi%20Wang%20and%20Tao%20Dai%20and%20Hang%20Guo%20and%20Bin%20Chen%20and%20Zhi%20Wang%20and%20Zhihao%20Ouyang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20The%20pre-trained%20point%20cloud%20model%20based%20on%20Masked%20Point%20Modeling%20%28MPM%29%20has%0Aexhibited%20substantial%20improvements%20across%20various%20tasks.%20However%2C%20these%20models%0Aheavily%20rely%20on%20the%20Transformer%2C%20leading%20to%20quadratic%20complexity%20and%20limited%0Adecoder%2C%20hindering%20their%20practice%20application.%20To%20address%20this%20limitation%2C%20we%0Afirst%20conduct%20a%20comprehensive%20analysis%20of%20existing%20Transformer-based%20MPM%2C%0Aemphasizing%20the%20idea%20that%20redundancy%20reduction%20is%20crucial%20for%20point%20cloud%0Aanalysis.%20To%20this%20end%2C%20we%20propose%20a%20Locally%20constrained%20Compact%20point%20cloud%0AModel%20%28LCM%29%20consisting%20of%20a%20locally%20constrained%20compact%20encoder%20and%20a%20locally%0Aconstrained%20Mamba-based%20decoder.%20Our%20encoder%20replaces%20self-attention%20with%20our%0Alocal%20aggregation%20layers%20to%20achieve%20an%20elegant%20balance%20between%20performance%20and%0Aefficiency.%20Considering%20the%20varying%20information%20density%20between%20masked%20and%0Aunmasked%20patches%20in%20the%20decoder%20inputs%20of%20MPM%2C%20we%20introduce%20a%20locally%0Aconstrained%20Mamba-based%20decoder.%20This%20decoder%20ensures%20linear%20complexity%20while%0Amaximizing%20the%20perception%20of%20point%20cloud%20geometry%20information%20from%20unmasked%0Apatches%20with%20higher%20information%20density.%20Extensive%20experimental%20results%20show%0Athat%20our%20compact%20model%20significantly%20surpasses%20existing%20Transformer-based%0Amodels%20in%20both%20performance%20and%20efficiency%2C%20especially%20our%20LCM-based%20Point-MAE%0Amodel%2C%20compared%20to%20the%20Transformer-based%20model%2C%20achieved%20an%20improvement%20of%0A2.24%25%2C%200.87%25%2C%20and%200.94%25%20in%20performance%20on%20the%20three%20variants%20of%20ScanObjectNN%0Awhile%20reducing%20parameters%20by%2088%25%20and%20computation%20by%2073%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCM%253A%2520Locally%2520Constrained%2520Compact%2520Point%2520Cloud%2520Model%2520for%2520Masked%2520Point%250A%2520%2520Modeling%26entry.906535625%3DYaohua%2520Zha%2520and%2520Naiqi%2520Li%2520and%2520Yanzi%2520Wang%2520and%2520Tao%2520Dai%2520and%2520Hang%2520Guo%2520and%2520Bin%2520Chen%2520and%2520Zhi%2520Wang%2520and%2520Zhihao%2520Ouyang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520The%2520pre-trained%2520point%2520cloud%2520model%2520based%2520on%2520Masked%2520Point%2520Modeling%2520%2528MPM%2529%2520has%250Aexhibited%2520substantial%2520improvements%2520across%2520various%2520tasks.%2520However%252C%2520these%2520models%250Aheavily%2520rely%2520on%2520the%2520Transformer%252C%2520leading%2520to%2520quadratic%2520complexity%2520and%2520limited%250Adecoder%252C%2520hindering%2520their%2520practice%2520application.%2520To%2520address%2520this%2520limitation%252C%2520we%250Afirst%2520conduct%2520a%2520comprehensive%2520analysis%2520of%2520existing%2520Transformer-based%2520MPM%252C%250Aemphasizing%2520the%2520idea%2520that%2520redundancy%2520reduction%2520is%2520crucial%2520for%2520point%2520cloud%250Aanalysis.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Locally%2520constrained%2520Compact%2520point%2520cloud%250AModel%2520%2528LCM%2529%2520consisting%2520of%2520a%2520locally%2520constrained%2520compact%2520encoder%2520and%2520a%2520locally%250Aconstrained%2520Mamba-based%2520decoder.%2520Our%2520encoder%2520replaces%2520self-attention%2520with%2520our%250Alocal%2520aggregation%2520layers%2520to%2520achieve%2520an%2520elegant%2520balance%2520between%2520performance%2520and%250Aefficiency.%2520Considering%2520the%2520varying%2520information%2520density%2520between%2520masked%2520and%250Aunmasked%2520patches%2520in%2520the%2520decoder%2520inputs%2520of%2520MPM%252C%2520we%2520introduce%2520a%2520locally%250Aconstrained%2520Mamba-based%2520decoder.%2520This%2520decoder%2520ensures%2520linear%2520complexity%2520while%250Amaximizing%2520the%2520perception%2520of%2520point%2520cloud%2520geometry%2520information%2520from%2520unmasked%250Apatches%2520with%2520higher%2520information%2520density.%2520Extensive%2520experimental%2520results%2520show%250Athat%2520our%2520compact%2520model%2520significantly%2520surpasses%2520existing%2520Transformer-based%250Amodels%2520in%2520both%2520performance%2520and%2520efficiency%252C%2520especially%2520our%2520LCM-based%2520Point-MAE%250Amodel%252C%2520compared%2520to%2520the%2520Transformer-based%2520model%252C%2520achieved%2520an%2520improvement%2520of%250A2.24%2525%252C%25200.87%2525%252C%2520and%25200.94%2525%2520in%2520performance%2520on%2520the%2520three%2520variants%2520of%2520ScanObjectNN%250Awhile%2520reducing%2520parameters%2520by%252088%2525%2520and%2520computation%2520by%252073%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCM%3A%20Locally%20Constrained%20Compact%20Point%20Cloud%20Model%20for%20Masked%20Point%0A%20%20Modeling&entry.906535625=Yaohua%20Zha%20and%20Naiqi%20Li%20and%20Yanzi%20Wang%20and%20Tao%20Dai%20and%20Hang%20Guo%20and%20Bin%20Chen%20and%20Zhi%20Wang%20and%20Zhihao%20Ouyang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20The%20pre-trained%20point%20cloud%20model%20based%20on%20Masked%20Point%20Modeling%20%28MPM%29%20has%0Aexhibited%20substantial%20improvements%20across%20various%20tasks.%20However%2C%20these%20models%0Aheavily%20rely%20on%20the%20Transformer%2C%20leading%20to%20quadratic%20complexity%20and%20limited%0Adecoder%2C%20hindering%20their%20practice%20application.%20To%20address%20this%20limitation%2C%20we%0Afirst%20conduct%20a%20comprehensive%20analysis%20of%20existing%20Transformer-based%20MPM%2C%0Aemphasizing%20the%20idea%20that%20redundancy%20reduction%20is%20crucial%20for%20point%20cloud%0Aanalysis.%20To%20this%20end%2C%20we%20propose%20a%20Locally%20constrained%20Compact%20point%20cloud%0AModel%20%28LCM%29%20consisting%20of%20a%20locally%20constrained%20compact%20encoder%20and%20a%20locally%0Aconstrained%20Mamba-based%20decoder.%20Our%20encoder%20replaces%20self-attention%20with%20our%0Alocal%20aggregation%20layers%20to%20achieve%20an%20elegant%20balance%20between%20performance%20and%0Aefficiency.%20Considering%20the%20varying%20information%20density%20between%20masked%20and%0Aunmasked%20patches%20in%20the%20decoder%20inputs%20of%20MPM%2C%20we%20introduce%20a%20locally%0Aconstrained%20Mamba-based%20decoder.%20This%20decoder%20ensures%20linear%20complexity%20while%0Amaximizing%20the%20perception%20of%20point%20cloud%20geometry%20information%20from%20unmasked%0Apatches%20with%20higher%20information%20density.%20Extensive%20experimental%20results%20show%0Athat%20our%20compact%20model%20significantly%20surpasses%20existing%20Transformer-based%0Amodels%20in%20both%20performance%20and%20efficiency%2C%20especially%20our%20LCM-based%20Point-MAE%0Amodel%2C%20compared%20to%20the%20Transformer-based%20model%2C%20achieved%20an%20improvement%20of%0A2.24%25%2C%200.87%25%2C%20and%200.94%25%20in%20performance%20on%20the%20three%20variants%20of%20ScanObjectNN%0Awhile%20reducing%20parameters%20by%2088%25%20and%20computation%20by%2073%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17149v1&entry.124074799=Read"},
{"title": "Collaborative Video Diffusion: Consistent Multi-video Generation with\n  Camera Control", "author": "Zhengfei Kuang and Shengqu Cai and Hao He and Yinghao Xu and Hongsheng Li and Leonidas Guibas and Gordon Wetzstein", "abstract": "  Research on video generation has recently made tremendous progress, enabling\nhigh-quality videos to be generated from text prompts or images. Adding control\nto the video generation process is an important goal moving forward and recent\napproaches that condition video generation models on camera trajectories make\nstrides towards it. Yet, it remains challenging to generate a video of the same\nscene from multiple different camera trajectories. Solutions to this\nmulti-video generation problem could enable large-scale 3D scene generation\nwith editable camera trajectories, among other applications. We introduce\ncollaborative video diffusion (CVD) as an important step towards this vision.\nThe CVD framework includes a novel cross-video synchronization module that\npromotes consistency between corresponding frames of the same video rendered\nfrom different camera poses using an epipolar attention mechanism. Trained on\ntop of a state-of-the-art camera-control module for video generation, CVD\ngenerates multiple videos rendered from different camera trajectories with\nsignificantly better consistency than baselines, as shown in extensive\nexperiments. Project page: https://collaborativevideodiffusion.github.io/.\n", "link": "http://arxiv.org/abs/2405.17414v1", "date": "2024-05-27", "relevancy": 2.6545, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6912}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.654}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Video%20Diffusion%3A%20Consistent%20Multi-video%20Generation%20with%0A%20%20Camera%20Control&body=Title%3A%20Collaborative%20Video%20Diffusion%3A%20Consistent%20Multi-video%20Generation%20with%0A%20%20Camera%20Control%0AAuthor%3A%20Zhengfei%20Kuang%20and%20Shengqu%20Cai%20and%20Hao%20He%20and%20Yinghao%20Xu%20and%20Hongsheng%20Li%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20Research%20on%20video%20generation%20has%20recently%20made%20tremendous%20progress%2C%20enabling%0Ahigh-quality%20videos%20to%20be%20generated%20from%20text%20prompts%20or%20images.%20Adding%20control%0Ato%20the%20video%20generation%20process%20is%20an%20important%20goal%20moving%20forward%20and%20recent%0Aapproaches%20that%20condition%20video%20generation%20models%20on%20camera%20trajectories%20make%0Astrides%20towards%20it.%20Yet%2C%20it%20remains%20challenging%20to%20generate%20a%20video%20of%20the%20same%0Ascene%20from%20multiple%20different%20camera%20trajectories.%20Solutions%20to%20this%0Amulti-video%20generation%20problem%20could%20enable%20large-scale%203D%20scene%20generation%0Awith%20editable%20camera%20trajectories%2C%20among%20other%20applications.%20We%20introduce%0Acollaborative%20video%20diffusion%20%28CVD%29%20as%20an%20important%20step%20towards%20this%20vision.%0AThe%20CVD%20framework%20includes%20a%20novel%20cross-video%20synchronization%20module%20that%0Apromotes%20consistency%20between%20corresponding%20frames%20of%20the%20same%20video%20rendered%0Afrom%20different%20camera%20poses%20using%20an%20epipolar%20attention%20mechanism.%20Trained%20on%0Atop%20of%20a%20state-of-the-art%20camera-control%20module%20for%20video%20generation%2C%20CVD%0Agenerates%20multiple%20videos%20rendered%20from%20different%20camera%20trajectories%20with%0Asignificantly%20better%20consistency%20than%20baselines%2C%20as%20shown%20in%20extensive%0Aexperiments.%20Project%20page%3A%20https%3A//collaborativevideodiffusion.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Video%2520Diffusion%253A%2520Consistent%2520Multi-video%2520Generation%2520with%250A%2520%2520Camera%2520Control%26entry.906535625%3DZhengfei%2520Kuang%2520and%2520Shengqu%2520Cai%2520and%2520Hao%2520He%2520and%2520Yinghao%2520Xu%2520and%2520Hongsheng%2520Li%2520and%2520Leonidas%2520Guibas%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520Research%2520on%2520video%2520generation%2520has%2520recently%2520made%2520tremendous%2520progress%252C%2520enabling%250Ahigh-quality%2520videos%2520to%2520be%2520generated%2520from%2520text%2520prompts%2520or%2520images.%2520Adding%2520control%250Ato%2520the%2520video%2520generation%2520process%2520is%2520an%2520important%2520goal%2520moving%2520forward%2520and%2520recent%250Aapproaches%2520that%2520condition%2520video%2520generation%2520models%2520on%2520camera%2520trajectories%2520make%250Astrides%2520towards%2520it.%2520Yet%252C%2520it%2520remains%2520challenging%2520to%2520generate%2520a%2520video%2520of%2520the%2520same%250Ascene%2520from%2520multiple%2520different%2520camera%2520trajectories.%2520Solutions%2520to%2520this%250Amulti-video%2520generation%2520problem%2520could%2520enable%2520large-scale%25203D%2520scene%2520generation%250Awith%2520editable%2520camera%2520trajectories%252C%2520among%2520other%2520applications.%2520We%2520introduce%250Acollaborative%2520video%2520diffusion%2520%2528CVD%2529%2520as%2520an%2520important%2520step%2520towards%2520this%2520vision.%250AThe%2520CVD%2520framework%2520includes%2520a%2520novel%2520cross-video%2520synchronization%2520module%2520that%250Apromotes%2520consistency%2520between%2520corresponding%2520frames%2520of%2520the%2520same%2520video%2520rendered%250Afrom%2520different%2520camera%2520poses%2520using%2520an%2520epipolar%2520attention%2520mechanism.%2520Trained%2520on%250Atop%2520of%2520a%2520state-of-the-art%2520camera-control%2520module%2520for%2520video%2520generation%252C%2520CVD%250Agenerates%2520multiple%2520videos%2520rendered%2520from%2520different%2520camera%2520trajectories%2520with%250Asignificantly%2520better%2520consistency%2520than%2520baselines%252C%2520as%2520shown%2520in%2520extensive%250Aexperiments.%2520Project%2520page%253A%2520https%253A//collaborativevideodiffusion.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Video%20Diffusion%3A%20Consistent%20Multi-video%20Generation%20with%0A%20%20Camera%20Control&entry.906535625=Zhengfei%20Kuang%20and%20Shengqu%20Cai%20and%20Hao%20He%20and%20Yinghao%20Xu%20and%20Hongsheng%20Li%20and%20Leonidas%20Guibas%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20Research%20on%20video%20generation%20has%20recently%20made%20tremendous%20progress%2C%20enabling%0Ahigh-quality%20videos%20to%20be%20generated%20from%20text%20prompts%20or%20images.%20Adding%20control%0Ato%20the%20video%20generation%20process%20is%20an%20important%20goal%20moving%20forward%20and%20recent%0Aapproaches%20that%20condition%20video%20generation%20models%20on%20camera%20trajectories%20make%0Astrides%20towards%20it.%20Yet%2C%20it%20remains%20challenging%20to%20generate%20a%20video%20of%20the%20same%0Ascene%20from%20multiple%20different%20camera%20trajectories.%20Solutions%20to%20this%0Amulti-video%20generation%20problem%20could%20enable%20large-scale%203D%20scene%20generation%0Awith%20editable%20camera%20trajectories%2C%20among%20other%20applications.%20We%20introduce%0Acollaborative%20video%20diffusion%20%28CVD%29%20as%20an%20important%20step%20towards%20this%20vision.%0AThe%20CVD%20framework%20includes%20a%20novel%20cross-video%20synchronization%20module%20that%0Apromotes%20consistency%20between%20corresponding%20frames%20of%20the%20same%20video%20rendered%0Afrom%20different%20camera%20poses%20using%20an%20epipolar%20attention%20mechanism.%20Trained%20on%0Atop%20of%20a%20state-of-the-art%20camera-control%20module%20for%20video%20generation%2C%20CVD%0Agenerates%20multiple%20videos%20rendered%20from%20different%20camera%20trajectories%20with%0Asignificantly%20better%20consistency%20than%20baselines%2C%20as%20shown%20in%20extensive%0Aexperiments.%20Project%20page%3A%20https%3A//collaborativevideodiffusion.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17414v1&entry.124074799=Read"},
{"title": "UniTable: Towards a Unified Framework for Table Recognition via\n  Self-Supervised Pretraining", "author": "ShengYun Peng and Aishwarya Chakravarthy and Seongmin Lee and Xiaojing Wang and Rajarajeswari Balasubramaniyan and Duen Horng Chau", "abstract": "  Tables convey factual and quantitative data with implicit conventions created\nby humans that are often challenging for machines to parse. Prior work on table\nrecognition (TR) has mainly centered around complex task-specific combinations\nof available inputs and tools. We present UniTable, a training framework that\nunifies both the training paradigm and training objective of TR. Its training\nparadigm combines the simplicity of purely pixel-level inputs with the\neffectiveness and scalability empowered by self-supervised pretraining from\ndiverse unannotated tabular images. Our framework unifies the training\nobjectives of all three TR tasks - extracting table structure, cell content,\nand cell bounding box - into a unified task-agnostic training objective:\nlanguage modeling. Extensive quantitative and qualitative analyses highlight\nUniTable's state-of-the-art (SOTA) performance on four of the largest TR\ndatasets. UniTable's table parsing capability has surpassed both existing TR\nmethods and general large vision-language models, e.g., GPT-4o, GPT-4-turbo\nwith vision, and LLaVA. Our code is publicly available at\nhttps://github.com/poloclub/unitable, featuring a Jupyter Notebook that\nincludes the complete inference pipeline, fine-tuned across multiple TR\ndatasets, supporting all three TR tasks.\n", "link": "http://arxiv.org/abs/2403.04822v2", "date": "2024-05-27", "relevancy": 2.6182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5129}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTable%3A%20Towards%20a%20Unified%20Framework%20for%20Table%20Recognition%20via%0A%20%20Self-Supervised%20Pretraining&body=Title%3A%20UniTable%3A%20Towards%20a%20Unified%20Framework%20for%20Table%20Recognition%20via%0A%20%20Self-Supervised%20Pretraining%0AAuthor%3A%20ShengYun%20Peng%20and%20Aishwarya%20Chakravarthy%20and%20Seongmin%20Lee%20and%20Xiaojing%20Wang%20and%20Rajarajeswari%20Balasubramaniyan%20and%20Duen%20Horng%20Chau%0AAbstract%3A%20%20%20Tables%20convey%20factual%20and%20quantitative%20data%20with%20implicit%20conventions%20created%0Aby%20humans%20that%20are%20often%20challenging%20for%20machines%20to%20parse.%20Prior%20work%20on%20table%0Arecognition%20%28TR%29%20has%20mainly%20centered%20around%20complex%20task-specific%20combinations%0Aof%20available%20inputs%20and%20tools.%20We%20present%20UniTable%2C%20a%20training%20framework%20that%0Aunifies%20both%20the%20training%20paradigm%20and%20training%20objective%20of%20TR.%20Its%20training%0Aparadigm%20combines%20the%20simplicity%20of%20purely%20pixel-level%20inputs%20with%20the%0Aeffectiveness%20and%20scalability%20empowered%20by%20self-supervised%20pretraining%20from%0Adiverse%20unannotated%20tabular%20images.%20Our%20framework%20unifies%20the%20training%0Aobjectives%20of%20all%20three%20TR%20tasks%20-%20extracting%20table%20structure%2C%20cell%20content%2C%0Aand%20cell%20bounding%20box%20-%20into%20a%20unified%20task-agnostic%20training%20objective%3A%0Alanguage%20modeling.%20Extensive%20quantitative%20and%20qualitative%20analyses%20highlight%0AUniTable%27s%20state-of-the-art%20%28SOTA%29%20performance%20on%20four%20of%20the%20largest%20TR%0Adatasets.%20UniTable%27s%20table%20parsing%20capability%20has%20surpassed%20both%20existing%20TR%0Amethods%20and%20general%20large%20vision-language%20models%2C%20e.g.%2C%20GPT-4o%2C%20GPT-4-turbo%0Awith%20vision%2C%20and%20LLaVA.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/poloclub/unitable%2C%20featuring%20a%20Jupyter%20Notebook%20that%0Aincludes%20the%20complete%20inference%20pipeline%2C%20fine-tuned%20across%20multiple%20TR%0Adatasets%2C%20supporting%20all%20three%20TR%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTable%253A%2520Towards%2520a%2520Unified%2520Framework%2520for%2520Table%2520Recognition%2520via%250A%2520%2520Self-Supervised%2520Pretraining%26entry.906535625%3DShengYun%2520Peng%2520and%2520Aishwarya%2520Chakravarthy%2520and%2520Seongmin%2520Lee%2520and%2520Xiaojing%2520Wang%2520and%2520Rajarajeswari%2520Balasubramaniyan%2520and%2520Duen%2520Horng%2520Chau%26entry.1292438233%3D%2520%2520Tables%2520convey%2520factual%2520and%2520quantitative%2520data%2520with%2520implicit%2520conventions%2520created%250Aby%2520humans%2520that%2520are%2520often%2520challenging%2520for%2520machines%2520to%2520parse.%2520Prior%2520work%2520on%2520table%250Arecognition%2520%2528TR%2529%2520has%2520mainly%2520centered%2520around%2520complex%2520task-specific%2520combinations%250Aof%2520available%2520inputs%2520and%2520tools.%2520We%2520present%2520UniTable%252C%2520a%2520training%2520framework%2520that%250Aunifies%2520both%2520the%2520training%2520paradigm%2520and%2520training%2520objective%2520of%2520TR.%2520Its%2520training%250Aparadigm%2520combines%2520the%2520simplicity%2520of%2520purely%2520pixel-level%2520inputs%2520with%2520the%250Aeffectiveness%2520and%2520scalability%2520empowered%2520by%2520self-supervised%2520pretraining%2520from%250Adiverse%2520unannotated%2520tabular%2520images.%2520Our%2520framework%2520unifies%2520the%2520training%250Aobjectives%2520of%2520all%2520three%2520TR%2520tasks%2520-%2520extracting%2520table%2520structure%252C%2520cell%2520content%252C%250Aand%2520cell%2520bounding%2520box%2520-%2520into%2520a%2520unified%2520task-agnostic%2520training%2520objective%253A%250Alanguage%2520modeling.%2520Extensive%2520quantitative%2520and%2520qualitative%2520analyses%2520highlight%250AUniTable%2527s%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520four%2520of%2520the%2520largest%2520TR%250Adatasets.%2520UniTable%2527s%2520table%2520parsing%2520capability%2520has%2520surpassed%2520both%2520existing%2520TR%250Amethods%2520and%2520general%2520large%2520vision-language%2520models%252C%2520e.g.%252C%2520GPT-4o%252C%2520GPT-4-turbo%250Awith%2520vision%252C%2520and%2520LLaVA.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/poloclub/unitable%252C%2520featuring%2520a%2520Jupyter%2520Notebook%2520that%250Aincludes%2520the%2520complete%2520inference%2520pipeline%252C%2520fine-tuned%2520across%2520multiple%2520TR%250Adatasets%252C%2520supporting%2520all%2520three%2520TR%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTable%3A%20Towards%20a%20Unified%20Framework%20for%20Table%20Recognition%20via%0A%20%20Self-Supervised%20Pretraining&entry.906535625=ShengYun%20Peng%20and%20Aishwarya%20Chakravarthy%20and%20Seongmin%20Lee%20and%20Xiaojing%20Wang%20and%20Rajarajeswari%20Balasubramaniyan%20and%20Duen%20Horng%20Chau&entry.1292438233=%20%20Tables%20convey%20factual%20and%20quantitative%20data%20with%20implicit%20conventions%20created%0Aby%20humans%20that%20are%20often%20challenging%20for%20machines%20to%20parse.%20Prior%20work%20on%20table%0Arecognition%20%28TR%29%20has%20mainly%20centered%20around%20complex%20task-specific%20combinations%0Aof%20available%20inputs%20and%20tools.%20We%20present%20UniTable%2C%20a%20training%20framework%20that%0Aunifies%20both%20the%20training%20paradigm%20and%20training%20objective%20of%20TR.%20Its%20training%0Aparadigm%20combines%20the%20simplicity%20of%20purely%20pixel-level%20inputs%20with%20the%0Aeffectiveness%20and%20scalability%20empowered%20by%20self-supervised%20pretraining%20from%0Adiverse%20unannotated%20tabular%20images.%20Our%20framework%20unifies%20the%20training%0Aobjectives%20of%20all%20three%20TR%20tasks%20-%20extracting%20table%20structure%2C%20cell%20content%2C%0Aand%20cell%20bounding%20box%20-%20into%20a%20unified%20task-agnostic%20training%20objective%3A%0Alanguage%20modeling.%20Extensive%20quantitative%20and%20qualitative%20analyses%20highlight%0AUniTable%27s%20state-of-the-art%20%28SOTA%29%20performance%20on%20four%20of%20the%20largest%20TR%0Adatasets.%20UniTable%27s%20table%20parsing%20capability%20has%20surpassed%20both%20existing%20TR%0Amethods%20and%20general%20large%20vision-language%20models%2C%20e.g.%2C%20GPT-4o%2C%20GPT-4-turbo%0Awith%20vision%2C%20and%20LLaVA.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/poloclub/unitable%2C%20featuring%20a%20Jupyter%20Notebook%20that%0Aincludes%20the%20complete%20inference%20pipeline%2C%20fine-tuned%20across%20multiple%20TR%0Adatasets%2C%20supporting%20all%20three%20TR%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04822v2&entry.124074799=Read"},
{"title": "EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs\n  for 3D Indoor Scenes", "author": "Ruichen Wang and Dinesh Manocha", "abstract": "  We present a novel machine-learning (ML) approach (EM-GANSim) for real-time\nelectromagnetic (EM) propagation that is used for wireless communication\nsimulation in 3D indoor environments. Our approach uses a modified conditional\nGenerative Adversarial Network (GAN) that incorporates encoded geometry and\ntransmitter location while adhering to the electromagnetic propagation theory.\nThe overall physically-inspired learning is able to predict the power\ndistribution in 3D scenes, which is represented using heatmaps. Our overall\naccuracy is comparable to ray tracing-based EM simulation, as evidenced by\nlower mean squared error values. Furthermore, our GAN-based method drastically\nreduces the computation time, achieving a 5X speedup on complex benchmarks. In\npractice, it can compute the signal strength in a few milliseconds on any\nlocation in 3D indoor environments. We also present a large dataset of 3D\nmodels and EM ray tracing-simulated heatmaps. To the best of our knowledge,\nEM-GANSim is the first real-time algorithm for EM simulation in complex 3D\nindoor environments. We plan to release the code and the dataset.\n", "link": "http://arxiv.org/abs/2405.17366v1", "date": "2024-05-27", "relevancy": 2.6001, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5296}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5253}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EM-GANSim%3A%20Real-time%20and%20Accurate%20EM%20Simulation%20Using%20Conditional%20GANs%0A%20%20for%203D%20Indoor%20Scenes&body=Title%3A%20EM-GANSim%3A%20Real-time%20and%20Accurate%20EM%20Simulation%20Using%20Conditional%20GANs%0A%20%20for%203D%20Indoor%20Scenes%0AAuthor%3A%20Ruichen%20Wang%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20novel%20machine-learning%20%28ML%29%20approach%20%28EM-GANSim%29%20for%20real-time%0Aelectromagnetic%20%28EM%29%20propagation%20that%20is%20used%20for%20wireless%20communication%0Asimulation%20in%203D%20indoor%20environments.%20Our%20approach%20uses%20a%20modified%20conditional%0AGenerative%20Adversarial%20Network%20%28GAN%29%20that%20incorporates%20encoded%20geometry%20and%0Atransmitter%20location%20while%20adhering%20to%20the%20electromagnetic%20propagation%20theory.%0AThe%20overall%20physically-inspired%20learning%20is%20able%20to%20predict%20the%20power%0Adistribution%20in%203D%20scenes%2C%20which%20is%20represented%20using%20heatmaps.%20Our%20overall%0Aaccuracy%20is%20comparable%20to%20ray%20tracing-based%20EM%20simulation%2C%20as%20evidenced%20by%0Alower%20mean%20squared%20error%20values.%20Furthermore%2C%20our%20GAN-based%20method%20drastically%0Areduces%20the%20computation%20time%2C%20achieving%20a%205X%20speedup%20on%20complex%20benchmarks.%20In%0Apractice%2C%20it%20can%20compute%20the%20signal%20strength%20in%20a%20few%20milliseconds%20on%20any%0Alocation%20in%203D%20indoor%20environments.%20We%20also%20present%20a%20large%20dataset%20of%203D%0Amodels%20and%20EM%20ray%20tracing-simulated%20heatmaps.%20To%20the%20best%20of%20our%20knowledge%2C%0AEM-GANSim%20is%20the%20first%20real-time%20algorithm%20for%20EM%20simulation%20in%20complex%203D%0Aindoor%20environments.%20We%20plan%20to%20release%20the%20code%20and%20the%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEM-GANSim%253A%2520Real-time%2520and%2520Accurate%2520EM%2520Simulation%2520Using%2520Conditional%2520GANs%250A%2520%2520for%25203D%2520Indoor%2520Scenes%26entry.906535625%3DRuichen%2520Wang%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520machine-learning%2520%2528ML%2529%2520approach%2520%2528EM-GANSim%2529%2520for%2520real-time%250Aelectromagnetic%2520%2528EM%2529%2520propagation%2520that%2520is%2520used%2520for%2520wireless%2520communication%250Asimulation%2520in%25203D%2520indoor%2520environments.%2520Our%2520approach%2520uses%2520a%2520modified%2520conditional%250AGenerative%2520Adversarial%2520Network%2520%2528GAN%2529%2520that%2520incorporates%2520encoded%2520geometry%2520and%250Atransmitter%2520location%2520while%2520adhering%2520to%2520the%2520electromagnetic%2520propagation%2520theory.%250AThe%2520overall%2520physically-inspired%2520learning%2520is%2520able%2520to%2520predict%2520the%2520power%250Adistribution%2520in%25203D%2520scenes%252C%2520which%2520is%2520represented%2520using%2520heatmaps.%2520Our%2520overall%250Aaccuracy%2520is%2520comparable%2520to%2520ray%2520tracing-based%2520EM%2520simulation%252C%2520as%2520evidenced%2520by%250Alower%2520mean%2520squared%2520error%2520values.%2520Furthermore%252C%2520our%2520GAN-based%2520method%2520drastically%250Areduces%2520the%2520computation%2520time%252C%2520achieving%2520a%25205X%2520speedup%2520on%2520complex%2520benchmarks.%2520In%250Apractice%252C%2520it%2520can%2520compute%2520the%2520signal%2520strength%2520in%2520a%2520few%2520milliseconds%2520on%2520any%250Alocation%2520in%25203D%2520indoor%2520environments.%2520We%2520also%2520present%2520a%2520large%2520dataset%2520of%25203D%250Amodels%2520and%2520EM%2520ray%2520tracing-simulated%2520heatmaps.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250AEM-GANSim%2520is%2520the%2520first%2520real-time%2520algorithm%2520for%2520EM%2520simulation%2520in%2520complex%25203D%250Aindoor%2520environments.%2520We%2520plan%2520to%2520release%2520the%2520code%2520and%2520the%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EM-GANSim%3A%20Real-time%20and%20Accurate%20EM%20Simulation%20Using%20Conditional%20GANs%0A%20%20for%203D%20Indoor%20Scenes&entry.906535625=Ruichen%20Wang%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20novel%20machine-learning%20%28ML%29%20approach%20%28EM-GANSim%29%20for%20real-time%0Aelectromagnetic%20%28EM%29%20propagation%20that%20is%20used%20for%20wireless%20communication%0Asimulation%20in%203D%20indoor%20environments.%20Our%20approach%20uses%20a%20modified%20conditional%0AGenerative%20Adversarial%20Network%20%28GAN%29%20that%20incorporates%20encoded%20geometry%20and%0Atransmitter%20location%20while%20adhering%20to%20the%20electromagnetic%20propagation%20theory.%0AThe%20overall%20physically-inspired%20learning%20is%20able%20to%20predict%20the%20power%0Adistribution%20in%203D%20scenes%2C%20which%20is%20represented%20using%20heatmaps.%20Our%20overall%0Aaccuracy%20is%20comparable%20to%20ray%20tracing-based%20EM%20simulation%2C%20as%20evidenced%20by%0Alower%20mean%20squared%20error%20values.%20Furthermore%2C%20our%20GAN-based%20method%20drastically%0Areduces%20the%20computation%20time%2C%20achieving%20a%205X%20speedup%20on%20complex%20benchmarks.%20In%0Apractice%2C%20it%20can%20compute%20the%20signal%20strength%20in%20a%20few%20milliseconds%20on%20any%0Alocation%20in%203D%20indoor%20environments.%20We%20also%20present%20a%20large%20dataset%20of%203D%0Amodels%20and%20EM%20ray%20tracing-simulated%20heatmaps.%20To%20the%20best%20of%20our%20knowledge%2C%0AEM-GANSim%20is%20the%20first%20real-time%20algorithm%20for%20EM%20simulation%20in%20complex%203D%0Aindoor%20environments.%20We%20plan%20to%20release%20the%20code%20and%20the%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17366v1&entry.124074799=Read"},
{"title": "Superpixelwise Low-rank Approximation based Partial Label Learning for\n  Hyperspectral Image Classification", "author": "Shujun Yang and Yu Zhang and Yao Ding and Danfeng Hong", "abstract": "  Insufficient prior knowledge of a captured hyperspectral image (HSI) scene\nmay lead the experts or the automatic labeling systems to offer incorrect\nlabels or ambiguous labels (i.e., assigning each training sample to a group of\ncandidate labels, among which only one of them is valid; this is also known as\npartial label learning) during the labeling process. Accordingly, how to learn\nfrom such data with ambiguous labels is a problem of great practical\nimportance. In this paper, we propose a novel superpixelwise low-rank\napproximation (LRA)-based partial label learning method, namely SLAP, which is\nthe first to take into account partial label learning in HSI classification.\nSLAP is mainly composed of two phases: disambiguating the training labels and\nacquiring the predictive model. Specifically, in the first phase, we propose a\nsuperpixelwise LRA-based model, preparing the affinity graph for the subsequent\nlabel propagation process while extracting the discriminative representation to\nenhance the following classification task of the second phase. Then to\ndisambiguate the training labels, label propagation propagates the labeling\ninformation via the affinity graph of training pixels. In the second phase, we\ntake advantage of the resulting disambiguated training labels and the\ndiscriminative representations to enhance the classification performance. The\nextensive experiments validate the advantage of the proposed SLAP method over\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.17110v1", "date": "2024-05-27", "relevancy": 2.593, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5624}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superpixelwise%20Low-rank%20Approximation%20based%20Partial%20Label%20Learning%20for%0A%20%20Hyperspectral%20Image%20Classification&body=Title%3A%20Superpixelwise%20Low-rank%20Approximation%20based%20Partial%20Label%20Learning%20for%0A%20%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Shujun%20Yang%20and%20Yu%20Zhang%20and%20Yao%20Ding%20and%20Danfeng%20Hong%0AAbstract%3A%20%20%20Insufficient%20prior%20knowledge%20of%20a%20captured%20hyperspectral%20image%20%28HSI%29%20scene%0Amay%20lead%20the%20experts%20or%20the%20automatic%20labeling%20systems%20to%20offer%20incorrect%0Alabels%20or%20ambiguous%20labels%20%28i.e.%2C%20assigning%20each%20training%20sample%20to%20a%20group%20of%0Acandidate%20labels%2C%20among%20which%20only%20one%20of%20them%20is%20valid%3B%20this%20is%20also%20known%20as%0Apartial%20label%20learning%29%20during%20the%20labeling%20process.%20Accordingly%2C%20how%20to%20learn%0Afrom%20such%20data%20with%20ambiguous%20labels%20is%20a%20problem%20of%20great%20practical%0Aimportance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20superpixelwise%20low-rank%0Aapproximation%20%28LRA%29-based%20partial%20label%20learning%20method%2C%20namely%20SLAP%2C%20which%20is%0Athe%20first%20to%20take%20into%20account%20partial%20label%20learning%20in%20HSI%20classification.%0ASLAP%20is%20mainly%20composed%20of%20two%20phases%3A%20disambiguating%20the%20training%20labels%20and%0Aacquiring%20the%20predictive%20model.%20Specifically%2C%20in%20the%20first%20phase%2C%20we%20propose%20a%0Asuperpixelwise%20LRA-based%20model%2C%20preparing%20the%20affinity%20graph%20for%20the%20subsequent%0Alabel%20propagation%20process%20while%20extracting%20the%20discriminative%20representation%20to%0Aenhance%20the%20following%20classification%20task%20of%20the%20second%20phase.%20Then%20to%0Adisambiguate%20the%20training%20labels%2C%20label%20propagation%20propagates%20the%20labeling%0Ainformation%20via%20the%20affinity%20graph%20of%20training%20pixels.%20In%20the%20second%20phase%2C%20we%0Atake%20advantage%20of%20the%20resulting%20disambiguated%20training%20labels%20and%20the%0Adiscriminative%20representations%20to%20enhance%20the%20classification%20performance.%20The%0Aextensive%20experiments%20validate%20the%20advantage%20of%20the%20proposed%20SLAP%20method%20over%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperpixelwise%2520Low-rank%2520Approximation%2520based%2520Partial%2520Label%2520Learning%2520for%250A%2520%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DShujun%2520Yang%2520and%2520Yu%2520Zhang%2520and%2520Yao%2520Ding%2520and%2520Danfeng%2520Hong%26entry.1292438233%3D%2520%2520Insufficient%2520prior%2520knowledge%2520of%2520a%2520captured%2520hyperspectral%2520image%2520%2528HSI%2529%2520scene%250Amay%2520lead%2520the%2520experts%2520or%2520the%2520automatic%2520labeling%2520systems%2520to%2520offer%2520incorrect%250Alabels%2520or%2520ambiguous%2520labels%2520%2528i.e.%252C%2520assigning%2520each%2520training%2520sample%2520to%2520a%2520group%2520of%250Acandidate%2520labels%252C%2520among%2520which%2520only%2520one%2520of%2520them%2520is%2520valid%253B%2520this%2520is%2520also%2520known%2520as%250Apartial%2520label%2520learning%2529%2520during%2520the%2520labeling%2520process.%2520Accordingly%252C%2520how%2520to%2520learn%250Afrom%2520such%2520data%2520with%2520ambiguous%2520labels%2520is%2520a%2520problem%2520of%2520great%2520practical%250Aimportance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520superpixelwise%2520low-rank%250Aapproximation%2520%2528LRA%2529-based%2520partial%2520label%2520learning%2520method%252C%2520namely%2520SLAP%252C%2520which%2520is%250Athe%2520first%2520to%2520take%2520into%2520account%2520partial%2520label%2520learning%2520in%2520HSI%2520classification.%250ASLAP%2520is%2520mainly%2520composed%2520of%2520two%2520phases%253A%2520disambiguating%2520the%2520training%2520labels%2520and%250Aacquiring%2520the%2520predictive%2520model.%2520Specifically%252C%2520in%2520the%2520first%2520phase%252C%2520we%2520propose%2520a%250Asuperpixelwise%2520LRA-based%2520model%252C%2520preparing%2520the%2520affinity%2520graph%2520for%2520the%2520subsequent%250Alabel%2520propagation%2520process%2520while%2520extracting%2520the%2520discriminative%2520representation%2520to%250Aenhance%2520the%2520following%2520classification%2520task%2520of%2520the%2520second%2520phase.%2520Then%2520to%250Adisambiguate%2520the%2520training%2520labels%252C%2520label%2520propagation%2520propagates%2520the%2520labeling%250Ainformation%2520via%2520the%2520affinity%2520graph%2520of%2520training%2520pixels.%2520In%2520the%2520second%2520phase%252C%2520we%250Atake%2520advantage%2520of%2520the%2520resulting%2520disambiguated%2520training%2520labels%2520and%2520the%250Adiscriminative%2520representations%2520to%2520enhance%2520the%2520classification%2520performance.%2520The%250Aextensive%2520experiments%2520validate%2520the%2520advantage%2520of%2520the%2520proposed%2520SLAP%2520method%2520over%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superpixelwise%20Low-rank%20Approximation%20based%20Partial%20Label%20Learning%20for%0A%20%20Hyperspectral%20Image%20Classification&entry.906535625=Shujun%20Yang%20and%20Yu%20Zhang%20and%20Yao%20Ding%20and%20Danfeng%20Hong&entry.1292438233=%20%20Insufficient%20prior%20knowledge%20of%20a%20captured%20hyperspectral%20image%20%28HSI%29%20scene%0Amay%20lead%20the%20experts%20or%20the%20automatic%20labeling%20systems%20to%20offer%20incorrect%0Alabels%20or%20ambiguous%20labels%20%28i.e.%2C%20assigning%20each%20training%20sample%20to%20a%20group%20of%0Acandidate%20labels%2C%20among%20which%20only%20one%20of%20them%20is%20valid%3B%20this%20is%20also%20known%20as%0Apartial%20label%20learning%29%20during%20the%20labeling%20process.%20Accordingly%2C%20how%20to%20learn%0Afrom%20such%20data%20with%20ambiguous%20labels%20is%20a%20problem%20of%20great%20practical%0Aimportance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20superpixelwise%20low-rank%0Aapproximation%20%28LRA%29-based%20partial%20label%20learning%20method%2C%20namely%20SLAP%2C%20which%20is%0Athe%20first%20to%20take%20into%20account%20partial%20label%20learning%20in%20HSI%20classification.%0ASLAP%20is%20mainly%20composed%20of%20two%20phases%3A%20disambiguating%20the%20training%20labels%20and%0Aacquiring%20the%20predictive%20model.%20Specifically%2C%20in%20the%20first%20phase%2C%20we%20propose%20a%0Asuperpixelwise%20LRA-based%20model%2C%20preparing%20the%20affinity%20graph%20for%20the%20subsequent%0Alabel%20propagation%20process%20while%20extracting%20the%20discriminative%20representation%20to%0Aenhance%20the%20following%20classification%20task%20of%20the%20second%20phase.%20Then%20to%0Adisambiguate%20the%20training%20labels%2C%20label%20propagation%20propagates%20the%20labeling%0Ainformation%20via%20the%20affinity%20graph%20of%20training%20pixels.%20In%20the%20second%20phase%2C%20we%0Atake%20advantage%20of%20the%20resulting%20disambiguated%20training%20labels%20and%20the%0Adiscriminative%20representations%20to%20enhance%20the%20classification%20performance.%20The%0Aextensive%20experiments%20validate%20the%20advantage%20of%20the%20proposed%20SLAP%20method%20over%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17110v1&entry.124074799=Read"},
{"title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks", "author": "Taraneh Younesian and Daniel Daza and Emile van Krieken and Thiviyan Thanapalasingam and Peter Bloem", "abstract": "  Graph neural networks (GNNs) learn to represent nodes by aggregating\ninformation from their neighbors. As GNNs increase in depth, their receptive\nfield grows exponentially, leading to high memory costs. Several existing\nmethods address this by sampling a small subset of nodes, scaling GNNs to much\nlarger graphs. These methods are primarily evaluated on homophilous graphs,\nwhere neighboring nodes often share the same label. However, most of these\nmethods rely on static heuristics that may not generalize across different\ngraphs or tasks. We argue that the sampling method should be adaptive,\nadjusting to the complex structural properties of each graph. To this end, we\nintroduce GRAPES, an adaptive sampling method that learns to identify the set\nof nodes crucial for training a GNN. GRAPES trains a second GNN to predict node\nsampling probabilities by optimizing the downstream task objective. We evaluate\nGRAPES on various node classification benchmarks, involving homophilous as well\nas heterophilous graphs. We demonstrate GRAPES' effectiveness in accuracy and\nscalability, particularly in multi-label heterophilous graphs. Unlike other\nsampling methods, GRAPES maintains high accuracy even with smaller sample sizes\nand, therefore, can scale to massive graphs. Our code is publicly available at\nhttps://github.com/dfdazac/grapes.\n", "link": "http://arxiv.org/abs/2310.03399v2", "date": "2024-05-27", "relevancy": 2.5892, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAPES%3A%20Learning%20to%20Sample%20Graphs%20for%20Scalable%20Graph%20Neural%20Networks&body=Title%3A%20GRAPES%3A%20Learning%20to%20Sample%20Graphs%20for%20Scalable%20Graph%20Neural%20Networks%0AAuthor%3A%20Taraneh%20Younesian%20and%20Daniel%20Daza%20and%20Emile%20van%20Krieken%20and%20Thiviyan%20Thanapalasingam%20and%20Peter%20Bloem%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20learn%20to%20represent%20nodes%20by%20aggregating%0Ainformation%20from%20their%20neighbors.%20As%20GNNs%20increase%20in%20depth%2C%20their%20receptive%0Afield%20grows%20exponentially%2C%20leading%20to%20high%20memory%20costs.%20Several%20existing%0Amethods%20address%20this%20by%20sampling%20a%20small%20subset%20of%20nodes%2C%20scaling%20GNNs%20to%20much%0Alarger%20graphs.%20These%20methods%20are%20primarily%20evaluated%20on%20homophilous%20graphs%2C%0Awhere%20neighboring%20nodes%20often%20share%20the%20same%20label.%20However%2C%20most%20of%20these%0Amethods%20rely%20on%20static%20heuristics%20that%20may%20not%20generalize%20across%20different%0Agraphs%20or%20tasks.%20We%20argue%20that%20the%20sampling%20method%20should%20be%20adaptive%2C%0Aadjusting%20to%20the%20complex%20structural%20properties%20of%20each%20graph.%20To%20this%20end%2C%20we%0Aintroduce%20GRAPES%2C%20an%20adaptive%20sampling%20method%20that%20learns%20to%20identify%20the%20set%0Aof%20nodes%20crucial%20for%20training%20a%20GNN.%20GRAPES%20trains%20a%20second%20GNN%20to%20predict%20node%0Asampling%20probabilities%20by%20optimizing%20the%20downstream%20task%20objective.%20We%20evaluate%0AGRAPES%20on%20various%20node%20classification%20benchmarks%2C%20involving%20homophilous%20as%20well%0Aas%20heterophilous%20graphs.%20We%20demonstrate%20GRAPES%27%20effectiveness%20in%20accuracy%20and%0Ascalability%2C%20particularly%20in%20multi-label%20heterophilous%20graphs.%20Unlike%20other%0Asampling%20methods%2C%20GRAPES%20maintains%20high%20accuracy%20even%20with%20smaller%20sample%20sizes%0Aand%2C%20therefore%2C%20can%20scale%20to%20massive%20graphs.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dfdazac/grapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAPES%253A%2520Learning%2520to%2520Sample%2520Graphs%2520for%2520Scalable%2520Graph%2520Neural%2520Networks%26entry.906535625%3DTaraneh%2520Younesian%2520and%2520Daniel%2520Daza%2520and%2520Emile%2520van%2520Krieken%2520and%2520Thiviyan%2520Thanapalasingam%2520and%2520Peter%2520Bloem%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520learn%2520to%2520represent%2520nodes%2520by%2520aggregating%250Ainformation%2520from%2520their%2520neighbors.%2520As%2520GNNs%2520increase%2520in%2520depth%252C%2520their%2520receptive%250Afield%2520grows%2520exponentially%252C%2520leading%2520to%2520high%2520memory%2520costs.%2520Several%2520existing%250Amethods%2520address%2520this%2520by%2520sampling%2520a%2520small%2520subset%2520of%2520nodes%252C%2520scaling%2520GNNs%2520to%2520much%250Alarger%2520graphs.%2520These%2520methods%2520are%2520primarily%2520evaluated%2520on%2520homophilous%2520graphs%252C%250Awhere%2520neighboring%2520nodes%2520often%2520share%2520the%2520same%2520label.%2520However%252C%2520most%2520of%2520these%250Amethods%2520rely%2520on%2520static%2520heuristics%2520that%2520may%2520not%2520generalize%2520across%2520different%250Agraphs%2520or%2520tasks.%2520We%2520argue%2520that%2520the%2520sampling%2520method%2520should%2520be%2520adaptive%252C%250Aadjusting%2520to%2520the%2520complex%2520structural%2520properties%2520of%2520each%2520graph.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520GRAPES%252C%2520an%2520adaptive%2520sampling%2520method%2520that%2520learns%2520to%2520identify%2520the%2520set%250Aof%2520nodes%2520crucial%2520for%2520training%2520a%2520GNN.%2520GRAPES%2520trains%2520a%2520second%2520GNN%2520to%2520predict%2520node%250Asampling%2520probabilities%2520by%2520optimizing%2520the%2520downstream%2520task%2520objective.%2520We%2520evaluate%250AGRAPES%2520on%2520various%2520node%2520classification%2520benchmarks%252C%2520involving%2520homophilous%2520as%2520well%250Aas%2520heterophilous%2520graphs.%2520We%2520demonstrate%2520GRAPES%2527%2520effectiveness%2520in%2520accuracy%2520and%250Ascalability%252C%2520particularly%2520in%2520multi-label%2520heterophilous%2520graphs.%2520Unlike%2520other%250Asampling%2520methods%252C%2520GRAPES%2520maintains%2520high%2520accuracy%2520even%2520with%2520smaller%2520sample%2520sizes%250Aand%252C%2520therefore%252C%2520can%2520scale%2520to%2520massive%2520graphs.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/dfdazac/grapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAPES%3A%20Learning%20to%20Sample%20Graphs%20for%20Scalable%20Graph%20Neural%20Networks&entry.906535625=Taraneh%20Younesian%20and%20Daniel%20Daza%20and%20Emile%20van%20Krieken%20and%20Thiviyan%20Thanapalasingam%20and%20Peter%20Bloem&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20learn%20to%20represent%20nodes%20by%20aggregating%0Ainformation%20from%20their%20neighbors.%20As%20GNNs%20increase%20in%20depth%2C%20their%20receptive%0Afield%20grows%20exponentially%2C%20leading%20to%20high%20memory%20costs.%20Several%20existing%0Amethods%20address%20this%20by%20sampling%20a%20small%20subset%20of%20nodes%2C%20scaling%20GNNs%20to%20much%0Alarger%20graphs.%20These%20methods%20are%20primarily%20evaluated%20on%20homophilous%20graphs%2C%0Awhere%20neighboring%20nodes%20often%20share%20the%20same%20label.%20However%2C%20most%20of%20these%0Amethods%20rely%20on%20static%20heuristics%20that%20may%20not%20generalize%20across%20different%0Agraphs%20or%20tasks.%20We%20argue%20that%20the%20sampling%20method%20should%20be%20adaptive%2C%0Aadjusting%20to%20the%20complex%20structural%20properties%20of%20each%20graph.%20To%20this%20end%2C%20we%0Aintroduce%20GRAPES%2C%20an%20adaptive%20sampling%20method%20that%20learns%20to%20identify%20the%20set%0Aof%20nodes%20crucial%20for%20training%20a%20GNN.%20GRAPES%20trains%20a%20second%20GNN%20to%20predict%20node%0Asampling%20probabilities%20by%20optimizing%20the%20downstream%20task%20objective.%20We%20evaluate%0AGRAPES%20on%20various%20node%20classification%20benchmarks%2C%20involving%20homophilous%20as%20well%0Aas%20heterophilous%20graphs.%20We%20demonstrate%20GRAPES%27%20effectiveness%20in%20accuracy%20and%0Ascalability%2C%20particularly%20in%20multi-label%20heterophilous%20graphs.%20Unlike%20other%0Asampling%20methods%2C%20GRAPES%20maintains%20high%20accuracy%20even%20with%20smaller%20sample%20sizes%0Aand%2C%20therefore%2C%20can%20scale%20to%20massive%20graphs.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/dfdazac/grapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03399v2&entry.124074799=Read"},
{"title": "Advancements in Tactile Hand Gesture Recognition for Enhanced\n  Human-Machine Interaction", "author": "Chiara Fumelli and Anirvan Dutta and Mohsen Kaboli", "abstract": "  Motivated by the growing interest in enhancing intuitive physical\nHuman-Machine Interaction (HRI/HVI), this study aims to propose a robust\ntactile hand gesture recognition system. We performed a comprehensive\nevaluation of different hand gesture recognition approaches for a large area\ntactile sensing interface (touch interface) constructed from conductive\ntextiles. Our evaluation encompassed traditional feature engineering methods,\nas well as contemporary deep learning techniques capable of real-time\ninterpretation of a range of hand gestures, accommodating variations in hand\nsizes, movement velocities, applied pressure levels, and interaction points.\nOur extensive analysis of the various methods makes a significant contribution\nto tactile-based gesture recognition in the field of human-machine interaction.\n", "link": "http://arxiv.org/abs/2405.17038v1", "date": "2024-05-27", "relevancy": 2.5783, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5392}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.513}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Tactile%20Hand%20Gesture%20Recognition%20for%20Enhanced%0A%20%20Human-Machine%20Interaction&body=Title%3A%20Advancements%20in%20Tactile%20Hand%20Gesture%20Recognition%20for%20Enhanced%0A%20%20Human-Machine%20Interaction%0AAuthor%3A%20Chiara%20Fumelli%20and%20Anirvan%20Dutta%20and%20Mohsen%20Kaboli%0AAbstract%3A%20%20%20Motivated%20by%20the%20growing%20interest%20in%20enhancing%20intuitive%20physical%0AHuman-Machine%20Interaction%20%28HRI/HVI%29%2C%20this%20study%20aims%20to%20propose%20a%20robust%0Atactile%20hand%20gesture%20recognition%20system.%20We%20performed%20a%20comprehensive%0Aevaluation%20of%20different%20hand%20gesture%20recognition%20approaches%20for%20a%20large%20area%0Atactile%20sensing%20interface%20%28touch%20interface%29%20constructed%20from%20conductive%0Atextiles.%20Our%20evaluation%20encompassed%20traditional%20feature%20engineering%20methods%2C%0Aas%20well%20as%20contemporary%20deep%20learning%20techniques%20capable%20of%20real-time%0Ainterpretation%20of%20a%20range%20of%20hand%20gestures%2C%20accommodating%20variations%20in%20hand%0Asizes%2C%20movement%20velocities%2C%20applied%20pressure%20levels%2C%20and%20interaction%20points.%0AOur%20extensive%20analysis%20of%20the%20various%20methods%20makes%20a%20significant%20contribution%0Ato%20tactile-based%20gesture%20recognition%20in%20the%20field%20of%20human-machine%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520in%2520Tactile%2520Hand%2520Gesture%2520Recognition%2520for%2520Enhanced%250A%2520%2520Human-Machine%2520Interaction%26entry.906535625%3DChiara%2520Fumelli%2520and%2520Anirvan%2520Dutta%2520and%2520Mohsen%2520Kaboli%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520growing%2520interest%2520in%2520enhancing%2520intuitive%2520physical%250AHuman-Machine%2520Interaction%2520%2528HRI/HVI%2529%252C%2520this%2520study%2520aims%2520to%2520propose%2520a%2520robust%250Atactile%2520hand%2520gesture%2520recognition%2520system.%2520We%2520performed%2520a%2520comprehensive%250Aevaluation%2520of%2520different%2520hand%2520gesture%2520recognition%2520approaches%2520for%2520a%2520large%2520area%250Atactile%2520sensing%2520interface%2520%2528touch%2520interface%2529%2520constructed%2520from%2520conductive%250Atextiles.%2520Our%2520evaluation%2520encompassed%2520traditional%2520feature%2520engineering%2520methods%252C%250Aas%2520well%2520as%2520contemporary%2520deep%2520learning%2520techniques%2520capable%2520of%2520real-time%250Ainterpretation%2520of%2520a%2520range%2520of%2520hand%2520gestures%252C%2520accommodating%2520variations%2520in%2520hand%250Asizes%252C%2520movement%2520velocities%252C%2520applied%2520pressure%2520levels%252C%2520and%2520interaction%2520points.%250AOur%2520extensive%2520analysis%2520of%2520the%2520various%2520methods%2520makes%2520a%2520significant%2520contribution%250Ato%2520tactile-based%2520gesture%2520recognition%2520in%2520the%2520field%2520of%2520human-machine%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Tactile%20Hand%20Gesture%20Recognition%20for%20Enhanced%0A%20%20Human-Machine%20Interaction&entry.906535625=Chiara%20Fumelli%20and%20Anirvan%20Dutta%20and%20Mohsen%20Kaboli&entry.1292438233=%20%20Motivated%20by%20the%20growing%20interest%20in%20enhancing%20intuitive%20physical%0AHuman-Machine%20Interaction%20%28HRI/HVI%29%2C%20this%20study%20aims%20to%20propose%20a%20robust%0Atactile%20hand%20gesture%20recognition%20system.%20We%20performed%20a%20comprehensive%0Aevaluation%20of%20different%20hand%20gesture%20recognition%20approaches%20for%20a%20large%20area%0Atactile%20sensing%20interface%20%28touch%20interface%29%20constructed%20from%20conductive%0Atextiles.%20Our%20evaluation%20encompassed%20traditional%20feature%20engineering%20methods%2C%0Aas%20well%20as%20contemporary%20deep%20learning%20techniques%20capable%20of%20real-time%0Ainterpretation%20of%20a%20range%20of%20hand%20gestures%2C%20accommodating%20variations%20in%20hand%0Asizes%2C%20movement%20velocities%2C%20applied%20pressure%20levels%2C%20and%20interaction%20points.%0AOur%20extensive%20analysis%20of%20the%20various%20methods%20makes%20a%20significant%20contribution%0Ato%20tactile-based%20gesture%20recognition%20in%20the%20field%20of%20human-machine%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17038v1&entry.124074799=Read"},
{"title": "Gaussian Embedding of Temporal Networks", "author": "Rapha\u00ebl Romero and Jefrey Lijffijt and Riccardo Rastelli and Marco Corneli and Tijl De Bie", "abstract": "  Representing the nodes of continuous-time temporal graphs in a\nlow-dimensional latent space has wide-ranging applications, from prediction to\nvisualization. Yet, analyzing continuous-time relational data with timestamped\ninteractions introduces unique challenges due to its sparsity. Merely embedding\nnodes as trajectories in the latent space overlooks this sparsity, emphasizing\nthe need to quantify uncertainty around the latent positions. In this paper, we\npropose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork\n\\textbf{E}mbedding), an innovative method that bridges two distinct strands of\nliterature: the statistical analysis of networks via Latent Space Models\n(LSM)\\cite{Hoff2002} and temporal graph machine learning. TGNE embeds nodes as\npiece-wise linear trajectories of Gaussian distributions in the latent space,\ncapturing both structural information and uncertainty around the trajectories.\nWe evaluate TGNE's effectiveness in reconstructing the original graph and\nmodelling uncertainty. The results demonstrate that TGNE generates competitive\ntime-varying embedding locations compared to common baselines for\nreconstructing unobserved edge interactions based on observed edges.\nFurthermore, the uncertainty estimates align with the time-varying degree\ndistribution in the network, providing valuable insights into the temporal\ndynamics of the graph. To facilitate reproducibility, we provide an open-source\nimplementation of TGNE at \\url{https://github.com/aida-ugent/tgne}.\n", "link": "http://arxiv.org/abs/2405.17253v1", "date": "2024-05-27", "relevancy": 2.5584, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5589}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5065}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Embedding%20of%20Temporal%20Networks&body=Title%3A%20Gaussian%20Embedding%20of%20Temporal%20Networks%0AAuthor%3A%20Rapha%C3%ABl%20Romero%20and%20Jefrey%20Lijffijt%20and%20Riccardo%20Rastelli%20and%20Marco%20Corneli%20and%20Tijl%20De%20Bie%0AAbstract%3A%20%20%20Representing%20the%20nodes%20of%20continuous-time%20temporal%20graphs%20in%20a%0Alow-dimensional%20latent%20space%20has%20wide-ranging%20applications%2C%20from%20prediction%20to%0Avisualization.%20Yet%2C%20analyzing%20continuous-time%20relational%20data%20with%20timestamped%0Ainteractions%20introduces%20unique%20challenges%20due%20to%20its%20sparsity.%20Merely%20embedding%0Anodes%20as%20trajectories%20in%20the%20latent%20space%20overlooks%20this%20sparsity%2C%20emphasizing%0Athe%20need%20to%20quantify%20uncertainty%20around%20the%20latent%20positions.%20In%20this%20paper%2C%20we%0Apropose%20TGNE%20%28%5Ctextbf%7BT%7Demporal%20%5Ctextbf%7BG%7Daussian%20%5Ctextbf%7BN%7Detwork%0A%5Ctextbf%7BE%7Dmbedding%29%2C%20an%20innovative%20method%20that%20bridges%20two%20distinct%20strands%20of%0Aliterature%3A%20the%20statistical%20analysis%20of%20networks%20via%20Latent%20Space%20Models%0A%28LSM%29%5Ccite%7BHoff2002%7D%20and%20temporal%20graph%20machine%20learning.%20TGNE%20embeds%20nodes%20as%0Apiece-wise%20linear%20trajectories%20of%20Gaussian%20distributions%20in%20the%20latent%20space%2C%0Acapturing%20both%20structural%20information%20and%20uncertainty%20around%20the%20trajectories.%0AWe%20evaluate%20TGNE%27s%20effectiveness%20in%20reconstructing%20the%20original%20graph%20and%0Amodelling%20uncertainty.%20The%20results%20demonstrate%20that%20TGNE%20generates%20competitive%0Atime-varying%20embedding%20locations%20compared%20to%20common%20baselines%20for%0Areconstructing%20unobserved%20edge%20interactions%20based%20on%20observed%20edges.%0AFurthermore%2C%20the%20uncertainty%20estimates%20align%20with%20the%20time-varying%20degree%0Adistribution%20in%20the%20network%2C%20providing%20valuable%20insights%20into%20the%20temporal%0Adynamics%20of%20the%20graph.%20To%20facilitate%20reproducibility%2C%20we%20provide%20an%20open-source%0Aimplementation%20of%20TGNE%20at%20%5Curl%7Bhttps%3A//github.com/aida-ugent/tgne%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Embedding%2520of%2520Temporal%2520Networks%26entry.906535625%3DRapha%25C3%25ABl%2520Romero%2520and%2520Jefrey%2520Lijffijt%2520and%2520Riccardo%2520Rastelli%2520and%2520Marco%2520Corneli%2520and%2520Tijl%2520De%2520Bie%26entry.1292438233%3D%2520%2520Representing%2520the%2520nodes%2520of%2520continuous-time%2520temporal%2520graphs%2520in%2520a%250Alow-dimensional%2520latent%2520space%2520has%2520wide-ranging%2520applications%252C%2520from%2520prediction%2520to%250Avisualization.%2520Yet%252C%2520analyzing%2520continuous-time%2520relational%2520data%2520with%2520timestamped%250Ainteractions%2520introduces%2520unique%2520challenges%2520due%2520to%2520its%2520sparsity.%2520Merely%2520embedding%250Anodes%2520as%2520trajectories%2520in%2520the%2520latent%2520space%2520overlooks%2520this%2520sparsity%252C%2520emphasizing%250Athe%2520need%2520to%2520quantify%2520uncertainty%2520around%2520the%2520latent%2520positions.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520TGNE%2520%2528%255Ctextbf%257BT%257Demporal%2520%255Ctextbf%257BG%257Daussian%2520%255Ctextbf%257BN%257Detwork%250A%255Ctextbf%257BE%257Dmbedding%2529%252C%2520an%2520innovative%2520method%2520that%2520bridges%2520two%2520distinct%2520strands%2520of%250Aliterature%253A%2520the%2520statistical%2520analysis%2520of%2520networks%2520via%2520Latent%2520Space%2520Models%250A%2528LSM%2529%255Ccite%257BHoff2002%257D%2520and%2520temporal%2520graph%2520machine%2520learning.%2520TGNE%2520embeds%2520nodes%2520as%250Apiece-wise%2520linear%2520trajectories%2520of%2520Gaussian%2520distributions%2520in%2520the%2520latent%2520space%252C%250Acapturing%2520both%2520structural%2520information%2520and%2520uncertainty%2520around%2520the%2520trajectories.%250AWe%2520evaluate%2520TGNE%2527s%2520effectiveness%2520in%2520reconstructing%2520the%2520original%2520graph%2520and%250Amodelling%2520uncertainty.%2520The%2520results%2520demonstrate%2520that%2520TGNE%2520generates%2520competitive%250Atime-varying%2520embedding%2520locations%2520compared%2520to%2520common%2520baselines%2520for%250Areconstructing%2520unobserved%2520edge%2520interactions%2520based%2520on%2520observed%2520edges.%250AFurthermore%252C%2520the%2520uncertainty%2520estimates%2520align%2520with%2520the%2520time-varying%2520degree%250Adistribution%2520in%2520the%2520network%252C%2520providing%2520valuable%2520insights%2520into%2520the%2520temporal%250Adynamics%2520of%2520the%2520graph.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520provide%2520an%2520open-source%250Aimplementation%2520of%2520TGNE%2520at%2520%255Curl%257Bhttps%253A//github.com/aida-ugent/tgne%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Embedding%20of%20Temporal%20Networks&entry.906535625=Rapha%C3%ABl%20Romero%20and%20Jefrey%20Lijffijt%20and%20Riccardo%20Rastelli%20and%20Marco%20Corneli%20and%20Tijl%20De%20Bie&entry.1292438233=%20%20Representing%20the%20nodes%20of%20continuous-time%20temporal%20graphs%20in%20a%0Alow-dimensional%20latent%20space%20has%20wide-ranging%20applications%2C%20from%20prediction%20to%0Avisualization.%20Yet%2C%20analyzing%20continuous-time%20relational%20data%20with%20timestamped%0Ainteractions%20introduces%20unique%20challenges%20due%20to%20its%20sparsity.%20Merely%20embedding%0Anodes%20as%20trajectories%20in%20the%20latent%20space%20overlooks%20this%20sparsity%2C%20emphasizing%0Athe%20need%20to%20quantify%20uncertainty%20around%20the%20latent%20positions.%20In%20this%20paper%2C%20we%0Apropose%20TGNE%20%28%5Ctextbf%7BT%7Demporal%20%5Ctextbf%7BG%7Daussian%20%5Ctextbf%7BN%7Detwork%0A%5Ctextbf%7BE%7Dmbedding%29%2C%20an%20innovative%20method%20that%20bridges%20two%20distinct%20strands%20of%0Aliterature%3A%20the%20statistical%20analysis%20of%20networks%20via%20Latent%20Space%20Models%0A%28LSM%29%5Ccite%7BHoff2002%7D%20and%20temporal%20graph%20machine%20learning.%20TGNE%20embeds%20nodes%20as%0Apiece-wise%20linear%20trajectories%20of%20Gaussian%20distributions%20in%20the%20latent%20space%2C%0Acapturing%20both%20structural%20information%20and%20uncertainty%20around%20the%20trajectories.%0AWe%20evaluate%20TGNE%27s%20effectiveness%20in%20reconstructing%20the%20original%20graph%20and%0Amodelling%20uncertainty.%20The%20results%20demonstrate%20that%20TGNE%20generates%20competitive%0Atime-varying%20embedding%20locations%20compared%20to%20common%20baselines%20for%0Areconstructing%20unobserved%20edge%20interactions%20based%20on%20observed%20edges.%0AFurthermore%2C%20the%20uncertainty%20estimates%20align%20with%20the%20time-varying%20degree%0Adistribution%20in%20the%20network%2C%20providing%20valuable%20insights%20into%20the%20temporal%0Adynamics%20of%20the%20graph.%20To%20facilitate%20reproducibility%2C%20we%20provide%20an%20open-source%0Aimplementation%20of%20TGNE%20at%20%5Curl%7Bhttps%3A//github.com/aida-ugent/tgne%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17253v1&entry.124074799=Read"},
{"title": "HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed\n  via Gaussian Splatting", "author": "Yuanhao Cai and Zihao Xiao and Yixun Liang and Minghan Qin and Yulun Zhang and Xiaokang Yang and Yaoyao Liu and Alan Yuille", "abstract": "  High dynamic range (HDR) novel view synthesis (NVS) aims to create\nphotorealistic images from novel viewpoints using HDR imaging techniques. The\nrendered HDR images capture a wider range of brightness levels containing more\ndetails of the scene than normal low dynamic range (LDR) images. Existing HDR\nNVS methods are mainly based on NeRF. They suffer from long training time and\nslow inference speed. In this paper, we propose a new framework, High Dynamic\nRange Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views\nand reconstruct LDR images with a user input exposure time. Specifically, we\ndesign a Dual Dynamic Range (DDR) Gaussian point cloud model that uses\nspherical harmonics to fit HDR color and employs an MLP-based tone-mapper to\nrender LDR color. The HDR and LDR colors are then fed into two Parallel\nDifferentiable Rasterization (PDR) processes to reconstruct HDR and LDR views.\nTo establish the data foundation for the research of 3D Gaussian\nsplatting-based methods in HDR NVS, we recalibrate the camera parameters and\ncompute the initial positions for Gaussian point clouds. Experiments\ndemonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by\n3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and\nonly requiring 6.3% training time. Code, models, and recalibrated data will be\npublicly available at https://github.com/caiyuanhao1998/HDR-GS\n", "link": "http://arxiv.org/abs/2405.15125v2", "date": "2024-05-27", "relevancy": 2.5513, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7032}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.599}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HDR-GS%3A%20Efficient%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20at%201000x%20Speed%0A%20%20via%20Gaussian%20Splatting&body=Title%3A%20HDR-GS%3A%20Efficient%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20at%201000x%20Speed%0A%20%20via%20Gaussian%20Splatting%0AAuthor%3A%20Yuanhao%20Cai%20and%20Zihao%20Xiao%20and%20Yixun%20Liang%20and%20Minghan%20Qin%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang%20and%20Yaoyao%20Liu%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20High%20dynamic%20range%20%28HDR%29%20novel%20view%20synthesis%20%28NVS%29%20aims%20to%20create%0Aphotorealistic%20images%20from%20novel%20viewpoints%20using%20HDR%20imaging%20techniques.%20The%0Arendered%20HDR%20images%20capture%20a%20wider%20range%20of%20brightness%20levels%20containing%20more%0Adetails%20of%20the%20scene%20than%20normal%20low%20dynamic%20range%20%28LDR%29%20images.%20Existing%20HDR%0ANVS%20methods%20are%20mainly%20based%20on%20NeRF.%20They%20suffer%20from%20long%20training%20time%20and%0Aslow%20inference%20speed.%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%2C%20High%20Dynamic%0ARange%20Gaussian%20Splatting%20%28HDR-GS%29%2C%20which%20can%20efficiently%20render%20novel%20HDR%20views%0Aand%20reconstruct%20LDR%20images%20with%20a%20user%20input%20exposure%20time.%20Specifically%2C%20we%0Adesign%20a%20Dual%20Dynamic%20Range%20%28DDR%29%20Gaussian%20point%20cloud%20model%20that%20uses%0Aspherical%20harmonics%20to%20fit%20HDR%20color%20and%20employs%20an%20MLP-based%20tone-mapper%20to%0Arender%20LDR%20color.%20The%20HDR%20and%20LDR%20colors%20are%20then%20fed%20into%20two%20Parallel%0ADifferentiable%20Rasterization%20%28PDR%29%20processes%20to%20reconstruct%20HDR%20and%20LDR%20views.%0ATo%20establish%20the%20data%20foundation%20for%20the%20research%20of%203D%20Gaussian%0Asplatting-based%20methods%20in%20HDR%20NVS%2C%20we%20recalibrate%20the%20camera%20parameters%20and%0Acompute%20the%20initial%20positions%20for%20Gaussian%20point%20clouds.%20Experiments%0Ademonstrate%20that%20our%20HDR-GS%20surpasses%20the%20state-of-the-art%20NeRF-based%20method%20by%0A3.84%20and%201.91%20dB%20on%20LDR%20and%20HDR%20NVS%20while%20enjoying%201000x%20inference%20speed%20and%0Aonly%20requiring%206.3%25%20training%20time.%20Code%2C%20models%2C%20and%20recalibrated%20data%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/caiyuanhao1998/HDR-GS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHDR-GS%253A%2520Efficient%2520High%2520Dynamic%2520Range%2520Novel%2520View%2520Synthesis%2520at%25201000x%2520Speed%250A%2520%2520via%2520Gaussian%2520Splatting%26entry.906535625%3DYuanhao%2520Cai%2520and%2520Zihao%2520Xiao%2520and%2520Yixun%2520Liang%2520and%2520Minghan%2520Qin%2520and%2520Yulun%2520Zhang%2520and%2520Xiaokang%2520Yang%2520and%2520Yaoyao%2520Liu%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520High%2520dynamic%2520range%2520%2528HDR%2529%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520aims%2520to%2520create%250Aphotorealistic%2520images%2520from%2520novel%2520viewpoints%2520using%2520HDR%2520imaging%2520techniques.%2520The%250Arendered%2520HDR%2520images%2520capture%2520a%2520wider%2520range%2520of%2520brightness%2520levels%2520containing%2520more%250Adetails%2520of%2520the%2520scene%2520than%2520normal%2520low%2520dynamic%2520range%2520%2528LDR%2529%2520images.%2520Existing%2520HDR%250ANVS%2520methods%2520are%2520mainly%2520based%2520on%2520NeRF.%2520They%2520suffer%2520from%2520long%2520training%2520time%2520and%250Aslow%2520inference%2520speed.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520High%2520Dynamic%250ARange%2520Gaussian%2520Splatting%2520%2528HDR-GS%2529%252C%2520which%2520can%2520efficiently%2520render%2520novel%2520HDR%2520views%250Aand%2520reconstruct%2520LDR%2520images%2520with%2520a%2520user%2520input%2520exposure%2520time.%2520Specifically%252C%2520we%250Adesign%2520a%2520Dual%2520Dynamic%2520Range%2520%2528DDR%2529%2520Gaussian%2520point%2520cloud%2520model%2520that%2520uses%250Aspherical%2520harmonics%2520to%2520fit%2520HDR%2520color%2520and%2520employs%2520an%2520MLP-based%2520tone-mapper%2520to%250Arender%2520LDR%2520color.%2520The%2520HDR%2520and%2520LDR%2520colors%2520are%2520then%2520fed%2520into%2520two%2520Parallel%250ADifferentiable%2520Rasterization%2520%2528PDR%2529%2520processes%2520to%2520reconstruct%2520HDR%2520and%2520LDR%2520views.%250ATo%2520establish%2520the%2520data%2520foundation%2520for%2520the%2520research%2520of%25203D%2520Gaussian%250Asplatting-based%2520methods%2520in%2520HDR%2520NVS%252C%2520we%2520recalibrate%2520the%2520camera%2520parameters%2520and%250Acompute%2520the%2520initial%2520positions%2520for%2520Gaussian%2520point%2520clouds.%2520Experiments%250Ademonstrate%2520that%2520our%2520HDR-GS%2520surpasses%2520the%2520state-of-the-art%2520NeRF-based%2520method%2520by%250A3.84%2520and%25201.91%2520dB%2520on%2520LDR%2520and%2520HDR%2520NVS%2520while%2520enjoying%25201000x%2520inference%2520speed%2520and%250Aonly%2520requiring%25206.3%2525%2520training%2520time.%2520Code%252C%2520models%252C%2520and%2520recalibrated%2520data%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/caiyuanhao1998/HDR-GS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDR-GS%3A%20Efficient%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20at%201000x%20Speed%0A%20%20via%20Gaussian%20Splatting&entry.906535625=Yuanhao%20Cai%20and%20Zihao%20Xiao%20and%20Yixun%20Liang%20and%20Minghan%20Qin%20and%20Yulun%20Zhang%20and%20Xiaokang%20Yang%20and%20Yaoyao%20Liu%20and%20Alan%20Yuille&entry.1292438233=%20%20High%20dynamic%20range%20%28HDR%29%20novel%20view%20synthesis%20%28NVS%29%20aims%20to%20create%0Aphotorealistic%20images%20from%20novel%20viewpoints%20using%20HDR%20imaging%20techniques.%20The%0Arendered%20HDR%20images%20capture%20a%20wider%20range%20of%20brightness%20levels%20containing%20more%0Adetails%20of%20the%20scene%20than%20normal%20low%20dynamic%20range%20%28LDR%29%20images.%20Existing%20HDR%0ANVS%20methods%20are%20mainly%20based%20on%20NeRF.%20They%20suffer%20from%20long%20training%20time%20and%0Aslow%20inference%20speed.%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%2C%20High%20Dynamic%0ARange%20Gaussian%20Splatting%20%28HDR-GS%29%2C%20which%20can%20efficiently%20render%20novel%20HDR%20views%0Aand%20reconstruct%20LDR%20images%20with%20a%20user%20input%20exposure%20time.%20Specifically%2C%20we%0Adesign%20a%20Dual%20Dynamic%20Range%20%28DDR%29%20Gaussian%20point%20cloud%20model%20that%20uses%0Aspherical%20harmonics%20to%20fit%20HDR%20color%20and%20employs%20an%20MLP-based%20tone-mapper%20to%0Arender%20LDR%20color.%20The%20HDR%20and%20LDR%20colors%20are%20then%20fed%20into%20two%20Parallel%0ADifferentiable%20Rasterization%20%28PDR%29%20processes%20to%20reconstruct%20HDR%20and%20LDR%20views.%0ATo%20establish%20the%20data%20foundation%20for%20the%20research%20of%203D%20Gaussian%0Asplatting-based%20methods%20in%20HDR%20NVS%2C%20we%20recalibrate%20the%20camera%20parameters%20and%0Acompute%20the%20initial%20positions%20for%20Gaussian%20point%20clouds.%20Experiments%0Ademonstrate%20that%20our%20HDR-GS%20surpasses%20the%20state-of-the-art%20NeRF-based%20method%20by%0A3.84%20and%201.91%20dB%20on%20LDR%20and%20HDR%20NVS%20while%20enjoying%201000x%20inference%20speed%20and%0Aonly%20requiring%206.3%25%20training%20time.%20Code%2C%20models%2C%20and%20recalibrated%20data%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/caiyuanhao1998/HDR-GS%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15125v2&entry.124074799=Read"},
{"title": "Privacy-Aware Visual Language Models", "author": "Laurens Samson and Nimrod Barazani and Sennay Ghebreab and Yuki M. Asano", "abstract": "  This paper aims to advance our understanding of how Visual Language Models\n(VLMs) handle privacy-sensitive information, a crucial concern as these\ntechnologies become integral to everyday life. To this end, we introduce a new\nbenchmark PrivBench, which contains images from 8 sensitive categories such as\npassports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this\nbenchmark and observe a generally limited understanding of privacy,\nhighlighting a significant area for model improvement. Based on this we\nintroduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs\nwith knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa\nand MiniGPT-v2, on this small dataset, we achieve strong gains in their ability\nto recognize sensitive content, outperforming even GPT4-V. At the same time, we\nshow that privacy-tuning only minimally affects the VLMs performance on\nstandard benchmarks such as VQA. Overall, this paper lays out a crucial\nchallenge for making VLMs effective in handling real-world data safely and\nprovides a simple recipe that takes the first step towards building\nprivacy-aware VLMs.\n", "link": "http://arxiv.org/abs/2405.17423v1", "date": "2024-05-27", "relevancy": 2.5416, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5193}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-Aware%20Visual%20Language%20Models&body=Title%3A%20Privacy-Aware%20Visual%20Language%20Models%0AAuthor%3A%20Laurens%20Samson%20and%20Nimrod%20Barazani%20and%20Sennay%20Ghebreab%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20advance%20our%20understanding%20of%20how%20Visual%20Language%20Models%0A%28VLMs%29%20handle%20privacy-sensitive%20information%2C%20a%20crucial%20concern%20as%20these%0Atechnologies%20become%20integral%20to%20everyday%20life.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Abenchmark%20PrivBench%2C%20which%20contains%20images%20from%208%20sensitive%20categories%20such%20as%0Apassports%2C%20or%20fingerprints.%20We%20evaluate%2010%20state-of-the-art%20VLMs%20on%20this%0Abenchmark%20and%20observe%20a%20generally%20limited%20understanding%20of%20privacy%2C%0Ahighlighting%20a%20significant%20area%20for%20model%20improvement.%20Based%20on%20this%20we%0Aintroduce%20PrivTune%2C%20a%20new%20instruction-tuning%20dataset%20aimed%20at%20equipping%20VLMs%0Awith%20knowledge%20about%20visual%20privacy.%20By%20tuning%20two%20pretrained%20VLMs%2C%20TinyLLaVa%0Aand%20MiniGPT-v2%2C%20on%20this%20small%20dataset%2C%20we%20achieve%20strong%20gains%20in%20their%20ability%0Ato%20recognize%20sensitive%20content%2C%20outperforming%20even%20GPT4-V.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20privacy-tuning%20only%20minimally%20affects%20the%20VLMs%20performance%20on%0Astandard%20benchmarks%20such%20as%20VQA.%20Overall%2C%20this%20paper%20lays%20out%20a%20crucial%0Achallenge%20for%20making%20VLMs%20effective%20in%20handling%20real-world%20data%20safely%20and%0Aprovides%20a%20simple%20recipe%20that%20takes%20the%20first%20step%20towards%20building%0Aprivacy-aware%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-Aware%2520Visual%2520Language%2520Models%26entry.906535625%3DLaurens%2520Samson%2520and%2520Nimrod%2520Barazani%2520and%2520Sennay%2520Ghebreab%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520advance%2520our%2520understanding%2520of%2520how%2520Visual%2520Language%2520Models%250A%2528VLMs%2529%2520handle%2520privacy-sensitive%2520information%252C%2520a%2520crucial%2520concern%2520as%2520these%250Atechnologies%2520become%2520integral%2520to%2520everyday%2520life.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520new%250Abenchmark%2520PrivBench%252C%2520which%2520contains%2520images%2520from%25208%2520sensitive%2520categories%2520such%2520as%250Apassports%252C%2520or%2520fingerprints.%2520We%2520evaluate%252010%2520state-of-the-art%2520VLMs%2520on%2520this%250Abenchmark%2520and%2520observe%2520a%2520generally%2520limited%2520understanding%2520of%2520privacy%252C%250Ahighlighting%2520a%2520significant%2520area%2520for%2520model%2520improvement.%2520Based%2520on%2520this%2520we%250Aintroduce%2520PrivTune%252C%2520a%2520new%2520instruction-tuning%2520dataset%2520aimed%2520at%2520equipping%2520VLMs%250Awith%2520knowledge%2520about%2520visual%2520privacy.%2520By%2520tuning%2520two%2520pretrained%2520VLMs%252C%2520TinyLLaVa%250Aand%2520MiniGPT-v2%252C%2520on%2520this%2520small%2520dataset%252C%2520we%2520achieve%2520strong%2520gains%2520in%2520their%2520ability%250Ato%2520recognize%2520sensitive%2520content%252C%2520outperforming%2520even%2520GPT4-V.%2520At%2520the%2520same%2520time%252C%2520we%250Ashow%2520that%2520privacy-tuning%2520only%2520minimally%2520affects%2520the%2520VLMs%2520performance%2520on%250Astandard%2520benchmarks%2520such%2520as%2520VQA.%2520Overall%252C%2520this%2520paper%2520lays%2520out%2520a%2520crucial%250Achallenge%2520for%2520making%2520VLMs%2520effective%2520in%2520handling%2520real-world%2520data%2520safely%2520and%250Aprovides%2520a%2520simple%2520recipe%2520that%2520takes%2520the%2520first%2520step%2520towards%2520building%250Aprivacy-aware%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-Aware%20Visual%20Language%20Models&entry.906535625=Laurens%20Samson%20and%20Nimrod%20Barazani%20and%20Sennay%20Ghebreab%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20This%20paper%20aims%20to%20advance%20our%20understanding%20of%20how%20Visual%20Language%20Models%0A%28VLMs%29%20handle%20privacy-sensitive%20information%2C%20a%20crucial%20concern%20as%20these%0Atechnologies%20become%20integral%20to%20everyday%20life.%20To%20this%20end%2C%20we%20introduce%20a%20new%0Abenchmark%20PrivBench%2C%20which%20contains%20images%20from%208%20sensitive%20categories%20such%20as%0Apassports%2C%20or%20fingerprints.%20We%20evaluate%2010%20state-of-the-art%20VLMs%20on%20this%0Abenchmark%20and%20observe%20a%20generally%20limited%20understanding%20of%20privacy%2C%0Ahighlighting%20a%20significant%20area%20for%20model%20improvement.%20Based%20on%20this%20we%0Aintroduce%20PrivTune%2C%20a%20new%20instruction-tuning%20dataset%20aimed%20at%20equipping%20VLMs%0Awith%20knowledge%20about%20visual%20privacy.%20By%20tuning%20two%20pretrained%20VLMs%2C%20TinyLLaVa%0Aand%20MiniGPT-v2%2C%20on%20this%20small%20dataset%2C%20we%20achieve%20strong%20gains%20in%20their%20ability%0Ato%20recognize%20sensitive%20content%2C%20outperforming%20even%20GPT4-V.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20privacy-tuning%20only%20minimally%20affects%20the%20VLMs%20performance%20on%0Astandard%20benchmarks%20such%20as%20VQA.%20Overall%2C%20this%20paper%20lays%20out%20a%20crucial%0Achallenge%20for%20making%20VLMs%20effective%20in%20handling%20real-world%20data%20safely%20and%0Aprovides%20a%20simple%20recipe%20that%20takes%20the%20first%20step%20towards%20building%0Aprivacy-aware%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17423v1&entry.124074799=Read"},
{"title": "Compositional Few-Shot Class-Incremental Learning", "author": "Yixiong Zou and Shanghang Zhang and Haichen Zhou and Yuhua Li and Ruixuan Li", "abstract": "  Few-shot class-incremental learning (FSCIL) is proposed to continually learn\nfrom novel classes with only a few samples after the (pre-)training on base\nclasses with sufficient data. However, this remains a challenge. In contrast,\nhumans can easily recognize novel classes with a few samples. Cognitive science\ndemonstrates that an important component of such human capability is\ncompositional learning. This involves identifying visual primitives from\nlearned knowledge and then composing new concepts using these transferred\nprimitives, making incremental learning both effective and interpretable. To\nimitate human compositional learning, we propose a cognitive-inspired method\nfor the FSCIL task. We define and build a compositional model based on set\nsimilarities, and then equip it with a primitive composition module and a\nprimitive reuse module. In the primitive composition module, we propose to\nutilize the Centered Kernel Alignment (CKA) similarity to approximate the\nsimilarity between primitive sets, allowing the training and evaluation based\non primitive compositions. In the primitive reuse module, we enhance primitive\nreusability by classifying inputs based on primitives replaced with the closest\nprimitives from other classes. Experiments on three datasets validate our\nmethod, showing it outperforms current state-of-the-art methods with improved\ninterpretability. Our code is available at\nhttps://github.com/Zoilsen/Comp-FSCIL.\n", "link": "http://arxiv.org/abs/2405.17022v1", "date": "2024-05-27", "relevancy": 2.5397, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5406}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compositional%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%20Compositional%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Yixiong%20Zou%20and%20Shanghang%20Zhang%20and%20Haichen%20Zhou%20and%20Yuhua%20Li%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20is%20proposed%20to%20continually%20learn%0Afrom%20novel%20classes%20with%20only%20a%20few%20samples%20after%20the%20%28pre-%29training%20on%20base%0Aclasses%20with%20sufficient%20data.%20However%2C%20this%20remains%20a%20challenge.%20In%20contrast%2C%0Ahumans%20can%20easily%20recognize%20novel%20classes%20with%20a%20few%20samples.%20Cognitive%20science%0Ademonstrates%20that%20an%20important%20component%20of%20such%20human%20capability%20is%0Acompositional%20learning.%20This%20involves%20identifying%20visual%20primitives%20from%0Alearned%20knowledge%20and%20then%20composing%20new%20concepts%20using%20these%20transferred%0Aprimitives%2C%20making%20incremental%20learning%20both%20effective%20and%20interpretable.%20To%0Aimitate%20human%20compositional%20learning%2C%20we%20propose%20a%20cognitive-inspired%20method%0Afor%20the%20FSCIL%20task.%20We%20define%20and%20build%20a%20compositional%20model%20based%20on%20set%0Asimilarities%2C%20and%20then%20equip%20it%20with%20a%20primitive%20composition%20module%20and%20a%0Aprimitive%20reuse%20module.%20In%20the%20primitive%20composition%20module%2C%20we%20propose%20to%0Autilize%20the%20Centered%20Kernel%20Alignment%20%28CKA%29%20similarity%20to%20approximate%20the%0Asimilarity%20between%20primitive%20sets%2C%20allowing%20the%20training%20and%20evaluation%20based%0Aon%20primitive%20compositions.%20In%20the%20primitive%20reuse%20module%2C%20we%20enhance%20primitive%0Areusability%20by%20classifying%20inputs%20based%20on%20primitives%20replaced%20with%20the%20closest%0Aprimitives%20from%20other%20classes.%20Experiments%20on%20three%20datasets%20validate%20our%0Amethod%2C%20showing%20it%20outperforms%20current%20state-of-the-art%20methods%20with%20improved%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Zoilsen/Comp-FSCIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompositional%2520Few-Shot%2520Class-Incremental%2520Learning%26entry.906535625%3DYixiong%2520Zou%2520and%2520Shanghang%2520Zhang%2520and%2520Haichen%2520Zhou%2520and%2520Yuhua%2520Li%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Few-shot%2520class-incremental%2520learning%2520%2528FSCIL%2529%2520is%2520proposed%2520to%2520continually%2520learn%250Afrom%2520novel%2520classes%2520with%2520only%2520a%2520few%2520samples%2520after%2520the%2520%2528pre-%2529training%2520on%2520base%250Aclasses%2520with%2520sufficient%2520data.%2520However%252C%2520this%2520remains%2520a%2520challenge.%2520In%2520contrast%252C%250Ahumans%2520can%2520easily%2520recognize%2520novel%2520classes%2520with%2520a%2520few%2520samples.%2520Cognitive%2520science%250Ademonstrates%2520that%2520an%2520important%2520component%2520of%2520such%2520human%2520capability%2520is%250Acompositional%2520learning.%2520This%2520involves%2520identifying%2520visual%2520primitives%2520from%250Alearned%2520knowledge%2520and%2520then%2520composing%2520new%2520concepts%2520using%2520these%2520transferred%250Aprimitives%252C%2520making%2520incremental%2520learning%2520both%2520effective%2520and%2520interpretable.%2520To%250Aimitate%2520human%2520compositional%2520learning%252C%2520we%2520propose%2520a%2520cognitive-inspired%2520method%250Afor%2520the%2520FSCIL%2520task.%2520We%2520define%2520and%2520build%2520a%2520compositional%2520model%2520based%2520on%2520set%250Asimilarities%252C%2520and%2520then%2520equip%2520it%2520with%2520a%2520primitive%2520composition%2520module%2520and%2520a%250Aprimitive%2520reuse%2520module.%2520In%2520the%2520primitive%2520composition%2520module%252C%2520we%2520propose%2520to%250Autilize%2520the%2520Centered%2520Kernel%2520Alignment%2520%2528CKA%2529%2520similarity%2520to%2520approximate%2520the%250Asimilarity%2520between%2520primitive%2520sets%252C%2520allowing%2520the%2520training%2520and%2520evaluation%2520based%250Aon%2520primitive%2520compositions.%2520In%2520the%2520primitive%2520reuse%2520module%252C%2520we%2520enhance%2520primitive%250Areusability%2520by%2520classifying%2520inputs%2520based%2520on%2520primitives%2520replaced%2520with%2520the%2520closest%250Aprimitives%2520from%2520other%2520classes.%2520Experiments%2520on%2520three%2520datasets%2520validate%2520our%250Amethod%252C%2520showing%2520it%2520outperforms%2520current%2520state-of-the-art%2520methods%2520with%2520improved%250Ainterpretability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Zoilsen/Comp-FSCIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Yixiong%20Zou%20and%20Shanghang%20Zhang%20and%20Haichen%20Zhou%20and%20Yuhua%20Li%20and%20Ruixuan%20Li&entry.1292438233=%20%20Few-shot%20class-incremental%20learning%20%28FSCIL%29%20is%20proposed%20to%20continually%20learn%0Afrom%20novel%20classes%20with%20only%20a%20few%20samples%20after%20the%20%28pre-%29training%20on%20base%0Aclasses%20with%20sufficient%20data.%20However%2C%20this%20remains%20a%20challenge.%20In%20contrast%2C%0Ahumans%20can%20easily%20recognize%20novel%20classes%20with%20a%20few%20samples.%20Cognitive%20science%0Ademonstrates%20that%20an%20important%20component%20of%20such%20human%20capability%20is%0Acompositional%20learning.%20This%20involves%20identifying%20visual%20primitives%20from%0Alearned%20knowledge%20and%20then%20composing%20new%20concepts%20using%20these%20transferred%0Aprimitives%2C%20making%20incremental%20learning%20both%20effective%20and%20interpretable.%20To%0Aimitate%20human%20compositional%20learning%2C%20we%20propose%20a%20cognitive-inspired%20method%0Afor%20the%20FSCIL%20task.%20We%20define%20and%20build%20a%20compositional%20model%20based%20on%20set%0Asimilarities%2C%20and%20then%20equip%20it%20with%20a%20primitive%20composition%20module%20and%20a%0Aprimitive%20reuse%20module.%20In%20the%20primitive%20composition%20module%2C%20we%20propose%20to%0Autilize%20the%20Centered%20Kernel%20Alignment%20%28CKA%29%20similarity%20to%20approximate%20the%0Asimilarity%20between%20primitive%20sets%2C%20allowing%20the%20training%20and%20evaluation%20based%0Aon%20primitive%20compositions.%20In%20the%20primitive%20reuse%20module%2C%20we%20enhance%20primitive%0Areusability%20by%20classifying%20inputs%20based%20on%20primitives%20replaced%20with%20the%20closest%0Aprimitives%20from%20other%20classes.%20Experiments%20on%20three%20datasets%20validate%20our%0Amethod%2C%20showing%20it%20outperforms%20current%20state-of-the-art%20methods%20with%20improved%0Ainterpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Zoilsen/Comp-FSCIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17022v1&entry.124074799=Read"},
{"title": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization", "author": "Dixuan Wang and Yanda Li and Junyuan Jiang and Zepeng Ding and Guochao Jiang and Jiaqing Liang and Deqing Yang", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. To demonstrate this flaw of LLMs, we\nconstruct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset\nfor Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs\nto challenge LLMs' tokenization. ADT consists of two subsets: the manually\nconstructed ADT-Human and the automatically generated ADT-Auto. Our empirical\nresults reveal that our ADT is highly effective on challenging the tokenization\nof leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus\ndegrading these LLMs' capabilities. Moreover, our method of automatic data\ngeneration has been proven efficient and robust, which can be applied to any\nopen-source LLMs. To the best of our knowledge, our study is the first to\ninvestigating LLMs' vulnerability in terms of challenging their token\nsegmentation, which will shed light on the subsequent research of improving\nLLMs' capabilities through optimizing their tokenization process and\nalgorithms.\n", "link": "http://arxiv.org/abs/2405.17067v1", "date": "2024-05-27", "relevancy": 2.5373, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization&body=Title%3A%20Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization%0AAuthor%3A%20Dixuan%20Wang%20and%20Yanda%20Li%20and%20Junyuan%20Jiang%20and%20Zepeng%20Ding%20and%20Guochao%20Jiang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20it%20was%20also%20witnessed%20that%20LLMs%20tend%0Ato%20produce%20inaccurate%20responses%20to%20specific%20queries.%20This%20deficiency%20can%20be%0Atraced%20to%20the%20tokenization%20step%20LLMs%20must%20undergo%2C%20which%20is%20an%20inevitable%0Alimitation%20inherent%20to%20all%20LLMs.%20In%20fact%2C%20incorrect%20tokenization%20is%20the%0Acritical%20point%20that%20hinders%20LLMs%20in%20understanding%20the%20input%20precisely%2C%20thus%0Aleading%20to%20unsatisfactory%20output.%20To%20demonstrate%20this%20flaw%20of%20LLMs%2C%20we%0Aconstruct%20an%20adversarial%20dataset%2C%20named%20as%20%24%5Ctextbf%7BADT%20%28Adversarial%20Dataset%0Afor%20Tokenizer%29%7D%24%2C%20which%20draws%20upon%20the%20vocabularies%20of%20various%20open-source%20LLMs%0Ato%20challenge%20LLMs%27%20tokenization.%20ADT%20consists%20of%20two%20subsets%3A%20the%20manually%0Aconstructed%20ADT-Human%20and%20the%20automatically%20generated%20ADT-Auto.%20Our%20empirical%0Aresults%20reveal%20that%20our%20ADT%20is%20highly%20effective%20on%20challenging%20the%20tokenization%0Aof%20leading%20LLMs%2C%20including%20GPT-4o%2C%20Llama-3%2C%20Qwen2.5-max%20and%20so%20on%2C%20thus%0Adegrading%20these%20LLMs%27%20capabilities.%20Moreover%2C%20our%20method%20of%20automatic%20data%0Ageneration%20has%20been%20proven%20efficient%20and%20robust%2C%20which%20can%20be%20applied%20to%20any%0Aopen-source%20LLMs.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%0Ainvestigating%20LLMs%27%20vulnerability%20in%20terms%20of%20challenging%20their%20token%0Asegmentation%2C%20which%20will%20shed%20light%20on%20the%20subsequent%20research%20of%20improving%0ALLMs%27%20capabilities%20through%20optimizing%20their%20tokenization%20process%20and%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenization%2520Matters%2521%2520Degrading%2520Large%2520Language%2520Models%2520through%250A%2520%2520Challenging%2520Their%2520Tokenization%26entry.906535625%3DDixuan%2520Wang%2520and%2520Yanda%2520Li%2520and%2520Junyuan%2520Jiang%2520and%2520Zepeng%2520Ding%2520and%2520Guochao%2520Jiang%2520and%2520Jiaqing%2520Liang%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520language%250Aunderstanding%2520and%2520generation.%2520Nonetheless%252C%2520it%2520was%2520also%2520witnessed%2520that%2520LLMs%2520tend%250Ato%2520produce%2520inaccurate%2520responses%2520to%2520specific%2520queries.%2520This%2520deficiency%2520can%2520be%250Atraced%2520to%2520the%2520tokenization%2520step%2520LLMs%2520must%2520undergo%252C%2520which%2520is%2520an%2520inevitable%250Alimitation%2520inherent%2520to%2520all%2520LLMs.%2520In%2520fact%252C%2520incorrect%2520tokenization%2520is%2520the%250Acritical%2520point%2520that%2520hinders%2520LLMs%2520in%2520understanding%2520the%2520input%2520precisely%252C%2520thus%250Aleading%2520to%2520unsatisfactory%2520output.%2520To%2520demonstrate%2520this%2520flaw%2520of%2520LLMs%252C%2520we%250Aconstruct%2520an%2520adversarial%2520dataset%252C%2520named%2520as%2520%2524%255Ctextbf%257BADT%2520%2528Adversarial%2520Dataset%250Afor%2520Tokenizer%2529%257D%2524%252C%2520which%2520draws%2520upon%2520the%2520vocabularies%2520of%2520various%2520open-source%2520LLMs%250Ato%2520challenge%2520LLMs%2527%2520tokenization.%2520ADT%2520consists%2520of%2520two%2520subsets%253A%2520the%2520manually%250Aconstructed%2520ADT-Human%2520and%2520the%2520automatically%2520generated%2520ADT-Auto.%2520Our%2520empirical%250Aresults%2520reveal%2520that%2520our%2520ADT%2520is%2520highly%2520effective%2520on%2520challenging%2520the%2520tokenization%250Aof%2520leading%2520LLMs%252C%2520including%2520GPT-4o%252C%2520Llama-3%252C%2520Qwen2.5-max%2520and%2520so%2520on%252C%2520thus%250Adegrading%2520these%2520LLMs%2527%2520capabilities.%2520Moreover%252C%2520our%2520method%2520of%2520automatic%2520data%250Ageneration%2520has%2520been%2520proven%2520efficient%2520and%2520robust%252C%2520which%2520can%2520be%2520applied%2520to%2520any%250Aopen-source%2520LLMs.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520study%2520is%2520the%2520first%2520to%250Ainvestigating%2520LLMs%2527%2520vulnerability%2520in%2520terms%2520of%2520challenging%2520their%2520token%250Asegmentation%252C%2520which%2520will%2520shed%2520light%2520on%2520the%2520subsequent%2520research%2520of%2520improving%250ALLMs%2527%2520capabilities%2520through%2520optimizing%2520their%2520tokenization%2520process%2520and%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenization%20Matters%21%20Degrading%20Large%20Language%20Models%20through%0A%20%20Challenging%20Their%20Tokenization&entry.906535625=Dixuan%20Wang%20and%20Yanda%20Li%20and%20Junyuan%20Jiang%20and%20Zepeng%20Ding%20and%20Guochao%20Jiang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20language%0Aunderstanding%20and%20generation.%20Nonetheless%2C%20it%20was%20also%20witnessed%20that%20LLMs%20tend%0Ato%20produce%20inaccurate%20responses%20to%20specific%20queries.%20This%20deficiency%20can%20be%0Atraced%20to%20the%20tokenization%20step%20LLMs%20must%20undergo%2C%20which%20is%20an%20inevitable%0Alimitation%20inherent%20to%20all%20LLMs.%20In%20fact%2C%20incorrect%20tokenization%20is%20the%0Acritical%20point%20that%20hinders%20LLMs%20in%20understanding%20the%20input%20precisely%2C%20thus%0Aleading%20to%20unsatisfactory%20output.%20To%20demonstrate%20this%20flaw%20of%20LLMs%2C%20we%0Aconstruct%20an%20adversarial%20dataset%2C%20named%20as%20%24%5Ctextbf%7BADT%20%28Adversarial%20Dataset%0Afor%20Tokenizer%29%7D%24%2C%20which%20draws%20upon%20the%20vocabularies%20of%20various%20open-source%20LLMs%0Ato%20challenge%20LLMs%27%20tokenization.%20ADT%20consists%20of%20two%20subsets%3A%20the%20manually%0Aconstructed%20ADT-Human%20and%20the%20automatically%20generated%20ADT-Auto.%20Our%20empirical%0Aresults%20reveal%20that%20our%20ADT%20is%20highly%20effective%20on%20challenging%20the%20tokenization%0Aof%20leading%20LLMs%2C%20including%20GPT-4o%2C%20Llama-3%2C%20Qwen2.5-max%20and%20so%20on%2C%20thus%0Adegrading%20these%20LLMs%27%20capabilities.%20Moreover%2C%20our%20method%20of%20automatic%20data%0Ageneration%20has%20been%20proven%20efficient%20and%20robust%2C%20which%20can%20be%20applied%20to%20any%0Aopen-source%20LLMs.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%0Ainvestigating%20LLMs%27%20vulnerability%20in%20terms%20of%20challenging%20their%20token%0Asegmentation%2C%20which%20will%20shed%20light%20on%20the%20subsequent%20research%20of%20improving%0ALLMs%27%20capabilities%20through%20optimizing%20their%20tokenization%20process%20and%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17067v1&entry.124074799=Read"},
{"title": "Polyhedral Complex Derivation from Piecewise Trilinear Networks", "author": "Jin-Hwa Kim", "abstract": "  Recent advancements in visualizing deep neural networks provide insights into\ntheir structures and mesh extraction from Continuous Piecewise Affine (CPWA)\nfunctions. Meanwhile, developments in neural surface representation learning\nincorporate non-linear positional encoding, addressing issues like spectral\nbias; however, this poses challenges in applying mesh extraction techniques\nbased on CPWA functions. Focusing on trilinear interpolating methods as\npositional encoding, we present theoretical insights and an analytical mesh\nextraction, showing the transformation of hypersurfaces to flat planes within\nthe trilinear region under the eikonal constraint. Moreover, we introduce a\nmethod for approximating intersecting points among three hypersurfaces\ncontributing to broader applications. We empirically validate correctness and\nparsimony through chamfer distance and efficiency, and angular distance, while\nexamining the correlation between the eikonal loss and the planarity of the\nhypersurfaces.\n", "link": "http://arxiv.org/abs/2402.10403v2", "date": "2024-05-27", "relevancy": 2.5332, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5278}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4987}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polyhedral%20Complex%20Derivation%20from%20Piecewise%20Trilinear%20Networks&body=Title%3A%20Polyhedral%20Complex%20Derivation%20from%20Piecewise%20Trilinear%20Networks%0AAuthor%3A%20Jin-Hwa%20Kim%0AAbstract%3A%20%20%20Recent%20advancements%20in%20visualizing%20deep%20neural%20networks%20provide%20insights%20into%0Atheir%20structures%20and%20mesh%20extraction%20from%20Continuous%20Piecewise%20Affine%20%28CPWA%29%0Afunctions.%20Meanwhile%2C%20developments%20in%20neural%20surface%20representation%20learning%0Aincorporate%20non-linear%20positional%20encoding%2C%20addressing%20issues%20like%20spectral%0Abias%3B%20however%2C%20this%20poses%20challenges%20in%20applying%20mesh%20extraction%20techniques%0Abased%20on%20CPWA%20functions.%20Focusing%20on%20trilinear%20interpolating%20methods%20as%0Apositional%20encoding%2C%20we%20present%20theoretical%20insights%20and%20an%20analytical%20mesh%0Aextraction%2C%20showing%20the%20transformation%20of%20hypersurfaces%20to%20flat%20planes%20within%0Athe%20trilinear%20region%20under%20the%20eikonal%20constraint.%20Moreover%2C%20we%20introduce%20a%0Amethod%20for%20approximating%20intersecting%20points%20among%20three%20hypersurfaces%0Acontributing%20to%20broader%20applications.%20We%20empirically%20validate%20correctness%20and%0Aparsimony%20through%20chamfer%20distance%20and%20efficiency%2C%20and%20angular%20distance%2C%20while%0Aexamining%20the%20correlation%20between%20the%20eikonal%20loss%20and%20the%20planarity%20of%20the%0Ahypersurfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyhedral%2520Complex%2520Derivation%2520from%2520Piecewise%2520Trilinear%2520Networks%26entry.906535625%3DJin-Hwa%2520Kim%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520visualizing%2520deep%2520neural%2520networks%2520provide%2520insights%2520into%250Atheir%2520structures%2520and%2520mesh%2520extraction%2520from%2520Continuous%2520Piecewise%2520Affine%2520%2528CPWA%2529%250Afunctions.%2520Meanwhile%252C%2520developments%2520in%2520neural%2520surface%2520representation%2520learning%250Aincorporate%2520non-linear%2520positional%2520encoding%252C%2520addressing%2520issues%2520like%2520spectral%250Abias%253B%2520however%252C%2520this%2520poses%2520challenges%2520in%2520applying%2520mesh%2520extraction%2520techniques%250Abased%2520on%2520CPWA%2520functions.%2520Focusing%2520on%2520trilinear%2520interpolating%2520methods%2520as%250Apositional%2520encoding%252C%2520we%2520present%2520theoretical%2520insights%2520and%2520an%2520analytical%2520mesh%250Aextraction%252C%2520showing%2520the%2520transformation%2520of%2520hypersurfaces%2520to%2520flat%2520planes%2520within%250Athe%2520trilinear%2520region%2520under%2520the%2520eikonal%2520constraint.%2520Moreover%252C%2520we%2520introduce%2520a%250Amethod%2520for%2520approximating%2520intersecting%2520points%2520among%2520three%2520hypersurfaces%250Acontributing%2520to%2520broader%2520applications.%2520We%2520empirically%2520validate%2520correctness%2520and%250Aparsimony%2520through%2520chamfer%2520distance%2520and%2520efficiency%252C%2520and%2520angular%2520distance%252C%2520while%250Aexamining%2520the%2520correlation%2520between%2520the%2520eikonal%2520loss%2520and%2520the%2520planarity%2520of%2520the%250Ahypersurfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polyhedral%20Complex%20Derivation%20from%20Piecewise%20Trilinear%20Networks&entry.906535625=Jin-Hwa%20Kim&entry.1292438233=%20%20Recent%20advancements%20in%20visualizing%20deep%20neural%20networks%20provide%20insights%20into%0Atheir%20structures%20and%20mesh%20extraction%20from%20Continuous%20Piecewise%20Affine%20%28CPWA%29%0Afunctions.%20Meanwhile%2C%20developments%20in%20neural%20surface%20representation%20learning%0Aincorporate%20non-linear%20positional%20encoding%2C%20addressing%20issues%20like%20spectral%0Abias%3B%20however%2C%20this%20poses%20challenges%20in%20applying%20mesh%20extraction%20techniques%0Abased%20on%20CPWA%20functions.%20Focusing%20on%20trilinear%20interpolating%20methods%20as%0Apositional%20encoding%2C%20we%20present%20theoretical%20insights%20and%20an%20analytical%20mesh%0Aextraction%2C%20showing%20the%20transformation%20of%20hypersurfaces%20to%20flat%20planes%20within%0Athe%20trilinear%20region%20under%20the%20eikonal%20constraint.%20Moreover%2C%20we%20introduce%20a%0Amethod%20for%20approximating%20intersecting%20points%20among%20three%20hypersurfaces%0Acontributing%20to%20broader%20applications.%20We%20empirically%20validate%20correctness%20and%0Aparsimony%20through%20chamfer%20distance%20and%20efficiency%2C%20and%20angular%20distance%2C%20while%0Aexamining%20the%20correlation%20between%20the%20eikonal%20loss%20and%20the%20planarity%20of%20the%0Ahypersurfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10403v2&entry.124074799=Read"},
{"title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic\n  Occupancy Prediction", "author": "Yuanhui Huang and Wenzhao Zheng and Yunpeng Zhang and Jie Zhou and Jiwen Lu", "abstract": "  3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and\nsemantics of the surrounding scene and is an important task for the robustness\nof vision-centric autonomous driving. Most existing methods employ dense grids\nsuch as voxels as scene representations, which ignore the sparsity of occupancy\nand the diversity of object scales and thus lead to unbalanced allocation of\nresources. To address this, we propose an object-centric representation to\ndescribe 3D scenes with sparse 3D semantic Gaussians where each Gaussian\nrepresents a flexible region of interest and its semantic features. We\naggregate information from images through the attention mechanism and\niteratively refine the properties of 3D Gaussians including position,\ncovariance, and semantics. We then propose an efficient Gaussian-to-voxel\nsplatting method to generate 3D occupancy predictions, which only aggregates\nthe neighboring Gaussians for a certain position. We conduct extensive\nexperiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental\nresults demonstrate that GaussianFormer achieves comparable performance with\nstate-of-the-art methods with only 17.8% - 24.8% of their memory consumption.\nCode is available at: https://github.com/huang-yh/GaussianFormer.\n", "link": "http://arxiv.org/abs/2405.17429v1", "date": "2024-05-27", "relevancy": 2.532, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6612}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianFormer%3A%20Scene%20as%20Gaussians%20for%20Vision-Based%203D%20Semantic%0A%20%20Occupancy%20Prediction&body=Title%3A%20GaussianFormer%3A%20Scene%20as%20Gaussians%20for%20Vision-Based%203D%20Semantic%0A%20%20Occupancy%20Prediction%0AAuthor%3A%20Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yunpeng%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%203D%20semantic%20occupancy%20prediction%20aims%20to%20obtain%203D%20fine-grained%20geometry%20and%0Asemantics%20of%20the%20surrounding%20scene%20and%20is%20an%20important%20task%20for%20the%20robustness%0Aof%20vision-centric%20autonomous%20driving.%20Most%20existing%20methods%20employ%20dense%20grids%0Asuch%20as%20voxels%20as%20scene%20representations%2C%20which%20ignore%20the%20sparsity%20of%20occupancy%0Aand%20the%20diversity%20of%20object%20scales%20and%20thus%20lead%20to%20unbalanced%20allocation%20of%0Aresources.%20To%20address%20this%2C%20we%20propose%20an%20object-centric%20representation%20to%0Adescribe%203D%20scenes%20with%20sparse%203D%20semantic%20Gaussians%20where%20each%20Gaussian%0Arepresents%20a%20flexible%20region%20of%20interest%20and%20its%20semantic%20features.%20We%0Aaggregate%20information%20from%20images%20through%20the%20attention%20mechanism%20and%0Aiteratively%20refine%20the%20properties%20of%203D%20Gaussians%20including%20position%2C%0Acovariance%2C%20and%20semantics.%20We%20then%20propose%20an%20efficient%20Gaussian-to-voxel%0Asplatting%20method%20to%20generate%203D%20occupancy%20predictions%2C%20which%20only%20aggregates%0Athe%20neighboring%20Gaussians%20for%20a%20certain%20position.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20widely%20adopted%20nuScenes%20and%20KITTI-360%20datasets.%20Experimental%0Aresults%20demonstrate%20that%20GaussianFormer%20achieves%20comparable%20performance%20with%0Astate-of-the-art%20methods%20with%20only%2017.8%25%20-%2024.8%25%20of%20their%20memory%20consumption.%0ACode%20is%20available%20at%3A%20https%3A//github.com/huang-yh/GaussianFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianFormer%253A%2520Scene%2520as%2520Gaussians%2520for%2520Vision-Based%25203D%2520Semantic%250A%2520%2520Occupancy%2520Prediction%26entry.906535625%3DYuanhui%2520Huang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yunpeng%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%25203D%2520semantic%2520occupancy%2520prediction%2520aims%2520to%2520obtain%25203D%2520fine-grained%2520geometry%2520and%250Asemantics%2520of%2520the%2520surrounding%2520scene%2520and%2520is%2520an%2520important%2520task%2520for%2520the%2520robustness%250Aof%2520vision-centric%2520autonomous%2520driving.%2520Most%2520existing%2520methods%2520employ%2520dense%2520grids%250Asuch%2520as%2520voxels%2520as%2520scene%2520representations%252C%2520which%2520ignore%2520the%2520sparsity%2520of%2520occupancy%250Aand%2520the%2520diversity%2520of%2520object%2520scales%2520and%2520thus%2520lead%2520to%2520unbalanced%2520allocation%2520of%250Aresources.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520object-centric%2520representation%2520to%250Adescribe%25203D%2520scenes%2520with%2520sparse%25203D%2520semantic%2520Gaussians%2520where%2520each%2520Gaussian%250Arepresents%2520a%2520flexible%2520region%2520of%2520interest%2520and%2520its%2520semantic%2520features.%2520We%250Aaggregate%2520information%2520from%2520images%2520through%2520the%2520attention%2520mechanism%2520and%250Aiteratively%2520refine%2520the%2520properties%2520of%25203D%2520Gaussians%2520including%2520position%252C%250Acovariance%252C%2520and%2520semantics.%2520We%2520then%2520propose%2520an%2520efficient%2520Gaussian-to-voxel%250Asplatting%2520method%2520to%2520generate%25203D%2520occupancy%2520predictions%252C%2520which%2520only%2520aggregates%250Athe%2520neighboring%2520Gaussians%2520for%2520a%2520certain%2520position.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520the%2520widely%2520adopted%2520nuScenes%2520and%2520KITTI-360%2520datasets.%2520Experimental%250Aresults%2520demonstrate%2520that%2520GaussianFormer%2520achieves%2520comparable%2520performance%2520with%250Astate-of-the-art%2520methods%2520with%2520only%252017.8%2525%2520-%252024.8%2525%2520of%2520their%2520memory%2520consumption.%250ACode%2520is%2520available%2520at%253A%2520https%253A//github.com/huang-yh/GaussianFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianFormer%3A%20Scene%20as%20Gaussians%20for%20Vision-Based%203D%20Semantic%0A%20%20Occupancy%20Prediction&entry.906535625=Yuanhui%20Huang%20and%20Wenzhao%20Zheng%20and%20Yunpeng%20Zhang%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%203D%20semantic%20occupancy%20prediction%20aims%20to%20obtain%203D%20fine-grained%20geometry%20and%0Asemantics%20of%20the%20surrounding%20scene%20and%20is%20an%20important%20task%20for%20the%20robustness%0Aof%20vision-centric%20autonomous%20driving.%20Most%20existing%20methods%20employ%20dense%20grids%0Asuch%20as%20voxels%20as%20scene%20representations%2C%20which%20ignore%20the%20sparsity%20of%20occupancy%0Aand%20the%20diversity%20of%20object%20scales%20and%20thus%20lead%20to%20unbalanced%20allocation%20of%0Aresources.%20To%20address%20this%2C%20we%20propose%20an%20object-centric%20representation%20to%0Adescribe%203D%20scenes%20with%20sparse%203D%20semantic%20Gaussians%20where%20each%20Gaussian%0Arepresents%20a%20flexible%20region%20of%20interest%20and%20its%20semantic%20features.%20We%0Aaggregate%20information%20from%20images%20through%20the%20attention%20mechanism%20and%0Aiteratively%20refine%20the%20properties%20of%203D%20Gaussians%20including%20position%2C%0Acovariance%2C%20and%20semantics.%20We%20then%20propose%20an%20efficient%20Gaussian-to-voxel%0Asplatting%20method%20to%20generate%203D%20occupancy%20predictions%2C%20which%20only%20aggregates%0Athe%20neighboring%20Gaussians%20for%20a%20certain%20position.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20widely%20adopted%20nuScenes%20and%20KITTI-360%20datasets.%20Experimental%0Aresults%20demonstrate%20that%20GaussianFormer%20achieves%20comparable%20performance%20with%0Astate-of-the-art%20methods%20with%20only%2017.8%25%20-%2024.8%25%20of%20their%20memory%20consumption.%0ACode%20is%20available%20at%3A%20https%3A//github.com/huang-yh/GaussianFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17429v1&entry.124074799=Read"},
{"title": "Socially-Aware Shared Control Navigation for Assistive Mobile Robots in\n  the Built Environment", "author": "Yifan Xu and Qianwei Wang and Vineet Kamat and Carol Menassa", "abstract": "  As the number of Persons with Disabilities (PWD), particularly those with one\nor more physical impairments, increases, there is an increasing demand for\nassistive robotic technologies that can support independent mobility in the\nbuilt environment and reduce the burden on caregivers. Current assistive\nmobility platforms (e.g., robotic wheelchairs) often fail to incorporate user\npreferences and control, leading to reduced trust and efficiency. Existing\nshared control algorithms do not allow the incorporation of the user control\npreferences inside the navigation framework or the path planning algorithm. In\naddition, existing dynamic local planner algorithms for robotic wheelchairs do\nnot take into account the social spaces of people, potentially leading such\nplatforms to infringe upon these areas and cause discomfort. To address these\nconcerns, this work introduces a novel socially-aware shared autonomy-based\nnavigation system for assistive mobile robotic platforms.\n  Our navigation framework comprises a Global Planner and a Local Planner. To\nimplement the Global Planner, the proposed approach introduces a novel User\nPreference Field (UPF) theory within its global planning framework, explicitly\nacknowledging user preferences to adeptly navigate away from congested areas.\nFor the Local Planner, we propose a Socially-aware Shared Control-based Model\nPredictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to\nadjust movements in real-time, integrating user preferences for safer, more\nautonomous navigation. Evaluation results show that our Global Planner aligns\nclosely with user preferences compared to baselines, and our Local Planner\ndemonstrates enhanced safety and efficiency in dynamic and static scenarios.\nThis integrated approach fosters trust and autonomy, crucial for the acceptance\nof assistive mobility technologies in the built environment.\n", "link": "http://arxiv.org/abs/2405.17279v1", "date": "2024-05-27", "relevancy": 2.5224, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.7055}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Socially-Aware%20Shared%20Control%20Navigation%20for%20Assistive%20Mobile%20Robots%20in%0A%20%20the%20Built%20Environment&body=Title%3A%20Socially-Aware%20Shared%20Control%20Navigation%20for%20Assistive%20Mobile%20Robots%20in%0A%20%20the%20Built%20Environment%0AAuthor%3A%20Yifan%20Xu%20and%20Qianwei%20Wang%20and%20Vineet%20Kamat%20and%20Carol%20Menassa%0AAbstract%3A%20%20%20As%20the%20number%20of%20Persons%20with%20Disabilities%20%28PWD%29%2C%20particularly%20those%20with%20one%0Aor%20more%20physical%20impairments%2C%20increases%2C%20there%20is%20an%20increasing%20demand%20for%0Aassistive%20robotic%20technologies%20that%20can%20support%20independent%20mobility%20in%20the%0Abuilt%20environment%20and%20reduce%20the%20burden%20on%20caregivers.%20Current%20assistive%0Amobility%20platforms%20%28e.g.%2C%20robotic%20wheelchairs%29%20often%20fail%20to%20incorporate%20user%0Apreferences%20and%20control%2C%20leading%20to%20reduced%20trust%20and%20efficiency.%20Existing%0Ashared%20control%20algorithms%20do%20not%20allow%20the%20incorporation%20of%20the%20user%20control%0Apreferences%20inside%20the%20navigation%20framework%20or%20the%20path%20planning%20algorithm.%20In%0Aaddition%2C%20existing%20dynamic%20local%20planner%20algorithms%20for%20robotic%20wheelchairs%20do%0Anot%20take%20into%20account%20the%20social%20spaces%20of%20people%2C%20potentially%20leading%20such%0Aplatforms%20to%20infringe%20upon%20these%20areas%20and%20cause%20discomfort.%20To%20address%20these%0Aconcerns%2C%20this%20work%20introduces%20a%20novel%20socially-aware%20shared%20autonomy-based%0Anavigation%20system%20for%20assistive%20mobile%20robotic%20platforms.%0A%20%20Our%20navigation%20framework%20comprises%20a%20Global%20Planner%20and%20a%20Local%20Planner.%20To%0Aimplement%20the%20Global%20Planner%2C%20the%20proposed%20approach%20introduces%20a%20novel%20User%0APreference%20Field%20%28UPF%29%20theory%20within%20its%20global%20planning%20framework%2C%20explicitly%0Aacknowledging%20user%20preferences%20to%20adeptly%20navigate%20away%20from%20congested%20areas.%0AFor%20the%20Local%20Planner%2C%20we%20propose%20a%20Socially-aware%20Shared%20Control-based%20Model%0APredictive%20Control%20with%20Dynamic%20Control%20Barrier%20Function%20%28SS-MPC-DCBF%29%20to%0Aadjust%20movements%20in%20real-time%2C%20integrating%20user%20preferences%20for%20safer%2C%20more%0Aautonomous%20navigation.%20Evaluation%20results%20show%20that%20our%20Global%20Planner%20aligns%0Aclosely%20with%20user%20preferences%20compared%20to%20baselines%2C%20and%20our%20Local%20Planner%0Ademonstrates%20enhanced%20safety%20and%20efficiency%20in%20dynamic%20and%20static%20scenarios.%0AThis%20integrated%20approach%20fosters%20trust%20and%20autonomy%2C%20crucial%20for%20the%20acceptance%0Aof%20assistive%20mobility%20technologies%20in%20the%20built%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocially-Aware%2520Shared%2520Control%2520Navigation%2520for%2520Assistive%2520Mobile%2520Robots%2520in%250A%2520%2520the%2520Built%2520Environment%26entry.906535625%3DYifan%2520Xu%2520and%2520Qianwei%2520Wang%2520and%2520Vineet%2520Kamat%2520and%2520Carol%2520Menassa%26entry.1292438233%3D%2520%2520As%2520the%2520number%2520of%2520Persons%2520with%2520Disabilities%2520%2528PWD%2529%252C%2520particularly%2520those%2520with%2520one%250Aor%2520more%2520physical%2520impairments%252C%2520increases%252C%2520there%2520is%2520an%2520increasing%2520demand%2520for%250Aassistive%2520robotic%2520technologies%2520that%2520can%2520support%2520independent%2520mobility%2520in%2520the%250Abuilt%2520environment%2520and%2520reduce%2520the%2520burden%2520on%2520caregivers.%2520Current%2520assistive%250Amobility%2520platforms%2520%2528e.g.%252C%2520robotic%2520wheelchairs%2529%2520often%2520fail%2520to%2520incorporate%2520user%250Apreferences%2520and%2520control%252C%2520leading%2520to%2520reduced%2520trust%2520and%2520efficiency.%2520Existing%250Ashared%2520control%2520algorithms%2520do%2520not%2520allow%2520the%2520incorporation%2520of%2520the%2520user%2520control%250Apreferences%2520inside%2520the%2520navigation%2520framework%2520or%2520the%2520path%2520planning%2520algorithm.%2520In%250Aaddition%252C%2520existing%2520dynamic%2520local%2520planner%2520algorithms%2520for%2520robotic%2520wheelchairs%2520do%250Anot%2520take%2520into%2520account%2520the%2520social%2520spaces%2520of%2520people%252C%2520potentially%2520leading%2520such%250Aplatforms%2520to%2520infringe%2520upon%2520these%2520areas%2520and%2520cause%2520discomfort.%2520To%2520address%2520these%250Aconcerns%252C%2520this%2520work%2520introduces%2520a%2520novel%2520socially-aware%2520shared%2520autonomy-based%250Anavigation%2520system%2520for%2520assistive%2520mobile%2520robotic%2520platforms.%250A%2520%2520Our%2520navigation%2520framework%2520comprises%2520a%2520Global%2520Planner%2520and%2520a%2520Local%2520Planner.%2520To%250Aimplement%2520the%2520Global%2520Planner%252C%2520the%2520proposed%2520approach%2520introduces%2520a%2520novel%2520User%250APreference%2520Field%2520%2528UPF%2529%2520theory%2520within%2520its%2520global%2520planning%2520framework%252C%2520explicitly%250Aacknowledging%2520user%2520preferences%2520to%2520adeptly%2520navigate%2520away%2520from%2520congested%2520areas.%250AFor%2520the%2520Local%2520Planner%252C%2520we%2520propose%2520a%2520Socially-aware%2520Shared%2520Control-based%2520Model%250APredictive%2520Control%2520with%2520Dynamic%2520Control%2520Barrier%2520Function%2520%2528SS-MPC-DCBF%2529%2520to%250Aadjust%2520movements%2520in%2520real-time%252C%2520integrating%2520user%2520preferences%2520for%2520safer%252C%2520more%250Aautonomous%2520navigation.%2520Evaluation%2520results%2520show%2520that%2520our%2520Global%2520Planner%2520aligns%250Aclosely%2520with%2520user%2520preferences%2520compared%2520to%2520baselines%252C%2520and%2520our%2520Local%2520Planner%250Ademonstrates%2520enhanced%2520safety%2520and%2520efficiency%2520in%2520dynamic%2520and%2520static%2520scenarios.%250AThis%2520integrated%2520approach%2520fosters%2520trust%2520and%2520autonomy%252C%2520crucial%2520for%2520the%2520acceptance%250Aof%2520assistive%2520mobility%2520technologies%2520in%2520the%2520built%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Socially-Aware%20Shared%20Control%20Navigation%20for%20Assistive%20Mobile%20Robots%20in%0A%20%20the%20Built%20Environment&entry.906535625=Yifan%20Xu%20and%20Qianwei%20Wang%20and%20Vineet%20Kamat%20and%20Carol%20Menassa&entry.1292438233=%20%20As%20the%20number%20of%20Persons%20with%20Disabilities%20%28PWD%29%2C%20particularly%20those%20with%20one%0Aor%20more%20physical%20impairments%2C%20increases%2C%20there%20is%20an%20increasing%20demand%20for%0Aassistive%20robotic%20technologies%20that%20can%20support%20independent%20mobility%20in%20the%0Abuilt%20environment%20and%20reduce%20the%20burden%20on%20caregivers.%20Current%20assistive%0Amobility%20platforms%20%28e.g.%2C%20robotic%20wheelchairs%29%20often%20fail%20to%20incorporate%20user%0Apreferences%20and%20control%2C%20leading%20to%20reduced%20trust%20and%20efficiency.%20Existing%0Ashared%20control%20algorithms%20do%20not%20allow%20the%20incorporation%20of%20the%20user%20control%0Apreferences%20inside%20the%20navigation%20framework%20or%20the%20path%20planning%20algorithm.%20In%0Aaddition%2C%20existing%20dynamic%20local%20planner%20algorithms%20for%20robotic%20wheelchairs%20do%0Anot%20take%20into%20account%20the%20social%20spaces%20of%20people%2C%20potentially%20leading%20such%0Aplatforms%20to%20infringe%20upon%20these%20areas%20and%20cause%20discomfort.%20To%20address%20these%0Aconcerns%2C%20this%20work%20introduces%20a%20novel%20socially-aware%20shared%20autonomy-based%0Anavigation%20system%20for%20assistive%20mobile%20robotic%20platforms.%0A%20%20Our%20navigation%20framework%20comprises%20a%20Global%20Planner%20and%20a%20Local%20Planner.%20To%0Aimplement%20the%20Global%20Planner%2C%20the%20proposed%20approach%20introduces%20a%20novel%20User%0APreference%20Field%20%28UPF%29%20theory%20within%20its%20global%20planning%20framework%2C%20explicitly%0Aacknowledging%20user%20preferences%20to%20adeptly%20navigate%20away%20from%20congested%20areas.%0AFor%20the%20Local%20Planner%2C%20we%20propose%20a%20Socially-aware%20Shared%20Control-based%20Model%0APredictive%20Control%20with%20Dynamic%20Control%20Barrier%20Function%20%28SS-MPC-DCBF%29%20to%0Aadjust%20movements%20in%20real-time%2C%20integrating%20user%20preferences%20for%20safer%2C%20more%0Aautonomous%20navigation.%20Evaluation%20results%20show%20that%20our%20Global%20Planner%20aligns%0Aclosely%20with%20user%20preferences%20compared%20to%20baselines%2C%20and%20our%20Local%20Planner%0Ademonstrates%20enhanced%20safety%20and%20efficiency%20in%20dynamic%20and%20static%20scenarios.%0AThis%20integrated%20approach%20fosters%20trust%20and%20autonomy%2C%20crucial%20for%20the%20acceptance%0Aof%20assistive%20mobility%20technologies%20in%20the%20built%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17279v1&entry.124074799=Read"},
{"title": "Tracking Small Birds by Detection Candidate Region Filtering and\n  Detection History-aware Association", "author": "Tingwei Liu and Yasutomo Kawanishi and Takahiro Komamizu and Ichiro Ide", "abstract": "  This paper focuses on tracking birds that appear small in a panoramic video.\nWhen the size of the tracked object is small in the image (small object\ntracking) and move quickly, object detection and association suffers. To\naddress these problems, we propose Adaptive Slicing Aided Hyper Inference\n(Adaptive SAHI), which reduces the candidate regions to apply detection, and\nDetection History-aware Similarity Criterion (DHSC), which accurately\nassociates objects in consecutive frames based on the detection history.\nExperiments on the NUBird2022 dataset verifies the effectiveness of the\nproposed method by showing improvements in both accuracy and speed.\n", "link": "http://arxiv.org/abs/2405.17323v1", "date": "2024-05-27", "relevancy": 2.4754, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4936}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20Small%20Birds%20by%20Detection%20Candidate%20Region%20Filtering%20and%0A%20%20Detection%20History-aware%20Association&body=Title%3A%20Tracking%20Small%20Birds%20by%20Detection%20Candidate%20Region%20Filtering%20and%0A%20%20Detection%20History-aware%20Association%0AAuthor%3A%20Tingwei%20Liu%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20tracking%20birds%20that%20appear%20small%20in%20a%20panoramic%20video.%0AWhen%20the%20size%20of%20the%20tracked%20object%20is%20small%20in%20the%20image%20%28small%20object%0Atracking%29%20and%20move%20quickly%2C%20object%20detection%20and%20association%20suffers.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20Adaptive%20Slicing%20Aided%20Hyper%20Inference%0A%28Adaptive%20SAHI%29%2C%20which%20reduces%20the%20candidate%20regions%20to%20apply%20detection%2C%20and%0ADetection%20History-aware%20Similarity%20Criterion%20%28DHSC%29%2C%20which%20accurately%0Aassociates%20objects%20in%20consecutive%20frames%20based%20on%20the%20detection%20history.%0AExperiments%20on%20the%20NUBird2022%20dataset%20verifies%20the%20effectiveness%20of%20the%0Aproposed%20method%20by%20showing%20improvements%20in%20both%20accuracy%20and%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520Small%2520Birds%2520by%2520Detection%2520Candidate%2520Region%2520Filtering%2520and%250A%2520%2520Detection%2520History-aware%2520Association%26entry.906535625%3DTingwei%2520Liu%2520and%2520Yasutomo%2520Kawanishi%2520and%2520Takahiro%2520Komamizu%2520and%2520Ichiro%2520Ide%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520tracking%2520birds%2520that%2520appear%2520small%2520in%2520a%2520panoramic%2520video.%250AWhen%2520the%2520size%2520of%2520the%2520tracked%2520object%2520is%2520small%2520in%2520the%2520image%2520%2528small%2520object%250Atracking%2529%2520and%2520move%2520quickly%252C%2520object%2520detection%2520and%2520association%2520suffers.%2520To%250Aaddress%2520these%2520problems%252C%2520we%2520propose%2520Adaptive%2520Slicing%2520Aided%2520Hyper%2520Inference%250A%2528Adaptive%2520SAHI%2529%252C%2520which%2520reduces%2520the%2520candidate%2520regions%2520to%2520apply%2520detection%252C%2520and%250ADetection%2520History-aware%2520Similarity%2520Criterion%2520%2528DHSC%2529%252C%2520which%2520accurately%250Aassociates%2520objects%2520in%2520consecutive%2520frames%2520based%2520on%2520the%2520detection%2520history.%250AExperiments%2520on%2520the%2520NUBird2022%2520dataset%2520verifies%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method%2520by%2520showing%2520improvements%2520in%2520both%2520accuracy%2520and%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20Small%20Birds%20by%20Detection%20Candidate%20Region%20Filtering%20and%0A%20%20Detection%20History-aware%20Association&entry.906535625=Tingwei%20Liu%20and%20Yasutomo%20Kawanishi%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide&entry.1292438233=%20%20This%20paper%20focuses%20on%20tracking%20birds%20that%20appear%20small%20in%20a%20panoramic%20video.%0AWhen%20the%20size%20of%20the%20tracked%20object%20is%20small%20in%20the%20image%20%28small%20object%0Atracking%29%20and%20move%20quickly%2C%20object%20detection%20and%20association%20suffers.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20Adaptive%20Slicing%20Aided%20Hyper%20Inference%0A%28Adaptive%20SAHI%29%2C%20which%20reduces%20the%20candidate%20regions%20to%20apply%20detection%2C%20and%0ADetection%20History-aware%20Similarity%20Criterion%20%28DHSC%29%2C%20which%20accurately%0Aassociates%20objects%20in%20consecutive%20frames%20based%20on%20the%20detection%20history.%0AExperiments%20on%20the%20NUBird2022%20dataset%20verifies%20the%20effectiveness%20of%20the%0Aproposed%20method%20by%20showing%20improvements%20in%20both%20accuracy%20and%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17323v1&entry.124074799=Read"},
{"title": "Towards Weakly-Supervised Hate Speech Classification Across Datasets", "author": "Yiping Jin and Leo Wanner and Vishakha Laxman Kadam and Alexander Shvets", "abstract": "  As pointed out by several scholars, current research on hate speech (HS)\nrecognition is characterized by unsystematic data creation strategies and\ndiverging annotation schemata. Subsequently, supervised-learning models tend to\ngeneralize poorly to datasets they were not trained on, and the performance of\nthe models trained on datasets labeled using different HS taxonomies cannot be\ncompared. To ease this problem, we propose applying extremely weak supervision\nthat only relies on the class name rather than on class samples from the\nannotated data. We demonstrate the effectiveness of a state-of-the-art\nweakly-supervised text classification model in various in-dataset and\ncross-dataset settings. Furthermore, we conduct an in-depth quantitative and\nqualitative analysis of the source of poor generalizability of HS\nclassification models.\n", "link": "http://arxiv.org/abs/2305.02637v3", "date": "2024-05-27", "relevancy": 2.4599, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5307}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4869}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Weakly-Supervised%20Hate%20Speech%20Classification%20Across%20Datasets&body=Title%3A%20Towards%20Weakly-Supervised%20Hate%20Speech%20Classification%20Across%20Datasets%0AAuthor%3A%20Yiping%20Jin%20and%20Leo%20Wanner%20and%20Vishakha%20Laxman%20Kadam%20and%20Alexander%20Shvets%0AAbstract%3A%20%20%20As%20pointed%20out%20by%20several%20scholars%2C%20current%20research%20on%20hate%20speech%20%28HS%29%0Arecognition%20is%20characterized%20by%20unsystematic%20data%20creation%20strategies%20and%0Adiverging%20annotation%20schemata.%20Subsequently%2C%20supervised-learning%20models%20tend%20to%0Ageneralize%20poorly%20to%20datasets%20they%20were%20not%20trained%20on%2C%20and%20the%20performance%20of%0Athe%20models%20trained%20on%20datasets%20labeled%20using%20different%20HS%20taxonomies%20cannot%20be%0Acompared.%20To%20ease%20this%20problem%2C%20we%20propose%20applying%20extremely%20weak%20supervision%0Athat%20only%20relies%20on%20the%20class%20name%20rather%20than%20on%20class%20samples%20from%20the%0Aannotated%20data.%20We%20demonstrate%20the%20effectiveness%20of%20a%20state-of-the-art%0Aweakly-supervised%20text%20classification%20model%20in%20various%20in-dataset%20and%0Across-dataset%20settings.%20Furthermore%2C%20we%20conduct%20an%20in-depth%20quantitative%20and%0Aqualitative%20analysis%20of%20the%20source%20of%20poor%20generalizability%20of%20HS%0Aclassification%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.02637v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Weakly-Supervised%2520Hate%2520Speech%2520Classification%2520Across%2520Datasets%26entry.906535625%3DYiping%2520Jin%2520and%2520Leo%2520Wanner%2520and%2520Vishakha%2520Laxman%2520Kadam%2520and%2520Alexander%2520Shvets%26entry.1292438233%3D%2520%2520As%2520pointed%2520out%2520by%2520several%2520scholars%252C%2520current%2520research%2520on%2520hate%2520speech%2520%2528HS%2529%250Arecognition%2520is%2520characterized%2520by%2520unsystematic%2520data%2520creation%2520strategies%2520and%250Adiverging%2520annotation%2520schemata.%2520Subsequently%252C%2520supervised-learning%2520models%2520tend%2520to%250Ageneralize%2520poorly%2520to%2520datasets%2520they%2520were%2520not%2520trained%2520on%252C%2520and%2520the%2520performance%2520of%250Athe%2520models%2520trained%2520on%2520datasets%2520labeled%2520using%2520different%2520HS%2520taxonomies%2520cannot%2520be%250Acompared.%2520To%2520ease%2520this%2520problem%252C%2520we%2520propose%2520applying%2520extremely%2520weak%2520supervision%250Athat%2520only%2520relies%2520on%2520the%2520class%2520name%2520rather%2520than%2520on%2520class%2520samples%2520from%2520the%250Aannotated%2520data.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520a%2520state-of-the-art%250Aweakly-supervised%2520text%2520classification%2520model%2520in%2520various%2520in-dataset%2520and%250Across-dataset%2520settings.%2520Furthermore%252C%2520we%2520conduct%2520an%2520in-depth%2520quantitative%2520and%250Aqualitative%2520analysis%2520of%2520the%2520source%2520of%2520poor%2520generalizability%2520of%2520HS%250Aclassification%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.02637v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Weakly-Supervised%20Hate%20Speech%20Classification%20Across%20Datasets&entry.906535625=Yiping%20Jin%20and%20Leo%20Wanner%20and%20Vishakha%20Laxman%20Kadam%20and%20Alexander%20Shvets&entry.1292438233=%20%20As%20pointed%20out%20by%20several%20scholars%2C%20current%20research%20on%20hate%20speech%20%28HS%29%0Arecognition%20is%20characterized%20by%20unsystematic%20data%20creation%20strategies%20and%0Adiverging%20annotation%20schemata.%20Subsequently%2C%20supervised-learning%20models%20tend%20to%0Ageneralize%20poorly%20to%20datasets%20they%20were%20not%20trained%20on%2C%20and%20the%20performance%20of%0Athe%20models%20trained%20on%20datasets%20labeled%20using%20different%20HS%20taxonomies%20cannot%20be%0Acompared.%20To%20ease%20this%20problem%2C%20we%20propose%20applying%20extremely%20weak%20supervision%0Athat%20only%20relies%20on%20the%20class%20name%20rather%20than%20on%20class%20samples%20from%20the%0Aannotated%20data.%20We%20demonstrate%20the%20effectiveness%20of%20a%20state-of-the-art%0Aweakly-supervised%20text%20classification%20model%20in%20various%20in-dataset%20and%0Across-dataset%20settings.%20Furthermore%2C%20we%20conduct%20an%20in-depth%20quantitative%20and%0Aqualitative%20analysis%20of%20the%20source%20of%20poor%20generalizability%20of%20HS%0Aclassification%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.02637v3&entry.124074799=Read"},
{"title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\n  Models", "author": "Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping", "abstract": "  Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.\n", "link": "http://arxiv.org/abs/2405.17428v1", "date": "2024-05-27", "relevancy": 2.4542, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5147}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NV-Embed%3A%20Improved%20Techniques%20for%20Training%20LLMs%20as%20Generalist%20Embedding%0A%20%20Models&body=Title%3A%20NV-Embed%3A%20Improved%20Techniques%20for%20Training%20LLMs%20as%20Generalist%20Embedding%0A%20%20Models%0AAuthor%3A%20Chankyu%20Lee%20and%20Rajarshi%20Roy%20and%20Mengyao%20Xu%20and%20Jonathan%20Raiman%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping%0AAbstract%3A%20%20%20Decoder-only%20large%20language%20model%20%28LLM%29-based%20embedding%20models%20are%20beginning%0Ato%20outperform%20BERT%20or%20T5-based%20embedding%20models%20in%20general-purpose%20text%0Aembedding%20tasks%2C%20including%20dense%20vector-based%20retrieval.%20In%20this%20work%2C%20we%0Aintroduce%20the%20NV-Embed%20model%20with%20a%20variety%20of%20architectural%20designs%20and%0Atraining%20procedures%20to%20significantly%20enhance%20the%20performance%20of%20LLM%20as%20a%0Aversatile%20embedding%20model%2C%20while%20maintaining%20its%20simplicity%20and%0Areproducibility.%20For%20model%20architecture%2C%20we%20propose%20a%20latent%20attention%20layer%20to%0Aobtain%20pooled%20embeddings%2C%20which%20consistently%20improves%20retrieval%20and%20downstream%0Atask%20accuracy%20compared%20to%20mean%20pooling%20or%20using%20the%20last%20%3CEOS%3E%20token%20embedding%0Afrom%20LLMs.%20To%20enhance%20representation%20learning%2C%20we%20remove%20the%20causal%20attention%0Amask%20of%20LLMs%20during%20contrastive%20training.%20For%20model%20training%2C%20we%20introduce%20a%0Atwo-stage%20contrastive%20instruction-tuning%20method.%20It%20first%20applies%20contrastive%0Atraining%20with%20instructions%20on%20retrieval%20datasets%2C%20utilizing%20in-batch%20negatives%0Aand%20curated%20hard%20negative%20examples.%20At%20stage-2%2C%20it%20blends%20various%20non-retrieval%0Adatasets%20into%20instruction%20tuning%2C%20which%20not%20only%20enhances%20non-retrieval%20task%0Aaccuracy%20but%20also%20improves%20retrieval%20performance.%20Combining%20these%20techniques%2C%0Aour%20NV-Embed%20model%2C%20using%20only%20publicly%20available%20data%2C%20has%20achieved%20a%0Arecord-high%20score%20of%2069.32%2C%20ranking%20No.%201%20on%20the%20Massive%20Text%20Embedding%0ABenchmark%20%28MTEB%29%20%28as%20of%20May%2024%2C%202024%29%2C%20with%2056%20tasks%2C%20encompassing%20retrieval%2C%0Areranking%2C%20classification%2C%20clustering%2C%20and%20semantic%20textual%20similarity%20tasks.%0ANotably%2C%20our%20model%20also%20attains%20the%20highest%20score%20of%2059.36%20on%2015%20retrieval%0Atasks%20in%20the%20MTEB%20benchmark%20%28also%20known%20as%20BEIR%29.%20We%20will%20open-source%20the%20model%0Aat%3A%20https%3A//huggingface.co/nvidia/NV-Embed-v1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNV-Embed%253A%2520Improved%2520Techniques%2520for%2520Training%2520LLMs%2520as%2520Generalist%2520Embedding%250A%2520%2520Models%26entry.906535625%3DChankyu%2520Lee%2520and%2520Rajarshi%2520Roy%2520and%2520Mengyao%2520Xu%2520and%2520Jonathan%2520Raiman%2520and%2520Mohammad%2520Shoeybi%2520and%2520Bryan%2520Catanzaro%2520and%2520Wei%2520Ping%26entry.1292438233%3D%2520%2520Decoder-only%2520large%2520language%2520model%2520%2528LLM%2529-based%2520embedding%2520models%2520are%2520beginning%250Ato%2520outperform%2520BERT%2520or%2520T5-based%2520embedding%2520models%2520in%2520general-purpose%2520text%250Aembedding%2520tasks%252C%2520including%2520dense%2520vector-based%2520retrieval.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520NV-Embed%2520model%2520with%2520a%2520variety%2520of%2520architectural%2520designs%2520and%250Atraining%2520procedures%2520to%2520significantly%2520enhance%2520the%2520performance%2520of%2520LLM%2520as%2520a%250Aversatile%2520embedding%2520model%252C%2520while%2520maintaining%2520its%2520simplicity%2520and%250Areproducibility.%2520For%2520model%2520architecture%252C%2520we%2520propose%2520a%2520latent%2520attention%2520layer%2520to%250Aobtain%2520pooled%2520embeddings%252C%2520which%2520consistently%2520improves%2520retrieval%2520and%2520downstream%250Atask%2520accuracy%2520compared%2520to%2520mean%2520pooling%2520or%2520using%2520the%2520last%2520%253CEOS%253E%2520token%2520embedding%250Afrom%2520LLMs.%2520To%2520enhance%2520representation%2520learning%252C%2520we%2520remove%2520the%2520causal%2520attention%250Amask%2520of%2520LLMs%2520during%2520contrastive%2520training.%2520For%2520model%2520training%252C%2520we%2520introduce%2520a%250Atwo-stage%2520contrastive%2520instruction-tuning%2520method.%2520It%2520first%2520applies%2520contrastive%250Atraining%2520with%2520instructions%2520on%2520retrieval%2520datasets%252C%2520utilizing%2520in-batch%2520negatives%250Aand%2520curated%2520hard%2520negative%2520examples.%2520At%2520stage-2%252C%2520it%2520blends%2520various%2520non-retrieval%250Adatasets%2520into%2520instruction%2520tuning%252C%2520which%2520not%2520only%2520enhances%2520non-retrieval%2520task%250Aaccuracy%2520but%2520also%2520improves%2520retrieval%2520performance.%2520Combining%2520these%2520techniques%252C%250Aour%2520NV-Embed%2520model%252C%2520using%2520only%2520publicly%2520available%2520data%252C%2520has%2520achieved%2520a%250Arecord-high%2520score%2520of%252069.32%252C%2520ranking%2520No.%25201%2520on%2520the%2520Massive%2520Text%2520Embedding%250ABenchmark%2520%2528MTEB%2529%2520%2528as%2520of%2520May%252024%252C%25202024%2529%252C%2520with%252056%2520tasks%252C%2520encompassing%2520retrieval%252C%250Areranking%252C%2520classification%252C%2520clustering%252C%2520and%2520semantic%2520textual%2520similarity%2520tasks.%250ANotably%252C%2520our%2520model%2520also%2520attains%2520the%2520highest%2520score%2520of%252059.36%2520on%252015%2520retrieval%250Atasks%2520in%2520the%2520MTEB%2520benchmark%2520%2528also%2520known%2520as%2520BEIR%2529.%2520We%2520will%2520open-source%2520the%2520model%250Aat%253A%2520https%253A//huggingface.co/nvidia/NV-Embed-v1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NV-Embed%3A%20Improved%20Techniques%20for%20Training%20LLMs%20as%20Generalist%20Embedding%0A%20%20Models&entry.906535625=Chankyu%20Lee%20and%20Rajarshi%20Roy%20and%20Mengyao%20Xu%20and%20Jonathan%20Raiman%20and%20Mohammad%20Shoeybi%20and%20Bryan%20Catanzaro%20and%20Wei%20Ping&entry.1292438233=%20%20Decoder-only%20large%20language%20model%20%28LLM%29-based%20embedding%20models%20are%20beginning%0Ato%20outperform%20BERT%20or%20T5-based%20embedding%20models%20in%20general-purpose%20text%0Aembedding%20tasks%2C%20including%20dense%20vector-based%20retrieval.%20In%20this%20work%2C%20we%0Aintroduce%20the%20NV-Embed%20model%20with%20a%20variety%20of%20architectural%20designs%20and%0Atraining%20procedures%20to%20significantly%20enhance%20the%20performance%20of%20LLM%20as%20a%0Aversatile%20embedding%20model%2C%20while%20maintaining%20its%20simplicity%20and%0Areproducibility.%20For%20model%20architecture%2C%20we%20propose%20a%20latent%20attention%20layer%20to%0Aobtain%20pooled%20embeddings%2C%20which%20consistently%20improves%20retrieval%20and%20downstream%0Atask%20accuracy%20compared%20to%20mean%20pooling%20or%20using%20the%20last%20%3CEOS%3E%20token%20embedding%0Afrom%20LLMs.%20To%20enhance%20representation%20learning%2C%20we%20remove%20the%20causal%20attention%0Amask%20of%20LLMs%20during%20contrastive%20training.%20For%20model%20training%2C%20we%20introduce%20a%0Atwo-stage%20contrastive%20instruction-tuning%20method.%20It%20first%20applies%20contrastive%0Atraining%20with%20instructions%20on%20retrieval%20datasets%2C%20utilizing%20in-batch%20negatives%0Aand%20curated%20hard%20negative%20examples.%20At%20stage-2%2C%20it%20blends%20various%20non-retrieval%0Adatasets%20into%20instruction%20tuning%2C%20which%20not%20only%20enhances%20non-retrieval%20task%0Aaccuracy%20but%20also%20improves%20retrieval%20performance.%20Combining%20these%20techniques%2C%0Aour%20NV-Embed%20model%2C%20using%20only%20publicly%20available%20data%2C%20has%20achieved%20a%0Arecord-high%20score%20of%2069.32%2C%20ranking%20No.%201%20on%20the%20Massive%20Text%20Embedding%0ABenchmark%20%28MTEB%29%20%28as%20of%20May%2024%2C%202024%29%2C%20with%2056%20tasks%2C%20encompassing%20retrieval%2C%0Areranking%2C%20classification%2C%20clustering%2C%20and%20semantic%20textual%20similarity%20tasks.%0ANotably%2C%20our%20model%20also%20attains%20the%20highest%20score%20of%2059.36%20on%2015%20retrieval%0Atasks%20in%20the%20MTEB%20benchmark%20%28also%20known%20as%20BEIR%29.%20We%20will%20open-source%20the%20model%0Aat%3A%20https%3A//huggingface.co/nvidia/NV-Embed-v1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17428v1&entry.124074799=Read"},
{"title": "Low Fidelity Digital Twin for Automated Driving Systems: Use Cases and\n  Automatic Generation", "author": "Jiri Vlasak and Jaroslav Klap\u00e1lek and Adam Kollar\u010d\u00edk and Michal Sojka and Zden\u011bk Hanz\u00e1lek", "abstract": "  Automated driving systems are an integral part of the automotive industry.\nTools such as Robot Operating System and simulators support their development.\nHowever, in the end, the developers must test their algorithms on a real\nvehicle. To better observe the difference between reality and simulation--the\nreality gap--digital twin technology offers real-time communication between the\nreal vehicle and its model. We present low fidelity digital twin generator and\ndescribe situations where automatic generation is preferable to high fidelity\nsimulation. We validated our approach of generating a virtual environment with\na vehicle model by replaying the data recorded from the real vehicle.\n", "link": "http://arxiv.org/abs/2405.13705v2", "date": "2024-05-27", "relevancy": 2.4198, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.484}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.484}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation&body=Title%3A%20Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation%0AAuthor%3A%20Jiri%20Vlasak%20and%20Jaroslav%20Klap%C3%A1lek%20and%20Adam%20Kollar%C4%8D%C3%ADk%20and%20Michal%20Sojka%20and%20Zden%C4%9Bk%20Hanz%C3%A1lek%0AAbstract%3A%20%20%20Automated%20driving%20systems%20are%20an%20integral%20part%20of%20the%20automotive%20industry.%0ATools%20such%20as%20Robot%20Operating%20System%20and%20simulators%20support%20their%20development.%0AHowever%2C%20in%20the%20end%2C%20the%20developers%20must%20test%20their%20algorithms%20on%20a%20real%0Avehicle.%20To%20better%20observe%20the%20difference%20between%20reality%20and%20simulation--the%0Areality%20gap--digital%20twin%20technology%20offers%20real-time%20communication%20between%20the%0Areal%20vehicle%20and%20its%20model.%20We%20present%20low%20fidelity%20digital%20twin%20generator%20and%0Adescribe%20situations%20where%20automatic%20generation%20is%20preferable%20to%20high%20fidelity%0Asimulation.%20We%20validated%20our%20approach%20of%20generating%20a%20virtual%20environment%20with%0Aa%20vehicle%20model%20by%20replaying%20the%20data%20recorded%20from%20the%20real%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Fidelity%2520Digital%2520Twin%2520for%2520Automated%2520Driving%2520Systems%253A%2520Use%2520Cases%2520and%250A%2520%2520Automatic%2520Generation%26entry.906535625%3DJiri%2520Vlasak%2520and%2520Jaroslav%2520Klap%25C3%25A1lek%2520and%2520Adam%2520Kollar%25C4%258D%25C3%25ADk%2520and%2520Michal%2520Sojka%2520and%2520Zden%25C4%259Bk%2520Hanz%25C3%25A1lek%26entry.1292438233%3D%2520%2520Automated%2520driving%2520systems%2520are%2520an%2520integral%2520part%2520of%2520the%2520automotive%2520industry.%250ATools%2520such%2520as%2520Robot%2520Operating%2520System%2520and%2520simulators%2520support%2520their%2520development.%250AHowever%252C%2520in%2520the%2520end%252C%2520the%2520developers%2520must%2520test%2520their%2520algorithms%2520on%2520a%2520real%250Avehicle.%2520To%2520better%2520observe%2520the%2520difference%2520between%2520reality%2520and%2520simulation--the%250Areality%2520gap--digital%2520twin%2520technology%2520offers%2520real-time%2520communication%2520between%2520the%250Areal%2520vehicle%2520and%2520its%2520model.%2520We%2520present%2520low%2520fidelity%2520digital%2520twin%2520generator%2520and%250Adescribe%2520situations%2520where%2520automatic%2520generation%2520is%2520preferable%2520to%2520high%2520fidelity%250Asimulation.%2520We%2520validated%2520our%2520approach%2520of%2520generating%2520a%2520virtual%2520environment%2520with%250Aa%2520vehicle%2520model%2520by%2520replaying%2520the%2520data%2520recorded%2520from%2520the%2520real%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Fidelity%20Digital%20Twin%20for%20Automated%20Driving%20Systems%3A%20Use%20Cases%20and%0A%20%20Automatic%20Generation&entry.906535625=Jiri%20Vlasak%20and%20Jaroslav%20Klap%C3%A1lek%20and%20Adam%20Kollar%C4%8D%C3%ADk%20and%20Michal%20Sojka%20and%20Zden%C4%9Bk%20Hanz%C3%A1lek&entry.1292438233=%20%20Automated%20driving%20systems%20are%20an%20integral%20part%20of%20the%20automotive%20industry.%0ATools%20such%20as%20Robot%20Operating%20System%20and%20simulators%20support%20their%20development.%0AHowever%2C%20in%20the%20end%2C%20the%20developers%20must%20test%20their%20algorithms%20on%20a%20real%0Avehicle.%20To%20better%20observe%20the%20difference%20between%20reality%20and%20simulation--the%0Areality%20gap--digital%20twin%20technology%20offers%20real-time%20communication%20between%20the%0Areal%20vehicle%20and%20its%20model.%20We%20present%20low%20fidelity%20digital%20twin%20generator%20and%0Adescribe%20situations%20where%20automatic%20generation%20is%20preferable%20to%20high%20fidelity%0Asimulation.%20We%20validated%20our%20approach%20of%20generating%20a%20virtual%20environment%20with%0Aa%20vehicle%20model%20by%20replaying%20the%20data%20recorded%20from%20the%20real%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13705v2&entry.124074799=Read"},
{"title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients", "author": "Royson Lee and Javier Fernandez-Marques and Shell Xu Hu and Da Li and Stefanos Laskaridis and \u0141ukasz Dudziak and Timothy Hospedales and Ferenc Husz\u00e1r and Nicholas D. Lane", "abstract": "  Federated learning (FL) has enabled distributed learning of a model across\nmultiple clients in a privacy-preserving manner. One of the main challenges of\nFL is to accommodate clients with varying hardware capacities; clients have\ndiffering compute and memory requirements. To tackle this challenge, recent\nstate-of-the-art approaches leverage the use of early exits. Nonetheless, these\napproaches fall short of mitigating the challenges of joint learning multiple\nexit classifiers, often relying on hand-picked heuristic solutions for\nknowledge distillation among classifiers and/or utilizing additional layers for\nweaker classifiers. In this work, instead of utilizing multiple classifiers, we\npropose a recurrent early exit approach named ReeFL that fuses features from\ndifferent sub-models into a single shared classifier. Specifically, we use a\ntransformer-based early-exit module shared among sub-models to i) better\nexploit multi-layer feature representations for task-specific prediction and\nii) modulate the feature representation of the backbone model for subsequent\npredictions. We additionally present a per-client self-distillation approach\nwhere the best sub-model is automatically selected as the teacher of the other\nsub-models at each client. Our experiments on standard image and speech\nclassification benchmarks across various emerging federated fine-tuning\nbaselines demonstrate ReeFL's effectiveness over previous works.\n", "link": "http://arxiv.org/abs/2405.14791v2", "date": "2024-05-27", "relevancy": 2.4173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4956}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients&body=Title%3A%20Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients%0AAuthor%3A%20Royson%20Lee%20and%20Javier%20Fernandez-Marques%20and%20Shell%20Xu%20Hu%20and%20Da%20Li%20and%20Stefanos%20Laskaridis%20and%20%C5%81ukasz%20Dudziak%20and%20Timothy%20Hospedales%20and%20Ferenc%20Husz%C3%A1r%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20enabled%20distributed%20learning%20of%20a%20model%20across%0Amultiple%20clients%20in%20a%20privacy-preserving%20manner.%20One%20of%20the%20main%20challenges%20of%0AFL%20is%20to%20accommodate%20clients%20with%20varying%20hardware%20capacities%3B%20clients%20have%0Adiffering%20compute%20and%20memory%20requirements.%20To%20tackle%20this%20challenge%2C%20recent%0Astate-of-the-art%20approaches%20leverage%20the%20use%20of%20early%20exits.%20Nonetheless%2C%20these%0Aapproaches%20fall%20short%20of%20mitigating%20the%20challenges%20of%20joint%20learning%20multiple%0Aexit%20classifiers%2C%20often%20relying%20on%20hand-picked%20heuristic%20solutions%20for%0Aknowledge%20distillation%20among%20classifiers%20and/or%20utilizing%20additional%20layers%20for%0Aweaker%20classifiers.%20In%20this%20work%2C%20instead%20of%20utilizing%20multiple%20classifiers%2C%20we%0Apropose%20a%20recurrent%20early%20exit%20approach%20named%20ReeFL%20that%20fuses%20features%20from%0Adifferent%20sub-models%20into%20a%20single%20shared%20classifier.%20Specifically%2C%20we%20use%20a%0Atransformer-based%20early-exit%20module%20shared%20among%20sub-models%20to%20i%29%20better%0Aexploit%20multi-layer%20feature%20representations%20for%20task-specific%20prediction%20and%0Aii%29%20modulate%20the%20feature%20representation%20of%20the%20backbone%20model%20for%20subsequent%0Apredictions.%20We%20additionally%20present%20a%20per-client%20self-distillation%20approach%0Awhere%20the%20best%20sub-model%20is%20automatically%20selected%20as%20the%20teacher%20of%20the%20other%0Asub-models%20at%20each%20client.%20Our%20experiments%20on%20standard%20image%20and%20speech%0Aclassification%20benchmarks%20across%20various%20emerging%20federated%20fine-tuning%0Abaselines%20demonstrate%20ReeFL%27s%20effectiveness%20over%20previous%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Early%2520Exits%2520for%2520Federated%2520Learning%2520with%2520Heterogeneous%2520Clients%26entry.906535625%3DRoyson%2520Lee%2520and%2520Javier%2520Fernandez-Marques%2520and%2520Shell%2520Xu%2520Hu%2520and%2520Da%2520Li%2520and%2520Stefanos%2520Laskaridis%2520and%2520%25C5%2581ukasz%2520Dudziak%2520and%2520Timothy%2520Hospedales%2520and%2520Ferenc%2520Husz%25C3%25A1r%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520enabled%2520distributed%2520learning%2520of%2520a%2520model%2520across%250Amultiple%2520clients%2520in%2520a%2520privacy-preserving%2520manner.%2520One%2520of%2520the%2520main%2520challenges%2520of%250AFL%2520is%2520to%2520accommodate%2520clients%2520with%2520varying%2520hardware%2520capacities%253B%2520clients%2520have%250Adiffering%2520compute%2520and%2520memory%2520requirements.%2520To%2520tackle%2520this%2520challenge%252C%2520recent%250Astate-of-the-art%2520approaches%2520leverage%2520the%2520use%2520of%2520early%2520exits.%2520Nonetheless%252C%2520these%250Aapproaches%2520fall%2520short%2520of%2520mitigating%2520the%2520challenges%2520of%2520joint%2520learning%2520multiple%250Aexit%2520classifiers%252C%2520often%2520relying%2520on%2520hand-picked%2520heuristic%2520solutions%2520for%250Aknowledge%2520distillation%2520among%2520classifiers%2520and/or%2520utilizing%2520additional%2520layers%2520for%250Aweaker%2520classifiers.%2520In%2520this%2520work%252C%2520instead%2520of%2520utilizing%2520multiple%2520classifiers%252C%2520we%250Apropose%2520a%2520recurrent%2520early%2520exit%2520approach%2520named%2520ReeFL%2520that%2520fuses%2520features%2520from%250Adifferent%2520sub-models%2520into%2520a%2520single%2520shared%2520classifier.%2520Specifically%252C%2520we%2520use%2520a%250Atransformer-based%2520early-exit%2520module%2520shared%2520among%2520sub-models%2520to%2520i%2529%2520better%250Aexploit%2520multi-layer%2520feature%2520representations%2520for%2520task-specific%2520prediction%2520and%250Aii%2529%2520modulate%2520the%2520feature%2520representation%2520of%2520the%2520backbone%2520model%2520for%2520subsequent%250Apredictions.%2520We%2520additionally%2520present%2520a%2520per-client%2520self-distillation%2520approach%250Awhere%2520the%2520best%2520sub-model%2520is%2520automatically%2520selected%2520as%2520the%2520teacher%2520of%2520the%2520other%250Asub-models%2520at%2520each%2520client.%2520Our%2520experiments%2520on%2520standard%2520image%2520and%2520speech%250Aclassification%2520benchmarks%2520across%2520various%2520emerging%2520federated%2520fine-tuning%250Abaselines%2520demonstrate%2520ReeFL%2527s%2520effectiveness%2520over%2520previous%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients&entry.906535625=Royson%20Lee%20and%20Javier%20Fernandez-Marques%20and%20Shell%20Xu%20Hu%20and%20Da%20Li%20and%20Stefanos%20Laskaridis%20and%20%C5%81ukasz%20Dudziak%20and%20Timothy%20Hospedales%20and%20Ferenc%20Husz%C3%A1r%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20enabled%20distributed%20learning%20of%20a%20model%20across%0Amultiple%20clients%20in%20a%20privacy-preserving%20manner.%20One%20of%20the%20main%20challenges%20of%0AFL%20is%20to%20accommodate%20clients%20with%20varying%20hardware%20capacities%3B%20clients%20have%0Adiffering%20compute%20and%20memory%20requirements.%20To%20tackle%20this%20challenge%2C%20recent%0Astate-of-the-art%20approaches%20leverage%20the%20use%20of%20early%20exits.%20Nonetheless%2C%20these%0Aapproaches%20fall%20short%20of%20mitigating%20the%20challenges%20of%20joint%20learning%20multiple%0Aexit%20classifiers%2C%20often%20relying%20on%20hand-picked%20heuristic%20solutions%20for%0Aknowledge%20distillation%20among%20classifiers%20and/or%20utilizing%20additional%20layers%20for%0Aweaker%20classifiers.%20In%20this%20work%2C%20instead%20of%20utilizing%20multiple%20classifiers%2C%20we%0Apropose%20a%20recurrent%20early%20exit%20approach%20named%20ReeFL%20that%20fuses%20features%20from%0Adifferent%20sub-models%20into%20a%20single%20shared%20classifier.%20Specifically%2C%20we%20use%20a%0Atransformer-based%20early-exit%20module%20shared%20among%20sub-models%20to%20i%29%20better%0Aexploit%20multi-layer%20feature%20representations%20for%20task-specific%20prediction%20and%0Aii%29%20modulate%20the%20feature%20representation%20of%20the%20backbone%20model%20for%20subsequent%0Apredictions.%20We%20additionally%20present%20a%20per-client%20self-distillation%20approach%0Awhere%20the%20best%20sub-model%20is%20automatically%20selected%20as%20the%20teacher%20of%20the%20other%0Asub-models%20at%20each%20client.%20Our%20experiments%20on%20standard%20image%20and%20speech%0Aclassification%20benchmarks%20across%20various%20emerging%20federated%20fine-tuning%0Abaselines%20demonstrate%20ReeFL%27s%20effectiveness%20over%20previous%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14791v2&entry.124074799=Read"},
{"title": "MVMS-RCN: A Dual-Domain Unfolding CT Reconstruction with\n  Multi-sparse-view and Multi-scale Refinement-correction", "author": "Xiaohong Fan and Ke Chen and Huaming Yi and Yin Yang and Jianping Zhang", "abstract": "  X-ray Computed Tomography (CT) is one of the most important diagnostic\nimaging techniques in clinical applications. Sparse-view CT imaging reduces the\nnumber of projection views to a lower radiation dose and alleviates the\npotential risk of radiation exposure. Most existing deep learning (DL) and deep\nunfolding sparse-view CT reconstruction methods: 1) do not fully use the\nprojection data; 2) do not always link their architecture designs to a\nmathematical theory; 3) do not flexibly deal with multi-sparse-view\nreconstruction assignments. This paper aims to use mathematical ideas and\ndesign optimal DL imaging algorithms for sparse-view tomography\nreconstructions. We propose a novel dual-domain deep unfolding unified\nframework that offers a great deal of flexibility for multi-sparse-view CT\nreconstruction with different sampling views through a single model. This\nframework combines the theoretical advantages of model-based methods with the\nsuperior reconstruction performance of DL-based methods, resulting in the\nexpected generalizability of DL. We propose a refinement module that utilizes\nunfolding projection domain to refine full-sparse-view projection errors, as\nwell as an image domain correction module that distills multi-scale geometric\nerror corrections to reconstruct sparse-view CT. This provides us with a new\nway to explore the potential of projection information and a new perspective on\ndesigning network architectures. All parameters of our proposed framework are\nlearnable end to end, and our method possesses the potential to be applied to\nplug-and-play reconstruction. Extensive experiments demonstrate that our\nframework is superior to other existing state-of-the-art methods. Our source\ncodes are available at https://github.com/fanxiaohong/MVMS-RCN.\n", "link": "http://arxiv.org/abs/2405.17141v1", "date": "2024-05-27", "relevancy": 2.4131, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6135}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6135}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction&body=Title%3A%20MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction%0AAuthor%3A%20Xiaohong%20Fan%20and%20Ke%20Chen%20and%20Huaming%20Yi%20and%20Yin%20Yang%20and%20Jianping%20Zhang%0AAbstract%3A%20%20%20X-ray%20Computed%20Tomography%20%28CT%29%20is%20one%20of%20the%20most%20important%20diagnostic%0Aimaging%20techniques%20in%20clinical%20applications.%20Sparse-view%20CT%20imaging%20reduces%20the%0Anumber%20of%20projection%20views%20to%20a%20lower%20radiation%20dose%20and%20alleviates%20the%0Apotential%20risk%20of%20radiation%20exposure.%20Most%20existing%20deep%20learning%20%28DL%29%20and%20deep%0Aunfolding%20sparse-view%20CT%20reconstruction%20methods%3A%201%29%20do%20not%20fully%20use%20the%0Aprojection%20data%3B%202%29%20do%20not%20always%20link%20their%20architecture%20designs%20to%20a%0Amathematical%20theory%3B%203%29%20do%20not%20flexibly%20deal%20with%20multi-sparse-view%0Areconstruction%20assignments.%20This%20paper%20aims%20to%20use%20mathematical%20ideas%20and%0Adesign%20optimal%20DL%20imaging%20algorithms%20for%20sparse-view%20tomography%0Areconstructions.%20We%20propose%20a%20novel%20dual-domain%20deep%20unfolding%20unified%0Aframework%20that%20offers%20a%20great%20deal%20of%20flexibility%20for%20multi-sparse-view%20CT%0Areconstruction%20with%20different%20sampling%20views%20through%20a%20single%20model.%20This%0Aframework%20combines%20the%20theoretical%20advantages%20of%20model-based%20methods%20with%20the%0Asuperior%20reconstruction%20performance%20of%20DL-based%20methods%2C%20resulting%20in%20the%0Aexpected%20generalizability%20of%20DL.%20We%20propose%20a%20refinement%20module%20that%20utilizes%0Aunfolding%20projection%20domain%20to%20refine%20full-sparse-view%20projection%20errors%2C%20as%0Awell%20as%20an%20image%20domain%20correction%20module%20that%20distills%20multi-scale%20geometric%0Aerror%20corrections%20to%20reconstruct%20sparse-view%20CT.%20This%20provides%20us%20with%20a%20new%0Away%20to%20explore%20the%20potential%20of%20projection%20information%20and%20a%20new%20perspective%20on%0Adesigning%20network%20architectures.%20All%20parameters%20of%20our%20proposed%20framework%20are%0Alearnable%20end%20to%20end%2C%20and%20our%20method%20possesses%20the%20potential%20to%20be%20applied%20to%0Aplug-and-play%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20is%20superior%20to%20other%20existing%20state-of-the-art%20methods.%20Our%20source%0Acodes%20are%20available%20at%20https%3A//github.com/fanxiaohong/MVMS-RCN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVMS-RCN%253A%2520A%2520Dual-Domain%2520Unfolding%2520CT%2520Reconstruction%2520with%250A%2520%2520Multi-sparse-view%2520and%2520Multi-scale%2520Refinement-correction%26entry.906535625%3DXiaohong%2520Fan%2520and%2520Ke%2520Chen%2520and%2520Huaming%2520Yi%2520and%2520Yin%2520Yang%2520and%2520Jianping%2520Zhang%26entry.1292438233%3D%2520%2520X-ray%2520Computed%2520Tomography%2520%2528CT%2529%2520is%2520one%2520of%2520the%2520most%2520important%2520diagnostic%250Aimaging%2520techniques%2520in%2520clinical%2520applications.%2520Sparse-view%2520CT%2520imaging%2520reduces%2520the%250Anumber%2520of%2520projection%2520views%2520to%2520a%2520lower%2520radiation%2520dose%2520and%2520alleviates%2520the%250Apotential%2520risk%2520of%2520radiation%2520exposure.%2520Most%2520existing%2520deep%2520learning%2520%2528DL%2529%2520and%2520deep%250Aunfolding%2520sparse-view%2520CT%2520reconstruction%2520methods%253A%25201%2529%2520do%2520not%2520fully%2520use%2520the%250Aprojection%2520data%253B%25202%2529%2520do%2520not%2520always%2520link%2520their%2520architecture%2520designs%2520to%2520a%250Amathematical%2520theory%253B%25203%2529%2520do%2520not%2520flexibly%2520deal%2520with%2520multi-sparse-view%250Areconstruction%2520assignments.%2520This%2520paper%2520aims%2520to%2520use%2520mathematical%2520ideas%2520and%250Adesign%2520optimal%2520DL%2520imaging%2520algorithms%2520for%2520sparse-view%2520tomography%250Areconstructions.%2520We%2520propose%2520a%2520novel%2520dual-domain%2520deep%2520unfolding%2520unified%250Aframework%2520that%2520offers%2520a%2520great%2520deal%2520of%2520flexibility%2520for%2520multi-sparse-view%2520CT%250Areconstruction%2520with%2520different%2520sampling%2520views%2520through%2520a%2520single%2520model.%2520This%250Aframework%2520combines%2520the%2520theoretical%2520advantages%2520of%2520model-based%2520methods%2520with%2520the%250Asuperior%2520reconstruction%2520performance%2520of%2520DL-based%2520methods%252C%2520resulting%2520in%2520the%250Aexpected%2520generalizability%2520of%2520DL.%2520We%2520propose%2520a%2520refinement%2520module%2520that%2520utilizes%250Aunfolding%2520projection%2520domain%2520to%2520refine%2520full-sparse-view%2520projection%2520errors%252C%2520as%250Awell%2520as%2520an%2520image%2520domain%2520correction%2520module%2520that%2520distills%2520multi-scale%2520geometric%250Aerror%2520corrections%2520to%2520reconstruct%2520sparse-view%2520CT.%2520This%2520provides%2520us%2520with%2520a%2520new%250Away%2520to%2520explore%2520the%2520potential%2520of%2520projection%2520information%2520and%2520a%2520new%2520perspective%2520on%250Adesigning%2520network%2520architectures.%2520All%2520parameters%2520of%2520our%2520proposed%2520framework%2520are%250Alearnable%2520end%2520to%2520end%252C%2520and%2520our%2520method%2520possesses%2520the%2520potential%2520to%2520be%2520applied%2520to%250Aplug-and-play%2520reconstruction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aframework%2520is%2520superior%2520to%2520other%2520existing%2520state-of-the-art%2520methods.%2520Our%2520source%250Acodes%2520are%2520available%2520at%2520https%253A//github.com/fanxiaohong/MVMS-RCN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVMS-RCN%3A%20A%20Dual-Domain%20Unfolding%20CT%20Reconstruction%20with%0A%20%20Multi-sparse-view%20and%20Multi-scale%20Refinement-correction&entry.906535625=Xiaohong%20Fan%20and%20Ke%20Chen%20and%20Huaming%20Yi%20and%20Yin%20Yang%20and%20Jianping%20Zhang&entry.1292438233=%20%20X-ray%20Computed%20Tomography%20%28CT%29%20is%20one%20of%20the%20most%20important%20diagnostic%0Aimaging%20techniques%20in%20clinical%20applications.%20Sparse-view%20CT%20imaging%20reduces%20the%0Anumber%20of%20projection%20views%20to%20a%20lower%20radiation%20dose%20and%20alleviates%20the%0Apotential%20risk%20of%20radiation%20exposure.%20Most%20existing%20deep%20learning%20%28DL%29%20and%20deep%0Aunfolding%20sparse-view%20CT%20reconstruction%20methods%3A%201%29%20do%20not%20fully%20use%20the%0Aprojection%20data%3B%202%29%20do%20not%20always%20link%20their%20architecture%20designs%20to%20a%0Amathematical%20theory%3B%203%29%20do%20not%20flexibly%20deal%20with%20multi-sparse-view%0Areconstruction%20assignments.%20This%20paper%20aims%20to%20use%20mathematical%20ideas%20and%0Adesign%20optimal%20DL%20imaging%20algorithms%20for%20sparse-view%20tomography%0Areconstructions.%20We%20propose%20a%20novel%20dual-domain%20deep%20unfolding%20unified%0Aframework%20that%20offers%20a%20great%20deal%20of%20flexibility%20for%20multi-sparse-view%20CT%0Areconstruction%20with%20different%20sampling%20views%20through%20a%20single%20model.%20This%0Aframework%20combines%20the%20theoretical%20advantages%20of%20model-based%20methods%20with%20the%0Asuperior%20reconstruction%20performance%20of%20DL-based%20methods%2C%20resulting%20in%20the%0Aexpected%20generalizability%20of%20DL.%20We%20propose%20a%20refinement%20module%20that%20utilizes%0Aunfolding%20projection%20domain%20to%20refine%20full-sparse-view%20projection%20errors%2C%20as%0Awell%20as%20an%20image%20domain%20correction%20module%20that%20distills%20multi-scale%20geometric%0Aerror%20corrections%20to%20reconstruct%20sparse-view%20CT.%20This%20provides%20us%20with%20a%20new%0Away%20to%20explore%20the%20potential%20of%20projection%20information%20and%20a%20new%20perspective%20on%0Adesigning%20network%20architectures.%20All%20parameters%20of%20our%20proposed%20framework%20are%0Alearnable%20end%20to%20end%2C%20and%20our%20method%20possesses%20the%20potential%20to%20be%20applied%20to%0Aplug-and-play%20reconstruction.%20Extensive%20experiments%20demonstrate%20that%20our%0Aframework%20is%20superior%20to%20other%20existing%20state-of-the-art%20methods.%20Our%20source%0Acodes%20are%20available%20at%20https%3A//github.com/fanxiaohong/MVMS-RCN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17141v1&entry.124074799=Read"},
{"title": "Novel Approaches for ML-Assisted Particle Track Reconstruction and Hit\n  Clustering", "author": "Uraz Odyurt and Nadezhda Dobreva and Zef Wolffs and Yue Zhao and Antonio Ferrer S\u00e1nchez and Roberto Ruiz de Austri Bazan and Jos\u00e9 D. Mart\u00edn-Guerrero and Ana-Lucia Varbanescu and Sascha Caron", "abstract": "  Track reconstruction is a vital aspect of High-Energy Physics (HEP) and plays\na critical role in major experiments. In this study, we delve into unexplored\navenues for particle track reconstruction and hit clustering. Firstly, we\nenhance the algorithmic design effort by utilising a simplified simulator\n(REDVID) to generate training data that is specifically composed for\nsimplicity. We demonstrate the effectiveness of this data in guiding the\ndevelopment of optimal network architectures. Additionally, we investigate the\napplication of image segmentation networks for this task, exploring their\npotential for accurate track reconstruction. Moreover, we approach the task\nfrom a different perspective by treating it as a hit sequence to track sequence\ntranslation problem. Specifically, we explore the utilisation of Transformer\narchitectures for tracking purposes. Our preliminary findings are covered in\ndetail. By considering this novel approach, we aim to uncover new insights and\npotential advancements in track reconstruction. This research sheds light on\npreviously unexplored methods and provides valuable insights for the field of\nparticle track reconstruction and hit clustering in HEP.\n", "link": "http://arxiv.org/abs/2405.17325v1", "date": "2024-05-27", "relevancy": 2.4053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4827}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Approaches%20for%20ML-Assisted%20Particle%20Track%20Reconstruction%20and%20Hit%0A%20%20Clustering&body=Title%3A%20Novel%20Approaches%20for%20ML-Assisted%20Particle%20Track%20Reconstruction%20and%20Hit%0A%20%20Clustering%0AAuthor%3A%20Uraz%20Odyurt%20and%20Nadezhda%20Dobreva%20and%20Zef%20Wolffs%20and%20Yue%20Zhao%20and%20Antonio%20Ferrer%20S%C3%A1nchez%20and%20Roberto%20Ruiz%20de%20Austri%20Bazan%20and%20Jos%C3%A9%20D.%20Mart%C3%ADn-Guerrero%20and%20Ana-Lucia%20Varbanescu%20and%20Sascha%20Caron%0AAbstract%3A%20%20%20Track%20reconstruction%20is%20a%20vital%20aspect%20of%20High-Energy%20Physics%20%28HEP%29%20and%20plays%0Aa%20critical%20role%20in%20major%20experiments.%20In%20this%20study%2C%20we%20delve%20into%20unexplored%0Aavenues%20for%20particle%20track%20reconstruction%20and%20hit%20clustering.%20Firstly%2C%20we%0Aenhance%20the%20algorithmic%20design%20effort%20by%20utilising%20a%20simplified%20simulator%0A%28REDVID%29%20to%20generate%20training%20data%20that%20is%20specifically%20composed%20for%0Asimplicity.%20We%20demonstrate%20the%20effectiveness%20of%20this%20data%20in%20guiding%20the%0Adevelopment%20of%20optimal%20network%20architectures.%20Additionally%2C%20we%20investigate%20the%0Aapplication%20of%20image%20segmentation%20networks%20for%20this%20task%2C%20exploring%20their%0Apotential%20for%20accurate%20track%20reconstruction.%20Moreover%2C%20we%20approach%20the%20task%0Afrom%20a%20different%20perspective%20by%20treating%20it%20as%20a%20hit%20sequence%20to%20track%20sequence%0Atranslation%20problem.%20Specifically%2C%20we%20explore%20the%20utilisation%20of%20Transformer%0Aarchitectures%20for%20tracking%20purposes.%20Our%20preliminary%20findings%20are%20covered%20in%0Adetail.%20By%20considering%20this%20novel%20approach%2C%20we%20aim%20to%20uncover%20new%20insights%20and%0Apotential%20advancements%20in%20track%20reconstruction.%20This%20research%20sheds%20light%20on%0Apreviously%20unexplored%20methods%20and%20provides%20valuable%20insights%20for%20the%20field%20of%0Aparticle%20track%20reconstruction%20and%20hit%20clustering%20in%20HEP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Approaches%2520for%2520ML-Assisted%2520Particle%2520Track%2520Reconstruction%2520and%2520Hit%250A%2520%2520Clustering%26entry.906535625%3DUraz%2520Odyurt%2520and%2520Nadezhda%2520Dobreva%2520and%2520Zef%2520Wolffs%2520and%2520Yue%2520Zhao%2520and%2520Antonio%2520Ferrer%2520S%25C3%25A1nchez%2520and%2520Roberto%2520Ruiz%2520de%2520Austri%2520Bazan%2520and%2520Jos%25C3%25A9%2520D.%2520Mart%25C3%25ADn-Guerrero%2520and%2520Ana-Lucia%2520Varbanescu%2520and%2520Sascha%2520Caron%26entry.1292438233%3D%2520%2520Track%2520reconstruction%2520is%2520a%2520vital%2520aspect%2520of%2520High-Energy%2520Physics%2520%2528HEP%2529%2520and%2520plays%250Aa%2520critical%2520role%2520in%2520major%2520experiments.%2520In%2520this%2520study%252C%2520we%2520delve%2520into%2520unexplored%250Aavenues%2520for%2520particle%2520track%2520reconstruction%2520and%2520hit%2520clustering.%2520Firstly%252C%2520we%250Aenhance%2520the%2520algorithmic%2520design%2520effort%2520by%2520utilising%2520a%2520simplified%2520simulator%250A%2528REDVID%2529%2520to%2520generate%2520training%2520data%2520that%2520is%2520specifically%2520composed%2520for%250Asimplicity.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520data%2520in%2520guiding%2520the%250Adevelopment%2520of%2520optimal%2520network%2520architectures.%2520Additionally%252C%2520we%2520investigate%2520the%250Aapplication%2520of%2520image%2520segmentation%2520networks%2520for%2520this%2520task%252C%2520exploring%2520their%250Apotential%2520for%2520accurate%2520track%2520reconstruction.%2520Moreover%252C%2520we%2520approach%2520the%2520task%250Afrom%2520a%2520different%2520perspective%2520by%2520treating%2520it%2520as%2520a%2520hit%2520sequence%2520to%2520track%2520sequence%250Atranslation%2520problem.%2520Specifically%252C%2520we%2520explore%2520the%2520utilisation%2520of%2520Transformer%250Aarchitectures%2520for%2520tracking%2520purposes.%2520Our%2520preliminary%2520findings%2520are%2520covered%2520in%250Adetail.%2520By%2520considering%2520this%2520novel%2520approach%252C%2520we%2520aim%2520to%2520uncover%2520new%2520insights%2520and%250Apotential%2520advancements%2520in%2520track%2520reconstruction.%2520This%2520research%2520sheds%2520light%2520on%250Apreviously%2520unexplored%2520methods%2520and%2520provides%2520valuable%2520insights%2520for%2520the%2520field%2520of%250Aparticle%2520track%2520reconstruction%2520and%2520hit%2520clustering%2520in%2520HEP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Approaches%20for%20ML-Assisted%20Particle%20Track%20Reconstruction%20and%20Hit%0A%20%20Clustering&entry.906535625=Uraz%20Odyurt%20and%20Nadezhda%20Dobreva%20and%20Zef%20Wolffs%20and%20Yue%20Zhao%20and%20Antonio%20Ferrer%20S%C3%A1nchez%20and%20Roberto%20Ruiz%20de%20Austri%20Bazan%20and%20Jos%C3%A9%20D.%20Mart%C3%ADn-Guerrero%20and%20Ana-Lucia%20Varbanescu%20and%20Sascha%20Caron&entry.1292438233=%20%20Track%20reconstruction%20is%20a%20vital%20aspect%20of%20High-Energy%20Physics%20%28HEP%29%20and%20plays%0Aa%20critical%20role%20in%20major%20experiments.%20In%20this%20study%2C%20we%20delve%20into%20unexplored%0Aavenues%20for%20particle%20track%20reconstruction%20and%20hit%20clustering.%20Firstly%2C%20we%0Aenhance%20the%20algorithmic%20design%20effort%20by%20utilising%20a%20simplified%20simulator%0A%28REDVID%29%20to%20generate%20training%20data%20that%20is%20specifically%20composed%20for%0Asimplicity.%20We%20demonstrate%20the%20effectiveness%20of%20this%20data%20in%20guiding%20the%0Adevelopment%20of%20optimal%20network%20architectures.%20Additionally%2C%20we%20investigate%20the%0Aapplication%20of%20image%20segmentation%20networks%20for%20this%20task%2C%20exploring%20their%0Apotential%20for%20accurate%20track%20reconstruction.%20Moreover%2C%20we%20approach%20the%20task%0Afrom%20a%20different%20perspective%20by%20treating%20it%20as%20a%20hit%20sequence%20to%20track%20sequence%0Atranslation%20problem.%20Specifically%2C%20we%20explore%20the%20utilisation%20of%20Transformer%0Aarchitectures%20for%20tracking%20purposes.%20Our%20preliminary%20findings%20are%20covered%20in%0Adetail.%20By%20considering%20this%20novel%20approach%2C%20we%20aim%20to%20uncover%20new%20insights%20and%0Apotential%20advancements%20in%20track%20reconstruction.%20This%20research%20sheds%20light%20on%0Apreviously%20unexplored%20methods%20and%20provides%20valuable%20insights%20for%20the%20field%20of%0Aparticle%20track%20reconstruction%20and%20hit%20clustering%20in%20HEP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17325v1&entry.124074799=Read"},
{"title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models", "author": "Junhao Zheng and Shengjie Qiu and Qianli Ma", "abstract": "  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.\n", "link": "http://arxiv.org/abs/2312.07887v4", "date": "2024-05-27", "relevancy": 2.3825, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4818}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models&body=Title%3A%20Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models%0AAuthor%3A%20Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Qianli%20Ma%0AAbstract%3A%20%20%20Incremental%20Learning%20%28IL%29%20has%20been%20a%20long-standing%20problem%20in%20both%20vision%20and%0ANatural%20Language%20Processing%20%28NLP%29%20communities.%20In%20recent%20years%2C%20as%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%20have%20achieved%20remarkable%20progress%20in%20various%20NLP%0Adownstream%20tasks%2C%20utilizing%20PLMs%20as%20backbones%20has%20become%20a%20common%20practice%20in%0Arecent%20research%20of%20IL%20in%20NLP.%20Most%20assume%20that%20catastrophic%20forgetting%20is%20the%0Abiggest%20obstacle%20to%20achieving%20superior%20IL%20performance%20and%20propose%20various%0Atechniques%20to%20overcome%20this%20issue.%20However%2C%20we%20find%20that%20this%20assumption%20is%0Aproblematic.%20Specifically%2C%20we%20revisit%20more%20than%2020%20methods%20on%20four%0Aclassification%20tasks%20%28Text%20Classification%2C%20Intent%20Classification%2C%20Relation%0AExtraction%2C%20and%20Named%20Entity%20Recognition%29%20under%20the%20two%20most%20popular%20IL%0Asettings%20%28Class-Incremental%20and%20Task-Incremental%29%20and%20reveal%20that%20most%20of%20them%0Aseverely%20underestimate%20the%20inherent%20anti-forgetting%20ability%20of%20PLMs.%20Based%20on%0Athe%20observation%2C%20we%20propose%20a%20frustratingly%20easy%20method%20called%20SEQ%2A%20for%20IL%20with%0APLMs.%20The%20results%20show%20that%20SEQ%2A%20has%20competitive%20or%20superior%20performance%0Acompared%20to%20state-of-the-art%20%28SOTA%29%20IL%20methods%20and%20requires%20considerably%20less%0Atrainable%20parameters%20and%20training%20time.%20These%20findings%20urge%20us%20to%20revisit%20the%0AIL%20with%20PLMs%20and%20encourage%20future%20studies%20to%20have%20a%20fundamental%20understanding%0Aof%20the%20catastrophic%20forgetting%20in%20PLMs.%20The%20data%2C%20code%20and%20scripts%20are%20publicly%0Aavailable%20at%0Ahttps%3A//github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07887v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520or%2520Recall%253F%2520Revisiting%2520Incremental%2520Learning%2520with%2520Pre-trained%250A%2520%2520Language%2520Models%26entry.906535625%3DJunhao%2520Zheng%2520and%2520Shengjie%2520Qiu%2520and%2520Qianli%2520Ma%26entry.1292438233%3D%2520%2520Incremental%2520Learning%2520%2528IL%2529%2520has%2520been%2520a%2520long-standing%2520problem%2520in%2520both%2520vision%2520and%250ANatural%2520Language%2520Processing%2520%2528NLP%2529%2520communities.%2520In%2520recent%2520years%252C%2520as%2520Pre-trained%250ALanguage%2520Models%2520%2528PLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520various%2520NLP%250Adownstream%2520tasks%252C%2520utilizing%2520PLMs%2520as%2520backbones%2520has%2520become%2520a%2520common%2520practice%2520in%250Arecent%2520research%2520of%2520IL%2520in%2520NLP.%2520Most%2520assume%2520that%2520catastrophic%2520forgetting%2520is%2520the%250Abiggest%2520obstacle%2520to%2520achieving%2520superior%2520IL%2520performance%2520and%2520propose%2520various%250Atechniques%2520to%2520overcome%2520this%2520issue.%2520However%252C%2520we%2520find%2520that%2520this%2520assumption%2520is%250Aproblematic.%2520Specifically%252C%2520we%2520revisit%2520more%2520than%252020%2520methods%2520on%2520four%250Aclassification%2520tasks%2520%2528Text%2520Classification%252C%2520Intent%2520Classification%252C%2520Relation%250AExtraction%252C%2520and%2520Named%2520Entity%2520Recognition%2529%2520under%2520the%2520two%2520most%2520popular%2520IL%250Asettings%2520%2528Class-Incremental%2520and%2520Task-Incremental%2529%2520and%2520reveal%2520that%2520most%2520of%2520them%250Aseverely%2520underestimate%2520the%2520inherent%2520anti-forgetting%2520ability%2520of%2520PLMs.%2520Based%2520on%250Athe%2520observation%252C%2520we%2520propose%2520a%2520frustratingly%2520easy%2520method%2520called%2520SEQ%252A%2520for%2520IL%2520with%250APLMs.%2520The%2520results%2520show%2520that%2520SEQ%252A%2520has%2520competitive%2520or%2520superior%2520performance%250Acompared%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520IL%2520methods%2520and%2520requires%2520considerably%2520less%250Atrainable%2520parameters%2520and%2520training%2520time.%2520These%2520findings%2520urge%2520us%2520to%2520revisit%2520the%250AIL%2520with%2520PLMs%2520and%2520encourage%2520future%2520studies%2520to%2520have%2520a%2520fundamental%2520understanding%250Aof%2520the%2520catastrophic%2520forgetting%2520in%2520PLMs.%2520The%2520data%252C%2520code%2520and%2520scripts%2520are%2520publicly%250Aavailable%2520at%250Ahttps%253A//github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07887v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20or%20Recall%3F%20Revisiting%20Incremental%20Learning%20with%20Pre-trained%0A%20%20Language%20Models&entry.906535625=Junhao%20Zheng%20and%20Shengjie%20Qiu%20and%20Qianli%20Ma&entry.1292438233=%20%20Incremental%20Learning%20%28IL%29%20has%20been%20a%20long-standing%20problem%20in%20both%20vision%20and%0ANatural%20Language%20Processing%20%28NLP%29%20communities.%20In%20recent%20years%2C%20as%20Pre-trained%0ALanguage%20Models%20%28PLMs%29%20have%20achieved%20remarkable%20progress%20in%20various%20NLP%0Adownstream%20tasks%2C%20utilizing%20PLMs%20as%20backbones%20has%20become%20a%20common%20practice%20in%0Arecent%20research%20of%20IL%20in%20NLP.%20Most%20assume%20that%20catastrophic%20forgetting%20is%20the%0Abiggest%20obstacle%20to%20achieving%20superior%20IL%20performance%20and%20propose%20various%0Atechniques%20to%20overcome%20this%20issue.%20However%2C%20we%20find%20that%20this%20assumption%20is%0Aproblematic.%20Specifically%2C%20we%20revisit%20more%20than%2020%20methods%20on%20four%0Aclassification%20tasks%20%28Text%20Classification%2C%20Intent%20Classification%2C%20Relation%0AExtraction%2C%20and%20Named%20Entity%20Recognition%29%20under%20the%20two%20most%20popular%20IL%0Asettings%20%28Class-Incremental%20and%20Task-Incremental%29%20and%20reveal%20that%20most%20of%20them%0Aseverely%20underestimate%20the%20inherent%20anti-forgetting%20ability%20of%20PLMs.%20Based%20on%0Athe%20observation%2C%20we%20propose%20a%20frustratingly%20easy%20method%20called%20SEQ%2A%20for%20IL%20with%0APLMs.%20The%20results%20show%20that%20SEQ%2A%20has%20competitive%20or%20superior%20performance%0Acompared%20to%20state-of-the-art%20%28SOTA%29%20IL%20methods%20and%20requires%20considerably%20less%0Atrainable%20parameters%20and%20training%20time.%20These%20findings%20urge%20us%20to%20revisit%20the%0AIL%20with%20PLMs%20and%20encourage%20future%20studies%20to%20have%20a%20fundamental%20understanding%0Aof%20the%20catastrophic%20forgetting%20in%20PLMs.%20The%20data%2C%20code%20and%20scripts%20are%20publicly%0Aavailable%20at%0Ahttps%3A//github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07887v4&entry.124074799=Read"},
{"title": "Benchmarking and Improving Bird's Eye View Perception Robustness in\n  Autonomous Driving", "author": "Shaoyuan Xie and Lingdong Kong and Wenwei Zhang and Jiawei Ren and Liang Pan and Kai Chen and Ziwei Liu", "abstract": "  Recent advancements in bird's eye view (BEV) representations have shown\nremarkable promise for in-vehicle 3D perception. However, while these methods\nhave achieved impressive results on standard benchmarks, their robustness in\nvaried conditions remains insufficiently assessed. In this study, we present\nRoboBEV, an extensive benchmark suite designed to evaluate the resilience of\nBEV algorithms. This suite incorporates a diverse set of camera corruption\ntypes, each examined over three severity levels. Our benchmarks also consider\nthe impact of complete sensor failures that occur when using multi-modal\nmodels. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception\nmodels spanning tasks like detection, map segmentation, depth estimation, and\noccupancy prediction. Our analyses reveal a noticeable correlation between the\nmodel's performance on in-distribution datasets and its resilience to\nout-of-distribution challenges. Our experimental results also underline the\nefficacy of strategies like pre-training and depth-free BEV transformations in\nenhancing robustness against out-of-distribution data. Furthermore, we observe\nthat leveraging extensive temporal information significantly improves the\nmodel's robustness. Based on our observations, we design an effective\nrobustness enhancement strategy based on the CLIP model. The insights from this\nstudy pave the way for the development of future BEV models that seamlessly\ncombine accuracy with real-world robustness.\n", "link": "http://arxiv.org/abs/2405.17426v1", "date": "2024-05-27", "relevancy": 2.3756, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20and%20Improving%20Bird%27s%20Eye%20View%20Perception%20Robustness%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Benchmarking%20and%20Improving%20Bird%27s%20Eye%20View%20Perception%20Robustness%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Shaoyuan%20Xie%20and%20Lingdong%20Kong%20and%20Wenwei%20Zhang%20and%20Jiawei%20Ren%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20bird%27s%20eye%20view%20%28BEV%29%20representations%20have%20shown%0Aremarkable%20promise%20for%20in-vehicle%203D%20perception.%20However%2C%20while%20these%20methods%0Ahave%20achieved%20impressive%20results%20on%20standard%20benchmarks%2C%20their%20robustness%20in%0Avaried%20conditions%20remains%20insufficiently%20assessed.%20In%20this%20study%2C%20we%20present%0ARoboBEV%2C%20an%20extensive%20benchmark%20suite%20designed%20to%20evaluate%20the%20resilience%20of%0ABEV%20algorithms.%20This%20suite%20incorporates%20a%20diverse%20set%20of%20camera%20corruption%0Atypes%2C%20each%20examined%20over%20three%20severity%20levels.%20Our%20benchmarks%20also%20consider%0Athe%20impact%20of%20complete%20sensor%20failures%20that%20occur%20when%20using%20multi-modal%0Amodels.%20Through%20RoboBEV%2C%20we%20assess%2033%20state-of-the-art%20BEV-based%20perception%0Amodels%20spanning%20tasks%20like%20detection%2C%20map%20segmentation%2C%20depth%20estimation%2C%20and%0Aoccupancy%20prediction.%20Our%20analyses%20reveal%20a%20noticeable%20correlation%20between%20the%0Amodel%27s%20performance%20on%20in-distribution%20datasets%20and%20its%20resilience%20to%0Aout-of-distribution%20challenges.%20Our%20experimental%20results%20also%20underline%20the%0Aefficacy%20of%20strategies%20like%20pre-training%20and%20depth-free%20BEV%20transformations%20in%0Aenhancing%20robustness%20against%20out-of-distribution%20data.%20Furthermore%2C%20we%20observe%0Athat%20leveraging%20extensive%20temporal%20information%20significantly%20improves%20the%0Amodel%27s%20robustness.%20Based%20on%20our%20observations%2C%20we%20design%20an%20effective%0Arobustness%20enhancement%20strategy%20based%20on%20the%20CLIP%20model.%20The%20insights%20from%20this%0Astudy%20pave%20the%20way%20for%20the%20development%20of%20future%20BEV%20models%20that%20seamlessly%0Acombine%20accuracy%20with%20real-world%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520and%2520Improving%2520Bird%2527s%2520Eye%2520View%2520Perception%2520Robustness%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DShaoyuan%2520Xie%2520and%2520Lingdong%2520Kong%2520and%2520Wenwei%2520Zhang%2520and%2520Jiawei%2520Ren%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Chen%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%2520representations%2520have%2520shown%250Aremarkable%2520promise%2520for%2520in-vehicle%25203D%2520perception.%2520However%252C%2520while%2520these%2520methods%250Ahave%2520achieved%2520impressive%2520results%2520on%2520standard%2520benchmarks%252C%2520their%2520robustness%2520in%250Avaried%2520conditions%2520remains%2520insufficiently%2520assessed.%2520In%2520this%2520study%252C%2520we%2520present%250ARoboBEV%252C%2520an%2520extensive%2520benchmark%2520suite%2520designed%2520to%2520evaluate%2520the%2520resilience%2520of%250ABEV%2520algorithms.%2520This%2520suite%2520incorporates%2520a%2520diverse%2520set%2520of%2520camera%2520corruption%250Atypes%252C%2520each%2520examined%2520over%2520three%2520severity%2520levels.%2520Our%2520benchmarks%2520also%2520consider%250Athe%2520impact%2520of%2520complete%2520sensor%2520failures%2520that%2520occur%2520when%2520using%2520multi-modal%250Amodels.%2520Through%2520RoboBEV%252C%2520we%2520assess%252033%2520state-of-the-art%2520BEV-based%2520perception%250Amodels%2520spanning%2520tasks%2520like%2520detection%252C%2520map%2520segmentation%252C%2520depth%2520estimation%252C%2520and%250Aoccupancy%2520prediction.%2520Our%2520analyses%2520reveal%2520a%2520noticeable%2520correlation%2520between%2520the%250Amodel%2527s%2520performance%2520on%2520in-distribution%2520datasets%2520and%2520its%2520resilience%2520to%250Aout-of-distribution%2520challenges.%2520Our%2520experimental%2520results%2520also%2520underline%2520the%250Aefficacy%2520of%2520strategies%2520like%2520pre-training%2520and%2520depth-free%2520BEV%2520transformations%2520in%250Aenhancing%2520robustness%2520against%2520out-of-distribution%2520data.%2520Furthermore%252C%2520we%2520observe%250Athat%2520leveraging%2520extensive%2520temporal%2520information%2520significantly%2520improves%2520the%250Amodel%2527s%2520robustness.%2520Based%2520on%2520our%2520observations%252C%2520we%2520design%2520an%2520effective%250Arobustness%2520enhancement%2520strategy%2520based%2520on%2520the%2520CLIP%2520model.%2520The%2520insights%2520from%2520this%250Astudy%2520pave%2520the%2520way%2520for%2520the%2520development%2520of%2520future%2520BEV%2520models%2520that%2520seamlessly%250Acombine%2520accuracy%2520with%2520real-world%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20and%20Improving%20Bird%27s%20Eye%20View%20Perception%20Robustness%20in%0A%20%20Autonomous%20Driving&entry.906535625=Shaoyuan%20Xie%20and%20Lingdong%20Kong%20and%20Wenwei%20Zhang%20and%20Jiawei%20Ren%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20bird%27s%20eye%20view%20%28BEV%29%20representations%20have%20shown%0Aremarkable%20promise%20for%20in-vehicle%203D%20perception.%20However%2C%20while%20these%20methods%0Ahave%20achieved%20impressive%20results%20on%20standard%20benchmarks%2C%20their%20robustness%20in%0Avaried%20conditions%20remains%20insufficiently%20assessed.%20In%20this%20study%2C%20we%20present%0ARoboBEV%2C%20an%20extensive%20benchmark%20suite%20designed%20to%20evaluate%20the%20resilience%20of%0ABEV%20algorithms.%20This%20suite%20incorporates%20a%20diverse%20set%20of%20camera%20corruption%0Atypes%2C%20each%20examined%20over%20three%20severity%20levels.%20Our%20benchmarks%20also%20consider%0Athe%20impact%20of%20complete%20sensor%20failures%20that%20occur%20when%20using%20multi-modal%0Amodels.%20Through%20RoboBEV%2C%20we%20assess%2033%20state-of-the-art%20BEV-based%20perception%0Amodels%20spanning%20tasks%20like%20detection%2C%20map%20segmentation%2C%20depth%20estimation%2C%20and%0Aoccupancy%20prediction.%20Our%20analyses%20reveal%20a%20noticeable%20correlation%20between%20the%0Amodel%27s%20performance%20on%20in-distribution%20datasets%20and%20its%20resilience%20to%0Aout-of-distribution%20challenges.%20Our%20experimental%20results%20also%20underline%20the%0Aefficacy%20of%20strategies%20like%20pre-training%20and%20depth-free%20BEV%20transformations%20in%0Aenhancing%20robustness%20against%20out-of-distribution%20data.%20Furthermore%2C%20we%20observe%0Athat%20leveraging%20extensive%20temporal%20information%20significantly%20improves%20the%0Amodel%27s%20robustness.%20Based%20on%20our%20observations%2C%20we%20design%20an%20effective%0Arobustness%20enhancement%20strategy%20based%20on%20the%20CLIP%20model.%20The%20insights%20from%20this%0Astudy%20pave%20the%20way%20for%20the%20development%20of%20future%20BEV%20models%20that%20seamlessly%0Acombine%20accuracy%20with%20real-world%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17426v1&entry.124074799=Read"},
{"title": "Reason3D: Searching and Reasoning 3D Segmentation via Large Language\n  Model", "author": "Kuan-Chih Huang and Xiangtai Li and Lu Qi and Shuicheng Yan and Ming-Hsuan Yang", "abstract": "  Recent advancements in multimodal large language models (LLMs) have shown\ntheir potential in various domains, especially concept reasoning. Despite these\ndevelopments, applications in understanding 3D environments remain limited.\nThis paper introduces Reason3D, a novel LLM designed for comprehensive 3D\nunderstanding. Reason3D takes point cloud data and text prompts as input to\nproduce textual responses and segmentation masks, facilitating advanced tasks\nlike 3D reasoning segmentation, hierarchical searching, express referring, and\nquestion answering with detailed mask outputs. Specifically, we propose a\nhierarchical mask decoder to locate small objects within expansive scenes. This\ndecoder initially generates a coarse location estimate covering the object's\ngeneral area. This foundational estimation facilitates a detailed,\ncoarse-to-fine segmentation strategy that significantly enhances the precision\nof object identification and segmentation. Experiments validate that Reason3D\nachieves remarkable results on large-scale ScanNet and Matterport3D datasets\nfor 3D express referring, 3D question answering, and 3D reasoning segmentation\ntasks. Code and models are available at:\nhttps://github.com/KuanchihHuang/Reason3D.\n", "link": "http://arxiv.org/abs/2405.17427v1", "date": "2024-05-27", "relevancy": 2.3731, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5963}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5949}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reason3D%3A%20Searching%20and%20Reasoning%203D%20Segmentation%20via%20Large%20Language%0A%20%20Model&body=Title%3A%20Reason3D%3A%20Searching%20and%20Reasoning%203D%20Segmentation%20via%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Kuan-Chih%20Huang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Shuicheng%20Yan%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20shown%0Atheir%20potential%20in%20various%20domains%2C%20especially%20concept%20reasoning.%20Despite%20these%0Adevelopments%2C%20applications%20in%20understanding%203D%20environments%20remain%20limited.%0AThis%20paper%20introduces%20Reason3D%2C%20a%20novel%20LLM%20designed%20for%20comprehensive%203D%0Aunderstanding.%20Reason3D%20takes%20point%20cloud%20data%20and%20text%20prompts%20as%20input%20to%0Aproduce%20textual%20responses%20and%20segmentation%20masks%2C%20facilitating%20advanced%20tasks%0Alike%203D%20reasoning%20segmentation%2C%20hierarchical%20searching%2C%20express%20referring%2C%20and%0Aquestion%20answering%20with%20detailed%20mask%20outputs.%20Specifically%2C%20we%20propose%20a%0Ahierarchical%20mask%20decoder%20to%20locate%20small%20objects%20within%20expansive%20scenes.%20This%0Adecoder%20initially%20generates%20a%20coarse%20location%20estimate%20covering%20the%20object%27s%0Ageneral%20area.%20This%20foundational%20estimation%20facilitates%20a%20detailed%2C%0Acoarse-to-fine%20segmentation%20strategy%20that%20significantly%20enhances%20the%20precision%0Aof%20object%20identification%20and%20segmentation.%20Experiments%20validate%20that%20Reason3D%0Aachieves%20remarkable%20results%20on%20large-scale%20ScanNet%20and%20Matterport3D%20datasets%0Afor%203D%20express%20referring%2C%203D%20question%20answering%2C%20and%203D%20reasoning%20segmentation%0Atasks.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/KuanchihHuang/Reason3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReason3D%253A%2520Searching%2520and%2520Reasoning%25203D%2520Segmentation%2520via%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DKuan-Chih%2520Huang%2520and%2520Xiangtai%2520Li%2520and%2520Lu%2520Qi%2520and%2520Shuicheng%2520Yan%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%250Atheir%2520potential%2520in%2520various%2520domains%252C%2520especially%2520concept%2520reasoning.%2520Despite%2520these%250Adevelopments%252C%2520applications%2520in%2520understanding%25203D%2520environments%2520remain%2520limited.%250AThis%2520paper%2520introduces%2520Reason3D%252C%2520a%2520novel%2520LLM%2520designed%2520for%2520comprehensive%25203D%250Aunderstanding.%2520Reason3D%2520takes%2520point%2520cloud%2520data%2520and%2520text%2520prompts%2520as%2520input%2520to%250Aproduce%2520textual%2520responses%2520and%2520segmentation%2520masks%252C%2520facilitating%2520advanced%2520tasks%250Alike%25203D%2520reasoning%2520segmentation%252C%2520hierarchical%2520searching%252C%2520express%2520referring%252C%2520and%250Aquestion%2520answering%2520with%2520detailed%2520mask%2520outputs.%2520Specifically%252C%2520we%2520propose%2520a%250Ahierarchical%2520mask%2520decoder%2520to%2520locate%2520small%2520objects%2520within%2520expansive%2520scenes.%2520This%250Adecoder%2520initially%2520generates%2520a%2520coarse%2520location%2520estimate%2520covering%2520the%2520object%2527s%250Ageneral%2520area.%2520This%2520foundational%2520estimation%2520facilitates%2520a%2520detailed%252C%250Acoarse-to-fine%2520segmentation%2520strategy%2520that%2520significantly%2520enhances%2520the%2520precision%250Aof%2520object%2520identification%2520and%2520segmentation.%2520Experiments%2520validate%2520that%2520Reason3D%250Aachieves%2520remarkable%2520results%2520on%2520large-scale%2520ScanNet%2520and%2520Matterport3D%2520datasets%250Afor%25203D%2520express%2520referring%252C%25203D%2520question%2520answering%252C%2520and%25203D%2520reasoning%2520segmentation%250Atasks.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/KuanchihHuang/Reason3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reason3D%3A%20Searching%20and%20Reasoning%203D%20Segmentation%20via%20Large%20Language%0A%20%20Model&entry.906535625=Kuan-Chih%20Huang%20and%20Xiangtai%20Li%20and%20Lu%20Qi%20and%20Shuicheng%20Yan%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28LLMs%29%20have%20shown%0Atheir%20potential%20in%20various%20domains%2C%20especially%20concept%20reasoning.%20Despite%20these%0Adevelopments%2C%20applications%20in%20understanding%203D%20environments%20remain%20limited.%0AThis%20paper%20introduces%20Reason3D%2C%20a%20novel%20LLM%20designed%20for%20comprehensive%203D%0Aunderstanding.%20Reason3D%20takes%20point%20cloud%20data%20and%20text%20prompts%20as%20input%20to%0Aproduce%20textual%20responses%20and%20segmentation%20masks%2C%20facilitating%20advanced%20tasks%0Alike%203D%20reasoning%20segmentation%2C%20hierarchical%20searching%2C%20express%20referring%2C%20and%0Aquestion%20answering%20with%20detailed%20mask%20outputs.%20Specifically%2C%20we%20propose%20a%0Ahierarchical%20mask%20decoder%20to%20locate%20small%20objects%20within%20expansive%20scenes.%20This%0Adecoder%20initially%20generates%20a%20coarse%20location%20estimate%20covering%20the%20object%27s%0Ageneral%20area.%20This%20foundational%20estimation%20facilitates%20a%20detailed%2C%0Acoarse-to-fine%20segmentation%20strategy%20that%20significantly%20enhances%20the%20precision%0Aof%20object%20identification%20and%20segmentation.%20Experiments%20validate%20that%20Reason3D%0Aachieves%20remarkable%20results%20on%20large-scale%20ScanNet%20and%20Matterport3D%20datasets%0Afor%203D%20express%20referring%2C%203D%20question%20answering%2C%20and%203D%20reasoning%20segmentation%0Atasks.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/KuanchihHuang/Reason3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17427v1&entry.124074799=Read"},
{"title": "SDL-MVS: View Space and Depth Deformable Learning Paradigm for\n  Multi-View Stereo Reconstruction in Remote Sensing", "author": "Yong-Qiang Mao and Hanbo Bi and Liangyu Xu and Kaiqiang Chen and Zhirui Wang and Xian Sun and Kun Fu", "abstract": "  Research on multi-view stereo based on remote sensing images has promoted the\ndevelopment of large-scale urban 3D reconstruction. However, remote sensing\nmulti-view image data suffers from the problems of occlusion and uneven\nbrightness between views during acquisition, which leads to the problem of\nblurred details in depth estimation. To solve the above problem, we re-examine\nthe deformable learning method in the Multi-View Stereo task and propose a\nnovel paradigm based on view Space and Depth deformable Learning (SDL-MVS),\naiming to learn deformable interactions of features in different view spaces\nand deformably model the depth ranges and intervals to enable high accurate\ndepth estimation. Specifically, to solve the problem of view noise caused by\nocclusion and uneven brightness, we propose a Progressive Space deformable\nSampling (PSS) mechanism, which performs deformable learning of sampling points\nin the 3D frustum space and the 2D image space in a progressive manner to embed\nsource features to the reference feature adaptively. To further optimize the\ndepth, we introduce Depth Hypothesis deformable Discretization (DHD), which\nachieves precise positioning of the depth prior by adaptively adjusting the\ndepth range hypothesis and performing deformable discretization of the depth\ninterval hypothesis. Finally, our SDL-MVS achieves explicit modeling of\nocclusion and uneven brightness faced in multi-view stereo through the\ndeformable learning paradigm of view space and depth, achieving accurate\nmulti-view depth estimation. Extensive experiments on LuoJia-MVS and WHU\ndatasets show that our SDL-MVS reaches state-of-the-art performance. It is\nworth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of\n98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the\npremise of three views as input.\n", "link": "http://arxiv.org/abs/2405.17140v1", "date": "2024-05-27", "relevancy": 2.3329, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5886}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDL-MVS%3A%20View%20Space%20and%20Depth%20Deformable%20Learning%20Paradigm%20for%0A%20%20Multi-View%20Stereo%20Reconstruction%20in%20Remote%20Sensing&body=Title%3A%20SDL-MVS%3A%20View%20Space%20and%20Depth%20Deformable%20Learning%20Paradigm%20for%0A%20%20Multi-View%20Stereo%20Reconstruction%20in%20Remote%20Sensing%0AAuthor%3A%20Yong-Qiang%20Mao%20and%20Hanbo%20Bi%20and%20Liangyu%20Xu%20and%20Kaiqiang%20Chen%20and%20Zhirui%20Wang%20and%20Xian%20Sun%20and%20Kun%20Fu%0AAbstract%3A%20%20%20Research%20on%20multi-view%20stereo%20based%20on%20remote%20sensing%20images%20has%20promoted%20the%0Adevelopment%20of%20large-scale%20urban%203D%20reconstruction.%20However%2C%20remote%20sensing%0Amulti-view%20image%20data%20suffers%20from%20the%20problems%20of%20occlusion%20and%20uneven%0Abrightness%20between%20views%20during%20acquisition%2C%20which%20leads%20to%20the%20problem%20of%0Ablurred%20details%20in%20depth%20estimation.%20To%20solve%20the%20above%20problem%2C%20we%20re-examine%0Athe%20deformable%20learning%20method%20in%20the%20Multi-View%20Stereo%20task%20and%20propose%20a%0Anovel%20paradigm%20based%20on%20view%20Space%20and%20Depth%20deformable%20Learning%20%28SDL-MVS%29%2C%0Aaiming%20to%20learn%20deformable%20interactions%20of%20features%20in%20different%20view%20spaces%0Aand%20deformably%20model%20the%20depth%20ranges%20and%20intervals%20to%20enable%20high%20accurate%0Adepth%20estimation.%20Specifically%2C%20to%20solve%20the%20problem%20of%20view%20noise%20caused%20by%0Aocclusion%20and%20uneven%20brightness%2C%20we%20propose%20a%20Progressive%20Space%20deformable%0ASampling%20%28PSS%29%20mechanism%2C%20which%20performs%20deformable%20learning%20of%20sampling%20points%0Ain%20the%203D%20frustum%20space%20and%20the%202D%20image%20space%20in%20a%20progressive%20manner%20to%20embed%0Asource%20features%20to%20the%20reference%20feature%20adaptively.%20To%20further%20optimize%20the%0Adepth%2C%20we%20introduce%20Depth%20Hypothesis%20deformable%20Discretization%20%28DHD%29%2C%20which%0Aachieves%20precise%20positioning%20of%20the%20depth%20prior%20by%20adaptively%20adjusting%20the%0Adepth%20range%20hypothesis%20and%20performing%20deformable%20discretization%20of%20the%20depth%0Ainterval%20hypothesis.%20Finally%2C%20our%20SDL-MVS%20achieves%20explicit%20modeling%20of%0Aocclusion%20and%20uneven%20brightness%20faced%20in%20multi-view%20stereo%20through%20the%0Adeformable%20learning%20paradigm%20of%20view%20space%20and%20depth%2C%20achieving%20accurate%0Amulti-view%20depth%20estimation.%20Extensive%20experiments%20on%20LuoJia-MVS%20and%20WHU%0Adatasets%20show%20that%20our%20SDL-MVS%20reaches%20state-of-the-art%20performance.%20It%20is%0Aworth%20noting%20that%20our%20SDL-MVS%20achieves%20an%20MAE%20error%20of%200.086%2C%20an%20accuracy%20of%0A98.9%25%20for%20%3C0.6m%2C%20and%2098.9%25%20for%20%3C3-interval%20on%20the%20LuoJia-MVS%20dataset%20under%20the%0Apremise%20of%20three%20views%20as%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDL-MVS%253A%2520View%2520Space%2520and%2520Depth%2520Deformable%2520Learning%2520Paradigm%2520for%250A%2520%2520Multi-View%2520Stereo%2520Reconstruction%2520in%2520Remote%2520Sensing%26entry.906535625%3DYong-Qiang%2520Mao%2520and%2520Hanbo%2520Bi%2520and%2520Liangyu%2520Xu%2520and%2520Kaiqiang%2520Chen%2520and%2520Zhirui%2520Wang%2520and%2520Xian%2520Sun%2520and%2520Kun%2520Fu%26entry.1292438233%3D%2520%2520Research%2520on%2520multi-view%2520stereo%2520based%2520on%2520remote%2520sensing%2520images%2520has%2520promoted%2520the%250Adevelopment%2520of%2520large-scale%2520urban%25203D%2520reconstruction.%2520However%252C%2520remote%2520sensing%250Amulti-view%2520image%2520data%2520suffers%2520from%2520the%2520problems%2520of%2520occlusion%2520and%2520uneven%250Abrightness%2520between%2520views%2520during%2520acquisition%252C%2520which%2520leads%2520to%2520the%2520problem%2520of%250Ablurred%2520details%2520in%2520depth%2520estimation.%2520To%2520solve%2520the%2520above%2520problem%252C%2520we%2520re-examine%250Athe%2520deformable%2520learning%2520method%2520in%2520the%2520Multi-View%2520Stereo%2520task%2520and%2520propose%2520a%250Anovel%2520paradigm%2520based%2520on%2520view%2520Space%2520and%2520Depth%2520deformable%2520Learning%2520%2528SDL-MVS%2529%252C%250Aaiming%2520to%2520learn%2520deformable%2520interactions%2520of%2520features%2520in%2520different%2520view%2520spaces%250Aand%2520deformably%2520model%2520the%2520depth%2520ranges%2520and%2520intervals%2520to%2520enable%2520high%2520accurate%250Adepth%2520estimation.%2520Specifically%252C%2520to%2520solve%2520the%2520problem%2520of%2520view%2520noise%2520caused%2520by%250Aocclusion%2520and%2520uneven%2520brightness%252C%2520we%2520propose%2520a%2520Progressive%2520Space%2520deformable%250ASampling%2520%2528PSS%2529%2520mechanism%252C%2520which%2520performs%2520deformable%2520learning%2520of%2520sampling%2520points%250Ain%2520the%25203D%2520frustum%2520space%2520and%2520the%25202D%2520image%2520space%2520in%2520a%2520progressive%2520manner%2520to%2520embed%250Asource%2520features%2520to%2520the%2520reference%2520feature%2520adaptively.%2520To%2520further%2520optimize%2520the%250Adepth%252C%2520we%2520introduce%2520Depth%2520Hypothesis%2520deformable%2520Discretization%2520%2528DHD%2529%252C%2520which%250Aachieves%2520precise%2520positioning%2520of%2520the%2520depth%2520prior%2520by%2520adaptively%2520adjusting%2520the%250Adepth%2520range%2520hypothesis%2520and%2520performing%2520deformable%2520discretization%2520of%2520the%2520depth%250Ainterval%2520hypothesis.%2520Finally%252C%2520our%2520SDL-MVS%2520achieves%2520explicit%2520modeling%2520of%250Aocclusion%2520and%2520uneven%2520brightness%2520faced%2520in%2520multi-view%2520stereo%2520through%2520the%250Adeformable%2520learning%2520paradigm%2520of%2520view%2520space%2520and%2520depth%252C%2520achieving%2520accurate%250Amulti-view%2520depth%2520estimation.%2520Extensive%2520experiments%2520on%2520LuoJia-MVS%2520and%2520WHU%250Adatasets%2520show%2520that%2520our%2520SDL-MVS%2520reaches%2520state-of-the-art%2520performance.%2520It%2520is%250Aworth%2520noting%2520that%2520our%2520SDL-MVS%2520achieves%2520an%2520MAE%2520error%2520of%25200.086%252C%2520an%2520accuracy%2520of%250A98.9%2525%2520for%2520%253C0.6m%252C%2520and%252098.9%2525%2520for%2520%253C3-interval%2520on%2520the%2520LuoJia-MVS%2520dataset%2520under%2520the%250Apremise%2520of%2520three%2520views%2520as%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDL-MVS%3A%20View%20Space%20and%20Depth%20Deformable%20Learning%20Paradigm%20for%0A%20%20Multi-View%20Stereo%20Reconstruction%20in%20Remote%20Sensing&entry.906535625=Yong-Qiang%20Mao%20and%20Hanbo%20Bi%20and%20Liangyu%20Xu%20and%20Kaiqiang%20Chen%20and%20Zhirui%20Wang%20and%20Xian%20Sun%20and%20Kun%20Fu&entry.1292438233=%20%20Research%20on%20multi-view%20stereo%20based%20on%20remote%20sensing%20images%20has%20promoted%20the%0Adevelopment%20of%20large-scale%20urban%203D%20reconstruction.%20However%2C%20remote%20sensing%0Amulti-view%20image%20data%20suffers%20from%20the%20problems%20of%20occlusion%20and%20uneven%0Abrightness%20between%20views%20during%20acquisition%2C%20which%20leads%20to%20the%20problem%20of%0Ablurred%20details%20in%20depth%20estimation.%20To%20solve%20the%20above%20problem%2C%20we%20re-examine%0Athe%20deformable%20learning%20method%20in%20the%20Multi-View%20Stereo%20task%20and%20propose%20a%0Anovel%20paradigm%20based%20on%20view%20Space%20and%20Depth%20deformable%20Learning%20%28SDL-MVS%29%2C%0Aaiming%20to%20learn%20deformable%20interactions%20of%20features%20in%20different%20view%20spaces%0Aand%20deformably%20model%20the%20depth%20ranges%20and%20intervals%20to%20enable%20high%20accurate%0Adepth%20estimation.%20Specifically%2C%20to%20solve%20the%20problem%20of%20view%20noise%20caused%20by%0Aocclusion%20and%20uneven%20brightness%2C%20we%20propose%20a%20Progressive%20Space%20deformable%0ASampling%20%28PSS%29%20mechanism%2C%20which%20performs%20deformable%20learning%20of%20sampling%20points%0Ain%20the%203D%20frustum%20space%20and%20the%202D%20image%20space%20in%20a%20progressive%20manner%20to%20embed%0Asource%20features%20to%20the%20reference%20feature%20adaptively.%20To%20further%20optimize%20the%0Adepth%2C%20we%20introduce%20Depth%20Hypothesis%20deformable%20Discretization%20%28DHD%29%2C%20which%0Aachieves%20precise%20positioning%20of%20the%20depth%20prior%20by%20adaptively%20adjusting%20the%0Adepth%20range%20hypothesis%20and%20performing%20deformable%20discretization%20of%20the%20depth%0Ainterval%20hypothesis.%20Finally%2C%20our%20SDL-MVS%20achieves%20explicit%20modeling%20of%0Aocclusion%20and%20uneven%20brightness%20faced%20in%20multi-view%20stereo%20through%20the%0Adeformable%20learning%20paradigm%20of%20view%20space%20and%20depth%2C%20achieving%20accurate%0Amulti-view%20depth%20estimation.%20Extensive%20experiments%20on%20LuoJia-MVS%20and%20WHU%0Adatasets%20show%20that%20our%20SDL-MVS%20reaches%20state-of-the-art%20performance.%20It%20is%0Aworth%20noting%20that%20our%20SDL-MVS%20achieves%20an%20MAE%20error%20of%200.086%2C%20an%20accuracy%20of%0A98.9%25%20for%20%3C0.6m%2C%20and%2098.9%25%20for%20%3C3-interval%20on%20the%20LuoJia-MVS%20dataset%20under%20the%0Apremise%20of%20three%20views%20as%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17140v1&entry.124074799=Read"},
{"title": "Federated Neuro-Symbolic Learning", "author": "Pengwei Xing and Songtao Lu and Han Yu", "abstract": "  Neuro-symbolic learning (NSL) models complex symbolic rule patterns into\nlatent variable distributions by neural networks, which reduces rule search\nspace and generates unseen rules to improve downstream task performance.\nCentralized NSL learning involves directly acquiring data from downstream\ntasks, which is not feasible for federated learning (FL). To address this\nlimitation, we shift the focus from such a one-to-one interactive\nneuro-symbolic paradigm to one-to-many Federated Neuro-Symbolic Learning\nframework (FedNSL) with latent variables as the FL communication medium. Built\non the basis of our novel reformulation of the NSL theory, FedNSL is capable of\nidentifying and addressing rule distribution heterogeneity through a simple and\neffective Kullback-Leibler (KL) divergence constraint on rule distribution\napplicable under the FL setting. It further theoretically adjusts variational\nexpectation maximization (V-EM) to reduce the rule search space across domains.\nThis is the first incorporation of distribution-coupled bilevel optimization\ninto FL. Extensive experiments based on both synthetic and real-world data\ndemonstrate significant advantages of FedNSL compared to five state-of-the-art\nmethods. It outperforms the best baseline by 17% and 29% in terms of unbalanced\naverage training accuracy and unseen average testing accuracy, respectively.\n", "link": "http://arxiv.org/abs/2308.15324v2", "date": "2024-05-27", "relevancy": 2.3224, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Neuro-Symbolic%20Learning&body=Title%3A%20Federated%20Neuro-Symbolic%20Learning%0AAuthor%3A%20Pengwei%20Xing%20and%20Songtao%20Lu%20and%20Han%20Yu%0AAbstract%3A%20%20%20Neuro-symbolic%20learning%20%28NSL%29%20models%20complex%20symbolic%20rule%20patterns%20into%0Alatent%20variable%20distributions%20by%20neural%20networks%2C%20which%20reduces%20rule%20search%0Aspace%20and%20generates%20unseen%20rules%20to%20improve%20downstream%20task%20performance.%0ACentralized%20NSL%20learning%20involves%20directly%20acquiring%20data%20from%20downstream%0Atasks%2C%20which%20is%20not%20feasible%20for%20federated%20learning%20%28FL%29.%20To%20address%20this%0Alimitation%2C%20we%20shift%20the%20focus%20from%20such%20a%20one-to-one%20interactive%0Aneuro-symbolic%20paradigm%20to%20one-to-many%20Federated%20Neuro-Symbolic%20Learning%0Aframework%20%28FedNSL%29%20with%20latent%20variables%20as%20the%20FL%20communication%20medium.%20Built%0Aon%20the%20basis%20of%20our%20novel%20reformulation%20of%20the%20NSL%20theory%2C%20FedNSL%20is%20capable%20of%0Aidentifying%20and%20addressing%20rule%20distribution%20heterogeneity%20through%20a%20simple%20and%0Aeffective%20Kullback-Leibler%20%28KL%29%20divergence%20constraint%20on%20rule%20distribution%0Aapplicable%20under%20the%20FL%20setting.%20It%20further%20theoretically%20adjusts%20variational%0Aexpectation%20maximization%20%28V-EM%29%20to%20reduce%20the%20rule%20search%20space%20across%20domains.%0AThis%20is%20the%20first%20incorporation%20of%20distribution-coupled%20bilevel%20optimization%0Ainto%20FL.%20Extensive%20experiments%20based%20on%20both%20synthetic%20and%20real-world%20data%0Ademonstrate%20significant%20advantages%20of%20FedNSL%20compared%20to%20five%20state-of-the-art%0Amethods.%20It%20outperforms%20the%20best%20baseline%20by%2017%25%20and%2029%25%20in%20terms%20of%20unbalanced%0Aaverage%20training%20accuracy%20and%20unseen%20average%20testing%20accuracy%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Neuro-Symbolic%2520Learning%26entry.906535625%3DPengwei%2520Xing%2520and%2520Songtao%2520Lu%2520and%2520Han%2520Yu%26entry.1292438233%3D%2520%2520Neuro-symbolic%2520learning%2520%2528NSL%2529%2520models%2520complex%2520symbolic%2520rule%2520patterns%2520into%250Alatent%2520variable%2520distributions%2520by%2520neural%2520networks%252C%2520which%2520reduces%2520rule%2520search%250Aspace%2520and%2520generates%2520unseen%2520rules%2520to%2520improve%2520downstream%2520task%2520performance.%250ACentralized%2520NSL%2520learning%2520involves%2520directly%2520acquiring%2520data%2520from%2520downstream%250Atasks%252C%2520which%2520is%2520not%2520feasible%2520for%2520federated%2520learning%2520%2528FL%2529.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520shift%2520the%2520focus%2520from%2520such%2520a%2520one-to-one%2520interactive%250Aneuro-symbolic%2520paradigm%2520to%2520one-to-many%2520Federated%2520Neuro-Symbolic%2520Learning%250Aframework%2520%2528FedNSL%2529%2520with%2520latent%2520variables%2520as%2520the%2520FL%2520communication%2520medium.%2520Built%250Aon%2520the%2520basis%2520of%2520our%2520novel%2520reformulation%2520of%2520the%2520NSL%2520theory%252C%2520FedNSL%2520is%2520capable%2520of%250Aidentifying%2520and%2520addressing%2520rule%2520distribution%2520heterogeneity%2520through%2520a%2520simple%2520and%250Aeffective%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%2520constraint%2520on%2520rule%2520distribution%250Aapplicable%2520under%2520the%2520FL%2520setting.%2520It%2520further%2520theoretically%2520adjusts%2520variational%250Aexpectation%2520maximization%2520%2528V-EM%2529%2520to%2520reduce%2520the%2520rule%2520search%2520space%2520across%2520domains.%250AThis%2520is%2520the%2520first%2520incorporation%2520of%2520distribution-coupled%2520bilevel%2520optimization%250Ainto%2520FL.%2520Extensive%2520experiments%2520based%2520on%2520both%2520synthetic%2520and%2520real-world%2520data%250Ademonstrate%2520significant%2520advantages%2520of%2520FedNSL%2520compared%2520to%2520five%2520state-of-the-art%250Amethods.%2520It%2520outperforms%2520the%2520best%2520baseline%2520by%252017%2525%2520and%252029%2525%2520in%2520terms%2520of%2520unbalanced%250Aaverage%2520training%2520accuracy%2520and%2520unseen%2520average%2520testing%2520accuracy%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Neuro-Symbolic%20Learning&entry.906535625=Pengwei%20Xing%20and%20Songtao%20Lu%20and%20Han%20Yu&entry.1292438233=%20%20Neuro-symbolic%20learning%20%28NSL%29%20models%20complex%20symbolic%20rule%20patterns%20into%0Alatent%20variable%20distributions%20by%20neural%20networks%2C%20which%20reduces%20rule%20search%0Aspace%20and%20generates%20unseen%20rules%20to%20improve%20downstream%20task%20performance.%0ACentralized%20NSL%20learning%20involves%20directly%20acquiring%20data%20from%20downstream%0Atasks%2C%20which%20is%20not%20feasible%20for%20federated%20learning%20%28FL%29.%20To%20address%20this%0Alimitation%2C%20we%20shift%20the%20focus%20from%20such%20a%20one-to-one%20interactive%0Aneuro-symbolic%20paradigm%20to%20one-to-many%20Federated%20Neuro-Symbolic%20Learning%0Aframework%20%28FedNSL%29%20with%20latent%20variables%20as%20the%20FL%20communication%20medium.%20Built%0Aon%20the%20basis%20of%20our%20novel%20reformulation%20of%20the%20NSL%20theory%2C%20FedNSL%20is%20capable%20of%0Aidentifying%20and%20addressing%20rule%20distribution%20heterogeneity%20through%20a%20simple%20and%0Aeffective%20Kullback-Leibler%20%28KL%29%20divergence%20constraint%20on%20rule%20distribution%0Aapplicable%20under%20the%20FL%20setting.%20It%20further%20theoretically%20adjusts%20variational%0Aexpectation%20maximization%20%28V-EM%29%20to%20reduce%20the%20rule%20search%20space%20across%20domains.%0AThis%20is%20the%20first%20incorporation%20of%20distribution-coupled%20bilevel%20optimization%0Ainto%20FL.%20Extensive%20experiments%20based%20on%20both%20synthetic%20and%20real-world%20data%0Ademonstrate%20significant%20advantages%20of%20FedNSL%20compared%20to%20five%20state-of-the-art%0Amethods.%20It%20outperforms%20the%20best%20baseline%20by%2017%25%20and%2029%25%20in%20terms%20of%20unbalanced%0Aaverage%20training%20accuracy%20and%20unseen%20average%20testing%20accuracy%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15324v2&entry.124074799=Read"},
{"title": "Benchmarking General Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) capabilities is becoming increasingly appealing\ntowards building general intelligence. Taking this concept one step further, we\ndraw a parallel to humans and many animals, who inherit primarily learning\ncapabilities but refine their memory and acquire diverse skills and knowledge\nthrough extensive lifelong experiences. This parallel inspires our approach to\ngeneral purpose in-context learning (GPICL). This paper introduces two\nlightweight but insightful benchmarks specifically crafted to train and\nevaluate GPICL functionalities. Each benchmark encompasses a wide range of\ndiverse tasks characterized by generation and interaction, minimal transferable\nknowledge, and long-term dependency. These features present significant\nchallenges for models that primarily rely on context or interactions to enhance\ntheir proficiency. We hope that these benchmarks will not only advance research\nin GPICL but also contribute significantly to the broader field of general\nintelligence.\n", "link": "http://arxiv.org/abs/2405.17234v1", "date": "2024-05-27", "relevancy": 2.3122, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4991}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4491}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General%20Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General%20Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20capabilities%20is%20becoming%20increasingly%20appealing%0Atowards%20building%20general%20intelligence.%20Taking%20this%20concept%20one%20step%20further%2C%20we%0Adraw%20a%20parallel%20to%20humans%20and%20many%20animals%2C%20who%20inherit%20primarily%20learning%0Acapabilities%20but%20refine%20their%20memory%20and%20acquire%20diverse%20skills%20and%20knowledge%0Athrough%20extensive%20lifelong%20experiences.%20This%20parallel%20inspires%20our%20approach%20to%0Ageneral%20purpose%20in-context%20learning%20%28GPICL%29.%20This%20paper%20introduces%20two%0Alightweight%20but%20insightful%20benchmarks%20specifically%20crafted%20to%20train%20and%0Aevaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20wide%20range%20of%0Adiverse%20tasks%20characterized%20by%20generation%20and%20interaction%2C%20minimal%20transferable%0Aknowledge%2C%20and%20long-term%20dependency.%20These%20features%20present%20significant%0Achallenges%20for%20models%20that%20primarily%20rely%20on%20context%20or%20interactions%20to%20enhance%0Atheir%20proficiency.%20We%20hope%20that%20these%20benchmarks%20will%20not%20only%20advance%20research%0Ain%20GPICL%20but%20also%20contribute%20significantly%20to%20the%20broader%20field%20of%20general%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General%2520Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520capabilities%2520is%2520becoming%2520increasingly%2520appealing%250Atowards%2520building%2520general%2520intelligence.%2520Taking%2520this%2520concept%2520one%2520step%2520further%252C%2520we%250Adraw%2520a%2520parallel%2520to%2520humans%2520and%2520many%2520animals%252C%2520who%2520inherit%2520primarily%2520learning%250Acapabilities%2520but%2520refine%2520their%2520memory%2520and%2520acquire%2520diverse%2520skills%2520and%2520knowledge%250Athrough%2520extensive%2520lifelong%2520experiences.%2520This%2520parallel%2520inspires%2520our%2520approach%2520to%250Ageneral%2520purpose%2520in-context%2520learning%2520%2528GPICL%2529.%2520This%2520paper%2520introduces%2520two%250Alightweight%2520but%2520insightful%2520benchmarks%2520specifically%2520crafted%2520to%2520train%2520and%250Aevaluate%2520GPICL%2520functionalities.%2520Each%2520benchmark%2520encompasses%2520a%2520wide%2520range%2520of%250Adiverse%2520tasks%2520characterized%2520by%2520generation%2520and%2520interaction%252C%2520minimal%2520transferable%250Aknowledge%252C%2520and%2520long-term%2520dependency.%2520These%2520features%2520present%2520significant%250Achallenges%2520for%2520models%2520that%2520primarily%2520rely%2520on%2520context%2520or%2520interactions%2520to%2520enhance%250Atheir%2520proficiency.%2520We%2520hope%2520that%2520these%2520benchmarks%2520will%2520not%2520only%2520advance%2520research%250Ain%2520GPICL%2520but%2520also%2520contribute%2520significantly%2520to%2520the%2520broader%2520field%2520of%2520general%250Aintelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General%20Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20capabilities%20is%20becoming%20increasingly%20appealing%0Atowards%20building%20general%20intelligence.%20Taking%20this%20concept%20one%20step%20further%2C%20we%0Adraw%20a%20parallel%20to%20humans%20and%20many%20animals%2C%20who%20inherit%20primarily%20learning%0Acapabilities%20but%20refine%20their%20memory%20and%20acquire%20diverse%20skills%20and%20knowledge%0Athrough%20extensive%20lifelong%20experiences.%20This%20parallel%20inspires%20our%20approach%20to%0Ageneral%20purpose%20in-context%20learning%20%28GPICL%29.%20This%20paper%20introduces%20two%0Alightweight%20but%20insightful%20benchmarks%20specifically%20crafted%20to%20train%20and%0Aevaluate%20GPICL%20functionalities.%20Each%20benchmark%20encompasses%20a%20wide%20range%20of%0Adiverse%20tasks%20characterized%20by%20generation%20and%20interaction%2C%20minimal%20transferable%0Aknowledge%2C%20and%20long-term%20dependency.%20These%20features%20present%20significant%0Achallenges%20for%20models%20that%20primarily%20rely%20on%20context%20or%20interactions%20to%20enhance%0Atheir%20proficiency.%20We%20hope%20that%20these%20benchmarks%20will%20not%20only%20advance%20research%0Ain%20GPICL%20but%20also%20contribute%20significantly%20to%20the%20broader%20field%20of%20general%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v1&entry.124074799=Read"},
{"title": "Partitioned Hankel-based Diffusion Models for Few-shot Low-dose CT\n  Reconstruction", "author": "Wenhao Zhang and Bin Huang and Shuyue Chen and Xiaoling Xu and Weiwen Wu and Qiegen Liu", "abstract": "  Low-dose computed tomography (LDCT) plays a vital role in clinical\napplications by mitigating radiation risks. Nevertheless, reducing radiation\ndoses significantly degrades image quality. Concurrently, common deep learning\nmethods demand extensive data, posing concerns about privacy, cost, and time\nconstraints. Consequently, we propose a few-shot low-dose CT reconstruction\nmethod using Partitioned Hankel-based Diffusion (PHD) models. During the prior\nlearning stage, the projection data is first transformed into multiple\npartitioned Hankel matrices. Structured tensors are then extracted from these\nmatrices to facilitate prior learning through multiple diffusion models. In the\niterative reconstruction stage, an iterative stochastic differential equation\nsolver is employed along with data consistency constraints to update the\nacquired projection data. Furthermore, penalized weighted least-squares and\ntotal variation techniques are introduced to enhance the resulting image\nquality. The results approximate those of normal-dose counterparts, validating\nPHD model as an effective and practical model for reducing artifacts and noise\nwhile preserving image quality.\n", "link": "http://arxiv.org/abs/2405.17167v1", "date": "2024-05-27", "relevancy": 2.3093, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6113}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5813}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partitioned%20Hankel-based%20Diffusion%20Models%20for%20Few-shot%20Low-dose%20CT%0A%20%20Reconstruction&body=Title%3A%20Partitioned%20Hankel-based%20Diffusion%20Models%20for%20Few-shot%20Low-dose%20CT%0A%20%20Reconstruction%0AAuthor%3A%20Wenhao%20Zhang%20and%20Bin%20Huang%20and%20Shuyue%20Chen%20and%20Xiaoling%20Xu%20and%20Weiwen%20Wu%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20plays%20a%20vital%20role%20in%20clinical%0Aapplications%20by%20mitigating%20radiation%20risks.%20Nevertheless%2C%20reducing%20radiation%0Adoses%20significantly%20degrades%20image%20quality.%20Concurrently%2C%20common%20deep%20learning%0Amethods%20demand%20extensive%20data%2C%20posing%20concerns%20about%20privacy%2C%20cost%2C%20and%20time%0Aconstraints.%20Consequently%2C%20we%20propose%20a%20few-shot%20low-dose%20CT%20reconstruction%0Amethod%20using%20Partitioned%20Hankel-based%20Diffusion%20%28PHD%29%20models.%20During%20the%20prior%0Alearning%20stage%2C%20the%20projection%20data%20is%20first%20transformed%20into%20multiple%0Apartitioned%20Hankel%20matrices.%20Structured%20tensors%20are%20then%20extracted%20from%20these%0Amatrices%20to%20facilitate%20prior%20learning%20through%20multiple%20diffusion%20models.%20In%20the%0Aiterative%20reconstruction%20stage%2C%20an%20iterative%20stochastic%20differential%20equation%0Asolver%20is%20employed%20along%20with%20data%20consistency%20constraints%20to%20update%20the%0Aacquired%20projection%20data.%20Furthermore%2C%20penalized%20weighted%20least-squares%20and%0Atotal%20variation%20techniques%20are%20introduced%20to%20enhance%20the%20resulting%20image%0Aquality.%20The%20results%20approximate%20those%20of%20normal-dose%20counterparts%2C%20validating%0APHD%20model%20as%20an%20effective%20and%20practical%20model%20for%20reducing%20artifacts%20and%20noise%0Awhile%20preserving%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartitioned%2520Hankel-based%2520Diffusion%2520Models%2520for%2520Few-shot%2520Low-dose%2520CT%250A%2520%2520Reconstruction%26entry.906535625%3DWenhao%2520Zhang%2520and%2520Bin%2520Huang%2520and%2520Shuyue%2520Chen%2520and%2520Xiaoling%2520Xu%2520and%2520Weiwen%2520Wu%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520plays%2520a%2520vital%2520role%2520in%2520clinical%250Aapplications%2520by%2520mitigating%2520radiation%2520risks.%2520Nevertheless%252C%2520reducing%2520radiation%250Adoses%2520significantly%2520degrades%2520image%2520quality.%2520Concurrently%252C%2520common%2520deep%2520learning%250Amethods%2520demand%2520extensive%2520data%252C%2520posing%2520concerns%2520about%2520privacy%252C%2520cost%252C%2520and%2520time%250Aconstraints.%2520Consequently%252C%2520we%2520propose%2520a%2520few-shot%2520low-dose%2520CT%2520reconstruction%250Amethod%2520using%2520Partitioned%2520Hankel-based%2520Diffusion%2520%2528PHD%2529%2520models.%2520During%2520the%2520prior%250Alearning%2520stage%252C%2520the%2520projection%2520data%2520is%2520first%2520transformed%2520into%2520multiple%250Apartitioned%2520Hankel%2520matrices.%2520Structured%2520tensors%2520are%2520then%2520extracted%2520from%2520these%250Amatrices%2520to%2520facilitate%2520prior%2520learning%2520through%2520multiple%2520diffusion%2520models.%2520In%2520the%250Aiterative%2520reconstruction%2520stage%252C%2520an%2520iterative%2520stochastic%2520differential%2520equation%250Asolver%2520is%2520employed%2520along%2520with%2520data%2520consistency%2520constraints%2520to%2520update%2520the%250Aacquired%2520projection%2520data.%2520Furthermore%252C%2520penalized%2520weighted%2520least-squares%2520and%250Atotal%2520variation%2520techniques%2520are%2520introduced%2520to%2520enhance%2520the%2520resulting%2520image%250Aquality.%2520The%2520results%2520approximate%2520those%2520of%2520normal-dose%2520counterparts%252C%2520validating%250APHD%2520model%2520as%2520an%2520effective%2520and%2520practical%2520model%2520for%2520reducing%2520artifacts%2520and%2520noise%250Awhile%2520preserving%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partitioned%20Hankel-based%20Diffusion%20Models%20for%20Few-shot%20Low-dose%20CT%0A%20%20Reconstruction&entry.906535625=Wenhao%20Zhang%20and%20Bin%20Huang%20and%20Shuyue%20Chen%20and%20Xiaoling%20Xu%20and%20Weiwen%20Wu%20and%20Qiegen%20Liu&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20plays%20a%20vital%20role%20in%20clinical%0Aapplications%20by%20mitigating%20radiation%20risks.%20Nevertheless%2C%20reducing%20radiation%0Adoses%20significantly%20degrades%20image%20quality.%20Concurrently%2C%20common%20deep%20learning%0Amethods%20demand%20extensive%20data%2C%20posing%20concerns%20about%20privacy%2C%20cost%2C%20and%20time%0Aconstraints.%20Consequently%2C%20we%20propose%20a%20few-shot%20low-dose%20CT%20reconstruction%0Amethod%20using%20Partitioned%20Hankel-based%20Diffusion%20%28PHD%29%20models.%20During%20the%20prior%0Alearning%20stage%2C%20the%20projection%20data%20is%20first%20transformed%20into%20multiple%0Apartitioned%20Hankel%20matrices.%20Structured%20tensors%20are%20then%20extracted%20from%20these%0Amatrices%20to%20facilitate%20prior%20learning%20through%20multiple%20diffusion%20models.%20In%20the%0Aiterative%20reconstruction%20stage%2C%20an%20iterative%20stochastic%20differential%20equation%0Asolver%20is%20employed%20along%20with%20data%20consistency%20constraints%20to%20update%20the%0Aacquired%20projection%20data.%20Furthermore%2C%20penalized%20weighted%20least-squares%20and%0Atotal%20variation%20techniques%20are%20introduced%20to%20enhance%20the%20resulting%20image%0Aquality.%20The%20results%20approximate%20those%20of%20normal-dose%20counterparts%2C%20validating%0APHD%20model%20as%20an%20effective%20and%20practical%20model%20for%20reducing%20artifacts%20and%20noise%0Awhile%20preserving%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17167v1&entry.124074799=Read"},
{"title": "Adapting LLaMA Decoder to Vision Transformer", "author": "Jiahao Wang and Wenqi Shao and Mengzhao Chen and Chengyue Wu and Yong Liu and Taiqiang Wu and Kaipeng Zhang and Songyang Zhang and Kai Chen and Ping Luo", "abstract": "  This work examines whether decoder-only Transformers such as LLaMA, which\nwere originally designed for large language models (LLMs), can be adapted to\nthe computer vision field. We first \"LLaMAfy\" a standard ViT step-by-step to\nalign with LLaMA's architecture, and find that directly applying a causal mask\nto the self-attention brings an attention collapse issue, resulting in the\nfailure to the network training. We suggest to reposition the class token\nbehind the image tokens with a post-sequence class token technique to overcome\nthis challenge, enabling causal self-attention to efficiently capture the\nentire image's information. Additionally, we develop a soft mask strategy that\ngradually introduces a causal mask to the self-attention at the onset of\ntraining to facilitate the optimization behavior. The tailored model, dubbed as\nimage LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct\nsupervised learning. Its causal self-attention boosts computational efficiency\nand learns complex representation by elevating attention map ranks. iLLaMA\nrivals the performance with its encoder-only counterparts, achieving 75.1%\nImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to\n$\\sim$310M and pre-training on ImageNet-21K further enhances the accuracy to\n86.0%. Extensive experiments demonstrate iLLaMA's reliable properties:\nshape-texture bias, calibration, quantization compatibility, ADE20K\nsegmentation and CIFAR transfer learning. We hope our study can kindle fresh\nviews to visual architectures in the wave of LLMs and inspire the development\nof unified multimodal models. Pre-trained models and codes are available\nhttps://github.com/techmonsterwang/iLLaMA.\n", "link": "http://arxiv.org/abs/2404.06773v4", "date": "2024-05-27", "relevancy": 2.3062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5633}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20LLaMA%20Decoder%20to%20Vision%20Transformer&body=Title%3A%20Adapting%20LLaMA%20Decoder%20to%20Vision%20Transformer%0AAuthor%3A%20Jiahao%20Wang%20and%20Wenqi%20Shao%20and%20Mengzhao%20Chen%20and%20Chengyue%20Wu%20and%20Yong%20Liu%20and%20Taiqiang%20Wu%20and%20Kaipeng%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen%20and%20Ping%20Luo%0AAbstract%3A%20%20%20This%20work%20examines%20whether%20decoder-only%20Transformers%20such%20as%20LLaMA%2C%20which%0Awere%20originally%20designed%20for%20large%20language%20models%20%28LLMs%29%2C%20can%20be%20adapted%20to%0Athe%20computer%20vision%20field.%20We%20first%20%22LLaMAfy%22%20a%20standard%20ViT%20step-by-step%20to%0Aalign%20with%20LLaMA%27s%20architecture%2C%20and%20find%20that%20directly%20applying%20a%20causal%20mask%0Ato%20the%20self-attention%20brings%20an%20attention%20collapse%20issue%2C%20resulting%20in%20the%0Afailure%20to%20the%20network%20training.%20We%20suggest%20to%20reposition%20the%20class%20token%0Abehind%20the%20image%20tokens%20with%20a%20post-sequence%20class%20token%20technique%20to%20overcome%0Athis%20challenge%2C%20enabling%20causal%20self-attention%20to%20efficiently%20capture%20the%0Aentire%20image%27s%20information.%20Additionally%2C%20we%20develop%20a%20soft%20mask%20strategy%20that%0Agradually%20introduces%20a%20causal%20mask%20to%20the%20self-attention%20at%20the%20onset%20of%0Atraining%20to%20facilitate%20the%20optimization%20behavior.%20The%20tailored%20model%2C%20dubbed%20as%0Aimage%20LLaMA%20%28iLLaMA%29%2C%20is%20akin%20to%20LLaMA%20in%20architecture%20and%20enables%20direct%0Asupervised%20learning.%20Its%20causal%20self-attention%20boosts%20computational%20efficiency%0Aand%20learns%20complex%20representation%20by%20elevating%20attention%20map%20ranks.%20iLLaMA%0Arivals%20the%20performance%20with%20its%20encoder-only%20counterparts%2C%20achieving%2075.1%25%0AImageNet%20top-1%20accuracy%20with%20only%205.7M%20parameters.%20Scaling%20the%20model%20to%0A%24%5Csim%24310M%20and%20pre-training%20on%20ImageNet-21K%20further%20enhances%20the%20accuracy%20to%0A86.0%25.%20Extensive%20experiments%20demonstrate%20iLLaMA%27s%20reliable%20properties%3A%0Ashape-texture%20bias%2C%20calibration%2C%20quantization%20compatibility%2C%20ADE20K%0Asegmentation%20and%20CIFAR%20transfer%20learning.%20We%20hope%20our%20study%20can%20kindle%20fresh%0Aviews%20to%20visual%20architectures%20in%20the%20wave%20of%20LLMs%20and%20inspire%20the%20development%0Aof%20unified%20multimodal%20models.%20Pre-trained%20models%20and%20codes%20are%20available%0Ahttps%3A//github.com/techmonsterwang/iLLaMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06773v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520LLaMA%2520Decoder%2520to%2520Vision%2520Transformer%26entry.906535625%3DJiahao%2520Wang%2520and%2520Wenqi%2520Shao%2520and%2520Mengzhao%2520Chen%2520and%2520Chengyue%2520Wu%2520and%2520Yong%2520Liu%2520and%2520Taiqiang%2520Wu%2520and%2520Kaipeng%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520This%2520work%2520examines%2520whether%2520decoder-only%2520Transformers%2520such%2520as%2520LLaMA%252C%2520which%250Awere%2520originally%2520designed%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520can%2520be%2520adapted%2520to%250Athe%2520computer%2520vision%2520field.%2520We%2520first%2520%2522LLaMAfy%2522%2520a%2520standard%2520ViT%2520step-by-step%2520to%250Aalign%2520with%2520LLaMA%2527s%2520architecture%252C%2520and%2520find%2520that%2520directly%2520applying%2520a%2520causal%2520mask%250Ato%2520the%2520self-attention%2520brings%2520an%2520attention%2520collapse%2520issue%252C%2520resulting%2520in%2520the%250Afailure%2520to%2520the%2520network%2520training.%2520We%2520suggest%2520to%2520reposition%2520the%2520class%2520token%250Abehind%2520the%2520image%2520tokens%2520with%2520a%2520post-sequence%2520class%2520token%2520technique%2520to%2520overcome%250Athis%2520challenge%252C%2520enabling%2520causal%2520self-attention%2520to%2520efficiently%2520capture%2520the%250Aentire%2520image%2527s%2520information.%2520Additionally%252C%2520we%2520develop%2520a%2520soft%2520mask%2520strategy%2520that%250Agradually%2520introduces%2520a%2520causal%2520mask%2520to%2520the%2520self-attention%2520at%2520the%2520onset%2520of%250Atraining%2520to%2520facilitate%2520the%2520optimization%2520behavior.%2520The%2520tailored%2520model%252C%2520dubbed%2520as%250Aimage%2520LLaMA%2520%2528iLLaMA%2529%252C%2520is%2520akin%2520to%2520LLaMA%2520in%2520architecture%2520and%2520enables%2520direct%250Asupervised%2520learning.%2520Its%2520causal%2520self-attention%2520boosts%2520computational%2520efficiency%250Aand%2520learns%2520complex%2520representation%2520by%2520elevating%2520attention%2520map%2520ranks.%2520iLLaMA%250Arivals%2520the%2520performance%2520with%2520its%2520encoder-only%2520counterparts%252C%2520achieving%252075.1%2525%250AImageNet%2520top-1%2520accuracy%2520with%2520only%25205.7M%2520parameters.%2520Scaling%2520the%2520model%2520to%250A%2524%255Csim%2524310M%2520and%2520pre-training%2520on%2520ImageNet-21K%2520further%2520enhances%2520the%2520accuracy%2520to%250A86.0%2525.%2520Extensive%2520experiments%2520demonstrate%2520iLLaMA%2527s%2520reliable%2520properties%253A%250Ashape-texture%2520bias%252C%2520calibration%252C%2520quantization%2520compatibility%252C%2520ADE20K%250Asegmentation%2520and%2520CIFAR%2520transfer%2520learning.%2520We%2520hope%2520our%2520study%2520can%2520kindle%2520fresh%250Aviews%2520to%2520visual%2520architectures%2520in%2520the%2520wave%2520of%2520LLMs%2520and%2520inspire%2520the%2520development%250Aof%2520unified%2520multimodal%2520models.%2520Pre-trained%2520models%2520and%2520codes%2520are%2520available%250Ahttps%253A//github.com/techmonsterwang/iLLaMA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06773v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20LLaMA%20Decoder%20to%20Vision%20Transformer&entry.906535625=Jiahao%20Wang%20and%20Wenqi%20Shao%20and%20Mengzhao%20Chen%20and%20Chengyue%20Wu%20and%20Yong%20Liu%20and%20Taiqiang%20Wu%20and%20Kaipeng%20Zhang%20and%20Songyang%20Zhang%20and%20Kai%20Chen%20and%20Ping%20Luo&entry.1292438233=%20%20This%20work%20examines%20whether%20decoder-only%20Transformers%20such%20as%20LLaMA%2C%20which%0Awere%20originally%20designed%20for%20large%20language%20models%20%28LLMs%29%2C%20can%20be%20adapted%20to%0Athe%20computer%20vision%20field.%20We%20first%20%22LLaMAfy%22%20a%20standard%20ViT%20step-by-step%20to%0Aalign%20with%20LLaMA%27s%20architecture%2C%20and%20find%20that%20directly%20applying%20a%20causal%20mask%0Ato%20the%20self-attention%20brings%20an%20attention%20collapse%20issue%2C%20resulting%20in%20the%0Afailure%20to%20the%20network%20training.%20We%20suggest%20to%20reposition%20the%20class%20token%0Abehind%20the%20image%20tokens%20with%20a%20post-sequence%20class%20token%20technique%20to%20overcome%0Athis%20challenge%2C%20enabling%20causal%20self-attention%20to%20efficiently%20capture%20the%0Aentire%20image%27s%20information.%20Additionally%2C%20we%20develop%20a%20soft%20mask%20strategy%20that%0Agradually%20introduces%20a%20causal%20mask%20to%20the%20self-attention%20at%20the%20onset%20of%0Atraining%20to%20facilitate%20the%20optimization%20behavior.%20The%20tailored%20model%2C%20dubbed%20as%0Aimage%20LLaMA%20%28iLLaMA%29%2C%20is%20akin%20to%20LLaMA%20in%20architecture%20and%20enables%20direct%0Asupervised%20learning.%20Its%20causal%20self-attention%20boosts%20computational%20efficiency%0Aand%20learns%20complex%20representation%20by%20elevating%20attention%20map%20ranks.%20iLLaMA%0Arivals%20the%20performance%20with%20its%20encoder-only%20counterparts%2C%20achieving%2075.1%25%0AImageNet%20top-1%20accuracy%20with%20only%205.7M%20parameters.%20Scaling%20the%20model%20to%0A%24%5Csim%24310M%20and%20pre-training%20on%20ImageNet-21K%20further%20enhances%20the%20accuracy%20to%0A86.0%25.%20Extensive%20experiments%20demonstrate%20iLLaMA%27s%20reliable%20properties%3A%0Ashape-texture%20bias%2C%20calibration%2C%20quantization%20compatibility%2C%20ADE20K%0Asegmentation%20and%20CIFAR%20transfer%20learning.%20We%20hope%20our%20study%20can%20kindle%20fresh%0Aviews%20to%20visual%20architectures%20in%20the%20wave%20of%20LLMs%20and%20inspire%20the%20development%0Aof%20unified%20multimodal%20models.%20Pre-trained%20models%20and%20codes%20are%20available%0Ahttps%3A//github.com/techmonsterwang/iLLaMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06773v4&entry.124074799=Read"},
{"title": "Spectral Greedy Coresets for Graph Neural Networks", "author": "Mucong Ding and Yinhan He and Jundong Li and Furong Huang", "abstract": "  The ubiquity of large-scale graphs in node-classification tasks significantly\nhinders the real-world applications of Graph Neural Networks (GNNs). Node\nsampling, graph coarsening, and dataset condensation are effective strategies\nfor enhancing data efficiency. However, owing to the interdependence of graph\nnodes, coreset selection, which selects subsets of the data examples, has not\nbeen successfully applied to speed up GNN training on large graphs, warranting\nspecial treatment. This paper studies graph coresets for GNNs and avoids the\ninterdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs\naround a node) based on their spectral embeddings. We decompose the coreset\nselection problem for GNNs into two phases: a coarse selection of widely spread\nego graphs and a refined selection to diversify their topologies. We design a\ngreedy algorithm that approximately optimizes both objectives. Our spectral\ngreedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates\nthe need for model pre-training, and applies to low-homophily graphs. Extensive\nexperiments on ten datasets demonstrate that SGGC outperforms other coreset\nmethods by a wide margin, generalizes well across GNN architectures, and is\nmuch faster than graph condensation.\n", "link": "http://arxiv.org/abs/2405.17404v1", "date": "2024-05-27", "relevancy": 2.3051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4538}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Greedy%20Coresets%20for%20Graph%20Neural%20Networks&body=Title%3A%20Spectral%20Greedy%20Coresets%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Mucong%20Ding%20and%20Yinhan%20He%20and%20Jundong%20Li%20and%20Furong%20Huang%0AAbstract%3A%20%20%20The%20ubiquity%20of%20large-scale%20graphs%20in%20node-classification%20tasks%20significantly%0Ahinders%20the%20real-world%20applications%20of%20Graph%20Neural%20Networks%20%28GNNs%29.%20Node%0Asampling%2C%20graph%20coarsening%2C%20and%20dataset%20condensation%20are%20effective%20strategies%0Afor%20enhancing%20data%20efficiency.%20However%2C%20owing%20to%20the%20interdependence%20of%20graph%0Anodes%2C%20coreset%20selection%2C%20which%20selects%20subsets%20of%20the%20data%20examples%2C%20has%20not%0Abeen%20successfully%20applied%20to%20speed%20up%20GNN%20training%20on%20large%20graphs%2C%20warranting%0Aspecial%20treatment.%20This%20paper%20studies%20graph%20coresets%20for%20GNNs%20and%20avoids%20the%0Ainterdependence%20issue%20by%20selecting%20ego-graphs%20%28i.e.%2C%20neighborhood%20subgraphs%0Aaround%20a%20node%29%20based%20on%20their%20spectral%20embeddings.%20We%20decompose%20the%20coreset%0Aselection%20problem%20for%20GNNs%20into%20two%20phases%3A%20a%20coarse%20selection%20of%20widely%20spread%0Aego%20graphs%20and%20a%20refined%20selection%20to%20diversify%20their%20topologies.%20We%20design%20a%0Agreedy%20algorithm%20that%20approximately%20optimizes%20both%20objectives.%20Our%20spectral%0Agreedy%20graph%20coreset%20%28SGGC%29%20scales%20to%20graphs%20with%20millions%20of%20nodes%2C%20obviates%0Athe%20need%20for%20model%20pre-training%2C%20and%20applies%20to%20low-homophily%20graphs.%20Extensive%0Aexperiments%20on%20ten%20datasets%20demonstrate%20that%20SGGC%20outperforms%20other%20coreset%0Amethods%20by%20a%20wide%20margin%2C%20generalizes%20well%20across%20GNN%20architectures%2C%20and%20is%0Amuch%20faster%20than%20graph%20condensation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Greedy%2520Coresets%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMucong%2520Ding%2520and%2520Yinhan%2520He%2520and%2520Jundong%2520Li%2520and%2520Furong%2520Huang%26entry.1292438233%3D%2520%2520The%2520ubiquity%2520of%2520large-scale%2520graphs%2520in%2520node-classification%2520tasks%2520significantly%250Ahinders%2520the%2520real-world%2520applications%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520Node%250Asampling%252C%2520graph%2520coarsening%252C%2520and%2520dataset%2520condensation%2520are%2520effective%2520strategies%250Afor%2520enhancing%2520data%2520efficiency.%2520However%252C%2520owing%2520to%2520the%2520interdependence%2520of%2520graph%250Anodes%252C%2520coreset%2520selection%252C%2520which%2520selects%2520subsets%2520of%2520the%2520data%2520examples%252C%2520has%2520not%250Abeen%2520successfully%2520applied%2520to%2520speed%2520up%2520GNN%2520training%2520on%2520large%2520graphs%252C%2520warranting%250Aspecial%2520treatment.%2520This%2520paper%2520studies%2520graph%2520coresets%2520for%2520GNNs%2520and%2520avoids%2520the%250Ainterdependence%2520issue%2520by%2520selecting%2520ego-graphs%2520%2528i.e.%252C%2520neighborhood%2520subgraphs%250Aaround%2520a%2520node%2529%2520based%2520on%2520their%2520spectral%2520embeddings.%2520We%2520decompose%2520the%2520coreset%250Aselection%2520problem%2520for%2520GNNs%2520into%2520two%2520phases%253A%2520a%2520coarse%2520selection%2520of%2520widely%2520spread%250Aego%2520graphs%2520and%2520a%2520refined%2520selection%2520to%2520diversify%2520their%2520topologies.%2520We%2520design%2520a%250Agreedy%2520algorithm%2520that%2520approximately%2520optimizes%2520both%2520objectives.%2520Our%2520spectral%250Agreedy%2520graph%2520coreset%2520%2528SGGC%2529%2520scales%2520to%2520graphs%2520with%2520millions%2520of%2520nodes%252C%2520obviates%250Athe%2520need%2520for%2520model%2520pre-training%252C%2520and%2520applies%2520to%2520low-homophily%2520graphs.%2520Extensive%250Aexperiments%2520on%2520ten%2520datasets%2520demonstrate%2520that%2520SGGC%2520outperforms%2520other%2520coreset%250Amethods%2520by%2520a%2520wide%2520margin%252C%2520generalizes%2520well%2520across%2520GNN%2520architectures%252C%2520and%2520is%250Amuch%2520faster%2520than%2520graph%2520condensation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Greedy%20Coresets%20for%20Graph%20Neural%20Networks&entry.906535625=Mucong%20Ding%20and%20Yinhan%20He%20and%20Jundong%20Li%20and%20Furong%20Huang&entry.1292438233=%20%20The%20ubiquity%20of%20large-scale%20graphs%20in%20node-classification%20tasks%20significantly%0Ahinders%20the%20real-world%20applications%20of%20Graph%20Neural%20Networks%20%28GNNs%29.%20Node%0Asampling%2C%20graph%20coarsening%2C%20and%20dataset%20condensation%20are%20effective%20strategies%0Afor%20enhancing%20data%20efficiency.%20However%2C%20owing%20to%20the%20interdependence%20of%20graph%0Anodes%2C%20coreset%20selection%2C%20which%20selects%20subsets%20of%20the%20data%20examples%2C%20has%20not%0Abeen%20successfully%20applied%20to%20speed%20up%20GNN%20training%20on%20large%20graphs%2C%20warranting%0Aspecial%20treatment.%20This%20paper%20studies%20graph%20coresets%20for%20GNNs%20and%20avoids%20the%0Ainterdependence%20issue%20by%20selecting%20ego-graphs%20%28i.e.%2C%20neighborhood%20subgraphs%0Aaround%20a%20node%29%20based%20on%20their%20spectral%20embeddings.%20We%20decompose%20the%20coreset%0Aselection%20problem%20for%20GNNs%20into%20two%20phases%3A%20a%20coarse%20selection%20of%20widely%20spread%0Aego%20graphs%20and%20a%20refined%20selection%20to%20diversify%20their%20topologies.%20We%20design%20a%0Agreedy%20algorithm%20that%20approximately%20optimizes%20both%20objectives.%20Our%20spectral%0Agreedy%20graph%20coreset%20%28SGGC%29%20scales%20to%20graphs%20with%20millions%20of%20nodes%2C%20obviates%0Athe%20need%20for%20model%20pre-training%2C%20and%20applies%20to%20low-homophily%20graphs.%20Extensive%0Aexperiments%20on%20ten%20datasets%20demonstrate%20that%20SGGC%20outperforms%20other%20coreset%0Amethods%20by%20a%20wide%20margin%2C%20generalizes%20well%20across%20GNN%20architectures%2C%20and%20is%0Amuch%20faster%20than%20graph%20condensation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17404v1&entry.124074799=Read"},
{"title": "Glauber Generative Model: Discrete Diffusion Models via Binary\n  Classification", "author": "Harshit Varma and Dheeraj Nagaraj and Karthikeyan Shanmugam", "abstract": "  We introduce the Glauber Generative Model (GGM), a new class of discrete\ndiffusion models, to obtain new samples from a distribution given samples from\na discrete space. GGM deploys a discrete Markov chain called the heat bath\ndynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens to a\nsample from a joint distribution of discrete tokens. Our novel conceptual\nframework provides an exact reduction of the task of learning the denoising\nMarkov chain to solving a class of binary classification tasks. More\nspecifically, the model learns to classify a given token in a noisy sequence as\nsignal or noise. In contrast, prior works on discrete diffusion models either\nsolve regression problems to learn importance ratios, or minimize loss\nfunctions given by variational approximations. We apply GGM to language\nmodeling and image generation, where images are discretized using image\ntokenizers like VQGANs. We show that it outperforms existing discrete diffusion\nmodels in language generation, and demonstrates strong performance for image\ngeneration without using dataset-specific image tokenizers. We also show that\nour model is capable of performing well in zero-shot control settings like text\nand image infilling.\n", "link": "http://arxiv.org/abs/2405.17035v1", "date": "2024-05-27", "relevancy": 2.2946, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification&body=Title%3A%20Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification%0AAuthor%3A%20Harshit%20Varma%20and%20Dheeraj%20Nagaraj%20and%20Karthikeyan%20Shanmugam%0AAbstract%3A%20%20%20We%20introduce%20the%20Glauber%20Generative%20Model%20%28GGM%29%2C%20a%20new%20class%20of%20discrete%0Adiffusion%20models%2C%20to%20obtain%20new%20samples%20from%20a%20distribution%20given%20samples%20from%0Aa%20discrete%20space.%20GGM%20deploys%20a%20discrete%20Markov%20chain%20called%20the%20heat%20bath%0Adynamics%20%28or%20the%20Glauber%20dynamics%29%20to%20denoise%20a%20sequence%20of%20noisy%20tokens%20to%20a%0Asample%20from%20a%20joint%20distribution%20of%20discrete%20tokens.%20Our%20novel%20conceptual%0Aframework%20provides%20an%20exact%20reduction%20of%20the%20task%20of%20learning%20the%20denoising%0AMarkov%20chain%20to%20solving%20a%20class%20of%20binary%20classification%20tasks.%20More%0Aspecifically%2C%20the%20model%20learns%20to%20classify%20a%20given%20token%20in%20a%20noisy%20sequence%20as%0Asignal%20or%20noise.%20In%20contrast%2C%20prior%20works%20on%20discrete%20diffusion%20models%20either%0Asolve%20regression%20problems%20to%20learn%20importance%20ratios%2C%20or%20minimize%20loss%0Afunctions%20given%20by%20variational%20approximations.%20We%20apply%20GGM%20to%20language%0Amodeling%20and%20image%20generation%2C%20where%20images%20are%20discretized%20using%20image%0Atokenizers%20like%20VQGANs.%20We%20show%20that%20it%20outperforms%20existing%20discrete%20diffusion%0Amodels%20in%20language%20generation%2C%20and%20demonstrates%20strong%20performance%20for%20image%0Ageneration%20without%20using%20dataset-specific%20image%20tokenizers.%20We%20also%20show%20that%0Aour%20model%20is%20capable%20of%20performing%20well%20in%20zero-shot%20control%20settings%20like%20text%0Aand%20image%20infilling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlauber%2520Generative%2520Model%253A%2520Discrete%2520Diffusion%2520Models%2520via%2520Binary%250A%2520%2520Classification%26entry.906535625%3DHarshit%2520Varma%2520and%2520Dheeraj%2520Nagaraj%2520and%2520Karthikeyan%2520Shanmugam%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Glauber%2520Generative%2520Model%2520%2528GGM%2529%252C%2520a%2520new%2520class%2520of%2520discrete%250Adiffusion%2520models%252C%2520to%2520obtain%2520new%2520samples%2520from%2520a%2520distribution%2520given%2520samples%2520from%250Aa%2520discrete%2520space.%2520GGM%2520deploys%2520a%2520discrete%2520Markov%2520chain%2520called%2520the%2520heat%2520bath%250Adynamics%2520%2528or%2520the%2520Glauber%2520dynamics%2529%2520to%2520denoise%2520a%2520sequence%2520of%2520noisy%2520tokens%2520to%2520a%250Asample%2520from%2520a%2520joint%2520distribution%2520of%2520discrete%2520tokens.%2520Our%2520novel%2520conceptual%250Aframework%2520provides%2520an%2520exact%2520reduction%2520of%2520the%2520task%2520of%2520learning%2520the%2520denoising%250AMarkov%2520chain%2520to%2520solving%2520a%2520class%2520of%2520binary%2520classification%2520tasks.%2520More%250Aspecifically%252C%2520the%2520model%2520learns%2520to%2520classify%2520a%2520given%2520token%2520in%2520a%2520noisy%2520sequence%2520as%250Asignal%2520or%2520noise.%2520In%2520contrast%252C%2520prior%2520works%2520on%2520discrete%2520diffusion%2520models%2520either%250Asolve%2520regression%2520problems%2520to%2520learn%2520importance%2520ratios%252C%2520or%2520minimize%2520loss%250Afunctions%2520given%2520by%2520variational%2520approximations.%2520We%2520apply%2520GGM%2520to%2520language%250Amodeling%2520and%2520image%2520generation%252C%2520where%2520images%2520are%2520discretized%2520using%2520image%250Atokenizers%2520like%2520VQGANs.%2520We%2520show%2520that%2520it%2520outperforms%2520existing%2520discrete%2520diffusion%250Amodels%2520in%2520language%2520generation%252C%2520and%2520demonstrates%2520strong%2520performance%2520for%2520image%250Ageneration%2520without%2520using%2520dataset-specific%2520image%2520tokenizers.%2520We%2520also%2520show%2520that%250Aour%2520model%2520is%2520capable%2520of%2520performing%2520well%2520in%2520zero-shot%2520control%2520settings%2520like%2520text%250Aand%2520image%2520infilling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glauber%20Generative%20Model%3A%20Discrete%20Diffusion%20Models%20via%20Binary%0A%20%20Classification&entry.906535625=Harshit%20Varma%20and%20Dheeraj%20Nagaraj%20and%20Karthikeyan%20Shanmugam&entry.1292438233=%20%20We%20introduce%20the%20Glauber%20Generative%20Model%20%28GGM%29%2C%20a%20new%20class%20of%20discrete%0Adiffusion%20models%2C%20to%20obtain%20new%20samples%20from%20a%20distribution%20given%20samples%20from%0Aa%20discrete%20space.%20GGM%20deploys%20a%20discrete%20Markov%20chain%20called%20the%20heat%20bath%0Adynamics%20%28or%20the%20Glauber%20dynamics%29%20to%20denoise%20a%20sequence%20of%20noisy%20tokens%20to%20a%0Asample%20from%20a%20joint%20distribution%20of%20discrete%20tokens.%20Our%20novel%20conceptual%0Aframework%20provides%20an%20exact%20reduction%20of%20the%20task%20of%20learning%20the%20denoising%0AMarkov%20chain%20to%20solving%20a%20class%20of%20binary%20classification%20tasks.%20More%0Aspecifically%2C%20the%20model%20learns%20to%20classify%20a%20given%20token%20in%20a%20noisy%20sequence%20as%0Asignal%20or%20noise.%20In%20contrast%2C%20prior%20works%20on%20discrete%20diffusion%20models%20either%0Asolve%20regression%20problems%20to%20learn%20importance%20ratios%2C%20or%20minimize%20loss%0Afunctions%20given%20by%20variational%20approximations.%20We%20apply%20GGM%20to%20language%0Amodeling%20and%20image%20generation%2C%20where%20images%20are%20discretized%20using%20image%0Atokenizers%20like%20VQGANs.%20We%20show%20that%20it%20outperforms%20existing%20discrete%20diffusion%0Amodels%20in%20language%20generation%2C%20and%20demonstrates%20strong%20performance%20for%20image%0Ageneration%20without%20using%20dataset-specific%20image%20tokenizers.%20We%20also%20show%20that%0Aour%20model%20is%20capable%20of%20performing%20well%20in%20zero-shot%20control%20settings%20like%20text%0Aand%20image%20infilling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17035v1&entry.124074799=Read"},
{"title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks", "author": "Renqiang Luo and Huafei Huang and Shuo Yu and Zhuoyang Han and Estrid He and Xiuzhen Zhang and Feng Xia", "abstract": "  Fairness-aware Graph Neural Networks (GNNs) often face a challenging\ntrade-off, where prioritizing fairness may require compromising utility. In\nthis work, we re-examine fairness through the lens of spectral graph theory,\naiming to reconcile fairness and utility within the framework of spectral graph\nlearning. We explore the correlation between sensitive features and spectrum in\nGNNs, using theoretical analysis to delineate the similarity between original\nsensitive features and those after convolution under different spectrum. Our\nanalysis reveals a reduction in the impact of similarity when the eigenvectors\nassociated with the largest magnitude eigenvalue exhibit directional\nsimilarity. Based on these theoretical insights, we propose FUGNN, a novel\nspectral graph learning approach that harmonizes the conflict between fairness\nand utility. FUGNN ensures algorithmic fairness and utility by truncating the\nspectrum and optimizing eigenvector distribution during the encoding process.\nThe fairness-aware eigenvector selection reduces the impact of convolution on\nsensitive features while concurrently minimizing the sacrifice of utility.\nFUGNN further optimizes the distribution of eigenvectors through a transformer\narchitecture. By incorporating the optimized spectrum into the graph\nconvolution network, FUGNN effectively learns node representations. Experiments\non six real-world datasets demonstrate the superiority of FUGNN over baseline\nmethods. The codes are available at https://github.com/yushuowiki/FUGNN.\n", "link": "http://arxiv.org/abs/2405.17034v1", "date": "2024-05-27", "relevancy": 2.2871, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4629}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4556}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks&body=Title%3A%20FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Shuo%20Yu%20and%20Zhuoyang%20Han%20and%20Estrid%20He%20and%20Xiuzhen%20Zhang%20and%20Feng%20Xia%0AAbstract%3A%20%20%20Fairness-aware%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20face%20a%20challenging%0Atrade-off%2C%20where%20prioritizing%20fairness%20may%20require%20compromising%20utility.%20In%0Athis%20work%2C%20we%20re-examine%20fairness%20through%20the%20lens%20of%20spectral%20graph%20theory%2C%0Aaiming%20to%20reconcile%20fairness%20and%20utility%20within%20the%20framework%20of%20spectral%20graph%0Alearning.%20We%20explore%20the%20correlation%20between%20sensitive%20features%20and%20spectrum%20in%0AGNNs%2C%20using%20theoretical%20analysis%20to%20delineate%20the%20similarity%20between%20original%0Asensitive%20features%20and%20those%20after%20convolution%20under%20different%20spectrum.%20Our%0Aanalysis%20reveals%20a%20reduction%20in%20the%20impact%20of%20similarity%20when%20the%20eigenvectors%0Aassociated%20with%20the%20largest%20magnitude%20eigenvalue%20exhibit%20directional%0Asimilarity.%20Based%20on%20these%20theoretical%20insights%2C%20we%20propose%20FUGNN%2C%20a%20novel%0Aspectral%20graph%20learning%20approach%20that%20harmonizes%20the%20conflict%20between%20fairness%0Aand%20utility.%20FUGNN%20ensures%20algorithmic%20fairness%20and%20utility%20by%20truncating%20the%0Aspectrum%20and%20optimizing%20eigenvector%20distribution%20during%20the%20encoding%20process.%0AThe%20fairness-aware%20eigenvector%20selection%20reduces%20the%20impact%20of%20convolution%20on%0Asensitive%20features%20while%20concurrently%20minimizing%20the%20sacrifice%20of%20utility.%0AFUGNN%20further%20optimizes%20the%20distribution%20of%20eigenvectors%20through%20a%20transformer%0Aarchitecture.%20By%20incorporating%20the%20optimized%20spectrum%20into%20the%20graph%0Aconvolution%20network%2C%20FUGNN%20effectively%20learns%20node%20representations.%20Experiments%0Aon%20six%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20FUGNN%20over%20baseline%0Amethods.%20The%20codes%20are%20available%20at%20https%3A//github.com/yushuowiki/FUGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFUGNN%253A%2520Harmonizing%2520Fairness%2520and%2520Utility%2520in%2520Graph%2520Neural%2520Networks%26entry.906535625%3DRenqiang%2520Luo%2520and%2520Huafei%2520Huang%2520and%2520Shuo%2520Yu%2520and%2520Zhuoyang%2520Han%2520and%2520Estrid%2520He%2520and%2520Xiuzhen%2520Zhang%2520and%2520Feng%2520Xia%26entry.1292438233%3D%2520%2520Fairness-aware%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520often%2520face%2520a%2520challenging%250Atrade-off%252C%2520where%2520prioritizing%2520fairness%2520may%2520require%2520compromising%2520utility.%2520In%250Athis%2520work%252C%2520we%2520re-examine%2520fairness%2520through%2520the%2520lens%2520of%2520spectral%2520graph%2520theory%252C%250Aaiming%2520to%2520reconcile%2520fairness%2520and%2520utility%2520within%2520the%2520framework%2520of%2520spectral%2520graph%250Alearning.%2520We%2520explore%2520the%2520correlation%2520between%2520sensitive%2520features%2520and%2520spectrum%2520in%250AGNNs%252C%2520using%2520theoretical%2520analysis%2520to%2520delineate%2520the%2520similarity%2520between%2520original%250Asensitive%2520features%2520and%2520those%2520after%2520convolution%2520under%2520different%2520spectrum.%2520Our%250Aanalysis%2520reveals%2520a%2520reduction%2520in%2520the%2520impact%2520of%2520similarity%2520when%2520the%2520eigenvectors%250Aassociated%2520with%2520the%2520largest%2520magnitude%2520eigenvalue%2520exhibit%2520directional%250Asimilarity.%2520Based%2520on%2520these%2520theoretical%2520insights%252C%2520we%2520propose%2520FUGNN%252C%2520a%2520novel%250Aspectral%2520graph%2520learning%2520approach%2520that%2520harmonizes%2520the%2520conflict%2520between%2520fairness%250Aand%2520utility.%2520FUGNN%2520ensures%2520algorithmic%2520fairness%2520and%2520utility%2520by%2520truncating%2520the%250Aspectrum%2520and%2520optimizing%2520eigenvector%2520distribution%2520during%2520the%2520encoding%2520process.%250AThe%2520fairness-aware%2520eigenvector%2520selection%2520reduces%2520the%2520impact%2520of%2520convolution%2520on%250Asensitive%2520features%2520while%2520concurrently%2520minimizing%2520the%2520sacrifice%2520of%2520utility.%250AFUGNN%2520further%2520optimizes%2520the%2520distribution%2520of%2520eigenvectors%2520through%2520a%2520transformer%250Aarchitecture.%2520By%2520incorporating%2520the%2520optimized%2520spectrum%2520into%2520the%2520graph%250Aconvolution%2520network%252C%2520FUGNN%2520effectively%2520learns%2520node%2520representations.%2520Experiments%250Aon%2520six%2520real-world%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520FUGNN%2520over%2520baseline%250Amethods.%2520The%2520codes%2520are%2520available%2520at%2520https%253A//github.com/yushuowiki/FUGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FUGNN%3A%20Harmonizing%20Fairness%20and%20Utility%20in%20Graph%20Neural%20Networks&entry.906535625=Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Shuo%20Yu%20and%20Zhuoyang%20Han%20and%20Estrid%20He%20and%20Xiuzhen%20Zhang%20and%20Feng%20Xia&entry.1292438233=%20%20Fairness-aware%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20face%20a%20challenging%0Atrade-off%2C%20where%20prioritizing%20fairness%20may%20require%20compromising%20utility.%20In%0Athis%20work%2C%20we%20re-examine%20fairness%20through%20the%20lens%20of%20spectral%20graph%20theory%2C%0Aaiming%20to%20reconcile%20fairness%20and%20utility%20within%20the%20framework%20of%20spectral%20graph%0Alearning.%20We%20explore%20the%20correlation%20between%20sensitive%20features%20and%20spectrum%20in%0AGNNs%2C%20using%20theoretical%20analysis%20to%20delineate%20the%20similarity%20between%20original%0Asensitive%20features%20and%20those%20after%20convolution%20under%20different%20spectrum.%20Our%0Aanalysis%20reveals%20a%20reduction%20in%20the%20impact%20of%20similarity%20when%20the%20eigenvectors%0Aassociated%20with%20the%20largest%20magnitude%20eigenvalue%20exhibit%20directional%0Asimilarity.%20Based%20on%20these%20theoretical%20insights%2C%20we%20propose%20FUGNN%2C%20a%20novel%0Aspectral%20graph%20learning%20approach%20that%20harmonizes%20the%20conflict%20between%20fairness%0Aand%20utility.%20FUGNN%20ensures%20algorithmic%20fairness%20and%20utility%20by%20truncating%20the%0Aspectrum%20and%20optimizing%20eigenvector%20distribution%20during%20the%20encoding%20process.%0AThe%20fairness-aware%20eigenvector%20selection%20reduces%20the%20impact%20of%20convolution%20on%0Asensitive%20features%20while%20concurrently%20minimizing%20the%20sacrifice%20of%20utility.%0AFUGNN%20further%20optimizes%20the%20distribution%20of%20eigenvectors%20through%20a%20transformer%0Aarchitecture.%20By%20incorporating%20the%20optimized%20spectrum%20into%20the%20graph%0Aconvolution%20network%2C%20FUGNN%20effectively%20learns%20node%20representations.%20Experiments%0Aon%20six%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20FUGNN%20over%20baseline%0Amethods.%20The%20codes%20are%20available%20at%20https%3A//github.com/yushuowiki/FUGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17034v1&entry.124074799=Read"},
{"title": "Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points,\n  Saddle Escaping, and Network Embedding", "author": "Zhengqing Wu and Berfin Simsek and Francois Ged", "abstract": "  In this paper, we investigate the loss landscape of one-hidden-layer neural\nnetworks with ReLU-like activation functions trained with the empirical squared\nloss. As the activation function is non-differentiable, it is so far unclear\nhow to completely characterize the stationary points. We propose the conditions\nfor stationarity that apply to both non-differentiable and differentiable\ncases. Additionally, we show that, if a stationary point does not contain\n\"escape neurons\", which are defined with first-order conditions, then it must\nbe a local minimum. Moreover, for the scalar-output case, the presence of an\nescape neuron guarantees that the stationary point is not a local minimum. Our\nresults refine the description of the saddle-to-saddle training process\nstarting from infinitesimally small (vanishing) initialization for shallow\nReLU-like networks, linking saddle escaping directly with the parameter changes\nof escape neurons. Moreover, we are also able to fully discuss how network\nembedding, which is to instantiate a narrower network within a wider network,\nreshapes the stationary points.\n", "link": "http://arxiv.org/abs/2402.05626v3", "date": "2024-05-27", "relevancy": 2.2697, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4603}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding&body=Title%3A%20Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding%0AAuthor%3A%20Zhengqing%20Wu%20and%20Berfin%20Simsek%20and%20Francois%20Ged%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20loss%20landscape%20of%20one-hidden-layer%20neural%0Anetworks%20with%20ReLU-like%20activation%20functions%20trained%20with%20the%20empirical%20squared%0Aloss.%20As%20the%20activation%20function%20is%20non-differentiable%2C%20it%20is%20so%20far%20unclear%0Ahow%20to%20completely%20characterize%20the%20stationary%20points.%20We%20propose%20the%20conditions%0Afor%20stationarity%20that%20apply%20to%20both%20non-differentiable%20and%20differentiable%0Acases.%20Additionally%2C%20we%20show%20that%2C%20if%20a%20stationary%20point%20does%20not%20contain%0A%22escape%20neurons%22%2C%20which%20are%20defined%20with%20first-order%20conditions%2C%20then%20it%20must%0Abe%20a%20local%20minimum.%20Moreover%2C%20for%20the%20scalar-output%20case%2C%20the%20presence%20of%20an%0Aescape%20neuron%20guarantees%20that%20the%20stationary%20point%20is%20not%20a%20local%20minimum.%20Our%0Aresults%20refine%20the%20description%20of%20the%20saddle-to-saddle%20training%20process%0Astarting%20from%20infinitesimally%20small%20%28vanishing%29%20initialization%20for%20shallow%0AReLU-like%20networks%2C%20linking%20saddle%20escaping%20directly%20with%20the%20parameter%20changes%0Aof%20escape%20neurons.%20Moreover%2C%20we%20are%20also%20able%20to%20fully%20discuss%20how%20network%0Aembedding%2C%20which%20is%20to%20instantiate%20a%20narrower%20network%20within%20a%20wider%20network%2C%0Areshapes%20the%20stationary%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05626v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoss%2520Landscape%2520of%2520Shallow%2520ReLU-like%2520Neural%2520Networks%253A%2520Stationary%2520Points%252C%250A%2520%2520Saddle%2520Escaping%252C%2520and%2520Network%2520Embedding%26entry.906535625%3DZhengqing%2520Wu%2520and%2520Berfin%2520Simsek%2520and%2520Francois%2520Ged%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520loss%2520landscape%2520of%2520one-hidden-layer%2520neural%250Anetworks%2520with%2520ReLU-like%2520activation%2520functions%2520trained%2520with%2520the%2520empirical%2520squared%250Aloss.%2520As%2520the%2520activation%2520function%2520is%2520non-differentiable%252C%2520it%2520is%2520so%2520far%2520unclear%250Ahow%2520to%2520completely%2520characterize%2520the%2520stationary%2520points.%2520We%2520propose%2520the%2520conditions%250Afor%2520stationarity%2520that%2520apply%2520to%2520both%2520non-differentiable%2520and%2520differentiable%250Acases.%2520Additionally%252C%2520we%2520show%2520that%252C%2520if%2520a%2520stationary%2520point%2520does%2520not%2520contain%250A%2522escape%2520neurons%2522%252C%2520which%2520are%2520defined%2520with%2520first-order%2520conditions%252C%2520then%2520it%2520must%250Abe%2520a%2520local%2520minimum.%2520Moreover%252C%2520for%2520the%2520scalar-output%2520case%252C%2520the%2520presence%2520of%2520an%250Aescape%2520neuron%2520guarantees%2520that%2520the%2520stationary%2520point%2520is%2520not%2520a%2520local%2520minimum.%2520Our%250Aresults%2520refine%2520the%2520description%2520of%2520the%2520saddle-to-saddle%2520training%2520process%250Astarting%2520from%2520infinitesimally%2520small%2520%2528vanishing%2529%2520initialization%2520for%2520shallow%250AReLU-like%2520networks%252C%2520linking%2520saddle%2520escaping%2520directly%2520with%2520the%2520parameter%2520changes%250Aof%2520escape%2520neurons.%2520Moreover%252C%2520we%2520are%2520also%2520able%2520to%2520fully%2520discuss%2520how%2520network%250Aembedding%252C%2520which%2520is%2520to%2520instantiate%2520a%2520narrower%2520network%2520within%2520a%2520wider%2520network%252C%250Areshapes%2520the%2520stationary%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05626v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loss%20Landscape%20of%20Shallow%20ReLU-like%20Neural%20Networks%3A%20Stationary%20Points%2C%0A%20%20Saddle%20Escaping%2C%20and%20Network%20Embedding&entry.906535625=Zhengqing%20Wu%20and%20Berfin%20Simsek%20and%20Francois%20Ged&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20loss%20landscape%20of%20one-hidden-layer%20neural%0Anetworks%20with%20ReLU-like%20activation%20functions%20trained%20with%20the%20empirical%20squared%0Aloss.%20As%20the%20activation%20function%20is%20non-differentiable%2C%20it%20is%20so%20far%20unclear%0Ahow%20to%20completely%20characterize%20the%20stationary%20points.%20We%20propose%20the%20conditions%0Afor%20stationarity%20that%20apply%20to%20both%20non-differentiable%20and%20differentiable%0Acases.%20Additionally%2C%20we%20show%20that%2C%20if%20a%20stationary%20point%20does%20not%20contain%0A%22escape%20neurons%22%2C%20which%20are%20defined%20with%20first-order%20conditions%2C%20then%20it%20must%0Abe%20a%20local%20minimum.%20Moreover%2C%20for%20the%20scalar-output%20case%2C%20the%20presence%20of%20an%0Aescape%20neuron%20guarantees%20that%20the%20stationary%20point%20is%20not%20a%20local%20minimum.%20Our%0Aresults%20refine%20the%20description%20of%20the%20saddle-to-saddle%20training%20process%0Astarting%20from%20infinitesimally%20small%20%28vanishing%29%20initialization%20for%20shallow%0AReLU-like%20networks%2C%20linking%20saddle%20escaping%20directly%20with%20the%20parameter%20changes%0Aof%20escape%20neurons.%20Moreover%2C%20we%20are%20also%20able%20to%20fully%20discuss%20how%20network%0Aembedding%2C%20which%20is%20to%20instantiate%20a%20narrower%20network%20within%20a%20wider%20network%2C%0Areshapes%20the%20stationary%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05626v3&entry.124074799=Read"},
{"title": "Can Generative Models Improve Self-Supervised Representation Learning?", "author": "Sana Ayromlou and Arash Afkanpour and Vahid Reza Khazaie and Fereshteh Forghani", "abstract": "  The rapid advancement in self-supervised learning (SSL) has highlighted its\npotential to leverage unlabeled data for learning rich visual representations.\nHowever, the existing SSL techniques, particularly those employing different\naugmentations of the same image, often rely on a limited set of simple\ntransformations that are not representative of real-world data variations. This\nconstrains the diversity and quality of samples, which leads to sub-optimal\nrepresentations. In this paper, we introduce a novel framework that enriches\nthe SSL paradigm by utilizing generative models to produce semantically\nconsistent image augmentations. By directly conditioning generative models on a\nsource image representation, our method enables the generation of diverse\naugmentations while maintaining the semantics of the source image, thus\noffering a richer set of data for self-supervised learning. Our extensive\nexperimental results on various SSL methods demonstrate that our framework\nsignificantly enhances the quality of learned visual representations by up to\n10\\% Top-1 accuracy in downstream tasks. This research demonstrates that\nincorporating generative models into the SSL workflow opens new avenues for\nexploring the potential of synthetic data. This development paves the way for\nmore robust and versatile representation learning techniques.\n", "link": "http://arxiv.org/abs/2403.05966v2", "date": "2024-05-27", "relevancy": 2.2547, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.569}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F&body=Title%3A%20Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F%0AAuthor%3A%20Sana%20Ayromlou%20and%20Arash%20Afkanpour%20and%20Vahid%20Reza%20Khazaie%20and%20Fereshteh%20Forghani%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20self-supervised%20learning%20%28SSL%29%20has%20highlighted%20its%0Apotential%20to%20leverage%20unlabeled%20data%20for%20learning%20rich%20visual%20representations.%0AHowever%2C%20the%20existing%20SSL%20techniques%2C%20particularly%20those%20employing%20different%0Aaugmentations%20of%20the%20same%20image%2C%20often%20rely%20on%20a%20limited%20set%20of%20simple%0Atransformations%20that%20are%20not%20representative%20of%20real-world%20data%20variations.%20This%0Aconstrains%20the%20diversity%20and%20quality%20of%20samples%2C%20which%20leads%20to%20sub-optimal%0Arepresentations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20that%20enriches%0Athe%20SSL%20paradigm%20by%20utilizing%20generative%20models%20to%20produce%20semantically%0Aconsistent%20image%20augmentations.%20By%20directly%20conditioning%20generative%20models%20on%20a%0Asource%20image%20representation%2C%20our%20method%20enables%20the%20generation%20of%20diverse%0Aaugmentations%20while%20maintaining%20the%20semantics%20of%20the%20source%20image%2C%20thus%0Aoffering%20a%20richer%20set%20of%20data%20for%20self-supervised%20learning.%20Our%20extensive%0Aexperimental%20results%20on%20various%20SSL%20methods%20demonstrate%20that%20our%20framework%0Asignificantly%20enhances%20the%20quality%20of%20learned%20visual%20representations%20by%20up%20to%0A10%5C%25%20Top-1%20accuracy%20in%20downstream%20tasks.%20This%20research%20demonstrates%20that%0Aincorporating%20generative%20models%20into%20the%20SSL%20workflow%20opens%20new%20avenues%20for%0Aexploring%20the%20potential%20of%20synthetic%20data.%20This%20development%20paves%20the%20way%20for%0Amore%20robust%20and%20versatile%20representation%20learning%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05966v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Generative%2520Models%2520Improve%2520Self-Supervised%2520Representation%2520Learning%253F%26entry.906535625%3DSana%2520Ayromlou%2520and%2520Arash%2520Afkanpour%2520and%2520Vahid%2520Reza%2520Khazaie%2520and%2520Fereshteh%2520Forghani%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520self-supervised%2520learning%2520%2528SSL%2529%2520has%2520highlighted%2520its%250Apotential%2520to%2520leverage%2520unlabeled%2520data%2520for%2520learning%2520rich%2520visual%2520representations.%250AHowever%252C%2520the%2520existing%2520SSL%2520techniques%252C%2520particularly%2520those%2520employing%2520different%250Aaugmentations%2520of%2520the%2520same%2520image%252C%2520often%2520rely%2520on%2520a%2520limited%2520set%2520of%2520simple%250Atransformations%2520that%2520are%2520not%2520representative%2520of%2520real-world%2520data%2520variations.%2520This%250Aconstrains%2520the%2520diversity%2520and%2520quality%2520of%2520samples%252C%2520which%2520leads%2520to%2520sub-optimal%250Arepresentations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520enriches%250Athe%2520SSL%2520paradigm%2520by%2520utilizing%2520generative%2520models%2520to%2520produce%2520semantically%250Aconsistent%2520image%2520augmentations.%2520By%2520directly%2520conditioning%2520generative%2520models%2520on%2520a%250Asource%2520image%2520representation%252C%2520our%2520method%2520enables%2520the%2520generation%2520of%2520diverse%250Aaugmentations%2520while%2520maintaining%2520the%2520semantics%2520of%2520the%2520source%2520image%252C%2520thus%250Aoffering%2520a%2520richer%2520set%2520of%2520data%2520for%2520self-supervised%2520learning.%2520Our%2520extensive%250Aexperimental%2520results%2520on%2520various%2520SSL%2520methods%2520demonstrate%2520that%2520our%2520framework%250Asignificantly%2520enhances%2520the%2520quality%2520of%2520learned%2520visual%2520representations%2520by%2520up%2520to%250A10%255C%2525%2520Top-1%2520accuracy%2520in%2520downstream%2520tasks.%2520This%2520research%2520demonstrates%2520that%250Aincorporating%2520generative%2520models%2520into%2520the%2520SSL%2520workflow%2520opens%2520new%2520avenues%2520for%250Aexploring%2520the%2520potential%2520of%2520synthetic%2520data.%2520This%2520development%2520paves%2520the%2520way%2520for%250Amore%2520robust%2520and%2520versatile%2520representation%2520learning%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05966v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Generative%20Models%20Improve%20Self-Supervised%20Representation%20Learning%3F&entry.906535625=Sana%20Ayromlou%20and%20Arash%20Afkanpour%20and%20Vahid%20Reza%20Khazaie%20and%20Fereshteh%20Forghani&entry.1292438233=%20%20The%20rapid%20advancement%20in%20self-supervised%20learning%20%28SSL%29%20has%20highlighted%20its%0Apotential%20to%20leverage%20unlabeled%20data%20for%20learning%20rich%20visual%20representations.%0AHowever%2C%20the%20existing%20SSL%20techniques%2C%20particularly%20those%20employing%20different%0Aaugmentations%20of%20the%20same%20image%2C%20often%20rely%20on%20a%20limited%20set%20of%20simple%0Atransformations%20that%20are%20not%20representative%20of%20real-world%20data%20variations.%20This%0Aconstrains%20the%20diversity%20and%20quality%20of%20samples%2C%20which%20leads%20to%20sub-optimal%0Arepresentations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20that%20enriches%0Athe%20SSL%20paradigm%20by%20utilizing%20generative%20models%20to%20produce%20semantically%0Aconsistent%20image%20augmentations.%20By%20directly%20conditioning%20generative%20models%20on%20a%0Asource%20image%20representation%2C%20our%20method%20enables%20the%20generation%20of%20diverse%0Aaugmentations%20while%20maintaining%20the%20semantics%20of%20the%20source%20image%2C%20thus%0Aoffering%20a%20richer%20set%20of%20data%20for%20self-supervised%20learning.%20Our%20extensive%0Aexperimental%20results%20on%20various%20SSL%20methods%20demonstrate%20that%20our%20framework%0Asignificantly%20enhances%20the%20quality%20of%20learned%20visual%20representations%20by%20up%20to%0A10%5C%25%20Top-1%20accuracy%20in%20downstream%20tasks.%20This%20research%20demonstrates%20that%0Aincorporating%20generative%20models%20into%20the%20SSL%20workflow%20opens%20new%20avenues%20for%0Aexploring%20the%20potential%20of%20synthetic%20data.%20This%20development%20paves%20the%20way%20for%0Amore%20robust%20and%20versatile%20representation%20learning%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05966v2&entry.124074799=Read"},
{"title": "TiC: Exploring Vision Transformer in Convolution", "author": "Song Zhang and Qingzhong Wang and Jiang Bian and Haoyi Xiong", "abstract": "  While models derived from Vision Transformers (ViTs) have been phonemically\nsurging, pre-trained models cannot seamlessly adapt to arbitrary resolution\nimages without altering the architecture and configuration, such as sampling\nthe positional encoding, limiting their flexibility for various vision tasks.\nFor instance, the Segment Anything Model (SAM) based on ViT-Huge requires all\ninput images to be resized to 1024$\\times$1024. To overcome this limitation, we\npropose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates\nSelf-Attention within generalized convolutions, including standard, dilated,\nand depthwise ones. Enabling transformers to handle images of varying sizes\nwithout retraining or rescaling, the use of MSA-Conv further reduces\ncomputational costs compared to global attention in ViT, which grows costly as\nimage size increases. Later, we present the Vision Transformer in Convolution\n(TiC) as a proof of concept for image classification with MSA-Conv, where two\ncapacity enhancing strategies, namely Multi-Directional Cyclic Shifted\nMechanism and Inter-Pooling Mechanism, have been proposed, through establishing\nlong-distance connections between tokens and enlarging the effective receptive\nfield. Extensive experiments have been carried out to validate the overall\neffectiveness of TiC. Additionally, ablation studies confirm the performance\nimprovement made by MSA-Conv and the two capacity enhancing strategies\nseparately. Note that our proposal aims at studying an alternative to the\nglobal attention used in ViT, while MSA-Conv meets our goal by making TiC\ncomparable to state-of-the-art on ImageNet-1K. Code will be released at\nhttps://github.com/zs670980918/MSA-Conv.\n", "link": "http://arxiv.org/abs/2310.04134v2", "date": "2024-05-27", "relevancy": 2.2528, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5763}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.572}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiC%3A%20Exploring%20Vision%20Transformer%20in%20Convolution&body=Title%3A%20TiC%3A%20Exploring%20Vision%20Transformer%20in%20Convolution%0AAuthor%3A%20Song%20Zhang%20and%20Qingzhong%20Wang%20and%20Jiang%20Bian%20and%20Haoyi%20Xiong%0AAbstract%3A%20%20%20While%20models%20derived%20from%20Vision%20Transformers%20%28ViTs%29%20have%20been%20phonemically%0Asurging%2C%20pre-trained%20models%20cannot%20seamlessly%20adapt%20to%20arbitrary%20resolution%0Aimages%20without%20altering%20the%20architecture%20and%20configuration%2C%20such%20as%20sampling%0Athe%20positional%20encoding%2C%20limiting%20their%20flexibility%20for%20various%20vision%20tasks.%0AFor%20instance%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20based%20on%20ViT-Huge%20requires%20all%0Ainput%20images%20to%20be%20resized%20to%201024%24%5Ctimes%241024.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20the%20Multi-Head%20Self-Attention%20Convolution%20%28MSA-Conv%29%20that%20incorporates%0ASelf-Attention%20within%20generalized%20convolutions%2C%20including%20standard%2C%20dilated%2C%0Aand%20depthwise%20ones.%20Enabling%20transformers%20to%20handle%20images%20of%20varying%20sizes%0Awithout%20retraining%20or%20rescaling%2C%20the%20use%20of%20MSA-Conv%20further%20reduces%0Acomputational%20costs%20compared%20to%20global%20attention%20in%20ViT%2C%20which%20grows%20costly%20as%0Aimage%20size%20increases.%20Later%2C%20we%20present%20the%20Vision%20Transformer%20in%20Convolution%0A%28TiC%29%20as%20a%20proof%20of%20concept%20for%20image%20classification%20with%20MSA-Conv%2C%20where%20two%0Acapacity%20enhancing%20strategies%2C%20namely%20Multi-Directional%20Cyclic%20Shifted%0AMechanism%20and%20Inter-Pooling%20Mechanism%2C%20have%20been%20proposed%2C%20through%20establishing%0Along-distance%20connections%20between%20tokens%20and%20enlarging%20the%20effective%20receptive%0Afield.%20Extensive%20experiments%20have%20been%20carried%20out%20to%20validate%20the%20overall%0Aeffectiveness%20of%20TiC.%20Additionally%2C%20ablation%20studies%20confirm%20the%20performance%0Aimprovement%20made%20by%20MSA-Conv%20and%20the%20two%20capacity%20enhancing%20strategies%0Aseparately.%20Note%20that%20our%20proposal%20aims%20at%20studying%20an%20alternative%20to%20the%0Aglobal%20attention%20used%20in%20ViT%2C%20while%20MSA-Conv%20meets%20our%20goal%20by%20making%20TiC%0Acomparable%20to%20state-of-the-art%20on%20ImageNet-1K.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/zs670980918/MSA-Conv.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiC%253A%2520Exploring%2520Vision%2520Transformer%2520in%2520Convolution%26entry.906535625%3DSong%2520Zhang%2520and%2520Qingzhong%2520Wang%2520and%2520Jiang%2520Bian%2520and%2520Haoyi%2520Xiong%26entry.1292438233%3D%2520%2520While%2520models%2520derived%2520from%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520been%2520phonemically%250Asurging%252C%2520pre-trained%2520models%2520cannot%2520seamlessly%2520adapt%2520to%2520arbitrary%2520resolution%250Aimages%2520without%2520altering%2520the%2520architecture%2520and%2520configuration%252C%2520such%2520as%2520sampling%250Athe%2520positional%2520encoding%252C%2520limiting%2520their%2520flexibility%2520for%2520various%2520vision%2520tasks.%250AFor%2520instance%252C%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520based%2520on%2520ViT-Huge%2520requires%2520all%250Ainput%2520images%2520to%2520be%2520resized%2520to%25201024%2524%255Ctimes%25241024.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520the%2520Multi-Head%2520Self-Attention%2520Convolution%2520%2528MSA-Conv%2529%2520that%2520incorporates%250ASelf-Attention%2520within%2520generalized%2520convolutions%252C%2520including%2520standard%252C%2520dilated%252C%250Aand%2520depthwise%2520ones.%2520Enabling%2520transformers%2520to%2520handle%2520images%2520of%2520varying%2520sizes%250Awithout%2520retraining%2520or%2520rescaling%252C%2520the%2520use%2520of%2520MSA-Conv%2520further%2520reduces%250Acomputational%2520costs%2520compared%2520to%2520global%2520attention%2520in%2520ViT%252C%2520which%2520grows%2520costly%2520as%250Aimage%2520size%2520increases.%2520Later%252C%2520we%2520present%2520the%2520Vision%2520Transformer%2520in%2520Convolution%250A%2528TiC%2529%2520as%2520a%2520proof%2520of%2520concept%2520for%2520image%2520classification%2520with%2520MSA-Conv%252C%2520where%2520two%250Acapacity%2520enhancing%2520strategies%252C%2520namely%2520Multi-Directional%2520Cyclic%2520Shifted%250AMechanism%2520and%2520Inter-Pooling%2520Mechanism%252C%2520have%2520been%2520proposed%252C%2520through%2520establishing%250Along-distance%2520connections%2520between%2520tokens%2520and%2520enlarging%2520the%2520effective%2520receptive%250Afield.%2520Extensive%2520experiments%2520have%2520been%2520carried%2520out%2520to%2520validate%2520the%2520overall%250Aeffectiveness%2520of%2520TiC.%2520Additionally%252C%2520ablation%2520studies%2520confirm%2520the%2520performance%250Aimprovement%2520made%2520by%2520MSA-Conv%2520and%2520the%2520two%2520capacity%2520enhancing%2520strategies%250Aseparately.%2520Note%2520that%2520our%2520proposal%2520aims%2520at%2520studying%2520an%2520alternative%2520to%2520the%250Aglobal%2520attention%2520used%2520in%2520ViT%252C%2520while%2520MSA-Conv%2520meets%2520our%2520goal%2520by%2520making%2520TiC%250Acomparable%2520to%2520state-of-the-art%2520on%2520ImageNet-1K.%2520Code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/zs670980918/MSA-Conv.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiC%3A%20Exploring%20Vision%20Transformer%20in%20Convolution&entry.906535625=Song%20Zhang%20and%20Qingzhong%20Wang%20and%20Jiang%20Bian%20and%20Haoyi%20Xiong&entry.1292438233=%20%20While%20models%20derived%20from%20Vision%20Transformers%20%28ViTs%29%20have%20been%20phonemically%0Asurging%2C%20pre-trained%20models%20cannot%20seamlessly%20adapt%20to%20arbitrary%20resolution%0Aimages%20without%20altering%20the%20architecture%20and%20configuration%2C%20such%20as%20sampling%0Athe%20positional%20encoding%2C%20limiting%20their%20flexibility%20for%20various%20vision%20tasks.%0AFor%20instance%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20based%20on%20ViT-Huge%20requires%20all%0Ainput%20images%20to%20be%20resized%20to%201024%24%5Ctimes%241024.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20the%20Multi-Head%20Self-Attention%20Convolution%20%28MSA-Conv%29%20that%20incorporates%0ASelf-Attention%20within%20generalized%20convolutions%2C%20including%20standard%2C%20dilated%2C%0Aand%20depthwise%20ones.%20Enabling%20transformers%20to%20handle%20images%20of%20varying%20sizes%0Awithout%20retraining%20or%20rescaling%2C%20the%20use%20of%20MSA-Conv%20further%20reduces%0Acomputational%20costs%20compared%20to%20global%20attention%20in%20ViT%2C%20which%20grows%20costly%20as%0Aimage%20size%20increases.%20Later%2C%20we%20present%20the%20Vision%20Transformer%20in%20Convolution%0A%28TiC%29%20as%20a%20proof%20of%20concept%20for%20image%20classification%20with%20MSA-Conv%2C%20where%20two%0Acapacity%20enhancing%20strategies%2C%20namely%20Multi-Directional%20Cyclic%20Shifted%0AMechanism%20and%20Inter-Pooling%20Mechanism%2C%20have%20been%20proposed%2C%20through%20establishing%0Along-distance%20connections%20between%20tokens%20and%20enlarging%20the%20effective%20receptive%0Afield.%20Extensive%20experiments%20have%20been%20carried%20out%20to%20validate%20the%20overall%0Aeffectiveness%20of%20TiC.%20Additionally%2C%20ablation%20studies%20confirm%20the%20performance%0Aimprovement%20made%20by%20MSA-Conv%20and%20the%20two%20capacity%20enhancing%20strategies%0Aseparately.%20Note%20that%20our%20proposal%20aims%20at%20studying%20an%20alternative%20to%20the%0Aglobal%20attention%20used%20in%20ViT%2C%20while%20MSA-Conv%20meets%20our%20goal%20by%20making%20TiC%0Acomparable%20to%20state-of-the-art%20on%20ImageNet-1K.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/zs670980918/MSA-Conv.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04134v2&entry.124074799=Read"},
{"title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion\n  Model Training", "author": "Kai Wang and Yukun Zhou and Mingjia Shi and Zhihang Yuan and Yuzhang Shang and Xiaojiang Peng and Hanwang Zhang and Yang You", "abstract": "  Training diffusion models is always a computation-intensive task. In this\npaper, we introduce a novel speed-up method for diffusion model training,\ncalled, which is based on a closer look at time steps. Our key findings are: i)\nTime steps can be empirically divided into acceleration, deceleration, and\nconvergence areas based on the process increment. ii) These time steps are\nimbalanced, with many concentrated in the convergence area. iii) The\nconcentrated steps provide limited benefits for diffusion training. To address\nthis, we design an asymmetric sampling strategy that reduces the frequency of\nsteps from the convergence area while increasing the sampling probability for\nsteps from other areas. Additionally, we propose a weighting strategy to\nemphasize the importance of time steps with rapid-change process increments. As\na plug-and-play and architecture-agnostic approach, SpeeD consistently achieves\n3-times acceleration across various diffusion architectures, datasets, and\ntasks. Notably, due to its simple design, our approach significantly reduces\nthe cost of diffusion model training with minimal overhead. Our research\nenables more researchers to train diffusion models at a lower cost.\n", "link": "http://arxiv.org/abs/2405.17403v1", "date": "2024-05-27", "relevancy": 2.2501, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6456}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training&body=Title%3A%20A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training%0AAuthor%3A%20Kai%20Wang%20and%20Yukun%20Zhou%20and%20Mingjia%20Shi%20and%20Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Xiaojiang%20Peng%20and%20Hanwang%20Zhang%20and%20Yang%20You%0AAbstract%3A%20%20%20Training%20diffusion%20models%20is%20always%20a%20computation-intensive%20task.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20speed-up%20method%20for%20diffusion%20model%20training%2C%0Acalled%2C%20which%20is%20based%20on%20a%20closer%20look%20at%20time%20steps.%20Our%20key%20findings%20are%3A%20i%29%0ATime%20steps%20can%20be%20empirically%20divided%20into%20acceleration%2C%20deceleration%2C%20and%0Aconvergence%20areas%20based%20on%20the%20process%20increment.%20ii%29%20These%20time%20steps%20are%0Aimbalanced%2C%20with%20many%20concentrated%20in%20the%20convergence%20area.%20iii%29%20The%0Aconcentrated%20steps%20provide%20limited%20benefits%20for%20diffusion%20training.%20To%20address%0Athis%2C%20we%20design%20an%20asymmetric%20sampling%20strategy%20that%20reduces%20the%20frequency%20of%0Asteps%20from%20the%20convergence%20area%20while%20increasing%20the%20sampling%20probability%20for%0Asteps%20from%20other%20areas.%20Additionally%2C%20we%20propose%20a%20weighting%20strategy%20to%0Aemphasize%20the%20importance%20of%20time%20steps%20with%20rapid-change%20process%20increments.%20As%0Aa%20plug-and-play%20and%20architecture-agnostic%20approach%2C%20SpeeD%20consistently%20achieves%0A3-times%20acceleration%20across%20various%20diffusion%20architectures%2C%20datasets%2C%20and%0Atasks.%20Notably%2C%20due%20to%20its%20simple%20design%2C%20our%20approach%20significantly%20reduces%0Athe%20cost%20of%20diffusion%20model%20training%20with%20minimal%20overhead.%20Our%20research%0Aenables%20more%20researchers%20to%20train%20diffusion%20models%20at%20a%20lower%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520Time%2520Steps%2520is%2520Worthy%2520of%2520Triple%2520Speed-Up%2520for%2520Diffusion%250A%2520%2520Model%2520Training%26entry.906535625%3DKai%2520Wang%2520and%2520Yukun%2520Zhou%2520and%2520Mingjia%2520Shi%2520and%2520Zhihang%2520Yuan%2520and%2520Yuzhang%2520Shang%2520and%2520Xiaojiang%2520Peng%2520and%2520Hanwang%2520Zhang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Training%2520diffusion%2520models%2520is%2520always%2520a%2520computation-intensive%2520task.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520speed-up%2520method%2520for%2520diffusion%2520model%2520training%252C%250Acalled%252C%2520which%2520is%2520based%2520on%2520a%2520closer%2520look%2520at%2520time%2520steps.%2520Our%2520key%2520findings%2520are%253A%2520i%2529%250ATime%2520steps%2520can%2520be%2520empirically%2520divided%2520into%2520acceleration%252C%2520deceleration%252C%2520and%250Aconvergence%2520areas%2520based%2520on%2520the%2520process%2520increment.%2520ii%2529%2520These%2520time%2520steps%2520are%250Aimbalanced%252C%2520with%2520many%2520concentrated%2520in%2520the%2520convergence%2520area.%2520iii%2529%2520The%250Aconcentrated%2520steps%2520provide%2520limited%2520benefits%2520for%2520diffusion%2520training.%2520To%2520address%250Athis%252C%2520we%2520design%2520an%2520asymmetric%2520sampling%2520strategy%2520that%2520reduces%2520the%2520frequency%2520of%250Asteps%2520from%2520the%2520convergence%2520area%2520while%2520increasing%2520the%2520sampling%2520probability%2520for%250Asteps%2520from%2520other%2520areas.%2520Additionally%252C%2520we%2520propose%2520a%2520weighting%2520strategy%2520to%250Aemphasize%2520the%2520importance%2520of%2520time%2520steps%2520with%2520rapid-change%2520process%2520increments.%2520As%250Aa%2520plug-and-play%2520and%2520architecture-agnostic%2520approach%252C%2520SpeeD%2520consistently%2520achieves%250A3-times%2520acceleration%2520across%2520various%2520diffusion%2520architectures%252C%2520datasets%252C%2520and%250Atasks.%2520Notably%252C%2520due%2520to%2520its%2520simple%2520design%252C%2520our%2520approach%2520significantly%2520reduces%250Athe%2520cost%2520of%2520diffusion%2520model%2520training%2520with%2520minimal%2520overhead.%2520Our%2520research%250Aenables%2520more%2520researchers%2520to%2520train%2520diffusion%2520models%2520at%2520a%2520lower%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20Time%20Steps%20is%20Worthy%20of%20Triple%20Speed-Up%20for%20Diffusion%0A%20%20Model%20Training&entry.906535625=Kai%20Wang%20and%20Yukun%20Zhou%20and%20Mingjia%20Shi%20and%20Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Xiaojiang%20Peng%20and%20Hanwang%20Zhang%20and%20Yang%20You&entry.1292438233=%20%20Training%20diffusion%20models%20is%20always%20a%20computation-intensive%20task.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20speed-up%20method%20for%20diffusion%20model%20training%2C%0Acalled%2C%20which%20is%20based%20on%20a%20closer%20look%20at%20time%20steps.%20Our%20key%20findings%20are%3A%20i%29%0ATime%20steps%20can%20be%20empirically%20divided%20into%20acceleration%2C%20deceleration%2C%20and%0Aconvergence%20areas%20based%20on%20the%20process%20increment.%20ii%29%20These%20time%20steps%20are%0Aimbalanced%2C%20with%20many%20concentrated%20in%20the%20convergence%20area.%20iii%29%20The%0Aconcentrated%20steps%20provide%20limited%20benefits%20for%20diffusion%20training.%20To%20address%0Athis%2C%20we%20design%20an%20asymmetric%20sampling%20strategy%20that%20reduces%20the%20frequency%20of%0Asteps%20from%20the%20convergence%20area%20while%20increasing%20the%20sampling%20probability%20for%0Asteps%20from%20other%20areas.%20Additionally%2C%20we%20propose%20a%20weighting%20strategy%20to%0Aemphasize%20the%20importance%20of%20time%20steps%20with%20rapid-change%20process%20increments.%20As%0Aa%20plug-and-play%20and%20architecture-agnostic%20approach%2C%20SpeeD%20consistently%20achieves%0A3-times%20acceleration%20across%20various%20diffusion%20architectures%2C%20datasets%2C%20and%0Atasks.%20Notably%2C%20due%20to%20its%20simple%20design%2C%20our%20approach%20significantly%20reduces%0Athe%20cost%20of%20diffusion%20model%20training%20with%20minimal%20overhead.%20Our%20research%0Aenables%20more%20researchers%20to%20train%20diffusion%20models%20at%20a%20lower%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17403v1&entry.124074799=Read"},
{"title": "LLM-Optic: Unveiling the Capabilities of Large Language Models for\n  Universal Visual Grounding", "author": "Haoyu Zhao and Wenhang Ge and Ying-cong Chen", "abstract": "  Visual grounding is an essential tool that links user-provided text queries\nwith query-specific regions within an image. Despite advancements in visual\ngrounding models, their ability to comprehend complex queries remains limited.\nTo overcome this limitation, we introduce LLM-Optic, an innovative method that\nutilizes Large Language Models (LLMs) as an optical lens to enhance existing\nvisual grounding models in comprehending complex text queries involving\nintricate text structures, multiple objects, or object spatial relationships,\nsituations that current models struggle with. LLM-Optic first employs an LLM as\na Text Grounder to interpret complex text queries and accurately identify\nobjects the user intends to locate. Then a pre-trained visual grounding model\nis used to generate candidate bounding boxes given the refined query by the\nText Grounder. After that, LLM-Optic annotates the candidate bounding boxes\nwith numerical marks to establish a connection between text and specific image\nregions, thereby linking two distinct modalities. Finally, it employs a Large\nMultimodal Model (LMM) as a Visual Grounder to select the marked candidate\nobjects that best correspond to the original text query. Through LLM-Optic, we\nhave achieved universal visual grounding, which allows for the detection of\narbitrary objects specified by arbitrary human language input. Importantly, our\nmethod achieves this enhancement without requiring additional training or\nfine-tuning. Extensive experiments across various challenging benchmarks\ndemonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding\ncapabilities.\n", "link": "http://arxiv.org/abs/2405.17104v1", "date": "2024-05-27", "relevancy": 2.2377, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Optic%3A%20Unveiling%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Universal%20Visual%20Grounding&body=Title%3A%20LLM-Optic%3A%20Unveiling%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Universal%20Visual%20Grounding%0AAuthor%3A%20Haoyu%20Zhao%20and%20Wenhang%20Ge%20and%20Ying-cong%20Chen%0AAbstract%3A%20%20%20Visual%20grounding%20is%20an%20essential%20tool%20that%20links%20user-provided%20text%20queries%0Awith%20query-specific%20regions%20within%20an%20image.%20Despite%20advancements%20in%20visual%0Agrounding%20models%2C%20their%20ability%20to%20comprehend%20complex%20queries%20remains%20limited.%0ATo%20overcome%20this%20limitation%2C%20we%20introduce%20LLM-Optic%2C%20an%20innovative%20method%20that%0Autilizes%20Large%20Language%20Models%20%28LLMs%29%20as%20an%20optical%20lens%20to%20enhance%20existing%0Avisual%20grounding%20models%20in%20comprehending%20complex%20text%20queries%20involving%0Aintricate%20text%20structures%2C%20multiple%20objects%2C%20or%20object%20spatial%20relationships%2C%0Asituations%20that%20current%20models%20struggle%20with.%20LLM-Optic%20first%20employs%20an%20LLM%20as%0Aa%20Text%20Grounder%20to%20interpret%20complex%20text%20queries%20and%20accurately%20identify%0Aobjects%20the%20user%20intends%20to%20locate.%20Then%20a%20pre-trained%20visual%20grounding%20model%0Ais%20used%20to%20generate%20candidate%20bounding%20boxes%20given%20the%20refined%20query%20by%20the%0AText%20Grounder.%20After%20that%2C%20LLM-Optic%20annotates%20the%20candidate%20bounding%20boxes%0Awith%20numerical%20marks%20to%20establish%20a%20connection%20between%20text%20and%20specific%20image%0Aregions%2C%20thereby%20linking%20two%20distinct%20modalities.%20Finally%2C%20it%20employs%20a%20Large%0AMultimodal%20Model%20%28LMM%29%20as%20a%20Visual%20Grounder%20to%20select%20the%20marked%20candidate%0Aobjects%20that%20best%20correspond%20to%20the%20original%20text%20query.%20Through%20LLM-Optic%2C%20we%0Ahave%20achieved%20universal%20visual%20grounding%2C%20which%20allows%20for%20the%20detection%20of%0Aarbitrary%20objects%20specified%20by%20arbitrary%20human%20language%20input.%20Importantly%2C%20our%0Amethod%20achieves%20this%20enhancement%20without%20requiring%20additional%20training%20or%0Afine-tuning.%20Extensive%20experiments%20across%20various%20challenging%20benchmarks%0Ademonstrate%20that%20LLM-Optic%20achieves%20state-of-the-art%20zero-shot%20visual%20grounding%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Optic%253A%2520Unveiling%2520the%2520Capabilities%2520of%2520Large%2520Language%2520Models%2520for%250A%2520%2520Universal%2520Visual%2520Grounding%26entry.906535625%3DHaoyu%2520Zhao%2520and%2520Wenhang%2520Ge%2520and%2520Ying-cong%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520is%2520an%2520essential%2520tool%2520that%2520links%2520user-provided%2520text%2520queries%250Awith%2520query-specific%2520regions%2520within%2520an%2520image.%2520Despite%2520advancements%2520in%2520visual%250Agrounding%2520models%252C%2520their%2520ability%2520to%2520comprehend%2520complex%2520queries%2520remains%2520limited.%250ATo%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520LLM-Optic%252C%2520an%2520innovative%2520method%2520that%250Autilizes%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520an%2520optical%2520lens%2520to%2520enhance%2520existing%250Avisual%2520grounding%2520models%2520in%2520comprehending%2520complex%2520text%2520queries%2520involving%250Aintricate%2520text%2520structures%252C%2520multiple%2520objects%252C%2520or%2520object%2520spatial%2520relationships%252C%250Asituations%2520that%2520current%2520models%2520struggle%2520with.%2520LLM-Optic%2520first%2520employs%2520an%2520LLM%2520as%250Aa%2520Text%2520Grounder%2520to%2520interpret%2520complex%2520text%2520queries%2520and%2520accurately%2520identify%250Aobjects%2520the%2520user%2520intends%2520to%2520locate.%2520Then%2520a%2520pre-trained%2520visual%2520grounding%2520model%250Ais%2520used%2520to%2520generate%2520candidate%2520bounding%2520boxes%2520given%2520the%2520refined%2520query%2520by%2520the%250AText%2520Grounder.%2520After%2520that%252C%2520LLM-Optic%2520annotates%2520the%2520candidate%2520bounding%2520boxes%250Awith%2520numerical%2520marks%2520to%2520establish%2520a%2520connection%2520between%2520text%2520and%2520specific%2520image%250Aregions%252C%2520thereby%2520linking%2520two%2520distinct%2520modalities.%2520Finally%252C%2520it%2520employs%2520a%2520Large%250AMultimodal%2520Model%2520%2528LMM%2529%2520as%2520a%2520Visual%2520Grounder%2520to%2520select%2520the%2520marked%2520candidate%250Aobjects%2520that%2520best%2520correspond%2520to%2520the%2520original%2520text%2520query.%2520Through%2520LLM-Optic%252C%2520we%250Ahave%2520achieved%2520universal%2520visual%2520grounding%252C%2520which%2520allows%2520for%2520the%2520detection%2520of%250Aarbitrary%2520objects%2520specified%2520by%2520arbitrary%2520human%2520language%2520input.%2520Importantly%252C%2520our%250Amethod%2520achieves%2520this%2520enhancement%2520without%2520requiring%2520additional%2520training%2520or%250Afine-tuning.%2520Extensive%2520experiments%2520across%2520various%2520challenging%2520benchmarks%250Ademonstrate%2520that%2520LLM-Optic%2520achieves%2520state-of-the-art%2520zero-shot%2520visual%2520grounding%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Optic%3A%20Unveiling%20the%20Capabilities%20of%20Large%20Language%20Models%20for%0A%20%20Universal%20Visual%20Grounding&entry.906535625=Haoyu%20Zhao%20and%20Wenhang%20Ge%20and%20Ying-cong%20Chen&entry.1292438233=%20%20Visual%20grounding%20is%20an%20essential%20tool%20that%20links%20user-provided%20text%20queries%0Awith%20query-specific%20regions%20within%20an%20image.%20Despite%20advancements%20in%20visual%0Agrounding%20models%2C%20their%20ability%20to%20comprehend%20complex%20queries%20remains%20limited.%0ATo%20overcome%20this%20limitation%2C%20we%20introduce%20LLM-Optic%2C%20an%20innovative%20method%20that%0Autilizes%20Large%20Language%20Models%20%28LLMs%29%20as%20an%20optical%20lens%20to%20enhance%20existing%0Avisual%20grounding%20models%20in%20comprehending%20complex%20text%20queries%20involving%0Aintricate%20text%20structures%2C%20multiple%20objects%2C%20or%20object%20spatial%20relationships%2C%0Asituations%20that%20current%20models%20struggle%20with.%20LLM-Optic%20first%20employs%20an%20LLM%20as%0Aa%20Text%20Grounder%20to%20interpret%20complex%20text%20queries%20and%20accurately%20identify%0Aobjects%20the%20user%20intends%20to%20locate.%20Then%20a%20pre-trained%20visual%20grounding%20model%0Ais%20used%20to%20generate%20candidate%20bounding%20boxes%20given%20the%20refined%20query%20by%20the%0AText%20Grounder.%20After%20that%2C%20LLM-Optic%20annotates%20the%20candidate%20bounding%20boxes%0Awith%20numerical%20marks%20to%20establish%20a%20connection%20between%20text%20and%20specific%20image%0Aregions%2C%20thereby%20linking%20two%20distinct%20modalities.%20Finally%2C%20it%20employs%20a%20Large%0AMultimodal%20Model%20%28LMM%29%20as%20a%20Visual%20Grounder%20to%20select%20the%20marked%20candidate%0Aobjects%20that%20best%20correspond%20to%20the%20original%20text%20query.%20Through%20LLM-Optic%2C%20we%0Ahave%20achieved%20universal%20visual%20grounding%2C%20which%20allows%20for%20the%20detection%20of%0Aarbitrary%20objects%20specified%20by%20arbitrary%20human%20language%20input.%20Importantly%2C%20our%0Amethod%20achieves%20this%20enhancement%20without%20requiring%20additional%20training%20or%0Afine-tuning.%20Extensive%20experiments%20across%20various%20challenging%20benchmarks%0Ademonstrate%20that%20LLM-Optic%20achieves%20state-of-the-art%20zero-shot%20visual%20grounding%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17104v1&entry.124074799=Read"},
{"title": "Sketch-and-Project Meets Newton Method: Global $\\mathcal O(k^{-2})$\n  Convergence with Low-Rank Updates", "author": "Slavom\u00edr Hanzely", "abstract": "  In this paper, we propose the first sketch-and-project Newton method with\nfast $\\mathcal O(k^{-2})$ global convergence rate for self-concordant\nfunctions. Our method, SGN, can be viewed in three ways: i) as a\nsketch-and-project algorithm projecting updates of Newton method, ii) as a\ncubically regularized Newton ethod in sketched subspaces, and iii) as a damped\nNewton method in sketched subspaces. SGN inherits best of all three worlds:\ncheap iteration costs of sketch-and-project methods, state-of-the-art $\\mathcal\nO(k^{-2})$ global convergence rate of full-rank Newton-like methods and the\nalgorithm simplicity of damped Newton methods. Finally, we demonstrate its\ncomparable empirical performance to baseline algorithms.\n", "link": "http://arxiv.org/abs/2305.13082v4", "date": "2024-05-27", "relevancy": 2.2272, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4604}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates&body=Title%3A%20Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates%0AAuthor%3A%20Slavom%C3%ADr%20Hanzely%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20sketch-and-project%20Newton%20method%20with%0Afast%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20for%20self-concordant%0Afunctions.%20Our%20method%2C%20SGN%2C%20can%20be%20viewed%20in%20three%20ways%3A%20i%29%20as%20a%0Asketch-and-project%20algorithm%20projecting%20updates%20of%20Newton%20method%2C%20ii%29%20as%20a%0Acubically%20regularized%20Newton%20ethod%20in%20sketched%20subspaces%2C%20and%20iii%29%20as%20a%20damped%0ANewton%20method%20in%20sketched%20subspaces.%20SGN%20inherits%20best%20of%20all%20three%20worlds%3A%0Acheap%20iteration%20costs%20of%20sketch-and-project%20methods%2C%20state-of-the-art%20%24%5Cmathcal%0AO%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20of%20full-rank%20Newton-like%20methods%20and%20the%0Aalgorithm%20simplicity%20of%20damped%20Newton%20methods.%20Finally%2C%20we%20demonstrate%20its%0Acomparable%20empirical%20performance%20to%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13082v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-and-Project%2520Meets%2520Newton%2520Method%253A%2520Global%2520%2524%255Cmathcal%2520O%2528k%255E%257B-2%257D%2529%2524%250A%2520%2520Convergence%2520with%2520Low-Rank%2520Updates%26entry.906535625%3DSlavom%25C3%25ADr%2520Hanzely%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520sketch-and-project%2520Newton%2520method%2520with%250Afast%2520%2524%255Cmathcal%2520O%2528k%255E%257B-2%257D%2529%2524%2520global%2520convergence%2520rate%2520for%2520self-concordant%250Afunctions.%2520Our%2520method%252C%2520SGN%252C%2520can%2520be%2520viewed%2520in%2520three%2520ways%253A%2520i%2529%2520as%2520a%250Asketch-and-project%2520algorithm%2520projecting%2520updates%2520of%2520Newton%2520method%252C%2520ii%2529%2520as%2520a%250Acubically%2520regularized%2520Newton%2520ethod%2520in%2520sketched%2520subspaces%252C%2520and%2520iii%2529%2520as%2520a%2520damped%250ANewton%2520method%2520in%2520sketched%2520subspaces.%2520SGN%2520inherits%2520best%2520of%2520all%2520three%2520worlds%253A%250Acheap%2520iteration%2520costs%2520of%2520sketch-and-project%2520methods%252C%2520state-of-the-art%2520%2524%255Cmathcal%250AO%2528k%255E%257B-2%257D%2529%2524%2520global%2520convergence%2520rate%2520of%2520full-rank%2520Newton-like%2520methods%2520and%2520the%250Aalgorithm%2520simplicity%2520of%2520damped%2520Newton%2520methods.%2520Finally%252C%2520we%2520demonstrate%2520its%250Acomparable%2520empirical%2520performance%2520to%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13082v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates&entry.906535625=Slavom%C3%ADr%20Hanzely&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20sketch-and-project%20Newton%20method%20with%0Afast%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20for%20self-concordant%0Afunctions.%20Our%20method%2C%20SGN%2C%20can%20be%20viewed%20in%20three%20ways%3A%20i%29%20as%20a%0Asketch-and-project%20algorithm%20projecting%20updates%20of%20Newton%20method%2C%20ii%29%20as%20a%0Acubically%20regularized%20Newton%20ethod%20in%20sketched%20subspaces%2C%20and%20iii%29%20as%20a%20damped%0ANewton%20method%20in%20sketched%20subspaces.%20SGN%20inherits%20best%20of%20all%20three%20worlds%3A%0Acheap%20iteration%20costs%20of%20sketch-and-project%20methods%2C%20state-of-the-art%20%24%5Cmathcal%0AO%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20of%20full-rank%20Newton-like%20methods%20and%20the%0Aalgorithm%20simplicity%20of%20damped%20Newton%20methods.%20Finally%2C%20we%20demonstrate%20its%0Acomparable%20empirical%20performance%20to%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13082v4&entry.124074799=Read"},
{"title": "Towards Stability of Parameter-free Optimization", "author": "Yijiang Pang and Shuyang Yu and Bao Hoang and Jiayu Zhou", "abstract": "  Hyperparameter tuning, particularly the selection of an appropriate learning\nrate in adaptive gradient training methods, remains a challenge. To tackle this\nchallenge, in this paper, we propose a novel parameter-free optimizer,\n\\textsc{AdamG} (Adam with the golden step size), designed to automatically\nadapt to diverse optimization problems without manual tuning. The core\ntechnique underlying \\textsc{AdamG} is our golden step size derived for the\nAdaGrad-Norm algorithm, which is expected to help AdaGrad-Norm preserve the\ntuning-free convergence and approximate the optimal step size in expectation\nw.r.t. various optimization scenarios. To better evaluate tuning-free\nperformance, we propose a novel evaluation criterion, \\textit{reliability}, to\ncomprehensively assess the efficacy of parameter-free optimizers in addition to\nclassical performance criteria. Empirical results demonstrate that compared\nwith other parameter-free baselines, \\textsc{AdamG} achieves superior\nperformance, which is consistently on par with Adam using a manually tuned\nlearning rate across various optimization tasks.\n", "link": "http://arxiv.org/abs/2405.04376v3", "date": "2024-05-27", "relevancy": 2.2265, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4491}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4434}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Stability%20of%20Parameter-free%20Optimization&body=Title%3A%20Towards%20Stability%20of%20Parameter-free%20Optimization%0AAuthor%3A%20Yijiang%20Pang%20and%20Shuyang%20Yu%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou%0AAbstract%3A%20%20%20Hyperparameter%20tuning%2C%20particularly%20the%20selection%20of%20an%20appropriate%20learning%0Arate%20in%20adaptive%20gradient%20training%20methods%2C%20remains%20a%20challenge.%20To%20tackle%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter-free%20optimizer%2C%0A%5Ctextsc%7BAdamG%7D%20%28Adam%20with%20the%20golden%20step%20size%29%2C%20designed%20to%20automatically%0Aadapt%20to%20diverse%20optimization%20problems%20without%20manual%20tuning.%20The%20core%0Atechnique%20underlying%20%5Ctextsc%7BAdamG%7D%20is%20our%20golden%20step%20size%20derived%20for%20the%0AAdaGrad-Norm%20algorithm%2C%20which%20is%20expected%20to%20help%20AdaGrad-Norm%20preserve%20the%0Atuning-free%20convergence%20and%20approximate%20the%20optimal%20step%20size%20in%20expectation%0Aw.r.t.%20various%20optimization%20scenarios.%20To%20better%20evaluate%20tuning-free%0Aperformance%2C%20we%20propose%20a%20novel%20evaluation%20criterion%2C%20%5Ctextit%7Breliability%7D%2C%20to%0Acomprehensively%20assess%20the%20efficacy%20of%20parameter-free%20optimizers%20in%20addition%20to%0Aclassical%20performance%20criteria.%20Empirical%20results%20demonstrate%20that%20compared%0Awith%20other%20parameter-free%20baselines%2C%20%5Ctextsc%7BAdamG%7D%20achieves%20superior%0Aperformance%2C%20which%20is%20consistently%20on%20par%20with%20Adam%20using%20a%20manually%20tuned%0Alearning%20rate%20across%20various%20optimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04376v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Stability%2520of%2520Parameter-free%2520Optimization%26entry.906535625%3DYijiang%2520Pang%2520and%2520Shuyang%2520Yu%2520and%2520Bao%2520Hoang%2520and%2520Jiayu%2520Zhou%26entry.1292438233%3D%2520%2520Hyperparameter%2520tuning%252C%2520particularly%2520the%2520selection%2520of%2520an%2520appropriate%2520learning%250Arate%2520in%2520adaptive%2520gradient%2520training%2520methods%252C%2520remains%2520a%2520challenge.%2520To%2520tackle%2520this%250Achallenge%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520parameter-free%2520optimizer%252C%250A%255Ctextsc%257BAdamG%257D%2520%2528Adam%2520with%2520the%2520golden%2520step%2520size%2529%252C%2520designed%2520to%2520automatically%250Aadapt%2520to%2520diverse%2520optimization%2520problems%2520without%2520manual%2520tuning.%2520The%2520core%250Atechnique%2520underlying%2520%255Ctextsc%257BAdamG%257D%2520is%2520our%2520golden%2520step%2520size%2520derived%2520for%2520the%250AAdaGrad-Norm%2520algorithm%252C%2520which%2520is%2520expected%2520to%2520help%2520AdaGrad-Norm%2520preserve%2520the%250Atuning-free%2520convergence%2520and%2520approximate%2520the%2520optimal%2520step%2520size%2520in%2520expectation%250Aw.r.t.%2520various%2520optimization%2520scenarios.%2520To%2520better%2520evaluate%2520tuning-free%250Aperformance%252C%2520we%2520propose%2520a%2520novel%2520evaluation%2520criterion%252C%2520%255Ctextit%257Breliability%257D%252C%2520to%250Acomprehensively%2520assess%2520the%2520efficacy%2520of%2520parameter-free%2520optimizers%2520in%2520addition%2520to%250Aclassical%2520performance%2520criteria.%2520Empirical%2520results%2520demonstrate%2520that%2520compared%250Awith%2520other%2520parameter-free%2520baselines%252C%2520%255Ctextsc%257BAdamG%257D%2520achieves%2520superior%250Aperformance%252C%2520which%2520is%2520consistently%2520on%2520par%2520with%2520Adam%2520using%2520a%2520manually%2520tuned%250Alearning%2520rate%2520across%2520various%2520optimization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04376v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Stability%20of%20Parameter-free%20Optimization&entry.906535625=Yijiang%20Pang%20and%20Shuyang%20Yu%20and%20Bao%20Hoang%20and%20Jiayu%20Zhou&entry.1292438233=%20%20Hyperparameter%20tuning%2C%20particularly%20the%20selection%20of%20an%20appropriate%20learning%0Arate%20in%20adaptive%20gradient%20training%20methods%2C%20remains%20a%20challenge.%20To%20tackle%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20parameter-free%20optimizer%2C%0A%5Ctextsc%7BAdamG%7D%20%28Adam%20with%20the%20golden%20step%20size%29%2C%20designed%20to%20automatically%0Aadapt%20to%20diverse%20optimization%20problems%20without%20manual%20tuning.%20The%20core%0Atechnique%20underlying%20%5Ctextsc%7BAdamG%7D%20is%20our%20golden%20step%20size%20derived%20for%20the%0AAdaGrad-Norm%20algorithm%2C%20which%20is%20expected%20to%20help%20AdaGrad-Norm%20preserve%20the%0Atuning-free%20convergence%20and%20approximate%20the%20optimal%20step%20size%20in%20expectation%0Aw.r.t.%20various%20optimization%20scenarios.%20To%20better%20evaluate%20tuning-free%0Aperformance%2C%20we%20propose%20a%20novel%20evaluation%20criterion%2C%20%5Ctextit%7Breliability%7D%2C%20to%0Acomprehensively%20assess%20the%20efficacy%20of%20parameter-free%20optimizers%20in%20addition%20to%0Aclassical%20performance%20criteria.%20Empirical%20results%20demonstrate%20that%20compared%0Awith%20other%20parameter-free%20baselines%2C%20%5Ctextsc%7BAdamG%7D%20achieves%20superior%0Aperformance%2C%20which%20is%20consistently%20on%20par%20with%20Adam%20using%20a%20manually%20tuned%0Alearning%20rate%20across%20various%20optimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04376v3&entry.124074799=Read"},
{"title": "Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection", "author": "Shuai Zeng and Wenzhao Zheng and Jiwen Lu and Haibin Yan", "abstract": "  3D object detection aims to recover the 3D information of concerning objects\nand serves as the fundamental task of autonomous driving perception. Its\nperformance greatly depends on the scale of labeled training data, yet it is\ncostly to obtain high-quality annotations for point cloud data. While\nconventional methods focus on generating pseudo-labels for unlabeled samples as\nsupplements for training, the structural nature of 3D point cloud data\nfacilitates the composition of objects and backgrounds to synthesize realistic\nscenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS)\nmethod to generate adaptive synthetic scenes to improve the generalization of\nthe detection models. We obtain pseudo-labels for unlabeled objects and\ngenerate diverse scenes with different compositions of objects and backgrounds.\nAs the scene synthesis is sensitive to the quality of pseudo-labels, we further\npropose a hardness-aware strategy to reduce the effect of low-quality\npseudo-labels and maintain a dynamic pseudo-database to ensure the diversity\nand quality of synthetic scenes. Extensive experimental results on the widely\nused KITTI and Waymo datasets demonstrate the superiority of the proposed HASS\nmethod, which outperforms existing semi-supervised learning methods on 3D\nobject detection. Code: https://github.com/wzzheng/HASS.\n", "link": "http://arxiv.org/abs/2405.17422v1", "date": "2024-05-27", "relevancy": 2.2225, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5735}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5726}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hardness-Aware%20Scene%20Synthesis%20for%20Semi-Supervised%203D%20Object%20Detection&body=Title%3A%20Hardness-Aware%20Scene%20Synthesis%20for%20Semi-Supervised%203D%20Object%20Detection%0AAuthor%3A%20Shuai%20Zeng%20and%20Wenzhao%20Zheng%20and%20Jiwen%20Lu%20and%20Haibin%20Yan%0AAbstract%3A%20%20%203D%20object%20detection%20aims%20to%20recover%20the%203D%20information%20of%20concerning%20objects%0Aand%20serves%20as%20the%20fundamental%20task%20of%20autonomous%20driving%20perception.%20Its%0Aperformance%20greatly%20depends%20on%20the%20scale%20of%20labeled%20training%20data%2C%20yet%20it%20is%0Acostly%20to%20obtain%20high-quality%20annotations%20for%20point%20cloud%20data.%20While%0Aconventional%20methods%20focus%20on%20generating%20pseudo-labels%20for%20unlabeled%20samples%20as%0Asupplements%20for%20training%2C%20the%20structural%20nature%20of%203D%20point%20cloud%20data%0Afacilitates%20the%20composition%20of%20objects%20and%20backgrounds%20to%20synthesize%20realistic%0Ascenes.%20Motivated%20by%20this%2C%20we%20propose%20a%20hardness-aware%20scene%20synthesis%20%28HASS%29%0Amethod%20to%20generate%20adaptive%20synthetic%20scenes%20to%20improve%20the%20generalization%20of%0Athe%20detection%20models.%20We%20obtain%20pseudo-labels%20for%20unlabeled%20objects%20and%0Agenerate%20diverse%20scenes%20with%20different%20compositions%20of%20objects%20and%20backgrounds.%0AAs%20the%20scene%20synthesis%20is%20sensitive%20to%20the%20quality%20of%20pseudo-labels%2C%20we%20further%0Apropose%20a%20hardness-aware%20strategy%20to%20reduce%20the%20effect%20of%20low-quality%0Apseudo-labels%20and%20maintain%20a%20dynamic%20pseudo-database%20to%20ensure%20the%20diversity%0Aand%20quality%20of%20synthetic%20scenes.%20Extensive%20experimental%20results%20on%20the%20widely%0Aused%20KITTI%20and%20Waymo%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20HASS%0Amethod%2C%20which%20outperforms%20existing%20semi-supervised%20learning%20methods%20on%203D%0Aobject%20detection.%20Code%3A%20https%3A//github.com/wzzheng/HASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHardness-Aware%2520Scene%2520Synthesis%2520for%2520Semi-Supervised%25203D%2520Object%2520Detection%26entry.906535625%3DShuai%2520Zeng%2520and%2520Wenzhao%2520Zheng%2520and%2520Jiwen%2520Lu%2520and%2520Haibin%2520Yan%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520aims%2520to%2520recover%2520the%25203D%2520information%2520of%2520concerning%2520objects%250Aand%2520serves%2520as%2520the%2520fundamental%2520task%2520of%2520autonomous%2520driving%2520perception.%2520Its%250Aperformance%2520greatly%2520depends%2520on%2520the%2520scale%2520of%2520labeled%2520training%2520data%252C%2520yet%2520it%2520is%250Acostly%2520to%2520obtain%2520high-quality%2520annotations%2520for%2520point%2520cloud%2520data.%2520While%250Aconventional%2520methods%2520focus%2520on%2520generating%2520pseudo-labels%2520for%2520unlabeled%2520samples%2520as%250Asupplements%2520for%2520training%252C%2520the%2520structural%2520nature%2520of%25203D%2520point%2520cloud%2520data%250Afacilitates%2520the%2520composition%2520of%2520objects%2520and%2520backgrounds%2520to%2520synthesize%2520realistic%250Ascenes.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520hardness-aware%2520scene%2520synthesis%2520%2528HASS%2529%250Amethod%2520to%2520generate%2520adaptive%2520synthetic%2520scenes%2520to%2520improve%2520the%2520generalization%2520of%250Athe%2520detection%2520models.%2520We%2520obtain%2520pseudo-labels%2520for%2520unlabeled%2520objects%2520and%250Agenerate%2520diverse%2520scenes%2520with%2520different%2520compositions%2520of%2520objects%2520and%2520backgrounds.%250AAs%2520the%2520scene%2520synthesis%2520is%2520sensitive%2520to%2520the%2520quality%2520of%2520pseudo-labels%252C%2520we%2520further%250Apropose%2520a%2520hardness-aware%2520strategy%2520to%2520reduce%2520the%2520effect%2520of%2520low-quality%250Apseudo-labels%2520and%2520maintain%2520a%2520dynamic%2520pseudo-database%2520to%2520ensure%2520the%2520diversity%250Aand%2520quality%2520of%2520synthetic%2520scenes.%2520Extensive%2520experimental%2520results%2520on%2520the%2520widely%250Aused%2520KITTI%2520and%2520Waymo%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520HASS%250Amethod%252C%2520which%2520outperforms%2520existing%2520semi-supervised%2520learning%2520methods%2520on%25203D%250Aobject%2520detection.%2520Code%253A%2520https%253A//github.com/wzzheng/HASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hardness-Aware%20Scene%20Synthesis%20for%20Semi-Supervised%203D%20Object%20Detection&entry.906535625=Shuai%20Zeng%20and%20Wenzhao%20Zheng%20and%20Jiwen%20Lu%20and%20Haibin%20Yan&entry.1292438233=%20%203D%20object%20detection%20aims%20to%20recover%20the%203D%20information%20of%20concerning%20objects%0Aand%20serves%20as%20the%20fundamental%20task%20of%20autonomous%20driving%20perception.%20Its%0Aperformance%20greatly%20depends%20on%20the%20scale%20of%20labeled%20training%20data%2C%20yet%20it%20is%0Acostly%20to%20obtain%20high-quality%20annotations%20for%20point%20cloud%20data.%20While%0Aconventional%20methods%20focus%20on%20generating%20pseudo-labels%20for%20unlabeled%20samples%20as%0Asupplements%20for%20training%2C%20the%20structural%20nature%20of%203D%20point%20cloud%20data%0Afacilitates%20the%20composition%20of%20objects%20and%20backgrounds%20to%20synthesize%20realistic%0Ascenes.%20Motivated%20by%20this%2C%20we%20propose%20a%20hardness-aware%20scene%20synthesis%20%28HASS%29%0Amethod%20to%20generate%20adaptive%20synthetic%20scenes%20to%20improve%20the%20generalization%20of%0Athe%20detection%20models.%20We%20obtain%20pseudo-labels%20for%20unlabeled%20objects%20and%0Agenerate%20diverse%20scenes%20with%20different%20compositions%20of%20objects%20and%20backgrounds.%0AAs%20the%20scene%20synthesis%20is%20sensitive%20to%20the%20quality%20of%20pseudo-labels%2C%20we%20further%0Apropose%20a%20hardness-aware%20strategy%20to%20reduce%20the%20effect%20of%20low-quality%0Apseudo-labels%20and%20maintain%20a%20dynamic%20pseudo-database%20to%20ensure%20the%20diversity%0Aand%20quality%20of%20synthetic%20scenes.%20Extensive%20experimental%20results%20on%20the%20widely%0Aused%20KITTI%20and%20Waymo%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20HASS%0Amethod%2C%20which%20outperforms%20existing%20semi-supervised%20learning%20methods%20on%203D%0Aobject%20detection.%20Code%3A%20https%3A//github.com/wzzheng/HASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17422v1&entry.124074799=Read"},
{"title": "Test-Time Adaptation for Depth Completion", "author": "Hyoungseob Park and Anjali Gupta and Alex Wong", "abstract": "  It is common to observe performance degradation when transferring models\ntrained on some (source) datasets to target testing data due to a domain gap\nbetween them. Existing methods for bridging this gap, such as domain adaptation\n(DA), may require the source data on which the model was trained (often not\navailable), while others, i.e., source-free DA, require many passes through the\ntesting data. We propose an online test-time adaptation method for depth\ncompletion, the task of inferring a dense depth map from a single image and\nassociated sparse depth map, that closes the performance gap in a single pass.\nWe first present a study on how the domain shift in each data modality affects\nmodel performance. Based on our observations that the sparse depth modality\nexhibits a much smaller covariate shift than the image, we design an embedding\nmodule trained in the source domain that preserves a mapping from features\nencoding only sparse depth to those encoding image and sparse depth. During\ntest time, sparse depth features are projected using this map as a proxy for\nsource domain features and are used as guidance to train a set of auxiliary\nparameters (i.e., adaptation layer) to align image and sparse depth features\nfrom the target test domain to that of the source domain. We evaluate our\nmethod on indoor and outdoor scenarios and show that it improves over baselines\nby an average of 21.1%.\n", "link": "http://arxiv.org/abs/2402.03312v4", "date": "2024-05-27", "relevancy": 2.2212, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5922}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Adaptation%20for%20Depth%20Completion&body=Title%3A%20Test-Time%20Adaptation%20for%20Depth%20Completion%0AAuthor%3A%20Hyoungseob%20Park%20and%20Anjali%20Gupta%20and%20Alex%20Wong%0AAbstract%3A%20%20%20It%20is%20common%20to%20observe%20performance%20degradation%20when%20transferring%20models%0Atrained%20on%20some%20%28source%29%20datasets%20to%20target%20testing%20data%20due%20to%20a%20domain%20gap%0Abetween%20them.%20Existing%20methods%20for%20bridging%20this%20gap%2C%20such%20as%20domain%20adaptation%0A%28DA%29%2C%20may%20require%20the%20source%20data%20on%20which%20the%20model%20was%20trained%20%28often%20not%0Aavailable%29%2C%20while%20others%2C%20i.e.%2C%20source-free%20DA%2C%20require%20many%20passes%20through%20the%0Atesting%20data.%20We%20propose%20an%20online%20test-time%20adaptation%20method%20for%20depth%0Acompletion%2C%20the%20task%20of%20inferring%20a%20dense%20depth%20map%20from%20a%20single%20image%20and%0Aassociated%20sparse%20depth%20map%2C%20that%20closes%20the%20performance%20gap%20in%20a%20single%20pass.%0AWe%20first%20present%20a%20study%20on%20how%20the%20domain%20shift%20in%20each%20data%20modality%20affects%0Amodel%20performance.%20Based%20on%20our%20observations%20that%20the%20sparse%20depth%20modality%0Aexhibits%20a%20much%20smaller%20covariate%20shift%20than%20the%20image%2C%20we%20design%20an%20embedding%0Amodule%20trained%20in%20the%20source%20domain%20that%20preserves%20a%20mapping%20from%20features%0Aencoding%20only%20sparse%20depth%20to%20those%20encoding%20image%20and%20sparse%20depth.%20During%0Atest%20time%2C%20sparse%20depth%20features%20are%20projected%20using%20this%20map%20as%20a%20proxy%20for%0Asource%20domain%20features%20and%20are%20used%20as%20guidance%20to%20train%20a%20set%20of%20auxiliary%0Aparameters%20%28i.e.%2C%20adaptation%20layer%29%20to%20align%20image%20and%20sparse%20depth%20features%0Afrom%20the%20target%20test%20domain%20to%20that%20of%20the%20source%20domain.%20We%20evaluate%20our%0Amethod%20on%20indoor%20and%20outdoor%20scenarios%20and%20show%20that%20it%20improves%20over%20baselines%0Aby%20an%20average%20of%2021.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03312v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Adaptation%2520for%2520Depth%2520Completion%26entry.906535625%3DHyoungseob%2520Park%2520and%2520Anjali%2520Gupta%2520and%2520Alex%2520Wong%26entry.1292438233%3D%2520%2520It%2520is%2520common%2520to%2520observe%2520performance%2520degradation%2520when%2520transferring%2520models%250Atrained%2520on%2520some%2520%2528source%2529%2520datasets%2520to%2520target%2520testing%2520data%2520due%2520to%2520a%2520domain%2520gap%250Abetween%2520them.%2520Existing%2520methods%2520for%2520bridging%2520this%2520gap%252C%2520such%2520as%2520domain%2520adaptation%250A%2528DA%2529%252C%2520may%2520require%2520the%2520source%2520data%2520on%2520which%2520the%2520model%2520was%2520trained%2520%2528often%2520not%250Aavailable%2529%252C%2520while%2520others%252C%2520i.e.%252C%2520source-free%2520DA%252C%2520require%2520many%2520passes%2520through%2520the%250Atesting%2520data.%2520We%2520propose%2520an%2520online%2520test-time%2520adaptation%2520method%2520for%2520depth%250Acompletion%252C%2520the%2520task%2520of%2520inferring%2520a%2520dense%2520depth%2520map%2520from%2520a%2520single%2520image%2520and%250Aassociated%2520sparse%2520depth%2520map%252C%2520that%2520closes%2520the%2520performance%2520gap%2520in%2520a%2520single%2520pass.%250AWe%2520first%2520present%2520a%2520study%2520on%2520how%2520the%2520domain%2520shift%2520in%2520each%2520data%2520modality%2520affects%250Amodel%2520performance.%2520Based%2520on%2520our%2520observations%2520that%2520the%2520sparse%2520depth%2520modality%250Aexhibits%2520a%2520much%2520smaller%2520covariate%2520shift%2520than%2520the%2520image%252C%2520we%2520design%2520an%2520embedding%250Amodule%2520trained%2520in%2520the%2520source%2520domain%2520that%2520preserves%2520a%2520mapping%2520from%2520features%250Aencoding%2520only%2520sparse%2520depth%2520to%2520those%2520encoding%2520image%2520and%2520sparse%2520depth.%2520During%250Atest%2520time%252C%2520sparse%2520depth%2520features%2520are%2520projected%2520using%2520this%2520map%2520as%2520a%2520proxy%2520for%250Asource%2520domain%2520features%2520and%2520are%2520used%2520as%2520guidance%2520to%2520train%2520a%2520set%2520of%2520auxiliary%250Aparameters%2520%2528i.e.%252C%2520adaptation%2520layer%2529%2520to%2520align%2520image%2520and%2520sparse%2520depth%2520features%250Afrom%2520the%2520target%2520test%2520domain%2520to%2520that%2520of%2520the%2520source%2520domain.%2520We%2520evaluate%2520our%250Amethod%2520on%2520indoor%2520and%2520outdoor%2520scenarios%2520and%2520show%2520that%2520it%2520improves%2520over%2520baselines%250Aby%2520an%2520average%2520of%252021.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03312v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Adaptation%20for%20Depth%20Completion&entry.906535625=Hyoungseob%20Park%20and%20Anjali%20Gupta%20and%20Alex%20Wong&entry.1292438233=%20%20It%20is%20common%20to%20observe%20performance%20degradation%20when%20transferring%20models%0Atrained%20on%20some%20%28source%29%20datasets%20to%20target%20testing%20data%20due%20to%20a%20domain%20gap%0Abetween%20them.%20Existing%20methods%20for%20bridging%20this%20gap%2C%20such%20as%20domain%20adaptation%0A%28DA%29%2C%20may%20require%20the%20source%20data%20on%20which%20the%20model%20was%20trained%20%28often%20not%0Aavailable%29%2C%20while%20others%2C%20i.e.%2C%20source-free%20DA%2C%20require%20many%20passes%20through%20the%0Atesting%20data.%20We%20propose%20an%20online%20test-time%20adaptation%20method%20for%20depth%0Acompletion%2C%20the%20task%20of%20inferring%20a%20dense%20depth%20map%20from%20a%20single%20image%20and%0Aassociated%20sparse%20depth%20map%2C%20that%20closes%20the%20performance%20gap%20in%20a%20single%20pass.%0AWe%20first%20present%20a%20study%20on%20how%20the%20domain%20shift%20in%20each%20data%20modality%20affects%0Amodel%20performance.%20Based%20on%20our%20observations%20that%20the%20sparse%20depth%20modality%0Aexhibits%20a%20much%20smaller%20covariate%20shift%20than%20the%20image%2C%20we%20design%20an%20embedding%0Amodule%20trained%20in%20the%20source%20domain%20that%20preserves%20a%20mapping%20from%20features%0Aencoding%20only%20sparse%20depth%20to%20those%20encoding%20image%20and%20sparse%20depth.%20During%0Atest%20time%2C%20sparse%20depth%20features%20are%20projected%20using%20this%20map%20as%20a%20proxy%20for%0Asource%20domain%20features%20and%20are%20used%20as%20guidance%20to%20train%20a%20set%20of%20auxiliary%0Aparameters%20%28i.e.%2C%20adaptation%20layer%29%20to%20align%20image%20and%20sparse%20depth%20features%0Afrom%20the%20target%20test%20domain%20to%20that%20of%20the%20source%20domain.%20We%20evaluate%20our%0Amethod%20on%20indoor%20and%20outdoor%20scenarios%20and%20show%20that%20it%20improves%20over%20baselines%0Aby%20an%20average%20of%2021.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03312v4&entry.124074799=Read"},
{"title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with\n  Next-Patch Prediction", "author": "Zikang Zhou and Haibo Hu and Xinhong Chen and Jianping Wang and Nan Guan and Kui Wu and Yung-Hui Li and Yu-Kai Huang and Chun Jason Xue", "abstract": "  Simulating realistic interactions among traffic agents is crucial for\nefficiently validating the safety of autonomous driving systems. Existing\nleading simulators primarily use an encoder-decoder structure to encode the\nhistorical trajectories for future simulation. However, such a paradigm\ncomplicates the model architecture, and the manual separation of history and\nfuture trajectories leads to low data utilization. To address these challenges,\nwe propose Behavior Generative Pre-trained Transformers (BehaviorGPT), a\ndecoder-only, autoregressive architecture designed to simulate the sequential\nmotion of multiple agents. Crucially, our approach discards the traditional\nseparation between \"history\" and \"future,\" treating each time step as the\n\"current\" one, resulting in a simpler, more parameter- and data-efficient\ndesign that scales seamlessly with data and computation. Additionally, we\nintroduce the Next-Patch Prediction Paradigm (NP3), which enables models to\nreason at the patch level of trajectories and capture long-range\nspatial-temporal interactions. BehaviorGPT ranks first across several metrics\non the Waymo Sim Agents Benchmark, demonstrating its exceptional performance in\nmulti-agent and agent-map interactions. We outperformed state-of-the-art models\nwith a realism score of 0.741 and improved the minADE metric to 1.540, with an\napproximately 91.6% reduction in model parameters.\n", "link": "http://arxiv.org/abs/2405.17372v1", "date": "2024-05-27", "relevancy": 2.2192, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5835}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5617}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction&body=Title%3A%20BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction%0AAuthor%3A%20Zikang%20Zhou%20and%20Haibo%20Hu%20and%20Xinhong%20Chen%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Kui%20Wu%20and%20Yung-Hui%20Li%20and%20Yu-Kai%20Huang%20and%20Chun%20Jason%20Xue%0AAbstract%3A%20%20%20Simulating%20realistic%20interactions%20among%20traffic%20agents%20is%20crucial%20for%0Aefficiently%20validating%20the%20safety%20of%20autonomous%20driving%20systems.%20Existing%0Aleading%20simulators%20primarily%20use%20an%20encoder-decoder%20structure%20to%20encode%20the%0Ahistorical%20trajectories%20for%20future%20simulation.%20However%2C%20such%20a%20paradigm%0Acomplicates%20the%20model%20architecture%2C%20and%20the%20manual%20separation%20of%20history%20and%0Afuture%20trajectories%20leads%20to%20low%20data%20utilization.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Behavior%20Generative%20Pre-trained%20Transformers%20%28BehaviorGPT%29%2C%20a%0Adecoder-only%2C%20autoregressive%20architecture%20designed%20to%20simulate%20the%20sequential%0Amotion%20of%20multiple%20agents.%20Crucially%2C%20our%20approach%20discards%20the%20traditional%0Aseparation%20between%20%22history%22%20and%20%22future%2C%22%20treating%20each%20time%20step%20as%20the%0A%22current%22%20one%2C%20resulting%20in%20a%20simpler%2C%20more%20parameter-%20and%20data-efficient%0Adesign%20that%20scales%20seamlessly%20with%20data%20and%20computation.%20Additionally%2C%20we%0Aintroduce%20the%20Next-Patch%20Prediction%20Paradigm%20%28NP3%29%2C%20which%20enables%20models%20to%0Areason%20at%20the%20patch%20level%20of%20trajectories%20and%20capture%20long-range%0Aspatial-temporal%20interactions.%20BehaviorGPT%20ranks%20first%20across%20several%20metrics%0Aon%20the%20Waymo%20Sim%20Agents%20Benchmark%2C%20demonstrating%20its%20exceptional%20performance%20in%0Amulti-agent%20and%20agent-map%20interactions.%20We%20outperformed%20state-of-the-art%20models%0Awith%20a%20realism%20score%20of%200.741%20and%20improved%20the%20minADE%20metric%20to%201.540%2C%20with%20an%0Aapproximately%2091.6%25%20reduction%20in%20model%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehaviorGPT%253A%2520Smart%2520Agent%2520Simulation%2520for%2520Autonomous%2520Driving%2520with%250A%2520%2520Next-Patch%2520Prediction%26entry.906535625%3DZikang%2520Zhou%2520and%2520Haibo%2520Hu%2520and%2520Xinhong%2520Chen%2520and%2520Jianping%2520Wang%2520and%2520Nan%2520Guan%2520and%2520Kui%2520Wu%2520and%2520Yung-Hui%2520Li%2520and%2520Yu-Kai%2520Huang%2520and%2520Chun%2520Jason%2520Xue%26entry.1292438233%3D%2520%2520Simulating%2520realistic%2520interactions%2520among%2520traffic%2520agents%2520is%2520crucial%2520for%250Aefficiently%2520validating%2520the%2520safety%2520of%2520autonomous%2520driving%2520systems.%2520Existing%250Aleading%2520simulators%2520primarily%2520use%2520an%2520encoder-decoder%2520structure%2520to%2520encode%2520the%250Ahistorical%2520trajectories%2520for%2520future%2520simulation.%2520However%252C%2520such%2520a%2520paradigm%250Acomplicates%2520the%2520model%2520architecture%252C%2520and%2520the%2520manual%2520separation%2520of%2520history%2520and%250Afuture%2520trajectories%2520leads%2520to%2520low%2520data%2520utilization.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520Behavior%2520Generative%2520Pre-trained%2520Transformers%2520%2528BehaviorGPT%2529%252C%2520a%250Adecoder-only%252C%2520autoregressive%2520architecture%2520designed%2520to%2520simulate%2520the%2520sequential%250Amotion%2520of%2520multiple%2520agents.%2520Crucially%252C%2520our%2520approach%2520discards%2520the%2520traditional%250Aseparation%2520between%2520%2522history%2522%2520and%2520%2522future%252C%2522%2520treating%2520each%2520time%2520step%2520as%2520the%250A%2522current%2522%2520one%252C%2520resulting%2520in%2520a%2520simpler%252C%2520more%2520parameter-%2520and%2520data-efficient%250Adesign%2520that%2520scales%2520seamlessly%2520with%2520data%2520and%2520computation.%2520Additionally%252C%2520we%250Aintroduce%2520the%2520Next-Patch%2520Prediction%2520Paradigm%2520%2528NP3%2529%252C%2520which%2520enables%2520models%2520to%250Areason%2520at%2520the%2520patch%2520level%2520of%2520trajectories%2520and%2520capture%2520long-range%250Aspatial-temporal%2520interactions.%2520BehaviorGPT%2520ranks%2520first%2520across%2520several%2520metrics%250Aon%2520the%2520Waymo%2520Sim%2520Agents%2520Benchmark%252C%2520demonstrating%2520its%2520exceptional%2520performance%2520in%250Amulti-agent%2520and%2520agent-map%2520interactions.%2520We%2520outperformed%2520state-of-the-art%2520models%250Awith%2520a%2520realism%2520score%2520of%25200.741%2520and%2520improved%2520the%2520minADE%2520metric%2520to%25201.540%252C%2520with%2520an%250Aapproximately%252091.6%2525%2520reduction%2520in%2520model%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BehaviorGPT%3A%20Smart%20Agent%20Simulation%20for%20Autonomous%20Driving%20with%0A%20%20Next-Patch%20Prediction&entry.906535625=Zikang%20Zhou%20and%20Haibo%20Hu%20and%20Xinhong%20Chen%20and%20Jianping%20Wang%20and%20Nan%20Guan%20and%20Kui%20Wu%20and%20Yung-Hui%20Li%20and%20Yu-Kai%20Huang%20and%20Chun%20Jason%20Xue&entry.1292438233=%20%20Simulating%20realistic%20interactions%20among%20traffic%20agents%20is%20crucial%20for%0Aefficiently%20validating%20the%20safety%20of%20autonomous%20driving%20systems.%20Existing%0Aleading%20simulators%20primarily%20use%20an%20encoder-decoder%20structure%20to%20encode%20the%0Ahistorical%20trajectories%20for%20future%20simulation.%20However%2C%20such%20a%20paradigm%0Acomplicates%20the%20model%20architecture%2C%20and%20the%20manual%20separation%20of%20history%20and%0Afuture%20trajectories%20leads%20to%20low%20data%20utilization.%20To%20address%20these%20challenges%2C%0Awe%20propose%20Behavior%20Generative%20Pre-trained%20Transformers%20%28BehaviorGPT%29%2C%20a%0Adecoder-only%2C%20autoregressive%20architecture%20designed%20to%20simulate%20the%20sequential%0Amotion%20of%20multiple%20agents.%20Crucially%2C%20our%20approach%20discards%20the%20traditional%0Aseparation%20between%20%22history%22%20and%20%22future%2C%22%20treating%20each%20time%20step%20as%20the%0A%22current%22%20one%2C%20resulting%20in%20a%20simpler%2C%20more%20parameter-%20and%20data-efficient%0Adesign%20that%20scales%20seamlessly%20with%20data%20and%20computation.%20Additionally%2C%20we%0Aintroduce%20the%20Next-Patch%20Prediction%20Paradigm%20%28NP3%29%2C%20which%20enables%20models%20to%0Areason%20at%20the%20patch%20level%20of%20trajectories%20and%20capture%20long-range%0Aspatial-temporal%20interactions.%20BehaviorGPT%20ranks%20first%20across%20several%20metrics%0Aon%20the%20Waymo%20Sim%20Agents%20Benchmark%2C%20demonstrating%20its%20exceptional%20performance%20in%0Amulti-agent%20and%20agent-map%20interactions.%20We%20outperformed%20state-of-the-art%20models%0Awith%20a%20realism%20score%20of%200.741%20and%20improved%20the%20minADE%20metric%20to%201.540%2C%20with%20an%0Aapproximately%2091.6%25%20reduction%20in%20model%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17372v1&entry.124074799=Read"},
{"title": "Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic\n  Space", "author": "Sheng Yang and Peihan Liu and Cengiz Pehlevan", "abstract": "  Hyperbolic spaces have increasingly been recognized for their outstanding\nperformance in handling data with inherent hierarchical structures compared to\ntheir Euclidean counterparts. However, learning in hyperbolic spaces poses\nsignificant challenges. In particular, extending support vector machines to\nhyperbolic spaces is in general a constrained non-convex optimization problem.\nPrevious and popular attempts to solve hyperbolic SVMs, primarily using\nprojected gradient descent, are generally sensitive to hyperparameters and\ninitializations, often leading to suboptimal solutions. In this work, by first\nrewriting the problem into a polynomial optimization, we apply semidefinite\nrelaxation and sparse moment-sum-of-squares relaxation to effectively\napproximate the optima. From extensive empirical experiments, these methods are\nshown to perform better than the projected gradient descent approach.\n", "link": "http://arxiv.org/abs/2405.17198v1", "date": "2024-05-27", "relevancy": 2.2185, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.446}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4431}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convex%20Relaxation%20for%20Solving%20Large-Margin%20Classifiers%20in%20Hyperbolic%0A%20%20Space&body=Title%3A%20Convex%20Relaxation%20for%20Solving%20Large-Margin%20Classifiers%20in%20Hyperbolic%0A%20%20Space%0AAuthor%3A%20Sheng%20Yang%20and%20Peihan%20Liu%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20Hyperbolic%20spaces%20have%20increasingly%20been%20recognized%20for%20their%20outstanding%0Aperformance%20in%20handling%20data%20with%20inherent%20hierarchical%20structures%20compared%20to%0Atheir%20Euclidean%20counterparts.%20However%2C%20learning%20in%20hyperbolic%20spaces%20poses%0Asignificant%20challenges.%20In%20particular%2C%20extending%20support%20vector%20machines%20to%0Ahyperbolic%20spaces%20is%20in%20general%20a%20constrained%20non-convex%20optimization%20problem.%0APrevious%20and%20popular%20attempts%20to%20solve%20hyperbolic%20SVMs%2C%20primarily%20using%0Aprojected%20gradient%20descent%2C%20are%20generally%20sensitive%20to%20hyperparameters%20and%0Ainitializations%2C%20often%20leading%20to%20suboptimal%20solutions.%20In%20this%20work%2C%20by%20first%0Arewriting%20the%20problem%20into%20a%20polynomial%20optimization%2C%20we%20apply%20semidefinite%0Arelaxation%20and%20sparse%20moment-sum-of-squares%20relaxation%20to%20effectively%0Aapproximate%20the%20optima.%20From%20extensive%20empirical%20experiments%2C%20these%20methods%20are%0Ashown%20to%20perform%20better%20than%20the%20projected%20gradient%20descent%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvex%2520Relaxation%2520for%2520Solving%2520Large-Margin%2520Classifiers%2520in%2520Hyperbolic%250A%2520%2520Space%26entry.906535625%3DSheng%2520Yang%2520and%2520Peihan%2520Liu%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520Hyperbolic%2520spaces%2520have%2520increasingly%2520been%2520recognized%2520for%2520their%2520outstanding%250Aperformance%2520in%2520handling%2520data%2520with%2520inherent%2520hierarchical%2520structures%2520compared%2520to%250Atheir%2520Euclidean%2520counterparts.%2520However%252C%2520learning%2520in%2520hyperbolic%2520spaces%2520poses%250Asignificant%2520challenges.%2520In%2520particular%252C%2520extending%2520support%2520vector%2520machines%2520to%250Ahyperbolic%2520spaces%2520is%2520in%2520general%2520a%2520constrained%2520non-convex%2520optimization%2520problem.%250APrevious%2520and%2520popular%2520attempts%2520to%2520solve%2520hyperbolic%2520SVMs%252C%2520primarily%2520using%250Aprojected%2520gradient%2520descent%252C%2520are%2520generally%2520sensitive%2520to%2520hyperparameters%2520and%250Ainitializations%252C%2520often%2520leading%2520to%2520suboptimal%2520solutions.%2520In%2520this%2520work%252C%2520by%2520first%250Arewriting%2520the%2520problem%2520into%2520a%2520polynomial%2520optimization%252C%2520we%2520apply%2520semidefinite%250Arelaxation%2520and%2520sparse%2520moment-sum-of-squares%2520relaxation%2520to%2520effectively%250Aapproximate%2520the%2520optima.%2520From%2520extensive%2520empirical%2520experiments%252C%2520these%2520methods%2520are%250Ashown%2520to%2520perform%2520better%2520than%2520the%2520projected%2520gradient%2520descent%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convex%20Relaxation%20for%20Solving%20Large-Margin%20Classifiers%20in%20Hyperbolic%0A%20%20Space&entry.906535625=Sheng%20Yang%20and%20Peihan%20Liu%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20Hyperbolic%20spaces%20have%20increasingly%20been%20recognized%20for%20their%20outstanding%0Aperformance%20in%20handling%20data%20with%20inherent%20hierarchical%20structures%20compared%20to%0Atheir%20Euclidean%20counterparts.%20However%2C%20learning%20in%20hyperbolic%20spaces%20poses%0Asignificant%20challenges.%20In%20particular%2C%20extending%20support%20vector%20machines%20to%0Ahyperbolic%20spaces%20is%20in%20general%20a%20constrained%20non-convex%20optimization%20problem.%0APrevious%20and%20popular%20attempts%20to%20solve%20hyperbolic%20SVMs%2C%20primarily%20using%0Aprojected%20gradient%20descent%2C%20are%20generally%20sensitive%20to%20hyperparameters%20and%0Ainitializations%2C%20often%20leading%20to%20suboptimal%20solutions.%20In%20this%20work%2C%20by%20first%0Arewriting%20the%20problem%20into%20a%20polynomial%20optimization%2C%20we%20apply%20semidefinite%0Arelaxation%20and%20sparse%20moment-sum-of-squares%20relaxation%20to%20effectively%0Aapproximate%20the%20optima.%20From%20extensive%20empirical%20experiments%2C%20these%20methods%20are%0Ashown%20to%20perform%20better%20than%20the%20projected%20gradient%20descent%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17198v1&entry.124074799=Read"},
{"title": "Tilt your Head: Activating the Hidden Spatial-Invariance of Classifiers", "author": "Johann Schmidt and Sebastian Stober", "abstract": "  Deep neural networks are applied in more and more areas of everyday life.\nHowever, they still lack essential abilities, such as robustly dealing with\nspatially transformed input signals. Approaches to mitigate this severe\nrobustness issue are limited to two pathways: Either models are implicitly\nregularised by increased sample variability (data augmentation) or explicitly\nconstrained by hard-coded inductive biases. The limiting factor of the former\nis the size of the data space, which renders sufficient sample coverage\nintractable. The latter is limited by the engineering effort required to\ndevelop such inductive biases for every possible scenario. Instead, we take\ninspiration from human behaviour, where percepts are modified by mental or\nphysical actions during inference. We propose a novel technique to emulate such\nan inference process for neural nets. This is achieved by traversing a\nsparsified inverse transformation tree during inference using parallel\nenergy-based evaluations. Our proposed inference algorithm, called Inverse\nTransformation Search (ITS), is model-agnostic and equips the model with\nzero-shot pseudo-invariance to spatially transformed inputs. We evaluated our\nmethod on several benchmark datasets, including a synthesised ImageNet test\nset. ITS outperforms the utilised baselines on all zero-shot test scenarios.\n", "link": "http://arxiv.org/abs/2405.03730v2", "date": "2024-05-27", "relevancy": 2.2184, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.557}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tilt%20your%20Head%3A%20Activating%20the%20Hidden%20Spatial-Invariance%20of%20Classifiers&body=Title%3A%20Tilt%20your%20Head%3A%20Activating%20the%20Hidden%20Spatial-Invariance%20of%20Classifiers%0AAuthor%3A%20Johann%20Schmidt%20and%20Sebastian%20Stober%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20applied%20in%20more%20and%20more%20areas%20of%20everyday%20life.%0AHowever%2C%20they%20still%20lack%20essential%20abilities%2C%20such%20as%20robustly%20dealing%20with%0Aspatially%20transformed%20input%20signals.%20Approaches%20to%20mitigate%20this%20severe%0Arobustness%20issue%20are%20limited%20to%20two%20pathways%3A%20Either%20models%20are%20implicitly%0Aregularised%20by%20increased%20sample%20variability%20%28data%20augmentation%29%20or%20explicitly%0Aconstrained%20by%20hard-coded%20inductive%20biases.%20The%20limiting%20factor%20of%20the%20former%0Ais%20the%20size%20of%20the%20data%20space%2C%20which%20renders%20sufficient%20sample%20coverage%0Aintractable.%20The%20latter%20is%20limited%20by%20the%20engineering%20effort%20required%20to%0Adevelop%20such%20inductive%20biases%20for%20every%20possible%20scenario.%20Instead%2C%20we%20take%0Ainspiration%20from%20human%20behaviour%2C%20where%20percepts%20are%20modified%20by%20mental%20or%0Aphysical%20actions%20during%20inference.%20We%20propose%20a%20novel%20technique%20to%20emulate%20such%0Aan%20inference%20process%20for%20neural%20nets.%20This%20is%20achieved%20by%20traversing%20a%0Asparsified%20inverse%20transformation%20tree%20during%20inference%20using%20parallel%0Aenergy-based%20evaluations.%20Our%20proposed%20inference%20algorithm%2C%20called%20Inverse%0ATransformation%20Search%20%28ITS%29%2C%20is%20model-agnostic%20and%20equips%20the%20model%20with%0Azero-shot%20pseudo-invariance%20to%20spatially%20transformed%20inputs.%20We%20evaluated%20our%0Amethod%20on%20several%20benchmark%20datasets%2C%20including%20a%20synthesised%20ImageNet%20test%0Aset.%20ITS%20outperforms%20the%20utilised%20baselines%20on%20all%20zero-shot%20test%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTilt%2520your%2520Head%253A%2520Activating%2520the%2520Hidden%2520Spatial-Invariance%2520of%2520Classifiers%26entry.906535625%3DJohann%2520Schmidt%2520and%2520Sebastian%2520Stober%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520applied%2520in%2520more%2520and%2520more%2520areas%2520of%2520everyday%2520life.%250AHowever%252C%2520they%2520still%2520lack%2520essential%2520abilities%252C%2520such%2520as%2520robustly%2520dealing%2520with%250Aspatially%2520transformed%2520input%2520signals.%2520Approaches%2520to%2520mitigate%2520this%2520severe%250Arobustness%2520issue%2520are%2520limited%2520to%2520two%2520pathways%253A%2520Either%2520models%2520are%2520implicitly%250Aregularised%2520by%2520increased%2520sample%2520variability%2520%2528data%2520augmentation%2529%2520or%2520explicitly%250Aconstrained%2520by%2520hard-coded%2520inductive%2520biases.%2520The%2520limiting%2520factor%2520of%2520the%2520former%250Ais%2520the%2520size%2520of%2520the%2520data%2520space%252C%2520which%2520renders%2520sufficient%2520sample%2520coverage%250Aintractable.%2520The%2520latter%2520is%2520limited%2520by%2520the%2520engineering%2520effort%2520required%2520to%250Adevelop%2520such%2520inductive%2520biases%2520for%2520every%2520possible%2520scenario.%2520Instead%252C%2520we%2520take%250Ainspiration%2520from%2520human%2520behaviour%252C%2520where%2520percepts%2520are%2520modified%2520by%2520mental%2520or%250Aphysical%2520actions%2520during%2520inference.%2520We%2520propose%2520a%2520novel%2520technique%2520to%2520emulate%2520such%250Aan%2520inference%2520process%2520for%2520neural%2520nets.%2520This%2520is%2520achieved%2520by%2520traversing%2520a%250Asparsified%2520inverse%2520transformation%2520tree%2520during%2520inference%2520using%2520parallel%250Aenergy-based%2520evaluations.%2520Our%2520proposed%2520inference%2520algorithm%252C%2520called%2520Inverse%250ATransformation%2520Search%2520%2528ITS%2529%252C%2520is%2520model-agnostic%2520and%2520equips%2520the%2520model%2520with%250Azero-shot%2520pseudo-invariance%2520to%2520spatially%2520transformed%2520inputs.%2520We%2520evaluated%2520our%250Amethod%2520on%2520several%2520benchmark%2520datasets%252C%2520including%2520a%2520synthesised%2520ImageNet%2520test%250Aset.%2520ITS%2520outperforms%2520the%2520utilised%2520baselines%2520on%2520all%2520zero-shot%2520test%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tilt%20your%20Head%3A%20Activating%20the%20Hidden%20Spatial-Invariance%20of%20Classifiers&entry.906535625=Johann%20Schmidt%20and%20Sebastian%20Stober&entry.1292438233=%20%20Deep%20neural%20networks%20are%20applied%20in%20more%20and%20more%20areas%20of%20everyday%20life.%0AHowever%2C%20they%20still%20lack%20essential%20abilities%2C%20such%20as%20robustly%20dealing%20with%0Aspatially%20transformed%20input%20signals.%20Approaches%20to%20mitigate%20this%20severe%0Arobustness%20issue%20are%20limited%20to%20two%20pathways%3A%20Either%20models%20are%20implicitly%0Aregularised%20by%20increased%20sample%20variability%20%28data%20augmentation%29%20or%20explicitly%0Aconstrained%20by%20hard-coded%20inductive%20biases.%20The%20limiting%20factor%20of%20the%20former%0Ais%20the%20size%20of%20the%20data%20space%2C%20which%20renders%20sufficient%20sample%20coverage%0Aintractable.%20The%20latter%20is%20limited%20by%20the%20engineering%20effort%20required%20to%0Adevelop%20such%20inductive%20biases%20for%20every%20possible%20scenario.%20Instead%2C%20we%20take%0Ainspiration%20from%20human%20behaviour%2C%20where%20percepts%20are%20modified%20by%20mental%20or%0Aphysical%20actions%20during%20inference.%20We%20propose%20a%20novel%20technique%20to%20emulate%20such%0Aan%20inference%20process%20for%20neural%20nets.%20This%20is%20achieved%20by%20traversing%20a%0Asparsified%20inverse%20transformation%20tree%20during%20inference%20using%20parallel%0Aenergy-based%20evaluations.%20Our%20proposed%20inference%20algorithm%2C%20called%20Inverse%0ATransformation%20Search%20%28ITS%29%2C%20is%20model-agnostic%20and%20equips%20the%20model%20with%0Azero-shot%20pseudo-invariance%20to%20spatially%20transformed%20inputs.%20We%20evaluated%20our%0Amethod%20on%20several%20benchmark%20datasets%2C%20including%20a%20synthesised%20ImageNet%20test%0Aset.%20ITS%20outperforms%20the%20utilised%20baselines%20on%20all%20zero-shot%20test%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03730v2&entry.124074799=Read"},
{"title": "SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with\n  Spike Streams", "author": "Kang Chen and Shiyan Chen and Jiyuan Zhang and Baoyue Zhang and Yajing Zheng and Tiejun Huang and Zhaofei Yu", "abstract": "  Reconstructing a sequence of sharp images from the blurry input is crucial\nfor enhancing our insights into the captured scene and poses a significant\nchallenge due to the limited temporal features embedded in the image. Spike\ncameras, sampling at rates up to 40,000 Hz, have proven effective in capturing\nmotion features and beneficial for solving this ill-posed problem. Nonetheless,\nexisting methods fall into the supervised learning paradigm, which suffers from\nnotable performance degradation when applied to real-world scenarios that\ndiverge from the synthetic training data domain. Moreover, the quality of\nreconstructed images is capped by the generated images based on motion analysis\ninterpolation, which inherently differs from the actual scene, affecting the\ngeneralization ability of these methods in real high-speed scenarios. To\naddress these challenges, we propose the first self-supervised framework for\nthe task of spike-guided motion deblurring. Our approach begins with the\nformulation of a spike-guided deblurring model that explores the theoretical\nrelationships among spike streams, blurry images, and their corresponding sharp\nsequences. We subsequently develop a self-supervised cascaded framework to\nalleviate the issues of spike noise and spatial-resolution mismatching\nencountered in the deblurring model. With knowledge distillation and\nre-blurring loss, we further design a lightweight deblur network to generate\nhigh-quality sequences with brightness and texture consistency with the\noriginal input. Quantitative and qualitative experiments conducted on our\nreal-world and synthetic datasets with spikes validate the superior\ngeneralization of the proposed framework. Our code, data and trained models\nwill be available at \\url{https://github.com/chenkang455/S-SDM}.\n", "link": "http://arxiv.org/abs/2403.09486v2", "date": "2024-05-27", "relevancy": 2.216, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5663}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5571}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams&body=Title%3A%20SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams%0AAuthor%3A%20Kang%20Chen%20and%20Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Baoyue%20Zhang%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu%0AAbstract%3A%20%20%20Reconstructing%20a%20sequence%20of%20sharp%20images%20from%20the%20blurry%20input%20is%20crucial%0Afor%20enhancing%20our%20insights%20into%20the%20captured%20scene%20and%20poses%20a%20significant%0Achallenge%20due%20to%20the%20limited%20temporal%20features%20embedded%20in%20the%20image.%20Spike%0Acameras%2C%20sampling%20at%20rates%20up%20to%2040%2C000%20Hz%2C%20have%20proven%20effective%20in%20capturing%0Amotion%20features%20and%20beneficial%20for%20solving%20this%20ill-posed%20problem.%20Nonetheless%2C%0Aexisting%20methods%20fall%20into%20the%20supervised%20learning%20paradigm%2C%20which%20suffers%20from%0Anotable%20performance%20degradation%20when%20applied%20to%20real-world%20scenarios%20that%0Adiverge%20from%20the%20synthetic%20training%20data%20domain.%20Moreover%2C%20the%20quality%20of%0Areconstructed%20images%20is%20capped%20by%20the%20generated%20images%20based%20on%20motion%20analysis%0Ainterpolation%2C%20which%20inherently%20differs%20from%20the%20actual%20scene%2C%20affecting%20the%0Ageneralization%20ability%20of%20these%20methods%20in%20real%20high-speed%20scenarios.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20the%20first%20self-supervised%20framework%20for%0Athe%20task%20of%20spike-guided%20motion%20deblurring.%20Our%20approach%20begins%20with%20the%0Aformulation%20of%20a%20spike-guided%20deblurring%20model%20that%20explores%20the%20theoretical%0Arelationships%20among%20spike%20streams%2C%20blurry%20images%2C%20and%20their%20corresponding%20sharp%0Asequences.%20We%20subsequently%20develop%20a%20self-supervised%20cascaded%20framework%20to%0Aalleviate%20the%20issues%20of%20spike%20noise%20and%20spatial-resolution%20mismatching%0Aencountered%20in%20the%20deblurring%20model.%20With%20knowledge%20distillation%20and%0Are-blurring%20loss%2C%20we%20further%20design%20a%20lightweight%20deblur%20network%20to%20generate%0Ahigh-quality%20sequences%20with%20brightness%20and%20texture%20consistency%20with%20the%0Aoriginal%20input.%20Quantitative%20and%20qualitative%20experiments%20conducted%20on%20our%0Areal-world%20and%20synthetic%20datasets%20with%20spikes%20validate%20the%20superior%0Ageneralization%20of%20the%20proposed%20framework.%20Our%20code%2C%20data%20and%20trained%20models%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenkang455/S-SDM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09486v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeReveal%253A%2520Unlocking%2520Temporal%2520Sequences%2520from%2520Real%2520Blurry%2520Inputs%2520with%250A%2520%2520Spike%2520Streams%26entry.906535625%3DKang%2520Chen%2520and%2520Shiyan%2520Chen%2520and%2520Jiyuan%2520Zhang%2520and%2520Baoyue%2520Zhang%2520and%2520Yajing%2520Zheng%2520and%2520Tiejun%2520Huang%2520and%2520Zhaofei%2520Yu%26entry.1292438233%3D%2520%2520Reconstructing%2520a%2520sequence%2520of%2520sharp%2520images%2520from%2520the%2520blurry%2520input%2520is%2520crucial%250Afor%2520enhancing%2520our%2520insights%2520into%2520the%2520captured%2520scene%2520and%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520limited%2520temporal%2520features%2520embedded%2520in%2520the%2520image.%2520Spike%250Acameras%252C%2520sampling%2520at%2520rates%2520up%2520to%252040%252C000%2520Hz%252C%2520have%2520proven%2520effective%2520in%2520capturing%250Amotion%2520features%2520and%2520beneficial%2520for%2520solving%2520this%2520ill-posed%2520problem.%2520Nonetheless%252C%250Aexisting%2520methods%2520fall%2520into%2520the%2520supervised%2520learning%2520paradigm%252C%2520which%2520suffers%2520from%250Anotable%2520performance%2520degradation%2520when%2520applied%2520to%2520real-world%2520scenarios%2520that%250Adiverge%2520from%2520the%2520synthetic%2520training%2520data%2520domain.%2520Moreover%252C%2520the%2520quality%2520of%250Areconstructed%2520images%2520is%2520capped%2520by%2520the%2520generated%2520images%2520based%2520on%2520motion%2520analysis%250Ainterpolation%252C%2520which%2520inherently%2520differs%2520from%2520the%2520actual%2520scene%252C%2520affecting%2520the%250Ageneralization%2520ability%2520of%2520these%2520methods%2520in%2520real%2520high-speed%2520scenarios.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520the%2520first%2520self-supervised%2520framework%2520for%250Athe%2520task%2520of%2520spike-guided%2520motion%2520deblurring.%2520Our%2520approach%2520begins%2520with%2520the%250Aformulation%2520of%2520a%2520spike-guided%2520deblurring%2520model%2520that%2520explores%2520the%2520theoretical%250Arelationships%2520among%2520spike%2520streams%252C%2520blurry%2520images%252C%2520and%2520their%2520corresponding%2520sharp%250Asequences.%2520We%2520subsequently%2520develop%2520a%2520self-supervised%2520cascaded%2520framework%2520to%250Aalleviate%2520the%2520issues%2520of%2520spike%2520noise%2520and%2520spatial-resolution%2520mismatching%250Aencountered%2520in%2520the%2520deblurring%2520model.%2520With%2520knowledge%2520distillation%2520and%250Are-blurring%2520loss%252C%2520we%2520further%2520design%2520a%2520lightweight%2520deblur%2520network%2520to%2520generate%250Ahigh-quality%2520sequences%2520with%2520brightness%2520and%2520texture%2520consistency%2520with%2520the%250Aoriginal%2520input.%2520Quantitative%2520and%2520qualitative%2520experiments%2520conducted%2520on%2520our%250Areal-world%2520and%2520synthetic%2520datasets%2520with%2520spikes%2520validate%2520the%2520superior%250Ageneralization%2520of%2520the%2520proposed%2520framework.%2520Our%2520code%252C%2520data%2520and%2520trained%2520models%250Awill%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/chenkang455/S-SDM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09486v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams&entry.906535625=Kang%20Chen%20and%20Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Baoyue%20Zhang%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu&entry.1292438233=%20%20Reconstructing%20a%20sequence%20of%20sharp%20images%20from%20the%20blurry%20input%20is%20crucial%0Afor%20enhancing%20our%20insights%20into%20the%20captured%20scene%20and%20poses%20a%20significant%0Achallenge%20due%20to%20the%20limited%20temporal%20features%20embedded%20in%20the%20image.%20Spike%0Acameras%2C%20sampling%20at%20rates%20up%20to%2040%2C000%20Hz%2C%20have%20proven%20effective%20in%20capturing%0Amotion%20features%20and%20beneficial%20for%20solving%20this%20ill-posed%20problem.%20Nonetheless%2C%0Aexisting%20methods%20fall%20into%20the%20supervised%20learning%20paradigm%2C%20which%20suffers%20from%0Anotable%20performance%20degradation%20when%20applied%20to%20real-world%20scenarios%20that%0Adiverge%20from%20the%20synthetic%20training%20data%20domain.%20Moreover%2C%20the%20quality%20of%0Areconstructed%20images%20is%20capped%20by%20the%20generated%20images%20based%20on%20motion%20analysis%0Ainterpolation%2C%20which%20inherently%20differs%20from%20the%20actual%20scene%2C%20affecting%20the%0Ageneralization%20ability%20of%20these%20methods%20in%20real%20high-speed%20scenarios.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20the%20first%20self-supervised%20framework%20for%0Athe%20task%20of%20spike-guided%20motion%20deblurring.%20Our%20approach%20begins%20with%20the%0Aformulation%20of%20a%20spike-guided%20deblurring%20model%20that%20explores%20the%20theoretical%0Arelationships%20among%20spike%20streams%2C%20blurry%20images%2C%20and%20their%20corresponding%20sharp%0Asequences.%20We%20subsequently%20develop%20a%20self-supervised%20cascaded%20framework%20to%0Aalleviate%20the%20issues%20of%20spike%20noise%20and%20spatial-resolution%20mismatching%0Aencountered%20in%20the%20deblurring%20model.%20With%20knowledge%20distillation%20and%0Are-blurring%20loss%2C%20we%20further%20design%20a%20lightweight%20deblur%20network%20to%20generate%0Ahigh-quality%20sequences%20with%20brightness%20and%20texture%20consistency%20with%20the%0Aoriginal%20input.%20Quantitative%20and%20qualitative%20experiments%20conducted%20on%20our%0Areal-world%20and%20synthetic%20datasets%20with%20spikes%20validate%20the%20superior%0Ageneralization%20of%20the%20proposed%20framework.%20Our%20code%2C%20data%20and%20trained%20models%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenkang455/S-SDM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09486v2&entry.124074799=Read"},
{"title": "LoRA Training in the NTK Regime has No Spurious Local Minima", "author": "Uijeong Jang and Jason D. Lee and Ernest K. Ryu", "abstract": "  Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.\n", "link": "http://arxiv.org/abs/2402.11867v2", "date": "2024-05-27", "relevancy": 2.2094, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4512}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4465}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20Training%20in%20the%20NTK%20Regime%20has%20No%20Spurious%20Local%20Minima&body=Title%3A%20LoRA%20Training%20in%20the%20NTK%20Regime%20has%20No%20Spurious%20Local%20Minima%0AAuthor%3A%20Uijeong%20Jang%20and%20Jason%20D.%20Lee%20and%20Ernest%20K.%20Ryu%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20has%20become%20the%20standard%20approach%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%20%28LLM%29%2C%20but%20our%0Atheoretical%20understanding%20of%20LoRA%20has%20been%20limited.%20In%20this%20work%2C%20we%0Atheoretically%20analyze%20LoRA%20fine-tuning%20in%20the%20neural%20tangent%20kernel%20%28NTK%29%0Aregime%20with%20%24N%24%20data%20points%2C%20showing%3A%20%28i%29%20full%20fine-tuning%20%28without%20LoRA%29%0Aadmits%20a%20low-rank%20solution%20of%20rank%20%24r%5Clesssim%20%5Csqrt%7BN%7D%24%3B%20%28ii%29%20using%20LoRA%20with%0Arank%20%24r%5Cgtrsim%20%5Csqrt%7BN%7D%24%20eliminates%20spurious%20local%20minima%2C%20allowing%20gradient%0Adescent%20to%20find%20the%20low-rank%20solutions%3B%20%28iii%29%20the%20low-rank%20solution%20found%20using%0ALoRA%20generalizes%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520Training%2520in%2520the%2520NTK%2520Regime%2520has%2520No%2520Spurious%2520Local%2520Minima%26entry.906535625%3DUijeong%2520Jang%2520and%2520Jason%2520D.%2520Lee%2520and%2520Ernest%2520K.%2520Ryu%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520has%2520become%2520the%2520standard%2520approach%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLM%2529%252C%2520but%2520our%250Atheoretical%2520understanding%2520of%2520LoRA%2520has%2520been%2520limited.%2520In%2520this%2520work%252C%2520we%250Atheoretically%2520analyze%2520LoRA%2520fine-tuning%2520in%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%250Aregime%2520with%2520%2524N%2524%2520data%2520points%252C%2520showing%253A%2520%2528i%2529%2520full%2520fine-tuning%2520%2528without%2520LoRA%2529%250Aadmits%2520a%2520low-rank%2520solution%2520of%2520rank%2520%2524r%255Clesssim%2520%255Csqrt%257BN%257D%2524%253B%2520%2528ii%2529%2520using%2520LoRA%2520with%250Arank%2520%2524r%255Cgtrsim%2520%255Csqrt%257BN%257D%2524%2520eliminates%2520spurious%2520local%2520minima%252C%2520allowing%2520gradient%250Adescent%2520to%2520find%2520the%2520low-rank%2520solutions%253B%2520%2528iii%2529%2520the%2520low-rank%2520solution%2520found%2520using%250ALoRA%2520generalizes%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20Training%20in%20the%20NTK%20Regime%20has%20No%20Spurious%20Local%20Minima&entry.906535625=Uijeong%20Jang%20and%20Jason%20D.%20Lee%20and%20Ernest%20K.%20Ryu&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20has%20become%20the%20standard%20approach%20for%0Aparameter-efficient%20fine-tuning%20of%20large%20language%20models%20%28LLM%29%2C%20but%20our%0Atheoretical%20understanding%20of%20LoRA%20has%20been%20limited.%20In%20this%20work%2C%20we%0Atheoretically%20analyze%20LoRA%20fine-tuning%20in%20the%20neural%20tangent%20kernel%20%28NTK%29%0Aregime%20with%20%24N%24%20data%20points%2C%20showing%3A%20%28i%29%20full%20fine-tuning%20%28without%20LoRA%29%0Aadmits%20a%20low-rank%20solution%20of%20rank%20%24r%5Clesssim%20%5Csqrt%7BN%7D%24%3B%20%28ii%29%20using%20LoRA%20with%0Arank%20%24r%5Cgtrsim%20%5Csqrt%7BN%7D%24%20eliminates%20spurious%20local%20minima%2C%20allowing%20gradient%0Adescent%20to%20find%20the%20low-rank%20solutions%3B%20%28iii%29%20the%20low-rank%20solution%20found%20using%0ALoRA%20generalizes%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11867v2&entry.124074799=Read"},
{"title": "Deep Feature Gaussian Processes for Single-Scene Aerosol Optical Depth\n  Reconstruction", "author": "Shengjie Liu and Lu Zhang", "abstract": "  Remote sensing data provide a low-cost solution for large-scale monitoring of\nair pollution via the retrieval of aerosol optical depth (AOD), but is often\nlimited by cloud contamination. Existing methods for AOD reconstruction rely on\ntemporal information. However, for remote sensing data at high spatial\nresolution, multi-temporal observations are often unavailable. In this letter,\nwe take advantage of deep representation learning from convolutional neural\nnetworks and propose Deep Feature Gaussian Processes (DFGP) for single-scene\nAOD reconstruction. By using deep learning, we transform the variables to a\nfeature space with better explainable power. By using Gaussian processes, we\nexplicitly consider the correlation between observed AOD and missing AOD in\nspatial and feature domains. Experiments on two AOD datasets with real-world\ncloud patterns showed that the proposed method outperformed deep CNN and random\nforest, achieving R$^2$ of 0.7431 on MODIS AOD and R$^2$ of 0.9211 on EMIT AOD,\ncompared to deep CNN's R$^2$ of 0.6507 and R$^2$ of 0.8619. The proposed\nmethods increased R$^2$ by over 0.35 compared to the popular random forest in\nAOD reconstruction. The data and code used in this study are available at\n\\url{https://skrisliu.com/dfgp}.\n", "link": "http://arxiv.org/abs/2405.17262v1", "date": "2024-05-27", "relevancy": 2.2085, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5634}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Feature%20Gaussian%20Processes%20for%20Single-Scene%20Aerosol%20Optical%20Depth%0A%20%20Reconstruction&body=Title%3A%20Deep%20Feature%20Gaussian%20Processes%20for%20Single-Scene%20Aerosol%20Optical%20Depth%0A%20%20Reconstruction%0AAuthor%3A%20Shengjie%20Liu%20and%20Lu%20Zhang%0AAbstract%3A%20%20%20Remote%20sensing%20data%20provide%20a%20low-cost%20solution%20for%20large-scale%20monitoring%20of%0Aair%20pollution%20via%20the%20retrieval%20of%20aerosol%20optical%20depth%20%28AOD%29%2C%20but%20is%20often%0Alimited%20by%20cloud%20contamination.%20Existing%20methods%20for%20AOD%20reconstruction%20rely%20on%0Atemporal%20information.%20However%2C%20for%20remote%20sensing%20data%20at%20high%20spatial%0Aresolution%2C%20multi-temporal%20observations%20are%20often%20unavailable.%20In%20this%20letter%2C%0Awe%20take%20advantage%20of%20deep%20representation%20learning%20from%20convolutional%20neural%0Anetworks%20and%20propose%20Deep%20Feature%20Gaussian%20Processes%20%28DFGP%29%20for%20single-scene%0AAOD%20reconstruction.%20By%20using%20deep%20learning%2C%20we%20transform%20the%20variables%20to%20a%0Afeature%20space%20with%20better%20explainable%20power.%20By%20using%20Gaussian%20processes%2C%20we%0Aexplicitly%20consider%20the%20correlation%20between%20observed%20AOD%20and%20missing%20AOD%20in%0Aspatial%20and%20feature%20domains.%20Experiments%20on%20two%20AOD%20datasets%20with%20real-world%0Acloud%20patterns%20showed%20that%20the%20proposed%20method%20outperformed%20deep%20CNN%20and%20random%0Aforest%2C%20achieving%20R%24%5E2%24%20of%200.7431%20on%20MODIS%20AOD%20and%20R%24%5E2%24%20of%200.9211%20on%20EMIT%20AOD%2C%0Acompared%20to%20deep%20CNN%27s%20R%24%5E2%24%20of%200.6507%20and%20R%24%5E2%24%20of%200.8619.%20The%20proposed%0Amethods%20increased%20R%24%5E2%24%20by%20over%200.35%20compared%20to%20the%20popular%20random%20forest%20in%0AAOD%20reconstruction.%20The%20data%20and%20code%20used%20in%20this%20study%20are%20available%20at%0A%5Curl%7Bhttps%3A//skrisliu.com/dfgp%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Feature%2520Gaussian%2520Processes%2520for%2520Single-Scene%2520Aerosol%2520Optical%2520Depth%250A%2520%2520Reconstruction%26entry.906535625%3DShengjie%2520Liu%2520and%2520Lu%2520Zhang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520data%2520provide%2520a%2520low-cost%2520solution%2520for%2520large-scale%2520monitoring%2520of%250Aair%2520pollution%2520via%2520the%2520retrieval%2520of%2520aerosol%2520optical%2520depth%2520%2528AOD%2529%252C%2520but%2520is%2520often%250Alimited%2520by%2520cloud%2520contamination.%2520Existing%2520methods%2520for%2520AOD%2520reconstruction%2520rely%2520on%250Atemporal%2520information.%2520However%252C%2520for%2520remote%2520sensing%2520data%2520at%2520high%2520spatial%250Aresolution%252C%2520multi-temporal%2520observations%2520are%2520often%2520unavailable.%2520In%2520this%2520letter%252C%250Awe%2520take%2520advantage%2520of%2520deep%2520representation%2520learning%2520from%2520convolutional%2520neural%250Anetworks%2520and%2520propose%2520Deep%2520Feature%2520Gaussian%2520Processes%2520%2528DFGP%2529%2520for%2520single-scene%250AAOD%2520reconstruction.%2520By%2520using%2520deep%2520learning%252C%2520we%2520transform%2520the%2520variables%2520to%2520a%250Afeature%2520space%2520with%2520better%2520explainable%2520power.%2520By%2520using%2520Gaussian%2520processes%252C%2520we%250Aexplicitly%2520consider%2520the%2520correlation%2520between%2520observed%2520AOD%2520and%2520missing%2520AOD%2520in%250Aspatial%2520and%2520feature%2520domains.%2520Experiments%2520on%2520two%2520AOD%2520datasets%2520with%2520real-world%250Acloud%2520patterns%2520showed%2520that%2520the%2520proposed%2520method%2520outperformed%2520deep%2520CNN%2520and%2520random%250Aforest%252C%2520achieving%2520R%2524%255E2%2524%2520of%25200.7431%2520on%2520MODIS%2520AOD%2520and%2520R%2524%255E2%2524%2520of%25200.9211%2520on%2520EMIT%2520AOD%252C%250Acompared%2520to%2520deep%2520CNN%2527s%2520R%2524%255E2%2524%2520of%25200.6507%2520and%2520R%2524%255E2%2524%2520of%25200.8619.%2520The%2520proposed%250Amethods%2520increased%2520R%2524%255E2%2524%2520by%2520over%25200.35%2520compared%2520to%2520the%2520popular%2520random%2520forest%2520in%250AAOD%2520reconstruction.%2520The%2520data%2520and%2520code%2520used%2520in%2520this%2520study%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//skrisliu.com/dfgp%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Feature%20Gaussian%20Processes%20for%20Single-Scene%20Aerosol%20Optical%20Depth%0A%20%20Reconstruction&entry.906535625=Shengjie%20Liu%20and%20Lu%20Zhang&entry.1292438233=%20%20Remote%20sensing%20data%20provide%20a%20low-cost%20solution%20for%20large-scale%20monitoring%20of%0Aair%20pollution%20via%20the%20retrieval%20of%20aerosol%20optical%20depth%20%28AOD%29%2C%20but%20is%20often%0Alimited%20by%20cloud%20contamination.%20Existing%20methods%20for%20AOD%20reconstruction%20rely%20on%0Atemporal%20information.%20However%2C%20for%20remote%20sensing%20data%20at%20high%20spatial%0Aresolution%2C%20multi-temporal%20observations%20are%20often%20unavailable.%20In%20this%20letter%2C%0Awe%20take%20advantage%20of%20deep%20representation%20learning%20from%20convolutional%20neural%0Anetworks%20and%20propose%20Deep%20Feature%20Gaussian%20Processes%20%28DFGP%29%20for%20single-scene%0AAOD%20reconstruction.%20By%20using%20deep%20learning%2C%20we%20transform%20the%20variables%20to%20a%0Afeature%20space%20with%20better%20explainable%20power.%20By%20using%20Gaussian%20processes%2C%20we%0Aexplicitly%20consider%20the%20correlation%20between%20observed%20AOD%20and%20missing%20AOD%20in%0Aspatial%20and%20feature%20domains.%20Experiments%20on%20two%20AOD%20datasets%20with%20real-world%0Acloud%20patterns%20showed%20that%20the%20proposed%20method%20outperformed%20deep%20CNN%20and%20random%0Aforest%2C%20achieving%20R%24%5E2%24%20of%200.7431%20on%20MODIS%20AOD%20and%20R%24%5E2%24%20of%200.9211%20on%20EMIT%20AOD%2C%0Acompared%20to%20deep%20CNN%27s%20R%24%5E2%24%20of%200.6507%20and%20R%24%5E2%24%20of%200.8619.%20The%20proposed%0Amethods%20increased%20R%24%5E2%24%20by%20over%200.35%20compared%20to%20the%20popular%20random%20forest%20in%0AAOD%20reconstruction.%20The%20data%20and%20code%20used%20in%20this%20study%20are%20available%20at%0A%5Curl%7Bhttps%3A//skrisliu.com/dfgp%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17262v1&entry.124074799=Read"},
{"title": "Improving Token-Based World Models with Parallel Observation Prediction", "author": "Lior Cohen and Kaixin Wang and Bingyi Kang and Shie Mannor", "abstract": "  Motivated by the success of Transformers when applied to sequences of\ndiscrete symbols, token-based world models (TBWMs) were recently proposed as\nsample-efficient methods. In TBWMs, the world model consumes agent experience\nas a language-like sequence of tokens, where each observation constitutes a\nsub-sequence. However, during imagination, the sequential token-by-token\ngeneration of next observations results in a severe bottleneck, leading to long\ntraining times, poor GPU utilization, and limited representations. To resolve\nthis bottleneck, we devise a novel Parallel Observation Prediction (POP)\nmechanism. POP augments a Retentive Network (RetNet) with a novel forward mode\ntailored to our reinforcement learning setting. We incorporate POP in a novel\nTBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster\nimagination compared to prior TBWMs. REM attains superhuman performance on 12\nout of 26 games of the Atari 100K benchmark, while training in less than 12\nhours. Our code is available at \\url{https://github.com/leor-c/REM}.\n", "link": "http://arxiv.org/abs/2402.05643v3", "date": "2024-05-27", "relevancy": 2.2062, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5886}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5278}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Token-Based%20World%20Models%20with%20Parallel%20Observation%20Prediction&body=Title%3A%20Improving%20Token-Based%20World%20Models%20with%20Parallel%20Observation%20Prediction%0AAuthor%3A%20Lior%20Cohen%20and%20Kaixin%20Wang%20and%20Bingyi%20Kang%20and%20Shie%20Mannor%0AAbstract%3A%20%20%20Motivated%20by%20the%20success%20of%20Transformers%20when%20applied%20to%20sequences%20of%0Adiscrete%20symbols%2C%20token-based%20world%20models%20%28TBWMs%29%20were%20recently%20proposed%20as%0Asample-efficient%20methods.%20In%20TBWMs%2C%20the%20world%20model%20consumes%20agent%20experience%0Aas%20a%20language-like%20sequence%20of%20tokens%2C%20where%20each%20observation%20constitutes%20a%0Asub-sequence.%20However%2C%20during%20imagination%2C%20the%20sequential%20token-by-token%0Ageneration%20of%20next%20observations%20results%20in%20a%20severe%20bottleneck%2C%20leading%20to%20long%0Atraining%20times%2C%20poor%20GPU%20utilization%2C%20and%20limited%20representations.%20To%20resolve%0Athis%20bottleneck%2C%20we%20devise%20a%20novel%20Parallel%20Observation%20Prediction%20%28POP%29%0Amechanism.%20POP%20augments%20a%20Retentive%20Network%20%28RetNet%29%20with%20a%20novel%20forward%20mode%0Atailored%20to%20our%20reinforcement%20learning%20setting.%20We%20incorporate%20POP%20in%20a%20novel%0ATBWM%20agent%20named%20REM%20%28Retentive%20Environment%20Model%29%2C%20showcasing%20a%2015.4x%20faster%0Aimagination%20compared%20to%20prior%20TBWMs.%20REM%20attains%20superhuman%20performance%20on%2012%0Aout%20of%2026%20games%20of%20the%20Atari%20100K%20benchmark%2C%20while%20training%20in%20less%20than%2012%0Ahours.%20Our%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/leor-c/REM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05643v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Token-Based%2520World%2520Models%2520with%2520Parallel%2520Observation%2520Prediction%26entry.906535625%3DLior%2520Cohen%2520and%2520Kaixin%2520Wang%2520and%2520Bingyi%2520Kang%2520and%2520Shie%2520Mannor%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520success%2520of%2520Transformers%2520when%2520applied%2520to%2520sequences%2520of%250Adiscrete%2520symbols%252C%2520token-based%2520world%2520models%2520%2528TBWMs%2529%2520were%2520recently%2520proposed%2520as%250Asample-efficient%2520methods.%2520In%2520TBWMs%252C%2520the%2520world%2520model%2520consumes%2520agent%2520experience%250Aas%2520a%2520language-like%2520sequence%2520of%2520tokens%252C%2520where%2520each%2520observation%2520constitutes%2520a%250Asub-sequence.%2520However%252C%2520during%2520imagination%252C%2520the%2520sequential%2520token-by-token%250Ageneration%2520of%2520next%2520observations%2520results%2520in%2520a%2520severe%2520bottleneck%252C%2520leading%2520to%2520long%250Atraining%2520times%252C%2520poor%2520GPU%2520utilization%252C%2520and%2520limited%2520representations.%2520To%2520resolve%250Athis%2520bottleneck%252C%2520we%2520devise%2520a%2520novel%2520Parallel%2520Observation%2520Prediction%2520%2528POP%2529%250Amechanism.%2520POP%2520augments%2520a%2520Retentive%2520Network%2520%2528RetNet%2529%2520with%2520a%2520novel%2520forward%2520mode%250Atailored%2520to%2520our%2520reinforcement%2520learning%2520setting.%2520We%2520incorporate%2520POP%2520in%2520a%2520novel%250ATBWM%2520agent%2520named%2520REM%2520%2528Retentive%2520Environment%2520Model%2529%252C%2520showcasing%2520a%252015.4x%2520faster%250Aimagination%2520compared%2520to%2520prior%2520TBWMs.%2520REM%2520attains%2520superhuman%2520performance%2520on%252012%250Aout%2520of%252026%2520games%2520of%2520the%2520Atari%2520100K%2520benchmark%252C%2520while%2520training%2520in%2520less%2520than%252012%250Ahours.%2520Our%2520code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/leor-c/REM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05643v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Token-Based%20World%20Models%20with%20Parallel%20Observation%20Prediction&entry.906535625=Lior%20Cohen%20and%20Kaixin%20Wang%20and%20Bingyi%20Kang%20and%20Shie%20Mannor&entry.1292438233=%20%20Motivated%20by%20the%20success%20of%20Transformers%20when%20applied%20to%20sequences%20of%0Adiscrete%20symbols%2C%20token-based%20world%20models%20%28TBWMs%29%20were%20recently%20proposed%20as%0Asample-efficient%20methods.%20In%20TBWMs%2C%20the%20world%20model%20consumes%20agent%20experience%0Aas%20a%20language-like%20sequence%20of%20tokens%2C%20where%20each%20observation%20constitutes%20a%0Asub-sequence.%20However%2C%20during%20imagination%2C%20the%20sequential%20token-by-token%0Ageneration%20of%20next%20observations%20results%20in%20a%20severe%20bottleneck%2C%20leading%20to%20long%0Atraining%20times%2C%20poor%20GPU%20utilization%2C%20and%20limited%20representations.%20To%20resolve%0Athis%20bottleneck%2C%20we%20devise%20a%20novel%20Parallel%20Observation%20Prediction%20%28POP%29%0Amechanism.%20POP%20augments%20a%20Retentive%20Network%20%28RetNet%29%20with%20a%20novel%20forward%20mode%0Atailored%20to%20our%20reinforcement%20learning%20setting.%20We%20incorporate%20POP%20in%20a%20novel%0ATBWM%20agent%20named%20REM%20%28Retentive%20Environment%20Model%29%2C%20showcasing%20a%2015.4x%20faster%0Aimagination%20compared%20to%20prior%20TBWMs.%20REM%20attains%20superhuman%20performance%20on%2012%0Aout%20of%2026%20games%20of%20the%20Atari%20100K%20benchmark%2C%20while%20training%20in%20less%20than%2012%0Ahours.%20Our%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/leor-c/REM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05643v3&entry.124074799=Read"},
{"title": "Enhanced Droplet Analysis Using Generative Adversarial Networks", "author": "Tan-Hanh Pham and Kim-Doang Nguyen", "abstract": "  Precision devices play an important role in enhancing production quality and\nproductivity in agricultural systems. Therefore, the optimization of these\ndevices is essential in precision agriculture. Recently, with the advancements\nof deep learning, there have been several studies aiming to harness its\ncapabilities for improving spray system performance. However, the effectiveness\nof these methods heavily depends on the size of the training dataset, which is\nexpensive and time-consuming to collect. To address the challenge of\ninsufficient training samples, we developed an image generator named DropletGAN\nto generate images of droplets. The DropletGAN model is trained by using a\nsmall dataset captured by a high-speed camera and capable of generating images\nwith progressively increasing resolution. The results demonstrate that the\nmodel can generate high-quality images with the size of 1024x1024. The\ngenerated images from the DropletGAN are evaluated using the Fr\\'echet\ninception distance (FID) with an FID score of 11.29. Furthermore, this research\nleverages recent advancements in computer vision and deep learning to develop a\nlight droplet detector using the synthetic dataset. As a result, the detection\nmodel achieves a 16.06% increase in mean average precision (mAP) when utilizing\nthe synthetic dataset. To the best of our knowledge, this work stands as the\nfirst to employ a generative model for augmenting droplet detection. Its\nsignificance lies not only in optimizing nozzle design for constructing\nefficient spray systems but also in addressing the common challenge of\ninsufficient data in various precision agriculture tasks. This work offers a\ncritical contribution to conserving resources while striving for optimal and\nsustainable agricultural practices.\n", "link": "http://arxiv.org/abs/2402.15909v3", "date": "2024-05-27", "relevancy": 2.2044, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5634}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5429}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Droplet%20Analysis%20Using%20Generative%20Adversarial%20Networks&body=Title%3A%20Enhanced%20Droplet%20Analysis%20Using%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Tan-Hanh%20Pham%20and%20Kim-Doang%20Nguyen%0AAbstract%3A%20%20%20Precision%20devices%20play%20an%20important%20role%20in%20enhancing%20production%20quality%20and%0Aproductivity%20in%20agricultural%20systems.%20Therefore%2C%20the%20optimization%20of%20these%0Adevices%20is%20essential%20in%20precision%20agriculture.%20Recently%2C%20with%20the%20advancements%0Aof%20deep%20learning%2C%20there%20have%20been%20several%20studies%20aiming%20to%20harness%20its%0Acapabilities%20for%20improving%20spray%20system%20performance.%20However%2C%20the%20effectiveness%0Aof%20these%20methods%20heavily%20depends%20on%20the%20size%20of%20the%20training%20dataset%2C%20which%20is%0Aexpensive%20and%20time-consuming%20to%20collect.%20To%20address%20the%20challenge%20of%0Ainsufficient%20training%20samples%2C%20we%20developed%20an%20image%20generator%20named%20DropletGAN%0Ato%20generate%20images%20of%20droplets.%20The%20DropletGAN%20model%20is%20trained%20by%20using%20a%0Asmall%20dataset%20captured%20by%20a%20high-speed%20camera%20and%20capable%20of%20generating%20images%0Awith%20progressively%20increasing%20resolution.%20The%20results%20demonstrate%20that%20the%0Amodel%20can%20generate%20high-quality%20images%20with%20the%20size%20of%201024x1024.%20The%0Agenerated%20images%20from%20the%20DropletGAN%20are%20evaluated%20using%20the%20Fr%5C%27echet%0Ainception%20distance%20%28FID%29%20with%20an%20FID%20score%20of%2011.29.%20Furthermore%2C%20this%20research%0Aleverages%20recent%20advancements%20in%20computer%20vision%20and%20deep%20learning%20to%20develop%20a%0Alight%20droplet%20detector%20using%20the%20synthetic%20dataset.%20As%20a%20result%2C%20the%20detection%0Amodel%20achieves%20a%2016.06%25%20increase%20in%20mean%20average%20precision%20%28mAP%29%20when%20utilizing%0Athe%20synthetic%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20stands%20as%20the%0Afirst%20to%20employ%20a%20generative%20model%20for%20augmenting%20droplet%20detection.%20Its%0Asignificance%20lies%20not%20only%20in%20optimizing%20nozzle%20design%20for%20constructing%0Aefficient%20spray%20systems%20but%20also%20in%20addressing%20the%20common%20challenge%20of%0Ainsufficient%20data%20in%20various%20precision%20agriculture%20tasks.%20This%20work%20offers%20a%0Acritical%20contribution%20to%20conserving%20resources%20while%20striving%20for%20optimal%20and%0Asustainable%20agricultural%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Droplet%2520Analysis%2520Using%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DTan-Hanh%2520Pham%2520and%2520Kim-Doang%2520Nguyen%26entry.1292438233%3D%2520%2520Precision%2520devices%2520play%2520an%2520important%2520role%2520in%2520enhancing%2520production%2520quality%2520and%250Aproductivity%2520in%2520agricultural%2520systems.%2520Therefore%252C%2520the%2520optimization%2520of%2520these%250Adevices%2520is%2520essential%2520in%2520precision%2520agriculture.%2520Recently%252C%2520with%2520the%2520advancements%250Aof%2520deep%2520learning%252C%2520there%2520have%2520been%2520several%2520studies%2520aiming%2520to%2520harness%2520its%250Acapabilities%2520for%2520improving%2520spray%2520system%2520performance.%2520However%252C%2520the%2520effectiveness%250Aof%2520these%2520methods%2520heavily%2520depends%2520on%2520the%2520size%2520of%2520the%2520training%2520dataset%252C%2520which%2520is%250Aexpensive%2520and%2520time-consuming%2520to%2520collect.%2520To%2520address%2520the%2520challenge%2520of%250Ainsufficient%2520training%2520samples%252C%2520we%2520developed%2520an%2520image%2520generator%2520named%2520DropletGAN%250Ato%2520generate%2520images%2520of%2520droplets.%2520The%2520DropletGAN%2520model%2520is%2520trained%2520by%2520using%2520a%250Asmall%2520dataset%2520captured%2520by%2520a%2520high-speed%2520camera%2520and%2520capable%2520of%2520generating%2520images%250Awith%2520progressively%2520increasing%2520resolution.%2520The%2520results%2520demonstrate%2520that%2520the%250Amodel%2520can%2520generate%2520high-quality%2520images%2520with%2520the%2520size%2520of%25201024x1024.%2520The%250Agenerated%2520images%2520from%2520the%2520DropletGAN%2520are%2520evaluated%2520using%2520the%2520Fr%255C%2527echet%250Ainception%2520distance%2520%2528FID%2529%2520with%2520an%2520FID%2520score%2520of%252011.29.%2520Furthermore%252C%2520this%2520research%250Aleverages%2520recent%2520advancements%2520in%2520computer%2520vision%2520and%2520deep%2520learning%2520to%2520develop%2520a%250Alight%2520droplet%2520detector%2520using%2520the%2520synthetic%2520dataset.%2520As%2520a%2520result%252C%2520the%2520detection%250Amodel%2520achieves%2520a%252016.06%2525%2520increase%2520in%2520mean%2520average%2520precision%2520%2528mAP%2529%2520when%2520utilizing%250Athe%2520synthetic%2520dataset.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520stands%2520as%2520the%250Afirst%2520to%2520employ%2520a%2520generative%2520model%2520for%2520augmenting%2520droplet%2520detection.%2520Its%250Asignificance%2520lies%2520not%2520only%2520in%2520optimizing%2520nozzle%2520design%2520for%2520constructing%250Aefficient%2520spray%2520systems%2520but%2520also%2520in%2520addressing%2520the%2520common%2520challenge%2520of%250Ainsufficient%2520data%2520in%2520various%2520precision%2520agriculture%2520tasks.%2520This%2520work%2520offers%2520a%250Acritical%2520contribution%2520to%2520conserving%2520resources%2520while%2520striving%2520for%2520optimal%2520and%250Asustainable%2520agricultural%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Droplet%20Analysis%20Using%20Generative%20Adversarial%20Networks&entry.906535625=Tan-Hanh%20Pham%20and%20Kim-Doang%20Nguyen&entry.1292438233=%20%20Precision%20devices%20play%20an%20important%20role%20in%20enhancing%20production%20quality%20and%0Aproductivity%20in%20agricultural%20systems.%20Therefore%2C%20the%20optimization%20of%20these%0Adevices%20is%20essential%20in%20precision%20agriculture.%20Recently%2C%20with%20the%20advancements%0Aof%20deep%20learning%2C%20there%20have%20been%20several%20studies%20aiming%20to%20harness%20its%0Acapabilities%20for%20improving%20spray%20system%20performance.%20However%2C%20the%20effectiveness%0Aof%20these%20methods%20heavily%20depends%20on%20the%20size%20of%20the%20training%20dataset%2C%20which%20is%0Aexpensive%20and%20time-consuming%20to%20collect.%20To%20address%20the%20challenge%20of%0Ainsufficient%20training%20samples%2C%20we%20developed%20an%20image%20generator%20named%20DropletGAN%0Ato%20generate%20images%20of%20droplets.%20The%20DropletGAN%20model%20is%20trained%20by%20using%20a%0Asmall%20dataset%20captured%20by%20a%20high-speed%20camera%20and%20capable%20of%20generating%20images%0Awith%20progressively%20increasing%20resolution.%20The%20results%20demonstrate%20that%20the%0Amodel%20can%20generate%20high-quality%20images%20with%20the%20size%20of%201024x1024.%20The%0Agenerated%20images%20from%20the%20DropletGAN%20are%20evaluated%20using%20the%20Fr%5C%27echet%0Ainception%20distance%20%28FID%29%20with%20an%20FID%20score%20of%2011.29.%20Furthermore%2C%20this%20research%0Aleverages%20recent%20advancements%20in%20computer%20vision%20and%20deep%20learning%20to%20develop%20a%0Alight%20droplet%20detector%20using%20the%20synthetic%20dataset.%20As%20a%20result%2C%20the%20detection%0Amodel%20achieves%20a%2016.06%25%20increase%20in%20mean%20average%20precision%20%28mAP%29%20when%20utilizing%0Athe%20synthetic%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20stands%20as%20the%0Afirst%20to%20employ%20a%20generative%20model%20for%20augmenting%20droplet%20detection.%20Its%0Asignificance%20lies%20not%20only%20in%20optimizing%20nozzle%20design%20for%20constructing%0Aefficient%20spray%20systems%20but%20also%20in%20addressing%20the%20common%20challenge%20of%0Ainsufficient%20data%20in%20various%20precision%20agriculture%20tasks.%20This%20work%20offers%20a%0Acritical%20contribution%20to%20conserving%20resources%20while%20striving%20for%20optimal%20and%0Asustainable%20agricultural%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15909v3&entry.124074799=Read"},
{"title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving", "author": "Avinash Nittur Ramesh and Aitor Correas-Serrano and Mar\u00eda Gonz\u00e1lez-Huici", "abstract": "  We present a novel synthetically generated multi-modal dataset, SCaRL, to\nenable the training and validation of autonomous driving solutions. Multi-modal\ndatasets are essential to attain the robustness and high accuracy required by\nautonomous systems in applications such as autonomous driving. As deep\nlearning-based solutions are becoming more prevalent for object detection,\nclassification, and tracking tasks, there is great demand for datasets\ncombining camera, lidar, and radar sensors. Existing real/synthetic datasets\nfor autonomous driving lack synchronized data collection from a complete sensor\nsuite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance,\nand depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from\nRadar; and 3D point clouds/2D maps of semantic, depth and Doppler data from\ncoherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which\nprovides data for diverse, dynamic scenarios and traffic conditions. SCaRL is\nthe first dataset to include synthetic synchronized data from coherent Lidar\nand MIMO radar sensors.\n  The dataset can be accessed here:\nhttps://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/\n", "link": "http://arxiv.org/abs/2405.17030v1", "date": "2024-05-27", "relevancy": 2.2022, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5523}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5517}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCaRL-%20A%20Synthetic%20Multi-Modal%20Dataset%20for%20Autonomous%20Driving&body=Title%3A%20SCaRL-%20A%20Synthetic%20Multi-Modal%20Dataset%20for%20Autonomous%20Driving%0AAuthor%3A%20Avinash%20Nittur%20Ramesh%20and%20Aitor%20Correas-Serrano%20and%20Mar%C3%ADa%20Gonz%C3%A1lez-Huici%0AAbstract%3A%20%20%20We%20present%20a%20novel%20synthetically%20generated%20multi-modal%20dataset%2C%20SCaRL%2C%20to%0Aenable%20the%20training%20and%20validation%20of%20autonomous%20driving%20solutions.%20Multi-modal%0Adatasets%20are%20essential%20to%20attain%20the%20robustness%20and%20high%20accuracy%20required%20by%0Aautonomous%20systems%20in%20applications%20such%20as%20autonomous%20driving.%20As%20deep%0Alearning-based%20solutions%20are%20becoming%20more%20prevalent%20for%20object%20detection%2C%0Aclassification%2C%20and%20tracking%20tasks%2C%20there%20is%20great%20demand%20for%20datasets%0Acombining%20camera%2C%20lidar%2C%20and%20radar%20sensors.%20Existing%20real/synthetic%20datasets%0Afor%20autonomous%20driving%20lack%20synchronized%20data%20collection%20from%20a%20complete%20sensor%0Asuite.%20SCaRL%20provides%20synchronized%20Synthetic%20data%20from%20RGB%2C%20semantic/instance%2C%0Aand%20depth%20Cameras%3B%20Range-Doppler-Azimuth/Elevation%20maps%20and%20raw%20data%20from%0ARadar%3B%20and%203D%20point%20clouds/2D%20maps%20of%20semantic%2C%20depth%20and%20Doppler%20data%20from%0Acoherent%20Lidar.%20SCaRL%20is%20a%20large%20dataset%20based%20on%20the%20CARLA%20Simulator%2C%20which%0Aprovides%20data%20for%20diverse%2C%20dynamic%20scenarios%20and%20traffic%20conditions.%20SCaRL%20is%0Athe%20first%20dataset%20to%20include%20synthetic%20synchronized%20data%20from%20coherent%20Lidar%0Aand%20MIMO%20radar%20sensors.%0A%20%20The%20dataset%20can%20be%20accessed%20here%3A%0Ahttps%3A//fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCaRL-%2520A%2520Synthetic%2520Multi-Modal%2520Dataset%2520for%2520Autonomous%2520Driving%26entry.906535625%3DAvinash%2520Nittur%2520Ramesh%2520and%2520Aitor%2520Correas-Serrano%2520and%2520Mar%25C3%25ADa%2520Gonz%25C3%25A1lez-Huici%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520synthetically%2520generated%2520multi-modal%2520dataset%252C%2520SCaRL%252C%2520to%250Aenable%2520the%2520training%2520and%2520validation%2520of%2520autonomous%2520driving%2520solutions.%2520Multi-modal%250Adatasets%2520are%2520essential%2520to%2520attain%2520the%2520robustness%2520and%2520high%2520accuracy%2520required%2520by%250Aautonomous%2520systems%2520in%2520applications%2520such%2520as%2520autonomous%2520driving.%2520As%2520deep%250Alearning-based%2520solutions%2520are%2520becoming%2520more%2520prevalent%2520for%2520object%2520detection%252C%250Aclassification%252C%2520and%2520tracking%2520tasks%252C%2520there%2520is%2520great%2520demand%2520for%2520datasets%250Acombining%2520camera%252C%2520lidar%252C%2520and%2520radar%2520sensors.%2520Existing%2520real/synthetic%2520datasets%250Afor%2520autonomous%2520driving%2520lack%2520synchronized%2520data%2520collection%2520from%2520a%2520complete%2520sensor%250Asuite.%2520SCaRL%2520provides%2520synchronized%2520Synthetic%2520data%2520from%2520RGB%252C%2520semantic/instance%252C%250Aand%2520depth%2520Cameras%253B%2520Range-Doppler-Azimuth/Elevation%2520maps%2520and%2520raw%2520data%2520from%250ARadar%253B%2520and%25203D%2520point%2520clouds/2D%2520maps%2520of%2520semantic%252C%2520depth%2520and%2520Doppler%2520data%2520from%250Acoherent%2520Lidar.%2520SCaRL%2520is%2520a%2520large%2520dataset%2520based%2520on%2520the%2520CARLA%2520Simulator%252C%2520which%250Aprovides%2520data%2520for%2520diverse%252C%2520dynamic%2520scenarios%2520and%2520traffic%2520conditions.%2520SCaRL%2520is%250Athe%2520first%2520dataset%2520to%2520include%2520synthetic%2520synchronized%2520data%2520from%2520coherent%2520Lidar%250Aand%2520MIMO%2520radar%2520sensors.%250A%2520%2520The%2520dataset%2520can%2520be%2520accessed%2520here%253A%250Ahttps%253A//fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCaRL-%20A%20Synthetic%20Multi-Modal%20Dataset%20for%20Autonomous%20Driving&entry.906535625=Avinash%20Nittur%20Ramesh%20and%20Aitor%20Correas-Serrano%20and%20Mar%C3%ADa%20Gonz%C3%A1lez-Huici&entry.1292438233=%20%20We%20present%20a%20novel%20synthetically%20generated%20multi-modal%20dataset%2C%20SCaRL%2C%20to%0Aenable%20the%20training%20and%20validation%20of%20autonomous%20driving%20solutions.%20Multi-modal%0Adatasets%20are%20essential%20to%20attain%20the%20robustness%20and%20high%20accuracy%20required%20by%0Aautonomous%20systems%20in%20applications%20such%20as%20autonomous%20driving.%20As%20deep%0Alearning-based%20solutions%20are%20becoming%20more%20prevalent%20for%20object%20detection%2C%0Aclassification%2C%20and%20tracking%20tasks%2C%20there%20is%20great%20demand%20for%20datasets%0Acombining%20camera%2C%20lidar%2C%20and%20radar%20sensors.%20Existing%20real/synthetic%20datasets%0Afor%20autonomous%20driving%20lack%20synchronized%20data%20collection%20from%20a%20complete%20sensor%0Asuite.%20SCaRL%20provides%20synchronized%20Synthetic%20data%20from%20RGB%2C%20semantic/instance%2C%0Aand%20depth%20Cameras%3B%20Range-Doppler-Azimuth/Elevation%20maps%20and%20raw%20data%20from%0ARadar%3B%20and%203D%20point%20clouds/2D%20maps%20of%20semantic%2C%20depth%20and%20Doppler%20data%20from%0Acoherent%20Lidar.%20SCaRL%20is%20a%20large%20dataset%20based%20on%20the%20CARLA%20Simulator%2C%20which%0Aprovides%20data%20for%20diverse%2C%20dynamic%20scenarios%20and%20traffic%20conditions.%20SCaRL%20is%0Athe%20first%20dataset%20to%20include%20synthetic%20synchronized%20data%20from%20coherent%20Lidar%0Aand%20MIMO%20radar%20sensors.%0A%20%20The%20dataset%20can%20be%20accessed%20here%3A%0Ahttps%3A//fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17030v1&entry.124074799=Read"},
{"title": "Vista: A Generalizable Driving World Model with High Fidelity and\n  Versatile Controllability", "author": "Shenyuan Gao and Jiazhi Yang and Li Chen and Kashyap Chitta and Yihang Qiu and Andreas Geiger and Jun Zhang and Hongyang Li", "abstract": "  World models can foresee the outcomes of different actions, which is of\nparamount importance for autonomous driving. Nevertheless, existing driving\nworld models still have limitations in generalization to unseen environments,\nprediction fidelity of critical details, and action controllability for\nflexible application. In this paper, we present Vista, a generalizable driving\nworld model with high fidelity and versatile controllability. Based on a\nsystematic diagnosis of existing methods, we introduce several key ingredients\nto address these limitations. To accurately predict real-world dynamics at high\nresolution, we propose two novel losses to promote the learning of moving\ninstances and structural information. We also devise an effective latent\nreplacement approach to inject historical frames as priors for coherent\nlong-horizon rollouts. For action controllability, we incorporate a versatile\nset of controls from high-level intentions (command, goal point) to low-level\nmaneuvers (trajectory, angle, and speed) through an efficient learning\nstrategy. After large-scale training, the capabilities of Vista can seamlessly\ngeneralize to different scenarios. Extensive experiments on multiple datasets\nshow that Vista outperforms the most advanced general-purpose video generator\nin over 70% of comparisons and surpasses the best-performing driving world\nmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize\nthe capacity of Vista itself to establish a generalizable reward for real-world\naction evaluation without accessing the ground truth actions.\n", "link": "http://arxiv.org/abs/2405.17398v1", "date": "2024-05-27", "relevancy": 2.1899, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5508}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&body=Title%3A%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability%0AAuthor%3A%20Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVista%253A%2520A%2520Generalizable%2520Driving%2520World%2520Model%2520with%2520High%2520Fidelity%2520and%250A%2520%2520Versatile%2520Controllability%26entry.906535625%3DShenyuan%2520Gao%2520and%2520Jiazhi%2520Yang%2520and%2520Li%2520Chen%2520and%2520Kashyap%2520Chitta%2520and%2520Yihang%2520Qiu%2520and%2520Andreas%2520Geiger%2520and%2520Jun%2520Zhang%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520World%2520models%2520can%2520foresee%2520the%2520outcomes%2520of%2520different%2520actions%252C%2520which%2520is%2520of%250Aparamount%2520importance%2520for%2520autonomous%2520driving.%2520Nevertheless%252C%2520existing%2520driving%250Aworld%2520models%2520still%2520have%2520limitations%2520in%2520generalization%2520to%2520unseen%2520environments%252C%250Aprediction%2520fidelity%2520of%2520critical%2520details%252C%2520and%2520action%2520controllability%2520for%250Aflexible%2520application.%2520In%2520this%2520paper%252C%2520we%2520present%2520Vista%252C%2520a%2520generalizable%2520driving%250Aworld%2520model%2520with%2520high%2520fidelity%2520and%2520versatile%2520controllability.%2520Based%2520on%2520a%250Asystematic%2520diagnosis%2520of%2520existing%2520methods%252C%2520we%2520introduce%2520several%2520key%2520ingredients%250Ato%2520address%2520these%2520limitations.%2520To%2520accurately%2520predict%2520real-world%2520dynamics%2520at%2520high%250Aresolution%252C%2520we%2520propose%2520two%2520novel%2520losses%2520to%2520promote%2520the%2520learning%2520of%2520moving%250Ainstances%2520and%2520structural%2520information.%2520We%2520also%2520devise%2520an%2520effective%2520latent%250Areplacement%2520approach%2520to%2520inject%2520historical%2520frames%2520as%2520priors%2520for%2520coherent%250Along-horizon%2520rollouts.%2520For%2520action%2520controllability%252C%2520we%2520incorporate%2520a%2520versatile%250Aset%2520of%2520controls%2520from%2520high-level%2520intentions%2520%2528command%252C%2520goal%2520point%2529%2520to%2520low-level%250Amaneuvers%2520%2528trajectory%252C%2520angle%252C%2520and%2520speed%2529%2520through%2520an%2520efficient%2520learning%250Astrategy.%2520After%2520large-scale%2520training%252C%2520the%2520capabilities%2520of%2520Vista%2520can%2520seamlessly%250Ageneralize%2520to%2520different%2520scenarios.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%250Ashow%2520that%2520Vista%2520outperforms%2520the%2520most%2520advanced%2520general-purpose%2520video%2520generator%250Ain%2520over%252070%2525%2520of%2520comparisons%2520and%2520surpasses%2520the%2520best-performing%2520driving%2520world%250Amodel%2520by%252055%2525%2520in%2520FID%2520and%252027%2525%2520in%2520FVD.%2520Moreover%252C%2520for%2520the%2520first%2520time%252C%2520we%2520utilize%250Athe%2520capacity%2520of%2520Vista%2520itself%2520to%2520establish%2520a%2520generalizable%2520reward%2520for%2520real-world%250Aaction%2520evaluation%2520without%2520accessing%2520the%2520ground%2520truth%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&entry.906535625=Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li&entry.1292438233=%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17398v1&entry.124074799=Read"},
{"title": "Graph Neural Networks on Quantum Computers", "author": "Yidong Liao and Xiao-Ming Zhang and Chris Ferrie", "abstract": "  Graph Neural Networks (GNNs) are powerful machine learning models that excel\nat analyzing structured data represented as graphs, demonstrating remarkable\nperformance in applications like social network analysis and recommendation\nsystems. However, classical GNNs face scalability challenges when dealing with\nlarge-scale graphs. This paper proposes frameworks for implementing GNNs on\nquantum computers to potentially address the challenges. We devise quantum\nalgorithms corresponding to the three fundamental types of classical GNNs:\nGraph Convolutional Networks, Graph Attention Networks, and Message-Passing\nGNNs. A complexity analysis of our quantum implementation of the Simplified\nGraph Convolutional (SGC) Network shows potential quantum advantages over its\nclassical counterpart, with significant improvements in time and space\ncomplexities. Our complexities can have trade-offs between the two: when\noptimizing for minimal circuit depth, our quantum SGC achieves logarithmic time\ncomplexity in the input sizes (albeit at the cost of linear space complexity).\nWhen optimizing for minimal qubit usage, the quantum SGC exhibits space\ncomplexity logarithmic in the input sizes, offering an exponential reduction\ncompared to classical SGCs, while still maintaining better time complexity.\nThese results suggest our Quantum GNN frameworks could efficiently process\nlarge-scale graphs. This work paves the way for implementing more advanced\nGraph Neural Network models on quantum computers, opening new possibilities in\nquantum machine learning for analyzing graph-structured data.\n", "link": "http://arxiv.org/abs/2405.17060v1", "date": "2024-05-27", "relevancy": 2.187, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4452}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.439}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20on%20Quantum%20Computers&body=Title%3A%20Graph%20Neural%20Networks%20on%20Quantum%20Computers%0AAuthor%3A%20Yidong%20Liao%20and%20Xiao-Ming%20Zhang%20and%20Chris%20Ferrie%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20machine%20learning%20models%20that%20excel%0Aat%20analyzing%20structured%20data%20represented%20as%20graphs%2C%20demonstrating%20remarkable%0Aperformance%20in%20applications%20like%20social%20network%20analysis%20and%20recommendation%0Asystems.%20However%2C%20classical%20GNNs%20face%20scalability%20challenges%20when%20dealing%20with%0Alarge-scale%20graphs.%20This%20paper%20proposes%20frameworks%20for%20implementing%20GNNs%20on%0Aquantum%20computers%20to%20potentially%20address%20the%20challenges.%20We%20devise%20quantum%0Aalgorithms%20corresponding%20to%20the%20three%20fundamental%20types%20of%20classical%20GNNs%3A%0AGraph%20Convolutional%20Networks%2C%20Graph%20Attention%20Networks%2C%20and%20Message-Passing%0AGNNs.%20A%20complexity%20analysis%20of%20our%20quantum%20implementation%20of%20the%20Simplified%0AGraph%20Convolutional%20%28SGC%29%20Network%20shows%20potential%20quantum%20advantages%20over%20its%0Aclassical%20counterpart%2C%20with%20significant%20improvements%20in%20time%20and%20space%0Acomplexities.%20Our%20complexities%20can%20have%20trade-offs%20between%20the%20two%3A%20when%0Aoptimizing%20for%20minimal%20circuit%20depth%2C%20our%20quantum%20SGC%20achieves%20logarithmic%20time%0Acomplexity%20in%20the%20input%20sizes%20%28albeit%20at%20the%20cost%20of%20linear%20space%20complexity%29.%0AWhen%20optimizing%20for%20minimal%20qubit%20usage%2C%20the%20quantum%20SGC%20exhibits%20space%0Acomplexity%20logarithmic%20in%20the%20input%20sizes%2C%20offering%20an%20exponential%20reduction%0Acompared%20to%20classical%20SGCs%2C%20while%20still%20maintaining%20better%20time%20complexity.%0AThese%20results%20suggest%20our%20Quantum%20GNN%20frameworks%20could%20efficiently%20process%0Alarge-scale%20graphs.%20This%20work%20paves%20the%20way%20for%20implementing%20more%20advanced%0AGraph%20Neural%20Network%20models%20on%20quantum%20computers%2C%20opening%20new%20possibilities%20in%0Aquantum%20machine%20learning%20for%20analyzing%20graph-structured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520on%2520Quantum%2520Computers%26entry.906535625%3DYidong%2520Liao%2520and%2520Xiao-Ming%2520Zhang%2520and%2520Chris%2520Ferrie%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520powerful%2520machine%2520learning%2520models%2520that%2520excel%250Aat%2520analyzing%2520structured%2520data%2520represented%2520as%2520graphs%252C%2520demonstrating%2520remarkable%250Aperformance%2520in%2520applications%2520like%2520social%2520network%2520analysis%2520and%2520recommendation%250Asystems.%2520However%252C%2520classical%2520GNNs%2520face%2520scalability%2520challenges%2520when%2520dealing%2520with%250Alarge-scale%2520graphs.%2520This%2520paper%2520proposes%2520frameworks%2520for%2520implementing%2520GNNs%2520on%250Aquantum%2520computers%2520to%2520potentially%2520address%2520the%2520challenges.%2520We%2520devise%2520quantum%250Aalgorithms%2520corresponding%2520to%2520the%2520three%2520fundamental%2520types%2520of%2520classical%2520GNNs%253A%250AGraph%2520Convolutional%2520Networks%252C%2520Graph%2520Attention%2520Networks%252C%2520and%2520Message-Passing%250AGNNs.%2520A%2520complexity%2520analysis%2520of%2520our%2520quantum%2520implementation%2520of%2520the%2520Simplified%250AGraph%2520Convolutional%2520%2528SGC%2529%2520Network%2520shows%2520potential%2520quantum%2520advantages%2520over%2520its%250Aclassical%2520counterpart%252C%2520with%2520significant%2520improvements%2520in%2520time%2520and%2520space%250Acomplexities.%2520Our%2520complexities%2520can%2520have%2520trade-offs%2520between%2520the%2520two%253A%2520when%250Aoptimizing%2520for%2520minimal%2520circuit%2520depth%252C%2520our%2520quantum%2520SGC%2520achieves%2520logarithmic%2520time%250Acomplexity%2520in%2520the%2520input%2520sizes%2520%2528albeit%2520at%2520the%2520cost%2520of%2520linear%2520space%2520complexity%2529.%250AWhen%2520optimizing%2520for%2520minimal%2520qubit%2520usage%252C%2520the%2520quantum%2520SGC%2520exhibits%2520space%250Acomplexity%2520logarithmic%2520in%2520the%2520input%2520sizes%252C%2520offering%2520an%2520exponential%2520reduction%250Acompared%2520to%2520classical%2520SGCs%252C%2520while%2520still%2520maintaining%2520better%2520time%2520complexity.%250AThese%2520results%2520suggest%2520our%2520Quantum%2520GNN%2520frameworks%2520could%2520efficiently%2520process%250Alarge-scale%2520graphs.%2520This%2520work%2520paves%2520the%2520way%2520for%2520implementing%2520more%2520advanced%250AGraph%2520Neural%2520Network%2520models%2520on%2520quantum%2520computers%252C%2520opening%2520new%2520possibilities%2520in%250Aquantum%2520machine%2520learning%2520for%2520analyzing%2520graph-structured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20on%20Quantum%20Computers&entry.906535625=Yidong%20Liao%20and%20Xiao-Ming%20Zhang%20and%20Chris%20Ferrie&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20machine%20learning%20models%20that%20excel%0Aat%20analyzing%20structured%20data%20represented%20as%20graphs%2C%20demonstrating%20remarkable%0Aperformance%20in%20applications%20like%20social%20network%20analysis%20and%20recommendation%0Asystems.%20However%2C%20classical%20GNNs%20face%20scalability%20challenges%20when%20dealing%20with%0Alarge-scale%20graphs.%20This%20paper%20proposes%20frameworks%20for%20implementing%20GNNs%20on%0Aquantum%20computers%20to%20potentially%20address%20the%20challenges.%20We%20devise%20quantum%0Aalgorithms%20corresponding%20to%20the%20three%20fundamental%20types%20of%20classical%20GNNs%3A%0AGraph%20Convolutional%20Networks%2C%20Graph%20Attention%20Networks%2C%20and%20Message-Passing%0AGNNs.%20A%20complexity%20analysis%20of%20our%20quantum%20implementation%20of%20the%20Simplified%0AGraph%20Convolutional%20%28SGC%29%20Network%20shows%20potential%20quantum%20advantages%20over%20its%0Aclassical%20counterpart%2C%20with%20significant%20improvements%20in%20time%20and%20space%0Acomplexities.%20Our%20complexities%20can%20have%20trade-offs%20between%20the%20two%3A%20when%0Aoptimizing%20for%20minimal%20circuit%20depth%2C%20our%20quantum%20SGC%20achieves%20logarithmic%20time%0Acomplexity%20in%20the%20input%20sizes%20%28albeit%20at%20the%20cost%20of%20linear%20space%20complexity%29.%0AWhen%20optimizing%20for%20minimal%20qubit%20usage%2C%20the%20quantum%20SGC%20exhibits%20space%0Acomplexity%20logarithmic%20in%20the%20input%20sizes%2C%20offering%20an%20exponential%20reduction%0Acompared%20to%20classical%20SGCs%2C%20while%20still%20maintaining%20better%20time%20complexity.%0AThese%20results%20suggest%20our%20Quantum%20GNN%20frameworks%20could%20efficiently%20process%0Alarge-scale%20graphs.%20This%20work%20paves%20the%20way%20for%20implementing%20more%20advanced%0AGraph%20Neural%20Network%20models%20on%20quantum%20computers%2C%20opening%20new%20possibilities%20in%0Aquantum%20machine%20learning%20for%20analyzing%20graph-structured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17060v1&entry.124074799=Read"},
{"title": "Large Language Model as a Policy Teacher for Training Reinforcement\n  Learning Agents", "author": "Zihao Zhou and Bin Hu and Chenyang Zhao and Pu Zhang and Bin Liu", "abstract": "  Recent studies have uncovered the potential of Large Language Models (LLMs)\nin addressing complex sequential decision-making tasks through the provision of\nhigh-level instructions. However, LLM-based agents lack specialization in\ntackling specific target problems, particularly in real-time dynamic\nenvironments. Additionally, deploying an LLM-based agent in practical scenarios\ncan be both costly and time-consuming. On the other hand, reinforcement\nlearning (RL) approaches train agents that specialize in the target task but\noften suffer from low sampling efficiency and high exploration costs. In this\npaper, we introduce a novel framework that addresses these challenges by\ntraining a smaller, specialized student RL agent using instructions from an\nLLM-based teacher agent. By incorporating the guidance from the teacher agent,\nthe student agent can distill the prior knowledge of the LLM into its own\nmodel. Consequently, the student agent can be trained with significantly less\ndata. Moreover, through further training with environment feedback, the student\nagent surpasses the capabilities of its teacher for completing the target task.\nWe conducted experiments on challenging MiniGrid and Habitat environments,\nspecifically designed for embodied AI research, to evaluate the effectiveness\nof our framework. The results clearly demonstrate that our approach achieves\nsuperior performance compared to strong baseline methods. Our code is available\nat https://github.com/ZJLAB-AMMI/LLM4Teach.\n", "link": "http://arxiv.org/abs/2311.13373v6", "date": "2024-05-27", "relevancy": 2.1649, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5851}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5123}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents&body=Title%3A%20Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents%0AAuthor%3A%20Zihao%20Zhou%20and%20Bin%20Hu%20and%20Chenyang%20Zhao%20and%20Pu%20Zhang%20and%20Bin%20Liu%0AAbstract%3A%20%20%20Recent%20studies%20have%20uncovered%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%0Ain%20addressing%20complex%20sequential%20decision-making%20tasks%20through%20the%20provision%20of%0Ahigh-level%20instructions.%20However%2C%20LLM-based%20agents%20lack%20specialization%20in%0Atackling%20specific%20target%20problems%2C%20particularly%20in%20real-time%20dynamic%0Aenvironments.%20Additionally%2C%20deploying%20an%20LLM-based%20agent%20in%20practical%20scenarios%0Acan%20be%20both%20costly%20and%20time-consuming.%20On%20the%20other%20hand%2C%20reinforcement%0Alearning%20%28RL%29%20approaches%20train%20agents%20that%20specialize%20in%20the%20target%20task%20but%0Aoften%20suffer%20from%20low%20sampling%20efficiency%20and%20high%20exploration%20costs.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20framework%20that%20addresses%20these%20challenges%20by%0Atraining%20a%20smaller%2C%20specialized%20student%20RL%20agent%20using%20instructions%20from%20an%0ALLM-based%20teacher%20agent.%20By%20incorporating%20the%20guidance%20from%20the%20teacher%20agent%2C%0Athe%20student%20agent%20can%20distill%20the%20prior%20knowledge%20of%20the%20LLM%20into%20its%20own%0Amodel.%20Consequently%2C%20the%20student%20agent%20can%20be%20trained%20with%20significantly%20less%0Adata.%20Moreover%2C%20through%20further%20training%20with%20environment%20feedback%2C%20the%20student%0Aagent%20surpasses%20the%20capabilities%20of%20its%20teacher%20for%20completing%20the%20target%20task.%0AWe%20conducted%20experiments%20on%20challenging%20MiniGrid%20and%20Habitat%20environments%2C%0Aspecifically%20designed%20for%20embodied%20AI%20research%2C%20to%20evaluate%20the%20effectiveness%0Aof%20our%20framework.%20The%20results%20clearly%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20compared%20to%20strong%20baseline%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ZJLAB-AMMI/LLM4Teach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13373v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520as%2520a%2520Policy%2520Teacher%2520for%2520Training%2520Reinforcement%250A%2520%2520Learning%2520Agents%26entry.906535625%3DZihao%2520Zhou%2520and%2520Bin%2520Hu%2520and%2520Chenyang%2520Zhao%2520and%2520Pu%2520Zhang%2520and%2520Bin%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520uncovered%2520the%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Ain%2520addressing%2520complex%2520sequential%2520decision-making%2520tasks%2520through%2520the%2520provision%2520of%250Ahigh-level%2520instructions.%2520However%252C%2520LLM-based%2520agents%2520lack%2520specialization%2520in%250Atackling%2520specific%2520target%2520problems%252C%2520particularly%2520in%2520real-time%2520dynamic%250Aenvironments.%2520Additionally%252C%2520deploying%2520an%2520LLM-based%2520agent%2520in%2520practical%2520scenarios%250Acan%2520be%2520both%2520costly%2520and%2520time-consuming.%2520On%2520the%2520other%2520hand%252C%2520reinforcement%250Alearning%2520%2528RL%2529%2520approaches%2520train%2520agents%2520that%2520specialize%2520in%2520the%2520target%2520task%2520but%250Aoften%2520suffer%2520from%2520low%2520sampling%2520efficiency%2520and%2520high%2520exploration%2520costs.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520addresses%2520these%2520challenges%2520by%250Atraining%2520a%2520smaller%252C%2520specialized%2520student%2520RL%2520agent%2520using%2520instructions%2520from%2520an%250ALLM-based%2520teacher%2520agent.%2520By%2520incorporating%2520the%2520guidance%2520from%2520the%2520teacher%2520agent%252C%250Athe%2520student%2520agent%2520can%2520distill%2520the%2520prior%2520knowledge%2520of%2520the%2520LLM%2520into%2520its%2520own%250Amodel.%2520Consequently%252C%2520the%2520student%2520agent%2520can%2520be%2520trained%2520with%2520significantly%2520less%250Adata.%2520Moreover%252C%2520through%2520further%2520training%2520with%2520environment%2520feedback%252C%2520the%2520student%250Aagent%2520surpasses%2520the%2520capabilities%2520of%2520its%2520teacher%2520for%2520completing%2520the%2520target%2520task.%250AWe%2520conducted%2520experiments%2520on%2520challenging%2520MiniGrid%2520and%2520Habitat%2520environments%252C%250Aspecifically%2520designed%2520for%2520embodied%2520AI%2520research%252C%2520to%2520evaluate%2520the%2520effectiveness%250Aof%2520our%2520framework.%2520The%2520results%2520clearly%2520demonstrate%2520that%2520our%2520approach%2520achieves%250Asuperior%2520performance%2520compared%2520to%2520strong%2520baseline%2520methods.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/ZJLAB-AMMI/LLM4Teach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13373v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20as%20a%20Policy%20Teacher%20for%20Training%20Reinforcement%0A%20%20Learning%20Agents&entry.906535625=Zihao%20Zhou%20and%20Bin%20Hu%20and%20Chenyang%20Zhao%20and%20Pu%20Zhang%20and%20Bin%20Liu&entry.1292438233=%20%20Recent%20studies%20have%20uncovered%20the%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%0Ain%20addressing%20complex%20sequential%20decision-making%20tasks%20through%20the%20provision%20of%0Ahigh-level%20instructions.%20However%2C%20LLM-based%20agents%20lack%20specialization%20in%0Atackling%20specific%20target%20problems%2C%20particularly%20in%20real-time%20dynamic%0Aenvironments.%20Additionally%2C%20deploying%20an%20LLM-based%20agent%20in%20practical%20scenarios%0Acan%20be%20both%20costly%20and%20time-consuming.%20On%20the%20other%20hand%2C%20reinforcement%0Alearning%20%28RL%29%20approaches%20train%20agents%20that%20specialize%20in%20the%20target%20task%20but%0Aoften%20suffer%20from%20low%20sampling%20efficiency%20and%20high%20exploration%20costs.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20framework%20that%20addresses%20these%20challenges%20by%0Atraining%20a%20smaller%2C%20specialized%20student%20RL%20agent%20using%20instructions%20from%20an%0ALLM-based%20teacher%20agent.%20By%20incorporating%20the%20guidance%20from%20the%20teacher%20agent%2C%0Athe%20student%20agent%20can%20distill%20the%20prior%20knowledge%20of%20the%20LLM%20into%20its%20own%0Amodel.%20Consequently%2C%20the%20student%20agent%20can%20be%20trained%20with%20significantly%20less%0Adata.%20Moreover%2C%20through%20further%20training%20with%20environment%20feedback%2C%20the%20student%0Aagent%20surpasses%20the%20capabilities%20of%20its%20teacher%20for%20completing%20the%20target%20task.%0AWe%20conducted%20experiments%20on%20challenging%20MiniGrid%20and%20Habitat%20environments%2C%0Aspecifically%20designed%20for%20embodied%20AI%20research%2C%20to%20evaluate%20the%20effectiveness%0Aof%20our%20framework.%20The%20results%20clearly%20demonstrate%20that%20our%20approach%20achieves%0Asuperior%20performance%20compared%20to%20strong%20baseline%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ZJLAB-AMMI/LLM4Teach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13373v6&entry.124074799=Read"},
{"title": "Fusing uncalibrated IMUs and handheld smartphone video to reconstruct\n  knee kinematics", "author": "J. D. Peiffer and Kunal Shah and Shawana Anarwala and Kayan Abdou and R. James Cotton", "abstract": "  Video and wearable sensor data provide complementary information about human\nmovement. Video provides a holistic understanding of the entire body in the\nworld while wearable sensors provide high-resolution measurements of specific\nbody segments. A robust method to fuse these modalities and obtain\nbiomechanically accurate kinematics would have substantial utility for clinical\nassessment and monitoring. While multiple video-sensor fusion methods exist,\nmost assume that a time-intensive, and often brittle, sensor-body calibration\nprocess has already been performed. In this work, we present a method to\ncombine handheld smartphone video and uncalibrated wearable sensor data at\ntheir full temporal resolution. Our monocular, video-only, biomechanical\nreconstruction already performs well, with only several degrees of error at the\nknee during walking compared to markerless motion capture. Reconstructing from\na fusion of video and wearable sensor data further reduces this error. We\nvalidate this in a mixture of people with no gait impairments, lower limb\nprosthesis users, and individuals with a history of stroke. We also show that\nsensor data allows tracking through periods of visual occlusion.\n", "link": "http://arxiv.org/abs/2405.17368v1", "date": "2024-05-27", "relevancy": 2.1643, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5819}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5643}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20uncalibrated%20IMUs%20and%20handheld%20smartphone%20video%20to%20reconstruct%0A%20%20knee%20kinematics&body=Title%3A%20Fusing%20uncalibrated%20IMUs%20and%20handheld%20smartphone%20video%20to%20reconstruct%0A%20%20knee%20kinematics%0AAuthor%3A%20J.%20D.%20Peiffer%20and%20Kunal%20Shah%20and%20Shawana%20Anarwala%20and%20Kayan%20Abdou%20and%20R.%20James%20Cotton%0AAbstract%3A%20%20%20Video%20and%20wearable%20sensor%20data%20provide%20complementary%20information%20about%20human%0Amovement.%20Video%20provides%20a%20holistic%20understanding%20of%20the%20entire%20body%20in%20the%0Aworld%20while%20wearable%20sensors%20provide%20high-resolution%20measurements%20of%20specific%0Abody%20segments.%20A%20robust%20method%20to%20fuse%20these%20modalities%20and%20obtain%0Abiomechanically%20accurate%20kinematics%20would%20have%20substantial%20utility%20for%20clinical%0Aassessment%20and%20monitoring.%20While%20multiple%20video-sensor%20fusion%20methods%20exist%2C%0Amost%20assume%20that%20a%20time-intensive%2C%20and%20often%20brittle%2C%20sensor-body%20calibration%0Aprocess%20has%20already%20been%20performed.%20In%20this%20work%2C%20we%20present%20a%20method%20to%0Acombine%20handheld%20smartphone%20video%20and%20uncalibrated%20wearable%20sensor%20data%20at%0Atheir%20full%20temporal%20resolution.%20Our%20monocular%2C%20video-only%2C%20biomechanical%0Areconstruction%20already%20performs%20well%2C%20with%20only%20several%20degrees%20of%20error%20at%20the%0Aknee%20during%20walking%20compared%20to%20markerless%20motion%20capture.%20Reconstructing%20from%0Aa%20fusion%20of%20video%20and%20wearable%20sensor%20data%20further%20reduces%20this%20error.%20We%0Avalidate%20this%20in%20a%20mixture%20of%20people%20with%20no%20gait%20impairments%2C%20lower%20limb%0Aprosthesis%20users%2C%20and%20individuals%20with%20a%20history%20of%20stroke.%20We%20also%20show%20that%0Asensor%20data%20allows%20tracking%20through%20periods%20of%20visual%20occlusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520uncalibrated%2520IMUs%2520and%2520handheld%2520smartphone%2520video%2520to%2520reconstruct%250A%2520%2520knee%2520kinematics%26entry.906535625%3DJ.%2520D.%2520Peiffer%2520and%2520Kunal%2520Shah%2520and%2520Shawana%2520Anarwala%2520and%2520Kayan%2520Abdou%2520and%2520R.%2520James%2520Cotton%26entry.1292438233%3D%2520%2520Video%2520and%2520wearable%2520sensor%2520data%2520provide%2520complementary%2520information%2520about%2520human%250Amovement.%2520Video%2520provides%2520a%2520holistic%2520understanding%2520of%2520the%2520entire%2520body%2520in%2520the%250Aworld%2520while%2520wearable%2520sensors%2520provide%2520high-resolution%2520measurements%2520of%2520specific%250Abody%2520segments.%2520A%2520robust%2520method%2520to%2520fuse%2520these%2520modalities%2520and%2520obtain%250Abiomechanically%2520accurate%2520kinematics%2520would%2520have%2520substantial%2520utility%2520for%2520clinical%250Aassessment%2520and%2520monitoring.%2520While%2520multiple%2520video-sensor%2520fusion%2520methods%2520exist%252C%250Amost%2520assume%2520that%2520a%2520time-intensive%252C%2520and%2520often%2520brittle%252C%2520sensor-body%2520calibration%250Aprocess%2520has%2520already%2520been%2520performed.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520method%2520to%250Acombine%2520handheld%2520smartphone%2520video%2520and%2520uncalibrated%2520wearable%2520sensor%2520data%2520at%250Atheir%2520full%2520temporal%2520resolution.%2520Our%2520monocular%252C%2520video-only%252C%2520biomechanical%250Areconstruction%2520already%2520performs%2520well%252C%2520with%2520only%2520several%2520degrees%2520of%2520error%2520at%2520the%250Aknee%2520during%2520walking%2520compared%2520to%2520markerless%2520motion%2520capture.%2520Reconstructing%2520from%250Aa%2520fusion%2520of%2520video%2520and%2520wearable%2520sensor%2520data%2520further%2520reduces%2520this%2520error.%2520We%250Avalidate%2520this%2520in%2520a%2520mixture%2520of%2520people%2520with%2520no%2520gait%2520impairments%252C%2520lower%2520limb%250Aprosthesis%2520users%252C%2520and%2520individuals%2520with%2520a%2520history%2520of%2520stroke.%2520We%2520also%2520show%2520that%250Asensor%2520data%2520allows%2520tracking%2520through%2520periods%2520of%2520visual%2520occlusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20uncalibrated%20IMUs%20and%20handheld%20smartphone%20video%20to%20reconstruct%0A%20%20knee%20kinematics&entry.906535625=J.%20D.%20Peiffer%20and%20Kunal%20Shah%20and%20Shawana%20Anarwala%20and%20Kayan%20Abdou%20and%20R.%20James%20Cotton&entry.1292438233=%20%20Video%20and%20wearable%20sensor%20data%20provide%20complementary%20information%20about%20human%0Amovement.%20Video%20provides%20a%20holistic%20understanding%20of%20the%20entire%20body%20in%20the%0Aworld%20while%20wearable%20sensors%20provide%20high-resolution%20measurements%20of%20specific%0Abody%20segments.%20A%20robust%20method%20to%20fuse%20these%20modalities%20and%20obtain%0Abiomechanically%20accurate%20kinematics%20would%20have%20substantial%20utility%20for%20clinical%0Aassessment%20and%20monitoring.%20While%20multiple%20video-sensor%20fusion%20methods%20exist%2C%0Amost%20assume%20that%20a%20time-intensive%2C%20and%20often%20brittle%2C%20sensor-body%20calibration%0Aprocess%20has%20already%20been%20performed.%20In%20this%20work%2C%20we%20present%20a%20method%20to%0Acombine%20handheld%20smartphone%20video%20and%20uncalibrated%20wearable%20sensor%20data%20at%0Atheir%20full%20temporal%20resolution.%20Our%20monocular%2C%20video-only%2C%20biomechanical%0Areconstruction%20already%20performs%20well%2C%20with%20only%20several%20degrees%20of%20error%20at%20the%0Aknee%20during%20walking%20compared%20to%20markerless%20motion%20capture.%20Reconstructing%20from%0Aa%20fusion%20of%20video%20and%20wearable%20sensor%20data%20further%20reduces%20this%20error.%20We%0Avalidate%20this%20in%20a%20mixture%20of%20people%20with%20no%20gait%20impairments%2C%20lower%20limb%0Aprosthesis%20users%2C%20and%20individuals%20with%20a%20history%20of%20stroke.%20We%20also%20show%20that%0Asensor%20data%20allows%20tracking%20through%20periods%20of%20visual%20occlusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17368v1&entry.124074799=Read"},
{"title": "Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional\n  Encoding", "author": "Niloofar Azizi and Mohsen Fayyaz and Horst Bischof", "abstract": "  Understanding human behavior fundamentally relies on accurate 3D human pose\nestimation. Graph Convolutional Networks (GCNs) have recently shown promising\nadvancements, delivering state-of-the-art performance with rather lightweight\narchitectures. In the context of graph-structured data, leveraging the\neigenvectors of the graph Laplacian matrix for positional encoding is\neffective. Yet, the approach does not specify how to handle scenarios where\nedges in the input graph are missing. To this end, we propose a novel\npositional encoding technique, PerturbPE, that extracts consistent and regular\ncomponents from the eigenbasis. Our method involves applying multiple\nperturbations and taking their average to extract the consistent and regular\ncomponent from the eigenbasis. PerturbPE leverages the Rayleigh-Schrodinger\nPerturbation Theorem (RSPT) for calculating the perturbed eigenvectors.\nEmploying this labeling technique enhances the robustness and generalizability\nof the model. Our results support our theoretical findings, e.g. our\nexperimental analysis observed a performance enhancement of up to $12\\%$ on the\nHuman3.6M dataset in instances where occlusion resulted in the absence of one\nedge. Furthermore, our novel approach significantly enhances performance in\nscenarios where two edges are missing, setting a new benchmark for\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2405.17397v1", "date": "2024-05-27", "relevancy": 2.1637, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion%20Handling%20in%203D%20Human%20Pose%20Estimation%20with%20Perturbed%20Positional%0A%20%20Encoding&body=Title%3A%20Occlusion%20Handling%20in%203D%20Human%20Pose%20Estimation%20with%20Perturbed%20Positional%0A%20%20Encoding%0AAuthor%3A%20Niloofar%20Azizi%20and%20Mohsen%20Fayyaz%20and%20Horst%20Bischof%0AAbstract%3A%20%20%20Understanding%20human%20behavior%20fundamentally%20relies%20on%20accurate%203D%20human%20pose%0Aestimation.%20Graph%20Convolutional%20Networks%20%28GCNs%29%20have%20recently%20shown%20promising%0Aadvancements%2C%20delivering%20state-of-the-art%20performance%20with%20rather%20lightweight%0Aarchitectures.%20In%20the%20context%20of%20graph-structured%20data%2C%20leveraging%20the%0Aeigenvectors%20of%20the%20graph%20Laplacian%20matrix%20for%20positional%20encoding%20is%0Aeffective.%20Yet%2C%20the%20approach%20does%20not%20specify%20how%20to%20handle%20scenarios%20where%0Aedges%20in%20the%20input%20graph%20are%20missing.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Apositional%20encoding%20technique%2C%20PerturbPE%2C%20that%20extracts%20consistent%20and%20regular%0Acomponents%20from%20the%20eigenbasis.%20Our%20method%20involves%20applying%20multiple%0Aperturbations%20and%20taking%20their%20average%20to%20extract%20the%20consistent%20and%20regular%0Acomponent%20from%20the%20eigenbasis.%20PerturbPE%20leverages%20the%20Rayleigh-Schrodinger%0APerturbation%20Theorem%20%28RSPT%29%20for%20calculating%20the%20perturbed%20eigenvectors.%0AEmploying%20this%20labeling%20technique%20enhances%20the%20robustness%20and%20generalizability%0Aof%20the%20model.%20Our%20results%20support%20our%20theoretical%20findings%2C%20e.g.%20our%0Aexperimental%20analysis%20observed%20a%20performance%20enhancement%20of%20up%20to%20%2412%5C%25%24%20on%20the%0AHuman3.6M%20dataset%20in%20instances%20where%20occlusion%20resulted%20in%20the%20absence%20of%20one%0Aedge.%20Furthermore%2C%20our%20novel%20approach%20significantly%20enhances%20performance%20in%0Ascenarios%20where%20two%20edges%20are%20missing%2C%20setting%20a%20new%20benchmark%20for%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion%2520Handling%2520in%25203D%2520Human%2520Pose%2520Estimation%2520with%2520Perturbed%2520Positional%250A%2520%2520Encoding%26entry.906535625%3DNiloofar%2520Azizi%2520and%2520Mohsen%2520Fayyaz%2520and%2520Horst%2520Bischof%26entry.1292438233%3D%2520%2520Understanding%2520human%2520behavior%2520fundamentally%2520relies%2520on%2520accurate%25203D%2520human%2520pose%250Aestimation.%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520have%2520recently%2520shown%2520promising%250Aadvancements%252C%2520delivering%2520state-of-the-art%2520performance%2520with%2520rather%2520lightweight%250Aarchitectures.%2520In%2520the%2520context%2520of%2520graph-structured%2520data%252C%2520leveraging%2520the%250Aeigenvectors%2520of%2520the%2520graph%2520Laplacian%2520matrix%2520for%2520positional%2520encoding%2520is%250Aeffective.%2520Yet%252C%2520the%2520approach%2520does%2520not%2520specify%2520how%2520to%2520handle%2520scenarios%2520where%250Aedges%2520in%2520the%2520input%2520graph%2520are%2520missing.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Apositional%2520encoding%2520technique%252C%2520PerturbPE%252C%2520that%2520extracts%2520consistent%2520and%2520regular%250Acomponents%2520from%2520the%2520eigenbasis.%2520Our%2520method%2520involves%2520applying%2520multiple%250Aperturbations%2520and%2520taking%2520their%2520average%2520to%2520extract%2520the%2520consistent%2520and%2520regular%250Acomponent%2520from%2520the%2520eigenbasis.%2520PerturbPE%2520leverages%2520the%2520Rayleigh-Schrodinger%250APerturbation%2520Theorem%2520%2528RSPT%2529%2520for%2520calculating%2520the%2520perturbed%2520eigenvectors.%250AEmploying%2520this%2520labeling%2520technique%2520enhances%2520the%2520robustness%2520and%2520generalizability%250Aof%2520the%2520model.%2520Our%2520results%2520support%2520our%2520theoretical%2520findings%252C%2520e.g.%2520our%250Aexperimental%2520analysis%2520observed%2520a%2520performance%2520enhancement%2520of%2520up%2520to%2520%252412%255C%2525%2524%2520on%2520the%250AHuman3.6M%2520dataset%2520in%2520instances%2520where%2520occlusion%2520resulted%2520in%2520the%2520absence%2520of%2520one%250Aedge.%2520Furthermore%252C%2520our%2520novel%2520approach%2520significantly%2520enhances%2520performance%2520in%250Ascenarios%2520where%2520two%2520edges%2520are%2520missing%252C%2520setting%2520a%2520new%2520benchmark%2520for%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion%20Handling%20in%203D%20Human%20Pose%20Estimation%20with%20Perturbed%20Positional%0A%20%20Encoding&entry.906535625=Niloofar%20Azizi%20and%20Mohsen%20Fayyaz%20and%20Horst%20Bischof&entry.1292438233=%20%20Understanding%20human%20behavior%20fundamentally%20relies%20on%20accurate%203D%20human%20pose%0Aestimation.%20Graph%20Convolutional%20Networks%20%28GCNs%29%20have%20recently%20shown%20promising%0Aadvancements%2C%20delivering%20state-of-the-art%20performance%20with%20rather%20lightweight%0Aarchitectures.%20In%20the%20context%20of%20graph-structured%20data%2C%20leveraging%20the%0Aeigenvectors%20of%20the%20graph%20Laplacian%20matrix%20for%20positional%20encoding%20is%0Aeffective.%20Yet%2C%20the%20approach%20does%20not%20specify%20how%20to%20handle%20scenarios%20where%0Aedges%20in%20the%20input%20graph%20are%20missing.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Apositional%20encoding%20technique%2C%20PerturbPE%2C%20that%20extracts%20consistent%20and%20regular%0Acomponents%20from%20the%20eigenbasis.%20Our%20method%20involves%20applying%20multiple%0Aperturbations%20and%20taking%20their%20average%20to%20extract%20the%20consistent%20and%20regular%0Acomponent%20from%20the%20eigenbasis.%20PerturbPE%20leverages%20the%20Rayleigh-Schrodinger%0APerturbation%20Theorem%20%28RSPT%29%20for%20calculating%20the%20perturbed%20eigenvectors.%0AEmploying%20this%20labeling%20technique%20enhances%20the%20robustness%20and%20generalizability%0Aof%20the%20model.%20Our%20results%20support%20our%20theoretical%20findings%2C%20e.g.%20our%0Aexperimental%20analysis%20observed%20a%20performance%20enhancement%20of%20up%20to%20%2412%5C%25%24%20on%20the%0AHuman3.6M%20dataset%20in%20instances%20where%20occlusion%20resulted%20in%20the%20absence%20of%20one%0Aedge.%20Furthermore%2C%20our%20novel%20approach%20significantly%20enhances%20performance%20in%0Ascenarios%20where%20two%20edges%20are%20missing%2C%20setting%20a%20new%20benchmark%20for%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17397v1&entry.124074799=Read"},
{"title": "A Recipe for Unbounded Data Augmentation in Visual Reinforcement\n  Learning", "author": "Abdulaziz Almuzairee and Nicklas Hansen and Henrik I. Christensen", "abstract": "  $Q$-learning algorithms are appealing for real-world applications due to\ntheir data-efficiency, but they are very prone to overfitting and training\ninstabilities when trained from visual observations. Prior work, namely SVEA,\nfinds that selective application of data augmentation can improve the visual\ngeneralization of RL agents without destabilizing training. We revisit its\nrecipe for data augmentation, and find an assumption that limits its\neffectiveness to augmentations of a photometric nature. Addressing these\nlimitations, we propose a generalized recipe, SADA, that works with wider\nvarieties of augmentations. We benchmark its effectiveness on DMC-GB2 -- our\nproposed extension of the popular DMControl Generalization Benchmark -- as well\nas tasks from Meta-World and the Distracting Control Suite, and find that our\nmethod, SADA, greatly improves training stability and generalization of RL\nagents across a diverse set of augmentations. Visualizations, code, and\nbenchmark: see https://aalmuzairee.github.io/SADA/\n", "link": "http://arxiv.org/abs/2405.17416v1", "date": "2024-05-27", "relevancy": 2.1632, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5571}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5295}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Recipe%20for%20Unbounded%20Data%20Augmentation%20in%20Visual%20Reinforcement%0A%20%20Learning&body=Title%3A%20A%20Recipe%20for%20Unbounded%20Data%20Augmentation%20in%20Visual%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Abdulaziz%20Almuzairee%20and%20Nicklas%20Hansen%20and%20Henrik%20I.%20Christensen%0AAbstract%3A%20%20%20%24Q%24-learning%20algorithms%20are%20appealing%20for%20real-world%20applications%20due%20to%0Atheir%20data-efficiency%2C%20but%20they%20are%20very%20prone%20to%20overfitting%20and%20training%0Ainstabilities%20when%20trained%20from%20visual%20observations.%20Prior%20work%2C%20namely%20SVEA%2C%0Afinds%20that%20selective%20application%20of%20data%20augmentation%20can%20improve%20the%20visual%0Ageneralization%20of%20RL%20agents%20without%20destabilizing%20training.%20We%20revisit%20its%0Arecipe%20for%20data%20augmentation%2C%20and%20find%20an%20assumption%20that%20limits%20its%0Aeffectiveness%20to%20augmentations%20of%20a%20photometric%20nature.%20Addressing%20these%0Alimitations%2C%20we%20propose%20a%20generalized%20recipe%2C%20SADA%2C%20that%20works%20with%20wider%0Avarieties%20of%20augmentations.%20We%20benchmark%20its%20effectiveness%20on%20DMC-GB2%20--%20our%0Aproposed%20extension%20of%20the%20popular%20DMControl%20Generalization%20Benchmark%20--%20as%20well%0Aas%20tasks%20from%20Meta-World%20and%20the%20Distracting%20Control%20Suite%2C%20and%20find%20that%20our%0Amethod%2C%20SADA%2C%20greatly%20improves%20training%20stability%20and%20generalization%20of%20RL%0Aagents%20across%20a%20diverse%20set%20of%20augmentations.%20Visualizations%2C%20code%2C%20and%0Abenchmark%3A%20see%20https%3A//aalmuzairee.github.io/SADA/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Recipe%2520for%2520Unbounded%2520Data%2520Augmentation%2520in%2520Visual%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DAbdulaziz%2520Almuzairee%2520and%2520Nicklas%2520Hansen%2520and%2520Henrik%2520I.%2520Christensen%26entry.1292438233%3D%2520%2520%2524Q%2524-learning%2520algorithms%2520are%2520appealing%2520for%2520real-world%2520applications%2520due%2520to%250Atheir%2520data-efficiency%252C%2520but%2520they%2520are%2520very%2520prone%2520to%2520overfitting%2520and%2520training%250Ainstabilities%2520when%2520trained%2520from%2520visual%2520observations.%2520Prior%2520work%252C%2520namely%2520SVEA%252C%250Afinds%2520that%2520selective%2520application%2520of%2520data%2520augmentation%2520can%2520improve%2520the%2520visual%250Ageneralization%2520of%2520RL%2520agents%2520without%2520destabilizing%2520training.%2520We%2520revisit%2520its%250Arecipe%2520for%2520data%2520augmentation%252C%2520and%2520find%2520an%2520assumption%2520that%2520limits%2520its%250Aeffectiveness%2520to%2520augmentations%2520of%2520a%2520photometric%2520nature.%2520Addressing%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520generalized%2520recipe%252C%2520SADA%252C%2520that%2520works%2520with%2520wider%250Avarieties%2520of%2520augmentations.%2520We%2520benchmark%2520its%2520effectiveness%2520on%2520DMC-GB2%2520--%2520our%250Aproposed%2520extension%2520of%2520the%2520popular%2520DMControl%2520Generalization%2520Benchmark%2520--%2520as%2520well%250Aas%2520tasks%2520from%2520Meta-World%2520and%2520the%2520Distracting%2520Control%2520Suite%252C%2520and%2520find%2520that%2520our%250Amethod%252C%2520SADA%252C%2520greatly%2520improves%2520training%2520stability%2520and%2520generalization%2520of%2520RL%250Aagents%2520across%2520a%2520diverse%2520set%2520of%2520augmentations.%2520Visualizations%252C%2520code%252C%2520and%250Abenchmark%253A%2520see%2520https%253A//aalmuzairee.github.io/SADA/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Recipe%20for%20Unbounded%20Data%20Augmentation%20in%20Visual%20Reinforcement%0A%20%20Learning&entry.906535625=Abdulaziz%20Almuzairee%20and%20Nicklas%20Hansen%20and%20Henrik%20I.%20Christensen&entry.1292438233=%20%20%24Q%24-learning%20algorithms%20are%20appealing%20for%20real-world%20applications%20due%20to%0Atheir%20data-efficiency%2C%20but%20they%20are%20very%20prone%20to%20overfitting%20and%20training%0Ainstabilities%20when%20trained%20from%20visual%20observations.%20Prior%20work%2C%20namely%20SVEA%2C%0Afinds%20that%20selective%20application%20of%20data%20augmentation%20can%20improve%20the%20visual%0Ageneralization%20of%20RL%20agents%20without%20destabilizing%20training.%20We%20revisit%20its%0Arecipe%20for%20data%20augmentation%2C%20and%20find%20an%20assumption%20that%20limits%20its%0Aeffectiveness%20to%20augmentations%20of%20a%20photometric%20nature.%20Addressing%20these%0Alimitations%2C%20we%20propose%20a%20generalized%20recipe%2C%20SADA%2C%20that%20works%20with%20wider%0Avarieties%20of%20augmentations.%20We%20benchmark%20its%20effectiveness%20on%20DMC-GB2%20--%20our%0Aproposed%20extension%20of%20the%20popular%20DMControl%20Generalization%20Benchmark%20--%20as%20well%0Aas%20tasks%20from%20Meta-World%20and%20the%20Distracting%20Control%20Suite%2C%20and%20find%20that%20our%0Amethod%2C%20SADA%2C%20greatly%20improves%20training%20stability%20and%20generalization%20of%20RL%0Aagents%20across%20a%20diverse%20set%20of%20augmentations.%20Visualizations%2C%20code%2C%20and%0Abenchmark%3A%20see%20https%3A//aalmuzairee.github.io/SADA/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17416v1&entry.124074799=Read"},
{"title": "Advancing Spiking Neural Networks towards Multiscale Spatiotemporal\n  Interaction Learning", "author": "Yimeng Shan and Malu Zhang and Rui-jie Zhu and Xuerui Qiu and Jason K. Eshraghian and Haicheng Qu", "abstract": "  Recent advancements in neuroscience research have propelled the development\nof Spiking Neural Networks (SNNs), which not only have the potential to further\nadvance neuroscience research but also serve as an energy-efficient alternative\nto Artificial Neural Networks (ANNs) due to their spike-driven characteristics.\nHowever, previous studies often neglected the multiscale information and its\nspatiotemporal correlation between event data, leading SNN models to\napproximate each frame of input events as static images. We hypothesize that\nthis oversimplification significantly contributes to the performance gap\nbetween SNNs and traditional ANNs. To address this issue, we have designed a\nSpiking Multiscale Attention (SMA) module that captures multiscale\nspatiotemporal interaction information. Furthermore, we developed a\nregularization method named Attention ZoneOut (AZO), which utilizes\nspatiotemporal attention weights to reduce the model's generalization error\nthrough pseudo-ensemble training. Our approach has achieved state-of-the-art\nresults on mainstream neural morphology datasets. Additionally, we have reached\na performance of 77.1% on the Imagenet-1K dataset using a 104-layer ResNet\narchitecture enhanced with SMA and AZO. This achievement confirms the\nstate-of-the-art performance of SNNs with non-transformer architectures and\nunderscores the effectiveness of our method in bridging the performance gap\nbetween SNN models and traditional ANN models.\n", "link": "http://arxiv.org/abs/2405.13672v2", "date": "2024-05-27", "relevancy": 2.1612, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5385}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Spiking%20Neural%20Networks%20towards%20Multiscale%20Spatiotemporal%0A%20%20Interaction%20Learning&body=Title%3A%20Advancing%20Spiking%20Neural%20Networks%20towards%20Multiscale%20Spatiotemporal%0A%20%20Interaction%20Learning%0AAuthor%3A%20Yimeng%20Shan%20and%20Malu%20Zhang%20and%20Rui-jie%20Zhu%20and%20Xuerui%20Qiu%20and%20Jason%20K.%20Eshraghian%20and%20Haicheng%20Qu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20neuroscience%20research%20have%20propelled%20the%20development%0Aof%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20which%20not%20only%20have%20the%20potential%20to%20further%0Aadvance%20neuroscience%20research%20but%20also%20serve%20as%20an%20energy-efficient%20alternative%0Ato%20Artificial%20Neural%20Networks%20%28ANNs%29%20due%20to%20their%20spike-driven%20characteristics.%0AHowever%2C%20previous%20studies%20often%20neglected%20the%20multiscale%20information%20and%20its%0Aspatiotemporal%20correlation%20between%20event%20data%2C%20leading%20SNN%20models%20to%0Aapproximate%20each%20frame%20of%20input%20events%20as%20static%20images.%20We%20hypothesize%20that%0Athis%20oversimplification%20significantly%20contributes%20to%20the%20performance%20gap%0Abetween%20SNNs%20and%20traditional%20ANNs.%20To%20address%20this%20issue%2C%20we%20have%20designed%20a%0ASpiking%20Multiscale%20Attention%20%28SMA%29%20module%20that%20captures%20multiscale%0Aspatiotemporal%20interaction%20information.%20Furthermore%2C%20we%20developed%20a%0Aregularization%20method%20named%20Attention%20ZoneOut%20%28AZO%29%2C%20which%20utilizes%0Aspatiotemporal%20attention%20weights%20to%20reduce%20the%20model%27s%20generalization%20error%0Athrough%20pseudo-ensemble%20training.%20Our%20approach%20has%20achieved%20state-of-the-art%0Aresults%20on%20mainstream%20neural%20morphology%20datasets.%20Additionally%2C%20we%20have%20reached%0Aa%20performance%20of%2077.1%25%20on%20the%20Imagenet-1K%20dataset%20using%20a%20104-layer%20ResNet%0Aarchitecture%20enhanced%20with%20SMA%20and%20AZO.%20This%20achievement%20confirms%20the%0Astate-of-the-art%20performance%20of%20SNNs%20with%20non-transformer%20architectures%20and%0Aunderscores%20the%20effectiveness%20of%20our%20method%20in%20bridging%20the%20performance%20gap%0Abetween%20SNN%20models%20and%20traditional%20ANN%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Spiking%2520Neural%2520Networks%2520towards%2520Multiscale%2520Spatiotemporal%250A%2520%2520Interaction%2520Learning%26entry.906535625%3DYimeng%2520Shan%2520and%2520Malu%2520Zhang%2520and%2520Rui-jie%2520Zhu%2520and%2520Xuerui%2520Qiu%2520and%2520Jason%2520K.%2520Eshraghian%2520and%2520Haicheng%2520Qu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520neuroscience%2520research%2520have%2520propelled%2520the%2520development%250Aof%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%252C%2520which%2520not%2520only%2520have%2520the%2520potential%2520to%2520further%250Aadvance%2520neuroscience%2520research%2520but%2520also%2520serve%2520as%2520an%2520energy-efficient%2520alternative%250Ato%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%2520due%2520to%2520their%2520spike-driven%2520characteristics.%250AHowever%252C%2520previous%2520studies%2520often%2520neglected%2520the%2520multiscale%2520information%2520and%2520its%250Aspatiotemporal%2520correlation%2520between%2520event%2520data%252C%2520leading%2520SNN%2520models%2520to%250Aapproximate%2520each%2520frame%2520of%2520input%2520events%2520as%2520static%2520images.%2520We%2520hypothesize%2520that%250Athis%2520oversimplification%2520significantly%2520contributes%2520to%2520the%2520performance%2520gap%250Abetween%2520SNNs%2520and%2520traditional%2520ANNs.%2520To%2520address%2520this%2520issue%252C%2520we%2520have%2520designed%2520a%250ASpiking%2520Multiscale%2520Attention%2520%2528SMA%2529%2520module%2520that%2520captures%2520multiscale%250Aspatiotemporal%2520interaction%2520information.%2520Furthermore%252C%2520we%2520developed%2520a%250Aregularization%2520method%2520named%2520Attention%2520ZoneOut%2520%2528AZO%2529%252C%2520which%2520utilizes%250Aspatiotemporal%2520attention%2520weights%2520to%2520reduce%2520the%2520model%2527s%2520generalization%2520error%250Athrough%2520pseudo-ensemble%2520training.%2520Our%2520approach%2520has%2520achieved%2520state-of-the-art%250Aresults%2520on%2520mainstream%2520neural%2520morphology%2520datasets.%2520Additionally%252C%2520we%2520have%2520reached%250Aa%2520performance%2520of%252077.1%2525%2520on%2520the%2520Imagenet-1K%2520dataset%2520using%2520a%2520104-layer%2520ResNet%250Aarchitecture%2520enhanced%2520with%2520SMA%2520and%2520AZO.%2520This%2520achievement%2520confirms%2520the%250Astate-of-the-art%2520performance%2520of%2520SNNs%2520with%2520non-transformer%2520architectures%2520and%250Aunderscores%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520bridging%2520the%2520performance%2520gap%250Abetween%2520SNN%2520models%2520and%2520traditional%2520ANN%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Spiking%20Neural%20Networks%20towards%20Multiscale%20Spatiotemporal%0A%20%20Interaction%20Learning&entry.906535625=Yimeng%20Shan%20and%20Malu%20Zhang%20and%20Rui-jie%20Zhu%20and%20Xuerui%20Qiu%20and%20Jason%20K.%20Eshraghian%20and%20Haicheng%20Qu&entry.1292438233=%20%20Recent%20advancements%20in%20neuroscience%20research%20have%20propelled%20the%20development%0Aof%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20which%20not%20only%20have%20the%20potential%20to%20further%0Aadvance%20neuroscience%20research%20but%20also%20serve%20as%20an%20energy-efficient%20alternative%0Ato%20Artificial%20Neural%20Networks%20%28ANNs%29%20due%20to%20their%20spike-driven%20characteristics.%0AHowever%2C%20previous%20studies%20often%20neglected%20the%20multiscale%20information%20and%20its%0Aspatiotemporal%20correlation%20between%20event%20data%2C%20leading%20SNN%20models%20to%0Aapproximate%20each%20frame%20of%20input%20events%20as%20static%20images.%20We%20hypothesize%20that%0Athis%20oversimplification%20significantly%20contributes%20to%20the%20performance%20gap%0Abetween%20SNNs%20and%20traditional%20ANNs.%20To%20address%20this%20issue%2C%20we%20have%20designed%20a%0ASpiking%20Multiscale%20Attention%20%28SMA%29%20module%20that%20captures%20multiscale%0Aspatiotemporal%20interaction%20information.%20Furthermore%2C%20we%20developed%20a%0Aregularization%20method%20named%20Attention%20ZoneOut%20%28AZO%29%2C%20which%20utilizes%0Aspatiotemporal%20attention%20weights%20to%20reduce%20the%20model%27s%20generalization%20error%0Athrough%20pseudo-ensemble%20training.%20Our%20approach%20has%20achieved%20state-of-the-art%0Aresults%20on%20mainstream%20neural%20morphology%20datasets.%20Additionally%2C%20we%20have%20reached%0Aa%20performance%20of%2077.1%25%20on%20the%20Imagenet-1K%20dataset%20using%20a%20104-layer%20ResNet%0Aarchitecture%20enhanced%20with%20SMA%20and%20AZO.%20This%20achievement%20confirms%20the%0Astate-of-the-art%20performance%20of%20SNNs%20with%20non-transformer%20architectures%20and%0Aunderscores%20the%20effectiveness%20of%20our%20method%20in%20bridging%20the%20performance%20gap%0Abetween%20SNN%20models%20and%20traditional%20ANN%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13672v2&entry.124074799=Read"},
{"title": "Content-Style Decoupling for Unsupervised Makeup Transfer without\n  Generating Pseudo Ground Truth", "author": "Zhaoyang Sun and Shengwu Xiong and Yaxiong Chen and Yi Rong", "abstract": "  The absence of real targets to guide the model training is one of the main\nproblems with the makeup transfer task. Most existing methods tackle this\nproblem by synthesizing pseudo ground truths (PGTs). However, the generated\nPGTs are often sub-optimal and their imprecision will eventually lead to\nperformance degradation. To alleviate this issue, in this paper, we propose a\nnovel Content-Style Decoupled Makeup Transfer (CSD-MT) method, which works in a\npurely unsupervised manner and thus eliminates the negative effects of\ngenerating PGTs. Specifically, based on the frequency characteristics analysis,\nwe assume that the low-frequency (LF) component of a face image is more\nassociated with its makeup style information, while the high-frequency (HF)\ncomponent is more related to its content details. This assumption allows CSD-MT\nto decouple the content and makeup style information in each face image through\nthe frequency decomposition. After that, CSD-MT realizes makeup transfer by\nmaximizing the consistency of these two types of information between the\ntransferred result and input images, respectively. Two newly designed loss\nfunctions are also introduced to further improve the transfer performance.\nExtensive quantitative and qualitative analyses show the effectiveness of our\nCSD-MT method. Our code is available at\nhttps://github.com/Snowfallingplum/CSD-MT.\n", "link": "http://arxiv.org/abs/2405.17240v1", "date": "2024-05-27", "relevancy": 2.1584, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.557}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5508}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-Style%20Decoupling%20for%20Unsupervised%20Makeup%20Transfer%20without%0A%20%20Generating%20Pseudo%20Ground%20Truth&body=Title%3A%20Content-Style%20Decoupling%20for%20Unsupervised%20Makeup%20Transfer%20without%0A%20%20Generating%20Pseudo%20Ground%20Truth%0AAuthor%3A%20Zhaoyang%20Sun%20and%20Shengwu%20Xiong%20and%20Yaxiong%20Chen%20and%20Yi%20Rong%0AAbstract%3A%20%20%20The%20absence%20of%20real%20targets%20to%20guide%20the%20model%20training%20is%20one%20of%20the%20main%0Aproblems%20with%20the%20makeup%20transfer%20task.%20Most%20existing%20methods%20tackle%20this%0Aproblem%20by%20synthesizing%20pseudo%20ground%20truths%20%28PGTs%29.%20However%2C%20the%20generated%0APGTs%20are%20often%20sub-optimal%20and%20their%20imprecision%20will%20eventually%20lead%20to%0Aperformance%20degradation.%20To%20alleviate%20this%20issue%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20Content-Style%20Decoupled%20Makeup%20Transfer%20%28CSD-MT%29%20method%2C%20which%20works%20in%20a%0Apurely%20unsupervised%20manner%20and%20thus%20eliminates%20the%20negative%20effects%20of%0Agenerating%20PGTs.%20Specifically%2C%20based%20on%20the%20frequency%20characteristics%20analysis%2C%0Awe%20assume%20that%20the%20low-frequency%20%28LF%29%20component%20of%20a%20face%20image%20is%20more%0Aassociated%20with%20its%20makeup%20style%20information%2C%20while%20the%20high-frequency%20%28HF%29%0Acomponent%20is%20more%20related%20to%20its%20content%20details.%20This%20assumption%20allows%20CSD-MT%0Ato%20decouple%20the%20content%20and%20makeup%20style%20information%20in%20each%20face%20image%20through%0Athe%20frequency%20decomposition.%20After%20that%2C%20CSD-MT%20realizes%20makeup%20transfer%20by%0Amaximizing%20the%20consistency%20of%20these%20two%20types%20of%20information%20between%20the%0Atransferred%20result%20and%20input%20images%2C%20respectively.%20Two%20newly%20designed%20loss%0Afunctions%20are%20also%20introduced%20to%20further%20improve%20the%20transfer%20performance.%0AExtensive%20quantitative%20and%20qualitative%20analyses%20show%20the%20effectiveness%20of%20our%0ACSD-MT%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Snowfallingplum/CSD-MT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-Style%2520Decoupling%2520for%2520Unsupervised%2520Makeup%2520Transfer%2520without%250A%2520%2520Generating%2520Pseudo%2520Ground%2520Truth%26entry.906535625%3DZhaoyang%2520Sun%2520and%2520Shengwu%2520Xiong%2520and%2520Yaxiong%2520Chen%2520and%2520Yi%2520Rong%26entry.1292438233%3D%2520%2520The%2520absence%2520of%2520real%2520targets%2520to%2520guide%2520the%2520model%2520training%2520is%2520one%2520of%2520the%2520main%250Aproblems%2520with%2520the%2520makeup%2520transfer%2520task.%2520Most%2520existing%2520methods%2520tackle%2520this%250Aproblem%2520by%2520synthesizing%2520pseudo%2520ground%2520truths%2520%2528PGTs%2529.%2520However%252C%2520the%2520generated%250APGTs%2520are%2520often%2520sub-optimal%2520and%2520their%2520imprecision%2520will%2520eventually%2520lead%2520to%250Aperformance%2520degradation.%2520To%2520alleviate%2520this%2520issue%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520Content-Style%2520Decoupled%2520Makeup%2520Transfer%2520%2528CSD-MT%2529%2520method%252C%2520which%2520works%2520in%2520a%250Apurely%2520unsupervised%2520manner%2520and%2520thus%2520eliminates%2520the%2520negative%2520effects%2520of%250Agenerating%2520PGTs.%2520Specifically%252C%2520based%2520on%2520the%2520frequency%2520characteristics%2520analysis%252C%250Awe%2520assume%2520that%2520the%2520low-frequency%2520%2528LF%2529%2520component%2520of%2520a%2520face%2520image%2520is%2520more%250Aassociated%2520with%2520its%2520makeup%2520style%2520information%252C%2520while%2520the%2520high-frequency%2520%2528HF%2529%250Acomponent%2520is%2520more%2520related%2520to%2520its%2520content%2520details.%2520This%2520assumption%2520allows%2520CSD-MT%250Ato%2520decouple%2520the%2520content%2520and%2520makeup%2520style%2520information%2520in%2520each%2520face%2520image%2520through%250Athe%2520frequency%2520decomposition.%2520After%2520that%252C%2520CSD-MT%2520realizes%2520makeup%2520transfer%2520by%250Amaximizing%2520the%2520consistency%2520of%2520these%2520two%2520types%2520of%2520information%2520between%2520the%250Atransferred%2520result%2520and%2520input%2520images%252C%2520respectively.%2520Two%2520newly%2520designed%2520loss%250Afunctions%2520are%2520also%2520introduced%2520to%2520further%2520improve%2520the%2520transfer%2520performance.%250AExtensive%2520quantitative%2520and%2520qualitative%2520analyses%2520show%2520the%2520effectiveness%2520of%2520our%250ACSD-MT%2520method.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Snowfallingplum/CSD-MT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-Style%20Decoupling%20for%20Unsupervised%20Makeup%20Transfer%20without%0A%20%20Generating%20Pseudo%20Ground%20Truth&entry.906535625=Zhaoyang%20Sun%20and%20Shengwu%20Xiong%20and%20Yaxiong%20Chen%20and%20Yi%20Rong&entry.1292438233=%20%20The%20absence%20of%20real%20targets%20to%20guide%20the%20model%20training%20is%20one%20of%20the%20main%0Aproblems%20with%20the%20makeup%20transfer%20task.%20Most%20existing%20methods%20tackle%20this%0Aproblem%20by%20synthesizing%20pseudo%20ground%20truths%20%28PGTs%29.%20However%2C%20the%20generated%0APGTs%20are%20often%20sub-optimal%20and%20their%20imprecision%20will%20eventually%20lead%20to%0Aperformance%20degradation.%20To%20alleviate%20this%20issue%2C%20in%20this%20paper%2C%20we%20propose%20a%0Anovel%20Content-Style%20Decoupled%20Makeup%20Transfer%20%28CSD-MT%29%20method%2C%20which%20works%20in%20a%0Apurely%20unsupervised%20manner%20and%20thus%20eliminates%20the%20negative%20effects%20of%0Agenerating%20PGTs.%20Specifically%2C%20based%20on%20the%20frequency%20characteristics%20analysis%2C%0Awe%20assume%20that%20the%20low-frequency%20%28LF%29%20component%20of%20a%20face%20image%20is%20more%0Aassociated%20with%20its%20makeup%20style%20information%2C%20while%20the%20high-frequency%20%28HF%29%0Acomponent%20is%20more%20related%20to%20its%20content%20details.%20This%20assumption%20allows%20CSD-MT%0Ato%20decouple%20the%20content%20and%20makeup%20style%20information%20in%20each%20face%20image%20through%0Athe%20frequency%20decomposition.%20After%20that%2C%20CSD-MT%20realizes%20makeup%20transfer%20by%0Amaximizing%20the%20consistency%20of%20these%20two%20types%20of%20information%20between%20the%0Atransferred%20result%20and%20input%20images%2C%20respectively.%20Two%20newly%20designed%20loss%0Afunctions%20are%20also%20introduced%20to%20further%20improve%20the%20transfer%20performance.%0AExtensive%20quantitative%20and%20qualitative%20analyses%20show%20the%20effectiveness%20of%20our%0ACSD-MT%20method.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Snowfallingplum/CSD-MT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17240v1&entry.124074799=Read"},
{"title": "An Introduction to Vision-Language Modeling", "author": "Florian Bordes and Richard Yuanzhe Pang and Anurag Ajay and Alexander C. Li and Adrien Bardes and Suzanne Petryk and Oscar Ma\u00f1as and Zhiqiu Lin and Anas Mahmoud and Bargav Jayaraman and Mark Ibrahim and Melissa Hall and Yunyang Xiong and Jonathan Lebensold and Candace Ross and Srihari Jayakumar and Chuan Guo and Diane Bouchacourt and Haider Al-Tahan and Karthik Padthe and Vasu Sharma and Hu Xu and Xiaoqing Ellen Tan and Megan Richards and Samuel Lavoie and Pietro Astolfi and Reyhane Askari Hemmat and Jun Chen and Kushal Tirumala and Rim Assouel and Mazda Moayeri and Arjang Talattof and Kamalika Chaudhuri and Zechun Liu and Xilun Chen and Quentin Garrido and Karen Ullrich and Aishwarya Agrawal and Kate Saenko and Asli Celikyilmaz and Vikas Chandra", "abstract": "  Following the recent popularity of Large Language Models (LLMs), several\nattempts have been made to extend them to the visual domain. From having a\nvisual assistant that could guide us through unfamiliar environments to\ngenerative models that produce images using only a high-level text description,\nthe vision-language model (VLM) applications will significantly impact our\nrelationship with technology. However, there are many challenges that need to\nbe addressed to improve the reliability of those models. While language is\ndiscrete, vision evolves in a much higher dimensional space in which concepts\ncannot always be easily discretized. To better understand the mechanics behind\nmapping vision to language, we present this introduction to VLMs which we hope\nwill help anyone who would like to enter the field. First, we introduce what\nVLMs are, how they work, and how to train them. Then, we present and discuss\napproaches to evaluate VLMs. Although this work primarily focuses on mapping\nimages to language, we also discuss extending VLMs to videos.\n", "link": "http://arxiv.org/abs/2405.17247v1", "date": "2024-05-27", "relevancy": 2.1571, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.561}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5274}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Introduction%20to%20Vision-Language%20Modeling&body=Title%3A%20An%20Introduction%20to%20Vision-Language%20Modeling%0AAuthor%3A%20Florian%20Bordes%20and%20Richard%20Yuanzhe%20Pang%20and%20Anurag%20Ajay%20and%20Alexander%20C.%20Li%20and%20Adrien%20Bardes%20and%20Suzanne%20Petryk%20and%20Oscar%20Ma%C3%B1as%20and%20Zhiqiu%20Lin%20and%20Anas%20Mahmoud%20and%20Bargav%20Jayaraman%20and%20Mark%20Ibrahim%20and%20Melissa%20Hall%20and%20Yunyang%20Xiong%20and%20Jonathan%20Lebensold%20and%20Candace%20Ross%20and%20Srihari%20Jayakumar%20and%20Chuan%20Guo%20and%20Diane%20Bouchacourt%20and%20Haider%20Al-Tahan%20and%20Karthik%20Padthe%20and%20Vasu%20Sharma%20and%20Hu%20Xu%20and%20Xiaoqing%20Ellen%20Tan%20and%20Megan%20Richards%20and%20Samuel%20Lavoie%20and%20Pietro%20Astolfi%20and%20Reyhane%20Askari%20Hemmat%20and%20Jun%20Chen%20and%20Kushal%20Tirumala%20and%20Rim%20Assouel%20and%20Mazda%20Moayeri%20and%20Arjang%20Talattof%20and%20Kamalika%20Chaudhuri%20and%20Zechun%20Liu%20and%20Xilun%20Chen%20and%20Quentin%20Garrido%20and%20Karen%20Ullrich%20and%20Aishwarya%20Agrawal%20and%20Kate%20Saenko%20and%20Asli%20Celikyilmaz%20and%20Vikas%20Chandra%0AAbstract%3A%20%20%20Following%20the%20recent%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20several%0Aattempts%20have%20been%20made%20to%20extend%20them%20to%20the%20visual%20domain.%20From%20having%20a%0Avisual%20assistant%20that%20could%20guide%20us%20through%20unfamiliar%20environments%20to%0Agenerative%20models%20that%20produce%20images%20using%20only%20a%20high-level%20text%20description%2C%0Athe%20vision-language%20model%20%28VLM%29%20applications%20will%20significantly%20impact%20our%0Arelationship%20with%20technology.%20However%2C%20there%20are%20many%20challenges%20that%20need%20to%0Abe%20addressed%20to%20improve%20the%20reliability%20of%20those%20models.%20While%20language%20is%0Adiscrete%2C%20vision%20evolves%20in%20a%20much%20higher%20dimensional%20space%20in%20which%20concepts%0Acannot%20always%20be%20easily%20discretized.%20To%20better%20understand%20the%20mechanics%20behind%0Amapping%20vision%20to%20language%2C%20we%20present%20this%20introduction%20to%20VLMs%20which%20we%20hope%0Awill%20help%20anyone%20who%20would%20like%20to%20enter%20the%20field.%20First%2C%20we%20introduce%20what%0AVLMs%20are%2C%20how%20they%20work%2C%20and%20how%20to%20train%20them.%20Then%2C%20we%20present%20and%20discuss%0Aapproaches%20to%20evaluate%20VLMs.%20Although%20this%20work%20primarily%20focuses%20on%20mapping%0Aimages%20to%20language%2C%20we%20also%20discuss%20extending%20VLMs%20to%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Introduction%2520to%2520Vision-Language%2520Modeling%26entry.906535625%3DFlorian%2520Bordes%2520and%2520Richard%2520Yuanzhe%2520Pang%2520and%2520Anurag%2520Ajay%2520and%2520Alexander%2520C.%2520Li%2520and%2520Adrien%2520Bardes%2520and%2520Suzanne%2520Petryk%2520and%2520Oscar%2520Ma%25C3%25B1as%2520and%2520Zhiqiu%2520Lin%2520and%2520Anas%2520Mahmoud%2520and%2520Bargav%2520Jayaraman%2520and%2520Mark%2520Ibrahim%2520and%2520Melissa%2520Hall%2520and%2520Yunyang%2520Xiong%2520and%2520Jonathan%2520Lebensold%2520and%2520Candace%2520Ross%2520and%2520Srihari%2520Jayakumar%2520and%2520Chuan%2520Guo%2520and%2520Diane%2520Bouchacourt%2520and%2520Haider%2520Al-Tahan%2520and%2520Karthik%2520Padthe%2520and%2520Vasu%2520Sharma%2520and%2520Hu%2520Xu%2520and%2520Xiaoqing%2520Ellen%2520Tan%2520and%2520Megan%2520Richards%2520and%2520Samuel%2520Lavoie%2520and%2520Pietro%2520Astolfi%2520and%2520Reyhane%2520Askari%2520Hemmat%2520and%2520Jun%2520Chen%2520and%2520Kushal%2520Tirumala%2520and%2520Rim%2520Assouel%2520and%2520Mazda%2520Moayeri%2520and%2520Arjang%2520Talattof%2520and%2520Kamalika%2520Chaudhuri%2520and%2520Zechun%2520Liu%2520and%2520Xilun%2520Chen%2520and%2520Quentin%2520Garrido%2520and%2520Karen%2520Ullrich%2520and%2520Aishwarya%2520Agrawal%2520and%2520Kate%2520Saenko%2520and%2520Asli%2520Celikyilmaz%2520and%2520Vikas%2520Chandra%26entry.1292438233%3D%2520%2520Following%2520the%2520recent%2520popularity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520several%250Aattempts%2520have%2520been%2520made%2520to%2520extend%2520them%2520to%2520the%2520visual%2520domain.%2520From%2520having%2520a%250Avisual%2520assistant%2520that%2520could%2520guide%2520us%2520through%2520unfamiliar%2520environments%2520to%250Agenerative%2520models%2520that%2520produce%2520images%2520using%2520only%2520a%2520high-level%2520text%2520description%252C%250Athe%2520vision-language%2520model%2520%2528VLM%2529%2520applications%2520will%2520significantly%2520impact%2520our%250Arelationship%2520with%2520technology.%2520However%252C%2520there%2520are%2520many%2520challenges%2520that%2520need%2520to%250Abe%2520addressed%2520to%2520improve%2520the%2520reliability%2520of%2520those%2520models.%2520While%2520language%2520is%250Adiscrete%252C%2520vision%2520evolves%2520in%2520a%2520much%2520higher%2520dimensional%2520space%2520in%2520which%2520concepts%250Acannot%2520always%2520be%2520easily%2520discretized.%2520To%2520better%2520understand%2520the%2520mechanics%2520behind%250Amapping%2520vision%2520to%2520language%252C%2520we%2520present%2520this%2520introduction%2520to%2520VLMs%2520which%2520we%2520hope%250Awill%2520help%2520anyone%2520who%2520would%2520like%2520to%2520enter%2520the%2520field.%2520First%252C%2520we%2520introduce%2520what%250AVLMs%2520are%252C%2520how%2520they%2520work%252C%2520and%2520how%2520to%2520train%2520them.%2520Then%252C%2520we%2520present%2520and%2520discuss%250Aapproaches%2520to%2520evaluate%2520VLMs.%2520Although%2520this%2520work%2520primarily%2520focuses%2520on%2520mapping%250Aimages%2520to%2520language%252C%2520we%2520also%2520discuss%2520extending%2520VLMs%2520to%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Introduction%20to%20Vision-Language%20Modeling&entry.906535625=Florian%20Bordes%20and%20Richard%20Yuanzhe%20Pang%20and%20Anurag%20Ajay%20and%20Alexander%20C.%20Li%20and%20Adrien%20Bardes%20and%20Suzanne%20Petryk%20and%20Oscar%20Ma%C3%B1as%20and%20Zhiqiu%20Lin%20and%20Anas%20Mahmoud%20and%20Bargav%20Jayaraman%20and%20Mark%20Ibrahim%20and%20Melissa%20Hall%20and%20Yunyang%20Xiong%20and%20Jonathan%20Lebensold%20and%20Candace%20Ross%20and%20Srihari%20Jayakumar%20and%20Chuan%20Guo%20and%20Diane%20Bouchacourt%20and%20Haider%20Al-Tahan%20and%20Karthik%20Padthe%20and%20Vasu%20Sharma%20and%20Hu%20Xu%20and%20Xiaoqing%20Ellen%20Tan%20and%20Megan%20Richards%20and%20Samuel%20Lavoie%20and%20Pietro%20Astolfi%20and%20Reyhane%20Askari%20Hemmat%20and%20Jun%20Chen%20and%20Kushal%20Tirumala%20and%20Rim%20Assouel%20and%20Mazda%20Moayeri%20and%20Arjang%20Talattof%20and%20Kamalika%20Chaudhuri%20and%20Zechun%20Liu%20and%20Xilun%20Chen%20and%20Quentin%20Garrido%20and%20Karen%20Ullrich%20and%20Aishwarya%20Agrawal%20and%20Kate%20Saenko%20and%20Asli%20Celikyilmaz%20and%20Vikas%20Chandra&entry.1292438233=%20%20Following%20the%20recent%20popularity%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20several%0Aattempts%20have%20been%20made%20to%20extend%20them%20to%20the%20visual%20domain.%20From%20having%20a%0Avisual%20assistant%20that%20could%20guide%20us%20through%20unfamiliar%20environments%20to%0Agenerative%20models%20that%20produce%20images%20using%20only%20a%20high-level%20text%20description%2C%0Athe%20vision-language%20model%20%28VLM%29%20applications%20will%20significantly%20impact%20our%0Arelationship%20with%20technology.%20However%2C%20there%20are%20many%20challenges%20that%20need%20to%0Abe%20addressed%20to%20improve%20the%20reliability%20of%20those%20models.%20While%20language%20is%0Adiscrete%2C%20vision%20evolves%20in%20a%20much%20higher%20dimensional%20space%20in%20which%20concepts%0Acannot%20always%20be%20easily%20discretized.%20To%20better%20understand%20the%20mechanics%20behind%0Amapping%20vision%20to%20language%2C%20we%20present%20this%20introduction%20to%20VLMs%20which%20we%20hope%0Awill%20help%20anyone%20who%20would%20like%20to%20enter%20the%20field.%20First%2C%20we%20introduce%20what%0AVLMs%20are%2C%20how%20they%20work%2C%20and%20how%20to%20train%20them.%20Then%2C%20we%20present%20and%20discuss%0Aapproaches%20to%20evaluate%20VLMs.%20Although%20this%20work%20primarily%20focuses%20on%20mapping%0Aimages%20to%20language%2C%20we%20also%20discuss%20extending%20VLMs%20to%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17247v1&entry.124074799=Read"},
{"title": "The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient\n  Discretization for Diffusion Models", "author": "Saravanan Kandasamy and Dheeraj Nagaraj", "abstract": "  Langevin Dynamics is a Stochastic Differential Equation (SDE) central to\nsampling and generative modeling and is implemented via time discretization.\nLangevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the\nsimplest and most studied algorithm. LMC can suffer from slow convergence -\nrequiring a large number of steps of small step-size to obtain good quality\nsamples. This becomes stark in the case of diffusion models where a large\nnumber of steps gives the best samples, but the quality degrades rapidly with\nsmaller number of steps. Randomized Midpoint Method has been recently proposed\nas a better discretization of Langevin dynamics for sampling from strongly\nlog-concave distributions. However, important applications such as diffusion\nmodels involve non-log concave densities and contain time varying drift. We\npropose its variant, the Poisson Midpoint Method, which approximates a small\nstep-size LMC with large step-sizes. We prove that this can obtain a quadratic\nspeed up of LMC under very weak assumptions. We apply our method to diffusion\nmodels for image generation and show that it maintains the quality of DDPM with\n1000 neural network calls with just 50-80 neural network calls and outperforms\nODE based methods with similar compute.\n", "link": "http://arxiv.org/abs/2405.17068v1", "date": "2024-05-27", "relevancy": 2.1474, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Poisson%20Midpoint%20Method%20for%20Langevin%20Dynamics%3A%20Provably%20Efficient%0A%20%20Discretization%20for%20Diffusion%20Models&body=Title%3A%20The%20Poisson%20Midpoint%20Method%20for%20Langevin%20Dynamics%3A%20Provably%20Efficient%0A%20%20Discretization%20for%20Diffusion%20Models%0AAuthor%3A%20Saravanan%20Kandasamy%20and%20Dheeraj%20Nagaraj%0AAbstract%3A%20%20%20Langevin%20Dynamics%20is%20a%20Stochastic%20Differential%20Equation%20%28SDE%29%20central%20to%0Asampling%20and%20generative%20modeling%20and%20is%20implemented%20via%20time%20discretization.%0ALangevin%20Monte%20Carlo%20%28LMC%29%2C%20based%20on%20the%20Euler-Maruyama%20discretization%2C%20is%20the%0Asimplest%20and%20most%20studied%20algorithm.%20LMC%20can%20suffer%20from%20slow%20convergence%20-%0Arequiring%20a%20large%20number%20of%20steps%20of%20small%20step-size%20to%20obtain%20good%20quality%0Asamples.%20This%20becomes%20stark%20in%20the%20case%20of%20diffusion%20models%20where%20a%20large%0Anumber%20of%20steps%20gives%20the%20best%20samples%2C%20but%20the%20quality%20degrades%20rapidly%20with%0Asmaller%20number%20of%20steps.%20Randomized%20Midpoint%20Method%20has%20been%20recently%20proposed%0Aas%20a%20better%20discretization%20of%20Langevin%20dynamics%20for%20sampling%20from%20strongly%0Alog-concave%20distributions.%20However%2C%20important%20applications%20such%20as%20diffusion%0Amodels%20involve%20non-log%20concave%20densities%20and%20contain%20time%20varying%20drift.%20We%0Apropose%20its%20variant%2C%20the%20Poisson%20Midpoint%20Method%2C%20which%20approximates%20a%20small%0Astep-size%20LMC%20with%20large%20step-sizes.%20We%20prove%20that%20this%20can%20obtain%20a%20quadratic%0Aspeed%20up%20of%20LMC%20under%20very%20weak%20assumptions.%20We%20apply%20our%20method%20to%20diffusion%0Amodels%20for%20image%20generation%20and%20show%20that%20it%20maintains%20the%20quality%20of%20DDPM%20with%0A1000%20neural%20network%20calls%20with%20just%2050-80%20neural%20network%20calls%20and%20outperforms%0AODE%20based%20methods%20with%20similar%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Poisson%2520Midpoint%2520Method%2520for%2520Langevin%2520Dynamics%253A%2520Provably%2520Efficient%250A%2520%2520Discretization%2520for%2520Diffusion%2520Models%26entry.906535625%3DSaravanan%2520Kandasamy%2520and%2520Dheeraj%2520Nagaraj%26entry.1292438233%3D%2520%2520Langevin%2520Dynamics%2520is%2520a%2520Stochastic%2520Differential%2520Equation%2520%2528SDE%2529%2520central%2520to%250Asampling%2520and%2520generative%2520modeling%2520and%2520is%2520implemented%2520via%2520time%2520discretization.%250ALangevin%2520Monte%2520Carlo%2520%2528LMC%2529%252C%2520based%2520on%2520the%2520Euler-Maruyama%2520discretization%252C%2520is%2520the%250Asimplest%2520and%2520most%2520studied%2520algorithm.%2520LMC%2520can%2520suffer%2520from%2520slow%2520convergence%2520-%250Arequiring%2520a%2520large%2520number%2520of%2520steps%2520of%2520small%2520step-size%2520to%2520obtain%2520good%2520quality%250Asamples.%2520This%2520becomes%2520stark%2520in%2520the%2520case%2520of%2520diffusion%2520models%2520where%2520a%2520large%250Anumber%2520of%2520steps%2520gives%2520the%2520best%2520samples%252C%2520but%2520the%2520quality%2520degrades%2520rapidly%2520with%250Asmaller%2520number%2520of%2520steps.%2520Randomized%2520Midpoint%2520Method%2520has%2520been%2520recently%2520proposed%250Aas%2520a%2520better%2520discretization%2520of%2520Langevin%2520dynamics%2520for%2520sampling%2520from%2520strongly%250Alog-concave%2520distributions.%2520However%252C%2520important%2520applications%2520such%2520as%2520diffusion%250Amodels%2520involve%2520non-log%2520concave%2520densities%2520and%2520contain%2520time%2520varying%2520drift.%2520We%250Apropose%2520its%2520variant%252C%2520the%2520Poisson%2520Midpoint%2520Method%252C%2520which%2520approximates%2520a%2520small%250Astep-size%2520LMC%2520with%2520large%2520step-sizes.%2520We%2520prove%2520that%2520this%2520can%2520obtain%2520a%2520quadratic%250Aspeed%2520up%2520of%2520LMC%2520under%2520very%2520weak%2520assumptions.%2520We%2520apply%2520our%2520method%2520to%2520diffusion%250Amodels%2520for%2520image%2520generation%2520and%2520show%2520that%2520it%2520maintains%2520the%2520quality%2520of%2520DDPM%2520with%250A1000%2520neural%2520network%2520calls%2520with%2520just%252050-80%2520neural%2520network%2520calls%2520and%2520outperforms%250AODE%2520based%2520methods%2520with%2520similar%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Poisson%20Midpoint%20Method%20for%20Langevin%20Dynamics%3A%20Provably%20Efficient%0A%20%20Discretization%20for%20Diffusion%20Models&entry.906535625=Saravanan%20Kandasamy%20and%20Dheeraj%20Nagaraj&entry.1292438233=%20%20Langevin%20Dynamics%20is%20a%20Stochastic%20Differential%20Equation%20%28SDE%29%20central%20to%0Asampling%20and%20generative%20modeling%20and%20is%20implemented%20via%20time%20discretization.%0ALangevin%20Monte%20Carlo%20%28LMC%29%2C%20based%20on%20the%20Euler-Maruyama%20discretization%2C%20is%20the%0Asimplest%20and%20most%20studied%20algorithm.%20LMC%20can%20suffer%20from%20slow%20convergence%20-%0Arequiring%20a%20large%20number%20of%20steps%20of%20small%20step-size%20to%20obtain%20good%20quality%0Asamples.%20This%20becomes%20stark%20in%20the%20case%20of%20diffusion%20models%20where%20a%20large%0Anumber%20of%20steps%20gives%20the%20best%20samples%2C%20but%20the%20quality%20degrades%20rapidly%20with%0Asmaller%20number%20of%20steps.%20Randomized%20Midpoint%20Method%20has%20been%20recently%20proposed%0Aas%20a%20better%20discretization%20of%20Langevin%20dynamics%20for%20sampling%20from%20strongly%0Alog-concave%20distributions.%20However%2C%20important%20applications%20such%20as%20diffusion%0Amodels%20involve%20non-log%20concave%20densities%20and%20contain%20time%20varying%20drift.%20We%0Apropose%20its%20variant%2C%20the%20Poisson%20Midpoint%20Method%2C%20which%20approximates%20a%20small%0Astep-size%20LMC%20with%20large%20step-sizes.%20We%20prove%20that%20this%20can%20obtain%20a%20quadratic%0Aspeed%20up%20of%20LMC%20under%20very%20weak%20assumptions.%20We%20apply%20our%20method%20to%20diffusion%0Amodels%20for%20image%20generation%20and%20show%20that%20it%20maintains%20the%20quality%20of%20DDPM%20with%0A1000%20neural%20network%20calls%20with%20just%2050-80%20neural%20network%20calls%20and%20outperforms%0AODE%20based%20methods%20with%20similar%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17068v1&entry.124074799=Read"},
{"title": "PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes", "author": "Tomohiro Hayase and Braun Sacha and Hikari Yanagawa and Itsuki Orito and Yuichi Hiroi", "abstract": "  Social VR platforms enable social, economic, and creative activities by\nallowing users to create and share their own virtual spaces. In social VR,\nphotography within a VR scene is an important indicator of visitors'\nactivities. Although automatic identification of photo spots within a VR scene\ncan facilitate the process of creating a VR scene and enhance the visitor\nexperience, there are challenges in quantitatively evaluating photos taken in\nthe VR scene and efficiently exploring the large VR scene. We propose PanoTree,\nan automated photo-spot explorer in VR scenes. To assess the aesthetics of\nimages captured in VR scenes, a deep scoring network is trained on a large\ndataset of photos collected by a social VR platform to determine whether humans\nare likely to take similar photos. Furthermore, we propose a Hierarchical\nOptimistic Optimization (HOO)-based search algorithm to efficiently explore 3D\nVR spaces with the reward from the scoring network. Our user study shows that\nthe scoring network achieves human-level performance in distinguishing randomly\ntaken images from those taken by humans. In addition, we show applications\nusing the explored photo spots, such as automatic thumbnail generation, support\nfor VR world creation, and visitor flow planning within a VR scene.\n", "link": "http://arxiv.org/abs/2405.17136v1", "date": "2024-05-27", "relevancy": 2.1217, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes&body=Title%3A%20PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes%0AAuthor%3A%20Tomohiro%20Hayase%20and%20Braun%20Sacha%20and%20Hikari%20Yanagawa%20and%20Itsuki%20Orito%20and%20Yuichi%20Hiroi%0AAbstract%3A%20%20%20Social%20VR%20platforms%20enable%20social%2C%20economic%2C%20and%20creative%20activities%20by%0Aallowing%20users%20to%20create%20and%20share%20their%20own%20virtual%20spaces.%20In%20social%20VR%2C%0Aphotography%20within%20a%20VR%20scene%20is%20an%20important%20indicator%20of%20visitors%27%0Aactivities.%20Although%20automatic%20identification%20of%20photo%20spots%20within%20a%20VR%20scene%0Acan%20facilitate%20the%20process%20of%20creating%20a%20VR%20scene%20and%20enhance%20the%20visitor%0Aexperience%2C%20there%20are%20challenges%20in%20quantitatively%20evaluating%20photos%20taken%20in%0Athe%20VR%20scene%20and%20efficiently%20exploring%20the%20large%20VR%20scene.%20We%20propose%20PanoTree%2C%0Aan%20automated%20photo-spot%20explorer%20in%20VR%20scenes.%20To%20assess%20the%20aesthetics%20of%0Aimages%20captured%20in%20VR%20scenes%2C%20a%20deep%20scoring%20network%20is%20trained%20on%20a%20large%0Adataset%20of%20photos%20collected%20by%20a%20social%20VR%20platform%20to%20determine%20whether%20humans%0Aare%20likely%20to%20take%20similar%20photos.%20Furthermore%2C%20we%20propose%20a%20Hierarchical%0AOptimistic%20Optimization%20%28HOO%29-based%20search%20algorithm%20to%20efficiently%20explore%203D%0AVR%20spaces%20with%20the%20reward%20from%20the%20scoring%20network.%20Our%20user%20study%20shows%20that%0Athe%20scoring%20network%20achieves%20human-level%20performance%20in%20distinguishing%20randomly%0Ataken%20images%20from%20those%20taken%20by%20humans.%20In%20addition%2C%20we%20show%20applications%0Ausing%20the%20explored%20photo%20spots%2C%20such%20as%20automatic%20thumbnail%20generation%2C%20support%0Afor%20VR%20world%20creation%2C%20and%20visitor%20flow%20planning%20within%20a%20VR%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoTree%253A%2520Autonomous%2520Photo-Spot%2520Explorer%2520in%2520Virtual%2520Reality%2520Scenes%26entry.906535625%3DTomohiro%2520Hayase%2520and%2520Braun%2520Sacha%2520and%2520Hikari%2520Yanagawa%2520and%2520Itsuki%2520Orito%2520and%2520Yuichi%2520Hiroi%26entry.1292438233%3D%2520%2520Social%2520VR%2520platforms%2520enable%2520social%252C%2520economic%252C%2520and%2520creative%2520activities%2520by%250Aallowing%2520users%2520to%2520create%2520and%2520share%2520their%2520own%2520virtual%2520spaces.%2520In%2520social%2520VR%252C%250Aphotography%2520within%2520a%2520VR%2520scene%2520is%2520an%2520important%2520indicator%2520of%2520visitors%2527%250Aactivities.%2520Although%2520automatic%2520identification%2520of%2520photo%2520spots%2520within%2520a%2520VR%2520scene%250Acan%2520facilitate%2520the%2520process%2520of%2520creating%2520a%2520VR%2520scene%2520and%2520enhance%2520the%2520visitor%250Aexperience%252C%2520there%2520are%2520challenges%2520in%2520quantitatively%2520evaluating%2520photos%2520taken%2520in%250Athe%2520VR%2520scene%2520and%2520efficiently%2520exploring%2520the%2520large%2520VR%2520scene.%2520We%2520propose%2520PanoTree%252C%250Aan%2520automated%2520photo-spot%2520explorer%2520in%2520VR%2520scenes.%2520To%2520assess%2520the%2520aesthetics%2520of%250Aimages%2520captured%2520in%2520VR%2520scenes%252C%2520a%2520deep%2520scoring%2520network%2520is%2520trained%2520on%2520a%2520large%250Adataset%2520of%2520photos%2520collected%2520by%2520a%2520social%2520VR%2520platform%2520to%2520determine%2520whether%2520humans%250Aare%2520likely%2520to%2520take%2520similar%2520photos.%2520Furthermore%252C%2520we%2520propose%2520a%2520Hierarchical%250AOptimistic%2520Optimization%2520%2528HOO%2529-based%2520search%2520algorithm%2520to%2520efficiently%2520explore%25203D%250AVR%2520spaces%2520with%2520the%2520reward%2520from%2520the%2520scoring%2520network.%2520Our%2520user%2520study%2520shows%2520that%250Athe%2520scoring%2520network%2520achieves%2520human-level%2520performance%2520in%2520distinguishing%2520randomly%250Ataken%2520images%2520from%2520those%2520taken%2520by%2520humans.%2520In%2520addition%252C%2520we%2520show%2520applications%250Ausing%2520the%2520explored%2520photo%2520spots%252C%2520such%2520as%2520automatic%2520thumbnail%2520generation%252C%2520support%250Afor%2520VR%2520world%2520creation%252C%2520and%2520visitor%2520flow%2520planning%2520within%2520a%2520VR%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes&entry.906535625=Tomohiro%20Hayase%20and%20Braun%20Sacha%20and%20Hikari%20Yanagawa%20and%20Itsuki%20Orito%20and%20Yuichi%20Hiroi&entry.1292438233=%20%20Social%20VR%20platforms%20enable%20social%2C%20economic%2C%20and%20creative%20activities%20by%0Aallowing%20users%20to%20create%20and%20share%20their%20own%20virtual%20spaces.%20In%20social%20VR%2C%0Aphotography%20within%20a%20VR%20scene%20is%20an%20important%20indicator%20of%20visitors%27%0Aactivities.%20Although%20automatic%20identification%20of%20photo%20spots%20within%20a%20VR%20scene%0Acan%20facilitate%20the%20process%20of%20creating%20a%20VR%20scene%20and%20enhance%20the%20visitor%0Aexperience%2C%20there%20are%20challenges%20in%20quantitatively%20evaluating%20photos%20taken%20in%0Athe%20VR%20scene%20and%20efficiently%20exploring%20the%20large%20VR%20scene.%20We%20propose%20PanoTree%2C%0Aan%20automated%20photo-spot%20explorer%20in%20VR%20scenes.%20To%20assess%20the%20aesthetics%20of%0Aimages%20captured%20in%20VR%20scenes%2C%20a%20deep%20scoring%20network%20is%20trained%20on%20a%20large%0Adataset%20of%20photos%20collected%20by%20a%20social%20VR%20platform%20to%20determine%20whether%20humans%0Aare%20likely%20to%20take%20similar%20photos.%20Furthermore%2C%20we%20propose%20a%20Hierarchical%0AOptimistic%20Optimization%20%28HOO%29-based%20search%20algorithm%20to%20efficiently%20explore%203D%0AVR%20spaces%20with%20the%20reward%20from%20the%20scoring%20network.%20Our%20user%20study%20shows%20that%0Athe%20scoring%20network%20achieves%20human-level%20performance%20in%20distinguishing%20randomly%0Ataken%20images%20from%20those%20taken%20by%20humans.%20In%20addition%2C%20we%20show%20applications%0Ausing%20the%20explored%20photo%20spots%2C%20such%20as%20automatic%20thumbnail%20generation%2C%20support%0Afor%20VR%20world%20creation%2C%20and%20visitor%20flow%20planning%20within%20a%20VR%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17136v1&entry.124074799=Read"},
{"title": "Enhancing Graph Transformers with Hierarchical Distance Structural\n  Encoding", "author": "Yuankai Luo and Hongkang Li and Lei Shi and Xiao-Ming Wu", "abstract": "  Graph transformers need strong inductive biases to derive meaningful\nattention scores. Yet, current methods often fall short in capturing longer\nranges, hierarchical structures, or community structures, which are common in\nvarious graphs such as molecules, social networks, and citation networks. This\npaper presents a Hierarchical Distance Structural Encoding (HDSE) method to\nmodel node distances in a graph, focusing on its multi-level, hierarchical\nnature. We introduce a novel framework to seamlessly integrate HDSE into the\nattention mechanism of existing graph transformers, allowing for simultaneous\napplication with other positional encodings. To apply graph transformers with\nHDSE to large-scale graphs, we further propose a high-level HDSE that\neffectively biases the linear transformers towards graph hierarchies. We\ntheoretically prove the superiority of HDSE over shortest path distances in\nterms of expressivity and generalization. Empirically, we demonstrate that\ngraph transformers with HDSE excel in graph classification, regression on 7\ngraph-level datasets, and node classification on 11 large-scale graphs,\nincluding those with up to a billion nodes.\n", "link": "http://arxiv.org/abs/2308.11129v4", "date": "2024-05-27", "relevancy": 2.1217, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5686}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5247}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Graph%20Transformers%20with%20Hierarchical%20Distance%20Structural%0A%20%20Encoding&body=Title%3A%20Enhancing%20Graph%20Transformers%20with%20Hierarchical%20Distance%20Structural%0A%20%20Encoding%0AAuthor%3A%20Yuankai%20Luo%20and%20Hongkang%20Li%20and%20Lei%20Shi%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Graph%20transformers%20need%20strong%20inductive%20biases%20to%20derive%20meaningful%0Aattention%20scores.%20Yet%2C%20current%20methods%20often%20fall%20short%20in%20capturing%20longer%0Aranges%2C%20hierarchical%20structures%2C%20or%20community%20structures%2C%20which%20are%20common%20in%0Avarious%20graphs%20such%20as%20molecules%2C%20social%20networks%2C%20and%20citation%20networks.%20This%0Apaper%20presents%20a%20Hierarchical%20Distance%20Structural%20Encoding%20%28HDSE%29%20method%20to%0Amodel%20node%20distances%20in%20a%20graph%2C%20focusing%20on%20its%20multi-level%2C%20hierarchical%0Anature.%20We%20introduce%20a%20novel%20framework%20to%20seamlessly%20integrate%20HDSE%20into%20the%0Aattention%20mechanism%20of%20existing%20graph%20transformers%2C%20allowing%20for%20simultaneous%0Aapplication%20with%20other%20positional%20encodings.%20To%20apply%20graph%20transformers%20with%0AHDSE%20to%20large-scale%20graphs%2C%20we%20further%20propose%20a%20high-level%20HDSE%20that%0Aeffectively%20biases%20the%20linear%20transformers%20towards%20graph%20hierarchies.%20We%0Atheoretically%20prove%20the%20superiority%20of%20HDSE%20over%20shortest%20path%20distances%20in%0Aterms%20of%20expressivity%20and%20generalization.%20Empirically%2C%20we%20demonstrate%20that%0Agraph%20transformers%20with%20HDSE%20excel%20in%20graph%20classification%2C%20regression%20on%207%0Agraph-level%20datasets%2C%20and%20node%20classification%20on%2011%20large-scale%20graphs%2C%0Aincluding%20those%20with%20up%20to%20a%20billion%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11129v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Graph%2520Transformers%2520with%2520Hierarchical%2520Distance%2520Structural%250A%2520%2520Encoding%26entry.906535625%3DYuankai%2520Luo%2520and%2520Hongkang%2520Li%2520and%2520Lei%2520Shi%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Graph%2520transformers%2520need%2520strong%2520inductive%2520biases%2520to%2520derive%2520meaningful%250Aattention%2520scores.%2520Yet%252C%2520current%2520methods%2520often%2520fall%2520short%2520in%2520capturing%2520longer%250Aranges%252C%2520hierarchical%2520structures%252C%2520or%2520community%2520structures%252C%2520which%2520are%2520common%2520in%250Avarious%2520graphs%2520such%2520as%2520molecules%252C%2520social%2520networks%252C%2520and%2520citation%2520networks.%2520This%250Apaper%2520presents%2520a%2520Hierarchical%2520Distance%2520Structural%2520Encoding%2520%2528HDSE%2529%2520method%2520to%250Amodel%2520node%2520distances%2520in%2520a%2520graph%252C%2520focusing%2520on%2520its%2520multi-level%252C%2520hierarchical%250Anature.%2520We%2520introduce%2520a%2520novel%2520framework%2520to%2520seamlessly%2520integrate%2520HDSE%2520into%2520the%250Aattention%2520mechanism%2520of%2520existing%2520graph%2520transformers%252C%2520allowing%2520for%2520simultaneous%250Aapplication%2520with%2520other%2520positional%2520encodings.%2520To%2520apply%2520graph%2520transformers%2520with%250AHDSE%2520to%2520large-scale%2520graphs%252C%2520we%2520further%2520propose%2520a%2520high-level%2520HDSE%2520that%250Aeffectively%2520biases%2520the%2520linear%2520transformers%2520towards%2520graph%2520hierarchies.%2520We%250Atheoretically%2520prove%2520the%2520superiority%2520of%2520HDSE%2520over%2520shortest%2520path%2520distances%2520in%250Aterms%2520of%2520expressivity%2520and%2520generalization.%2520Empirically%252C%2520we%2520demonstrate%2520that%250Agraph%2520transformers%2520with%2520HDSE%2520excel%2520in%2520graph%2520classification%252C%2520regression%2520on%25207%250Agraph-level%2520datasets%252C%2520and%2520node%2520classification%2520on%252011%2520large-scale%2520graphs%252C%250Aincluding%2520those%2520with%2520up%2520to%2520a%2520billion%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.11129v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Graph%20Transformers%20with%20Hierarchical%20Distance%20Structural%0A%20%20Encoding&entry.906535625=Yuankai%20Luo%20and%20Hongkang%20Li%20and%20Lei%20Shi%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Graph%20transformers%20need%20strong%20inductive%20biases%20to%20derive%20meaningful%0Aattention%20scores.%20Yet%2C%20current%20methods%20often%20fall%20short%20in%20capturing%20longer%0Aranges%2C%20hierarchical%20structures%2C%20or%20community%20structures%2C%20which%20are%20common%20in%0Avarious%20graphs%20such%20as%20molecules%2C%20social%20networks%2C%20and%20citation%20networks.%20This%0Apaper%20presents%20a%20Hierarchical%20Distance%20Structural%20Encoding%20%28HDSE%29%20method%20to%0Amodel%20node%20distances%20in%20a%20graph%2C%20focusing%20on%20its%20multi-level%2C%20hierarchical%0Anature.%20We%20introduce%20a%20novel%20framework%20to%20seamlessly%20integrate%20HDSE%20into%20the%0Aattention%20mechanism%20of%20existing%20graph%20transformers%2C%20allowing%20for%20simultaneous%0Aapplication%20with%20other%20positional%20encodings.%20To%20apply%20graph%20transformers%20with%0AHDSE%20to%20large-scale%20graphs%2C%20we%20further%20propose%20a%20high-level%20HDSE%20that%0Aeffectively%20biases%20the%20linear%20transformers%20towards%20graph%20hierarchies.%20We%0Atheoretically%20prove%20the%20superiority%20of%20HDSE%20over%20shortest%20path%20distances%20in%0Aterms%20of%20expressivity%20and%20generalization.%20Empirically%2C%20we%20demonstrate%20that%0Agraph%20transformers%20with%20HDSE%20excel%20in%20graph%20classification%2C%20regression%20on%207%0Agraph-level%20datasets%2C%20and%20node%20classification%20on%2011%20large-scale%20graphs%2C%0Aincluding%20those%20with%20up%20to%20a%20billion%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11129v4&entry.124074799=Read"},
{"title": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object\n  Discovery", "author": "Anand Gopalakrishnan and Aleksandar Stani\u0107 and J\u00fcrgen Schmidhuber and Michael Curtis Mozer", "abstract": "  Current state-of-the-art synchrony-based models encode object bindings with\ncomplex-valued activations and compute with real-valued weights in feedforward\narchitectures. We argue for the computational advantages of a recurrent\narchitecture with complex-valued weights. We propose a fully convolutional\nautoencoder, SynCx, that performs iterative constraint satisfaction: at each\niteration, a hidden layer bottleneck encodes statistically regular\nconfigurations of features in particular phase relationships; over iterations,\nlocal constraints propagate and the model converges to a globally consistent\nconfiguration of phase assignments. Binding is achieved simply by the\nmatrix-vector product operation between complex-valued weights and activations,\nwithout the need for additional mechanisms that have been incorporated into\ncurrent synchrony-based models. SynCx outperforms or is strongly competitive\nwith current models for unsupervised object discovery. SynCx also avoids\ncertain systematic grouping errors of current models, such as the inability to\nseparate similarly colored objects without additional supervision.\n", "link": "http://arxiv.org/abs/2405.17283v1", "date": "2024-05-27", "relevancy": 2.1186, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5379}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5373}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery&body=Title%3A%20Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery%0AAuthor%3A%20Anand%20Gopalakrishnan%20and%20Aleksandar%20Stani%C4%87%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Michael%20Curtis%20Mozer%0AAbstract%3A%20%20%20Current%20state-of-the-art%20synchrony-based%20models%20encode%20object%20bindings%20with%0Acomplex-valued%20activations%20and%20compute%20with%20real-valued%20weights%20in%20feedforward%0Aarchitectures.%20We%20argue%20for%20the%20computational%20advantages%20of%20a%20recurrent%0Aarchitecture%20with%20complex-valued%20weights.%20We%20propose%20a%20fully%20convolutional%0Aautoencoder%2C%20SynCx%2C%20that%20performs%20iterative%20constraint%20satisfaction%3A%20at%20each%0Aiteration%2C%20a%20hidden%20layer%20bottleneck%20encodes%20statistically%20regular%0Aconfigurations%20of%20features%20in%20particular%20phase%20relationships%3B%20over%20iterations%2C%0Alocal%20constraints%20propagate%20and%20the%20model%20converges%20to%20a%20globally%20consistent%0Aconfiguration%20of%20phase%20assignments.%20Binding%20is%20achieved%20simply%20by%20the%0Amatrix-vector%20product%20operation%20between%20complex-valued%20weights%20and%20activations%2C%0Awithout%20the%20need%20for%20additional%20mechanisms%20that%20have%20been%20incorporated%20into%0Acurrent%20synchrony-based%20models.%20SynCx%20outperforms%20or%20is%20strongly%20competitive%0Awith%20current%20models%20for%20unsupervised%20object%20discovery.%20SynCx%20also%20avoids%0Acertain%20systematic%20grouping%20errors%20of%20current%20models%2C%20such%20as%20the%20inability%20to%0Aseparate%20similarly%20colored%20objects%20without%20additional%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Complex-Weighted%2520Autoencoders%2520for%2520Unsupervised%2520Object%250A%2520%2520Discovery%26entry.906535625%3DAnand%2520Gopalakrishnan%2520and%2520Aleksandar%2520Stani%25C4%2587%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%2520and%2520Michael%2520Curtis%2520Mozer%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520synchrony-based%2520models%2520encode%2520object%2520bindings%2520with%250Acomplex-valued%2520activations%2520and%2520compute%2520with%2520real-valued%2520weights%2520in%2520feedforward%250Aarchitectures.%2520We%2520argue%2520for%2520the%2520computational%2520advantages%2520of%2520a%2520recurrent%250Aarchitecture%2520with%2520complex-valued%2520weights.%2520We%2520propose%2520a%2520fully%2520convolutional%250Aautoencoder%252C%2520SynCx%252C%2520that%2520performs%2520iterative%2520constraint%2520satisfaction%253A%2520at%2520each%250Aiteration%252C%2520a%2520hidden%2520layer%2520bottleneck%2520encodes%2520statistically%2520regular%250Aconfigurations%2520of%2520features%2520in%2520particular%2520phase%2520relationships%253B%2520over%2520iterations%252C%250Alocal%2520constraints%2520propagate%2520and%2520the%2520model%2520converges%2520to%2520a%2520globally%2520consistent%250Aconfiguration%2520of%2520phase%2520assignments.%2520Binding%2520is%2520achieved%2520simply%2520by%2520the%250Amatrix-vector%2520product%2520operation%2520between%2520complex-valued%2520weights%2520and%2520activations%252C%250Awithout%2520the%2520need%2520for%2520additional%2520mechanisms%2520that%2520have%2520been%2520incorporated%2520into%250Acurrent%2520synchrony-based%2520models.%2520SynCx%2520outperforms%2520or%2520is%2520strongly%2520competitive%250Awith%2520current%2520models%2520for%2520unsupervised%2520object%2520discovery.%2520SynCx%2520also%2520avoids%250Acertain%2520systematic%2520grouping%2520errors%2520of%2520current%2520models%252C%2520such%2520as%2520the%2520inability%2520to%250Aseparate%2520similarly%2520colored%2520objects%2520without%2520additional%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Complex-Weighted%20Autoencoders%20for%20Unsupervised%20Object%0A%20%20Discovery&entry.906535625=Anand%20Gopalakrishnan%20and%20Aleksandar%20Stani%C4%87%20and%20J%C3%BCrgen%20Schmidhuber%20and%20Michael%20Curtis%20Mozer&entry.1292438233=%20%20Current%20state-of-the-art%20synchrony-based%20models%20encode%20object%20bindings%20with%0Acomplex-valued%20activations%20and%20compute%20with%20real-valued%20weights%20in%20feedforward%0Aarchitectures.%20We%20argue%20for%20the%20computational%20advantages%20of%20a%20recurrent%0Aarchitecture%20with%20complex-valued%20weights.%20We%20propose%20a%20fully%20convolutional%0Aautoencoder%2C%20SynCx%2C%20that%20performs%20iterative%20constraint%20satisfaction%3A%20at%20each%0Aiteration%2C%20a%20hidden%20layer%20bottleneck%20encodes%20statistically%20regular%0Aconfigurations%20of%20features%20in%20particular%20phase%20relationships%3B%20over%20iterations%2C%0Alocal%20constraints%20propagate%20and%20the%20model%20converges%20to%20a%20globally%20consistent%0Aconfiguration%20of%20phase%20assignments.%20Binding%20is%20achieved%20simply%20by%20the%0Amatrix-vector%20product%20operation%20between%20complex-valued%20weights%20and%20activations%2C%0Awithout%20the%20need%20for%20additional%20mechanisms%20that%20have%20been%20incorporated%20into%0Acurrent%20synchrony-based%20models.%20SynCx%20outperforms%20or%20is%20strongly%20competitive%0Awith%20current%20models%20for%20unsupervised%20object%20discovery.%20SynCx%20also%20avoids%0Acertain%20systematic%20grouping%20errors%20of%20current%20models%2C%20such%20as%20the%20inability%20to%0Aseparate%20similarly%20colored%20objects%20without%20additional%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17283v1&entry.124074799=Read"},
{"title": "Exploiting the Layered Intrinsic Dimensionality of Deep Models for\n  Practical Adversarial Training", "author": "Enes Altinisik and Safa Messaoud and Husrev Taha Sencar and Hassan Sajjad and Sanjay Chawla", "abstract": "  Despite being a heavily researched topic, Adversarial Training (AT) is\nrarely, if ever, deployed in practical AI systems for two primary reasons: (i)\nthe gained robustness is frequently accompanied by a drop in generalization and\n(ii) generating adversarial examples (AEs) is computationally prohibitively\nexpensive. To address these limitations, we propose SMAAT, a new AT algorithm\nthat leverages the manifold conjecture, stating that off-manifold AEs lead to\nbetter robustness while on-manifold AEs result in better generalization.\nSpecifically, SMAAT aims at generating a higher proportion of off-manifold AEs\nby perturbing the intermediate deepnet layer with the lowest intrinsic\ndimension. This systematically results in better scalability compared to\nclassical AT as it reduces the PGD chains length required for generating the\nAEs. Additionally, our study provides, to the best of our knowledge, the first\nexplanation for the difference in the generalization and robustness trends\nbetween vision and language models, ie., AT results in a drop in generalization\nin vision models whereas, in encoder-based language models, generalization\neither improves or remains unchanged. We show that vision transformers and\ndecoder-based models tend to have low intrinsic dimensionality in the earlier\nlayers of the network (more off-manifold AEs), while encoder-based models have\nlow intrinsic dimensionality in the later layers. We demonstrate the efficacy\nof SMAAT; on several tasks, including robustifying (i) sentiment classifiers,\n(ii) safety filters in decoder-based models, and (iii) retrievers in RAG\nsetups. SMAAT requires only 25-33% of the GPU time compared to standard AT,\nwhile significantly improving robustness across all applications and\nmaintaining comparable generalization.\n", "link": "http://arxiv.org/abs/2405.17130v1", "date": "2024-05-27", "relevancy": 2.1087, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Layered%20Intrinsic%20Dimensionality%20of%20Deep%20Models%20for%0A%20%20Practical%20Adversarial%20Training&body=Title%3A%20Exploiting%20the%20Layered%20Intrinsic%20Dimensionality%20of%20Deep%20Models%20for%0A%20%20Practical%20Adversarial%20Training%0AAuthor%3A%20Enes%20Altinisik%20and%20Safa%20Messaoud%20and%20Husrev%20Taha%20Sencar%20and%20Hassan%20Sajjad%20and%20Sanjay%20Chawla%0AAbstract%3A%20%20%20Despite%20being%20a%20heavily%20researched%20topic%2C%20Adversarial%20Training%20%28AT%29%20is%0Ararely%2C%20if%20ever%2C%20deployed%20in%20practical%20AI%20systems%20for%20two%20primary%20reasons%3A%20%28i%29%0Athe%20gained%20robustness%20is%20frequently%20accompanied%20by%20a%20drop%20in%20generalization%20and%0A%28ii%29%20generating%20adversarial%20examples%20%28AEs%29%20is%20computationally%20prohibitively%0Aexpensive.%20To%20address%20these%20limitations%2C%20we%20propose%20SMAAT%2C%20a%20new%20AT%20algorithm%0Athat%20leverages%20the%20manifold%20conjecture%2C%20stating%20that%20off-manifold%20AEs%20lead%20to%0Abetter%20robustness%20while%20on-manifold%20AEs%20result%20in%20better%20generalization.%0ASpecifically%2C%20SMAAT%20aims%20at%20generating%20a%20higher%20proportion%20of%20off-manifold%20AEs%0Aby%20perturbing%20the%20intermediate%20deepnet%20layer%20with%20the%20lowest%20intrinsic%0Adimension.%20This%20systematically%20results%20in%20better%20scalability%20compared%20to%0Aclassical%20AT%20as%20it%20reduces%20the%20PGD%20chains%20length%20required%20for%20generating%20the%0AAEs.%20Additionally%2C%20our%20study%20provides%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Aexplanation%20for%20the%20difference%20in%20the%20generalization%20and%20robustness%20trends%0Abetween%20vision%20and%20language%20models%2C%20ie.%2C%20AT%20results%20in%20a%20drop%20in%20generalization%0Ain%20vision%20models%20whereas%2C%20in%20encoder-based%20language%20models%2C%20generalization%0Aeither%20improves%20or%20remains%20unchanged.%20We%20show%20that%20vision%20transformers%20and%0Adecoder-based%20models%20tend%20to%20have%20low%20intrinsic%20dimensionality%20in%20the%20earlier%0Alayers%20of%20the%20network%20%28more%20off-manifold%20AEs%29%2C%20while%20encoder-based%20models%20have%0Alow%20intrinsic%20dimensionality%20in%20the%20later%20layers.%20We%20demonstrate%20the%20efficacy%0Aof%20SMAAT%3B%20on%20several%20tasks%2C%20including%20robustifying%20%28i%29%20sentiment%20classifiers%2C%0A%28ii%29%20safety%20filters%20in%20decoder-based%20models%2C%20and%20%28iii%29%20retrievers%20in%20RAG%0Asetups.%20SMAAT%20requires%20only%2025-33%25%20of%20the%20GPU%20time%20compared%20to%20standard%20AT%2C%0Awhile%20significantly%20improving%20robustness%20across%20all%20applications%20and%0Amaintaining%20comparable%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Layered%2520Intrinsic%2520Dimensionality%2520of%2520Deep%2520Models%2520for%250A%2520%2520Practical%2520Adversarial%2520Training%26entry.906535625%3DEnes%2520Altinisik%2520and%2520Safa%2520Messaoud%2520and%2520Husrev%2520Taha%2520Sencar%2520and%2520Hassan%2520Sajjad%2520and%2520Sanjay%2520Chawla%26entry.1292438233%3D%2520%2520Despite%2520being%2520a%2520heavily%2520researched%2520topic%252C%2520Adversarial%2520Training%2520%2528AT%2529%2520is%250Ararely%252C%2520if%2520ever%252C%2520deployed%2520in%2520practical%2520AI%2520systems%2520for%2520two%2520primary%2520reasons%253A%2520%2528i%2529%250Athe%2520gained%2520robustness%2520is%2520frequently%2520accompanied%2520by%2520a%2520drop%2520in%2520generalization%2520and%250A%2528ii%2529%2520generating%2520adversarial%2520examples%2520%2528AEs%2529%2520is%2520computationally%2520prohibitively%250Aexpensive.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SMAAT%252C%2520a%2520new%2520AT%2520algorithm%250Athat%2520leverages%2520the%2520manifold%2520conjecture%252C%2520stating%2520that%2520off-manifold%2520AEs%2520lead%2520to%250Abetter%2520robustness%2520while%2520on-manifold%2520AEs%2520result%2520in%2520better%2520generalization.%250ASpecifically%252C%2520SMAAT%2520aims%2520at%2520generating%2520a%2520higher%2520proportion%2520of%2520off-manifold%2520AEs%250Aby%2520perturbing%2520the%2520intermediate%2520deepnet%2520layer%2520with%2520the%2520lowest%2520intrinsic%250Adimension.%2520This%2520systematically%2520results%2520in%2520better%2520scalability%2520compared%2520to%250Aclassical%2520AT%2520as%2520it%2520reduces%2520the%2520PGD%2520chains%2520length%2520required%2520for%2520generating%2520the%250AAEs.%2520Additionally%252C%2520our%2520study%2520provides%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%250Aexplanation%2520for%2520the%2520difference%2520in%2520the%2520generalization%2520and%2520robustness%2520trends%250Abetween%2520vision%2520and%2520language%2520models%252C%2520ie.%252C%2520AT%2520results%2520in%2520a%2520drop%2520in%2520generalization%250Ain%2520vision%2520models%2520whereas%252C%2520in%2520encoder-based%2520language%2520models%252C%2520generalization%250Aeither%2520improves%2520or%2520remains%2520unchanged.%2520We%2520show%2520that%2520vision%2520transformers%2520and%250Adecoder-based%2520models%2520tend%2520to%2520have%2520low%2520intrinsic%2520dimensionality%2520in%2520the%2520earlier%250Alayers%2520of%2520the%2520network%2520%2528more%2520off-manifold%2520AEs%2529%252C%2520while%2520encoder-based%2520models%2520have%250Alow%2520intrinsic%2520dimensionality%2520in%2520the%2520later%2520layers.%2520We%2520demonstrate%2520the%2520efficacy%250Aof%2520SMAAT%253B%2520on%2520several%2520tasks%252C%2520including%2520robustifying%2520%2528i%2529%2520sentiment%2520classifiers%252C%250A%2528ii%2529%2520safety%2520filters%2520in%2520decoder-based%2520models%252C%2520and%2520%2528iii%2529%2520retrievers%2520in%2520RAG%250Asetups.%2520SMAAT%2520requires%2520only%252025-33%2525%2520of%2520the%2520GPU%2520time%2520compared%2520to%2520standard%2520AT%252C%250Awhile%2520significantly%2520improving%2520robustness%2520across%2520all%2520applications%2520and%250Amaintaining%2520comparable%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Layered%20Intrinsic%20Dimensionality%20of%20Deep%20Models%20for%0A%20%20Practical%20Adversarial%20Training&entry.906535625=Enes%20Altinisik%20and%20Safa%20Messaoud%20and%20Husrev%20Taha%20Sencar%20and%20Hassan%20Sajjad%20and%20Sanjay%20Chawla&entry.1292438233=%20%20Despite%20being%20a%20heavily%20researched%20topic%2C%20Adversarial%20Training%20%28AT%29%20is%0Ararely%2C%20if%20ever%2C%20deployed%20in%20practical%20AI%20systems%20for%20two%20primary%20reasons%3A%20%28i%29%0Athe%20gained%20robustness%20is%20frequently%20accompanied%20by%20a%20drop%20in%20generalization%20and%0A%28ii%29%20generating%20adversarial%20examples%20%28AEs%29%20is%20computationally%20prohibitively%0Aexpensive.%20To%20address%20these%20limitations%2C%20we%20propose%20SMAAT%2C%20a%20new%20AT%20algorithm%0Athat%20leverages%20the%20manifold%20conjecture%2C%20stating%20that%20off-manifold%20AEs%20lead%20to%0Abetter%20robustness%20while%20on-manifold%20AEs%20result%20in%20better%20generalization.%0ASpecifically%2C%20SMAAT%20aims%20at%20generating%20a%20higher%20proportion%20of%20off-manifold%20AEs%0Aby%20perturbing%20the%20intermediate%20deepnet%20layer%20with%20the%20lowest%20intrinsic%0Adimension.%20This%20systematically%20results%20in%20better%20scalability%20compared%20to%0Aclassical%20AT%20as%20it%20reduces%20the%20PGD%20chains%20length%20required%20for%20generating%20the%0AAEs.%20Additionally%2C%20our%20study%20provides%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%0Aexplanation%20for%20the%20difference%20in%20the%20generalization%20and%20robustness%20trends%0Abetween%20vision%20and%20language%20models%2C%20ie.%2C%20AT%20results%20in%20a%20drop%20in%20generalization%0Ain%20vision%20models%20whereas%2C%20in%20encoder-based%20language%20models%2C%20generalization%0Aeither%20improves%20or%20remains%20unchanged.%20We%20show%20that%20vision%20transformers%20and%0Adecoder-based%20models%20tend%20to%20have%20low%20intrinsic%20dimensionality%20in%20the%20earlier%0Alayers%20of%20the%20network%20%28more%20off-manifold%20AEs%29%2C%20while%20encoder-based%20models%20have%0Alow%20intrinsic%20dimensionality%20in%20the%20later%20layers.%20We%20demonstrate%20the%20efficacy%0Aof%20SMAAT%3B%20on%20several%20tasks%2C%20including%20robustifying%20%28i%29%20sentiment%20classifiers%2C%0A%28ii%29%20safety%20filters%20in%20decoder-based%20models%2C%20and%20%28iii%29%20retrievers%20in%20RAG%0Asetups.%20SMAAT%20requires%20only%2025-33%25%20of%20the%20GPU%20time%20compared%20to%20standard%20AT%2C%0Awhile%20significantly%20improving%20robustness%20across%20all%20applications%20and%0Amaintaining%20comparable%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17130v1&entry.124074799=Read"},
{"title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens", "author": "Houxing Ren and Mingjie Zhan and Zhongyuan Wu and Hongsheng Li", "abstract": "  In infilling tasks, sub-tokens, representing instances where a complete token\nis segmented into two parts, often emerge at the boundaries of prefixes,\nmiddles, and suffixes. Traditional methods focused on training models at the\ntoken level, leading to sub-optimal performance in character-level infilling\ntasks during the inference stage. Alternately, some approaches considered\ncharacter-level infilling, but they relied on predicting sub-tokens in\ninference, yet this strategy diminished ability in character-level infilling\ntasks due to the large perplexity of the model on sub-tokens. In this paper, we\nintroduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and\nEnding character constraints. The proposed method addresses character-level\ninfilling tasks by utilizing a line-level format to avoid predicting any\nsub-token in inference. In addition, we incorporate two special tokens to\nsignify the rest of the incomplete lines, thereby enhancing generation\nguidance. Extensive experiments demonstrate that our proposed approach\nsurpasses previous methods, offering a significant advantage. Code is available\nat https://github.com/SenseLLM/FIM-SE.\n", "link": "http://arxiv.org/abs/2405.17103v1", "date": "2024-05-27", "relevancy": 2.1071, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4352}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Character-level%20Text%20Infilling%20by%20Eliminating%20Sub-Tokens&body=Title%3A%20Empowering%20Character-level%20Text%20Infilling%20by%20Eliminating%20Sub-Tokens%0AAuthor%3A%20Houxing%20Ren%20and%20Mingjie%20Zhan%20and%20Zhongyuan%20Wu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20In%20infilling%20tasks%2C%20sub-tokens%2C%20representing%20instances%20where%20a%20complete%20token%0Ais%20segmented%20into%20two%20parts%2C%20often%20emerge%20at%20the%20boundaries%20of%20prefixes%2C%0Amiddles%2C%20and%20suffixes.%20Traditional%20methods%20focused%20on%20training%20models%20at%20the%0Atoken%20level%2C%20leading%20to%20sub-optimal%20performance%20in%20character-level%20infilling%0Atasks%20during%20the%20inference%20stage.%20Alternately%2C%20some%20approaches%20considered%0Acharacter-level%20infilling%2C%20but%20they%20relied%20on%20predicting%20sub-tokens%20in%0Ainference%2C%20yet%20this%20strategy%20diminished%20ability%20in%20character-level%20infilling%0Atasks%20due%20to%20the%20large%20perplexity%20of%20the%20model%20on%20sub-tokens.%20In%20this%20paper%2C%20we%0Aintroduce%20FIM-SE%2C%20which%20stands%20for%20Fill-In-the-Middle%20with%20both%20Starting%20and%0AEnding%20character%20constraints.%20The%20proposed%20method%20addresses%20character-level%0Ainfilling%20tasks%20by%20utilizing%20a%20line-level%20format%20to%20avoid%20predicting%20any%0Asub-token%20in%20inference.%20In%20addition%2C%20we%20incorporate%20two%20special%20tokens%20to%0Asignify%20the%20rest%20of%20the%20incomplete%20lines%2C%20thereby%20enhancing%20generation%0Aguidance.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20approach%0Asurpasses%20previous%20methods%2C%20offering%20a%20significant%20advantage.%20Code%20is%20available%0Aat%20https%3A//github.com/SenseLLM/FIM-SE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Character-level%2520Text%2520Infilling%2520by%2520Eliminating%2520Sub-Tokens%26entry.906535625%3DHouxing%2520Ren%2520and%2520Mingjie%2520Zhan%2520and%2520Zhongyuan%2520Wu%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520In%2520infilling%2520tasks%252C%2520sub-tokens%252C%2520representing%2520instances%2520where%2520a%2520complete%2520token%250Ais%2520segmented%2520into%2520two%2520parts%252C%2520often%2520emerge%2520at%2520the%2520boundaries%2520of%2520prefixes%252C%250Amiddles%252C%2520and%2520suffixes.%2520Traditional%2520methods%2520focused%2520on%2520training%2520models%2520at%2520the%250Atoken%2520level%252C%2520leading%2520to%2520sub-optimal%2520performance%2520in%2520character-level%2520infilling%250Atasks%2520during%2520the%2520inference%2520stage.%2520Alternately%252C%2520some%2520approaches%2520considered%250Acharacter-level%2520infilling%252C%2520but%2520they%2520relied%2520on%2520predicting%2520sub-tokens%2520in%250Ainference%252C%2520yet%2520this%2520strategy%2520diminished%2520ability%2520in%2520character-level%2520infilling%250Atasks%2520due%2520to%2520the%2520large%2520perplexity%2520of%2520the%2520model%2520on%2520sub-tokens.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520FIM-SE%252C%2520which%2520stands%2520for%2520Fill-In-the-Middle%2520with%2520both%2520Starting%2520and%250AEnding%2520character%2520constraints.%2520The%2520proposed%2520method%2520addresses%2520character-level%250Ainfilling%2520tasks%2520by%2520utilizing%2520a%2520line-level%2520format%2520to%2520avoid%2520predicting%2520any%250Asub-token%2520in%2520inference.%2520In%2520addition%252C%2520we%2520incorporate%2520two%2520special%2520tokens%2520to%250Asignify%2520the%2520rest%2520of%2520the%2520incomplete%2520lines%252C%2520thereby%2520enhancing%2520generation%250Aguidance.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520approach%250Asurpasses%2520previous%2520methods%252C%2520offering%2520a%2520significant%2520advantage.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/SenseLLM/FIM-SE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Character-level%20Text%20Infilling%20by%20Eliminating%20Sub-Tokens&entry.906535625=Houxing%20Ren%20and%20Mingjie%20Zhan%20and%20Zhongyuan%20Wu%20and%20Hongsheng%20Li&entry.1292438233=%20%20In%20infilling%20tasks%2C%20sub-tokens%2C%20representing%20instances%20where%20a%20complete%20token%0Ais%20segmented%20into%20two%20parts%2C%20often%20emerge%20at%20the%20boundaries%20of%20prefixes%2C%0Amiddles%2C%20and%20suffixes.%20Traditional%20methods%20focused%20on%20training%20models%20at%20the%0Atoken%20level%2C%20leading%20to%20sub-optimal%20performance%20in%20character-level%20infilling%0Atasks%20during%20the%20inference%20stage.%20Alternately%2C%20some%20approaches%20considered%0Acharacter-level%20infilling%2C%20but%20they%20relied%20on%20predicting%20sub-tokens%20in%0Ainference%2C%20yet%20this%20strategy%20diminished%20ability%20in%20character-level%20infilling%0Atasks%20due%20to%20the%20large%20perplexity%20of%20the%20model%20on%20sub-tokens.%20In%20this%20paper%2C%20we%0Aintroduce%20FIM-SE%2C%20which%20stands%20for%20Fill-In-the-Middle%20with%20both%20Starting%20and%0AEnding%20character%20constraints.%20The%20proposed%20method%20addresses%20character-level%0Ainfilling%20tasks%20by%20utilizing%20a%20line-level%20format%20to%20avoid%20predicting%20any%0Asub-token%20in%20inference.%20In%20addition%2C%20we%20incorporate%20two%20special%20tokens%20to%0Asignify%20the%20rest%20of%20the%20incomplete%20lines%2C%20thereby%20enhancing%20generation%0Aguidance.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20approach%0Asurpasses%20previous%20methods%2C%20offering%20a%20significant%20advantage.%20Code%20is%20available%0Aat%20https%3A//github.com/SenseLLM/FIM-SE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17103v1&entry.124074799=Read"},
{"title": "Survey of Graph Neural Network for Internet of Things and NextG Networks", "author": "Sabarish Krishna Moorthy and Jithin Jagannath", "abstract": "  The exponential increase in Internet of Things (IoT) devices coupled with 6G\npushing towards higher data rates and connected devices has sparked a surge in\ndata. Consequently, harnessing the full potential of data-driven machine\nlearning has become one of the important thrusts. In addition to the\nadvancement in wireless technology, it is important to efficiently use the\nresources available and meet the users' requirements. Graph Neural Networks\n(GNNs) have emerged as a promising paradigm for effectively modeling and\nextracting insights which inherently exhibit complex network structures due to\nits high performance and accuracy, scalability, adaptability, and resource\nefficiency. There is a lack of a comprehensive survey that focuses on the\napplications and advances GNN has made in the context of IoT and Next\nGeneration (NextG) networks. To bridge that gap, this survey starts by\nproviding a detailed description of GNN's terminologies, architecture, and the\ndifferent types of GNNs. Then we provide a comprehensive survey of the\nadvancements in applying GNNs for IoT from the perspective of data fusion and\nintrusion detection. Thereafter, we survey the impact GNN has made in improving\nspectrum awareness. Next, we provide a detailed account of how GNN has been\nleveraged for networking and tactical systems. Through this survey, we aim to\nprovide a comprehensive resource for researchers to learn more about GNN in the\ncontext of wireless networks, and understand its state-of-the-art use cases\nwhile contrasting to other machine learning approaches. Finally, we also\ndiscussed the challenges and wide range of future research directions to\nfurther motivate the use of GNN for IoT and NextG Networks.\n", "link": "http://arxiv.org/abs/2405.17309v1", "date": "2024-05-27", "relevancy": 2.0942, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4226}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4183}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20Graph%20Neural%20Network%20for%20Internet%20of%20Things%20and%20NextG%20Networks&body=Title%3A%20Survey%20of%20Graph%20Neural%20Network%20for%20Internet%20of%20Things%20and%20NextG%20Networks%0AAuthor%3A%20Sabarish%20Krishna%20Moorthy%20and%20Jithin%20Jagannath%0AAbstract%3A%20%20%20The%20exponential%20increase%20in%20Internet%20of%20Things%20%28IoT%29%20devices%20coupled%20with%206G%0Apushing%20towards%20higher%20data%20rates%20and%20connected%20devices%20has%20sparked%20a%20surge%20in%0Adata.%20Consequently%2C%20harnessing%20the%20full%20potential%20of%20data-driven%20machine%0Alearning%20has%20become%20one%20of%20the%20important%20thrusts.%20In%20addition%20to%20the%0Aadvancement%20in%20wireless%20technology%2C%20it%20is%20important%20to%20efficiently%20use%20the%0Aresources%20available%20and%20meet%20the%20users%27%20requirements.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20a%20promising%20paradigm%20for%20effectively%20modeling%20and%0Aextracting%20insights%20which%20inherently%20exhibit%20complex%20network%20structures%20due%20to%0Aits%20high%20performance%20and%20accuracy%2C%20scalability%2C%20adaptability%2C%20and%20resource%0Aefficiency.%20There%20is%20a%20lack%20of%20a%20comprehensive%20survey%20that%20focuses%20on%20the%0Aapplications%20and%20advances%20GNN%20has%20made%20in%20the%20context%20of%20IoT%20and%20Next%0AGeneration%20%28NextG%29%20networks.%20To%20bridge%20that%20gap%2C%20this%20survey%20starts%20by%0Aproviding%20a%20detailed%20description%20of%20GNN%27s%20terminologies%2C%20architecture%2C%20and%20the%0Adifferent%20types%20of%20GNNs.%20Then%20we%20provide%20a%20comprehensive%20survey%20of%20the%0Aadvancements%20in%20applying%20GNNs%20for%20IoT%20from%20the%20perspective%20of%20data%20fusion%20and%0Aintrusion%20detection.%20Thereafter%2C%20we%20survey%20the%20impact%20GNN%20has%20made%20in%20improving%0Aspectrum%20awareness.%20Next%2C%20we%20provide%20a%20detailed%20account%20of%20how%20GNN%20has%20been%0Aleveraged%20for%20networking%20and%20tactical%20systems.%20Through%20this%20survey%2C%20we%20aim%20to%0Aprovide%20a%20comprehensive%20resource%20for%20researchers%20to%20learn%20more%20about%20GNN%20in%20the%0Acontext%20of%20wireless%20networks%2C%20and%20understand%20its%20state-of-the-art%20use%20cases%0Awhile%20contrasting%20to%20other%20machine%20learning%20approaches.%20Finally%2C%20we%20also%0Adiscussed%20the%20challenges%20and%20wide%20range%20of%20future%20research%20directions%20to%0Afurther%20motivate%20the%20use%20of%20GNN%20for%20IoT%20and%20NextG%20Networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520Graph%2520Neural%2520Network%2520for%2520Internet%2520of%2520Things%2520and%2520NextG%2520Networks%26entry.906535625%3DSabarish%2520Krishna%2520Moorthy%2520and%2520Jithin%2520Jagannath%26entry.1292438233%3D%2520%2520The%2520exponential%2520increase%2520in%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520devices%2520coupled%2520with%25206G%250Apushing%2520towards%2520higher%2520data%2520rates%2520and%2520connected%2520devices%2520has%2520sparked%2520a%2520surge%2520in%250Adata.%2520Consequently%252C%2520harnessing%2520the%2520full%2520potential%2520of%2520data-driven%2520machine%250Alearning%2520has%2520become%2520one%2520of%2520the%2520important%2520thrusts.%2520In%2520addition%2520to%2520the%250Aadvancement%2520in%2520wireless%2520technology%252C%2520it%2520is%2520important%2520to%2520efficiently%2520use%2520the%250Aresources%2520available%2520and%2520meet%2520the%2520users%2527%2520requirements.%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520effectively%2520modeling%2520and%250Aextracting%2520insights%2520which%2520inherently%2520exhibit%2520complex%2520network%2520structures%2520due%2520to%250Aits%2520high%2520performance%2520and%2520accuracy%252C%2520scalability%252C%2520adaptability%252C%2520and%2520resource%250Aefficiency.%2520There%2520is%2520a%2520lack%2520of%2520a%2520comprehensive%2520survey%2520that%2520focuses%2520on%2520the%250Aapplications%2520and%2520advances%2520GNN%2520has%2520made%2520in%2520the%2520context%2520of%2520IoT%2520and%2520Next%250AGeneration%2520%2528NextG%2529%2520networks.%2520To%2520bridge%2520that%2520gap%252C%2520this%2520survey%2520starts%2520by%250Aproviding%2520a%2520detailed%2520description%2520of%2520GNN%2527s%2520terminologies%252C%2520architecture%252C%2520and%2520the%250Adifferent%2520types%2520of%2520GNNs.%2520Then%2520we%2520provide%2520a%2520comprehensive%2520survey%2520of%2520the%250Aadvancements%2520in%2520applying%2520GNNs%2520for%2520IoT%2520from%2520the%2520perspective%2520of%2520data%2520fusion%2520and%250Aintrusion%2520detection.%2520Thereafter%252C%2520we%2520survey%2520the%2520impact%2520GNN%2520has%2520made%2520in%2520improving%250Aspectrum%2520awareness.%2520Next%252C%2520we%2520provide%2520a%2520detailed%2520account%2520of%2520how%2520GNN%2520has%2520been%250Aleveraged%2520for%2520networking%2520and%2520tactical%2520systems.%2520Through%2520this%2520survey%252C%2520we%2520aim%2520to%250Aprovide%2520a%2520comprehensive%2520resource%2520for%2520researchers%2520to%2520learn%2520more%2520about%2520GNN%2520in%2520the%250Acontext%2520of%2520wireless%2520networks%252C%2520and%2520understand%2520its%2520state-of-the-art%2520use%2520cases%250Awhile%2520contrasting%2520to%2520other%2520machine%2520learning%2520approaches.%2520Finally%252C%2520we%2520also%250Adiscussed%2520the%2520challenges%2520and%2520wide%2520range%2520of%2520future%2520research%2520directions%2520to%250Afurther%2520motivate%2520the%2520use%2520of%2520GNN%2520for%2520IoT%2520and%2520NextG%2520Networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20Graph%20Neural%20Network%20for%20Internet%20of%20Things%20and%20NextG%20Networks&entry.906535625=Sabarish%20Krishna%20Moorthy%20and%20Jithin%20Jagannath&entry.1292438233=%20%20The%20exponential%20increase%20in%20Internet%20of%20Things%20%28IoT%29%20devices%20coupled%20with%206G%0Apushing%20towards%20higher%20data%20rates%20and%20connected%20devices%20has%20sparked%20a%20surge%20in%0Adata.%20Consequently%2C%20harnessing%20the%20full%20potential%20of%20data-driven%20machine%0Alearning%20has%20become%20one%20of%20the%20important%20thrusts.%20In%20addition%20to%20the%0Aadvancement%20in%20wireless%20technology%2C%20it%20is%20important%20to%20efficiently%20use%20the%0Aresources%20available%20and%20meet%20the%20users%27%20requirements.%20Graph%20Neural%20Networks%0A%28GNNs%29%20have%20emerged%20as%20a%20promising%20paradigm%20for%20effectively%20modeling%20and%0Aextracting%20insights%20which%20inherently%20exhibit%20complex%20network%20structures%20due%20to%0Aits%20high%20performance%20and%20accuracy%2C%20scalability%2C%20adaptability%2C%20and%20resource%0Aefficiency.%20There%20is%20a%20lack%20of%20a%20comprehensive%20survey%20that%20focuses%20on%20the%0Aapplications%20and%20advances%20GNN%20has%20made%20in%20the%20context%20of%20IoT%20and%20Next%0AGeneration%20%28NextG%29%20networks.%20To%20bridge%20that%20gap%2C%20this%20survey%20starts%20by%0Aproviding%20a%20detailed%20description%20of%20GNN%27s%20terminologies%2C%20architecture%2C%20and%20the%0Adifferent%20types%20of%20GNNs.%20Then%20we%20provide%20a%20comprehensive%20survey%20of%20the%0Aadvancements%20in%20applying%20GNNs%20for%20IoT%20from%20the%20perspective%20of%20data%20fusion%20and%0Aintrusion%20detection.%20Thereafter%2C%20we%20survey%20the%20impact%20GNN%20has%20made%20in%20improving%0Aspectrum%20awareness.%20Next%2C%20we%20provide%20a%20detailed%20account%20of%20how%20GNN%20has%20been%0Aleveraged%20for%20networking%20and%20tactical%20systems.%20Through%20this%20survey%2C%20we%20aim%20to%0Aprovide%20a%20comprehensive%20resource%20for%20researchers%20to%20learn%20more%20about%20GNN%20in%20the%0Acontext%20of%20wireless%20networks%2C%20and%20understand%20its%20state-of-the-art%20use%20cases%0Awhile%20contrasting%20to%20other%20machine%20learning%20approaches.%20Finally%2C%20we%20also%0Adiscussed%20the%20challenges%20and%20wide%20range%20of%20future%20research%20directions%20to%0Afurther%20motivate%20the%20use%20of%20GNN%20for%20IoT%20and%20NextG%20Networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17309v1&entry.124074799=Read"},
{"title": "Opinion-Guided Reinforcement Learning", "author": "Kyanna Dagenais and Istvan David", "abstract": "  Human guidance is often desired in reinforcement learning to improve the\nperformance of the learning agent. However, human insights are often mere\nopinions and educated guesses rather than well-formulated arguments. While\nopinions are subject to uncertainty, e.g., due to partial informedness or\nignorance about a problem, they also emerge earlier than hard evidence could be\nproduced. Thus, guiding reinforcement learning agents through opinions offers\nthe potential for more performant learning processes, but comes with the\nchallenge of modeling and managing opinions in a formal way. In this article,\nwe present a method to guide reinforcement learning agents through opinions. To\nthis end, we provide an end-to-end method to model and manage advisors'\nopinions. To assess the utility of the approach, we evaluate it with synthetic\nand human advisors, at different levels of uncertainty, and under multiple\nadvise strategies. Our results indicate that opinions, even if uncertain,\nimprove the performance of reinforcement learning agents, resulting in higher\nrewards, more efficient exploration, and a better reinforced policy. Although\nwe demonstrate our approach in a simplified topological running example, our\napproach is applicable to complex problems with higher dimensions as well.\n", "link": "http://arxiv.org/abs/2405.17287v1", "date": "2024-05-27", "relevancy": 2.0869, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opinion-Guided%20Reinforcement%20Learning&body=Title%3A%20Opinion-Guided%20Reinforcement%20Learning%0AAuthor%3A%20Kyanna%20Dagenais%20and%20Istvan%20David%0AAbstract%3A%20%20%20Human%20guidance%20is%20often%20desired%20in%20reinforcement%20learning%20to%20improve%20the%0Aperformance%20of%20the%20learning%20agent.%20However%2C%20human%20insights%20are%20often%20mere%0Aopinions%20and%20educated%20guesses%20rather%20than%20well-formulated%20arguments.%20While%0Aopinions%20are%20subject%20to%20uncertainty%2C%20e.g.%2C%20due%20to%20partial%20informedness%20or%0Aignorance%20about%20a%20problem%2C%20they%20also%20emerge%20earlier%20than%20hard%20evidence%20could%20be%0Aproduced.%20Thus%2C%20guiding%20reinforcement%20learning%20agents%20through%20opinions%20offers%0Athe%20potential%20for%20more%20performant%20learning%20processes%2C%20but%20comes%20with%20the%0Achallenge%20of%20modeling%20and%20managing%20opinions%20in%20a%20formal%20way.%20In%20this%20article%2C%0Awe%20present%20a%20method%20to%20guide%20reinforcement%20learning%20agents%20through%20opinions.%20To%0Athis%20end%2C%20we%20provide%20an%20end-to-end%20method%20to%20model%20and%20manage%20advisors%27%0Aopinions.%20To%20assess%20the%20utility%20of%20the%20approach%2C%20we%20evaluate%20it%20with%20synthetic%0Aand%20human%20advisors%2C%20at%20different%20levels%20of%20uncertainty%2C%20and%20under%20multiple%0Aadvise%20strategies.%20Our%20results%20indicate%20that%20opinions%2C%20even%20if%20uncertain%2C%0Aimprove%20the%20performance%20of%20reinforcement%20learning%20agents%2C%20resulting%20in%20higher%0Arewards%2C%20more%20efficient%20exploration%2C%20and%20a%20better%20reinforced%20policy.%20Although%0Awe%20demonstrate%20our%20approach%20in%20a%20simplified%20topological%20running%20example%2C%20our%0Aapproach%20is%20applicable%20to%20complex%20problems%20with%20higher%20dimensions%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpinion-Guided%2520Reinforcement%2520Learning%26entry.906535625%3DKyanna%2520Dagenais%2520and%2520Istvan%2520David%26entry.1292438233%3D%2520%2520Human%2520guidance%2520is%2520often%2520desired%2520in%2520reinforcement%2520learning%2520to%2520improve%2520the%250Aperformance%2520of%2520the%2520learning%2520agent.%2520However%252C%2520human%2520insights%2520are%2520often%2520mere%250Aopinions%2520and%2520educated%2520guesses%2520rather%2520than%2520well-formulated%2520arguments.%2520While%250Aopinions%2520are%2520subject%2520to%2520uncertainty%252C%2520e.g.%252C%2520due%2520to%2520partial%2520informedness%2520or%250Aignorance%2520about%2520a%2520problem%252C%2520they%2520also%2520emerge%2520earlier%2520than%2520hard%2520evidence%2520could%2520be%250Aproduced.%2520Thus%252C%2520guiding%2520reinforcement%2520learning%2520agents%2520through%2520opinions%2520offers%250Athe%2520potential%2520for%2520more%2520performant%2520learning%2520processes%252C%2520but%2520comes%2520with%2520the%250Achallenge%2520of%2520modeling%2520and%2520managing%2520opinions%2520in%2520a%2520formal%2520way.%2520In%2520this%2520article%252C%250Awe%2520present%2520a%2520method%2520to%2520guide%2520reinforcement%2520learning%2520agents%2520through%2520opinions.%2520To%250Athis%2520end%252C%2520we%2520provide%2520an%2520end-to-end%2520method%2520to%2520model%2520and%2520manage%2520advisors%2527%250Aopinions.%2520To%2520assess%2520the%2520utility%2520of%2520the%2520approach%252C%2520we%2520evaluate%2520it%2520with%2520synthetic%250Aand%2520human%2520advisors%252C%2520at%2520different%2520levels%2520of%2520uncertainty%252C%2520and%2520under%2520multiple%250Aadvise%2520strategies.%2520Our%2520results%2520indicate%2520that%2520opinions%252C%2520even%2520if%2520uncertain%252C%250Aimprove%2520the%2520performance%2520of%2520reinforcement%2520learning%2520agents%252C%2520resulting%2520in%2520higher%250Arewards%252C%2520more%2520efficient%2520exploration%252C%2520and%2520a%2520better%2520reinforced%2520policy.%2520Although%250Awe%2520demonstrate%2520our%2520approach%2520in%2520a%2520simplified%2520topological%2520running%2520example%252C%2520our%250Aapproach%2520is%2520applicable%2520to%2520complex%2520problems%2520with%2520higher%2520dimensions%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opinion-Guided%20Reinforcement%20Learning&entry.906535625=Kyanna%20Dagenais%20and%20Istvan%20David&entry.1292438233=%20%20Human%20guidance%20is%20often%20desired%20in%20reinforcement%20learning%20to%20improve%20the%0Aperformance%20of%20the%20learning%20agent.%20However%2C%20human%20insights%20are%20often%20mere%0Aopinions%20and%20educated%20guesses%20rather%20than%20well-formulated%20arguments.%20While%0Aopinions%20are%20subject%20to%20uncertainty%2C%20e.g.%2C%20due%20to%20partial%20informedness%20or%0Aignorance%20about%20a%20problem%2C%20they%20also%20emerge%20earlier%20than%20hard%20evidence%20could%20be%0Aproduced.%20Thus%2C%20guiding%20reinforcement%20learning%20agents%20through%20opinions%20offers%0Athe%20potential%20for%20more%20performant%20learning%20processes%2C%20but%20comes%20with%20the%0Achallenge%20of%20modeling%20and%20managing%20opinions%20in%20a%20formal%20way.%20In%20this%20article%2C%0Awe%20present%20a%20method%20to%20guide%20reinforcement%20learning%20agents%20through%20opinions.%20To%0Athis%20end%2C%20we%20provide%20an%20end-to-end%20method%20to%20model%20and%20manage%20advisors%27%0Aopinions.%20To%20assess%20the%20utility%20of%20the%20approach%2C%20we%20evaluate%20it%20with%20synthetic%0Aand%20human%20advisors%2C%20at%20different%20levels%20of%20uncertainty%2C%20and%20under%20multiple%0Aadvise%20strategies.%20Our%20results%20indicate%20that%20opinions%2C%20even%20if%20uncertain%2C%0Aimprove%20the%20performance%20of%20reinforcement%20learning%20agents%2C%20resulting%20in%20higher%0Arewards%2C%20more%20efficient%20exploration%2C%20and%20a%20better%20reinforced%20policy.%20Although%0Awe%20demonstrate%20our%20approach%20in%20a%20simplified%20topological%20running%20example%2C%20our%0Aapproach%20is%20applicable%20to%20complex%20problems%20with%20higher%20dimensions%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17287v1&entry.124074799=Read"},
{"title": "HeNCler: Node Clustering in Heterophilous Graphs through Learned\n  Asymmetric Similarity", "author": "Sonny Achten and Francesco Tonin and Volkan Cevher and Johan A. K. Suykens", "abstract": "  Clustering nodes in heterophilous graphs presents unique challenges due to\nthe asymmetric relationships often overlooked by traditional methods, which\nmoreover assume that good clustering corresponds to high intra-cluster and low\ninter-cluster connectivity. To address these issues, we introduce HeNCler - a\nnovel approach for Heterophilous Node Clustering. Our method begins by defining\na weighted kernel singular value decomposition to create an asymmetric\nsimilarity graph, applicable to both directed and undirected graphs. We further\nestablish that the dual problem of this formulation aligns with asymmetric\nkernel spectral clustering, interpreting learned graph similarities without\nrelying on homophily. We demonstrate the ability to solve the primal problem\ndirectly, circumventing the computational difficulties of the dual approach.\nExperimental evidence confirms that HeNCler significantly enhances performance\nin node clustering tasks within heterophilous graph contexts.\n", "link": "http://arxiv.org/abs/2405.17050v1", "date": "2024-05-27", "relevancy": 2.0839, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.423}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeNCler%3A%20Node%20Clustering%20in%20Heterophilous%20Graphs%20through%20Learned%0A%20%20Asymmetric%20Similarity&body=Title%3A%20HeNCler%3A%20Node%20Clustering%20in%20Heterophilous%20Graphs%20through%20Learned%0A%20%20Asymmetric%20Similarity%0AAuthor%3A%20Sonny%20Achten%20and%20Francesco%20Tonin%20and%20Volkan%20Cevher%20and%20Johan%20A.%20K.%20Suykens%0AAbstract%3A%20%20%20Clustering%20nodes%20in%20heterophilous%20graphs%20presents%20unique%20challenges%20due%20to%0Athe%20asymmetric%20relationships%20often%20overlooked%20by%20traditional%20methods%2C%20which%0Amoreover%20assume%20that%20good%20clustering%20corresponds%20to%20high%20intra-cluster%20and%20low%0Ainter-cluster%20connectivity.%20To%20address%20these%20issues%2C%20we%20introduce%20HeNCler%20-%20a%0Anovel%20approach%20for%20Heterophilous%20Node%20Clustering.%20Our%20method%20begins%20by%20defining%0Aa%20weighted%20kernel%20singular%20value%20decomposition%20to%20create%20an%20asymmetric%0Asimilarity%20graph%2C%20applicable%20to%20both%20directed%20and%20undirected%20graphs.%20We%20further%0Aestablish%20that%20the%20dual%20problem%20of%20this%20formulation%20aligns%20with%20asymmetric%0Akernel%20spectral%20clustering%2C%20interpreting%20learned%20graph%20similarities%20without%0Arelying%20on%20homophily.%20We%20demonstrate%20the%20ability%20to%20solve%20the%20primal%20problem%0Adirectly%2C%20circumventing%20the%20computational%20difficulties%20of%20the%20dual%20approach.%0AExperimental%20evidence%20confirms%20that%20HeNCler%20significantly%20enhances%20performance%0Ain%20node%20clustering%20tasks%20within%20heterophilous%20graph%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeNCler%253A%2520Node%2520Clustering%2520in%2520Heterophilous%2520Graphs%2520through%2520Learned%250A%2520%2520Asymmetric%2520Similarity%26entry.906535625%3DSonny%2520Achten%2520and%2520Francesco%2520Tonin%2520and%2520Volkan%2520Cevher%2520and%2520Johan%2520A.%2520K.%2520Suykens%26entry.1292438233%3D%2520%2520Clustering%2520nodes%2520in%2520heterophilous%2520graphs%2520presents%2520unique%2520challenges%2520due%2520to%250Athe%2520asymmetric%2520relationships%2520often%2520overlooked%2520by%2520traditional%2520methods%252C%2520which%250Amoreover%2520assume%2520that%2520good%2520clustering%2520corresponds%2520to%2520high%2520intra-cluster%2520and%2520low%250Ainter-cluster%2520connectivity.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520HeNCler%2520-%2520a%250Anovel%2520approach%2520for%2520Heterophilous%2520Node%2520Clustering.%2520Our%2520method%2520begins%2520by%2520defining%250Aa%2520weighted%2520kernel%2520singular%2520value%2520decomposition%2520to%2520create%2520an%2520asymmetric%250Asimilarity%2520graph%252C%2520applicable%2520to%2520both%2520directed%2520and%2520undirected%2520graphs.%2520We%2520further%250Aestablish%2520that%2520the%2520dual%2520problem%2520of%2520this%2520formulation%2520aligns%2520with%2520asymmetric%250Akernel%2520spectral%2520clustering%252C%2520interpreting%2520learned%2520graph%2520similarities%2520without%250Arelying%2520on%2520homophily.%2520We%2520demonstrate%2520the%2520ability%2520to%2520solve%2520the%2520primal%2520problem%250Adirectly%252C%2520circumventing%2520the%2520computational%2520difficulties%2520of%2520the%2520dual%2520approach.%250AExperimental%2520evidence%2520confirms%2520that%2520HeNCler%2520significantly%2520enhances%2520performance%250Ain%2520node%2520clustering%2520tasks%2520within%2520heterophilous%2520graph%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeNCler%3A%20Node%20Clustering%20in%20Heterophilous%20Graphs%20through%20Learned%0A%20%20Asymmetric%20Similarity&entry.906535625=Sonny%20Achten%20and%20Francesco%20Tonin%20and%20Volkan%20Cevher%20and%20Johan%20A.%20K.%20Suykens&entry.1292438233=%20%20Clustering%20nodes%20in%20heterophilous%20graphs%20presents%20unique%20challenges%20due%20to%0Athe%20asymmetric%20relationships%20often%20overlooked%20by%20traditional%20methods%2C%20which%0Amoreover%20assume%20that%20good%20clustering%20corresponds%20to%20high%20intra-cluster%20and%20low%0Ainter-cluster%20connectivity.%20To%20address%20these%20issues%2C%20we%20introduce%20HeNCler%20-%20a%0Anovel%20approach%20for%20Heterophilous%20Node%20Clustering.%20Our%20method%20begins%20by%20defining%0Aa%20weighted%20kernel%20singular%20value%20decomposition%20to%20create%20an%20asymmetric%0Asimilarity%20graph%2C%20applicable%20to%20both%20directed%20and%20undirected%20graphs.%20We%20further%0Aestablish%20that%20the%20dual%20problem%20of%20this%20formulation%20aligns%20with%20asymmetric%0Akernel%20spectral%20clustering%2C%20interpreting%20learned%20graph%20similarities%20without%0Arelying%20on%20homophily.%20We%20demonstrate%20the%20ability%20to%20solve%20the%20primal%20problem%0Adirectly%2C%20circumventing%20the%20computational%20difficulties%20of%20the%20dual%20approach.%0AExperimental%20evidence%20confirms%20that%20HeNCler%20significantly%20enhances%20performance%0Ain%20node%20clustering%20tasks%20within%20heterophilous%20graph%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17050v1&entry.124074799=Read"},
{"title": "Position: Foundation Agents as the Paradigm Shift for Decision Making", "author": "Xiaoqian Liu and Xingzhou Lou and Jianbin Jiao and Junge Zhang", "abstract": "  Decision making demands intricate interplay between perception, memory, and\nreasoning to discern optimal policies. Conventional approaches to decision\nmaking face challenges related to low sample efficiency and poor\ngeneralization. In contrast, foundation models in language and vision has\nshowcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\nconstruction of foundation agents as a transformative shift in the learning\nparadigm of agents. This proposal is underpinned by the formulation of\nfoundation agents with its fundamental characteristics and challenges motivated\nby the success of large language models (LLMs). Moreover, we specify the\nroadmap of foundation agents from large interactive data collection or\ngeneration, to self-supervised pretraining and adaptation, and knowledge and\nvalue alignment with LLMs. Lastly, we pinpoint critical research questions\nderived from the formulation and delineate trends for foundation agents\nsupported by real-world use cases, addressing both technical and theoretical\naspects to propel the field towards a more comprehensive and impactful future.\n", "link": "http://arxiv.org/abs/2405.17009v1", "date": "2024-05-27", "relevancy": 2.0831, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5253}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&body=Title%3A%20Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making%0AAuthor%3A%20Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang%0AAbstract%3A%20%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20has%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20its%20fundamental%20characteristics%20and%20challenges%20motivated%0Aby%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%20the%0Aroadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Foundation%2520Agents%2520as%2520the%2520Paradigm%2520Shift%2520for%2520Decision%2520Making%26entry.906535625%3DXiaoqian%2520Liu%2520and%2520Xingzhou%2520Lou%2520and%2520Jianbin%2520Jiao%2520and%2520Junge%2520Zhang%26entry.1292438233%3D%2520%2520Decision%2520making%2520demands%2520intricate%2520interplay%2520between%2520perception%252C%2520memory%252C%2520and%250Areasoning%2520to%2520discern%2520optimal%2520policies.%2520Conventional%2520approaches%2520to%2520decision%250Amaking%2520face%2520challenges%2520related%2520to%2520low%2520sample%2520efficiency%2520and%2520poor%250Ageneralization.%2520In%2520contrast%252C%2520foundation%2520models%2520in%2520language%2520and%2520vision%2520has%250Ashowcased%2520rapid%2520adaptation%2520to%2520diverse%2520new%2520tasks.%2520Therefore%252C%2520we%2520advocate%2520for%2520the%250Aconstruction%2520of%2520foundation%2520agents%2520as%2520a%2520transformative%2520shift%2520in%2520the%2520learning%250Aparadigm%2520of%2520agents.%2520This%2520proposal%2520is%2520underpinned%2520by%2520the%2520formulation%2520of%250Afoundation%2520agents%2520with%2520its%2520fundamental%2520characteristics%2520and%2520challenges%2520motivated%250Aby%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Moreover%252C%2520we%2520specify%2520the%250Aroadmap%2520of%2520foundation%2520agents%2520from%2520large%2520interactive%2520data%2520collection%2520or%250Ageneration%252C%2520to%2520self-supervised%2520pretraining%2520and%2520adaptation%252C%2520and%2520knowledge%2520and%250Avalue%2520alignment%2520with%2520LLMs.%2520Lastly%252C%2520we%2520pinpoint%2520critical%2520research%2520questions%250Aderived%2520from%2520the%2520formulation%2520and%2520delineate%2520trends%2520for%2520foundation%2520agents%250Asupported%2520by%2520real-world%2520use%2520cases%252C%2520addressing%2520both%2520technical%2520and%2520theoretical%250Aaspects%2520to%2520propel%2520the%2520field%2520towards%2520a%2520more%2520comprehensive%2520and%2520impactful%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Foundation%20Agents%20as%20the%20Paradigm%20Shift%20for%20Decision%20Making&entry.906535625=Xiaoqian%20Liu%20and%20Xingzhou%20Lou%20and%20Jianbin%20Jiao%20and%20Junge%20Zhang&entry.1292438233=%20%20Decision%20making%20demands%20intricate%20interplay%20between%20perception%2C%20memory%2C%20and%0Areasoning%20to%20discern%20optimal%20policies.%20Conventional%20approaches%20to%20decision%0Amaking%20face%20challenges%20related%20to%20low%20sample%20efficiency%20and%20poor%0Ageneralization.%20In%20contrast%2C%20foundation%20models%20in%20language%20and%20vision%20has%0Ashowcased%20rapid%20adaptation%20to%20diverse%20new%20tasks.%20Therefore%2C%20we%20advocate%20for%20the%0Aconstruction%20of%20foundation%20agents%20as%20a%20transformative%20shift%20in%20the%20learning%0Aparadigm%20of%20agents.%20This%20proposal%20is%20underpinned%20by%20the%20formulation%20of%0Afoundation%20agents%20with%20its%20fundamental%20characteristics%20and%20challenges%20motivated%0Aby%20the%20success%20of%20large%20language%20models%20%28LLMs%29.%20Moreover%2C%20we%20specify%20the%0Aroadmap%20of%20foundation%20agents%20from%20large%20interactive%20data%20collection%20or%0Ageneration%2C%20to%20self-supervised%20pretraining%20and%20adaptation%2C%20and%20knowledge%20and%0Avalue%20alignment%20with%20LLMs.%20Lastly%2C%20we%20pinpoint%20critical%20research%20questions%0Aderived%20from%20the%20formulation%20and%20delineate%20trends%20for%20foundation%20agents%0Asupported%20by%20real-world%20use%20cases%2C%20addressing%20both%20technical%20and%20theoretical%0Aaspects%20to%20propel%20the%20field%20towards%20a%20more%20comprehensive%20and%20impactful%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17009v1&entry.124074799=Read"},
{"title": "Simultaneous Deep Learning of Myocardium Segmentation and T2\n  Quantification for Acute Myocardial Infarction MRI", "author": "Yirong Zhou and Chengyan Wang and Mengtian Lu and Kunyuan Guo and Zi Wang and Dan Ruan and Rui Guo and Peijun Zhao and Jianhua Wang and Naiming Wu and Jianzhong Lin and Yinyin Chen and Hang Jin and Lianxin Xie and Lilan Wu and Liuhong Zhu and Jianjun Zhou and Congbo Cai and He Wang and Xiaobo Qu", "abstract": "  In cardiac Magnetic Resonance Imaging (MRI) analysis, simultaneous myocardial\nsegmentation and T2 quantification are crucial for assessing myocardial\npathologies. Existing methods often address these tasks separately, limiting\ntheir synergistic potential. To address this, we propose SQNet, a dual-task\nnetwork integrating Transformer and Convolutional Neural Network (CNN)\ncomponents. SQNet features a T2-refine fusion decoder for quantitative\nanalysis, leveraging global features from the Transformer, and a segmentation\ndecoder with multiple local region supervision for enhanced accuracy. A tight\ncoupling module aligns and fuses CNN and Transformer branch features, enabling\nSQNet to focus on myocardium regions. Evaluation on healthy controls (HC) and\nacute myocardial infarction patients (AMI) demonstrates superior segmentation\ndice scores (89.3/89.2) compared to state-of-the-art methods (87.7/87.9). T2\nquantification yields strong linear correlations (Pearson coefficients:\n0.84/0.93) with label values for HC/AMI, indicating accurate mapping.\nRadiologist evaluations confirm SQNet's superior image quality scores\n(4.60/4.58 for segmentation, 4.32/4.42 for T2 quantification) over\nstate-of-the-art methods (4.50/4.44 for segmentation, 3.59/4.37 for T2\nquantification). SQNet thus offers accurate simultaneous segmentation and\nquantification, enhancing cardiac disease diagnosis, such as AMI.\n", "link": "http://arxiv.org/abs/2405.10570v2", "date": "2024-05-27", "relevancy": 2.0784, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Deep%20Learning%20of%20Myocardium%20Segmentation%20and%20T2%0A%20%20Quantification%20for%20Acute%20Myocardial%20Infarction%20MRI&body=Title%3A%20Simultaneous%20Deep%20Learning%20of%20Myocardium%20Segmentation%20and%20T2%0A%20%20Quantification%20for%20Acute%20Myocardial%20Infarction%20MRI%0AAuthor%3A%20Yirong%20Zhou%20and%20Chengyan%20Wang%20and%20Mengtian%20Lu%20and%20Kunyuan%20Guo%20and%20Zi%20Wang%20and%20Dan%20Ruan%20and%20Rui%20Guo%20and%20Peijun%20Zhao%20and%20Jianhua%20Wang%20and%20Naiming%20Wu%20and%20Jianzhong%20Lin%20and%20Yinyin%20Chen%20and%20Hang%20Jin%20and%20Lianxin%20Xie%20and%20Lilan%20Wu%20and%20Liuhong%20Zhu%20and%20Jianjun%20Zhou%20and%20Congbo%20Cai%20and%20He%20Wang%20and%20Xiaobo%20Qu%0AAbstract%3A%20%20%20In%20cardiac%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20analysis%2C%20simultaneous%20myocardial%0Asegmentation%20and%20T2%20quantification%20are%20crucial%20for%20assessing%20myocardial%0Apathologies.%20Existing%20methods%20often%20address%20these%20tasks%20separately%2C%20limiting%0Atheir%20synergistic%20potential.%20To%20address%20this%2C%20we%20propose%20SQNet%2C%20a%20dual-task%0Anetwork%20integrating%20Transformer%20and%20Convolutional%20Neural%20Network%20%28CNN%29%0Acomponents.%20SQNet%20features%20a%20T2-refine%20fusion%20decoder%20for%20quantitative%0Aanalysis%2C%20leveraging%20global%20features%20from%20the%20Transformer%2C%20and%20a%20segmentation%0Adecoder%20with%20multiple%20local%20region%20supervision%20for%20enhanced%20accuracy.%20A%20tight%0Acoupling%20module%20aligns%20and%20fuses%20CNN%20and%20Transformer%20branch%20features%2C%20enabling%0ASQNet%20to%20focus%20on%20myocardium%20regions.%20Evaluation%20on%20healthy%20controls%20%28HC%29%20and%0Aacute%20myocardial%20infarction%20patients%20%28AMI%29%20demonstrates%20superior%20segmentation%0Adice%20scores%20%2889.3/89.2%29%20compared%20to%20state-of-the-art%20methods%20%2887.7/87.9%29.%20T2%0Aquantification%20yields%20strong%20linear%20correlations%20%28Pearson%20coefficients%3A%0A0.84/0.93%29%20with%20label%20values%20for%20HC/AMI%2C%20indicating%20accurate%20mapping.%0ARadiologist%20evaluations%20confirm%20SQNet%27s%20superior%20image%20quality%20scores%0A%284.60/4.58%20for%20segmentation%2C%204.32/4.42%20for%20T2%20quantification%29%20over%0Astate-of-the-art%20methods%20%284.50/4.44%20for%20segmentation%2C%203.59/4.37%20for%20T2%0Aquantification%29.%20SQNet%20thus%20offers%20accurate%20simultaneous%20segmentation%20and%0Aquantification%2C%20enhancing%20cardiac%20disease%20diagnosis%2C%20such%20as%20AMI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Deep%2520Learning%2520of%2520Myocardium%2520Segmentation%2520and%2520T2%250A%2520%2520Quantification%2520for%2520Acute%2520Myocardial%2520Infarction%2520MRI%26entry.906535625%3DYirong%2520Zhou%2520and%2520Chengyan%2520Wang%2520and%2520Mengtian%2520Lu%2520and%2520Kunyuan%2520Guo%2520and%2520Zi%2520Wang%2520and%2520Dan%2520Ruan%2520and%2520Rui%2520Guo%2520and%2520Peijun%2520Zhao%2520and%2520Jianhua%2520Wang%2520and%2520Naiming%2520Wu%2520and%2520Jianzhong%2520Lin%2520and%2520Yinyin%2520Chen%2520and%2520Hang%2520Jin%2520and%2520Lianxin%2520Xie%2520and%2520Lilan%2520Wu%2520and%2520Liuhong%2520Zhu%2520and%2520Jianjun%2520Zhou%2520and%2520Congbo%2520Cai%2520and%2520He%2520Wang%2520and%2520Xiaobo%2520Qu%26entry.1292438233%3D%2520%2520In%2520cardiac%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520analysis%252C%2520simultaneous%2520myocardial%250Asegmentation%2520and%2520T2%2520quantification%2520are%2520crucial%2520for%2520assessing%2520myocardial%250Apathologies.%2520Existing%2520methods%2520often%2520address%2520these%2520tasks%2520separately%252C%2520limiting%250Atheir%2520synergistic%2520potential.%2520To%2520address%2520this%252C%2520we%2520propose%2520SQNet%252C%2520a%2520dual-task%250Anetwork%2520integrating%2520Transformer%2520and%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%250Acomponents.%2520SQNet%2520features%2520a%2520T2-refine%2520fusion%2520decoder%2520for%2520quantitative%250Aanalysis%252C%2520leveraging%2520global%2520features%2520from%2520the%2520Transformer%252C%2520and%2520a%2520segmentation%250Adecoder%2520with%2520multiple%2520local%2520region%2520supervision%2520for%2520enhanced%2520accuracy.%2520A%2520tight%250Acoupling%2520module%2520aligns%2520and%2520fuses%2520CNN%2520and%2520Transformer%2520branch%2520features%252C%2520enabling%250ASQNet%2520to%2520focus%2520on%2520myocardium%2520regions.%2520Evaluation%2520on%2520healthy%2520controls%2520%2528HC%2529%2520and%250Aacute%2520myocardial%2520infarction%2520patients%2520%2528AMI%2529%2520demonstrates%2520superior%2520segmentation%250Adice%2520scores%2520%252889.3/89.2%2529%2520compared%2520to%2520state-of-the-art%2520methods%2520%252887.7/87.9%2529.%2520T2%250Aquantification%2520yields%2520strong%2520linear%2520correlations%2520%2528Pearson%2520coefficients%253A%250A0.84/0.93%2529%2520with%2520label%2520values%2520for%2520HC/AMI%252C%2520indicating%2520accurate%2520mapping.%250ARadiologist%2520evaluations%2520confirm%2520SQNet%2527s%2520superior%2520image%2520quality%2520scores%250A%25284.60/4.58%2520for%2520segmentation%252C%25204.32/4.42%2520for%2520T2%2520quantification%2529%2520over%250Astate-of-the-art%2520methods%2520%25284.50/4.44%2520for%2520segmentation%252C%25203.59/4.37%2520for%2520T2%250Aquantification%2529.%2520SQNet%2520thus%2520offers%2520accurate%2520simultaneous%2520segmentation%2520and%250Aquantification%252C%2520enhancing%2520cardiac%2520disease%2520diagnosis%252C%2520such%2520as%2520AMI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Deep%20Learning%20of%20Myocardium%20Segmentation%20and%20T2%0A%20%20Quantification%20for%20Acute%20Myocardial%20Infarction%20MRI&entry.906535625=Yirong%20Zhou%20and%20Chengyan%20Wang%20and%20Mengtian%20Lu%20and%20Kunyuan%20Guo%20and%20Zi%20Wang%20and%20Dan%20Ruan%20and%20Rui%20Guo%20and%20Peijun%20Zhao%20and%20Jianhua%20Wang%20and%20Naiming%20Wu%20and%20Jianzhong%20Lin%20and%20Yinyin%20Chen%20and%20Hang%20Jin%20and%20Lianxin%20Xie%20and%20Lilan%20Wu%20and%20Liuhong%20Zhu%20and%20Jianjun%20Zhou%20and%20Congbo%20Cai%20and%20He%20Wang%20and%20Xiaobo%20Qu&entry.1292438233=%20%20In%20cardiac%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20analysis%2C%20simultaneous%20myocardial%0Asegmentation%20and%20T2%20quantification%20are%20crucial%20for%20assessing%20myocardial%0Apathologies.%20Existing%20methods%20often%20address%20these%20tasks%20separately%2C%20limiting%0Atheir%20synergistic%20potential.%20To%20address%20this%2C%20we%20propose%20SQNet%2C%20a%20dual-task%0Anetwork%20integrating%20Transformer%20and%20Convolutional%20Neural%20Network%20%28CNN%29%0Acomponents.%20SQNet%20features%20a%20T2-refine%20fusion%20decoder%20for%20quantitative%0Aanalysis%2C%20leveraging%20global%20features%20from%20the%20Transformer%2C%20and%20a%20segmentation%0Adecoder%20with%20multiple%20local%20region%20supervision%20for%20enhanced%20accuracy.%20A%20tight%0Acoupling%20module%20aligns%20and%20fuses%20CNN%20and%20Transformer%20branch%20features%2C%20enabling%0ASQNet%20to%20focus%20on%20myocardium%20regions.%20Evaluation%20on%20healthy%20controls%20%28HC%29%20and%0Aacute%20myocardial%20infarction%20patients%20%28AMI%29%20demonstrates%20superior%20segmentation%0Adice%20scores%20%2889.3/89.2%29%20compared%20to%20state-of-the-art%20methods%20%2887.7/87.9%29.%20T2%0Aquantification%20yields%20strong%20linear%20correlations%20%28Pearson%20coefficients%3A%0A0.84/0.93%29%20with%20label%20values%20for%20HC/AMI%2C%20indicating%20accurate%20mapping.%0ARadiologist%20evaluations%20confirm%20SQNet%27s%20superior%20image%20quality%20scores%0A%284.60/4.58%20for%20segmentation%2C%204.32/4.42%20for%20T2%20quantification%29%20over%0Astate-of-the-art%20methods%20%284.50/4.44%20for%20segmentation%2C%203.59/4.37%20for%20T2%0Aquantification%29.%20SQNet%20thus%20offers%20accurate%20simultaneous%20segmentation%20and%0Aquantification%2C%20enhancing%20cardiac%20disease%20diagnosis%2C%20such%20as%20AMI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10570v2&entry.124074799=Read"},
{"title": "Improving Data-aware and Parameter-aware Robustness for Continual\n  Learning", "author": "Hanxi Xiao and Fan Lyu", "abstract": "  The goal of Continual Learning (CL) task is to continuously learn multiple\nnew tasks sequentially while achieving a balance between the plasticity and\nstability of new and old knowledge. This paper analyzes that this insufficiency\narises from the ineffective handling of outliers, leading to abnormal gradients\nand unexpected model updates. To address this issue, we enhance the data-aware\nand parameter-aware robustness of CL, proposing a Robust Continual Learning\n(RCL) method. From the data perspective, we develop a contrastive loss based on\nthe concepts of uniformity and alignment, forming a feature distribution that\nis more applicable to outliers. From the parameter perspective, we present a\nforward strategy for worst-case perturbation and apply robust gradient\nprojection to the parameters. The experimental results on three benchmarks show\nthat the proposed method effectively maintains robustness and achieves new\nstate-of-the-art (SOTA) results. The code is available at:\nhttps://github.com/HanxiXiao/RCL\n", "link": "http://arxiv.org/abs/2405.17054v1", "date": "2024-05-27", "relevancy": 2.077, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5209}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Data-aware%20and%20Parameter-aware%20Robustness%20for%20Continual%0A%20%20Learning&body=Title%3A%20Improving%20Data-aware%20and%20Parameter-aware%20Robustness%20for%20Continual%0A%20%20Learning%0AAuthor%3A%20Hanxi%20Xiao%20and%20Fan%20Lyu%0AAbstract%3A%20%20%20The%20goal%20of%20Continual%20Learning%20%28CL%29%20task%20is%20to%20continuously%20learn%20multiple%0Anew%20tasks%20sequentially%20while%20achieving%20a%20balance%20between%20the%20plasticity%20and%0Astability%20of%20new%20and%20old%20knowledge.%20This%20paper%20analyzes%20that%20this%20insufficiency%0Aarises%20from%20the%20ineffective%20handling%20of%20outliers%2C%20leading%20to%20abnormal%20gradients%0Aand%20unexpected%20model%20updates.%20To%20address%20this%20issue%2C%20we%20enhance%20the%20data-aware%0Aand%20parameter-aware%20robustness%20of%20CL%2C%20proposing%20a%20Robust%20Continual%20Learning%0A%28RCL%29%20method.%20From%20the%20data%20perspective%2C%20we%20develop%20a%20contrastive%20loss%20based%20on%0Athe%20concepts%20of%20uniformity%20and%20alignment%2C%20forming%20a%20feature%20distribution%20that%0Ais%20more%20applicable%20to%20outliers.%20From%20the%20parameter%20perspective%2C%20we%20present%20a%0Aforward%20strategy%20for%20worst-case%20perturbation%20and%20apply%20robust%20gradient%0Aprojection%20to%20the%20parameters.%20The%20experimental%20results%20on%20three%20benchmarks%20show%0Athat%20the%20proposed%20method%20effectively%20maintains%20robustness%20and%20achieves%20new%0Astate-of-the-art%20%28SOTA%29%20results.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HanxiXiao/RCL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Data-aware%2520and%2520Parameter-aware%2520Robustness%2520for%2520Continual%250A%2520%2520Learning%26entry.906535625%3DHanxi%2520Xiao%2520and%2520Fan%2520Lyu%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520Continual%2520Learning%2520%2528CL%2529%2520task%2520is%2520to%2520continuously%2520learn%2520multiple%250Anew%2520tasks%2520sequentially%2520while%2520achieving%2520a%2520balance%2520between%2520the%2520plasticity%2520and%250Astability%2520of%2520new%2520and%2520old%2520knowledge.%2520This%2520paper%2520analyzes%2520that%2520this%2520insufficiency%250Aarises%2520from%2520the%2520ineffective%2520handling%2520of%2520outliers%252C%2520leading%2520to%2520abnormal%2520gradients%250Aand%2520unexpected%2520model%2520updates.%2520To%2520address%2520this%2520issue%252C%2520we%2520enhance%2520the%2520data-aware%250Aand%2520parameter-aware%2520robustness%2520of%2520CL%252C%2520proposing%2520a%2520Robust%2520Continual%2520Learning%250A%2528RCL%2529%2520method.%2520From%2520the%2520data%2520perspective%252C%2520we%2520develop%2520a%2520contrastive%2520loss%2520based%2520on%250Athe%2520concepts%2520of%2520uniformity%2520and%2520alignment%252C%2520forming%2520a%2520feature%2520distribution%2520that%250Ais%2520more%2520applicable%2520to%2520outliers.%2520From%2520the%2520parameter%2520perspective%252C%2520we%2520present%2520a%250Aforward%2520strategy%2520for%2520worst-case%2520perturbation%2520and%2520apply%2520robust%2520gradient%250Aprojection%2520to%2520the%2520parameters.%2520The%2520experimental%2520results%2520on%2520three%2520benchmarks%2520show%250Athat%2520the%2520proposed%2520method%2520effectively%2520maintains%2520robustness%2520and%2520achieves%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520results.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/HanxiXiao/RCL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Data-aware%20and%20Parameter-aware%20Robustness%20for%20Continual%0A%20%20Learning&entry.906535625=Hanxi%20Xiao%20and%20Fan%20Lyu&entry.1292438233=%20%20The%20goal%20of%20Continual%20Learning%20%28CL%29%20task%20is%20to%20continuously%20learn%20multiple%0Anew%20tasks%20sequentially%20while%20achieving%20a%20balance%20between%20the%20plasticity%20and%0Astability%20of%20new%20and%20old%20knowledge.%20This%20paper%20analyzes%20that%20this%20insufficiency%0Aarises%20from%20the%20ineffective%20handling%20of%20outliers%2C%20leading%20to%20abnormal%20gradients%0Aand%20unexpected%20model%20updates.%20To%20address%20this%20issue%2C%20we%20enhance%20the%20data-aware%0Aand%20parameter-aware%20robustness%20of%20CL%2C%20proposing%20a%20Robust%20Continual%20Learning%0A%28RCL%29%20method.%20From%20the%20data%20perspective%2C%20we%20develop%20a%20contrastive%20loss%20based%20on%0Athe%20concepts%20of%20uniformity%20and%20alignment%2C%20forming%20a%20feature%20distribution%20that%0Ais%20more%20applicable%20to%20outliers.%20From%20the%20parameter%20perspective%2C%20we%20present%20a%0Aforward%20strategy%20for%20worst-case%20perturbation%20and%20apply%20robust%20gradient%0Aprojection%20to%20the%20parameters.%20The%20experimental%20results%20on%20three%20benchmarks%20show%0Athat%20the%20proposed%20method%20effectively%20maintains%20robustness%20and%20achieves%20new%0Astate-of-the-art%20%28SOTA%29%20results.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HanxiXiao/RCL%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17054v1&entry.124074799=Read"},
{"title": "R2D2 image reconstruction with model uncertainty quantification in radio\n  astronomy", "author": "Amir Aghabiglou and Chung San Chu and Arwa Dabbech and Yves Wiaux", "abstract": "  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)\napproach was recently introduced for Radio-Interferometric (RI) imaging in\nastronomy. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. In\nthis work, we investigate the robustness of the R2D2 image estimation process,\nby studying the uncertainty associated with its series of learned models.\nAdopting an ensemble averaging approach, multiple series can be trained,\narising from different random DNN initializations of the training process at\neach iteration. The resulting multiple R2D2 instances can also be leveraged to\ngenerate ``R2D2 samples'', from which empirical mean and standard deviation\nendow the algorithm with a joint estimation and uncertainty quantification\nfunctionality. Focusing on RI imaging, and adopting a telescope-specific\napproach, multiple R2D2 instances were trained to encompass the most general\nobservation setting of the Very Large Array (VLA). Simulations and real-data\nexperiments confirm that: (i) R2D2's image estimation capability is superior to\nthat of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction\ncapability (arising from series with only few DNNs) makes the computation of\nmultiple reconstruction samples and of uncertainty maps practical even at large\nimage dimension; (iii) it is characterized by a very low model uncertainty.\n", "link": "http://arxiv.org/abs/2403.18052v2", "date": "2024-05-27", "relevancy": 2.073, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5377}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5149}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2D2%20image%20reconstruction%20with%20model%20uncertainty%20quantification%20in%20radio%0A%20%20astronomy&body=Title%3A%20R2D2%20image%20reconstruction%20with%20model%20uncertainty%20quantification%20in%20radio%0A%20%20astronomy%0AAuthor%3A%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Arwa%20Dabbech%20and%20Yves%20Wiaux%0AAbstract%3A%20%20%20The%20%60%60Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%20range%20imaging%27%27%20%28R2D2%29%0Aapproach%20was%20recently%20introduced%20for%20Radio-Interferometric%20%28RI%29%20imaging%20in%0Aastronomy.%20R2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%0Aiteratively%20estimated%20as%20outputs%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20taking%20the%0Aprevious%20iteration%27s%20image%20estimate%20and%20associated%20data%20residual%20as%20inputs.%20In%0Athis%20work%2C%20we%20investigate%20the%20robustness%20of%20the%20R2D2%20image%20estimation%20process%2C%0Aby%20studying%20the%20uncertainty%20associated%20with%20its%20series%20of%20learned%20models.%0AAdopting%20an%20ensemble%20averaging%20approach%2C%20multiple%20series%20can%20be%20trained%2C%0Aarising%20from%20different%20random%20DNN%20initializations%20of%20the%20training%20process%20at%0Aeach%20iteration.%20The%20resulting%20multiple%20R2D2%20instances%20can%20also%20be%20leveraged%20to%0Agenerate%20%60%60R2D2%20samples%27%27%2C%20from%20which%20empirical%20mean%20and%20standard%20deviation%0Aendow%20the%20algorithm%20with%20a%20joint%20estimation%20and%20uncertainty%20quantification%0Afunctionality.%20Focusing%20on%20RI%20imaging%2C%20and%20adopting%20a%20telescope-specific%0Aapproach%2C%20multiple%20R2D2%20instances%20were%20trained%20to%20encompass%20the%20most%20general%0Aobservation%20setting%20of%20the%20Very%20Large%20Array%20%28VLA%29.%20Simulations%20and%20real-data%0Aexperiments%20confirm%20that%3A%20%28i%29%20R2D2%27s%20image%20estimation%20capability%20is%20superior%20to%0Athat%20of%20the%20state-of-the-art%20algorithms%3B%20%28ii%29%20its%20ultra-fast%20reconstruction%0Acapability%20%28arising%20from%20series%20with%20only%20few%20DNNs%29%20makes%20the%20computation%20of%0Amultiple%20reconstruction%20samples%20and%20of%20uncertainty%20maps%20practical%20even%20at%20large%0Aimage%20dimension%3B%20%28iii%29%20it%20is%20characterized%20by%20a%20very%20low%20model%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2D2%2520image%2520reconstruction%2520with%2520model%2520uncertainty%2520quantification%2520in%2520radio%250A%2520%2520astronomy%26entry.906535625%3DAmir%2520Aghabiglou%2520and%2520Chung%2520San%2520Chu%2520and%2520Arwa%2520Dabbech%2520and%2520Yves%2520Wiaux%26entry.1292438233%3D%2520%2520The%2520%2560%2560Residual-to-Residual%2520DNN%2520series%2520for%2520high-Dynamic%2520range%2520imaging%2527%2527%2520%2528R2D2%2529%250Aapproach%2520was%2520recently%2520introduced%2520for%2520Radio-Interferometric%2520%2528RI%2529%2520imaging%2520in%250Aastronomy.%2520R2D2%2527s%2520reconstruction%2520is%2520formed%2520as%2520a%2520series%2520of%2520residual%2520images%252C%250Aiteratively%2520estimated%2520as%2520outputs%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520taking%2520the%250Aprevious%2520iteration%2527s%2520image%2520estimate%2520and%2520associated%2520data%2520residual%2520as%2520inputs.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520the%2520robustness%2520of%2520the%2520R2D2%2520image%2520estimation%2520process%252C%250Aby%2520studying%2520the%2520uncertainty%2520associated%2520with%2520its%2520series%2520of%2520learned%2520models.%250AAdopting%2520an%2520ensemble%2520averaging%2520approach%252C%2520multiple%2520series%2520can%2520be%2520trained%252C%250Aarising%2520from%2520different%2520random%2520DNN%2520initializations%2520of%2520the%2520training%2520process%2520at%250Aeach%2520iteration.%2520The%2520resulting%2520multiple%2520R2D2%2520instances%2520can%2520also%2520be%2520leveraged%2520to%250Agenerate%2520%2560%2560R2D2%2520samples%2527%2527%252C%2520from%2520which%2520empirical%2520mean%2520and%2520standard%2520deviation%250Aendow%2520the%2520algorithm%2520with%2520a%2520joint%2520estimation%2520and%2520uncertainty%2520quantification%250Afunctionality.%2520Focusing%2520on%2520RI%2520imaging%252C%2520and%2520adopting%2520a%2520telescope-specific%250Aapproach%252C%2520multiple%2520R2D2%2520instances%2520were%2520trained%2520to%2520encompass%2520the%2520most%2520general%250Aobservation%2520setting%2520of%2520the%2520Very%2520Large%2520Array%2520%2528VLA%2529.%2520Simulations%2520and%2520real-data%250Aexperiments%2520confirm%2520that%253A%2520%2528i%2529%2520R2D2%2527s%2520image%2520estimation%2520capability%2520is%2520superior%2520to%250Athat%2520of%2520the%2520state-of-the-art%2520algorithms%253B%2520%2528ii%2529%2520its%2520ultra-fast%2520reconstruction%250Acapability%2520%2528arising%2520from%2520series%2520with%2520only%2520few%2520DNNs%2529%2520makes%2520the%2520computation%2520of%250Amultiple%2520reconstruction%2520samples%2520and%2520of%2520uncertainty%2520maps%2520practical%2520even%2520at%2520large%250Aimage%2520dimension%253B%2520%2528iii%2529%2520it%2520is%2520characterized%2520by%2520a%2520very%2520low%2520model%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2D2%20image%20reconstruction%20with%20model%20uncertainty%20quantification%20in%20radio%0A%20%20astronomy&entry.906535625=Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Arwa%20Dabbech%20and%20Yves%20Wiaux&entry.1292438233=%20%20The%20%60%60Residual-to-Residual%20DNN%20series%20for%20high-Dynamic%20range%20imaging%27%27%20%28R2D2%29%0Aapproach%20was%20recently%20introduced%20for%20Radio-Interferometric%20%28RI%29%20imaging%20in%0Aastronomy.%20R2D2%27s%20reconstruction%20is%20formed%20as%20a%20series%20of%20residual%20images%2C%0Aiteratively%20estimated%20as%20outputs%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20taking%20the%0Aprevious%20iteration%27s%20image%20estimate%20and%20associated%20data%20residual%20as%20inputs.%20In%0Athis%20work%2C%20we%20investigate%20the%20robustness%20of%20the%20R2D2%20image%20estimation%20process%2C%0Aby%20studying%20the%20uncertainty%20associated%20with%20its%20series%20of%20learned%20models.%0AAdopting%20an%20ensemble%20averaging%20approach%2C%20multiple%20series%20can%20be%20trained%2C%0Aarising%20from%20different%20random%20DNN%20initializations%20of%20the%20training%20process%20at%0Aeach%20iteration.%20The%20resulting%20multiple%20R2D2%20instances%20can%20also%20be%20leveraged%20to%0Agenerate%20%60%60R2D2%20samples%27%27%2C%20from%20which%20empirical%20mean%20and%20standard%20deviation%0Aendow%20the%20algorithm%20with%20a%20joint%20estimation%20and%20uncertainty%20quantification%0Afunctionality.%20Focusing%20on%20RI%20imaging%2C%20and%20adopting%20a%20telescope-specific%0Aapproach%2C%20multiple%20R2D2%20instances%20were%20trained%20to%20encompass%20the%20most%20general%0Aobservation%20setting%20of%20the%20Very%20Large%20Array%20%28VLA%29.%20Simulations%20and%20real-data%0Aexperiments%20confirm%20that%3A%20%28i%29%20R2D2%27s%20image%20estimation%20capability%20is%20superior%20to%0Athat%20of%20the%20state-of-the-art%20algorithms%3B%20%28ii%29%20its%20ultra-fast%20reconstruction%0Acapability%20%28arising%20from%20series%20with%20only%20few%20DNNs%29%20makes%20the%20computation%20of%0Amultiple%20reconstruction%20samples%20and%20of%20uncertainty%20maps%20practical%20even%20at%20large%0Aimage%20dimension%3B%20%28iii%29%20it%20is%20characterized%20by%20a%20very%20low%20model%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18052v2&entry.124074799=Read"},
{"title": "Facilitating Advanced Sentinel-2 Analysis Through a Simplified\n  Computation of Nadir BRDF Adjusted Reflectance", "author": "David Montero and Miguel D. Mahecha and C\u00e9sar Aybar and Clemens Mosig and Sebastian Wieneke", "abstract": "  The Sentinel-2 (S2) mission from the European Space Agency's Copernicus\nprogram provides essential data for Earth surface analysis. Its Level-2A\nproducts deliver high-to-medium resolution (10-60 m) surface reflectance (SR)\ndata through the MultiSpectral Instrument (MSI). To enhance the accuracy and\ncomparability of SR data, adjustments simulating a nadir viewing perspective\nare essential. These corrections address the anisotropic nature of SR and the\nvariability in sun and observation angles, ensuring consistent image\ncomparisons over time and under different conditions. The $c$-factor method, a\nsimple yet effective algorithm, adjusts observed S2 SR by using the MODIS BRDF\nmodel to achieve Nadir BRDF Adjusted Reflectance (NBAR). Despite the\nstraightforward application of the $c$-factor to individual images, a cohesive\nPython framework for its application across multiple S2 images and Earth System\nData Cubes (ESDCs) from cloud-stored data has been lacking. Here we introduce\nsen2nbar, a Python package crafted to convert S2 SR data to NBAR, supporting\nboth individual images and ESDCs derived from cloud-stored data. This package\nsimplifies the conversion of S2 SR data to NBAR via a single function,\norganized into modules for efficient process management. By facilitating NBAR\nconversion for both SAFE files and ESDCs from SpatioTemporal Asset Catalogs\n(STAC), sen2nbar is developed as a flexible tool that can handle diverse data\nformat requirements. We anticipate that sen2nbar will considerably contribute\nto the standardization and harmonization of S2 data, offering a robust solution\nfor a diverse range of users across various applications. sen2nbar is an\nopen-source tool available at https://github.com/ESDS-Leipzig/sen2nbar.\n", "link": "http://arxiv.org/abs/2404.15812v2", "date": "2024-05-27", "relevancy": 1.3448, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facilitating%20Advanced%20Sentinel-2%20Analysis%20Through%20a%20Simplified%0A%20%20Computation%20of%20Nadir%20BRDF%20Adjusted%20Reflectance&body=Title%3A%20Facilitating%20Advanced%20Sentinel-2%20Analysis%20Through%20a%20Simplified%0A%20%20Computation%20of%20Nadir%20BRDF%20Adjusted%20Reflectance%0AAuthor%3A%20David%20Montero%20and%20Miguel%20D.%20Mahecha%20and%20C%C3%A9sar%20Aybar%20and%20Clemens%20Mosig%20and%20Sebastian%20Wieneke%0AAbstract%3A%20%20%20The%20Sentinel-2%20%28S2%29%20mission%20from%20the%20European%20Space%20Agency%27s%20Copernicus%0Aprogram%20provides%20essential%20data%20for%20Earth%20surface%20analysis.%20Its%20Level-2A%0Aproducts%20deliver%20high-to-medium%20resolution%20%2810-60%20m%29%20surface%20reflectance%20%28SR%29%0Adata%20through%20the%20MultiSpectral%20Instrument%20%28MSI%29.%20To%20enhance%20the%20accuracy%20and%0Acomparability%20of%20SR%20data%2C%20adjustments%20simulating%20a%20nadir%20viewing%20perspective%0Aare%20essential.%20These%20corrections%20address%20the%20anisotropic%20nature%20of%20SR%20and%20the%0Avariability%20in%20sun%20and%20observation%20angles%2C%20ensuring%20consistent%20image%0Acomparisons%20over%20time%20and%20under%20different%20conditions.%20The%20%24c%24-factor%20method%2C%20a%0Asimple%20yet%20effective%20algorithm%2C%20adjusts%20observed%20S2%20SR%20by%20using%20the%20MODIS%20BRDF%0Amodel%20to%20achieve%20Nadir%20BRDF%20Adjusted%20Reflectance%20%28NBAR%29.%20Despite%20the%0Astraightforward%20application%20of%20the%20%24c%24-factor%20to%20individual%20images%2C%20a%20cohesive%0APython%20framework%20for%20its%20application%20across%20multiple%20S2%20images%20and%20Earth%20System%0AData%20Cubes%20%28ESDCs%29%20from%20cloud-stored%20data%20has%20been%20lacking.%20Here%20we%20introduce%0Asen2nbar%2C%20a%20Python%20package%20crafted%20to%20convert%20S2%20SR%20data%20to%20NBAR%2C%20supporting%0Aboth%20individual%20images%20and%20ESDCs%20derived%20from%20cloud-stored%20data.%20This%20package%0Asimplifies%20the%20conversion%20of%20S2%20SR%20data%20to%20NBAR%20via%20a%20single%20function%2C%0Aorganized%20into%20modules%20for%20efficient%20process%20management.%20By%20facilitating%20NBAR%0Aconversion%20for%20both%20SAFE%20files%20and%20ESDCs%20from%20SpatioTemporal%20Asset%20Catalogs%0A%28STAC%29%2C%20sen2nbar%20is%20developed%20as%20a%20flexible%20tool%20that%20can%20handle%20diverse%20data%0Aformat%20requirements.%20We%20anticipate%20that%20sen2nbar%20will%20considerably%20contribute%0Ato%20the%20standardization%20and%20harmonization%20of%20S2%20data%2C%20offering%20a%20robust%20solution%0Afor%20a%20diverse%20range%20of%20users%20across%20various%20applications.%20sen2nbar%20is%20an%0Aopen-source%20tool%20available%20at%20https%3A//github.com/ESDS-Leipzig/sen2nbar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15812v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacilitating%2520Advanced%2520Sentinel-2%2520Analysis%2520Through%2520a%2520Simplified%250A%2520%2520Computation%2520of%2520Nadir%2520BRDF%2520Adjusted%2520Reflectance%26entry.906535625%3DDavid%2520Montero%2520and%2520Miguel%2520D.%2520Mahecha%2520and%2520C%25C3%25A9sar%2520Aybar%2520and%2520Clemens%2520Mosig%2520and%2520Sebastian%2520Wieneke%26entry.1292438233%3D%2520%2520The%2520Sentinel-2%2520%2528S2%2529%2520mission%2520from%2520the%2520European%2520Space%2520Agency%2527s%2520Copernicus%250Aprogram%2520provides%2520essential%2520data%2520for%2520Earth%2520surface%2520analysis.%2520Its%2520Level-2A%250Aproducts%2520deliver%2520high-to-medium%2520resolution%2520%252810-60%2520m%2529%2520surface%2520reflectance%2520%2528SR%2529%250Adata%2520through%2520the%2520MultiSpectral%2520Instrument%2520%2528MSI%2529.%2520To%2520enhance%2520the%2520accuracy%2520and%250Acomparability%2520of%2520SR%2520data%252C%2520adjustments%2520simulating%2520a%2520nadir%2520viewing%2520perspective%250Aare%2520essential.%2520These%2520corrections%2520address%2520the%2520anisotropic%2520nature%2520of%2520SR%2520and%2520the%250Avariability%2520in%2520sun%2520and%2520observation%2520angles%252C%2520ensuring%2520consistent%2520image%250Acomparisons%2520over%2520time%2520and%2520under%2520different%2520conditions.%2520The%2520%2524c%2524-factor%2520method%252C%2520a%250Asimple%2520yet%2520effective%2520algorithm%252C%2520adjusts%2520observed%2520S2%2520SR%2520by%2520using%2520the%2520MODIS%2520BRDF%250Amodel%2520to%2520achieve%2520Nadir%2520BRDF%2520Adjusted%2520Reflectance%2520%2528NBAR%2529.%2520Despite%2520the%250Astraightforward%2520application%2520of%2520the%2520%2524c%2524-factor%2520to%2520individual%2520images%252C%2520a%2520cohesive%250APython%2520framework%2520for%2520its%2520application%2520across%2520multiple%2520S2%2520images%2520and%2520Earth%2520System%250AData%2520Cubes%2520%2528ESDCs%2529%2520from%2520cloud-stored%2520data%2520has%2520been%2520lacking.%2520Here%2520we%2520introduce%250Asen2nbar%252C%2520a%2520Python%2520package%2520crafted%2520to%2520convert%2520S2%2520SR%2520data%2520to%2520NBAR%252C%2520supporting%250Aboth%2520individual%2520images%2520and%2520ESDCs%2520derived%2520from%2520cloud-stored%2520data.%2520This%2520package%250Asimplifies%2520the%2520conversion%2520of%2520S2%2520SR%2520data%2520to%2520NBAR%2520via%2520a%2520single%2520function%252C%250Aorganized%2520into%2520modules%2520for%2520efficient%2520process%2520management.%2520By%2520facilitating%2520NBAR%250Aconversion%2520for%2520both%2520SAFE%2520files%2520and%2520ESDCs%2520from%2520SpatioTemporal%2520Asset%2520Catalogs%250A%2528STAC%2529%252C%2520sen2nbar%2520is%2520developed%2520as%2520a%2520flexible%2520tool%2520that%2520can%2520handle%2520diverse%2520data%250Aformat%2520requirements.%2520We%2520anticipate%2520that%2520sen2nbar%2520will%2520considerably%2520contribute%250Ato%2520the%2520standardization%2520and%2520harmonization%2520of%2520S2%2520data%252C%2520offering%2520a%2520robust%2520solution%250Afor%2520a%2520diverse%2520range%2520of%2520users%2520across%2520various%2520applications.%2520sen2nbar%2520is%2520an%250Aopen-source%2520tool%2520available%2520at%2520https%253A//github.com/ESDS-Leipzig/sen2nbar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15812v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facilitating%20Advanced%20Sentinel-2%20Analysis%20Through%20a%20Simplified%0A%20%20Computation%20of%20Nadir%20BRDF%20Adjusted%20Reflectance&entry.906535625=David%20Montero%20and%20Miguel%20D.%20Mahecha%20and%20C%C3%A9sar%20Aybar%20and%20Clemens%20Mosig%20and%20Sebastian%20Wieneke&entry.1292438233=%20%20The%20Sentinel-2%20%28S2%29%20mission%20from%20the%20European%20Space%20Agency%27s%20Copernicus%0Aprogram%20provides%20essential%20data%20for%20Earth%20surface%20analysis.%20Its%20Level-2A%0Aproducts%20deliver%20high-to-medium%20resolution%20%2810-60%20m%29%20surface%20reflectance%20%28SR%29%0Adata%20through%20the%20MultiSpectral%20Instrument%20%28MSI%29.%20To%20enhance%20the%20accuracy%20and%0Acomparability%20of%20SR%20data%2C%20adjustments%20simulating%20a%20nadir%20viewing%20perspective%0Aare%20essential.%20These%20corrections%20address%20the%20anisotropic%20nature%20of%20SR%20and%20the%0Avariability%20in%20sun%20and%20observation%20angles%2C%20ensuring%20consistent%20image%0Acomparisons%20over%20time%20and%20under%20different%20conditions.%20The%20%24c%24-factor%20method%2C%20a%0Asimple%20yet%20effective%20algorithm%2C%20adjusts%20observed%20S2%20SR%20by%20using%20the%20MODIS%20BRDF%0Amodel%20to%20achieve%20Nadir%20BRDF%20Adjusted%20Reflectance%20%28NBAR%29.%20Despite%20the%0Astraightforward%20application%20of%20the%20%24c%24-factor%20to%20individual%20images%2C%20a%20cohesive%0APython%20framework%20for%20its%20application%20across%20multiple%20S2%20images%20and%20Earth%20System%0AData%20Cubes%20%28ESDCs%29%20from%20cloud-stored%20data%20has%20been%20lacking.%20Here%20we%20introduce%0Asen2nbar%2C%20a%20Python%20package%20crafted%20to%20convert%20S2%20SR%20data%20to%20NBAR%2C%20supporting%0Aboth%20individual%20images%20and%20ESDCs%20derived%20from%20cloud-stored%20data.%20This%20package%0Asimplifies%20the%20conversion%20of%20S2%20SR%20data%20to%20NBAR%20via%20a%20single%20function%2C%0Aorganized%20into%20modules%20for%20efficient%20process%20management.%20By%20facilitating%20NBAR%0Aconversion%20for%20both%20SAFE%20files%20and%20ESDCs%20from%20SpatioTemporal%20Asset%20Catalogs%0A%28STAC%29%2C%20sen2nbar%20is%20developed%20as%20a%20flexible%20tool%20that%20can%20handle%20diverse%20data%0Aformat%20requirements.%20We%20anticipate%20that%20sen2nbar%20will%20considerably%20contribute%0Ato%20the%20standardization%20and%20harmonization%20of%20S2%20data%2C%20offering%20a%20robust%20solution%0Afor%20a%20diverse%20range%20of%20users%20across%20various%20applications.%20sen2nbar%20is%20an%0Aopen-source%20tool%20available%20at%20https%3A//github.com/ESDS-Leipzig/sen2nbar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15812v2&entry.124074799=Read"},
{"title": "Causal Temporal Regime Structure Learning", "author": "Abdellah Rahmani and Pascal Frossard", "abstract": "  We address the challenge of structure learning from multivariate time series\nthat are characterized by a sequence of different, unknown regimes. We\nintroduce a new optimization-based method (CASTOR), that concurrently learns\nthe Directed Acyclic Graph (DAG) for each regime and determine the number of\nregimes along with their sequential arrangement. Through the optimization of a\nscore function via an expectation maximization (EM) algorithm, CASTOR\nalternates between learning the regime indices (Expectation step) and inferring\ncausal relationships in each regime (Maximization step). We further prove the\nidentifiability of regimes and DAGs within the CASTOR framework. We conduct\nextensive experiments and show that our method consistently outperforms causal\ndiscovery models across various settings (linear and nonlinear causal\nrelationships) and datasets (synthetic and real data).\n", "link": "http://arxiv.org/abs/2311.01412v2", "date": "2024-05-27", "relevancy": 1.7608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4418}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.44}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Temporal%20Regime%20Structure%20Learning&body=Title%3A%20Causal%20Temporal%20Regime%20Structure%20Learning%0AAuthor%3A%20Abdellah%20Rahmani%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20structure%20learning%20from%20multivariate%20time%20series%0Athat%20are%20characterized%20by%20a%20sequence%20of%20different%2C%20unknown%20regimes.%20We%0Aintroduce%20a%20new%20optimization-based%20method%20%28CASTOR%29%2C%20that%20concurrently%20learns%0Athe%20Directed%20Acyclic%20Graph%20%28DAG%29%20for%20each%20regime%20and%20determine%20the%20number%20of%0Aregimes%20along%20with%20their%20sequential%20arrangement.%20Through%20the%20optimization%20of%20a%0Ascore%20function%20via%20an%20expectation%20maximization%20%28EM%29%20algorithm%2C%20CASTOR%0Aalternates%20between%20learning%20the%20regime%20indices%20%28Expectation%20step%29%20and%20inferring%0Acausal%20relationships%20in%20each%20regime%20%28Maximization%20step%29.%20We%20further%20prove%20the%0Aidentifiability%20of%20regimes%20and%20DAGs%20within%20the%20CASTOR%20framework.%20We%20conduct%0Aextensive%20experiments%20and%20show%20that%20our%20method%20consistently%20outperforms%20causal%0Adiscovery%20models%20across%20various%20settings%20%28linear%20and%20nonlinear%20causal%0Arelationships%29%20and%20datasets%20%28synthetic%20and%20real%20data%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Temporal%2520Regime%2520Structure%2520Learning%26entry.906535625%3DAbdellah%2520Rahmani%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520structure%2520learning%2520from%2520multivariate%2520time%2520series%250Athat%2520are%2520characterized%2520by%2520a%2520sequence%2520of%2520different%252C%2520unknown%2520regimes.%2520We%250Aintroduce%2520a%2520new%2520optimization-based%2520method%2520%2528CASTOR%2529%252C%2520that%2520concurrently%2520learns%250Athe%2520Directed%2520Acyclic%2520Graph%2520%2528DAG%2529%2520for%2520each%2520regime%2520and%2520determine%2520the%2520number%2520of%250Aregimes%2520along%2520with%2520their%2520sequential%2520arrangement.%2520Through%2520the%2520optimization%2520of%2520a%250Ascore%2520function%2520via%2520an%2520expectation%2520maximization%2520%2528EM%2529%2520algorithm%252C%2520CASTOR%250Aalternates%2520between%2520learning%2520the%2520regime%2520indices%2520%2528Expectation%2520step%2529%2520and%2520inferring%250Acausal%2520relationships%2520in%2520each%2520regime%2520%2528Maximization%2520step%2529.%2520We%2520further%2520prove%2520the%250Aidentifiability%2520of%2520regimes%2520and%2520DAGs%2520within%2520the%2520CASTOR%2520framework.%2520We%2520conduct%250Aextensive%2520experiments%2520and%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520causal%250Adiscovery%2520models%2520across%2520various%2520settings%2520%2528linear%2520and%2520nonlinear%2520causal%250Arelationships%2529%2520and%2520datasets%2520%2528synthetic%2520and%2520real%2520data%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Temporal%20Regime%20Structure%20Learning&entry.906535625=Abdellah%20Rahmani%20and%20Pascal%20Frossard&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20structure%20learning%20from%20multivariate%20time%20series%0Athat%20are%20characterized%20by%20a%20sequence%20of%20different%2C%20unknown%20regimes.%20We%0Aintroduce%20a%20new%20optimization-based%20method%20%28CASTOR%29%2C%20that%20concurrently%20learns%0Athe%20Directed%20Acyclic%20Graph%20%28DAG%29%20for%20each%20regime%20and%20determine%20the%20number%20of%0Aregimes%20along%20with%20their%20sequential%20arrangement.%20Through%20the%20optimization%20of%20a%0Ascore%20function%20via%20an%20expectation%20maximization%20%28EM%29%20algorithm%2C%20CASTOR%0Aalternates%20between%20learning%20the%20regime%20indices%20%28Expectation%20step%29%20and%20inferring%0Acausal%20relationships%20in%20each%20regime%20%28Maximization%20step%29.%20We%20further%20prove%20the%0Aidentifiability%20of%20regimes%20and%20DAGs%20within%20the%20CASTOR%20framework.%20We%20conduct%0Aextensive%20experiments%20and%20show%20that%20our%20method%20consistently%20outperforms%20causal%0Adiscovery%20models%20across%20various%20settings%20%28linear%20and%20nonlinear%20causal%0Arelationships%29%20and%20datasets%20%28synthetic%20and%20real%20data%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01412v2&entry.124074799=Read"},
{"title": "Finding good policies in average-reward Markov Decision Processes\n  without prior knowledge", "author": "Adrienne Tuynman and R\u00e9my Degenne and Emilie Kaufmann", "abstract": "  We revisit the identification of an $\\varepsilon$-optimal policy in\naverage-reward Markov Decision Processes (MDP). In such MDPs, two measures of\ncomplexity have appeared in the literature: the diameter, $D$, and the optimal\nbias span, $H$, which satisfy $H\\leq D$. Prior work have studied the complexity\nof $\\varepsilon$-optimal policy identification only when a generative model is\navailable. In this case, it is known that there exists an MDP with $D \\simeq H$\nfor which the sample complexity to output an $\\varepsilon$-optimal policy is\n$\\Omega(SAD/\\varepsilon^2)$ where $S$ and $A$ are the sizes of the state and\naction spaces. Recently, an algorithm with a sample complexity of order\n$SAH/\\varepsilon^2$ has been proposed, but it requires the knowledge of $H$. We\nfirst show that the sample complexity required to estimate $H$ is not bounded\nby any function of $S,A$ and $H$, ruling out the possibility to easily make the\nprevious algorithm agnostic to $H$. By relying instead on a diameter estimation\nprocedure, we propose the first algorithm for $(\\varepsilon,\\delta)$-PAC policy\nidentification that does not need any form of prior knowledge on the MDP. Its\nsample complexity scales in $SAD/\\varepsilon^2$ in the regime of small\n$\\varepsilon$, which is near-optimal. In the online setting, our first\ncontribution is a lower bound which implies that a sample complexity polynomial\nin $H$ cannot be achieved in this setting. Then, we propose an online algorithm\nwith a sample complexity in $SAD^2/\\varepsilon^2$, as well as a novel approach\nbased on a data-dependent stopping rule that we believe is promising to further\nreduce this bound.\n", "link": "http://arxiv.org/abs/2405.17108v1", "date": "2024-05-27", "relevancy": 1.2595, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4416}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4168}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20good%20policies%20in%20average-reward%20Markov%20Decision%20Processes%0A%20%20without%20prior%20knowledge&body=Title%3A%20Finding%20good%20policies%20in%20average-reward%20Markov%20Decision%20Processes%0A%20%20without%20prior%20knowledge%0AAuthor%3A%20Adrienne%20Tuynman%20and%20R%C3%A9my%20Degenne%20and%20Emilie%20Kaufmann%0AAbstract%3A%20%20%20We%20revisit%20the%20identification%20of%20an%20%24%5Cvarepsilon%24-optimal%20policy%20in%0Aaverage-reward%20Markov%20Decision%20Processes%20%28MDP%29.%20In%20such%20MDPs%2C%20two%20measures%20of%0Acomplexity%20have%20appeared%20in%20the%20literature%3A%20the%20diameter%2C%20%24D%24%2C%20and%20the%20optimal%0Abias%20span%2C%20%24H%24%2C%20which%20satisfy%20%24H%5Cleq%20D%24.%20Prior%20work%20have%20studied%20the%20complexity%0Aof%20%24%5Cvarepsilon%24-optimal%20policy%20identification%20only%20when%20a%20generative%20model%20is%0Aavailable.%20In%20this%20case%2C%20it%20is%20known%20that%20there%20exists%20an%20MDP%20with%20%24D%20%5Csimeq%20H%24%0Afor%20which%20the%20sample%20complexity%20to%20output%20an%20%24%5Cvarepsilon%24-optimal%20policy%20is%0A%24%5COmega%28SAD/%5Cvarepsilon%5E2%29%24%20where%20%24S%24%20and%20%24A%24%20are%20the%20sizes%20of%20the%20state%20and%0Aaction%20spaces.%20Recently%2C%20an%20algorithm%20with%20a%20sample%20complexity%20of%20order%0A%24SAH/%5Cvarepsilon%5E2%24%20has%20been%20proposed%2C%20but%20it%20requires%20the%20knowledge%20of%20%24H%24.%20We%0Afirst%20show%20that%20the%20sample%20complexity%20required%20to%20estimate%20%24H%24%20is%20not%20bounded%0Aby%20any%20function%20of%20%24S%2CA%24%20and%20%24H%24%2C%20ruling%20out%20the%20possibility%20to%20easily%20make%20the%0Aprevious%20algorithm%20agnostic%20to%20%24H%24.%20By%20relying%20instead%20on%20a%20diameter%20estimation%0Aprocedure%2C%20we%20propose%20the%20first%20algorithm%20for%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-PAC%20policy%0Aidentification%20that%20does%20not%20need%20any%20form%20of%20prior%20knowledge%20on%20the%20MDP.%20Its%0Asample%20complexity%20scales%20in%20%24SAD/%5Cvarepsilon%5E2%24%20in%20the%20regime%20of%20small%0A%24%5Cvarepsilon%24%2C%20which%20is%20near-optimal.%20In%20the%20online%20setting%2C%20our%20first%0Acontribution%20is%20a%20lower%20bound%20which%20implies%20that%20a%20sample%20complexity%20polynomial%0Ain%20%24H%24%20cannot%20be%20achieved%20in%20this%20setting.%20Then%2C%20we%20propose%20an%20online%20algorithm%0Awith%20a%20sample%20complexity%20in%20%24SAD%5E2/%5Cvarepsilon%5E2%24%2C%20as%20well%20as%20a%20novel%20approach%0Abased%20on%20a%20data-dependent%20stopping%20rule%20that%20we%20believe%20is%20promising%20to%20further%0Areduce%20this%20bound.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520good%2520policies%2520in%2520average-reward%2520Markov%2520Decision%2520Processes%250A%2520%2520without%2520prior%2520knowledge%26entry.906535625%3DAdrienne%2520Tuynman%2520and%2520R%25C3%25A9my%2520Degenne%2520and%2520Emilie%2520Kaufmann%26entry.1292438233%3D%2520%2520We%2520revisit%2520the%2520identification%2520of%2520an%2520%2524%255Cvarepsilon%2524-optimal%2520policy%2520in%250Aaverage-reward%2520Markov%2520Decision%2520Processes%2520%2528MDP%2529.%2520In%2520such%2520MDPs%252C%2520two%2520measures%2520of%250Acomplexity%2520have%2520appeared%2520in%2520the%2520literature%253A%2520the%2520diameter%252C%2520%2524D%2524%252C%2520and%2520the%2520optimal%250Abias%2520span%252C%2520%2524H%2524%252C%2520which%2520satisfy%2520%2524H%255Cleq%2520D%2524.%2520Prior%2520work%2520have%2520studied%2520the%2520complexity%250Aof%2520%2524%255Cvarepsilon%2524-optimal%2520policy%2520identification%2520only%2520when%2520a%2520generative%2520model%2520is%250Aavailable.%2520In%2520this%2520case%252C%2520it%2520is%2520known%2520that%2520there%2520exists%2520an%2520MDP%2520with%2520%2524D%2520%255Csimeq%2520H%2524%250Afor%2520which%2520the%2520sample%2520complexity%2520to%2520output%2520an%2520%2524%255Cvarepsilon%2524-optimal%2520policy%2520is%250A%2524%255COmega%2528SAD/%255Cvarepsilon%255E2%2529%2524%2520where%2520%2524S%2524%2520and%2520%2524A%2524%2520are%2520the%2520sizes%2520of%2520the%2520state%2520and%250Aaction%2520spaces.%2520Recently%252C%2520an%2520algorithm%2520with%2520a%2520sample%2520complexity%2520of%2520order%250A%2524SAH/%255Cvarepsilon%255E2%2524%2520has%2520been%2520proposed%252C%2520but%2520it%2520requires%2520the%2520knowledge%2520of%2520%2524H%2524.%2520We%250Afirst%2520show%2520that%2520the%2520sample%2520complexity%2520required%2520to%2520estimate%2520%2524H%2524%2520is%2520not%2520bounded%250Aby%2520any%2520function%2520of%2520%2524S%252CA%2524%2520and%2520%2524H%2524%252C%2520ruling%2520out%2520the%2520possibility%2520to%2520easily%2520make%2520the%250Aprevious%2520algorithm%2520agnostic%2520to%2520%2524H%2524.%2520By%2520relying%2520instead%2520on%2520a%2520diameter%2520estimation%250Aprocedure%252C%2520we%2520propose%2520the%2520first%2520algorithm%2520for%2520%2524%2528%255Cvarepsilon%252C%255Cdelta%2529%2524-PAC%2520policy%250Aidentification%2520that%2520does%2520not%2520need%2520any%2520form%2520of%2520prior%2520knowledge%2520on%2520the%2520MDP.%2520Its%250Asample%2520complexity%2520scales%2520in%2520%2524SAD/%255Cvarepsilon%255E2%2524%2520in%2520the%2520regime%2520of%2520small%250A%2524%255Cvarepsilon%2524%252C%2520which%2520is%2520near-optimal.%2520In%2520the%2520online%2520setting%252C%2520our%2520first%250Acontribution%2520is%2520a%2520lower%2520bound%2520which%2520implies%2520that%2520a%2520sample%2520complexity%2520polynomial%250Ain%2520%2524H%2524%2520cannot%2520be%2520achieved%2520in%2520this%2520setting.%2520Then%252C%2520we%2520propose%2520an%2520online%2520algorithm%250Awith%2520a%2520sample%2520complexity%2520in%2520%2524SAD%255E2/%255Cvarepsilon%255E2%2524%252C%2520as%2520well%2520as%2520a%2520novel%2520approach%250Abased%2520on%2520a%2520data-dependent%2520stopping%2520rule%2520that%2520we%2520believe%2520is%2520promising%2520to%2520further%250Areduce%2520this%2520bound.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20good%20policies%20in%20average-reward%20Markov%20Decision%20Processes%0A%20%20without%20prior%20knowledge&entry.906535625=Adrienne%20Tuynman%20and%20R%C3%A9my%20Degenne%20and%20Emilie%20Kaufmann&entry.1292438233=%20%20We%20revisit%20the%20identification%20of%20an%20%24%5Cvarepsilon%24-optimal%20policy%20in%0Aaverage-reward%20Markov%20Decision%20Processes%20%28MDP%29.%20In%20such%20MDPs%2C%20two%20measures%20of%0Acomplexity%20have%20appeared%20in%20the%20literature%3A%20the%20diameter%2C%20%24D%24%2C%20and%20the%20optimal%0Abias%20span%2C%20%24H%24%2C%20which%20satisfy%20%24H%5Cleq%20D%24.%20Prior%20work%20have%20studied%20the%20complexity%0Aof%20%24%5Cvarepsilon%24-optimal%20policy%20identification%20only%20when%20a%20generative%20model%20is%0Aavailable.%20In%20this%20case%2C%20it%20is%20known%20that%20there%20exists%20an%20MDP%20with%20%24D%20%5Csimeq%20H%24%0Afor%20which%20the%20sample%20complexity%20to%20output%20an%20%24%5Cvarepsilon%24-optimal%20policy%20is%0A%24%5COmega%28SAD/%5Cvarepsilon%5E2%29%24%20where%20%24S%24%20and%20%24A%24%20are%20the%20sizes%20of%20the%20state%20and%0Aaction%20spaces.%20Recently%2C%20an%20algorithm%20with%20a%20sample%20complexity%20of%20order%0A%24SAH/%5Cvarepsilon%5E2%24%20has%20been%20proposed%2C%20but%20it%20requires%20the%20knowledge%20of%20%24H%24.%20We%0Afirst%20show%20that%20the%20sample%20complexity%20required%20to%20estimate%20%24H%24%20is%20not%20bounded%0Aby%20any%20function%20of%20%24S%2CA%24%20and%20%24H%24%2C%20ruling%20out%20the%20possibility%20to%20easily%20make%20the%0Aprevious%20algorithm%20agnostic%20to%20%24H%24.%20By%20relying%20instead%20on%20a%20diameter%20estimation%0Aprocedure%2C%20we%20propose%20the%20first%20algorithm%20for%20%24%28%5Cvarepsilon%2C%5Cdelta%29%24-PAC%20policy%0Aidentification%20that%20does%20not%20need%20any%20form%20of%20prior%20knowledge%20on%20the%20MDP.%20Its%0Asample%20complexity%20scales%20in%20%24SAD/%5Cvarepsilon%5E2%24%20in%20the%20regime%20of%20small%0A%24%5Cvarepsilon%24%2C%20which%20is%20near-optimal.%20In%20the%20online%20setting%2C%20our%20first%0Acontribution%20is%20a%20lower%20bound%20which%20implies%20that%20a%20sample%20complexity%20polynomial%0Ain%20%24H%24%20cannot%20be%20achieved%20in%20this%20setting.%20Then%2C%20we%20propose%20an%20online%20algorithm%0Awith%20a%20sample%20complexity%20in%20%24SAD%5E2/%5Cvarepsilon%5E2%24%2C%20as%20well%20as%20a%20novel%20approach%0Abased%20on%20a%20data-dependent%20stopping%20rule%20that%20we%20believe%20is%20promising%20to%20further%0Areduce%20this%20bound.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17108v1&entry.124074799=Read"},
{"title": "Universal Adversarial Defense in Remote Sensing Based on Pre-trained\n  Denoising Diffusion Models", "author": "Weikang Yu and Yonghao Xu and Pedram Ghamisi", "abstract": "  Deep neural networks (DNNs) have risen to prominence as key solutions in\nnumerous AI applications for earth observation (AI4EO). However, their\nsusceptibility to adversarial examples poses a critical challenge, compromising\nthe reliability of AI4EO algorithms. This paper presents a novel Universal\nAdversarial Defense approach in Remote Sensing Imagery (UAD-RS), leveraging\npre-trained diffusion models to protect DNNs against universal adversarial\nexamples exhibiting heterogeneous patterns. Specifically, a universal\nadversarial purification framework is developed utilizing pre-trained diffusion\nmodels to mitigate adversarial perturbations through the introduction of\nGaussian noise and subsequent purification of the perturbations from\nadversarial examples. Additionally, an Adaptive Noise Level Selection (ANLS)\nmechanism is introduced to determine the optimal noise level for the\npurification framework with a task-guided Frechet Inception Distance (FID)\nranking strategy, thereby enhancing purification performance. Consequently,\nonly a single pre-trained diffusion model is required for purifying universal\nadversarial samples with heterogeneous patterns across each dataset,\nsignificantly reducing training efforts for multiple attack settings while\nmaintaining high performance without prior knowledge of adversarial\nperturbations. Experimental results on four heterogeneous RS datasets, focusing\non scene classification and semantic segmentation, demonstrate that UAD-RS\noutperforms state-of-the-art adversarial purification approaches, providing\nuniversal defense against seven commonly encountered adversarial perturbations.\nCodes and the pre-trained models are available online\n(https://github.com/EricYu97/UAD-RS).\n", "link": "http://arxiv.org/abs/2307.16865v3", "date": "2024-05-27", "relevancy": 1.6831, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Adversarial%20Defense%20in%20Remote%20Sensing%20Based%20on%20Pre-trained%0A%20%20Denoising%20Diffusion%20Models&body=Title%3A%20Universal%20Adversarial%20Defense%20in%20Remote%20Sensing%20Based%20on%20Pre-trained%0A%20%20Denoising%20Diffusion%20Models%0AAuthor%3A%20Weikang%20Yu%20and%20Yonghao%20Xu%20and%20Pedram%20Ghamisi%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20risen%20to%20prominence%20as%20key%20solutions%20in%0Anumerous%20AI%20applications%20for%20earth%20observation%20%28AI4EO%29.%20However%2C%20their%0Asusceptibility%20to%20adversarial%20examples%20poses%20a%20critical%20challenge%2C%20compromising%0Athe%20reliability%20of%20AI4EO%20algorithms.%20This%20paper%20presents%20a%20novel%20Universal%0AAdversarial%20Defense%20approach%20in%20Remote%20Sensing%20Imagery%20%28UAD-RS%29%2C%20leveraging%0Apre-trained%20diffusion%20models%20to%20protect%20DNNs%20against%20universal%20adversarial%0Aexamples%20exhibiting%20heterogeneous%20patterns.%20Specifically%2C%20a%20universal%0Aadversarial%20purification%20framework%20is%20developed%20utilizing%20pre-trained%20diffusion%0Amodels%20to%20mitigate%20adversarial%20perturbations%20through%20the%20introduction%20of%0AGaussian%20noise%20and%20subsequent%20purification%20of%20the%20perturbations%20from%0Aadversarial%20examples.%20Additionally%2C%20an%20Adaptive%20Noise%20Level%20Selection%20%28ANLS%29%0Amechanism%20is%20introduced%20to%20determine%20the%20optimal%20noise%20level%20for%20the%0Apurification%20framework%20with%20a%20task-guided%20Frechet%20Inception%20Distance%20%28FID%29%0Aranking%20strategy%2C%20thereby%20enhancing%20purification%20performance.%20Consequently%2C%0Aonly%20a%20single%20pre-trained%20diffusion%20model%20is%20required%20for%20purifying%20universal%0Aadversarial%20samples%20with%20heterogeneous%20patterns%20across%20each%20dataset%2C%0Asignificantly%20reducing%20training%20efforts%20for%20multiple%20attack%20settings%20while%0Amaintaining%20high%20performance%20without%20prior%20knowledge%20of%20adversarial%0Aperturbations.%20Experimental%20results%20on%20four%20heterogeneous%20RS%20datasets%2C%20focusing%0Aon%20scene%20classification%20and%20semantic%20segmentation%2C%20demonstrate%20that%20UAD-RS%0Aoutperforms%20state-of-the-art%20adversarial%20purification%20approaches%2C%20providing%0Auniversal%20defense%20against%20seven%20commonly%20encountered%20adversarial%20perturbations.%0ACodes%20and%20the%20pre-trained%20models%20are%20available%20online%0A%28https%3A//github.com/EricYu97/UAD-RS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Adversarial%2520Defense%2520in%2520Remote%2520Sensing%2520Based%2520on%2520Pre-trained%250A%2520%2520Denoising%2520Diffusion%2520Models%26entry.906535625%3DWeikang%2520Yu%2520and%2520Yonghao%2520Xu%2520and%2520Pedram%2520Ghamisi%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520risen%2520to%2520prominence%2520as%2520key%2520solutions%2520in%250Anumerous%2520AI%2520applications%2520for%2520earth%2520observation%2520%2528AI4EO%2529.%2520However%252C%2520their%250Asusceptibility%2520to%2520adversarial%2520examples%2520poses%2520a%2520critical%2520challenge%252C%2520compromising%250Athe%2520reliability%2520of%2520AI4EO%2520algorithms.%2520This%2520paper%2520presents%2520a%2520novel%2520Universal%250AAdversarial%2520Defense%2520approach%2520in%2520Remote%2520Sensing%2520Imagery%2520%2528UAD-RS%2529%252C%2520leveraging%250Apre-trained%2520diffusion%2520models%2520to%2520protect%2520DNNs%2520against%2520universal%2520adversarial%250Aexamples%2520exhibiting%2520heterogeneous%2520patterns.%2520Specifically%252C%2520a%2520universal%250Aadversarial%2520purification%2520framework%2520is%2520developed%2520utilizing%2520pre-trained%2520diffusion%250Amodels%2520to%2520mitigate%2520adversarial%2520perturbations%2520through%2520the%2520introduction%2520of%250AGaussian%2520noise%2520and%2520subsequent%2520purification%2520of%2520the%2520perturbations%2520from%250Aadversarial%2520examples.%2520Additionally%252C%2520an%2520Adaptive%2520Noise%2520Level%2520Selection%2520%2528ANLS%2529%250Amechanism%2520is%2520introduced%2520to%2520determine%2520the%2520optimal%2520noise%2520level%2520for%2520the%250Apurification%2520framework%2520with%2520a%2520task-guided%2520Frechet%2520Inception%2520Distance%2520%2528FID%2529%250Aranking%2520strategy%252C%2520thereby%2520enhancing%2520purification%2520performance.%2520Consequently%252C%250Aonly%2520a%2520single%2520pre-trained%2520diffusion%2520model%2520is%2520required%2520for%2520purifying%2520universal%250Aadversarial%2520samples%2520with%2520heterogeneous%2520patterns%2520across%2520each%2520dataset%252C%250Asignificantly%2520reducing%2520training%2520efforts%2520for%2520multiple%2520attack%2520settings%2520while%250Amaintaining%2520high%2520performance%2520without%2520prior%2520knowledge%2520of%2520adversarial%250Aperturbations.%2520Experimental%2520results%2520on%2520four%2520heterogeneous%2520RS%2520datasets%252C%2520focusing%250Aon%2520scene%2520classification%2520and%2520semantic%2520segmentation%252C%2520demonstrate%2520that%2520UAD-RS%250Aoutperforms%2520state-of-the-art%2520adversarial%2520purification%2520approaches%252C%2520providing%250Auniversal%2520defense%2520against%2520seven%2520commonly%2520encountered%2520adversarial%2520perturbations.%250ACodes%2520and%2520the%2520pre-trained%2520models%2520are%2520available%2520online%250A%2528https%253A//github.com/EricYu97/UAD-RS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Adversarial%20Defense%20in%20Remote%20Sensing%20Based%20on%20Pre-trained%0A%20%20Denoising%20Diffusion%20Models&entry.906535625=Weikang%20Yu%20and%20Yonghao%20Xu%20and%20Pedram%20Ghamisi&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20risen%20to%20prominence%20as%20key%20solutions%20in%0Anumerous%20AI%20applications%20for%20earth%20observation%20%28AI4EO%29.%20However%2C%20their%0Asusceptibility%20to%20adversarial%20examples%20poses%20a%20critical%20challenge%2C%20compromising%0Athe%20reliability%20of%20AI4EO%20algorithms.%20This%20paper%20presents%20a%20novel%20Universal%0AAdversarial%20Defense%20approach%20in%20Remote%20Sensing%20Imagery%20%28UAD-RS%29%2C%20leveraging%0Apre-trained%20diffusion%20models%20to%20protect%20DNNs%20against%20universal%20adversarial%0Aexamples%20exhibiting%20heterogeneous%20patterns.%20Specifically%2C%20a%20universal%0Aadversarial%20purification%20framework%20is%20developed%20utilizing%20pre-trained%20diffusion%0Amodels%20to%20mitigate%20adversarial%20perturbations%20through%20the%20introduction%20of%0AGaussian%20noise%20and%20subsequent%20purification%20of%20the%20perturbations%20from%0Aadversarial%20examples.%20Additionally%2C%20an%20Adaptive%20Noise%20Level%20Selection%20%28ANLS%29%0Amechanism%20is%20introduced%20to%20determine%20the%20optimal%20noise%20level%20for%20the%0Apurification%20framework%20with%20a%20task-guided%20Frechet%20Inception%20Distance%20%28FID%29%0Aranking%20strategy%2C%20thereby%20enhancing%20purification%20performance.%20Consequently%2C%0Aonly%20a%20single%20pre-trained%20diffusion%20model%20is%20required%20for%20purifying%20universal%0Aadversarial%20samples%20with%20heterogeneous%20patterns%20across%20each%20dataset%2C%0Asignificantly%20reducing%20training%20efforts%20for%20multiple%20attack%20settings%20while%0Amaintaining%20high%20performance%20without%20prior%20knowledge%20of%20adversarial%0Aperturbations.%20Experimental%20results%20on%20four%20heterogeneous%20RS%20datasets%2C%20focusing%0Aon%20scene%20classification%20and%20semantic%20segmentation%2C%20demonstrate%20that%20UAD-RS%0Aoutperforms%20state-of-the-art%20adversarial%20purification%20approaches%2C%20providing%0Auniversal%20defense%20against%20seven%20commonly%20encountered%20adversarial%20perturbations.%0ACodes%20and%20the%20pre-trained%20models%20are%20available%20online%0A%28https%3A//github.com/EricYu97/UAD-RS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16865v3&entry.124074799=Read"},
{"title": "Coordinating robotized construction using advanced robotic simulation:\n  The case of collaborative brick wall assembly", "author": "Mohammad Reza Kolani and Stavros Nousias and Andr\u00e9 Borrmann", "abstract": "  Utilizing robotic systems in the construction industry is gaining popularity\ndue to their build time, precision, and efficiency. In this paper, we introduce\na system that allows the coordination of multiple manipulator robots for\nconstruction activities. As a case study, we chose robotic brick wall assembly.\nBy utilizing a multi robot system where arm manipulators collaborate with each\nother, the entirety of a potentially long wall can be assembled simultaneously.\nHowever, the reduction of overall bricklaying time is dependent on the\nminimization of time required for each individual manipulator. In this paper,\nwe execute the simulation with various placements of material and the robots\nbase, as well as different robot configurations, to determine the optimal\nposition of the robot and material and the best configuration for the robot.\nThe simulation results provide users with insights into how to find the best\nplacement of robots and raw materials for brick wall assembly.\n", "link": "http://arxiv.org/abs/2405.17171v1", "date": "2024-05-27", "relevancy": 1.4415, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinating%20robotized%20construction%20using%20advanced%20robotic%20simulation%3A%0A%20%20The%20case%20of%20collaborative%20brick%20wall%20assembly&body=Title%3A%20Coordinating%20robotized%20construction%20using%20advanced%20robotic%20simulation%3A%0A%20%20The%20case%20of%20collaborative%20brick%20wall%20assembly%0AAuthor%3A%20Mohammad%20Reza%20Kolani%20and%20Stavros%20Nousias%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20Utilizing%20robotic%20systems%20in%20the%20construction%20industry%20is%20gaining%20popularity%0Adue%20to%20their%20build%20time%2C%20precision%2C%20and%20efficiency.%20In%20this%20paper%2C%20we%20introduce%0Aa%20system%20that%20allows%20the%20coordination%20of%20multiple%20manipulator%20robots%20for%0Aconstruction%20activities.%20As%20a%20case%20study%2C%20we%20chose%20robotic%20brick%20wall%20assembly.%0ABy%20utilizing%20a%20multi%20robot%20system%20where%20arm%20manipulators%20collaborate%20with%20each%0Aother%2C%20the%20entirety%20of%20a%20potentially%20long%20wall%20can%20be%20assembled%20simultaneously.%0AHowever%2C%20the%20reduction%20of%20overall%20bricklaying%20time%20is%20dependent%20on%20the%0Aminimization%20of%20time%20required%20for%20each%20individual%20manipulator.%20In%20this%20paper%2C%0Awe%20execute%20the%20simulation%20with%20various%20placements%20of%20material%20and%20the%20robots%0Abase%2C%20as%20well%20as%20different%20robot%20configurations%2C%20to%20determine%20the%20optimal%0Aposition%20of%20the%20robot%20and%20material%20and%20the%20best%20configuration%20for%20the%20robot.%0AThe%20simulation%20results%20provide%20users%20with%20insights%20into%20how%20to%20find%20the%20best%0Aplacement%20of%20robots%20and%20raw%20materials%20for%20brick%20wall%20assembly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinating%2520robotized%2520construction%2520using%2520advanced%2520robotic%2520simulation%253A%250A%2520%2520The%2520case%2520of%2520collaborative%2520brick%2520wall%2520assembly%26entry.906535625%3DMohammad%2520Reza%2520Kolani%2520and%2520Stavros%2520Nousias%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520Utilizing%2520robotic%2520systems%2520in%2520the%2520construction%2520industry%2520is%2520gaining%2520popularity%250Adue%2520to%2520their%2520build%2520time%252C%2520precision%252C%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Aa%2520system%2520that%2520allows%2520the%2520coordination%2520of%2520multiple%2520manipulator%2520robots%2520for%250Aconstruction%2520activities.%2520As%2520a%2520case%2520study%252C%2520we%2520chose%2520robotic%2520brick%2520wall%2520assembly.%250ABy%2520utilizing%2520a%2520multi%2520robot%2520system%2520where%2520arm%2520manipulators%2520collaborate%2520with%2520each%250Aother%252C%2520the%2520entirety%2520of%2520a%2520potentially%2520long%2520wall%2520can%2520be%2520assembled%2520simultaneously.%250AHowever%252C%2520the%2520reduction%2520of%2520overall%2520bricklaying%2520time%2520is%2520dependent%2520on%2520the%250Aminimization%2520of%2520time%2520required%2520for%2520each%2520individual%2520manipulator.%2520In%2520this%2520paper%252C%250Awe%2520execute%2520the%2520simulation%2520with%2520various%2520placements%2520of%2520material%2520and%2520the%2520robots%250Abase%252C%2520as%2520well%2520as%2520different%2520robot%2520configurations%252C%2520to%2520determine%2520the%2520optimal%250Aposition%2520of%2520the%2520robot%2520and%2520material%2520and%2520the%2520best%2520configuration%2520for%2520the%2520robot.%250AThe%2520simulation%2520results%2520provide%2520users%2520with%2520insights%2520into%2520how%2520to%2520find%2520the%2520best%250Aplacement%2520of%2520robots%2520and%2520raw%2520materials%2520for%2520brick%2520wall%2520assembly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinating%20robotized%20construction%20using%20advanced%20robotic%20simulation%3A%0A%20%20The%20case%20of%20collaborative%20brick%20wall%20assembly&entry.906535625=Mohammad%20Reza%20Kolani%20and%20Stavros%20Nousias%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20Utilizing%20robotic%20systems%20in%20the%20construction%20industry%20is%20gaining%20popularity%0Adue%20to%20their%20build%20time%2C%20precision%2C%20and%20efficiency.%20In%20this%20paper%2C%20we%20introduce%0Aa%20system%20that%20allows%20the%20coordination%20of%20multiple%20manipulator%20robots%20for%0Aconstruction%20activities.%20As%20a%20case%20study%2C%20we%20chose%20robotic%20brick%20wall%20assembly.%0ABy%20utilizing%20a%20multi%20robot%20system%20where%20arm%20manipulators%20collaborate%20with%20each%0Aother%2C%20the%20entirety%20of%20a%20potentially%20long%20wall%20can%20be%20assembled%20simultaneously.%0AHowever%2C%20the%20reduction%20of%20overall%20bricklaying%20time%20is%20dependent%20on%20the%0Aminimization%20of%20time%20required%20for%20each%20individual%20manipulator.%20In%20this%20paper%2C%0Awe%20execute%20the%20simulation%20with%20various%20placements%20of%20material%20and%20the%20robots%0Abase%2C%20as%20well%20as%20different%20robot%20configurations%2C%20to%20determine%20the%20optimal%0Aposition%20of%20the%20robot%20and%20material%20and%20the%20best%20configuration%20for%20the%20robot.%0AThe%20simulation%20results%20provide%20users%20with%20insights%20into%20how%20to%20find%20the%20best%0Aplacement%20of%20robots%20and%20raw%20materials%20for%20brick%20wall%20assembly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17171v1&entry.124074799=Read"},
{"title": "CoSLight: Co-optimizing Collaborator Selection and Decision-making to\n  Enhance Traffic Signal Control", "author": "Jingqing Ruan and Ziyue Li and Hua Wei and Haoyuan Jiang and Jiaming Lu and Xuantang Xiong and Hangyu Mao and Rui Zhao", "abstract": "  Effective multi-intersection collaboration is pivotal for\nreinforcement-learning-based traffic signal control to alleviate congestion.\nExisting work mainly chooses neighboring intersections as collaborators.\nHowever, quite an amount of congestion, even some wide-range congestion, is\ncaused by non-neighbors failing to collaborate. To address these issues, we\npropose to separate the collaborator selection as a second policy to be\nlearned, concurrently being updated with the original signal-controlling\npolicy. Specifically, the selection policy in real-time adaptively selects the\nbest teammates according to phase- and intersection-level features. Empirical\nresults on both synthetic and real-world datasets provide robust validation for\nthe superiority of our approach, offering significant improvements over\nexisting state-of-the-art methods. The code is available at\nhttps://github.com/AnonymousAccountss/CoSLight.\n", "link": "http://arxiv.org/abs/2405.17152v1", "date": "2024-05-27", "relevancy": 1.822, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4807}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoSLight%3A%20Co-optimizing%20Collaborator%20Selection%20and%20Decision-making%20to%0A%20%20Enhance%20Traffic%20Signal%20Control&body=Title%3A%20CoSLight%3A%20Co-optimizing%20Collaborator%20Selection%20and%20Decision-making%20to%0A%20%20Enhance%20Traffic%20Signal%20Control%0AAuthor%3A%20Jingqing%20Ruan%20and%20Ziyue%20Li%20and%20Hua%20Wei%20and%20Haoyuan%20Jiang%20and%20Jiaming%20Lu%20and%20Xuantang%20Xiong%20and%20Hangyu%20Mao%20and%20Rui%20Zhao%0AAbstract%3A%20%20%20Effective%20multi-intersection%20collaboration%20is%20pivotal%20for%0Areinforcement-learning-based%20traffic%20signal%20control%20to%20alleviate%20congestion.%0AExisting%20work%20mainly%20chooses%20neighboring%20intersections%20as%20collaborators.%0AHowever%2C%20quite%20an%20amount%20of%20congestion%2C%20even%20some%20wide-range%20congestion%2C%20is%0Acaused%20by%20non-neighbors%20failing%20to%20collaborate.%20To%20address%20these%20issues%2C%20we%0Apropose%20to%20separate%20the%20collaborator%20selection%20as%20a%20second%20policy%20to%20be%0Alearned%2C%20concurrently%20being%20updated%20with%20the%20original%20signal-controlling%0Apolicy.%20Specifically%2C%20the%20selection%20policy%20in%20real-time%20adaptively%20selects%20the%0Abest%20teammates%20according%20to%20phase-%20and%20intersection-level%20features.%20Empirical%0Aresults%20on%20both%20synthetic%20and%20real-world%20datasets%20provide%20robust%20validation%20for%0Athe%20superiority%20of%20our%20approach%2C%20offering%20significant%20improvements%20over%0Aexisting%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AnonymousAccountss/CoSLight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoSLight%253A%2520Co-optimizing%2520Collaborator%2520Selection%2520and%2520Decision-making%2520to%250A%2520%2520Enhance%2520Traffic%2520Signal%2520Control%26entry.906535625%3DJingqing%2520Ruan%2520and%2520Ziyue%2520Li%2520and%2520Hua%2520Wei%2520and%2520Haoyuan%2520Jiang%2520and%2520Jiaming%2520Lu%2520and%2520Xuantang%2520Xiong%2520and%2520Hangyu%2520Mao%2520and%2520Rui%2520Zhao%26entry.1292438233%3D%2520%2520Effective%2520multi-intersection%2520collaboration%2520is%2520pivotal%2520for%250Areinforcement-learning-based%2520traffic%2520signal%2520control%2520to%2520alleviate%2520congestion.%250AExisting%2520work%2520mainly%2520chooses%2520neighboring%2520intersections%2520as%2520collaborators.%250AHowever%252C%2520quite%2520an%2520amount%2520of%2520congestion%252C%2520even%2520some%2520wide-range%2520congestion%252C%2520is%250Acaused%2520by%2520non-neighbors%2520failing%2520to%2520collaborate.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520to%2520separate%2520the%2520collaborator%2520selection%2520as%2520a%2520second%2520policy%2520to%2520be%250Alearned%252C%2520concurrently%2520being%2520updated%2520with%2520the%2520original%2520signal-controlling%250Apolicy.%2520Specifically%252C%2520the%2520selection%2520policy%2520in%2520real-time%2520adaptively%2520selects%2520the%250Abest%2520teammates%2520according%2520to%2520phase-%2520and%2520intersection-level%2520features.%2520Empirical%250Aresults%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520provide%2520robust%2520validation%2520for%250Athe%2520superiority%2520of%2520our%2520approach%252C%2520offering%2520significant%2520improvements%2520over%250Aexisting%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AnonymousAccountss/CoSLight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoSLight%3A%20Co-optimizing%20Collaborator%20Selection%20and%20Decision-making%20to%0A%20%20Enhance%20Traffic%20Signal%20Control&entry.906535625=Jingqing%20Ruan%20and%20Ziyue%20Li%20and%20Hua%20Wei%20and%20Haoyuan%20Jiang%20and%20Jiaming%20Lu%20and%20Xuantang%20Xiong%20and%20Hangyu%20Mao%20and%20Rui%20Zhao&entry.1292438233=%20%20Effective%20multi-intersection%20collaboration%20is%20pivotal%20for%0Areinforcement-learning-based%20traffic%20signal%20control%20to%20alleviate%20congestion.%0AExisting%20work%20mainly%20chooses%20neighboring%20intersections%20as%20collaborators.%0AHowever%2C%20quite%20an%20amount%20of%20congestion%2C%20even%20some%20wide-range%20congestion%2C%20is%0Acaused%20by%20non-neighbors%20failing%20to%20collaborate.%20To%20address%20these%20issues%2C%20we%0Apropose%20to%20separate%20the%20collaborator%20selection%20as%20a%20second%20policy%20to%20be%0Alearned%2C%20concurrently%20being%20updated%20with%20the%20original%20signal-controlling%0Apolicy.%20Specifically%2C%20the%20selection%20policy%20in%20real-time%20adaptively%20selects%20the%0Abest%20teammates%20according%20to%20phase-%20and%20intersection-level%20features.%20Empirical%0Aresults%20on%20both%20synthetic%20and%20real-world%20datasets%20provide%20robust%20validation%20for%0Athe%20superiority%20of%20our%20approach%2C%20offering%20significant%20improvements%20over%0Aexisting%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AnonymousAccountss/CoSLight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17152v1&entry.124074799=Read"},
{"title": "Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler\n  Solutions", "author": "Nagabhushan Somraj and Sai Harsha Mupparaju and Adithyan Karanayil and Rajiv Soundararajan", "abstract": "  Neural Radiance Fields (NeRF) show impressive performance in photo-realistic\nfree-view rendering of scenes. Recent improvements on the NeRF such as TensoRF\nand ZipNeRF employ explicit models for faster optimization and rendering, as\ncompared to the NeRF that employs an implicit representation. However, both\nimplicit and explicit radiance fields require dense sampling of images in the\ngiven scene. Their performance degrades significantly when only a sparse set of\nviews is available. Researchers find that supervising the depth estimated by a\nradiance field helps train it effectively with fewer views. The depth\nsupervision is obtained either using classical approaches or neural networks\npre-trained on a large dataset. While the former may provide only sparse\nsupervision, the latter may suffer from generalization issues. As opposed to\nthe earlier approaches, we seek to learn the depth supervision by designing\naugmented models and training them along with the main radiance field. Further,\nwe aim to design a framework of regularizations that can work across different\nimplicit and explicit radiance fields. We observe that certain features of\nthese radiance field models overfit to the observed images in the sparse-input\nscenario. Our key finding is that reducing the capability of the radiance\nfields with respect to positional encoding, the number of decomposed tensor\ncomponents or the size of the hash table, constrains the model to learn simpler\nsolutions, which estimate better depth in certain regions. By designing\naugmented models based on such reduced capabilities, we obtain better depth\nsupervision for the main radiance field. We achieve state-of-the-art\nview-synthesis performance with sparse input views on popular datasets\ncontaining forward-facing and 360$^\\circ$ scenes by employing the above\nregularizations.\n", "link": "http://arxiv.org/abs/2404.19015v3", "date": "2024-05-27", "relevancy": 1.6935, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5695}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5686}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple-RF%3A%20Regularizing%20Sparse%20Input%20Radiance%20Fields%20with%20Simpler%0A%20%20Solutions&body=Title%3A%20Simple-RF%3A%20Regularizing%20Sparse%20Input%20Radiance%20Fields%20with%20Simpler%0A%20%20Solutions%0AAuthor%3A%20Nagabhushan%20Somraj%20and%20Sai%20Harsha%20Mupparaju%20and%20Adithyan%20Karanayil%20and%20Rajiv%20Soundararajan%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20show%20impressive%20performance%20in%20photo-realistic%0Afree-view%20rendering%20of%20scenes.%20Recent%20improvements%20on%20the%20NeRF%20such%20as%20TensoRF%0Aand%20ZipNeRF%20employ%20explicit%20models%20for%20faster%20optimization%20and%20rendering%2C%20as%0Acompared%20to%20the%20NeRF%20that%20employs%20an%20implicit%20representation.%20However%2C%20both%0Aimplicit%20and%20explicit%20radiance%20fields%20require%20dense%20sampling%20of%20images%20in%20the%0Agiven%20scene.%20Their%20performance%20degrades%20significantly%20when%20only%20a%20sparse%20set%20of%0Aviews%20is%20available.%20Researchers%20find%20that%20supervising%20the%20depth%20estimated%20by%20a%0Aradiance%20field%20helps%20train%20it%20effectively%20with%20fewer%20views.%20The%20depth%0Asupervision%20is%20obtained%20either%20using%20classical%20approaches%20or%20neural%20networks%0Apre-trained%20on%20a%20large%20dataset.%20While%20the%20former%20may%20provide%20only%20sparse%0Asupervision%2C%20the%20latter%20may%20suffer%20from%20generalization%20issues.%20As%20opposed%20to%0Athe%20earlier%20approaches%2C%20we%20seek%20to%20learn%20the%20depth%20supervision%20by%20designing%0Aaugmented%20models%20and%20training%20them%20along%20with%20the%20main%20radiance%20field.%20Further%2C%0Awe%20aim%20to%20design%20a%20framework%20of%20regularizations%20that%20can%20work%20across%20different%0Aimplicit%20and%20explicit%20radiance%20fields.%20We%20observe%20that%20certain%20features%20of%0Athese%20radiance%20field%20models%20overfit%20to%20the%20observed%20images%20in%20the%20sparse-input%0Ascenario.%20Our%20key%20finding%20is%20that%20reducing%20the%20capability%20of%20the%20radiance%0Afields%20with%20respect%20to%20positional%20encoding%2C%20the%20number%20of%20decomposed%20tensor%0Acomponents%20or%20the%20size%20of%20the%20hash%20table%2C%20constrains%20the%20model%20to%20learn%20simpler%0Asolutions%2C%20which%20estimate%20better%20depth%20in%20certain%20regions.%20By%20designing%0Aaugmented%20models%20based%20on%20such%20reduced%20capabilities%2C%20we%20obtain%20better%20depth%0Asupervision%20for%20the%20main%20radiance%20field.%20We%20achieve%20state-of-the-art%0Aview-synthesis%20performance%20with%20sparse%20input%20views%20on%20popular%20datasets%0Acontaining%20forward-facing%20and%20360%24%5E%5Ccirc%24%20scenes%20by%20employing%20the%20above%0Aregularizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19015v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple-RF%253A%2520Regularizing%2520Sparse%2520Input%2520Radiance%2520Fields%2520with%2520Simpler%250A%2520%2520Solutions%26entry.906535625%3DNagabhushan%2520Somraj%2520and%2520Sai%2520Harsha%2520Mupparaju%2520and%2520Adithyan%2520Karanayil%2520and%2520Rajiv%2520Soundararajan%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520show%2520impressive%2520performance%2520in%2520photo-realistic%250Afree-view%2520rendering%2520of%2520scenes.%2520Recent%2520improvements%2520on%2520the%2520NeRF%2520such%2520as%2520TensoRF%250Aand%2520ZipNeRF%2520employ%2520explicit%2520models%2520for%2520faster%2520optimization%2520and%2520rendering%252C%2520as%250Acompared%2520to%2520the%2520NeRF%2520that%2520employs%2520an%2520implicit%2520representation.%2520However%252C%2520both%250Aimplicit%2520and%2520explicit%2520radiance%2520fields%2520require%2520dense%2520sampling%2520of%2520images%2520in%2520the%250Agiven%2520scene.%2520Their%2520performance%2520degrades%2520significantly%2520when%2520only%2520a%2520sparse%2520set%2520of%250Aviews%2520is%2520available.%2520Researchers%2520find%2520that%2520supervising%2520the%2520depth%2520estimated%2520by%2520a%250Aradiance%2520field%2520helps%2520train%2520it%2520effectively%2520with%2520fewer%2520views.%2520The%2520depth%250Asupervision%2520is%2520obtained%2520either%2520using%2520classical%2520approaches%2520or%2520neural%2520networks%250Apre-trained%2520on%2520a%2520large%2520dataset.%2520While%2520the%2520former%2520may%2520provide%2520only%2520sparse%250Asupervision%252C%2520the%2520latter%2520may%2520suffer%2520from%2520generalization%2520issues.%2520As%2520opposed%2520to%250Athe%2520earlier%2520approaches%252C%2520we%2520seek%2520to%2520learn%2520the%2520depth%2520supervision%2520by%2520designing%250Aaugmented%2520models%2520and%2520training%2520them%2520along%2520with%2520the%2520main%2520radiance%2520field.%2520Further%252C%250Awe%2520aim%2520to%2520design%2520a%2520framework%2520of%2520regularizations%2520that%2520can%2520work%2520across%2520different%250Aimplicit%2520and%2520explicit%2520radiance%2520fields.%2520We%2520observe%2520that%2520certain%2520features%2520of%250Athese%2520radiance%2520field%2520models%2520overfit%2520to%2520the%2520observed%2520images%2520in%2520the%2520sparse-input%250Ascenario.%2520Our%2520key%2520finding%2520is%2520that%2520reducing%2520the%2520capability%2520of%2520the%2520radiance%250Afields%2520with%2520respect%2520to%2520positional%2520encoding%252C%2520the%2520number%2520of%2520decomposed%2520tensor%250Acomponents%2520or%2520the%2520size%2520of%2520the%2520hash%2520table%252C%2520constrains%2520the%2520model%2520to%2520learn%2520simpler%250Asolutions%252C%2520which%2520estimate%2520better%2520depth%2520in%2520certain%2520regions.%2520By%2520designing%250Aaugmented%2520models%2520based%2520on%2520such%2520reduced%2520capabilities%252C%2520we%2520obtain%2520better%2520depth%250Asupervision%2520for%2520the%2520main%2520radiance%2520field.%2520We%2520achieve%2520state-of-the-art%250Aview-synthesis%2520performance%2520with%2520sparse%2520input%2520views%2520on%2520popular%2520datasets%250Acontaining%2520forward-facing%2520and%2520360%2524%255E%255Ccirc%2524%2520scenes%2520by%2520employing%2520the%2520above%250Aregularizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19015v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple-RF%3A%20Regularizing%20Sparse%20Input%20Radiance%20Fields%20with%20Simpler%0A%20%20Solutions&entry.906535625=Nagabhushan%20Somraj%20and%20Sai%20Harsha%20Mupparaju%20and%20Adithyan%20Karanayil%20and%20Rajiv%20Soundararajan&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20show%20impressive%20performance%20in%20photo-realistic%0Afree-view%20rendering%20of%20scenes.%20Recent%20improvements%20on%20the%20NeRF%20such%20as%20TensoRF%0Aand%20ZipNeRF%20employ%20explicit%20models%20for%20faster%20optimization%20and%20rendering%2C%20as%0Acompared%20to%20the%20NeRF%20that%20employs%20an%20implicit%20representation.%20However%2C%20both%0Aimplicit%20and%20explicit%20radiance%20fields%20require%20dense%20sampling%20of%20images%20in%20the%0Agiven%20scene.%20Their%20performance%20degrades%20significantly%20when%20only%20a%20sparse%20set%20of%0Aviews%20is%20available.%20Researchers%20find%20that%20supervising%20the%20depth%20estimated%20by%20a%0Aradiance%20field%20helps%20train%20it%20effectively%20with%20fewer%20views.%20The%20depth%0Asupervision%20is%20obtained%20either%20using%20classical%20approaches%20or%20neural%20networks%0Apre-trained%20on%20a%20large%20dataset.%20While%20the%20former%20may%20provide%20only%20sparse%0Asupervision%2C%20the%20latter%20may%20suffer%20from%20generalization%20issues.%20As%20opposed%20to%0Athe%20earlier%20approaches%2C%20we%20seek%20to%20learn%20the%20depth%20supervision%20by%20designing%0Aaugmented%20models%20and%20training%20them%20along%20with%20the%20main%20radiance%20field.%20Further%2C%0Awe%20aim%20to%20design%20a%20framework%20of%20regularizations%20that%20can%20work%20across%20different%0Aimplicit%20and%20explicit%20radiance%20fields.%20We%20observe%20that%20certain%20features%20of%0Athese%20radiance%20field%20models%20overfit%20to%20the%20observed%20images%20in%20the%20sparse-input%0Ascenario.%20Our%20key%20finding%20is%20that%20reducing%20the%20capability%20of%20the%20radiance%0Afields%20with%20respect%20to%20positional%20encoding%2C%20the%20number%20of%20decomposed%20tensor%0Acomponents%20or%20the%20size%20of%20the%20hash%20table%2C%20constrains%20the%20model%20to%20learn%20simpler%0Asolutions%2C%20which%20estimate%20better%20depth%20in%20certain%20regions.%20By%20designing%0Aaugmented%20models%20based%20on%20such%20reduced%20capabilities%2C%20we%20obtain%20better%20depth%0Asupervision%20for%20the%20main%20radiance%20field.%20We%20achieve%20state-of-the-art%0Aview-synthesis%20performance%20with%20sparse%20input%20views%20on%20popular%20datasets%0Acontaining%20forward-facing%20and%20360%24%5E%5Ccirc%24%20scenes%20by%20employing%20the%20above%0Aregularizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19015v3&entry.124074799=Read"},
{"title": "Learning Personalized Decision Support Policies", "author": "Umang Bhatt and Valerie Chen and Katherine M. Collins and Parameswaran Kamalaruban and Emma Kallina and Adrian Weller and Ameet Talwalkar", "abstract": "  Individual human decision-makers may benefit from different forms of support\nto improve decision outcomes, but when each form of support will yield better\noutcomes? In this work, we posit that personalizing access to decision support\ntools can be an effective mechanism for instantiating the appropriate use of AI\nassistance. Specifically, we propose the general problem of learning a decision\nsupport policy that, for a given input, chooses which form of support to\nprovide to decision-makers for whom we initially have no prior information. We\ndevelop $\\texttt{Modiste}$, an interactive tool to learn personalized decision\nsupport policies. $\\texttt{Modiste}$ leverages stochastic contextual bandit\ntechniques to personalize a decision support policy for each decision-maker and\nsupports extensions to the multi-objective setting to account for auxiliary\nobjectives like the cost of support. We find that personalized policies\noutperform offline policies, and, in the cost-aware setting, reduce the\nincurred cost with minimal degradation to performance. Our experiments include\nvarious realistic forms of support (e.g., expert consensus and predictions from\na large language model) on vision and language tasks. Our human subject\nexperiments validate our computational experiments, demonstrating that\npersonalization can yield benefits in practice for real users, who interact\nwith $\\texttt{Modiste}$.\n", "link": "http://arxiv.org/abs/2304.06701v2", "date": "2024-05-27", "relevancy": 0.9807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5101}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4819}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Personalized%20Decision%20Support%20Policies&body=Title%3A%20Learning%20Personalized%20Decision%20Support%20Policies%0AAuthor%3A%20Umang%20Bhatt%20and%20Valerie%20Chen%20and%20Katherine%20M.%20Collins%20and%20Parameswaran%20Kamalaruban%20and%20Emma%20Kallina%20and%20Adrian%20Weller%20and%20Ameet%20Talwalkar%0AAbstract%3A%20%20%20Individual%20human%20decision-makers%20may%20benefit%20from%20different%20forms%20of%20support%0Ato%20improve%20decision%20outcomes%2C%20but%20when%20each%20form%20of%20support%20will%20yield%20better%0Aoutcomes%3F%20In%20this%20work%2C%20we%20posit%20that%20personalizing%20access%20to%20decision%20support%0Atools%20can%20be%20an%20effective%20mechanism%20for%20instantiating%20the%20appropriate%20use%20of%20AI%0Aassistance.%20Specifically%2C%20we%20propose%20the%20general%20problem%20of%20learning%20a%20decision%0Asupport%20policy%20that%2C%20for%20a%20given%20input%2C%20chooses%20which%20form%20of%20support%20to%0Aprovide%20to%20decision-makers%20for%20whom%20we%20initially%20have%20no%20prior%20information.%20We%0Adevelop%20%24%5Ctexttt%7BModiste%7D%24%2C%20an%20interactive%20tool%20to%20learn%20personalized%20decision%0Asupport%20policies.%20%24%5Ctexttt%7BModiste%7D%24%20leverages%20stochastic%20contextual%20bandit%0Atechniques%20to%20personalize%20a%20decision%20support%20policy%20for%20each%20decision-maker%20and%0Asupports%20extensions%20to%20the%20multi-objective%20setting%20to%20account%20for%20auxiliary%0Aobjectives%20like%20the%20cost%20of%20support.%20We%20find%20that%20personalized%20policies%0Aoutperform%20offline%20policies%2C%20and%2C%20in%20the%20cost-aware%20setting%2C%20reduce%20the%0Aincurred%20cost%20with%20minimal%20degradation%20to%20performance.%20Our%20experiments%20include%0Avarious%20realistic%20forms%20of%20support%20%28e.g.%2C%20expert%20consensus%20and%20predictions%20from%0Aa%20large%20language%20model%29%20on%20vision%20and%20language%20tasks.%20Our%20human%20subject%0Aexperiments%20validate%20our%20computational%20experiments%2C%20demonstrating%20that%0Apersonalization%20can%20yield%20benefits%20in%20practice%20for%20real%20users%2C%20who%20interact%0Awith%20%24%5Ctexttt%7BModiste%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Personalized%2520Decision%2520Support%2520Policies%26entry.906535625%3DUmang%2520Bhatt%2520and%2520Valerie%2520Chen%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Parameswaran%2520Kamalaruban%2520and%2520Emma%2520Kallina%2520and%2520Adrian%2520Weller%2520and%2520Ameet%2520Talwalkar%26entry.1292438233%3D%2520%2520Individual%2520human%2520decision-makers%2520may%2520benefit%2520from%2520different%2520forms%2520of%2520support%250Ato%2520improve%2520decision%2520outcomes%252C%2520but%2520when%2520each%2520form%2520of%2520support%2520will%2520yield%2520better%250Aoutcomes%253F%2520In%2520this%2520work%252C%2520we%2520posit%2520that%2520personalizing%2520access%2520to%2520decision%2520support%250Atools%2520can%2520be%2520an%2520effective%2520mechanism%2520for%2520instantiating%2520the%2520appropriate%2520use%2520of%2520AI%250Aassistance.%2520Specifically%252C%2520we%2520propose%2520the%2520general%2520problem%2520of%2520learning%2520a%2520decision%250Asupport%2520policy%2520that%252C%2520for%2520a%2520given%2520input%252C%2520chooses%2520which%2520form%2520of%2520support%2520to%250Aprovide%2520to%2520decision-makers%2520for%2520whom%2520we%2520initially%2520have%2520no%2520prior%2520information.%2520We%250Adevelop%2520%2524%255Ctexttt%257BModiste%257D%2524%252C%2520an%2520interactive%2520tool%2520to%2520learn%2520personalized%2520decision%250Asupport%2520policies.%2520%2524%255Ctexttt%257BModiste%257D%2524%2520leverages%2520stochastic%2520contextual%2520bandit%250Atechniques%2520to%2520personalize%2520a%2520decision%2520support%2520policy%2520for%2520each%2520decision-maker%2520and%250Asupports%2520extensions%2520to%2520the%2520multi-objective%2520setting%2520to%2520account%2520for%2520auxiliary%250Aobjectives%2520like%2520the%2520cost%2520of%2520support.%2520We%2520find%2520that%2520personalized%2520policies%250Aoutperform%2520offline%2520policies%252C%2520and%252C%2520in%2520the%2520cost-aware%2520setting%252C%2520reduce%2520the%250Aincurred%2520cost%2520with%2520minimal%2520degradation%2520to%2520performance.%2520Our%2520experiments%2520include%250Avarious%2520realistic%2520forms%2520of%2520support%2520%2528e.g.%252C%2520expert%2520consensus%2520and%2520predictions%2520from%250Aa%2520large%2520language%2520model%2529%2520on%2520vision%2520and%2520language%2520tasks.%2520Our%2520human%2520subject%250Aexperiments%2520validate%2520our%2520computational%2520experiments%252C%2520demonstrating%2520that%250Apersonalization%2520can%2520yield%2520benefits%2520in%2520practice%2520for%2520real%2520users%252C%2520who%2520interact%250Awith%2520%2524%255Ctexttt%257BModiste%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Personalized%20Decision%20Support%20Policies&entry.906535625=Umang%20Bhatt%20and%20Valerie%20Chen%20and%20Katherine%20M.%20Collins%20and%20Parameswaran%20Kamalaruban%20and%20Emma%20Kallina%20and%20Adrian%20Weller%20and%20Ameet%20Talwalkar&entry.1292438233=%20%20Individual%20human%20decision-makers%20may%20benefit%20from%20different%20forms%20of%20support%0Ato%20improve%20decision%20outcomes%2C%20but%20when%20each%20form%20of%20support%20will%20yield%20better%0Aoutcomes%3F%20In%20this%20work%2C%20we%20posit%20that%20personalizing%20access%20to%20decision%20support%0Atools%20can%20be%20an%20effective%20mechanism%20for%20instantiating%20the%20appropriate%20use%20of%20AI%0Aassistance.%20Specifically%2C%20we%20propose%20the%20general%20problem%20of%20learning%20a%20decision%0Asupport%20policy%20that%2C%20for%20a%20given%20input%2C%20chooses%20which%20form%20of%20support%20to%0Aprovide%20to%20decision-makers%20for%20whom%20we%20initially%20have%20no%20prior%20information.%20We%0Adevelop%20%24%5Ctexttt%7BModiste%7D%24%2C%20an%20interactive%20tool%20to%20learn%20personalized%20decision%0Asupport%20policies.%20%24%5Ctexttt%7BModiste%7D%24%20leverages%20stochastic%20contextual%20bandit%0Atechniques%20to%20personalize%20a%20decision%20support%20policy%20for%20each%20decision-maker%20and%0Asupports%20extensions%20to%20the%20multi-objective%20setting%20to%20account%20for%20auxiliary%0Aobjectives%20like%20the%20cost%20of%20support.%20We%20find%20that%20personalized%20policies%0Aoutperform%20offline%20policies%2C%20and%2C%20in%20the%20cost-aware%20setting%2C%20reduce%20the%0Aincurred%20cost%20with%20minimal%20degradation%20to%20performance.%20Our%20experiments%20include%0Avarious%20realistic%20forms%20of%20support%20%28e.g.%2C%20expert%20consensus%20and%20predictions%20from%0Aa%20large%20language%20model%29%20on%20vision%20and%20language%20tasks.%20Our%20human%20subject%0Aexperiments%20validate%20our%20computational%20experiments%2C%20demonstrating%20that%0Apersonalization%20can%20yield%20benefits%20in%20practice%20for%20real%20users%2C%20who%20interact%0Awith%20%24%5Ctexttt%7BModiste%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06701v2&entry.124074799=Read"},
{"title": "Generative Plant Growth Simulation from Sequence-Informed Environmental\n  Conditions", "author": "Mohamed Debbagh and Yixue Liu and Zhouzhou Zheng and Xintong Jiang and Shangpeng Sun and Mark Lefsrud", "abstract": "  A plant growth simulation can be characterized as a reconstructed visual\nrepresentation of a plant or plant system. The phenotypic characteristics and\nplant structures are controlled by the scene environment and other contextual\nattributes. Considering the temporal dependencies and compounding effects of\nvarious factors on growth trajectories, we formulate a probabilistic approach\nto the simulation task by solving a frame synthesis and pattern recognition\nproblem. We introduce a sequence-informed plant growth simulation framework\n(SI-PGS) that employs a conditional generative model to implicitly learn a\ndistribution of possible plant representations within a dynamic scene from a\nfusion of low dimensional temporal sensor and context data. Methods such as\ncontrolled latent sampling and recurrent output connections are used to improve\ncoherence in the plant structures between frames of predictions. In this work,\nwe demonstrate that SI-PGS is able to capture temporal dependencies and\ncontinuously generate realistic frames of plant growth.\n", "link": "http://arxiv.org/abs/2405.14796v2", "date": "2024-05-27", "relevancy": 1.6912, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5948}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5367}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Plant%20Growth%20Simulation%20from%20Sequence-Informed%20Environmental%0A%20%20Conditions&body=Title%3A%20Generative%20Plant%20Growth%20Simulation%20from%20Sequence-Informed%20Environmental%0A%20%20Conditions%0AAuthor%3A%20Mohamed%20Debbagh%20and%20Yixue%20Liu%20and%20Zhouzhou%20Zheng%20and%20Xintong%20Jiang%20and%20Shangpeng%20Sun%20and%20Mark%20Lefsrud%0AAbstract%3A%20%20%20A%20plant%20growth%20simulation%20can%20be%20characterized%20as%20a%20reconstructed%20visual%0Arepresentation%20of%20a%20plant%20or%20plant%20system.%20The%20phenotypic%20characteristics%20and%0Aplant%20structures%20are%20controlled%20by%20the%20scene%20environment%20and%20other%20contextual%0Aattributes.%20Considering%20the%20temporal%20dependencies%20and%20compounding%20effects%20of%0Avarious%20factors%20on%20growth%20trajectories%2C%20we%20formulate%20a%20probabilistic%20approach%0Ato%20the%20simulation%20task%20by%20solving%20a%20frame%20synthesis%20and%20pattern%20recognition%0Aproblem.%20We%20introduce%20a%20sequence-informed%20plant%20growth%20simulation%20framework%0A%28SI-PGS%29%20that%20employs%20a%20conditional%20generative%20model%20to%20implicitly%20learn%20a%0Adistribution%20of%20possible%20plant%20representations%20within%20a%20dynamic%20scene%20from%20a%0Afusion%20of%20low%20dimensional%20temporal%20sensor%20and%20context%20data.%20Methods%20such%20as%0Acontrolled%20latent%20sampling%20and%20recurrent%20output%20connections%20are%20used%20to%20improve%0Acoherence%20in%20the%20plant%20structures%20between%20frames%20of%20predictions.%20In%20this%20work%2C%0Awe%20demonstrate%20that%20SI-PGS%20is%20able%20to%20capture%20temporal%20dependencies%20and%0Acontinuously%20generate%20realistic%20frames%20of%20plant%20growth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Plant%2520Growth%2520Simulation%2520from%2520Sequence-Informed%2520Environmental%250A%2520%2520Conditions%26entry.906535625%3DMohamed%2520Debbagh%2520and%2520Yixue%2520Liu%2520and%2520Zhouzhou%2520Zheng%2520and%2520Xintong%2520Jiang%2520and%2520Shangpeng%2520Sun%2520and%2520Mark%2520Lefsrud%26entry.1292438233%3D%2520%2520A%2520plant%2520growth%2520simulation%2520can%2520be%2520characterized%2520as%2520a%2520reconstructed%2520visual%250Arepresentation%2520of%2520a%2520plant%2520or%2520plant%2520system.%2520The%2520phenotypic%2520characteristics%2520and%250Aplant%2520structures%2520are%2520controlled%2520by%2520the%2520scene%2520environment%2520and%2520other%2520contextual%250Aattributes.%2520Considering%2520the%2520temporal%2520dependencies%2520and%2520compounding%2520effects%2520of%250Avarious%2520factors%2520on%2520growth%2520trajectories%252C%2520we%2520formulate%2520a%2520probabilistic%2520approach%250Ato%2520the%2520simulation%2520task%2520by%2520solving%2520a%2520frame%2520synthesis%2520and%2520pattern%2520recognition%250Aproblem.%2520We%2520introduce%2520a%2520sequence-informed%2520plant%2520growth%2520simulation%2520framework%250A%2528SI-PGS%2529%2520that%2520employs%2520a%2520conditional%2520generative%2520model%2520to%2520implicitly%2520learn%2520a%250Adistribution%2520of%2520possible%2520plant%2520representations%2520within%2520a%2520dynamic%2520scene%2520from%2520a%250Afusion%2520of%2520low%2520dimensional%2520temporal%2520sensor%2520and%2520context%2520data.%2520Methods%2520such%2520as%250Acontrolled%2520latent%2520sampling%2520and%2520recurrent%2520output%2520connections%2520are%2520used%2520to%2520improve%250Acoherence%2520in%2520the%2520plant%2520structures%2520between%2520frames%2520of%2520predictions.%2520In%2520this%2520work%252C%250Awe%2520demonstrate%2520that%2520SI-PGS%2520is%2520able%2520to%2520capture%2520temporal%2520dependencies%2520and%250Acontinuously%2520generate%2520realistic%2520frames%2520of%2520plant%2520growth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Plant%20Growth%20Simulation%20from%20Sequence-Informed%20Environmental%0A%20%20Conditions&entry.906535625=Mohamed%20Debbagh%20and%20Yixue%20Liu%20and%20Zhouzhou%20Zheng%20and%20Xintong%20Jiang%20and%20Shangpeng%20Sun%20and%20Mark%20Lefsrud&entry.1292438233=%20%20A%20plant%20growth%20simulation%20can%20be%20characterized%20as%20a%20reconstructed%20visual%0Arepresentation%20of%20a%20plant%20or%20plant%20system.%20The%20phenotypic%20characteristics%20and%0Aplant%20structures%20are%20controlled%20by%20the%20scene%20environment%20and%20other%20contextual%0Aattributes.%20Considering%20the%20temporal%20dependencies%20and%20compounding%20effects%20of%0Avarious%20factors%20on%20growth%20trajectories%2C%20we%20formulate%20a%20probabilistic%20approach%0Ato%20the%20simulation%20task%20by%20solving%20a%20frame%20synthesis%20and%20pattern%20recognition%0Aproblem.%20We%20introduce%20a%20sequence-informed%20plant%20growth%20simulation%20framework%0A%28SI-PGS%29%20that%20employs%20a%20conditional%20generative%20model%20to%20implicitly%20learn%20a%0Adistribution%20of%20possible%20plant%20representations%20within%20a%20dynamic%20scene%20from%20a%0Afusion%20of%20low%20dimensional%20temporal%20sensor%20and%20context%20data.%20Methods%20such%20as%0Acontrolled%20latent%20sampling%20and%20recurrent%20output%20connections%20are%20used%20to%20improve%0Acoherence%20in%20the%20plant%20structures%20between%20frames%20of%20predictions.%20In%20this%20work%2C%0Awe%20demonstrate%20that%20SI-PGS%20is%20able%20to%20capture%20temporal%20dependencies%20and%0Acontinuously%20generate%20realistic%20frames%20of%20plant%20growth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14796v2&entry.124074799=Read"},
{"title": "Multi-view Disparity Estimation Using a Novel Gradient Consistency Model", "author": "James L. Gray and Aous T. Naman and David S. Taubman", "abstract": "  Variational approaches to disparity estimation typically use a linearised\nbrightness constancy constraint, which only applies in smooth regions and over\nsmall distances. Accordingly, current variational approaches rely on a schedule\nto progressively include image data. This paper proposes the use of Gradient\nConsistency information to assess the validity of the linearisation; this\ninformation is used to determine the weights applied to the data term as part\nof an analytically inspired Gradient Consistency Model. The Gradient\nConsistency Model penalises the data term for view pairs that have a mismatch\nbetween the spatial gradients in the source view and the spatial gradients in\nthe target view. Instead of relying on a tuned or learned schedule, the\nGradient Consistency Model is self-scheduling, since the weights evolve as the\nalgorithm progresses. We show that the Gradient Consistency Model outperforms\nstandard coarse-to-fine schemes and the recently proposed progressive inclusion\nof views approach in both rate of convergence and accuracy.\n", "link": "http://arxiv.org/abs/2405.17029v1", "date": "2024-05-27", "relevancy": 1.6006, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5366}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5364}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Disparity%20Estimation%20Using%20a%20Novel%20Gradient%20Consistency%20Model&body=Title%3A%20Multi-view%20Disparity%20Estimation%20Using%20a%20Novel%20Gradient%20Consistency%20Model%0AAuthor%3A%20James%20L.%20Gray%20and%20Aous%20T.%20Naman%20and%20David%20S.%20Taubman%0AAbstract%3A%20%20%20Variational%20approaches%20to%20disparity%20estimation%20typically%20use%20a%20linearised%0Abrightness%20constancy%20constraint%2C%20which%20only%20applies%20in%20smooth%20regions%20and%20over%0Asmall%20distances.%20Accordingly%2C%20current%20variational%20approaches%20rely%20on%20a%20schedule%0Ato%20progressively%20include%20image%20data.%20This%20paper%20proposes%20the%20use%20of%20Gradient%0AConsistency%20information%20to%20assess%20the%20validity%20of%20the%20linearisation%3B%20this%0Ainformation%20is%20used%20to%20determine%20the%20weights%20applied%20to%20the%20data%20term%20as%20part%0Aof%20an%20analytically%20inspired%20Gradient%20Consistency%20Model.%20The%20Gradient%0AConsistency%20Model%20penalises%20the%20data%20term%20for%20view%20pairs%20that%20have%20a%20mismatch%0Abetween%20the%20spatial%20gradients%20in%20the%20source%20view%20and%20the%20spatial%20gradients%20in%0Athe%20target%20view.%20Instead%20of%20relying%20on%20a%20tuned%20or%20learned%20schedule%2C%20the%0AGradient%20Consistency%20Model%20is%20self-scheduling%2C%20since%20the%20weights%20evolve%20as%20the%0Aalgorithm%20progresses.%20We%20show%20that%20the%20Gradient%20Consistency%20Model%20outperforms%0Astandard%20coarse-to-fine%20schemes%20and%20the%20recently%20proposed%20progressive%20inclusion%0Aof%20views%20approach%20in%20both%20rate%20of%20convergence%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Disparity%2520Estimation%2520Using%2520a%2520Novel%2520Gradient%2520Consistency%2520Model%26entry.906535625%3DJames%2520L.%2520Gray%2520and%2520Aous%2520T.%2520Naman%2520and%2520David%2520S.%2520Taubman%26entry.1292438233%3D%2520%2520Variational%2520approaches%2520to%2520disparity%2520estimation%2520typically%2520use%2520a%2520linearised%250Abrightness%2520constancy%2520constraint%252C%2520which%2520only%2520applies%2520in%2520smooth%2520regions%2520and%2520over%250Asmall%2520distances.%2520Accordingly%252C%2520current%2520variational%2520approaches%2520rely%2520on%2520a%2520schedule%250Ato%2520progressively%2520include%2520image%2520data.%2520This%2520paper%2520proposes%2520the%2520use%2520of%2520Gradient%250AConsistency%2520information%2520to%2520assess%2520the%2520validity%2520of%2520the%2520linearisation%253B%2520this%250Ainformation%2520is%2520used%2520to%2520determine%2520the%2520weights%2520applied%2520to%2520the%2520data%2520term%2520as%2520part%250Aof%2520an%2520analytically%2520inspired%2520Gradient%2520Consistency%2520Model.%2520The%2520Gradient%250AConsistency%2520Model%2520penalises%2520the%2520data%2520term%2520for%2520view%2520pairs%2520that%2520have%2520a%2520mismatch%250Abetween%2520the%2520spatial%2520gradients%2520in%2520the%2520source%2520view%2520and%2520the%2520spatial%2520gradients%2520in%250Athe%2520target%2520view.%2520Instead%2520of%2520relying%2520on%2520a%2520tuned%2520or%2520learned%2520schedule%252C%2520the%250AGradient%2520Consistency%2520Model%2520is%2520self-scheduling%252C%2520since%2520the%2520weights%2520evolve%2520as%2520the%250Aalgorithm%2520progresses.%2520We%2520show%2520that%2520the%2520Gradient%2520Consistency%2520Model%2520outperforms%250Astandard%2520coarse-to-fine%2520schemes%2520and%2520the%2520recently%2520proposed%2520progressive%2520inclusion%250Aof%2520views%2520approach%2520in%2520both%2520rate%2520of%2520convergence%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Disparity%20Estimation%20Using%20a%20Novel%20Gradient%20Consistency%20Model&entry.906535625=James%20L.%20Gray%20and%20Aous%20T.%20Naman%20and%20David%20S.%20Taubman&entry.1292438233=%20%20Variational%20approaches%20to%20disparity%20estimation%20typically%20use%20a%20linearised%0Abrightness%20constancy%20constraint%2C%20which%20only%20applies%20in%20smooth%20regions%20and%20over%0Asmall%20distances.%20Accordingly%2C%20current%20variational%20approaches%20rely%20on%20a%20schedule%0Ato%20progressively%20include%20image%20data.%20This%20paper%20proposes%20the%20use%20of%20Gradient%0AConsistency%20information%20to%20assess%20the%20validity%20of%20the%20linearisation%3B%20this%0Ainformation%20is%20used%20to%20determine%20the%20weights%20applied%20to%20the%20data%20term%20as%20part%0Aof%20an%20analytically%20inspired%20Gradient%20Consistency%20Model.%20The%20Gradient%0AConsistency%20Model%20penalises%20the%20data%20term%20for%20view%20pairs%20that%20have%20a%20mismatch%0Abetween%20the%20spatial%20gradients%20in%20the%20source%20view%20and%20the%20spatial%20gradients%20in%0Athe%20target%20view.%20Instead%20of%20relying%20on%20a%20tuned%20or%20learned%20schedule%2C%20the%0AGradient%20Consistency%20Model%20is%20self-scheduling%2C%20since%20the%20weights%20evolve%20as%20the%0Aalgorithm%20progresses.%20We%20show%20that%20the%20Gradient%20Consistency%20Model%20outperforms%0Astandard%20coarse-to-fine%20schemes%20and%20the%20recently%20proposed%20progressive%20inclusion%0Aof%20views%20approach%20in%20both%20rate%20of%20convergence%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17029v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


