<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251110.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering", "author": "Yusen Xie and Zhenmin Huang and Jianhao Jiao and Dimitrios Kanoulas and Jun Ma", "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.", "link": "http://arxiv.org/abs/2510.12174v2", "date": "2025-11-13", "relevancy": 3.5085, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.718}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7028}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGS%3A%20Unified%20Geometry-Aware%20Gaussian%20Splatting%20for%20Multimodal%20Rendering&body=Title%3A%20UniGS%3A%20Unified%20Geometry-Aware%20Gaussian%20Splatting%20for%20Multimodal%20Rendering%0AAuthor%3A%20Yusen%20Xie%20and%20Zhenmin%20Huang%20and%20Jianhao%20Jiao%20and%20Dimitrios%20Kanoulas%20and%20Jun%20Ma%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20UniGS%2C%20a%20unified%20map%20representation%20and%20differentiable%20framework%20for%20high-fidelity%20multimodal%203D%20reconstruction%20based%20on%203D%20Gaussian%20Splatting.%20Our%20framework%20integrates%20a%20CUDA-accelerated%20rasterization%20pipeline%20capable%20of%20rendering%20photo-realistic%20RGB%20images%2C%20geometrically%20accurate%20depth%20maps%2C%20consistent%20surface%20normals%2C%20and%20semantic%20logits%20simultaneously.%20We%20redesign%20the%20rasterization%20to%20render%20depth%20via%20differentiable%20ray-ellipsoid%20intersection%20rather%20than%20using%20Gaussian%20centers%2C%20enabling%20effective%20optimization%20of%20rotation%20and%20scale%20attribute%20through%20analytic%20depth%20gradients.%20Furthermore%2C%20we%20derive%20the%20analytic%20gradient%20formulation%20for%20surface%20normal%20rendering%2C%20ensuring%20geometric%20consistency%20among%20reconstructed%203D%20scenes.%20To%20improve%20computational%20and%20storage%20efficiency%2C%20we%20introduce%20a%20learnable%20attribute%20that%20enables%20differentiable%20pruning%20of%20Gaussians%20with%20minimal%20contribution%20during%20training.%20Quantitative%20and%20qualitative%20experiments%20demonstrate%20state-of-the-art%20reconstruction%20accuracy%20across%20all%20modalities%2C%20validating%20the%20efficacy%20of%20our%20geometry-aware%20paradigm.%20Source%20code%20and%20multimodal%20viewer%20will%20be%20available%20on%20GitHub.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGS%253A%2520Unified%2520Geometry-Aware%2520Gaussian%2520Splatting%2520for%2520Multimodal%2520Rendering%26entry.906535625%3DYusen%2520Xie%2520and%2520Zhenmin%2520Huang%2520and%2520Jianhao%2520Jiao%2520and%2520Dimitrios%2520Kanoulas%2520and%2520Jun%2520Ma%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520UniGS%252C%2520a%2520unified%2520map%2520representation%2520and%2520differentiable%2520framework%2520for%2520high-fidelity%2520multimodal%25203D%2520reconstruction%2520based%2520on%25203D%2520Gaussian%2520Splatting.%2520Our%2520framework%2520integrates%2520a%2520CUDA-accelerated%2520rasterization%2520pipeline%2520capable%2520of%2520rendering%2520photo-realistic%2520RGB%2520images%252C%2520geometrically%2520accurate%2520depth%2520maps%252C%2520consistent%2520surface%2520normals%252C%2520and%2520semantic%2520logits%2520simultaneously.%2520We%2520redesign%2520the%2520rasterization%2520to%2520render%2520depth%2520via%2520differentiable%2520ray-ellipsoid%2520intersection%2520rather%2520than%2520using%2520Gaussian%2520centers%252C%2520enabling%2520effective%2520optimization%2520of%2520rotation%2520and%2520scale%2520attribute%2520through%2520analytic%2520depth%2520gradients.%2520Furthermore%252C%2520we%2520derive%2520the%2520analytic%2520gradient%2520formulation%2520for%2520surface%2520normal%2520rendering%252C%2520ensuring%2520geometric%2520consistency%2520among%2520reconstructed%25203D%2520scenes.%2520To%2520improve%2520computational%2520and%2520storage%2520efficiency%252C%2520we%2520introduce%2520a%2520learnable%2520attribute%2520that%2520enables%2520differentiable%2520pruning%2520of%2520Gaussians%2520with%2520minimal%2520contribution%2520during%2520training.%2520Quantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520state-of-the-art%2520reconstruction%2520accuracy%2520across%2520all%2520modalities%252C%2520validating%2520the%2520efficacy%2520of%2520our%2520geometry-aware%2520paradigm.%2520Source%2520code%2520and%2520multimodal%2520viewer%2520will%2520be%2520available%2520on%2520GitHub.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGS%3A%20Unified%20Geometry-Aware%20Gaussian%20Splatting%20for%20Multimodal%20Rendering&entry.906535625=Yusen%20Xie%20and%20Zhenmin%20Huang%20and%20Jianhao%20Jiao%20and%20Dimitrios%20Kanoulas%20and%20Jun%20Ma&entry.1292438233=In%20this%20paper%2C%20we%20propose%20UniGS%2C%20a%20unified%20map%20representation%20and%20differentiable%20framework%20for%20high-fidelity%20multimodal%203D%20reconstruction%20based%20on%203D%20Gaussian%20Splatting.%20Our%20framework%20integrates%20a%20CUDA-accelerated%20rasterization%20pipeline%20capable%20of%20rendering%20photo-realistic%20RGB%20images%2C%20geometrically%20accurate%20depth%20maps%2C%20consistent%20surface%20normals%2C%20and%20semantic%20logits%20simultaneously.%20We%20redesign%20the%20rasterization%20to%20render%20depth%20via%20differentiable%20ray-ellipsoid%20intersection%20rather%20than%20using%20Gaussian%20centers%2C%20enabling%20effective%20optimization%20of%20rotation%20and%20scale%20attribute%20through%20analytic%20depth%20gradients.%20Furthermore%2C%20we%20derive%20the%20analytic%20gradient%20formulation%20for%20surface%20normal%20rendering%2C%20ensuring%20geometric%20consistency%20among%20reconstructed%203D%20scenes.%20To%20improve%20computational%20and%20storage%20efficiency%2C%20we%20introduce%20a%20learnable%20attribute%20that%20enables%20differentiable%20pruning%20of%20Gaussians%20with%20minimal%20contribution%20during%20training.%20Quantitative%20and%20qualitative%20experiments%20demonstrate%20state-of-the-art%20reconstruction%20accuracy%20across%20all%20modalities%2C%20validating%20the%20efficacy%20of%20our%20geometry-aware%20paradigm.%20Source%20code%20and%20multimodal%20viewer%20will%20be%20available%20on%20GitHub.&entry.1838667208=http%3A//arxiv.org/abs/2510.12174v2&entry.124074799=Read"},
{"title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision", "author": "Yu Deng and Baozhu Zhao and Junyan Su and Xiaohan Zhang and Qi Liu", "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.", "link": "http://arxiv.org/abs/2511.10316v1", "date": "2025-11-13", "relevancy": 3.2485, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6757}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6506}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Consistent%203D%20Gaussian%20Splatting%20via%20Physical%20Defocus%20Modeling%20and%20Multi-View%20Geometric%20Supervision&body=Title%3A%20Depth-Consistent%203D%20Gaussian%20Splatting%20via%20Physical%20Defocus%20Modeling%20and%20Multi-View%20Geometric%20Supervision%0AAuthor%3A%20Yu%20Deng%20and%20Baozhu%20Zhao%20and%20Junyan%20Su%20and%20Xiaohan%20Zhang%20and%20Qi%20Liu%0AAbstract%3A%20Three-dimensional%20reconstruction%20in%20scenes%20with%20extreme%20depth%20variations%20remains%20challenging%20due%20to%20inconsistent%20supervisory%20signals%20between%20near-field%20and%20far-field%20regions.%20Existing%20methods%20fail%20to%20simultaneously%20address%20inaccurate%20depth%20estimation%20in%20distant%20areas%20and%20structural%20degradation%20in%20close-range%20regions.%20This%20paper%20proposes%20a%20novel%20computational%20framework%20that%20integrates%20depth-of-field%20supervision%20and%20multi-view%20consistency%20supervision%20to%20advance%203D%20Gaussian%20Splatting.%20Our%20approach%20comprises%20two%20core%20components%3A%20%281%29%20Depth-of-field%20Supervision%20employs%20a%20scale-recovered%20monocular%20depth%20estimator%20%28e.g.%2C%20Metric3D%29%20to%20generate%20depth%20priors%2C%20leverages%20defocus%20convolution%20to%20synthesize%20physically%20accurate%20defocused%20images%2C%20and%20enforces%20geometric%20consistency%20through%20a%20novel%20depth-of-field%20loss%2C%20thereby%20enhancing%20depth%20fidelity%20in%20both%20far-field%20and%20near-field%20regions%3B%20%282%29%20Multi-View%20Consistency%20Supervision%20employing%20LoFTR-based%20semi-dense%20feature%20matching%20to%20minimize%20cross-view%20geometric%20errors%20and%20enforce%20depth%20consistency%20via%20least%20squares%20optimization%20of%20reliable%20matched%20points.%20By%20unifying%20defocus%20physics%20with%20multi-view%20geometric%20constraints%2C%20our%20method%20achieves%20superior%20depth%20fidelity%2C%20demonstrating%20a%200.8%20dB%20PSNR%20improvement%20over%20the%20state-of-the-art%20method%20on%20the%20Waymo%20Open%20Dataset.%20This%20framework%20bridges%20physical%20imaging%20principles%20and%20learning-based%20depth%20regularization%2C%20offering%20a%20scalable%20solution%20for%20complex%20depth%20stratification%20in%20urban%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Consistent%25203D%2520Gaussian%2520Splatting%2520via%2520Physical%2520Defocus%2520Modeling%2520and%2520Multi-View%2520Geometric%2520Supervision%26entry.906535625%3DYu%2520Deng%2520and%2520Baozhu%2520Zhao%2520and%2520Junyan%2520Su%2520and%2520Xiaohan%2520Zhang%2520and%2520Qi%2520Liu%26entry.1292438233%3DThree-dimensional%2520reconstruction%2520in%2520scenes%2520with%2520extreme%2520depth%2520variations%2520remains%2520challenging%2520due%2520to%2520inconsistent%2520supervisory%2520signals%2520between%2520near-field%2520and%2520far-field%2520regions.%2520Existing%2520methods%2520fail%2520to%2520simultaneously%2520address%2520inaccurate%2520depth%2520estimation%2520in%2520distant%2520areas%2520and%2520structural%2520degradation%2520in%2520close-range%2520regions.%2520This%2520paper%2520proposes%2520a%2520novel%2520computational%2520framework%2520that%2520integrates%2520depth-of-field%2520supervision%2520and%2520multi-view%2520consistency%2520supervision%2520to%2520advance%25203D%2520Gaussian%2520Splatting.%2520Our%2520approach%2520comprises%2520two%2520core%2520components%253A%2520%25281%2529%2520Depth-of-field%2520Supervision%2520employs%2520a%2520scale-recovered%2520monocular%2520depth%2520estimator%2520%2528e.g.%252C%2520Metric3D%2529%2520to%2520generate%2520depth%2520priors%252C%2520leverages%2520defocus%2520convolution%2520to%2520synthesize%2520physically%2520accurate%2520defocused%2520images%252C%2520and%2520enforces%2520geometric%2520consistency%2520through%2520a%2520novel%2520depth-of-field%2520loss%252C%2520thereby%2520enhancing%2520depth%2520fidelity%2520in%2520both%2520far-field%2520and%2520near-field%2520regions%253B%2520%25282%2529%2520Multi-View%2520Consistency%2520Supervision%2520employing%2520LoFTR-based%2520semi-dense%2520feature%2520matching%2520to%2520minimize%2520cross-view%2520geometric%2520errors%2520and%2520enforce%2520depth%2520consistency%2520via%2520least%2520squares%2520optimization%2520of%2520reliable%2520matched%2520points.%2520By%2520unifying%2520defocus%2520physics%2520with%2520multi-view%2520geometric%2520constraints%252C%2520our%2520method%2520achieves%2520superior%2520depth%2520fidelity%252C%2520demonstrating%2520a%25200.8%2520dB%2520PSNR%2520improvement%2520over%2520the%2520state-of-the-art%2520method%2520on%2520the%2520Waymo%2520Open%2520Dataset.%2520This%2520framework%2520bridges%2520physical%2520imaging%2520principles%2520and%2520learning-based%2520depth%2520regularization%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520complex%2520depth%2520stratification%2520in%2520urban%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Consistent%203D%20Gaussian%20Splatting%20via%20Physical%20Defocus%20Modeling%20and%20Multi-View%20Geometric%20Supervision&entry.906535625=Yu%20Deng%20and%20Baozhu%20Zhao%20and%20Junyan%20Su%20and%20Xiaohan%20Zhang%20and%20Qi%20Liu&entry.1292438233=Three-dimensional%20reconstruction%20in%20scenes%20with%20extreme%20depth%20variations%20remains%20challenging%20due%20to%20inconsistent%20supervisory%20signals%20between%20near-field%20and%20far-field%20regions.%20Existing%20methods%20fail%20to%20simultaneously%20address%20inaccurate%20depth%20estimation%20in%20distant%20areas%20and%20structural%20degradation%20in%20close-range%20regions.%20This%20paper%20proposes%20a%20novel%20computational%20framework%20that%20integrates%20depth-of-field%20supervision%20and%20multi-view%20consistency%20supervision%20to%20advance%203D%20Gaussian%20Splatting.%20Our%20approach%20comprises%20two%20core%20components%3A%20%281%29%20Depth-of-field%20Supervision%20employs%20a%20scale-recovered%20monocular%20depth%20estimator%20%28e.g.%2C%20Metric3D%29%20to%20generate%20depth%20priors%2C%20leverages%20defocus%20convolution%20to%20synthesize%20physically%20accurate%20defocused%20images%2C%20and%20enforces%20geometric%20consistency%20through%20a%20novel%20depth-of-field%20loss%2C%20thereby%20enhancing%20depth%20fidelity%20in%20both%20far-field%20and%20near-field%20regions%3B%20%282%29%20Multi-View%20Consistency%20Supervision%20employing%20LoFTR-based%20semi-dense%20feature%20matching%20to%20minimize%20cross-view%20geometric%20errors%20and%20enforce%20depth%20consistency%20via%20least%20squares%20optimization%20of%20reliable%20matched%20points.%20By%20unifying%20defocus%20physics%20with%20multi-view%20geometric%20constraints%2C%20our%20method%20achieves%20superior%20depth%20fidelity%2C%20demonstrating%20a%200.8%20dB%20PSNR%20improvement%20over%20the%20state-of-the-art%20method%20on%20the%20Waymo%20Open%20Dataset.%20This%20framework%20bridges%20physical%20imaging%20principles%20and%20learning-based%20depth%20regularization%2C%20offering%20a%20scalable%20solution%20for%20complex%20depth%20stratification%20in%20urban%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.10316v1&entry.124074799=Read"},
{"title": "Depth Anything 3: Recovering the Visual Space from Any Views", "author": "Haotong Lin and Sili Chen and Junhao Liew and Donny Y. Chen and Zhenyu Li and Guang Shi and Jiashi Feng and Bingyi Kang", "abstract": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.", "link": "http://arxiv.org/abs/2511.10647v1", "date": "2025-11-13", "relevancy": 3.193, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6431}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Anything%203%3A%20Recovering%20the%20Visual%20Space%20from%20Any%20Views&body=Title%3A%20Depth%20Anything%203%3A%20Recovering%20the%20Visual%20Space%20from%20Any%20Views%0AAuthor%3A%20Haotong%20Lin%20and%20Sili%20Chen%20and%20Junhao%20Liew%20and%20Donny%20Y.%20Chen%20and%20Zhenyu%20Li%20and%20Guang%20Shi%20and%20Jiashi%20Feng%20and%20Bingyi%20Kang%0AAbstract%3A%20We%20present%20Depth%20Anything%203%20%28DA3%29%2C%20a%20model%20that%20predicts%20spatially%20consistent%20geometry%20from%20an%20arbitrary%20number%20of%20visual%20inputs%2C%20with%20or%20without%20known%20camera%20poses.%20In%20pursuit%20of%20minimal%20modeling%2C%20DA3%20yields%20two%20key%20insights%3A%20a%20single%20plain%20transformer%20%28e.g.%2C%20vanilla%20DINO%20encoder%29%20is%20sufficient%20as%20a%20backbone%20without%20architectural%20specialization%2C%20and%20a%20singular%20depth-ray%20prediction%20target%20obviates%20the%20need%20for%20complex%20multi-task%20learning.%20Through%20our%20teacher-student%20training%20paradigm%2C%20the%20model%20achieves%20a%20level%20of%20detail%20and%20generalization%20on%20par%20with%20Depth%20Anything%202%20%28DA2%29.%20We%20establish%20a%20new%20visual%20geometry%20benchmark%20covering%20camera%20pose%20estimation%2C%20any-view%20geometry%20and%20visual%20rendering.%20On%20this%20benchmark%2C%20DA3%20sets%20a%20new%20state-of-the-art%20across%20all%20tasks%2C%20surpassing%20prior%20SOTA%20VGGT%20by%20an%20average%20of%2044.3%25%20in%20camera%20pose%20accuracy%20and%2025.1%25%20in%20geometric%20accuracy.%20Moreover%2C%20it%20outperforms%20DA2%20in%20monocular%20depth%20estimation.%20All%20models%20are%20trained%20exclusively%20on%20public%20academic%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Anything%25203%253A%2520Recovering%2520the%2520Visual%2520Space%2520from%2520Any%2520Views%26entry.906535625%3DHaotong%2520Lin%2520and%2520Sili%2520Chen%2520and%2520Junhao%2520Liew%2520and%2520Donny%2520Y.%2520Chen%2520and%2520Zhenyu%2520Li%2520and%2520Guang%2520Shi%2520and%2520Jiashi%2520Feng%2520and%2520Bingyi%2520Kang%26entry.1292438233%3DWe%2520present%2520Depth%2520Anything%25203%2520%2528DA3%2529%252C%2520a%2520model%2520that%2520predicts%2520spatially%2520consistent%2520geometry%2520from%2520an%2520arbitrary%2520number%2520of%2520visual%2520inputs%252C%2520with%2520or%2520without%2520known%2520camera%2520poses.%2520In%2520pursuit%2520of%2520minimal%2520modeling%252C%2520DA3%2520yields%2520two%2520key%2520insights%253A%2520a%2520single%2520plain%2520transformer%2520%2528e.g.%252C%2520vanilla%2520DINO%2520encoder%2529%2520is%2520sufficient%2520as%2520a%2520backbone%2520without%2520architectural%2520specialization%252C%2520and%2520a%2520singular%2520depth-ray%2520prediction%2520target%2520obviates%2520the%2520need%2520for%2520complex%2520multi-task%2520learning.%2520Through%2520our%2520teacher-student%2520training%2520paradigm%252C%2520the%2520model%2520achieves%2520a%2520level%2520of%2520detail%2520and%2520generalization%2520on%2520par%2520with%2520Depth%2520Anything%25202%2520%2528DA2%2529.%2520We%2520establish%2520a%2520new%2520visual%2520geometry%2520benchmark%2520covering%2520camera%2520pose%2520estimation%252C%2520any-view%2520geometry%2520and%2520visual%2520rendering.%2520On%2520this%2520benchmark%252C%2520DA3%2520sets%2520a%2520new%2520state-of-the-art%2520across%2520all%2520tasks%252C%2520surpassing%2520prior%2520SOTA%2520VGGT%2520by%2520an%2520average%2520of%252044.3%2525%2520in%2520camera%2520pose%2520accuracy%2520and%252025.1%2525%2520in%2520geometric%2520accuracy.%2520Moreover%252C%2520it%2520outperforms%2520DA2%2520in%2520monocular%2520depth%2520estimation.%2520All%2520models%2520are%2520trained%2520exclusively%2520on%2520public%2520academic%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Anything%203%3A%20Recovering%20the%20Visual%20Space%20from%20Any%20Views&entry.906535625=Haotong%20Lin%20and%20Sili%20Chen%20and%20Junhao%20Liew%20and%20Donny%20Y.%20Chen%20and%20Zhenyu%20Li%20and%20Guang%20Shi%20and%20Jiashi%20Feng%20and%20Bingyi%20Kang&entry.1292438233=We%20present%20Depth%20Anything%203%20%28DA3%29%2C%20a%20model%20that%20predicts%20spatially%20consistent%20geometry%20from%20an%20arbitrary%20number%20of%20visual%20inputs%2C%20with%20or%20without%20known%20camera%20poses.%20In%20pursuit%20of%20minimal%20modeling%2C%20DA3%20yields%20two%20key%20insights%3A%20a%20single%20plain%20transformer%20%28e.g.%2C%20vanilla%20DINO%20encoder%29%20is%20sufficient%20as%20a%20backbone%20without%20architectural%20specialization%2C%20and%20a%20singular%20depth-ray%20prediction%20target%20obviates%20the%20need%20for%20complex%20multi-task%20learning.%20Through%20our%20teacher-student%20training%20paradigm%2C%20the%20model%20achieves%20a%20level%20of%20detail%20and%20generalization%20on%20par%20with%20Depth%20Anything%202%20%28DA2%29.%20We%20establish%20a%20new%20visual%20geometry%20benchmark%20covering%20camera%20pose%20estimation%2C%20any-view%20geometry%20and%20visual%20rendering.%20On%20this%20benchmark%2C%20DA3%20sets%20a%20new%20state-of-the-art%20across%20all%20tasks%2C%20surpassing%20prior%20SOTA%20VGGT%20by%20an%20average%20of%2044.3%25%20in%20camera%20pose%20accuracy%20and%2025.1%25%20in%20geometric%20accuracy.%20Moreover%2C%20it%20outperforms%20DA2%20in%20monocular%20depth%20estimation.%20All%20models%20are%20trained%20exclusively%20on%20public%20academic%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.10647v1&entry.124074799=Read"},
{"title": "Remodeling Semantic Relationships in Vision-Language Fine-Tuning", "author": "Xiangyang Wu and Liu Liu and Baosheng Yu and Jiayan Qiu and Zhenwei Shi", "abstract": "Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.", "link": "http://arxiv.org/abs/2511.08238v2", "date": "2025-11-13", "relevancy": 3.0693, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remodeling%20Semantic%20Relationships%20in%20Vision-Language%20Fine-Tuning&body=Title%3A%20Remodeling%20Semantic%20Relationships%20in%20Vision-Language%20Fine-Tuning%0AAuthor%3A%20Xiangyang%20Wu%20and%20Liu%20Liu%20and%20Baosheng%20Yu%20and%20Jiayan%20Qiu%20and%20Zhenwei%20Shi%0AAbstract%3A%20Vision-language%20fine-tuning%20has%20emerged%20as%20an%20efficient%20paradigm%20for%20constructing%20multimodal%20foundation%20models.%20While%20textual%20context%20often%20highlights%20semantic%20relationships%20within%20an%20image%2C%20existing%20fine-tuning%20methods%20typically%20overlook%20this%20information%20when%20aligning%20vision%20and%20language%2C%20thus%20leading%20to%20suboptimal%20performance.%20Toward%20solving%20this%20problem%2C%20we%20propose%20a%20method%20that%20can%20improve%20multimodal%20alignment%20and%20fusion%20based%20on%20both%20semantics%20and%20relationships.Specifically%2C%20we%20first%20extract%20multilevel%20semantic%20features%20from%20different%20vision%20encoder%20to%20capture%20more%20visual%20cues%20of%20the%20relationships.%20Then%2C%20we%20learn%20to%20project%20the%20vision%20features%20to%20group%20related%20semantics%2C%20among%20which%20are%20more%20likely%20to%20have%20relationships.%20Finally%2C%20we%20fuse%20the%20visual%20features%20with%20the%20textual%20by%20using%20inheritable%20cross-attention%2C%20where%20we%20globally%20remove%20the%20redundant%20visual%20relationships%20by%20discarding%20visual-language%20feature%20pairs%20with%20low%20correlation.%20We%20evaluate%20our%20proposed%20method%20on%20eight%20foundation%20models%20and%20two%20downstream%20tasks%2C%20visual%20question%20answering%20and%20image%20captioning%2C%20and%20show%20that%20it%20outperforms%20all%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemodeling%2520Semantic%2520Relationships%2520in%2520Vision-Language%2520Fine-Tuning%26entry.906535625%3DXiangyang%2520Wu%2520and%2520Liu%2520Liu%2520and%2520Baosheng%2520Yu%2520and%2520Jiayan%2520Qiu%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3DVision-language%2520fine-tuning%2520has%2520emerged%2520as%2520an%2520efficient%2520paradigm%2520for%2520constructing%2520multimodal%2520foundation%2520models.%2520While%2520textual%2520context%2520often%2520highlights%2520semantic%2520relationships%2520within%2520an%2520image%252C%2520existing%2520fine-tuning%2520methods%2520typically%2520overlook%2520this%2520information%2520when%2520aligning%2520vision%2520and%2520language%252C%2520thus%2520leading%2520to%2520suboptimal%2520performance.%2520Toward%2520solving%2520this%2520problem%252C%2520we%2520propose%2520a%2520method%2520that%2520can%2520improve%2520multimodal%2520alignment%2520and%2520fusion%2520based%2520on%2520both%2520semantics%2520and%2520relationships.Specifically%252C%2520we%2520first%2520extract%2520multilevel%2520semantic%2520features%2520from%2520different%2520vision%2520encoder%2520to%2520capture%2520more%2520visual%2520cues%2520of%2520the%2520relationships.%2520Then%252C%2520we%2520learn%2520to%2520project%2520the%2520vision%2520features%2520to%2520group%2520related%2520semantics%252C%2520among%2520which%2520are%2520more%2520likely%2520to%2520have%2520relationships.%2520Finally%252C%2520we%2520fuse%2520the%2520visual%2520features%2520with%2520the%2520textual%2520by%2520using%2520inheritable%2520cross-attention%252C%2520where%2520we%2520globally%2520remove%2520the%2520redundant%2520visual%2520relationships%2520by%2520discarding%2520visual-language%2520feature%2520pairs%2520with%2520low%2520correlation.%2520We%2520evaluate%2520our%2520proposed%2520method%2520on%2520eight%2520foundation%2520models%2520and%2520two%2520downstream%2520tasks%252C%2520visual%2520question%2520answering%2520and%2520image%2520captioning%252C%2520and%2520show%2520that%2520it%2520outperforms%2520all%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remodeling%20Semantic%20Relationships%20in%20Vision-Language%20Fine-Tuning&entry.906535625=Xiangyang%20Wu%20and%20Liu%20Liu%20and%20Baosheng%20Yu%20and%20Jiayan%20Qiu%20and%20Zhenwei%20Shi&entry.1292438233=Vision-language%20fine-tuning%20has%20emerged%20as%20an%20efficient%20paradigm%20for%20constructing%20multimodal%20foundation%20models.%20While%20textual%20context%20often%20highlights%20semantic%20relationships%20within%20an%20image%2C%20existing%20fine-tuning%20methods%20typically%20overlook%20this%20information%20when%20aligning%20vision%20and%20language%2C%20thus%20leading%20to%20suboptimal%20performance.%20Toward%20solving%20this%20problem%2C%20we%20propose%20a%20method%20that%20can%20improve%20multimodal%20alignment%20and%20fusion%20based%20on%20both%20semantics%20and%20relationships.Specifically%2C%20we%20first%20extract%20multilevel%20semantic%20features%20from%20different%20vision%20encoder%20to%20capture%20more%20visual%20cues%20of%20the%20relationships.%20Then%2C%20we%20learn%20to%20project%20the%20vision%20features%20to%20group%20related%20semantics%2C%20among%20which%20are%20more%20likely%20to%20have%20relationships.%20Finally%2C%20we%20fuse%20the%20visual%20features%20with%20the%20textual%20by%20using%20inheritable%20cross-attention%2C%20where%20we%20globally%20remove%20the%20redundant%20visual%20relationships%20by%20discarding%20visual-language%20feature%20pairs%20with%20low%20correlation.%20We%20evaluate%20our%20proposed%20method%20on%20eight%20foundation%20models%20and%20two%20downstream%20tasks%2C%20visual%20question%20answering%20and%20image%20captioning%2C%20and%20show%20that%20it%20outperforms%20all%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.08238v2&entry.124074799=Read"},
{"title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded", "author": "Haosong Peng and Hao Li and Yalun Dai and Yushi Lan and Yihang Luo and Tianyu Qi and Zhengshen Zhang and Yufeng Zhan and Junfei Zhang and Wenchao Xu and Ziwei Liu", "abstract": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.", "link": "http://arxiv.org/abs/2511.10560v1", "date": "2025-11-13", "relevancy": 3.0386, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniVGGT%3A%20Omni-Modality%20Driven%20Visual%20Geometry%20Grounded&body=Title%3A%20OmniVGGT%3A%20Omni-Modality%20Driven%20Visual%20Geometry%20Grounded%0AAuthor%3A%20Haosong%20Peng%20and%20Hao%20Li%20and%20Yalun%20Dai%20and%20Yushi%20Lan%20and%20Yihang%20Luo%20and%20Tianyu%20Qi%20and%20Zhengshen%20Zhang%20and%20Yufeng%20Zhan%20and%20Junfei%20Zhang%20and%20Wenchao%20Xu%20and%20Ziwei%20Liu%0AAbstract%3A%20General%203D%20foundation%20models%20have%20started%20to%20lead%20the%20trend%20of%20unifying%20diverse%20vision%20tasks%2C%20yet%20most%20assume%20RGB-only%20inputs%20and%20ignore%20readily%20available%20geometric%20cues%20%28e.g.%2C%20camera%20intrinsics%2C%20poses%2C%20and%20depth%20maps%29.%20To%20address%20this%20issue%2C%20we%20introduce%20OmniVGGT%2C%20a%20novel%20framework%20that%20can%20effectively%20benefit%20from%20an%20arbitrary%20number%20of%20auxiliary%20geometric%20modalities%20during%20both%20training%20and%20inference.%20In%20our%20framework%2C%20a%20GeoAdapter%20is%20proposed%20to%20encode%20depth%20and%20camera%20intrinsics/extrinsics%20into%20a%20spatial%20foundation%20model.%20It%20employs%20zero-initialized%20convolutions%20to%20progressively%20inject%20geometric%20information%20without%20disrupting%20the%20foundation%20model%27s%20representation%20space.%20This%20design%20ensures%20stable%20optimization%20with%20negligible%20overhead%2C%20maintaining%20inference%20speed%20comparable%20to%20VGGT%20even%20with%20multiple%20additional%20inputs.%20Additionally%2C%20a%20stochastic%20multimodal%20fusion%20regimen%20is%20proposed%2C%20which%20randomly%20samples%20modality%20subsets%20per%20instance%20during%20training.%20This%20enables%20an%20arbitrary%20number%20of%20modality%20inputs%20during%20testing%20and%20promotes%20learning%20robust%20spatial%20representations%20instead%20of%20overfitting%20to%20auxiliary%20cues.%20Comprehensive%20experiments%20on%20monocular/multi-view%20depth%20estimation%2C%20multi-view%20stereo%2C%20and%20camera%20pose%20estimation%20demonstrate%20that%20OmniVGGT%20outperforms%20prior%20methods%20with%20auxiliary%20inputs%20and%20achieves%20state-of-the-art%20results%20even%20with%20RGB-only%20input.%20To%20further%20highlight%20its%20practical%20utility%2C%20we%20integrated%20OmniVGGT%20into%20vision-language-action%20%28VLA%29%20models.%20The%20enhanced%20VLA%20model%20by%20OmniVGGT%20not%20only%20outperforms%20the%20vanilla%20point-cloud-based%20baseline%20on%20mainstream%20benchmarks%2C%20but%20also%20effectively%20leverages%20accessible%20auxiliary%20inputs%20to%20achieve%20consistent%20gains%20on%20robotic%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniVGGT%253A%2520Omni-Modality%2520Driven%2520Visual%2520Geometry%2520Grounded%26entry.906535625%3DHaosong%2520Peng%2520and%2520Hao%2520Li%2520and%2520Yalun%2520Dai%2520and%2520Yushi%2520Lan%2520and%2520Yihang%2520Luo%2520and%2520Tianyu%2520Qi%2520and%2520Zhengshen%2520Zhang%2520and%2520Yufeng%2520Zhan%2520and%2520Junfei%2520Zhang%2520and%2520Wenchao%2520Xu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DGeneral%25203D%2520foundation%2520models%2520have%2520started%2520to%2520lead%2520the%2520trend%2520of%2520unifying%2520diverse%2520vision%2520tasks%252C%2520yet%2520most%2520assume%2520RGB-only%2520inputs%2520and%2520ignore%2520readily%2520available%2520geometric%2520cues%2520%2528e.g.%252C%2520camera%2520intrinsics%252C%2520poses%252C%2520and%2520depth%2520maps%2529.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520OmniVGGT%252C%2520a%2520novel%2520framework%2520that%2520can%2520effectively%2520benefit%2520from%2520an%2520arbitrary%2520number%2520of%2520auxiliary%2520geometric%2520modalities%2520during%2520both%2520training%2520and%2520inference.%2520In%2520our%2520framework%252C%2520a%2520GeoAdapter%2520is%2520proposed%2520to%2520encode%2520depth%2520and%2520camera%2520intrinsics/extrinsics%2520into%2520a%2520spatial%2520foundation%2520model.%2520It%2520employs%2520zero-initialized%2520convolutions%2520to%2520progressively%2520inject%2520geometric%2520information%2520without%2520disrupting%2520the%2520foundation%2520model%2527s%2520representation%2520space.%2520This%2520design%2520ensures%2520stable%2520optimization%2520with%2520negligible%2520overhead%252C%2520maintaining%2520inference%2520speed%2520comparable%2520to%2520VGGT%2520even%2520with%2520multiple%2520additional%2520inputs.%2520Additionally%252C%2520a%2520stochastic%2520multimodal%2520fusion%2520regimen%2520is%2520proposed%252C%2520which%2520randomly%2520samples%2520modality%2520subsets%2520per%2520instance%2520during%2520training.%2520This%2520enables%2520an%2520arbitrary%2520number%2520of%2520modality%2520inputs%2520during%2520testing%2520and%2520promotes%2520learning%2520robust%2520spatial%2520representations%2520instead%2520of%2520overfitting%2520to%2520auxiliary%2520cues.%2520Comprehensive%2520experiments%2520on%2520monocular/multi-view%2520depth%2520estimation%252C%2520multi-view%2520stereo%252C%2520and%2520camera%2520pose%2520estimation%2520demonstrate%2520that%2520OmniVGGT%2520outperforms%2520prior%2520methods%2520with%2520auxiliary%2520inputs%2520and%2520achieves%2520state-of-the-art%2520results%2520even%2520with%2520RGB-only%2520input.%2520To%2520further%2520highlight%2520its%2520practical%2520utility%252C%2520we%2520integrated%2520OmniVGGT%2520into%2520vision-language-action%2520%2528VLA%2529%2520models.%2520The%2520enhanced%2520VLA%2520model%2520by%2520OmniVGGT%2520not%2520only%2520outperforms%2520the%2520vanilla%2520point-cloud-based%2520baseline%2520on%2520mainstream%2520benchmarks%252C%2520but%2520also%2520effectively%2520leverages%2520accessible%2520auxiliary%2520inputs%2520to%2520achieve%2520consistent%2520gains%2520on%2520robotic%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniVGGT%3A%20Omni-Modality%20Driven%20Visual%20Geometry%20Grounded&entry.906535625=Haosong%20Peng%20and%20Hao%20Li%20and%20Yalun%20Dai%20and%20Yushi%20Lan%20and%20Yihang%20Luo%20and%20Tianyu%20Qi%20and%20Zhengshen%20Zhang%20and%20Yufeng%20Zhan%20and%20Junfei%20Zhang%20and%20Wenchao%20Xu%20and%20Ziwei%20Liu&entry.1292438233=General%203D%20foundation%20models%20have%20started%20to%20lead%20the%20trend%20of%20unifying%20diverse%20vision%20tasks%2C%20yet%20most%20assume%20RGB-only%20inputs%20and%20ignore%20readily%20available%20geometric%20cues%20%28e.g.%2C%20camera%20intrinsics%2C%20poses%2C%20and%20depth%20maps%29.%20To%20address%20this%20issue%2C%20we%20introduce%20OmniVGGT%2C%20a%20novel%20framework%20that%20can%20effectively%20benefit%20from%20an%20arbitrary%20number%20of%20auxiliary%20geometric%20modalities%20during%20both%20training%20and%20inference.%20In%20our%20framework%2C%20a%20GeoAdapter%20is%20proposed%20to%20encode%20depth%20and%20camera%20intrinsics/extrinsics%20into%20a%20spatial%20foundation%20model.%20It%20employs%20zero-initialized%20convolutions%20to%20progressively%20inject%20geometric%20information%20without%20disrupting%20the%20foundation%20model%27s%20representation%20space.%20This%20design%20ensures%20stable%20optimization%20with%20negligible%20overhead%2C%20maintaining%20inference%20speed%20comparable%20to%20VGGT%20even%20with%20multiple%20additional%20inputs.%20Additionally%2C%20a%20stochastic%20multimodal%20fusion%20regimen%20is%20proposed%2C%20which%20randomly%20samples%20modality%20subsets%20per%20instance%20during%20training.%20This%20enables%20an%20arbitrary%20number%20of%20modality%20inputs%20during%20testing%20and%20promotes%20learning%20robust%20spatial%20representations%20instead%20of%20overfitting%20to%20auxiliary%20cues.%20Comprehensive%20experiments%20on%20monocular/multi-view%20depth%20estimation%2C%20multi-view%20stereo%2C%20and%20camera%20pose%20estimation%20demonstrate%20that%20OmniVGGT%20outperforms%20prior%20methods%20with%20auxiliary%20inputs%20and%20achieves%20state-of-the-art%20results%20even%20with%20RGB-only%20input.%20To%20further%20highlight%20its%20practical%20utility%2C%20we%20integrated%20OmniVGGT%20into%20vision-language-action%20%28VLA%29%20models.%20The%20enhanced%20VLA%20model%20by%20OmniVGGT%20not%20only%20outperforms%20the%20vanilla%20point-cloud-based%20baseline%20on%20mainstream%20benchmarks%2C%20but%20also%20effectively%20leverages%20accessible%20auxiliary%20inputs%20to%20achieve%20consistent%20gains%20on%20robotic%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.10560v1&entry.124074799=Read"},
{"title": "Generating Attribute-Aware Human Motions from Textual Prompt", "author": "Xinghan Wang and Kun Xu and Fei Li and Cao Sheng and Jiazhong Yu and Yadong Mu", "abstract": "Text-driven human motion generation has recently attracted considerable attention, allowing models to generate human motions based on textual descriptions. However, current methods neglect the influence of human attributes-such as age, gender, weight, and height-which are key factors shaping human motion patterns. This work represents a pilot exploration for bridging this gap. We conceptualize each motion as comprising both attribute information and action semantics, where textual descriptions align exclusively with action semantics. To achieve this, a new framework inspired by Structural Causal Models is proposed to decouple action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled generation. The resulting model is capable of generating attribute-aware motion aligned with the user's text and attribute inputs. For evaluation, we introduce a comprehensive dataset containing attribute annotations for text-motion pairs, setting the first benchmark for attribute-aware motion generation. Extensive experiments validate our model's effectiveness.", "link": "http://arxiv.org/abs/2506.21912v2", "date": "2025-11-13", "relevancy": 2.9903, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6139}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5972}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Attribute-Aware%20Human%20Motions%20from%20Textual%20Prompt&body=Title%3A%20Generating%20Attribute-Aware%20Human%20Motions%20from%20Textual%20Prompt%0AAuthor%3A%20Xinghan%20Wang%20and%20Kun%20Xu%20and%20Fei%20Li%20and%20Cao%20Sheng%20and%20Jiazhong%20Yu%20and%20Yadong%20Mu%0AAbstract%3A%20Text-driven%20human%20motion%20generation%20has%20recently%20attracted%20considerable%20attention%2C%20allowing%20models%20to%20generate%20human%20motions%20based%20on%20textual%20descriptions.%20However%2C%20current%20methods%20neglect%20the%20influence%20of%20human%20attributes-such%20as%20age%2C%20gender%2C%20weight%2C%20and%20height-which%20are%20key%20factors%20shaping%20human%20motion%20patterns.%20This%20work%20represents%20a%20pilot%20exploration%20for%20bridging%20this%20gap.%20We%20conceptualize%20each%20motion%20as%20comprising%20both%20attribute%20information%20and%20action%20semantics%2C%20where%20textual%20descriptions%20align%20exclusively%20with%20action%20semantics.%20To%20achieve%20this%2C%20a%20new%20framework%20inspired%20by%20Structural%20Causal%20Models%20is%20proposed%20to%20decouple%20action%20semantics%20from%20human%20attributes%2C%20enabling%20text-to-semantics%20prediction%20and%20attribute-controlled%20generation.%20The%20resulting%20model%20is%20capable%20of%20generating%20attribute-aware%20motion%20aligned%20with%20the%20user%27s%20text%20and%20attribute%20inputs.%20For%20evaluation%2C%20we%20introduce%20a%20comprehensive%20dataset%20containing%20attribute%20annotations%20for%20text-motion%20pairs%2C%20setting%20the%20first%20benchmark%20for%20attribute-aware%20motion%20generation.%20Extensive%20experiments%20validate%20our%20model%27s%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2506.21912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Attribute-Aware%2520Human%2520Motions%2520from%2520Textual%2520Prompt%26entry.906535625%3DXinghan%2520Wang%2520and%2520Kun%2520Xu%2520and%2520Fei%2520Li%2520and%2520Cao%2520Sheng%2520and%2520Jiazhong%2520Yu%2520and%2520Yadong%2520Mu%26entry.1292438233%3DText-driven%2520human%2520motion%2520generation%2520has%2520recently%2520attracted%2520considerable%2520attention%252C%2520allowing%2520models%2520to%2520generate%2520human%2520motions%2520based%2520on%2520textual%2520descriptions.%2520However%252C%2520current%2520methods%2520neglect%2520the%2520influence%2520of%2520human%2520attributes-such%2520as%2520age%252C%2520gender%252C%2520weight%252C%2520and%2520height-which%2520are%2520key%2520factors%2520shaping%2520human%2520motion%2520patterns.%2520This%2520work%2520represents%2520a%2520pilot%2520exploration%2520for%2520bridging%2520this%2520gap.%2520We%2520conceptualize%2520each%2520motion%2520as%2520comprising%2520both%2520attribute%2520information%2520and%2520action%2520semantics%252C%2520where%2520textual%2520descriptions%2520align%2520exclusively%2520with%2520action%2520semantics.%2520To%2520achieve%2520this%252C%2520a%2520new%2520framework%2520inspired%2520by%2520Structural%2520Causal%2520Models%2520is%2520proposed%2520to%2520decouple%2520action%2520semantics%2520from%2520human%2520attributes%252C%2520enabling%2520text-to-semantics%2520prediction%2520and%2520attribute-controlled%2520generation.%2520The%2520resulting%2520model%2520is%2520capable%2520of%2520generating%2520attribute-aware%2520motion%2520aligned%2520with%2520the%2520user%2527s%2520text%2520and%2520attribute%2520inputs.%2520For%2520evaluation%252C%2520we%2520introduce%2520a%2520comprehensive%2520dataset%2520containing%2520attribute%2520annotations%2520for%2520text-motion%2520pairs%252C%2520setting%2520the%2520first%2520benchmark%2520for%2520attribute-aware%2520motion%2520generation.%2520Extensive%2520experiments%2520validate%2520our%2520model%2527s%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Attribute-Aware%20Human%20Motions%20from%20Textual%20Prompt&entry.906535625=Xinghan%20Wang%20and%20Kun%20Xu%20and%20Fei%20Li%20and%20Cao%20Sheng%20and%20Jiazhong%20Yu%20and%20Yadong%20Mu&entry.1292438233=Text-driven%20human%20motion%20generation%20has%20recently%20attracted%20considerable%20attention%2C%20allowing%20models%20to%20generate%20human%20motions%20based%20on%20textual%20descriptions.%20However%2C%20current%20methods%20neglect%20the%20influence%20of%20human%20attributes-such%20as%20age%2C%20gender%2C%20weight%2C%20and%20height-which%20are%20key%20factors%20shaping%20human%20motion%20patterns.%20This%20work%20represents%20a%20pilot%20exploration%20for%20bridging%20this%20gap.%20We%20conceptualize%20each%20motion%20as%20comprising%20both%20attribute%20information%20and%20action%20semantics%2C%20where%20textual%20descriptions%20align%20exclusively%20with%20action%20semantics.%20To%20achieve%20this%2C%20a%20new%20framework%20inspired%20by%20Structural%20Causal%20Models%20is%20proposed%20to%20decouple%20action%20semantics%20from%20human%20attributes%2C%20enabling%20text-to-semantics%20prediction%20and%20attribute-controlled%20generation.%20The%20resulting%20model%20is%20capable%20of%20generating%20attribute-aware%20motion%20aligned%20with%20the%20user%27s%20text%20and%20attribute%20inputs.%20For%20evaluation%2C%20we%20introduce%20a%20comprehensive%20dataset%20containing%20attribute%20annotations%20for%20text-motion%20pairs%2C%20setting%20the%20first%20benchmark%20for%20attribute-aware%20motion%20generation.%20Extensive%20experiments%20validate%20our%20model%27s%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2506.21912v2&entry.124074799=Read"},
{"title": "Rethinking Visual Information Processing in Multimodal LLMs", "author": "Dongwan Kim and Viresh Ranjan and Takashi Nagata and Arnab Dhua and Amit Kumar K C", "abstract": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.", "link": "http://arxiv.org/abs/2511.10301v1", "date": "2025-11-13", "relevancy": 2.937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5885}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Visual%20Information%20Processing%20in%20Multimodal%20LLMs&body=Title%3A%20Rethinking%20Visual%20Information%20Processing%20in%20Multimodal%20LLMs%0AAuthor%3A%20Dongwan%20Kim%20and%20Viresh%20Ranjan%20and%20Takashi%20Nagata%20and%20Arnab%20Dhua%20and%20Amit%20Kumar%20K%20C%0AAbstract%3A%20Despite%20the%20remarkable%20success%20of%20the%20LLaVA%20architecture%20for%20vision-language%20tasks%2C%20its%20design%20inherently%20struggles%20to%20effectively%20integrate%20visual%20features%20due%20to%20the%20inherent%20mismatch%20between%20text%20and%20vision%20modalities.%20We%20tackle%20this%20issue%20from%20a%20novel%20perspective%20in%20which%20the%20LLM%20not%20only%20serves%20as%20a%20language%20model%20but%20also%20a%20powerful%20vision%20encoder.%20To%20this%20end%2C%20we%20present%20LLaViT%20-%20Large%20Language%20Models%20as%20extended%20Vision%20Transformers%20-%20which%20enables%20the%20LLM%20to%20simultaneously%20function%20as%20a%20vision%20encoder%20through%20three%20key%20modifications%3A%20%281%29%20learning%20separate%20QKV%20projections%20for%20vision%20modality%2C%20%282%29%20enabling%20bidirectional%20attention%20on%20visual%20tokens%2C%20and%20%283%29%20incorporating%20both%20global%20and%20local%20visual%20representations.%20Through%20extensive%20controlled%20experiments%20on%20a%20wide%20range%20of%20LLMs%2C%20we%20demonstrate%20that%20LLaViT%20significantly%20outperforms%20the%20baseline%20LLaVA%20method%20on%20a%20multitude%20of%20benchmarks%2C%20even%20surpassing%20models%20with%20double%20its%20parameter%20count%2C%20establishing%20a%20more%20effective%20approach%20to%20vision-language%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Visual%2520Information%2520Processing%2520in%2520Multimodal%2520LLMs%26entry.906535625%3DDongwan%2520Kim%2520and%2520Viresh%2520Ranjan%2520and%2520Takashi%2520Nagata%2520and%2520Arnab%2520Dhua%2520and%2520Amit%2520Kumar%2520K%2520C%26entry.1292438233%3DDespite%2520the%2520remarkable%2520success%2520of%2520the%2520LLaVA%2520architecture%2520for%2520vision-language%2520tasks%252C%2520its%2520design%2520inherently%2520struggles%2520to%2520effectively%2520integrate%2520visual%2520features%2520due%2520to%2520the%2520inherent%2520mismatch%2520between%2520text%2520and%2520vision%2520modalities.%2520We%2520tackle%2520this%2520issue%2520from%2520a%2520novel%2520perspective%2520in%2520which%2520the%2520LLM%2520not%2520only%2520serves%2520as%2520a%2520language%2520model%2520but%2520also%2520a%2520powerful%2520vision%2520encoder.%2520To%2520this%2520end%252C%2520we%2520present%2520LLaViT%2520-%2520Large%2520Language%2520Models%2520as%2520extended%2520Vision%2520Transformers%2520-%2520which%2520enables%2520the%2520LLM%2520to%2520simultaneously%2520function%2520as%2520a%2520vision%2520encoder%2520through%2520three%2520key%2520modifications%253A%2520%25281%2529%2520learning%2520separate%2520QKV%2520projections%2520for%2520vision%2520modality%252C%2520%25282%2529%2520enabling%2520bidirectional%2520attention%2520on%2520visual%2520tokens%252C%2520and%2520%25283%2529%2520incorporating%2520both%2520global%2520and%2520local%2520visual%2520representations.%2520Through%2520extensive%2520controlled%2520experiments%2520on%2520a%2520wide%2520range%2520of%2520LLMs%252C%2520we%2520demonstrate%2520that%2520LLaViT%2520significantly%2520outperforms%2520the%2520baseline%2520LLaVA%2520method%2520on%2520a%2520multitude%2520of%2520benchmarks%252C%2520even%2520surpassing%2520models%2520with%2520double%2520its%2520parameter%2520count%252C%2520establishing%2520a%2520more%2520effective%2520approach%2520to%2520vision-language%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Visual%20Information%20Processing%20in%20Multimodal%20LLMs&entry.906535625=Dongwan%20Kim%20and%20Viresh%20Ranjan%20and%20Takashi%20Nagata%20and%20Arnab%20Dhua%20and%20Amit%20Kumar%20K%20C&entry.1292438233=Despite%20the%20remarkable%20success%20of%20the%20LLaVA%20architecture%20for%20vision-language%20tasks%2C%20its%20design%20inherently%20struggles%20to%20effectively%20integrate%20visual%20features%20due%20to%20the%20inherent%20mismatch%20between%20text%20and%20vision%20modalities.%20We%20tackle%20this%20issue%20from%20a%20novel%20perspective%20in%20which%20the%20LLM%20not%20only%20serves%20as%20a%20language%20model%20but%20also%20a%20powerful%20vision%20encoder.%20To%20this%20end%2C%20we%20present%20LLaViT%20-%20Large%20Language%20Models%20as%20extended%20Vision%20Transformers%20-%20which%20enables%20the%20LLM%20to%20simultaneously%20function%20as%20a%20vision%20encoder%20through%20three%20key%20modifications%3A%20%281%29%20learning%20separate%20QKV%20projections%20for%20vision%20modality%2C%20%282%29%20enabling%20bidirectional%20attention%20on%20visual%20tokens%2C%20and%20%283%29%20incorporating%20both%20global%20and%20local%20visual%20representations.%20Through%20extensive%20controlled%20experiments%20on%20a%20wide%20range%20of%20LLMs%2C%20we%20demonstrate%20that%20LLaViT%20significantly%20outperforms%20the%20baseline%20LLaVA%20method%20on%20a%20multitude%20of%20benchmarks%2C%20even%20surpassing%20models%20with%20double%20its%20parameter%20count%2C%20establishing%20a%20more%20effective%20approach%20to%20vision-language%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.10301v1&entry.124074799=Read"},
{"title": "Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment", "author": "Wenti Yin and Huaxin Zhang and Xiang Wang and Yuqing Lu and Yicheng Zhang and Bingquan Gong and Jialong Zuo and Li Yu and Changxin Gao and Nong Sang", "abstract": "Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.10334v1", "date": "2025-11-13", "relevancy": 2.9159, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Tell%20Apart%3A%20Weakly%20Supervised%20Video%20Anomaly%20Detection%20via%20Disentangled%20Semantic%20Alignment&body=Title%3A%20Learning%20to%20Tell%20Apart%3A%20Weakly%20Supervised%20Video%20Anomaly%20Detection%20via%20Disentangled%20Semantic%20Alignment%0AAuthor%3A%20Wenti%20Yin%20and%20Huaxin%20Zhang%20and%20Xiang%20Wang%20and%20Yuqing%20Lu%20and%20Yicheng%20Zhang%20and%20Bingquan%20Gong%20and%20Jialong%20Zuo%20and%20Li%20Yu%20and%20Changxin%20Gao%20and%20Nong%20Sang%0AAbstract%3A%20Recent%20advancements%20in%20weakly-supervised%20video%20anomaly%20detection%20have%20achieved%20remarkable%20performance%20by%20applying%20the%20multiple%20instance%20learning%20paradigm%20based%20on%20multimodal%20foundation%20models%20such%20as%20CLIP%20to%20highlight%20anomalous%20instances%20and%20classify%20categories.%20However%2C%20their%20objectives%20may%20tend%20to%20detect%20the%20most%20salient%20response%20segments%2C%20while%20neglecting%20to%20mine%20diverse%20normal%20patterns%20separated%20from%20anomalies%2C%20and%20are%20prone%20to%20category%20confusion%20due%20to%20similar%20appearance%2C%20leading%20to%20unsatisfactory%20fine-grained%20classification%20results.%20Therefore%2C%20we%20propose%20a%20novel%20Disentangled%20Semantic%20Alignment%20Network%20%28DSANet%29%20to%20explicitly%20separate%20abnormal%20and%20normal%20features%20from%20coarse-grained%20and%20fine-grained%20aspects%2C%20enhancing%20the%20distinguishability.%20Specifically%2C%20at%20the%20coarse-grained%20level%2C%20we%20introduce%20a%20self-guided%20normality%20modeling%20branch%20that%20reconstructs%20input%20video%20features%20under%20the%20guidance%20of%20learned%20normal%20prototypes%2C%20encouraging%20the%20model%20to%20exploit%20normality%20cues%20inherent%20in%20the%20video%2C%20thereby%20improving%20the%20temporal%20separation%20of%20normal%20patterns%20and%20anomalous%20events.%20At%20the%20fine-grained%20level%2C%20we%20present%20a%20decoupled%20contrastive%20semantic%20alignment%20mechanism%2C%20which%20first%20temporally%20decomposes%20each%20video%20into%20event-centric%20and%20background-centric%20components%20using%20frame-level%20anomaly%20scores%20and%20then%20applies%20visual-language%20contrastive%20learning%20to%20enhance%20class-discriminative%20representations.%20Comprehensive%20experiments%20on%20two%20standard%20benchmarks%2C%20namely%20XD-Violence%20and%20UCF-Crime%2C%20demonstrate%20that%20DSANet%20outperforms%20existing%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Tell%2520Apart%253A%2520Weakly%2520Supervised%2520Video%2520Anomaly%2520Detection%2520via%2520Disentangled%2520Semantic%2520Alignment%26entry.906535625%3DWenti%2520Yin%2520and%2520Huaxin%2520Zhang%2520and%2520Xiang%2520Wang%2520and%2520Yuqing%2520Lu%2520and%2520Yicheng%2520Zhang%2520and%2520Bingquan%2520Gong%2520and%2520Jialong%2520Zuo%2520and%2520Li%2520Yu%2520and%2520Changxin%2520Gao%2520and%2520Nong%2520Sang%26entry.1292438233%3DRecent%2520advancements%2520in%2520weakly-supervised%2520video%2520anomaly%2520detection%2520have%2520achieved%2520remarkable%2520performance%2520by%2520applying%2520the%2520multiple%2520instance%2520learning%2520paradigm%2520based%2520on%2520multimodal%2520foundation%2520models%2520such%2520as%2520CLIP%2520to%2520highlight%2520anomalous%2520instances%2520and%2520classify%2520categories.%2520However%252C%2520their%2520objectives%2520may%2520tend%2520to%2520detect%2520the%2520most%2520salient%2520response%2520segments%252C%2520while%2520neglecting%2520to%2520mine%2520diverse%2520normal%2520patterns%2520separated%2520from%2520anomalies%252C%2520and%2520are%2520prone%2520to%2520category%2520confusion%2520due%2520to%2520similar%2520appearance%252C%2520leading%2520to%2520unsatisfactory%2520fine-grained%2520classification%2520results.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520Disentangled%2520Semantic%2520Alignment%2520Network%2520%2528DSANet%2529%2520to%2520explicitly%2520separate%2520abnormal%2520and%2520normal%2520features%2520from%2520coarse-grained%2520and%2520fine-grained%2520aspects%252C%2520enhancing%2520the%2520distinguishability.%2520Specifically%252C%2520at%2520the%2520coarse-grained%2520level%252C%2520we%2520introduce%2520a%2520self-guided%2520normality%2520modeling%2520branch%2520that%2520reconstructs%2520input%2520video%2520features%2520under%2520the%2520guidance%2520of%2520learned%2520normal%2520prototypes%252C%2520encouraging%2520the%2520model%2520to%2520exploit%2520normality%2520cues%2520inherent%2520in%2520the%2520video%252C%2520thereby%2520improving%2520the%2520temporal%2520separation%2520of%2520normal%2520patterns%2520and%2520anomalous%2520events.%2520At%2520the%2520fine-grained%2520level%252C%2520we%2520present%2520a%2520decoupled%2520contrastive%2520semantic%2520alignment%2520mechanism%252C%2520which%2520first%2520temporally%2520decomposes%2520each%2520video%2520into%2520event-centric%2520and%2520background-centric%2520components%2520using%2520frame-level%2520anomaly%2520scores%2520and%2520then%2520applies%2520visual-language%2520contrastive%2520learning%2520to%2520enhance%2520class-discriminative%2520representations.%2520Comprehensive%2520experiments%2520on%2520two%2520standard%2520benchmarks%252C%2520namely%2520XD-Violence%2520and%2520UCF-Crime%252C%2520demonstrate%2520that%2520DSANet%2520outperforms%2520existing%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Tell%20Apart%3A%20Weakly%20Supervised%20Video%20Anomaly%20Detection%20via%20Disentangled%20Semantic%20Alignment&entry.906535625=Wenti%20Yin%20and%20Huaxin%20Zhang%20and%20Xiang%20Wang%20and%20Yuqing%20Lu%20and%20Yicheng%20Zhang%20and%20Bingquan%20Gong%20and%20Jialong%20Zuo%20and%20Li%20Yu%20and%20Changxin%20Gao%20and%20Nong%20Sang&entry.1292438233=Recent%20advancements%20in%20weakly-supervised%20video%20anomaly%20detection%20have%20achieved%20remarkable%20performance%20by%20applying%20the%20multiple%20instance%20learning%20paradigm%20based%20on%20multimodal%20foundation%20models%20such%20as%20CLIP%20to%20highlight%20anomalous%20instances%20and%20classify%20categories.%20However%2C%20their%20objectives%20may%20tend%20to%20detect%20the%20most%20salient%20response%20segments%2C%20while%20neglecting%20to%20mine%20diverse%20normal%20patterns%20separated%20from%20anomalies%2C%20and%20are%20prone%20to%20category%20confusion%20due%20to%20similar%20appearance%2C%20leading%20to%20unsatisfactory%20fine-grained%20classification%20results.%20Therefore%2C%20we%20propose%20a%20novel%20Disentangled%20Semantic%20Alignment%20Network%20%28DSANet%29%20to%20explicitly%20separate%20abnormal%20and%20normal%20features%20from%20coarse-grained%20and%20fine-grained%20aspects%2C%20enhancing%20the%20distinguishability.%20Specifically%2C%20at%20the%20coarse-grained%20level%2C%20we%20introduce%20a%20self-guided%20normality%20modeling%20branch%20that%20reconstructs%20input%20video%20features%20under%20the%20guidance%20of%20learned%20normal%20prototypes%2C%20encouraging%20the%20model%20to%20exploit%20normality%20cues%20inherent%20in%20the%20video%2C%20thereby%20improving%20the%20temporal%20separation%20of%20normal%20patterns%20and%20anomalous%20events.%20At%20the%20fine-grained%20level%2C%20we%20present%20a%20decoupled%20contrastive%20semantic%20alignment%20mechanism%2C%20which%20first%20temporally%20decomposes%20each%20video%20into%20event-centric%20and%20background-centric%20components%20using%20frame-level%20anomaly%20scores%20and%20then%20applies%20visual-language%20contrastive%20learning%20to%20enhance%20class-discriminative%20representations.%20Comprehensive%20experiments%20on%20two%20standard%20benchmarks%2C%20namely%20XD-Violence%20and%20UCF-Crime%2C%20demonstrate%20that%20DSANet%20outperforms%20existing%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.10334v1&entry.124074799=Read"},
{"title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "author": "Shihao Ji and Zihui Song", "abstract": "The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.", "link": "http://arxiv.org/abs/2510.16781v2", "date": "2025-11-13", "relevancy": 2.9067, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Xiaoice%3A%20Training-Free%20Video%20Understanding%20via%20Self-Supervised%20Spatio-Temporal%20Clustering%20of%20Semantic%20Features&body=Title%3A%20Xiaoice%3A%20Training-Free%20Video%20Understanding%20via%20Self-Supervised%20Spatio-Temporal%20Clustering%20of%20Semantic%20Features%0AAuthor%3A%20Shihao%20Ji%20and%20Zihui%20Song%0AAbstract%3A%20The%20remarkable%20zero-shot%20reasoning%20capabilities%20of%20large-scale%20Visual%20Language%20Models%20%28VLMs%29%20on%20static%20images%20have%20yet%20to%20be%20fully%20translated%20to%20the%20video%20domain.%20Conventional%20video%20understanding%20models%20often%20rely%20on%20extensive%2C%20task-specific%20training%20on%20annotated%20datasets%2C%20a%20process%20that%20is%20both%20costly%20and%20limited%20in%20scalability.%20This%20paper%20introduces%20a%20novel%2C%20training-free%20framework%20for%20video%20understanding%20that%20circumvents%20end-to-end%20training%20by%20synergistically%20combining%20the%20rich%20semantic%20priors%20of%20pre-trained%20VLMs%20with%20classic%20machine%20learning%20algorithms%20for%20pattern%20discovery.%20Our%20core%20idea%20is%20to%20reframe%20video%20understanding%20as%20a%20self-supervised%20spatio-temporal%20clustering%20problem%20within%20a%20high-dimensional%20semantic%20feature%20space.%20The%20proposed%20pipeline%20first%20transforms%20a%20video%20stream%20into%20a%20semantic%20feature%20trajectory%20using%20the%20frozen%20visual%20encoder%20of%20a%20pre-trained%20VLM.%20Subsequently%2C%20we%20employ%20Kernel%20Temporal%20Segmentation%20%28KTS%29%2C%20a%20robust%20machine%20learning%20technique%2C%20to%20partition%20the%20continuous%20feature%20stream%20into%20discrete%2C%20semantically%20coherent%20event%20segments.%20These%20segments%20are%20then%20subjected%20to%20unsupervised%20density-based%20clustering%20to%20identify%20recurring%20macroscopic%20scenes%20and%20themes%20throughout%20the%20video.%20By%20selecting%20representative%20keyframes%20from%20each%20discovered%20cluster%20and%20leveraging%20the%20VLM%27s%20generative%20capabilities%20for%20textual%20description%2C%20our%20framework%20automatically%20produces%20a%20structured%2C%20multi-modal%20summary%20of%20the%20video%20content.%20This%20approach%20provides%20an%20effective%2C%20interpretable%2C%20and%20model-agnostic%20pathway%20for%20zero-shot%2C%20automated%20structural%20analysis%20of%20video%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16781v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXiaoice%253A%2520Training-Free%2520Video%2520Understanding%2520via%2520Self-Supervised%2520Spatio-Temporal%2520Clustering%2520of%2520Semantic%2520Features%26entry.906535625%3DShihao%2520Ji%2520and%2520Zihui%2520Song%26entry.1292438233%3DThe%2520remarkable%2520zero-shot%2520reasoning%2520capabilities%2520of%2520large-scale%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520on%2520static%2520images%2520have%2520yet%2520to%2520be%2520fully%2520translated%2520to%2520the%2520video%2520domain.%2520Conventional%2520video%2520understanding%2520models%2520often%2520rely%2520on%2520extensive%252C%2520task-specific%2520training%2520on%2520annotated%2520datasets%252C%2520a%2520process%2520that%2520is%2520both%2520costly%2520and%2520limited%2520in%2520scalability.%2520This%2520paper%2520introduces%2520a%2520novel%252C%2520training-free%2520framework%2520for%2520video%2520understanding%2520that%2520circumvents%2520end-to-end%2520training%2520by%2520synergistically%2520combining%2520the%2520rich%2520semantic%2520priors%2520of%2520pre-trained%2520VLMs%2520with%2520classic%2520machine%2520learning%2520algorithms%2520for%2520pattern%2520discovery.%2520Our%2520core%2520idea%2520is%2520to%2520reframe%2520video%2520understanding%2520as%2520a%2520self-supervised%2520spatio-temporal%2520clustering%2520problem%2520within%2520a%2520high-dimensional%2520semantic%2520feature%2520space.%2520The%2520proposed%2520pipeline%2520first%2520transforms%2520a%2520video%2520stream%2520into%2520a%2520semantic%2520feature%2520trajectory%2520using%2520the%2520frozen%2520visual%2520encoder%2520of%2520a%2520pre-trained%2520VLM.%2520Subsequently%252C%2520we%2520employ%2520Kernel%2520Temporal%2520Segmentation%2520%2528KTS%2529%252C%2520a%2520robust%2520machine%2520learning%2520technique%252C%2520to%2520partition%2520the%2520continuous%2520feature%2520stream%2520into%2520discrete%252C%2520semantically%2520coherent%2520event%2520segments.%2520These%2520segments%2520are%2520then%2520subjected%2520to%2520unsupervised%2520density-based%2520clustering%2520to%2520identify%2520recurring%2520macroscopic%2520scenes%2520and%2520themes%2520throughout%2520the%2520video.%2520By%2520selecting%2520representative%2520keyframes%2520from%2520each%2520discovered%2520cluster%2520and%2520leveraging%2520the%2520VLM%2527s%2520generative%2520capabilities%2520for%2520textual%2520description%252C%2520our%2520framework%2520automatically%2520produces%2520a%2520structured%252C%2520multi-modal%2520summary%2520of%2520the%2520video%2520content.%2520This%2520approach%2520provides%2520an%2520effective%252C%2520interpretable%252C%2520and%2520model-agnostic%2520pathway%2520for%2520zero-shot%252C%2520automated%2520structural%2520analysis%2520of%2520video%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16781v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Xiaoice%3A%20Training-Free%20Video%20Understanding%20via%20Self-Supervised%20Spatio-Temporal%20Clustering%20of%20Semantic%20Features&entry.906535625=Shihao%20Ji%20and%20Zihui%20Song&entry.1292438233=The%20remarkable%20zero-shot%20reasoning%20capabilities%20of%20large-scale%20Visual%20Language%20Models%20%28VLMs%29%20on%20static%20images%20have%20yet%20to%20be%20fully%20translated%20to%20the%20video%20domain.%20Conventional%20video%20understanding%20models%20often%20rely%20on%20extensive%2C%20task-specific%20training%20on%20annotated%20datasets%2C%20a%20process%20that%20is%20both%20costly%20and%20limited%20in%20scalability.%20This%20paper%20introduces%20a%20novel%2C%20training-free%20framework%20for%20video%20understanding%20that%20circumvents%20end-to-end%20training%20by%20synergistically%20combining%20the%20rich%20semantic%20priors%20of%20pre-trained%20VLMs%20with%20classic%20machine%20learning%20algorithms%20for%20pattern%20discovery.%20Our%20core%20idea%20is%20to%20reframe%20video%20understanding%20as%20a%20self-supervised%20spatio-temporal%20clustering%20problem%20within%20a%20high-dimensional%20semantic%20feature%20space.%20The%20proposed%20pipeline%20first%20transforms%20a%20video%20stream%20into%20a%20semantic%20feature%20trajectory%20using%20the%20frozen%20visual%20encoder%20of%20a%20pre-trained%20VLM.%20Subsequently%2C%20we%20employ%20Kernel%20Temporal%20Segmentation%20%28KTS%29%2C%20a%20robust%20machine%20learning%20technique%2C%20to%20partition%20the%20continuous%20feature%20stream%20into%20discrete%2C%20semantically%20coherent%20event%20segments.%20These%20segments%20are%20then%20subjected%20to%20unsupervised%20density-based%20clustering%20to%20identify%20recurring%20macroscopic%20scenes%20and%20themes%20throughout%20the%20video.%20By%20selecting%20representative%20keyframes%20from%20each%20discovered%20cluster%20and%20leveraging%20the%20VLM%27s%20generative%20capabilities%20for%20textual%20description%2C%20our%20framework%20automatically%20produces%20a%20structured%2C%20multi-modal%20summary%20of%20the%20video%20content.%20This%20approach%20provides%20an%20effective%2C%20interpretable%2C%20and%20model-agnostic%20pathway%20for%20zero-shot%2C%20automated%20structural%20analysis%20of%20video%20content.&entry.1838667208=http%3A//arxiv.org/abs/2510.16781v2&entry.124074799=Read"},
{"title": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs", "author": "Minye Shao and Sihan Guo and Xinrun Li and Xingyu Miao and Haoran Duan and Yang Long", "abstract": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.", "link": "http://arxiv.org/abs/2511.09540v2", "date": "2025-11-13", "relevancy": 2.8776, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs&body=Title%3A%20vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs%0AAuthor%3A%20Minye%20Shao%20and%20Sihan%20Guo%20and%20Xinrun%20Li%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Long%0AAbstract%3A%20Recent%20advances%20in%20context%20optimization%20%28CoOp%29%20guided%20by%20large%20language%20model%20%28LLM%29-distilled%20medical%20semantic%20priors%20offer%20a%20scalable%20alternative%20to%20manual%20prompt%20engineering%20and%20full%20fine-tuning%20for%20adapting%20biomedical%20CLIP-based%20vision-language%20models%20%28VLMs%29.%20However%2C%20prompt%20learning%20in%20this%20context%20is%20challenged%20by%20semantic%20misalignment%20between%20LLMs%20and%20CLIP%20variants%20due%20to%20divergent%20training%20corpora%20and%20model%20architectures%3B%20it%20further%20lacks%20scalability%20across%20continuously%20evolving%20families%20of%20foundation%20models.%20More%20critically%2C%20pairwise%20multimodal%20alignment%20via%20conventional%20Euclidean-space%20optimization%20lacks%20the%20capacity%20to%20model%20unified%20representations%20or%20apply%20localized%20geometric%20constraints%2C%20which%20tends%20to%20amplify%20modality%20gaps%20in%20complex%20biomedical%20imaging%20and%20destabilize%20few-shot%20adaptation.%20In%20this%20work%2C%20we%20propose%20vMFCoOp%2C%20a%20framework%20that%20inversely%20estimates%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20a%20shared%20Hyperspherical%20Manifold%2C%20aligning%20semantic%20biases%20between%20arbitrary%20LLMs%20and%20CLIP%20backbones%20via%20Unified%20Semantic%20Anchors%20to%20achieve%20robust%20biomedical%20prompting%20and%20superior%20few-shot%20classification.%20Grounded%20in%20three%20complementary%20constraints%2C%20vMFCoOp%20demonstrates%20consistent%20improvements%20across%2014%20medical%20datasets%2C%2012%20medical%20imaging%20modalities%2C%20and%2013%20anatomical%20regions%2C%20outperforming%20state-of-the-art%20methods%20in%20accuracy%2C%20generalization%2C%20and%20clinical%20applicability.%20This%20work%20aims%20to%20continuously%20expand%20to%20encompass%20more%20downstream%20applications%2C%20and%20the%20corresponding%20resources%20are%20intended%20to%20be%20shared%20through%20https%3A//github.com/VinyehShaw/UniEqui.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvMFCoOp%253A%2520Towards%2520Equilibrium%2520on%2520a%2520Unified%2520Hyperspherical%2520Manifold%2520for%2520Prompting%2520Biomedical%2520VLMs%26entry.906535625%3DMinye%2520Shao%2520and%2520Sihan%2520Guo%2520and%2520Xinrun%2520Li%2520and%2520Xingyu%2520Miao%2520and%2520Haoran%2520Duan%2520and%2520Yang%2520Long%26entry.1292438233%3DRecent%2520advances%2520in%2520context%2520optimization%2520%2528CoOp%2529%2520guided%2520by%2520large%2520language%2520model%2520%2528LLM%2529-distilled%2520medical%2520semantic%2520priors%2520offer%2520a%2520scalable%2520alternative%2520to%2520manual%2520prompt%2520engineering%2520and%2520full%2520fine-tuning%2520for%2520adapting%2520biomedical%2520CLIP-based%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520prompt%2520learning%2520in%2520this%2520context%2520is%2520challenged%2520by%2520semantic%2520misalignment%2520between%2520LLMs%2520and%2520CLIP%2520variants%2520due%2520to%2520divergent%2520training%2520corpora%2520and%2520model%2520architectures%253B%2520it%2520further%2520lacks%2520scalability%2520across%2520continuously%2520evolving%2520families%2520of%2520foundation%2520models.%2520More%2520critically%252C%2520pairwise%2520multimodal%2520alignment%2520via%2520conventional%2520Euclidean-space%2520optimization%2520lacks%2520the%2520capacity%2520to%2520model%2520unified%2520representations%2520or%2520apply%2520localized%2520geometric%2520constraints%252C%2520which%2520tends%2520to%2520amplify%2520modality%2520gaps%2520in%2520complex%2520biomedical%2520imaging%2520and%2520destabilize%2520few-shot%2520adaptation.%2520In%2520this%2520work%252C%2520we%2520propose%2520vMFCoOp%252C%2520a%2520framework%2520that%2520inversely%2520estimates%2520von%2520Mises-Fisher%2520%2528vMF%2529%2520distributions%2520on%2520a%2520shared%2520Hyperspherical%2520Manifold%252C%2520aligning%2520semantic%2520biases%2520between%2520arbitrary%2520LLMs%2520and%2520CLIP%2520backbones%2520via%2520Unified%2520Semantic%2520Anchors%2520to%2520achieve%2520robust%2520biomedical%2520prompting%2520and%2520superior%2520few-shot%2520classification.%2520Grounded%2520in%2520three%2520complementary%2520constraints%252C%2520vMFCoOp%2520demonstrates%2520consistent%2520improvements%2520across%252014%2520medical%2520datasets%252C%252012%2520medical%2520imaging%2520modalities%252C%2520and%252013%2520anatomical%2520regions%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520accuracy%252C%2520generalization%252C%2520and%2520clinical%2520applicability.%2520This%2520work%2520aims%2520to%2520continuously%2520expand%2520to%2520encompass%2520more%2520downstream%2520applications%252C%2520and%2520the%2520corresponding%2520resources%2520are%2520intended%2520to%2520be%2520shared%2520through%2520https%253A//github.com/VinyehShaw/UniEqui.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vMFCoOp%3A%20Towards%20Equilibrium%20on%20a%20Unified%20Hyperspherical%20Manifold%20for%20Prompting%20Biomedical%20VLMs&entry.906535625=Minye%20Shao%20and%20Sihan%20Guo%20and%20Xinrun%20Li%20and%20Xingyu%20Miao%20and%20Haoran%20Duan%20and%20Yang%20Long&entry.1292438233=Recent%20advances%20in%20context%20optimization%20%28CoOp%29%20guided%20by%20large%20language%20model%20%28LLM%29-distilled%20medical%20semantic%20priors%20offer%20a%20scalable%20alternative%20to%20manual%20prompt%20engineering%20and%20full%20fine-tuning%20for%20adapting%20biomedical%20CLIP-based%20vision-language%20models%20%28VLMs%29.%20However%2C%20prompt%20learning%20in%20this%20context%20is%20challenged%20by%20semantic%20misalignment%20between%20LLMs%20and%20CLIP%20variants%20due%20to%20divergent%20training%20corpora%20and%20model%20architectures%3B%20it%20further%20lacks%20scalability%20across%20continuously%20evolving%20families%20of%20foundation%20models.%20More%20critically%2C%20pairwise%20multimodal%20alignment%20via%20conventional%20Euclidean-space%20optimization%20lacks%20the%20capacity%20to%20model%20unified%20representations%20or%20apply%20localized%20geometric%20constraints%2C%20which%20tends%20to%20amplify%20modality%20gaps%20in%20complex%20biomedical%20imaging%20and%20destabilize%20few-shot%20adaptation.%20In%20this%20work%2C%20we%20propose%20vMFCoOp%2C%20a%20framework%20that%20inversely%20estimates%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20a%20shared%20Hyperspherical%20Manifold%2C%20aligning%20semantic%20biases%20between%20arbitrary%20LLMs%20and%20CLIP%20backbones%20via%20Unified%20Semantic%20Anchors%20to%20achieve%20robust%20biomedical%20prompting%20and%20superior%20few-shot%20classification.%20Grounded%20in%20three%20complementary%20constraints%2C%20vMFCoOp%20demonstrates%20consistent%20improvements%20across%2014%20medical%20datasets%2C%2012%20medical%20imaging%20modalities%2C%20and%2013%20anatomical%20regions%2C%20outperforming%20state-of-the-art%20methods%20in%20accuracy%2C%20generalization%2C%20and%20clinical%20applicability.%20This%20work%20aims%20to%20continuously%20expand%20to%20encompass%20more%20downstream%20applications%2C%20and%20the%20corresponding%20resources%20are%20intended%20to%20be%20shared%20through%20https%3A//github.com/VinyehShaw/UniEqui.&entry.1838667208=http%3A//arxiv.org/abs/2511.09540v2&entry.124074799=Read"},
{"title": "From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis", "author": "Yen Nhi Truong Vu and Dan Guo and Sripad Joshi and Harshit Kumar and Jason Su and Thomas Paul Matthews", "abstract": "Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&M. M&M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&M-3D outperforms previous top baseline by 4% for classification and 10% for localization.", "link": "http://arxiv.org/abs/2511.10597v1", "date": "2025-11-13", "relevancy": 2.869, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%202D%20to%203D%20Without%20Extra%20Baggage%3A%20Data-Efficient%20Cancer%20Detection%20in%20Digital%20Breast%20Tomosynthesis&body=Title%3A%20From%202D%20to%203D%20Without%20Extra%20Baggage%3A%20Data-Efficient%20Cancer%20Detection%20in%20Digital%20Breast%20Tomosynthesis%0AAuthor%3A%20Yen%20Nhi%20Truong%20Vu%20and%20Dan%20Guo%20and%20Sripad%20Joshi%20and%20Harshit%20Kumar%20and%20Jason%20Su%20and%20Thomas%20Paul%20Matthews%0AAbstract%3A%20Digital%20Breast%20Tomosynthesis%20%28DBT%29%20enhances%20finding%20visibility%20for%20breast%20cancer%20detection%20by%20providing%20volumetric%20information%20that%20reduces%20the%20impact%20of%20overlapping%20tissues%3B%20however%2C%20limited%20annotated%20data%20has%20constrained%20the%20development%20of%20deep%20learning%20models%20for%20DBT.%20To%20address%20data%20scarcity%2C%20existing%20methods%20attempt%20to%20reuse%202D%20full-field%20digital%20mammography%20%28FFDM%29%20models%20by%20either%20flattening%20DBT%20volumes%20or%20processing%20slices%20individually%2C%20thus%20discarding%20volumetric%20information.%20Alternatively%2C%203D%20reasoning%20approaches%20introduce%20complex%20architectures%20that%20require%20more%20DBT%20training%20data.%20Tackling%20these%20drawbacks%2C%20we%20propose%20M%26M-3D%2C%20an%20architecture%20that%20enables%20learnable%203D%20reasoning%20while%20remaining%20parameter-free%20relative%20to%20its%20FFDM%20counterpart%2C%20M%26M.%20M%26M-3D%20constructs%20malignancy-guided%203D%20features%2C%20and%203D%20reasoning%20is%20learned%20through%20repeatedly%20mixing%20these%203D%20features%20with%20slice-level%20information.%20This%20is%20achieved%20by%20modifying%20operations%20in%20M%26M%20without%20adding%20parameters%2C%20thus%20enabling%20direct%20weight%20transfer%20from%20FFDM.%20Extensive%20experiments%20show%20that%20M%26M-3D%20surpasses%202D%20projection%20and%203D%20slice-based%20methods%20by%2011-54%25%20for%20localization%20and%203-10%25%20for%20classification.%20Additionally%2C%20M%26M-3D%20outperforms%20complex%203D%20reasoning%20variants%20by%2020-47%25%20for%20localization%20and%202-10%25%20for%20classification%20in%20the%20low-data%20regime%2C%20while%20matching%20their%20performance%20in%20high-data%20regime.%20On%20the%20popular%20BCS-DBT%20benchmark%2C%20M%26M-3D%20outperforms%20previous%20top%20baseline%20by%204%25%20for%20classification%20and%2010%25%20for%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%25202D%2520to%25203D%2520Without%2520Extra%2520Baggage%253A%2520Data-Efficient%2520Cancer%2520Detection%2520in%2520Digital%2520Breast%2520Tomosynthesis%26entry.906535625%3DYen%2520Nhi%2520Truong%2520Vu%2520and%2520Dan%2520Guo%2520and%2520Sripad%2520Joshi%2520and%2520Harshit%2520Kumar%2520and%2520Jason%2520Su%2520and%2520Thomas%2520Paul%2520Matthews%26entry.1292438233%3DDigital%2520Breast%2520Tomosynthesis%2520%2528DBT%2529%2520enhances%2520finding%2520visibility%2520for%2520breast%2520cancer%2520detection%2520by%2520providing%2520volumetric%2520information%2520that%2520reduces%2520the%2520impact%2520of%2520overlapping%2520tissues%253B%2520however%252C%2520limited%2520annotated%2520data%2520has%2520constrained%2520the%2520development%2520of%2520deep%2520learning%2520models%2520for%2520DBT.%2520To%2520address%2520data%2520scarcity%252C%2520existing%2520methods%2520attempt%2520to%2520reuse%25202D%2520full-field%2520digital%2520mammography%2520%2528FFDM%2529%2520models%2520by%2520either%2520flattening%2520DBT%2520volumes%2520or%2520processing%2520slices%2520individually%252C%2520thus%2520discarding%2520volumetric%2520information.%2520Alternatively%252C%25203D%2520reasoning%2520approaches%2520introduce%2520complex%2520architectures%2520that%2520require%2520more%2520DBT%2520training%2520data.%2520Tackling%2520these%2520drawbacks%252C%2520we%2520propose%2520M%2526M-3D%252C%2520an%2520architecture%2520that%2520enables%2520learnable%25203D%2520reasoning%2520while%2520remaining%2520parameter-free%2520relative%2520to%2520its%2520FFDM%2520counterpart%252C%2520M%2526M.%2520M%2526M-3D%2520constructs%2520malignancy-guided%25203D%2520features%252C%2520and%25203D%2520reasoning%2520is%2520learned%2520through%2520repeatedly%2520mixing%2520these%25203D%2520features%2520with%2520slice-level%2520information.%2520This%2520is%2520achieved%2520by%2520modifying%2520operations%2520in%2520M%2526M%2520without%2520adding%2520parameters%252C%2520thus%2520enabling%2520direct%2520weight%2520transfer%2520from%2520FFDM.%2520Extensive%2520experiments%2520show%2520that%2520M%2526M-3D%2520surpasses%25202D%2520projection%2520and%25203D%2520slice-based%2520methods%2520by%252011-54%2525%2520for%2520localization%2520and%25203-10%2525%2520for%2520classification.%2520Additionally%252C%2520M%2526M-3D%2520outperforms%2520complex%25203D%2520reasoning%2520variants%2520by%252020-47%2525%2520for%2520localization%2520and%25202-10%2525%2520for%2520classification%2520in%2520the%2520low-data%2520regime%252C%2520while%2520matching%2520their%2520performance%2520in%2520high-data%2520regime.%2520On%2520the%2520popular%2520BCS-DBT%2520benchmark%252C%2520M%2526M-3D%2520outperforms%2520previous%2520top%2520baseline%2520by%25204%2525%2520for%2520classification%2520and%252010%2525%2520for%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%202D%20to%203D%20Without%20Extra%20Baggage%3A%20Data-Efficient%20Cancer%20Detection%20in%20Digital%20Breast%20Tomosynthesis&entry.906535625=Yen%20Nhi%20Truong%20Vu%20and%20Dan%20Guo%20and%20Sripad%20Joshi%20and%20Harshit%20Kumar%20and%20Jason%20Su%20and%20Thomas%20Paul%20Matthews&entry.1292438233=Digital%20Breast%20Tomosynthesis%20%28DBT%29%20enhances%20finding%20visibility%20for%20breast%20cancer%20detection%20by%20providing%20volumetric%20information%20that%20reduces%20the%20impact%20of%20overlapping%20tissues%3B%20however%2C%20limited%20annotated%20data%20has%20constrained%20the%20development%20of%20deep%20learning%20models%20for%20DBT.%20To%20address%20data%20scarcity%2C%20existing%20methods%20attempt%20to%20reuse%202D%20full-field%20digital%20mammography%20%28FFDM%29%20models%20by%20either%20flattening%20DBT%20volumes%20or%20processing%20slices%20individually%2C%20thus%20discarding%20volumetric%20information.%20Alternatively%2C%203D%20reasoning%20approaches%20introduce%20complex%20architectures%20that%20require%20more%20DBT%20training%20data.%20Tackling%20these%20drawbacks%2C%20we%20propose%20M%26M-3D%2C%20an%20architecture%20that%20enables%20learnable%203D%20reasoning%20while%20remaining%20parameter-free%20relative%20to%20its%20FFDM%20counterpart%2C%20M%26M.%20M%26M-3D%20constructs%20malignancy-guided%203D%20features%2C%20and%203D%20reasoning%20is%20learned%20through%20repeatedly%20mixing%20these%203D%20features%20with%20slice-level%20information.%20This%20is%20achieved%20by%20modifying%20operations%20in%20M%26M%20without%20adding%20parameters%2C%20thus%20enabling%20direct%20weight%20transfer%20from%20FFDM.%20Extensive%20experiments%20show%20that%20M%26M-3D%20surpasses%202D%20projection%20and%203D%20slice-based%20methods%20by%2011-54%25%20for%20localization%20and%203-10%25%20for%20classification.%20Additionally%2C%20M%26M-3D%20outperforms%20complex%203D%20reasoning%20variants%20by%2020-47%25%20for%20localization%20and%202-10%25%20for%20classification%20in%20the%20low-data%20regime%2C%20while%20matching%20their%20performance%20in%20high-data%20regime.%20On%20the%20popular%20BCS-DBT%20benchmark%2C%20M%26M-3D%20outperforms%20previous%20top%20baseline%20by%204%25%20for%20classification%20and%2010%25%20for%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2511.10597v1&entry.124074799=Read"},
{"title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization", "author": "Minheng Chen and Youyong Kong", "abstract": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.", "link": "http://arxiv.org/abs/2509.06890v2", "date": "2025-11-13", "relevancy": 2.8266, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6152}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization&body=Title%3A%20Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization%0AAuthor%3A%20Minheng%20Chen%20and%20Youyong%20Kong%0AAbstract%3A%20Intraoperative%202D/3D%20registration%20aligns%20preoperative%203D%20volumes%20with%20real-time%202D%20radiographs%2C%20enabling%20accurate%20localization%20of%20instruments%20and%20implants.%20A%20recent%20fully%20differentiable%20similarity%20learning%20framework%20approximates%20geodesic%20distances%20on%20SE%283%29%2C%20expanding%20the%20capture%20range%20of%20registration%20and%20mitigating%20the%20effects%20of%20substantial%20disturbances%2C%20but%20existing%20Euclidean%20approximations%20distort%20manifold%20structure%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%20explore%20similarity%20learning%20in%20non-Euclidean%20spherical%20feature%20spaces%20to%20better%20capture%20and%20fit%20complex%20manifold%20structure.%20We%20extract%20feature%20embeddings%20using%20a%20CNN-Transformer%20encoder%2C%20project%20them%20into%20spherical%20space%2C%20and%20approximate%20their%20geodesic%20distances%20with%20Riemannian%20distances%20in%20the%20bi-invariant%20SO%284%29%20space.%20This%20enables%20a%20more%20expressive%20and%20geometrically%20consistent%20deep%20similarity%20metric%2C%20enhancing%20the%20ability%20to%20distinguish%20subtle%20pose%20differences.%20During%20inference%2C%20we%20replace%20gradient%20descent%20with%20fully%20differentiable%20Levenberg-Marquardt%20optimization%20to%20accelerate%20convergence.%20Experiments%20on%20real%20and%20synthetic%20datasets%20show%20superior%20accuracy%20in%20both%20patient-specific%20and%20patient-agnostic%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntraoperative%25202D/3D%2520Registration%2520via%2520Spherical%2520Similarity%2520Learning%2520and%2520Inference-Time%2520Differentiable%2520Levenberg-Marquardt%2520Optimization%26entry.906535625%3DMinheng%2520Chen%2520and%2520Youyong%2520Kong%26entry.1292438233%3DIntraoperative%25202D/3D%2520registration%2520aligns%2520preoperative%25203D%2520volumes%2520with%2520real-time%25202D%2520radiographs%252C%2520enabling%2520accurate%2520localization%2520of%2520instruments%2520and%2520implants.%2520A%2520recent%2520fully%2520differentiable%2520similarity%2520learning%2520framework%2520approximates%2520geodesic%2520distances%2520on%2520SE%25283%2529%252C%2520expanding%2520the%2520capture%2520range%2520of%2520registration%2520and%2520mitigating%2520the%2520effects%2520of%2520substantial%2520disturbances%252C%2520but%2520existing%2520Euclidean%2520approximations%2520distort%2520manifold%2520structure%2520and%2520slow%2520convergence.%2520To%2520address%2520these%2520limitations%252C%2520we%2520explore%2520similarity%2520learning%2520in%2520non-Euclidean%2520spherical%2520feature%2520spaces%2520to%2520better%2520capture%2520and%2520fit%2520complex%2520manifold%2520structure.%2520We%2520extract%2520feature%2520embeddings%2520using%2520a%2520CNN-Transformer%2520encoder%252C%2520project%2520them%2520into%2520spherical%2520space%252C%2520and%2520approximate%2520their%2520geodesic%2520distances%2520with%2520Riemannian%2520distances%2520in%2520the%2520bi-invariant%2520SO%25284%2529%2520space.%2520This%2520enables%2520a%2520more%2520expressive%2520and%2520geometrically%2520consistent%2520deep%2520similarity%2520metric%252C%2520enhancing%2520the%2520ability%2520to%2520distinguish%2520subtle%2520pose%2520differences.%2520During%2520inference%252C%2520we%2520replace%2520gradient%2520descent%2520with%2520fully%2520differentiable%2520Levenberg-Marquardt%2520optimization%2520to%2520accelerate%2520convergence.%2520Experiments%2520on%2520real%2520and%2520synthetic%2520datasets%2520show%2520superior%2520accuracy%2520in%2520both%2520patient-specific%2520and%2520patient-agnostic%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intraoperative%202D/3D%20Registration%20via%20Spherical%20Similarity%20Learning%20and%20Inference-Time%20Differentiable%20Levenberg-Marquardt%20Optimization&entry.906535625=Minheng%20Chen%20and%20Youyong%20Kong&entry.1292438233=Intraoperative%202D/3D%20registration%20aligns%20preoperative%203D%20volumes%20with%20real-time%202D%20radiographs%2C%20enabling%20accurate%20localization%20of%20instruments%20and%20implants.%20A%20recent%20fully%20differentiable%20similarity%20learning%20framework%20approximates%20geodesic%20distances%20on%20SE%283%29%2C%20expanding%20the%20capture%20range%20of%20registration%20and%20mitigating%20the%20effects%20of%20substantial%20disturbances%2C%20but%20existing%20Euclidean%20approximations%20distort%20manifold%20structure%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%20explore%20similarity%20learning%20in%20non-Euclidean%20spherical%20feature%20spaces%20to%20better%20capture%20and%20fit%20complex%20manifold%20structure.%20We%20extract%20feature%20embeddings%20using%20a%20CNN-Transformer%20encoder%2C%20project%20them%20into%20spherical%20space%2C%20and%20approximate%20their%20geodesic%20distances%20with%20Riemannian%20distances%20in%20the%20bi-invariant%20SO%284%29%20space.%20This%20enables%20a%20more%20expressive%20and%20geometrically%20consistent%20deep%20similarity%20metric%2C%20enhancing%20the%20ability%20to%20distinguish%20subtle%20pose%20differences.%20During%20inference%2C%20we%20replace%20gradient%20descent%20with%20fully%20differentiable%20Levenberg-Marquardt%20optimization%20to%20accelerate%20convergence.%20Experiments%20on%20real%20and%20synthetic%20datasets%20show%20superior%20accuracy%20in%20both%20patient-specific%20and%20patient-agnostic%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2509.06890v2&entry.124074799=Read"},
{"title": "CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification", "author": "Xiaomei Yang and Xizhan Gao and Sijie Niu and Fa Zhu and Guang Feng and Xiaofeng Qu and David Camacho", "abstract": "This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.", "link": "http://arxiv.org/abs/2511.10309v1", "date": "2025-11-13", "relevancy": 2.8163, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP4VI-ReID%3A%20Learning%20Modality-shared%20Representations%20via%20CLIP%20Semantic%20Bridge%20for%20Visible-Infrared%20Person%20Re-identification&body=Title%3A%20CLIP4VI-ReID%3A%20Learning%20Modality-shared%20Representations%20via%20CLIP%20Semantic%20Bridge%20for%20Visible-Infrared%20Person%20Re-identification%0AAuthor%3A%20Xiaomei%20Yang%20and%20Xizhan%20Gao%20and%20Sijie%20Niu%20and%20Fa%20Zhu%20and%20Guang%20Feng%20and%20Xiaofeng%20Qu%20and%20David%20Camacho%0AAbstract%3A%20This%20paper%20proposes%20a%20novel%20CLIP-driven%20modality-shared%20representation%20learning%20network%20named%20CLIP4VI-ReID%20for%20VI-ReID%20task%2C%20which%20consists%20of%20Text%20Semantic%20Generation%20%28TSG%29%2C%20Infrared%20Feature%20Embedding%20%28IFE%29%2C%20and%20High-level%20Semantic%20Alignment%20%28HSA%29.%20Specifically%2C%20considering%20the%20huge%20gap%20in%20the%20physical%20characteristics%20between%20natural%20images%20and%20infrared%20images%2C%20the%20TSG%20is%20designed%20to%20generate%20text%20semantics%20only%20for%20visible%20images%2C%20thereby%20enabling%20preliminary%20visible-text%20modality%20alignment.%20Then%2C%20the%20IFE%20is%20proposed%20to%20rectify%20the%20feature%20embeddings%20of%20infrared%20images%20using%20the%20generated%20text%20semantics.%20This%20process%20injects%20id-related%20semantics%20into%20the%20shared%20image%20encoder%2C%20enhancing%20its%20adaptability%20to%20the%20infrared%20modality.%20Besides%2C%20with%20text%20serving%20as%20a%20bridge%2C%20it%20enables%20indirect%20visible-infrared%20modality%20alignment.%20Finally%2C%20the%20HSA%20is%20established%20to%20refine%20the%20high-level%20semantic%20alignment.%20This%20process%20ensures%20that%20the%20fine-tuned%20text%20semantics%20only%20contain%20id-related%20information%2C%20thereby%20achieving%20more%20accurate%20cross-modal%20alignment%20and%20enhancing%20the%20discriminability%20of%20the%20learned%20modal-shared%20representations.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20CLIP4VI-ReID%20achieves%20superior%20performance%20than%20other%20state-of-the-art%20methods%20on%20some%20widely%20used%20VI-ReID%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP4VI-ReID%253A%2520Learning%2520Modality-shared%2520Representations%2520via%2520CLIP%2520Semantic%2520Bridge%2520for%2520Visible-Infrared%2520Person%2520Re-identification%26entry.906535625%3DXiaomei%2520Yang%2520and%2520Xizhan%2520Gao%2520and%2520Sijie%2520Niu%2520and%2520Fa%2520Zhu%2520and%2520Guang%2520Feng%2520and%2520Xiaofeng%2520Qu%2520and%2520David%2520Camacho%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520novel%2520CLIP-driven%2520modality-shared%2520representation%2520learning%2520network%2520named%2520CLIP4VI-ReID%2520for%2520VI-ReID%2520task%252C%2520which%2520consists%2520of%2520Text%2520Semantic%2520Generation%2520%2528TSG%2529%252C%2520Infrared%2520Feature%2520Embedding%2520%2528IFE%2529%252C%2520and%2520High-level%2520Semantic%2520Alignment%2520%2528HSA%2529.%2520Specifically%252C%2520considering%2520the%2520huge%2520gap%2520in%2520the%2520physical%2520characteristics%2520between%2520natural%2520images%2520and%2520infrared%2520images%252C%2520the%2520TSG%2520is%2520designed%2520to%2520generate%2520text%2520semantics%2520only%2520for%2520visible%2520images%252C%2520thereby%2520enabling%2520preliminary%2520visible-text%2520modality%2520alignment.%2520Then%252C%2520the%2520IFE%2520is%2520proposed%2520to%2520rectify%2520the%2520feature%2520embeddings%2520of%2520infrared%2520images%2520using%2520the%2520generated%2520text%2520semantics.%2520This%2520process%2520injects%2520id-related%2520semantics%2520into%2520the%2520shared%2520image%2520encoder%252C%2520enhancing%2520its%2520adaptability%2520to%2520the%2520infrared%2520modality.%2520Besides%252C%2520with%2520text%2520serving%2520as%2520a%2520bridge%252C%2520it%2520enables%2520indirect%2520visible-infrared%2520modality%2520alignment.%2520Finally%252C%2520the%2520HSA%2520is%2520established%2520to%2520refine%2520the%2520high-level%2520semantic%2520alignment.%2520This%2520process%2520ensures%2520that%2520the%2520fine-tuned%2520text%2520semantics%2520only%2520contain%2520id-related%2520information%252C%2520thereby%2520achieving%2520more%2520accurate%2520cross-modal%2520alignment%2520and%2520enhancing%2520the%2520discriminability%2520of%2520the%2520learned%2520modal-shared%2520representations.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520CLIP4VI-ReID%2520achieves%2520superior%2520performance%2520than%2520other%2520state-of-the-art%2520methods%2520on%2520some%2520widely%2520used%2520VI-ReID%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP4VI-ReID%3A%20Learning%20Modality-shared%20Representations%20via%20CLIP%20Semantic%20Bridge%20for%20Visible-Infrared%20Person%20Re-identification&entry.906535625=Xiaomei%20Yang%20and%20Xizhan%20Gao%20and%20Sijie%20Niu%20and%20Fa%20Zhu%20and%20Guang%20Feng%20and%20Xiaofeng%20Qu%20and%20David%20Camacho&entry.1292438233=This%20paper%20proposes%20a%20novel%20CLIP-driven%20modality-shared%20representation%20learning%20network%20named%20CLIP4VI-ReID%20for%20VI-ReID%20task%2C%20which%20consists%20of%20Text%20Semantic%20Generation%20%28TSG%29%2C%20Infrared%20Feature%20Embedding%20%28IFE%29%2C%20and%20High-level%20Semantic%20Alignment%20%28HSA%29.%20Specifically%2C%20considering%20the%20huge%20gap%20in%20the%20physical%20characteristics%20between%20natural%20images%20and%20infrared%20images%2C%20the%20TSG%20is%20designed%20to%20generate%20text%20semantics%20only%20for%20visible%20images%2C%20thereby%20enabling%20preliminary%20visible-text%20modality%20alignment.%20Then%2C%20the%20IFE%20is%20proposed%20to%20rectify%20the%20feature%20embeddings%20of%20infrared%20images%20using%20the%20generated%20text%20semantics.%20This%20process%20injects%20id-related%20semantics%20into%20the%20shared%20image%20encoder%2C%20enhancing%20its%20adaptability%20to%20the%20infrared%20modality.%20Besides%2C%20with%20text%20serving%20as%20a%20bridge%2C%20it%20enables%20indirect%20visible-infrared%20modality%20alignment.%20Finally%2C%20the%20HSA%20is%20established%20to%20refine%20the%20high-level%20semantic%20alignment.%20This%20process%20ensures%20that%20the%20fine-tuned%20text%20semantics%20only%20contain%20id-related%20information%2C%20thereby%20achieving%20more%20accurate%20cross-modal%20alignment%20and%20enhancing%20the%20discriminability%20of%20the%20learned%20modal-shared%20representations.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20CLIP4VI-ReID%20achieves%20superior%20performance%20than%20other%20state-of-the-art%20methods%20on%20some%20widely%20used%20VI-ReID%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.10309v1&entry.124074799=Read"},
{"title": "GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval", "author": "Hao Zou and Runqing Zhang and Xue Zhou and Jianxiao Zou", "abstract": "Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.", "link": "http://arxiv.org/abs/2511.10154v1", "date": "2025-11-13", "relevancy": 2.7748, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5571}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5551}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEA%3A%20Generation-Enhanced%20Alignment%20for%20Text-to-Image%20Person%20Retrieval&body=Title%3A%20GEA%3A%20Generation-Enhanced%20Alignment%20for%20Text-to-Image%20Person%20Retrieval%0AAuthor%3A%20Hao%20Zou%20and%20Runqing%20Zhang%20and%20Xue%20Zhou%20and%20Jianxiao%20Zou%0AAbstract%3A%20Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20aims%20to%20retrieve%20person%20images%20based%20on%20natural%20language%20descriptions.%20Although%20many%20TIPR%20methods%20have%20achieved%20promising%20results%2C%20sometimes%20textual%20queries%20cannot%20accurately%20and%20comprehensively%20reflect%20the%20content%20of%20the%20image%2C%20leading%20to%20poor%20cross-modal%20alignment%20and%20overfitting%20to%20limited%20datasets.%20Moreover%2C%20the%20inherent%20modality%20gap%20between%20text%20and%20image%20further%20amplifies%20these%20issues%2C%20making%20accurate%20cross-modal%20retrieval%20even%20more%20challenging.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Generation-Enhanced%20Alignment%20%28GEA%29%20from%20a%20generative%20perspective.%20GEA%20contains%20two%20parallel%20modules%3A%20%281%29%20Text-Guided%20Token%20Enhancement%20%28TGTE%29%2C%20which%20introduces%20diffusion-generated%20images%20as%20intermediate%20semantic%20representations%20to%20bridge%20the%20gap%20between%20text%20and%20visual%20patterns.%20These%20generated%20images%20enrich%20the%20semantic%20representation%20of%20text%20and%20facilitate%20cross-modal%20alignment.%20%282%29%20Generative%20Intermediate%20Fusion%20%28GIF%29%2C%20which%20combines%20cross-attention%20between%20generated%20images%2C%20original%20images%2C%20and%20text%20features%20to%20generate%20a%20unified%20representation%20optimized%20by%20triplet%20alignment%20loss.%20We%20conduct%20extensive%20experiments%20on%20three%20public%20TIPR%20datasets%2C%20CUHK-PEDES%2C%20RSTPReid%2C%20and%20ICFG-PEDES%2C%20to%20evaluate%20the%20performance%20of%20GEA.%20The%20results%20justify%20the%20effectiveness%20of%20our%20method.%20More%20implementation%20details%20and%20extended%20results%20are%20available%20at%20https%3A//github.com/sugelamyd123/Sup-for-GEA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEA%253A%2520Generation-Enhanced%2520Alignment%2520for%2520Text-to-Image%2520Person%2520Retrieval%26entry.906535625%3DHao%2520Zou%2520and%2520Runqing%2520Zhang%2520and%2520Xue%2520Zhou%2520and%2520Jianxiao%2520Zou%26entry.1292438233%3DText-to-Image%2520Person%2520Retrieval%2520%2528TIPR%2529%2520aims%2520to%2520retrieve%2520person%2520images%2520based%2520on%2520natural%2520language%2520descriptions.%2520Although%2520many%2520TIPR%2520methods%2520have%2520achieved%2520promising%2520results%252C%2520sometimes%2520textual%2520queries%2520cannot%2520accurately%2520and%2520comprehensively%2520reflect%2520the%2520content%2520of%2520the%2520image%252C%2520leading%2520to%2520poor%2520cross-modal%2520alignment%2520and%2520overfitting%2520to%2520limited%2520datasets.%2520Moreover%252C%2520the%2520inherent%2520modality%2520gap%2520between%2520text%2520and%2520image%2520further%2520amplifies%2520these%2520issues%252C%2520making%2520accurate%2520cross-modal%2520retrieval%2520even%2520more%2520challenging.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Generation-Enhanced%2520Alignment%2520%2528GEA%2529%2520from%2520a%2520generative%2520perspective.%2520GEA%2520contains%2520two%2520parallel%2520modules%253A%2520%25281%2529%2520Text-Guided%2520Token%2520Enhancement%2520%2528TGTE%2529%252C%2520which%2520introduces%2520diffusion-generated%2520images%2520as%2520intermediate%2520semantic%2520representations%2520to%2520bridge%2520the%2520gap%2520between%2520text%2520and%2520visual%2520patterns.%2520These%2520generated%2520images%2520enrich%2520the%2520semantic%2520representation%2520of%2520text%2520and%2520facilitate%2520cross-modal%2520alignment.%2520%25282%2529%2520Generative%2520Intermediate%2520Fusion%2520%2528GIF%2529%252C%2520which%2520combines%2520cross-attention%2520between%2520generated%2520images%252C%2520original%2520images%252C%2520and%2520text%2520features%2520to%2520generate%2520a%2520unified%2520representation%2520optimized%2520by%2520triplet%2520alignment%2520loss.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520three%2520public%2520TIPR%2520datasets%252C%2520CUHK-PEDES%252C%2520RSTPReid%252C%2520and%2520ICFG-PEDES%252C%2520to%2520evaluate%2520the%2520performance%2520of%2520GEA.%2520The%2520results%2520justify%2520the%2520effectiveness%2520of%2520our%2520method.%2520More%2520implementation%2520details%2520and%2520extended%2520results%2520are%2520available%2520at%2520https%253A//github.com/sugelamyd123/Sup-for-GEA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEA%3A%20Generation-Enhanced%20Alignment%20for%20Text-to-Image%20Person%20Retrieval&entry.906535625=Hao%20Zou%20and%20Runqing%20Zhang%20and%20Xue%20Zhou%20and%20Jianxiao%20Zou&entry.1292438233=Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20aims%20to%20retrieve%20person%20images%20based%20on%20natural%20language%20descriptions.%20Although%20many%20TIPR%20methods%20have%20achieved%20promising%20results%2C%20sometimes%20textual%20queries%20cannot%20accurately%20and%20comprehensively%20reflect%20the%20content%20of%20the%20image%2C%20leading%20to%20poor%20cross-modal%20alignment%20and%20overfitting%20to%20limited%20datasets.%20Moreover%2C%20the%20inherent%20modality%20gap%20between%20text%20and%20image%20further%20amplifies%20these%20issues%2C%20making%20accurate%20cross-modal%20retrieval%20even%20more%20challenging.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Generation-Enhanced%20Alignment%20%28GEA%29%20from%20a%20generative%20perspective.%20GEA%20contains%20two%20parallel%20modules%3A%20%281%29%20Text-Guided%20Token%20Enhancement%20%28TGTE%29%2C%20which%20introduces%20diffusion-generated%20images%20as%20intermediate%20semantic%20representations%20to%20bridge%20the%20gap%20between%20text%20and%20visual%20patterns.%20These%20generated%20images%20enrich%20the%20semantic%20representation%20of%20text%20and%20facilitate%20cross-modal%20alignment.%20%282%29%20Generative%20Intermediate%20Fusion%20%28GIF%29%2C%20which%20combines%20cross-attention%20between%20generated%20images%2C%20original%20images%2C%20and%20text%20features%20to%20generate%20a%20unified%20representation%20optimized%20by%20triplet%20alignment%20loss.%20We%20conduct%20extensive%20experiments%20on%20three%20public%20TIPR%20datasets%2C%20CUHK-PEDES%2C%20RSTPReid%2C%20and%20ICFG-PEDES%2C%20to%20evaluate%20the%20performance%20of%20GEA.%20The%20results%20justify%20the%20effectiveness%20of%20our%20method.%20More%20implementation%20details%20and%20extended%20results%20are%20available%20at%20https%3A//github.com/sugelamyd123/Sup-for-GEA.&entry.1838667208=http%3A//arxiv.org/abs/2511.10154v1&entry.124074799=Read"},
{"title": "Two Heads are Better than One: Robust Learning Meets Multi-branch Models", "author": "Zongyuan Zhang and Qingwen Bu and Tianyang Duan and Zheng Lin and Yuhao Qing and Zihan Fang and Heming Cui and Dong Huang", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \\textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple multi-branch neural network and propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\u03b5= 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\\% and +9.07\\%).", "link": "http://arxiv.org/abs/2208.08083v4", "date": "2025-11-13", "relevancy": 2.7186, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5725}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5479}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Heads%20are%20Better%20than%20One%3A%20Robust%20Learning%20Meets%20Multi-branch%20Models&body=Title%3A%20Two%20Heads%20are%20Better%20than%20One%3A%20Robust%20Learning%20Meets%20Multi-branch%20Models%0AAuthor%3A%20Zongyuan%20Zhang%20and%20Qingwen%20Bu%20and%20Tianyang%20Duan%20and%20Zheng%20Lin%20and%20Yuhao%20Qing%20and%20Zihan%20Fang%20and%20Heming%20Cui%20and%20Dong%20Huang%0AAbstract%3A%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20adversarial%20examples%2C%20in%20which%20DNNs%20are%20misled%20to%20false%20outputs%20due%20to%20inputs%20containing%20imperceptible%20perturbations.%20Adversarial%20training%2C%20a%20reliable%20and%20effective%20method%20of%20defense%2C%20may%20significantly%20reduce%20the%20vulnerability%20of%20neural%20networks%20and%20becomes%20the%20de%20facto%20standard%20for%20robust%20learning.%20While%20many%20recent%20works%20practice%20the%20data-centric%20philosophy%2C%20such%20as%20how%20to%20generate%20better%20adversarial%20examples%20or%20use%20generative%20models%20to%20produce%20additional%20training%20data%2C%20we%20look%20back%20to%20the%20models%20themselves%20and%20revisit%20the%20adversarial%20robustness%20from%20the%20perspective%20of%20deep%20feature%20distribution%20as%20an%20insightful%20complementarity.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BBranch%20Orthogonality%20adveRsarial%20Training%7D%20%28BORT%29%20to%20obtain%20state-of-the-art%20performance%20with%20solely%20the%20original%20dataset%20for%20adversarial%20training.%20To%20practice%20our%20design%20idea%20of%20integrating%20multiple%20orthogonal%20solution%20spaces%2C%20we%20leverage%20a%20simple%20multi-branch%20neural%20network%20and%20propose%20a%20corresponding%20loss%20function%2C%20branch-orthogonal%20loss%2C%20to%20make%20each%20solution%20space%20of%20the%20multi-branch%20model%20orthogonal.%20We%20evaluate%20our%20approach%20on%20CIFAR-10%2C%20CIFAR-100%20and%20SVHN%20against%20%24%5Cell_%7B%5Cinfty%7D%24%20norm-bounded%20perturbations%20of%20size%20%24%CE%B5%3D%208/255%24%2C%20respectively.%20Exhaustive%20experiments%20are%20conducted%20to%20show%20that%20our%20method%20goes%20beyond%20all%20state-of-the-art%20methods%20without%20any%20tricks.%20Compared%20to%20all%20methods%20that%20do%20not%20use%20additional%20data%20for%20training%2C%20our%20models%20achieve%2067.3%5C%25%20and%2041.5%5C%25%20robust%20accuracy%20on%20CIFAR-10%20and%20CIFAR-100%20%28improving%20upon%20the%20state-of-the-art%20by%20%2B7.23%5C%25%20and%20%2B9.07%5C%25%29.%0ALink%3A%20http%3A//arxiv.org/abs/2208.08083v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Heads%2520are%2520Better%2520than%2520One%253A%2520Robust%2520Learning%2520Meets%2520Multi-branch%2520Models%26entry.906535625%3DZongyuan%2520Zhang%2520and%2520Qingwen%2520Bu%2520and%2520Tianyang%2520Duan%2520and%2520Zheng%2520Lin%2520and%2520Yuhao%2520Qing%2520and%2520Zihan%2520Fang%2520and%2520Heming%2520Cui%2520and%2520Dong%2520Huang%26entry.1292438233%3DDeep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%2520to%2520adversarial%2520examples%252C%2520in%2520which%2520DNNs%2520are%2520misled%2520to%2520false%2520outputs%2520due%2520to%2520inputs%2520containing%2520imperceptible%2520perturbations.%2520Adversarial%2520training%252C%2520a%2520reliable%2520and%2520effective%2520method%2520of%2520defense%252C%2520may%2520significantly%2520reduce%2520the%2520vulnerability%2520of%2520neural%2520networks%2520and%2520becomes%2520the%2520de%2520facto%2520standard%2520for%2520robust%2520learning.%2520While%2520many%2520recent%2520works%2520practice%2520the%2520data-centric%2520philosophy%252C%2520such%2520as%2520how%2520to%2520generate%2520better%2520adversarial%2520examples%2520or%2520use%2520generative%2520models%2520to%2520produce%2520additional%2520training%2520data%252C%2520we%2520look%2520back%2520to%2520the%2520models%2520themselves%2520and%2520revisit%2520the%2520adversarial%2520robustness%2520from%2520the%2520perspective%2520of%2520deep%2520feature%2520distribution%2520as%2520an%2520insightful%2520complementarity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextit%257BBranch%2520Orthogonality%2520adveRsarial%2520Training%257D%2520%2528BORT%2529%2520to%2520obtain%2520state-of-the-art%2520performance%2520with%2520solely%2520the%2520original%2520dataset%2520for%2520adversarial%2520training.%2520To%2520practice%2520our%2520design%2520idea%2520of%2520integrating%2520multiple%2520orthogonal%2520solution%2520spaces%252C%2520we%2520leverage%2520a%2520simple%2520multi-branch%2520neural%2520network%2520and%2520propose%2520a%2520corresponding%2520loss%2520function%252C%2520branch-orthogonal%2520loss%252C%2520to%2520make%2520each%2520solution%2520space%2520of%2520the%2520multi-branch%2520model%2520orthogonal.%2520We%2520evaluate%2520our%2520approach%2520on%2520CIFAR-10%252C%2520CIFAR-100%2520and%2520SVHN%2520against%2520%2524%255Cell_%257B%255Cinfty%257D%2524%2520norm-bounded%2520perturbations%2520of%2520size%2520%2524%25CE%25B5%253D%25208/255%2524%252C%2520respectively.%2520Exhaustive%2520experiments%2520are%2520conducted%2520to%2520show%2520that%2520our%2520method%2520goes%2520beyond%2520all%2520state-of-the-art%2520methods%2520without%2520any%2520tricks.%2520Compared%2520to%2520all%2520methods%2520that%2520do%2520not%2520use%2520additional%2520data%2520for%2520training%252C%2520our%2520models%2520achieve%252067.3%255C%2525%2520and%252041.5%255C%2525%2520robust%2520accuracy%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520%2528improving%2520upon%2520the%2520state-of-the-art%2520by%2520%252B7.23%255C%2525%2520and%2520%252B9.07%255C%2525%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.08083v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Heads%20are%20Better%20than%20One%3A%20Robust%20Learning%20Meets%20Multi-branch%20Models&entry.906535625=Zongyuan%20Zhang%20and%20Qingwen%20Bu%20and%20Tianyang%20Duan%20and%20Zheng%20Lin%20and%20Yuhao%20Qing%20and%20Zihan%20Fang%20and%20Heming%20Cui%20and%20Dong%20Huang&entry.1292438233=Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20adversarial%20examples%2C%20in%20which%20DNNs%20are%20misled%20to%20false%20outputs%20due%20to%20inputs%20containing%20imperceptible%20perturbations.%20Adversarial%20training%2C%20a%20reliable%20and%20effective%20method%20of%20defense%2C%20may%20significantly%20reduce%20the%20vulnerability%20of%20neural%20networks%20and%20becomes%20the%20de%20facto%20standard%20for%20robust%20learning.%20While%20many%20recent%20works%20practice%20the%20data-centric%20philosophy%2C%20such%20as%20how%20to%20generate%20better%20adversarial%20examples%20or%20use%20generative%20models%20to%20produce%20additional%20training%20data%2C%20we%20look%20back%20to%20the%20models%20themselves%20and%20revisit%20the%20adversarial%20robustness%20from%20the%20perspective%20of%20deep%20feature%20distribution%20as%20an%20insightful%20complementarity.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BBranch%20Orthogonality%20adveRsarial%20Training%7D%20%28BORT%29%20to%20obtain%20state-of-the-art%20performance%20with%20solely%20the%20original%20dataset%20for%20adversarial%20training.%20To%20practice%20our%20design%20idea%20of%20integrating%20multiple%20orthogonal%20solution%20spaces%2C%20we%20leverage%20a%20simple%20multi-branch%20neural%20network%20and%20propose%20a%20corresponding%20loss%20function%2C%20branch-orthogonal%20loss%2C%20to%20make%20each%20solution%20space%20of%20the%20multi-branch%20model%20orthogonal.%20We%20evaluate%20our%20approach%20on%20CIFAR-10%2C%20CIFAR-100%20and%20SVHN%20against%20%24%5Cell_%7B%5Cinfty%7D%24%20norm-bounded%20perturbations%20of%20size%20%24%CE%B5%3D%208/255%24%2C%20respectively.%20Exhaustive%20experiments%20are%20conducted%20to%20show%20that%20our%20method%20goes%20beyond%20all%20state-of-the-art%20methods%20without%20any%20tricks.%20Compared%20to%20all%20methods%20that%20do%20not%20use%20additional%20data%20for%20training%2C%20our%20models%20achieve%2067.3%5C%25%20and%2041.5%5C%25%20robust%20accuracy%20on%20CIFAR-10%20and%20CIFAR-100%20%28improving%20upon%20the%20state-of-the-art%20by%20%2B7.23%5C%25%20and%20%2B9.07%5C%25%29.&entry.1838667208=http%3A//arxiv.org/abs/2208.08083v4&entry.124074799=Read"},
{"title": "Panda: Test-Time Adaptation with Negative Data Augmentation", "author": "Ruxi Deng and Wenxuan Bao and Tianxin Wei and Jingrui He", "abstract": "Pretrained VLMs exhibit strong zero-shot classification capabilities, but their predictions degrade significantly under common image corruptions. To improve robustness, many test-time adaptation (TTA) methods adopt positive data augmentation (PDA), which generates multiple views of each test sample to reduce prediction variance. However, these methods suffer from two key limitations. First, it introduces considerable computational overhead due to the large number of augmentations required per image. Second, it fails to mitigate prediction bias, where the model tends to predict certain classes disproportionately under corruption, as PDA operates on corrupted inputs and typically does not remove the corruption itself. To address these challenges, we propose Panda, a novel TTA method based on negative data augmentation (NDA). Unlike positive augmentations that preserve object semantics, Panda generates negative augmentations by disrupting semantic content. It divides images into patches and randomly assembles them from a shared patch pool. These negatively augmented images retain corruption-specific features while discarding object-relevant signals. We then subtract the mean feature of these negative samples from the original image feature, effectively suppressing corruption-related components while preserving class-relevant information. This mitigates prediction bias under distribution shifts. Panda allows augmentation to be shared across samples within a batch, resulting in minimal computational overhead. Panda can be seamlessly integrated into existing test-time adaptation frameworks and substantially improve their robustness. Our experiments indicate that Panda delivers superior performance compared to PDA methods, and a wide range of TTA methods exhibit significantly enhanced performance when integrated with Panda. Our code is available at https://github.com/ruxideng/Panda .", "link": "http://arxiv.org/abs/2511.10481v1", "date": "2025-11-13", "relevancy": 2.7081, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5327}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panda%3A%20Test-Time%20Adaptation%20with%20Negative%20Data%20Augmentation&body=Title%3A%20Panda%3A%20Test-Time%20Adaptation%20with%20Negative%20Data%20Augmentation%0AAuthor%3A%20Ruxi%20Deng%20and%20Wenxuan%20Bao%20and%20Tianxin%20Wei%20and%20Jingrui%20He%0AAbstract%3A%20Pretrained%20VLMs%20exhibit%20strong%20zero-shot%20classification%20capabilities%2C%20but%20their%20predictions%20degrade%20significantly%20under%20common%20image%20corruptions.%20To%20improve%20robustness%2C%20many%20test-time%20adaptation%20%28TTA%29%20methods%20adopt%20positive%20data%20augmentation%20%28PDA%29%2C%20which%20generates%20multiple%20views%20of%20each%20test%20sample%20to%20reduce%20prediction%20variance.%20However%2C%20these%20methods%20suffer%20from%20two%20key%20limitations.%20First%2C%20it%20introduces%20considerable%20computational%20overhead%20due%20to%20the%20large%20number%20of%20augmentations%20required%20per%20image.%20Second%2C%20it%20fails%20to%20mitigate%20prediction%20bias%2C%20where%20the%20model%20tends%20to%20predict%20certain%20classes%20disproportionately%20under%20corruption%2C%20as%20PDA%20operates%20on%20corrupted%20inputs%20and%20typically%20does%20not%20remove%20the%20corruption%20itself.%20To%20address%20these%20challenges%2C%20we%20propose%20Panda%2C%20a%20novel%20TTA%20method%20based%20on%20negative%20data%20augmentation%20%28NDA%29.%20Unlike%20positive%20augmentations%20that%20preserve%20object%20semantics%2C%20Panda%20generates%20negative%20augmentations%20by%20disrupting%20semantic%20content.%20It%20divides%20images%20into%20patches%20and%20randomly%20assembles%20them%20from%20a%20shared%20patch%20pool.%20These%20negatively%20augmented%20images%20retain%20corruption-specific%20features%20while%20discarding%20object-relevant%20signals.%20We%20then%20subtract%20the%20mean%20feature%20of%20these%20negative%20samples%20from%20the%20original%20image%20feature%2C%20effectively%20suppressing%20corruption-related%20components%20while%20preserving%20class-relevant%20information.%20This%20mitigates%20prediction%20bias%20under%20distribution%20shifts.%20Panda%20allows%20augmentation%20to%20be%20shared%20across%20samples%20within%20a%20batch%2C%20resulting%20in%20minimal%20computational%20overhead.%20Panda%20can%20be%20seamlessly%20integrated%20into%20existing%20test-time%20adaptation%20frameworks%20and%20substantially%20improve%20their%20robustness.%20Our%20experiments%20indicate%20that%20Panda%20delivers%20superior%20performance%20compared%20to%20PDA%20methods%2C%20and%20a%20wide%20range%20of%20TTA%20methods%20exhibit%20significantly%20enhanced%20performance%20when%20integrated%20with%20Panda.%20Our%20code%20is%20available%20at%20https%3A//github.com/ruxideng/Panda%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanda%253A%2520Test-Time%2520Adaptation%2520with%2520Negative%2520Data%2520Augmentation%26entry.906535625%3DRuxi%2520Deng%2520and%2520Wenxuan%2520Bao%2520and%2520Tianxin%2520Wei%2520and%2520Jingrui%2520He%26entry.1292438233%3DPretrained%2520VLMs%2520exhibit%2520strong%2520zero-shot%2520classification%2520capabilities%252C%2520but%2520their%2520predictions%2520degrade%2520significantly%2520under%2520common%2520image%2520corruptions.%2520To%2520improve%2520robustness%252C%2520many%2520test-time%2520adaptation%2520%2528TTA%2529%2520methods%2520adopt%2520positive%2520data%2520augmentation%2520%2528PDA%2529%252C%2520which%2520generates%2520multiple%2520views%2520of%2520each%2520test%2520sample%2520to%2520reduce%2520prediction%2520variance.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520two%2520key%2520limitations.%2520First%252C%2520it%2520introduces%2520considerable%2520computational%2520overhead%2520due%2520to%2520the%2520large%2520number%2520of%2520augmentations%2520required%2520per%2520image.%2520Second%252C%2520it%2520fails%2520to%2520mitigate%2520prediction%2520bias%252C%2520where%2520the%2520model%2520tends%2520to%2520predict%2520certain%2520classes%2520disproportionately%2520under%2520corruption%252C%2520as%2520PDA%2520operates%2520on%2520corrupted%2520inputs%2520and%2520typically%2520does%2520not%2520remove%2520the%2520corruption%2520itself.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Panda%252C%2520a%2520novel%2520TTA%2520method%2520based%2520on%2520negative%2520data%2520augmentation%2520%2528NDA%2529.%2520Unlike%2520positive%2520augmentations%2520that%2520preserve%2520object%2520semantics%252C%2520Panda%2520generates%2520negative%2520augmentations%2520by%2520disrupting%2520semantic%2520content.%2520It%2520divides%2520images%2520into%2520patches%2520and%2520randomly%2520assembles%2520them%2520from%2520a%2520shared%2520patch%2520pool.%2520These%2520negatively%2520augmented%2520images%2520retain%2520corruption-specific%2520features%2520while%2520discarding%2520object-relevant%2520signals.%2520We%2520then%2520subtract%2520the%2520mean%2520feature%2520of%2520these%2520negative%2520samples%2520from%2520the%2520original%2520image%2520feature%252C%2520effectively%2520suppressing%2520corruption-related%2520components%2520while%2520preserving%2520class-relevant%2520information.%2520This%2520mitigates%2520prediction%2520bias%2520under%2520distribution%2520shifts.%2520Panda%2520allows%2520augmentation%2520to%2520be%2520shared%2520across%2520samples%2520within%2520a%2520batch%252C%2520resulting%2520in%2520minimal%2520computational%2520overhead.%2520Panda%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520test-time%2520adaptation%2520frameworks%2520and%2520substantially%2520improve%2520their%2520robustness.%2520Our%2520experiments%2520indicate%2520that%2520Panda%2520delivers%2520superior%2520performance%2520compared%2520to%2520PDA%2520methods%252C%2520and%2520a%2520wide%2520range%2520of%2520TTA%2520methods%2520exhibit%2520significantly%2520enhanced%2520performance%2520when%2520integrated%2520with%2520Panda.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ruxideng/Panda%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panda%3A%20Test-Time%20Adaptation%20with%20Negative%20Data%20Augmentation&entry.906535625=Ruxi%20Deng%20and%20Wenxuan%20Bao%20and%20Tianxin%20Wei%20and%20Jingrui%20He&entry.1292438233=Pretrained%20VLMs%20exhibit%20strong%20zero-shot%20classification%20capabilities%2C%20but%20their%20predictions%20degrade%20significantly%20under%20common%20image%20corruptions.%20To%20improve%20robustness%2C%20many%20test-time%20adaptation%20%28TTA%29%20methods%20adopt%20positive%20data%20augmentation%20%28PDA%29%2C%20which%20generates%20multiple%20views%20of%20each%20test%20sample%20to%20reduce%20prediction%20variance.%20However%2C%20these%20methods%20suffer%20from%20two%20key%20limitations.%20First%2C%20it%20introduces%20considerable%20computational%20overhead%20due%20to%20the%20large%20number%20of%20augmentations%20required%20per%20image.%20Second%2C%20it%20fails%20to%20mitigate%20prediction%20bias%2C%20where%20the%20model%20tends%20to%20predict%20certain%20classes%20disproportionately%20under%20corruption%2C%20as%20PDA%20operates%20on%20corrupted%20inputs%20and%20typically%20does%20not%20remove%20the%20corruption%20itself.%20To%20address%20these%20challenges%2C%20we%20propose%20Panda%2C%20a%20novel%20TTA%20method%20based%20on%20negative%20data%20augmentation%20%28NDA%29.%20Unlike%20positive%20augmentations%20that%20preserve%20object%20semantics%2C%20Panda%20generates%20negative%20augmentations%20by%20disrupting%20semantic%20content.%20It%20divides%20images%20into%20patches%20and%20randomly%20assembles%20them%20from%20a%20shared%20patch%20pool.%20These%20negatively%20augmented%20images%20retain%20corruption-specific%20features%20while%20discarding%20object-relevant%20signals.%20We%20then%20subtract%20the%20mean%20feature%20of%20these%20negative%20samples%20from%20the%20original%20image%20feature%2C%20effectively%20suppressing%20corruption-related%20components%20while%20preserving%20class-relevant%20information.%20This%20mitigates%20prediction%20bias%20under%20distribution%20shifts.%20Panda%20allows%20augmentation%20to%20be%20shared%20across%20samples%20within%20a%20batch%2C%20resulting%20in%20minimal%20computational%20overhead.%20Panda%20can%20be%20seamlessly%20integrated%20into%20existing%20test-time%20adaptation%20frameworks%20and%20substantially%20improve%20their%20robustness.%20Our%20experiments%20indicate%20that%20Panda%20delivers%20superior%20performance%20compared%20to%20PDA%20methods%2C%20and%20a%20wide%20range%20of%20TTA%20methods%20exhibit%20significantly%20enhanced%20performance%20when%20integrated%20with%20Panda.%20Our%20code%20is%20available%20at%20https%3A//github.com/ruxideng/Panda%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.10481v1&entry.124074799=Read"},
{"title": "PepTriX: A Framework for Explainable Peptide Analysis through Protein Language Models", "author": "Vincent Schilling and Akshat Dubey and Georges Hattab", "abstract": "Peptide classification tasks, such as predicting toxicity and HIV inhibition, are fundamental to bioinformatics and drug discovery. Traditional approaches rely heavily on handcrafted encodings of one-dimensional (1D) peptide sequences, which can limit generalizability across tasks and datasets. Recently, protein language models (PLMs), such as ESM-2 and ESMFold, have demonstrated strong predictive performance. However, they face two critical challenges. First, fine-tuning is computationally costly. Second, their complex latent representations hinder interpretability for domain experts. Additionally, many frameworks have been developed for specific types of peptide classification, lacking generalization. These limitations restrict the ability to connect model predictions to biologically relevant motifs and structural properties. To address these limitations, we present PepTriX, a novel framework that integrates one dimensional (1D) sequence embeddings and three-dimensional (3D) structural features via a graph attention network enhanced with contrastive training and cross-modal co-attention. PepTriX automatically adapts to diverse datasets, producing task-specific peptide vectors while retaining biological plausibility. After evaluation by domain experts, we found that PepTriX performs remarkably well across multiple peptide classification tasks and provides interpretable insights into the structural and biophysical motifs that drive predictions. Thus, PepTriX offers both predictive robustness and interpretable validation, bridging the gap between performance-driven peptide-level models (PLMs) and domain-level understanding in peptide research.", "link": "http://arxiv.org/abs/2511.10244v1", "date": "2025-11-13", "relevancy": 2.7022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PepTriX%3A%20A%20Framework%20for%20Explainable%20Peptide%20Analysis%20through%20Protein%20Language%20Models&body=Title%3A%20PepTriX%3A%20A%20Framework%20for%20Explainable%20Peptide%20Analysis%20through%20Protein%20Language%20Models%0AAuthor%3A%20Vincent%20Schilling%20and%20Akshat%20Dubey%20and%20Georges%20Hattab%0AAbstract%3A%20Peptide%20classification%20tasks%2C%20such%20as%20predicting%20toxicity%20and%20HIV%20inhibition%2C%20are%20fundamental%20to%20bioinformatics%20and%20drug%20discovery.%20Traditional%20approaches%20rely%20heavily%20on%20handcrafted%20encodings%20of%20one-dimensional%20%281D%29%20peptide%20sequences%2C%20which%20can%20limit%20generalizability%20across%20tasks%20and%20datasets.%20Recently%2C%20protein%20language%20models%20%28PLMs%29%2C%20such%20as%20ESM-2%20and%20ESMFold%2C%20have%20demonstrated%20strong%20predictive%20performance.%20However%2C%20they%20face%20two%20critical%20challenges.%20First%2C%20fine-tuning%20is%20computationally%20costly.%20Second%2C%20their%20complex%20latent%20representations%20hinder%20interpretability%20for%20domain%20experts.%20Additionally%2C%20many%20frameworks%20have%20been%20developed%20for%20specific%20types%20of%20peptide%20classification%2C%20lacking%20generalization.%20These%20limitations%20restrict%20the%20ability%20to%20connect%20model%20predictions%20to%20biologically%20relevant%20motifs%20and%20structural%20properties.%20To%20address%20these%20limitations%2C%20we%20present%20PepTriX%2C%20a%20novel%20framework%20that%20integrates%20one%20dimensional%20%281D%29%20sequence%20embeddings%20and%20three-dimensional%20%283D%29%20structural%20features%20via%20a%20graph%20attention%20network%20enhanced%20with%20contrastive%20training%20and%20cross-modal%20co-attention.%20PepTriX%20automatically%20adapts%20to%20diverse%20datasets%2C%20producing%20task-specific%20peptide%20vectors%20while%20retaining%20biological%20plausibility.%20After%20evaluation%20by%20domain%20experts%2C%20we%20found%20that%20PepTriX%20performs%20remarkably%20well%20across%20multiple%20peptide%20classification%20tasks%20and%20provides%20interpretable%20insights%20into%20the%20structural%20and%20biophysical%20motifs%20that%20drive%20predictions.%20Thus%2C%20PepTriX%20offers%20both%20predictive%20robustness%20and%20interpretable%20validation%2C%20bridging%20the%20gap%20between%20performance-driven%20peptide-level%20models%20%28PLMs%29%20and%20domain-level%20understanding%20in%20peptide%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPepTriX%253A%2520A%2520Framework%2520for%2520Explainable%2520Peptide%2520Analysis%2520through%2520Protein%2520Language%2520Models%26entry.906535625%3DVincent%2520Schilling%2520and%2520Akshat%2520Dubey%2520and%2520Georges%2520Hattab%26entry.1292438233%3DPeptide%2520classification%2520tasks%252C%2520such%2520as%2520predicting%2520toxicity%2520and%2520HIV%2520inhibition%252C%2520are%2520fundamental%2520to%2520bioinformatics%2520and%2520drug%2520discovery.%2520Traditional%2520approaches%2520rely%2520heavily%2520on%2520handcrafted%2520encodings%2520of%2520one-dimensional%2520%25281D%2529%2520peptide%2520sequences%252C%2520which%2520can%2520limit%2520generalizability%2520across%2520tasks%2520and%2520datasets.%2520Recently%252C%2520protein%2520language%2520models%2520%2528PLMs%2529%252C%2520such%2520as%2520ESM-2%2520and%2520ESMFold%252C%2520have%2520demonstrated%2520strong%2520predictive%2520performance.%2520However%252C%2520they%2520face%2520two%2520critical%2520challenges.%2520First%252C%2520fine-tuning%2520is%2520computationally%2520costly.%2520Second%252C%2520their%2520complex%2520latent%2520representations%2520hinder%2520interpretability%2520for%2520domain%2520experts.%2520Additionally%252C%2520many%2520frameworks%2520have%2520been%2520developed%2520for%2520specific%2520types%2520of%2520peptide%2520classification%252C%2520lacking%2520generalization.%2520These%2520limitations%2520restrict%2520the%2520ability%2520to%2520connect%2520model%2520predictions%2520to%2520biologically%2520relevant%2520motifs%2520and%2520structural%2520properties.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520PepTriX%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520one%2520dimensional%2520%25281D%2529%2520sequence%2520embeddings%2520and%2520three-dimensional%2520%25283D%2529%2520structural%2520features%2520via%2520a%2520graph%2520attention%2520network%2520enhanced%2520with%2520contrastive%2520training%2520and%2520cross-modal%2520co-attention.%2520PepTriX%2520automatically%2520adapts%2520to%2520diverse%2520datasets%252C%2520producing%2520task-specific%2520peptide%2520vectors%2520while%2520retaining%2520biological%2520plausibility.%2520After%2520evaluation%2520by%2520domain%2520experts%252C%2520we%2520found%2520that%2520PepTriX%2520performs%2520remarkably%2520well%2520across%2520multiple%2520peptide%2520classification%2520tasks%2520and%2520provides%2520interpretable%2520insights%2520into%2520the%2520structural%2520and%2520biophysical%2520motifs%2520that%2520drive%2520predictions.%2520Thus%252C%2520PepTriX%2520offers%2520both%2520predictive%2520robustness%2520and%2520interpretable%2520validation%252C%2520bridging%2520the%2520gap%2520between%2520performance-driven%2520peptide-level%2520models%2520%2528PLMs%2529%2520and%2520domain-level%2520understanding%2520in%2520peptide%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PepTriX%3A%20A%20Framework%20for%20Explainable%20Peptide%20Analysis%20through%20Protein%20Language%20Models&entry.906535625=Vincent%20Schilling%20and%20Akshat%20Dubey%20and%20Georges%20Hattab&entry.1292438233=Peptide%20classification%20tasks%2C%20such%20as%20predicting%20toxicity%20and%20HIV%20inhibition%2C%20are%20fundamental%20to%20bioinformatics%20and%20drug%20discovery.%20Traditional%20approaches%20rely%20heavily%20on%20handcrafted%20encodings%20of%20one-dimensional%20%281D%29%20peptide%20sequences%2C%20which%20can%20limit%20generalizability%20across%20tasks%20and%20datasets.%20Recently%2C%20protein%20language%20models%20%28PLMs%29%2C%20such%20as%20ESM-2%20and%20ESMFold%2C%20have%20demonstrated%20strong%20predictive%20performance.%20However%2C%20they%20face%20two%20critical%20challenges.%20First%2C%20fine-tuning%20is%20computationally%20costly.%20Second%2C%20their%20complex%20latent%20representations%20hinder%20interpretability%20for%20domain%20experts.%20Additionally%2C%20many%20frameworks%20have%20been%20developed%20for%20specific%20types%20of%20peptide%20classification%2C%20lacking%20generalization.%20These%20limitations%20restrict%20the%20ability%20to%20connect%20model%20predictions%20to%20biologically%20relevant%20motifs%20and%20structural%20properties.%20To%20address%20these%20limitations%2C%20we%20present%20PepTriX%2C%20a%20novel%20framework%20that%20integrates%20one%20dimensional%20%281D%29%20sequence%20embeddings%20and%20three-dimensional%20%283D%29%20structural%20features%20via%20a%20graph%20attention%20network%20enhanced%20with%20contrastive%20training%20and%20cross-modal%20co-attention.%20PepTriX%20automatically%20adapts%20to%20diverse%20datasets%2C%20producing%20task-specific%20peptide%20vectors%20while%20retaining%20biological%20plausibility.%20After%20evaluation%20by%20domain%20experts%2C%20we%20found%20that%20PepTriX%20performs%20remarkably%20well%20across%20multiple%20peptide%20classification%20tasks%20and%20provides%20interpretable%20insights%20into%20the%20structural%20and%20biophysical%20motifs%20that%20drive%20predictions.%20Thus%2C%20PepTriX%20offers%20both%20predictive%20robustness%20and%20interpretable%20validation%2C%20bridging%20the%20gap%20between%20performance-driven%20peptide-level%20models%20%28PLMs%29%20and%20domain-level%20understanding%20in%20peptide%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.10244v1&entry.124074799=Read"},
{"title": "Generalizable Slum Detection from Satellite Imagery with Mixture-of-Experts", "author": "Sumin Lee and Sungwon Park and Jeasurk Yang and Jihee Kim and Meeyoung Cha", "abstract": "Satellite-based slum segmentation holds significant promise in generating global estimates of urban poverty. However, the morphological heterogeneity of informal settlements presents a major challenge, hindering the ability of models trained on specific regions to generalize effectively to unseen locations. To address this, we introduce a large-scale high-resolution dataset and propose GRAM (Generalized Region-Aware Mixture-of-Experts), a two-phase test-time adaptation framework that enables robust slum segmentation without requiring labeled data from target regions. We compile a million-scale satellite imagery dataset from 12 cities across four continents for source training. Using this dataset, the model employs a Mixture-of-Experts architecture to capture region-specific slum characteristics while learning universal features through a shared backbone. During adaptation, prediction consistency across experts filters out unreliable pseudo-labels, allowing the model to generalize effectively to previously unseen regions. GRAM outperforms state-of-the-art baselines in low-resource settings such as African cities, offering a scalable and label-efficient solution for global slum mapping and data-driven urban planning.", "link": "http://arxiv.org/abs/2511.10300v1", "date": "2025-11-13", "relevancy": 2.6944, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5346}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Slum%20Detection%20from%20Satellite%20Imagery%20with%20Mixture-of-Experts&body=Title%3A%20Generalizable%20Slum%20Detection%20from%20Satellite%20Imagery%20with%20Mixture-of-Experts%0AAuthor%3A%20Sumin%20Lee%20and%20Sungwon%20Park%20and%20Jeasurk%20Yang%20and%20Jihee%20Kim%20and%20Meeyoung%20Cha%0AAbstract%3A%20Satellite-based%20slum%20segmentation%20holds%20significant%20promise%20in%20generating%20global%20estimates%20of%20urban%20poverty.%20However%2C%20the%20morphological%20heterogeneity%20of%20informal%20settlements%20presents%20a%20major%20challenge%2C%20hindering%20the%20ability%20of%20models%20trained%20on%20specific%20regions%20to%20generalize%20effectively%20to%20unseen%20locations.%20To%20address%20this%2C%20we%20introduce%20a%20large-scale%20high-resolution%20dataset%20and%20propose%20GRAM%20%28Generalized%20Region-Aware%20Mixture-of-Experts%29%2C%20a%20two-phase%20test-time%20adaptation%20framework%20that%20enables%20robust%20slum%20segmentation%20without%20requiring%20labeled%20data%20from%20target%20regions.%20We%20compile%20a%20million-scale%20satellite%20imagery%20dataset%20from%2012%20cities%20across%20four%20continents%20for%20source%20training.%20Using%20this%20dataset%2C%20the%20model%20employs%20a%20Mixture-of-Experts%20architecture%20to%20capture%20region-specific%20slum%20characteristics%20while%20learning%20universal%20features%20through%20a%20shared%20backbone.%20During%20adaptation%2C%20prediction%20consistency%20across%20experts%20filters%20out%20unreliable%20pseudo-labels%2C%20allowing%20the%20model%20to%20generalize%20effectively%20to%20previously%20unseen%20regions.%20GRAM%20outperforms%20state-of-the-art%20baselines%20in%20low-resource%20settings%20such%20as%20African%20cities%2C%20offering%20a%20scalable%20and%20label-efficient%20solution%20for%20global%20slum%20mapping%20and%20data-driven%20urban%20planning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Slum%2520Detection%2520from%2520Satellite%2520Imagery%2520with%2520Mixture-of-Experts%26entry.906535625%3DSumin%2520Lee%2520and%2520Sungwon%2520Park%2520and%2520Jeasurk%2520Yang%2520and%2520Jihee%2520Kim%2520and%2520Meeyoung%2520Cha%26entry.1292438233%3DSatellite-based%2520slum%2520segmentation%2520holds%2520significant%2520promise%2520in%2520generating%2520global%2520estimates%2520of%2520urban%2520poverty.%2520However%252C%2520the%2520morphological%2520heterogeneity%2520of%2520informal%2520settlements%2520presents%2520a%2520major%2520challenge%252C%2520hindering%2520the%2520ability%2520of%2520models%2520trained%2520on%2520specific%2520regions%2520to%2520generalize%2520effectively%2520to%2520unseen%2520locations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520large-scale%2520high-resolution%2520dataset%2520and%2520propose%2520GRAM%2520%2528Generalized%2520Region-Aware%2520Mixture-of-Experts%2529%252C%2520a%2520two-phase%2520test-time%2520adaptation%2520framework%2520that%2520enables%2520robust%2520slum%2520segmentation%2520without%2520requiring%2520labeled%2520data%2520from%2520target%2520regions.%2520We%2520compile%2520a%2520million-scale%2520satellite%2520imagery%2520dataset%2520from%252012%2520cities%2520across%2520four%2520continents%2520for%2520source%2520training.%2520Using%2520this%2520dataset%252C%2520the%2520model%2520employs%2520a%2520Mixture-of-Experts%2520architecture%2520to%2520capture%2520region-specific%2520slum%2520characteristics%2520while%2520learning%2520universal%2520features%2520through%2520a%2520shared%2520backbone.%2520During%2520adaptation%252C%2520prediction%2520consistency%2520across%2520experts%2520filters%2520out%2520unreliable%2520pseudo-labels%252C%2520allowing%2520the%2520model%2520to%2520generalize%2520effectively%2520to%2520previously%2520unseen%2520regions.%2520GRAM%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520low-resource%2520settings%2520such%2520as%2520African%2520cities%252C%2520offering%2520a%2520scalable%2520and%2520label-efficient%2520solution%2520for%2520global%2520slum%2520mapping%2520and%2520data-driven%2520urban%2520planning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Slum%20Detection%20from%20Satellite%20Imagery%20with%20Mixture-of-Experts&entry.906535625=Sumin%20Lee%20and%20Sungwon%20Park%20and%20Jeasurk%20Yang%20and%20Jihee%20Kim%20and%20Meeyoung%20Cha&entry.1292438233=Satellite-based%20slum%20segmentation%20holds%20significant%20promise%20in%20generating%20global%20estimates%20of%20urban%20poverty.%20However%2C%20the%20morphological%20heterogeneity%20of%20informal%20settlements%20presents%20a%20major%20challenge%2C%20hindering%20the%20ability%20of%20models%20trained%20on%20specific%20regions%20to%20generalize%20effectively%20to%20unseen%20locations.%20To%20address%20this%2C%20we%20introduce%20a%20large-scale%20high-resolution%20dataset%20and%20propose%20GRAM%20%28Generalized%20Region-Aware%20Mixture-of-Experts%29%2C%20a%20two-phase%20test-time%20adaptation%20framework%20that%20enables%20robust%20slum%20segmentation%20without%20requiring%20labeled%20data%20from%20target%20regions.%20We%20compile%20a%20million-scale%20satellite%20imagery%20dataset%20from%2012%20cities%20across%20four%20continents%20for%20source%20training.%20Using%20this%20dataset%2C%20the%20model%20employs%20a%20Mixture-of-Experts%20architecture%20to%20capture%20region-specific%20slum%20characteristics%20while%20learning%20universal%20features%20through%20a%20shared%20backbone.%20During%20adaptation%2C%20prediction%20consistency%20across%20experts%20filters%20out%20unreliable%20pseudo-labels%2C%20allowing%20the%20model%20to%20generalize%20effectively%20to%20previously%20unseen%20regions.%20GRAM%20outperforms%20state-of-the-art%20baselines%20in%20low-resource%20settings%20such%20as%20African%20cities%2C%20offering%20a%20scalable%20and%20label-efficient%20solution%20for%20global%20slum%20mapping%20and%20data-driven%20urban%20planning.&entry.1838667208=http%3A//arxiv.org/abs/2511.10300v1&entry.124074799=Read"},
{"title": "Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space", "author": "Zhicheng Cai and Hao Zhu and Linsen Chen and Qiu Shen and Xun Cao", "abstract": "Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INR's representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.", "link": "http://arxiv.org/abs/2511.10142v1", "date": "2025-11-13", "relevancy": 2.6914, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5665}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.529}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Split-Layer%3A%20Enhancing%20Implicit%20Neural%20Representation%20by%20Maximizing%20the%20Dimensionality%20of%20Feature%20Space&body=Title%3A%20Split-Layer%3A%20Enhancing%20Implicit%20Neural%20Representation%20by%20Maximizing%20the%20Dimensionality%20of%20Feature%20Space%0AAuthor%3A%20Zhicheng%20Cai%20and%20Hao%20Zhu%20and%20Linsen%20Chen%20and%20Qiu%20Shen%20and%20Xun%20Cao%0AAbstract%3A%20Implicit%20neural%20representation%20%28INR%29%20models%20signals%20as%20continuous%20functions%20using%20neural%20networks%2C%20offering%20efficient%20and%20differentiable%20optimization%20for%20inverse%20problems%20across%20diverse%20disciplines.%20However%2C%20the%20representational%20capacity%20of%20INR%20defined%20by%20the%20range%20of%20functions%20the%20neural%20network%20can%20characterize%2C%20is%20inherently%20limited%20by%20the%20low-dimensional%20feature%20space%20in%20conventional%20multilayer%20perceptron%20%28MLP%29%20architectures.%20While%20widening%20the%20MLP%20can%20linearly%20increase%20feature%20space%20dimensionality%2C%20it%20also%20leads%20to%20a%20quadratic%20growth%20in%20computational%20and%20memory%20costs.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20split-layer%2C%20a%20novel%20reformulation%20of%20MLP%20construction.%20The%20split-layer%20divides%20each%20layer%20into%20multiple%20parallel%20branches%20and%20integrates%20their%20outputs%20via%20Hadamard%20product%2C%20effectively%20constructing%20a%20high-degree%20polynomial%20space.%20This%20approach%20significantly%20enhances%20INR%27s%20representational%20capacity%20by%20expanding%20the%20feature%20space%20dimensionality%20without%20incurring%20prohibitive%20computational%20overhead.%20Extensive%20experiments%20demonstrate%20that%20the%20split-layer%20substantially%20improves%20INR%20performance%2C%20surpassing%20existing%20methods%20across%20multiple%20tasks%2C%20including%202D%20image%20fitting%2C%202D%20CT%20reconstruction%2C%203D%20shape%20representation%2C%20and%205D%20novel%20view%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplit-Layer%253A%2520Enhancing%2520Implicit%2520Neural%2520Representation%2520by%2520Maximizing%2520the%2520Dimensionality%2520of%2520Feature%2520Space%26entry.906535625%3DZhicheng%2520Cai%2520and%2520Hao%2520Zhu%2520and%2520Linsen%2520Chen%2520and%2520Qiu%2520Shen%2520and%2520Xun%2520Cao%26entry.1292438233%3DImplicit%2520neural%2520representation%2520%2528INR%2529%2520models%2520signals%2520as%2520continuous%2520functions%2520using%2520neural%2520networks%252C%2520offering%2520efficient%2520and%2520differentiable%2520optimization%2520for%2520inverse%2520problems%2520across%2520diverse%2520disciplines.%2520However%252C%2520the%2520representational%2520capacity%2520of%2520INR%2520defined%2520by%2520the%2520range%2520of%2520functions%2520the%2520neural%2520network%2520can%2520characterize%252C%2520is%2520inherently%2520limited%2520by%2520the%2520low-dimensional%2520feature%2520space%2520in%2520conventional%2520multilayer%2520perceptron%2520%2528MLP%2529%2520architectures.%2520While%2520widening%2520the%2520MLP%2520can%2520linearly%2520increase%2520feature%2520space%2520dimensionality%252C%2520it%2520also%2520leads%2520to%2520a%2520quadratic%2520growth%2520in%2520computational%2520and%2520memory%2520costs.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520the%2520split-layer%252C%2520a%2520novel%2520reformulation%2520of%2520MLP%2520construction.%2520The%2520split-layer%2520divides%2520each%2520layer%2520into%2520multiple%2520parallel%2520branches%2520and%2520integrates%2520their%2520outputs%2520via%2520Hadamard%2520product%252C%2520effectively%2520constructing%2520a%2520high-degree%2520polynomial%2520space.%2520This%2520approach%2520significantly%2520enhances%2520INR%2527s%2520representational%2520capacity%2520by%2520expanding%2520the%2520feature%2520space%2520dimensionality%2520without%2520incurring%2520prohibitive%2520computational%2520overhead.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520split-layer%2520substantially%2520improves%2520INR%2520performance%252C%2520surpassing%2520existing%2520methods%2520across%2520multiple%2520tasks%252C%2520including%25202D%2520image%2520fitting%252C%25202D%2520CT%2520reconstruction%252C%25203D%2520shape%2520representation%252C%2520and%25205D%2520novel%2520view%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Split-Layer%3A%20Enhancing%20Implicit%20Neural%20Representation%20by%20Maximizing%20the%20Dimensionality%20of%20Feature%20Space&entry.906535625=Zhicheng%20Cai%20and%20Hao%20Zhu%20and%20Linsen%20Chen%20and%20Qiu%20Shen%20and%20Xun%20Cao&entry.1292438233=Implicit%20neural%20representation%20%28INR%29%20models%20signals%20as%20continuous%20functions%20using%20neural%20networks%2C%20offering%20efficient%20and%20differentiable%20optimization%20for%20inverse%20problems%20across%20diverse%20disciplines.%20However%2C%20the%20representational%20capacity%20of%20INR%20defined%20by%20the%20range%20of%20functions%20the%20neural%20network%20can%20characterize%2C%20is%20inherently%20limited%20by%20the%20low-dimensional%20feature%20space%20in%20conventional%20multilayer%20perceptron%20%28MLP%29%20architectures.%20While%20widening%20the%20MLP%20can%20linearly%20increase%20feature%20space%20dimensionality%2C%20it%20also%20leads%20to%20a%20quadratic%20growth%20in%20computational%20and%20memory%20costs.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20split-layer%2C%20a%20novel%20reformulation%20of%20MLP%20construction.%20The%20split-layer%20divides%20each%20layer%20into%20multiple%20parallel%20branches%20and%20integrates%20their%20outputs%20via%20Hadamard%20product%2C%20effectively%20constructing%20a%20high-degree%20polynomial%20space.%20This%20approach%20significantly%20enhances%20INR%27s%20representational%20capacity%20by%20expanding%20the%20feature%20space%20dimensionality%20without%20incurring%20prohibitive%20computational%20overhead.%20Extensive%20experiments%20demonstrate%20that%20the%20split-layer%20substantially%20improves%20INR%20performance%2C%20surpassing%20existing%20methods%20across%20multiple%20tasks%2C%20including%202D%20image%20fitting%2C%202D%20CT%20reconstruction%2C%203D%20shape%20representation%2C%20and%205D%20novel%20view%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2511.10142v1&entry.124074799=Read"},
{"title": "Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention", "author": "Zhe Xu and Zhicai Wang and Junkang Wu and Jinda Lu and Xiang Wang", "abstract": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, making erroneous judgments about the presence of objects in images. We propose this primar- ily stems from spurious correlations arising when models strongly associate highly co-occurring objects during train- ing, leading to hallucinated objects influenced by visual con- text. Current benchmarks mainly focus on hallucination de- tection but lack a formal characterization and quantitative evaluation of spurious correlations in LVLMs. To address this, we introduce causal analysis into the object recognition scenario of LVLMs, establishing a Structural Causal Model (SCM). Utilizing the language of causality, we formally de- fine spurious correlations arising from co-occurrence bias. To quantify the influence induced by these spurious correla- tions, we develop Causal-HalBench, a benchmark specifically constructed with counterfactual samples and integrated with comprehensive causal metrics designed to assess model ro- bustness against spurious correlations. Concurrently, we pro- pose an extensible pipeline for the construction of these coun- terfactual samples, leveraging the capabilities of proprietary LVLMs and Text-to-Image (T2I) models for their genera- tion. Our evaluations on mainstream LVLMs using Causal- HalBench demonstrate these models exhibit susceptibility to spurious correlations, albeit to varying extents.", "link": "http://arxiv.org/abs/2511.10268v1", "date": "2025-11-13", "relevancy": 2.6355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-HalBench%3A%20Uncovering%20LVLMs%20Object%20Hallucinations%20Through%20Causal%20Intervention&body=Title%3A%20Causal-HalBench%3A%20Uncovering%20LVLMs%20Object%20Hallucinations%20Through%20Causal%20Intervention%0AAuthor%3A%20Zhe%20Xu%20and%20Zhicai%20Wang%20and%20Junkang%20Wu%20and%20Jinda%20Lu%20and%20Xiang%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%20making%20erroneous%20judgments%20about%20the%20presence%20of%20objects%20in%20images.%20We%20propose%20this%20primar-%20ily%20stems%20from%20spurious%20correlations%20arising%20when%20models%20strongly%20associate%20highly%20co-occurring%20objects%20during%20train-%20ing%2C%20leading%20to%20hallucinated%20objects%20influenced%20by%20visual%20con-%20text.%20Current%20benchmarks%20mainly%20focus%20on%20hallucination%20de-%20tection%20but%20lack%20a%20formal%20characterization%20and%20quantitative%20evaluation%20of%20spurious%20correlations%20in%20LVLMs.%20To%20address%20this%2C%20we%20introduce%20causal%20analysis%20into%20the%20object%20recognition%20scenario%20of%20LVLMs%2C%20establishing%20a%20Structural%20Causal%20Model%20%28SCM%29.%20Utilizing%20the%20language%20of%20causality%2C%20we%20formally%20de-%20fine%20spurious%20correlations%20arising%20from%20co-occurrence%20bias.%20To%20quantify%20the%20influence%20induced%20by%20these%20spurious%20correla-%20tions%2C%20we%20develop%20Causal-HalBench%2C%20a%20benchmark%20specifically%20constructed%20with%20counterfactual%20samples%20and%20integrated%20with%20comprehensive%20causal%20metrics%20designed%20to%20assess%20model%20ro-%20bustness%20against%20spurious%20correlations.%20Concurrently%2C%20we%20pro-%20pose%20an%20extensible%20pipeline%20for%20the%20construction%20of%20these%20coun-%20terfactual%20samples%2C%20leveraging%20the%20capabilities%20of%20proprietary%20LVLMs%20and%20Text-to-Image%20%28T2I%29%20models%20for%20their%20genera-%20tion.%20Our%20evaluations%20on%20mainstream%20LVLMs%20using%20Causal-%20HalBench%20demonstrate%20these%20models%20exhibit%20susceptibility%20to%20spurious%20correlations%2C%20albeit%20to%20varying%20extents.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-HalBench%253A%2520Uncovering%2520LVLMs%2520Object%2520Hallucinations%2520Through%2520Causal%2520Intervention%26entry.906535625%3DZhe%2520Xu%2520and%2520Zhicai%2520Wang%2520and%2520Junkang%2520Wu%2520and%2520Jinda%2520Lu%2520and%2520Xiang%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520often%2520suffer%2520from%2520object%2520hallucination%252C%2520making%2520erroneous%2520judgments%2520about%2520the%2520presence%2520of%2520objects%2520in%2520images.%2520We%2520propose%2520this%2520primar-%2520ily%2520stems%2520from%2520spurious%2520correlations%2520arising%2520when%2520models%2520strongly%2520associate%2520highly%2520co-occurring%2520objects%2520during%2520train-%2520ing%252C%2520leading%2520to%2520hallucinated%2520objects%2520influenced%2520by%2520visual%2520con-%2520text.%2520Current%2520benchmarks%2520mainly%2520focus%2520on%2520hallucination%2520de-%2520tection%2520but%2520lack%2520a%2520formal%2520characterization%2520and%2520quantitative%2520evaluation%2520of%2520spurious%2520correlations%2520in%2520LVLMs.%2520To%2520address%2520this%252C%2520we%2520introduce%2520causal%2520analysis%2520into%2520the%2520object%2520recognition%2520scenario%2520of%2520LVLMs%252C%2520establishing%2520a%2520Structural%2520Causal%2520Model%2520%2528SCM%2529.%2520Utilizing%2520the%2520language%2520of%2520causality%252C%2520we%2520formally%2520de-%2520fine%2520spurious%2520correlations%2520arising%2520from%2520co-occurrence%2520bias.%2520To%2520quantify%2520the%2520influence%2520induced%2520by%2520these%2520spurious%2520correla-%2520tions%252C%2520we%2520develop%2520Causal-HalBench%252C%2520a%2520benchmark%2520specifically%2520constructed%2520with%2520counterfactual%2520samples%2520and%2520integrated%2520with%2520comprehensive%2520causal%2520metrics%2520designed%2520to%2520assess%2520model%2520ro-%2520bustness%2520against%2520spurious%2520correlations.%2520Concurrently%252C%2520we%2520pro-%2520pose%2520an%2520extensible%2520pipeline%2520for%2520the%2520construction%2520of%2520these%2520coun-%2520terfactual%2520samples%252C%2520leveraging%2520the%2520capabilities%2520of%2520proprietary%2520LVLMs%2520and%2520Text-to-Image%2520%2528T2I%2529%2520models%2520for%2520their%2520genera-%2520tion.%2520Our%2520evaluations%2520on%2520mainstream%2520LVLMs%2520using%2520Causal-%2520HalBench%2520demonstrate%2520these%2520models%2520exhibit%2520susceptibility%2520to%2520spurious%2520correlations%252C%2520albeit%2520to%2520varying%2520extents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-HalBench%3A%20Uncovering%20LVLMs%20Object%20Hallucinations%20Through%20Causal%20Intervention&entry.906535625=Zhe%20Xu%20and%20Zhicai%20Wang%20and%20Junkang%20Wu%20and%20Jinda%20Lu%20and%20Xiang%20Wang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%20making%20erroneous%20judgments%20about%20the%20presence%20of%20objects%20in%20images.%20We%20propose%20this%20primar-%20ily%20stems%20from%20spurious%20correlations%20arising%20when%20models%20strongly%20associate%20highly%20co-occurring%20objects%20during%20train-%20ing%2C%20leading%20to%20hallucinated%20objects%20influenced%20by%20visual%20con-%20text.%20Current%20benchmarks%20mainly%20focus%20on%20hallucination%20de-%20tection%20but%20lack%20a%20formal%20characterization%20and%20quantitative%20evaluation%20of%20spurious%20correlations%20in%20LVLMs.%20To%20address%20this%2C%20we%20introduce%20causal%20analysis%20into%20the%20object%20recognition%20scenario%20of%20LVLMs%2C%20establishing%20a%20Structural%20Causal%20Model%20%28SCM%29.%20Utilizing%20the%20language%20of%20causality%2C%20we%20formally%20de-%20fine%20spurious%20correlations%20arising%20from%20co-occurrence%20bias.%20To%20quantify%20the%20influence%20induced%20by%20these%20spurious%20correla-%20tions%2C%20we%20develop%20Causal-HalBench%2C%20a%20benchmark%20specifically%20constructed%20with%20counterfactual%20samples%20and%20integrated%20with%20comprehensive%20causal%20metrics%20designed%20to%20assess%20model%20ro-%20bustness%20against%20spurious%20correlations.%20Concurrently%2C%20we%20pro-%20pose%20an%20extensible%20pipeline%20for%20the%20construction%20of%20these%20coun-%20terfactual%20samples%2C%20leveraging%20the%20capabilities%20of%20proprietary%20LVLMs%20and%20Text-to-Image%20%28T2I%29%20models%20for%20their%20genera-%20tion.%20Our%20evaluations%20on%20mainstream%20LVLMs%20using%20Causal-%20HalBench%20demonstrate%20these%20models%20exhibit%20susceptibility%20to%20spurious%20correlations%2C%20albeit%20to%20varying%20extents.&entry.1838667208=http%3A//arxiv.org/abs/2511.10268v1&entry.124074799=Read"},
{"title": "Semi-Unified Sparse Dictionary Learning with Learnable Top-K LISTA and FISTA Encoders", "author": "Fengsheng Lin and Shengyi Yan and Trac Duy Tran", "abstract": "We present a semi-unified sparse dictionary learning framework that bridges the gap between classical sparse models and modern deep architectures. Specifically, the method integrates strict Top-$K$ LISTA and its convex FISTA-based variant (LISTAConv) into the discriminative LC-KSVD2 model, enabling co-evolution between the sparse encoder and the dictionary under supervised or unsupervised regimes. This unified design retains the interpretability of traditional sparse coding while benefiting from efficient, differentiable training.\n  We further establish a PALM-style convergence analysis for the convex variant, ensuring theoretical stability under block alternation. Experimentally, our method achieves 95.6\\% on CIFAR-10, 86.3\\% on CIFAR-100, and 88.5\\% on TinyImageNet with faster convergence and lower memory cost ($<$4GB GPU). The results confirm that the proposed LC-KSVD2 + LISTA/LISTAConv pipeline offers an interpretable and computationally efficient alternative for modern deep architectures.", "link": "http://arxiv.org/abs/2511.10575v1", "date": "2025-11-13", "relevancy": 2.6349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Unified%20Sparse%20Dictionary%20Learning%20with%20Learnable%20Top-K%20LISTA%20and%20FISTA%20Encoders&body=Title%3A%20Semi-Unified%20Sparse%20Dictionary%20Learning%20with%20Learnable%20Top-K%20LISTA%20and%20FISTA%20Encoders%0AAuthor%3A%20Fengsheng%20Lin%20and%20Shengyi%20Yan%20and%20Trac%20Duy%20Tran%0AAbstract%3A%20We%20present%20a%20semi-unified%20sparse%20dictionary%20learning%20framework%20that%20bridges%20the%20gap%20between%20classical%20sparse%20models%20and%20modern%20deep%20architectures.%20Specifically%2C%20the%20method%20integrates%20strict%20Top-%24K%24%20LISTA%20and%20its%20convex%20FISTA-based%20variant%20%28LISTAConv%29%20into%20the%20discriminative%20LC-KSVD2%20model%2C%20enabling%20co-evolution%20between%20the%20sparse%20encoder%20and%20the%20dictionary%20under%20supervised%20or%20unsupervised%20regimes.%20This%20unified%20design%20retains%20the%20interpretability%20of%20traditional%20sparse%20coding%20while%20benefiting%20from%20efficient%2C%20differentiable%20training.%0A%20%20We%20further%20establish%20a%20PALM-style%20convergence%20analysis%20for%20the%20convex%20variant%2C%20ensuring%20theoretical%20stability%20under%20block%20alternation.%20Experimentally%2C%20our%20method%20achieves%2095.6%5C%25%20on%20CIFAR-10%2C%2086.3%5C%25%20on%20CIFAR-100%2C%20and%2088.5%5C%25%20on%20TinyImageNet%20with%20faster%20convergence%20and%20lower%20memory%20cost%20%28%24%3C%244GB%20GPU%29.%20The%20results%20confirm%20that%20the%20proposed%20LC-KSVD2%20%2B%20LISTA/LISTAConv%20pipeline%20offers%20an%20interpretable%20and%20computationally%20efficient%20alternative%20for%20modern%20deep%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Unified%2520Sparse%2520Dictionary%2520Learning%2520with%2520Learnable%2520Top-K%2520LISTA%2520and%2520FISTA%2520Encoders%26entry.906535625%3DFengsheng%2520Lin%2520and%2520Shengyi%2520Yan%2520and%2520Trac%2520Duy%2520Tran%26entry.1292438233%3DWe%2520present%2520a%2520semi-unified%2520sparse%2520dictionary%2520learning%2520framework%2520that%2520bridges%2520the%2520gap%2520between%2520classical%2520sparse%2520models%2520and%2520modern%2520deep%2520architectures.%2520Specifically%252C%2520the%2520method%2520integrates%2520strict%2520Top-%2524K%2524%2520LISTA%2520and%2520its%2520convex%2520FISTA-based%2520variant%2520%2528LISTAConv%2529%2520into%2520the%2520discriminative%2520LC-KSVD2%2520model%252C%2520enabling%2520co-evolution%2520between%2520the%2520sparse%2520encoder%2520and%2520the%2520dictionary%2520under%2520supervised%2520or%2520unsupervised%2520regimes.%2520This%2520unified%2520design%2520retains%2520the%2520interpretability%2520of%2520traditional%2520sparse%2520coding%2520while%2520benefiting%2520from%2520efficient%252C%2520differentiable%2520training.%250A%2520%2520We%2520further%2520establish%2520a%2520PALM-style%2520convergence%2520analysis%2520for%2520the%2520convex%2520variant%252C%2520ensuring%2520theoretical%2520stability%2520under%2520block%2520alternation.%2520Experimentally%252C%2520our%2520method%2520achieves%252095.6%255C%2525%2520on%2520CIFAR-10%252C%252086.3%255C%2525%2520on%2520CIFAR-100%252C%2520and%252088.5%255C%2525%2520on%2520TinyImageNet%2520with%2520faster%2520convergence%2520and%2520lower%2520memory%2520cost%2520%2528%2524%253C%25244GB%2520GPU%2529.%2520The%2520results%2520confirm%2520that%2520the%2520proposed%2520LC-KSVD2%2520%252B%2520LISTA/LISTAConv%2520pipeline%2520offers%2520an%2520interpretable%2520and%2520computationally%2520efficient%2520alternative%2520for%2520modern%2520deep%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Unified%20Sparse%20Dictionary%20Learning%20with%20Learnable%20Top-K%20LISTA%20and%20FISTA%20Encoders&entry.906535625=Fengsheng%20Lin%20and%20Shengyi%20Yan%20and%20Trac%20Duy%20Tran&entry.1292438233=We%20present%20a%20semi-unified%20sparse%20dictionary%20learning%20framework%20that%20bridges%20the%20gap%20between%20classical%20sparse%20models%20and%20modern%20deep%20architectures.%20Specifically%2C%20the%20method%20integrates%20strict%20Top-%24K%24%20LISTA%20and%20its%20convex%20FISTA-based%20variant%20%28LISTAConv%29%20into%20the%20discriminative%20LC-KSVD2%20model%2C%20enabling%20co-evolution%20between%20the%20sparse%20encoder%20and%20the%20dictionary%20under%20supervised%20or%20unsupervised%20regimes.%20This%20unified%20design%20retains%20the%20interpretability%20of%20traditional%20sparse%20coding%20while%20benefiting%20from%20efficient%2C%20differentiable%20training.%0A%20%20We%20further%20establish%20a%20PALM-style%20convergence%20analysis%20for%20the%20convex%20variant%2C%20ensuring%20theoretical%20stability%20under%20block%20alternation.%20Experimentally%2C%20our%20method%20achieves%2095.6%5C%25%20on%20CIFAR-10%2C%2086.3%5C%25%20on%20CIFAR-100%2C%20and%2088.5%5C%25%20on%20TinyImageNet%20with%20faster%20convergence%20and%20lower%20memory%20cost%20%28%24%3C%244GB%20GPU%29.%20The%20results%20confirm%20that%20the%20proposed%20LC-KSVD2%20%2B%20LISTA/LISTAConv%20pipeline%20offers%20an%20interpretable%20and%20computationally%20efficient%20alternative%20for%20modern%20deep%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2511.10575v1&entry.124074799=Read"},
{"title": "Dynamic Avatar-Scene Rendering from Human-centric Context", "author": "Wenqing Wang and Haosen Yang and Josef Kittler and Xiatian Zhu", "abstract": "Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.", "link": "http://arxiv.org/abs/2511.10539v1", "date": "2025-11-13", "relevancy": 2.6258, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6826}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.638}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Avatar-Scene%20Rendering%20from%20Human-centric%20Context&body=Title%3A%20Dynamic%20Avatar-Scene%20Rendering%20from%20Human-centric%20Context%0AAuthor%3A%20Wenqing%20Wang%20and%20Haosen%20Yang%20and%20Josef%20Kittler%20and%20Xiatian%20Zhu%0AAbstract%3A%20Reconstructing%20dynamic%20humans%20interacting%20with%20real-world%20environments%20from%20monocular%20videos%20is%20an%20important%20and%20challenging%20task.%20Despite%20considerable%20progress%20in%204D%20neural%20rendering%2C%20existing%20approaches%20either%20model%20dynamic%20scenes%20holistically%20or%20model%20scenes%20and%20backgrounds%20separately%20aim%20to%20introduce%20parametric%20human%20priors.%20However%2C%20these%20approaches%20either%20neglect%20distinct%20motion%20characteristics%20of%20various%20components%20in%20scene%20especially%20human%2C%20leading%20to%20incomplete%20reconstructions%2C%20or%20ignore%20the%20information%20exchange%20between%20the%20separately%20modeled%20components%2C%20resulting%20in%20spatial%20inconsistencies%20and%20visual%20artifacts%20at%20human-scene%20boundaries.%20To%20address%20this%2C%20we%20propose%20%7B%5Cbf%20Separate-then-Map%7D%20%28StM%29%20strategy%20that%20introduces%20a%20dedicated%20information%20mapping%20mechanism%20to%20bridge%20separately%20defined%20and%20optimized%20models.%20Our%20method%20employs%20a%20shared%20transformation%20function%20for%20each%20Gaussian%20attribute%20to%20unify%20separately%20modeled%20components%2C%20enhancing%20computational%20efficiency%20by%20avoiding%20exhaustive%20pairwise%20interactions%20while%20ensuring%20spatial%20and%20visual%20coherence%20between%20humans%20and%20their%20surroundings.%20Extensive%20experiments%20on%20monocular%20video%20datasets%20demonstrate%20that%20StM%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%20visual%20quality%20and%20rendering%20accuracy%2C%20particularly%20at%20challenging%20human-scene%20interaction%20boundaries.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Avatar-Scene%2520Rendering%2520from%2520Human-centric%2520Context%26entry.906535625%3DWenqing%2520Wang%2520and%2520Haosen%2520Yang%2520and%2520Josef%2520Kittler%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3DReconstructing%2520dynamic%2520humans%2520interacting%2520with%2520real-world%2520environments%2520from%2520monocular%2520videos%2520is%2520an%2520important%2520and%2520challenging%2520task.%2520Despite%2520considerable%2520progress%2520in%25204D%2520neural%2520rendering%252C%2520existing%2520approaches%2520either%2520model%2520dynamic%2520scenes%2520holistically%2520or%2520model%2520scenes%2520and%2520backgrounds%2520separately%2520aim%2520to%2520introduce%2520parametric%2520human%2520priors.%2520However%252C%2520these%2520approaches%2520either%2520neglect%2520distinct%2520motion%2520characteristics%2520of%2520various%2520components%2520in%2520scene%2520especially%2520human%252C%2520leading%2520to%2520incomplete%2520reconstructions%252C%2520or%2520ignore%2520the%2520information%2520exchange%2520between%2520the%2520separately%2520modeled%2520components%252C%2520resulting%2520in%2520spatial%2520inconsistencies%2520and%2520visual%2520artifacts%2520at%2520human-scene%2520boundaries.%2520To%2520address%2520this%252C%2520we%2520propose%2520%257B%255Cbf%2520Separate-then-Map%257D%2520%2528StM%2529%2520strategy%2520that%2520introduces%2520a%2520dedicated%2520information%2520mapping%2520mechanism%2520to%2520bridge%2520separately%2520defined%2520and%2520optimized%2520models.%2520Our%2520method%2520employs%2520a%2520shared%2520transformation%2520function%2520for%2520each%2520Gaussian%2520attribute%2520to%2520unify%2520separately%2520modeled%2520components%252C%2520enhancing%2520computational%2520efficiency%2520by%2520avoiding%2520exhaustive%2520pairwise%2520interactions%2520while%2520ensuring%2520spatial%2520and%2520visual%2520coherence%2520between%2520humans%2520and%2520their%2520surroundings.%2520Extensive%2520experiments%2520on%2520monocular%2520video%2520datasets%2520demonstrate%2520that%2520StM%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520both%2520visual%2520quality%2520and%2520rendering%2520accuracy%252C%2520particularly%2520at%2520challenging%2520human-scene%2520interaction%2520boundaries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Avatar-Scene%20Rendering%20from%20Human-centric%20Context&entry.906535625=Wenqing%20Wang%20and%20Haosen%20Yang%20and%20Josef%20Kittler%20and%20Xiatian%20Zhu&entry.1292438233=Reconstructing%20dynamic%20humans%20interacting%20with%20real-world%20environments%20from%20monocular%20videos%20is%20an%20important%20and%20challenging%20task.%20Despite%20considerable%20progress%20in%204D%20neural%20rendering%2C%20existing%20approaches%20either%20model%20dynamic%20scenes%20holistically%20or%20model%20scenes%20and%20backgrounds%20separately%20aim%20to%20introduce%20parametric%20human%20priors.%20However%2C%20these%20approaches%20either%20neglect%20distinct%20motion%20characteristics%20of%20various%20components%20in%20scene%20especially%20human%2C%20leading%20to%20incomplete%20reconstructions%2C%20or%20ignore%20the%20information%20exchange%20between%20the%20separately%20modeled%20components%2C%20resulting%20in%20spatial%20inconsistencies%20and%20visual%20artifacts%20at%20human-scene%20boundaries.%20To%20address%20this%2C%20we%20propose%20%7B%5Cbf%20Separate-then-Map%7D%20%28StM%29%20strategy%20that%20introduces%20a%20dedicated%20information%20mapping%20mechanism%20to%20bridge%20separately%20defined%20and%20optimized%20models.%20Our%20method%20employs%20a%20shared%20transformation%20function%20for%20each%20Gaussian%20attribute%20to%20unify%20separately%20modeled%20components%2C%20enhancing%20computational%20efficiency%20by%20avoiding%20exhaustive%20pairwise%20interactions%20while%20ensuring%20spatial%20and%20visual%20coherence%20between%20humans%20and%20their%20surroundings.%20Extensive%20experiments%20on%20monocular%20video%20datasets%20demonstrate%20that%20StM%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%20visual%20quality%20and%20rendering%20accuracy%2C%20particularly%20at%20challenging%20human-scene%20interaction%20boundaries.&entry.1838667208=http%3A//arxiv.org/abs/2511.10539v1&entry.124074799=Read"},
{"title": "Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting", "author": "Feng Wang and Tianxiang Chen and Shuyue Wei and Qian Chu and Yi Zhang and Yifan Sun and Zhiming Zheng", "abstract": "Spatio-temporal graphs are powerful tools for modeling complex dependencies in traffic time series. However, the distributed nature of real-world traffic data across multiple stakeholders poses significant challenges in modeling and reconstructing inter-client spatial dependencies while adhering to data locality constraints. Existing methods primarily address static dependencies, overlooking their dynamic nature and resulting in suboptimal performance. In response, we propose Federated Spatio-Temporal Graph with Dynamic Inter-Client Dependencies (FedSTGD), a framework designed to model and reconstruct dynamic inter-client spatial dependencies in federated learning. FedSTGD incorporates a federated nonlinear computation decomposition module to approximate complex graph operations. This is complemented by a graph node embedding augmentation module, which alleviates performance degradation arising from the decomposition. These modules are coordinated through a client-server collective learning protocol, which decomposes dynamic inter-client spatial dependency learning tasks into lightweight, parallelizable subtasks. Extensive experiments on four real-world datasets demonstrate that FedSTGD achieves superior performance over state-of-the-art baselines in terms of RMSE, MAE, and MAPE, approaching that of centralized baselines. Ablation studies confirm the contribution of each module in addressing dynamic inter-client spatial dependencies, while sensitivity analysis highlights the robustness of FedSTGD to variations in hyperparameters.", "link": "http://arxiv.org/abs/2511.10434v1", "date": "2025-11-13", "relevancy": 2.5943, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5274}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Dynamic%20Inter-Client%20Spatial%20Dependencies%3A%20A%20Federated%20Spatio-Temporal%20Graph%20Learning%20Method%20for%20Traffic%20Flow%20Forecasting&body=Title%3A%20Unlocking%20Dynamic%20Inter-Client%20Spatial%20Dependencies%3A%20A%20Federated%20Spatio-Temporal%20Graph%20Learning%20Method%20for%20Traffic%20Flow%20Forecasting%0AAuthor%3A%20Feng%20Wang%20and%20Tianxiang%20Chen%20and%20Shuyue%20Wei%20and%20Qian%20Chu%20and%20Yi%20Zhang%20and%20Yifan%20Sun%20and%20Zhiming%20Zheng%0AAbstract%3A%20Spatio-temporal%20graphs%20are%20powerful%20tools%20for%20modeling%20complex%20dependencies%20in%20traffic%20time%20series.%20However%2C%20the%20distributed%20nature%20of%20real-world%20traffic%20data%20across%20multiple%20stakeholders%20poses%20significant%20challenges%20in%20modeling%20and%20reconstructing%20inter-client%20spatial%20dependencies%20while%20adhering%20to%20data%20locality%20constraints.%20Existing%20methods%20primarily%20address%20static%20dependencies%2C%20overlooking%20their%20dynamic%20nature%20and%20resulting%20in%20suboptimal%20performance.%20In%20response%2C%20we%20propose%20Federated%20Spatio-Temporal%20Graph%20with%20Dynamic%20Inter-Client%20Dependencies%20%28FedSTGD%29%2C%20a%20framework%20designed%20to%20model%20and%20reconstruct%20dynamic%20inter-client%20spatial%20dependencies%20in%20federated%20learning.%20FedSTGD%20incorporates%20a%20federated%20nonlinear%20computation%20decomposition%20module%20to%20approximate%20complex%20graph%20operations.%20This%20is%20complemented%20by%20a%20graph%20node%20embedding%20augmentation%20module%2C%20which%20alleviates%20performance%20degradation%20arising%20from%20the%20decomposition.%20These%20modules%20are%20coordinated%20through%20a%20client-server%20collective%20learning%20protocol%2C%20which%20decomposes%20dynamic%20inter-client%20spatial%20dependency%20learning%20tasks%20into%20lightweight%2C%20parallelizable%20subtasks.%20Extensive%20experiments%20on%20four%20real-world%20datasets%20demonstrate%20that%20FedSTGD%20achieves%20superior%20performance%20over%20state-of-the-art%20baselines%20in%20terms%20of%20RMSE%2C%20MAE%2C%20and%20MAPE%2C%20approaching%20that%20of%20centralized%20baselines.%20Ablation%20studies%20confirm%20the%20contribution%20of%20each%20module%20in%20addressing%20dynamic%20inter-client%20spatial%20dependencies%2C%20while%20sensitivity%20analysis%20highlights%20the%20robustness%20of%20FedSTGD%20to%20variations%20in%20hyperparameters.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Dynamic%2520Inter-Client%2520Spatial%2520Dependencies%253A%2520A%2520Federated%2520Spatio-Temporal%2520Graph%2520Learning%2520Method%2520for%2520Traffic%2520Flow%2520Forecasting%26entry.906535625%3DFeng%2520Wang%2520and%2520Tianxiang%2520Chen%2520and%2520Shuyue%2520Wei%2520and%2520Qian%2520Chu%2520and%2520Yi%2520Zhang%2520and%2520Yifan%2520Sun%2520and%2520Zhiming%2520Zheng%26entry.1292438233%3DSpatio-temporal%2520graphs%2520are%2520powerful%2520tools%2520for%2520modeling%2520complex%2520dependencies%2520in%2520traffic%2520time%2520series.%2520However%252C%2520the%2520distributed%2520nature%2520of%2520real-world%2520traffic%2520data%2520across%2520multiple%2520stakeholders%2520poses%2520significant%2520challenges%2520in%2520modeling%2520and%2520reconstructing%2520inter-client%2520spatial%2520dependencies%2520while%2520adhering%2520to%2520data%2520locality%2520constraints.%2520Existing%2520methods%2520primarily%2520address%2520static%2520dependencies%252C%2520overlooking%2520their%2520dynamic%2520nature%2520and%2520resulting%2520in%2520suboptimal%2520performance.%2520In%2520response%252C%2520we%2520propose%2520Federated%2520Spatio-Temporal%2520Graph%2520with%2520Dynamic%2520Inter-Client%2520Dependencies%2520%2528FedSTGD%2529%252C%2520a%2520framework%2520designed%2520to%2520model%2520and%2520reconstruct%2520dynamic%2520inter-client%2520spatial%2520dependencies%2520in%2520federated%2520learning.%2520FedSTGD%2520incorporates%2520a%2520federated%2520nonlinear%2520computation%2520decomposition%2520module%2520to%2520approximate%2520complex%2520graph%2520operations.%2520This%2520is%2520complemented%2520by%2520a%2520graph%2520node%2520embedding%2520augmentation%2520module%252C%2520which%2520alleviates%2520performance%2520degradation%2520arising%2520from%2520the%2520decomposition.%2520These%2520modules%2520are%2520coordinated%2520through%2520a%2520client-server%2520collective%2520learning%2520protocol%252C%2520which%2520decomposes%2520dynamic%2520inter-client%2520spatial%2520dependency%2520learning%2520tasks%2520into%2520lightweight%252C%2520parallelizable%2520subtasks.%2520Extensive%2520experiments%2520on%2520four%2520real-world%2520datasets%2520demonstrate%2520that%2520FedSTGD%2520achieves%2520superior%2520performance%2520over%2520state-of-the-art%2520baselines%2520in%2520terms%2520of%2520RMSE%252C%2520MAE%252C%2520and%2520MAPE%252C%2520approaching%2520that%2520of%2520centralized%2520baselines.%2520Ablation%2520studies%2520confirm%2520the%2520contribution%2520of%2520each%2520module%2520in%2520addressing%2520dynamic%2520inter-client%2520spatial%2520dependencies%252C%2520while%2520sensitivity%2520analysis%2520highlights%2520the%2520robustness%2520of%2520FedSTGD%2520to%2520variations%2520in%2520hyperparameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Dynamic%20Inter-Client%20Spatial%20Dependencies%3A%20A%20Federated%20Spatio-Temporal%20Graph%20Learning%20Method%20for%20Traffic%20Flow%20Forecasting&entry.906535625=Feng%20Wang%20and%20Tianxiang%20Chen%20and%20Shuyue%20Wei%20and%20Qian%20Chu%20and%20Yi%20Zhang%20and%20Yifan%20Sun%20and%20Zhiming%20Zheng&entry.1292438233=Spatio-temporal%20graphs%20are%20powerful%20tools%20for%20modeling%20complex%20dependencies%20in%20traffic%20time%20series.%20However%2C%20the%20distributed%20nature%20of%20real-world%20traffic%20data%20across%20multiple%20stakeholders%20poses%20significant%20challenges%20in%20modeling%20and%20reconstructing%20inter-client%20spatial%20dependencies%20while%20adhering%20to%20data%20locality%20constraints.%20Existing%20methods%20primarily%20address%20static%20dependencies%2C%20overlooking%20their%20dynamic%20nature%20and%20resulting%20in%20suboptimal%20performance.%20In%20response%2C%20we%20propose%20Federated%20Spatio-Temporal%20Graph%20with%20Dynamic%20Inter-Client%20Dependencies%20%28FedSTGD%29%2C%20a%20framework%20designed%20to%20model%20and%20reconstruct%20dynamic%20inter-client%20spatial%20dependencies%20in%20federated%20learning.%20FedSTGD%20incorporates%20a%20federated%20nonlinear%20computation%20decomposition%20module%20to%20approximate%20complex%20graph%20operations.%20This%20is%20complemented%20by%20a%20graph%20node%20embedding%20augmentation%20module%2C%20which%20alleviates%20performance%20degradation%20arising%20from%20the%20decomposition.%20These%20modules%20are%20coordinated%20through%20a%20client-server%20collective%20learning%20protocol%2C%20which%20decomposes%20dynamic%20inter-client%20spatial%20dependency%20learning%20tasks%20into%20lightweight%2C%20parallelizable%20subtasks.%20Extensive%20experiments%20on%20four%20real-world%20datasets%20demonstrate%20that%20FedSTGD%20achieves%20superior%20performance%20over%20state-of-the-art%20baselines%20in%20terms%20of%20RMSE%2C%20MAE%2C%20and%20MAPE%2C%20approaching%20that%20of%20centralized%20baselines.%20Ablation%20studies%20confirm%20the%20contribution%20of%20each%20module%20in%20addressing%20dynamic%20inter-client%20spatial%20dependencies%2C%20while%20sensitivity%20analysis%20highlights%20the%20robustness%20of%20FedSTGD%20to%20variations%20in%20hyperparameters.&entry.1838667208=http%3A//arxiv.org/abs/2511.10434v1&entry.124074799=Read"},
{"title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "author": "Qiayuan Liao and Takara E. Truong and Xiaoyu Huang and Yuman Gao and Guy Tevet and Koushil Sreenath and C. Karen Liu", "abstract": "The human-like form of humanoid robots positions them uniquely to achieve the agility and versatility in motor skills that humans possess. Learning from human demonstrations offers a scalable approach to acquiring these capabilities. However, prior works either produce unnatural motions or rely on motion-specific tuning to achieve satisfactory naturalness. Furthermore, these methods are often motion- or goal-specific, lacking the versatility to compose diverse skills, especially when solving unseen tasks. We present BeyondMimic, a framework that scales to diverse motions and carries the versatility to compose them seamlessly in tackling unseen downstream tasks. At heart, a compact motion-tracking formulation enables mastering a wide range of radically agile behaviors, including aerial cartwheels, spin-kicks, flip-kicks, and sprinting, with a single setup and shared hyperparameters, all while achieving state-of-the-art human-like performance. Moving beyond the mere imitation of existing motions, we propose a unified latent diffusion model that empowers versatile goal specification, seamless task switching, and dynamic composition of these agile behaviors. Leveraging classifier guidance, a diffusion-specific technique for test-time optimization toward novel objectives, our model extends its capability to solve downstream tasks never encountered during training, including motion inpainting, joystick teleoperation, and obstacle avoidance, and transfers these skills zero-shot to real hardware. This work opens new frontiers for humanoid robots by pushing the limits of scalable human-like motor skill acquisition from human motion and advancing seamless motion synthesis that achieves generalization and versatility beyond training setups.", "link": "http://arxiv.org/abs/2508.08241v4", "date": "2025-11-13", "relevancy": 2.5875, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7233}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6055}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%20Guided%20Diffusion&body=Title%3A%20BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%20Guided%20Diffusion%0AAuthor%3A%20Qiayuan%20Liao%20and%20Takara%20E.%20Truong%20and%20Xiaoyu%20Huang%20and%20Yuman%20Gao%20and%20Guy%20Tevet%20and%20Koushil%20Sreenath%20and%20C.%20Karen%20Liu%0AAbstract%3A%20The%20human-like%20form%20of%20humanoid%20robots%20positions%20them%20uniquely%20to%20achieve%20the%20agility%20and%20versatility%20in%20motor%20skills%20that%20humans%20possess.%20Learning%20from%20human%20demonstrations%20offers%20a%20scalable%20approach%20to%20acquiring%20these%20capabilities.%20However%2C%20prior%20works%20either%20produce%20unnatural%20motions%20or%20rely%20on%20motion-specific%20tuning%20to%20achieve%20satisfactory%20naturalness.%20Furthermore%2C%20these%20methods%20are%20often%20motion-%20or%20goal-specific%2C%20lacking%20the%20versatility%20to%20compose%20diverse%20skills%2C%20especially%20when%20solving%20unseen%20tasks.%20We%20present%20BeyondMimic%2C%20a%20framework%20that%20scales%20to%20diverse%20motions%20and%20carries%20the%20versatility%20to%20compose%20them%20seamlessly%20in%20tackling%20unseen%20downstream%20tasks.%20At%20heart%2C%20a%20compact%20motion-tracking%20formulation%20enables%20mastering%20a%20wide%20range%20of%20radically%20agile%20behaviors%2C%20including%20aerial%20cartwheels%2C%20spin-kicks%2C%20flip-kicks%2C%20and%20sprinting%2C%20with%20a%20single%20setup%20and%20shared%20hyperparameters%2C%20all%20while%20achieving%20state-of-the-art%20human-like%20performance.%20Moving%20beyond%20the%20mere%20imitation%20of%20existing%20motions%2C%20we%20propose%20a%20unified%20latent%20diffusion%20model%20that%20empowers%20versatile%20goal%20specification%2C%20seamless%20task%20switching%2C%20and%20dynamic%20composition%20of%20these%20agile%20behaviors.%20Leveraging%20classifier%20guidance%2C%20a%20diffusion-specific%20technique%20for%20test-time%20optimization%20toward%20novel%20objectives%2C%20our%20model%20extends%20its%20capability%20to%20solve%20downstream%20tasks%20never%20encountered%20during%20training%2C%20including%20motion%20inpainting%2C%20joystick%20teleoperation%2C%20and%20obstacle%20avoidance%2C%20and%20transfers%20these%20skills%20zero-shot%20to%20real%20hardware.%20This%20work%20opens%20new%20frontiers%20for%20humanoid%20robots%20by%20pushing%20the%20limits%20of%20scalable%20human-like%20motor%20skill%20acquisition%20from%20human%20motion%20and%20advancing%20seamless%20motion%20synthesis%20that%20achieves%20generalization%20and%20versatility%20beyond%20training%20setups.%0ALink%3A%20http%3A//arxiv.org/abs/2508.08241v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyondMimic%253A%2520From%2520Motion%2520Tracking%2520to%2520Versatile%2520Humanoid%2520Control%2520via%2520Guided%2520Diffusion%26entry.906535625%3DQiayuan%2520Liao%2520and%2520Takara%2520E.%2520Truong%2520and%2520Xiaoyu%2520Huang%2520and%2520Yuman%2520Gao%2520and%2520Guy%2520Tevet%2520and%2520Koushil%2520Sreenath%2520and%2520C.%2520Karen%2520Liu%26entry.1292438233%3DThe%2520human-like%2520form%2520of%2520humanoid%2520robots%2520positions%2520them%2520uniquely%2520to%2520achieve%2520the%2520agility%2520and%2520versatility%2520in%2520motor%2520skills%2520that%2520humans%2520possess.%2520Learning%2520from%2520human%2520demonstrations%2520offers%2520a%2520scalable%2520approach%2520to%2520acquiring%2520these%2520capabilities.%2520However%252C%2520prior%2520works%2520either%2520produce%2520unnatural%2520motions%2520or%2520rely%2520on%2520motion-specific%2520tuning%2520to%2520achieve%2520satisfactory%2520naturalness.%2520Furthermore%252C%2520these%2520methods%2520are%2520often%2520motion-%2520or%2520goal-specific%252C%2520lacking%2520the%2520versatility%2520to%2520compose%2520diverse%2520skills%252C%2520especially%2520when%2520solving%2520unseen%2520tasks.%2520We%2520present%2520BeyondMimic%252C%2520a%2520framework%2520that%2520scales%2520to%2520diverse%2520motions%2520and%2520carries%2520the%2520versatility%2520to%2520compose%2520them%2520seamlessly%2520in%2520tackling%2520unseen%2520downstream%2520tasks.%2520At%2520heart%252C%2520a%2520compact%2520motion-tracking%2520formulation%2520enables%2520mastering%2520a%2520wide%2520range%2520of%2520radically%2520agile%2520behaviors%252C%2520including%2520aerial%2520cartwheels%252C%2520spin-kicks%252C%2520flip-kicks%252C%2520and%2520sprinting%252C%2520with%2520a%2520single%2520setup%2520and%2520shared%2520hyperparameters%252C%2520all%2520while%2520achieving%2520state-of-the-art%2520human-like%2520performance.%2520Moving%2520beyond%2520the%2520mere%2520imitation%2520of%2520existing%2520motions%252C%2520we%2520propose%2520a%2520unified%2520latent%2520diffusion%2520model%2520that%2520empowers%2520versatile%2520goal%2520specification%252C%2520seamless%2520task%2520switching%252C%2520and%2520dynamic%2520composition%2520of%2520these%2520agile%2520behaviors.%2520Leveraging%2520classifier%2520guidance%252C%2520a%2520diffusion-specific%2520technique%2520for%2520test-time%2520optimization%2520toward%2520novel%2520objectives%252C%2520our%2520model%2520extends%2520its%2520capability%2520to%2520solve%2520downstream%2520tasks%2520never%2520encountered%2520during%2520training%252C%2520including%2520motion%2520inpainting%252C%2520joystick%2520teleoperation%252C%2520and%2520obstacle%2520avoidance%252C%2520and%2520transfers%2520these%2520skills%2520zero-shot%2520to%2520real%2520hardware.%2520This%2520work%2520opens%2520new%2520frontiers%2520for%2520humanoid%2520robots%2520by%2520pushing%2520the%2520limits%2520of%2520scalable%2520human-like%2520motor%2520skill%2520acquisition%2520from%2520human%2520motion%2520and%2520advancing%2520seamless%2520motion%2520synthesis%2520that%2520achieves%2520generalization%2520and%2520versatility%2520beyond%2520training%2520setups.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08241v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeyondMimic%3A%20From%20Motion%20Tracking%20to%20Versatile%20Humanoid%20Control%20via%20Guided%20Diffusion&entry.906535625=Qiayuan%20Liao%20and%20Takara%20E.%20Truong%20and%20Xiaoyu%20Huang%20and%20Yuman%20Gao%20and%20Guy%20Tevet%20and%20Koushil%20Sreenath%20and%20C.%20Karen%20Liu&entry.1292438233=The%20human-like%20form%20of%20humanoid%20robots%20positions%20them%20uniquely%20to%20achieve%20the%20agility%20and%20versatility%20in%20motor%20skills%20that%20humans%20possess.%20Learning%20from%20human%20demonstrations%20offers%20a%20scalable%20approach%20to%20acquiring%20these%20capabilities.%20However%2C%20prior%20works%20either%20produce%20unnatural%20motions%20or%20rely%20on%20motion-specific%20tuning%20to%20achieve%20satisfactory%20naturalness.%20Furthermore%2C%20these%20methods%20are%20often%20motion-%20or%20goal-specific%2C%20lacking%20the%20versatility%20to%20compose%20diverse%20skills%2C%20especially%20when%20solving%20unseen%20tasks.%20We%20present%20BeyondMimic%2C%20a%20framework%20that%20scales%20to%20diverse%20motions%20and%20carries%20the%20versatility%20to%20compose%20them%20seamlessly%20in%20tackling%20unseen%20downstream%20tasks.%20At%20heart%2C%20a%20compact%20motion-tracking%20formulation%20enables%20mastering%20a%20wide%20range%20of%20radically%20agile%20behaviors%2C%20including%20aerial%20cartwheels%2C%20spin-kicks%2C%20flip-kicks%2C%20and%20sprinting%2C%20with%20a%20single%20setup%20and%20shared%20hyperparameters%2C%20all%20while%20achieving%20state-of-the-art%20human-like%20performance.%20Moving%20beyond%20the%20mere%20imitation%20of%20existing%20motions%2C%20we%20propose%20a%20unified%20latent%20diffusion%20model%20that%20empowers%20versatile%20goal%20specification%2C%20seamless%20task%20switching%2C%20and%20dynamic%20composition%20of%20these%20agile%20behaviors.%20Leveraging%20classifier%20guidance%2C%20a%20diffusion-specific%20technique%20for%20test-time%20optimization%20toward%20novel%20objectives%2C%20our%20model%20extends%20its%20capability%20to%20solve%20downstream%20tasks%20never%20encountered%20during%20training%2C%20including%20motion%20inpainting%2C%20joystick%20teleoperation%2C%20and%20obstacle%20avoidance%2C%20and%20transfers%20these%20skills%20zero-shot%20to%20real%20hardware.%20This%20work%20opens%20new%20frontiers%20for%20humanoid%20robots%20by%20pushing%20the%20limits%20of%20scalable%20human-like%20motor%20skill%20acquisition%20from%20human%20motion%20and%20advancing%20seamless%20motion%20synthesis%20that%20achieves%20generalization%20and%20versatility%20beyond%20training%20setups.&entry.1838667208=http%3A//arxiv.org/abs/2508.08241v4&entry.124074799=Read"},
{"title": "3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound", "author": "Alomar Antonia and Rubio Ricardo and Albaiges Gerard and Salort-Benejam Laura and Caminal Julia and Prat Maria and Rueda Carolina and Cortes Berta and Piella Gemma and Sukno Federico", "abstract": "Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.\n  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.\n  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.", "link": "http://arxiv.org/abs/2511.10412v1", "date": "2025-11-13", "relevancy": 2.5865, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4981}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DFETUS%3A%20Standardizing%20Fetal%20Facial%20Planes%20in%203D%20Ultrasound&body=Title%3A%203DFETUS%3A%20Standardizing%20Fetal%20Facial%20Planes%20in%203D%20Ultrasound%0AAuthor%3A%20Alomar%20Antonia%20and%20Rubio%20Ricardo%20and%20Albaiges%20Gerard%20and%20Salort-Benejam%20Laura%20and%20Caminal%20Julia%20and%20Prat%20Maria%20and%20Rueda%20Carolina%20and%20Cortes%20Berta%20and%20Piella%20Gemma%20and%20Sukno%20Federico%0AAbstract%3A%20Acquiring%20standard%20facial%20planes%20during%20routine%20fetal%20ultrasound%20%28US%29%20examinations%20is%20often%20challenging%20due%20to%20fetal%20movement%2C%20variability%20in%20orientation%2C%20and%20operator-dependent%20expertise.%20These%20factors%20contribute%20to%20inconsistencies%2C%20increased%20examination%20time%2C%20and%20potential%20diagnostic%20bias.%0A%20%20To%20address%20these%20challenges%20in%20the%20context%20of%20facial%20assessment%2C%20we%20present%3A%201%29%20GT%2B%2B%2C%20a%20robust%20algorithm%20that%20estimates%20standard%20facial%20planes%20from%203D%20US%20volumes%20using%20annotated%20anatomical%20landmarks%3B%20and%202%29%203DFETUS%2C%20a%20deep%20learning%20model%20that%20automates%20and%20standardizes%20their%20localization%20in%203D%20fetal%20US%20volumes.%0A%20%20We%20evaluated%20our%20methods%20both%20qualitatively%2C%20through%20expert%20clinical%20review%2C%20and%20quantitatively.%20The%20proposed%20approach%20achieved%20a%20mean%20translation%20error%20of%204.13%20mm%20and%20a%20mean%20rotation%20error%20of%207.93%20degrees%20per%20plane%2C%20outperforming%20other%20state-of-the-art%20methods%20on%203D%20US%20volumes.%20Clinical%20assessments%20further%20confirmed%20the%20effectiveness%20of%20both%20GT%2B%2B%20and%203DFETUS%2C%20demonstrating%20statistically%20significant%20improvements%20in%20plane%20estimation%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DFETUS%253A%2520Standardizing%2520Fetal%2520Facial%2520Planes%2520in%25203D%2520Ultrasound%26entry.906535625%3DAlomar%2520Antonia%2520and%2520Rubio%2520Ricardo%2520and%2520Albaiges%2520Gerard%2520and%2520Salort-Benejam%2520Laura%2520and%2520Caminal%2520Julia%2520and%2520Prat%2520Maria%2520and%2520Rueda%2520Carolina%2520and%2520Cortes%2520Berta%2520and%2520Piella%2520Gemma%2520and%2520Sukno%2520Federico%26entry.1292438233%3DAcquiring%2520standard%2520facial%2520planes%2520during%2520routine%2520fetal%2520ultrasound%2520%2528US%2529%2520examinations%2520is%2520often%2520challenging%2520due%2520to%2520fetal%2520movement%252C%2520variability%2520in%2520orientation%252C%2520and%2520operator-dependent%2520expertise.%2520These%2520factors%2520contribute%2520to%2520inconsistencies%252C%2520increased%2520examination%2520time%252C%2520and%2520potential%2520diagnostic%2520bias.%250A%2520%2520To%2520address%2520these%2520challenges%2520in%2520the%2520context%2520of%2520facial%2520assessment%252C%2520we%2520present%253A%25201%2529%2520GT%252B%252B%252C%2520a%2520robust%2520algorithm%2520that%2520estimates%2520standard%2520facial%2520planes%2520from%25203D%2520US%2520volumes%2520using%2520annotated%2520anatomical%2520landmarks%253B%2520and%25202%2529%25203DFETUS%252C%2520a%2520deep%2520learning%2520model%2520that%2520automates%2520and%2520standardizes%2520their%2520localization%2520in%25203D%2520fetal%2520US%2520volumes.%250A%2520%2520We%2520evaluated%2520our%2520methods%2520both%2520qualitatively%252C%2520through%2520expert%2520clinical%2520review%252C%2520and%2520quantitatively.%2520The%2520proposed%2520approach%2520achieved%2520a%2520mean%2520translation%2520error%2520of%25204.13%2520mm%2520and%2520a%2520mean%2520rotation%2520error%2520of%25207.93%2520degrees%2520per%2520plane%252C%2520outperforming%2520other%2520state-of-the-art%2520methods%2520on%25203D%2520US%2520volumes.%2520Clinical%2520assessments%2520further%2520confirmed%2520the%2520effectiveness%2520of%2520both%2520GT%252B%252B%2520and%25203DFETUS%252C%2520demonstrating%2520statistically%2520significant%2520improvements%2520in%2520plane%2520estimation%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DFETUS%3A%20Standardizing%20Fetal%20Facial%20Planes%20in%203D%20Ultrasound&entry.906535625=Alomar%20Antonia%20and%20Rubio%20Ricardo%20and%20Albaiges%20Gerard%20and%20Salort-Benejam%20Laura%20and%20Caminal%20Julia%20and%20Prat%20Maria%20and%20Rueda%20Carolina%20and%20Cortes%20Berta%20and%20Piella%20Gemma%20and%20Sukno%20Federico&entry.1292438233=Acquiring%20standard%20facial%20planes%20during%20routine%20fetal%20ultrasound%20%28US%29%20examinations%20is%20often%20challenging%20due%20to%20fetal%20movement%2C%20variability%20in%20orientation%2C%20and%20operator-dependent%20expertise.%20These%20factors%20contribute%20to%20inconsistencies%2C%20increased%20examination%20time%2C%20and%20potential%20diagnostic%20bias.%0A%20%20To%20address%20these%20challenges%20in%20the%20context%20of%20facial%20assessment%2C%20we%20present%3A%201%29%20GT%2B%2B%2C%20a%20robust%20algorithm%20that%20estimates%20standard%20facial%20planes%20from%203D%20US%20volumes%20using%20annotated%20anatomical%20landmarks%3B%20and%202%29%203DFETUS%2C%20a%20deep%20learning%20model%20that%20automates%20and%20standardizes%20their%20localization%20in%203D%20fetal%20US%20volumes.%0A%20%20We%20evaluated%20our%20methods%20both%20qualitatively%2C%20through%20expert%20clinical%20review%2C%20and%20quantitatively.%20The%20proposed%20approach%20achieved%20a%20mean%20translation%20error%20of%204.13%20mm%20and%20a%20mean%20rotation%20error%20of%207.93%20degrees%20per%20plane%2C%20outperforming%20other%20state-of-the-art%20methods%20on%203D%20US%20volumes.%20Clinical%20assessments%20further%20confirmed%20the%20effectiveness%20of%20both%20GT%2B%2B%20and%203DFETUS%2C%20demonstrating%20statistically%20significant%20improvements%20in%20plane%20estimation%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2511.10412v1&entry.124074799=Read"},
{"title": "Boosting Adversarial Transferability via Ensemble Non-Attention", "author": "Yipeng Zou and Qin Liu and Jie Wu and Yu Peng and Guo Chen and Hui Zhou and Guanghui Ye", "abstract": "Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.", "link": "http://arxiv.org/abs/2511.08937v2", "date": "2025-11-13", "relevancy": 2.5835, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5513}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Adversarial%20Transferability%20via%20Ensemble%20Non-Attention&body=Title%3A%20Boosting%20Adversarial%20Transferability%20via%20Ensemble%20Non-Attention%0AAuthor%3A%20Yipeng%20Zou%20and%20Qin%20Liu%20and%20Jie%20Wu%20and%20Yu%20Peng%20and%20Guo%20Chen%20and%20Hui%20Zhou%20and%20Guanghui%20Ye%0AAbstract%3A%20Ensemble%20attacks%20integrate%20the%20outputs%20of%20surrogate%20models%20with%20diverse%20architectures%2C%20which%20can%20be%20combined%20with%20various%20gradient-based%20attacks%20to%20improve%20adversarial%20transferability.%20However%2C%20previous%20work%20shows%20unsatisfactory%20attack%20performance%20when%20transferring%20across%20heterogeneous%20model%20architectures.%20The%20main%20reason%20is%20that%20the%20gradient%20update%20directions%20of%20heterogeneous%20surrogate%20models%20differ%20widely%2C%20making%20it%20hard%20to%20reduce%20the%20gradient%20variance%20of%20ensemble%20models%20while%20making%20the%20best%20of%20individual%20model.%20To%20tackle%20this%20challenge%2C%20we%20design%20a%20novel%20ensemble%20attack%2C%20NAMEA%2C%20which%20for%20the%20first%20time%20integrates%20the%20gradients%20from%20the%20non-attention%20areas%20of%20ensemble%20models%20into%20the%20iterative%20gradient%20optimization%20process.%20Our%20design%20is%20inspired%20by%20the%20observation%20that%20the%20attention%20areas%20of%20heterogeneous%20models%20vary%20sharply%2C%20thus%20the%20non-attention%20areas%20of%20ViTs%20are%20likely%20to%20be%20the%20focus%20of%20CNNs%20and%20vice%20versa.%20Therefore%2C%20we%20merge%20the%20gradients%20respectively%20from%20the%20attention%20and%20non-attention%20areas%20of%20ensemble%20models%20so%20as%20to%20fuse%20the%20transfer%20information%20of%20CNNs%20and%20ViTs.%20Specifically%2C%20we%20pioneer%20a%20new%20way%20of%20decoupling%20the%20gradients%20of%20non-attention%20areas%20from%20those%20of%20attention%20areas%2C%20while%20merging%20gradients%20by%20meta-learning.%20Empirical%20evaluations%20on%20ImageNet%20dataset%20indicate%20that%20NAMEA%20outperforms%20AdaEA%20and%20SMER%2C%20the%20state-of-the-art%20ensemble%20attacks%20by%20an%20average%20of%2015.0%25%20and%209.6%25%2C%20respectively.%20This%20work%20is%20the%20first%20attempt%20to%20explore%20the%20power%20of%20ensemble%20non-attention%20in%20boosting%20cross-architecture%20transferability%2C%20providing%20new%20insights%20into%20launching%20ensemble%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Adversarial%2520Transferability%2520via%2520Ensemble%2520Non-Attention%26entry.906535625%3DYipeng%2520Zou%2520and%2520Qin%2520Liu%2520and%2520Jie%2520Wu%2520and%2520Yu%2520Peng%2520and%2520Guo%2520Chen%2520and%2520Hui%2520Zhou%2520and%2520Guanghui%2520Ye%26entry.1292438233%3DEnsemble%2520attacks%2520integrate%2520the%2520outputs%2520of%2520surrogate%2520models%2520with%2520diverse%2520architectures%252C%2520which%2520can%2520be%2520combined%2520with%2520various%2520gradient-based%2520attacks%2520to%2520improve%2520adversarial%2520transferability.%2520However%252C%2520previous%2520work%2520shows%2520unsatisfactory%2520attack%2520performance%2520when%2520transferring%2520across%2520heterogeneous%2520model%2520architectures.%2520The%2520main%2520reason%2520is%2520that%2520the%2520gradient%2520update%2520directions%2520of%2520heterogeneous%2520surrogate%2520models%2520differ%2520widely%252C%2520making%2520it%2520hard%2520to%2520reduce%2520the%2520gradient%2520variance%2520of%2520ensemble%2520models%2520while%2520making%2520the%2520best%2520of%2520individual%2520model.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520design%2520a%2520novel%2520ensemble%2520attack%252C%2520NAMEA%252C%2520which%2520for%2520the%2520first%2520time%2520integrates%2520the%2520gradients%2520from%2520the%2520non-attention%2520areas%2520of%2520ensemble%2520models%2520into%2520the%2520iterative%2520gradient%2520optimization%2520process.%2520Our%2520design%2520is%2520inspired%2520by%2520the%2520observation%2520that%2520the%2520attention%2520areas%2520of%2520heterogeneous%2520models%2520vary%2520sharply%252C%2520thus%2520the%2520non-attention%2520areas%2520of%2520ViTs%2520are%2520likely%2520to%2520be%2520the%2520focus%2520of%2520CNNs%2520and%2520vice%2520versa.%2520Therefore%252C%2520we%2520merge%2520the%2520gradients%2520respectively%2520from%2520the%2520attention%2520and%2520non-attention%2520areas%2520of%2520ensemble%2520models%2520so%2520as%2520to%2520fuse%2520the%2520transfer%2520information%2520of%2520CNNs%2520and%2520ViTs.%2520Specifically%252C%2520we%2520pioneer%2520a%2520new%2520way%2520of%2520decoupling%2520the%2520gradients%2520of%2520non-attention%2520areas%2520from%2520those%2520of%2520attention%2520areas%252C%2520while%2520merging%2520gradients%2520by%2520meta-learning.%2520Empirical%2520evaluations%2520on%2520ImageNet%2520dataset%2520indicate%2520that%2520NAMEA%2520outperforms%2520AdaEA%2520and%2520SMER%252C%2520the%2520state-of-the-art%2520ensemble%2520attacks%2520by%2520an%2520average%2520of%252015.0%2525%2520and%25209.6%2525%252C%2520respectively.%2520This%2520work%2520is%2520the%2520first%2520attempt%2520to%2520explore%2520the%2520power%2520of%2520ensemble%2520non-attention%2520in%2520boosting%2520cross-architecture%2520transferability%252C%2520providing%2520new%2520insights%2520into%2520launching%2520ensemble%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Adversarial%20Transferability%20via%20Ensemble%20Non-Attention&entry.906535625=Yipeng%20Zou%20and%20Qin%20Liu%20and%20Jie%20Wu%20and%20Yu%20Peng%20and%20Guo%20Chen%20and%20Hui%20Zhou%20and%20Guanghui%20Ye&entry.1292438233=Ensemble%20attacks%20integrate%20the%20outputs%20of%20surrogate%20models%20with%20diverse%20architectures%2C%20which%20can%20be%20combined%20with%20various%20gradient-based%20attacks%20to%20improve%20adversarial%20transferability.%20However%2C%20previous%20work%20shows%20unsatisfactory%20attack%20performance%20when%20transferring%20across%20heterogeneous%20model%20architectures.%20The%20main%20reason%20is%20that%20the%20gradient%20update%20directions%20of%20heterogeneous%20surrogate%20models%20differ%20widely%2C%20making%20it%20hard%20to%20reduce%20the%20gradient%20variance%20of%20ensemble%20models%20while%20making%20the%20best%20of%20individual%20model.%20To%20tackle%20this%20challenge%2C%20we%20design%20a%20novel%20ensemble%20attack%2C%20NAMEA%2C%20which%20for%20the%20first%20time%20integrates%20the%20gradients%20from%20the%20non-attention%20areas%20of%20ensemble%20models%20into%20the%20iterative%20gradient%20optimization%20process.%20Our%20design%20is%20inspired%20by%20the%20observation%20that%20the%20attention%20areas%20of%20heterogeneous%20models%20vary%20sharply%2C%20thus%20the%20non-attention%20areas%20of%20ViTs%20are%20likely%20to%20be%20the%20focus%20of%20CNNs%20and%20vice%20versa.%20Therefore%2C%20we%20merge%20the%20gradients%20respectively%20from%20the%20attention%20and%20non-attention%20areas%20of%20ensemble%20models%20so%20as%20to%20fuse%20the%20transfer%20information%20of%20CNNs%20and%20ViTs.%20Specifically%2C%20we%20pioneer%20a%20new%20way%20of%20decoupling%20the%20gradients%20of%20non-attention%20areas%20from%20those%20of%20attention%20areas%2C%20while%20merging%20gradients%20by%20meta-learning.%20Empirical%20evaluations%20on%20ImageNet%20dataset%20indicate%20that%20NAMEA%20outperforms%20AdaEA%20and%20SMER%2C%20the%20state-of-the-art%20ensemble%20attacks%20by%20an%20average%20of%2015.0%25%20and%209.6%25%2C%20respectively.%20This%20work%20is%20the%20first%20attempt%20to%20explore%20the%20power%20of%20ensemble%20non-attention%20in%20boosting%20cross-architecture%20transferability%2C%20providing%20new%20insights%20into%20launching%20ensemble%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2511.08937v2&entry.124074799=Read"},
{"title": "Reducing the Scope of Language Models", "author": "David Yunis and Siyu Huo and Chulaka Gunasekara and Danish Contractor", "abstract": "Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.", "link": "http://arxiv.org/abs/2410.21597v3", "date": "2025-11-13", "relevancy": 2.5633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5434}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20the%20Scope%20of%20Language%20Models&body=Title%3A%20Reducing%20the%20Scope%20of%20Language%20Models%0AAuthor%3A%20David%20Yunis%20and%20Siyu%20Huo%20and%20Chulaka%20Gunasekara%20and%20Danish%20Contractor%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20deployed%20in%20a%20wide%20variety%20of%20user-facing%20applications.%20Typically%2C%20these%20deployments%20have%20some%20specific%20purpose%2C%20like%20answering%20questions%20grounded%20on%20documentation%20or%20acting%20as%20coding%20assistants%2C%20but%20they%20require%20general%20language%20understanding.%20In%20such%20deployments%2C%20LLMs%20should%20respond%20only%20to%20queries%20that%20align%20with%20the%20intended%20purpose%20and%20reject%20all%20other%20requests%2C%20such%20as%20generating%20poetry%20or%20answering%20questions%20about%20physics%2C%20a%20task%20we%20refer%20to%20as%20%60scoping%27.%20We%20conduct%20a%20comprehensive%20empirical%20evaluation%20of%20various%20methods%2C%20ranging%20from%20prompting%2C%20fine-tuning%20to%20preference%20learning%20and%20the%20recently%20proposed%20general%20alignment%20technique%20known%20as%20Circuit%20Breakers%20%28CB%29.%20Across%20three%20families%20of%20language%20models%20and%20a%20broad%20variety%20of%20tasks%2C%20we%20show%20that%20it%20is%20possible%20to%20scope%20language%20models.%20We%20examine%20scoping%20for%20multiple%20topics%2C%20and%20fine-grained%20topics.%20We%20ablate%20diversity%20of%20irrelevant%20queries%2C%20layer%20different%20techniques%2C%20conduct%20adversarial%20evaluations%20and%20more.%20Among%20other%20results%2C%20we%20find%20that%20when%20diverse%20examples%20of%20irrelevant%20queries%20are%20available%2C%20simple%20supervised%20fine-tuning%20produces%20the%20best%20results%2C%20but%20when%20such%20diversity%20is%20low%2C%20Circuit%20Breakers%20perform%20quite%20well.%20One%20can%20often%20get%20the%20benefits%20of%20both%20methods%20by%20layering%20them%20in%20succession.%20We%20intend%20our%20study%20to%20serve%20as%20a%20practitioner%27s%20guide%20to%20scoping%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2410.21597v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520the%2520Scope%2520of%2520Language%2520Models%26entry.906535625%3DDavid%2520Yunis%2520and%2520Siyu%2520Huo%2520and%2520Chulaka%2520Gunasekara%2520and%2520Danish%2520Contractor%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520deployed%2520in%2520a%2520wide%2520variety%2520of%2520user-facing%2520applications.%2520Typically%252C%2520these%2520deployments%2520have%2520some%2520specific%2520purpose%252C%2520like%2520answering%2520questions%2520grounded%2520on%2520documentation%2520or%2520acting%2520as%2520coding%2520assistants%252C%2520but%2520they%2520require%2520general%2520language%2520understanding.%2520In%2520such%2520deployments%252C%2520LLMs%2520should%2520respond%2520only%2520to%2520queries%2520that%2520align%2520with%2520the%2520intended%2520purpose%2520and%2520reject%2520all%2520other%2520requests%252C%2520such%2520as%2520generating%2520poetry%2520or%2520answering%2520questions%2520about%2520physics%252C%2520a%2520task%2520we%2520refer%2520to%2520as%2520%2560scoping%2527.%2520We%2520conduct%2520a%2520comprehensive%2520empirical%2520evaluation%2520of%2520various%2520methods%252C%2520ranging%2520from%2520prompting%252C%2520fine-tuning%2520to%2520preference%2520learning%2520and%2520the%2520recently%2520proposed%2520general%2520alignment%2520technique%2520known%2520as%2520Circuit%2520Breakers%2520%2528CB%2529.%2520Across%2520three%2520families%2520of%2520language%2520models%2520and%2520a%2520broad%2520variety%2520of%2520tasks%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520scope%2520language%2520models.%2520We%2520examine%2520scoping%2520for%2520multiple%2520topics%252C%2520and%2520fine-grained%2520topics.%2520We%2520ablate%2520diversity%2520of%2520irrelevant%2520queries%252C%2520layer%2520different%2520techniques%252C%2520conduct%2520adversarial%2520evaluations%2520and%2520more.%2520Among%2520other%2520results%252C%2520we%2520find%2520that%2520when%2520diverse%2520examples%2520of%2520irrelevant%2520queries%2520are%2520available%252C%2520simple%2520supervised%2520fine-tuning%2520produces%2520the%2520best%2520results%252C%2520but%2520when%2520such%2520diversity%2520is%2520low%252C%2520Circuit%2520Breakers%2520perform%2520quite%2520well.%2520One%2520can%2520often%2520get%2520the%2520benefits%2520of%2520both%2520methods%2520by%2520layering%2520them%2520in%2520succession.%2520We%2520intend%2520our%2520study%2520to%2520serve%2520as%2520a%2520practitioner%2527s%2520guide%2520to%2520scoping%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21597v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20the%20Scope%20of%20Language%20Models&entry.906535625=David%20Yunis%20and%20Siyu%20Huo%20and%20Chulaka%20Gunasekara%20and%20Danish%20Contractor&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20deployed%20in%20a%20wide%20variety%20of%20user-facing%20applications.%20Typically%2C%20these%20deployments%20have%20some%20specific%20purpose%2C%20like%20answering%20questions%20grounded%20on%20documentation%20or%20acting%20as%20coding%20assistants%2C%20but%20they%20require%20general%20language%20understanding.%20In%20such%20deployments%2C%20LLMs%20should%20respond%20only%20to%20queries%20that%20align%20with%20the%20intended%20purpose%20and%20reject%20all%20other%20requests%2C%20such%20as%20generating%20poetry%20or%20answering%20questions%20about%20physics%2C%20a%20task%20we%20refer%20to%20as%20%60scoping%27.%20We%20conduct%20a%20comprehensive%20empirical%20evaluation%20of%20various%20methods%2C%20ranging%20from%20prompting%2C%20fine-tuning%20to%20preference%20learning%20and%20the%20recently%20proposed%20general%20alignment%20technique%20known%20as%20Circuit%20Breakers%20%28CB%29.%20Across%20three%20families%20of%20language%20models%20and%20a%20broad%20variety%20of%20tasks%2C%20we%20show%20that%20it%20is%20possible%20to%20scope%20language%20models.%20We%20examine%20scoping%20for%20multiple%20topics%2C%20and%20fine-grained%20topics.%20We%20ablate%20diversity%20of%20irrelevant%20queries%2C%20layer%20different%20techniques%2C%20conduct%20adversarial%20evaluations%20and%20more.%20Among%20other%20results%2C%20we%20find%20that%20when%20diverse%20examples%20of%20irrelevant%20queries%20are%20available%2C%20simple%20supervised%20fine-tuning%20produces%20the%20best%20results%2C%20but%20when%20such%20diversity%20is%20low%2C%20Circuit%20Breakers%20perform%20quite%20well.%20One%20can%20often%20get%20the%20benefits%20of%20both%20methods%20by%20layering%20them%20in%20succession.%20We%20intend%20our%20study%20to%20serve%20as%20a%20practitioner%27s%20guide%20to%20scoping%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2410.21597v3&entry.124074799=Read"},
{"title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning", "author": "Zihan Gao and Yifei Xu and Jacob Thebault-Spieker", "abstract": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.", "link": "http://arxiv.org/abs/2511.10459v1", "date": "2025-11-13", "relevancy": 2.5383, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5135}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocalBench%3A%20Benchmarking%20LLMs%20on%20County-Level%20Local%20Knowledge%20and%20Reasoning&body=Title%3A%20LocalBench%3A%20Benchmarking%20LLMs%20on%20County-Level%20Local%20Knowledge%20and%20Reasoning%0AAuthor%3A%20Zihan%20Gao%20and%20Yifei%20Xu%20and%20Jacob%20Thebault-Spieker%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20evaluated%20on%20macro-scale%20geographic%20tasks%2C%20such%20as%20global%20factual%20recall%2C%20event%20summarization%2C%20and%20regional%20reasoning.%20Yet%2C%20their%20ability%20to%20handle%20hyper-local%20knowledge%20remains%20poorly%20understood.%20This%20gap%20is%20increasingly%20consequential%20as%20real-world%20applications%2C%20from%20civic%20platforms%20to%20community%20journalism%2C%20demand%20AI%20systems%20that%20can%20reason%20about%20neighborhood-specific%20dynamics%2C%20cultural%20narratives%2C%20and%20local%20governance.%20Existing%20benchmarks%20fall%20short%20in%20capturing%20this%20complexity%2C%20often%20relying%20on%20coarse-grained%20data%20or%20isolated%20references.%20We%20present%20LocalBench%2C%20the%20first%20benchmark%20designed%20to%20systematically%20evaluate%20LLMs%20on%20county-level%20local%20knowledge%20across%20the%20United%20States.%20Grounded%20in%20the%20Localness%20Conceptual%20Framework%2C%20LocalBench%20includes%2014%2C782%20validated%20question-answer%20pairs%20across%20526%20U.S.%20counties%20in%2049%20states%2C%20integrating%20diverse%20sources%20such%20as%20Census%20statistics%2C%20local%20subreddit%20discourse%2C%20and%20regional%20news.%20It%20spans%20physical%2C%20cognitive%2C%20and%20relational%20dimensions%20of%20locality.%20Using%20LocalBench%2C%20we%20evaluate%2013%20state-of-the-art%20LLMs%20under%20both%20closed-book%20and%20web-augmented%20settings.%20Our%20findings%20reveal%20critical%20limitations%3A%20even%20the%20best-performing%20models%20reach%20only%2056.8%25%20accuracy%20on%20narrative-style%20questions%20and%20perform%20below%2015.5%25%20on%20numerical%20reasoning.%20Moreover%2C%20larger%20model%20size%20and%20web%20augmentation%20do%20not%20guarantee%20better%20performance%2C%20for%20example%2C%20search%20improves%20Gemini%27s%20accuracy%20by%20%2B13.6%25%2C%20but%20reduces%20GPT-series%20performance%20by%20-11.4%25.%20These%20results%20underscore%20the%20urgent%20need%20for%20language%20models%20that%20can%20support%20equitable%2C%20place-aware%20AI%20systems%3A%20capable%20of%20engaging%20with%20the%20diverse%2C%20fine-grained%20realities%20of%20local%20communities%20across%20geographic%20and%20cultural%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalBench%253A%2520Benchmarking%2520LLMs%2520on%2520County-Level%2520Local%2520Knowledge%2520and%2520Reasoning%26entry.906535625%3DZihan%2520Gao%2520and%2520Yifei%2520Xu%2520and%2520Jacob%2520Thebault-Spieker%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520evaluated%2520on%2520macro-scale%2520geographic%2520tasks%252C%2520such%2520as%2520global%2520factual%2520recall%252C%2520event%2520summarization%252C%2520and%2520regional%2520reasoning.%2520Yet%252C%2520their%2520ability%2520to%2520handle%2520hyper-local%2520knowledge%2520remains%2520poorly%2520understood.%2520This%2520gap%2520is%2520increasingly%2520consequential%2520as%2520real-world%2520applications%252C%2520from%2520civic%2520platforms%2520to%2520community%2520journalism%252C%2520demand%2520AI%2520systems%2520that%2520can%2520reason%2520about%2520neighborhood-specific%2520dynamics%252C%2520cultural%2520narratives%252C%2520and%2520local%2520governance.%2520Existing%2520benchmarks%2520fall%2520short%2520in%2520capturing%2520this%2520complexity%252C%2520often%2520relying%2520on%2520coarse-grained%2520data%2520or%2520isolated%2520references.%2520We%2520present%2520LocalBench%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520systematically%2520evaluate%2520LLMs%2520on%2520county-level%2520local%2520knowledge%2520across%2520the%2520United%2520States.%2520Grounded%2520in%2520the%2520Localness%2520Conceptual%2520Framework%252C%2520LocalBench%2520includes%252014%252C782%2520validated%2520question-answer%2520pairs%2520across%2520526%2520U.S.%2520counties%2520in%252049%2520states%252C%2520integrating%2520diverse%2520sources%2520such%2520as%2520Census%2520statistics%252C%2520local%2520subreddit%2520discourse%252C%2520and%2520regional%2520news.%2520It%2520spans%2520physical%252C%2520cognitive%252C%2520and%2520relational%2520dimensions%2520of%2520locality.%2520Using%2520LocalBench%252C%2520we%2520evaluate%252013%2520state-of-the-art%2520LLMs%2520under%2520both%2520closed-book%2520and%2520web-augmented%2520settings.%2520Our%2520findings%2520reveal%2520critical%2520limitations%253A%2520even%2520the%2520best-performing%2520models%2520reach%2520only%252056.8%2525%2520accuracy%2520on%2520narrative-style%2520questions%2520and%2520perform%2520below%252015.5%2525%2520on%2520numerical%2520reasoning.%2520Moreover%252C%2520larger%2520model%2520size%2520and%2520web%2520augmentation%2520do%2520not%2520guarantee%2520better%2520performance%252C%2520for%2520example%252C%2520search%2520improves%2520Gemini%2527s%2520accuracy%2520by%2520%252B13.6%2525%252C%2520but%2520reduces%2520GPT-series%2520performance%2520by%2520-11.4%2525.%2520These%2520results%2520underscore%2520the%2520urgent%2520need%2520for%2520language%2520models%2520that%2520can%2520support%2520equitable%252C%2520place-aware%2520AI%2520systems%253A%2520capable%2520of%2520engaging%2520with%2520the%2520diverse%252C%2520fine-grained%2520realities%2520of%2520local%2520communities%2520across%2520geographic%2520and%2520cultural%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalBench%3A%20Benchmarking%20LLMs%20on%20County-Level%20Local%20Knowledge%20and%20Reasoning&entry.906535625=Zihan%20Gao%20and%20Yifei%20Xu%20and%20Jacob%20Thebault-Spieker&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20evaluated%20on%20macro-scale%20geographic%20tasks%2C%20such%20as%20global%20factual%20recall%2C%20event%20summarization%2C%20and%20regional%20reasoning.%20Yet%2C%20their%20ability%20to%20handle%20hyper-local%20knowledge%20remains%20poorly%20understood.%20This%20gap%20is%20increasingly%20consequential%20as%20real-world%20applications%2C%20from%20civic%20platforms%20to%20community%20journalism%2C%20demand%20AI%20systems%20that%20can%20reason%20about%20neighborhood-specific%20dynamics%2C%20cultural%20narratives%2C%20and%20local%20governance.%20Existing%20benchmarks%20fall%20short%20in%20capturing%20this%20complexity%2C%20often%20relying%20on%20coarse-grained%20data%20or%20isolated%20references.%20We%20present%20LocalBench%2C%20the%20first%20benchmark%20designed%20to%20systematically%20evaluate%20LLMs%20on%20county-level%20local%20knowledge%20across%20the%20United%20States.%20Grounded%20in%20the%20Localness%20Conceptual%20Framework%2C%20LocalBench%20includes%2014%2C782%20validated%20question-answer%20pairs%20across%20526%20U.S.%20counties%20in%2049%20states%2C%20integrating%20diverse%20sources%20such%20as%20Census%20statistics%2C%20local%20subreddit%20discourse%2C%20and%20regional%20news.%20It%20spans%20physical%2C%20cognitive%2C%20and%20relational%20dimensions%20of%20locality.%20Using%20LocalBench%2C%20we%20evaluate%2013%20state-of-the-art%20LLMs%20under%20both%20closed-book%20and%20web-augmented%20settings.%20Our%20findings%20reveal%20critical%20limitations%3A%20even%20the%20best-performing%20models%20reach%20only%2056.8%25%20accuracy%20on%20narrative-style%20questions%20and%20perform%20below%2015.5%25%20on%20numerical%20reasoning.%20Moreover%2C%20larger%20model%20size%20and%20web%20augmentation%20do%20not%20guarantee%20better%20performance%2C%20for%20example%2C%20search%20improves%20Gemini%27s%20accuracy%20by%20%2B13.6%25%2C%20but%20reduces%20GPT-series%20performance%20by%20-11.4%25.%20These%20results%20underscore%20the%20urgent%20need%20for%20language%20models%20that%20can%20support%20equitable%2C%20place-aware%20AI%20systems%3A%20capable%20of%20engaging%20with%20the%20diverse%2C%20fine-grained%20realities%20of%20local%20communities%20across%20geographic%20and%20cultural%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2511.10459v1&entry.124074799=Read"},
{"title": "H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification", "author": "Yongji Zhang and Siqi Li and Kuiyang Huang and Yue Gao and Yu Jiang", "abstract": "Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations. Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis. However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy. To address these limitations, we propose H3Former, a novel token-to-region framework that leverages high-order semantic relations to aggregate local fine-grained representations with structured region-level modeling. Specifically, we propose the Semantic-Aware Aggregation Module (SAAM), which exploits multi-scale contextual cues to dynamically construct a weighted hypergraph among tokens. By applying hypergraph convolution, SAAM captures high-order semantic dependencies and progressively aggregates token features into compact region-level representations. Furthermore, we introduce the Hyperbolic Hierarchical Contrastive Loss (HHCL), which enforces hierarchical semantic constraints in a non-Euclidean embedding space. The HHCL enhances inter-class separability and intra-class consistency while preserving the intrinsic hierarchical relationships among fine-grained categories. Comprehensive experiments conducted on four standard FGVC benchmarks validate the superiority of our H3Former framework.", "link": "http://arxiv.org/abs/2511.10260v1", "date": "2025-11-13", "relevancy": 2.5351, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H3Former%3A%20Hypergraph-based%20Semantic-Aware%20Aggregation%20via%20Hyperbolic%20Hierarchical%20Contrastive%20Loss%20for%20Fine-Grained%20Visual%20Classification&body=Title%3A%20H3Former%3A%20Hypergraph-based%20Semantic-Aware%20Aggregation%20via%20Hyperbolic%20Hierarchical%20Contrastive%20Loss%20for%20Fine-Grained%20Visual%20Classification%0AAuthor%3A%20Yongji%20Zhang%20and%20Siqi%20Li%20and%20Kuiyang%20Huang%20and%20Yue%20Gao%20and%20Yu%20Jiang%0AAbstract%3A%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20remains%20a%20challenging%20task%20due%20to%20subtle%20inter-class%20differences%20and%20large%20intra-class%20variations.%20Existing%20approaches%20typically%20rely%20on%20feature-selection%20mechanisms%20or%20region-proposal%20strategies%20to%20localize%20discriminative%20regions%20for%20semantic%20analysis.%20However%2C%20these%20methods%20often%20fail%20to%20capture%20discriminative%20cues%20comprehensively%20while%20introducing%20substantial%20category-agnostic%20redundancy.%20To%20address%20these%20limitations%2C%20we%20propose%20H3Former%2C%20a%20novel%20token-to-region%20framework%20that%20leverages%20high-order%20semantic%20relations%20to%20aggregate%20local%20fine-grained%20representations%20with%20structured%20region-level%20modeling.%20Specifically%2C%20we%20propose%20the%20Semantic-Aware%20Aggregation%20Module%20%28SAAM%29%2C%20which%20exploits%20multi-scale%20contextual%20cues%20to%20dynamically%20construct%20a%20weighted%20hypergraph%20among%20tokens.%20By%20applying%20hypergraph%20convolution%2C%20SAAM%20captures%20high-order%20semantic%20dependencies%20and%20progressively%20aggregates%20token%20features%20into%20compact%20region-level%20representations.%20Furthermore%2C%20we%20introduce%20the%20Hyperbolic%20Hierarchical%20Contrastive%20Loss%20%28HHCL%29%2C%20which%20enforces%20hierarchical%20semantic%20constraints%20in%20a%20non-Euclidean%20embedding%20space.%20The%20HHCL%20enhances%20inter-class%20separability%20and%20intra-class%20consistency%20while%20preserving%20the%20intrinsic%20hierarchical%20relationships%20among%20fine-grained%20categories.%20Comprehensive%20experiments%20conducted%20on%20four%20standard%20FGVC%20benchmarks%20validate%20the%20superiority%20of%20our%20H3Former%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH3Former%253A%2520Hypergraph-based%2520Semantic-Aware%2520Aggregation%2520via%2520Hyperbolic%2520Hierarchical%2520Contrastive%2520Loss%2520for%2520Fine-Grained%2520Visual%2520Classification%26entry.906535625%3DYongji%2520Zhang%2520and%2520Siqi%2520Li%2520and%2520Kuiyang%2520Huang%2520and%2520Yue%2520Gao%2520and%2520Yu%2520Jiang%26entry.1292438233%3DFine-Grained%2520Visual%2520Classification%2520%2528FGVC%2529%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520subtle%2520inter-class%2520differences%2520and%2520large%2520intra-class%2520variations.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520feature-selection%2520mechanisms%2520or%2520region-proposal%2520strategies%2520to%2520localize%2520discriminative%2520regions%2520for%2520semantic%2520analysis.%2520However%252C%2520these%2520methods%2520often%2520fail%2520to%2520capture%2520discriminative%2520cues%2520comprehensively%2520while%2520introducing%2520substantial%2520category-agnostic%2520redundancy.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520H3Former%252C%2520a%2520novel%2520token-to-region%2520framework%2520that%2520leverages%2520high-order%2520semantic%2520relations%2520to%2520aggregate%2520local%2520fine-grained%2520representations%2520with%2520structured%2520region-level%2520modeling.%2520Specifically%252C%2520we%2520propose%2520the%2520Semantic-Aware%2520Aggregation%2520Module%2520%2528SAAM%2529%252C%2520which%2520exploits%2520multi-scale%2520contextual%2520cues%2520to%2520dynamically%2520construct%2520a%2520weighted%2520hypergraph%2520among%2520tokens.%2520By%2520applying%2520hypergraph%2520convolution%252C%2520SAAM%2520captures%2520high-order%2520semantic%2520dependencies%2520and%2520progressively%2520aggregates%2520token%2520features%2520into%2520compact%2520region-level%2520representations.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Hyperbolic%2520Hierarchical%2520Contrastive%2520Loss%2520%2528HHCL%2529%252C%2520which%2520enforces%2520hierarchical%2520semantic%2520constraints%2520in%2520a%2520non-Euclidean%2520embedding%2520space.%2520The%2520HHCL%2520enhances%2520inter-class%2520separability%2520and%2520intra-class%2520consistency%2520while%2520preserving%2520the%2520intrinsic%2520hierarchical%2520relationships%2520among%2520fine-grained%2520categories.%2520Comprehensive%2520experiments%2520conducted%2520on%2520four%2520standard%2520FGVC%2520benchmarks%2520validate%2520the%2520superiority%2520of%2520our%2520H3Former%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H3Former%3A%20Hypergraph-based%20Semantic-Aware%20Aggregation%20via%20Hyperbolic%20Hierarchical%20Contrastive%20Loss%20for%20Fine-Grained%20Visual%20Classification&entry.906535625=Yongji%20Zhang%20and%20Siqi%20Li%20and%20Kuiyang%20Huang%20and%20Yue%20Gao%20and%20Yu%20Jiang&entry.1292438233=Fine-Grained%20Visual%20Classification%20%28FGVC%29%20remains%20a%20challenging%20task%20due%20to%20subtle%20inter-class%20differences%20and%20large%20intra-class%20variations.%20Existing%20approaches%20typically%20rely%20on%20feature-selection%20mechanisms%20or%20region-proposal%20strategies%20to%20localize%20discriminative%20regions%20for%20semantic%20analysis.%20However%2C%20these%20methods%20often%20fail%20to%20capture%20discriminative%20cues%20comprehensively%20while%20introducing%20substantial%20category-agnostic%20redundancy.%20To%20address%20these%20limitations%2C%20we%20propose%20H3Former%2C%20a%20novel%20token-to-region%20framework%20that%20leverages%20high-order%20semantic%20relations%20to%20aggregate%20local%20fine-grained%20representations%20with%20structured%20region-level%20modeling.%20Specifically%2C%20we%20propose%20the%20Semantic-Aware%20Aggregation%20Module%20%28SAAM%29%2C%20which%20exploits%20multi-scale%20contextual%20cues%20to%20dynamically%20construct%20a%20weighted%20hypergraph%20among%20tokens.%20By%20applying%20hypergraph%20convolution%2C%20SAAM%20captures%20high-order%20semantic%20dependencies%20and%20progressively%20aggregates%20token%20features%20into%20compact%20region-level%20representations.%20Furthermore%2C%20we%20introduce%20the%20Hyperbolic%20Hierarchical%20Contrastive%20Loss%20%28HHCL%29%2C%20which%20enforces%20hierarchical%20semantic%20constraints%20in%20a%20non-Euclidean%20embedding%20space.%20The%20HHCL%20enhances%20inter-class%20separability%20and%20intra-class%20consistency%20while%20preserving%20the%20intrinsic%20hierarchical%20relationships%20among%20fine-grained%20categories.%20Comprehensive%20experiments%20conducted%20on%20four%20standard%20FGVC%20benchmarks%20validate%20the%20superiority%20of%20our%20H3Former%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2511.10260v1&entry.124074799=Read"},
{"title": "Out-of-Context Misinformation Detection via Variational Domain-Invariant Learning with Test-Time Training", "author": "Xi Yang and Han Zhang and Zhijian Lin and Yibiao Hu and Hong Han", "abstract": "Out-of-context misinformation (OOC) is a low-cost form of misinformation in news reports, which refers to place authentic images into out-of-context or fabricated image-text pairings. This problem has attracted significant attention from researchers in recent years. Current methods focus on assessing image-text consistency or generating explanations. However, these approaches assume that the training and test data are drawn from the same distribution. When encountering novel news domains, models tend to perform poorly due to the lack of prior knowledge. To address this challenge, we propose \\textbf{VDT} to enhance the domain adaptation capability for OOC misinformation detection by learning domain-invariant features and test-time training mechanisms. Domain-Invariant Variational Align module is employed to jointly encodes source and target domain data to learn a separable distributional space domain-invariant features. For preserving semantic integrity, we utilize domain consistency constraint module to reconstruct the source and target domain latent distribution. During testing phase, we adopt the test-time training strategy and confidence-variance filtering module to dynamically updating the VAE encoder and classifier, facilitating the model's adaptation to the target domain distribution. Extensive experiments conducted on the benchmark dataset NewsCLIPpings demonstrate that our method outperforms state-of-the-art baselines under most domain adaptation settings.", "link": "http://arxiv.org/abs/2511.10213v1", "date": "2025-11-13", "relevancy": 2.5311, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5069}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-Context%20Misinformation%20Detection%20via%20Variational%20Domain-Invariant%20Learning%20with%20Test-Time%20Training&body=Title%3A%20Out-of-Context%20Misinformation%20Detection%20via%20Variational%20Domain-Invariant%20Learning%20with%20Test-Time%20Training%0AAuthor%3A%20Xi%20Yang%20and%20Han%20Zhang%20and%20Zhijian%20Lin%20and%20Yibiao%20Hu%20and%20Hong%20Han%0AAbstract%3A%20Out-of-context%20misinformation%20%28OOC%29%20is%20a%20low-cost%20form%20of%20misinformation%20in%20news%20reports%2C%20which%20refers%20to%20place%20authentic%20images%20into%20out-of-context%20or%20fabricated%20image-text%20pairings.%20This%20problem%20has%20attracted%20significant%20attention%20from%20researchers%20in%20recent%20years.%20Current%20methods%20focus%20on%20assessing%20image-text%20consistency%20or%20generating%20explanations.%20However%2C%20these%20approaches%20assume%20that%20the%20training%20and%20test%20data%20are%20drawn%20from%20the%20same%20distribution.%20When%20encountering%20novel%20news%20domains%2C%20models%20tend%20to%20perform%20poorly%20due%20to%20the%20lack%20of%20prior%20knowledge.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BVDT%7D%20to%20enhance%20the%20domain%20adaptation%20capability%20for%20OOC%20misinformation%20detection%20by%20learning%20domain-invariant%20features%20and%20test-time%20training%20mechanisms.%20Domain-Invariant%20Variational%20Align%20module%20is%20employed%20to%20jointly%20encodes%20source%20and%20target%20domain%20data%20to%20learn%20a%20separable%20distributional%20space%20domain-invariant%20features.%20For%20preserving%20semantic%20integrity%2C%20we%20utilize%20domain%20consistency%20constraint%20module%20to%20reconstruct%20the%20source%20and%20target%20domain%20latent%20distribution.%20During%20testing%20phase%2C%20we%20adopt%20the%20test-time%20training%20strategy%20and%20confidence-variance%20filtering%20module%20to%20dynamically%20updating%20the%20VAE%20encoder%20and%20classifier%2C%20facilitating%20the%20model%27s%20adaptation%20to%20the%20target%20domain%20distribution.%20Extensive%20experiments%20conducted%20on%20the%20benchmark%20dataset%20NewsCLIPpings%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20under%20most%20domain%20adaptation%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-Context%2520Misinformation%2520Detection%2520via%2520Variational%2520Domain-Invariant%2520Learning%2520with%2520Test-Time%2520Training%26entry.906535625%3DXi%2520Yang%2520and%2520Han%2520Zhang%2520and%2520Zhijian%2520Lin%2520and%2520Yibiao%2520Hu%2520and%2520Hong%2520Han%26entry.1292438233%3DOut-of-context%2520misinformation%2520%2528OOC%2529%2520is%2520a%2520low-cost%2520form%2520of%2520misinformation%2520in%2520news%2520reports%252C%2520which%2520refers%2520to%2520place%2520authentic%2520images%2520into%2520out-of-context%2520or%2520fabricated%2520image-text%2520pairings.%2520This%2520problem%2520has%2520attracted%2520significant%2520attention%2520from%2520researchers%2520in%2520recent%2520years.%2520Current%2520methods%2520focus%2520on%2520assessing%2520image-text%2520consistency%2520or%2520generating%2520explanations.%2520However%252C%2520these%2520approaches%2520assume%2520that%2520the%2520training%2520and%2520test%2520data%2520are%2520drawn%2520from%2520the%2520same%2520distribution.%2520When%2520encountering%2520novel%2520news%2520domains%252C%2520models%2520tend%2520to%2520perform%2520poorly%2520due%2520to%2520the%2520lack%2520of%2520prior%2520knowledge.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%255Ctextbf%257BVDT%257D%2520to%2520enhance%2520the%2520domain%2520adaptation%2520capability%2520for%2520OOC%2520misinformation%2520detection%2520by%2520learning%2520domain-invariant%2520features%2520and%2520test-time%2520training%2520mechanisms.%2520Domain-Invariant%2520Variational%2520Align%2520module%2520is%2520employed%2520to%2520jointly%2520encodes%2520source%2520and%2520target%2520domain%2520data%2520to%2520learn%2520a%2520separable%2520distributional%2520space%2520domain-invariant%2520features.%2520For%2520preserving%2520semantic%2520integrity%252C%2520we%2520utilize%2520domain%2520consistency%2520constraint%2520module%2520to%2520reconstruct%2520the%2520source%2520and%2520target%2520domain%2520latent%2520distribution.%2520During%2520testing%2520phase%252C%2520we%2520adopt%2520the%2520test-time%2520training%2520strategy%2520and%2520confidence-variance%2520filtering%2520module%2520to%2520dynamically%2520updating%2520the%2520VAE%2520encoder%2520and%2520classifier%252C%2520facilitating%2520the%2520model%2527s%2520adaptation%2520to%2520the%2520target%2520domain%2520distribution.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520benchmark%2520dataset%2520NewsCLIPpings%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520under%2520most%2520domain%2520adaptation%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-Context%20Misinformation%20Detection%20via%20Variational%20Domain-Invariant%20Learning%20with%20Test-Time%20Training&entry.906535625=Xi%20Yang%20and%20Han%20Zhang%20and%20Zhijian%20Lin%20and%20Yibiao%20Hu%20and%20Hong%20Han&entry.1292438233=Out-of-context%20misinformation%20%28OOC%29%20is%20a%20low-cost%20form%20of%20misinformation%20in%20news%20reports%2C%20which%20refers%20to%20place%20authentic%20images%20into%20out-of-context%20or%20fabricated%20image-text%20pairings.%20This%20problem%20has%20attracted%20significant%20attention%20from%20researchers%20in%20recent%20years.%20Current%20methods%20focus%20on%20assessing%20image-text%20consistency%20or%20generating%20explanations.%20However%2C%20these%20approaches%20assume%20that%20the%20training%20and%20test%20data%20are%20drawn%20from%20the%20same%20distribution.%20When%20encountering%20novel%20news%20domains%2C%20models%20tend%20to%20perform%20poorly%20due%20to%20the%20lack%20of%20prior%20knowledge.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BVDT%7D%20to%20enhance%20the%20domain%20adaptation%20capability%20for%20OOC%20misinformation%20detection%20by%20learning%20domain-invariant%20features%20and%20test-time%20training%20mechanisms.%20Domain-Invariant%20Variational%20Align%20module%20is%20employed%20to%20jointly%20encodes%20source%20and%20target%20domain%20data%20to%20learn%20a%20separable%20distributional%20space%20domain-invariant%20features.%20For%20preserving%20semantic%20integrity%2C%20we%20utilize%20domain%20consistency%20constraint%20module%20to%20reconstruct%20the%20source%20and%20target%20domain%20latent%20distribution.%20During%20testing%20phase%2C%20we%20adopt%20the%20test-time%20training%20strategy%20and%20confidence-variance%20filtering%20module%20to%20dynamically%20updating%20the%20VAE%20encoder%20and%20classifier%2C%20facilitating%20the%20model%27s%20adaptation%20to%20the%20target%20domain%20distribution.%20Extensive%20experiments%20conducted%20on%20the%20benchmark%20dataset%20NewsCLIPpings%20demonstrate%20that%20our%20method%20outperforms%20state-of-the-art%20baselines%20under%20most%20domain%20adaptation%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.10213v1&entry.124074799=Read"},
{"title": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection", "author": "Feng Ding and Wenhui Yi and Yunpeng Zhou and Xinan He and Hong Rao and Shu Hu", "abstract": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.", "link": "http://arxiv.org/abs/2511.10150v1", "date": "2025-11-13", "relevancy": 2.5246, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5112}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5064}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection&body=Title%3A%20Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection%0AAuthor%3A%20Feng%20Ding%20and%20Wenhui%20Yi%20and%20Yunpeng%20Zhou%20and%20Xinan%20He%20and%20Hong%20Rao%20and%20Shu%20Hu%0AAbstract%3A%20Fairness%20is%20a%20core%20element%20in%20the%20trustworthy%20deployment%20of%20deepfake%20detection%20models%2C%20especially%20in%20the%20field%20of%20digital%20identity%20security.%20Biases%20in%20detection%20models%20toward%20different%20demographic%20groups%2C%20such%20as%20gender%20and%20race%2C%20may%20lead%20to%20systemic%20misjudgments%2C%20exacerbating%20the%20digital%20divide%20and%20social%20inequities.%20However%2C%20current%20fairness-enhanced%20detectors%20often%20improve%20fairness%20at%20the%20cost%20of%20detection%20accuracy.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dual-mechanism%20collaborative%20optimization%20framework.%20Our%20proposed%20method%20innovatively%20integrates%20structural%20fairness%20decoupling%20and%20global%20distribution%20alignment%3A%20decoupling%20channels%20sensitive%20to%20demographic%20groups%20at%20the%20model%20architectural%20level%2C%20and%20subsequently%20reducing%20the%20distance%20between%20the%20overall%20sample%20distribution%20and%20the%20distributions%20corresponding%20to%20each%20demographic%20group%20at%20the%20feature%20level.%20Experimental%20results%20demonstrate%20that%2C%20compared%20with%20other%20methods%2C%20our%20framework%20improves%20both%20inter-group%20and%20intra-group%20fairness%20while%20maintaining%20overall%20detection%20accuracy%20across%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Bias%252C%2520Aligning%2520Distributions%253A%2520Synergistic%2520Fairness%2520Optimization%2520for%2520Deepfake%2520Detection%26entry.906535625%3DFeng%2520Ding%2520and%2520Wenhui%2520Yi%2520and%2520Yunpeng%2520Zhou%2520and%2520Xinan%2520He%2520and%2520Hong%2520Rao%2520and%2520Shu%2520Hu%26entry.1292438233%3DFairness%2520is%2520a%2520core%2520element%2520in%2520the%2520trustworthy%2520deployment%2520of%2520deepfake%2520detection%2520models%252C%2520especially%2520in%2520the%2520field%2520of%2520digital%2520identity%2520security.%2520Biases%2520in%2520detection%2520models%2520toward%2520different%2520demographic%2520groups%252C%2520such%2520as%2520gender%2520and%2520race%252C%2520may%2520lead%2520to%2520systemic%2520misjudgments%252C%2520exacerbating%2520the%2520digital%2520divide%2520and%2520social%2520inequities.%2520However%252C%2520current%2520fairness-enhanced%2520detectors%2520often%2520improve%2520fairness%2520at%2520the%2520cost%2520of%2520detection%2520accuracy.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520dual-mechanism%2520collaborative%2520optimization%2520framework.%2520Our%2520proposed%2520method%2520innovatively%2520integrates%2520structural%2520fairness%2520decoupling%2520and%2520global%2520distribution%2520alignment%253A%2520decoupling%2520channels%2520sensitive%2520to%2520demographic%2520groups%2520at%2520the%2520model%2520architectural%2520level%252C%2520and%2520subsequently%2520reducing%2520the%2520distance%2520between%2520the%2520overall%2520sample%2520distribution%2520and%2520the%2520distributions%2520corresponding%2520to%2520each%2520demographic%2520group%2520at%2520the%2520feature%2520level.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520compared%2520with%2520other%2520methods%252C%2520our%2520framework%2520improves%2520both%2520inter-group%2520and%2520intra-group%2520fairness%2520while%2520maintaining%2520overall%2520detection%2520accuracy%2520across%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Bias%2C%20Aligning%20Distributions%3A%20Synergistic%20Fairness%20Optimization%20for%20Deepfake%20Detection&entry.906535625=Feng%20Ding%20and%20Wenhui%20Yi%20and%20Yunpeng%20Zhou%20and%20Xinan%20He%20and%20Hong%20Rao%20and%20Shu%20Hu&entry.1292438233=Fairness%20is%20a%20core%20element%20in%20the%20trustworthy%20deployment%20of%20deepfake%20detection%20models%2C%20especially%20in%20the%20field%20of%20digital%20identity%20security.%20Biases%20in%20detection%20models%20toward%20different%20demographic%20groups%2C%20such%20as%20gender%20and%20race%2C%20may%20lead%20to%20systemic%20misjudgments%2C%20exacerbating%20the%20digital%20divide%20and%20social%20inequities.%20However%2C%20current%20fairness-enhanced%20detectors%20often%20improve%20fairness%20at%20the%20cost%20of%20detection%20accuracy.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20dual-mechanism%20collaborative%20optimization%20framework.%20Our%20proposed%20method%20innovatively%20integrates%20structural%20fairness%20decoupling%20and%20global%20distribution%20alignment%3A%20decoupling%20channels%20sensitive%20to%20demographic%20groups%20at%20the%20model%20architectural%20level%2C%20and%20subsequently%20reducing%20the%20distance%20between%20the%20overall%20sample%20distribution%20and%20the%20distributions%20corresponding%20to%20each%20demographic%20group%20at%20the%20feature%20level.%20Experimental%20results%20demonstrate%20that%2C%20compared%20with%20other%20methods%2C%20our%20framework%20improves%20both%20inter-group%20and%20intra-group%20fairness%20while%20maintaining%20overall%20detection%20accuracy%20across%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2511.10150v1&entry.124074799=Read"},
{"title": "Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis", "author": "Jiulong Wu and Yucheng Shen and Lingyong Yan and Haixin Sun and Deguo Xia and Jizhou Huang and Min Cao", "abstract": "Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.", "link": "http://arxiv.org/abs/2511.10254v1", "date": "2025-11-13", "relevancy": 2.499, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial-R1%3A%20Aligning%20Reasoning%20and%20Recognition%20for%20Facial%20Emotion%20Analysis&body=Title%3A%20Facial-R1%3A%20Aligning%20Reasoning%20and%20Recognition%20for%20Facial%20Emotion%20Analysis%0AAuthor%3A%20Jiulong%20Wu%20and%20Yucheng%20Shen%20and%20Lingyong%20Yan%20and%20Haixin%20Sun%20and%20Deguo%20Xia%20and%20Jizhou%20Huang%20and%20Min%20Cao%0AAbstract%3A%20Facial%20Emotion%20Analysis%20%28FEA%29%20extends%20traditional%20facial%20emotion%20recognition%20by%20incorporating%20explainable%2C%20fine-grained%20reasoning.%20The%20task%20integrates%20three%20subtasks%3A%20emotion%20recognition%2C%20facial%20Action%20Unit%20%28AU%29%20recognition%2C%20and%20AU-based%20emotion%20reasoning%20to%20model%20affective%20states%20jointly.%20While%20recent%20approaches%20leverage%20Vision-Language%20Models%20%28VLMs%29%20and%20achieve%20promising%20results%2C%20they%20face%20two%20critical%20limitations%3A%20%281%29%20hallucinated%20reasoning%2C%20where%20VLMs%20generate%20plausible%20but%20inaccurate%20explanations%20due%20to%20insufficient%20emotion-specific%20knowledge%3B%20and%20%282%29%20misalignment%20between%20emotion%20reasoning%20and%20recognition%2C%20caused%20by%20fragmented%20connections%20between%20observed%20facial%20features%20and%20final%20labels.%20We%20propose%20Facial-R1%2C%20a%20three-stage%20alignment%20framework%20that%20effectively%20addresses%20both%20challenges%20with%20minimal%20supervision.%20First%2C%20we%20employ%20instruction%20fine-tuning%20to%20establish%20basic%20emotional%20reasoning%20capability.%20Second%2C%20we%20introduce%20reinforcement%20training%20guided%20by%20emotion%20and%20AU%20labels%20as%20reward%20signals%2C%20which%20explicitly%20aligns%20the%20generated%20reasoning%20process%20with%20the%20predicted%20emotion.%20Third%2C%20we%20design%20a%20data%20synthesis%20pipeline%20that%20iteratively%20leverages%20the%20prior%20stages%20to%20expand%20the%20training%20dataset%2C%20enabling%20scalable%20self-improvement%20of%20the%20model.%20Built%20upon%20this%20framework%2C%20we%20introduce%20FEA-20K%2C%20a%20benchmark%20dataset%20comprising%2017%2C737%20training%20and%201%2C688%20test%20samples%20with%20fine-grained%20emotion%20analysis%20annotations.%20Extensive%20experiments%20across%20eight%20standard%20benchmarks%20demonstrate%20that%20Facial-R1%20achieves%20state-of-the-art%20performance%20in%20FEA%2C%20with%20strong%20generalization%20and%20robust%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial-R1%253A%2520Aligning%2520Reasoning%2520and%2520Recognition%2520for%2520Facial%2520Emotion%2520Analysis%26entry.906535625%3DJiulong%2520Wu%2520and%2520Yucheng%2520Shen%2520and%2520Lingyong%2520Yan%2520and%2520Haixin%2520Sun%2520and%2520Deguo%2520Xia%2520and%2520Jizhou%2520Huang%2520and%2520Min%2520Cao%26entry.1292438233%3DFacial%2520Emotion%2520Analysis%2520%2528FEA%2529%2520extends%2520traditional%2520facial%2520emotion%2520recognition%2520by%2520incorporating%2520explainable%252C%2520fine-grained%2520reasoning.%2520The%2520task%2520integrates%2520three%2520subtasks%253A%2520emotion%2520recognition%252C%2520facial%2520Action%2520Unit%2520%2528AU%2529%2520recognition%252C%2520and%2520AU-based%2520emotion%2520reasoning%2520to%2520model%2520affective%2520states%2520jointly.%2520While%2520recent%2520approaches%2520leverage%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520achieve%2520promising%2520results%252C%2520they%2520face%2520two%2520critical%2520limitations%253A%2520%25281%2529%2520hallucinated%2520reasoning%252C%2520where%2520VLMs%2520generate%2520plausible%2520but%2520inaccurate%2520explanations%2520due%2520to%2520insufficient%2520emotion-specific%2520knowledge%253B%2520and%2520%25282%2529%2520misalignment%2520between%2520emotion%2520reasoning%2520and%2520recognition%252C%2520caused%2520by%2520fragmented%2520connections%2520between%2520observed%2520facial%2520features%2520and%2520final%2520labels.%2520We%2520propose%2520Facial-R1%252C%2520a%2520three-stage%2520alignment%2520framework%2520that%2520effectively%2520addresses%2520both%2520challenges%2520with%2520minimal%2520supervision.%2520First%252C%2520we%2520employ%2520instruction%2520fine-tuning%2520to%2520establish%2520basic%2520emotional%2520reasoning%2520capability.%2520Second%252C%2520we%2520introduce%2520reinforcement%2520training%2520guided%2520by%2520emotion%2520and%2520AU%2520labels%2520as%2520reward%2520signals%252C%2520which%2520explicitly%2520aligns%2520the%2520generated%2520reasoning%2520process%2520with%2520the%2520predicted%2520emotion.%2520Third%252C%2520we%2520design%2520a%2520data%2520synthesis%2520pipeline%2520that%2520iteratively%2520leverages%2520the%2520prior%2520stages%2520to%2520expand%2520the%2520training%2520dataset%252C%2520enabling%2520scalable%2520self-improvement%2520of%2520the%2520model.%2520Built%2520upon%2520this%2520framework%252C%2520we%2520introduce%2520FEA-20K%252C%2520a%2520benchmark%2520dataset%2520comprising%252017%252C737%2520training%2520and%25201%252C688%2520test%2520samples%2520with%2520fine-grained%2520emotion%2520analysis%2520annotations.%2520Extensive%2520experiments%2520across%2520eight%2520standard%2520benchmarks%2520demonstrate%2520that%2520Facial-R1%2520achieves%2520state-of-the-art%2520performance%2520in%2520FEA%252C%2520with%2520strong%2520generalization%2520and%2520robust%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial-R1%3A%20Aligning%20Reasoning%20and%20Recognition%20for%20Facial%20Emotion%20Analysis&entry.906535625=Jiulong%20Wu%20and%20Yucheng%20Shen%20and%20Lingyong%20Yan%20and%20Haixin%20Sun%20and%20Deguo%20Xia%20and%20Jizhou%20Huang%20and%20Min%20Cao&entry.1292438233=Facial%20Emotion%20Analysis%20%28FEA%29%20extends%20traditional%20facial%20emotion%20recognition%20by%20incorporating%20explainable%2C%20fine-grained%20reasoning.%20The%20task%20integrates%20three%20subtasks%3A%20emotion%20recognition%2C%20facial%20Action%20Unit%20%28AU%29%20recognition%2C%20and%20AU-based%20emotion%20reasoning%20to%20model%20affective%20states%20jointly.%20While%20recent%20approaches%20leverage%20Vision-Language%20Models%20%28VLMs%29%20and%20achieve%20promising%20results%2C%20they%20face%20two%20critical%20limitations%3A%20%281%29%20hallucinated%20reasoning%2C%20where%20VLMs%20generate%20plausible%20but%20inaccurate%20explanations%20due%20to%20insufficient%20emotion-specific%20knowledge%3B%20and%20%282%29%20misalignment%20between%20emotion%20reasoning%20and%20recognition%2C%20caused%20by%20fragmented%20connections%20between%20observed%20facial%20features%20and%20final%20labels.%20We%20propose%20Facial-R1%2C%20a%20three-stage%20alignment%20framework%20that%20effectively%20addresses%20both%20challenges%20with%20minimal%20supervision.%20First%2C%20we%20employ%20instruction%20fine-tuning%20to%20establish%20basic%20emotional%20reasoning%20capability.%20Second%2C%20we%20introduce%20reinforcement%20training%20guided%20by%20emotion%20and%20AU%20labels%20as%20reward%20signals%2C%20which%20explicitly%20aligns%20the%20generated%20reasoning%20process%20with%20the%20predicted%20emotion.%20Third%2C%20we%20design%20a%20data%20synthesis%20pipeline%20that%20iteratively%20leverages%20the%20prior%20stages%20to%20expand%20the%20training%20dataset%2C%20enabling%20scalable%20self-improvement%20of%20the%20model.%20Built%20upon%20this%20framework%2C%20we%20introduce%20FEA-20K%2C%20a%20benchmark%20dataset%20comprising%2017%2C737%20training%20and%201%2C688%20test%20samples%20with%20fine-grained%20emotion%20analysis%20annotations.%20Extensive%20experiments%20across%20eight%20standard%20benchmarks%20demonstrate%20that%20Facial-R1%20achieves%20state-of-the-art%20performance%20in%20FEA%2C%20with%20strong%20generalization%20and%20robust%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2511.10254v1&entry.124074799=Read"},
{"title": "GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models", "author": "Mariam Mahran and Katharina Simbeck", "abstract": "Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations.", "link": "http://arxiv.org/abs/2510.01252v3", "date": "2025-11-13", "relevancy": 2.4882, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%20Representations%20in%20Large%20Language%20Models&body=Title%3A%20GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%20Representations%20in%20Large%20Language%20Models%0AAuthor%3A%20Mariam%20Mahran%20and%20Katharina%20Simbeck%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20massive%2C%20unstructured%20corpora%2C%20making%20it%20unclear%20which%20social%20patterns%20and%20biases%20they%20absorb%20and%20later%20reproduce.%20Existing%20evaluations%20typically%20examine%20outputs%20or%20activations%2C%20but%20rarely%20connect%20them%20back%20to%20the%20pre-training%20data.%20We%20introduce%20a%20pipeline%20that%20couples%20LLMs%20with%20sparse%20autoencoders%20%28SAEs%29%20to%20trace%20how%20different%20themes%20are%20encoded%20during%20training.%20As%20a%20controlled%20case%20study%2C%20we%20trained%20a%20GPT-style%20model%20on%2037%20nineteenth-century%20novels%20by%20ten%20female%20authors%2C%20a%20corpus%20centered%20on%20themes%20such%20as%20gender%2C%20marriage%2C%20class%2C%20and%20morality.%20By%20applying%20SAEs%20across%20layers%20and%20probing%20with%20eleven%20social%20and%20moral%20categories%2C%20we%20mapped%20sparse%20features%20to%20human-interpretable%20concepts.%20The%20analysis%20revealed%20stable%20thematic%20backbones%20%28most%20prominently%20around%20gender%20and%20kinship%29%20and%20showed%20how%20associations%20expand%20and%20entangle%20with%20depth.%20More%20broadly%2C%20we%20argue%20that%20the%20LLM%2BSAEs%20pipeline%20offers%20a%20scalable%20framework%20for%20auditing%20how%20cultural%20assumptions%20from%20the%20data%20are%20embedded%20in%20model%20representations.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01252v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520and%2520Prejudice%253A%2520A%2520Sparse%2520Approach%2520to%2520Understanding%2520Learned%2520Representations%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMariam%2520Mahran%2520and%2520Katharina%2520Simbeck%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520massive%252C%2520unstructured%2520corpora%252C%2520making%2520it%2520unclear%2520which%2520social%2520patterns%2520and%2520biases%2520they%2520absorb%2520and%2520later%2520reproduce.%2520Existing%2520evaluations%2520typically%2520examine%2520outputs%2520or%2520activations%252C%2520but%2520rarely%2520connect%2520them%2520back%2520to%2520the%2520pre-training%2520data.%2520We%2520introduce%2520a%2520pipeline%2520that%2520couples%2520LLMs%2520with%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520to%2520trace%2520how%2520different%2520themes%2520are%2520encoded%2520during%2520training.%2520As%2520a%2520controlled%2520case%2520study%252C%2520we%2520trained%2520a%2520GPT-style%2520model%2520on%252037%2520nineteenth-century%2520novels%2520by%2520ten%2520female%2520authors%252C%2520a%2520corpus%2520centered%2520on%2520themes%2520such%2520as%2520gender%252C%2520marriage%252C%2520class%252C%2520and%2520morality.%2520By%2520applying%2520SAEs%2520across%2520layers%2520and%2520probing%2520with%2520eleven%2520social%2520and%2520moral%2520categories%252C%2520we%2520mapped%2520sparse%2520features%2520to%2520human-interpretable%2520concepts.%2520The%2520analysis%2520revealed%2520stable%2520thematic%2520backbones%2520%2528most%2520prominently%2520around%2520gender%2520and%2520kinship%2529%2520and%2520showed%2520how%2520associations%2520expand%2520and%2520entangle%2520with%2520depth.%2520More%2520broadly%252C%2520we%2520argue%2520that%2520the%2520LLM%252BSAEs%2520pipeline%2520offers%2520a%2520scalable%2520framework%2520for%2520auditing%2520how%2520cultural%2520assumptions%2520from%2520the%2520data%2520are%2520embedded%2520in%2520model%2520representations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01252v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20and%20Prejudice%3A%20A%20Sparse%20Approach%20to%20Understanding%20Learned%20Representations%20in%20Large%20Language%20Models&entry.906535625=Mariam%20Mahran%20and%20Katharina%20Simbeck&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20massive%2C%20unstructured%20corpora%2C%20making%20it%20unclear%20which%20social%20patterns%20and%20biases%20they%20absorb%20and%20later%20reproduce.%20Existing%20evaluations%20typically%20examine%20outputs%20or%20activations%2C%20but%20rarely%20connect%20them%20back%20to%20the%20pre-training%20data.%20We%20introduce%20a%20pipeline%20that%20couples%20LLMs%20with%20sparse%20autoencoders%20%28SAEs%29%20to%20trace%20how%20different%20themes%20are%20encoded%20during%20training.%20As%20a%20controlled%20case%20study%2C%20we%20trained%20a%20GPT-style%20model%20on%2037%20nineteenth-century%20novels%20by%20ten%20female%20authors%2C%20a%20corpus%20centered%20on%20themes%20such%20as%20gender%2C%20marriage%2C%20class%2C%20and%20morality.%20By%20applying%20SAEs%20across%20layers%20and%20probing%20with%20eleven%20social%20and%20moral%20categories%2C%20we%20mapped%20sparse%20features%20to%20human-interpretable%20concepts.%20The%20analysis%20revealed%20stable%20thematic%20backbones%20%28most%20prominently%20around%20gender%20and%20kinship%29%20and%20showed%20how%20associations%20expand%20and%20entangle%20with%20depth.%20More%20broadly%2C%20we%20argue%20that%20the%20LLM%2BSAEs%20pipeline%20offers%20a%20scalable%20framework%20for%20auditing%20how%20cultural%20assumptions%20from%20the%20data%20are%20embedded%20in%20model%20representations.&entry.1838667208=http%3A//arxiv.org/abs/2510.01252v3&entry.124074799=Read"},
{"title": "Completion of partial structures using Patterson maps with the CrysFormer machine learning model", "author": "Tom Pan and Evan Dramko and Mitchell D. Miller and Anastasios Kyrillidis and George N. Phillips", "abstract": "Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.", "link": "http://arxiv.org/abs/2511.10440v1", "date": "2025-11-13", "relevancy": 2.48, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4985}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4985}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Completion%20of%20partial%20structures%20using%20Patterson%20maps%20with%20the%20CrysFormer%20machine%20learning%20model&body=Title%3A%20Completion%20of%20partial%20structures%20using%20Patterson%20maps%20with%20the%20CrysFormer%20machine%20learning%20model%0AAuthor%3A%20Tom%20Pan%20and%20Evan%20Dramko%20and%20Mitchell%20D.%20Miller%20and%20Anastasios%20Kyrillidis%20and%20George%20N.%20Phillips%0AAbstract%3A%20Protein%20structure%20determination%20has%20long%20been%20one%20of%20the%20primary%20challenges%20of%20structural%20biology%2C%20to%20which%20deep%20machine%20learning%20%28ML%29-based%20approaches%20have%20increasingly%20been%20applied.%20However%2C%20these%20ML%20models%20generally%20do%20not%20incorporate%20the%20experimental%20measurements%20directly%2C%20such%20as%20X-ray%20crystallographic%20diffraction%20data.%20To%20this%20end%2C%20we%20explore%20an%20approach%20that%20more%20tightly%20couples%20these%20traditional%20crystallographic%20and%20recent%20ML-based%20methods%2C%20by%20training%20a%20hybrid%203-d%20vision%20transformer%20and%20convolutional%20network%20on%20inputs%20from%20both%20domains.%20We%20make%20use%20of%20two%20distinct%20input%20constructs%20/%20Patterson%20maps%2C%20which%20are%20directly%20obtainable%20from%20crystallographic%20data%2C%20and%20%60%60partial%20structure%27%27%20template%20maps%20derived%20from%20predicted%20structures%20deposited%20in%20the%20AlphaFold%20Protein%20Structure%20Database%20with%20subsequently%20omitted%20residues.%20With%20these%2C%20we%20predict%20electron%20density%20maps%20that%20are%20then%20post-processed%20into%20atomic%20models%20through%20standard%20crystallographic%20refinement%20processes.%20Introducing%20an%20initial%20dataset%20of%20small%20protein%20fragments%20taken%20from%20Protein%20Data%20Bank%20entries%20and%20placing%20them%20in%20hypothetical%20crystal%20settings%2C%20we%20demonstrate%20that%20our%20method%20is%20effective%20at%20both%20improving%20the%20phases%20of%20the%20crystallographic%20structure%20factors%20and%20completing%20the%20regions%20missing%20from%20partial%20structure%20templates%2C%20as%20well%20as%20improving%20the%20agreement%20of%20the%20electron%20density%20maps%20with%20the%20ground%20truth%20atomic%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompletion%2520of%2520partial%2520structures%2520using%2520Patterson%2520maps%2520with%2520the%2520CrysFormer%2520machine%2520learning%2520model%26entry.906535625%3DTom%2520Pan%2520and%2520Evan%2520Dramko%2520and%2520Mitchell%2520D.%2520Miller%2520and%2520Anastasios%2520Kyrillidis%2520and%2520George%2520N.%2520Phillips%26entry.1292438233%3DProtein%2520structure%2520determination%2520has%2520long%2520been%2520one%2520of%2520the%2520primary%2520challenges%2520of%2520structural%2520biology%252C%2520to%2520which%2520deep%2520machine%2520learning%2520%2528ML%2529-based%2520approaches%2520have%2520increasingly%2520been%2520applied.%2520However%252C%2520these%2520ML%2520models%2520generally%2520do%2520not%2520incorporate%2520the%2520experimental%2520measurements%2520directly%252C%2520such%2520as%2520X-ray%2520crystallographic%2520diffraction%2520data.%2520To%2520this%2520end%252C%2520we%2520explore%2520an%2520approach%2520that%2520more%2520tightly%2520couples%2520these%2520traditional%2520crystallographic%2520and%2520recent%2520ML-based%2520methods%252C%2520by%2520training%2520a%2520hybrid%25203-d%2520vision%2520transformer%2520and%2520convolutional%2520network%2520on%2520inputs%2520from%2520both%2520domains.%2520We%2520make%2520use%2520of%2520two%2520distinct%2520input%2520constructs%2520/%2520Patterson%2520maps%252C%2520which%2520are%2520directly%2520obtainable%2520from%2520crystallographic%2520data%252C%2520and%2520%2560%2560partial%2520structure%2527%2527%2520template%2520maps%2520derived%2520from%2520predicted%2520structures%2520deposited%2520in%2520the%2520AlphaFold%2520Protein%2520Structure%2520Database%2520with%2520subsequently%2520omitted%2520residues.%2520With%2520these%252C%2520we%2520predict%2520electron%2520density%2520maps%2520that%2520are%2520then%2520post-processed%2520into%2520atomic%2520models%2520through%2520standard%2520crystallographic%2520refinement%2520processes.%2520Introducing%2520an%2520initial%2520dataset%2520of%2520small%2520protein%2520fragments%2520taken%2520from%2520Protein%2520Data%2520Bank%2520entries%2520and%2520placing%2520them%2520in%2520hypothetical%2520crystal%2520settings%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520is%2520effective%2520at%2520both%2520improving%2520the%2520phases%2520of%2520the%2520crystallographic%2520structure%2520factors%2520and%2520completing%2520the%2520regions%2520missing%2520from%2520partial%2520structure%2520templates%252C%2520as%2520well%2520as%2520improving%2520the%2520agreement%2520of%2520the%2520electron%2520density%2520maps%2520with%2520the%2520ground%2520truth%2520atomic%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Completion%20of%20partial%20structures%20using%20Patterson%20maps%20with%20the%20CrysFormer%20machine%20learning%20model&entry.906535625=Tom%20Pan%20and%20Evan%20Dramko%20and%20Mitchell%20D.%20Miller%20and%20Anastasios%20Kyrillidis%20and%20George%20N.%20Phillips&entry.1292438233=Protein%20structure%20determination%20has%20long%20been%20one%20of%20the%20primary%20challenges%20of%20structural%20biology%2C%20to%20which%20deep%20machine%20learning%20%28ML%29-based%20approaches%20have%20increasingly%20been%20applied.%20However%2C%20these%20ML%20models%20generally%20do%20not%20incorporate%20the%20experimental%20measurements%20directly%2C%20such%20as%20X-ray%20crystallographic%20diffraction%20data.%20To%20this%20end%2C%20we%20explore%20an%20approach%20that%20more%20tightly%20couples%20these%20traditional%20crystallographic%20and%20recent%20ML-based%20methods%2C%20by%20training%20a%20hybrid%203-d%20vision%20transformer%20and%20convolutional%20network%20on%20inputs%20from%20both%20domains.%20We%20make%20use%20of%20two%20distinct%20input%20constructs%20/%20Patterson%20maps%2C%20which%20are%20directly%20obtainable%20from%20crystallographic%20data%2C%20and%20%60%60partial%20structure%27%27%20template%20maps%20derived%20from%20predicted%20structures%20deposited%20in%20the%20AlphaFold%20Protein%20Structure%20Database%20with%20subsequently%20omitted%20residues.%20With%20these%2C%20we%20predict%20electron%20density%20maps%20that%20are%20then%20post-processed%20into%20atomic%20models%20through%20standard%20crystallographic%20refinement%20processes.%20Introducing%20an%20initial%20dataset%20of%20small%20protein%20fragments%20taken%20from%20Protein%20Data%20Bank%20entries%20and%20placing%20them%20in%20hypothetical%20crystal%20settings%2C%20we%20demonstrate%20that%20our%20method%20is%20effective%20at%20both%20improving%20the%20phases%20of%20the%20crystallographic%20structure%20factors%20and%20completing%20the%20regions%20missing%20from%20partial%20structure%20templates%2C%20as%20well%20as%20improving%20the%20agreement%20of%20the%20electron%20density%20maps%20with%20the%20ground%20truth%20atomic%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2511.10440v1&entry.124074799=Read"},
{"title": "ImageSet2Text: Describing Sets of Images through Text", "author": "Piera Riccio and Francesco Galati and Kajetan Schweighofer and Noa Garcia and Nuria Oliver", "abstract": "In the era of large-scale visual data, understanding collections of images is a challenging yet important task. To this end, we introduce ImageSet2Text, a novel method to automatically generate natural language descriptions of image sets. Based on large language models, visual-question answering chains, an external lexical graph, and CLIP-based verification, ImageSet2Text iteratively extracts key concepts from image subsets and organizes them into a structured concept graph. We conduct extensive experiments evaluating the quality of the generated descriptions in terms of accuracy, completeness, and user satisfaction. We also examine the method's behavior through ablation studies, scalability assessments, and failure analyses. Results demonstrate that ImageSet2Text combines data-driven AI and symbolic representations to reliably summarize large image collections for a wide range of applications.", "link": "http://arxiv.org/abs/2503.19361v2", "date": "2025-11-13", "relevancy": 2.4737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImageSet2Text%3A%20Describing%20Sets%20of%20Images%20through%20Text&body=Title%3A%20ImageSet2Text%3A%20Describing%20Sets%20of%20Images%20through%20Text%0AAuthor%3A%20Piera%20Riccio%20and%20Francesco%20Galati%20and%20Kajetan%20Schweighofer%20and%20Noa%20Garcia%20and%20Nuria%20Oliver%0AAbstract%3A%20In%20the%20era%20of%20large-scale%20visual%20data%2C%20understanding%20collections%20of%20images%20is%20a%20challenging%20yet%20important%20task.%20To%20this%20end%2C%20we%20introduce%20ImageSet2Text%2C%20a%20novel%20method%20to%20automatically%20generate%20natural%20language%20descriptions%20of%20image%20sets.%20Based%20on%20large%20language%20models%2C%20visual-question%20answering%20chains%2C%20an%20external%20lexical%20graph%2C%20and%20CLIP-based%20verification%2C%20ImageSet2Text%20iteratively%20extracts%20key%20concepts%20from%20image%20subsets%20and%20organizes%20them%20into%20a%20structured%20concept%20graph.%20We%20conduct%20extensive%20experiments%20evaluating%20the%20quality%20of%20the%20generated%20descriptions%20in%20terms%20of%20accuracy%2C%20completeness%2C%20and%20user%20satisfaction.%20We%20also%20examine%20the%20method%27s%20behavior%20through%20ablation%20studies%2C%20scalability%20assessments%2C%20and%20failure%20analyses.%20Results%20demonstrate%20that%20ImageSet2Text%20combines%20data-driven%20AI%20and%20symbolic%20representations%20to%20reliably%20summarize%20large%20image%20collections%20for%20a%20wide%20range%20of%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImageSet2Text%253A%2520Describing%2520Sets%2520of%2520Images%2520through%2520Text%26entry.906535625%3DPiera%2520Riccio%2520and%2520Francesco%2520Galati%2520and%2520Kajetan%2520Schweighofer%2520and%2520Noa%2520Garcia%2520and%2520Nuria%2520Oliver%26entry.1292438233%3DIn%2520the%2520era%2520of%2520large-scale%2520visual%2520data%252C%2520understanding%2520collections%2520of%2520images%2520is%2520a%2520challenging%2520yet%2520important%2520task.%2520To%2520this%2520end%252C%2520we%2520introduce%2520ImageSet2Text%252C%2520a%2520novel%2520method%2520to%2520automatically%2520generate%2520natural%2520language%2520descriptions%2520of%2520image%2520sets.%2520Based%2520on%2520large%2520language%2520models%252C%2520visual-question%2520answering%2520chains%252C%2520an%2520external%2520lexical%2520graph%252C%2520and%2520CLIP-based%2520verification%252C%2520ImageSet2Text%2520iteratively%2520extracts%2520key%2520concepts%2520from%2520image%2520subsets%2520and%2520organizes%2520them%2520into%2520a%2520structured%2520concept%2520graph.%2520We%2520conduct%2520extensive%2520experiments%2520evaluating%2520the%2520quality%2520of%2520the%2520generated%2520descriptions%2520in%2520terms%2520of%2520accuracy%252C%2520completeness%252C%2520and%2520user%2520satisfaction.%2520We%2520also%2520examine%2520the%2520method%2527s%2520behavior%2520through%2520ablation%2520studies%252C%2520scalability%2520assessments%252C%2520and%2520failure%2520analyses.%2520Results%2520demonstrate%2520that%2520ImageSet2Text%2520combines%2520data-driven%2520AI%2520and%2520symbolic%2520representations%2520to%2520reliably%2520summarize%2520large%2520image%2520collections%2520for%2520a%2520wide%2520range%2520of%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImageSet2Text%3A%20Describing%20Sets%20of%20Images%20through%20Text&entry.906535625=Piera%20Riccio%20and%20Francesco%20Galati%20and%20Kajetan%20Schweighofer%20and%20Noa%20Garcia%20and%20Nuria%20Oliver&entry.1292438233=In%20the%20era%20of%20large-scale%20visual%20data%2C%20understanding%20collections%20of%20images%20is%20a%20challenging%20yet%20important%20task.%20To%20this%20end%2C%20we%20introduce%20ImageSet2Text%2C%20a%20novel%20method%20to%20automatically%20generate%20natural%20language%20descriptions%20of%20image%20sets.%20Based%20on%20large%20language%20models%2C%20visual-question%20answering%20chains%2C%20an%20external%20lexical%20graph%2C%20and%20CLIP-based%20verification%2C%20ImageSet2Text%20iteratively%20extracts%20key%20concepts%20from%20image%20subsets%20and%20organizes%20them%20into%20a%20structured%20concept%20graph.%20We%20conduct%20extensive%20experiments%20evaluating%20the%20quality%20of%20the%20generated%20descriptions%20in%20terms%20of%20accuracy%2C%20completeness%2C%20and%20user%20satisfaction.%20We%20also%20examine%20the%20method%27s%20behavior%20through%20ablation%20studies%2C%20scalability%20assessments%2C%20and%20failure%20analyses.%20Results%20demonstrate%20that%20ImageSet2Text%20combines%20data-driven%20AI%20and%20symbolic%20representations%20to%20reliably%20summarize%20large%20image%20collections%20for%20a%20wide%20range%20of%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2503.19361v2&entry.124074799=Read"},
{"title": "Enhanced Structured Lasso Pruning with Class-wise Information", "author": "Xiang Liu and Mingchen Li and Xia Li and Leigang Qu and Guangsu Wang and Zifan Peng and Yijun Song and Zemin Liu and Linshan Jiang and Jialin Li", "abstract": "Modern applications require lightweight neural network models. Most existing neural network pruning methods focus on removing unimportant filters; however, these may result in the loss of statistical information after pruning due to failing to consider the class-wise information. In this paper, we employ the structured lasso from the perspective of utilizing precise class-wise information for model pruning with the help of Information Bottleneck theory, which guides us to ensure the retention of statistical information before and after pruning. With these techniques, we propose two novel adaptive network pruning schemes in parallel: sparse graph-structured lasso pruning with Information Bottleneck (sGLP-IB) and sparse tree-guided lasso pruning with Information Bottleneck (sTLP-IB). The key component is that we prune the model filters utilizing sGLP-IB and sTLP-IB with more precise structured class-wise relatedness. Compared to multiple state-of-the-art methods, our approaches achieve the best performance across three datasets and six model structures on extensive experiments. For example, with the VGG16 model based on the CIFAR-10 dataset, we can reduce the parameters by 85%, decrease the FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% better than the original). For large-scale ImageNet, we can reduce the parameters by 55% while keeping the accuracy at 76.12% (only drop 0.03%) using the ResNet architecture. In summary, we succeed in reducing the model size and computational resource usage while maintaining the effectiveness of accuracy.", "link": "http://arxiv.org/abs/2502.09125v2", "date": "2025-11-13", "relevancy": 2.4671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.492}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Structured%20Lasso%20Pruning%20with%20Class-wise%20Information&body=Title%3A%20Enhanced%20Structured%20Lasso%20Pruning%20with%20Class-wise%20Information%0AAuthor%3A%20Xiang%20Liu%20and%20Mingchen%20Li%20and%20Xia%20Li%20and%20Leigang%20Qu%20and%20Guangsu%20Wang%20and%20Zifan%20Peng%20and%20Yijun%20Song%20and%20Zemin%20Liu%20and%20Linshan%20Jiang%20and%20Jialin%20Li%0AAbstract%3A%20Modern%20applications%20require%20lightweight%20neural%20network%20models.%20Most%20existing%20neural%20network%20pruning%20methods%20focus%20on%20removing%20unimportant%20filters%3B%20however%2C%20these%20may%20result%20in%20the%20loss%20of%20statistical%20information%20after%20pruning%20due%20to%20failing%20to%20consider%20the%20class-wise%20information.%20In%20this%20paper%2C%20we%20employ%20the%20structured%20lasso%20from%20the%20perspective%20of%20utilizing%20precise%20class-wise%20information%20for%20model%20pruning%20with%20the%20help%20of%20Information%20Bottleneck%20theory%2C%20which%20guides%20us%20to%20ensure%20the%20retention%20of%20statistical%20information%20before%20and%20after%20pruning.%20With%20these%20techniques%2C%20we%20propose%20two%20novel%20adaptive%20network%20pruning%20schemes%20in%20parallel%3A%20sparse%20graph-structured%20lasso%20pruning%20with%20Information%20Bottleneck%20%28sGLP-IB%29%20and%20sparse%20tree-guided%20lasso%20pruning%20with%20Information%20Bottleneck%20%28sTLP-IB%29.%20The%20key%20component%20is%20that%20we%20prune%20the%20model%20filters%20utilizing%20sGLP-IB%20and%20sTLP-IB%20with%20more%20precise%20structured%20class-wise%20relatedness.%20Compared%20to%20multiple%20state-of-the-art%20methods%2C%20our%20approaches%20achieve%20the%20best%20performance%20across%20three%20datasets%20and%20six%20model%20structures%20on%20extensive%20experiments.%20For%20example%2C%20with%20the%20VGG16%20model%20based%20on%20the%20CIFAR-10%20dataset%2C%20we%20can%20reduce%20the%20parameters%20by%2085%25%2C%20decrease%20the%20FLOPs%20by%2061%25%2C%20and%20maintain%20an%20accuracy%20of%2094.10%25%20%280.14%25%20better%20than%20the%20original%29.%20For%20large-scale%20ImageNet%2C%20we%20can%20reduce%20the%20parameters%20by%2055%25%20while%20keeping%20the%20accuracy%20at%2076.12%25%20%28only%20drop%200.03%25%29%20using%20the%20ResNet%20architecture.%20In%20summary%2C%20we%20succeed%20in%20reducing%20the%20model%20size%20and%20computational%20resource%20usage%20while%20maintaining%20the%20effectiveness%20of%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2502.09125v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Structured%2520Lasso%2520Pruning%2520with%2520Class-wise%2520Information%26entry.906535625%3DXiang%2520Liu%2520and%2520Mingchen%2520Li%2520and%2520Xia%2520Li%2520and%2520Leigang%2520Qu%2520and%2520Guangsu%2520Wang%2520and%2520Zifan%2520Peng%2520and%2520Yijun%2520Song%2520and%2520Zemin%2520Liu%2520and%2520Linshan%2520Jiang%2520and%2520Jialin%2520Li%26entry.1292438233%3DModern%2520applications%2520require%2520lightweight%2520neural%2520network%2520models.%2520Most%2520existing%2520neural%2520network%2520pruning%2520methods%2520focus%2520on%2520removing%2520unimportant%2520filters%253B%2520however%252C%2520these%2520may%2520result%2520in%2520the%2520loss%2520of%2520statistical%2520information%2520after%2520pruning%2520due%2520to%2520failing%2520to%2520consider%2520the%2520class-wise%2520information.%2520In%2520this%2520paper%252C%2520we%2520employ%2520the%2520structured%2520lasso%2520from%2520the%2520perspective%2520of%2520utilizing%2520precise%2520class-wise%2520information%2520for%2520model%2520pruning%2520with%2520the%2520help%2520of%2520Information%2520Bottleneck%2520theory%252C%2520which%2520guides%2520us%2520to%2520ensure%2520the%2520retention%2520of%2520statistical%2520information%2520before%2520and%2520after%2520pruning.%2520With%2520these%2520techniques%252C%2520we%2520propose%2520two%2520novel%2520adaptive%2520network%2520pruning%2520schemes%2520in%2520parallel%253A%2520sparse%2520graph-structured%2520lasso%2520pruning%2520with%2520Information%2520Bottleneck%2520%2528sGLP-IB%2529%2520and%2520sparse%2520tree-guided%2520lasso%2520pruning%2520with%2520Information%2520Bottleneck%2520%2528sTLP-IB%2529.%2520The%2520key%2520component%2520is%2520that%2520we%2520prune%2520the%2520model%2520filters%2520utilizing%2520sGLP-IB%2520and%2520sTLP-IB%2520with%2520more%2520precise%2520structured%2520class-wise%2520relatedness.%2520Compared%2520to%2520multiple%2520state-of-the-art%2520methods%252C%2520our%2520approaches%2520achieve%2520the%2520best%2520performance%2520across%2520three%2520datasets%2520and%2520six%2520model%2520structures%2520on%2520extensive%2520experiments.%2520For%2520example%252C%2520with%2520the%2520VGG16%2520model%2520based%2520on%2520the%2520CIFAR-10%2520dataset%252C%2520we%2520can%2520reduce%2520the%2520parameters%2520by%252085%2525%252C%2520decrease%2520the%2520FLOPs%2520by%252061%2525%252C%2520and%2520maintain%2520an%2520accuracy%2520of%252094.10%2525%2520%25280.14%2525%2520better%2520than%2520the%2520original%2529.%2520For%2520large-scale%2520ImageNet%252C%2520we%2520can%2520reduce%2520the%2520parameters%2520by%252055%2525%2520while%2520keeping%2520the%2520accuracy%2520at%252076.12%2525%2520%2528only%2520drop%25200.03%2525%2529%2520using%2520the%2520ResNet%2520architecture.%2520In%2520summary%252C%2520we%2520succeed%2520in%2520reducing%2520the%2520model%2520size%2520and%2520computational%2520resource%2520usage%2520while%2520maintaining%2520the%2520effectiveness%2520of%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09125v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Structured%20Lasso%20Pruning%20with%20Class-wise%20Information&entry.906535625=Xiang%20Liu%20and%20Mingchen%20Li%20and%20Xia%20Li%20and%20Leigang%20Qu%20and%20Guangsu%20Wang%20and%20Zifan%20Peng%20and%20Yijun%20Song%20and%20Zemin%20Liu%20and%20Linshan%20Jiang%20and%20Jialin%20Li&entry.1292438233=Modern%20applications%20require%20lightweight%20neural%20network%20models.%20Most%20existing%20neural%20network%20pruning%20methods%20focus%20on%20removing%20unimportant%20filters%3B%20however%2C%20these%20may%20result%20in%20the%20loss%20of%20statistical%20information%20after%20pruning%20due%20to%20failing%20to%20consider%20the%20class-wise%20information.%20In%20this%20paper%2C%20we%20employ%20the%20structured%20lasso%20from%20the%20perspective%20of%20utilizing%20precise%20class-wise%20information%20for%20model%20pruning%20with%20the%20help%20of%20Information%20Bottleneck%20theory%2C%20which%20guides%20us%20to%20ensure%20the%20retention%20of%20statistical%20information%20before%20and%20after%20pruning.%20With%20these%20techniques%2C%20we%20propose%20two%20novel%20adaptive%20network%20pruning%20schemes%20in%20parallel%3A%20sparse%20graph-structured%20lasso%20pruning%20with%20Information%20Bottleneck%20%28sGLP-IB%29%20and%20sparse%20tree-guided%20lasso%20pruning%20with%20Information%20Bottleneck%20%28sTLP-IB%29.%20The%20key%20component%20is%20that%20we%20prune%20the%20model%20filters%20utilizing%20sGLP-IB%20and%20sTLP-IB%20with%20more%20precise%20structured%20class-wise%20relatedness.%20Compared%20to%20multiple%20state-of-the-art%20methods%2C%20our%20approaches%20achieve%20the%20best%20performance%20across%20three%20datasets%20and%20six%20model%20structures%20on%20extensive%20experiments.%20For%20example%2C%20with%20the%20VGG16%20model%20based%20on%20the%20CIFAR-10%20dataset%2C%20we%20can%20reduce%20the%20parameters%20by%2085%25%2C%20decrease%20the%20FLOPs%20by%2061%25%2C%20and%20maintain%20an%20accuracy%20of%2094.10%25%20%280.14%25%20better%20than%20the%20original%29.%20For%20large-scale%20ImageNet%2C%20we%20can%20reduce%20the%20parameters%20by%2055%25%20while%20keeping%20the%20accuracy%20at%2076.12%25%20%28only%20drop%200.03%25%29%20using%20the%20ResNet%20architecture.%20In%20summary%2C%20we%20succeed%20in%20reducing%20the%20model%20size%20and%20computational%20resource%20usage%20while%20maintaining%20the%20effectiveness%20of%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2502.09125v2&entry.124074799=Read"},
{"title": "MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns", "author": "Jiarui Zhang and Yuliang Liu and Zijun Wu and Guosheng Pang and Zhili Ye and Yupei Zhong and Junteng Ma and Tao Wei and Haiyang Xu and Weikai Chen and Zeen Wang and Qiangjun Ji and Fanxi Zhou and Qi Zhang and Yuanrui Hu and Jiahao Liu and Zhang Li and Ziyang Zhang and Qiang Liu and Xiang Bai", "abstract": "Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.", "link": "http://arxiv.org/abs/2511.10390v1", "date": "2025-11-13", "relevancy": 2.4535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonkeyOCR%20v1.5%20Technical%20Report%3A%20Unlocking%20Robust%20Document%20Parsing%20for%20Complex%20Patterns&body=Title%3A%20MonkeyOCR%20v1.5%20Technical%20Report%3A%20Unlocking%20Robust%20Document%20Parsing%20for%20Complex%20Patterns%0AAuthor%3A%20Jiarui%20Zhang%20and%20Yuliang%20Liu%20and%20Zijun%20Wu%20and%20Guosheng%20Pang%20and%20Zhili%20Ye%20and%20Yupei%20Zhong%20and%20Junteng%20Ma%20and%20Tao%20Wei%20and%20Haiyang%20Xu%20and%20Weikai%20Chen%20and%20Zeen%20Wang%20and%20Qiangjun%20Ji%20and%20Fanxi%20Zhou%20and%20Qi%20Zhang%20and%20Yuanrui%20Hu%20and%20Jiahao%20Liu%20and%20Zhang%20Li%20and%20Ziyang%20Zhang%20and%20Qiang%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20Document%20parsing%20is%20a%20core%20task%20in%20document%20intelligence%2C%20supporting%20applications%20such%20as%20information%20extraction%2C%20retrieval-augmented%20generation%2C%20and%20automated%20document%20analysis.%20However%2C%20real-world%20documents%20often%20feature%20complex%20layouts%20with%20multi-level%20tables%2C%20embedded%20images%20or%20formulas%2C%20and%20cross-page%20structures%2C%20which%20remain%20challenging%20for%20existing%20OCR%20systems.%20We%20introduce%20MonkeyOCR%20v1.5%2C%20a%20unified%20vision-language%20framework%20that%20enhances%20both%20layout%20understanding%20and%20content%20recognition%20through%20a%20two-stage%20parsing%20pipeline.%20The%20first%20stage%20employs%20a%20large%20multimodal%20model%20to%20jointly%20predict%20document%20layout%20and%20reading%20order%2C%20leveraging%20visual%20information%20to%20ensure%20structural%20and%20sequential%20consistency.%20The%20second%20stage%20performs%20localized%20recognition%20of%20text%2C%20formulas%2C%20and%20tables%20within%20detected%20regions%2C%20maintaining%20high%20visual%20fidelity%20while%20reducing%20error%20propagation.%20To%20address%20complex%20table%20structures%2C%20we%20propose%20a%20visual%20consistency-based%20reinforcement%20learning%20scheme%20that%20evaluates%20recognition%20quality%20via%20render-and-compare%20alignment%2C%20improving%20structural%20accuracy%20without%20manual%20annotations.%20Additionally%2C%20two%20specialized%20modules%2C%20Image-Decoupled%20Table%20Parsing%20and%20Type-Guided%20Table%20Merging%2C%20are%20introduced%20to%20enable%20reliable%20parsing%20of%20tables%20containing%20embedded%20images%20and%20reconstruction%20of%20tables%20crossing%20pages%20or%20columns.%20Comprehensive%20experiments%20on%20OmniDocBench%20v1.5%20demonstrate%20that%20MonkeyOCR%20v1.5%20achieves%20state-of-the-art%20performance%2C%20outperforming%20PPOCR-VL%20and%20MinerU%202.5%20while%20showing%20exceptional%20robustness%20in%20visually%20complex%20document%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonkeyOCR%2520v1.5%2520Technical%2520Report%253A%2520Unlocking%2520Robust%2520Document%2520Parsing%2520for%2520Complex%2520Patterns%26entry.906535625%3DJiarui%2520Zhang%2520and%2520Yuliang%2520Liu%2520and%2520Zijun%2520Wu%2520and%2520Guosheng%2520Pang%2520and%2520Zhili%2520Ye%2520and%2520Yupei%2520Zhong%2520and%2520Junteng%2520Ma%2520and%2520Tao%2520Wei%2520and%2520Haiyang%2520Xu%2520and%2520Weikai%2520Chen%2520and%2520Zeen%2520Wang%2520and%2520Qiangjun%2520Ji%2520and%2520Fanxi%2520Zhou%2520and%2520Qi%2520Zhang%2520and%2520Yuanrui%2520Hu%2520and%2520Jiahao%2520Liu%2520and%2520Zhang%2520Li%2520and%2520Ziyang%2520Zhang%2520and%2520Qiang%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3DDocument%2520parsing%2520is%2520a%2520core%2520task%2520in%2520document%2520intelligence%252C%2520supporting%2520applications%2520such%2520as%2520information%2520extraction%252C%2520retrieval-augmented%2520generation%252C%2520and%2520automated%2520document%2520analysis.%2520However%252C%2520real-world%2520documents%2520often%2520feature%2520complex%2520layouts%2520with%2520multi-level%2520tables%252C%2520embedded%2520images%2520or%2520formulas%252C%2520and%2520cross-page%2520structures%252C%2520which%2520remain%2520challenging%2520for%2520existing%2520OCR%2520systems.%2520We%2520introduce%2520MonkeyOCR%2520v1.5%252C%2520a%2520unified%2520vision-language%2520framework%2520that%2520enhances%2520both%2520layout%2520understanding%2520and%2520content%2520recognition%2520through%2520a%2520two-stage%2520parsing%2520pipeline.%2520The%2520first%2520stage%2520employs%2520a%2520large%2520multimodal%2520model%2520to%2520jointly%2520predict%2520document%2520layout%2520and%2520reading%2520order%252C%2520leveraging%2520visual%2520information%2520to%2520ensure%2520structural%2520and%2520sequential%2520consistency.%2520The%2520second%2520stage%2520performs%2520localized%2520recognition%2520of%2520text%252C%2520formulas%252C%2520and%2520tables%2520within%2520detected%2520regions%252C%2520maintaining%2520high%2520visual%2520fidelity%2520while%2520reducing%2520error%2520propagation.%2520To%2520address%2520complex%2520table%2520structures%252C%2520we%2520propose%2520a%2520visual%2520consistency-based%2520reinforcement%2520learning%2520scheme%2520that%2520evaluates%2520recognition%2520quality%2520via%2520render-and-compare%2520alignment%252C%2520improving%2520structural%2520accuracy%2520without%2520manual%2520annotations.%2520Additionally%252C%2520two%2520specialized%2520modules%252C%2520Image-Decoupled%2520Table%2520Parsing%2520and%2520Type-Guided%2520Table%2520Merging%252C%2520are%2520introduced%2520to%2520enable%2520reliable%2520parsing%2520of%2520tables%2520containing%2520embedded%2520images%2520and%2520reconstruction%2520of%2520tables%2520crossing%2520pages%2520or%2520columns.%2520Comprehensive%2520experiments%2520on%2520OmniDocBench%2520v1.5%2520demonstrate%2520that%2520MonkeyOCR%2520v1.5%2520achieves%2520state-of-the-art%2520performance%252C%2520outperforming%2520PPOCR-VL%2520and%2520MinerU%25202.5%2520while%2520showing%2520exceptional%2520robustness%2520in%2520visually%2520complex%2520document%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonkeyOCR%20v1.5%20Technical%20Report%3A%20Unlocking%20Robust%20Document%20Parsing%20for%20Complex%20Patterns&entry.906535625=Jiarui%20Zhang%20and%20Yuliang%20Liu%20and%20Zijun%20Wu%20and%20Guosheng%20Pang%20and%20Zhili%20Ye%20and%20Yupei%20Zhong%20and%20Junteng%20Ma%20and%20Tao%20Wei%20and%20Haiyang%20Xu%20and%20Weikai%20Chen%20and%20Zeen%20Wang%20and%20Qiangjun%20Ji%20and%20Fanxi%20Zhou%20and%20Qi%20Zhang%20and%20Yuanrui%20Hu%20and%20Jiahao%20Liu%20and%20Zhang%20Li%20and%20Ziyang%20Zhang%20and%20Qiang%20Liu%20and%20Xiang%20Bai&entry.1292438233=Document%20parsing%20is%20a%20core%20task%20in%20document%20intelligence%2C%20supporting%20applications%20such%20as%20information%20extraction%2C%20retrieval-augmented%20generation%2C%20and%20automated%20document%20analysis.%20However%2C%20real-world%20documents%20often%20feature%20complex%20layouts%20with%20multi-level%20tables%2C%20embedded%20images%20or%20formulas%2C%20and%20cross-page%20structures%2C%20which%20remain%20challenging%20for%20existing%20OCR%20systems.%20We%20introduce%20MonkeyOCR%20v1.5%2C%20a%20unified%20vision-language%20framework%20that%20enhances%20both%20layout%20understanding%20and%20content%20recognition%20through%20a%20two-stage%20parsing%20pipeline.%20The%20first%20stage%20employs%20a%20large%20multimodal%20model%20to%20jointly%20predict%20document%20layout%20and%20reading%20order%2C%20leveraging%20visual%20information%20to%20ensure%20structural%20and%20sequential%20consistency.%20The%20second%20stage%20performs%20localized%20recognition%20of%20text%2C%20formulas%2C%20and%20tables%20within%20detected%20regions%2C%20maintaining%20high%20visual%20fidelity%20while%20reducing%20error%20propagation.%20To%20address%20complex%20table%20structures%2C%20we%20propose%20a%20visual%20consistency-based%20reinforcement%20learning%20scheme%20that%20evaluates%20recognition%20quality%20via%20render-and-compare%20alignment%2C%20improving%20structural%20accuracy%20without%20manual%20annotations.%20Additionally%2C%20two%20specialized%20modules%2C%20Image-Decoupled%20Table%20Parsing%20and%20Type-Guided%20Table%20Merging%2C%20are%20introduced%20to%20enable%20reliable%20parsing%20of%20tables%20containing%20embedded%20images%20and%20reconstruction%20of%20tables%20crossing%20pages%20or%20columns.%20Comprehensive%20experiments%20on%20OmniDocBench%20v1.5%20demonstrate%20that%20MonkeyOCR%20v1.5%20achieves%20state-of-the-art%20performance%2C%20outperforming%20PPOCR-VL%20and%20MinerU%202.5%20while%20showing%20exceptional%20robustness%20in%20visually%20complex%20document%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2511.10390v1&entry.124074799=Read"},
{"title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "author": "Shenzhi Wang and Le Yu and Chang Gao and Chujie Zheng and Shixuan Liu and Rui Lu and Kai Dang and Xionghui Chen and Jianxin Yang and Zhenru Zhang and Yuqiong Liu and An Yang and Andrew Zhao and Yang Yue and Shiji Song and Bowen Yu and Gao Huang and Junyang Lin", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.", "link": "http://arxiv.org/abs/2506.01939v2", "date": "2025-11-13", "relevancy": 2.4526, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%2080/20%20Rule%3A%20High-Entropy%20Minority%20Tokens%20Drive%20Effective%20Reinforcement%20Learning%20for%20LLM%20Reasoning&body=Title%3A%20Beyond%20the%2080/20%20Rule%3A%20High-Entropy%20Minority%20Tokens%20Drive%20Effective%20Reinforcement%20Learning%20for%20LLM%20Reasoning%0AAuthor%3A%20Shenzhi%20Wang%20and%20Le%20Yu%20and%20Chang%20Gao%20and%20Chujie%20Zheng%20and%20Shixuan%20Liu%20and%20Rui%20Lu%20and%20Kai%20Dang%20and%20Xionghui%20Chen%20and%20Jianxin%20Yang%20and%20Zhenru%20Zhang%20and%20Yuqiong%20Liu%20and%20An%20Yang%20and%20Andrew%20Zhao%20and%20Yang%20Yue%20and%20Shiji%20Song%20and%20Bowen%20Yu%20and%20Gao%20Huang%20and%20Junyang%20Lin%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20enhancing%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20while%20its%20mechanisms%20are%20not%20yet%20well%20understood.%20In%20this%20work%2C%20we%20undertake%20a%20pioneering%20exploration%20of%20RLVR%20through%20the%20novel%20perspective%20of%20token%20entropy%20patterns%2C%20comprehensively%20analyzing%20how%20different%20tokens%20influence%20reasoning%20performance.%20By%20examining%20token%20entropy%20patterns%20in%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20we%20observe%20that%20only%20a%20small%20fraction%20of%20tokens%20exhibit%20high%20entropy%2C%20and%20these%20tokens%20act%20as%20critical%20forks%20that%20steer%20the%20model%20toward%20diverse%20reasoning%20pathways.%20Furthermore%2C%20studying%20how%20entropy%20patterns%20evolve%20during%20RLVR%20training%20reveals%20that%20RLVR%20largely%20adheres%20to%20the%20base%20model%27s%20entropy%20patterns%2C%20primarily%20adjusting%20the%20entropy%20of%20high-entropy%20tokens.%20These%20findings%20highlight%20the%20significance%20of%20high-entropy%20tokens%20%28i.e.%2C%20forking%20tokens%29%20to%20RLVR.%20We%20ultimately%20improve%20RLVR%20by%20restricting%20policy%20gradient%20updates%20to%20forking%20tokens%20and%20uncover%20a%20finding%20even%20beyond%20the%2080/20%20rule%3A%20utilizing%20only%2020%25%20of%20the%20tokens%20while%20maintaining%20performance%20comparable%20to%20full-gradient%20updates%20on%20the%20Qwen3-8B%20base%20model%20and%20significantly%20surpassing%20full-gradient%20updates%20on%20the%20Qwen3-32B%20%28%2B11.04%20on%20AIME%2725%20and%20%2B7.71%20on%20AIME%2724%29%20and%20Qwen3-14B%20%28%2B4.79%20on%20AIME%2725%20and%20%2B5.21%20on%20AIME%2724%29%20base%20models%2C%20highlighting%20a%20strong%20scaling%20trend.%20In%20contrast%2C%20training%20exclusively%20on%20the%2080%25%20lowest-entropy%20tokens%20leads%20to%20a%20marked%20decline%20in%20performance.%20These%20findings%20indicate%20that%20the%20efficacy%20of%20RLVR%20primarily%20arises%20from%20optimizing%20the%20high-entropy%20tokens%20that%20decide%20reasoning%20directions.%20Collectively%2C%20our%20results%20highlight%20the%20potential%20to%20understand%20RLVR%20through%20a%20token-entropy%20perspective%20and%20optimize%20RLVR%20by%20leveraging%20high-entropy%20minority%20tokens%20to%20further%20improve%20LLM%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%252080/20%2520Rule%253A%2520High-Entropy%2520Minority%2520Tokens%2520Drive%2520Effective%2520Reinforcement%2520Learning%2520for%2520LLM%2520Reasoning%26entry.906535625%3DShenzhi%2520Wang%2520and%2520Le%2520Yu%2520and%2520Chang%2520Gao%2520and%2520Chujie%2520Zheng%2520and%2520Shixuan%2520Liu%2520and%2520Rui%2520Lu%2520and%2520Kai%2520Dang%2520and%2520Xionghui%2520Chen%2520and%2520Jianxin%2520Yang%2520and%2520Zhenru%2520Zhang%2520and%2520Yuqiong%2520Liu%2520and%2520An%2520Yang%2520and%2520Andrew%2520Zhao%2520and%2520Yang%2520Yue%2520and%2520Shiji%2520Song%2520and%2520Bowen%2520Yu%2520and%2520Gao%2520Huang%2520and%2520Junyang%2520Lin%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520to%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520while%2520its%2520mechanisms%2520are%2520not%2520yet%2520well%2520understood.%2520In%2520this%2520work%252C%2520we%2520undertake%2520a%2520pioneering%2520exploration%2520of%2520RLVR%2520through%2520the%2520novel%2520perspective%2520of%2520token%2520entropy%2520patterns%252C%2520comprehensively%2520analyzing%2520how%2520different%2520tokens%2520influence%2520reasoning%2520performance.%2520By%2520examining%2520token%2520entropy%2520patterns%2520in%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520we%2520observe%2520that%2520only%2520a%2520small%2520fraction%2520of%2520tokens%2520exhibit%2520high%2520entropy%252C%2520and%2520these%2520tokens%2520act%2520as%2520critical%2520forks%2520that%2520steer%2520the%2520model%2520toward%2520diverse%2520reasoning%2520pathways.%2520Furthermore%252C%2520studying%2520how%2520entropy%2520patterns%2520evolve%2520during%2520RLVR%2520training%2520reveals%2520that%2520RLVR%2520largely%2520adheres%2520to%2520the%2520base%2520model%2527s%2520entropy%2520patterns%252C%2520primarily%2520adjusting%2520the%2520entropy%2520of%2520high-entropy%2520tokens.%2520These%2520findings%2520highlight%2520the%2520significance%2520of%2520high-entropy%2520tokens%2520%2528i.e.%252C%2520forking%2520tokens%2529%2520to%2520RLVR.%2520We%2520ultimately%2520improve%2520RLVR%2520by%2520restricting%2520policy%2520gradient%2520updates%2520to%2520forking%2520tokens%2520and%2520uncover%2520a%2520finding%2520even%2520beyond%2520the%252080/20%2520rule%253A%2520utilizing%2520only%252020%2525%2520of%2520the%2520tokens%2520while%2520maintaining%2520performance%2520comparable%2520to%2520full-gradient%2520updates%2520on%2520the%2520Qwen3-8B%2520base%2520model%2520and%2520significantly%2520surpassing%2520full-gradient%2520updates%2520on%2520the%2520Qwen3-32B%2520%2528%252B11.04%2520on%2520AIME%252725%2520and%2520%252B7.71%2520on%2520AIME%252724%2529%2520and%2520Qwen3-14B%2520%2528%252B4.79%2520on%2520AIME%252725%2520and%2520%252B5.21%2520on%2520AIME%252724%2529%2520base%2520models%252C%2520highlighting%2520a%2520strong%2520scaling%2520trend.%2520In%2520contrast%252C%2520training%2520exclusively%2520on%2520the%252080%2525%2520lowest-entropy%2520tokens%2520leads%2520to%2520a%2520marked%2520decline%2520in%2520performance.%2520These%2520findings%2520indicate%2520that%2520the%2520efficacy%2520of%2520RLVR%2520primarily%2520arises%2520from%2520optimizing%2520the%2520high-entropy%2520tokens%2520that%2520decide%2520reasoning%2520directions.%2520Collectively%252C%2520our%2520results%2520highlight%2520the%2520potential%2520to%2520understand%2520RLVR%2520through%2520a%2520token-entropy%2520perspective%2520and%2520optimize%2520RLVR%2520by%2520leveraging%2520high-entropy%2520minority%2520tokens%2520to%2520further%2520improve%2520LLM%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%2080/20%20Rule%3A%20High-Entropy%20Minority%20Tokens%20Drive%20Effective%20Reinforcement%20Learning%20for%20LLM%20Reasoning&entry.906535625=Shenzhi%20Wang%20and%20Le%20Yu%20and%20Chang%20Gao%20and%20Chujie%20Zheng%20and%20Shixuan%20Liu%20and%20Rui%20Lu%20and%20Kai%20Dang%20and%20Xionghui%20Chen%20and%20Jianxin%20Yang%20and%20Zhenru%20Zhang%20and%20Yuqiong%20Liu%20and%20An%20Yang%20and%20Andrew%20Zhao%20and%20Yang%20Yue%20and%20Shiji%20Song%20and%20Bowen%20Yu%20and%20Gao%20Huang%20and%20Junyang%20Lin&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20enhancing%20the%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20while%20its%20mechanisms%20are%20not%20yet%20well%20understood.%20In%20this%20work%2C%20we%20undertake%20a%20pioneering%20exploration%20of%20RLVR%20through%20the%20novel%20perspective%20of%20token%20entropy%20patterns%2C%20comprehensively%20analyzing%20how%20different%20tokens%20influence%20reasoning%20performance.%20By%20examining%20token%20entropy%20patterns%20in%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20we%20observe%20that%20only%20a%20small%20fraction%20of%20tokens%20exhibit%20high%20entropy%2C%20and%20these%20tokens%20act%20as%20critical%20forks%20that%20steer%20the%20model%20toward%20diverse%20reasoning%20pathways.%20Furthermore%2C%20studying%20how%20entropy%20patterns%20evolve%20during%20RLVR%20training%20reveals%20that%20RLVR%20largely%20adheres%20to%20the%20base%20model%27s%20entropy%20patterns%2C%20primarily%20adjusting%20the%20entropy%20of%20high-entropy%20tokens.%20These%20findings%20highlight%20the%20significance%20of%20high-entropy%20tokens%20%28i.e.%2C%20forking%20tokens%29%20to%20RLVR.%20We%20ultimately%20improve%20RLVR%20by%20restricting%20policy%20gradient%20updates%20to%20forking%20tokens%20and%20uncover%20a%20finding%20even%20beyond%20the%2080/20%20rule%3A%20utilizing%20only%2020%25%20of%20the%20tokens%20while%20maintaining%20performance%20comparable%20to%20full-gradient%20updates%20on%20the%20Qwen3-8B%20base%20model%20and%20significantly%20surpassing%20full-gradient%20updates%20on%20the%20Qwen3-32B%20%28%2B11.04%20on%20AIME%2725%20and%20%2B7.71%20on%20AIME%2724%29%20and%20Qwen3-14B%20%28%2B4.79%20on%20AIME%2725%20and%20%2B5.21%20on%20AIME%2724%29%20base%20models%2C%20highlighting%20a%20strong%20scaling%20trend.%20In%20contrast%2C%20training%20exclusively%20on%20the%2080%25%20lowest-entropy%20tokens%20leads%20to%20a%20marked%20decline%20in%20performance.%20These%20findings%20indicate%20that%20the%20efficacy%20of%20RLVR%20primarily%20arises%20from%20optimizing%20the%20high-entropy%20tokens%20that%20decide%20reasoning%20directions.%20Collectively%2C%20our%20results%20highlight%20the%20potential%20to%20understand%20RLVR%20through%20a%20token-entropy%20perspective%20and%20optimize%20RLVR%20by%20leveraging%20high-entropy%20minority%20tokens%20to%20further%20improve%20LLM%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2506.01939v2&entry.124074799=Read"},
{"title": "A Novel Sliced Fused Gromov-Wasserstein Distance", "author": "Moritz Piening and Robert Beinert", "abstract": "The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are powerful tools for comparing heterogeneous data. Their computation is, however, challenging since both distances are based on non-convex, quadratic optimal transport (OT) problems. Leveraging 1D OT, a sliced version of GW has been proposed to lower the computational burden. Unfortunately, this sliced version is restricted to Euclidean geometry and loses invariance to isometries, strongly limiting its application in practice. To overcome these issues, we propose a novel slicing technique for GW as well as for FGW that is based on an appropriate lower bound, hierarchical OT, and suitable quadrature rules for the underlying 1D OT problems. Our novel sliced FGW significantly reduces the numerical effort while remaining invariant to isometric transformations and allowing the comparison of arbitrary geometries. We show that our new distance actually defines a pseudo-metric for structured spaces that bounds FGW from below and study its interpolation properties between sliced Wasserstein and GW. Since we avoid the underlying quadratic program, our sliced distance is numerically more robust and reliable than the original GW and FGW distance; especially in the context of shape retrieval and graph isomorphism testing.", "link": "http://arxiv.org/abs/2508.02364v2", "date": "2025-11-13", "relevancy": 2.4418, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4998}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4928}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance&body=Title%3A%20A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance%0AAuthor%3A%20Moritz%20Piening%20and%20Robert%20Beinert%0AAbstract%3A%20The%20Gromov--Wasserstein%20%28GW%29%20distance%20and%20its%20fused%20extension%20%28FGW%29%20are%20powerful%20tools%20for%20comparing%20heterogeneous%20data.%20Their%20computation%20is%2C%20however%2C%20challenging%20since%20both%20distances%20are%20based%20on%20non-convex%2C%20quadratic%20optimal%20transport%20%28OT%29%20problems.%20Leveraging%201D%20OT%2C%20a%20sliced%20version%20of%20GW%20has%20been%20proposed%20to%20lower%20the%20computational%20burden.%20Unfortunately%2C%20this%20sliced%20version%20is%20restricted%20to%20Euclidean%20geometry%20and%20loses%20invariance%20to%20isometries%2C%20strongly%20limiting%20its%20application%20in%20practice.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20novel%20slicing%20technique%20for%20GW%20as%20well%20as%20for%20FGW%20that%20is%20based%20on%20an%20appropriate%20lower%20bound%2C%20hierarchical%20OT%2C%20and%20suitable%20quadrature%20rules%20for%20the%20underlying%201D%20OT%20problems.%20Our%20novel%20sliced%20FGW%20significantly%20reduces%20the%20numerical%20effort%20while%20remaining%20invariant%20to%20isometric%20transformations%20and%20allowing%20the%20comparison%20of%20arbitrary%20geometries.%20We%20show%20that%20our%20new%20distance%20actually%20defines%20a%20pseudo-metric%20for%20structured%20spaces%20that%20bounds%20FGW%20from%20below%20and%20study%20its%20interpolation%20properties%20between%20sliced%20Wasserstein%20and%20GW.%20Since%20we%20avoid%20the%20underlying%20quadratic%20program%2C%20our%20sliced%20distance%20is%20numerically%20more%20robust%20and%20reliable%20than%20the%20original%20GW%20and%20FGW%20distance%3B%20especially%20in%20the%20context%20of%20shape%20retrieval%20and%20graph%20isomorphism%20testing.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Sliced%2520Fused%2520Gromov-Wasserstein%2520Distance%26entry.906535625%3DMoritz%2520Piening%2520and%2520Robert%2520Beinert%26entry.1292438233%3DThe%2520Gromov--Wasserstein%2520%2528GW%2529%2520distance%2520and%2520its%2520fused%2520extension%2520%2528FGW%2529%2520are%2520powerful%2520tools%2520for%2520comparing%2520heterogeneous%2520data.%2520Their%2520computation%2520is%252C%2520however%252C%2520challenging%2520since%2520both%2520distances%2520are%2520based%2520on%2520non-convex%252C%2520quadratic%2520optimal%2520transport%2520%2528OT%2529%2520problems.%2520Leveraging%25201D%2520OT%252C%2520a%2520sliced%2520version%2520of%2520GW%2520has%2520been%2520proposed%2520to%2520lower%2520the%2520computational%2520burden.%2520Unfortunately%252C%2520this%2520sliced%2520version%2520is%2520restricted%2520to%2520Euclidean%2520geometry%2520and%2520loses%2520invariance%2520to%2520isometries%252C%2520strongly%2520limiting%2520its%2520application%2520in%2520practice.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520slicing%2520technique%2520for%2520GW%2520as%2520well%2520as%2520for%2520FGW%2520that%2520is%2520based%2520on%2520an%2520appropriate%2520lower%2520bound%252C%2520hierarchical%2520OT%252C%2520and%2520suitable%2520quadrature%2520rules%2520for%2520the%2520underlying%25201D%2520OT%2520problems.%2520Our%2520novel%2520sliced%2520FGW%2520significantly%2520reduces%2520the%2520numerical%2520effort%2520while%2520remaining%2520invariant%2520to%2520isometric%2520transformations%2520and%2520allowing%2520the%2520comparison%2520of%2520arbitrary%2520geometries.%2520We%2520show%2520that%2520our%2520new%2520distance%2520actually%2520defines%2520a%2520pseudo-metric%2520for%2520structured%2520spaces%2520that%2520bounds%2520FGW%2520from%2520below%2520and%2520study%2520its%2520interpolation%2520properties%2520between%2520sliced%2520Wasserstein%2520and%2520GW.%2520Since%2520we%2520avoid%2520the%2520underlying%2520quadratic%2520program%252C%2520our%2520sliced%2520distance%2520is%2520numerically%2520more%2520robust%2520and%2520reliable%2520than%2520the%2520original%2520GW%2520and%2520FGW%2520distance%253B%2520especially%2520in%2520the%2520context%2520of%2520shape%2520retrieval%2520and%2520graph%2520isomorphism%2520testing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Sliced%20Fused%20Gromov-Wasserstein%20Distance&entry.906535625=Moritz%20Piening%20and%20Robert%20Beinert&entry.1292438233=The%20Gromov--Wasserstein%20%28GW%29%20distance%20and%20its%20fused%20extension%20%28FGW%29%20are%20powerful%20tools%20for%20comparing%20heterogeneous%20data.%20Their%20computation%20is%2C%20however%2C%20challenging%20since%20both%20distances%20are%20based%20on%20non-convex%2C%20quadratic%20optimal%20transport%20%28OT%29%20problems.%20Leveraging%201D%20OT%2C%20a%20sliced%20version%20of%20GW%20has%20been%20proposed%20to%20lower%20the%20computational%20burden.%20Unfortunately%2C%20this%20sliced%20version%20is%20restricted%20to%20Euclidean%20geometry%20and%20loses%20invariance%20to%20isometries%2C%20strongly%20limiting%20its%20application%20in%20practice.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20novel%20slicing%20technique%20for%20GW%20as%20well%20as%20for%20FGW%20that%20is%20based%20on%20an%20appropriate%20lower%20bound%2C%20hierarchical%20OT%2C%20and%20suitable%20quadrature%20rules%20for%20the%20underlying%201D%20OT%20problems.%20Our%20novel%20sliced%20FGW%20significantly%20reduces%20the%20numerical%20effort%20while%20remaining%20invariant%20to%20isometric%20transformations%20and%20allowing%20the%20comparison%20of%20arbitrary%20geometries.%20We%20show%20that%20our%20new%20distance%20actually%20defines%20a%20pseudo-metric%20for%20structured%20spaces%20that%20bounds%20FGW%20from%20below%20and%20study%20its%20interpolation%20properties%20between%20sliced%20Wasserstein%20and%20GW.%20Since%20we%20avoid%20the%20underlying%20quadratic%20program%2C%20our%20sliced%20distance%20is%20numerically%20more%20robust%20and%20reliable%20than%20the%20original%20GW%20and%20FGW%20distance%3B%20especially%20in%20the%20context%20of%20shape%20retrieval%20and%20graph%20isomorphism%20testing.&entry.1838667208=http%3A//arxiv.org/abs/2508.02364v2&entry.124074799=Read"},
{"title": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners", "author": "Daniel Herbst and Lea Karbeska and Divyanshu Kumar and Akanksha Ahuja and Fatemeh Gholamzadeh Nasrabadi and Fabrizio Frasca", "abstract": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.", "link": "http://arxiv.org/abs/2511.10234v1", "date": "2025-11-13", "relevancy": 2.4312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners&body=Title%3A%20Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners%0AAuthor%3A%20Daniel%20Herbst%20and%20Lea%20Karbeska%20and%20Divyanshu%20Kumar%20and%20Akanksha%20Ahuja%20and%20Fatemeh%20Gholamzadeh%20Nasrabadi%20and%20Fabrizio%20Frasca%0AAbstract%3A%20While%20promising%2C%20graph%20reasoners%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20lack%20built-in%20invariance%20to%20symmetries%20in%20graph%20representations.%20Operating%20on%20sequential%20graph%20serializations%2C%20LLMs%20can%20produce%20different%20outputs%20under%20node%20reindexing%2C%20edge%20reordering%2C%20or%20formatting%20changes%2C%20raising%20robustness%20concerns.%20We%20systematically%20analyze%20these%20effects%2C%20studying%20how%20fine-tuning%20impacts%20encoding%20sensitivity%20as%20well%20generalization%20on%20unseen%20tasks.%20We%20propose%20a%20principled%20decomposition%20of%20graph%20serializations%20into%20node%20labeling%2C%20edge%20encoding%2C%20and%20syntax%2C%20and%20evaluate%20LLM%20robustness%20to%20variations%20of%20each%20of%20these%20factors%20on%20a%20comprehensive%20benchmarking%20suite.%20We%20also%20contribute%20a%20novel%20set%20of%20spectral%20tasks%20to%20further%20assess%20generalization%20abilities%20of%20fine-tuned%20reasoners.%20Results%20show%20that%20larger%20%28non-fine-tuned%29%20models%20are%20more%20robust.%20Fine-tuning%20reduces%20sensitivity%20to%20node%20relabeling%20but%20may%20increase%20it%20to%20variations%20in%20structure%20and%20format%2C%20while%20it%20does%20not%20consistently%20improve%20performance%20on%20unseen%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Serialization%253A%2520Invariance%2520and%2520Generalization%2520of%2520LLM%2520Graph%2520Reasoners%26entry.906535625%3DDaniel%2520Herbst%2520and%2520Lea%2520Karbeska%2520and%2520Divyanshu%2520Kumar%2520and%2520Akanksha%2520Ahuja%2520and%2520Fatemeh%2520Gholamzadeh%2520Nasrabadi%2520and%2520Fabrizio%2520Frasca%26entry.1292438233%3DWhile%2520promising%252C%2520graph%2520reasoners%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520lack%2520built-in%2520invariance%2520to%2520symmetries%2520in%2520graph%2520representations.%2520Operating%2520on%2520sequential%2520graph%2520serializations%252C%2520LLMs%2520can%2520produce%2520different%2520outputs%2520under%2520node%2520reindexing%252C%2520edge%2520reordering%252C%2520or%2520formatting%2520changes%252C%2520raising%2520robustness%2520concerns.%2520We%2520systematically%2520analyze%2520these%2520effects%252C%2520studying%2520how%2520fine-tuning%2520impacts%2520encoding%2520sensitivity%2520as%2520well%2520generalization%2520on%2520unseen%2520tasks.%2520We%2520propose%2520a%2520principled%2520decomposition%2520of%2520graph%2520serializations%2520into%2520node%2520labeling%252C%2520edge%2520encoding%252C%2520and%2520syntax%252C%2520and%2520evaluate%2520LLM%2520robustness%2520to%2520variations%2520of%2520each%2520of%2520these%2520factors%2520on%2520a%2520comprehensive%2520benchmarking%2520suite.%2520We%2520also%2520contribute%2520a%2520novel%2520set%2520of%2520spectral%2520tasks%2520to%2520further%2520assess%2520generalization%2520abilities%2520of%2520fine-tuned%2520reasoners.%2520Results%2520show%2520that%2520larger%2520%2528non-fine-tuned%2529%2520models%2520are%2520more%2520robust.%2520Fine-tuning%2520reduces%2520sensitivity%2520to%2520node%2520relabeling%2520but%2520may%2520increase%2520it%2520to%2520variations%2520in%2520structure%2520and%2520format%252C%2520while%2520it%2520does%2520not%2520consistently%2520improve%2520performance%2520on%2520unseen%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Serialization%3A%20Invariance%20and%20Generalization%20of%20LLM%20Graph%20Reasoners&entry.906535625=Daniel%20Herbst%20and%20Lea%20Karbeska%20and%20Divyanshu%20Kumar%20and%20Akanksha%20Ahuja%20and%20Fatemeh%20Gholamzadeh%20Nasrabadi%20and%20Fabrizio%20Frasca&entry.1292438233=While%20promising%2C%20graph%20reasoners%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20lack%20built-in%20invariance%20to%20symmetries%20in%20graph%20representations.%20Operating%20on%20sequential%20graph%20serializations%2C%20LLMs%20can%20produce%20different%20outputs%20under%20node%20reindexing%2C%20edge%20reordering%2C%20or%20formatting%20changes%2C%20raising%20robustness%20concerns.%20We%20systematically%20analyze%20these%20effects%2C%20studying%20how%20fine-tuning%20impacts%20encoding%20sensitivity%20as%20well%20generalization%20on%20unseen%20tasks.%20We%20propose%20a%20principled%20decomposition%20of%20graph%20serializations%20into%20node%20labeling%2C%20edge%20encoding%2C%20and%20syntax%2C%20and%20evaluate%20LLM%20robustness%20to%20variations%20of%20each%20of%20these%20factors%20on%20a%20comprehensive%20benchmarking%20suite.%20We%20also%20contribute%20a%20novel%20set%20of%20spectral%20tasks%20to%20further%20assess%20generalization%20abilities%20of%20fine-tuned%20reasoners.%20Results%20show%20that%20larger%20%28non-fine-tuned%29%20models%20are%20more%20robust.%20Fine-tuning%20reduces%20sensitivity%20to%20node%20relabeling%20but%20may%20increase%20it%20to%20variations%20in%20structure%20and%20format%2C%20while%20it%20does%20not%20consistently%20improve%20performance%20on%20unseen%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.10234v1&entry.124074799=Read"},
{"title": "Efficient quantification on large-scale networks", "author": "Alessio Micheli and Alejandro Moreo and Marco Podda and Fabrizio Sebastiani and William Simoni and Domenico Tortorella", "abstract": "Network quantification (NQ) is the problem of estimating the proportions of nodes belonging to each class in subsets of unlabelled graph nodes. When prior probability shift is at play, this task cannot be effectively addressed by first classifying the nodes and then counting the class predictions. In addition, unlike non-relational quantification, NQ demands enhanced flexibility in order to capture a broad range of connectivity patterns, resilience to the challenge of heterophily, and scalability to large networks. In order to meet these stringent requirements, we introduce XNQ, a novel method that synergizes the flexibility and efficiency of the unsupervised node embeddings computed by randomized recursive Graph Neural Networks, with an Expectation-Maximization algorithm that provides a robust quantification-aware adjustment to the output probabilities of a calibrated node classifier. In an extensive evaluation, in which we also validate the design choices underpinning XNQ through comprehensive ablation experiments, we find that XNQ consistently and significantly improves on the best network quantification methods to date, thereby setting the new state of the art for this challenging task. XNQ also provides a training speed-up of up to 10x-100x over other methods based on graph learning.", "link": "http://arxiv.org/abs/2503.15267v2", "date": "2025-11-13", "relevancy": 2.4169, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4726}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20quantification%20on%20large-scale%20networks&body=Title%3A%20Efficient%20quantification%20on%20large-scale%20networks%0AAuthor%3A%20Alessio%20Micheli%20and%20Alejandro%20Moreo%20and%20Marco%20Podda%20and%20Fabrizio%20Sebastiani%20and%20William%20Simoni%20and%20Domenico%20Tortorella%0AAbstract%3A%20Network%20quantification%20%28NQ%29%20is%20the%20problem%20of%20estimating%20the%20proportions%20of%20nodes%20belonging%20to%20each%20class%20in%20subsets%20of%20unlabelled%20graph%20nodes.%20When%20prior%20probability%20shift%20is%20at%20play%2C%20this%20task%20cannot%20be%20effectively%20addressed%20by%20first%20classifying%20the%20nodes%20and%20then%20counting%20the%20class%20predictions.%20In%20addition%2C%20unlike%20non-relational%20quantification%2C%20NQ%20demands%20enhanced%20flexibility%20in%20order%20to%20capture%20a%20broad%20range%20of%20connectivity%20patterns%2C%20resilience%20to%20the%20challenge%20of%20heterophily%2C%20and%20scalability%20to%20large%20networks.%20In%20order%20to%20meet%20these%20stringent%20requirements%2C%20we%20introduce%20XNQ%2C%20a%20novel%20method%20that%20synergizes%20the%20flexibility%20and%20efficiency%20of%20the%20unsupervised%20node%20embeddings%20computed%20by%20randomized%20recursive%20Graph%20Neural%20Networks%2C%20with%20an%20Expectation-Maximization%20algorithm%20that%20provides%20a%20robust%20quantification-aware%20adjustment%20to%20the%20output%20probabilities%20of%20a%20calibrated%20node%20classifier.%20In%20an%20extensive%20evaluation%2C%20in%20which%20we%20also%20validate%20the%20design%20choices%20underpinning%20XNQ%20through%20comprehensive%20ablation%20experiments%2C%20we%20find%20that%20XNQ%20consistently%20and%20significantly%20improves%20on%20the%20best%20network%20quantification%20methods%20to%20date%2C%20thereby%20setting%20the%20new%20state%20of%20the%20art%20for%20this%20challenging%20task.%20XNQ%20also%20provides%20a%20training%20speed-up%20of%20up%20to%2010x-100x%20over%20other%20methods%20based%20on%20graph%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2503.15267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520quantification%2520on%2520large-scale%2520networks%26entry.906535625%3DAlessio%2520Micheli%2520and%2520Alejandro%2520Moreo%2520and%2520Marco%2520Podda%2520and%2520Fabrizio%2520Sebastiani%2520and%2520William%2520Simoni%2520and%2520Domenico%2520Tortorella%26entry.1292438233%3DNetwork%2520quantification%2520%2528NQ%2529%2520is%2520the%2520problem%2520of%2520estimating%2520the%2520proportions%2520of%2520nodes%2520belonging%2520to%2520each%2520class%2520in%2520subsets%2520of%2520unlabelled%2520graph%2520nodes.%2520When%2520prior%2520probability%2520shift%2520is%2520at%2520play%252C%2520this%2520task%2520cannot%2520be%2520effectively%2520addressed%2520by%2520first%2520classifying%2520the%2520nodes%2520and%2520then%2520counting%2520the%2520class%2520predictions.%2520In%2520addition%252C%2520unlike%2520non-relational%2520quantification%252C%2520NQ%2520demands%2520enhanced%2520flexibility%2520in%2520order%2520to%2520capture%2520a%2520broad%2520range%2520of%2520connectivity%2520patterns%252C%2520resilience%2520to%2520the%2520challenge%2520of%2520heterophily%252C%2520and%2520scalability%2520to%2520large%2520networks.%2520In%2520order%2520to%2520meet%2520these%2520stringent%2520requirements%252C%2520we%2520introduce%2520XNQ%252C%2520a%2520novel%2520method%2520that%2520synergizes%2520the%2520flexibility%2520and%2520efficiency%2520of%2520the%2520unsupervised%2520node%2520embeddings%2520computed%2520by%2520randomized%2520recursive%2520Graph%2520Neural%2520Networks%252C%2520with%2520an%2520Expectation-Maximization%2520algorithm%2520that%2520provides%2520a%2520robust%2520quantification-aware%2520adjustment%2520to%2520the%2520output%2520probabilities%2520of%2520a%2520calibrated%2520node%2520classifier.%2520In%2520an%2520extensive%2520evaluation%252C%2520in%2520which%2520we%2520also%2520validate%2520the%2520design%2520choices%2520underpinning%2520XNQ%2520through%2520comprehensive%2520ablation%2520experiments%252C%2520we%2520find%2520that%2520XNQ%2520consistently%2520and%2520significantly%2520improves%2520on%2520the%2520best%2520network%2520quantification%2520methods%2520to%2520date%252C%2520thereby%2520setting%2520the%2520new%2520state%2520of%2520the%2520art%2520for%2520this%2520challenging%2520task.%2520XNQ%2520also%2520provides%2520a%2520training%2520speed-up%2520of%2520up%2520to%252010x-100x%2520over%2520other%2520methods%2520based%2520on%2520graph%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20quantification%20on%20large-scale%20networks&entry.906535625=Alessio%20Micheli%20and%20Alejandro%20Moreo%20and%20Marco%20Podda%20and%20Fabrizio%20Sebastiani%20and%20William%20Simoni%20and%20Domenico%20Tortorella&entry.1292438233=Network%20quantification%20%28NQ%29%20is%20the%20problem%20of%20estimating%20the%20proportions%20of%20nodes%20belonging%20to%20each%20class%20in%20subsets%20of%20unlabelled%20graph%20nodes.%20When%20prior%20probability%20shift%20is%20at%20play%2C%20this%20task%20cannot%20be%20effectively%20addressed%20by%20first%20classifying%20the%20nodes%20and%20then%20counting%20the%20class%20predictions.%20In%20addition%2C%20unlike%20non-relational%20quantification%2C%20NQ%20demands%20enhanced%20flexibility%20in%20order%20to%20capture%20a%20broad%20range%20of%20connectivity%20patterns%2C%20resilience%20to%20the%20challenge%20of%20heterophily%2C%20and%20scalability%20to%20large%20networks.%20In%20order%20to%20meet%20these%20stringent%20requirements%2C%20we%20introduce%20XNQ%2C%20a%20novel%20method%20that%20synergizes%20the%20flexibility%20and%20efficiency%20of%20the%20unsupervised%20node%20embeddings%20computed%20by%20randomized%20recursive%20Graph%20Neural%20Networks%2C%20with%20an%20Expectation-Maximization%20algorithm%20that%20provides%20a%20robust%20quantification-aware%20adjustment%20to%20the%20output%20probabilities%20of%20a%20calibrated%20node%20classifier.%20In%20an%20extensive%20evaluation%2C%20in%20which%20we%20also%20validate%20the%20design%20choices%20underpinning%20XNQ%20through%20comprehensive%20ablation%20experiments%2C%20we%20find%20that%20XNQ%20consistently%20and%20significantly%20improves%20on%20the%20best%20network%20quantification%20methods%20to%20date%2C%20thereby%20setting%20the%20new%20state%20of%20the%20art%20for%20this%20challenging%20task.%20XNQ%20also%20provides%20a%20training%20speed-up%20of%20up%20to%2010x-100x%20over%20other%20methods%20based%20on%20graph%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2503.15267v2&entry.124074799=Read"},
{"title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation", "author": "Daniele Perlo and Vladimir Despotovic and Selma Boudissa and Sang-Yoon Kim and Petr Nazarov and Yanrong Zhang and Max Wintermark and Olivier Keunen", "abstract": "We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357", "link": "http://arxiv.org/abs/2511.10431v1", "date": "2025-11-13", "relevancy": 2.3896, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5011}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.482}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation&body=Title%3A%20RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation%0AAuthor%3A%20Daniele%20Perlo%20and%20Vladimir%20Despotovic%20and%20Selma%20Boudissa%20and%20Sang-Yoon%20Kim%20and%20Petr%20Nazarov%20and%20Yanrong%20Zhang%20and%20Max%20Wintermark%20and%20Olivier%20Keunen%0AAbstract%3A%20We%20introduce%20a%20curated%20video%20dataset%20of%20laboratory%20rodents%20for%20automatic%20detection%20of%20convulsive%20events.%20The%20dataset%20contains%20short%20%2810~s%29%20top-down%20and%20side-view%20video%20clips%20of%20individual%20rodents%2C%20labeled%20at%20clip%20level%20as%20normal%20activity%20or%20seizure.%20It%20includes%2010%2C101%20negative%20samples%20and%202%2C952%20positive%20samples%20collected%20from%2019%20subjects.%20We%20describe%20the%20data%20curation%2C%20annotation%20protocol%20and%20preprocessing%20pipeline%2C%20and%20report%20baseline%20experiments%20using%20a%20transformer-based%20video%20classifier%20%28TimeSformer%29.%20Experiments%20employ%20five-fold%20cross-validation%20with%20strict%20subject-wise%20partitioning%20to%20prevent%20data%20leakage%20%28no%20subject%20appears%20in%20more%20than%20one%20fold%29.%20Results%20show%20that%20the%20TimeSformer%20architecture%20enables%20discrimination%20between%20seizure%20and%20normal%20activity%20with%20an%20average%20F1-score%20of%2097%25.%20The%20dataset%20and%20baseline%20code%20are%20publicly%20released%20to%20support%20reproducible%20research%20on%20non-invasive%2C%20video-based%20monitoring%20in%20preclinical%20epilepsy%20research.%20RodEpil%20Dataset%20access%20-%20DOI%3A%2010.5281/zenodo.17601357%0ALink%3A%20http%3A//arxiv.org/abs/2511.10431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRodEpil%253A%2520A%2520Video%2520Dataset%2520of%2520Laboratory%2520Rodents%2520for%2520Seizure%2520Detection%2520and%2520Benchmark%2520Evaluation%26entry.906535625%3DDaniele%2520Perlo%2520and%2520Vladimir%2520Despotovic%2520and%2520Selma%2520Boudissa%2520and%2520Sang-Yoon%2520Kim%2520and%2520Petr%2520Nazarov%2520and%2520Yanrong%2520Zhang%2520and%2520Max%2520Wintermark%2520and%2520Olivier%2520Keunen%26entry.1292438233%3DWe%2520introduce%2520a%2520curated%2520video%2520dataset%2520of%2520laboratory%2520rodents%2520for%2520automatic%2520detection%2520of%2520convulsive%2520events.%2520The%2520dataset%2520contains%2520short%2520%252810~s%2529%2520top-down%2520and%2520side-view%2520video%2520clips%2520of%2520individual%2520rodents%252C%2520labeled%2520at%2520clip%2520level%2520as%2520normal%2520activity%2520or%2520seizure.%2520It%2520includes%252010%252C101%2520negative%2520samples%2520and%25202%252C952%2520positive%2520samples%2520collected%2520from%252019%2520subjects.%2520We%2520describe%2520the%2520data%2520curation%252C%2520annotation%2520protocol%2520and%2520preprocessing%2520pipeline%252C%2520and%2520report%2520baseline%2520experiments%2520using%2520a%2520transformer-based%2520video%2520classifier%2520%2528TimeSformer%2529.%2520Experiments%2520employ%2520five-fold%2520cross-validation%2520with%2520strict%2520subject-wise%2520partitioning%2520to%2520prevent%2520data%2520leakage%2520%2528no%2520subject%2520appears%2520in%2520more%2520than%2520one%2520fold%2529.%2520Results%2520show%2520that%2520the%2520TimeSformer%2520architecture%2520enables%2520discrimination%2520between%2520seizure%2520and%2520normal%2520activity%2520with%2520an%2520average%2520F1-score%2520of%252097%2525.%2520The%2520dataset%2520and%2520baseline%2520code%2520are%2520publicly%2520released%2520to%2520support%2520reproducible%2520research%2520on%2520non-invasive%252C%2520video-based%2520monitoring%2520in%2520preclinical%2520epilepsy%2520research.%2520RodEpil%2520Dataset%2520access%2520-%2520DOI%253A%252010.5281/zenodo.17601357%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation&entry.906535625=Daniele%20Perlo%20and%20Vladimir%20Despotovic%20and%20Selma%20Boudissa%20and%20Sang-Yoon%20Kim%20and%20Petr%20Nazarov%20and%20Yanrong%20Zhang%20and%20Max%20Wintermark%20and%20Olivier%20Keunen&entry.1292438233=We%20introduce%20a%20curated%20video%20dataset%20of%20laboratory%20rodents%20for%20automatic%20detection%20of%20convulsive%20events.%20The%20dataset%20contains%20short%20%2810~s%29%20top-down%20and%20side-view%20video%20clips%20of%20individual%20rodents%2C%20labeled%20at%20clip%20level%20as%20normal%20activity%20or%20seizure.%20It%20includes%2010%2C101%20negative%20samples%20and%202%2C952%20positive%20samples%20collected%20from%2019%20subjects.%20We%20describe%20the%20data%20curation%2C%20annotation%20protocol%20and%20preprocessing%20pipeline%2C%20and%20report%20baseline%20experiments%20using%20a%20transformer-based%20video%20classifier%20%28TimeSformer%29.%20Experiments%20employ%20five-fold%20cross-validation%20with%20strict%20subject-wise%20partitioning%20to%20prevent%20data%20leakage%20%28no%20subject%20appears%20in%20more%20than%20one%20fold%29.%20Results%20show%20that%20the%20TimeSformer%20architecture%20enables%20discrimination%20between%20seizure%20and%20normal%20activity%20with%20an%20average%20F1-score%20of%2097%25.%20The%20dataset%20and%20baseline%20code%20are%20publicly%20released%20to%20support%20reproducible%20research%20on%20non-invasive%2C%20video-based%20monitoring%20in%20preclinical%20epilepsy%20research.%20RodEpil%20Dataset%20access%20-%20DOI%3A%2010.5281/zenodo.17601357&entry.1838667208=http%3A//arxiv.org/abs/2511.10431v1&entry.124074799=Read"},
{"title": "SphereDiff: Tuning-free 360\u00b0 Static and Dynamic Panorama Generation via Spherical Latent Representation", "author": "Minho Park and Taewoong Kang and Jooyeol Yun and Sungwon Hwang and Jaegul Choo", "abstract": "The increasing demand for AR/VR applications has highlighted the need for high-quality content, such as 360\u00b0 live wallpapers. However, generating high-quality 360\u00b0 panoramic contents remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or adopt tuning-free methods that still rely on ERP latent representations, often resulting in distracting distortions near the poles. In this paper, we introduce SphereDiff, a novel approach for synthesizing 360\u00b0 static and live wallpaper with state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures consistent quality across all perspectives, including near the poles. Then, we extend MultiDiffusion to spherical latent representation and propose a dynamic spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality. Our method outperforms existing approaches in generating 360\u00b0 static and live wallpaper, making it a robust solution for immersive AR/VR applications. The code is available here. https://github.com/pmh9960/SphereDiff", "link": "http://arxiv.org/abs/2504.14396v2", "date": "2025-11-13", "relevancy": 2.386, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6094}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SphereDiff%3A%20Tuning-free%20360%C2%B0%20Static%20and%20Dynamic%20Panorama%20Generation%20via%20Spherical%20Latent%20Representation&body=Title%3A%20SphereDiff%3A%20Tuning-free%20360%C2%B0%20Static%20and%20Dynamic%20Panorama%20Generation%20via%20Spherical%20Latent%20Representation%0AAuthor%3A%20Minho%20Park%20and%20Taewoong%20Kang%20and%20Jooyeol%20Yun%20and%20Sungwon%20Hwang%20and%20Jaegul%20Choo%0AAbstract%3A%20The%20increasing%20demand%20for%20AR/VR%20applications%20has%20highlighted%20the%20need%20for%20high-quality%20content%2C%20such%20as%20360%C2%B0%20live%20wallpapers.%20However%2C%20generating%20high-quality%20360%C2%B0%20panoramic%20contents%20remains%20a%20challenging%20task%20due%20to%20the%20severe%20distortions%20introduced%20by%20equirectangular%20projection%20%28ERP%29.%20Existing%20approaches%20either%20fine-tune%20pretrained%20diffusion%20models%20on%20limited%20ERP%20datasets%20or%20adopt%20tuning-free%20methods%20that%20still%20rely%20on%20ERP%20latent%20representations%2C%20often%20resulting%20in%20distracting%20distortions%20near%20the%20poles.%20In%20this%20paper%2C%20we%20introduce%20SphereDiff%2C%20a%20novel%20approach%20for%20synthesizing%20360%C2%B0%20static%20and%20live%20wallpaper%20with%20state-of-the-art%20diffusion%20models%20without%20additional%20tuning.%20We%20define%20a%20spherical%20latent%20representation%20that%20ensures%20consistent%20quality%20across%20all%20perspectives%2C%20including%20near%20the%20poles.%20Then%2C%20we%20extend%20MultiDiffusion%20to%20spherical%20latent%20representation%20and%20propose%20a%20dynamic%20spherical%20latent%20sampling%20method%20to%20enable%20direct%20use%20of%20pretrained%20diffusion%20models.%20Moreover%2C%20we%20introduce%20distortion-aware%20weighted%20averaging%20to%20further%20improve%20the%20generation%20quality.%20Our%20method%20outperforms%20existing%20approaches%20in%20generating%20360%C2%B0%20static%20and%20live%20wallpaper%2C%20making%20it%20a%20robust%20solution%20for%20immersive%20AR/VR%20applications.%20The%20code%20is%20available%20here.%20https%3A//github.com/pmh9960/SphereDiff%0ALink%3A%20http%3A//arxiv.org/abs/2504.14396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSphereDiff%253A%2520Tuning-free%2520360%25C2%25B0%2520Static%2520and%2520Dynamic%2520Panorama%2520Generation%2520via%2520Spherical%2520Latent%2520Representation%26entry.906535625%3DMinho%2520Park%2520and%2520Taewoong%2520Kang%2520and%2520Jooyeol%2520Yun%2520and%2520Sungwon%2520Hwang%2520and%2520Jaegul%2520Choo%26entry.1292438233%3DThe%2520increasing%2520demand%2520for%2520AR/VR%2520applications%2520has%2520highlighted%2520the%2520need%2520for%2520high-quality%2520content%252C%2520such%2520as%2520360%25C2%25B0%2520live%2520wallpapers.%2520However%252C%2520generating%2520high-quality%2520360%25C2%25B0%2520panoramic%2520contents%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520severe%2520distortions%2520introduced%2520by%2520equirectangular%2520projection%2520%2528ERP%2529.%2520Existing%2520approaches%2520either%2520fine-tune%2520pretrained%2520diffusion%2520models%2520on%2520limited%2520ERP%2520datasets%2520or%2520adopt%2520tuning-free%2520methods%2520that%2520still%2520rely%2520on%2520ERP%2520latent%2520representations%252C%2520often%2520resulting%2520in%2520distracting%2520distortions%2520near%2520the%2520poles.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SphereDiff%252C%2520a%2520novel%2520approach%2520for%2520synthesizing%2520360%25C2%25B0%2520static%2520and%2520live%2520wallpaper%2520with%2520state-of-the-art%2520diffusion%2520models%2520without%2520additional%2520tuning.%2520We%2520define%2520a%2520spherical%2520latent%2520representation%2520that%2520ensures%2520consistent%2520quality%2520across%2520all%2520perspectives%252C%2520including%2520near%2520the%2520poles.%2520Then%252C%2520we%2520extend%2520MultiDiffusion%2520to%2520spherical%2520latent%2520representation%2520and%2520propose%2520a%2520dynamic%2520spherical%2520latent%2520sampling%2520method%2520to%2520enable%2520direct%2520use%2520of%2520pretrained%2520diffusion%2520models.%2520Moreover%252C%2520we%2520introduce%2520distortion-aware%2520weighted%2520averaging%2520to%2520further%2520improve%2520the%2520generation%2520quality.%2520Our%2520method%2520outperforms%2520existing%2520approaches%2520in%2520generating%2520360%25C2%25B0%2520static%2520and%2520live%2520wallpaper%252C%2520making%2520it%2520a%2520robust%2520solution%2520for%2520immersive%2520AR/VR%2520applications.%2520The%2520code%2520is%2520available%2520here.%2520https%253A//github.com/pmh9960/SphereDiff%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SphereDiff%3A%20Tuning-free%20360%C2%B0%20Static%20and%20Dynamic%20Panorama%20Generation%20via%20Spherical%20Latent%20Representation&entry.906535625=Minho%20Park%20and%20Taewoong%20Kang%20and%20Jooyeol%20Yun%20and%20Sungwon%20Hwang%20and%20Jaegul%20Choo&entry.1292438233=The%20increasing%20demand%20for%20AR/VR%20applications%20has%20highlighted%20the%20need%20for%20high-quality%20content%2C%20such%20as%20360%C2%B0%20live%20wallpapers.%20However%2C%20generating%20high-quality%20360%C2%B0%20panoramic%20contents%20remains%20a%20challenging%20task%20due%20to%20the%20severe%20distortions%20introduced%20by%20equirectangular%20projection%20%28ERP%29.%20Existing%20approaches%20either%20fine-tune%20pretrained%20diffusion%20models%20on%20limited%20ERP%20datasets%20or%20adopt%20tuning-free%20methods%20that%20still%20rely%20on%20ERP%20latent%20representations%2C%20often%20resulting%20in%20distracting%20distortions%20near%20the%20poles.%20In%20this%20paper%2C%20we%20introduce%20SphereDiff%2C%20a%20novel%20approach%20for%20synthesizing%20360%C2%B0%20static%20and%20live%20wallpaper%20with%20state-of-the-art%20diffusion%20models%20without%20additional%20tuning.%20We%20define%20a%20spherical%20latent%20representation%20that%20ensures%20consistent%20quality%20across%20all%20perspectives%2C%20including%20near%20the%20poles.%20Then%2C%20we%20extend%20MultiDiffusion%20to%20spherical%20latent%20representation%20and%20propose%20a%20dynamic%20spherical%20latent%20sampling%20method%20to%20enable%20direct%20use%20of%20pretrained%20diffusion%20models.%20Moreover%2C%20we%20introduce%20distortion-aware%20weighted%20averaging%20to%20further%20improve%20the%20generation%20quality.%20Our%20method%20outperforms%20existing%20approaches%20in%20generating%20360%C2%B0%20static%20and%20live%20wallpaper%2C%20making%20it%20a%20robust%20solution%20for%20immersive%20AR/VR%20applications.%20The%20code%20is%20available%20here.%20https%3A//github.com/pmh9960/SphereDiff&entry.1838667208=http%3A//arxiv.org/abs/2504.14396v2&entry.124074799=Read"},
{"title": "Towards Leveraging Sequential Structure in Animal Vocalizations", "author": "Eklavya Sarkar and Mathew Magimai. -Doss", "abstract": "Animal vocalizations contain sequential structures that carry important communicative information, yet most computational bioacoustics studies average the extracted frame-level features across the temporal axis, discarding the order of the sub-units within a vocalization. This paper investigates whether discrete acoustic token sequences, derived through vector quantization and gumbel-softmax vector quantization of extracted self-supervised speech model representations can effectively capture and leverage temporal information. To that end, pairwise distance analysis of token sequences generated from HuBERT embeddings shows that they can discriminate call-types and callers across four bioacoustics datasets. Sequence classification experiments using $k$-Nearest Neighbour with Levenshtein distance show that the vector-quantized token sequences yield reasonable call-type and caller classification performances, and hold promise as alternative feature representations towards leveraging sequential information in animal vocalizations.", "link": "http://arxiv.org/abs/2511.10190v1", "date": "2025-11-13", "relevancy": 2.373, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Leveraging%20Sequential%20Structure%20in%20Animal%20Vocalizations&body=Title%3A%20Towards%20Leveraging%20Sequential%20Structure%20in%20Animal%20Vocalizations%0AAuthor%3A%20Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss%0AAbstract%3A%20Animal%20vocalizations%20contain%20sequential%20structures%20that%20carry%20important%20communicative%20information%2C%20yet%20most%20computational%20bioacoustics%20studies%20average%20the%20extracted%20frame-level%20features%20across%20the%20temporal%20axis%2C%20discarding%20the%20order%20of%20the%20sub-units%20within%20a%20vocalization.%20This%20paper%20investigates%20whether%20discrete%20acoustic%20token%20sequences%2C%20derived%20through%20vector%20quantization%20and%20gumbel-softmax%20vector%20quantization%20of%20extracted%20self-supervised%20speech%20model%20representations%20can%20effectively%20capture%20and%20leverage%20temporal%20information.%20To%20that%20end%2C%20pairwise%20distance%20analysis%20of%20token%20sequences%20generated%20from%20HuBERT%20embeddings%20shows%20that%20they%20can%20discriminate%20call-types%20and%20callers%20across%20four%20bioacoustics%20datasets.%20Sequence%20classification%20experiments%20using%20%24k%24-Nearest%20Neighbour%20with%20Levenshtein%20distance%20show%20that%20the%20vector-quantized%20token%20sequences%20yield%20reasonable%20call-type%20and%20caller%20classification%20performances%2C%20and%20hold%20promise%20as%20alternative%20feature%20representations%20towards%20leveraging%20sequential%20information%20in%20animal%20vocalizations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Leveraging%2520Sequential%2520Structure%2520in%2520Animal%2520Vocalizations%26entry.906535625%3DEklavya%2520Sarkar%2520and%2520Mathew%2520Magimai.%2520-Doss%26entry.1292438233%3DAnimal%2520vocalizations%2520contain%2520sequential%2520structures%2520that%2520carry%2520important%2520communicative%2520information%252C%2520yet%2520most%2520computational%2520bioacoustics%2520studies%2520average%2520the%2520extracted%2520frame-level%2520features%2520across%2520the%2520temporal%2520axis%252C%2520discarding%2520the%2520order%2520of%2520the%2520sub-units%2520within%2520a%2520vocalization.%2520This%2520paper%2520investigates%2520whether%2520discrete%2520acoustic%2520token%2520sequences%252C%2520derived%2520through%2520vector%2520quantization%2520and%2520gumbel-softmax%2520vector%2520quantization%2520of%2520extracted%2520self-supervised%2520speech%2520model%2520representations%2520can%2520effectively%2520capture%2520and%2520leverage%2520temporal%2520information.%2520To%2520that%2520end%252C%2520pairwise%2520distance%2520analysis%2520of%2520token%2520sequences%2520generated%2520from%2520HuBERT%2520embeddings%2520shows%2520that%2520they%2520can%2520discriminate%2520call-types%2520and%2520callers%2520across%2520four%2520bioacoustics%2520datasets.%2520Sequence%2520classification%2520experiments%2520using%2520%2524k%2524-Nearest%2520Neighbour%2520with%2520Levenshtein%2520distance%2520show%2520that%2520the%2520vector-quantized%2520token%2520sequences%2520yield%2520reasonable%2520call-type%2520and%2520caller%2520classification%2520performances%252C%2520and%2520hold%2520promise%2520as%2520alternative%2520feature%2520representations%2520towards%2520leveraging%2520sequential%2520information%2520in%2520animal%2520vocalizations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Leveraging%20Sequential%20Structure%20in%20Animal%20Vocalizations&entry.906535625=Eklavya%20Sarkar%20and%20Mathew%20Magimai.%20-Doss&entry.1292438233=Animal%20vocalizations%20contain%20sequential%20structures%20that%20carry%20important%20communicative%20information%2C%20yet%20most%20computational%20bioacoustics%20studies%20average%20the%20extracted%20frame-level%20features%20across%20the%20temporal%20axis%2C%20discarding%20the%20order%20of%20the%20sub-units%20within%20a%20vocalization.%20This%20paper%20investigates%20whether%20discrete%20acoustic%20token%20sequences%2C%20derived%20through%20vector%20quantization%20and%20gumbel-softmax%20vector%20quantization%20of%20extracted%20self-supervised%20speech%20model%20representations%20can%20effectively%20capture%20and%20leverage%20temporal%20information.%20To%20that%20end%2C%20pairwise%20distance%20analysis%20of%20token%20sequences%20generated%20from%20HuBERT%20embeddings%20shows%20that%20they%20can%20discriminate%20call-types%20and%20callers%20across%20four%20bioacoustics%20datasets.%20Sequence%20classification%20experiments%20using%20%24k%24-Nearest%20Neighbour%20with%20Levenshtein%20distance%20show%20that%20the%20vector-quantized%20token%20sequences%20yield%20reasonable%20call-type%20and%20caller%20classification%20performances%2C%20and%20hold%20promise%20as%20alternative%20feature%20representations%20towards%20leveraging%20sequential%20information%20in%20animal%20vocalizations.&entry.1838667208=http%3A//arxiv.org/abs/2511.10190v1&entry.124074799=Read"},
{"title": "MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models", "author": "He Zhang and Wenqian Cui and Haoning Xu and Xiaohui Li and Lei Zhu and Shaohua Ma and Irwin King", "abstract": "Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future.", "link": "http://arxiv.org/abs/2511.10262v1", "date": "2025-11-13", "relevancy": 2.3638, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MTR-DuplexBench%3A%20Towards%20a%20Comprehensive%20Evaluation%20of%20Multi-Round%20Conversations%20for%20Full-Duplex%20Speech%20Language%20Models&body=Title%3A%20MTR-DuplexBench%3A%20Towards%20a%20Comprehensive%20Evaluation%20of%20Multi-Round%20Conversations%20for%20Full-Duplex%20Speech%20Language%20Models%0AAuthor%3A%20He%20Zhang%20and%20Wenqian%20Cui%20and%20Haoning%20Xu%20and%20Xiaohui%20Li%20and%20Lei%20Zhu%20and%20Shaohua%20Ma%20and%20Irwin%20King%0AAbstract%3A%20Full-Duplex%20Speech%20Language%20Models%20%28FD-SLMs%29%20enable%20real-time%2C%20overlapping%20conversational%20interactions%2C%20offering%20a%20more%20dynamic%20user%20experience%20compared%20to%20traditional%20half-duplex%20models.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20evaluating%20single-round%20interactions%20and%20conversational%20features%2C%20neglecting%20the%20complexities%20of%20multi-round%20communication%20and%20critical%20capabilities%20such%20as%20instruction%20following%20and%20safety.%20Evaluating%20FD-SLMs%20in%20multi-round%20settings%20poses%20significant%20challenges%2C%20including%20blurred%20turn%20boundaries%20in%20communication%20and%20context%20inconsistency%20during%20model%20inference.%20To%20address%20these%20gaps%2C%20we%20introduce%20MTR-DuplexBench%2C%20a%20novel%20benchmark%20that%20segments%20continuous%20full-duplex%20dialogues%20into%20discrete%20turns%2C%20enabling%20comprehensive%2C%20turn-by-turn%20evaluation%20of%20FD-SLMs%20across%20dialogue%20quality%2C%20conversational%20dynamics%2C%20instruction%20following%2C%20and%20safety.%20Experimental%20results%20reveal%20that%20current%20FD-SLMs%20face%20difficulties%20in%20maintaining%20consistent%20performance%20across%20multiple%20rounds%20and%20evaluation%20dimensions%2C%20highlighting%20the%20necessity%20and%20effectiveness%20of%20our%20proposed%20benchmark.%20The%20benchmark%20and%20code%20will%20be%20available%20in%20the%20future.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMTR-DuplexBench%253A%2520Towards%2520a%2520Comprehensive%2520Evaluation%2520of%2520Multi-Round%2520Conversations%2520for%2520Full-Duplex%2520Speech%2520Language%2520Models%26entry.906535625%3DHe%2520Zhang%2520and%2520Wenqian%2520Cui%2520and%2520Haoning%2520Xu%2520and%2520Xiaohui%2520Li%2520and%2520Lei%2520Zhu%2520and%2520Shaohua%2520Ma%2520and%2520Irwin%2520King%26entry.1292438233%3DFull-Duplex%2520Speech%2520Language%2520Models%2520%2528FD-SLMs%2529%2520enable%2520real-time%252C%2520overlapping%2520conversational%2520interactions%252C%2520offering%2520a%2520more%2520dynamic%2520user%2520experience%2520compared%2520to%2520traditional%2520half-duplex%2520models.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520evaluating%2520single-round%2520interactions%2520and%2520conversational%2520features%252C%2520neglecting%2520the%2520complexities%2520of%2520multi-round%2520communication%2520and%2520critical%2520capabilities%2520such%2520as%2520instruction%2520following%2520and%2520safety.%2520Evaluating%2520FD-SLMs%2520in%2520multi-round%2520settings%2520poses%2520significant%2520challenges%252C%2520including%2520blurred%2520turn%2520boundaries%2520in%2520communication%2520and%2520context%2520inconsistency%2520during%2520model%2520inference.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520MTR-DuplexBench%252C%2520a%2520novel%2520benchmark%2520that%2520segments%2520continuous%2520full-duplex%2520dialogues%2520into%2520discrete%2520turns%252C%2520enabling%2520comprehensive%252C%2520turn-by-turn%2520evaluation%2520of%2520FD-SLMs%2520across%2520dialogue%2520quality%252C%2520conversational%2520dynamics%252C%2520instruction%2520following%252C%2520and%2520safety.%2520Experimental%2520results%2520reveal%2520that%2520current%2520FD-SLMs%2520face%2520difficulties%2520in%2520maintaining%2520consistent%2520performance%2520across%2520multiple%2520rounds%2520and%2520evaluation%2520dimensions%252C%2520highlighting%2520the%2520necessity%2520and%2520effectiveness%2520of%2520our%2520proposed%2520benchmark.%2520The%2520benchmark%2520and%2520code%2520will%2520be%2520available%2520in%2520the%2520future.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTR-DuplexBench%3A%20Towards%20a%20Comprehensive%20Evaluation%20of%20Multi-Round%20Conversations%20for%20Full-Duplex%20Speech%20Language%20Models&entry.906535625=He%20Zhang%20and%20Wenqian%20Cui%20and%20Haoning%20Xu%20and%20Xiaohui%20Li%20and%20Lei%20Zhu%20and%20Shaohua%20Ma%20and%20Irwin%20King&entry.1292438233=Full-Duplex%20Speech%20Language%20Models%20%28FD-SLMs%29%20enable%20real-time%2C%20overlapping%20conversational%20interactions%2C%20offering%20a%20more%20dynamic%20user%20experience%20compared%20to%20traditional%20half-duplex%20models.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20evaluating%20single-round%20interactions%20and%20conversational%20features%2C%20neglecting%20the%20complexities%20of%20multi-round%20communication%20and%20critical%20capabilities%20such%20as%20instruction%20following%20and%20safety.%20Evaluating%20FD-SLMs%20in%20multi-round%20settings%20poses%20significant%20challenges%2C%20including%20blurred%20turn%20boundaries%20in%20communication%20and%20context%20inconsistency%20during%20model%20inference.%20To%20address%20these%20gaps%2C%20we%20introduce%20MTR-DuplexBench%2C%20a%20novel%20benchmark%20that%20segments%20continuous%20full-duplex%20dialogues%20into%20discrete%20turns%2C%20enabling%20comprehensive%2C%20turn-by-turn%20evaluation%20of%20FD-SLMs%20across%20dialogue%20quality%2C%20conversational%20dynamics%2C%20instruction%20following%2C%20and%20safety.%20Experimental%20results%20reveal%20that%20current%20FD-SLMs%20face%20difficulties%20in%20maintaining%20consistent%20performance%20across%20multiple%20rounds%20and%20evaluation%20dimensions%2C%20highlighting%20the%20necessity%20and%20effectiveness%20of%20our%20proposed%20benchmark.%20The%20benchmark%20and%20code%20will%20be%20available%20in%20the%20future.&entry.1838667208=http%3A//arxiv.org/abs/2511.10262v1&entry.124074799=Read"},
{"title": "EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization", "author": "Yuancheng Sun and Yuxuan Ren and Zhaoming Chen and Xu Han and Kang Liu and Qiwei Ye", "abstract": "Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.", "link": "http://arxiv.org/abs/2511.10165v1", "date": "2025-11-13", "relevancy": 2.3452, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4803}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4642}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPO%3A%20Diverse%20and%20Realistic%20Protein%20Ensemble%20Generation%20via%20Energy%20Preference%20Optimization&body=Title%3A%20EPO%3A%20Diverse%20and%20Realistic%20Protein%20Ensemble%20Generation%20via%20Energy%20Preference%20Optimization%0AAuthor%3A%20Yuancheng%20Sun%20and%20Yuxuan%20Ren%20and%20Zhaoming%20Chen%20and%20Xu%20Han%20and%20Kang%20Liu%20and%20Qiwei%20Ye%0AAbstract%3A%20Accurate%20exploration%20of%20protein%20conformational%20ensembles%20is%20essential%20for%20uncovering%20function%20but%20remains%20hard%20because%20molecular-dynamics%20%28MD%29%20simulations%20suffer%20from%20high%20computational%20costs%20and%20energy-barrier%20trapping.%20This%20paper%20presents%20Energy%20Preference%20Optimization%20%28EPO%29%2C%20an%20online%20refinement%20algorithm%20that%20turns%20a%20pretrained%20protein%20ensemble%20generator%20into%20an%20energy-aware%20sampler%20without%20extra%20MD%20trajectories.%20Specifically%2C%20EPO%20leverages%20stochastic%20differential%20equation%20sampling%20to%20explore%20the%20conformational%20landscape%20and%20incorporates%20a%20novel%20energy-ranking%20mechanism%20based%20on%20list-wise%20preference%20optimization.%20Crucially%2C%20EPO%20introduces%20a%20practical%20upper%20bound%20to%20efficiently%20approximate%20the%20intractable%20probability%20of%20long%20sampling%20trajectories%20in%20continuous-time%20generative%20models%2C%20making%20it%20easily%20adaptable%20to%20existing%20pretrained%20generators.%20On%20Tetrapeptides%2C%20ATLAS%2C%20and%20Fast-Folding%20benchmarks%2C%20EPO%20successfully%20generates%20diverse%20and%20physically%20realistic%20ensembles%2C%20establishing%20a%20new%20state-of-the-art%20in%20nine%20evaluation%20metrics.%20These%20results%20demonstrate%20that%20energy-only%20preference%20signals%20can%20efficiently%20steer%20generative%20models%20toward%20thermodynamically%20consistent%20conformational%20ensembles%2C%20providing%20an%20alternative%20to%20long%20MD%20simulations%20and%20widening%20the%20applicability%20of%20learned%20potentials%20in%20structural%20biology%20and%20drug%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPO%253A%2520Diverse%2520and%2520Realistic%2520Protein%2520Ensemble%2520Generation%2520via%2520Energy%2520Preference%2520Optimization%26entry.906535625%3DYuancheng%2520Sun%2520and%2520Yuxuan%2520Ren%2520and%2520Zhaoming%2520Chen%2520and%2520Xu%2520Han%2520and%2520Kang%2520Liu%2520and%2520Qiwei%2520Ye%26entry.1292438233%3DAccurate%2520exploration%2520of%2520protein%2520conformational%2520ensembles%2520is%2520essential%2520for%2520uncovering%2520function%2520but%2520remains%2520hard%2520because%2520molecular-dynamics%2520%2528MD%2529%2520simulations%2520suffer%2520from%2520high%2520computational%2520costs%2520and%2520energy-barrier%2520trapping.%2520This%2520paper%2520presents%2520Energy%2520Preference%2520Optimization%2520%2528EPO%2529%252C%2520an%2520online%2520refinement%2520algorithm%2520that%2520turns%2520a%2520pretrained%2520protein%2520ensemble%2520generator%2520into%2520an%2520energy-aware%2520sampler%2520without%2520extra%2520MD%2520trajectories.%2520Specifically%252C%2520EPO%2520leverages%2520stochastic%2520differential%2520equation%2520sampling%2520to%2520explore%2520the%2520conformational%2520landscape%2520and%2520incorporates%2520a%2520novel%2520energy-ranking%2520mechanism%2520based%2520on%2520list-wise%2520preference%2520optimization.%2520Crucially%252C%2520EPO%2520introduces%2520a%2520practical%2520upper%2520bound%2520to%2520efficiently%2520approximate%2520the%2520intractable%2520probability%2520of%2520long%2520sampling%2520trajectories%2520in%2520continuous-time%2520generative%2520models%252C%2520making%2520it%2520easily%2520adaptable%2520to%2520existing%2520pretrained%2520generators.%2520On%2520Tetrapeptides%252C%2520ATLAS%252C%2520and%2520Fast-Folding%2520benchmarks%252C%2520EPO%2520successfully%2520generates%2520diverse%2520and%2520physically%2520realistic%2520ensembles%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520nine%2520evaluation%2520metrics.%2520These%2520results%2520demonstrate%2520that%2520energy-only%2520preference%2520signals%2520can%2520efficiently%2520steer%2520generative%2520models%2520toward%2520thermodynamically%2520consistent%2520conformational%2520ensembles%252C%2520providing%2520an%2520alternative%2520to%2520long%2520MD%2520simulations%2520and%2520widening%2520the%2520applicability%2520of%2520learned%2520potentials%2520in%2520structural%2520biology%2520and%2520drug%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPO%3A%20Diverse%20and%20Realistic%20Protein%20Ensemble%20Generation%20via%20Energy%20Preference%20Optimization&entry.906535625=Yuancheng%20Sun%20and%20Yuxuan%20Ren%20and%20Zhaoming%20Chen%20and%20Xu%20Han%20and%20Kang%20Liu%20and%20Qiwei%20Ye&entry.1292438233=Accurate%20exploration%20of%20protein%20conformational%20ensembles%20is%20essential%20for%20uncovering%20function%20but%20remains%20hard%20because%20molecular-dynamics%20%28MD%29%20simulations%20suffer%20from%20high%20computational%20costs%20and%20energy-barrier%20trapping.%20This%20paper%20presents%20Energy%20Preference%20Optimization%20%28EPO%29%2C%20an%20online%20refinement%20algorithm%20that%20turns%20a%20pretrained%20protein%20ensemble%20generator%20into%20an%20energy-aware%20sampler%20without%20extra%20MD%20trajectories.%20Specifically%2C%20EPO%20leverages%20stochastic%20differential%20equation%20sampling%20to%20explore%20the%20conformational%20landscape%20and%20incorporates%20a%20novel%20energy-ranking%20mechanism%20based%20on%20list-wise%20preference%20optimization.%20Crucially%2C%20EPO%20introduces%20a%20practical%20upper%20bound%20to%20efficiently%20approximate%20the%20intractable%20probability%20of%20long%20sampling%20trajectories%20in%20continuous-time%20generative%20models%2C%20making%20it%20easily%20adaptable%20to%20existing%20pretrained%20generators.%20On%20Tetrapeptides%2C%20ATLAS%2C%20and%20Fast-Folding%20benchmarks%2C%20EPO%20successfully%20generates%20diverse%20and%20physically%20realistic%20ensembles%2C%20establishing%20a%20new%20state-of-the-art%20in%20nine%20evaluation%20metrics.%20These%20results%20demonstrate%20that%20energy-only%20preference%20signals%20can%20efficiently%20steer%20generative%20models%20toward%20thermodynamically%20consistent%20conformational%20ensembles%2C%20providing%20an%20alternative%20to%20long%20MD%20simulations%20and%20widening%20the%20applicability%20of%20learned%20potentials%20in%20structural%20biology%20and%20drug%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2511.10165v1&entry.124074799=Read"},
{"title": "VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations", "author": "Qianqian Qiao and DanDan Zheng and Yihang Bo and Bao Peng and Heng Huang and Longteng Jiang and Huaye Wang and Jingdong Chen and Jun Zhou and Xin Jin", "abstract": "Video aesthetic assessment, a vital area in multimedia computing, integrates computer vision with human cognition. Its progress is limited by the lack of standardized datasets and robust models, as the temporal dynamics of video and multimodal fusion challenges hinder direct application of image-based methods. This study introduces VADB, the largest video aesthetic database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, including overall and attribute-specific aesthetic scores, rich language comments and objective tags. We propose VADB-Net, a dual-modal pre-training framework with a two-stage training strategy, which outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks. The dataset and source code are available at https://github.com/BestiVictory/VADB.", "link": "http://arxiv.org/abs/2510.25238v2", "date": "2025-11-13", "relevancy": 2.3269, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6104}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.563}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VADB%3A%20A%20Large-Scale%20Video%20Aesthetic%20Database%20with%20Professional%20and%20Multi-Dimensional%20Annotations&body=Title%3A%20VADB%3A%20A%20Large-Scale%20Video%20Aesthetic%20Database%20with%20Professional%20and%20Multi-Dimensional%20Annotations%0AAuthor%3A%20Qianqian%20Qiao%20and%20DanDan%20Zheng%20and%20Yihang%20Bo%20and%20Bao%20Peng%20and%20Heng%20Huang%20and%20Longteng%20Jiang%20and%20Huaye%20Wang%20and%20Jingdong%20Chen%20and%20Jun%20Zhou%20and%20Xin%20Jin%0AAbstract%3A%20Video%20aesthetic%20assessment%2C%20a%20vital%20area%20in%20multimedia%20computing%2C%20integrates%20computer%20vision%20with%20human%20cognition.%20Its%20progress%20is%20limited%20by%20the%20lack%20of%20standardized%20datasets%20and%20robust%20models%2C%20as%20the%20temporal%20dynamics%20of%20video%20and%20multimodal%20fusion%20challenges%20hinder%20direct%20application%20of%20image-based%20methods.%20This%20study%20introduces%20VADB%2C%20the%20largest%20video%20aesthetic%20database%20with%2010%2C490%20diverse%20videos%20annotated%20by%2037%20professionals%20across%20multiple%20aesthetic%20dimensions%2C%20including%20overall%20and%20attribute-specific%20aesthetic%20scores%2C%20rich%20language%20comments%20and%20objective%20tags.%20We%20propose%20VADB-Net%2C%20a%20dual-modal%20pre-training%20framework%20with%20a%20two-stage%20training%20strategy%2C%20which%20outperforms%20existing%20video%20quality%20assessment%20models%20in%20scoring%20tasks%20and%20supports%20downstream%20video%20aesthetic%20assessment%20tasks.%20The%20dataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/BestiVictory/VADB.%0ALink%3A%20http%3A//arxiv.org/abs/2510.25238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVADB%253A%2520A%2520Large-Scale%2520Video%2520Aesthetic%2520Database%2520with%2520Professional%2520and%2520Multi-Dimensional%2520Annotations%26entry.906535625%3DQianqian%2520Qiao%2520and%2520DanDan%2520Zheng%2520and%2520Yihang%2520Bo%2520and%2520Bao%2520Peng%2520and%2520Heng%2520Huang%2520and%2520Longteng%2520Jiang%2520and%2520Huaye%2520Wang%2520and%2520Jingdong%2520Chen%2520and%2520Jun%2520Zhou%2520and%2520Xin%2520Jin%26entry.1292438233%3DVideo%2520aesthetic%2520assessment%252C%2520a%2520vital%2520area%2520in%2520multimedia%2520computing%252C%2520integrates%2520computer%2520vision%2520with%2520human%2520cognition.%2520Its%2520progress%2520is%2520limited%2520by%2520the%2520lack%2520of%2520standardized%2520datasets%2520and%2520robust%2520models%252C%2520as%2520the%2520temporal%2520dynamics%2520of%2520video%2520and%2520multimodal%2520fusion%2520challenges%2520hinder%2520direct%2520application%2520of%2520image-based%2520methods.%2520This%2520study%2520introduces%2520VADB%252C%2520the%2520largest%2520video%2520aesthetic%2520database%2520with%252010%252C490%2520diverse%2520videos%2520annotated%2520by%252037%2520professionals%2520across%2520multiple%2520aesthetic%2520dimensions%252C%2520including%2520overall%2520and%2520attribute-specific%2520aesthetic%2520scores%252C%2520rich%2520language%2520comments%2520and%2520objective%2520tags.%2520We%2520propose%2520VADB-Net%252C%2520a%2520dual-modal%2520pre-training%2520framework%2520with%2520a%2520two-stage%2520training%2520strategy%252C%2520which%2520outperforms%2520existing%2520video%2520quality%2520assessment%2520models%2520in%2520scoring%2520tasks%2520and%2520supports%2520downstream%2520video%2520aesthetic%2520assessment%2520tasks.%2520The%2520dataset%2520and%2520source%2520code%2520are%2520available%2520at%2520https%253A//github.com/BestiVictory/VADB.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VADB%3A%20A%20Large-Scale%20Video%20Aesthetic%20Database%20with%20Professional%20and%20Multi-Dimensional%20Annotations&entry.906535625=Qianqian%20Qiao%20and%20DanDan%20Zheng%20and%20Yihang%20Bo%20and%20Bao%20Peng%20and%20Heng%20Huang%20and%20Longteng%20Jiang%20and%20Huaye%20Wang%20and%20Jingdong%20Chen%20and%20Jun%20Zhou%20and%20Xin%20Jin&entry.1292438233=Video%20aesthetic%20assessment%2C%20a%20vital%20area%20in%20multimedia%20computing%2C%20integrates%20computer%20vision%20with%20human%20cognition.%20Its%20progress%20is%20limited%20by%20the%20lack%20of%20standardized%20datasets%20and%20robust%20models%2C%20as%20the%20temporal%20dynamics%20of%20video%20and%20multimodal%20fusion%20challenges%20hinder%20direct%20application%20of%20image-based%20methods.%20This%20study%20introduces%20VADB%2C%20the%20largest%20video%20aesthetic%20database%20with%2010%2C490%20diverse%20videos%20annotated%20by%2037%20professionals%20across%20multiple%20aesthetic%20dimensions%2C%20including%20overall%20and%20attribute-specific%20aesthetic%20scores%2C%20rich%20language%20comments%20and%20objective%20tags.%20We%20propose%20VADB-Net%2C%20a%20dual-modal%20pre-training%20framework%20with%20a%20two-stage%20training%20strategy%2C%20which%20outperforms%20existing%20video%20quality%20assessment%20models%20in%20scoring%20tasks%20and%20supports%20downstream%20video%20aesthetic%20assessment%20tasks.%20The%20dataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/BestiVictory/VADB.&entry.1838667208=http%3A//arxiv.org/abs/2510.25238v2&entry.124074799=Read"},
{"title": "FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment", "author": "Yongji Zhang and Siqi Li and Yue Gao and Yu Jiang", "abstract": "Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2511.10250v1", "date": "2025-11-13", "relevancy": 2.3249, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4897}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineSkiing%3A%20A%20Fine-grained%20Benchmark%20for%20Skiing%20Action%20Quality%20Assessment&body=Title%3A%20FineSkiing%3A%20A%20Fine-grained%20Benchmark%20for%20Skiing%20Action%20Quality%20Assessment%0AAuthor%3A%20Yongji%20Zhang%20and%20Siqi%20Li%20and%20Yue%20Gao%20and%20Yu%20Jiang%0AAbstract%3A%20Action%20Quality%20Assessment%20%28AQA%29%20aims%20to%20evaluate%20and%20score%20sports%20actions%2C%20which%20has%20attracted%20widespread%20interest%20in%20recent%20years.%20Existing%20AQA%20methods%20primarily%20predict%20scores%20based%20on%20features%20extracted%20from%20the%20entire%20video%2C%20resulting%20in%20limited%20interpretability%20and%20reliability.%20Meanwhile%2C%20existing%20AQA%20datasets%20also%20lack%20fine-grained%20annotations%20for%20action%20scores%2C%20especially%20for%20deduction%20items%20and%20sub-score%20annotations.%20In%20this%20paper%2C%20we%20construct%20the%20first%20AQA%20dataset%20containing%20fine-grained%20sub-score%20and%20deduction%20annotations%20for%20aerial%20skiing%2C%20which%20will%20be%20released%20as%20a%20new%20benchmark.%20For%20the%20technical%20challenges%2C%20we%20propose%20a%20novel%20AQA%20method%2C%20named%20JudgeMind%2C%20which%20significantly%20enhances%20performance%20and%20reliability%20by%20simulating%20the%20judgment%20and%20scoring%20mindset%20of%20professional%20referees.%20Our%20method%20segments%20the%20input%20action%20video%20into%20different%20stages%20and%20scores%20each%20stage%20to%20enhance%20accuracy.%20Then%2C%20we%20propose%20a%20stage-aware%20feature%20enhancement%20and%20fusion%20module%20to%20boost%20the%20perception%20of%20stage-specific%20key%20regions%20and%20enhance%20the%20robustness%20to%20visual%20changes%20caused%20by%20frequent%20camera%20viewpoints%20switching.%20In%20addition%2C%20we%20propose%20a%20knowledge-based%20grade-aware%20decoder%20to%20incorporate%20possible%20deduction%20items%20as%20prior%20knowledge%20to%20predict%20more%20accurate%20and%20reliable%20scores.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineSkiing%253A%2520A%2520Fine-grained%2520Benchmark%2520for%2520Skiing%2520Action%2520Quality%2520Assessment%26entry.906535625%3DYongji%2520Zhang%2520and%2520Siqi%2520Li%2520and%2520Yue%2520Gao%2520and%2520Yu%2520Jiang%26entry.1292438233%3DAction%2520Quality%2520Assessment%2520%2528AQA%2529%2520aims%2520to%2520evaluate%2520and%2520score%2520sports%2520actions%252C%2520which%2520has%2520attracted%2520widespread%2520interest%2520in%2520recent%2520years.%2520Existing%2520AQA%2520methods%2520primarily%2520predict%2520scores%2520based%2520on%2520features%2520extracted%2520from%2520the%2520entire%2520video%252C%2520resulting%2520in%2520limited%2520interpretability%2520and%2520reliability.%2520Meanwhile%252C%2520existing%2520AQA%2520datasets%2520also%2520lack%2520fine-grained%2520annotations%2520for%2520action%2520scores%252C%2520especially%2520for%2520deduction%2520items%2520and%2520sub-score%2520annotations.%2520In%2520this%2520paper%252C%2520we%2520construct%2520the%2520first%2520AQA%2520dataset%2520containing%2520fine-grained%2520sub-score%2520and%2520deduction%2520annotations%2520for%2520aerial%2520skiing%252C%2520which%2520will%2520be%2520released%2520as%2520a%2520new%2520benchmark.%2520For%2520the%2520technical%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520AQA%2520method%252C%2520named%2520JudgeMind%252C%2520which%2520significantly%2520enhances%2520performance%2520and%2520reliability%2520by%2520simulating%2520the%2520judgment%2520and%2520scoring%2520mindset%2520of%2520professional%2520referees.%2520Our%2520method%2520segments%2520the%2520input%2520action%2520video%2520into%2520different%2520stages%2520and%2520scores%2520each%2520stage%2520to%2520enhance%2520accuracy.%2520Then%252C%2520we%2520propose%2520a%2520stage-aware%2520feature%2520enhancement%2520and%2520fusion%2520module%2520to%2520boost%2520the%2520perception%2520of%2520stage-specific%2520key%2520regions%2520and%2520enhance%2520the%2520robustness%2520to%2520visual%2520changes%2520caused%2520by%2520frequent%2520camera%2520viewpoints%2520switching.%2520In%2520addition%252C%2520we%2520propose%2520a%2520knowledge-based%2520grade-aware%2520decoder%2520to%2520incorporate%2520possible%2520deduction%2520items%2520as%2520prior%2520knowledge%2520to%2520predict%2520more%2520accurate%2520and%2520reliable%2520scores.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineSkiing%3A%20A%20Fine-grained%20Benchmark%20for%20Skiing%20Action%20Quality%20Assessment&entry.906535625=Yongji%20Zhang%20and%20Siqi%20Li%20and%20Yue%20Gao%20and%20Yu%20Jiang&entry.1292438233=Action%20Quality%20Assessment%20%28AQA%29%20aims%20to%20evaluate%20and%20score%20sports%20actions%2C%20which%20has%20attracted%20widespread%20interest%20in%20recent%20years.%20Existing%20AQA%20methods%20primarily%20predict%20scores%20based%20on%20features%20extracted%20from%20the%20entire%20video%2C%20resulting%20in%20limited%20interpretability%20and%20reliability.%20Meanwhile%2C%20existing%20AQA%20datasets%20also%20lack%20fine-grained%20annotations%20for%20action%20scores%2C%20especially%20for%20deduction%20items%20and%20sub-score%20annotations.%20In%20this%20paper%2C%20we%20construct%20the%20first%20AQA%20dataset%20containing%20fine-grained%20sub-score%20and%20deduction%20annotations%20for%20aerial%20skiing%2C%20which%20will%20be%20released%20as%20a%20new%20benchmark.%20For%20the%20technical%20challenges%2C%20we%20propose%20a%20novel%20AQA%20method%2C%20named%20JudgeMind%2C%20which%20significantly%20enhances%20performance%20and%20reliability%20by%20simulating%20the%20judgment%20and%20scoring%20mindset%20of%20professional%20referees.%20Our%20method%20segments%20the%20input%20action%20video%20into%20different%20stages%20and%20scores%20each%20stage%20to%20enhance%20accuracy.%20Then%2C%20we%20propose%20a%20stage-aware%20feature%20enhancement%20and%20fusion%20module%20to%20boost%20the%20perception%20of%20stage-specific%20key%20regions%20and%20enhance%20the%20robustness%20to%20visual%20changes%20caused%20by%20frequent%20camera%20viewpoints%20switching.%20In%20addition%2C%20we%20propose%20a%20knowledge-based%20grade-aware%20decoder%20to%20incorporate%20possible%20deduction%20items%20as%20prior%20knowledge%20to%20predict%20more%20accurate%20and%20reliable%20scores.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.10250v1&entry.124074799=Read"},
{"title": "LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures", "author": "Wenzhe He and Xiaojun Chen and Ruiqi Wang and Ruihui Li and Huilong Pi and Jiapeng Zhang and Zhuo Tang and Kenli Li", "abstract": "3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.", "link": "http://arxiv.org/abs/2511.10209v1", "date": "2025-11-13", "relevancy": 2.318, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5827}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5779}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiNeXt%3A%20Revisiting%20LiDAR%20Completion%20with%20Efficient%20Non-Diffusion%20Architectures&body=Title%3A%20LiNeXt%3A%20Revisiting%20LiDAR%20Completion%20with%20Efficient%20Non-Diffusion%20Architectures%0AAuthor%3A%20Wenzhe%20He%20and%20Xiaojun%20Chen%20and%20Ruiqi%20Wang%20and%20Ruihui%20Li%20and%20Huilong%20Pi%20and%20Jiapeng%20Zhang%20and%20Zhuo%20Tang%20and%20Kenli%20Li%0AAbstract%3A%203D%20LiDAR%20scene%20completion%20from%20point%20clouds%20is%20a%20fundamental%20component%20of%20perception%20systems%20in%20autonomous%20vehicles.%20Previous%20methods%20have%20predominantly%20employed%20diffusion%20models%20for%20high-fidelity%20reconstruction.%20However%2C%20their%20multi-step%20iterative%20sampling%20incurs%20significant%20computational%20overhead%2C%20limiting%20its%20real-time%20applicability.%20To%20address%20this%2C%20we%20propose%20LiNeXt-a%20lightweight%2C%20non-diffusion%20network%20optimized%20for%20rapid%20and%20accurate%20point%20cloud%20completion.%20Specifically%2C%20LiNeXt%20first%20applies%20the%20Noise-to-Coarse%20%28N2C%29%20Module%20to%20denoise%20the%20input%20noisy%20point%20cloud%20in%20a%20single%20pass%2C%20thereby%20obviating%20the%20multi-step%20iterative%20sampling%20of%20diffusion-based%20methods.%20The%20Refine%20Module%20then%20takes%20the%20coarse%20point%20cloud%20and%20its%20intermediate%20features%20from%20the%20N2C%20Module%20to%20perform%20more%20precise%20refinement%2C%20further%20enhancing%20structural%20completeness.%20Furthermore%2C%20we%20observe%20that%20LiDAR%20point%20clouds%20exhibit%20a%20distance-dependent%20spatial%20distribution%2C%20being%20densely%20sampled%20at%20proximal%20ranges%20and%20sparsely%20sampled%20at%20distal%20ranges.%20Accordingly%2C%20we%20propose%20the%20Distance-aware%20Selected%20Repeat%20strategy%20to%20generate%20a%20more%20uniformly%20distributed%20noisy%20point%20cloud.%20On%20the%20SemanticKITTI%20dataset%2C%20LiNeXt%20achieves%20a%20199.8x%20speedup%20in%20inference%2C%20reduces%20Chamfer%20Distance%20by%2050.7%25%2C%20and%20uses%20only%206.1%25%20of%20the%20parameters%20compared%20with%20LiDiff.%20These%20results%20demonstrate%20the%20superior%20efficiency%20and%20effectiveness%20of%20LiNeXt%20for%20real-time%20scene%20completion.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiNeXt%253A%2520Revisiting%2520LiDAR%2520Completion%2520with%2520Efficient%2520Non-Diffusion%2520Architectures%26entry.906535625%3DWenzhe%2520He%2520and%2520Xiaojun%2520Chen%2520and%2520Ruiqi%2520Wang%2520and%2520Ruihui%2520Li%2520and%2520Huilong%2520Pi%2520and%2520Jiapeng%2520Zhang%2520and%2520Zhuo%2520Tang%2520and%2520Kenli%2520Li%26entry.1292438233%3D3D%2520LiDAR%2520scene%2520completion%2520from%2520point%2520clouds%2520is%2520a%2520fundamental%2520component%2520of%2520perception%2520systems%2520in%2520autonomous%2520vehicles.%2520Previous%2520methods%2520have%2520predominantly%2520employed%2520diffusion%2520models%2520for%2520high-fidelity%2520reconstruction.%2520However%252C%2520their%2520multi-step%2520iterative%2520sampling%2520incurs%2520significant%2520computational%2520overhead%252C%2520limiting%2520its%2520real-time%2520applicability.%2520To%2520address%2520this%252C%2520we%2520propose%2520LiNeXt-a%2520lightweight%252C%2520non-diffusion%2520network%2520optimized%2520for%2520rapid%2520and%2520accurate%2520point%2520cloud%2520completion.%2520Specifically%252C%2520LiNeXt%2520first%2520applies%2520the%2520Noise-to-Coarse%2520%2528N2C%2529%2520Module%2520to%2520denoise%2520the%2520input%2520noisy%2520point%2520cloud%2520in%2520a%2520single%2520pass%252C%2520thereby%2520obviating%2520the%2520multi-step%2520iterative%2520sampling%2520of%2520diffusion-based%2520methods.%2520The%2520Refine%2520Module%2520then%2520takes%2520the%2520coarse%2520point%2520cloud%2520and%2520its%2520intermediate%2520features%2520from%2520the%2520N2C%2520Module%2520to%2520perform%2520more%2520precise%2520refinement%252C%2520further%2520enhancing%2520structural%2520completeness.%2520Furthermore%252C%2520we%2520observe%2520that%2520LiDAR%2520point%2520clouds%2520exhibit%2520a%2520distance-dependent%2520spatial%2520distribution%252C%2520being%2520densely%2520sampled%2520at%2520proximal%2520ranges%2520and%2520sparsely%2520sampled%2520at%2520distal%2520ranges.%2520Accordingly%252C%2520we%2520propose%2520the%2520Distance-aware%2520Selected%2520Repeat%2520strategy%2520to%2520generate%2520a%2520more%2520uniformly%2520distributed%2520noisy%2520point%2520cloud.%2520On%2520the%2520SemanticKITTI%2520dataset%252C%2520LiNeXt%2520achieves%2520a%2520199.8x%2520speedup%2520in%2520inference%252C%2520reduces%2520Chamfer%2520Distance%2520by%252050.7%2525%252C%2520and%2520uses%2520only%25206.1%2525%2520of%2520the%2520parameters%2520compared%2520with%2520LiDiff.%2520These%2520results%2520demonstrate%2520the%2520superior%2520efficiency%2520and%2520effectiveness%2520of%2520LiNeXt%2520for%2520real-time%2520scene%2520completion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiNeXt%3A%20Revisiting%20LiDAR%20Completion%20with%20Efficient%20Non-Diffusion%20Architectures&entry.906535625=Wenzhe%20He%20and%20Xiaojun%20Chen%20and%20Ruiqi%20Wang%20and%20Ruihui%20Li%20and%20Huilong%20Pi%20and%20Jiapeng%20Zhang%20and%20Zhuo%20Tang%20and%20Kenli%20Li&entry.1292438233=3D%20LiDAR%20scene%20completion%20from%20point%20clouds%20is%20a%20fundamental%20component%20of%20perception%20systems%20in%20autonomous%20vehicles.%20Previous%20methods%20have%20predominantly%20employed%20diffusion%20models%20for%20high-fidelity%20reconstruction.%20However%2C%20their%20multi-step%20iterative%20sampling%20incurs%20significant%20computational%20overhead%2C%20limiting%20its%20real-time%20applicability.%20To%20address%20this%2C%20we%20propose%20LiNeXt-a%20lightweight%2C%20non-diffusion%20network%20optimized%20for%20rapid%20and%20accurate%20point%20cloud%20completion.%20Specifically%2C%20LiNeXt%20first%20applies%20the%20Noise-to-Coarse%20%28N2C%29%20Module%20to%20denoise%20the%20input%20noisy%20point%20cloud%20in%20a%20single%20pass%2C%20thereby%20obviating%20the%20multi-step%20iterative%20sampling%20of%20diffusion-based%20methods.%20The%20Refine%20Module%20then%20takes%20the%20coarse%20point%20cloud%20and%20its%20intermediate%20features%20from%20the%20N2C%20Module%20to%20perform%20more%20precise%20refinement%2C%20further%20enhancing%20structural%20completeness.%20Furthermore%2C%20we%20observe%20that%20LiDAR%20point%20clouds%20exhibit%20a%20distance-dependent%20spatial%20distribution%2C%20being%20densely%20sampled%20at%20proximal%20ranges%20and%20sparsely%20sampled%20at%20distal%20ranges.%20Accordingly%2C%20we%20propose%20the%20Distance-aware%20Selected%20Repeat%20strategy%20to%20generate%20a%20more%20uniformly%20distributed%20noisy%20point%20cloud.%20On%20the%20SemanticKITTI%20dataset%2C%20LiNeXt%20achieves%20a%20199.8x%20speedup%20in%20inference%2C%20reduces%20Chamfer%20Distance%20by%2050.7%25%2C%20and%20uses%20only%206.1%25%20of%20the%20parameters%20compared%20with%20LiDiff.%20These%20results%20demonstrate%20the%20superior%20efficiency%20and%20effectiveness%20of%20LiNeXt%20for%20real-time%20scene%20completion.&entry.1838667208=http%3A//arxiv.org/abs/2511.10209v1&entry.124074799=Read"},
{"title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs", "author": "Tianhao Peng and Haochen Wang and Yuanxing Zhang and Zekun Wang and Zili Wang and Gavin Chang and Jian Yang and Shihao Li and Yanghai Wang and Xintao Wang and Houyi Li and Wei Ji and Pengfei Wan and Steven Huang and Zhaoxiang Zhang and Jiaheng Liu", "abstract": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.", "link": "http://arxiv.org/abs/2511.07250v2", "date": "2025-11-13", "relevancy": 2.3077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVU-Eval%3A%20Towards%20Multi-Video%20Understanding%20Evaluation%20for%20Multimodal%20LLMs&body=Title%3A%20MVU-Eval%3A%20Towards%20Multi-Video%20Understanding%20Evaluation%20for%20Multimodal%20LLMs%0AAuthor%3A%20Tianhao%20Peng%20and%20Haochen%20Wang%20and%20Yuanxing%20Zhang%20and%20Zekun%20Wang%20and%20Zili%20Wang%20and%20Gavin%20Chang%20and%20Jian%20Yang%20and%20Shihao%20Li%20and%20Yanghai%20Wang%20and%20Xintao%20Wang%20and%20Houyi%20Li%20and%20Wei%20Ji%20and%20Pengfei%20Wan%20and%20Steven%20Huang%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu%0AAbstract%3A%20The%20advent%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20expanded%20AI%20capabilities%20to%20visual%20modalities%2C%20yet%20existing%20evaluation%20benchmarks%20remain%20limited%20to%20single-video%20understanding%2C%20overlooking%20the%20critical%20need%20for%20multi-video%20understanding%20in%20real-world%20scenarios%20%28e.g.%2C%20sports%20analytics%20and%20autonomous%20driving%29.%20To%20address%20this%20significant%20gap%2C%20we%20introduce%20MVU-Eval%2C%20the%20first%20comprehensive%20benchmark%20for%20evaluating%20Multi-Video%20Understanding%20for%20MLLMs.%20Specifically%2C%20our%20MVU-Eval%20mainly%20assesses%20eight%20core%20competencies%20through%201%2C824%20meticulously%20curated%20question-answer%20pairs%20spanning%204%2C959%20videos%20from%20diverse%20domains%2C%20addressing%20both%20fundamental%20perception%20tasks%20and%20high-order%20reasoning%20tasks.%20These%20capabilities%20are%20rigorously%20aligned%20with%20real-world%20applications%20such%20as%20multi-sensor%20synthesis%20in%20autonomous%20systems%20and%20cross-angle%20sports%20analytics.%20Through%20extensive%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20models%2C%20we%20reveal%20significant%20performance%20discrepancies%20and%20limitations%20in%20current%20MLLMs%27%20ability%20to%20perform%20understanding%20across%20multiple%20videos.%20The%20benchmark%20will%20be%20made%20publicly%20available%20to%20foster%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVU-Eval%253A%2520Towards%2520Multi-Video%2520Understanding%2520Evaluation%2520for%2520Multimodal%2520LLMs%26entry.906535625%3DTianhao%2520Peng%2520and%2520Haochen%2520Wang%2520and%2520Yuanxing%2520Zhang%2520and%2520Zekun%2520Wang%2520and%2520Zili%2520Wang%2520and%2520Gavin%2520Chang%2520and%2520Jian%2520Yang%2520and%2520Shihao%2520Li%2520and%2520Yanghai%2520Wang%2520and%2520Xintao%2520Wang%2520and%2520Houyi%2520Li%2520and%2520Wei%2520Ji%2520and%2520Pengfei%2520Wan%2520and%2520Steven%2520Huang%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3DThe%2520advent%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520expanded%2520AI%2520capabilities%2520to%2520visual%2520modalities%252C%2520yet%2520existing%2520evaluation%2520benchmarks%2520remain%2520limited%2520to%2520single-video%2520understanding%252C%2520overlooking%2520the%2520critical%2520need%2520for%2520multi-video%2520understanding%2520in%2520real-world%2520scenarios%2520%2528e.g.%252C%2520sports%2520analytics%2520and%2520autonomous%2520driving%2529.%2520To%2520address%2520this%2520significant%2520gap%252C%2520we%2520introduce%2520MVU-Eval%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%2520evaluating%2520Multi-Video%2520Understanding%2520for%2520MLLMs.%2520Specifically%252C%2520our%2520MVU-Eval%2520mainly%2520assesses%2520eight%2520core%2520competencies%2520through%25201%252C824%2520meticulously%2520curated%2520question-answer%2520pairs%2520spanning%25204%252C959%2520videos%2520from%2520diverse%2520domains%252C%2520addressing%2520both%2520fundamental%2520perception%2520tasks%2520and%2520high-order%2520reasoning%2520tasks.%2520These%2520capabilities%2520are%2520rigorously%2520aligned%2520with%2520real-world%2520applications%2520such%2520as%2520multi-sensor%2520synthesis%2520in%2520autonomous%2520systems%2520and%2520cross-angle%2520sports%2520analytics.%2520Through%2520extensive%2520evaluation%2520of%2520state-of-the-art%2520open-source%2520and%2520closed-source%2520models%252C%2520we%2520reveal%2520significant%2520performance%2520discrepancies%2520and%2520limitations%2520in%2520current%2520MLLMs%2527%2520ability%2520to%2520perform%2520understanding%2520across%2520multiple%2520videos.%2520The%2520benchmark%2520will%2520be%2520made%2520publicly%2520available%2520to%2520foster%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVU-Eval%3A%20Towards%20Multi-Video%20Understanding%20Evaluation%20for%20Multimodal%20LLMs&entry.906535625=Tianhao%20Peng%20and%20Haochen%20Wang%20and%20Yuanxing%20Zhang%20and%20Zekun%20Wang%20and%20Zili%20Wang%20and%20Gavin%20Chang%20and%20Jian%20Yang%20and%20Shihao%20Li%20and%20Yanghai%20Wang%20and%20Xintao%20Wang%20and%20Houyi%20Li%20and%20Wei%20Ji%20and%20Pengfei%20Wan%20and%20Steven%20Huang%20and%20Zhaoxiang%20Zhang%20and%20Jiaheng%20Liu&entry.1292438233=The%20advent%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20expanded%20AI%20capabilities%20to%20visual%20modalities%2C%20yet%20existing%20evaluation%20benchmarks%20remain%20limited%20to%20single-video%20understanding%2C%20overlooking%20the%20critical%20need%20for%20multi-video%20understanding%20in%20real-world%20scenarios%20%28e.g.%2C%20sports%20analytics%20and%20autonomous%20driving%29.%20To%20address%20this%20significant%20gap%2C%20we%20introduce%20MVU-Eval%2C%20the%20first%20comprehensive%20benchmark%20for%20evaluating%20Multi-Video%20Understanding%20for%20MLLMs.%20Specifically%2C%20our%20MVU-Eval%20mainly%20assesses%20eight%20core%20competencies%20through%201%2C824%20meticulously%20curated%20question-answer%20pairs%20spanning%204%2C959%20videos%20from%20diverse%20domains%2C%20addressing%20both%20fundamental%20perception%20tasks%20and%20high-order%20reasoning%20tasks.%20These%20capabilities%20are%20rigorously%20aligned%20with%20real-world%20applications%20such%20as%20multi-sensor%20synthesis%20in%20autonomous%20systems%20and%20cross-angle%20sports%20analytics.%20Through%20extensive%20evaluation%20of%20state-of-the-art%20open-source%20and%20closed-source%20models%2C%20we%20reveal%20significant%20performance%20discrepancies%20and%20limitations%20in%20current%20MLLMs%27%20ability%20to%20perform%20understanding%20across%20multiple%20videos.%20The%20benchmark%20will%20be%20made%20publicly%20available%20to%20foster%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.07250v2&entry.124074799=Read"},
{"title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals", "author": "Shruti Singh Baghel and Yash Pratap Singh Rathore and Sushovan Jena and Anurag Pradhan and Amit Shukla and Arnav Bhavsar and Pawan Goyal", "abstract": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.", "link": "http://arxiv.org/abs/2511.10615v1", "date": "2025-11-13", "relevancy": 2.2995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Blind%20and%20Low-Vision%20Accessibility%20of%20Lightweight%20VLMs%20and%20Custom%20LLM-Evals&body=Title%3A%20Towards%20Blind%20and%20Low-Vision%20Accessibility%20of%20Lightweight%20VLMs%20and%20Custom%20LLM-Evals%0AAuthor%3A%20Shruti%20Singh%20Baghel%20and%20Yash%20Pratap%20Singh%20Rathore%20and%20Sushovan%20Jena%20and%20Anurag%20Pradhan%20and%20Amit%20Shukla%20and%20Arnav%20Bhavsar%20and%20Pawan%20Goyal%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20understanding%20and%20generating%20video%20descriptions%20but%20their%20high%20memory%2C%20computation%2C%20and%20deployment%20demands%20hinder%20practical%20use%20particularly%20for%20blind%20and%20low-vision%20%28BLV%29%20users%20who%20depend%20on%20detailed%2C%20context-aware%20descriptions.%20To%20study%20the%20effect%20of%20model%20size%20on%20accessibility-focused%20description%20quality%2C%20we%20evaluate%20SmolVLM2%20variants%20with%20500M%20and%202.2B%20parameters%20across%20two%20diverse%20datasets%3A%20AVCaps%20%28outdoor%29%2C%20and%20Charades%20%28indoor%29.%20In%20this%20work%2C%20we%20introduce%20two%20novel%20evaluation%20frameworks%20specifically%20designed%20for%20BLV%20accessibility%20assessment%3A%20the%20Multi-Context%20BLV%20Framework%20evaluating%20spatial%20orientation%2C%20social%20interaction%2C%20action%20events%2C%20and%20ambience%20contexts%3B%20and%20the%20Navigational%20Assistance%20Framework%20focusing%20on%20mobility-critical%20information.%20Additionally%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20four%20different%20prompt%20design%20strategies%20and%20deploy%20both%20models%20on%20a%20smartphone%2C%20evaluating%20FP32%20and%20INT8%20precision%20variants%20to%20assess%20real-world%20performance%20constraints%20on%20resource-limited%20mobile%20devices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Blind%2520and%2520Low-Vision%2520Accessibility%2520of%2520Lightweight%2520VLMs%2520and%2520Custom%2520LLM-Evals%26entry.906535625%3DShruti%2520Singh%2520Baghel%2520and%2520Yash%2520Pratap%2520Singh%2520Rathore%2520and%2520Sushovan%2520Jena%2520and%2520Anurag%2520Pradhan%2520and%2520Amit%2520Shukla%2520and%2520Arnav%2520Bhavsar%2520and%2520Pawan%2520Goyal%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520understanding%2520and%2520generating%2520video%2520descriptions%2520but%2520their%2520high%2520memory%252C%2520computation%252C%2520and%2520deployment%2520demands%2520hinder%2520practical%2520use%2520particularly%2520for%2520blind%2520and%2520low-vision%2520%2528BLV%2529%2520users%2520who%2520depend%2520on%2520detailed%252C%2520context-aware%2520descriptions.%2520To%2520study%2520the%2520effect%2520of%2520model%2520size%2520on%2520accessibility-focused%2520description%2520quality%252C%2520we%2520evaluate%2520SmolVLM2%2520variants%2520with%2520500M%2520and%25202.2B%2520parameters%2520across%2520two%2520diverse%2520datasets%253A%2520AVCaps%2520%2528outdoor%2529%252C%2520and%2520Charades%2520%2528indoor%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%2520two%2520novel%2520evaluation%2520frameworks%2520specifically%2520designed%2520for%2520BLV%2520accessibility%2520assessment%253A%2520the%2520Multi-Context%2520BLV%2520Framework%2520evaluating%2520spatial%2520orientation%252C%2520social%2520interaction%252C%2520action%2520events%252C%2520and%2520ambience%2520contexts%253B%2520and%2520the%2520Navigational%2520Assistance%2520Framework%2520focusing%2520on%2520mobility-critical%2520information.%2520Additionally%252C%2520we%2520conduct%2520a%2520systematic%2520evaluation%2520of%2520four%2520different%2520prompt%2520design%2520strategies%2520and%2520deploy%2520both%2520models%2520on%2520a%2520smartphone%252C%2520evaluating%2520FP32%2520and%2520INT8%2520precision%2520variants%2520to%2520assess%2520real-world%2520performance%2520constraints%2520on%2520resource-limited%2520mobile%2520devices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Blind%20and%20Low-Vision%20Accessibility%20of%20Lightweight%20VLMs%20and%20Custom%20LLM-Evals&entry.906535625=Shruti%20Singh%20Baghel%20and%20Yash%20Pratap%20Singh%20Rathore%20and%20Sushovan%20Jena%20and%20Anurag%20Pradhan%20and%20Amit%20Shukla%20and%20Arnav%20Bhavsar%20and%20Pawan%20Goyal&entry.1292438233=Large%20Vision-Language%20Models%20%28VLMs%29%20excel%20at%20understanding%20and%20generating%20video%20descriptions%20but%20their%20high%20memory%2C%20computation%2C%20and%20deployment%20demands%20hinder%20practical%20use%20particularly%20for%20blind%20and%20low-vision%20%28BLV%29%20users%20who%20depend%20on%20detailed%2C%20context-aware%20descriptions.%20To%20study%20the%20effect%20of%20model%20size%20on%20accessibility-focused%20description%20quality%2C%20we%20evaluate%20SmolVLM2%20variants%20with%20500M%20and%202.2B%20parameters%20across%20two%20diverse%20datasets%3A%20AVCaps%20%28outdoor%29%2C%20and%20Charades%20%28indoor%29.%20In%20this%20work%2C%20we%20introduce%20two%20novel%20evaluation%20frameworks%20specifically%20designed%20for%20BLV%20accessibility%20assessment%3A%20the%20Multi-Context%20BLV%20Framework%20evaluating%20spatial%20orientation%2C%20social%20interaction%2C%20action%20events%2C%20and%20ambience%20contexts%3B%20and%20the%20Navigational%20Assistance%20Framework%20focusing%20on%20mobility-critical%20information.%20Additionally%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20four%20different%20prompt%20design%20strategies%20and%20deploy%20both%20models%20on%20a%20smartphone%2C%20evaluating%20FP32%20and%20INT8%20precision%20variants%20to%20assess%20real-world%20performance%20constraints%20on%20resource-limited%20mobile%20devices.&entry.1838667208=http%3A//arxiv.org/abs/2511.10615v1&entry.124074799=Read"},
{"title": "Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models", "author": "Alexander Htet Kyaw and Richa Gupta and Dhruv Shah and Anoop Sinha and Kory Mathewson and Stefanie Pender and Sachin Chitta and Yotto Koga and Faez Ahmed and Lawrence Sass and Randall Davis", "abstract": "Advances in 3D generative AI have enabled the creation of physical objects from text prompts, but challenges remain in creating objects involving multiple component types. We present a pipeline that integrates 3D generative AI with vision-language models (VLMs) to enable the robotic assembly of multi-component objects from natural language. Our method leverages VLMs for zero-shot, multi-modal reasoning about geometry and functionality to decompose AI-generated meshes into multi-component 3D models using predefined structural and panel components. We demonstrate that a VLM is capable of determining which mesh regions need panel components in addition to structural components, based on the object's geometry and functionality. Evaluation across test objects shows that users preferred the VLM-generated assignments 90.6% of the time, compared to 59.4% for rule-based and 2.5% for random assignment. Lastly, the system allows users to refine component assignments through conversational feedback, enabling greater human control and agency in making physical objects with generative AI and robotics.", "link": "http://arxiv.org/abs/2511.02162v3", "date": "2025-11-13", "relevancy": 2.2879, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5788}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20to%20Robotic%20Assembly%20of%20Multi%20Component%20Objects%20using%203D%20Generative%20AI%20and%20Vision%20Language%20Models&body=Title%3A%20Text%20to%20Robotic%20Assembly%20of%20Multi%20Component%20Objects%20using%203D%20Generative%20AI%20and%20Vision%20Language%20Models%0AAuthor%3A%20Alexander%20Htet%20Kyaw%20and%20Richa%20Gupta%20and%20Dhruv%20Shah%20and%20Anoop%20Sinha%20and%20Kory%20Mathewson%20and%20Stefanie%20Pender%20and%20Sachin%20Chitta%20and%20Yotto%20Koga%20and%20Faez%20Ahmed%20and%20Lawrence%20Sass%20and%20Randall%20Davis%0AAbstract%3A%20Advances%20in%203D%20generative%20AI%20have%20enabled%20the%20creation%20of%20physical%20objects%20from%20text%20prompts%2C%20but%20challenges%20remain%20in%20creating%20objects%20involving%20multiple%20component%20types.%20We%20present%20a%20pipeline%20that%20integrates%203D%20generative%20AI%20with%20vision-language%20models%20%28VLMs%29%20to%20enable%20the%20robotic%20assembly%20of%20multi-component%20objects%20from%20natural%20language.%20Our%20method%20leverages%20VLMs%20for%20zero-shot%2C%20multi-modal%20reasoning%20about%20geometry%20and%20functionality%20to%20decompose%20AI-generated%20meshes%20into%20multi-component%203D%20models%20using%20predefined%20structural%20and%20panel%20components.%20We%20demonstrate%20that%20a%20VLM%20is%20capable%20of%20determining%20which%20mesh%20regions%20need%20panel%20components%20in%20addition%20to%20structural%20components%2C%20based%20on%20the%20object%27s%20geometry%20and%20functionality.%20Evaluation%20across%20test%20objects%20shows%20that%20users%20preferred%20the%20VLM-generated%20assignments%2090.6%25%20of%20the%20time%2C%20compared%20to%2059.4%25%20for%20rule-based%20and%202.5%25%20for%20random%20assignment.%20Lastly%2C%20the%20system%20allows%20users%20to%20refine%20component%20assignments%20through%20conversational%20feedback%2C%20enabling%20greater%20human%20control%20and%20agency%20in%20making%20physical%20objects%20with%20generative%20AI%20and%20robotics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02162v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520to%2520Robotic%2520Assembly%2520of%2520Multi%2520Component%2520Objects%2520using%25203D%2520Generative%2520AI%2520and%2520Vision%2520Language%2520Models%26entry.906535625%3DAlexander%2520Htet%2520Kyaw%2520and%2520Richa%2520Gupta%2520and%2520Dhruv%2520Shah%2520and%2520Anoop%2520Sinha%2520and%2520Kory%2520Mathewson%2520and%2520Stefanie%2520Pender%2520and%2520Sachin%2520Chitta%2520and%2520Yotto%2520Koga%2520and%2520Faez%2520Ahmed%2520and%2520Lawrence%2520Sass%2520and%2520Randall%2520Davis%26entry.1292438233%3DAdvances%2520in%25203D%2520generative%2520AI%2520have%2520enabled%2520the%2520creation%2520of%2520physical%2520objects%2520from%2520text%2520prompts%252C%2520but%2520challenges%2520remain%2520in%2520creating%2520objects%2520involving%2520multiple%2520component%2520types.%2520We%2520present%2520a%2520pipeline%2520that%2520integrates%25203D%2520generative%2520AI%2520with%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520enable%2520the%2520robotic%2520assembly%2520of%2520multi-component%2520objects%2520from%2520natural%2520language.%2520Our%2520method%2520leverages%2520VLMs%2520for%2520zero-shot%252C%2520multi-modal%2520reasoning%2520about%2520geometry%2520and%2520functionality%2520to%2520decompose%2520AI-generated%2520meshes%2520into%2520multi-component%25203D%2520models%2520using%2520predefined%2520structural%2520and%2520panel%2520components.%2520We%2520demonstrate%2520that%2520a%2520VLM%2520is%2520capable%2520of%2520determining%2520which%2520mesh%2520regions%2520need%2520panel%2520components%2520in%2520addition%2520to%2520structural%2520components%252C%2520based%2520on%2520the%2520object%2527s%2520geometry%2520and%2520functionality.%2520Evaluation%2520across%2520test%2520objects%2520shows%2520that%2520users%2520preferred%2520the%2520VLM-generated%2520assignments%252090.6%2525%2520of%2520the%2520time%252C%2520compared%2520to%252059.4%2525%2520for%2520rule-based%2520and%25202.5%2525%2520for%2520random%2520assignment.%2520Lastly%252C%2520the%2520system%2520allows%2520users%2520to%2520refine%2520component%2520assignments%2520through%2520conversational%2520feedback%252C%2520enabling%2520greater%2520human%2520control%2520and%2520agency%2520in%2520making%2520physical%2520objects%2520with%2520generative%2520AI%2520and%2520robotics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02162v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20to%20Robotic%20Assembly%20of%20Multi%20Component%20Objects%20using%203D%20Generative%20AI%20and%20Vision%20Language%20Models&entry.906535625=Alexander%20Htet%20Kyaw%20and%20Richa%20Gupta%20and%20Dhruv%20Shah%20and%20Anoop%20Sinha%20and%20Kory%20Mathewson%20and%20Stefanie%20Pender%20and%20Sachin%20Chitta%20and%20Yotto%20Koga%20and%20Faez%20Ahmed%20and%20Lawrence%20Sass%20and%20Randall%20Davis&entry.1292438233=Advances%20in%203D%20generative%20AI%20have%20enabled%20the%20creation%20of%20physical%20objects%20from%20text%20prompts%2C%20but%20challenges%20remain%20in%20creating%20objects%20involving%20multiple%20component%20types.%20We%20present%20a%20pipeline%20that%20integrates%203D%20generative%20AI%20with%20vision-language%20models%20%28VLMs%29%20to%20enable%20the%20robotic%20assembly%20of%20multi-component%20objects%20from%20natural%20language.%20Our%20method%20leverages%20VLMs%20for%20zero-shot%2C%20multi-modal%20reasoning%20about%20geometry%20and%20functionality%20to%20decompose%20AI-generated%20meshes%20into%20multi-component%203D%20models%20using%20predefined%20structural%20and%20panel%20components.%20We%20demonstrate%20that%20a%20VLM%20is%20capable%20of%20determining%20which%20mesh%20regions%20need%20panel%20components%20in%20addition%20to%20structural%20components%2C%20based%20on%20the%20object%27s%20geometry%20and%20functionality.%20Evaluation%20across%20test%20objects%20shows%20that%20users%20preferred%20the%20VLM-generated%20assignments%2090.6%25%20of%20the%20time%2C%20compared%20to%2059.4%25%20for%20rule-based%20and%202.5%25%20for%20random%20assignment.%20Lastly%2C%20the%20system%20allows%20users%20to%20refine%20component%20assignments%20through%20conversational%20feedback%2C%20enabling%20greater%20human%20control%20and%20agency%20in%20making%20physical%20objects%20with%20generative%20AI%20and%20robotics.&entry.1838667208=http%3A//arxiv.org/abs/2511.02162v3&entry.124074799=Read"},
{"title": "GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models", "author": "Oussema Dhaouadi and Johannes Meier and Jacques Kaiser and Daniel Cremers", "abstract": "Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at https://deepscenario.github.io/GrounDiff/.", "link": "http://arxiv.org/abs/2511.10391v1", "date": "2025-11-13", "relevancy": 2.2853, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.578}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.57}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrounDiff%3A%20Diffusion-Based%20Ground%20Surface%20Generation%20from%20Digital%20Surface%20Models&body=Title%3A%20GrounDiff%3A%20Diffusion-Based%20Ground%20Surface%20Generation%20from%20Digital%20Surface%20Models%0AAuthor%3A%20Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Jacques%20Kaiser%20and%20Daniel%20Cremers%0AAbstract%3A%20Digital%20Terrain%20Models%20%28DTMs%29%20represent%20the%20bare-earth%20elevation%20and%20are%20important%20in%20numerous%20geospatial%20applications.%20Such%20data%20models%20cannot%20be%20directly%20measured%20by%20sensors%20and%20are%20typically%20generated%20from%20Digital%20Surface%20Models%20%28DSMs%29%20derived%20from%20LiDAR%20or%20photogrammetry.%20Traditional%20filtering%20approaches%20rely%20on%20manually%20tuned%20parameters%2C%20while%20learning-based%20methods%20require%20well-designed%20architectures%2C%20often%20combined%20with%20post-processing.%20To%20address%20these%20challenges%2C%20we%20introduce%20Ground%20Diffusion%20%28GrounDiff%29%2C%20the%20first%20diffusion-based%20framework%20that%20iteratively%20removes%20non-ground%20structures%20by%20formulating%20the%20problem%20as%20a%20denoising%20task.%20We%20incorporate%20a%20gated%20design%20with%20confidence-guided%20generation%20that%20enables%20selective%20filtering.%20To%20increase%20scalability%2C%20we%20further%20propose%20Prior-Guided%20Stitching%20%28PrioStitch%29%2C%20which%20employs%20a%20downsampled%20global%20prior%20automatically%20generated%20using%20GrounDiff%20to%20guide%20local%20high-resolution%20predictions.%20We%20evaluate%20our%20method%20on%20the%20DSM-to-DTM%20translation%20task%20across%20diverse%20datasets%2C%20showing%20that%20GrounDiff%20consistently%20outperforms%20deep%20learning-based%20state-of-the-art%20methods%2C%20reducing%20RMSE%20by%20up%20to%2093%25%20on%20ALS2DTM%20and%20up%20to%2047%25%20on%20USGS%20benchmarks.%20In%20the%20task%20of%20road%20reconstruction%2C%20which%20requires%20both%20high%20precision%20and%20smoothness%2C%20our%20method%20achieves%20up%20to%2081%25%20lower%20distance%20error%20compared%20to%20specialized%20techniques%20on%20the%20GeRoD%20benchmark%2C%20while%20maintaining%20competitive%20surface%20smoothness%20using%20only%20DSM%20inputs%2C%20without%20task-specific%20optimization.%20Our%20variant%20for%20road%20reconstruction%2C%20GrounDiff%2B%2C%20is%20specifically%20designed%20to%20produce%20even%20smoother%20surfaces%2C%20further%20surpassing%20state-of-the-art%20methods.%20The%20project%20page%20is%20available%20at%20https%3A//deepscenario.github.io/GrounDiff/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounDiff%253A%2520Diffusion-Based%2520Ground%2520Surface%2520Generation%2520from%2520Digital%2520Surface%2520Models%26entry.906535625%3DOussema%2520Dhaouadi%2520and%2520Johannes%2520Meier%2520and%2520Jacques%2520Kaiser%2520and%2520Daniel%2520Cremers%26entry.1292438233%3DDigital%2520Terrain%2520Models%2520%2528DTMs%2529%2520represent%2520the%2520bare-earth%2520elevation%2520and%2520are%2520important%2520in%2520numerous%2520geospatial%2520applications.%2520Such%2520data%2520models%2520cannot%2520be%2520directly%2520measured%2520by%2520sensors%2520and%2520are%2520typically%2520generated%2520from%2520Digital%2520Surface%2520Models%2520%2528DSMs%2529%2520derived%2520from%2520LiDAR%2520or%2520photogrammetry.%2520Traditional%2520filtering%2520approaches%2520rely%2520on%2520manually%2520tuned%2520parameters%252C%2520while%2520learning-based%2520methods%2520require%2520well-designed%2520architectures%252C%2520often%2520combined%2520with%2520post-processing.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Ground%2520Diffusion%2520%2528GrounDiff%2529%252C%2520the%2520first%2520diffusion-based%2520framework%2520that%2520iteratively%2520removes%2520non-ground%2520structures%2520by%2520formulating%2520the%2520problem%2520as%2520a%2520denoising%2520task.%2520We%2520incorporate%2520a%2520gated%2520design%2520with%2520confidence-guided%2520generation%2520that%2520enables%2520selective%2520filtering.%2520To%2520increase%2520scalability%252C%2520we%2520further%2520propose%2520Prior-Guided%2520Stitching%2520%2528PrioStitch%2529%252C%2520which%2520employs%2520a%2520downsampled%2520global%2520prior%2520automatically%2520generated%2520using%2520GrounDiff%2520to%2520guide%2520local%2520high-resolution%2520predictions.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520DSM-to-DTM%2520translation%2520task%2520across%2520diverse%2520datasets%252C%2520showing%2520that%2520GrounDiff%2520consistently%2520outperforms%2520deep%2520learning-based%2520state-of-the-art%2520methods%252C%2520reducing%2520RMSE%2520by%2520up%2520to%252093%2525%2520on%2520ALS2DTM%2520and%2520up%2520to%252047%2525%2520on%2520USGS%2520benchmarks.%2520In%2520the%2520task%2520of%2520road%2520reconstruction%252C%2520which%2520requires%2520both%2520high%2520precision%2520and%2520smoothness%252C%2520our%2520method%2520achieves%2520up%2520to%252081%2525%2520lower%2520distance%2520error%2520compared%2520to%2520specialized%2520techniques%2520on%2520the%2520GeRoD%2520benchmark%252C%2520while%2520maintaining%2520competitive%2520surface%2520smoothness%2520using%2520only%2520DSM%2520inputs%252C%2520without%2520task-specific%2520optimization.%2520Our%2520variant%2520for%2520road%2520reconstruction%252C%2520GrounDiff%252B%252C%2520is%2520specifically%2520designed%2520to%2520produce%2520even%2520smoother%2520surfaces%252C%2520further%2520surpassing%2520state-of-the-art%2520methods.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//deepscenario.github.io/GrounDiff/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrounDiff%3A%20Diffusion-Based%20Ground%20Surface%20Generation%20from%20Digital%20Surface%20Models&entry.906535625=Oussema%20Dhaouadi%20and%20Johannes%20Meier%20and%20Jacques%20Kaiser%20and%20Daniel%20Cremers&entry.1292438233=Digital%20Terrain%20Models%20%28DTMs%29%20represent%20the%20bare-earth%20elevation%20and%20are%20important%20in%20numerous%20geospatial%20applications.%20Such%20data%20models%20cannot%20be%20directly%20measured%20by%20sensors%20and%20are%20typically%20generated%20from%20Digital%20Surface%20Models%20%28DSMs%29%20derived%20from%20LiDAR%20or%20photogrammetry.%20Traditional%20filtering%20approaches%20rely%20on%20manually%20tuned%20parameters%2C%20while%20learning-based%20methods%20require%20well-designed%20architectures%2C%20often%20combined%20with%20post-processing.%20To%20address%20these%20challenges%2C%20we%20introduce%20Ground%20Diffusion%20%28GrounDiff%29%2C%20the%20first%20diffusion-based%20framework%20that%20iteratively%20removes%20non-ground%20structures%20by%20formulating%20the%20problem%20as%20a%20denoising%20task.%20We%20incorporate%20a%20gated%20design%20with%20confidence-guided%20generation%20that%20enables%20selective%20filtering.%20To%20increase%20scalability%2C%20we%20further%20propose%20Prior-Guided%20Stitching%20%28PrioStitch%29%2C%20which%20employs%20a%20downsampled%20global%20prior%20automatically%20generated%20using%20GrounDiff%20to%20guide%20local%20high-resolution%20predictions.%20We%20evaluate%20our%20method%20on%20the%20DSM-to-DTM%20translation%20task%20across%20diverse%20datasets%2C%20showing%20that%20GrounDiff%20consistently%20outperforms%20deep%20learning-based%20state-of-the-art%20methods%2C%20reducing%20RMSE%20by%20up%20to%2093%25%20on%20ALS2DTM%20and%20up%20to%2047%25%20on%20USGS%20benchmarks.%20In%20the%20task%20of%20road%20reconstruction%2C%20which%20requires%20both%20high%20precision%20and%20smoothness%2C%20our%20method%20achieves%20up%20to%2081%25%20lower%20distance%20error%20compared%20to%20specialized%20techniques%20on%20the%20GeRoD%20benchmark%2C%20while%20maintaining%20competitive%20surface%20smoothness%20using%20only%20DSM%20inputs%2C%20without%20task-specific%20optimization.%20Our%20variant%20for%20road%20reconstruction%2C%20GrounDiff%2B%2C%20is%20specifically%20designed%20to%20produce%20even%20smoother%20surfaces%2C%20further%20surpassing%20state-of-the-art%20methods.%20The%20project%20page%20is%20available%20at%20https%3A//deepscenario.github.io/GrounDiff/.&entry.1838667208=http%3A//arxiv.org/abs/2511.10391v1&entry.124074799=Read"},
{"title": "Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization", "author": "Ashutosh Anshul and Shreyas Gopal and Deepu Rajan and Eng Siong Chng", "abstract": "Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.", "link": "http://arxiv.org/abs/2511.10212v1", "date": "2025-11-13", "relevancy": 2.2833, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5833}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5717}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Frame%20Feature%20Prediction%20for%20Multimodal%20Deepfake%20Detection%20and%20Temporal%20Localization&body=Title%3A%20Next-Frame%20Feature%20Prediction%20for%20Multimodal%20Deepfake%20Detection%20and%20Temporal%20Localization%0AAuthor%3A%20Ashutosh%20Anshul%20and%20Shreyas%20Gopal%20and%20Deepu%20Rajan%20and%20Eng%20Siong%20Chng%0AAbstract%3A%20Recent%20multimodal%20deepfake%20detection%20methods%20designed%20for%20generalization%20conjecture%20that%20single-stage%20supervised%20training%20struggles%20to%20generalize%20across%20unseen%20manipulations%20and%20datasets.%20However%2C%20such%20approaches%20that%20target%20generalization%20require%20pretraining%20over%20real%20samples.%20Additionally%2C%20these%20methods%20primarily%20focus%20on%20detecting%20audio-visual%20inconsistencies%20and%20may%20overlook%20intra-modal%20artifacts%20causing%20them%20to%20fail%20against%20manipulations%20that%20preserve%20audio-visual%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20single-stage%20training%20framework%20that%20enhances%20generalization%20by%20incorporating%20next-frame%20prediction%20for%20both%20uni-modal%20and%20cross-modal%20features.%20Additionally%2C%20we%20introduce%20a%20window-level%20attention%20mechanism%20to%20capture%20discrepancies%20between%20predicted%20and%20actual%20frames%2C%20enabling%20the%20model%20to%20detect%20local%20artifacts%20around%20every%20frame%2C%20which%20is%20crucial%20for%20accurately%20classifying%20fully%20manipulated%20videos%20and%20effectively%20localizing%20deepfake%20segments%20in%20partially%20spoofed%20samples.%20Our%20model%2C%20evaluated%20on%20multiple%20benchmark%20datasets%2C%20demonstrates%20strong%20generalization%20and%20precise%20temporal%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Frame%2520Feature%2520Prediction%2520for%2520Multimodal%2520Deepfake%2520Detection%2520and%2520Temporal%2520Localization%26entry.906535625%3DAshutosh%2520Anshul%2520and%2520Shreyas%2520Gopal%2520and%2520Deepu%2520Rajan%2520and%2520Eng%2520Siong%2520Chng%26entry.1292438233%3DRecent%2520multimodal%2520deepfake%2520detection%2520methods%2520designed%2520for%2520generalization%2520conjecture%2520that%2520single-stage%2520supervised%2520training%2520struggles%2520to%2520generalize%2520across%2520unseen%2520manipulations%2520and%2520datasets.%2520However%252C%2520such%2520approaches%2520that%2520target%2520generalization%2520require%2520pretraining%2520over%2520real%2520samples.%2520Additionally%252C%2520these%2520methods%2520primarily%2520focus%2520on%2520detecting%2520audio-visual%2520inconsistencies%2520and%2520may%2520overlook%2520intra-modal%2520artifacts%2520causing%2520them%2520to%2520fail%2520against%2520manipulations%2520that%2520preserve%2520audio-visual%2520alignment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520single-stage%2520training%2520framework%2520that%2520enhances%2520generalization%2520by%2520incorporating%2520next-frame%2520prediction%2520for%2520both%2520uni-modal%2520and%2520cross-modal%2520features.%2520Additionally%252C%2520we%2520introduce%2520a%2520window-level%2520attention%2520mechanism%2520to%2520capture%2520discrepancies%2520between%2520predicted%2520and%2520actual%2520frames%252C%2520enabling%2520the%2520model%2520to%2520detect%2520local%2520artifacts%2520around%2520every%2520frame%252C%2520which%2520is%2520crucial%2520for%2520accurately%2520classifying%2520fully%2520manipulated%2520videos%2520and%2520effectively%2520localizing%2520deepfake%2520segments%2520in%2520partially%2520spoofed%2520samples.%2520Our%2520model%252C%2520evaluated%2520on%2520multiple%2520benchmark%2520datasets%252C%2520demonstrates%2520strong%2520generalization%2520and%2520precise%2520temporal%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Frame%20Feature%20Prediction%20for%20Multimodal%20Deepfake%20Detection%20and%20Temporal%20Localization&entry.906535625=Ashutosh%20Anshul%20and%20Shreyas%20Gopal%20and%20Deepu%20Rajan%20and%20Eng%20Siong%20Chng&entry.1292438233=Recent%20multimodal%20deepfake%20detection%20methods%20designed%20for%20generalization%20conjecture%20that%20single-stage%20supervised%20training%20struggles%20to%20generalize%20across%20unseen%20manipulations%20and%20datasets.%20However%2C%20such%20approaches%20that%20target%20generalization%20require%20pretraining%20over%20real%20samples.%20Additionally%2C%20these%20methods%20primarily%20focus%20on%20detecting%20audio-visual%20inconsistencies%20and%20may%20overlook%20intra-modal%20artifacts%20causing%20them%20to%20fail%20against%20manipulations%20that%20preserve%20audio-visual%20alignment.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20single-stage%20training%20framework%20that%20enhances%20generalization%20by%20incorporating%20next-frame%20prediction%20for%20both%20uni-modal%20and%20cross-modal%20features.%20Additionally%2C%20we%20introduce%20a%20window-level%20attention%20mechanism%20to%20capture%20discrepancies%20between%20predicted%20and%20actual%20frames%2C%20enabling%20the%20model%20to%20detect%20local%20artifacts%20around%20every%20frame%2C%20which%20is%20crucial%20for%20accurately%20classifying%20fully%20manipulated%20videos%20and%20effectively%20localizing%20deepfake%20segments%20in%20partially%20spoofed%20samples.%20Our%20model%2C%20evaluated%20on%20multiple%20benchmark%20datasets%2C%20demonstrates%20strong%20generalization%20and%20precise%20temporal%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2511.10212v1&entry.124074799=Read"},
{"title": "Advanced Black-Box Tuning of Large Language Models with Limited API Calls", "author": "Zhikang Xie and Weilin Wan and Peizhu Gong and Weizhong Zhang and Cheng Jin", "abstract": "Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with \"LogitMap Pairs\" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.", "link": "http://arxiv.org/abs/2511.10210v1", "date": "2025-11-13", "relevancy": 2.2759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.453}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Black-Box%20Tuning%20of%20Large%20Language%20Models%20with%20Limited%20API%20Calls&body=Title%3A%20Advanced%20Black-Box%20Tuning%20of%20Large%20Language%20Models%20with%20Limited%20API%20Calls%0AAuthor%3A%20Zhikang%20Xie%20and%20Weilin%20Wan%20and%20Peizhu%20Gong%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%0AAbstract%3A%20Black-box%20tuning%20is%20an%20emerging%20paradigm%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20better%20achieve%20desired%20behaviors%2C%20particularly%20when%20direct%20access%20to%20model%20parameters%20is%20unavailable.%20Current%20strategies%2C%20however%2C%20often%20present%20a%20dilemma%20of%20suboptimal%20extremes%3A%20either%20separately%20train%20a%20small%20proxy%20model%20and%20then%20use%20it%20to%20shift%20the%20predictions%20of%20the%20foundation%20model%2C%20offering%20notable%20efficiency%20but%20often%20yielding%20limited%20improvement%3B%20or%20making%20API%20calls%20in%20each%20tuning%20iteration%20to%20the%20foundation%20model%2C%20which%20entails%20prohibitive%20computational%20costs.%20Therefore%2C%20we%20propose%20a%20novel%20advanced%20black-box%20tuning%20method%20for%20LLMs%20with%20limited%20API%20calls.%20Our%20core%20strategy%20involves%20training%20a%20Gaussian%20Process%20%28GP%29%20surrogate%20model%20with%20%22LogitMap%20Pairs%22%20derived%20from%20querying%20the%20foundation%20model%20on%20a%20minimal%20but%20highly%20informative%20training%20subset.%20This%20surrogate%20can%20approximate%20the%20outputs%20of%20the%20foundation%20model%20to%20guide%20the%20training%20of%20the%20proxy%20model%2C%20thereby%20effectively%20reducing%20the%20need%20for%20direct%20queries%20to%20the%20foundation%20model.%20Extensive%20experiments%20verify%20that%20our%20approach%20elevates%20pre-trained%20language%20model%20accuracy%20from%2055.92%25%20to%2086.85%25%2C%20reducing%20the%20frequency%20of%20API%20queries%20to%20merely%201.38%25.%20This%20significantly%20outperforms%20offline%20approaches%20that%20operate%20entirely%20without%20API%20access.%20Notably%2C%20our%20method%20also%20achieves%20comparable%20or%20superior%20accuracy%20to%20query-intensive%20approaches%2C%20while%20significantly%20reducing%20API%20costs.%20This%20offers%20a%20robust%20and%20high-efficiency%20paradigm%20for%20language%20model%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Black-Box%2520Tuning%2520of%2520Large%2520Language%2520Models%2520with%2520Limited%2520API%2520Calls%26entry.906535625%3DZhikang%2520Xie%2520and%2520Weilin%2520Wan%2520and%2520Peizhu%2520Gong%2520and%2520Weizhong%2520Zhang%2520and%2520Cheng%2520Jin%26entry.1292438233%3DBlack-box%2520tuning%2520is%2520an%2520emerging%2520paradigm%2520for%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520better%2520achieve%2520desired%2520behaviors%252C%2520particularly%2520when%2520direct%2520access%2520to%2520model%2520parameters%2520is%2520unavailable.%2520Current%2520strategies%252C%2520however%252C%2520often%2520present%2520a%2520dilemma%2520of%2520suboptimal%2520extremes%253A%2520either%2520separately%2520train%2520a%2520small%2520proxy%2520model%2520and%2520then%2520use%2520it%2520to%2520shift%2520the%2520predictions%2520of%2520the%2520foundation%2520model%252C%2520offering%2520notable%2520efficiency%2520but%2520often%2520yielding%2520limited%2520improvement%253B%2520or%2520making%2520API%2520calls%2520in%2520each%2520tuning%2520iteration%2520to%2520the%2520foundation%2520model%252C%2520which%2520entails%2520prohibitive%2520computational%2520costs.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520advanced%2520black-box%2520tuning%2520method%2520for%2520LLMs%2520with%2520limited%2520API%2520calls.%2520Our%2520core%2520strategy%2520involves%2520training%2520a%2520Gaussian%2520Process%2520%2528GP%2529%2520surrogate%2520model%2520with%2520%2522LogitMap%2520Pairs%2522%2520derived%2520from%2520querying%2520the%2520foundation%2520model%2520on%2520a%2520minimal%2520but%2520highly%2520informative%2520training%2520subset.%2520This%2520surrogate%2520can%2520approximate%2520the%2520outputs%2520of%2520the%2520foundation%2520model%2520to%2520guide%2520the%2520training%2520of%2520the%2520proxy%2520model%252C%2520thereby%2520effectively%2520reducing%2520the%2520need%2520for%2520direct%2520queries%2520to%2520the%2520foundation%2520model.%2520Extensive%2520experiments%2520verify%2520that%2520our%2520approach%2520elevates%2520pre-trained%2520language%2520model%2520accuracy%2520from%252055.92%2525%2520to%252086.85%2525%252C%2520reducing%2520the%2520frequency%2520of%2520API%2520queries%2520to%2520merely%25201.38%2525.%2520This%2520significantly%2520outperforms%2520offline%2520approaches%2520that%2520operate%2520entirely%2520without%2520API%2520access.%2520Notably%252C%2520our%2520method%2520also%2520achieves%2520comparable%2520or%2520superior%2520accuracy%2520to%2520query-intensive%2520approaches%252C%2520while%2520significantly%2520reducing%2520API%2520costs.%2520This%2520offers%2520a%2520robust%2520and%2520high-efficiency%2520paradigm%2520for%2520language%2520model%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Black-Box%20Tuning%20of%20Large%20Language%20Models%20with%20Limited%20API%20Calls&entry.906535625=Zhikang%20Xie%20and%20Weilin%20Wan%20and%20Peizhu%20Gong%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=Black-box%20tuning%20is%20an%20emerging%20paradigm%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20better%20achieve%20desired%20behaviors%2C%20particularly%20when%20direct%20access%20to%20model%20parameters%20is%20unavailable.%20Current%20strategies%2C%20however%2C%20often%20present%20a%20dilemma%20of%20suboptimal%20extremes%3A%20either%20separately%20train%20a%20small%20proxy%20model%20and%20then%20use%20it%20to%20shift%20the%20predictions%20of%20the%20foundation%20model%2C%20offering%20notable%20efficiency%20but%20often%20yielding%20limited%20improvement%3B%20or%20making%20API%20calls%20in%20each%20tuning%20iteration%20to%20the%20foundation%20model%2C%20which%20entails%20prohibitive%20computational%20costs.%20Therefore%2C%20we%20propose%20a%20novel%20advanced%20black-box%20tuning%20method%20for%20LLMs%20with%20limited%20API%20calls.%20Our%20core%20strategy%20involves%20training%20a%20Gaussian%20Process%20%28GP%29%20surrogate%20model%20with%20%22LogitMap%20Pairs%22%20derived%20from%20querying%20the%20foundation%20model%20on%20a%20minimal%20but%20highly%20informative%20training%20subset.%20This%20surrogate%20can%20approximate%20the%20outputs%20of%20the%20foundation%20model%20to%20guide%20the%20training%20of%20the%20proxy%20model%2C%20thereby%20effectively%20reducing%20the%20need%20for%20direct%20queries%20to%20the%20foundation%20model.%20Extensive%20experiments%20verify%20that%20our%20approach%20elevates%20pre-trained%20language%20model%20accuracy%20from%2055.92%25%20to%2086.85%25%2C%20reducing%20the%20frequency%20of%20API%20queries%20to%20merely%201.38%25.%20This%20significantly%20outperforms%20offline%20approaches%20that%20operate%20entirely%20without%20API%20access.%20Notably%2C%20our%20method%20also%20achieves%20comparable%20or%20superior%20accuracy%20to%20query-intensive%20approaches%2C%20while%20significantly%20reducing%20API%20costs.%20This%20offers%20a%20robust%20and%20high-efficiency%20paradigm%20for%20language%20model%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2511.10210v1&entry.124074799=Read"},
{"title": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution", "author": "Maryam Zolnoori and Hossein Azadmaleki and Yasaman Haghbin and Ali Zolnour and Mohammad Javad Momeni Nezhad and Sina Rashidi and Mehdi Naserian and Elyas Esmaeili and Sepehr Karimi Arpanahi", "abstract": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.", "link": "http://arxiv.org/abs/2511.08132v2", "date": "2025-11-13", "relevancy": 2.2754, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4733}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20National%20Institute%20on%20Aging%20PREPARE%20Challenge%3A%20Early%20Detection%20of%20Cognitive%20Impairment%20Using%20Speech%20--%20The%20SpeechCARE%20Solution&body=Title%3A%20National%20Institute%20on%20Aging%20PREPARE%20Challenge%3A%20Early%20Detection%20of%20Cognitive%20Impairment%20Using%20Speech%20--%20The%20SpeechCARE%20Solution%0AAuthor%3A%20Maryam%20Zolnoori%20and%20Hossein%20Azadmaleki%20and%20Yasaman%20Haghbin%20and%20Ali%20Zolnour%20and%20Mohammad%20Javad%20Momeni%20Nezhad%20and%20Sina%20Rashidi%20and%20Mehdi%20Naserian%20and%20Elyas%20Esmaeili%20and%20Sepehr%20Karimi%20Arpanahi%0AAbstract%3A%20Alzheimer%27s%20disease%20and%20related%20dementias%20%28ADRD%29%20affect%20one%20in%20five%20adults%20over%2060%2C%20yet%20more%20than%20half%20of%20individuals%20with%20cognitive%20decline%20remain%20undiagnosed.%20Speech-based%20assessments%20show%20promise%20for%20early%20detection%2C%20as%20phonetic%20motor%20planning%20deficits%20alter%20acoustic%20features%20%28e.g.%2C%20pitch%2C%20tone%29%2C%20while%20memory%20and%20language%20impairments%20lead%20to%20syntactic%20and%20semantic%20errors.%20However%2C%20conventional%20speech-processing%20pipelines%20with%20hand-crafted%20features%20or%20general-purpose%20audio%20classifiers%20often%20exhibit%20limited%20performance%20and%20generalizability.%20To%20address%20these%20limitations%2C%20we%20introduce%20SpeechCARE%2C%20a%20multimodal%20speech%20processing%20pipeline%20that%20leverages%20pretrained%2C%20multilingual%20acoustic%20and%20linguistic%20transformer%20models%20to%20capture%20subtle%20speech-related%20cues%20associated%20with%20cognitive%20impairment.%20Inspired%20by%20the%20Mixture%20of%20Experts%20%28MoE%29%20paradigm%2C%20SpeechCARE%20employs%20a%20dynamic%20fusion%20architecture%20that%20weights%20transformer-based%20acoustic%2C%20linguistic%2C%20and%20demographic%20inputs%2C%20allowing%20integration%20of%20additional%20modalities%20%28e.g.%2C%20social%20factors%2C%20imaging%29%20and%20enhancing%20robustness%20across%20diverse%20tasks.%20Its%20robust%20preprocessing%20includes%20automatic%20transcription%2C%20large%20language%20model%20%28LLM%29-based%20anomaly%20detection%2C%20and%20task%20identification.%20A%20SHAP-based%20explainability%20module%20and%20LLM%20reasoning%20highlight%20each%20modality%27s%20contribution%20to%20decision-making.%20SpeechCARE%20achieved%20AUC%20%3D%200.88%20and%20F1%20%3D%200.72%20for%20classifying%20cognitively%20healthy%2C%20MCI%2C%20and%20AD%20individuals%2C%20with%20AUC%20%3D%200.90%20and%20F1%20%3D%200.62%20for%20MCI%20detection.%20Bias%20analysis%20showed%20minimal%20disparities%2C%20except%20for%20adults%20over%2080.%20Mitigation%20techniques%20included%20oversampling%20and%20weighted%20loss.%20Future%20work%20includes%20deployment%20in%20real-world%20care%20settings%20%28e.g.%2C%20VNS%20Health%2C%20Columbia%20ADRC%29%20and%20EHR-integrated%20explainability%20for%20underrepresented%20populations%20in%20New%20York%20City.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNational%2520Institute%2520on%2520Aging%2520PREPARE%2520Challenge%253A%2520Early%2520Detection%2520of%2520Cognitive%2520Impairment%2520Using%2520Speech%2520--%2520The%2520SpeechCARE%2520Solution%26entry.906535625%3DMaryam%2520Zolnoori%2520and%2520Hossein%2520Azadmaleki%2520and%2520Yasaman%2520Haghbin%2520and%2520Ali%2520Zolnour%2520and%2520Mohammad%2520Javad%2520Momeni%2520Nezhad%2520and%2520Sina%2520Rashidi%2520and%2520Mehdi%2520Naserian%2520and%2520Elyas%2520Esmaeili%2520and%2520Sepehr%2520Karimi%2520Arpanahi%26entry.1292438233%3DAlzheimer%2527s%2520disease%2520and%2520related%2520dementias%2520%2528ADRD%2529%2520affect%2520one%2520in%2520five%2520adults%2520over%252060%252C%2520yet%2520more%2520than%2520half%2520of%2520individuals%2520with%2520cognitive%2520decline%2520remain%2520undiagnosed.%2520Speech-based%2520assessments%2520show%2520promise%2520for%2520early%2520detection%252C%2520as%2520phonetic%2520motor%2520planning%2520deficits%2520alter%2520acoustic%2520features%2520%2528e.g.%252C%2520pitch%252C%2520tone%2529%252C%2520while%2520memory%2520and%2520language%2520impairments%2520lead%2520to%2520syntactic%2520and%2520semantic%2520errors.%2520However%252C%2520conventional%2520speech-processing%2520pipelines%2520with%2520hand-crafted%2520features%2520or%2520general-purpose%2520audio%2520classifiers%2520often%2520exhibit%2520limited%2520performance%2520and%2520generalizability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520SpeechCARE%252C%2520a%2520multimodal%2520speech%2520processing%2520pipeline%2520that%2520leverages%2520pretrained%252C%2520multilingual%2520acoustic%2520and%2520linguistic%2520transformer%2520models%2520to%2520capture%2520subtle%2520speech-related%2520cues%2520associated%2520with%2520cognitive%2520impairment.%2520Inspired%2520by%2520the%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520paradigm%252C%2520SpeechCARE%2520employs%2520a%2520dynamic%2520fusion%2520architecture%2520that%2520weights%2520transformer-based%2520acoustic%252C%2520linguistic%252C%2520and%2520demographic%2520inputs%252C%2520allowing%2520integration%2520of%2520additional%2520modalities%2520%2528e.g.%252C%2520social%2520factors%252C%2520imaging%2529%2520and%2520enhancing%2520robustness%2520across%2520diverse%2520tasks.%2520Its%2520robust%2520preprocessing%2520includes%2520automatic%2520transcription%252C%2520large%2520language%2520model%2520%2528LLM%2529-based%2520anomaly%2520detection%252C%2520and%2520task%2520identification.%2520A%2520SHAP-based%2520explainability%2520module%2520and%2520LLM%2520reasoning%2520highlight%2520each%2520modality%2527s%2520contribution%2520to%2520decision-making.%2520SpeechCARE%2520achieved%2520AUC%2520%253D%25200.88%2520and%2520F1%2520%253D%25200.72%2520for%2520classifying%2520cognitively%2520healthy%252C%2520MCI%252C%2520and%2520AD%2520individuals%252C%2520with%2520AUC%2520%253D%25200.90%2520and%2520F1%2520%253D%25200.62%2520for%2520MCI%2520detection.%2520Bias%2520analysis%2520showed%2520minimal%2520disparities%252C%2520except%2520for%2520adults%2520over%252080.%2520Mitigation%2520techniques%2520included%2520oversampling%2520and%2520weighted%2520loss.%2520Future%2520work%2520includes%2520deployment%2520in%2520real-world%2520care%2520settings%2520%2528e.g.%252C%2520VNS%2520Health%252C%2520Columbia%2520ADRC%2529%2520and%2520EHR-integrated%2520explainability%2520for%2520underrepresented%2520populations%2520in%2520New%2520York%2520City.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=National%20Institute%20on%20Aging%20PREPARE%20Challenge%3A%20Early%20Detection%20of%20Cognitive%20Impairment%20Using%20Speech%20--%20The%20SpeechCARE%20Solution&entry.906535625=Maryam%20Zolnoori%20and%20Hossein%20Azadmaleki%20and%20Yasaman%20Haghbin%20and%20Ali%20Zolnour%20and%20Mohammad%20Javad%20Momeni%20Nezhad%20and%20Sina%20Rashidi%20and%20Mehdi%20Naserian%20and%20Elyas%20Esmaeili%20and%20Sepehr%20Karimi%20Arpanahi&entry.1292438233=Alzheimer%27s%20disease%20and%20related%20dementias%20%28ADRD%29%20affect%20one%20in%20five%20adults%20over%2060%2C%20yet%20more%20than%20half%20of%20individuals%20with%20cognitive%20decline%20remain%20undiagnosed.%20Speech-based%20assessments%20show%20promise%20for%20early%20detection%2C%20as%20phonetic%20motor%20planning%20deficits%20alter%20acoustic%20features%20%28e.g.%2C%20pitch%2C%20tone%29%2C%20while%20memory%20and%20language%20impairments%20lead%20to%20syntactic%20and%20semantic%20errors.%20However%2C%20conventional%20speech-processing%20pipelines%20with%20hand-crafted%20features%20or%20general-purpose%20audio%20classifiers%20often%20exhibit%20limited%20performance%20and%20generalizability.%20To%20address%20these%20limitations%2C%20we%20introduce%20SpeechCARE%2C%20a%20multimodal%20speech%20processing%20pipeline%20that%20leverages%20pretrained%2C%20multilingual%20acoustic%20and%20linguistic%20transformer%20models%20to%20capture%20subtle%20speech-related%20cues%20associated%20with%20cognitive%20impairment.%20Inspired%20by%20the%20Mixture%20of%20Experts%20%28MoE%29%20paradigm%2C%20SpeechCARE%20employs%20a%20dynamic%20fusion%20architecture%20that%20weights%20transformer-based%20acoustic%2C%20linguistic%2C%20and%20demographic%20inputs%2C%20allowing%20integration%20of%20additional%20modalities%20%28e.g.%2C%20social%20factors%2C%20imaging%29%20and%20enhancing%20robustness%20across%20diverse%20tasks.%20Its%20robust%20preprocessing%20includes%20automatic%20transcription%2C%20large%20language%20model%20%28LLM%29-based%20anomaly%20detection%2C%20and%20task%20identification.%20A%20SHAP-based%20explainability%20module%20and%20LLM%20reasoning%20highlight%20each%20modality%27s%20contribution%20to%20decision-making.%20SpeechCARE%20achieved%20AUC%20%3D%200.88%20and%20F1%20%3D%200.72%20for%20classifying%20cognitively%20healthy%2C%20MCI%2C%20and%20AD%20individuals%2C%20with%20AUC%20%3D%200.90%20and%20F1%20%3D%200.62%20for%20MCI%20detection.%20Bias%20analysis%20showed%20minimal%20disparities%2C%20except%20for%20adults%20over%2080.%20Mitigation%20techniques%20included%20oversampling%20and%20weighted%20loss.%20Future%20work%20includes%20deployment%20in%20real-world%20care%20settings%20%28e.g.%2C%20VNS%20Health%2C%20Columbia%20ADRC%29%20and%20EHR-integrated%20explainability%20for%20underrepresented%20populations%20in%20New%20York%20City.&entry.1838667208=http%3A//arxiv.org/abs/2511.08132v2&entry.124074799=Read"},
{"title": "Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing", "author": "Ousmane Touat and Jezekael Brunon and Yacine Belal and Julien Nicolas and C\u00e9sar Sabater and Mohamed Maouche and Sonia Ben Mokhtar", "abstract": "The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.", "link": "http://arxiv.org/abs/2412.12837v4", "date": "2025-11-13", "relevancy": 2.2692, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4576}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4552}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20the%20Vulnerability%20of%20Decentralized%20Learning%20to%20Membership%20Inference%20Attacks%20Through%20the%20Lens%20of%20Graph%20Mixing&body=Title%3A%20Exposing%20the%20Vulnerability%20of%20Decentralized%20Learning%20to%20Membership%20Inference%20Attacks%20Through%20the%20Lens%20of%20Graph%20Mixing%0AAuthor%3A%20Ousmane%20Touat%20and%20Jezekael%20Brunon%20and%20Yacine%20Belal%20and%20Julien%20Nicolas%20and%20C%C3%A9sar%20Sabater%20and%20Mohamed%20Maouche%20and%20Sonia%20Ben%20Mokhtar%0AAbstract%3A%20The%20primary%20promise%20of%20decentralized%20learning%20is%20to%20allow%20users%20to%20engage%20in%20the%20training%20of%20machine%20learning%20models%20in%20a%20collaborative%20manner%20while%20keeping%20their%20data%20on%20their%20premises%20and%20without%20relying%20on%20any%20central%20entity.%20However%2C%20this%20paradigm%20necessitates%20the%20exchange%20of%20model%20parameters%20or%20gradients%20between%20peers.%20Such%20exchanges%20can%20be%20exploited%20to%20infer%20sensitive%20information%20about%20training%20data%2C%20which%20is%20achieved%20through%20privacy%20attacks%20%28e.g.%2C%20Membership%20Inference%20Attacks%20--%20MIA%29.%20In%20order%20to%20devise%20effective%20defense%20mechanisms%2C%20it%20is%20important%20to%20understand%20the%20factors%20that%20increase/reduce%20the%20vulnerability%20of%20a%20given%20decentralized%20learning%20architecture%20to%20MIA.%20In%20this%20study%2C%20we%20extensively%20explore%20the%20vulnerability%20to%20MIA%20of%20various%20decentralized%20learning%20architectures%20by%20varying%20the%20graph%20structure%20%28e.g.%2C%20number%20of%20neighbors%29%2C%20the%20graph%20dynamics%2C%20and%20the%20aggregation%20strategy%2C%20across%20diverse%20datasets%20and%20data%20distributions.%20Our%20key%20finding%2C%20which%20to%20the%20best%20of%20our%20knowledge%20we%20are%20the%20first%20to%20report%2C%20is%20that%20the%20vulnerability%20to%20MIA%20is%20heavily%20correlated%20to%20%28i%29%20the%20local%20model%20mixing%20strategy%20performed%20by%20each%20node%20upon%20reception%20of%20models%20from%20neighboring%20nodes%20and%20%28ii%29%20the%20global%20mixing%20properties%20of%20the%20communication%20graph.%20We%20illustrate%20these%20results%20experimentally%20using%20four%20datasets%20and%20by%20theoretically%20analyzing%20the%20mixing%20properties%20of%20various%20decentralized%20architectures.%20We%20also%20empirically%20show%20that%20enhancing%20mixing%20properties%20is%20highly%20beneficial%20when%20combined%20with%20other%20privacy-preserving%20techniques%20such%20as%20Differential%20Privacy.%20Our%20paper%20draws%20a%20set%20of%20lessons%20learned%20for%20devising%20decentralized%20learning%20systems%20that%20reduce%20by%20design%20the%20vulnerability%20to%20MIA.%0ALink%3A%20http%3A//arxiv.org/abs/2412.12837v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520the%2520Vulnerability%2520of%2520Decentralized%2520Learning%2520to%2520Membership%2520Inference%2520Attacks%2520Through%2520the%2520Lens%2520of%2520Graph%2520Mixing%26entry.906535625%3DOusmane%2520Touat%2520and%2520Jezekael%2520Brunon%2520and%2520Yacine%2520Belal%2520and%2520Julien%2520Nicolas%2520and%2520C%25C3%25A9sar%2520Sabater%2520and%2520Mohamed%2520Maouche%2520and%2520Sonia%2520Ben%2520Mokhtar%26entry.1292438233%3DThe%2520primary%2520promise%2520of%2520decentralized%2520learning%2520is%2520to%2520allow%2520users%2520to%2520engage%2520in%2520the%2520training%2520of%2520machine%2520learning%2520models%2520in%2520a%2520collaborative%2520manner%2520while%2520keeping%2520their%2520data%2520on%2520their%2520premises%2520and%2520without%2520relying%2520on%2520any%2520central%2520entity.%2520However%252C%2520this%2520paradigm%2520necessitates%2520the%2520exchange%2520of%2520model%2520parameters%2520or%2520gradients%2520between%2520peers.%2520Such%2520exchanges%2520can%2520be%2520exploited%2520to%2520infer%2520sensitive%2520information%2520about%2520training%2520data%252C%2520which%2520is%2520achieved%2520through%2520privacy%2520attacks%2520%2528e.g.%252C%2520Membership%2520Inference%2520Attacks%2520--%2520MIA%2529.%2520In%2520order%2520to%2520devise%2520effective%2520defense%2520mechanisms%252C%2520it%2520is%2520important%2520to%2520understand%2520the%2520factors%2520that%2520increase/reduce%2520the%2520vulnerability%2520of%2520a%2520given%2520decentralized%2520learning%2520architecture%2520to%2520MIA.%2520In%2520this%2520study%252C%2520we%2520extensively%2520explore%2520the%2520vulnerability%2520to%2520MIA%2520of%2520various%2520decentralized%2520learning%2520architectures%2520by%2520varying%2520the%2520graph%2520structure%2520%2528e.g.%252C%2520number%2520of%2520neighbors%2529%252C%2520the%2520graph%2520dynamics%252C%2520and%2520the%2520aggregation%2520strategy%252C%2520across%2520diverse%2520datasets%2520and%2520data%2520distributions.%2520Our%2520key%2520finding%252C%2520which%2520to%2520the%2520best%2520of%2520our%2520knowledge%2520we%2520are%2520the%2520first%2520to%2520report%252C%2520is%2520that%2520the%2520vulnerability%2520to%2520MIA%2520is%2520heavily%2520correlated%2520to%2520%2528i%2529%2520the%2520local%2520model%2520mixing%2520strategy%2520performed%2520by%2520each%2520node%2520upon%2520reception%2520of%2520models%2520from%2520neighboring%2520nodes%2520and%2520%2528ii%2529%2520the%2520global%2520mixing%2520properties%2520of%2520the%2520communication%2520graph.%2520We%2520illustrate%2520these%2520results%2520experimentally%2520using%2520four%2520datasets%2520and%2520by%2520theoretically%2520analyzing%2520the%2520mixing%2520properties%2520of%2520various%2520decentralized%2520architectures.%2520We%2520also%2520empirically%2520show%2520that%2520enhancing%2520mixing%2520properties%2520is%2520highly%2520beneficial%2520when%2520combined%2520with%2520other%2520privacy-preserving%2520techniques%2520such%2520as%2520Differential%2520Privacy.%2520Our%2520paper%2520draws%2520a%2520set%2520of%2520lessons%2520learned%2520for%2520devising%2520decentralized%2520learning%2520systems%2520that%2520reduce%2520by%2520design%2520the%2520vulnerability%2520to%2520MIA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12837v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20the%20Vulnerability%20of%20Decentralized%20Learning%20to%20Membership%20Inference%20Attacks%20Through%20the%20Lens%20of%20Graph%20Mixing&entry.906535625=Ousmane%20Touat%20and%20Jezekael%20Brunon%20and%20Yacine%20Belal%20and%20Julien%20Nicolas%20and%20C%C3%A9sar%20Sabater%20and%20Mohamed%20Maouche%20and%20Sonia%20Ben%20Mokhtar&entry.1292438233=The%20primary%20promise%20of%20decentralized%20learning%20is%20to%20allow%20users%20to%20engage%20in%20the%20training%20of%20machine%20learning%20models%20in%20a%20collaborative%20manner%20while%20keeping%20their%20data%20on%20their%20premises%20and%20without%20relying%20on%20any%20central%20entity.%20However%2C%20this%20paradigm%20necessitates%20the%20exchange%20of%20model%20parameters%20or%20gradients%20between%20peers.%20Such%20exchanges%20can%20be%20exploited%20to%20infer%20sensitive%20information%20about%20training%20data%2C%20which%20is%20achieved%20through%20privacy%20attacks%20%28e.g.%2C%20Membership%20Inference%20Attacks%20--%20MIA%29.%20In%20order%20to%20devise%20effective%20defense%20mechanisms%2C%20it%20is%20important%20to%20understand%20the%20factors%20that%20increase/reduce%20the%20vulnerability%20of%20a%20given%20decentralized%20learning%20architecture%20to%20MIA.%20In%20this%20study%2C%20we%20extensively%20explore%20the%20vulnerability%20to%20MIA%20of%20various%20decentralized%20learning%20architectures%20by%20varying%20the%20graph%20structure%20%28e.g.%2C%20number%20of%20neighbors%29%2C%20the%20graph%20dynamics%2C%20and%20the%20aggregation%20strategy%2C%20across%20diverse%20datasets%20and%20data%20distributions.%20Our%20key%20finding%2C%20which%20to%20the%20best%20of%20our%20knowledge%20we%20are%20the%20first%20to%20report%2C%20is%20that%20the%20vulnerability%20to%20MIA%20is%20heavily%20correlated%20to%20%28i%29%20the%20local%20model%20mixing%20strategy%20performed%20by%20each%20node%20upon%20reception%20of%20models%20from%20neighboring%20nodes%20and%20%28ii%29%20the%20global%20mixing%20properties%20of%20the%20communication%20graph.%20We%20illustrate%20these%20results%20experimentally%20using%20four%20datasets%20and%20by%20theoretically%20analyzing%20the%20mixing%20properties%20of%20various%20decentralized%20architectures.%20We%20also%20empirically%20show%20that%20enhancing%20mixing%20properties%20is%20highly%20beneficial%20when%20combined%20with%20other%20privacy-preserving%20techniques%20such%20as%20Differential%20Privacy.%20Our%20paper%20draws%20a%20set%20of%20lessons%20learned%20for%20devising%20decentralized%20learning%20systems%20that%20reduce%20by%20design%20the%20vulnerability%20to%20MIA.&entry.1838667208=http%3A//arxiv.org/abs/2412.12837v4&entry.124074799=Read"},
{"title": "HD$^2$-SSC: High-Dimension High-Density Semantic Scene Completion for Autonomous Driving", "author": "Zhiwen Yang and Yuxin Peng", "abstract": "Camera-based 3D semantic scene completion (SSC) plays a crucial role in autonomous driving, enabling voxelized 3D scene understanding for effective scene perception and decision-making. Existing SSC methods have shown efficacy in improving 3D scene representations, but suffer from the inherent input-output dimension gap and annotation-reality density gap, where the 2D planner view from input images with sparse annotated labels leads to inferior prediction of real-world dense occupancy with a 3D stereoscopic view. In light of this, we propose the corresponding High-Dimension High-Density Semantic Scene Completion (HD$^2$-SSC) framework with expanded pixel semantics and refined voxel occupancies. To bridge the dimension gap, a High-dimension Semantic Decoupling module is designed to expand 2D image features along a pseudo third dimension, decoupling coarse pixel semantics from occlusions, and then identify focal regions with fine semantics to enrich image features. To mitigate the density gap, a High-density Occupancy Refinement module is devised with a \"detect-and-refine\" architecture to leverage contextual geometric and semantic structures for enhanced semantic density with the completion of missing voxels and correction of erroneous ones. Extensive experiments and analyses on the SemanticKITTI and SSCBench-KITTI-360 datasets validate the effectiveness of our HD$^2$-SSC framework.", "link": "http://arxiv.org/abs/2511.07925v2", "date": "2025-11-13", "relevancy": 2.2689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HD%24%5E2%24-SSC%3A%20High-Dimension%20High-Density%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving&body=Title%3A%20HD%24%5E2%24-SSC%3A%20High-Dimension%20High-Density%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving%0AAuthor%3A%20Zhiwen%20Yang%20and%20Yuxin%20Peng%0AAbstract%3A%20Camera-based%203D%20semantic%20scene%20completion%20%28SSC%29%20plays%20a%20crucial%20role%20in%20autonomous%20driving%2C%20enabling%20voxelized%203D%20scene%20understanding%20for%20effective%20scene%20perception%20and%20decision-making.%20Existing%20SSC%20methods%20have%20shown%20efficacy%20in%20improving%203D%20scene%20representations%2C%20but%20suffer%20from%20the%20inherent%20input-output%20dimension%20gap%20and%20annotation-reality%20density%20gap%2C%20where%20the%202D%20planner%20view%20from%20input%20images%20with%20sparse%20annotated%20labels%20leads%20to%20inferior%20prediction%20of%20real-world%20dense%20occupancy%20with%20a%203D%20stereoscopic%20view.%20In%20light%20of%20this%2C%20we%20propose%20the%20corresponding%20High-Dimension%20High-Density%20Semantic%20Scene%20Completion%20%28HD%24%5E2%24-SSC%29%20framework%20with%20expanded%20pixel%20semantics%20and%20refined%20voxel%20occupancies.%20To%20bridge%20the%20dimension%20gap%2C%20a%20High-dimension%20Semantic%20Decoupling%20module%20is%20designed%20to%20expand%202D%20image%20features%20along%20a%20pseudo%20third%20dimension%2C%20decoupling%20coarse%20pixel%20semantics%20from%20occlusions%2C%20and%20then%20identify%20focal%20regions%20with%20fine%20semantics%20to%20enrich%20image%20features.%20To%20mitigate%20the%20density%20gap%2C%20a%20High-density%20Occupancy%20Refinement%20module%20is%20devised%20with%20a%20%22detect-and-refine%22%20architecture%20to%20leverage%20contextual%20geometric%20and%20semantic%20structures%20for%20enhanced%20semantic%20density%20with%20the%20completion%20of%20missing%20voxels%20and%20correction%20of%20erroneous%20ones.%20Extensive%20experiments%20and%20analyses%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI-360%20datasets%20validate%20the%20effectiveness%20of%20our%20HD%24%5E2%24-SSC%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHD%2524%255E2%2524-SSC%253A%2520High-Dimension%2520High-Density%2520Semantic%2520Scene%2520Completion%2520for%2520Autonomous%2520Driving%26entry.906535625%3DZhiwen%2520Yang%2520and%2520Yuxin%2520Peng%26entry.1292438233%3DCamera-based%25203D%2520semantic%2520scene%2520completion%2520%2528SSC%2529%2520plays%2520a%2520crucial%2520role%2520in%2520autonomous%2520driving%252C%2520enabling%2520voxelized%25203D%2520scene%2520understanding%2520for%2520effective%2520scene%2520perception%2520and%2520decision-making.%2520Existing%2520SSC%2520methods%2520have%2520shown%2520efficacy%2520in%2520improving%25203D%2520scene%2520representations%252C%2520but%2520suffer%2520from%2520the%2520inherent%2520input-output%2520dimension%2520gap%2520and%2520annotation-reality%2520density%2520gap%252C%2520where%2520the%25202D%2520planner%2520view%2520from%2520input%2520images%2520with%2520sparse%2520annotated%2520labels%2520leads%2520to%2520inferior%2520prediction%2520of%2520real-world%2520dense%2520occupancy%2520with%2520a%25203D%2520stereoscopic%2520view.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520the%2520corresponding%2520High-Dimension%2520High-Density%2520Semantic%2520Scene%2520Completion%2520%2528HD%2524%255E2%2524-SSC%2529%2520framework%2520with%2520expanded%2520pixel%2520semantics%2520and%2520refined%2520voxel%2520occupancies.%2520To%2520bridge%2520the%2520dimension%2520gap%252C%2520a%2520High-dimension%2520Semantic%2520Decoupling%2520module%2520is%2520designed%2520to%2520expand%25202D%2520image%2520features%2520along%2520a%2520pseudo%2520third%2520dimension%252C%2520decoupling%2520coarse%2520pixel%2520semantics%2520from%2520occlusions%252C%2520and%2520then%2520identify%2520focal%2520regions%2520with%2520fine%2520semantics%2520to%2520enrich%2520image%2520features.%2520To%2520mitigate%2520the%2520density%2520gap%252C%2520a%2520High-density%2520Occupancy%2520Refinement%2520module%2520is%2520devised%2520with%2520a%2520%2522detect-and-refine%2522%2520architecture%2520to%2520leverage%2520contextual%2520geometric%2520and%2520semantic%2520structures%2520for%2520enhanced%2520semantic%2520density%2520with%2520the%2520completion%2520of%2520missing%2520voxels%2520and%2520correction%2520of%2520erroneous%2520ones.%2520Extensive%2520experiments%2520and%2520analyses%2520on%2520the%2520SemanticKITTI%2520and%2520SSCBench-KITTI-360%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520HD%2524%255E2%2524-SSC%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HD%24%5E2%24-SSC%3A%20High-Dimension%20High-Density%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving&entry.906535625=Zhiwen%20Yang%20and%20Yuxin%20Peng&entry.1292438233=Camera-based%203D%20semantic%20scene%20completion%20%28SSC%29%20plays%20a%20crucial%20role%20in%20autonomous%20driving%2C%20enabling%20voxelized%203D%20scene%20understanding%20for%20effective%20scene%20perception%20and%20decision-making.%20Existing%20SSC%20methods%20have%20shown%20efficacy%20in%20improving%203D%20scene%20representations%2C%20but%20suffer%20from%20the%20inherent%20input-output%20dimension%20gap%20and%20annotation-reality%20density%20gap%2C%20where%20the%202D%20planner%20view%20from%20input%20images%20with%20sparse%20annotated%20labels%20leads%20to%20inferior%20prediction%20of%20real-world%20dense%20occupancy%20with%20a%203D%20stereoscopic%20view.%20In%20light%20of%20this%2C%20we%20propose%20the%20corresponding%20High-Dimension%20High-Density%20Semantic%20Scene%20Completion%20%28HD%24%5E2%24-SSC%29%20framework%20with%20expanded%20pixel%20semantics%20and%20refined%20voxel%20occupancies.%20To%20bridge%20the%20dimension%20gap%2C%20a%20High-dimension%20Semantic%20Decoupling%20module%20is%20designed%20to%20expand%202D%20image%20features%20along%20a%20pseudo%20third%20dimension%2C%20decoupling%20coarse%20pixel%20semantics%20from%20occlusions%2C%20and%20then%20identify%20focal%20regions%20with%20fine%20semantics%20to%20enrich%20image%20features.%20To%20mitigate%20the%20density%20gap%2C%20a%20High-density%20Occupancy%20Refinement%20module%20is%20devised%20with%20a%20%22detect-and-refine%22%20architecture%20to%20leverage%20contextual%20geometric%20and%20semantic%20structures%20for%20enhanced%20semantic%20density%20with%20the%20completion%20of%20missing%20voxels%20and%20correction%20of%20erroneous%20ones.%20Extensive%20experiments%20and%20analyses%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI-360%20datasets%20validate%20the%20effectiveness%20of%20our%20HD%24%5E2%24-SSC%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2511.07925v2&entry.124074799=Read"},
{"title": "LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts", "author": "Qifeng Cai and Hao Liang and Hejun Dong and Meiyi Qiang and Ruichuan An and Zhaoyang Han and Zhengzhou Zhu and Bin Cui and Wentao Zhang", "abstract": "Long videos contain a vast amount of information, making video-text retrieval an essential and challenging task in multimodal learning. However, existing benchmarks suffer from limited video duration, low-quality captions, and coarse annotation granularity, which hinder the evaluation of advanced video-text retrieval methods. To address these limitations, we introduce LoVR, a benchmark specifically designed for long video-text retrieval. LoVR contains 467 long videos and over 40,804 fine-grained clips with high-quality captions. To overcome the issue of poor machine-generated annotations, we propose an efficient caption generation framework that integrates VLM automatic generation, caption quality scoring, and dynamic refinement. This pipeline improves annotation accuracy while maintaining scalability. Furthermore, we introduce a semantic fusion method to generate coherent full-video captions without losing important contextual information. Our benchmark introduces longer videos, more detailed captions, and a larger-scale dataset, presenting new challenges for video understanding and retrieval. Extensive experiments on various advanced embedding models demonstrate that LoVR is a challenging benchmark, revealing the limitations of current approaches and providing valuable insights for future research. We release the code and dataset link at https://github.com/TechNomad-ds/LoVR-benchmark", "link": "http://arxiv.org/abs/2505.13928v3", "date": "2025-11-13", "relevancy": 2.2651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoVR%3A%20A%20Benchmark%20for%20Long%20Video%20Retrieval%20in%20Multimodal%20Contexts&body=Title%3A%20LoVR%3A%20A%20Benchmark%20for%20Long%20Video%20Retrieval%20in%20Multimodal%20Contexts%0AAuthor%3A%20Qifeng%20Cai%20and%20Hao%20Liang%20and%20Hejun%20Dong%20and%20Meiyi%20Qiang%20and%20Ruichuan%20An%20and%20Zhaoyang%20Han%20and%20Zhengzhou%20Zhu%20and%20Bin%20Cui%20and%20Wentao%20Zhang%0AAbstract%3A%20Long%20videos%20contain%20a%20vast%20amount%20of%20information%2C%20making%20video-text%20retrieval%20an%20essential%20and%20challenging%20task%20in%20multimodal%20learning.%20However%2C%20existing%20benchmarks%20suffer%20from%20limited%20video%20duration%2C%20low-quality%20captions%2C%20and%20coarse%20annotation%20granularity%2C%20which%20hinder%20the%20evaluation%20of%20advanced%20video-text%20retrieval%20methods.%20To%20address%20these%20limitations%2C%20we%20introduce%20LoVR%2C%20a%20benchmark%20specifically%20designed%20for%20long%20video-text%20retrieval.%20LoVR%20contains%20467%20long%20videos%20and%20over%2040%2C804%20fine-grained%20clips%20with%20high-quality%20captions.%20To%20overcome%20the%20issue%20of%20poor%20machine-generated%20annotations%2C%20we%20propose%20an%20efficient%20caption%20generation%20framework%20that%20integrates%20VLM%20automatic%20generation%2C%20caption%20quality%20scoring%2C%20and%20dynamic%20refinement.%20This%20pipeline%20improves%20annotation%20accuracy%20while%20maintaining%20scalability.%20Furthermore%2C%20we%20introduce%20a%20semantic%20fusion%20method%20to%20generate%20coherent%20full-video%20captions%20without%20losing%20important%20contextual%20information.%20Our%20benchmark%20introduces%20longer%20videos%2C%20more%20detailed%20captions%2C%20and%20a%20larger-scale%20dataset%2C%20presenting%20new%20challenges%20for%20video%20understanding%20and%20retrieval.%20Extensive%20experiments%20on%20various%20advanced%20embedding%20models%20demonstrate%20that%20LoVR%20is%20a%20challenging%20benchmark%2C%20revealing%20the%20limitations%20of%20current%20approaches%20and%20providing%20valuable%20insights%20for%20future%20research.%20We%20release%20the%20code%20and%20dataset%20link%20at%20https%3A//github.com/TechNomad-ds/LoVR-benchmark%0ALink%3A%20http%3A//arxiv.org/abs/2505.13928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoVR%253A%2520A%2520Benchmark%2520for%2520Long%2520Video%2520Retrieval%2520in%2520Multimodal%2520Contexts%26entry.906535625%3DQifeng%2520Cai%2520and%2520Hao%2520Liang%2520and%2520Hejun%2520Dong%2520and%2520Meiyi%2520Qiang%2520and%2520Ruichuan%2520An%2520and%2520Zhaoyang%2520Han%2520and%2520Zhengzhou%2520Zhu%2520and%2520Bin%2520Cui%2520and%2520Wentao%2520Zhang%26entry.1292438233%3DLong%2520videos%2520contain%2520a%2520vast%2520amount%2520of%2520information%252C%2520making%2520video-text%2520retrieval%2520an%2520essential%2520and%2520challenging%2520task%2520in%2520multimodal%2520learning.%2520However%252C%2520existing%2520benchmarks%2520suffer%2520from%2520limited%2520video%2520duration%252C%2520low-quality%2520captions%252C%2520and%2520coarse%2520annotation%2520granularity%252C%2520which%2520hinder%2520the%2520evaluation%2520of%2520advanced%2520video-text%2520retrieval%2520methods.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520LoVR%252C%2520a%2520benchmark%2520specifically%2520designed%2520for%2520long%2520video-text%2520retrieval.%2520LoVR%2520contains%2520467%2520long%2520videos%2520and%2520over%252040%252C804%2520fine-grained%2520clips%2520with%2520high-quality%2520captions.%2520To%2520overcome%2520the%2520issue%2520of%2520poor%2520machine-generated%2520annotations%252C%2520we%2520propose%2520an%2520efficient%2520caption%2520generation%2520framework%2520that%2520integrates%2520VLM%2520automatic%2520generation%252C%2520caption%2520quality%2520scoring%252C%2520and%2520dynamic%2520refinement.%2520This%2520pipeline%2520improves%2520annotation%2520accuracy%2520while%2520maintaining%2520scalability.%2520Furthermore%252C%2520we%2520introduce%2520a%2520semantic%2520fusion%2520method%2520to%2520generate%2520coherent%2520full-video%2520captions%2520without%2520losing%2520important%2520contextual%2520information.%2520Our%2520benchmark%2520introduces%2520longer%2520videos%252C%2520more%2520detailed%2520captions%252C%2520and%2520a%2520larger-scale%2520dataset%252C%2520presenting%2520new%2520challenges%2520for%2520video%2520understanding%2520and%2520retrieval.%2520Extensive%2520experiments%2520on%2520various%2520advanced%2520embedding%2520models%2520demonstrate%2520that%2520LoVR%2520is%2520a%2520challenging%2520benchmark%252C%2520revealing%2520the%2520limitations%2520of%2520current%2520approaches%2520and%2520providing%2520valuable%2520insights%2520for%2520future%2520research.%2520We%2520release%2520the%2520code%2520and%2520dataset%2520link%2520at%2520https%253A//github.com/TechNomad-ds/LoVR-benchmark%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoVR%3A%20A%20Benchmark%20for%20Long%20Video%20Retrieval%20in%20Multimodal%20Contexts&entry.906535625=Qifeng%20Cai%20and%20Hao%20Liang%20and%20Hejun%20Dong%20and%20Meiyi%20Qiang%20and%20Ruichuan%20An%20and%20Zhaoyang%20Han%20and%20Zhengzhou%20Zhu%20and%20Bin%20Cui%20and%20Wentao%20Zhang&entry.1292438233=Long%20videos%20contain%20a%20vast%20amount%20of%20information%2C%20making%20video-text%20retrieval%20an%20essential%20and%20challenging%20task%20in%20multimodal%20learning.%20However%2C%20existing%20benchmarks%20suffer%20from%20limited%20video%20duration%2C%20low-quality%20captions%2C%20and%20coarse%20annotation%20granularity%2C%20which%20hinder%20the%20evaluation%20of%20advanced%20video-text%20retrieval%20methods.%20To%20address%20these%20limitations%2C%20we%20introduce%20LoVR%2C%20a%20benchmark%20specifically%20designed%20for%20long%20video-text%20retrieval.%20LoVR%20contains%20467%20long%20videos%20and%20over%2040%2C804%20fine-grained%20clips%20with%20high-quality%20captions.%20To%20overcome%20the%20issue%20of%20poor%20machine-generated%20annotations%2C%20we%20propose%20an%20efficient%20caption%20generation%20framework%20that%20integrates%20VLM%20automatic%20generation%2C%20caption%20quality%20scoring%2C%20and%20dynamic%20refinement.%20This%20pipeline%20improves%20annotation%20accuracy%20while%20maintaining%20scalability.%20Furthermore%2C%20we%20introduce%20a%20semantic%20fusion%20method%20to%20generate%20coherent%20full-video%20captions%20without%20losing%20important%20contextual%20information.%20Our%20benchmark%20introduces%20longer%20videos%2C%20more%20detailed%20captions%2C%20and%20a%20larger-scale%20dataset%2C%20presenting%20new%20challenges%20for%20video%20understanding%20and%20retrieval.%20Extensive%20experiments%20on%20various%20advanced%20embedding%20models%20demonstrate%20that%20LoVR%20is%20a%20challenging%20benchmark%2C%20revealing%20the%20limitations%20of%20current%20approaches%20and%20providing%20valuable%20insights%20for%20future%20research.%20We%20release%20the%20code%20and%20dataset%20link%20at%20https%3A//github.com/TechNomad-ds/LoVR-benchmark&entry.1838667208=http%3A//arxiv.org/abs/2505.13928v3&entry.124074799=Read"},
{"title": "Persona-Aware Alignment Framework for Personalized Dialogue Generation", "author": "Guanrong Li and Xinyu Liu and Zhen Wu and Xinyu Dai", "abstract": "Personalized dialogue generation aims to leverage persona profiles and dialogue history to generate persona-relevant and consistent responses. Mainstream models typically rely on token-level language model training with persona dialogue data, such as Next Token Prediction, to implicitly achieve personalization, making these methods tend to neglect the given personas and generate generic responses. To address this issue, we propose a novel Persona-Aware Alignment Framework (PAL), which directly treats persona alignment as the training objective of dialogue generation. Specifically, PAL employs a two-stage training method including Persona-aware Learning and Persona Alignment, equipped with an easy-to-use inference strategy Select then Generate, to improve persona sensitivity and generate more persona-relevant responses at the semantics level. Through extensive experiments, we demonstrate that our framework outperforms many state-of-the-art personalized dialogue methods and large language models.", "link": "http://arxiv.org/abs/2511.10215v1", "date": "2025-11-13", "relevancy": 2.2643, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4488}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persona-Aware%20Alignment%20Framework%20for%20Personalized%20Dialogue%20Generation&body=Title%3A%20Persona-Aware%20Alignment%20Framework%20for%20Personalized%20Dialogue%20Generation%0AAuthor%3A%20Guanrong%20Li%20and%20Xinyu%20Liu%20and%20Zhen%20Wu%20and%20Xinyu%20Dai%0AAbstract%3A%20Personalized%20dialogue%20generation%20aims%20to%20leverage%20persona%20profiles%20and%20dialogue%20history%20to%20generate%20persona-relevant%20and%20consistent%20responses.%20Mainstream%20models%20typically%20rely%20on%20token-level%20language%20model%20training%20with%20persona%20dialogue%20data%2C%20such%20as%20Next%20Token%20Prediction%2C%20to%20implicitly%20achieve%20personalization%2C%20making%20these%20methods%20tend%20to%20neglect%20the%20given%20personas%20and%20generate%20generic%20responses.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20Persona-Aware%20Alignment%20Framework%20%28PAL%29%2C%20which%20directly%20treats%20persona%20alignment%20as%20the%20training%20objective%20of%20dialogue%20generation.%20Specifically%2C%20PAL%20employs%20a%20two-stage%20training%20method%20including%20Persona-aware%20Learning%20and%20Persona%20Alignment%2C%20equipped%20with%20an%20easy-to-use%20inference%20strategy%20Select%20then%20Generate%2C%20to%20improve%20persona%20sensitivity%20and%20generate%20more%20persona-relevant%20responses%20at%20the%20semantics%20level.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20framework%20outperforms%20many%20state-of-the-art%20personalized%20dialogue%20methods%20and%20large%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersona-Aware%2520Alignment%2520Framework%2520for%2520Personalized%2520Dialogue%2520Generation%26entry.906535625%3DGuanrong%2520Li%2520and%2520Xinyu%2520Liu%2520and%2520Zhen%2520Wu%2520and%2520Xinyu%2520Dai%26entry.1292438233%3DPersonalized%2520dialogue%2520generation%2520aims%2520to%2520leverage%2520persona%2520profiles%2520and%2520dialogue%2520history%2520to%2520generate%2520persona-relevant%2520and%2520consistent%2520responses.%2520Mainstream%2520models%2520typically%2520rely%2520on%2520token-level%2520language%2520model%2520training%2520with%2520persona%2520dialogue%2520data%252C%2520such%2520as%2520Next%2520Token%2520Prediction%252C%2520to%2520implicitly%2520achieve%2520personalization%252C%2520making%2520these%2520methods%2520tend%2520to%2520neglect%2520the%2520given%2520personas%2520and%2520generate%2520generic%2520responses.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520Persona-Aware%2520Alignment%2520Framework%2520%2528PAL%2529%252C%2520which%2520directly%2520treats%2520persona%2520alignment%2520as%2520the%2520training%2520objective%2520of%2520dialogue%2520generation.%2520Specifically%252C%2520PAL%2520employs%2520a%2520two-stage%2520training%2520method%2520including%2520Persona-aware%2520Learning%2520and%2520Persona%2520Alignment%252C%2520equipped%2520with%2520an%2520easy-to-use%2520inference%2520strategy%2520Select%2520then%2520Generate%252C%2520to%2520improve%2520persona%2520sensitivity%2520and%2520generate%2520more%2520persona-relevant%2520responses%2520at%2520the%2520semantics%2520level.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520many%2520state-of-the-art%2520personalized%2520dialogue%2520methods%2520and%2520large%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persona-Aware%20Alignment%20Framework%20for%20Personalized%20Dialogue%20Generation&entry.906535625=Guanrong%20Li%20and%20Xinyu%20Liu%20and%20Zhen%20Wu%20and%20Xinyu%20Dai&entry.1292438233=Personalized%20dialogue%20generation%20aims%20to%20leverage%20persona%20profiles%20and%20dialogue%20history%20to%20generate%20persona-relevant%20and%20consistent%20responses.%20Mainstream%20models%20typically%20rely%20on%20token-level%20language%20model%20training%20with%20persona%20dialogue%20data%2C%20such%20as%20Next%20Token%20Prediction%2C%20to%20implicitly%20achieve%20personalization%2C%20making%20these%20methods%20tend%20to%20neglect%20the%20given%20personas%20and%20generate%20generic%20responses.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20Persona-Aware%20Alignment%20Framework%20%28PAL%29%2C%20which%20directly%20treats%20persona%20alignment%20as%20the%20training%20objective%20of%20dialogue%20generation.%20Specifically%2C%20PAL%20employs%20a%20two-stage%20training%20method%20including%20Persona-aware%20Learning%20and%20Persona%20Alignment%2C%20equipped%20with%20an%20easy-to-use%20inference%20strategy%20Select%20then%20Generate%2C%20to%20improve%20persona%20sensitivity%20and%20generate%20more%20persona-relevant%20responses%20at%20the%20semantics%20level.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20framework%20outperforms%20many%20state-of-the-art%20personalized%20dialogue%20methods%20and%20large%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.10215v1&entry.124074799=Read"},
{"title": "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning", "author": "Yanbei Jiang and Chao Lei and Yihao Ding and Krista Ehinger and Jey Han Lau", "abstract": "Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.", "link": "http://arxiv.org/abs/2511.10279v1", "date": "2025-11-13", "relevancy": 2.2635, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROPA%3A%20Toward%20Process-level%20Optimization%20in%20Visual%20Reasoning%20via%20Reinforcement%20Learning&body=Title%3A%20PROPA%3A%20Toward%20Process-level%20Optimization%20in%20Visual%20Reasoning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Yanbei%20Jiang%20and%20Chao%20Lei%20and%20Yihao%20Ding%20and%20Krista%20Ehinger%20and%20Jey%20Han%20Lau%0AAbstract%3A%20Despite%20significant%20progress%2C%20Vision-Language%20Models%20%28VLMs%29%20still%20struggle%20with%20complex%20visual%20reasoning%2C%20where%20multi-step%20dependencies%20cause%20early%20errors%20to%20cascade%20through%20the%20reasoning%20chain.%20Existing%20post-training%20paradigms%20are%20limited%3A%20Supervised%20Fine-Tuning%20%28SFT%29%20relies%20on%20costly%20step-level%20annotations%2C%20while%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20methods%20like%20GRPO%20provide%20only%20sparse%2C%20outcome-level%20feedback%2C%20hindering%20stable%20optimization.%20We%20introduce%20PROPA%20%28Process-level%20Reasoning%20Optimization%20with%20interleaved%20Policy%20Alignment%29%2C%20a%20novel%20framework%20that%20integrates%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20with%20GRPO%20to%20generate%20dense%2C%20process-level%20rewards%20and%20optimize%20reasoning%20at%20each%20intermediate%20step%20without%20human%20annotations.%20To%20overcome%20the%20cold-start%20problem%2C%20PROPA%20interleaves%20GRPO%20updates%20with%20SFT%2C%20enabling%20the%20model%20to%20learn%20from%20both%20successful%20and%20failed%20reasoning%20trajectories.%20A%20Process%20Reward%20Model%20%28PRM%29%20is%20further%20trained%20to%20guide%20inference-time%20search%2C%20aligning%20the%20test-time%20search%20with%20the%20training%20signal.%20Across%20seven%20benchmarks%20and%20four%20VLM%20backbones%2C%20PROPA%20consistently%20outperforms%20both%20SFT-%20and%20RLVR-based%20baselines.%20It%20achieves%20up%20to%2017.0%25%20gains%20on%20in-domain%20tasks%20and%2021.0%25%20gains%20on%20out-of-domain%20tasks%20compared%20to%20existing%20state-of-the-art%2C%20establishing%20a%20strong%20reasoning%20and%20generalization%20capability%20for%20visual%20reasoning%20tasks.%20The%20code%20isavailable%20at%3A%20https%3A//github.com/YanbeiJiang/PROPA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROPA%253A%2520Toward%2520Process-level%2520Optimization%2520in%2520Visual%2520Reasoning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DYanbei%2520Jiang%2520and%2520Chao%2520Lei%2520and%2520Yihao%2520Ding%2520and%2520Krista%2520Ehinger%2520and%2520Jey%2520Han%2520Lau%26entry.1292438233%3DDespite%2520significant%2520progress%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520still%2520struggle%2520with%2520complex%2520visual%2520reasoning%252C%2520where%2520multi-step%2520dependencies%2520cause%2520early%2520errors%2520to%2520cascade%2520through%2520the%2520reasoning%2520chain.%2520Existing%2520post-training%2520paradigms%2520are%2520limited%253A%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520relies%2520on%2520costly%2520step-level%2520annotations%252C%2520while%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520methods%2520like%2520GRPO%2520provide%2520only%2520sparse%252C%2520outcome-level%2520feedback%252C%2520hindering%2520stable%2520optimization.%2520We%2520introduce%2520PROPA%2520%2528Process-level%2520Reasoning%2520Optimization%2520with%2520interleaved%2520Policy%2520Alignment%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%2520with%2520GRPO%2520to%2520generate%2520dense%252C%2520process-level%2520rewards%2520and%2520optimize%2520reasoning%2520at%2520each%2520intermediate%2520step%2520without%2520human%2520annotations.%2520To%2520overcome%2520the%2520cold-start%2520problem%252C%2520PROPA%2520interleaves%2520GRPO%2520updates%2520with%2520SFT%252C%2520enabling%2520the%2520model%2520to%2520learn%2520from%2520both%2520successful%2520and%2520failed%2520reasoning%2520trajectories.%2520A%2520Process%2520Reward%2520Model%2520%2528PRM%2529%2520is%2520further%2520trained%2520to%2520guide%2520inference-time%2520search%252C%2520aligning%2520the%2520test-time%2520search%2520with%2520the%2520training%2520signal.%2520Across%2520seven%2520benchmarks%2520and%2520four%2520VLM%2520backbones%252C%2520PROPA%2520consistently%2520outperforms%2520both%2520SFT-%2520and%2520RLVR-based%2520baselines.%2520It%2520achieves%2520up%2520to%252017.0%2525%2520gains%2520on%2520in-domain%2520tasks%2520and%252021.0%2525%2520gains%2520on%2520out-of-domain%2520tasks%2520compared%2520to%2520existing%2520state-of-the-art%252C%2520establishing%2520a%2520strong%2520reasoning%2520and%2520generalization%2520capability%2520for%2520visual%2520reasoning%2520tasks.%2520The%2520code%2520isavailable%2520at%253A%2520https%253A//github.com/YanbeiJiang/PROPA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROPA%3A%20Toward%20Process-level%20Optimization%20in%20Visual%20Reasoning%20via%20Reinforcement%20Learning&entry.906535625=Yanbei%20Jiang%20and%20Chao%20Lei%20and%20Yihao%20Ding%20and%20Krista%20Ehinger%20and%20Jey%20Han%20Lau&entry.1292438233=Despite%20significant%20progress%2C%20Vision-Language%20Models%20%28VLMs%29%20still%20struggle%20with%20complex%20visual%20reasoning%2C%20where%20multi-step%20dependencies%20cause%20early%20errors%20to%20cascade%20through%20the%20reasoning%20chain.%20Existing%20post-training%20paradigms%20are%20limited%3A%20Supervised%20Fine-Tuning%20%28SFT%29%20relies%20on%20costly%20step-level%20annotations%2C%20while%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20methods%20like%20GRPO%20provide%20only%20sparse%2C%20outcome-level%20feedback%2C%20hindering%20stable%20optimization.%20We%20introduce%20PROPA%20%28Process-level%20Reasoning%20Optimization%20with%20interleaved%20Policy%20Alignment%29%2C%20a%20novel%20framework%20that%20integrates%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20with%20GRPO%20to%20generate%20dense%2C%20process-level%20rewards%20and%20optimize%20reasoning%20at%20each%20intermediate%20step%20without%20human%20annotations.%20To%20overcome%20the%20cold-start%20problem%2C%20PROPA%20interleaves%20GRPO%20updates%20with%20SFT%2C%20enabling%20the%20model%20to%20learn%20from%20both%20successful%20and%20failed%20reasoning%20trajectories.%20A%20Process%20Reward%20Model%20%28PRM%29%20is%20further%20trained%20to%20guide%20inference-time%20search%2C%20aligning%20the%20test-time%20search%20with%20the%20training%20signal.%20Across%20seven%20benchmarks%20and%20four%20VLM%20backbones%2C%20PROPA%20consistently%20outperforms%20both%20SFT-%20and%20RLVR-based%20baselines.%20It%20achieves%20up%20to%2017.0%25%20gains%20on%20in-domain%20tasks%20and%2021.0%25%20gains%20on%20out-of-domain%20tasks%20compared%20to%20existing%20state-of-the-art%2C%20establishing%20a%20strong%20reasoning%20and%20generalization%20capability%20for%20visual%20reasoning%20tasks.%20The%20code%20isavailable%20at%3A%20https%3A//github.com/YanbeiJiang/PROPA.&entry.1838667208=http%3A//arxiv.org/abs/2511.10279v1&entry.124074799=Read"},
{"title": "Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance", "author": "\u00c7a\u011fr\u0131 Eser and Zeynep Sonat Baltac\u0131 and Emre Akba\u015f and Sinan Kalkan", "abstract": "Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.", "link": "http://arxiv.org/abs/2511.10475v1", "date": "2025-11-13", "relevancy": 2.2621, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4733}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4422}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Dimensionality%20as%20a%20Model-Free%20Measure%20of%20Class%20Imbalance&body=Title%3A%20Intrinsic%20Dimensionality%20as%20a%20Model-Free%20Measure%20of%20Class%20Imbalance%0AAuthor%3A%20%C3%87a%C4%9Fr%C4%B1%20Eser%20and%20Zeynep%20Sonat%20Baltac%C4%B1%20and%20Emre%20Akba%C5%9F%20and%20Sinan%20Kalkan%0AAbstract%3A%20Imbalance%20in%20classification%20tasks%20is%20commonly%20quantified%20by%20the%20cardinalities%20of%20examples%20across%20classes.%20This%2C%20however%2C%20disregards%20the%20presence%20of%20redundant%20examples%20and%20inherent%20differences%20in%20the%20learning%20difficulties%20of%20classes.%20Alternatively%2C%20one%20can%20use%20complex%20measures%20such%20as%20training%20loss%20and%20uncertainty%2C%20which%2C%20however%2C%20depend%20on%20training%20a%20machine%20learning%20model.%20Our%20paper%20proposes%20using%20data%20Intrinsic%20Dimensionality%20%28ID%29%20as%20an%20easy-to-compute%2C%20model-free%20measure%20of%20imbalance%20that%20can%20be%20seamlessly%20incorporated%20into%20various%20imbalance%20mitigation%20methods.%20Our%20results%20across%20five%20different%20datasets%20with%20a%20diverse%20range%20of%20imbalance%20ratios%20show%20that%20ID%20consistently%20outperforms%20cardinality-based%20re-weighting%20and%20re-sampling%20techniques%20used%20in%20the%20literature.%20Moreover%2C%20we%20show%20that%20combining%20ID%20with%20cardinality%20can%20further%20improve%20performance.%20Code%3A%20https%3A//github.com/cagries/IDIM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Dimensionality%2520as%2520a%2520Model-Free%2520Measure%2520of%2520Class%2520Imbalance%26entry.906535625%3D%25C3%2587a%25C4%259Fr%25C4%25B1%2520Eser%2520and%2520Zeynep%2520Sonat%2520Baltac%25C4%25B1%2520and%2520Emre%2520Akba%25C5%259F%2520and%2520Sinan%2520Kalkan%26entry.1292438233%3DImbalance%2520in%2520classification%2520tasks%2520is%2520commonly%2520quantified%2520by%2520the%2520cardinalities%2520of%2520examples%2520across%2520classes.%2520This%252C%2520however%252C%2520disregards%2520the%2520presence%2520of%2520redundant%2520examples%2520and%2520inherent%2520differences%2520in%2520the%2520learning%2520difficulties%2520of%2520classes.%2520Alternatively%252C%2520one%2520can%2520use%2520complex%2520measures%2520such%2520as%2520training%2520loss%2520and%2520uncertainty%252C%2520which%252C%2520however%252C%2520depend%2520on%2520training%2520a%2520machine%2520learning%2520model.%2520Our%2520paper%2520proposes%2520using%2520data%2520Intrinsic%2520Dimensionality%2520%2528ID%2529%2520as%2520an%2520easy-to-compute%252C%2520model-free%2520measure%2520of%2520imbalance%2520that%2520can%2520be%2520seamlessly%2520incorporated%2520into%2520various%2520imbalance%2520mitigation%2520methods.%2520Our%2520results%2520across%2520five%2520different%2520datasets%2520with%2520a%2520diverse%2520range%2520of%2520imbalance%2520ratios%2520show%2520that%2520ID%2520consistently%2520outperforms%2520cardinality-based%2520re-weighting%2520and%2520re-sampling%2520techniques%2520used%2520in%2520the%2520literature.%2520Moreover%252C%2520we%2520show%2520that%2520combining%2520ID%2520with%2520cardinality%2520can%2520further%2520improve%2520performance.%2520Code%253A%2520https%253A//github.com/cagries/IDIM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Dimensionality%20as%20a%20Model-Free%20Measure%20of%20Class%20Imbalance&entry.906535625=%C3%87a%C4%9Fr%C4%B1%20Eser%20and%20Zeynep%20Sonat%20Baltac%C4%B1%20and%20Emre%20Akba%C5%9F%20and%20Sinan%20Kalkan&entry.1292438233=Imbalance%20in%20classification%20tasks%20is%20commonly%20quantified%20by%20the%20cardinalities%20of%20examples%20across%20classes.%20This%2C%20however%2C%20disregards%20the%20presence%20of%20redundant%20examples%20and%20inherent%20differences%20in%20the%20learning%20difficulties%20of%20classes.%20Alternatively%2C%20one%20can%20use%20complex%20measures%20such%20as%20training%20loss%20and%20uncertainty%2C%20which%2C%20however%2C%20depend%20on%20training%20a%20machine%20learning%20model.%20Our%20paper%20proposes%20using%20data%20Intrinsic%20Dimensionality%20%28ID%29%20as%20an%20easy-to-compute%2C%20model-free%20measure%20of%20imbalance%20that%20can%20be%20seamlessly%20incorporated%20into%20various%20imbalance%20mitigation%20methods.%20Our%20results%20across%20five%20different%20datasets%20with%20a%20diverse%20range%20of%20imbalance%20ratios%20show%20that%20ID%20consistently%20outperforms%20cardinality-based%20re-weighting%20and%20re-sampling%20techniques%20used%20in%20the%20literature.%20Moreover%2C%20we%20show%20that%20combining%20ID%20with%20cardinality%20can%20further%20improve%20performance.%20Code%3A%20https%3A//github.com/cagries/IDIM.&entry.1838667208=http%3A//arxiv.org/abs/2511.10475v1&entry.124074799=Read"},
{"title": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models", "author": "Asia Belfiore and Jonathan Passerat-Palmbach and Dmitrii Usynin", "abstract": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.", "link": "http://arxiv.org/abs/2511.07503v2", "date": "2025-11-13", "relevancy": 2.2535, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4527}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4515}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models&body=Title%3A%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models%0AAuthor%3A%20Asia%20Belfiore%20and%20Jonathan%20Passerat-Palmbach%20and%20Dmitrii%20Usynin%0AAbstract%3A%20The%20increased%20availability%20of%20genetic%20data%20has%20transformed%20genomics%20research%2C%20but%20raised%20many%20privacy%20concerns%20regarding%20its%20handling%20due%20to%20its%20sensitive%20nature.%20This%20work%20explores%20the%20use%20of%20language%20models%20%28LMs%29%20for%20the%20generation%20of%20synthetic%20genetic%20mutation%20profiles%2C%20leveraging%20differential%20privacy%20%28DP%29%20for%20the%20protection%20of%20sensitive%20genetic%20data.%20We%20empirically%20evaluate%20the%20privacy%20guarantees%20of%20our%20DP%20modes%20by%20introducing%20a%20novel%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attack%20%28biHMIA%29%2C%20which%20combines%20traditional%20black%20box%20MIA%20with%20contextual%20genomics%20metrics%20for%20enhanced%20attack%20power.%20Our%20experiments%20show%20that%20both%20small%20and%20large%20transformer%20GPT-like%20models%20are%20viable%20synthetic%20variant%20generators%20for%20small-scale%20genomics%2C%20and%20that%20our%20hybrid%20attack%20leads%2C%20on%20average%2C%20to%20higher%20adversarial%20success%20compared%20to%20traditional%20metric-based%20MIAs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiologically-Informed%2520Hybrid%2520Membership%2520Inference%2520Attacks%2520on%2520Generative%2520Genomic%2520Models%26entry.906535625%3DAsia%2520Belfiore%2520and%2520Jonathan%2520Passerat-Palmbach%2520and%2520Dmitrii%2520Usynin%26entry.1292438233%3DThe%2520increased%2520availability%2520of%2520genetic%2520data%2520has%2520transformed%2520genomics%2520research%252C%2520but%2520raised%2520many%2520privacy%2520concerns%2520regarding%2520its%2520handling%2520due%2520to%2520its%2520sensitive%2520nature.%2520This%2520work%2520explores%2520the%2520use%2520of%2520language%2520models%2520%2528LMs%2529%2520for%2520the%2520generation%2520of%2520synthetic%2520genetic%2520mutation%2520profiles%252C%2520leveraging%2520differential%2520privacy%2520%2528DP%2529%2520for%2520the%2520protection%2520of%2520sensitive%2520genetic%2520data.%2520We%2520empirically%2520evaluate%2520the%2520privacy%2520guarantees%2520of%2520our%2520DP%2520modes%2520by%2520introducing%2520a%2520novel%2520Biologically-Informed%2520Hybrid%2520Membership%2520Inference%2520Attack%2520%2528biHMIA%2529%252C%2520which%2520combines%2520traditional%2520black%2520box%2520MIA%2520with%2520contextual%2520genomics%2520metrics%2520for%2520enhanced%2520attack%2520power.%2520Our%2520experiments%2520show%2520that%2520both%2520small%2520and%2520large%2520transformer%2520GPT-like%2520models%2520are%2520viable%2520synthetic%2520variant%2520generators%2520for%2520small-scale%2520genomics%252C%2520and%2520that%2520our%2520hybrid%2520attack%2520leads%252C%2520on%2520average%252C%2520to%2520higher%2520adversarial%2520success%2520compared%2520to%2520traditional%2520metric-based%2520MIAs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biologically-Informed%20Hybrid%20Membership%20Inference%20Attacks%20on%20Generative%20Genomic%20Models&entry.906535625=Asia%20Belfiore%20and%20Jonathan%20Passerat-Palmbach%20and%20Dmitrii%20Usynin&entry.1292438233=The%20increased%20availability%20of%20genetic%20data%20has%20transformed%20genomics%20research%2C%20but%20raised%20many%20privacy%20concerns%20regarding%20its%20handling%20due%20to%20its%20sensitive%20nature.%20This%20work%20explores%20the%20use%20of%20language%20models%20%28LMs%29%20for%20the%20generation%20of%20synthetic%20genetic%20mutation%20profiles%2C%20leveraging%20differential%20privacy%20%28DP%29%20for%20the%20protection%20of%20sensitive%20genetic%20data.%20We%20empirically%20evaluate%20the%20privacy%20guarantees%20of%20our%20DP%20modes%20by%20introducing%20a%20novel%20Biologically-Informed%20Hybrid%20Membership%20Inference%20Attack%20%28biHMIA%29%2C%20which%20combines%20traditional%20black%20box%20MIA%20with%20contextual%20genomics%20metrics%20for%20enhanced%20attack%20power.%20Our%20experiments%20show%20that%20both%20small%20and%20large%20transformer%20GPT-like%20models%20are%20viable%20synthetic%20variant%20generators%20for%20small-scale%20genomics%2C%20and%20that%20our%20hybrid%20attack%20leads%2C%20on%20average%2C%20to%20higher%20adversarial%20success%20compared%20to%20traditional%20metric-based%20MIAs.&entry.1838667208=http%3A//arxiv.org/abs/2511.07503v2&entry.124074799=Read"},
{"title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers", "author": "Oded Schlesinger and Amirhossein Farzam and J. Matias Di Martino and Guillermo Sapiro", "abstract": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .", "link": "http://arxiv.org/abs/2511.10488v1", "date": "2025-11-13", "relevancy": 2.2486, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6035}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPOT%3A%20Sparsification%20with%20Attention%20Dynamics%20via%20Token%20Relevance%20in%20Vision%20Transformers&body=Title%3A%20SPOT%3A%20Sparsification%20with%20Attention%20Dynamics%20via%20Token%20Relevance%20in%20Vision%20Transformers%0AAuthor%3A%20Oded%20Schlesinger%20and%20Amirhossein%20Farzam%20and%20J.%20Matias%20Di%20Martino%20and%20Guillermo%20Sapiro%0AAbstract%3A%20While%20Vision%20Transformers%20%28ViT%29%20have%20demonstrated%20remarkable%20performance%20across%20diverse%20tasks%2C%20their%20computational%20demands%20are%20substantial%2C%20scaling%20quadratically%20with%20the%20number%20of%20processed%20tokens.%20Compact%20attention%20representations%2C%20reflecting%20token%20interaction%20distributions%2C%20can%20guide%20early%20detection%20and%20reduction%20of%20less%20salient%20tokens%20prior%20to%20attention%20computation.%20Motivated%20by%20this%2C%20we%20present%20SParsification%20with%20attentiOn%20dynamics%20via%20Token%20relevance%20%28SPOT%29%2C%20a%20framework%20for%20early%20detection%20of%20redundant%20tokens%20within%20ViTs%20that%20leverages%20token%20embeddings%2C%20interactions%2C%20and%20attention%20dynamics%20across%20layers%20to%20infer%20token%20importance%2C%20resulting%20in%20a%20more%20context-aware%20and%20interpretable%20relevance%20detection%20process.%20SPOT%20informs%20token%20sparsification%20and%20facilitates%20the%20elimination%20of%20such%20tokens%2C%20improving%20computational%20efficiency%20without%20sacrificing%20performance.%20SPOT%20employs%20computationally%20lightweight%20predictors%20that%20can%20be%20plugged%20into%20various%20ViT%20architectures%20and%20learn%20to%20derive%20effective%20input-specific%20token%20prioritization%20across%20layers.%20Its%20versatile%20design%20supports%20a%20range%20of%20performance%20levels%20adaptable%20to%20varying%20resource%20constraints.%20Empirical%20evaluations%20demonstrate%20significant%20efficiency%20gains%20of%20up%20to%2040%25%20compared%20to%20standard%20ViTs%2C%20while%20maintaining%20or%20even%20improving%20accuracy.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/odedsc/SPOT%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPOT%253A%2520Sparsification%2520with%2520Attention%2520Dynamics%2520via%2520Token%2520Relevance%2520in%2520Vision%2520Transformers%26entry.906535625%3DOded%2520Schlesinger%2520and%2520Amirhossein%2520Farzam%2520and%2520J.%2520Matias%2520Di%2520Martino%2520and%2520Guillermo%2520Sapiro%26entry.1292438233%3DWhile%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520diverse%2520tasks%252C%2520their%2520computational%2520demands%2520are%2520substantial%252C%2520scaling%2520quadratically%2520with%2520the%2520number%2520of%2520processed%2520tokens.%2520Compact%2520attention%2520representations%252C%2520reflecting%2520token%2520interaction%2520distributions%252C%2520can%2520guide%2520early%2520detection%2520and%2520reduction%2520of%2520less%2520salient%2520tokens%2520prior%2520to%2520attention%2520computation.%2520Motivated%2520by%2520this%252C%2520we%2520present%2520SParsification%2520with%2520attentiOn%2520dynamics%2520via%2520Token%2520relevance%2520%2528SPOT%2529%252C%2520a%2520framework%2520for%2520early%2520detection%2520of%2520redundant%2520tokens%2520within%2520ViTs%2520that%2520leverages%2520token%2520embeddings%252C%2520interactions%252C%2520and%2520attention%2520dynamics%2520across%2520layers%2520to%2520infer%2520token%2520importance%252C%2520resulting%2520in%2520a%2520more%2520context-aware%2520and%2520interpretable%2520relevance%2520detection%2520process.%2520SPOT%2520informs%2520token%2520sparsification%2520and%2520facilitates%2520the%2520elimination%2520of%2520such%2520tokens%252C%2520improving%2520computational%2520efficiency%2520without%2520sacrificing%2520performance.%2520SPOT%2520employs%2520computationally%2520lightweight%2520predictors%2520that%2520can%2520be%2520plugged%2520into%2520various%2520ViT%2520architectures%2520and%2520learn%2520to%2520derive%2520effective%2520input-specific%2520token%2520prioritization%2520across%2520layers.%2520Its%2520versatile%2520design%2520supports%2520a%2520range%2520of%2520performance%2520levels%2520adaptable%2520to%2520varying%2520resource%2520constraints.%2520Empirical%2520evaluations%2520demonstrate%2520significant%2520efficiency%2520gains%2520of%2520up%2520to%252040%2525%2520compared%2520to%2520standard%2520ViTs%252C%2520while%2520maintaining%2520or%2520even%2520improving%2520accuracy.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/odedsc/SPOT%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%3A%20Sparsification%20with%20Attention%20Dynamics%20via%20Token%20Relevance%20in%20Vision%20Transformers&entry.906535625=Oded%20Schlesinger%20and%20Amirhossein%20Farzam%20and%20J.%20Matias%20Di%20Martino%20and%20Guillermo%20Sapiro&entry.1292438233=While%20Vision%20Transformers%20%28ViT%29%20have%20demonstrated%20remarkable%20performance%20across%20diverse%20tasks%2C%20their%20computational%20demands%20are%20substantial%2C%20scaling%20quadratically%20with%20the%20number%20of%20processed%20tokens.%20Compact%20attention%20representations%2C%20reflecting%20token%20interaction%20distributions%2C%20can%20guide%20early%20detection%20and%20reduction%20of%20less%20salient%20tokens%20prior%20to%20attention%20computation.%20Motivated%20by%20this%2C%20we%20present%20SParsification%20with%20attentiOn%20dynamics%20via%20Token%20relevance%20%28SPOT%29%2C%20a%20framework%20for%20early%20detection%20of%20redundant%20tokens%20within%20ViTs%20that%20leverages%20token%20embeddings%2C%20interactions%2C%20and%20attention%20dynamics%20across%20layers%20to%20infer%20token%20importance%2C%20resulting%20in%20a%20more%20context-aware%20and%20interpretable%20relevance%20detection%20process.%20SPOT%20informs%20token%20sparsification%20and%20facilitates%20the%20elimination%20of%20such%20tokens%2C%20improving%20computational%20efficiency%20without%20sacrificing%20performance.%20SPOT%20employs%20computationally%20lightweight%20predictors%20that%20can%20be%20plugged%20into%20various%20ViT%20architectures%20and%20learn%20to%20derive%20effective%20input-specific%20token%20prioritization%20across%20layers.%20Its%20versatile%20design%20supports%20a%20range%20of%20performance%20levels%20adaptable%20to%20varying%20resource%20constraints.%20Empirical%20evaluations%20demonstrate%20significant%20efficiency%20gains%20of%20up%20to%2040%25%20compared%20to%20standard%20ViTs%2C%20while%20maintaining%20or%20even%20improving%20accuracy.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/odedsc/SPOT%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.10488v1&entry.124074799=Read"},
{"title": "Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles", "author": "Younggun Kim and Mohamed Abdel-Aty and Beomsik Cho and Seonghoon Ryoo and Soomok Lee", "abstract": "Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Furthermore, MSCN enhances feature robustness by training with unseen domain point clouds generated from the source domain, enabling the model to acquire domain-invariant representations. Extensive cross-domain experiments demonstrate that MSCN achieves an average accuracy of 82.0%, surpassing the strong baseline PointTransformer by 15.8%, confirming its effectiveness under real-world domain shifts. Our code is available at https://github.com/MLMLab/MSCN.", "link": "http://arxiv.org/abs/2501.16289v5", "date": "2025-11-13", "relevancy": 2.2478, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5636}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5618}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&body=Title%3A%20Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%20Cloud%20Recognition%20of%20Autonomous%20Vehicles%0AAuthor%3A%20Younggun%20Kim%20and%20Mohamed%20Abdel-Aty%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee%0AAbstract%3A%20Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%20field%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%20However%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%20challenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%20variability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%20accuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%20Structural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%20recognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%20local%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%20Layers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%20from%20point%20clouds.%20Furthermore%2C%20MSCN%20enhances%20feature%20robustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20generated%20from%20the%20source%20domain%2C%20enabling%20the%20model%20to%20acquire%20domain-invariant%20representations.%20Extensive%20cross-domain%20experiments%20demonstrate%20that%20MSCN%20achieves%20an%20average%20accuracy%20of%2082.0%25%2C%20surpassing%20the%20strong%20baseline%20PointTransformer%20by%2015.8%25%2C%20confirming%20its%20effectiveness%20under%20real-world%20domain%20shifts.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.%0ALink%3A%20http%3A//arxiv.org/abs/2501.16289v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Structural%2520Convolution%2520Network%2520for%2520Domain-Invariant%2520Point%2520Cloud%2520Recognition%2520of%2520Autonomous%2520Vehicles%26entry.906535625%3DYounggun%2520Kim%2520and%2520Mohamed%2520Abdel-Aty%2520and%2520Beomsik%2520Cho%2520and%2520Seonghoon%2520Ryoo%2520and%2520Soomok%2520Lee%26entry.1292438233%3DPoint%2520cloud%2520representation%2520has%2520recently%2520become%2520a%2520research%2520hotspot%2520in%2520the%2520field%2520of%2520computer%2520vision%2520and%2520has%2520been%2520utilized%2520for%2520autonomous%2520vehicles.%2520However%252C%2520adapting%2520deep%2520learning%2520networks%2520for%2520point%2520cloud%2520data%2520recognition%2520is%2520challenging%2520due%2520to%2520the%2520variability%2520in%2520datasets%2520and%2520sensor%2520technologies.%2520This%2520variability%2520underscores%2520the%2520necessity%2520for%2520adaptive%2520techniques%2520to%2520maintain%2520accuracy%2520under%2520different%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Multi-View%2520Structural%2520Convolution%2520Network%2520%2528MSCN%2529%2520designed%2520for%2520domain-invariant%2520point%2520cloud%2520recognition.%2520MSCN%2520comprises%2520Structural%2520Convolution%2520Layers%2520%2528SCL%2529%2520that%2520extract%2520local%2520context%2520geometric%2520features%2520from%2520point%2520clouds%2520and%2520Structural%2520Aggregation%2520Layers%2520%2528SAL%2529%2520that%2520extract%2520and%2520aggregate%2520both%2520local%2520and%2520overall%2520context%2520features%2520from%2520point%2520clouds.%2520Furthermore%252C%2520MSCN%2520enhances%2520feature%2520robustness%2520by%2520training%2520with%2520unseen%2520domain%2520point%2520clouds%2520generated%2520from%2520the%2520source%2520domain%252C%2520enabling%2520the%2520model%2520to%2520acquire%2520domain-invariant%2520representations.%2520Extensive%2520cross-domain%2520experiments%2520demonstrate%2520that%2520MSCN%2520achieves%2520an%2520average%2520accuracy%2520of%252082.0%2525%252C%2520surpassing%2520the%2520strong%2520baseline%2520PointTransformer%2520by%252015.8%2525%252C%2520confirming%2520its%2520effectiveness%2520under%2520real-world%2520domain%2520shifts.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/MLMLab/MSCN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16289v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Structural%20Convolution%20Network%20for%20Domain-Invariant%20Point%20Cloud%20Recognition%20of%20Autonomous%20Vehicles&entry.906535625=Younggun%20Kim%20and%20Mohamed%20Abdel-Aty%20and%20Beomsik%20Cho%20and%20Seonghoon%20Ryoo%20and%20Soomok%20Lee&entry.1292438233=Point%20cloud%20representation%20has%20recently%20become%20a%20research%20hotspot%20in%20the%20field%20of%20computer%20vision%20and%20has%20been%20utilized%20for%20autonomous%20vehicles.%20However%2C%20adapting%20deep%20learning%20networks%20for%20point%20cloud%20data%20recognition%20is%20challenging%20due%20to%20the%20variability%20in%20datasets%20and%20sensor%20technologies.%20This%20variability%20underscores%20the%20necessity%20for%20adaptive%20techniques%20to%20maintain%20accuracy%20under%20different%20conditions.%20In%20this%20paper%2C%20we%20present%20the%20Multi-View%20Structural%20Convolution%20Network%20%28MSCN%29%20designed%20for%20domain-invariant%20point%20cloud%20recognition.%20MSCN%20comprises%20Structural%20Convolution%20Layers%20%28SCL%29%20that%20extract%20local%20context%20geometric%20features%20from%20point%20clouds%20and%20Structural%20Aggregation%20Layers%20%28SAL%29%20that%20extract%20and%20aggregate%20both%20local%20and%20overall%20context%20features%20from%20point%20clouds.%20Furthermore%2C%20MSCN%20enhances%20feature%20robustness%20by%20training%20with%20unseen%20domain%20point%20clouds%20generated%20from%20the%20source%20domain%2C%20enabling%20the%20model%20to%20acquire%20domain-invariant%20representations.%20Extensive%20cross-domain%20experiments%20demonstrate%20that%20MSCN%20achieves%20an%20average%20accuracy%20of%2082.0%25%2C%20surpassing%20the%20strong%20baseline%20PointTransformer%20by%2015.8%25%2C%20confirming%20its%20effectiveness%20under%20real-world%20domain%20shifts.%20Our%20code%20is%20available%20at%20https%3A//github.com/MLMLab/MSCN.&entry.1838667208=http%3A//arxiv.org/abs/2501.16289v5&entry.124074799=Read"},
{"title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations", "author": "Zakaria El Kassimi and Fares Fourati and Mohamed-Slim Alouini", "abstract": "We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.", "link": "http://arxiv.org/abs/2509.09651v2", "date": "2025-11-13", "relevancy": 2.2449, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4674}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4412}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Generation%20for%20Reliable%20Interpretation%20of%20Radio%20Regulations&body=Title%3A%20Retrieval-Augmented%20Generation%20for%20Reliable%20Interpretation%20of%20Radio%20Regulations%0AAuthor%3A%20Zakaria%20El%20Kassimi%20and%20Fares%20Fourati%20and%20Mohamed-Slim%20Alouini%0AAbstract%3A%20We%20study%20question%20answering%20in%20the%20domain%20of%20radio%20regulations%2C%20a%20legally%20sensitive%20and%20high-stakes%20area.%20We%20propose%20a%20telecom-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20pipeline%20and%20introduce%2C%20to%20our%20knowledge%2C%20the%20first%20multiple-choice%20evaluation%20set%20for%20this%20domain%2C%20constructed%20from%20authoritative%20sources%20using%20automated%20filtering%20and%20human%20validation.%20To%20assess%20retrieval%20quality%2C%20we%20define%20a%20domain-specific%20retrieval%20metric%2C%20under%20which%20our%20retriever%20achieves%20approximately%2097%25%20accuracy.%20Beyond%20retrieval%2C%20our%20approach%20consistently%20improves%20generation%20accuracy%20across%20all%20tested%20models.%20In%20particular%2C%20while%20naively%20inserting%20documents%20without%20structured%20retrieval%20yields%20only%20marginal%20gains%20for%20GPT-4o%20%28less%20than%201%25%29%2C%20applying%20our%20pipeline%20results%20in%20nearly%20a%2012%25%20relative%20improvement.%20These%20findings%20demonstrate%20that%20carefully%20targeted%20grounding%20provides%20a%20simple%20yet%20strong%20baseline%20and%20an%20effective%20domain-specific%20solution%20for%20regulatory%20question%20answering.%20All%20code%20and%20evaluation%20scripts%2C%20along%20with%20our%20derived%20question-answer%20dataset%2C%20are%20available%20at%20https%3A//github.com/Zakaria010/Radio-RAG.%0ALink%3A%20http%3A//arxiv.org/abs/2509.09651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Generation%2520for%2520Reliable%2520Interpretation%2520of%2520Radio%2520Regulations%26entry.906535625%3DZakaria%2520El%2520Kassimi%2520and%2520Fares%2520Fourati%2520and%2520Mohamed-Slim%2520Alouini%26entry.1292438233%3DWe%2520study%2520question%2520answering%2520in%2520the%2520domain%2520of%2520radio%2520regulations%252C%2520a%2520legally%2520sensitive%2520and%2520high-stakes%2520area.%2520We%2520propose%2520a%2520telecom-specific%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520pipeline%2520and%2520introduce%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520multiple-choice%2520evaluation%2520set%2520for%2520this%2520domain%252C%2520constructed%2520from%2520authoritative%2520sources%2520using%2520automated%2520filtering%2520and%2520human%2520validation.%2520To%2520assess%2520retrieval%2520quality%252C%2520we%2520define%2520a%2520domain-specific%2520retrieval%2520metric%252C%2520under%2520which%2520our%2520retriever%2520achieves%2520approximately%252097%2525%2520accuracy.%2520Beyond%2520retrieval%252C%2520our%2520approach%2520consistently%2520improves%2520generation%2520accuracy%2520across%2520all%2520tested%2520models.%2520In%2520particular%252C%2520while%2520naively%2520inserting%2520documents%2520without%2520structured%2520retrieval%2520yields%2520only%2520marginal%2520gains%2520for%2520GPT-4o%2520%2528less%2520than%25201%2525%2529%252C%2520applying%2520our%2520pipeline%2520results%2520in%2520nearly%2520a%252012%2525%2520relative%2520improvement.%2520These%2520findings%2520demonstrate%2520that%2520carefully%2520targeted%2520grounding%2520provides%2520a%2520simple%2520yet%2520strong%2520baseline%2520and%2520an%2520effective%2520domain-specific%2520solution%2520for%2520regulatory%2520question%2520answering.%2520All%2520code%2520and%2520evaluation%2520scripts%252C%2520along%2520with%2520our%2520derived%2520question-answer%2520dataset%252C%2520are%2520available%2520at%2520https%253A//github.com/Zakaria010/Radio-RAG.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Generation%20for%20Reliable%20Interpretation%20of%20Radio%20Regulations&entry.906535625=Zakaria%20El%20Kassimi%20and%20Fares%20Fourati%20and%20Mohamed-Slim%20Alouini&entry.1292438233=We%20study%20question%20answering%20in%20the%20domain%20of%20radio%20regulations%2C%20a%20legally%20sensitive%20and%20high-stakes%20area.%20We%20propose%20a%20telecom-specific%20Retrieval-Augmented%20Generation%20%28RAG%29%20pipeline%20and%20introduce%2C%20to%20our%20knowledge%2C%20the%20first%20multiple-choice%20evaluation%20set%20for%20this%20domain%2C%20constructed%20from%20authoritative%20sources%20using%20automated%20filtering%20and%20human%20validation.%20To%20assess%20retrieval%20quality%2C%20we%20define%20a%20domain-specific%20retrieval%20metric%2C%20under%20which%20our%20retriever%20achieves%20approximately%2097%25%20accuracy.%20Beyond%20retrieval%2C%20our%20approach%20consistently%20improves%20generation%20accuracy%20across%20all%20tested%20models.%20In%20particular%2C%20while%20naively%20inserting%20documents%20without%20structured%20retrieval%20yields%20only%20marginal%20gains%20for%20GPT-4o%20%28less%20than%201%25%29%2C%20applying%20our%20pipeline%20results%20in%20nearly%20a%2012%25%20relative%20improvement.%20These%20findings%20demonstrate%20that%20carefully%20targeted%20grounding%20provides%20a%20simple%20yet%20strong%20baseline%20and%20an%20effective%20domain-specific%20solution%20for%20regulatory%20question%20answering.%20All%20code%20and%20evaluation%20scripts%2C%20along%20with%20our%20derived%20question-answer%20dataset%2C%20are%20available%20at%20https%3A//github.com/Zakaria010/Radio-RAG.&entry.1838667208=http%3A//arxiv.org/abs/2509.09651v2&entry.124074799=Read"},
{"title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "author": "Ivona Krchova and Mariana Vargas Vieyra and Mario Scriminaci and Andrey Sidorenko", "abstract": "Machine learning development critically depends on access to high-quality data. However, increasing restrictions due to privacy, proprietary interests, and ethical concerns have created significant barriers to data accessibility. Synthetic data offers a viable solution by enabling safe, broad data usage without compromising sensitive information. This paper presents the MOSTLY AI Synthetic Data Software Development Kit (SDK), an open-source toolkit designed specifically for synthesizing high-quality tabular data. The SDK integrates robust features such as differential privacy guarantees, fairness-aware data generation, and automated quality assurance into a flexible and accessible Python interface. Leveraging the TabularARGN autoregressive framework, the SDK supports diverse data types and complex multi-table and sequential datasets, delivering competitive performance with notable improvements in speed and usability. Currently deployed both as a cloud service and locally installable software, the SDK has seen rapid adoption, highlighting its practicality in addressing real-world data bottlenecks and promoting widespread data democratization.", "link": "http://arxiv.org/abs/2508.00718v2", "date": "2025-11-13", "relevancy": 2.2352, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4516}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4516}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK&body=Title%3A%20Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK%0AAuthor%3A%20Ivona%20Krchova%20and%20Mariana%20Vargas%20Vieyra%20and%20Mario%20Scriminaci%20and%20Andrey%20Sidorenko%0AAbstract%3A%20Machine%20learning%20development%20critically%20depends%20on%20access%20to%20high-quality%20data.%20However%2C%20increasing%20restrictions%20due%20to%20privacy%2C%20proprietary%20interests%2C%20and%20ethical%20concerns%20have%20created%20significant%20barriers%20to%20data%20accessibility.%20Synthetic%20data%20offers%20a%20viable%20solution%20by%20enabling%20safe%2C%20broad%20data%20usage%20without%20compromising%20sensitive%20information.%20This%20paper%20presents%20the%20MOSTLY%20AI%20Synthetic%20Data%20Software%20Development%20Kit%20%28SDK%29%2C%20an%20open-source%20toolkit%20designed%20specifically%20for%20synthesizing%20high-quality%20tabular%20data.%20The%20SDK%20integrates%20robust%20features%20such%20as%20differential%20privacy%20guarantees%2C%20fairness-aware%20data%20generation%2C%20and%20automated%20quality%20assurance%20into%20a%20flexible%20and%20accessible%20Python%20interface.%20Leveraging%20the%20TabularARGN%20autoregressive%20framework%2C%20the%20SDK%20supports%20diverse%20data%20types%20and%20complex%20multi-table%20and%20sequential%20datasets%2C%20delivering%20competitive%20performance%20with%20notable%20improvements%20in%20speed%20and%20usability.%20Currently%20deployed%20both%20as%20a%20cloud%20service%20and%20locally%20installable%20software%2C%20the%20SDK%20has%20seen%20rapid%20adoption%2C%20highlighting%20its%20practicality%20in%20addressing%20real-world%20data%20bottlenecks%20and%20promoting%20widespread%20data%20democratization.%0ALink%3A%20http%3A//arxiv.org/abs/2508.00718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemocratizing%2520Tabular%2520Data%2520Access%2520with%2520an%2520Open%2524%255Cunicode%257Bx2013%257D%2524Source%2520Synthetic%2524%255Cunicode%257Bx2013%257D%2524Data%2520SDK%26entry.906535625%3DIvona%2520Krchova%2520and%2520Mariana%2520Vargas%2520Vieyra%2520and%2520Mario%2520Scriminaci%2520and%2520Andrey%2520Sidorenko%26entry.1292438233%3DMachine%2520learning%2520development%2520critically%2520depends%2520on%2520access%2520to%2520high-quality%2520data.%2520However%252C%2520increasing%2520restrictions%2520due%2520to%2520privacy%252C%2520proprietary%2520interests%252C%2520and%2520ethical%2520concerns%2520have%2520created%2520significant%2520barriers%2520to%2520data%2520accessibility.%2520Synthetic%2520data%2520offers%2520a%2520viable%2520solution%2520by%2520enabling%2520safe%252C%2520broad%2520data%2520usage%2520without%2520compromising%2520sensitive%2520information.%2520This%2520paper%2520presents%2520the%2520MOSTLY%2520AI%2520Synthetic%2520Data%2520Software%2520Development%2520Kit%2520%2528SDK%2529%252C%2520an%2520open-source%2520toolkit%2520designed%2520specifically%2520for%2520synthesizing%2520high-quality%2520tabular%2520data.%2520The%2520SDK%2520integrates%2520robust%2520features%2520such%2520as%2520differential%2520privacy%2520guarantees%252C%2520fairness-aware%2520data%2520generation%252C%2520and%2520automated%2520quality%2520assurance%2520into%2520a%2520flexible%2520and%2520accessible%2520Python%2520interface.%2520Leveraging%2520the%2520TabularARGN%2520autoregressive%2520framework%252C%2520the%2520SDK%2520supports%2520diverse%2520data%2520types%2520and%2520complex%2520multi-table%2520and%2520sequential%2520datasets%252C%2520delivering%2520competitive%2520performance%2520with%2520notable%2520improvements%2520in%2520speed%2520and%2520usability.%2520Currently%2520deployed%2520both%2520as%2520a%2520cloud%2520service%2520and%2520locally%2520installable%2520software%252C%2520the%2520SDK%2520has%2520seen%2520rapid%2520adoption%252C%2520highlighting%2520its%2520practicality%2520in%2520addressing%2520real-world%2520data%2520bottlenecks%2520and%2520promoting%2520widespread%2520data%2520democratization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Democratizing%20Tabular%20Data%20Access%20with%20an%20Open%24%5Cunicode%7Bx2013%7D%24Source%20Synthetic%24%5Cunicode%7Bx2013%7D%24Data%20SDK&entry.906535625=Ivona%20Krchova%20and%20Mariana%20Vargas%20Vieyra%20and%20Mario%20Scriminaci%20and%20Andrey%20Sidorenko&entry.1292438233=Machine%20learning%20development%20critically%20depends%20on%20access%20to%20high-quality%20data.%20However%2C%20increasing%20restrictions%20due%20to%20privacy%2C%20proprietary%20interests%2C%20and%20ethical%20concerns%20have%20created%20significant%20barriers%20to%20data%20accessibility.%20Synthetic%20data%20offers%20a%20viable%20solution%20by%20enabling%20safe%2C%20broad%20data%20usage%20without%20compromising%20sensitive%20information.%20This%20paper%20presents%20the%20MOSTLY%20AI%20Synthetic%20Data%20Software%20Development%20Kit%20%28SDK%29%2C%20an%20open-source%20toolkit%20designed%20specifically%20for%20synthesizing%20high-quality%20tabular%20data.%20The%20SDK%20integrates%20robust%20features%20such%20as%20differential%20privacy%20guarantees%2C%20fairness-aware%20data%20generation%2C%20and%20automated%20quality%20assurance%20into%20a%20flexible%20and%20accessible%20Python%20interface.%20Leveraging%20the%20TabularARGN%20autoregressive%20framework%2C%20the%20SDK%20supports%20diverse%20data%20types%20and%20complex%20multi-table%20and%20sequential%20datasets%2C%20delivering%20competitive%20performance%20with%20notable%20improvements%20in%20speed%20and%20usability.%20Currently%20deployed%20both%20as%20a%20cloud%20service%20and%20locally%20installable%20software%2C%20the%20SDK%20has%20seen%20rapid%20adoption%2C%20highlighting%20its%20practicality%20in%20addressing%20real-world%20data%20bottlenecks%20and%20promoting%20widespread%20data%20democratization.&entry.1838667208=http%3A//arxiv.org/abs/2508.00718v2&entry.124074799=Read"},
{"title": "AgentEvolver: Towards Efficient Self-Evolving Agent System", "author": "Yunpeng Zhai and Shuchang Tao and Cheng Chen and Anni Zou and Ziqian Chen and Qingxu Fu and Shinji Mai and Li Yu and Jiaji Deng and Zouying Cao and Zhaoyang Liu and Bolin Ding and Jingren Zhou", "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.", "link": "http://arxiv.org/abs/2511.10395v1", "date": "2025-11-13", "relevancy": 2.2292, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.573}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5637}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentEvolver%3A%20Towards%20Efficient%20Self-Evolving%20Agent%20System&body=Title%3A%20AgentEvolver%3A%20Towards%20Efficient%20Self-Evolving%20Agent%20System%0AAuthor%3A%20Yunpeng%20Zhai%20and%20Shuchang%20Tao%20and%20Cheng%20Chen%20and%20Anni%20Zou%20and%20Ziqian%20Chen%20and%20Qingxu%20Fu%20and%20Shinji%20Mai%20and%20Li%20Yu%20and%20Jiaji%20Deng%20and%20Zouying%20Cao%20and%20Zhaoyang%20Liu%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20Autonomous%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20the%20potential%20to%20significantly%20enhance%20human%20productivity%20by%20reasoning%2C%20using%20tools%2C%20and%20executing%20complex%20tasks%20in%20diverse%20environments.%20However%2C%20current%20approaches%20to%20developing%20such%20agents%20remain%20costly%20and%20inefficient%2C%20as%20they%20typically%20require%20manually%20constructed%20task%20datasets%20and%20reinforcement%20learning%20%28RL%29%20pipelines%20with%20extensive%20random%20exploration.%20These%20limitations%20lead%20to%20prohibitively%20high%20data-construction%20costs%2C%20low%20exploration%20efficiency%2C%20and%20poor%20sample%20utilization.%20To%20address%20these%20challenges%2C%20we%20present%20AgentEvolver%2C%20a%20self-evolving%20agent%20system%20that%20leverages%20the%20semantic%20understanding%20and%20reasoning%20capabilities%20of%20LLMs%20to%20drive%20autonomous%20agent%20learning.%20AgentEvolver%20introduces%20three%20synergistic%20mechanisms%3A%20%28i%29%20self-questioning%2C%20which%20enables%20curiosity-driven%20task%20generation%20in%20novel%20environments%2C%20reducing%20dependence%20on%20handcrafted%20datasets%3B%20%28ii%29%20self-navigating%2C%20which%20improves%20exploration%20efficiency%20through%20experience%20reuse%20and%20hybrid%20policy%20guidance%3B%20and%20%28iii%29%20self-attributing%2C%20which%20enhances%20sample%20efficiency%20by%20assigning%20differentiated%20rewards%20to%20trajectory%20states%20and%20actions%20based%20on%20their%20contribution.%20By%20integrating%20these%20mechanisms%20into%20a%20unified%20framework%2C%20AgentEvolver%20enables%20scalable%2C%20cost-effective%2C%20and%20continual%20improvement%20of%20agent%20capabilities.%20Preliminary%20experiments%20indicate%20that%20AgentEvolver%20achieves%20more%20efficient%20exploration%2C%20better%20sample%20utilization%2C%20and%20faster%20adaptation%20compared%20to%20traditional%20RL-based%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentEvolver%253A%2520Towards%2520Efficient%2520Self-Evolving%2520Agent%2520System%26entry.906535625%3DYunpeng%2520Zhai%2520and%2520Shuchang%2520Tao%2520and%2520Cheng%2520Chen%2520and%2520Anni%2520Zou%2520and%2520Ziqian%2520Chen%2520and%2520Qingxu%2520Fu%2520and%2520Shinji%2520Mai%2520and%2520Li%2520Yu%2520and%2520Jiaji%2520Deng%2520and%2520Zouying%2520Cao%2520and%2520Zhaoyang%2520Liu%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3DAutonomous%2520agents%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520the%2520potential%2520to%2520significantly%2520enhance%2520human%2520productivity%2520by%2520reasoning%252C%2520using%2520tools%252C%2520and%2520executing%2520complex%2520tasks%2520in%2520diverse%2520environments.%2520However%252C%2520current%2520approaches%2520to%2520developing%2520such%2520agents%2520remain%2520costly%2520and%2520inefficient%252C%2520as%2520they%2520typically%2520require%2520manually%2520constructed%2520task%2520datasets%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520pipelines%2520with%2520extensive%2520random%2520exploration.%2520These%2520limitations%2520lead%2520to%2520prohibitively%2520high%2520data-construction%2520costs%252C%2520low%2520exploration%2520efficiency%252C%2520and%2520poor%2520sample%2520utilization.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520AgentEvolver%252C%2520a%2520self-evolving%2520agent%2520system%2520that%2520leverages%2520the%2520semantic%2520understanding%2520and%2520reasoning%2520capabilities%2520of%2520LLMs%2520to%2520drive%2520autonomous%2520agent%2520learning.%2520AgentEvolver%2520introduces%2520three%2520synergistic%2520mechanisms%253A%2520%2528i%2529%2520self-questioning%252C%2520which%2520enables%2520curiosity-driven%2520task%2520generation%2520in%2520novel%2520environments%252C%2520reducing%2520dependence%2520on%2520handcrafted%2520datasets%253B%2520%2528ii%2529%2520self-navigating%252C%2520which%2520improves%2520exploration%2520efficiency%2520through%2520experience%2520reuse%2520and%2520hybrid%2520policy%2520guidance%253B%2520and%2520%2528iii%2529%2520self-attributing%252C%2520which%2520enhances%2520sample%2520efficiency%2520by%2520assigning%2520differentiated%2520rewards%2520to%2520trajectory%2520states%2520and%2520actions%2520based%2520on%2520their%2520contribution.%2520By%2520integrating%2520these%2520mechanisms%2520into%2520a%2520unified%2520framework%252C%2520AgentEvolver%2520enables%2520scalable%252C%2520cost-effective%252C%2520and%2520continual%2520improvement%2520of%2520agent%2520capabilities.%2520Preliminary%2520experiments%2520indicate%2520that%2520AgentEvolver%2520achieves%2520more%2520efficient%2520exploration%252C%2520better%2520sample%2520utilization%252C%2520and%2520faster%2520adaptation%2520compared%2520to%2520traditional%2520RL-based%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentEvolver%3A%20Towards%20Efficient%20Self-Evolving%20Agent%20System&entry.906535625=Yunpeng%20Zhai%20and%20Shuchang%20Tao%20and%20Cheng%20Chen%20and%20Anni%20Zou%20and%20Ziqian%20Chen%20and%20Qingxu%20Fu%20and%20Shinji%20Mai%20and%20Li%20Yu%20and%20Jiaji%20Deng%20and%20Zouying%20Cao%20and%20Zhaoyang%20Liu%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=Autonomous%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20the%20potential%20to%20significantly%20enhance%20human%20productivity%20by%20reasoning%2C%20using%20tools%2C%20and%20executing%20complex%20tasks%20in%20diverse%20environments.%20However%2C%20current%20approaches%20to%20developing%20such%20agents%20remain%20costly%20and%20inefficient%2C%20as%20they%20typically%20require%20manually%20constructed%20task%20datasets%20and%20reinforcement%20learning%20%28RL%29%20pipelines%20with%20extensive%20random%20exploration.%20These%20limitations%20lead%20to%20prohibitively%20high%20data-construction%20costs%2C%20low%20exploration%20efficiency%2C%20and%20poor%20sample%20utilization.%20To%20address%20these%20challenges%2C%20we%20present%20AgentEvolver%2C%20a%20self-evolving%20agent%20system%20that%20leverages%20the%20semantic%20understanding%20and%20reasoning%20capabilities%20of%20LLMs%20to%20drive%20autonomous%20agent%20learning.%20AgentEvolver%20introduces%20three%20synergistic%20mechanisms%3A%20%28i%29%20self-questioning%2C%20which%20enables%20curiosity-driven%20task%20generation%20in%20novel%20environments%2C%20reducing%20dependence%20on%20handcrafted%20datasets%3B%20%28ii%29%20self-navigating%2C%20which%20improves%20exploration%20efficiency%20through%20experience%20reuse%20and%20hybrid%20policy%20guidance%3B%20and%20%28iii%29%20self-attributing%2C%20which%20enhances%20sample%20efficiency%20by%20assigning%20differentiated%20rewards%20to%20trajectory%20states%20and%20actions%20based%20on%20their%20contribution.%20By%20integrating%20these%20mechanisms%20into%20a%20unified%20framework%2C%20AgentEvolver%20enables%20scalable%2C%20cost-effective%2C%20and%20continual%20improvement%20of%20agent%20capabilities.%20Preliminary%20experiments%20indicate%20that%20AgentEvolver%20achieves%20more%20efficient%20exploration%2C%20better%20sample%20utilization%2C%20and%20faster%20adaptation%20compared%20to%20traditional%20RL-based%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.10395v1&entry.124074799=Read"},
{"title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space", "author": "Huijie Liu and Shuhao Cui and Haoxiang Cao and Shuai Ma and Kai Wu and Guoliang Kang", "abstract": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.", "link": "http://arxiv.org/abs/2511.10555v1", "date": "2025-11-13", "relevancy": 2.2257, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5817}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5468}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space&body=Title%3A%20A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space%0AAuthor%3A%20Huijie%20Liu%20and%20Shuhao%20Cui%20and%20Haoxiang%20Cao%20and%20Shuai%20Ma%20and%20Kai%20Wu%20and%20Guoliang%20Kang%0AAbstract%3A%20Innovative%20visual%20stylization%20is%20a%20cornerstone%20of%20artistic%20creation%2C%20yet%20generating%20novel%20and%20consistent%20visual%20styles%20remains%20a%20significant%20challenge.%20Existing%20generative%20approaches%20typically%20rely%20on%20lengthy%20textual%20prompts%2C%20reference%20images%2C%20or%20parameter-efficient%20fine-tuning%20to%20guide%20style-aware%20image%20generation%2C%20but%20often%20struggle%20with%20style%20consistency%2C%20limited%20creativity%2C%20and%20complex%20style%20representations.%20In%20this%20paper%2C%20we%20affirm%20that%20a%20style%20is%20worth%20one%20numerical%20code%20by%20introducing%20the%20novel%20task%2C%20code-to-style%20image%20generation%2C%20which%20produces%20images%20with%20novel%2C%20consistent%20visual%20styles%20conditioned%20solely%20on%20a%20numerical%20style%20code.%20To%20date%2C%20this%20field%20has%20only%20been%20primarily%20explored%20by%20the%20industry%20%28e.g.%2C%20Midjourney%29%2C%20with%20no%20open-source%20research%20from%20the%20academic%20community.%20To%20fill%20this%20gap%2C%20we%20propose%20CoTyle%2C%20the%20first%20open-source%20method%20for%20this%20task.%20Specifically%2C%20we%20first%20train%20a%20discrete%20style%20codebook%20from%20a%20collection%20of%20images%20to%20extract%20style%20embeddings.%20These%20embeddings%20serve%20as%20conditions%20for%20a%20text-to-image%20diffusion%20model%20%28T2I-DM%29%20to%20generate%20stylistic%20images.%20Subsequently%2C%20we%20train%20an%20autoregressive%20style%20generator%20on%20the%20discrete%20style%20embeddings%20to%20model%20their%20distribution%2C%20allowing%20the%20synthesis%20of%20novel%20style%20embeddings.%20During%20inference%2C%20a%20numerical%20style%20code%20is%20mapped%20to%20a%20unique%20style%20embedding%20by%20the%20style%20generator%2C%20and%20this%20embedding%20guides%20the%20T2I-DM%20to%20generate%20images%20in%20the%20corresponding%20style.%20Unlike%20existing%20methods%2C%20our%20method%20offers%20unparalleled%20simplicity%20and%20diversity%2C%20unlocking%20a%20vast%20space%20of%20reproducible%20styles%20from%20minimal%20input.%20Extensive%20experiments%20validate%20that%20CoTyle%20effectively%20turns%20a%20numerical%20code%20into%20a%20style%20controller%2C%20demonstrating%20a%20style%20is%20worth%20one%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Style%2520is%2520Worth%2520One%2520Code%253A%2520Unlocking%2520Code-to-Style%2520Image%2520Generation%2520with%2520Discrete%2520Style%2520Space%26entry.906535625%3DHuijie%2520Liu%2520and%2520Shuhao%2520Cui%2520and%2520Haoxiang%2520Cao%2520and%2520Shuai%2520Ma%2520and%2520Kai%2520Wu%2520and%2520Guoliang%2520Kang%26entry.1292438233%3DInnovative%2520visual%2520stylization%2520is%2520a%2520cornerstone%2520of%2520artistic%2520creation%252C%2520yet%2520generating%2520novel%2520and%2520consistent%2520visual%2520styles%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520generative%2520approaches%2520typically%2520rely%2520on%2520lengthy%2520textual%2520prompts%252C%2520reference%2520images%252C%2520or%2520parameter-efficient%2520fine-tuning%2520to%2520guide%2520style-aware%2520image%2520generation%252C%2520but%2520often%2520struggle%2520with%2520style%2520consistency%252C%2520limited%2520creativity%252C%2520and%2520complex%2520style%2520representations.%2520In%2520this%2520paper%252C%2520we%2520affirm%2520that%2520a%2520style%2520is%2520worth%2520one%2520numerical%2520code%2520by%2520introducing%2520the%2520novel%2520task%252C%2520code-to-style%2520image%2520generation%252C%2520which%2520produces%2520images%2520with%2520novel%252C%2520consistent%2520visual%2520styles%2520conditioned%2520solely%2520on%2520a%2520numerical%2520style%2520code.%2520To%2520date%252C%2520this%2520field%2520has%2520only%2520been%2520primarily%2520explored%2520by%2520the%2520industry%2520%2528e.g.%252C%2520Midjourney%2529%252C%2520with%2520no%2520open-source%2520research%2520from%2520the%2520academic%2520community.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520CoTyle%252C%2520the%2520first%2520open-source%2520method%2520for%2520this%2520task.%2520Specifically%252C%2520we%2520first%2520train%2520a%2520discrete%2520style%2520codebook%2520from%2520a%2520collection%2520of%2520images%2520to%2520extract%2520style%2520embeddings.%2520These%2520embeddings%2520serve%2520as%2520conditions%2520for%2520a%2520text-to-image%2520diffusion%2520model%2520%2528T2I-DM%2529%2520to%2520generate%2520stylistic%2520images.%2520Subsequently%252C%2520we%2520train%2520an%2520autoregressive%2520style%2520generator%2520on%2520the%2520discrete%2520style%2520embeddings%2520to%2520model%2520their%2520distribution%252C%2520allowing%2520the%2520synthesis%2520of%2520novel%2520style%2520embeddings.%2520During%2520inference%252C%2520a%2520numerical%2520style%2520code%2520is%2520mapped%2520to%2520a%2520unique%2520style%2520embedding%2520by%2520the%2520style%2520generator%252C%2520and%2520this%2520embedding%2520guides%2520the%2520T2I-DM%2520to%2520generate%2520images%2520in%2520the%2520corresponding%2520style.%2520Unlike%2520existing%2520methods%252C%2520our%2520method%2520offers%2520unparalleled%2520simplicity%2520and%2520diversity%252C%2520unlocking%2520a%2520vast%2520space%2520of%2520reproducible%2520styles%2520from%2520minimal%2520input.%2520Extensive%2520experiments%2520validate%2520that%2520CoTyle%2520effectively%2520turns%2520a%2520numerical%2520code%2520into%2520a%2520style%2520controller%252C%2520demonstrating%2520a%2520style%2520is%2520worth%2520one%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Style%20is%20Worth%20One%20Code%3A%20Unlocking%20Code-to-Style%20Image%20Generation%20with%20Discrete%20Style%20Space&entry.906535625=Huijie%20Liu%20and%20Shuhao%20Cui%20and%20Haoxiang%20Cao%20and%20Shuai%20Ma%20and%20Kai%20Wu%20and%20Guoliang%20Kang&entry.1292438233=Innovative%20visual%20stylization%20is%20a%20cornerstone%20of%20artistic%20creation%2C%20yet%20generating%20novel%20and%20consistent%20visual%20styles%20remains%20a%20significant%20challenge.%20Existing%20generative%20approaches%20typically%20rely%20on%20lengthy%20textual%20prompts%2C%20reference%20images%2C%20or%20parameter-efficient%20fine-tuning%20to%20guide%20style-aware%20image%20generation%2C%20but%20often%20struggle%20with%20style%20consistency%2C%20limited%20creativity%2C%20and%20complex%20style%20representations.%20In%20this%20paper%2C%20we%20affirm%20that%20a%20style%20is%20worth%20one%20numerical%20code%20by%20introducing%20the%20novel%20task%2C%20code-to-style%20image%20generation%2C%20which%20produces%20images%20with%20novel%2C%20consistent%20visual%20styles%20conditioned%20solely%20on%20a%20numerical%20style%20code.%20To%20date%2C%20this%20field%20has%20only%20been%20primarily%20explored%20by%20the%20industry%20%28e.g.%2C%20Midjourney%29%2C%20with%20no%20open-source%20research%20from%20the%20academic%20community.%20To%20fill%20this%20gap%2C%20we%20propose%20CoTyle%2C%20the%20first%20open-source%20method%20for%20this%20task.%20Specifically%2C%20we%20first%20train%20a%20discrete%20style%20codebook%20from%20a%20collection%20of%20images%20to%20extract%20style%20embeddings.%20These%20embeddings%20serve%20as%20conditions%20for%20a%20text-to-image%20diffusion%20model%20%28T2I-DM%29%20to%20generate%20stylistic%20images.%20Subsequently%2C%20we%20train%20an%20autoregressive%20style%20generator%20on%20the%20discrete%20style%20embeddings%20to%20model%20their%20distribution%2C%20allowing%20the%20synthesis%20of%20novel%20style%20embeddings.%20During%20inference%2C%20a%20numerical%20style%20code%20is%20mapped%20to%20a%20unique%20style%20embedding%20by%20the%20style%20generator%2C%20and%20this%20embedding%20guides%20the%20T2I-DM%20to%20generate%20images%20in%20the%20corresponding%20style.%20Unlike%20existing%20methods%2C%20our%20method%20offers%20unparalleled%20simplicity%20and%20diversity%2C%20unlocking%20a%20vast%20space%20of%20reproducible%20styles%20from%20minimal%20input.%20Extensive%20experiments%20validate%20that%20CoTyle%20effectively%20turns%20a%20numerical%20code%20into%20a%20style%20controller%2C%20demonstrating%20a%20style%20is%20worth%20one%20code.&entry.1838667208=http%3A//arxiv.org/abs/2511.10555v1&entry.124074799=Read"},
{"title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study", "author": "Yuqi Zhu and Yi Zhong and Jintian Zhang and Ziheng Zhang and Shuofei Qiao and Yujie Luo and Lun Du and Da Zheng and Ningyu Zhang and Huajun Chen", "abstract": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.", "link": "http://arxiv.org/abs/2506.19794v5", "date": "2025-11-13", "relevancy": 2.2085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5637}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%20Empirical%20Study&body=Title%3A%20Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%20Empirical%20Study%0AAuthor%3A%20Yuqi%20Zhu%20and%20Yi%20Zhong%20and%20Jintian%20Zhang%20and%20Ziheng%20Zhang%20and%20Shuofei%20Qiao%20and%20Yujie%20Luo%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20hold%20promise%20in%20automating%20data%20analysis%20tasks%2C%20yet%20open-source%20models%20face%20significant%20limitations%20in%20these%20kinds%20of%20reasoning-intensive%20scenarios.%20In%20this%20work%2C%20we%20investigate%20strategies%20to%20enhance%20the%20data%20analysis%20capabilities%20of%20open-source%20LLMs.%20By%20curating%20a%20seed%20dataset%20of%20diverse%2C%20realistic%20scenarios%2C%20we%20evaluate%20model%20behavior%20across%20three%20core%20dimensions%3A%20data%20understanding%2C%20code%20generation%2C%20and%20strategic%20planning.%20Our%20analysis%20reveals%20three%20key%20findings%3A%20%281%29%20Strategic%20planning%20quality%20serves%20as%20the%20primary%20determinant%20of%20model%20performance%3B%20%282%29%20Interaction%20design%20and%20task%20complexity%20significantly%20influence%20reasoning%20capabilities%3B%20%283%29%20Data%20quality%20demonstrates%20a%20greater%20impact%20than%20diversity%20in%20achieving%20optimal%20performance.%20We%20leverage%20these%20insights%20to%20develop%20a%20data%20synthesis%20methodology%2C%20demonstrating%20significant%20improvements%20in%20open-source%20LLMs%27%20analytical%20reasoning%20capabilities.%20Code%20is%20available%20at%20https%3A//github.com/zjunlp/DataMind.%0ALink%3A%20http%3A//arxiv.org/abs/2506.19794v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Do%2520Open-Source%2520LLMs%2520Struggle%2520with%2520Data%2520Analysis%253F%2520A%2520Systematic%2520Empirical%2520Study%26entry.906535625%3DYuqi%2520Zhu%2520and%2520Yi%2520Zhong%2520and%2520Jintian%2520Zhang%2520and%2520Ziheng%2520Zhang%2520and%2520Shuofei%2520Qiao%2520and%2520Yujie%2520Luo%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Ningyu%2520Zhang%2520and%2520Huajun%2520Chen%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520hold%2520promise%2520in%2520automating%2520data%2520analysis%2520tasks%252C%2520yet%2520open-source%2520models%2520face%2520significant%2520limitations%2520in%2520these%2520kinds%2520of%2520reasoning-intensive%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520investigate%2520strategies%2520to%2520enhance%2520the%2520data%2520analysis%2520capabilities%2520of%2520open-source%2520LLMs.%2520By%2520curating%2520a%2520seed%2520dataset%2520of%2520diverse%252C%2520realistic%2520scenarios%252C%2520we%2520evaluate%2520model%2520behavior%2520across%2520three%2520core%2520dimensions%253A%2520data%2520understanding%252C%2520code%2520generation%252C%2520and%2520strategic%2520planning.%2520Our%2520analysis%2520reveals%2520three%2520key%2520findings%253A%2520%25281%2529%2520Strategic%2520planning%2520quality%2520serves%2520as%2520the%2520primary%2520determinant%2520of%2520model%2520performance%253B%2520%25282%2529%2520Interaction%2520design%2520and%2520task%2520complexity%2520significantly%2520influence%2520reasoning%2520capabilities%253B%2520%25283%2529%2520Data%2520quality%2520demonstrates%2520a%2520greater%2520impact%2520than%2520diversity%2520in%2520achieving%2520optimal%2520performance.%2520We%2520leverage%2520these%2520insights%2520to%2520develop%2520a%2520data%2520synthesis%2520methodology%252C%2520demonstrating%2520significant%2520improvements%2520in%2520open-source%2520LLMs%2527%2520analytical%2520reasoning%2520capabilities.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/zjunlp/DataMind.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19794v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%20Empirical%20Study&entry.906535625=Yuqi%20Zhu%20and%20Yi%20Zhong%20and%20Jintian%20Zhang%20and%20Ziheng%20Zhang%20and%20Shuofei%20Qiao%20and%20Yujie%20Luo%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Ningyu%20Zhang%20and%20Huajun%20Chen&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20hold%20promise%20in%20automating%20data%20analysis%20tasks%2C%20yet%20open-source%20models%20face%20significant%20limitations%20in%20these%20kinds%20of%20reasoning-intensive%20scenarios.%20In%20this%20work%2C%20we%20investigate%20strategies%20to%20enhance%20the%20data%20analysis%20capabilities%20of%20open-source%20LLMs.%20By%20curating%20a%20seed%20dataset%20of%20diverse%2C%20realistic%20scenarios%2C%20we%20evaluate%20model%20behavior%20across%20three%20core%20dimensions%3A%20data%20understanding%2C%20code%20generation%2C%20and%20strategic%20planning.%20Our%20analysis%20reveals%20three%20key%20findings%3A%20%281%29%20Strategic%20planning%20quality%20serves%20as%20the%20primary%20determinant%20of%20model%20performance%3B%20%282%29%20Interaction%20design%20and%20task%20complexity%20significantly%20influence%20reasoning%20capabilities%3B%20%283%29%20Data%20quality%20demonstrates%20a%20greater%20impact%20than%20diversity%20in%20achieving%20optimal%20performance.%20We%20leverage%20these%20insights%20to%20develop%20a%20data%20synthesis%20methodology%2C%20demonstrating%20significant%20improvements%20in%20open-source%20LLMs%27%20analytical%20reasoning%20capabilities.%20Code%20is%20available%20at%20https%3A//github.com/zjunlp/DataMind.&entry.1838667208=http%3A//arxiv.org/abs/2506.19794v5&entry.124074799=Read"},
{"title": "Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands", "author": "Tishya Chhabra and Manisha Bajpai and Walter Zesk and Skylar Tibbits", "abstract": "We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.", "link": "http://arxiv.org/abs/2511.10177v1", "date": "2025-11-13", "relevancy": 2.2063, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20a%20Geospatial%20Foundation%20Model%20for%20Coastline%20Delineation%20in%20Small%20Sandy%20Islands&body=Title%3A%20Utilizing%20a%20Geospatial%20Foundation%20Model%20for%20Coastline%20Delineation%20in%20Small%20Sandy%20Islands%0AAuthor%3A%20Tishya%20Chhabra%20and%20Manisha%20Bajpai%20and%20Walter%20Zesk%20and%20Skylar%20Tibbits%0AAbstract%3A%20We%20present%20an%20initial%20evaluation%20of%20NASA%20and%20IBM%27s%20Prithvi-EO-2.0%20geospatial%20foundation%20model%20on%20shoreline%20delineation%20of%20small%20sandy%20islands%20using%20satellite%20images.%20We%20curated%20and%20labeled%20a%20dataset%20of%20225%20multispectral%20images%20of%20two%20Maldivian%20islands%2C%20which%20we%20publicly%20release%2C%20and%20fine-tuned%20both%20the%20300M%20and%20600M%20parameter%20versions%20of%20Prithvi%20on%20training%20subsets%20ranging%20from%205%20to%20181%20images.%20Our%20experiments%20show%20that%20even%20with%20as%20few%20as%205%20training%20images%2C%20the%20models%20achieve%20high%20performance%20%28F1%20of%200.94%2C%20IoU%20of%200.79%29.%20Our%20results%20demonstrate%20the%20strong%20transfer%20learning%20capability%20of%20Prithvi%2C%20underscoring%20the%20potential%20of%20such%20models%20to%20support%20coastal%20monitoring%20in%20data-poor%20regions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520a%2520Geospatial%2520Foundation%2520Model%2520for%2520Coastline%2520Delineation%2520in%2520Small%2520Sandy%2520Islands%26entry.906535625%3DTishya%2520Chhabra%2520and%2520Manisha%2520Bajpai%2520and%2520Walter%2520Zesk%2520and%2520Skylar%2520Tibbits%26entry.1292438233%3DWe%2520present%2520an%2520initial%2520evaluation%2520of%2520NASA%2520and%2520IBM%2527s%2520Prithvi-EO-2.0%2520geospatial%2520foundation%2520model%2520on%2520shoreline%2520delineation%2520of%2520small%2520sandy%2520islands%2520using%2520satellite%2520images.%2520We%2520curated%2520and%2520labeled%2520a%2520dataset%2520of%2520225%2520multispectral%2520images%2520of%2520two%2520Maldivian%2520islands%252C%2520which%2520we%2520publicly%2520release%252C%2520and%2520fine-tuned%2520both%2520the%2520300M%2520and%2520600M%2520parameter%2520versions%2520of%2520Prithvi%2520on%2520training%2520subsets%2520ranging%2520from%25205%2520to%2520181%2520images.%2520Our%2520experiments%2520show%2520that%2520even%2520with%2520as%2520few%2520as%25205%2520training%2520images%252C%2520the%2520models%2520achieve%2520high%2520performance%2520%2528F1%2520of%25200.94%252C%2520IoU%2520of%25200.79%2529.%2520Our%2520results%2520demonstrate%2520the%2520strong%2520transfer%2520learning%2520capability%2520of%2520Prithvi%252C%2520underscoring%2520the%2520potential%2520of%2520such%2520models%2520to%2520support%2520coastal%2520monitoring%2520in%2520data-poor%2520regions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20a%20Geospatial%20Foundation%20Model%20for%20Coastline%20Delineation%20in%20Small%20Sandy%20Islands&entry.906535625=Tishya%20Chhabra%20and%20Manisha%20Bajpai%20and%20Walter%20Zesk%20and%20Skylar%20Tibbits&entry.1292438233=We%20present%20an%20initial%20evaluation%20of%20NASA%20and%20IBM%27s%20Prithvi-EO-2.0%20geospatial%20foundation%20model%20on%20shoreline%20delineation%20of%20small%20sandy%20islands%20using%20satellite%20images.%20We%20curated%20and%20labeled%20a%20dataset%20of%20225%20multispectral%20images%20of%20two%20Maldivian%20islands%2C%20which%20we%20publicly%20release%2C%20and%20fine-tuned%20both%20the%20300M%20and%20600M%20parameter%20versions%20of%20Prithvi%20on%20training%20subsets%20ranging%20from%205%20to%20181%20images.%20Our%20experiments%20show%20that%20even%20with%20as%20few%20as%205%20training%20images%2C%20the%20models%20achieve%20high%20performance%20%28F1%20of%200.94%2C%20IoU%20of%200.79%29.%20Our%20results%20demonstrate%20the%20strong%20transfer%20learning%20capability%20of%20Prithvi%2C%20underscoring%20the%20potential%20of%20such%20models%20to%20support%20coastal%20monitoring%20in%20data-poor%20regions.&entry.1838667208=http%3A//arxiv.org/abs/2511.10177v1&entry.124074799=Read"},
{"title": "An Efficient Training Pipeline for Reasoning Graphical User Interface Agents", "author": "Georgios Pantazopoulos and Eda B. \u00d6zyi\u011fit", "abstract": "Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic datasets.This work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought-augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.", "link": "http://arxiv.org/abs/2511.08172v2", "date": "2025-11-13", "relevancy": 2.1892, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5551}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5421}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Training%20Pipeline%20for%20Reasoning%20Graphical%20User%20Interface%20Agents&body=Title%3A%20An%20Efficient%20Training%20Pipeline%20for%20Reasoning%20Graphical%20User%20Interface%20Agents%0AAuthor%3A%20Georgios%20Pantazopoulos%20and%20Eda%20B.%20%C3%96zyi%C4%9Fit%0AAbstract%3A%20Visual%20grounding%20is%20the%20task%20of%20localising%20image%20regions%20from%20natural%20language%20queries%20and%20is%20critical%20for%20reasoning%20capable%20Graphical%20User%20Interface%20agents.%20Many%20existing%20methods%20rely%20on%20massive%2C%20noisy%20synthetic%20datasets.This%20work%20introduces%20an%20efficient%20training%20pipeline%20that%20combines%20model-based%20data%20filtering%20with%20parameter-efficient%20fine-tuning.%20From%204.8M%20synthetic%20examples%2C%2012K%20clean%20and%20diverse%20instances%20are%20curated%20by%20first%20identifying%20challenging%20cases%2C%20removing%20misaligned%20and%20then%20selecting%20a%20diverse%20set%20of%20multimodal%20instances.%20On%20this%20data%2C%20a%203B-parameter%20Vision-Language%20Model%20is%20trained%20under%20three%20regimes%3A%20supervised%20fine-tuning%2C%20chain-of-thought-augmented%20fine-tuning%2C%20and%20reinforcement%20learning%20via%20Group%20Relative%20Policy%20Optimization.%20Models%20trained%20with%20the%20filtered%20data%20and%20lightweight%20training%20strategies%20match%20or%20surpass%20larger%20baselines%20on%20benchmarks%20such%20as%20ScreenSpot%2C%20Multimodal-Mind2Web%2C%20and%20AndroidControl.%20These%20results%20demonstrate%20that%20principled%20data%20curation%20and%20robust%20adaptation%20can%20rival%20large-scale%20training%2C%20enabling%20compact%20yet%20capable%20multimodal%20reasoning%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08172v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Training%2520Pipeline%2520for%2520Reasoning%2520Graphical%2520User%2520Interface%2520Agents%26entry.906535625%3DGeorgios%2520Pantazopoulos%2520and%2520Eda%2520B.%2520%25C3%2596zyi%25C4%259Fit%26entry.1292438233%3DVisual%2520grounding%2520is%2520the%2520task%2520of%2520localising%2520image%2520regions%2520from%2520natural%2520language%2520queries%2520and%2520is%2520critical%2520for%2520reasoning%2520capable%2520Graphical%2520User%2520Interface%2520agents.%2520Many%2520existing%2520methods%2520rely%2520on%2520massive%252C%2520noisy%2520synthetic%2520datasets.This%2520work%2520introduces%2520an%2520efficient%2520training%2520pipeline%2520that%2520combines%2520model-based%2520data%2520filtering%2520with%2520parameter-efficient%2520fine-tuning.%2520From%25204.8M%2520synthetic%2520examples%252C%252012K%2520clean%2520and%2520diverse%2520instances%2520are%2520curated%2520by%2520first%2520identifying%2520challenging%2520cases%252C%2520removing%2520misaligned%2520and%2520then%2520selecting%2520a%2520diverse%2520set%2520of%2520multimodal%2520instances.%2520On%2520this%2520data%252C%2520a%25203B-parameter%2520Vision-Language%2520Model%2520is%2520trained%2520under%2520three%2520regimes%253A%2520supervised%2520fine-tuning%252C%2520chain-of-thought-augmented%2520fine-tuning%252C%2520and%2520reinforcement%2520learning%2520via%2520Group%2520Relative%2520Policy%2520Optimization.%2520Models%2520trained%2520with%2520the%2520filtered%2520data%2520and%2520lightweight%2520training%2520strategies%2520match%2520or%2520surpass%2520larger%2520baselines%2520on%2520benchmarks%2520such%2520as%2520ScreenSpot%252C%2520Multimodal-Mind2Web%252C%2520and%2520AndroidControl.%2520These%2520results%2520demonstrate%2520that%2520principled%2520data%2520curation%2520and%2520robust%2520adaptation%2520can%2520rival%2520large-scale%2520training%252C%2520enabling%2520compact%2520yet%2520capable%2520multimodal%2520reasoning%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08172v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Training%20Pipeline%20for%20Reasoning%20Graphical%20User%20Interface%20Agents&entry.906535625=Georgios%20Pantazopoulos%20and%20Eda%20B.%20%C3%96zyi%C4%9Fit&entry.1292438233=Visual%20grounding%20is%20the%20task%20of%20localising%20image%20regions%20from%20natural%20language%20queries%20and%20is%20critical%20for%20reasoning%20capable%20Graphical%20User%20Interface%20agents.%20Many%20existing%20methods%20rely%20on%20massive%2C%20noisy%20synthetic%20datasets.This%20work%20introduces%20an%20efficient%20training%20pipeline%20that%20combines%20model-based%20data%20filtering%20with%20parameter-efficient%20fine-tuning.%20From%204.8M%20synthetic%20examples%2C%2012K%20clean%20and%20diverse%20instances%20are%20curated%20by%20first%20identifying%20challenging%20cases%2C%20removing%20misaligned%20and%20then%20selecting%20a%20diverse%20set%20of%20multimodal%20instances.%20On%20this%20data%2C%20a%203B-parameter%20Vision-Language%20Model%20is%20trained%20under%20three%20regimes%3A%20supervised%20fine-tuning%2C%20chain-of-thought-augmented%20fine-tuning%2C%20and%20reinforcement%20learning%20via%20Group%20Relative%20Policy%20Optimization.%20Models%20trained%20with%20the%20filtered%20data%20and%20lightweight%20training%20strategies%20match%20or%20surpass%20larger%20baselines%20on%20benchmarks%20such%20as%20ScreenSpot%2C%20Multimodal-Mind2Web%2C%20and%20AndroidControl.%20These%20results%20demonstrate%20that%20principled%20data%20curation%20and%20robust%20adaptation%20can%20rival%20large-scale%20training%2C%20enabling%20compact%20yet%20capable%20multimodal%20reasoning%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2511.08172v2&entry.124074799=Read"},
{"title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes", "author": "Tejas Sudharshan Mathai and Anisa V. Prasad and Xinya Wang and Praveen T. S. Balamuralikrishna and Yan Zhuang and Abhinav Suri and Jianfei Liu and Perry J. Pickhardt and Ronald M. Summers", "abstract": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.", "link": "http://arxiv.org/abs/2511.10484v1", "date": "2025-11-13", "relevancy": 2.189, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4315}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utility%20of%20Pancreas%20Surface%20Lobularity%20as%20a%20CT%20Biomarker%20for%20Opportunistic%20Screening%20of%20Type%202%20Diabetes&body=Title%3A%20Utility%20of%20Pancreas%20Surface%20Lobularity%20as%20a%20CT%20Biomarker%20for%20Opportunistic%20Screening%20of%20Type%202%20Diabetes%0AAuthor%3A%20Tejas%20Sudharshan%20Mathai%20and%20Anisa%20V.%20Prasad%20and%20Xinya%20Wang%20and%20Praveen%20T.%20S.%20Balamuralikrishna%20and%20Yan%20Zhuang%20and%20Abhinav%20Suri%20and%20Jianfei%20Liu%20and%20Perry%20J.%20Pickhardt%20and%20Ronald%20M.%20Summers%0AAbstract%3A%20Type%202%20Diabetes%20Mellitus%20%28T2DM%29%20is%20a%20chronic%20metabolic%20disease%20that%20affects%20millions%20of%20people%20worldwide.%20Early%20detection%20is%20crucial%20as%20it%20can%20alter%20pancreas%20function%20through%20morphological%20changes%20and%20increased%20deposition%20of%20ectopic%20fat%2C%20eventually%20leading%20to%20organ%20damage.%20While%20studies%20have%20shown%20an%20association%20between%20T2DM%20and%20pancreas%20volume%20and%20fat%20content%2C%20the%20role%20of%20increased%20pancreatic%20surface%20lobularity%20%28PSL%29%20in%20patients%20with%20T2DM%20has%20not%20been%20fully%20investigated.%20In%20this%20pilot%20work%2C%20we%20propose%20a%20fully%20automated%20approach%20to%20delineate%20the%20pancreas%20and%20other%20abdominal%20structures%2C%20derive%20CT%20imaging%20biomarkers%2C%20and%20opportunistically%20screen%20for%20T2DM.%20Four%20deep%20learning-based%20models%20were%20used%20to%20segment%20the%20pancreas%20in%20an%20internal%20dataset%20of%20584%20patients%20%28297%20males%2C%20437%20non-diabetic%2C%20age%3A%2045%24%5Cpm%2415%20years%29.%20PSL%20was%20automatically%20detected%20and%20it%20was%20higher%20for%20diabetic%20patients%20%28p%3D0.01%29%20at%204.26%20%24%5Cpm%24%208.32%20compared%20to%203.19%20%24%5Cpm%24%203.62%20for%20non-diabetic%20patients.%20The%20PancAP%20model%20achieved%20the%20highest%20Dice%20score%20of%200.79%20%24%5Cpm%24%200.17%20and%20lowest%20ASSD%20error%20of%201.94%20%24%5Cpm%24%202.63%20mm%20%28p%24%3C%240.05%29.%20For%20predicting%20T2DM%2C%20a%20multivariate%20model%20trained%20with%20CT%20biomarkers%20attained%200.90%20AUC%2C%2066.7%5C%25%20sensitivity%2C%20and%2091.9%5C%25%20specificity.%20Our%20results%20suggest%20that%20PSL%20is%20useful%20for%20T2DM%20screening%20and%20could%20potentially%20help%20predict%20the%20early%20onset%20of%20T2DM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtility%2520of%2520Pancreas%2520Surface%2520Lobularity%2520as%2520a%2520CT%2520Biomarker%2520for%2520Opportunistic%2520Screening%2520of%2520Type%25202%2520Diabetes%26entry.906535625%3DTejas%2520Sudharshan%2520Mathai%2520and%2520Anisa%2520V.%2520Prasad%2520and%2520Xinya%2520Wang%2520and%2520Praveen%2520T.%2520S.%2520Balamuralikrishna%2520and%2520Yan%2520Zhuang%2520and%2520Abhinav%2520Suri%2520and%2520Jianfei%2520Liu%2520and%2520Perry%2520J.%2520Pickhardt%2520and%2520Ronald%2520M.%2520Summers%26entry.1292438233%3DType%25202%2520Diabetes%2520Mellitus%2520%2528T2DM%2529%2520is%2520a%2520chronic%2520metabolic%2520disease%2520that%2520affects%2520millions%2520of%2520people%2520worldwide.%2520Early%2520detection%2520is%2520crucial%2520as%2520it%2520can%2520alter%2520pancreas%2520function%2520through%2520morphological%2520changes%2520and%2520increased%2520deposition%2520of%2520ectopic%2520fat%252C%2520eventually%2520leading%2520to%2520organ%2520damage.%2520While%2520studies%2520have%2520shown%2520an%2520association%2520between%2520T2DM%2520and%2520pancreas%2520volume%2520and%2520fat%2520content%252C%2520the%2520role%2520of%2520increased%2520pancreatic%2520surface%2520lobularity%2520%2528PSL%2529%2520in%2520patients%2520with%2520T2DM%2520has%2520not%2520been%2520fully%2520investigated.%2520In%2520this%2520pilot%2520work%252C%2520we%2520propose%2520a%2520fully%2520automated%2520approach%2520to%2520delineate%2520the%2520pancreas%2520and%2520other%2520abdominal%2520structures%252C%2520derive%2520CT%2520imaging%2520biomarkers%252C%2520and%2520opportunistically%2520screen%2520for%2520T2DM.%2520Four%2520deep%2520learning-based%2520models%2520were%2520used%2520to%2520segment%2520the%2520pancreas%2520in%2520an%2520internal%2520dataset%2520of%2520584%2520patients%2520%2528297%2520males%252C%2520437%2520non-diabetic%252C%2520age%253A%252045%2524%255Cpm%252415%2520years%2529.%2520PSL%2520was%2520automatically%2520detected%2520and%2520it%2520was%2520higher%2520for%2520diabetic%2520patients%2520%2528p%253D0.01%2529%2520at%25204.26%2520%2524%255Cpm%2524%25208.32%2520compared%2520to%25203.19%2520%2524%255Cpm%2524%25203.62%2520for%2520non-diabetic%2520patients.%2520The%2520PancAP%2520model%2520achieved%2520the%2520highest%2520Dice%2520score%2520of%25200.79%2520%2524%255Cpm%2524%25200.17%2520and%2520lowest%2520ASSD%2520error%2520of%25201.94%2520%2524%255Cpm%2524%25202.63%2520mm%2520%2528p%2524%253C%25240.05%2529.%2520For%2520predicting%2520T2DM%252C%2520a%2520multivariate%2520model%2520trained%2520with%2520CT%2520biomarkers%2520attained%25200.90%2520AUC%252C%252066.7%255C%2525%2520sensitivity%252C%2520and%252091.9%255C%2525%2520specificity.%2520Our%2520results%2520suggest%2520that%2520PSL%2520is%2520useful%2520for%2520T2DM%2520screening%2520and%2520could%2520potentially%2520help%2520predict%2520the%2520early%2520onset%2520of%2520T2DM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utility%20of%20Pancreas%20Surface%20Lobularity%20as%20a%20CT%20Biomarker%20for%20Opportunistic%20Screening%20of%20Type%202%20Diabetes&entry.906535625=Tejas%20Sudharshan%20Mathai%20and%20Anisa%20V.%20Prasad%20and%20Xinya%20Wang%20and%20Praveen%20T.%20S.%20Balamuralikrishna%20and%20Yan%20Zhuang%20and%20Abhinav%20Suri%20and%20Jianfei%20Liu%20and%20Perry%20J.%20Pickhardt%20and%20Ronald%20M.%20Summers&entry.1292438233=Type%202%20Diabetes%20Mellitus%20%28T2DM%29%20is%20a%20chronic%20metabolic%20disease%20that%20affects%20millions%20of%20people%20worldwide.%20Early%20detection%20is%20crucial%20as%20it%20can%20alter%20pancreas%20function%20through%20morphological%20changes%20and%20increased%20deposition%20of%20ectopic%20fat%2C%20eventually%20leading%20to%20organ%20damage.%20While%20studies%20have%20shown%20an%20association%20between%20T2DM%20and%20pancreas%20volume%20and%20fat%20content%2C%20the%20role%20of%20increased%20pancreatic%20surface%20lobularity%20%28PSL%29%20in%20patients%20with%20T2DM%20has%20not%20been%20fully%20investigated.%20In%20this%20pilot%20work%2C%20we%20propose%20a%20fully%20automated%20approach%20to%20delineate%20the%20pancreas%20and%20other%20abdominal%20structures%2C%20derive%20CT%20imaging%20biomarkers%2C%20and%20opportunistically%20screen%20for%20T2DM.%20Four%20deep%20learning-based%20models%20were%20used%20to%20segment%20the%20pancreas%20in%20an%20internal%20dataset%20of%20584%20patients%20%28297%20males%2C%20437%20non-diabetic%2C%20age%3A%2045%24%5Cpm%2415%20years%29.%20PSL%20was%20automatically%20detected%20and%20it%20was%20higher%20for%20diabetic%20patients%20%28p%3D0.01%29%20at%204.26%20%24%5Cpm%24%208.32%20compared%20to%203.19%20%24%5Cpm%24%203.62%20for%20non-diabetic%20patients.%20The%20PancAP%20model%20achieved%20the%20highest%20Dice%20score%20of%200.79%20%24%5Cpm%24%200.17%20and%20lowest%20ASSD%20error%20of%201.94%20%24%5Cpm%24%202.63%20mm%20%28p%24%3C%240.05%29.%20For%20predicting%20T2DM%2C%20a%20multivariate%20model%20trained%20with%20CT%20biomarkers%20attained%200.90%20AUC%2C%2066.7%5C%25%20sensitivity%2C%20and%2091.9%5C%25%20specificity.%20Our%20results%20suggest%20that%20PSL%20is%20useful%20for%20T2DM%20screening%20and%20could%20potentially%20help%20predict%20the%20early%20onset%20of%20T2DM.&entry.1838667208=http%3A//arxiv.org/abs/2511.10484v1&entry.124074799=Read"},
{"title": "HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction", "author": "Yueran Zhao and Zhang Zhang and Chao Sun and Tianze Wang and Chao Yue and Nuoran Li", "abstract": "Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.", "link": "http://arxiv.org/abs/2511.10211v1", "date": "2025-11-13", "relevancy": 2.1848, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeatV2X%3A%20Scalable%20Heterogeneous%20Collaborative%20Perception%20via%20Efficient%20Alignment%20and%20Interaction&body=Title%3A%20HeatV2X%3A%20Scalable%20Heterogeneous%20Collaborative%20Perception%20via%20Efficient%20Alignment%20and%20Interaction%0AAuthor%3A%20Yueran%20Zhao%20and%20Zhang%20Zhang%20and%20Chao%20Sun%20and%20Tianze%20Wang%20and%20Chao%20Yue%20and%20Nuoran%20Li%0AAbstract%3A%20Vehicle-to-Everything%20%28V2X%29%20collaborative%20perception%20extends%20sensing%20beyond%20single%20vehicle%20limits%20through%20transmission.%20However%2C%20as%20more%20agents%20participate%2C%20existing%20frameworks%20face%20two%20key%20challenges%3A%20%281%29%20the%20participating%20agents%20are%20inherently%20multi-modal%20and%20heterogeneous%2C%20and%20%282%29%20the%20collaborative%20framework%20must%20be%20scalable%20to%20accommodate%20new%20agents.%20The%20former%20requires%20effective%20cross-agent%20feature%20alignment%20to%20mitigate%20heterogeneity%20loss%2C%20while%20the%20latter%20renders%20full-parameter%20training%20impractical%2C%20highlighting%20the%20importance%20of%20scalable%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20Heterogeneous%20Adaptation%20%28HeatV2X%29%2C%20a%20scalable%20collaborative%20framework.%20We%20first%20train%20a%20high-performance%20agent%20based%20on%20heterogeneous%20graph%20attention%20as%20the%20foundation%20for%20collaborative%20learning.%20Then%2C%20we%20design%20Local%20Heterogeneous%20Fine-Tuning%20and%20Global%20Collaborative%20Fine-Tuning%20to%20achieve%20effective%20alignment%20and%20interaction%20among%20heterogeneous%20agents.%20The%20former%20efficiently%20extracts%20modality-specific%20differences%20using%20Hetero-Aware%20Adapters%2C%20while%20the%20latter%20employs%20the%20Multi-Cognitive%20Adapter%20to%20enhance%20cross-agent%20collaboration%20and%20fully%20exploit%20the%20fusion%20potential.%20These%20designs%20enable%20substantial%20performance%20improvement%20of%20the%20collaborative%20framework%20with%20minimal%20training%20cost.%20We%20evaluate%20our%20approach%20on%20the%20OPV2V-H%20and%20DAIR-V2X%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20perception%20performance%20with%20significantly%20reduced%20training%20overhead%2C%20outperforming%20existing%20state-of-the-art%20approaches.%20Our%20implementation%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeatV2X%253A%2520Scalable%2520Heterogeneous%2520Collaborative%2520Perception%2520via%2520Efficient%2520Alignment%2520and%2520Interaction%26entry.906535625%3DYueran%2520Zhao%2520and%2520Zhang%2520Zhang%2520and%2520Chao%2520Sun%2520and%2520Tianze%2520Wang%2520and%2520Chao%2520Yue%2520and%2520Nuoran%2520Li%26entry.1292438233%3DVehicle-to-Everything%2520%2528V2X%2529%2520collaborative%2520perception%2520extends%2520sensing%2520beyond%2520single%2520vehicle%2520limits%2520through%2520transmission.%2520However%252C%2520as%2520more%2520agents%2520participate%252C%2520existing%2520frameworks%2520face%2520two%2520key%2520challenges%253A%2520%25281%2529%2520the%2520participating%2520agents%2520are%2520inherently%2520multi-modal%2520and%2520heterogeneous%252C%2520and%2520%25282%2529%2520the%2520collaborative%2520framework%2520must%2520be%2520scalable%2520to%2520accommodate%2520new%2520agents.%2520The%2520former%2520requires%2520effective%2520cross-agent%2520feature%2520alignment%2520to%2520mitigate%2520heterogeneity%2520loss%252C%2520while%2520the%2520latter%2520renders%2520full-parameter%2520training%2520impractical%252C%2520highlighting%2520the%2520importance%2520of%2520scalable%2520adaptation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Heterogeneous%2520Adaptation%2520%2528HeatV2X%2529%252C%2520a%2520scalable%2520collaborative%2520framework.%2520We%2520first%2520train%2520a%2520high-performance%2520agent%2520based%2520on%2520heterogeneous%2520graph%2520attention%2520as%2520the%2520foundation%2520for%2520collaborative%2520learning.%2520Then%252C%2520we%2520design%2520Local%2520Heterogeneous%2520Fine-Tuning%2520and%2520Global%2520Collaborative%2520Fine-Tuning%2520to%2520achieve%2520effective%2520alignment%2520and%2520interaction%2520among%2520heterogeneous%2520agents.%2520The%2520former%2520efficiently%2520extracts%2520modality-specific%2520differences%2520using%2520Hetero-Aware%2520Adapters%252C%2520while%2520the%2520latter%2520employs%2520the%2520Multi-Cognitive%2520Adapter%2520to%2520enhance%2520cross-agent%2520collaboration%2520and%2520fully%2520exploit%2520the%2520fusion%2520potential.%2520These%2520designs%2520enable%2520substantial%2520performance%2520improvement%2520of%2520the%2520collaborative%2520framework%2520with%2520minimal%2520training%2520cost.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520OPV2V-H%2520and%2520DAIR-V2X%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520perception%2520performance%2520with%2520significantly%2520reduced%2520training%2520overhead%252C%2520outperforming%2520existing%2520state-of-the-art%2520approaches.%2520Our%2520implementation%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeatV2X%3A%20Scalable%20Heterogeneous%20Collaborative%20Perception%20via%20Efficient%20Alignment%20and%20Interaction&entry.906535625=Yueran%20Zhao%20and%20Zhang%20Zhang%20and%20Chao%20Sun%20and%20Tianze%20Wang%20and%20Chao%20Yue%20and%20Nuoran%20Li&entry.1292438233=Vehicle-to-Everything%20%28V2X%29%20collaborative%20perception%20extends%20sensing%20beyond%20single%20vehicle%20limits%20through%20transmission.%20However%2C%20as%20more%20agents%20participate%2C%20existing%20frameworks%20face%20two%20key%20challenges%3A%20%281%29%20the%20participating%20agents%20are%20inherently%20multi-modal%20and%20heterogeneous%2C%20and%20%282%29%20the%20collaborative%20framework%20must%20be%20scalable%20to%20accommodate%20new%20agents.%20The%20former%20requires%20effective%20cross-agent%20feature%20alignment%20to%20mitigate%20heterogeneity%20loss%2C%20while%20the%20latter%20renders%20full-parameter%20training%20impractical%2C%20highlighting%20the%20importance%20of%20scalable%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20Heterogeneous%20Adaptation%20%28HeatV2X%29%2C%20a%20scalable%20collaborative%20framework.%20We%20first%20train%20a%20high-performance%20agent%20based%20on%20heterogeneous%20graph%20attention%20as%20the%20foundation%20for%20collaborative%20learning.%20Then%2C%20we%20design%20Local%20Heterogeneous%20Fine-Tuning%20and%20Global%20Collaborative%20Fine-Tuning%20to%20achieve%20effective%20alignment%20and%20interaction%20among%20heterogeneous%20agents.%20The%20former%20efficiently%20extracts%20modality-specific%20differences%20using%20Hetero-Aware%20Adapters%2C%20while%20the%20latter%20employs%20the%20Multi-Cognitive%20Adapter%20to%20enhance%20cross-agent%20collaboration%20and%20fully%20exploit%20the%20fusion%20potential.%20These%20designs%20enable%20substantial%20performance%20improvement%20of%20the%20collaborative%20framework%20with%20minimal%20training%20cost.%20We%20evaluate%20our%20approach%20on%20the%20OPV2V-H%20and%20DAIR-V2X%20datasets.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%20superior%20perception%20performance%20with%20significantly%20reduced%20training%20overhead%2C%20outperforming%20existing%20state-of-the-art%20approaches.%20Our%20implementation%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2511.10211v1&entry.124074799=Read"},
{"title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker", "author": "Qi Li and Shaheer U. Saeed and Yuliang Huang and Mingyuan Luo and Zhongnuo Yan and Jiongquan Chen and Xin Yang and Dong Ni and Nektarios Winter and Phuc Nguyen and Lucas Steinberger and Caelan Haney and Yuan Zhao and Mingjie Jiang and Bowen Ren and SiYeoul Lee and Seonho Kim and MinKyung Seo and MinWoo Kim and Yimeng Dou and Zhiwei Zhang and Yin Li and Tomy Varghese and Dean C. Barratt and Matthew J. Clarkson and Tom Vercauteren and Yipeng Hu", "abstract": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes from sequences of 2D ultrasound images without relying on external tracking systems. By eliminating the need for optical or electromagnetic trackers, this approach offers a low-cost, portable, and widely deployable alternative to more expensive volumetric ultrasound imaging systems, particularly valuable in resource-constrained clinical settings. However, predicting long-distance transformations and handling complex probe trajectories remain challenging. The TUS-REC2024 Challenge establishes the first benchmark for trackerless 3D freehand ultrasound reconstruction by providing a large publicly available dataset, along with a baseline model and a rigorous evaluation framework. By the submission deadline, the Challenge had attracted 43 registered teams, of which 6 teams submitted 21 valid dockerized solutions. The submitted methods span a wide range of approaches, including the state space model, the recurrent model, the registration-driven volume refinement, the attention mechanism, and the physics-informed model. This paper provides a comprehensive background introduction and literature review in the field, presents an overview of the challenge design and dataset, and offers a comparative analysis of submitted methods across multiple evaluation metrics. These analyses highlight both the progress and the current limitations of state-of-the-art approaches in this domain and provide insights for future research directions. All data and code are publicly available to facilitate ongoing development and reproducibility. As a live and evolving benchmark, it is designed to be continuously iterated and improved. The Challenge was held at MICCAI 2024 and is organised again at MICCAI 2025, reflecting its sustained commitment to advancing this field.", "link": "http://arxiv.org/abs/2506.21765v2", "date": "2025-11-13", "relevancy": 2.1792, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUS-REC2024%3A%20A%20Challenge%20to%20Reconstruct%203D%20Freehand%20Ultrasound%20Without%20External%20Tracker&body=Title%3A%20TUS-REC2024%3A%20A%20Challenge%20to%20Reconstruct%203D%20Freehand%20Ultrasound%20Without%20External%20Tracker%0AAuthor%3A%20Qi%20Li%20and%20Shaheer%20U.%20Saeed%20and%20Yuliang%20Huang%20and%20Mingyuan%20Luo%20and%20Zhongnuo%20Yan%20and%20Jiongquan%20Chen%20and%20Xin%20Yang%20and%20Dong%20Ni%20and%20Nektarios%20Winter%20and%20Phuc%20Nguyen%20and%20Lucas%20Steinberger%20and%20Caelan%20Haney%20and%20Yuan%20Zhao%20and%20Mingjie%20Jiang%20and%20Bowen%20Ren%20and%20SiYeoul%20Lee%20and%20Seonho%20Kim%20and%20MinKyung%20Seo%20and%20MinWoo%20Kim%20and%20Yimeng%20Dou%20and%20Zhiwei%20Zhang%20and%20Yin%20Li%20and%20Tomy%20Varghese%20and%20Dean%20C.%20Barratt%20and%20Matthew%20J.%20Clarkson%20and%20Tom%20Vercauteren%20and%20Yipeng%20Hu%0AAbstract%3A%20Trackerless%20freehand%20ultrasound%20reconstruction%20aims%20to%20reconstruct%203D%20volumes%20from%20sequences%20of%202D%20ultrasound%20images%20without%20relying%20on%20external%20tracking%20systems.%20By%20eliminating%20the%20need%20for%20optical%20or%20electromagnetic%20trackers%2C%20this%20approach%20offers%20a%20low-cost%2C%20portable%2C%20and%20widely%20deployable%20alternative%20to%20more%20expensive%20volumetric%20ultrasound%20imaging%20systems%2C%20particularly%20valuable%20in%20resource-constrained%20clinical%20settings.%20However%2C%20predicting%20long-distance%20transformations%20and%20handling%20complex%20probe%20trajectories%20remain%20challenging.%20The%20TUS-REC2024%20Challenge%20establishes%20the%20first%20benchmark%20for%20trackerless%203D%20freehand%20ultrasound%20reconstruction%20by%20providing%20a%20large%20publicly%20available%20dataset%2C%20along%20with%20a%20baseline%20model%20and%20a%20rigorous%20evaluation%20framework.%20By%20the%20submission%20deadline%2C%20the%20Challenge%20had%20attracted%2043%20registered%20teams%2C%20of%20which%206%20teams%20submitted%2021%20valid%20dockerized%20solutions.%20The%20submitted%20methods%20span%20a%20wide%20range%20of%20approaches%2C%20including%20the%20state%20space%20model%2C%20the%20recurrent%20model%2C%20the%20registration-driven%20volume%20refinement%2C%20the%20attention%20mechanism%2C%20and%20the%20physics-informed%20model.%20This%20paper%20provides%20a%20comprehensive%20background%20introduction%20and%20literature%20review%20in%20the%20field%2C%20presents%20an%20overview%20of%20the%20challenge%20design%20and%20dataset%2C%20and%20offers%20a%20comparative%20analysis%20of%20submitted%20methods%20across%20multiple%20evaluation%20metrics.%20These%20analyses%20highlight%20both%20the%20progress%20and%20the%20current%20limitations%20of%20state-of-the-art%20approaches%20in%20this%20domain%20and%20provide%20insights%20for%20future%20research%20directions.%20All%20data%20and%20code%20are%20publicly%20available%20to%20facilitate%20ongoing%20development%20and%20reproducibility.%20As%20a%20live%20and%20evolving%20benchmark%2C%20it%20is%20designed%20to%20be%20continuously%20iterated%20and%20improved.%20The%20Challenge%20was%20held%20at%20MICCAI%202024%20and%20is%20organised%20again%20at%20MICCAI%202025%2C%20reflecting%20its%20sustained%20commitment%20to%20advancing%20this%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2506.21765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUS-REC2024%253A%2520A%2520Challenge%2520to%2520Reconstruct%25203D%2520Freehand%2520Ultrasound%2520Without%2520External%2520Tracker%26entry.906535625%3DQi%2520Li%2520and%2520Shaheer%2520U.%2520Saeed%2520and%2520Yuliang%2520Huang%2520and%2520Mingyuan%2520Luo%2520and%2520Zhongnuo%2520Yan%2520and%2520Jiongquan%2520Chen%2520and%2520Xin%2520Yang%2520and%2520Dong%2520Ni%2520and%2520Nektarios%2520Winter%2520and%2520Phuc%2520Nguyen%2520and%2520Lucas%2520Steinberger%2520and%2520Caelan%2520Haney%2520and%2520Yuan%2520Zhao%2520and%2520Mingjie%2520Jiang%2520and%2520Bowen%2520Ren%2520and%2520SiYeoul%2520Lee%2520and%2520Seonho%2520Kim%2520and%2520MinKyung%2520Seo%2520and%2520MinWoo%2520Kim%2520and%2520Yimeng%2520Dou%2520and%2520Zhiwei%2520Zhang%2520and%2520Yin%2520Li%2520and%2520Tomy%2520Varghese%2520and%2520Dean%2520C.%2520Barratt%2520and%2520Matthew%2520J.%2520Clarkson%2520and%2520Tom%2520Vercauteren%2520and%2520Yipeng%2520Hu%26entry.1292438233%3DTrackerless%2520freehand%2520ultrasound%2520reconstruction%2520aims%2520to%2520reconstruct%25203D%2520volumes%2520from%2520sequences%2520of%25202D%2520ultrasound%2520images%2520without%2520relying%2520on%2520external%2520tracking%2520systems.%2520By%2520eliminating%2520the%2520need%2520for%2520optical%2520or%2520electromagnetic%2520trackers%252C%2520this%2520approach%2520offers%2520a%2520low-cost%252C%2520portable%252C%2520and%2520widely%2520deployable%2520alternative%2520to%2520more%2520expensive%2520volumetric%2520ultrasound%2520imaging%2520systems%252C%2520particularly%2520valuable%2520in%2520resource-constrained%2520clinical%2520settings.%2520However%252C%2520predicting%2520long-distance%2520transformations%2520and%2520handling%2520complex%2520probe%2520trajectories%2520remain%2520challenging.%2520The%2520TUS-REC2024%2520Challenge%2520establishes%2520the%2520first%2520benchmark%2520for%2520trackerless%25203D%2520freehand%2520ultrasound%2520reconstruction%2520by%2520providing%2520a%2520large%2520publicly%2520available%2520dataset%252C%2520along%2520with%2520a%2520baseline%2520model%2520and%2520a%2520rigorous%2520evaluation%2520framework.%2520By%2520the%2520submission%2520deadline%252C%2520the%2520Challenge%2520had%2520attracted%252043%2520registered%2520teams%252C%2520of%2520which%25206%2520teams%2520submitted%252021%2520valid%2520dockerized%2520solutions.%2520The%2520submitted%2520methods%2520span%2520a%2520wide%2520range%2520of%2520approaches%252C%2520including%2520the%2520state%2520space%2520model%252C%2520the%2520recurrent%2520model%252C%2520the%2520registration-driven%2520volume%2520refinement%252C%2520the%2520attention%2520mechanism%252C%2520and%2520the%2520physics-informed%2520model.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520background%2520introduction%2520and%2520literature%2520review%2520in%2520the%2520field%252C%2520presents%2520an%2520overview%2520of%2520the%2520challenge%2520design%2520and%2520dataset%252C%2520and%2520offers%2520a%2520comparative%2520analysis%2520of%2520submitted%2520methods%2520across%2520multiple%2520evaluation%2520metrics.%2520These%2520analyses%2520highlight%2520both%2520the%2520progress%2520and%2520the%2520current%2520limitations%2520of%2520state-of-the-art%2520approaches%2520in%2520this%2520domain%2520and%2520provide%2520insights%2520for%2520future%2520research%2520directions.%2520All%2520data%2520and%2520code%2520are%2520publicly%2520available%2520to%2520facilitate%2520ongoing%2520development%2520and%2520reproducibility.%2520As%2520a%2520live%2520and%2520evolving%2520benchmark%252C%2520it%2520is%2520designed%2520to%2520be%2520continuously%2520iterated%2520and%2520improved.%2520The%2520Challenge%2520was%2520held%2520at%2520MICCAI%25202024%2520and%2520is%2520organised%2520again%2520at%2520MICCAI%25202025%252C%2520reflecting%2520its%2520sustained%2520commitment%2520to%2520advancing%2520this%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUS-REC2024%3A%20A%20Challenge%20to%20Reconstruct%203D%20Freehand%20Ultrasound%20Without%20External%20Tracker&entry.906535625=Qi%20Li%20and%20Shaheer%20U.%20Saeed%20and%20Yuliang%20Huang%20and%20Mingyuan%20Luo%20and%20Zhongnuo%20Yan%20and%20Jiongquan%20Chen%20and%20Xin%20Yang%20and%20Dong%20Ni%20and%20Nektarios%20Winter%20and%20Phuc%20Nguyen%20and%20Lucas%20Steinberger%20and%20Caelan%20Haney%20and%20Yuan%20Zhao%20and%20Mingjie%20Jiang%20and%20Bowen%20Ren%20and%20SiYeoul%20Lee%20and%20Seonho%20Kim%20and%20MinKyung%20Seo%20and%20MinWoo%20Kim%20and%20Yimeng%20Dou%20and%20Zhiwei%20Zhang%20and%20Yin%20Li%20and%20Tomy%20Varghese%20and%20Dean%20C.%20Barratt%20and%20Matthew%20J.%20Clarkson%20and%20Tom%20Vercauteren%20and%20Yipeng%20Hu&entry.1292438233=Trackerless%20freehand%20ultrasound%20reconstruction%20aims%20to%20reconstruct%203D%20volumes%20from%20sequences%20of%202D%20ultrasound%20images%20without%20relying%20on%20external%20tracking%20systems.%20By%20eliminating%20the%20need%20for%20optical%20or%20electromagnetic%20trackers%2C%20this%20approach%20offers%20a%20low-cost%2C%20portable%2C%20and%20widely%20deployable%20alternative%20to%20more%20expensive%20volumetric%20ultrasound%20imaging%20systems%2C%20particularly%20valuable%20in%20resource-constrained%20clinical%20settings.%20However%2C%20predicting%20long-distance%20transformations%20and%20handling%20complex%20probe%20trajectories%20remain%20challenging.%20The%20TUS-REC2024%20Challenge%20establishes%20the%20first%20benchmark%20for%20trackerless%203D%20freehand%20ultrasound%20reconstruction%20by%20providing%20a%20large%20publicly%20available%20dataset%2C%20along%20with%20a%20baseline%20model%20and%20a%20rigorous%20evaluation%20framework.%20By%20the%20submission%20deadline%2C%20the%20Challenge%20had%20attracted%2043%20registered%20teams%2C%20of%20which%206%20teams%20submitted%2021%20valid%20dockerized%20solutions.%20The%20submitted%20methods%20span%20a%20wide%20range%20of%20approaches%2C%20including%20the%20state%20space%20model%2C%20the%20recurrent%20model%2C%20the%20registration-driven%20volume%20refinement%2C%20the%20attention%20mechanism%2C%20and%20the%20physics-informed%20model.%20This%20paper%20provides%20a%20comprehensive%20background%20introduction%20and%20literature%20review%20in%20the%20field%2C%20presents%20an%20overview%20of%20the%20challenge%20design%20and%20dataset%2C%20and%20offers%20a%20comparative%20analysis%20of%20submitted%20methods%20across%20multiple%20evaluation%20metrics.%20These%20analyses%20highlight%20both%20the%20progress%20and%20the%20current%20limitations%20of%20state-of-the-art%20approaches%20in%20this%20domain%20and%20provide%20insights%20for%20future%20research%20directions.%20All%20data%20and%20code%20are%20publicly%20available%20to%20facilitate%20ongoing%20development%20and%20reproducibility.%20As%20a%20live%20and%20evolving%20benchmark%2C%20it%20is%20designed%20to%20be%20continuously%20iterated%20and%20improved.%20The%20Challenge%20was%20held%20at%20MICCAI%202024%20and%20is%20organised%20again%20at%20MICCAI%202025%2C%20reflecting%20its%20sustained%20commitment%20to%20advancing%20this%20field.&entry.1838667208=http%3A//arxiv.org/abs/2506.21765v2&entry.124074799=Read"},
{"title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning", "author": "Vincenzo Carletti and Pasquale Foggia and Carlo Mazzocca and Giuseppe Parrella and Mario Vento", "abstract": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.", "link": "http://arxiv.org/abs/2511.10502v1", "date": "2025-11-13", "relevancy": 2.1791, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4476}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4318}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Detectability%20of%20Active%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning&body=Title%3A%20On%20the%20Detectability%20of%20Active%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning%0AAuthor%3A%20Vincenzo%20Carletti%20and%20Pasquale%20Foggia%20and%20Carlo%20Mazzocca%20and%20Giuseppe%20Parrella%20and%20Mario%20Vento%0AAbstract%3A%20One%20of%20the%20key%20advantages%20of%20Federated%20Learning%20%28FL%29%20is%20its%20ability%20to%20collaboratively%20train%20a%20Machine%20Learning%20%28ML%29%20model%20while%20keeping%20clients%27%20data%20on-site.%20However%2C%20this%20can%20create%20a%20false%20sense%20of%20security.%20Despite%20not%20sharing%20private%20data%20increases%20the%20overall%20privacy%2C%20prior%20studies%20have%20shown%20that%20gradients%20exchanged%20during%20the%20FL%20training%20remain%20vulnerable%20to%20Gradient%20Inversion%20Attacks%20%28GIAs%29.%20These%20attacks%20allow%20reconstructing%20the%20clients%27%20local%20data%2C%20breaking%20the%20privacy%20promise%20of%20FL.%20GIAs%20can%20be%20launched%20by%20either%20a%20passive%20or%20an%20active%20server.%20In%20the%20latter%20case%2C%20a%20malicious%20server%20manipulates%20the%20global%20model%20to%20facilitate%20data%20reconstruction.%20While%20effective%2C%20earlier%20attacks%20falling%20under%20this%20category%20have%20been%20demonstrated%20to%20be%20detectable%20by%20clients%2C%20limiting%20their%20real-world%20applicability.%20Recently%2C%20novel%20active%20GIAs%20have%20emerged%2C%20claiming%20to%20be%20far%20stealthier%20than%20previous%20approaches.%20This%20work%20provides%20the%20first%20comprehensive%20analysis%20of%20these%20claims%2C%20investigating%20four%20state-of-the-art%20GIAs.%20We%20propose%20novel%20lightweight%20client-side%20detection%20techniques%2C%20based%20on%20statistically%20improbable%20weight%20structures%20and%20anomalous%20loss%20and%20gradient%20dynamics.%20Extensive%20evaluation%20across%20several%20configurations%20demonstrates%20that%20our%20methods%20enable%20clients%20to%20effectively%20detect%20active%20GIAs%20without%20any%20modifications%20to%20the%20FL%20training%20protocol.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Detectability%2520of%2520Active%2520Gradient%2520Inversion%2520Attacks%2520in%2520Federated%2520Learning%26entry.906535625%3DVincenzo%2520Carletti%2520and%2520Pasquale%2520Foggia%2520and%2520Carlo%2520Mazzocca%2520and%2520Giuseppe%2520Parrella%2520and%2520Mario%2520Vento%26entry.1292438233%3DOne%2520of%2520the%2520key%2520advantages%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520its%2520ability%2520to%2520collaboratively%2520train%2520a%2520Machine%2520Learning%2520%2528ML%2529%2520model%2520while%2520keeping%2520clients%2527%2520data%2520on-site.%2520However%252C%2520this%2520can%2520create%2520a%2520false%2520sense%2520of%2520security.%2520Despite%2520not%2520sharing%2520private%2520data%2520increases%2520the%2520overall%2520privacy%252C%2520prior%2520studies%2520have%2520shown%2520that%2520gradients%2520exchanged%2520during%2520the%2520FL%2520training%2520remain%2520vulnerable%2520to%2520Gradient%2520Inversion%2520Attacks%2520%2528GIAs%2529.%2520These%2520attacks%2520allow%2520reconstructing%2520the%2520clients%2527%2520local%2520data%252C%2520breaking%2520the%2520privacy%2520promise%2520of%2520FL.%2520GIAs%2520can%2520be%2520launched%2520by%2520either%2520a%2520passive%2520or%2520an%2520active%2520server.%2520In%2520the%2520latter%2520case%252C%2520a%2520malicious%2520server%2520manipulates%2520the%2520global%2520model%2520to%2520facilitate%2520data%2520reconstruction.%2520While%2520effective%252C%2520earlier%2520attacks%2520falling%2520under%2520this%2520category%2520have%2520been%2520demonstrated%2520to%2520be%2520detectable%2520by%2520clients%252C%2520limiting%2520their%2520real-world%2520applicability.%2520Recently%252C%2520novel%2520active%2520GIAs%2520have%2520emerged%252C%2520claiming%2520to%2520be%2520far%2520stealthier%2520than%2520previous%2520approaches.%2520This%2520work%2520provides%2520the%2520first%2520comprehensive%2520analysis%2520of%2520these%2520claims%252C%2520investigating%2520four%2520state-of-the-art%2520GIAs.%2520We%2520propose%2520novel%2520lightweight%2520client-side%2520detection%2520techniques%252C%2520based%2520on%2520statistically%2520improbable%2520weight%2520structures%2520and%2520anomalous%2520loss%2520and%2520gradient%2520dynamics.%2520Extensive%2520evaluation%2520across%2520several%2520configurations%2520demonstrates%2520that%2520our%2520methods%2520enable%2520clients%2520to%2520effectively%2520detect%2520active%2520GIAs%2520without%2520any%2520modifications%2520to%2520the%2520FL%2520training%2520protocol.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Detectability%20of%20Active%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning&entry.906535625=Vincenzo%20Carletti%20and%20Pasquale%20Foggia%20and%20Carlo%20Mazzocca%20and%20Giuseppe%20Parrella%20and%20Mario%20Vento&entry.1292438233=One%20of%20the%20key%20advantages%20of%20Federated%20Learning%20%28FL%29%20is%20its%20ability%20to%20collaboratively%20train%20a%20Machine%20Learning%20%28ML%29%20model%20while%20keeping%20clients%27%20data%20on-site.%20However%2C%20this%20can%20create%20a%20false%20sense%20of%20security.%20Despite%20not%20sharing%20private%20data%20increases%20the%20overall%20privacy%2C%20prior%20studies%20have%20shown%20that%20gradients%20exchanged%20during%20the%20FL%20training%20remain%20vulnerable%20to%20Gradient%20Inversion%20Attacks%20%28GIAs%29.%20These%20attacks%20allow%20reconstructing%20the%20clients%27%20local%20data%2C%20breaking%20the%20privacy%20promise%20of%20FL.%20GIAs%20can%20be%20launched%20by%20either%20a%20passive%20or%20an%20active%20server.%20In%20the%20latter%20case%2C%20a%20malicious%20server%20manipulates%20the%20global%20model%20to%20facilitate%20data%20reconstruction.%20While%20effective%2C%20earlier%20attacks%20falling%20under%20this%20category%20have%20been%20demonstrated%20to%20be%20detectable%20by%20clients%2C%20limiting%20their%20real-world%20applicability.%20Recently%2C%20novel%20active%20GIAs%20have%20emerged%2C%20claiming%20to%20be%20far%20stealthier%20than%20previous%20approaches.%20This%20work%20provides%20the%20first%20comprehensive%20analysis%20of%20these%20claims%2C%20investigating%20four%20state-of-the-art%20GIAs.%20We%20propose%20novel%20lightweight%20client-side%20detection%20techniques%2C%20based%20on%20statistically%20improbable%20weight%20structures%20and%20anomalous%20loss%20and%20gradient%20dynamics.%20Extensive%20evaluation%20across%20several%20configurations%20demonstrates%20that%20our%20methods%20enable%20clients%20to%20effectively%20detect%20active%20GIAs%20without%20any%20modifications%20to%20the%20FL%20training%20protocol.&entry.1838667208=http%3A//arxiv.org/abs/2511.10502v1&entry.124074799=Read"},
{"title": "SAMIRO: Spatial Attention Mutual Information Regularization with a Pre-trained Model as Oracle for Lane Detection", "author": "Hyunjong Lee and Jangho Lee and Jaekoo Lee", "abstract": "Lane detection is an important topic in the future mobility solutions. Real-world environmental challenges such as background clutter, varying illumination, and occlusions pose significant obstacles to effective lane detection, particularly when relying on data-driven approaches that require substantial effort and cost for data collection and annotation. To address these issues, lane detection methods must leverage contextual and global information from surrounding lanes and objects. In this paper, we propose a Spatial Attention Mutual Information Regularization with a pre-trained model as an Oracle, called SAMIRO. SAMIRO enhances lane detection performance by transferring knowledge from a pretrained model while preserving domain-agnostic spatial information. Leveraging SAMIRO's plug-and-play characteristic, we integrate it into various state-of-the-art lane detection approaches and conduct extensive experiments on major benchmarks such as CULane, Tusimple, and LLAMAS. The results demonstrate that SAMIRO consistently improves performance across different models and datasets. The code will be made available upon publication.", "link": "http://arxiv.org/abs/2511.10385v1", "date": "2025-11-13", "relevancy": 2.1763, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMIRO%3A%20Spatial%20Attention%20Mutual%20Information%20Regularization%20with%20a%20Pre-trained%20Model%20as%20Oracle%20for%20Lane%20Detection&body=Title%3A%20SAMIRO%3A%20Spatial%20Attention%20Mutual%20Information%20Regularization%20with%20a%20Pre-trained%20Model%20as%20Oracle%20for%20Lane%20Detection%0AAuthor%3A%20Hyunjong%20Lee%20and%20Jangho%20Lee%20and%20Jaekoo%20Lee%0AAbstract%3A%20Lane%20detection%20is%20an%20important%20topic%20in%20the%20future%20mobility%20solutions.%20Real-world%20environmental%20challenges%20such%20as%20background%20clutter%2C%20varying%20illumination%2C%20and%20occlusions%20pose%20significant%20obstacles%20to%20effective%20lane%20detection%2C%20particularly%20when%20relying%20on%20data-driven%20approaches%20that%20require%20substantial%20effort%20and%20cost%20for%20data%20collection%20and%20annotation.%20To%20address%20these%20issues%2C%20lane%20detection%20methods%20must%20leverage%20contextual%20and%20global%20information%20from%20surrounding%20lanes%20and%20objects.%20In%20this%20paper%2C%20we%20propose%20a%20Spatial%20Attention%20Mutual%20Information%20Regularization%20with%20a%20pre-trained%20model%20as%20an%20Oracle%2C%20called%20SAMIRO.%20SAMIRO%20enhances%20lane%20detection%20performance%20by%20transferring%20knowledge%20from%20a%20pretrained%20model%20while%20preserving%20domain-agnostic%20spatial%20information.%20Leveraging%20SAMIRO%27s%20plug-and-play%20characteristic%2C%20we%20integrate%20it%20into%20various%20state-of-the-art%20lane%20detection%20approaches%20and%20conduct%20extensive%20experiments%20on%20major%20benchmarks%20such%20as%20CULane%2C%20Tusimple%2C%20and%20LLAMAS.%20The%20results%20demonstrate%20that%20SAMIRO%20consistently%20improves%20performance%20across%20different%20models%20and%20datasets.%20The%20code%20will%20be%20made%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMIRO%253A%2520Spatial%2520Attention%2520Mutual%2520Information%2520Regularization%2520with%2520a%2520Pre-trained%2520Model%2520as%2520Oracle%2520for%2520Lane%2520Detection%26entry.906535625%3DHyunjong%2520Lee%2520and%2520Jangho%2520Lee%2520and%2520Jaekoo%2520Lee%26entry.1292438233%3DLane%2520detection%2520is%2520an%2520important%2520topic%2520in%2520the%2520future%2520mobility%2520solutions.%2520Real-world%2520environmental%2520challenges%2520such%2520as%2520background%2520clutter%252C%2520varying%2520illumination%252C%2520and%2520occlusions%2520pose%2520significant%2520obstacles%2520to%2520effective%2520lane%2520detection%252C%2520particularly%2520when%2520relying%2520on%2520data-driven%2520approaches%2520that%2520require%2520substantial%2520effort%2520and%2520cost%2520for%2520data%2520collection%2520and%2520annotation.%2520To%2520address%2520these%2520issues%252C%2520lane%2520detection%2520methods%2520must%2520leverage%2520contextual%2520and%2520global%2520information%2520from%2520surrounding%2520lanes%2520and%2520objects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Spatial%2520Attention%2520Mutual%2520Information%2520Regularization%2520with%2520a%2520pre-trained%2520model%2520as%2520an%2520Oracle%252C%2520called%2520SAMIRO.%2520SAMIRO%2520enhances%2520lane%2520detection%2520performance%2520by%2520transferring%2520knowledge%2520from%2520a%2520pretrained%2520model%2520while%2520preserving%2520domain-agnostic%2520spatial%2520information.%2520Leveraging%2520SAMIRO%2527s%2520plug-and-play%2520characteristic%252C%2520we%2520integrate%2520it%2520into%2520various%2520state-of-the-art%2520lane%2520detection%2520approaches%2520and%2520conduct%2520extensive%2520experiments%2520on%2520major%2520benchmarks%2520such%2520as%2520CULane%252C%2520Tusimple%252C%2520and%2520LLAMAS.%2520The%2520results%2520demonstrate%2520that%2520SAMIRO%2520consistently%2520improves%2520performance%2520across%2520different%2520models%2520and%2520datasets.%2520The%2520code%2520will%2520be%2520made%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMIRO%3A%20Spatial%20Attention%20Mutual%20Information%20Regularization%20with%20a%20Pre-trained%20Model%20as%20Oracle%20for%20Lane%20Detection&entry.906535625=Hyunjong%20Lee%20and%20Jangho%20Lee%20and%20Jaekoo%20Lee&entry.1292438233=Lane%20detection%20is%20an%20important%20topic%20in%20the%20future%20mobility%20solutions.%20Real-world%20environmental%20challenges%20such%20as%20background%20clutter%2C%20varying%20illumination%2C%20and%20occlusions%20pose%20significant%20obstacles%20to%20effective%20lane%20detection%2C%20particularly%20when%20relying%20on%20data-driven%20approaches%20that%20require%20substantial%20effort%20and%20cost%20for%20data%20collection%20and%20annotation.%20To%20address%20these%20issues%2C%20lane%20detection%20methods%20must%20leverage%20contextual%20and%20global%20information%20from%20surrounding%20lanes%20and%20objects.%20In%20this%20paper%2C%20we%20propose%20a%20Spatial%20Attention%20Mutual%20Information%20Regularization%20with%20a%20pre-trained%20model%20as%20an%20Oracle%2C%20called%20SAMIRO.%20SAMIRO%20enhances%20lane%20detection%20performance%20by%20transferring%20knowledge%20from%20a%20pretrained%20model%20while%20preserving%20domain-agnostic%20spatial%20information.%20Leveraging%20SAMIRO%27s%20plug-and-play%20characteristic%2C%20we%20integrate%20it%20into%20various%20state-of-the-art%20lane%20detection%20approaches%20and%20conduct%20extensive%20experiments%20on%20major%20benchmarks%20such%20as%20CULane%2C%20Tusimple%2C%20and%20LLAMAS.%20The%20results%20demonstrate%20that%20SAMIRO%20consistently%20improves%20performance%20across%20different%20models%20and%20datasets.%20The%20code%20will%20be%20made%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2511.10385v1&entry.124074799=Read"},
{"title": "Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising", "author": "Yusuf Talha Basak and Mehmet Ozan Unal and Metin Ertas and Isa Yildirim", "abstract": "Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct", "link": "http://arxiv.org/abs/2511.10500v1", "date": "2025-11-13", "relevancy": 2.1737, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5496}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5426}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Total%20Variation%20with%20Lambda%20Mapping%20for%20Low-Dose%20CT%20Denoising&body=Title%3A%20Learnable%20Total%20Variation%20with%20Lambda%20Mapping%20for%20Low-Dose%20CT%20Denoising%0AAuthor%3A%20Yusuf%20Talha%20Basak%20and%20Mehmet%20Ozan%20Unal%20and%20Metin%20Ertas%20and%20Isa%20Yildirim%0AAbstract%3A%20Although%20Total%20Variation%20%28TV%29%20performs%20well%20in%20noise%20reduction%20and%20edge%20preservation%20on%20images%2C%20its%20dependence%20on%20the%20lambda%20parameter%20limits%20its%20efficiency%20and%20makes%20it%20difficult%20to%20use%20effectively.%20In%20this%20study%2C%20we%20present%20a%20Learnable%20Total%20Variation%20%28LTV%29%20framework%20that%20couples%20an%20unrolled%20TV%20solver%20with%20a%20data-driven%20Lambda%20Mapping%20Network%20%28LambdaNet%29%20predicting%20a%20per-pixel%20regularization%20map.%20The%20pipeline%20is%20trained%20end-to-end%20so%20that%20reconstruction%20and%20regularization%20are%20optimized%20jointly%2C%20yielding%20spatially%20adaptive%20smoothing%3A%20strong%20in%20homogeneous%20regions%2C%20relaxed%20near%20anatomical%20boundaries.%20Experiments%20on%20the%20DeepLesion%20dataset%2C%20using%20a%20realistic%20noise%20model%20adapted%20from%20the%20LoDoPaB-CT%20methodology%2C%20show%20consistent%20gains%20over%20classical%20TV%20and%20FBP%2BU-Net%3A%20%2B2.9%20dB%20PSNR%20and%20%2B6%25%20SSIM%20on%20average.%20LTV%20provides%20an%20interpretable%20alternative%20to%20black-box%20CNNs%20and%20a%20basis%20for%203D%20and%20data-consistency-driven%20reconstruction.%20Our%20codes%20are%20available%20at%3A%20https%3A//github.com/itu-biai/deep_tv_for_ldct%0ALink%3A%20http%3A//arxiv.org/abs/2511.10500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Total%2520Variation%2520with%2520Lambda%2520Mapping%2520for%2520Low-Dose%2520CT%2520Denoising%26entry.906535625%3DYusuf%2520Talha%2520Basak%2520and%2520Mehmet%2520Ozan%2520Unal%2520and%2520Metin%2520Ertas%2520and%2520Isa%2520Yildirim%26entry.1292438233%3DAlthough%2520Total%2520Variation%2520%2528TV%2529%2520performs%2520well%2520in%2520noise%2520reduction%2520and%2520edge%2520preservation%2520on%2520images%252C%2520its%2520dependence%2520on%2520the%2520lambda%2520parameter%2520limits%2520its%2520efficiency%2520and%2520makes%2520it%2520difficult%2520to%2520use%2520effectively.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520Learnable%2520Total%2520Variation%2520%2528LTV%2529%2520framework%2520that%2520couples%2520an%2520unrolled%2520TV%2520solver%2520with%2520a%2520data-driven%2520Lambda%2520Mapping%2520Network%2520%2528LambdaNet%2529%2520predicting%2520a%2520per-pixel%2520regularization%2520map.%2520The%2520pipeline%2520is%2520trained%2520end-to-end%2520so%2520that%2520reconstruction%2520and%2520regularization%2520are%2520optimized%2520jointly%252C%2520yielding%2520spatially%2520adaptive%2520smoothing%253A%2520strong%2520in%2520homogeneous%2520regions%252C%2520relaxed%2520near%2520anatomical%2520boundaries.%2520Experiments%2520on%2520the%2520DeepLesion%2520dataset%252C%2520using%2520a%2520realistic%2520noise%2520model%2520adapted%2520from%2520the%2520LoDoPaB-CT%2520methodology%252C%2520show%2520consistent%2520gains%2520over%2520classical%2520TV%2520and%2520FBP%252BU-Net%253A%2520%252B2.9%2520dB%2520PSNR%2520and%2520%252B6%2525%2520SSIM%2520on%2520average.%2520LTV%2520provides%2520an%2520interpretable%2520alternative%2520to%2520black-box%2520CNNs%2520and%2520a%2520basis%2520for%25203D%2520and%2520data-consistency-driven%2520reconstruction.%2520Our%2520codes%2520are%2520available%2520at%253A%2520https%253A//github.com/itu-biai/deep_tv_for_ldct%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Total%20Variation%20with%20Lambda%20Mapping%20for%20Low-Dose%20CT%20Denoising&entry.906535625=Yusuf%20Talha%20Basak%20and%20Mehmet%20Ozan%20Unal%20and%20Metin%20Ertas%20and%20Isa%20Yildirim&entry.1292438233=Although%20Total%20Variation%20%28TV%29%20performs%20well%20in%20noise%20reduction%20and%20edge%20preservation%20on%20images%2C%20its%20dependence%20on%20the%20lambda%20parameter%20limits%20its%20efficiency%20and%20makes%20it%20difficult%20to%20use%20effectively.%20In%20this%20study%2C%20we%20present%20a%20Learnable%20Total%20Variation%20%28LTV%29%20framework%20that%20couples%20an%20unrolled%20TV%20solver%20with%20a%20data-driven%20Lambda%20Mapping%20Network%20%28LambdaNet%29%20predicting%20a%20per-pixel%20regularization%20map.%20The%20pipeline%20is%20trained%20end-to-end%20so%20that%20reconstruction%20and%20regularization%20are%20optimized%20jointly%2C%20yielding%20spatially%20adaptive%20smoothing%3A%20strong%20in%20homogeneous%20regions%2C%20relaxed%20near%20anatomical%20boundaries.%20Experiments%20on%20the%20DeepLesion%20dataset%2C%20using%20a%20realistic%20noise%20model%20adapted%20from%20the%20LoDoPaB-CT%20methodology%2C%20show%20consistent%20gains%20over%20classical%20TV%20and%20FBP%2BU-Net%3A%20%2B2.9%20dB%20PSNR%20and%20%2B6%25%20SSIM%20on%20average.%20LTV%20provides%20an%20interpretable%20alternative%20to%20black-box%20CNNs%20and%20a%20basis%20for%203D%20and%20data-consistency-driven%20reconstruction.%20Our%20codes%20are%20available%20at%3A%20https%3A//github.com/itu-biai/deep_tv_for_ldct&entry.1838667208=http%3A//arxiv.org/abs/2511.10500v1&entry.124074799=Read"},
{"title": "LPLC: A Dataset for License Plate Legibility Classification", "author": "Lucas Wojcik and Gabriel E. Lima and Valfride Nascimento and Eduil Nascimento and Rayson Laroca and David Menotti", "abstract": "Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera/image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at https://github.com/lmlwojcik/lplc-dataset.", "link": "http://arxiv.org/abs/2508.18425v2", "date": "2025-11-13", "relevancy": 2.1515, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5361}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LPLC%3A%20A%20Dataset%20for%20License%20Plate%20Legibility%20Classification&body=Title%3A%20LPLC%3A%20A%20Dataset%20for%20License%20Plate%20Legibility%20Classification%0AAuthor%3A%20Lucas%20Wojcik%20and%20Gabriel%20E.%20Lima%20and%20Valfride%20Nascimento%20and%20Eduil%20Nascimento%20and%20Rayson%20Laroca%20and%20David%20Menotti%0AAbstract%3A%20Automatic%20License%20Plate%20Recognition%20%28ALPR%29%20faces%20a%20major%20challenge%20when%20dealing%20with%20illegible%20license%20plates%20%28LPs%29.%20While%20reconstruction%20methods%20such%20as%20super-resolution%20%28SR%29%20have%20emerged%2C%20the%20core%20issue%20of%20recognizing%20these%20low-quality%20LPs%20remains%20unresolved.%20To%20optimize%20model%20performance%20and%20computational%20efficiency%2C%20image%20pre-processing%20should%20be%20applied%20selectively%20to%20cases%20that%20require%20enhanced%20legibility.%20To%20support%20research%20in%20this%20area%2C%20we%20introduce%20a%20novel%20dataset%20comprising%2010%2C210%20images%20of%20vehicles%20with%2012%2C687%20annotated%20LPs%20for%20legibility%20classification%20%28the%20LPLC%20dataset%29.%20The%20images%20span%20a%20wide%20range%20of%20vehicle%20types%2C%20lighting%20conditions%2C%20and%20camera/image%20quality%20levels.%20We%20adopt%20a%20fine-grained%20annotation%20strategy%20that%20includes%20vehicle-%20and%20LP-level%20occlusions%2C%20four%20legibility%20categories%20%28perfect%2C%20good%2C%20poor%2C%20and%20illegible%29%2C%20and%20character%20labels%20for%20three%20categories%20%28excluding%20illegible%20LPs%29.%20As%20a%20benchmark%2C%20we%20propose%20a%20classification%20task%20using%20three%20image%20recognition%20networks%20to%20determine%20whether%20an%20LP%20image%20is%20good%20enough%2C%20requires%20super-resolution%2C%20or%20is%20completely%20unrecoverable.%20The%20overall%20F1%20score%2C%20which%20remained%20below%2080%25%20for%20all%20three%20baseline%20models%20%28ViT%2C%20ResNet%2C%20and%20YOLO%29%2C%20together%20with%20the%20analyses%20of%20SR%20and%20LP%20recognition%20methods%2C%20highlights%20the%20difficulty%20of%20the%20task%20and%20reinforces%20the%20need%20for%20further%20research.%20The%20proposed%20dataset%20is%20publicly%20available%20at%20https%3A//github.com/lmlwojcik/lplc-dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLPLC%253A%2520A%2520Dataset%2520for%2520License%2520Plate%2520Legibility%2520Classification%26entry.906535625%3DLucas%2520Wojcik%2520and%2520Gabriel%2520E.%2520Lima%2520and%2520Valfride%2520Nascimento%2520and%2520Eduil%2520Nascimento%2520and%2520Rayson%2520Laroca%2520and%2520David%2520Menotti%26entry.1292438233%3DAutomatic%2520License%2520Plate%2520Recognition%2520%2528ALPR%2529%2520faces%2520a%2520major%2520challenge%2520when%2520dealing%2520with%2520illegible%2520license%2520plates%2520%2528LPs%2529.%2520While%2520reconstruction%2520methods%2520such%2520as%2520super-resolution%2520%2528SR%2529%2520have%2520emerged%252C%2520the%2520core%2520issue%2520of%2520recognizing%2520these%2520low-quality%2520LPs%2520remains%2520unresolved.%2520To%2520optimize%2520model%2520performance%2520and%2520computational%2520efficiency%252C%2520image%2520pre-processing%2520should%2520be%2520applied%2520selectively%2520to%2520cases%2520that%2520require%2520enhanced%2520legibility.%2520To%2520support%2520research%2520in%2520this%2520area%252C%2520we%2520introduce%2520a%2520novel%2520dataset%2520comprising%252010%252C210%2520images%2520of%2520vehicles%2520with%252012%252C687%2520annotated%2520LPs%2520for%2520legibility%2520classification%2520%2528the%2520LPLC%2520dataset%2529.%2520The%2520images%2520span%2520a%2520wide%2520range%2520of%2520vehicle%2520types%252C%2520lighting%2520conditions%252C%2520and%2520camera/image%2520quality%2520levels.%2520We%2520adopt%2520a%2520fine-grained%2520annotation%2520strategy%2520that%2520includes%2520vehicle-%2520and%2520LP-level%2520occlusions%252C%2520four%2520legibility%2520categories%2520%2528perfect%252C%2520good%252C%2520poor%252C%2520and%2520illegible%2529%252C%2520and%2520character%2520labels%2520for%2520three%2520categories%2520%2528excluding%2520illegible%2520LPs%2529.%2520As%2520a%2520benchmark%252C%2520we%2520propose%2520a%2520classification%2520task%2520using%2520three%2520image%2520recognition%2520networks%2520to%2520determine%2520whether%2520an%2520LP%2520image%2520is%2520good%2520enough%252C%2520requires%2520super-resolution%252C%2520or%2520is%2520completely%2520unrecoverable.%2520The%2520overall%2520F1%2520score%252C%2520which%2520remained%2520below%252080%2525%2520for%2520all%2520three%2520baseline%2520models%2520%2528ViT%252C%2520ResNet%252C%2520and%2520YOLO%2529%252C%2520together%2520with%2520the%2520analyses%2520of%2520SR%2520and%2520LP%2520recognition%2520methods%252C%2520highlights%2520the%2520difficulty%2520of%2520the%2520task%2520and%2520reinforces%2520the%2520need%2520for%2520further%2520research.%2520The%2520proposed%2520dataset%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/lmlwojcik/lplc-dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LPLC%3A%20A%20Dataset%20for%20License%20Plate%20Legibility%20Classification&entry.906535625=Lucas%20Wojcik%20and%20Gabriel%20E.%20Lima%20and%20Valfride%20Nascimento%20and%20Eduil%20Nascimento%20and%20Rayson%20Laroca%20and%20David%20Menotti&entry.1292438233=Automatic%20License%20Plate%20Recognition%20%28ALPR%29%20faces%20a%20major%20challenge%20when%20dealing%20with%20illegible%20license%20plates%20%28LPs%29.%20While%20reconstruction%20methods%20such%20as%20super-resolution%20%28SR%29%20have%20emerged%2C%20the%20core%20issue%20of%20recognizing%20these%20low-quality%20LPs%20remains%20unresolved.%20To%20optimize%20model%20performance%20and%20computational%20efficiency%2C%20image%20pre-processing%20should%20be%20applied%20selectively%20to%20cases%20that%20require%20enhanced%20legibility.%20To%20support%20research%20in%20this%20area%2C%20we%20introduce%20a%20novel%20dataset%20comprising%2010%2C210%20images%20of%20vehicles%20with%2012%2C687%20annotated%20LPs%20for%20legibility%20classification%20%28the%20LPLC%20dataset%29.%20The%20images%20span%20a%20wide%20range%20of%20vehicle%20types%2C%20lighting%20conditions%2C%20and%20camera/image%20quality%20levels.%20We%20adopt%20a%20fine-grained%20annotation%20strategy%20that%20includes%20vehicle-%20and%20LP-level%20occlusions%2C%20four%20legibility%20categories%20%28perfect%2C%20good%2C%20poor%2C%20and%20illegible%29%2C%20and%20character%20labels%20for%20three%20categories%20%28excluding%20illegible%20LPs%29.%20As%20a%20benchmark%2C%20we%20propose%20a%20classification%20task%20using%20three%20image%20recognition%20networks%20to%20determine%20whether%20an%20LP%20image%20is%20good%20enough%2C%20requires%20super-resolution%2C%20or%20is%20completely%20unrecoverable.%20The%20overall%20F1%20score%2C%20which%20remained%20below%2080%25%20for%20all%20three%20baseline%20models%20%28ViT%2C%20ResNet%2C%20and%20YOLO%29%2C%20together%20with%20the%20analyses%20of%20SR%20and%20LP%20recognition%20methods%2C%20highlights%20the%20difficulty%20of%20the%20task%20and%20reinforces%20the%20need%20for%20further%20research.%20The%20proposed%20dataset%20is%20publicly%20available%20at%20https%3A//github.com/lmlwojcik/lplc-dataset.&entry.1838667208=http%3A//arxiv.org/abs/2508.18425v2&entry.124074799=Read"},
{"title": "Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection", "author": "Fanxiao Li and Jiaying Wu and Tingchao Fu and Yunyun Dong and Bingbing Song and Wei Zhou", "abstract": "The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a model's internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.", "link": "http://arxiv.org/abs/2508.12711v2", "date": "2025-11-13", "relevancy": 2.1393, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5525}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5346}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection&body=Title%3A%20Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection%0AAuthor%3A%20Fanxiao%20Li%20and%20Jiaying%20Wu%20and%20Tingchao%20Fu%20and%20Yunyun%20Dong%20and%20Bingbing%20Song%20and%20Wei%20Zhou%0AAbstract%3A%20The%20proliferation%20of%20multimodal%20misinformation%20poses%20growing%20threats%20to%20public%20discourse%20and%20societal%20trust.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20enabled%20recent%20progress%20in%20multimodal%20misinformation%20detection%20%28MMD%29%2C%20the%20rise%20of%20generative%20AI%20%28GenAI%29%20tools%20introduces%20a%20new%20challenge%3A%20GenAI-driven%20news%20diversity%2C%20characterized%20by%20highly%20varied%20and%20complex%20content.%20We%20show%20that%20this%20diversity%20induces%20multi-level%20drift%2C%20comprising%20%281%29%20model-level%20misperception%20drift%2C%20where%20stylistic%20variations%20disrupt%20a%20model%27s%20internal%20reasoning%2C%20and%20%282%29%20evidence-level%20drift%2C%20where%20expression%20diversity%20degrades%20the%20quality%20or%20relevance%20of%20retrieved%20external%20evidence.%20These%20drifts%20significantly%20degrade%20the%20robustness%20of%20current%20LVLM-based%20MMD%20systems.%20To%20systematically%20study%20this%20problem%2C%20we%20introduce%20DriftBench%2C%20a%20large-scale%20benchmark%20comprising%2016%2C000%20news%20instances%20across%20six%20categories%20of%20diversification.%20We%20design%20three%20evaluation%20tasks%3A%20%281%29%20robustness%20of%20truth%20verification%20under%20multi-level%20drift%3B%20%282%29%20susceptibility%20to%20adversarial%20evidence%20contamination%20generated%20by%20GenAI%3B%20and%20%283%29%20analysis%20of%20reasoning%20consistency%20across%20diverse%20inputs.%20Experiments%20with%20six%20state-of-the-art%20LVLM-based%20detectors%20show%20substantial%20performance%20drops%20%28average%20F1%20-14.8%25%29%20and%20increasingly%20unstable%20reasoning%20traces%2C%20with%20even%20more%20severe%20failures%20under%20adversarial%20evidence%20injection.%20Our%20findings%20uncover%20fundamental%20vulnerabilities%20in%20existing%20MMD%20systems%20and%20suggest%20an%20urgent%20need%20for%20more%20resilient%20approaches%20in%20the%20GenAI%20era.%0ALink%3A%20http%3A//arxiv.org/abs/2508.12711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrifting%2520Away%2520from%2520Truth%253A%2520GenAI-Driven%2520News%2520Diversity%2520Challenges%2520LVLM-Based%2520Misinformation%2520Detection%26entry.906535625%3DFanxiao%2520Li%2520and%2520Jiaying%2520Wu%2520and%2520Tingchao%2520Fu%2520and%2520Yunyun%2520Dong%2520and%2520Bingbing%2520Song%2520and%2520Wei%2520Zhou%26entry.1292438233%3DThe%2520proliferation%2520of%2520multimodal%2520misinformation%2520poses%2520growing%2520threats%2520to%2520public%2520discourse%2520and%2520societal%2520trust.%2520While%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520enabled%2520recent%2520progress%2520in%2520multimodal%2520misinformation%2520detection%2520%2528MMD%2529%252C%2520the%2520rise%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520tools%2520introduces%2520a%2520new%2520challenge%253A%2520GenAI-driven%2520news%2520diversity%252C%2520characterized%2520by%2520highly%2520varied%2520and%2520complex%2520content.%2520We%2520show%2520that%2520this%2520diversity%2520induces%2520multi-level%2520drift%252C%2520comprising%2520%25281%2529%2520model-level%2520misperception%2520drift%252C%2520where%2520stylistic%2520variations%2520disrupt%2520a%2520model%2527s%2520internal%2520reasoning%252C%2520and%2520%25282%2529%2520evidence-level%2520drift%252C%2520where%2520expression%2520diversity%2520degrades%2520the%2520quality%2520or%2520relevance%2520of%2520retrieved%2520external%2520evidence.%2520These%2520drifts%2520significantly%2520degrade%2520the%2520robustness%2520of%2520current%2520LVLM-based%2520MMD%2520systems.%2520To%2520systematically%2520study%2520this%2520problem%252C%2520we%2520introduce%2520DriftBench%252C%2520a%2520large-scale%2520benchmark%2520comprising%252016%252C000%2520news%2520instances%2520across%2520six%2520categories%2520of%2520diversification.%2520We%2520design%2520three%2520evaluation%2520tasks%253A%2520%25281%2529%2520robustness%2520of%2520truth%2520verification%2520under%2520multi-level%2520drift%253B%2520%25282%2529%2520susceptibility%2520to%2520adversarial%2520evidence%2520contamination%2520generated%2520by%2520GenAI%253B%2520and%2520%25283%2529%2520analysis%2520of%2520reasoning%2520consistency%2520across%2520diverse%2520inputs.%2520Experiments%2520with%2520six%2520state-of-the-art%2520LVLM-based%2520detectors%2520show%2520substantial%2520performance%2520drops%2520%2528average%2520F1%2520-14.8%2525%2529%2520and%2520increasingly%2520unstable%2520reasoning%2520traces%252C%2520with%2520even%2520more%2520severe%2520failures%2520under%2520adversarial%2520evidence%2520injection.%2520Our%2520findings%2520uncover%2520fundamental%2520vulnerabilities%2520in%2520existing%2520MMD%2520systems%2520and%2520suggest%2520an%2520urgent%2520need%2520for%2520more%2520resilient%2520approaches%2520in%2520the%2520GenAI%2520era.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drifting%20Away%20from%20Truth%3A%20GenAI-Driven%20News%20Diversity%20Challenges%20LVLM-Based%20Misinformation%20Detection&entry.906535625=Fanxiao%20Li%20and%20Jiaying%20Wu%20and%20Tingchao%20Fu%20and%20Yunyun%20Dong%20and%20Bingbing%20Song%20and%20Wei%20Zhou&entry.1292438233=The%20proliferation%20of%20multimodal%20misinformation%20poses%20growing%20threats%20to%20public%20discourse%20and%20societal%20trust.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20enabled%20recent%20progress%20in%20multimodal%20misinformation%20detection%20%28MMD%29%2C%20the%20rise%20of%20generative%20AI%20%28GenAI%29%20tools%20introduces%20a%20new%20challenge%3A%20GenAI-driven%20news%20diversity%2C%20characterized%20by%20highly%20varied%20and%20complex%20content.%20We%20show%20that%20this%20diversity%20induces%20multi-level%20drift%2C%20comprising%20%281%29%20model-level%20misperception%20drift%2C%20where%20stylistic%20variations%20disrupt%20a%20model%27s%20internal%20reasoning%2C%20and%20%282%29%20evidence-level%20drift%2C%20where%20expression%20diversity%20degrades%20the%20quality%20or%20relevance%20of%20retrieved%20external%20evidence.%20These%20drifts%20significantly%20degrade%20the%20robustness%20of%20current%20LVLM-based%20MMD%20systems.%20To%20systematically%20study%20this%20problem%2C%20we%20introduce%20DriftBench%2C%20a%20large-scale%20benchmark%20comprising%2016%2C000%20news%20instances%20across%20six%20categories%20of%20diversification.%20We%20design%20three%20evaluation%20tasks%3A%20%281%29%20robustness%20of%20truth%20verification%20under%20multi-level%20drift%3B%20%282%29%20susceptibility%20to%20adversarial%20evidence%20contamination%20generated%20by%20GenAI%3B%20and%20%283%29%20analysis%20of%20reasoning%20consistency%20across%20diverse%20inputs.%20Experiments%20with%20six%20state-of-the-art%20LVLM-based%20detectors%20show%20substantial%20performance%20drops%20%28average%20F1%20-14.8%25%29%20and%20increasingly%20unstable%20reasoning%20traces%2C%20with%20even%20more%20severe%20failures%20under%20adversarial%20evidence%20injection.%20Our%20findings%20uncover%20fundamental%20vulnerabilities%20in%20existing%20MMD%20systems%20and%20suggest%20an%20urgent%20need%20for%20more%20resilient%20approaches%20in%20the%20GenAI%20era.&entry.1838667208=http%3A//arxiv.org/abs/2508.12711v2&entry.124074799=Read"},
{"title": "Torch-Uncertainty: A Deep Learning Framework for Uncertainty Quantification", "author": "Adrien Lafage and Olivier Laurent and Firas Gabetni and Gianni Franchi", "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable performance across various domains, including computer vision and natural language processing. However, they often struggle to accurately quantify the uncertainty of their predictions, limiting their broader adoption in critical real-world applications. Uncertainty Quantification (UQ) for Deep Learning seeks to address this challenge by providing methods to improve the reliability of uncertainty estimates. Although numerous techniques have been proposed, a unified tool offering a seamless workflow to evaluate and integrate these methods remains lacking. To bridge this gap, we introduce Torch-Uncertainty, a PyTorch and Lightning-based framework designed to streamline DNN training and evaluation with UQ techniques and metrics. In this paper, we outline the foundational principles of our library and present comprehensive experimental results that benchmark a diverse set of UQ methods across classification, segmentation, and regression tasks. Our library is available at https://github.com/ENSTA-U2IS-AI/Torch-Uncertainty", "link": "http://arxiv.org/abs/2511.10282v1", "date": "2025-11-13", "relevancy": 2.1379, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Torch-Uncertainty%3A%20A%20Deep%20Learning%20Framework%20for%20Uncertainty%20Quantification&body=Title%3A%20Torch-Uncertainty%3A%20A%20Deep%20Learning%20Framework%20for%20Uncertainty%20Quantification%0AAuthor%3A%20Adrien%20Lafage%20and%20Olivier%20Laurent%20and%20Firas%20Gabetni%20and%20Gianni%20Franchi%0AAbstract%3A%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20demonstrated%20remarkable%20performance%20across%20various%20domains%2C%20including%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%20they%20often%20struggle%20to%20accurately%20quantify%20the%20uncertainty%20of%20their%20predictions%2C%20limiting%20their%20broader%20adoption%20in%20critical%20real-world%20applications.%20Uncertainty%20Quantification%20%28UQ%29%20for%20Deep%20Learning%20seeks%20to%20address%20this%20challenge%20by%20providing%20methods%20to%20improve%20the%20reliability%20of%20uncertainty%20estimates.%20Although%20numerous%20techniques%20have%20been%20proposed%2C%20a%20unified%20tool%20offering%20a%20seamless%20workflow%20to%20evaluate%20and%20integrate%20these%20methods%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Torch-Uncertainty%2C%20a%20PyTorch%20and%20Lightning-based%20framework%20designed%20to%20streamline%20DNN%20training%20and%20evaluation%20with%20UQ%20techniques%20and%20metrics.%20In%20this%20paper%2C%20we%20outline%20the%20foundational%20principles%20of%20our%20library%20and%20present%20comprehensive%20experimental%20results%20that%20benchmark%20a%20diverse%20set%20of%20UQ%20methods%20across%20classification%2C%20segmentation%2C%20and%20regression%20tasks.%20Our%20library%20is%20available%20at%20https%3A//github.com/ENSTA-U2IS-AI/Torch-Uncertainty%0ALink%3A%20http%3A//arxiv.org/abs/2511.10282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTorch-Uncertainty%253A%2520A%2520Deep%2520Learning%2520Framework%2520for%2520Uncertainty%2520Quantification%26entry.906535625%3DAdrien%2520Lafage%2520and%2520Olivier%2520Laurent%2520and%2520Firas%2520Gabetni%2520and%2520Gianni%2520Franchi%26entry.1292438233%3DDeep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520various%2520domains%252C%2520including%2520computer%2520vision%2520and%2520natural%2520language%2520processing.%2520However%252C%2520they%2520often%2520struggle%2520to%2520accurately%2520quantify%2520the%2520uncertainty%2520of%2520their%2520predictions%252C%2520limiting%2520their%2520broader%2520adoption%2520in%2520critical%2520real-world%2520applications.%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520for%2520Deep%2520Learning%2520seeks%2520to%2520address%2520this%2520challenge%2520by%2520providing%2520methods%2520to%2520improve%2520the%2520reliability%2520of%2520uncertainty%2520estimates.%2520Although%2520numerous%2520techniques%2520have%2520been%2520proposed%252C%2520a%2520unified%2520tool%2520offering%2520a%2520seamless%2520workflow%2520to%2520evaluate%2520and%2520integrate%2520these%2520methods%2520remains%2520lacking.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Torch-Uncertainty%252C%2520a%2520PyTorch%2520and%2520Lightning-based%2520framework%2520designed%2520to%2520streamline%2520DNN%2520training%2520and%2520evaluation%2520with%2520UQ%2520techniques%2520and%2520metrics.%2520In%2520this%2520paper%252C%2520we%2520outline%2520the%2520foundational%2520principles%2520of%2520our%2520library%2520and%2520present%2520comprehensive%2520experimental%2520results%2520that%2520benchmark%2520a%2520diverse%2520set%2520of%2520UQ%2520methods%2520across%2520classification%252C%2520segmentation%252C%2520and%2520regression%2520tasks.%2520Our%2520library%2520is%2520available%2520at%2520https%253A//github.com/ENSTA-U2IS-AI/Torch-Uncertainty%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Torch-Uncertainty%3A%20A%20Deep%20Learning%20Framework%20for%20Uncertainty%20Quantification&entry.906535625=Adrien%20Lafage%20and%20Olivier%20Laurent%20and%20Firas%20Gabetni%20and%20Gianni%20Franchi&entry.1292438233=Deep%20Neural%20Networks%20%28DNNs%29%20have%20demonstrated%20remarkable%20performance%20across%20various%20domains%2C%20including%20computer%20vision%20and%20natural%20language%20processing.%20However%2C%20they%20often%20struggle%20to%20accurately%20quantify%20the%20uncertainty%20of%20their%20predictions%2C%20limiting%20their%20broader%20adoption%20in%20critical%20real-world%20applications.%20Uncertainty%20Quantification%20%28UQ%29%20for%20Deep%20Learning%20seeks%20to%20address%20this%20challenge%20by%20providing%20methods%20to%20improve%20the%20reliability%20of%20uncertainty%20estimates.%20Although%20numerous%20techniques%20have%20been%20proposed%2C%20a%20unified%20tool%20offering%20a%20seamless%20workflow%20to%20evaluate%20and%20integrate%20these%20methods%20remains%20lacking.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Torch-Uncertainty%2C%20a%20PyTorch%20and%20Lightning-based%20framework%20designed%20to%20streamline%20DNN%20training%20and%20evaluation%20with%20UQ%20techniques%20and%20metrics.%20In%20this%20paper%2C%20we%20outline%20the%20foundational%20principles%20of%20our%20library%20and%20present%20comprehensive%20experimental%20results%20that%20benchmark%20a%20diverse%20set%20of%20UQ%20methods%20across%20classification%2C%20segmentation%2C%20and%20regression%20tasks.%20Our%20library%20is%20available%20at%20https%3A//github.com/ENSTA-U2IS-AI/Torch-Uncertainty&entry.1838667208=http%3A//arxiv.org/abs/2511.10282v1&entry.124074799=Read"},
{"title": "DenoGrad: Deep Gradient Denoising Framework for Enhancing the Performance of Interpretable AI Models", "author": "J. Javier Alonso-Ramos and Ignacio Aguilera-Martos and Andr\u00e9s Herrera-Poyatos and Francisco Herrera", "abstract": "The performance of Machine Learning (ML) models, particularly those operating within the Interpretable Artificial Intelligence (Interpretable AI) framework, is significantly affected by the presence of noise in both training and production data. Denoising has therefore become a critical preprocessing step, typically categorized into instance removal and instance correction techniques. However, existing correction approaches often degrade performance or oversimplify the problem by altering the original data distribution. This leads to unrealistic scenarios and biased models, which is particularly problematic in contexts where interpretable AI models are employed, as their interpretability depends on the fidelity of the underlying data patterns. In this paper, we argue that defining noise independently of the solution may be ineffective, as its nature can vary significantly across tasks and datasets. Using a task-specific high quality solution as a reference can provide a more precise and adaptable noise definition. To this end, we propose DenoGrad, a novel Gradient-based instance Denoiser framework that leverages gradients from an accurate Deep Learning (DL) model trained on the target data -- regardless of the specific task -- to detect and adjust noisy samples. Unlike conventional approaches, DenoGrad dynamically corrects noisy instances, preserving problem's data distribution, and improving AI models robustness. DenoGrad is validated on both tabular and time series datasets under various noise settings against the state-of-the-art. DenoGrad outperforms existing denoising strategies, enhancing the performance of interpretable IA models while standing out as the only high quality approach that preserves the original data distribution.", "link": "http://arxiv.org/abs/2511.10161v1", "date": "2025-11-13", "relevancy": 2.1352, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5569}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5199}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenoGrad%3A%20Deep%20Gradient%20Denoising%20Framework%20for%20Enhancing%20the%20Performance%20of%20Interpretable%20AI%20Models&body=Title%3A%20DenoGrad%3A%20Deep%20Gradient%20Denoising%20Framework%20for%20Enhancing%20the%20Performance%20of%20Interpretable%20AI%20Models%0AAuthor%3A%20J.%20Javier%20Alonso-Ramos%20and%20Ignacio%20Aguilera-Martos%20and%20Andr%C3%A9s%20Herrera-Poyatos%20and%20Francisco%20Herrera%0AAbstract%3A%20The%20performance%20of%20Machine%20Learning%20%28ML%29%20models%2C%20particularly%20those%20operating%20within%20the%20Interpretable%20Artificial%20Intelligence%20%28Interpretable%20AI%29%20framework%2C%20is%20significantly%20affected%20by%20the%20presence%20of%20noise%20in%20both%20training%20and%20production%20data.%20Denoising%20has%20therefore%20become%20a%20critical%20preprocessing%20step%2C%20typically%20categorized%20into%20instance%20removal%20and%20instance%20correction%20techniques.%20However%2C%20existing%20correction%20approaches%20often%20degrade%20performance%20or%20oversimplify%20the%20problem%20by%20altering%20the%20original%20data%20distribution.%20This%20leads%20to%20unrealistic%20scenarios%20and%20biased%20models%2C%20which%20is%20particularly%20problematic%20in%20contexts%20where%20interpretable%20AI%20models%20are%20employed%2C%20as%20their%20interpretability%20depends%20on%20the%20fidelity%20of%20the%20underlying%20data%20patterns.%20In%20this%20paper%2C%20we%20argue%20that%20defining%20noise%20independently%20of%20the%20solution%20may%20be%20ineffective%2C%20as%20its%20nature%20can%20vary%20significantly%20across%20tasks%20and%20datasets.%20Using%20a%20task-specific%20high%20quality%20solution%20as%20a%20reference%20can%20provide%20a%20more%20precise%20and%20adaptable%20noise%20definition.%20To%20this%20end%2C%20we%20propose%20DenoGrad%2C%20a%20novel%20Gradient-based%20instance%20Denoiser%20framework%20that%20leverages%20gradients%20from%20an%20accurate%20Deep%20Learning%20%28DL%29%20model%20trained%20on%20the%20target%20data%20--%20regardless%20of%20the%20specific%20task%20--%20to%20detect%20and%20adjust%20noisy%20samples.%20Unlike%20conventional%20approaches%2C%20DenoGrad%20dynamically%20corrects%20noisy%20instances%2C%20preserving%20problem%27s%20data%20distribution%2C%20and%20improving%20AI%20models%20robustness.%20DenoGrad%20is%20validated%20on%20both%20tabular%20and%20time%20series%20datasets%20under%20various%20noise%20settings%20against%20the%20state-of-the-art.%20DenoGrad%20outperforms%20existing%20denoising%20strategies%2C%20enhancing%20the%20performance%20of%20interpretable%20IA%20models%20while%20standing%20out%20as%20the%20only%20high%20quality%20approach%20that%20preserves%20the%20original%20data%20distribution.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoGrad%253A%2520Deep%2520Gradient%2520Denoising%2520Framework%2520for%2520Enhancing%2520the%2520Performance%2520of%2520Interpretable%2520AI%2520Models%26entry.906535625%3DJ.%2520Javier%2520Alonso-Ramos%2520and%2520Ignacio%2520Aguilera-Martos%2520and%2520Andr%25C3%25A9s%2520Herrera-Poyatos%2520and%2520Francisco%2520Herrera%26entry.1292438233%3DThe%2520performance%2520of%2520Machine%2520Learning%2520%2528ML%2529%2520models%252C%2520particularly%2520those%2520operating%2520within%2520the%2520Interpretable%2520Artificial%2520Intelligence%2520%2528Interpretable%2520AI%2529%2520framework%252C%2520is%2520significantly%2520affected%2520by%2520the%2520presence%2520of%2520noise%2520in%2520both%2520training%2520and%2520production%2520data.%2520Denoising%2520has%2520therefore%2520become%2520a%2520critical%2520preprocessing%2520step%252C%2520typically%2520categorized%2520into%2520instance%2520removal%2520and%2520instance%2520correction%2520techniques.%2520However%252C%2520existing%2520correction%2520approaches%2520often%2520degrade%2520performance%2520or%2520oversimplify%2520the%2520problem%2520by%2520altering%2520the%2520original%2520data%2520distribution.%2520This%2520leads%2520to%2520unrealistic%2520scenarios%2520and%2520biased%2520models%252C%2520which%2520is%2520particularly%2520problematic%2520in%2520contexts%2520where%2520interpretable%2520AI%2520models%2520are%2520employed%252C%2520as%2520their%2520interpretability%2520depends%2520on%2520the%2520fidelity%2520of%2520the%2520underlying%2520data%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520defining%2520noise%2520independently%2520of%2520the%2520solution%2520may%2520be%2520ineffective%252C%2520as%2520its%2520nature%2520can%2520vary%2520significantly%2520across%2520tasks%2520and%2520datasets.%2520Using%2520a%2520task-specific%2520high%2520quality%2520solution%2520as%2520a%2520reference%2520can%2520provide%2520a%2520more%2520precise%2520and%2520adaptable%2520noise%2520definition.%2520To%2520this%2520end%252C%2520we%2520propose%2520DenoGrad%252C%2520a%2520novel%2520Gradient-based%2520instance%2520Denoiser%2520framework%2520that%2520leverages%2520gradients%2520from%2520an%2520accurate%2520Deep%2520Learning%2520%2528DL%2529%2520model%2520trained%2520on%2520the%2520target%2520data%2520--%2520regardless%2520of%2520the%2520specific%2520task%2520--%2520to%2520detect%2520and%2520adjust%2520noisy%2520samples.%2520Unlike%2520conventional%2520approaches%252C%2520DenoGrad%2520dynamically%2520corrects%2520noisy%2520instances%252C%2520preserving%2520problem%2527s%2520data%2520distribution%252C%2520and%2520improving%2520AI%2520models%2520robustness.%2520DenoGrad%2520is%2520validated%2520on%2520both%2520tabular%2520and%2520time%2520series%2520datasets%2520under%2520various%2520noise%2520settings%2520against%2520the%2520state-of-the-art.%2520DenoGrad%2520outperforms%2520existing%2520denoising%2520strategies%252C%2520enhancing%2520the%2520performance%2520of%2520interpretable%2520IA%2520models%2520while%2520standing%2520out%2520as%2520the%2520only%2520high%2520quality%2520approach%2520that%2520preserves%2520the%2520original%2520data%2520distribution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenoGrad%3A%20Deep%20Gradient%20Denoising%20Framework%20for%20Enhancing%20the%20Performance%20of%20Interpretable%20AI%20Models&entry.906535625=J.%20Javier%20Alonso-Ramos%20and%20Ignacio%20Aguilera-Martos%20and%20Andr%C3%A9s%20Herrera-Poyatos%20and%20Francisco%20Herrera&entry.1292438233=The%20performance%20of%20Machine%20Learning%20%28ML%29%20models%2C%20particularly%20those%20operating%20within%20the%20Interpretable%20Artificial%20Intelligence%20%28Interpretable%20AI%29%20framework%2C%20is%20significantly%20affected%20by%20the%20presence%20of%20noise%20in%20both%20training%20and%20production%20data.%20Denoising%20has%20therefore%20become%20a%20critical%20preprocessing%20step%2C%20typically%20categorized%20into%20instance%20removal%20and%20instance%20correction%20techniques.%20However%2C%20existing%20correction%20approaches%20often%20degrade%20performance%20or%20oversimplify%20the%20problem%20by%20altering%20the%20original%20data%20distribution.%20This%20leads%20to%20unrealistic%20scenarios%20and%20biased%20models%2C%20which%20is%20particularly%20problematic%20in%20contexts%20where%20interpretable%20AI%20models%20are%20employed%2C%20as%20their%20interpretability%20depends%20on%20the%20fidelity%20of%20the%20underlying%20data%20patterns.%20In%20this%20paper%2C%20we%20argue%20that%20defining%20noise%20independently%20of%20the%20solution%20may%20be%20ineffective%2C%20as%20its%20nature%20can%20vary%20significantly%20across%20tasks%20and%20datasets.%20Using%20a%20task-specific%20high%20quality%20solution%20as%20a%20reference%20can%20provide%20a%20more%20precise%20and%20adaptable%20noise%20definition.%20To%20this%20end%2C%20we%20propose%20DenoGrad%2C%20a%20novel%20Gradient-based%20instance%20Denoiser%20framework%20that%20leverages%20gradients%20from%20an%20accurate%20Deep%20Learning%20%28DL%29%20model%20trained%20on%20the%20target%20data%20--%20regardless%20of%20the%20specific%20task%20--%20to%20detect%20and%20adjust%20noisy%20samples.%20Unlike%20conventional%20approaches%2C%20DenoGrad%20dynamically%20corrects%20noisy%20instances%2C%20preserving%20problem%27s%20data%20distribution%2C%20and%20improving%20AI%20models%20robustness.%20DenoGrad%20is%20validated%20on%20both%20tabular%20and%20time%20series%20datasets%20under%20various%20noise%20settings%20against%20the%20state-of-the-art.%20DenoGrad%20outperforms%20existing%20denoising%20strategies%2C%20enhancing%20the%20performance%20of%20interpretable%20IA%20models%20while%20standing%20out%20as%20the%20only%20high%20quality%20approach%20that%20preserves%20the%20original%20data%20distribution.&entry.1838667208=http%3A//arxiv.org/abs/2511.10161v1&entry.124074799=Read"},
{"title": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models", "author": "Zhengtao Zou and Ya Gao and Jiarui Guan and Bin Li and Pekka Marttinen", "abstract": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.", "link": "http://arxiv.org/abs/2511.10292v1", "date": "2025-11-13", "relevancy": 2.1184, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Residual-Update%20Steering%20for%20Low-Overhead%20Hallucination%20Mitigation%20in%20Large%20Vision%20Language%20Models&body=Title%3A%20Adaptive%20Residual-Update%20Steering%20for%20Low-Overhead%20Hallucination%20Mitigation%20in%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Zhengtao%20Zou%20and%20Ya%20Gao%20and%20Jiarui%20Guan%20and%20Bin%20Li%20and%20Pekka%20Marttinen%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%20generating%20text%20inconsistent%20with%20visual%20inputs%2C%20which%20can%20critically%20undermine%20their%20reliability.%20Existing%20inference-time%20interventions%20to%20mitigate%20this%20issue%20present%20a%20challenging%20trade-off%3A%20while%20methods%20that%20steer%20internal%20states%20or%20adjust%20output%20logits%20can%20be%20effective%2C%20they%20often%20incur%20substantial%20computational%20overhead%2C%20typically%20requiring%20extra%20forward%20passes.%20This%20efficiency%20bottleneck%20can%20limit%20their%20practicality%20for%20real-world%2C%20latency-sensitive%20deployments.%20In%20this%20work%2C%20we%20aim%20to%20address%20this%20trade-off%20with%20Residual-Update%20Directed%20DEcoding%20Regulation%20%28RUDDER%29%2C%20a%20low-overhead%20framework%20that%20steers%20LVLMs%20towards%20visually-grounded%20generation.%20RUDDER%20is%20built%20on%20two%20key%20innovations%3A%20%281%29%20Contextual%20Activation%20Residual%20Direction%20%28CARD%29%20vector%2C%20a%20per-sample%20visual%20evidence%20vector%20extracted%20from%20the%20residual%20update%20of%20a%20self-attention%20layer%20during%20a%20single%2C%20standard%20forward%20pass.%20%282%29%20A%20Bayesian-inspired%20adaptive%20gate%20that%20performs%20token-wise%20injection%2C%20applying%20a%20corrective%20signal%20whose%20strength%20is%20conditioned%20on%20the%20model%27s%20deviation%20from%20the%20visual%20context.%20Extensive%20experiments%20on%20key%20hallucination%20benchmarks%2C%20including%20POPE%20and%20CHAIR%2C%20indicate%20that%20RUDDER%20achieves%20performance%20comparable%20to%20state-of-the-art%20methods%20while%20introducing%20negligible%20computational%20latency%2C%20validating%20RUDDER%20as%20a%20pragmatic%20and%20effective%20approach%20for%20improving%20LVLMs%27%20reliability%20without%20a%20significant%20compromise%20on%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Residual-Update%2520Steering%2520for%2520Low-Overhead%2520Hallucination%2520Mitigation%2520in%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DZhengtao%2520Zou%2520and%2520Ya%2520Gao%2520and%2520Jiarui%2520Guan%2520and%2520Bin%2520Li%2520and%2520Pekka%2520Marttinen%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520often%2520suffer%2520from%2520object%2520hallucination%252C%2520generating%2520text%2520inconsistent%2520with%2520visual%2520inputs%252C%2520which%2520can%2520critically%2520undermine%2520their%2520reliability.%2520Existing%2520inference-time%2520interventions%2520to%2520mitigate%2520this%2520issue%2520present%2520a%2520challenging%2520trade-off%253A%2520while%2520methods%2520that%2520steer%2520internal%2520states%2520or%2520adjust%2520output%2520logits%2520can%2520be%2520effective%252C%2520they%2520often%2520incur%2520substantial%2520computational%2520overhead%252C%2520typically%2520requiring%2520extra%2520forward%2520passes.%2520This%2520efficiency%2520bottleneck%2520can%2520limit%2520their%2520practicality%2520for%2520real-world%252C%2520latency-sensitive%2520deployments.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520address%2520this%2520trade-off%2520with%2520Residual-Update%2520Directed%2520DEcoding%2520Regulation%2520%2528RUDDER%2529%252C%2520a%2520low-overhead%2520framework%2520that%2520steers%2520LVLMs%2520towards%2520visually-grounded%2520generation.%2520RUDDER%2520is%2520built%2520on%2520two%2520key%2520innovations%253A%2520%25281%2529%2520Contextual%2520Activation%2520Residual%2520Direction%2520%2528CARD%2529%2520vector%252C%2520a%2520per-sample%2520visual%2520evidence%2520vector%2520extracted%2520from%2520the%2520residual%2520update%2520of%2520a%2520self-attention%2520layer%2520during%2520a%2520single%252C%2520standard%2520forward%2520pass.%2520%25282%2529%2520A%2520Bayesian-inspired%2520adaptive%2520gate%2520that%2520performs%2520token-wise%2520injection%252C%2520applying%2520a%2520corrective%2520signal%2520whose%2520strength%2520is%2520conditioned%2520on%2520the%2520model%2527s%2520deviation%2520from%2520the%2520visual%2520context.%2520Extensive%2520experiments%2520on%2520key%2520hallucination%2520benchmarks%252C%2520including%2520POPE%2520and%2520CHAIR%252C%2520indicate%2520that%2520RUDDER%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520methods%2520while%2520introducing%2520negligible%2520computational%2520latency%252C%2520validating%2520RUDDER%2520as%2520a%2520pragmatic%2520and%2520effective%2520approach%2520for%2520improving%2520LVLMs%2527%2520reliability%2520without%2520a%2520significant%2520compromise%2520on%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Residual-Update%20Steering%20for%20Low-Overhead%20Hallucination%20Mitigation%20in%20Large%20Vision%20Language%20Models&entry.906535625=Zhengtao%20Zou%20and%20Ya%20Gao%20and%20Jiarui%20Guan%20and%20Bin%20Li%20and%20Pekka%20Marttinen&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20often%20suffer%20from%20object%20hallucination%2C%20generating%20text%20inconsistent%20with%20visual%20inputs%2C%20which%20can%20critically%20undermine%20their%20reliability.%20Existing%20inference-time%20interventions%20to%20mitigate%20this%20issue%20present%20a%20challenging%20trade-off%3A%20while%20methods%20that%20steer%20internal%20states%20or%20adjust%20output%20logits%20can%20be%20effective%2C%20they%20often%20incur%20substantial%20computational%20overhead%2C%20typically%20requiring%20extra%20forward%20passes.%20This%20efficiency%20bottleneck%20can%20limit%20their%20practicality%20for%20real-world%2C%20latency-sensitive%20deployments.%20In%20this%20work%2C%20we%20aim%20to%20address%20this%20trade-off%20with%20Residual-Update%20Directed%20DEcoding%20Regulation%20%28RUDDER%29%2C%20a%20low-overhead%20framework%20that%20steers%20LVLMs%20towards%20visually-grounded%20generation.%20RUDDER%20is%20built%20on%20two%20key%20innovations%3A%20%281%29%20Contextual%20Activation%20Residual%20Direction%20%28CARD%29%20vector%2C%20a%20per-sample%20visual%20evidence%20vector%20extracted%20from%20the%20residual%20update%20of%20a%20self-attention%20layer%20during%20a%20single%2C%20standard%20forward%20pass.%20%282%29%20A%20Bayesian-inspired%20adaptive%20gate%20that%20performs%20token-wise%20injection%2C%20applying%20a%20corrective%20signal%20whose%20strength%20is%20conditioned%20on%20the%20model%27s%20deviation%20from%20the%20visual%20context.%20Extensive%20experiments%20on%20key%20hallucination%20benchmarks%2C%20including%20POPE%20and%20CHAIR%2C%20indicate%20that%20RUDDER%20achieves%20performance%20comparable%20to%20state-of-the-art%20methods%20while%20introducing%20negligible%20computational%20latency%2C%20validating%20RUDDER%20as%20a%20pragmatic%20and%20effective%20approach%20for%20improving%20LVLMs%27%20reliability%20without%20a%20significant%20compromise%20on%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.10292v1&entry.124074799=Read"},
{"title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding", "author": "Yunkai Zhang and Qiang Zhang and  Feng and  Lin and Ruizhong Qiu and Hanchao Yu and Jason Liu and Yinglong Xia and Zhuoran Yu and Zeyu Zheng and Diji Yang", "abstract": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.", "link": "http://arxiv.org/abs/2511.10492v1", "date": "2025-11-13", "relevancy": 2.1094, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5257}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Waste%20It%3A%20Guiding%20Generative%20Recommenders%20with%20Structured%20Human%20Priors%20via%20Multi-head%20Decoding&body=Title%3A%20Don%27t%20Waste%20It%3A%20Guiding%20Generative%20Recommenders%20with%20Structured%20Human%20Priors%20via%20Multi-head%20Decoding%0AAuthor%3A%20Yunkai%20Zhang%20and%20Qiang%20Zhang%20and%20%20Feng%20and%20%20Lin%20and%20Ruizhong%20Qiu%20and%20Hanchao%20Yu%20and%20Jason%20Liu%20and%20Yinglong%20Xia%20and%20Zhuoran%20Yu%20and%20Zeyu%20Zheng%20and%20Diji%20Yang%0AAbstract%3A%20Optimizing%20recommender%20systems%20for%20objectives%20beyond%20accuracy%2C%20such%20as%20diversity%2C%20novelty%2C%20and%20personalization%2C%20is%20crucial%20for%20long-term%20user%20satisfaction.%20To%20this%20end%2C%20industrial%20practitioners%20have%20accumulated%20vast%20amounts%20of%20structured%20domain%20knowledge%2C%20which%20we%20term%20human%20priors%20%28e.g.%2C%20item%20taxonomies%2C%20temporal%20patterns%29.%20This%20knowledge%20is%20typically%20applied%20through%20post-hoc%20adjustments%20during%20ranking%20or%20post-ranking.%20However%2C%20this%20approach%20remains%20decoupled%20from%20the%20core%20model%20learning%2C%20which%20is%20particularly%20undesirable%20as%20the%20industry%20shifts%20to%20end-to-end%20generative%20recommendation%20foundation%20models.%20On%20the%20other%20hand%2C%20many%20methods%20targeting%20these%20beyond-accuracy%20objectives%20often%20require%20architecture-specific%20modifications%20and%20discard%20these%20valuable%20human%20priors%20by%20learning%20user%20intent%20in%20a%20fully%20unsupervised%20manner.%0A%20%20Instead%20of%20discarding%20the%20human%20priors%20accumulated%20over%20years%20of%20practice%2C%20we%20introduce%20a%20backbone-agnostic%20framework%20that%20seamlessly%20integrates%20these%20human%20priors%20directly%20into%20the%20end-to-end%20training%20of%20generative%20recommenders.%20With%20lightweight%2C%20prior-conditioned%20adapter%20heads%20inspired%20by%20efficient%20LLM%20decoding%20strategies%2C%20our%20approach%20guides%20the%20model%20to%20disentangle%20user%20intent%20along%20human-understandable%20axes%20%28e.g.%2C%20interaction%20types%2C%20long-%20vs.%20short-term%20interests%29.%20We%20also%20introduce%20a%20hierarchical%20composition%20strategy%20for%20modeling%20complex%20interactions%20across%20different%20prior%20types.%20Extensive%20experiments%20on%20three%20large-scale%20datasets%20demonstrate%20that%20our%20method%20significantly%20enhances%20both%20accuracy%20and%20beyond-accuracy%20objectives.%20We%20also%20show%20that%20human%20priors%20allow%20the%20backbone%20model%20to%20more%20effectively%20leverage%20longer%20context%20lengths%20and%20larger%20model%20sizes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Waste%2520It%253A%2520Guiding%2520Generative%2520Recommenders%2520with%2520Structured%2520Human%2520Priors%2520via%2520Multi-head%2520Decoding%26entry.906535625%3DYunkai%2520Zhang%2520and%2520Qiang%2520Zhang%2520and%2520%2520Feng%2520and%2520%2520Lin%2520and%2520Ruizhong%2520Qiu%2520and%2520Hanchao%2520Yu%2520and%2520Jason%2520Liu%2520and%2520Yinglong%2520Xia%2520and%2520Zhuoran%2520Yu%2520and%2520Zeyu%2520Zheng%2520and%2520Diji%2520Yang%26entry.1292438233%3DOptimizing%2520recommender%2520systems%2520for%2520objectives%2520beyond%2520accuracy%252C%2520such%2520as%2520diversity%252C%2520novelty%252C%2520and%2520personalization%252C%2520is%2520crucial%2520for%2520long-term%2520user%2520satisfaction.%2520To%2520this%2520end%252C%2520industrial%2520practitioners%2520have%2520accumulated%2520vast%2520amounts%2520of%2520structured%2520domain%2520knowledge%252C%2520which%2520we%2520term%2520human%2520priors%2520%2528e.g.%252C%2520item%2520taxonomies%252C%2520temporal%2520patterns%2529.%2520This%2520knowledge%2520is%2520typically%2520applied%2520through%2520post-hoc%2520adjustments%2520during%2520ranking%2520or%2520post-ranking.%2520However%252C%2520this%2520approach%2520remains%2520decoupled%2520from%2520the%2520core%2520model%2520learning%252C%2520which%2520is%2520particularly%2520undesirable%2520as%2520the%2520industry%2520shifts%2520to%2520end-to-end%2520generative%2520recommendation%2520foundation%2520models.%2520On%2520the%2520other%2520hand%252C%2520many%2520methods%2520targeting%2520these%2520beyond-accuracy%2520objectives%2520often%2520require%2520architecture-specific%2520modifications%2520and%2520discard%2520these%2520valuable%2520human%2520priors%2520by%2520learning%2520user%2520intent%2520in%2520a%2520fully%2520unsupervised%2520manner.%250A%2520%2520Instead%2520of%2520discarding%2520the%2520human%2520priors%2520accumulated%2520over%2520years%2520of%2520practice%252C%2520we%2520introduce%2520a%2520backbone-agnostic%2520framework%2520that%2520seamlessly%2520integrates%2520these%2520human%2520priors%2520directly%2520into%2520the%2520end-to-end%2520training%2520of%2520generative%2520recommenders.%2520With%2520lightweight%252C%2520prior-conditioned%2520adapter%2520heads%2520inspired%2520by%2520efficient%2520LLM%2520decoding%2520strategies%252C%2520our%2520approach%2520guides%2520the%2520model%2520to%2520disentangle%2520user%2520intent%2520along%2520human-understandable%2520axes%2520%2528e.g.%252C%2520interaction%2520types%252C%2520long-%2520vs.%2520short-term%2520interests%2529.%2520We%2520also%2520introduce%2520a%2520hierarchical%2520composition%2520strategy%2520for%2520modeling%2520complex%2520interactions%2520across%2520different%2520prior%2520types.%2520Extensive%2520experiments%2520on%2520three%2520large-scale%2520datasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520enhances%2520both%2520accuracy%2520and%2520beyond-accuracy%2520objectives.%2520We%2520also%2520show%2520that%2520human%2520priors%2520allow%2520the%2520backbone%2520model%2520to%2520more%2520effectively%2520leverage%2520longer%2520context%2520lengths%2520and%2520larger%2520model%2520sizes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Waste%20It%3A%20Guiding%20Generative%20Recommenders%20with%20Structured%20Human%20Priors%20via%20Multi-head%20Decoding&entry.906535625=Yunkai%20Zhang%20and%20Qiang%20Zhang%20and%20%20Feng%20and%20%20Lin%20and%20Ruizhong%20Qiu%20and%20Hanchao%20Yu%20and%20Jason%20Liu%20and%20Yinglong%20Xia%20and%20Zhuoran%20Yu%20and%20Zeyu%20Zheng%20and%20Diji%20Yang&entry.1292438233=Optimizing%20recommender%20systems%20for%20objectives%20beyond%20accuracy%2C%20such%20as%20diversity%2C%20novelty%2C%20and%20personalization%2C%20is%20crucial%20for%20long-term%20user%20satisfaction.%20To%20this%20end%2C%20industrial%20practitioners%20have%20accumulated%20vast%20amounts%20of%20structured%20domain%20knowledge%2C%20which%20we%20term%20human%20priors%20%28e.g.%2C%20item%20taxonomies%2C%20temporal%20patterns%29.%20This%20knowledge%20is%20typically%20applied%20through%20post-hoc%20adjustments%20during%20ranking%20or%20post-ranking.%20However%2C%20this%20approach%20remains%20decoupled%20from%20the%20core%20model%20learning%2C%20which%20is%20particularly%20undesirable%20as%20the%20industry%20shifts%20to%20end-to-end%20generative%20recommendation%20foundation%20models.%20On%20the%20other%20hand%2C%20many%20methods%20targeting%20these%20beyond-accuracy%20objectives%20often%20require%20architecture-specific%20modifications%20and%20discard%20these%20valuable%20human%20priors%20by%20learning%20user%20intent%20in%20a%20fully%20unsupervised%20manner.%0A%20%20Instead%20of%20discarding%20the%20human%20priors%20accumulated%20over%20years%20of%20practice%2C%20we%20introduce%20a%20backbone-agnostic%20framework%20that%20seamlessly%20integrates%20these%20human%20priors%20directly%20into%20the%20end-to-end%20training%20of%20generative%20recommenders.%20With%20lightweight%2C%20prior-conditioned%20adapter%20heads%20inspired%20by%20efficient%20LLM%20decoding%20strategies%2C%20our%20approach%20guides%20the%20model%20to%20disentangle%20user%20intent%20along%20human-understandable%20axes%20%28e.g.%2C%20interaction%20types%2C%20long-%20vs.%20short-term%20interests%29.%20We%20also%20introduce%20a%20hierarchical%20composition%20strategy%20for%20modeling%20complex%20interactions%20across%20different%20prior%20types.%20Extensive%20experiments%20on%20three%20large-scale%20datasets%20demonstrate%20that%20our%20method%20significantly%20enhances%20both%20accuracy%20and%20beyond-accuracy%20objectives.%20We%20also%20show%20that%20human%20priors%20allow%20the%20backbone%20model%20to%20more%20effectively%20leverage%20longer%20context%20lengths%20and%20larger%20model%20sizes.&entry.1838667208=http%3A//arxiv.org/abs/2511.10492v1&entry.124074799=Read"},
{"title": "On Stealing Graph Neural Network Models", "author": "Marcin Podhajski and Jan Dubi\u0144ski and Franziska Boenisch and Adam Dziedzic and Agnieszka Pr\u0119gowska and Tomasz P. Michalak", "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.", "link": "http://arxiv.org/abs/2511.07170v2", "date": "2025-11-13", "relevancy": 2.0977, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4313}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Stealing%20Graph%20Neural%20Network%20Models&body=Title%3A%20On%20Stealing%20Graph%20Neural%20Network%20Models%0AAuthor%3A%20Marcin%20Podhajski%20and%20Jan%20Dubi%C5%84ski%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%20and%20Agnieszka%20Pr%C4%99gowska%20and%20Tomasz%20P.%20Michalak%0AAbstract%3A%20Current%20graph%20neural%20network%20%28GNN%29%20model-stealing%20methods%20rely%20heavily%20on%20queries%20to%20the%20victim%20model%2C%20assuming%20no%20hard%20query%20limits.%20However%2C%20in%20reality%2C%20the%20number%20of%20allowed%20queries%20can%20be%20severely%20limited.%20In%20this%20paper%2C%20we%20demonstrate%20how%20an%20adversary%20can%20extract%20a%20GNN%20with%20very%20limited%20interactions%20with%20the%20model.%20Our%20approach%20first%20enables%20the%20adversary%20to%20obtain%20the%20model%20backbone%20without%20making%20direct%20queries%20to%20the%20victim%20model%20and%20then%20to%20strategically%20utilize%20a%20fixed%20query%20limit%20to%20extract%20the%20most%20informative%20data.%20The%20experiments%20on%20eight%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20attack%2C%20even%20under%20a%20very%20restricted%20query%20limit%20and%20under%20defense%20against%20model%20extraction%20in%20place.%20Our%20findings%20underscore%20the%20need%20for%20robust%20defenses%20against%20GNN%20model%20extraction%20threats.%0ALink%3A%20http%3A//arxiv.org/abs/2511.07170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Stealing%2520Graph%2520Neural%2520Network%2520Models%26entry.906535625%3DMarcin%2520Podhajski%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Franziska%2520Boenisch%2520and%2520Adam%2520Dziedzic%2520and%2520Agnieszka%2520Pr%25C4%2599gowska%2520and%2520Tomasz%2520P.%2520Michalak%26entry.1292438233%3DCurrent%2520graph%2520neural%2520network%2520%2528GNN%2529%2520model-stealing%2520methods%2520rely%2520heavily%2520on%2520queries%2520to%2520the%2520victim%2520model%252C%2520assuming%2520no%2520hard%2520query%2520limits.%2520However%252C%2520in%2520reality%252C%2520the%2520number%2520of%2520allowed%2520queries%2520can%2520be%2520severely%2520limited.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520how%2520an%2520adversary%2520can%2520extract%2520a%2520GNN%2520with%2520very%2520limited%2520interactions%2520with%2520the%2520model.%2520Our%2520approach%2520first%2520enables%2520the%2520adversary%2520to%2520obtain%2520the%2520model%2520backbone%2520without%2520making%2520direct%2520queries%2520to%2520the%2520victim%2520model%2520and%2520then%2520to%2520strategically%2520utilize%2520a%2520fixed%2520query%2520limit%2520to%2520extract%2520the%2520most%2520informative%2520data.%2520The%2520experiments%2520on%2520eight%2520real-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520attack%252C%2520even%2520under%2520a%2520very%2520restricted%2520query%2520limit%2520and%2520under%2520defense%2520against%2520model%2520extraction%2520in%2520place.%2520Our%2520findings%2520underscore%2520the%2520need%2520for%2520robust%2520defenses%2520against%2520GNN%2520model%2520extraction%2520threats.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.07170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Stealing%20Graph%20Neural%20Network%20Models&entry.906535625=Marcin%20Podhajski%20and%20Jan%20Dubi%C5%84ski%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%20and%20Agnieszka%20Pr%C4%99gowska%20and%20Tomasz%20P.%20Michalak&entry.1292438233=Current%20graph%20neural%20network%20%28GNN%29%20model-stealing%20methods%20rely%20heavily%20on%20queries%20to%20the%20victim%20model%2C%20assuming%20no%20hard%20query%20limits.%20However%2C%20in%20reality%2C%20the%20number%20of%20allowed%20queries%20can%20be%20severely%20limited.%20In%20this%20paper%2C%20we%20demonstrate%20how%20an%20adversary%20can%20extract%20a%20GNN%20with%20very%20limited%20interactions%20with%20the%20model.%20Our%20approach%20first%20enables%20the%20adversary%20to%20obtain%20the%20model%20backbone%20without%20making%20direct%20queries%20to%20the%20victim%20model%20and%20then%20to%20strategically%20utilize%20a%20fixed%20query%20limit%20to%20extract%20the%20most%20informative%20data.%20The%20experiments%20on%20eight%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20attack%2C%20even%20under%20a%20very%20restricted%20query%20limit%20and%20under%20defense%20against%20model%20extraction%20in%20place.%20Our%20findings%20underscore%20the%20need%20for%20robust%20defenses%20against%20GNN%20model%20extraction%20threats.&entry.1838667208=http%3A//arxiv.org/abs/2511.07170v2&entry.124074799=Read"},
{"title": "Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs", "author": "Miles Wang-Henderson and Ben Kaufman and Edward Williams and Ryan Pederson and Matteo Rossi and Owen Howell and Carl Underkoffler and Narbe Mardirossian and John Parkhill", "abstract": "Batched synthesis and testing of molecular designs is the key bottleneck of drug development. There has been great interest in leveraging biomolecular foundation models as surrogates to accelerate this process. In this work, we show how to obtain scalable probabilistic surrogates of binding affinity for use in Batch Bayesian Optimization (Batch BO). This demands parallel acquisition functions that hedge between designs and the ability to rapidly sample from a joint predictive density to approximate them. Through the framework of Epistemic Neural Networks (ENNs), we obtain scalable joint predictive distributions of binding affinity on top of representations taken from large structure-informed models. Key to this work is an investigation into the importance of prior networks in ENNs and how to pretrain them on synthetic data to improve downstream performance in Batch BO. Their utility is demonstrated by rediscovering known potent EGFR inhibitors on a semi-synthetic benchmark in up to 5x fewer iterations, as well as potent inhibitors from a real-world small-molecule library in up to 10x fewer iterations, offering a promising solution for large-scale drug discovery applications.", "link": "http://arxiv.org/abs/2511.10590v1", "date": "2025-11-13", "relevancy": 2.093, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5403}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrained%20Joint%20Predictions%20for%20Scalable%20Batch%20Bayesian%20Optimization%20of%20Molecular%20Designs&body=Title%3A%20Pretrained%20Joint%20Predictions%20for%20Scalable%20Batch%20Bayesian%20Optimization%20of%20Molecular%20Designs%0AAuthor%3A%20Miles%20Wang-Henderson%20and%20Ben%20Kaufman%20and%20Edward%20Williams%20and%20Ryan%20Pederson%20and%20Matteo%20Rossi%20and%20Owen%20Howell%20and%20Carl%20Underkoffler%20and%20Narbe%20Mardirossian%20and%20John%20Parkhill%0AAbstract%3A%20Batched%20synthesis%20and%20testing%20of%20molecular%20designs%20is%20the%20key%20bottleneck%20of%20drug%20development.%20There%20has%20been%20great%20interest%20in%20leveraging%20biomolecular%20foundation%20models%20as%20surrogates%20to%20accelerate%20this%20process.%20In%20this%20work%2C%20we%20show%20how%20to%20obtain%20scalable%20probabilistic%20surrogates%20of%20binding%20affinity%20for%20use%20in%20Batch%20Bayesian%20Optimization%20%28Batch%20BO%29.%20This%20demands%20parallel%20acquisition%20functions%20that%20hedge%20between%20designs%20and%20the%20ability%20to%20rapidly%20sample%20from%20a%20joint%20predictive%20density%20to%20approximate%20them.%20Through%20the%20framework%20of%20Epistemic%20Neural%20Networks%20%28ENNs%29%2C%20we%20obtain%20scalable%20joint%20predictive%20distributions%20of%20binding%20affinity%20on%20top%20of%20representations%20taken%20from%20large%20structure-informed%20models.%20Key%20to%20this%20work%20is%20an%20investigation%20into%20the%20importance%20of%20prior%20networks%20in%20ENNs%20and%20how%20to%20pretrain%20them%20on%20synthetic%20data%20to%20improve%20downstream%20performance%20in%20Batch%20BO.%20Their%20utility%20is%20demonstrated%20by%20rediscovering%20known%20potent%20EGFR%20inhibitors%20on%20a%20semi-synthetic%20benchmark%20in%20up%20to%205x%20fewer%20iterations%2C%20as%20well%20as%20potent%20inhibitors%20from%20a%20real-world%20small-molecule%20library%20in%20up%20to%2010x%20fewer%20iterations%2C%20offering%20a%20promising%20solution%20for%20large-scale%20drug%20discovery%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrained%2520Joint%2520Predictions%2520for%2520Scalable%2520Batch%2520Bayesian%2520Optimization%2520of%2520Molecular%2520Designs%26entry.906535625%3DMiles%2520Wang-Henderson%2520and%2520Ben%2520Kaufman%2520and%2520Edward%2520Williams%2520and%2520Ryan%2520Pederson%2520and%2520Matteo%2520Rossi%2520and%2520Owen%2520Howell%2520and%2520Carl%2520Underkoffler%2520and%2520Narbe%2520Mardirossian%2520and%2520John%2520Parkhill%26entry.1292438233%3DBatched%2520synthesis%2520and%2520testing%2520of%2520molecular%2520designs%2520is%2520the%2520key%2520bottleneck%2520of%2520drug%2520development.%2520There%2520has%2520been%2520great%2520interest%2520in%2520leveraging%2520biomolecular%2520foundation%2520models%2520as%2520surrogates%2520to%2520accelerate%2520this%2520process.%2520In%2520this%2520work%252C%2520we%2520show%2520how%2520to%2520obtain%2520scalable%2520probabilistic%2520surrogates%2520of%2520binding%2520affinity%2520for%2520use%2520in%2520Batch%2520Bayesian%2520Optimization%2520%2528Batch%2520BO%2529.%2520This%2520demands%2520parallel%2520acquisition%2520functions%2520that%2520hedge%2520between%2520designs%2520and%2520the%2520ability%2520to%2520rapidly%2520sample%2520from%2520a%2520joint%2520predictive%2520density%2520to%2520approximate%2520them.%2520Through%2520the%2520framework%2520of%2520Epistemic%2520Neural%2520Networks%2520%2528ENNs%2529%252C%2520we%2520obtain%2520scalable%2520joint%2520predictive%2520distributions%2520of%2520binding%2520affinity%2520on%2520top%2520of%2520representations%2520taken%2520from%2520large%2520structure-informed%2520models.%2520Key%2520to%2520this%2520work%2520is%2520an%2520investigation%2520into%2520the%2520importance%2520of%2520prior%2520networks%2520in%2520ENNs%2520and%2520how%2520to%2520pretrain%2520them%2520on%2520synthetic%2520data%2520to%2520improve%2520downstream%2520performance%2520in%2520Batch%2520BO.%2520Their%2520utility%2520is%2520demonstrated%2520by%2520rediscovering%2520known%2520potent%2520EGFR%2520inhibitors%2520on%2520a%2520semi-synthetic%2520benchmark%2520in%2520up%2520to%25205x%2520fewer%2520iterations%252C%2520as%2520well%2520as%2520potent%2520inhibitors%2520from%2520a%2520real-world%2520small-molecule%2520library%2520in%2520up%2520to%252010x%2520fewer%2520iterations%252C%2520offering%2520a%2520promising%2520solution%2520for%2520large-scale%2520drug%2520discovery%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrained%20Joint%20Predictions%20for%20Scalable%20Batch%20Bayesian%20Optimization%20of%20Molecular%20Designs&entry.906535625=Miles%20Wang-Henderson%20and%20Ben%20Kaufman%20and%20Edward%20Williams%20and%20Ryan%20Pederson%20and%20Matteo%20Rossi%20and%20Owen%20Howell%20and%20Carl%20Underkoffler%20and%20Narbe%20Mardirossian%20and%20John%20Parkhill&entry.1292438233=Batched%20synthesis%20and%20testing%20of%20molecular%20designs%20is%20the%20key%20bottleneck%20of%20drug%20development.%20There%20has%20been%20great%20interest%20in%20leveraging%20biomolecular%20foundation%20models%20as%20surrogates%20to%20accelerate%20this%20process.%20In%20this%20work%2C%20we%20show%20how%20to%20obtain%20scalable%20probabilistic%20surrogates%20of%20binding%20affinity%20for%20use%20in%20Batch%20Bayesian%20Optimization%20%28Batch%20BO%29.%20This%20demands%20parallel%20acquisition%20functions%20that%20hedge%20between%20designs%20and%20the%20ability%20to%20rapidly%20sample%20from%20a%20joint%20predictive%20density%20to%20approximate%20them.%20Through%20the%20framework%20of%20Epistemic%20Neural%20Networks%20%28ENNs%29%2C%20we%20obtain%20scalable%20joint%20predictive%20distributions%20of%20binding%20affinity%20on%20top%20of%20representations%20taken%20from%20large%20structure-informed%20models.%20Key%20to%20this%20work%20is%20an%20investigation%20into%20the%20importance%20of%20prior%20networks%20in%20ENNs%20and%20how%20to%20pretrain%20them%20on%20synthetic%20data%20to%20improve%20downstream%20performance%20in%20Batch%20BO.%20Their%20utility%20is%20demonstrated%20by%20rediscovering%20known%20potent%20EGFR%20inhibitors%20on%20a%20semi-synthetic%20benchmark%20in%20up%20to%205x%20fewer%20iterations%2C%20as%20well%20as%20potent%20inhibitors%20from%20a%20real-world%20small-molecule%20library%20in%20up%20to%2010x%20fewer%20iterations%2C%20offering%20a%20promising%20solution%20for%20large-scale%20drug%20discovery%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.10590v1&entry.124074799=Read"},
{"title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping", "author": "Zack Dewis and Yimin Zhu and Zhengsen Xu and Mabel Heffring and Saeid Taleghanidoozdoozan and Kaylee Xiao and Motasem Alkayid and Lincoln Linlin Xu", "abstract": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.", "link": "http://arxiv.org/abs/2511.10604v1", "date": "2025-11-13", "relevancy": 2.0912, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multitask%20GLocal%20OBIA-Mamba%20for%20Sentinel-2%20Landcover%20Mapping&body=Title%3A%20Multitask%20GLocal%20OBIA-Mamba%20for%20Sentinel-2%20Landcover%20Mapping%0AAuthor%3A%20Zack%20Dewis%20and%20Yimin%20Zhu%20and%20Zhengsen%20Xu%20and%20Mabel%20Heffring%20and%20Saeid%20Taleghanidoozdoozan%20and%20Kaylee%20Xiao%20and%20Motasem%20Alkayid%20and%20Lincoln%20Linlin%20Xu%0AAbstract%3A%20Although%20Sentinel-2%20based%20land%20use%20and%20land%20cover%20%28LULC%29%20classification%20is%20critical%20for%20various%20environmental%20monitoring%20applications%2C%20it%20is%20a%20very%20difficult%20task%20due%20to%20some%20key%20data%20challenges%20%28e.g.%2C%20spatial%20heterogeneity%2C%20context%20information%2C%20signature%20ambiguity%29.%20This%20paper%20presents%20a%20novel%20Multitask%20Glocal%20OBIA-Mamba%20%28MSOM%29%20for%20enhanced%20Sentinel-2%20classification%20with%20the%20following%20contributions.%20First%2C%20an%20object-based%20image%20analysis%20%28OBIA%29%20Mamba%20model%20%28OBIA-Mamba%29%20is%20designed%20to%20reduce%20redundant%20computation%20without%20compromising%20fine-grained%20details%20by%20using%20superpixels%20as%20Mamba%20tokens.%20Second%2C%20a%20global-local%20%28GLocal%29%20dual-branch%20convolutional%20neural%20network%20%28CNN%29-mamba%20architecture%20is%20designed%20to%20jointly%20model%20local%20spatial%20detail%20and%20global%20contextual%20information.%20Third%2C%20a%20multitask%20optimization%20framework%20is%20designed%20to%20employ%20dual%20loss%20functions%20to%20balance%20local%20precision%20with%20global%20consistency.%20The%20proposed%20approach%20is%20tested%20on%20Sentinel-2%20imagery%20in%20Alberta%2C%20Canada%2C%20in%20comparison%20with%20several%20advanced%20classification%20approaches%2C%20and%20the%20results%20demonstrate%20that%20the%20proposed%20approach%20achieves%20higher%20classification%20accuracy%20and%20finer%20details%20that%20the%20other%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultitask%2520GLocal%2520OBIA-Mamba%2520for%2520Sentinel-2%2520Landcover%2520Mapping%26entry.906535625%3DZack%2520Dewis%2520and%2520Yimin%2520Zhu%2520and%2520Zhengsen%2520Xu%2520and%2520Mabel%2520Heffring%2520and%2520Saeid%2520Taleghanidoozdoozan%2520and%2520Kaylee%2520Xiao%2520and%2520Motasem%2520Alkayid%2520and%2520Lincoln%2520Linlin%2520Xu%26entry.1292438233%3DAlthough%2520Sentinel-2%2520based%2520land%2520use%2520and%2520land%2520cover%2520%2528LULC%2529%2520classification%2520is%2520critical%2520for%2520various%2520environmental%2520monitoring%2520applications%252C%2520it%2520is%2520a%2520very%2520difficult%2520task%2520due%2520to%2520some%2520key%2520data%2520challenges%2520%2528e.g.%252C%2520spatial%2520heterogeneity%252C%2520context%2520information%252C%2520signature%2520ambiguity%2529.%2520This%2520paper%2520presents%2520a%2520novel%2520Multitask%2520Glocal%2520OBIA-Mamba%2520%2528MSOM%2529%2520for%2520enhanced%2520Sentinel-2%2520classification%2520with%2520the%2520following%2520contributions.%2520First%252C%2520an%2520object-based%2520image%2520analysis%2520%2528OBIA%2529%2520Mamba%2520model%2520%2528OBIA-Mamba%2529%2520is%2520designed%2520to%2520reduce%2520redundant%2520computation%2520without%2520compromising%2520fine-grained%2520details%2520by%2520using%2520superpixels%2520as%2520Mamba%2520tokens.%2520Second%252C%2520a%2520global-local%2520%2528GLocal%2529%2520dual-branch%2520convolutional%2520neural%2520network%2520%2528CNN%2529-mamba%2520architecture%2520is%2520designed%2520to%2520jointly%2520model%2520local%2520spatial%2520detail%2520and%2520global%2520contextual%2520information.%2520Third%252C%2520a%2520multitask%2520optimization%2520framework%2520is%2520designed%2520to%2520employ%2520dual%2520loss%2520functions%2520to%2520balance%2520local%2520precision%2520with%2520global%2520consistency.%2520The%2520proposed%2520approach%2520is%2520tested%2520on%2520Sentinel-2%2520imagery%2520in%2520Alberta%252C%2520Canada%252C%2520in%2520comparison%2520with%2520several%2520advanced%2520classification%2520approaches%252C%2520and%2520the%2520results%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520achieves%2520higher%2520classification%2520accuracy%2520and%2520finer%2520details%2520that%2520the%2520other%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multitask%20GLocal%20OBIA-Mamba%20for%20Sentinel-2%20Landcover%20Mapping&entry.906535625=Zack%20Dewis%20and%20Yimin%20Zhu%20and%20Zhengsen%20Xu%20and%20Mabel%20Heffring%20and%20Saeid%20Taleghanidoozdoozan%20and%20Kaylee%20Xiao%20and%20Motasem%20Alkayid%20and%20Lincoln%20Linlin%20Xu&entry.1292438233=Although%20Sentinel-2%20based%20land%20use%20and%20land%20cover%20%28LULC%29%20classification%20is%20critical%20for%20various%20environmental%20monitoring%20applications%2C%20it%20is%20a%20very%20difficult%20task%20due%20to%20some%20key%20data%20challenges%20%28e.g.%2C%20spatial%20heterogeneity%2C%20context%20information%2C%20signature%20ambiguity%29.%20This%20paper%20presents%20a%20novel%20Multitask%20Glocal%20OBIA-Mamba%20%28MSOM%29%20for%20enhanced%20Sentinel-2%20classification%20with%20the%20following%20contributions.%20First%2C%20an%20object-based%20image%20analysis%20%28OBIA%29%20Mamba%20model%20%28OBIA-Mamba%29%20is%20designed%20to%20reduce%20redundant%20computation%20without%20compromising%20fine-grained%20details%20by%20using%20superpixels%20as%20Mamba%20tokens.%20Second%2C%20a%20global-local%20%28GLocal%29%20dual-branch%20convolutional%20neural%20network%20%28CNN%29-mamba%20architecture%20is%20designed%20to%20jointly%20model%20local%20spatial%20detail%20and%20global%20contextual%20information.%20Third%2C%20a%20multitask%20optimization%20framework%20is%20designed%20to%20employ%20dual%20loss%20functions%20to%20balance%20local%20precision%20with%20global%20consistency.%20The%20proposed%20approach%20is%20tested%20on%20Sentinel-2%20imagery%20in%20Alberta%2C%20Canada%2C%20in%20comparison%20with%20several%20advanced%20classification%20approaches%2C%20and%20the%20results%20demonstrate%20that%20the%20proposed%20approach%20achieves%20higher%20classification%20accuracy%20and%20finer%20details%20that%20the%20other%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.10604v1&entry.124074799=Read"},
{"title": "Enhancing Conflict Resolution in Language Models via Abstract Argumentation", "author": "Zhaoqun Li and Xiaotong Fang and Chen Chen and Mengze Li and Beishui Liao", "abstract": "In recent years, large language models (LLMs) have made significant advancements in developing human-like and engaging dialogue systems. However, in tasks such as consensus-building and persuasion, LLMs often struggle to resolve conflicts arising from incomplete or inconsistent information, revealing their limitations in real-world applications. Given these limitations, abstract argumentation, a specialized logical framework designed to resolve conflicts and inconsistencies, becomes particularly relevant. In this paper, we aim to enhance the conflict-solving capabilities of LLMs by leveraging formal abstract argumentation, integrating language model learning with symbolic computation. To achieve this, we develop and curate a dataset comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of the argument acceptability computation process. Subsequently, we fine-tune LLMs on this dataset, focusing on abstract conflict resolution tasks. As a comparative baseline, LLMs are also evaluated using a chain-of-thought approach, however, they fail to solve the conflict-based arguments effectively. Our experiments demonstrate that process explanations play a crucial role in learning. Models trained with explanations exhibit superior generalization accuracy compared to those trained solely on question-answer pairs. Furthermore, leveraging LLMs' self-explanation capabilities, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks.", "link": "http://arxiv.org/abs/2412.16725v2", "date": "2025-11-13", "relevancy": 2.0893, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Conflict%20Resolution%20in%20Language%20Models%20via%20Abstract%20Argumentation&body=Title%3A%20Enhancing%20Conflict%20Resolution%20in%20Language%20Models%20via%20Abstract%20Argumentation%0AAuthor%3A%20Zhaoqun%20Li%20and%20Xiaotong%20Fang%20and%20Chen%20Chen%20and%20Mengze%20Li%20and%20Beishui%20Liao%0AAbstract%3A%20In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20developing%20human-like%20and%20engaging%20dialogue%20systems.%20However%2C%20in%20tasks%20such%20as%20consensus-building%20and%20persuasion%2C%20LLMs%20often%20struggle%20to%20resolve%20conflicts%20arising%20from%20incomplete%20or%20inconsistent%20information%2C%20revealing%20their%20limitations%20in%20real-world%20applications.%20Given%20these%20limitations%2C%20abstract%20argumentation%2C%20a%20specialized%20logical%20framework%20designed%20to%20resolve%20conflicts%20and%20inconsistencies%2C%20becomes%20particularly%20relevant.%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20conflict-solving%20capabilities%20of%20LLMs%20by%20leveraging%20formal%20abstract%20argumentation%2C%20integrating%20language%20model%20learning%20with%20symbolic%20computation.%20To%20achieve%20this%2C%20we%20develop%20and%20curate%20a%20dataset%20comprising%20diverse%20abstract%20argumentation%20frameworks%2C%20accompanied%20by%20detailed%20explanations%20of%20the%20argument%20acceptability%20computation%20process.%20Subsequently%2C%20we%20fine-tune%20LLMs%20on%20this%20dataset%2C%20focusing%20on%20abstract%20conflict%20resolution%20tasks.%20As%20a%20comparative%20baseline%2C%20LLMs%20are%20also%20evaluated%20using%20a%20chain-of-thought%20approach%2C%20however%2C%20they%20fail%20to%20solve%20the%20conflict-based%20arguments%20effectively.%20Our%20experiments%20demonstrate%20that%20process%20explanations%20play%20a%20crucial%20role%20in%20learning.%20Models%20trained%20with%20explanations%20exhibit%20superior%20generalization%20accuracy%20compared%20to%20those%20trained%20solely%20on%20question-answer%20pairs.%20Furthermore%2C%20leveraging%20LLMs%27%20self-explanation%20capabilities%2C%20our%20approach%20provides%20detailed%20illustrations%20that%20mitigate%20the%20lack%20of%20transparency%20typically%20associated%20with%20neural%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2412.16725v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Conflict%2520Resolution%2520in%2520Language%2520Models%2520via%2520Abstract%2520Argumentation%26entry.906535625%3DZhaoqun%2520Li%2520and%2520Xiaotong%2520Fang%2520and%2520Chen%2520Chen%2520and%2520Mengze%2520Li%2520and%2520Beishui%2520Liao%26entry.1292438233%3DIn%2520recent%2520years%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520advancements%2520in%2520developing%2520human-like%2520and%2520engaging%2520dialogue%2520systems.%2520However%252C%2520in%2520tasks%2520such%2520as%2520consensus-building%2520and%2520persuasion%252C%2520LLMs%2520often%2520struggle%2520to%2520resolve%2520conflicts%2520arising%2520from%2520incomplete%2520or%2520inconsistent%2520information%252C%2520revealing%2520their%2520limitations%2520in%2520real-world%2520applications.%2520Given%2520these%2520limitations%252C%2520abstract%2520argumentation%252C%2520a%2520specialized%2520logical%2520framework%2520designed%2520to%2520resolve%2520conflicts%2520and%2520inconsistencies%252C%2520becomes%2520particularly%2520relevant.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520enhance%2520the%2520conflict-solving%2520capabilities%2520of%2520LLMs%2520by%2520leveraging%2520formal%2520abstract%2520argumentation%252C%2520integrating%2520language%2520model%2520learning%2520with%2520symbolic%2520computation.%2520To%2520achieve%2520this%252C%2520we%2520develop%2520and%2520curate%2520a%2520dataset%2520comprising%2520diverse%2520abstract%2520argumentation%2520frameworks%252C%2520accompanied%2520by%2520detailed%2520explanations%2520of%2520the%2520argument%2520acceptability%2520computation%2520process.%2520Subsequently%252C%2520we%2520fine-tune%2520LLMs%2520on%2520this%2520dataset%252C%2520focusing%2520on%2520abstract%2520conflict%2520resolution%2520tasks.%2520As%2520a%2520comparative%2520baseline%252C%2520LLMs%2520are%2520also%2520evaluated%2520using%2520a%2520chain-of-thought%2520approach%252C%2520however%252C%2520they%2520fail%2520to%2520solve%2520the%2520conflict-based%2520arguments%2520effectively.%2520Our%2520experiments%2520demonstrate%2520that%2520process%2520explanations%2520play%2520a%2520crucial%2520role%2520in%2520learning.%2520Models%2520trained%2520with%2520explanations%2520exhibit%2520superior%2520generalization%2520accuracy%2520compared%2520to%2520those%2520trained%2520solely%2520on%2520question-answer%2520pairs.%2520Furthermore%252C%2520leveraging%2520LLMs%2527%2520self-explanation%2520capabilities%252C%2520our%2520approach%2520provides%2520detailed%2520illustrations%2520that%2520mitigate%2520the%2520lack%2520of%2520transparency%2520typically%2520associated%2520with%2520neural%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16725v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Conflict%20Resolution%20in%20Language%20Models%20via%20Abstract%20Argumentation&entry.906535625=Zhaoqun%20Li%20and%20Xiaotong%20Fang%20and%20Chen%20Chen%20and%20Mengze%20Li%20and%20Beishui%20Liao&entry.1292438233=In%20recent%20years%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20developing%20human-like%20and%20engaging%20dialogue%20systems.%20However%2C%20in%20tasks%20such%20as%20consensus-building%20and%20persuasion%2C%20LLMs%20often%20struggle%20to%20resolve%20conflicts%20arising%20from%20incomplete%20or%20inconsistent%20information%2C%20revealing%20their%20limitations%20in%20real-world%20applications.%20Given%20these%20limitations%2C%20abstract%20argumentation%2C%20a%20specialized%20logical%20framework%20designed%20to%20resolve%20conflicts%20and%20inconsistencies%2C%20becomes%20particularly%20relevant.%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20conflict-solving%20capabilities%20of%20LLMs%20by%20leveraging%20formal%20abstract%20argumentation%2C%20integrating%20language%20model%20learning%20with%20symbolic%20computation.%20To%20achieve%20this%2C%20we%20develop%20and%20curate%20a%20dataset%20comprising%20diverse%20abstract%20argumentation%20frameworks%2C%20accompanied%20by%20detailed%20explanations%20of%20the%20argument%20acceptability%20computation%20process.%20Subsequently%2C%20we%20fine-tune%20LLMs%20on%20this%20dataset%2C%20focusing%20on%20abstract%20conflict%20resolution%20tasks.%20As%20a%20comparative%20baseline%2C%20LLMs%20are%20also%20evaluated%20using%20a%20chain-of-thought%20approach%2C%20however%2C%20they%20fail%20to%20solve%20the%20conflict-based%20arguments%20effectively.%20Our%20experiments%20demonstrate%20that%20process%20explanations%20play%20a%20crucial%20role%20in%20learning.%20Models%20trained%20with%20explanations%20exhibit%20superior%20generalization%20accuracy%20compared%20to%20those%20trained%20solely%20on%20question-answer%20pairs.%20Furthermore%2C%20leveraging%20LLMs%27%20self-explanation%20capabilities%2C%20our%20approach%20provides%20detailed%20illustrations%20that%20mitigate%20the%20lack%20of%20transparency%20typically%20associated%20with%20neural%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2412.16725v2&entry.124074799=Read"},
{"title": "LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization", "author": "Ronghuan Wu and Wanchao Su and Jing Liao", "abstract": "Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored optimization-based and learning-based layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeeler's success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity.", "link": "http://arxiv.org/abs/2505.23740v3", "date": "2025-11-13", "relevancy": 2.087, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5501}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5015}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerPeeler%3A%20Autoregressive%20Peeling%20for%20Layer-wise%20Image%20Vectorization&body=Title%3A%20LayerPeeler%3A%20Autoregressive%20Peeling%20for%20Layer-wise%20Image%20Vectorization%0AAuthor%3A%20Ronghuan%20Wu%20and%20Wanchao%20Su%20and%20Jing%20Liao%0AAbstract%3A%20Image%20vectorization%20is%20a%20powerful%20technique%20that%20converts%20raster%20images%20into%20vector%20graphics%2C%20enabling%20enhanced%20flexibility%20and%20interactivity.%20However%2C%20popular%20image%20vectorization%20tools%20struggle%20with%20occluded%20regions%2C%20producing%20incomplete%20or%20fragmented%20shapes%20that%20hinder%20editability.%20While%20recent%20advancements%20have%20explored%20optimization-based%20and%20learning-based%20layer-wise%20image%20vectorization%2C%20these%20methods%20face%20limitations%20in%20vectorization%20quality%20and%20flexibility.%20In%20this%20paper%2C%20we%20introduce%20LayerPeeler%2C%20a%20novel%20layer-wise%20image%20vectorization%20approach%20that%20addresses%20these%20challenges%20through%20a%20progressive%20simplification%20paradigm.%20The%20key%20to%20LayerPeeler%27s%20success%20lies%20in%20its%20autoregressive%20peeling%20strategy%3A%20by%20identifying%20and%20removing%20the%20topmost%20non-occluded%20layers%20while%20recovering%20underlying%20content%2C%20we%20generate%20vector%20graphics%20with%20complete%20paths%20and%20coherent%20layer%20structures.%20Our%20method%20leverages%20vision-language%20models%20to%20construct%20a%20layer%20graph%20that%20captures%20occlusion%20relationships%20among%20elements%2C%20enabling%20precise%20detection%20and%20description%20for%20non-occluded%20layers.%20These%20descriptive%20captions%20are%20used%20as%20editing%20instructions%20for%20a%20finetuned%20image%20diffusion%20model%20to%20remove%20the%20identified%20layers.%20To%20ensure%20accurate%20removal%2C%20we%20employ%20localized%20attention%20control%20that%20precisely%20guides%20the%20model%20to%20target%20regions%20while%20faithfully%20preserving%20the%20surrounding%20content.%20To%20support%20this%2C%20we%20contribute%20a%20large-scale%20dataset%20specifically%20designed%20for%20layer%20peeling%20tasks.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20LayerPeeler%20significantly%20outperforms%20existing%20techniques%2C%20producing%20vectorization%20results%20with%20superior%20path%20semantics%2C%20geometric%20regularity%2C%20and%20visual%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2505.23740v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerPeeler%253A%2520Autoregressive%2520Peeling%2520for%2520Layer-wise%2520Image%2520Vectorization%26entry.906535625%3DRonghuan%2520Wu%2520and%2520Wanchao%2520Su%2520and%2520Jing%2520Liao%26entry.1292438233%3DImage%2520vectorization%2520is%2520a%2520powerful%2520technique%2520that%2520converts%2520raster%2520images%2520into%2520vector%2520graphics%252C%2520enabling%2520enhanced%2520flexibility%2520and%2520interactivity.%2520However%252C%2520popular%2520image%2520vectorization%2520tools%2520struggle%2520with%2520occluded%2520regions%252C%2520producing%2520incomplete%2520or%2520fragmented%2520shapes%2520that%2520hinder%2520editability.%2520While%2520recent%2520advancements%2520have%2520explored%2520optimization-based%2520and%2520learning-based%2520layer-wise%2520image%2520vectorization%252C%2520these%2520methods%2520face%2520limitations%2520in%2520vectorization%2520quality%2520and%2520flexibility.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LayerPeeler%252C%2520a%2520novel%2520layer-wise%2520image%2520vectorization%2520approach%2520that%2520addresses%2520these%2520challenges%2520through%2520a%2520progressive%2520simplification%2520paradigm.%2520The%2520key%2520to%2520LayerPeeler%2527s%2520success%2520lies%2520in%2520its%2520autoregressive%2520peeling%2520strategy%253A%2520by%2520identifying%2520and%2520removing%2520the%2520topmost%2520non-occluded%2520layers%2520while%2520recovering%2520underlying%2520content%252C%2520we%2520generate%2520vector%2520graphics%2520with%2520complete%2520paths%2520and%2520coherent%2520layer%2520structures.%2520Our%2520method%2520leverages%2520vision-language%2520models%2520to%2520construct%2520a%2520layer%2520graph%2520that%2520captures%2520occlusion%2520relationships%2520among%2520elements%252C%2520enabling%2520precise%2520detection%2520and%2520description%2520for%2520non-occluded%2520layers.%2520These%2520descriptive%2520captions%2520are%2520used%2520as%2520editing%2520instructions%2520for%2520a%2520finetuned%2520image%2520diffusion%2520model%2520to%2520remove%2520the%2520identified%2520layers.%2520To%2520ensure%2520accurate%2520removal%252C%2520we%2520employ%2520localized%2520attention%2520control%2520that%2520precisely%2520guides%2520the%2520model%2520to%2520target%2520regions%2520while%2520faithfully%2520preserving%2520the%2520surrounding%2520content.%2520To%2520support%2520this%252C%2520we%2520contribute%2520a%2520large-scale%2520dataset%2520specifically%2520designed%2520for%2520layer%2520peeling%2520tasks.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520that%2520LayerPeeler%2520significantly%2520outperforms%2520existing%2520techniques%252C%2520producing%2520vectorization%2520results%2520with%2520superior%2520path%2520semantics%252C%2520geometric%2520regularity%252C%2520and%2520visual%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23740v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerPeeler%3A%20Autoregressive%20Peeling%20for%20Layer-wise%20Image%20Vectorization&entry.906535625=Ronghuan%20Wu%20and%20Wanchao%20Su%20and%20Jing%20Liao&entry.1292438233=Image%20vectorization%20is%20a%20powerful%20technique%20that%20converts%20raster%20images%20into%20vector%20graphics%2C%20enabling%20enhanced%20flexibility%20and%20interactivity.%20However%2C%20popular%20image%20vectorization%20tools%20struggle%20with%20occluded%20regions%2C%20producing%20incomplete%20or%20fragmented%20shapes%20that%20hinder%20editability.%20While%20recent%20advancements%20have%20explored%20optimization-based%20and%20learning-based%20layer-wise%20image%20vectorization%2C%20these%20methods%20face%20limitations%20in%20vectorization%20quality%20and%20flexibility.%20In%20this%20paper%2C%20we%20introduce%20LayerPeeler%2C%20a%20novel%20layer-wise%20image%20vectorization%20approach%20that%20addresses%20these%20challenges%20through%20a%20progressive%20simplification%20paradigm.%20The%20key%20to%20LayerPeeler%27s%20success%20lies%20in%20its%20autoregressive%20peeling%20strategy%3A%20by%20identifying%20and%20removing%20the%20topmost%20non-occluded%20layers%20while%20recovering%20underlying%20content%2C%20we%20generate%20vector%20graphics%20with%20complete%20paths%20and%20coherent%20layer%20structures.%20Our%20method%20leverages%20vision-language%20models%20to%20construct%20a%20layer%20graph%20that%20captures%20occlusion%20relationships%20among%20elements%2C%20enabling%20precise%20detection%20and%20description%20for%20non-occluded%20layers.%20These%20descriptive%20captions%20are%20used%20as%20editing%20instructions%20for%20a%20finetuned%20image%20diffusion%20model%20to%20remove%20the%20identified%20layers.%20To%20ensure%20accurate%20removal%2C%20we%20employ%20localized%20attention%20control%20that%20precisely%20guides%20the%20model%20to%20target%20regions%20while%20faithfully%20preserving%20the%20surrounding%20content.%20To%20support%20this%2C%20we%20contribute%20a%20large-scale%20dataset%20specifically%20designed%20for%20layer%20peeling%20tasks.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20LayerPeeler%20significantly%20outperforms%20existing%20techniques%2C%20producing%20vectorization%20results%20with%20superior%20path%20semantics%2C%20geometric%20regularity%2C%20and%20visual%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2505.23740v3&entry.124074799=Read"},
{"title": "Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations", "author": "Willem Bonnaff\u00e9 and Yang Hu and Andrea Chatrian and Mengran Fan and Stefano Malacrino and Sandy Figiel and CRUK ICGC Prostate Group and Srinivasa R. Rao and Richard Colling and Richard J. Bryant and Freddie C. Hamdy and Dan J. Woodcock and Ian G. Mills and Clare Verrill and Jens Rittscher", "abstract": "Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.", "link": "http://arxiv.org/abs/2511.10432v1", "date": "2025-11-13", "relevancy": 2.0733, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.442}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histology-informed%20tiling%20of%20whole%20tissue%20sections%20improves%20the%20interpretability%20and%20predictability%20of%20cancer%20relapse%20and%20genetic%20alterations&body=Title%3A%20Histology-informed%20tiling%20of%20whole%20tissue%20sections%20improves%20the%20interpretability%20and%20predictability%20of%20cancer%20relapse%20and%20genetic%20alterations%0AAuthor%3A%20Willem%20Bonnaff%C3%A9%20and%20Yang%20Hu%20and%20Andrea%20Chatrian%20and%20Mengran%20Fan%20and%20Stefano%20Malacrino%20and%20Sandy%20Figiel%20and%20CRUK%20ICGC%20Prostate%20Group%20and%20Srinivasa%20R.%20Rao%20and%20Richard%20Colling%20and%20Richard%20J.%20Bryant%20and%20Freddie%20C.%20Hamdy%20and%20Dan%20J.%20Woodcock%20and%20Ian%20G.%20Mills%20and%20Clare%20Verrill%20and%20Jens%20Rittscher%0AAbstract%3A%20Histopathologists%20establish%20cancer%20grade%20by%20assessing%20histological%20structures%2C%20such%20as%20glands%20in%20prostate%20cancer.%20Yet%2C%20digital%20pathology%20pipelines%20often%20rely%20on%20grid-based%20tiling%20that%20ignores%20tissue%20architecture.%20This%20introduces%20irrelevant%20information%20and%20limits%20interpretability.%20We%20introduce%20histology-informed%20tiling%20%28HIT%29%2C%20which%20uses%20semantic%20segmentation%20to%20extract%20glands%20from%20whole%20slide%20images%20%28WSIs%29%20as%20biologically%20meaningful%20input%20patches%20for%20multiple-instance%20learning%20%28MIL%29%20and%20phenotyping.%20Trained%20on%20137%20samples%20from%20the%20ProMPT%20cohort%2C%20HIT%20achieved%20a%20gland-level%20Dice%20score%20of%200.83%20%2B/-%200.17.%20By%20extracting%20380%2C000%20glands%20from%20760%20WSIs%20across%20ICGC-C%20and%20TCGA-PRAD%20cohorts%2C%20HIT%20improved%20MIL%20models%20AUCs%20by%2010%25%20for%20detecting%20copy%20number%20variation%20%28CNVs%29%20in%20genes%20related%20to%20epithelial-mesenchymal%20transitions%20%28EMT%29%20and%20MYC%2C%20and%20revealed%2015%20gland%20clusters%2C%20several%20of%20which%20were%20associated%20with%20cancer%20relapse%2C%20oncogenic%20mutations%2C%20and%20high%20Gleason.%20Therefore%2C%20HIT%20improved%20the%20accuracy%20and%20interpretability%20of%20MIL%20predictions%2C%20while%20streamlining%20computations%20by%20focussing%20on%20biologically%20meaningful%20structures%20during%20feature%20extraction.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistology-informed%2520tiling%2520of%2520whole%2520tissue%2520sections%2520improves%2520the%2520interpretability%2520and%2520predictability%2520of%2520cancer%2520relapse%2520and%2520genetic%2520alterations%26entry.906535625%3DWillem%2520Bonnaff%25C3%25A9%2520and%2520Yang%2520Hu%2520and%2520Andrea%2520Chatrian%2520and%2520Mengran%2520Fan%2520and%2520Stefano%2520Malacrino%2520and%2520Sandy%2520Figiel%2520and%2520CRUK%2520ICGC%2520Prostate%2520Group%2520and%2520Srinivasa%2520R.%2520Rao%2520and%2520Richard%2520Colling%2520and%2520Richard%2520J.%2520Bryant%2520and%2520Freddie%2520C.%2520Hamdy%2520and%2520Dan%2520J.%2520Woodcock%2520and%2520Ian%2520G.%2520Mills%2520and%2520Clare%2520Verrill%2520and%2520Jens%2520Rittscher%26entry.1292438233%3DHistopathologists%2520establish%2520cancer%2520grade%2520by%2520assessing%2520histological%2520structures%252C%2520such%2520as%2520glands%2520in%2520prostate%2520cancer.%2520Yet%252C%2520digital%2520pathology%2520pipelines%2520often%2520rely%2520on%2520grid-based%2520tiling%2520that%2520ignores%2520tissue%2520architecture.%2520This%2520introduces%2520irrelevant%2520information%2520and%2520limits%2520interpretability.%2520We%2520introduce%2520histology-informed%2520tiling%2520%2528HIT%2529%252C%2520which%2520uses%2520semantic%2520segmentation%2520to%2520extract%2520glands%2520from%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520as%2520biologically%2520meaningful%2520input%2520patches%2520for%2520multiple-instance%2520learning%2520%2528MIL%2529%2520and%2520phenotyping.%2520Trained%2520on%2520137%2520samples%2520from%2520the%2520ProMPT%2520cohort%252C%2520HIT%2520achieved%2520a%2520gland-level%2520Dice%2520score%2520of%25200.83%2520%252B/-%25200.17.%2520By%2520extracting%2520380%252C000%2520glands%2520from%2520760%2520WSIs%2520across%2520ICGC-C%2520and%2520TCGA-PRAD%2520cohorts%252C%2520HIT%2520improved%2520MIL%2520models%2520AUCs%2520by%252010%2525%2520for%2520detecting%2520copy%2520number%2520variation%2520%2528CNVs%2529%2520in%2520genes%2520related%2520to%2520epithelial-mesenchymal%2520transitions%2520%2528EMT%2529%2520and%2520MYC%252C%2520and%2520revealed%252015%2520gland%2520clusters%252C%2520several%2520of%2520which%2520were%2520associated%2520with%2520cancer%2520relapse%252C%2520oncogenic%2520mutations%252C%2520and%2520high%2520Gleason.%2520Therefore%252C%2520HIT%2520improved%2520the%2520accuracy%2520and%2520interpretability%2520of%2520MIL%2520predictions%252C%2520while%2520streamlining%2520computations%2520by%2520focussing%2520on%2520biologically%2520meaningful%2520structures%2520during%2520feature%2520extraction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histology-informed%20tiling%20of%20whole%20tissue%20sections%20improves%20the%20interpretability%20and%20predictability%20of%20cancer%20relapse%20and%20genetic%20alterations&entry.906535625=Willem%20Bonnaff%C3%A9%20and%20Yang%20Hu%20and%20Andrea%20Chatrian%20and%20Mengran%20Fan%20and%20Stefano%20Malacrino%20and%20Sandy%20Figiel%20and%20CRUK%20ICGC%20Prostate%20Group%20and%20Srinivasa%20R.%20Rao%20and%20Richard%20Colling%20and%20Richard%20J.%20Bryant%20and%20Freddie%20C.%20Hamdy%20and%20Dan%20J.%20Woodcock%20and%20Ian%20G.%20Mills%20and%20Clare%20Verrill%20and%20Jens%20Rittscher&entry.1292438233=Histopathologists%20establish%20cancer%20grade%20by%20assessing%20histological%20structures%2C%20such%20as%20glands%20in%20prostate%20cancer.%20Yet%2C%20digital%20pathology%20pipelines%20often%20rely%20on%20grid-based%20tiling%20that%20ignores%20tissue%20architecture.%20This%20introduces%20irrelevant%20information%20and%20limits%20interpretability.%20We%20introduce%20histology-informed%20tiling%20%28HIT%29%2C%20which%20uses%20semantic%20segmentation%20to%20extract%20glands%20from%20whole%20slide%20images%20%28WSIs%29%20as%20biologically%20meaningful%20input%20patches%20for%20multiple-instance%20learning%20%28MIL%29%20and%20phenotyping.%20Trained%20on%20137%20samples%20from%20the%20ProMPT%20cohort%2C%20HIT%20achieved%20a%20gland-level%20Dice%20score%20of%200.83%20%2B/-%200.17.%20By%20extracting%20380%2C000%20glands%20from%20760%20WSIs%20across%20ICGC-C%20and%20TCGA-PRAD%20cohorts%2C%20HIT%20improved%20MIL%20models%20AUCs%20by%2010%25%20for%20detecting%20copy%20number%20variation%20%28CNVs%29%20in%20genes%20related%20to%20epithelial-mesenchymal%20transitions%20%28EMT%29%20and%20MYC%2C%20and%20revealed%2015%20gland%20clusters%2C%20several%20of%20which%20were%20associated%20with%20cancer%20relapse%2C%20oncogenic%20mutations%2C%20and%20high%20Gleason.%20Therefore%2C%20HIT%20improved%20the%20accuracy%20and%20interpretability%20of%20MIL%20predictions%2C%20while%20streamlining%20computations%20by%20focussing%20on%20biologically%20meaningful%20structures%20during%20feature%20extraction.&entry.1838667208=http%3A//arxiv.org/abs/2511.10432v1&entry.124074799=Read"},
{"title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation", "author": "Kai-Hendrik Cohrs and Zuzanna Osika and Maria Gonzalez-Calabuig and Vishal Nedungadi and Ruben Cartuyvels and Steffen Knoblauch and Joppe Massant and Shruti Nath and Patrick Ebel and Vasileios Sitokonstantinou", "abstract": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.", "link": "http://arxiv.org/abs/2511.10370v1", "date": "2025-11-13", "relevancy": 1.4882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5009}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4929}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHRUG-FM%3A%20Reliability-Aware%20Foundation%20Models%20for%20Earth%20Observation&body=Title%3A%20SHRUG-FM%3A%20Reliability-Aware%20Foundation%20Models%20for%20Earth%20Observation%0AAuthor%3A%20Kai-Hendrik%20Cohrs%20and%20Zuzanna%20Osika%20and%20Maria%20Gonzalez-Calabuig%20and%20Vishal%20Nedungadi%20and%20Ruben%20Cartuyvels%20and%20Steffen%20Knoblauch%20and%20Joppe%20Massant%20and%20Shruti%20Nath%20and%20Patrick%20Ebel%20and%20Vasileios%20Sitokonstantinou%0AAbstract%3A%20Geospatial%20foundation%20models%20for%20Earth%20observation%20often%20fail%20to%20perform%20reliably%20in%20environments%20underrepresented%20during%20pretraining.%20We%20introduce%20SHRUG-FM%2C%20a%20framework%20for%20reliability-aware%20prediction%20that%20integrates%20three%20complementary%20signals%3A%20out-of-distribution%20%28OOD%29%20detection%20in%20the%20input%20space%2C%20OOD%20detection%20in%20the%20embedding%20space%20and%20task-specific%20predictive%20uncertainty.%20Applied%20to%20burn%20scar%20segmentation%2C%20SHRUG-FM%20shows%20that%20OOD%20scores%20correlate%20with%20lower%20performance%20in%20specific%20environmental%20conditions%2C%20while%20uncertainty-based%20flags%20help%20discard%20many%20poorly%20performing%20predictions.%20Linking%20these%20flags%20to%20land%20cover%20attributes%20from%20HydroATLAS%20shows%20that%20failures%20are%20not%20random%20but%20concentrated%20in%20certain%20geographies%2C%20such%20as%20low-elevation%20zones%20and%20large%20river%20areas%2C%20likely%20due%20to%20underrepresentation%20in%20pretraining%20data.%20SHRUG-FM%20provides%20a%20pathway%20toward%20safer%20and%20more%20interpretable%20deployment%20of%20GFMs%20in%20climate-sensitive%20applications%2C%20helping%20bridge%20the%20gap%20between%20benchmark%20performance%20and%20real-world%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10370v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHRUG-FM%253A%2520Reliability-Aware%2520Foundation%2520Models%2520for%2520Earth%2520Observation%26entry.906535625%3DKai-Hendrik%2520Cohrs%2520and%2520Zuzanna%2520Osika%2520and%2520Maria%2520Gonzalez-Calabuig%2520and%2520Vishal%2520Nedungadi%2520and%2520Ruben%2520Cartuyvels%2520and%2520Steffen%2520Knoblauch%2520and%2520Joppe%2520Massant%2520and%2520Shruti%2520Nath%2520and%2520Patrick%2520Ebel%2520and%2520Vasileios%2520Sitokonstantinou%26entry.1292438233%3DGeospatial%2520foundation%2520models%2520for%2520Earth%2520observation%2520often%2520fail%2520to%2520perform%2520reliably%2520in%2520environments%2520underrepresented%2520during%2520pretraining.%2520We%2520introduce%2520SHRUG-FM%252C%2520a%2520framework%2520for%2520reliability-aware%2520prediction%2520that%2520integrates%2520three%2520complementary%2520signals%253A%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520in%2520the%2520input%2520space%252C%2520OOD%2520detection%2520in%2520the%2520embedding%2520space%2520and%2520task-specific%2520predictive%2520uncertainty.%2520Applied%2520to%2520burn%2520scar%2520segmentation%252C%2520SHRUG-FM%2520shows%2520that%2520OOD%2520scores%2520correlate%2520with%2520lower%2520performance%2520in%2520specific%2520environmental%2520conditions%252C%2520while%2520uncertainty-based%2520flags%2520help%2520discard%2520many%2520poorly%2520performing%2520predictions.%2520Linking%2520these%2520flags%2520to%2520land%2520cover%2520attributes%2520from%2520HydroATLAS%2520shows%2520that%2520failures%2520are%2520not%2520random%2520but%2520concentrated%2520in%2520certain%2520geographies%252C%2520such%2520as%2520low-elevation%2520zones%2520and%2520large%2520river%2520areas%252C%2520likely%2520due%2520to%2520underrepresentation%2520in%2520pretraining%2520data.%2520SHRUG-FM%2520provides%2520a%2520pathway%2520toward%2520safer%2520and%2520more%2520interpretable%2520deployment%2520of%2520GFMs%2520in%2520climate-sensitive%2520applications%252C%2520helping%2520bridge%2520the%2520gap%2520between%2520benchmark%2520performance%2520and%2520real-world%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10370v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHRUG-FM%3A%20Reliability-Aware%20Foundation%20Models%20for%20Earth%20Observation&entry.906535625=Kai-Hendrik%20Cohrs%20and%20Zuzanna%20Osika%20and%20Maria%20Gonzalez-Calabuig%20and%20Vishal%20Nedungadi%20and%20Ruben%20Cartuyvels%20and%20Steffen%20Knoblauch%20and%20Joppe%20Massant%20and%20Shruti%20Nath%20and%20Patrick%20Ebel%20and%20Vasileios%20Sitokonstantinou&entry.1292438233=Geospatial%20foundation%20models%20for%20Earth%20observation%20often%20fail%20to%20perform%20reliably%20in%20environments%20underrepresented%20during%20pretraining.%20We%20introduce%20SHRUG-FM%2C%20a%20framework%20for%20reliability-aware%20prediction%20that%20integrates%20three%20complementary%20signals%3A%20out-of-distribution%20%28OOD%29%20detection%20in%20the%20input%20space%2C%20OOD%20detection%20in%20the%20embedding%20space%20and%20task-specific%20predictive%20uncertainty.%20Applied%20to%20burn%20scar%20segmentation%2C%20SHRUG-FM%20shows%20that%20OOD%20scores%20correlate%20with%20lower%20performance%20in%20specific%20environmental%20conditions%2C%20while%20uncertainty-based%20flags%20help%20discard%20many%20poorly%20performing%20predictions.%20Linking%20these%20flags%20to%20land%20cover%20attributes%20from%20HydroATLAS%20shows%20that%20failures%20are%20not%20random%20but%20concentrated%20in%20certain%20geographies%2C%20such%20as%20low-elevation%20zones%20and%20large%20river%20areas%2C%20likely%20due%20to%20underrepresentation%20in%20pretraining%20data.%20SHRUG-FM%20provides%20a%20pathway%20toward%20safer%20and%20more%20interpretable%20deployment%20of%20GFMs%20in%20climate-sensitive%20applications%2C%20helping%20bridge%20the%20gap%20between%20benchmark%20performance%20and%20real-world%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2511.10370v1&entry.124074799=Read"},
{"title": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages", "author": "Guduru Manoj and Neel Prabhanjan Rachamalla and Ashish Kulkarni and Gautam Rajeev and Jay Piplodiya and Arul Menezes and Shaharukh Khan and Souvik Rana and Manya Sah and Chandra Khatri and Shubham Agarwal", "abstract": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.", "link": "http://arxiv.org/abs/2511.10338v1", "date": "2025-11-13", "relevancy": 1.7879, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4624}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4506}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BhashaKritika%3A%20Building%20Synthetic%20Pretraining%20Data%20at%20Scale%20for%20Indic%20Languages&body=Title%3A%20BhashaKritika%3A%20Building%20Synthetic%20Pretraining%20Data%20at%20Scale%20for%20Indic%20Languages%0AAuthor%3A%20Guduru%20Manoj%20and%20Neel%20Prabhanjan%20Rachamalla%20and%20Ashish%20Kulkarni%20and%20Gautam%20Rajeev%20and%20Jay%20Piplodiya%20and%20Arul%20Menezes%20and%20Shaharukh%20Khan%20and%20Souvik%20Rana%20and%20Manya%20Sah%20and%20Chandra%20Khatri%20and%20Shubham%20Agarwal%0AAbstract%3A%20In%20the%20context%20of%20pretraining%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20synthetic%20data%20has%20emerged%20as%20an%20alternative%20for%20generating%20high-quality%20pretraining%20data%20at%20scale.%20This%20is%20particularly%20beneficial%20in%20low-resource%20language%20settings%20where%20the%20benefits%20of%20recent%20LLMs%20have%20been%20unevenly%20distributed%20across%20languages.%20In%20this%20work%2C%20we%20present%20a%20systematic%20study%20on%20the%20generation%20and%20evaluation%20of%20synthetic%20multilingual%20pretraining%20data%20for%20Indic%20languages%2C%20where%20we%20construct%20a%20large-scale%20synthetic%20dataset%20BhashaKritika%2C%20comprising%20540B%20tokens%20using%205%20different%20techniques%20for%2010%20languages.%20We%20explore%20the%20impact%20of%20grounding%20generation%20in%20documents%2C%20personas%2C%20and%20topics.%20We%20analyze%20how%20language%20choice%2C%20both%20in%20the%20prompt%20instructions%20and%20document%20grounding%2C%20affects%20data%20quality%2C%20and%20we%20compare%20translations%20of%20English%20content%20with%20native%20generation%20in%20Indic%20languages.%20To%20support%20scalable%20and%20language-sensitive%20evaluation%2C%20we%20introduce%20a%20modular%20quality%20evaluation%20pipeline%20that%20integrates%20script%20and%20language%20detection%2C%20metadata%20consistency%20checks%2C%20n-gram%20repetition%20analysis%2C%20and%20perplexity-based%20filtering%20using%20KenLM%20models.%20Our%20framework%20enables%20robust%20quality%20control%20across%20diverse%20scripts%20and%20linguistic%20contexts.%20Empirical%20results%20through%20model%20runs%20reveal%20key%20trade-offs%20in%20generation%20strategies%20and%20highlight%20best%20practices%20for%20constructing%20effective%20multilingual%20corpora.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBhashaKritika%253A%2520Building%2520Synthetic%2520Pretraining%2520Data%2520at%2520Scale%2520for%2520Indic%2520Languages%26entry.906535625%3DGuduru%2520Manoj%2520and%2520Neel%2520Prabhanjan%2520Rachamalla%2520and%2520Ashish%2520Kulkarni%2520and%2520Gautam%2520Rajeev%2520and%2520Jay%2520Piplodiya%2520and%2520Arul%2520Menezes%2520and%2520Shaharukh%2520Khan%2520and%2520Souvik%2520Rana%2520and%2520Manya%2520Sah%2520and%2520Chandra%2520Khatri%2520and%2520Shubham%2520Agarwal%26entry.1292438233%3DIn%2520the%2520context%2520of%2520pretraining%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520synthetic%2520data%2520has%2520emerged%2520as%2520an%2520alternative%2520for%2520generating%2520high-quality%2520pretraining%2520data%2520at%2520scale.%2520This%2520is%2520particularly%2520beneficial%2520in%2520low-resource%2520language%2520settings%2520where%2520the%2520benefits%2520of%2520recent%2520LLMs%2520have%2520been%2520unevenly%2520distributed%2520across%2520languages.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520systematic%2520study%2520on%2520the%2520generation%2520and%2520evaluation%2520of%2520synthetic%2520multilingual%2520pretraining%2520data%2520for%2520Indic%2520languages%252C%2520where%2520we%2520construct%2520a%2520large-scale%2520synthetic%2520dataset%2520BhashaKritika%252C%2520comprising%2520540B%2520tokens%2520using%25205%2520different%2520techniques%2520for%252010%2520languages.%2520We%2520explore%2520the%2520impact%2520of%2520grounding%2520generation%2520in%2520documents%252C%2520personas%252C%2520and%2520topics.%2520We%2520analyze%2520how%2520language%2520choice%252C%2520both%2520in%2520the%2520prompt%2520instructions%2520and%2520document%2520grounding%252C%2520affects%2520data%2520quality%252C%2520and%2520we%2520compare%2520translations%2520of%2520English%2520content%2520with%2520native%2520generation%2520in%2520Indic%2520languages.%2520To%2520support%2520scalable%2520and%2520language-sensitive%2520evaluation%252C%2520we%2520introduce%2520a%2520modular%2520quality%2520evaluation%2520pipeline%2520that%2520integrates%2520script%2520and%2520language%2520detection%252C%2520metadata%2520consistency%2520checks%252C%2520n-gram%2520repetition%2520analysis%252C%2520and%2520perplexity-based%2520filtering%2520using%2520KenLM%2520models.%2520Our%2520framework%2520enables%2520robust%2520quality%2520control%2520across%2520diverse%2520scripts%2520and%2520linguistic%2520contexts.%2520Empirical%2520results%2520through%2520model%2520runs%2520reveal%2520key%2520trade-offs%2520in%2520generation%2520strategies%2520and%2520highlight%2520best%2520practices%2520for%2520constructing%2520effective%2520multilingual%2520corpora.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BhashaKritika%3A%20Building%20Synthetic%20Pretraining%20Data%20at%20Scale%20for%20Indic%20Languages&entry.906535625=Guduru%20Manoj%20and%20Neel%20Prabhanjan%20Rachamalla%20and%20Ashish%20Kulkarni%20and%20Gautam%20Rajeev%20and%20Jay%20Piplodiya%20and%20Arul%20Menezes%20and%20Shaharukh%20Khan%20and%20Souvik%20Rana%20and%20Manya%20Sah%20and%20Chandra%20Khatri%20and%20Shubham%20Agarwal&entry.1292438233=In%20the%20context%20of%20pretraining%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20synthetic%20data%20has%20emerged%20as%20an%20alternative%20for%20generating%20high-quality%20pretraining%20data%20at%20scale.%20This%20is%20particularly%20beneficial%20in%20low-resource%20language%20settings%20where%20the%20benefits%20of%20recent%20LLMs%20have%20been%20unevenly%20distributed%20across%20languages.%20In%20this%20work%2C%20we%20present%20a%20systematic%20study%20on%20the%20generation%20and%20evaluation%20of%20synthetic%20multilingual%20pretraining%20data%20for%20Indic%20languages%2C%20where%20we%20construct%20a%20large-scale%20synthetic%20dataset%20BhashaKritika%2C%20comprising%20540B%20tokens%20using%205%20different%20techniques%20for%2010%20languages.%20We%20explore%20the%20impact%20of%20grounding%20generation%20in%20documents%2C%20personas%2C%20and%20topics.%20We%20analyze%20how%20language%20choice%2C%20both%20in%20the%20prompt%20instructions%20and%20document%20grounding%2C%20affects%20data%20quality%2C%20and%20we%20compare%20translations%20of%20English%20content%20with%20native%20generation%20in%20Indic%20languages.%20To%20support%20scalable%20and%20language-sensitive%20evaluation%2C%20we%20introduce%20a%20modular%20quality%20evaluation%20pipeline%20that%20integrates%20script%20and%20language%20detection%2C%20metadata%20consistency%20checks%2C%20n-gram%20repetition%20analysis%2C%20and%20perplexity-based%20filtering%20using%20KenLM%20models.%20Our%20framework%20enables%20robust%20quality%20control%20across%20diverse%20scripts%20and%20linguistic%20contexts.%20Empirical%20results%20through%20model%20runs%20reveal%20key%20trade-offs%20in%20generation%20strategies%20and%20highlight%20best%20practices%20for%20constructing%20effective%20multilingual%20corpora.&entry.1838667208=http%3A//arxiv.org/abs/2511.10338v1&entry.124074799=Read"},
{"title": "Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection", "author": "Patrick Feifel and Benedikt Franke and Frank Bonarens and Frank K\u00f6ster and Arne Raulf and Friedhelm Schwenker", "abstract": "Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.", "link": "http://arxiv.org/abs/2511.10308v1", "date": "2025-11-13", "relevancy": 1.9947, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5019}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Evaluation%20of%20Deep%20Neural%20Networks%20for%20Pedestrian%20Detection&body=Title%3A%20Revisiting%20Evaluation%20of%20Deep%20Neural%20Networks%20for%20Pedestrian%20Detection%0AAuthor%3A%20Patrick%20Feifel%20and%20Benedikt%20Franke%20and%20Frank%20Bonarens%20and%20Frank%20K%C3%B6ster%20and%20Arne%20Raulf%20and%20Friedhelm%20Schwenker%0AAbstract%3A%20Reliable%20pedestrian%20detection%20represents%20a%20crucial%20step%20towards%20automated%20driving%20systems.%20However%2C%20the%20current%20performance%20benchmarks%20exhibit%20weaknesses.%20The%20currently%20applied%20metrics%20for%20various%20subsets%20of%20a%20validation%20dataset%20prohibit%20a%20realistic%20performance%20evaluation%20of%20a%20DNN%20for%20pedestrian%20detection.%20As%20image%20segmentation%20supplies%20fine-grained%20information%20about%20a%20street%20scene%2C%20it%20can%20serve%20as%20a%20starting%20point%20to%20automatically%20distinguish%20between%20different%20types%20of%20errors%20during%20the%20evaluation%20of%20a%20pedestrian%20detector.%20In%20this%20work%2C%20eight%20different%20error%20categories%20for%20pedestrian%20detection%20are%20proposed%20and%20new%20metrics%20are%20proposed%20for%20performance%20comparison%20along%20these%20error%20categories.%20We%20use%20the%20new%20metrics%20to%20compare%20various%20backbones%20for%20a%20simplified%20version%20of%20the%20APD%2C%20and%20show%20a%20more%20fine-grained%20and%20robust%20way%20to%20compare%20models%20with%20each%20other%20especially%20in%20terms%20of%20safety-critical%20performance.%20We%20achieve%20SOTA%20on%20CityPersons-reasonable%20%28without%20extra%20training%20data%29%20by%20using%20a%20rather%20simple%20architecture.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Evaluation%2520of%2520Deep%2520Neural%2520Networks%2520for%2520Pedestrian%2520Detection%26entry.906535625%3DPatrick%2520Feifel%2520and%2520Benedikt%2520Franke%2520and%2520Frank%2520Bonarens%2520and%2520Frank%2520K%25C3%25B6ster%2520and%2520Arne%2520Raulf%2520and%2520Friedhelm%2520Schwenker%26entry.1292438233%3DReliable%2520pedestrian%2520detection%2520represents%2520a%2520crucial%2520step%2520towards%2520automated%2520driving%2520systems.%2520However%252C%2520the%2520current%2520performance%2520benchmarks%2520exhibit%2520weaknesses.%2520The%2520currently%2520applied%2520metrics%2520for%2520various%2520subsets%2520of%2520a%2520validation%2520dataset%2520prohibit%2520a%2520realistic%2520performance%2520evaluation%2520of%2520a%2520DNN%2520for%2520pedestrian%2520detection.%2520As%2520image%2520segmentation%2520supplies%2520fine-grained%2520information%2520about%2520a%2520street%2520scene%252C%2520it%2520can%2520serve%2520as%2520a%2520starting%2520point%2520to%2520automatically%2520distinguish%2520between%2520different%2520types%2520of%2520errors%2520during%2520the%2520evaluation%2520of%2520a%2520pedestrian%2520detector.%2520In%2520this%2520work%252C%2520eight%2520different%2520error%2520categories%2520for%2520pedestrian%2520detection%2520are%2520proposed%2520and%2520new%2520metrics%2520are%2520proposed%2520for%2520performance%2520comparison%2520along%2520these%2520error%2520categories.%2520We%2520use%2520the%2520new%2520metrics%2520to%2520compare%2520various%2520backbones%2520for%2520a%2520simplified%2520version%2520of%2520the%2520APD%252C%2520and%2520show%2520a%2520more%2520fine-grained%2520and%2520robust%2520way%2520to%2520compare%2520models%2520with%2520each%2520other%2520especially%2520in%2520terms%2520of%2520safety-critical%2520performance.%2520We%2520achieve%2520SOTA%2520on%2520CityPersons-reasonable%2520%2528without%2520extra%2520training%2520data%2529%2520by%2520using%2520a%2520rather%2520simple%2520architecture.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Evaluation%20of%20Deep%20Neural%20Networks%20for%20Pedestrian%20Detection&entry.906535625=Patrick%20Feifel%20and%20Benedikt%20Franke%20and%20Frank%20Bonarens%20and%20Frank%20K%C3%B6ster%20and%20Arne%20Raulf%20and%20Friedhelm%20Schwenker&entry.1292438233=Reliable%20pedestrian%20detection%20represents%20a%20crucial%20step%20towards%20automated%20driving%20systems.%20However%2C%20the%20current%20performance%20benchmarks%20exhibit%20weaknesses.%20The%20currently%20applied%20metrics%20for%20various%20subsets%20of%20a%20validation%20dataset%20prohibit%20a%20realistic%20performance%20evaluation%20of%20a%20DNN%20for%20pedestrian%20detection.%20As%20image%20segmentation%20supplies%20fine-grained%20information%20about%20a%20street%20scene%2C%20it%20can%20serve%20as%20a%20starting%20point%20to%20automatically%20distinguish%20between%20different%20types%20of%20errors%20during%20the%20evaluation%20of%20a%20pedestrian%20detector.%20In%20this%20work%2C%20eight%20different%20error%20categories%20for%20pedestrian%20detection%20are%20proposed%20and%20new%20metrics%20are%20proposed%20for%20performance%20comparison%20along%20these%20error%20categories.%20We%20use%20the%20new%20metrics%20to%20compare%20various%20backbones%20for%20a%20simplified%20version%20of%20the%20APD%2C%20and%20show%20a%20more%20fine-grained%20and%20robust%20way%20to%20compare%20models%20with%20each%20other%20especially%20in%20terms%20of%20safety-critical%20performance.%20We%20achieve%20SOTA%20on%20CityPersons-reasonable%20%28without%20extra%20training%20data%29%20by%20using%20a%20rather%20simple%20architecture.&entry.1838667208=http%3A//arxiv.org/abs/2511.10308v1&entry.124074799=Read"},
{"title": "ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation", "author": "Tobias Christian Nauen and Brian Moser and Federico Raue and Stanislav Frolov and Andreas Dengel", "abstract": "Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at https://github.com/tobna/ForAug.", "link": "http://arxiv.org/abs/2503.09399v2", "date": "2025-11-13", "relevancy": 1.6048, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5675}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5293}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForAug%3A%20Recombining%20Foregrounds%20and%20Backgrounds%20to%20Improve%20Vision%20Transformer%20Training%20with%20Bias%20Mitigation&body=Title%3A%20ForAug%3A%20Recombining%20Foregrounds%20and%20Backgrounds%20to%20Improve%20Vision%20Transformer%20Training%20with%20Bias%20Mitigation%0AAuthor%3A%20Tobias%20Christian%20Nauen%20and%20Brian%20Moser%20and%20Federico%20Raue%20and%20Stanislav%20Frolov%20and%20Andreas%20Dengel%0AAbstract%3A%20Transformers%2C%20particularly%20Vision%20Transformers%20%28ViTs%29%2C%20have%20achieved%20state-of-the-art%20performance%20in%20large-scale%20image%20classification.%20However%2C%20they%20often%20require%20large%20amounts%20of%20data%20and%20can%20exhibit%20biases%20that%20limit%20their%20robustness%20and%20generalizability.%20This%20paper%20introduces%20ForAug%2C%20a%20novel%20data%20augmentation%20scheme%20that%20addresses%20these%20challenges%20and%20explicitly%20includes%20inductive%20biases%2C%20which%20commonly%20are%20part%20of%20the%20neural%20network%20architecture%2C%20into%20the%20training%20data.%20ForAug%20is%20constructed%20by%20using%20pretrained%20foundation%20models%20to%20separate%20and%20recombine%20foreground%20objects%20with%20different%20backgrounds%2C%20enabling%20fine-grained%20control%20over%20image%20composition%20during%20training.%20It%20thus%20increases%20the%20data%20diversity%20and%20effective%20number%20of%20training%20samples.%20We%20demonstrate%20that%20training%20on%20ForNet%2C%20the%20application%20of%20ForAug%20to%20ImageNet%2C%20significantly%20improves%20the%20accuracy%20of%20ViTs%20and%20other%20architectures%20by%20up%20to%204.5%20percentage%20points%20%28p.p.%29%20on%20ImageNet%20and%207.3%20p.p.%20on%20downstream%20tasks.%20Importantly%2C%20ForAug%20enables%20novel%20ways%20of%20analyzing%20model%20behavior%20and%20quantifying%20biases.%20Namely%2C%20we%20introduce%20metrics%20for%20background%20robustness%2C%20foreground%20focus%2C%20center%20bias%2C%20and%20size%20bias%20and%20show%20that%20training%20on%20ForNet%20substantially%20reduces%20these%20biases%20compared%20to%20training%20on%20ImageNet.%20In%20summary%2C%20ForAug%20provides%20a%20valuable%20tool%20for%20analyzing%20and%20mitigating%20biases%2C%20enabling%20the%20development%20of%20more%20robust%20and%20reliable%20computer%20vision%20models.%20Our%20code%20and%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/tobna/ForAug.%0ALink%3A%20http%3A//arxiv.org/abs/2503.09399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForAug%253A%2520Recombining%2520Foregrounds%2520and%2520Backgrounds%2520to%2520Improve%2520Vision%2520Transformer%2520Training%2520with%2520Bias%2520Mitigation%26entry.906535625%3DTobias%2520Christian%2520Nauen%2520and%2520Brian%2520Moser%2520and%2520Federico%2520Raue%2520and%2520Stanislav%2520Frolov%2520and%2520Andreas%2520Dengel%26entry.1292438233%3DTransformers%252C%2520particularly%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520have%2520achieved%2520state-of-the-art%2520performance%2520in%2520large-scale%2520image%2520classification.%2520However%252C%2520they%2520often%2520require%2520large%2520amounts%2520of%2520data%2520and%2520can%2520exhibit%2520biases%2520that%2520limit%2520their%2520robustness%2520and%2520generalizability.%2520This%2520paper%2520introduces%2520ForAug%252C%2520a%2520novel%2520data%2520augmentation%2520scheme%2520that%2520addresses%2520these%2520challenges%2520and%2520explicitly%2520includes%2520inductive%2520biases%252C%2520which%2520commonly%2520are%2520part%2520of%2520the%2520neural%2520network%2520architecture%252C%2520into%2520the%2520training%2520data.%2520ForAug%2520is%2520constructed%2520by%2520using%2520pretrained%2520foundation%2520models%2520to%2520separate%2520and%2520recombine%2520foreground%2520objects%2520with%2520different%2520backgrounds%252C%2520enabling%2520fine-grained%2520control%2520over%2520image%2520composition%2520during%2520training.%2520It%2520thus%2520increases%2520the%2520data%2520diversity%2520and%2520effective%2520number%2520of%2520training%2520samples.%2520We%2520demonstrate%2520that%2520training%2520on%2520ForNet%252C%2520the%2520application%2520of%2520ForAug%2520to%2520ImageNet%252C%2520significantly%2520improves%2520the%2520accuracy%2520of%2520ViTs%2520and%2520other%2520architectures%2520by%2520up%2520to%25204.5%2520percentage%2520points%2520%2528p.p.%2529%2520on%2520ImageNet%2520and%25207.3%2520p.p.%2520on%2520downstream%2520tasks.%2520Importantly%252C%2520ForAug%2520enables%2520novel%2520ways%2520of%2520analyzing%2520model%2520behavior%2520and%2520quantifying%2520biases.%2520Namely%252C%2520we%2520introduce%2520metrics%2520for%2520background%2520robustness%252C%2520foreground%2520focus%252C%2520center%2520bias%252C%2520and%2520size%2520bias%2520and%2520show%2520that%2520training%2520on%2520ForNet%2520substantially%2520reduces%2520these%2520biases%2520compared%2520to%2520training%2520on%2520ImageNet.%2520In%2520summary%252C%2520ForAug%2520provides%2520a%2520valuable%2520tool%2520for%2520analyzing%2520and%2520mitigating%2520biases%252C%2520enabling%2520the%2520development%2520of%2520more%2520robust%2520and%2520reliable%2520computer%2520vision%2520models.%2520Our%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/tobna/ForAug.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForAug%3A%20Recombining%20Foregrounds%20and%20Backgrounds%20to%20Improve%20Vision%20Transformer%20Training%20with%20Bias%20Mitigation&entry.906535625=Tobias%20Christian%20Nauen%20and%20Brian%20Moser%20and%20Federico%20Raue%20and%20Stanislav%20Frolov%20and%20Andreas%20Dengel&entry.1292438233=Transformers%2C%20particularly%20Vision%20Transformers%20%28ViTs%29%2C%20have%20achieved%20state-of-the-art%20performance%20in%20large-scale%20image%20classification.%20However%2C%20they%20often%20require%20large%20amounts%20of%20data%20and%20can%20exhibit%20biases%20that%20limit%20their%20robustness%20and%20generalizability.%20This%20paper%20introduces%20ForAug%2C%20a%20novel%20data%20augmentation%20scheme%20that%20addresses%20these%20challenges%20and%20explicitly%20includes%20inductive%20biases%2C%20which%20commonly%20are%20part%20of%20the%20neural%20network%20architecture%2C%20into%20the%20training%20data.%20ForAug%20is%20constructed%20by%20using%20pretrained%20foundation%20models%20to%20separate%20and%20recombine%20foreground%20objects%20with%20different%20backgrounds%2C%20enabling%20fine-grained%20control%20over%20image%20composition%20during%20training.%20It%20thus%20increases%20the%20data%20diversity%20and%20effective%20number%20of%20training%20samples.%20We%20demonstrate%20that%20training%20on%20ForNet%2C%20the%20application%20of%20ForAug%20to%20ImageNet%2C%20significantly%20improves%20the%20accuracy%20of%20ViTs%20and%20other%20architectures%20by%20up%20to%204.5%20percentage%20points%20%28p.p.%29%20on%20ImageNet%20and%207.3%20p.p.%20on%20downstream%20tasks.%20Importantly%2C%20ForAug%20enables%20novel%20ways%20of%20analyzing%20model%20behavior%20and%20quantifying%20biases.%20Namely%2C%20we%20introduce%20metrics%20for%20background%20robustness%2C%20foreground%20focus%2C%20center%20bias%2C%20and%20size%20bias%20and%20show%20that%20training%20on%20ForNet%20substantially%20reduces%20these%20biases%20compared%20to%20training%20on%20ImageNet.%20In%20summary%2C%20ForAug%20provides%20a%20valuable%20tool%20for%20analyzing%20and%20mitigating%20biases%2C%20enabling%20the%20development%20of%20more%20robust%20and%20reliable%20computer%20vision%20models.%20Our%20code%20and%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/tobna/ForAug.&entry.1838667208=http%3A//arxiv.org/abs/2503.09399v2&entry.124074799=Read"},
{"title": "Transfer in Reinforcement Learning via Regret Bounds for Learning Agents", "author": "Adrienne Tuynman and Ronald Ortner", "abstract": "We present an approach for the quantification of the usefulness of transfer in reinforcement learning via regret bounds for a multi-agent setting. Considering a number of $\\aleph$ agents operating in the same Markov decision process, however possibly with different reward functions, we consider the regret each agent suffers with respect to an optimal policy maximizing her average reward. We show that when the agents share their observations the total regret of all agents is smaller by a factor of $\\sqrt{\\aleph}$ compared to the case when each agent has to rely on the information collected by herself. This result demonstrates how considering the regret in multi-agent settings can provide theoretical bounds on the benefit of sharing observations in transfer learning.", "link": "http://arxiv.org/abs/2202.01182v2", "date": "2025-11-13", "relevancy": 1.8939, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5143}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4759}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20in%20Reinforcement%20Learning%20via%20Regret%20Bounds%20for%20Learning%20Agents&body=Title%3A%20Transfer%20in%20Reinforcement%20Learning%20via%20Regret%20Bounds%20for%20Learning%20Agents%0AAuthor%3A%20Adrienne%20Tuynman%20and%20Ronald%20Ortner%0AAbstract%3A%20We%20present%20an%20approach%20for%20the%20quantification%20of%20the%20usefulness%20of%20transfer%20in%20reinforcement%20learning%20via%20regret%20bounds%20for%20a%20multi-agent%20setting.%20Considering%20a%20number%20of%20%24%5Caleph%24%20agents%20operating%20in%20the%20same%20Markov%20decision%20process%2C%20however%20possibly%20with%20different%20reward%20functions%2C%20we%20consider%20the%20regret%20each%20agent%20suffers%20with%20respect%20to%20an%20optimal%20policy%20maximizing%20her%20average%20reward.%20We%20show%20that%20when%20the%20agents%20share%20their%20observations%20the%20total%20regret%20of%20all%20agents%20is%20smaller%20by%20a%20factor%20of%20%24%5Csqrt%7B%5Caleph%7D%24%20compared%20to%20the%20case%20when%20each%20agent%20has%20to%20rely%20on%20the%20information%20collected%20by%20herself.%20This%20result%20demonstrates%20how%20considering%20the%20regret%20in%20multi-agent%20settings%20can%20provide%20theoretical%20bounds%20on%20the%20benefit%20of%20sharing%20observations%20in%20transfer%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2202.01182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520in%2520Reinforcement%2520Learning%2520via%2520Regret%2520Bounds%2520for%2520Learning%2520Agents%26entry.906535625%3DAdrienne%2520Tuynman%2520and%2520Ronald%2520Ortner%26entry.1292438233%3DWe%2520present%2520an%2520approach%2520for%2520the%2520quantification%2520of%2520the%2520usefulness%2520of%2520transfer%2520in%2520reinforcement%2520learning%2520via%2520regret%2520bounds%2520for%2520a%2520multi-agent%2520setting.%2520Considering%2520a%2520number%2520of%2520%2524%255Caleph%2524%2520agents%2520operating%2520in%2520the%2520same%2520Markov%2520decision%2520process%252C%2520however%2520possibly%2520with%2520different%2520reward%2520functions%252C%2520we%2520consider%2520the%2520regret%2520each%2520agent%2520suffers%2520with%2520respect%2520to%2520an%2520optimal%2520policy%2520maximizing%2520her%2520average%2520reward.%2520We%2520show%2520that%2520when%2520the%2520agents%2520share%2520their%2520observations%2520the%2520total%2520regret%2520of%2520all%2520agents%2520is%2520smaller%2520by%2520a%2520factor%2520of%2520%2524%255Csqrt%257B%255Caleph%257D%2524%2520compared%2520to%2520the%2520case%2520when%2520each%2520agent%2520has%2520to%2520rely%2520on%2520the%2520information%2520collected%2520by%2520herself.%2520This%2520result%2520demonstrates%2520how%2520considering%2520the%2520regret%2520in%2520multi-agent%2520settings%2520can%2520provide%2520theoretical%2520bounds%2520on%2520the%2520benefit%2520of%2520sharing%2520observations%2520in%2520transfer%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.01182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20in%20Reinforcement%20Learning%20via%20Regret%20Bounds%20for%20Learning%20Agents&entry.906535625=Adrienne%20Tuynman%20and%20Ronald%20Ortner&entry.1292438233=We%20present%20an%20approach%20for%20the%20quantification%20of%20the%20usefulness%20of%20transfer%20in%20reinforcement%20learning%20via%20regret%20bounds%20for%20a%20multi-agent%20setting.%20Considering%20a%20number%20of%20%24%5Caleph%24%20agents%20operating%20in%20the%20same%20Markov%20decision%20process%2C%20however%20possibly%20with%20different%20reward%20functions%2C%20we%20consider%20the%20regret%20each%20agent%20suffers%20with%20respect%20to%20an%20optimal%20policy%20maximizing%20her%20average%20reward.%20We%20show%20that%20when%20the%20agents%20share%20their%20observations%20the%20total%20regret%20of%20all%20agents%20is%20smaller%20by%20a%20factor%20of%20%24%5Csqrt%7B%5Caleph%7D%24%20compared%20to%20the%20case%20when%20each%20agent%20has%20to%20rely%20on%20the%20information%20collected%20by%20herself.%20This%20result%20demonstrates%20how%20considering%20the%20regret%20in%20multi-agent%20settings%20can%20provide%20theoretical%20bounds%20on%20the%20benefit%20of%20sharing%20observations%20in%20transfer%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2202.01182v2&entry.124074799=Read"},
{"title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data", "author": "Simon Donike and Cesar Aybar and Julio Contreras and Luis G\u00f3mez-Chova", "abstract": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.", "link": "http://arxiv.org/abs/2511.10461v1", "date": "2025-11-13", "relevancy": 2.0613, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5568}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4918}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenSR-SRGAN%3A%20A%20Flexible%20Super-Resolution%20Framework%20for%20Multispectral%20Earth%20Observation%20Data&body=Title%3A%20OpenSR-SRGAN%3A%20A%20Flexible%20Super-Resolution%20Framework%20for%20Multispectral%20Earth%20Observation%20Data%0AAuthor%3A%20Simon%20Donike%20and%20Cesar%20Aybar%20and%20Julio%20Contreras%20and%20Luis%20G%C3%B3mez-Chova%0AAbstract%3A%20We%20present%20OpenSR-SRGAN%2C%20an%20open%20and%20modular%20framework%20for%20single-image%20super-resolution%20in%20Earth%20Observation.%20The%20software%20provides%20a%20unified%20implementation%20of%20SRGAN-style%20models%20that%20is%20easy%20to%20configure%2C%20extend%2C%20and%20apply%20to%20multispectral%20satellite%20data%20such%20as%20Sentinel-2.%20Instead%20of%20requiring%20users%20to%20modify%20model%20code%2C%20OpenSR-SRGAN%20exposes%20generators%2C%20discriminators%2C%20loss%20functions%2C%20and%20training%20schedules%20through%20concise%20configuration%20files%2C%20making%20it%20straightforward%20to%20switch%20between%20architectures%2C%20scale%20factors%2C%20and%20band%20setups.%20The%20framework%20is%20designed%20as%20a%20practical%20tool%20and%20benchmark%20implementation%20rather%20than%20a%20state-of-the-art%20model.%20It%20ships%20with%20ready-to-use%20configurations%20for%20common%20remote%20sensing%20scenarios%2C%20sensible%20default%20settings%20for%20adversarial%20training%2C%20and%20built-in%20hooks%20for%20logging%2C%20validation%2C%20and%20large-scene%20inference.%20By%20turning%20GAN-based%20super-resolution%20into%20a%20configuration-driven%20workflow%2C%20OpenSR-SRGAN%20lowers%20the%20entry%20barrier%20for%20researchers%20and%20practitioners%20who%20wish%20to%20experiment%20with%20SRGANs%2C%20compare%20models%20in%20a%20reproducible%20way%2C%20and%20deploy%20super-resolution%20pipelines%20across%20diverse%20Earth-observation%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenSR-SRGAN%253A%2520A%2520Flexible%2520Super-Resolution%2520Framework%2520for%2520Multispectral%2520Earth%2520Observation%2520Data%26entry.906535625%3DSimon%2520Donike%2520and%2520Cesar%2520Aybar%2520and%2520Julio%2520Contreras%2520and%2520Luis%2520G%25C3%25B3mez-Chova%26entry.1292438233%3DWe%2520present%2520OpenSR-SRGAN%252C%2520an%2520open%2520and%2520modular%2520framework%2520for%2520single-image%2520super-resolution%2520in%2520Earth%2520Observation.%2520The%2520software%2520provides%2520a%2520unified%2520implementation%2520of%2520SRGAN-style%2520models%2520that%2520is%2520easy%2520to%2520configure%252C%2520extend%252C%2520and%2520apply%2520to%2520multispectral%2520satellite%2520data%2520such%2520as%2520Sentinel-2.%2520Instead%2520of%2520requiring%2520users%2520to%2520modify%2520model%2520code%252C%2520OpenSR-SRGAN%2520exposes%2520generators%252C%2520discriminators%252C%2520loss%2520functions%252C%2520and%2520training%2520schedules%2520through%2520concise%2520configuration%2520files%252C%2520making%2520it%2520straightforward%2520to%2520switch%2520between%2520architectures%252C%2520scale%2520factors%252C%2520and%2520band%2520setups.%2520The%2520framework%2520is%2520designed%2520as%2520a%2520practical%2520tool%2520and%2520benchmark%2520implementation%2520rather%2520than%2520a%2520state-of-the-art%2520model.%2520It%2520ships%2520with%2520ready-to-use%2520configurations%2520for%2520common%2520remote%2520sensing%2520scenarios%252C%2520sensible%2520default%2520settings%2520for%2520adversarial%2520training%252C%2520and%2520built-in%2520hooks%2520for%2520logging%252C%2520validation%252C%2520and%2520large-scene%2520inference.%2520By%2520turning%2520GAN-based%2520super-resolution%2520into%2520a%2520configuration-driven%2520workflow%252C%2520OpenSR-SRGAN%2520lowers%2520the%2520entry%2520barrier%2520for%2520researchers%2520and%2520practitioners%2520who%2520wish%2520to%2520experiment%2520with%2520SRGANs%252C%2520compare%2520models%2520in%2520a%2520reproducible%2520way%252C%2520and%2520deploy%2520super-resolution%2520pipelines%2520across%2520diverse%2520Earth-observation%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenSR-SRGAN%3A%20A%20Flexible%20Super-Resolution%20Framework%20for%20Multispectral%20Earth%20Observation%20Data&entry.906535625=Simon%20Donike%20and%20Cesar%20Aybar%20and%20Julio%20Contreras%20and%20Luis%20G%C3%B3mez-Chova&entry.1292438233=We%20present%20OpenSR-SRGAN%2C%20an%20open%20and%20modular%20framework%20for%20single-image%20super-resolution%20in%20Earth%20Observation.%20The%20software%20provides%20a%20unified%20implementation%20of%20SRGAN-style%20models%20that%20is%20easy%20to%20configure%2C%20extend%2C%20and%20apply%20to%20multispectral%20satellite%20data%20such%20as%20Sentinel-2.%20Instead%20of%20requiring%20users%20to%20modify%20model%20code%2C%20OpenSR-SRGAN%20exposes%20generators%2C%20discriminators%2C%20loss%20functions%2C%20and%20training%20schedules%20through%20concise%20configuration%20files%2C%20making%20it%20straightforward%20to%20switch%20between%20architectures%2C%20scale%20factors%2C%20and%20band%20setups.%20The%20framework%20is%20designed%20as%20a%20practical%20tool%20and%20benchmark%20implementation%20rather%20than%20a%20state-of-the-art%20model.%20It%20ships%20with%20ready-to-use%20configurations%20for%20common%20remote%20sensing%20scenarios%2C%20sensible%20default%20settings%20for%20adversarial%20training%2C%20and%20built-in%20hooks%20for%20logging%2C%20validation%2C%20and%20large-scene%20inference.%20By%20turning%20GAN-based%20super-resolution%20into%20a%20configuration-driven%20workflow%2C%20OpenSR-SRGAN%20lowers%20the%20entry%20barrier%20for%20researchers%20and%20practitioners%20who%20wish%20to%20experiment%20with%20SRGANs%2C%20compare%20models%20in%20a%20reproducible%20way%2C%20and%20deploy%20super-resolution%20pipelines%20across%20diverse%20Earth-observation%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.10461v1&entry.124074799=Read"},
{"title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware", "author": "Martin Braas and Lukas Esterle", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.", "link": "http://arxiv.org/abs/2511.10277v1", "date": "2025-11-13", "relevancy": 1.816, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fixed-Persona%20SLMs%20with%20Modular%20Memory%3A%20Scalable%20NPC%20Dialogue%20on%20Consumer%20Hardware&body=Title%3A%20Fixed-Persona%20SLMs%20with%20Modular%20Memory%3A%20Scalable%20NPC%20Dialogue%20on%20Consumer%20Hardware%0AAuthor%3A%20Martin%20Braas%20and%20Lukas%20Esterle%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20generating%20human-like%20text%2C%20yet%20their%20applicability%20to%20dialogue%20systems%20in%20computer%20games%20remains%20limited.%20This%20limitation%20arises%20from%20their%20substantial%20hardware%20requirements%2C%20latency%20constraints%2C%20and%20the%20necessity%20to%20maintain%20clearly%20defined%20knowledge%20boundaries%20within%20a%20game%20setting.%20In%20this%20paper%2C%20we%20propose%20a%20modular%20NPC%20dialogue%20system%20that%20leverages%20Small%20Language%20Models%20%28SLMs%29%2C%20fine-tuned%20to%20encode%20specific%20NPC%20personas%20and%20integrated%20with%20runtime-swappable%20memory%20modules.%20These%20memory%20modules%20preserve%20character-specific%20conversational%20context%20and%20world%20knowledge%2C%20enabling%20expressive%20interactions%20and%20long-term%20memory%20without%20retraining%20or%20model%20reloading%20during%20gameplay.%20We%20comprehensively%20evaluate%20our%20system%20using%20three%20open-source%20SLMs%3A%20DistilGPT-2%2C%20TinyLlama-1.1B-Chat%2C%20and%20Mistral-7B-Instruct%2C%20trained%20on%20synthetic%20persona-aligned%20data%20and%20benchmarked%20on%20consumer-grade%20hardware.%20While%20our%20approach%20is%20motivated%20by%20applications%20in%20gaming%2C%20its%20modular%20design%20and%20persona-driven%20memory%20architecture%20hold%20significant%20potential%20for%20broader%20adoption%20in%20domains%20requiring%20expressive%2C%20scalable%2C%20and%20memory-rich%20conversational%20agents%2C%20such%20as%20virtual%20assistants%2C%20customer%20support%20bots%2C%20or%20interactive%20educational%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFixed-Persona%2520SLMs%2520with%2520Modular%2520Memory%253A%2520Scalable%2520NPC%2520Dialogue%2520on%2520Consumer%2520Hardware%26entry.906535625%3DMartin%2520Braas%2520and%2520Lukas%2520Esterle%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520generating%2520human-like%2520text%252C%2520yet%2520their%2520applicability%2520to%2520dialogue%2520systems%2520in%2520computer%2520games%2520remains%2520limited.%2520This%2520limitation%2520arises%2520from%2520their%2520substantial%2520hardware%2520requirements%252C%2520latency%2520constraints%252C%2520and%2520the%2520necessity%2520to%2520maintain%2520clearly%2520defined%2520knowledge%2520boundaries%2520within%2520a%2520game%2520setting.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520modular%2520NPC%2520dialogue%2520system%2520that%2520leverages%2520Small%2520Language%2520Models%2520%2528SLMs%2529%252C%2520fine-tuned%2520to%2520encode%2520specific%2520NPC%2520personas%2520and%2520integrated%2520with%2520runtime-swappable%2520memory%2520modules.%2520These%2520memory%2520modules%2520preserve%2520character-specific%2520conversational%2520context%2520and%2520world%2520knowledge%252C%2520enabling%2520expressive%2520interactions%2520and%2520long-term%2520memory%2520without%2520retraining%2520or%2520model%2520reloading%2520during%2520gameplay.%2520We%2520comprehensively%2520evaluate%2520our%2520system%2520using%2520three%2520open-source%2520SLMs%253A%2520DistilGPT-2%252C%2520TinyLlama-1.1B-Chat%252C%2520and%2520Mistral-7B-Instruct%252C%2520trained%2520on%2520synthetic%2520persona-aligned%2520data%2520and%2520benchmarked%2520on%2520consumer-grade%2520hardware.%2520While%2520our%2520approach%2520is%2520motivated%2520by%2520applications%2520in%2520gaming%252C%2520its%2520modular%2520design%2520and%2520persona-driven%2520memory%2520architecture%2520hold%2520significant%2520potential%2520for%2520broader%2520adoption%2520in%2520domains%2520requiring%2520expressive%252C%2520scalable%252C%2520and%2520memory-rich%2520conversational%2520agents%252C%2520such%2520as%2520virtual%2520assistants%252C%2520customer%2520support%2520bots%252C%2520or%2520interactive%2520educational%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fixed-Persona%20SLMs%20with%20Modular%20Memory%3A%20Scalable%20NPC%20Dialogue%20on%20Consumer%20Hardware&entry.906535625=Martin%20Braas%20and%20Lukas%20Esterle&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20generating%20human-like%20text%2C%20yet%20their%20applicability%20to%20dialogue%20systems%20in%20computer%20games%20remains%20limited.%20This%20limitation%20arises%20from%20their%20substantial%20hardware%20requirements%2C%20latency%20constraints%2C%20and%20the%20necessity%20to%20maintain%20clearly%20defined%20knowledge%20boundaries%20within%20a%20game%20setting.%20In%20this%20paper%2C%20we%20propose%20a%20modular%20NPC%20dialogue%20system%20that%20leverages%20Small%20Language%20Models%20%28SLMs%29%2C%20fine-tuned%20to%20encode%20specific%20NPC%20personas%20and%20integrated%20with%20runtime-swappable%20memory%20modules.%20These%20memory%20modules%20preserve%20character-specific%20conversational%20context%20and%20world%20knowledge%2C%20enabling%20expressive%20interactions%20and%20long-term%20memory%20without%20retraining%20or%20model%20reloading%20during%20gameplay.%20We%20comprehensively%20evaluate%20our%20system%20using%20three%20open-source%20SLMs%3A%20DistilGPT-2%2C%20TinyLlama-1.1B-Chat%2C%20and%20Mistral-7B-Instruct%2C%20trained%20on%20synthetic%20persona-aligned%20data%20and%20benchmarked%20on%20consumer-grade%20hardware.%20While%20our%20approach%20is%20motivated%20by%20applications%20in%20gaming%2C%20its%20modular%20design%20and%20persona-driven%20memory%20architecture%20hold%20significant%20potential%20for%20broader%20adoption%20in%20domains%20requiring%20expressive%2C%20scalable%2C%20and%20memory-rich%20conversational%20agents%2C%20such%20as%20virtual%20assistants%2C%20customer%20support%20bots%2C%20or%20interactive%20educational%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.10277v1&entry.124074799=Read"},
{"title": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam", "author": "Gerd Kortemeyer and Alexander Caspar and Daria Horica", "abstract": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.", "link": "http://arxiv.org/abs/2510.05162v2", "date": "2025-11-13", "relevancy": 1.3628, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial-Intelligence%20Grading%20Assistance%20for%20Handwritten%20Components%20of%20a%20Calculus%20Exam&body=Title%3A%20Artificial-Intelligence%20Grading%20Assistance%20for%20Handwritten%20Components%20of%20a%20Calculus%20Exam%0AAuthor%3A%20Gerd%20Kortemeyer%20and%20Alexander%20Caspar%20and%20Daria%20Horica%0AAbstract%3A%20We%20investigate%20whether%20contemporary%20multimodal%20LLMs%20can%20assist%20with%20grading%20open-ended%20calculus%20at%20scale%20without%20eroding%20validity.%20In%20a%20large%20first-year%20exam%2C%20students%27%20handwritten%20work%20was%20graded%20by%20GPT-5%20against%20the%20same%20rubric%20used%20by%20teaching%20assistants%20%28TAs%29%2C%20with%20fractional%20credit%20permitted%3B%20TA%20rubric%20decisions%20served%20as%20ground%20truth.%20We%20calibrated%20a%20human-in-the-loop%20filter%20that%20combines%20a%20partial-credit%20threshold%20with%20an%20Item%20Response%20Theory%20%282PL%29%20risk%20measure%20based%20on%20the%20deviation%20between%20the%20AI%20score%20and%20the%20model-expected%20score%20for%20each%20student-item.%20Unfiltered%20AI-TA%20agreement%20was%20moderate%2C%20adequate%20for%20low-stakes%20feedback%20but%20not%20for%20high-stakes%20use.%20Confidence%20filtering%20made%20the%20workload-quality%20trade-off%20explicit%3A%20under%20stricter%20settings%2C%20AI%20delivered%20human-level%20accuracy%2C%20but%20also%20left%20roughly%2070%25%20of%20the%20items%20to%20be%20graded%20by%20humans.%20Psychometric%20patterns%20were%20constrained%20by%20low%20stakes%20on%20the%20open-ended%20portion%2C%20a%20small%20set%20of%20rubric%20checkpoints%2C%20and%20occasional%20misalignment%20between%20designated%20answer%20regions%20and%20where%20work%20appeared.%20Practical%20adjustments%20such%20as%20slightly%20higher%20weight%20and%20protected%20time%2C%20a%20few%20rubric-visible%20substeps%2C%20stronger%20spatial%20anchoring%20should%20raise%20ceiling%20performance.%20Overall%2C%20calibrated%20confidence%20and%20conservative%20routing%20enable%20AI%20to%20reliably%20handle%20a%20sizable%20subset%20of%20routine%20cases%20while%20reserving%20expert%20judgment%20for%20ambiguous%20or%20pedagogically%20rich%20responses.%0ALink%3A%20http%3A//arxiv.org/abs/2510.05162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial-Intelligence%2520Grading%2520Assistance%2520for%2520Handwritten%2520Components%2520of%2520a%2520Calculus%2520Exam%26entry.906535625%3DGerd%2520Kortemeyer%2520and%2520Alexander%2520Caspar%2520and%2520Daria%2520Horica%26entry.1292438233%3DWe%2520investigate%2520whether%2520contemporary%2520multimodal%2520LLMs%2520can%2520assist%2520with%2520grading%2520open-ended%2520calculus%2520at%2520scale%2520without%2520eroding%2520validity.%2520In%2520a%2520large%2520first-year%2520exam%252C%2520students%2527%2520handwritten%2520work%2520was%2520graded%2520by%2520GPT-5%2520against%2520the%2520same%2520rubric%2520used%2520by%2520teaching%2520assistants%2520%2528TAs%2529%252C%2520with%2520fractional%2520credit%2520permitted%253B%2520TA%2520rubric%2520decisions%2520served%2520as%2520ground%2520truth.%2520We%2520calibrated%2520a%2520human-in-the-loop%2520filter%2520that%2520combines%2520a%2520partial-credit%2520threshold%2520with%2520an%2520Item%2520Response%2520Theory%2520%25282PL%2529%2520risk%2520measure%2520based%2520on%2520the%2520deviation%2520between%2520the%2520AI%2520score%2520and%2520the%2520model-expected%2520score%2520for%2520each%2520student-item.%2520Unfiltered%2520AI-TA%2520agreement%2520was%2520moderate%252C%2520adequate%2520for%2520low-stakes%2520feedback%2520but%2520not%2520for%2520high-stakes%2520use.%2520Confidence%2520filtering%2520made%2520the%2520workload-quality%2520trade-off%2520explicit%253A%2520under%2520stricter%2520settings%252C%2520AI%2520delivered%2520human-level%2520accuracy%252C%2520but%2520also%2520left%2520roughly%252070%2525%2520of%2520the%2520items%2520to%2520be%2520graded%2520by%2520humans.%2520Psychometric%2520patterns%2520were%2520constrained%2520by%2520low%2520stakes%2520on%2520the%2520open-ended%2520portion%252C%2520a%2520small%2520set%2520of%2520rubric%2520checkpoints%252C%2520and%2520occasional%2520misalignment%2520between%2520designated%2520answer%2520regions%2520and%2520where%2520work%2520appeared.%2520Practical%2520adjustments%2520such%2520as%2520slightly%2520higher%2520weight%2520and%2520protected%2520time%252C%2520a%2520few%2520rubric-visible%2520substeps%252C%2520stronger%2520spatial%2520anchoring%2520should%2520raise%2520ceiling%2520performance.%2520Overall%252C%2520calibrated%2520confidence%2520and%2520conservative%2520routing%2520enable%2520AI%2520to%2520reliably%2520handle%2520a%2520sizable%2520subset%2520of%2520routine%2520cases%2520while%2520reserving%2520expert%2520judgment%2520for%2520ambiguous%2520or%2520pedagogically%2520rich%2520responses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial-Intelligence%20Grading%20Assistance%20for%20Handwritten%20Components%20of%20a%20Calculus%20Exam&entry.906535625=Gerd%20Kortemeyer%20and%20Alexander%20Caspar%20and%20Daria%20Horica&entry.1292438233=We%20investigate%20whether%20contemporary%20multimodal%20LLMs%20can%20assist%20with%20grading%20open-ended%20calculus%20at%20scale%20without%20eroding%20validity.%20In%20a%20large%20first-year%20exam%2C%20students%27%20handwritten%20work%20was%20graded%20by%20GPT-5%20against%20the%20same%20rubric%20used%20by%20teaching%20assistants%20%28TAs%29%2C%20with%20fractional%20credit%20permitted%3B%20TA%20rubric%20decisions%20served%20as%20ground%20truth.%20We%20calibrated%20a%20human-in-the-loop%20filter%20that%20combines%20a%20partial-credit%20threshold%20with%20an%20Item%20Response%20Theory%20%282PL%29%20risk%20measure%20based%20on%20the%20deviation%20between%20the%20AI%20score%20and%20the%20model-expected%20score%20for%20each%20student-item.%20Unfiltered%20AI-TA%20agreement%20was%20moderate%2C%20adequate%20for%20low-stakes%20feedback%20but%20not%20for%20high-stakes%20use.%20Confidence%20filtering%20made%20the%20workload-quality%20trade-off%20explicit%3A%20under%20stricter%20settings%2C%20AI%20delivered%20human-level%20accuracy%2C%20but%20also%20left%20roughly%2070%25%20of%20the%20items%20to%20be%20graded%20by%20humans.%20Psychometric%20patterns%20were%20constrained%20by%20low%20stakes%20on%20the%20open-ended%20portion%2C%20a%20small%20set%20of%20rubric%20checkpoints%2C%20and%20occasional%20misalignment%20between%20designated%20answer%20regions%20and%20where%20work%20appeared.%20Practical%20adjustments%20such%20as%20slightly%20higher%20weight%20and%20protected%20time%2C%20a%20few%20rubric-visible%20substeps%2C%20stronger%20spatial%20anchoring%20should%20raise%20ceiling%20performance.%20Overall%2C%20calibrated%20confidence%20and%20conservative%20routing%20enable%20AI%20to%20reliably%20handle%20a%20sizable%20subset%20of%20routine%20cases%20while%20reserving%20expert%20judgment%20for%20ambiguous%20or%20pedagogically%20rich%20responses.&entry.1838667208=http%3A//arxiv.org/abs/2510.05162v2&entry.124074799=Read"},
{"title": "How Evaluation Choices Distort the Outcome of Generative Drug Discovery", "author": "R\u0131za \u00d6z\u00e7elik and Francesca Grisoni", "abstract": "\"How to evaluate the de novo designs proposed by a generative model?\" Despite the transformative potential of generative deep learning in drug discovery, this seemingly simple question has no clear answer. The absence of standardized guidelines challenges both the benchmarking of generative approaches and the selection of molecules for prospective studies. In this work, we take a fresh - critical and constructive - perspective on de novo design evaluation. By training chemical language models, we analyze approximately 1 billion molecule designs and discover principles consistent across different neural networks and datasets. We uncover a key confounder: the size of the generated molecular library significantly impacts evaluation outcomes, often leading to misleading model comparisons. We find increasing the number of designs as a remedy and propose new and compute-efficient metrics to compute at large-scale. We also identify critical pitfalls in commonly used metrics - such as uniqueness and distributional similarity - that can distort assessments of generative performance. To address these issues, we propose new and refined strategies for reliable model comparison and design evaluation. Furthermore, when examining molecule selection and sampling strategies, our findings reveal the constraints to diversify the generated libraries and draw new parallels and distinctions between deep learning and drug discovery. We anticipate our findings to help reshape evaluation pipelines in generative drug discovery, paving the way for more reliable and reproducible generative modeling approaches.", "link": "http://arxiv.org/abs/2501.05457v2", "date": "2025-11-13", "relevancy": 1.5279, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5311}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Evaluation%20Choices%20Distort%20the%20Outcome%20of%20Generative%20Drug%20Discovery&body=Title%3A%20How%20Evaluation%20Choices%20Distort%20the%20Outcome%20of%20Generative%20Drug%20Discovery%0AAuthor%3A%20R%C4%B1za%20%C3%96z%C3%A7elik%20and%20Francesca%20Grisoni%0AAbstract%3A%20%22How%20to%20evaluate%20the%20de%20novo%20designs%20proposed%20by%20a%20generative%20model%3F%22%20Despite%20the%20transformative%20potential%20of%20generative%20deep%20learning%20in%20drug%20discovery%2C%20this%20seemingly%20simple%20question%20has%20no%20clear%20answer.%20The%20absence%20of%20standardized%20guidelines%20challenges%20both%20the%20benchmarking%20of%20generative%20approaches%20and%20the%20selection%20of%20molecules%20for%20prospective%20studies.%20In%20this%20work%2C%20we%20take%20a%20fresh%20-%20critical%20and%20constructive%20-%20perspective%20on%20de%20novo%20design%20evaluation.%20By%20training%20chemical%20language%20models%2C%20we%20analyze%20approximately%201%20billion%20molecule%20designs%20and%20discover%20principles%20consistent%20across%20different%20neural%20networks%20and%20datasets.%20We%20uncover%20a%20key%20confounder%3A%20the%20size%20of%20the%20generated%20molecular%20library%20significantly%20impacts%20evaluation%20outcomes%2C%20often%20leading%20to%20misleading%20model%20comparisons.%20We%20find%20increasing%20the%20number%20of%20designs%20as%20a%20remedy%20and%20propose%20new%20and%20compute-efficient%20metrics%20to%20compute%20at%20large-scale.%20We%20also%20identify%20critical%20pitfalls%20in%20commonly%20used%20metrics%20-%20such%20as%20uniqueness%20and%20distributional%20similarity%20-%20that%20can%20distort%20assessments%20of%20generative%20performance.%20To%20address%20these%20issues%2C%20we%20propose%20new%20and%20refined%20strategies%20for%20reliable%20model%20comparison%20and%20design%20evaluation.%20Furthermore%2C%20when%20examining%20molecule%20selection%20and%20sampling%20strategies%2C%20our%20findings%20reveal%20the%20constraints%20to%20diversify%20the%20generated%20libraries%20and%20draw%20new%20parallels%20and%20distinctions%20between%20deep%20learning%20and%20drug%20discovery.%20We%20anticipate%20our%20findings%20to%20help%20reshape%20evaluation%20pipelines%20in%20generative%20drug%20discovery%2C%20paving%20the%20way%20for%20more%20reliable%20and%20reproducible%20generative%20modeling%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2501.05457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Evaluation%2520Choices%2520Distort%2520the%2520Outcome%2520of%2520Generative%2520Drug%2520Discovery%26entry.906535625%3DR%25C4%25B1za%2520%25C3%2596z%25C3%25A7elik%2520and%2520Francesca%2520Grisoni%26entry.1292438233%3D%2522How%2520to%2520evaluate%2520the%2520de%2520novo%2520designs%2520proposed%2520by%2520a%2520generative%2520model%253F%2522%2520Despite%2520the%2520transformative%2520potential%2520of%2520generative%2520deep%2520learning%2520in%2520drug%2520discovery%252C%2520this%2520seemingly%2520simple%2520question%2520has%2520no%2520clear%2520answer.%2520The%2520absence%2520of%2520standardized%2520guidelines%2520challenges%2520both%2520the%2520benchmarking%2520of%2520generative%2520approaches%2520and%2520the%2520selection%2520of%2520molecules%2520for%2520prospective%2520studies.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520fresh%2520-%2520critical%2520and%2520constructive%2520-%2520perspective%2520on%2520de%2520novo%2520design%2520evaluation.%2520By%2520training%2520chemical%2520language%2520models%252C%2520we%2520analyze%2520approximately%25201%2520billion%2520molecule%2520designs%2520and%2520discover%2520principles%2520consistent%2520across%2520different%2520neural%2520networks%2520and%2520datasets.%2520We%2520uncover%2520a%2520key%2520confounder%253A%2520the%2520size%2520of%2520the%2520generated%2520molecular%2520library%2520significantly%2520impacts%2520evaluation%2520outcomes%252C%2520often%2520leading%2520to%2520misleading%2520model%2520comparisons.%2520We%2520find%2520increasing%2520the%2520number%2520of%2520designs%2520as%2520a%2520remedy%2520and%2520propose%2520new%2520and%2520compute-efficient%2520metrics%2520to%2520compute%2520at%2520large-scale.%2520We%2520also%2520identify%2520critical%2520pitfalls%2520in%2520commonly%2520used%2520metrics%2520-%2520such%2520as%2520uniqueness%2520and%2520distributional%2520similarity%2520-%2520that%2520can%2520distort%2520assessments%2520of%2520generative%2520performance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520new%2520and%2520refined%2520strategies%2520for%2520reliable%2520model%2520comparison%2520and%2520design%2520evaluation.%2520Furthermore%252C%2520when%2520examining%2520molecule%2520selection%2520and%2520sampling%2520strategies%252C%2520our%2520findings%2520reveal%2520the%2520constraints%2520to%2520diversify%2520the%2520generated%2520libraries%2520and%2520draw%2520new%2520parallels%2520and%2520distinctions%2520between%2520deep%2520learning%2520and%2520drug%2520discovery.%2520We%2520anticipate%2520our%2520findings%2520to%2520help%2520reshape%2520evaluation%2520pipelines%2520in%2520generative%2520drug%2520discovery%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520reproducible%2520generative%2520modeling%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Evaluation%20Choices%20Distort%20the%20Outcome%20of%20Generative%20Drug%20Discovery&entry.906535625=R%C4%B1za%20%C3%96z%C3%A7elik%20and%20Francesca%20Grisoni&entry.1292438233=%22How%20to%20evaluate%20the%20de%20novo%20designs%20proposed%20by%20a%20generative%20model%3F%22%20Despite%20the%20transformative%20potential%20of%20generative%20deep%20learning%20in%20drug%20discovery%2C%20this%20seemingly%20simple%20question%20has%20no%20clear%20answer.%20The%20absence%20of%20standardized%20guidelines%20challenges%20both%20the%20benchmarking%20of%20generative%20approaches%20and%20the%20selection%20of%20molecules%20for%20prospective%20studies.%20In%20this%20work%2C%20we%20take%20a%20fresh%20-%20critical%20and%20constructive%20-%20perspective%20on%20de%20novo%20design%20evaluation.%20By%20training%20chemical%20language%20models%2C%20we%20analyze%20approximately%201%20billion%20molecule%20designs%20and%20discover%20principles%20consistent%20across%20different%20neural%20networks%20and%20datasets.%20We%20uncover%20a%20key%20confounder%3A%20the%20size%20of%20the%20generated%20molecular%20library%20significantly%20impacts%20evaluation%20outcomes%2C%20often%20leading%20to%20misleading%20model%20comparisons.%20We%20find%20increasing%20the%20number%20of%20designs%20as%20a%20remedy%20and%20propose%20new%20and%20compute-efficient%20metrics%20to%20compute%20at%20large-scale.%20We%20also%20identify%20critical%20pitfalls%20in%20commonly%20used%20metrics%20-%20such%20as%20uniqueness%20and%20distributional%20similarity%20-%20that%20can%20distort%20assessments%20of%20generative%20performance.%20To%20address%20these%20issues%2C%20we%20propose%20new%20and%20refined%20strategies%20for%20reliable%20model%20comparison%20and%20design%20evaluation.%20Furthermore%2C%20when%20examining%20molecule%20selection%20and%20sampling%20strategies%2C%20our%20findings%20reveal%20the%20constraints%20to%20diversify%20the%20generated%20libraries%20and%20draw%20new%20parallels%20and%20distinctions%20between%20deep%20learning%20and%20drug%20discovery.%20We%20anticipate%20our%20findings%20to%20help%20reshape%20evaluation%20pipelines%20in%20generative%20drug%20discovery%2C%20paving%20the%20way%20for%20more%20reliable%20and%20reproducible%20generative%20modeling%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2501.05457v2&entry.124074799=Read"},
{"title": "Quantum Information Ordering and Differential Privacy", "author": "Ayanava Dasgupta and Naqueeb Ahmad Warsi and Masahito Hayashi", "abstract": "We study quantum differential privacy (QDP) by defining a notion of the order of informativeness between two pairs of quantum states. In particular, we show that if the hypothesis testing divergence of the one pair dominates over that of the other pair, then this dominance holds for every $f$-divergence. This approach completely characterizes $(\\varepsilon,\u03b4)$-QDP mechanisms by identifying the most informative $(\\varepsilon,\u03b4)$-DP quantum state pairs. We apply this to analyze the stability of quantum differentially private learning algorithms, generalizing classical results to the case $\u03b4>0$. Additionally, we study precise limits for privatized hypothesis testing and privatized quantum parameter estimation, including tight upper-bounds on the quantum Fisher information under QDP. Finally, we establish near-optimal contraction bounds for differentially private quantum channels with respect to the hockey-stick divergence.", "link": "http://arxiv.org/abs/2511.01467v2", "date": "2025-11-13", "relevancy": 1.3951, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3481}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Information%20Ordering%20and%20Differential%20Privacy&body=Title%3A%20Quantum%20Information%20Ordering%20and%20Differential%20Privacy%0AAuthor%3A%20Ayanava%20Dasgupta%20and%20Naqueeb%20Ahmad%20Warsi%20and%20Masahito%20Hayashi%0AAbstract%3A%20We%20study%20quantum%20differential%20privacy%20%28QDP%29%20by%20defining%20a%20notion%20of%20the%20order%20of%20informativeness%20between%20two%20pairs%20of%20quantum%20states.%20In%20particular%2C%20we%20show%20that%20if%20the%20hypothesis%20testing%20divergence%20of%20the%20one%20pair%20dominates%20over%20that%20of%20the%20other%20pair%2C%20then%20this%20dominance%20holds%20for%20every%20%24f%24-divergence.%20This%20approach%20completely%20characterizes%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-QDP%20mechanisms%20by%20identifying%20the%20most%20informative%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-DP%20quantum%20state%20pairs.%20We%20apply%20this%20to%20analyze%20the%20stability%20of%20quantum%20differentially%20private%20learning%20algorithms%2C%20generalizing%20classical%20results%20to%20the%20case%20%24%CE%B4%3E0%24.%20Additionally%2C%20we%20study%20precise%20limits%20for%20privatized%20hypothesis%20testing%20and%20privatized%20quantum%20parameter%20estimation%2C%20including%20tight%20upper-bounds%20on%20the%20quantum%20Fisher%20information%20under%20QDP.%20Finally%2C%20we%20establish%20near-optimal%20contraction%20bounds%20for%20differentially%20private%20quantum%20channels%20with%20respect%20to%20the%20hockey-stick%20divergence.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Information%2520Ordering%2520and%2520Differential%2520Privacy%26entry.906535625%3DAyanava%2520Dasgupta%2520and%2520Naqueeb%2520Ahmad%2520Warsi%2520and%2520Masahito%2520Hayashi%26entry.1292438233%3DWe%2520study%2520quantum%2520differential%2520privacy%2520%2528QDP%2529%2520by%2520defining%2520a%2520notion%2520of%2520the%2520order%2520of%2520informativeness%2520between%2520two%2520pairs%2520of%2520quantum%2520states.%2520In%2520particular%252C%2520we%2520show%2520that%2520if%2520the%2520hypothesis%2520testing%2520divergence%2520of%2520the%2520one%2520pair%2520dominates%2520over%2520that%2520of%2520the%2520other%2520pair%252C%2520then%2520this%2520dominance%2520holds%2520for%2520every%2520%2524f%2524-divergence.%2520This%2520approach%2520completely%2520characterizes%2520%2524%2528%255Cvarepsilon%252C%25CE%25B4%2529%2524-QDP%2520mechanisms%2520by%2520identifying%2520the%2520most%2520informative%2520%2524%2528%255Cvarepsilon%252C%25CE%25B4%2529%2524-DP%2520quantum%2520state%2520pairs.%2520We%2520apply%2520this%2520to%2520analyze%2520the%2520stability%2520of%2520quantum%2520differentially%2520private%2520learning%2520algorithms%252C%2520generalizing%2520classical%2520results%2520to%2520the%2520case%2520%2524%25CE%25B4%253E0%2524.%2520Additionally%252C%2520we%2520study%2520precise%2520limits%2520for%2520privatized%2520hypothesis%2520testing%2520and%2520privatized%2520quantum%2520parameter%2520estimation%252C%2520including%2520tight%2520upper-bounds%2520on%2520the%2520quantum%2520Fisher%2520information%2520under%2520QDP.%2520Finally%252C%2520we%2520establish%2520near-optimal%2520contraction%2520bounds%2520for%2520differentially%2520private%2520quantum%2520channels%2520with%2520respect%2520to%2520the%2520hockey-stick%2520divergence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Information%20Ordering%20and%20Differential%20Privacy&entry.906535625=Ayanava%20Dasgupta%20and%20Naqueeb%20Ahmad%20Warsi%20and%20Masahito%20Hayashi&entry.1292438233=We%20study%20quantum%20differential%20privacy%20%28QDP%29%20by%20defining%20a%20notion%20of%20the%20order%20of%20informativeness%20between%20two%20pairs%20of%20quantum%20states.%20In%20particular%2C%20we%20show%20that%20if%20the%20hypothesis%20testing%20divergence%20of%20the%20one%20pair%20dominates%20over%20that%20of%20the%20other%20pair%2C%20then%20this%20dominance%20holds%20for%20every%20%24f%24-divergence.%20This%20approach%20completely%20characterizes%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-QDP%20mechanisms%20by%20identifying%20the%20most%20informative%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-DP%20quantum%20state%20pairs.%20We%20apply%20this%20to%20analyze%20the%20stability%20of%20quantum%20differentially%20private%20learning%20algorithms%2C%20generalizing%20classical%20results%20to%20the%20case%20%24%CE%B4%3E0%24.%20Additionally%2C%20we%20study%20precise%20limits%20for%20privatized%20hypothesis%20testing%20and%20privatized%20quantum%20parameter%20estimation%2C%20including%20tight%20upper-bounds%20on%20the%20quantum%20Fisher%20information%20under%20QDP.%20Finally%2C%20we%20establish%20near-optimal%20contraction%20bounds%20for%20differentially%20private%20quantum%20channels%20with%20respect%20to%20the%20hockey-stick%20divergence.&entry.1838667208=http%3A//arxiv.org/abs/2511.01467v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


