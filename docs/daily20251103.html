<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251102.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular\n  Videos", "author": "Xuankai Zhang and Junjin Xiao and Qing Zhang", "abstract": "  This paper presents a unified framework that allows high-quality dynamic\nGaussian Splatting from both defocused and motion-blurred monocular videos. Due\nto the significant difference between the formation processes of defocus blur\nand motion blur, existing methods are tailored for either one of them, lacking\nthe ability to simultaneously deal with both of them. Although the two can be\njointly modeled as blur kernel-based convolution, the inherent difficulty in\nestimating accurate blur kernels greatly limits the progress in this direction.\nIn this work, we go a step further towards this direction. Particularly, we\npropose to estimate per-pixel reliable blur kernels using a blur prediction\nnetwork that exploits blur-related scene and camera information and is subject\nto a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian\ndensification strategy to mitigate the lack of Gaussians for incomplete\nregions, and boost the performance of novel view synthesis by incorporating\nunseen view information to constrain scene optimization. Extensive experiments\nshow that our method outperforms the state-of-the-art methods in generating\nphotorealistic novel view synthesis from defocused and motion-blurred monocular\nvideos. Our code is available at https://github.com/hhhddddddd/dydeblur.\n", "link": "http://arxiv.org/abs/2510.10691v3", "date": "2025-10-31", "relevancy": 3.2164, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6932}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6272}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Gaussian%20Splatting%20from%20Defocused%20and%20Motion-blurred%20Monocular%0A%20%20Videos&body=Title%3A%20Dynamic%20Gaussian%20Splatting%20from%20Defocused%20and%20Motion-blurred%20Monocular%0A%20%20Videos%0AAuthor%3A%20Xuankai%20Zhang%20and%20Junjin%20Xiao%20and%20Qing%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20unified%20framework%20that%20allows%20high-quality%20dynamic%0AGaussian%20Splatting%20from%20both%20defocused%20and%20motion-blurred%20monocular%20videos.%20Due%0Ato%20the%20significant%20difference%20between%20the%20formation%20processes%20of%20defocus%20blur%0Aand%20motion%20blur%2C%20existing%20methods%20are%20tailored%20for%20either%20one%20of%20them%2C%20lacking%0Athe%20ability%20to%20simultaneously%20deal%20with%20both%20of%20them.%20Although%20the%20two%20can%20be%0Ajointly%20modeled%20as%20blur%20kernel-based%20convolution%2C%20the%20inherent%20difficulty%20in%0Aestimating%20accurate%20blur%20kernels%20greatly%20limits%20the%20progress%20in%20this%20direction.%0AIn%20this%20work%2C%20we%20go%20a%20step%20further%20towards%20this%20direction.%20Particularly%2C%20we%0Apropose%20to%20estimate%20per-pixel%20reliable%20blur%20kernels%20using%20a%20blur%20prediction%0Anetwork%20that%20exploits%20blur-related%20scene%20and%20camera%20information%20and%20is%20subject%0Ato%20a%20blur-aware%20sparsity%20constraint.%20Besides%2C%20we%20introduce%20a%20dynamic%20Gaussian%0Adensification%20strategy%20to%20mitigate%20the%20lack%20of%20Gaussians%20for%20incomplete%0Aregions%2C%20and%20boost%20the%20performance%20of%20novel%20view%20synthesis%20by%20incorporating%0Aunseen%20view%20information%20to%20constrain%20scene%20optimization.%20Extensive%20experiments%0Ashow%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20in%20generating%0Aphotorealistic%20novel%20view%20synthesis%20from%20defocused%20and%20motion-blurred%20monocular%0Avideos.%20Our%20code%20is%20available%20at%20https%3A//github.com/hhhddddddd/dydeblur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10691v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Gaussian%2520Splatting%2520from%2520Defocused%2520and%2520Motion-blurred%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DXuankai%2520Zhang%2520and%2520Junjin%2520Xiao%2520and%2520Qing%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520unified%2520framework%2520that%2520allows%2520high-quality%2520dynamic%250AGaussian%2520Splatting%2520from%2520both%2520defocused%2520and%2520motion-blurred%2520monocular%2520videos.%2520Due%250Ato%2520the%2520significant%2520difference%2520between%2520the%2520formation%2520processes%2520of%2520defocus%2520blur%250Aand%2520motion%2520blur%252C%2520existing%2520methods%2520are%2520tailored%2520for%2520either%2520one%2520of%2520them%252C%2520lacking%250Athe%2520ability%2520to%2520simultaneously%2520deal%2520with%2520both%2520of%2520them.%2520Although%2520the%2520two%2520can%2520be%250Ajointly%2520modeled%2520as%2520blur%2520kernel-based%2520convolution%252C%2520the%2520inherent%2520difficulty%2520in%250Aestimating%2520accurate%2520blur%2520kernels%2520greatly%2520limits%2520the%2520progress%2520in%2520this%2520direction.%250AIn%2520this%2520work%252C%2520we%2520go%2520a%2520step%2520further%2520towards%2520this%2520direction.%2520Particularly%252C%2520we%250Apropose%2520to%2520estimate%2520per-pixel%2520reliable%2520blur%2520kernels%2520using%2520a%2520blur%2520prediction%250Anetwork%2520that%2520exploits%2520blur-related%2520scene%2520and%2520camera%2520information%2520and%2520is%2520subject%250Ato%2520a%2520blur-aware%2520sparsity%2520constraint.%2520Besides%252C%2520we%2520introduce%2520a%2520dynamic%2520Gaussian%250Adensification%2520strategy%2520to%2520mitigate%2520the%2520lack%2520of%2520Gaussians%2520for%2520incomplete%250Aregions%252C%2520and%2520boost%2520the%2520performance%2520of%2520novel%2520view%2520synthesis%2520by%2520incorporating%250Aunseen%2520view%2520information%2520to%2520constrain%2520scene%2520optimization.%2520Extensive%2520experiments%250Ashow%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520generating%250Aphotorealistic%2520novel%2520view%2520synthesis%2520from%2520defocused%2520and%2520motion-blurred%2520monocular%250Avideos.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hhhddddddd/dydeblur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10691v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Gaussian%20Splatting%20from%20Defocused%20and%20Motion-blurred%20Monocular%0A%20%20Videos&entry.906535625=Xuankai%20Zhang%20and%20Junjin%20Xiao%20and%20Qing%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20unified%20framework%20that%20allows%20high-quality%20dynamic%0AGaussian%20Splatting%20from%20both%20defocused%20and%20motion-blurred%20monocular%20videos.%20Due%0Ato%20the%20significant%20difference%20between%20the%20formation%20processes%20of%20defocus%20blur%0Aand%20motion%20blur%2C%20existing%20methods%20are%20tailored%20for%20either%20one%20of%20them%2C%20lacking%0Athe%20ability%20to%20simultaneously%20deal%20with%20both%20of%20them.%20Although%20the%20two%20can%20be%0Ajointly%20modeled%20as%20blur%20kernel-based%20convolution%2C%20the%20inherent%20difficulty%20in%0Aestimating%20accurate%20blur%20kernels%20greatly%20limits%20the%20progress%20in%20this%20direction.%0AIn%20this%20work%2C%20we%20go%20a%20step%20further%20towards%20this%20direction.%20Particularly%2C%20we%0Apropose%20to%20estimate%20per-pixel%20reliable%20blur%20kernels%20using%20a%20blur%20prediction%0Anetwork%20that%20exploits%20blur-related%20scene%20and%20camera%20information%20and%20is%20subject%0Ato%20a%20blur-aware%20sparsity%20constraint.%20Besides%2C%20we%20introduce%20a%20dynamic%20Gaussian%0Adensification%20strategy%20to%20mitigate%20the%20lack%20of%20Gaussians%20for%20incomplete%0Aregions%2C%20and%20boost%20the%20performance%20of%20novel%20view%20synthesis%20by%20incorporating%0Aunseen%20view%20information%20to%20constrain%20scene%20optimization.%20Extensive%20experiments%0Ashow%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods%20in%20generating%0Aphotorealistic%20novel%20view%20synthesis%20from%20defocused%20and%20motion-blurred%20monocular%0Avideos.%20Our%20code%20is%20available%20at%20https%3A//github.com/hhhddddddd/dydeblur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10691v3&entry.124074799=Read"},
{"title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning", "author": "Yuhong Liu and Beichen Zhang and Yuhang Zang and Yuhang Cao and Long Xing and Xiaoyi Dong and Haodong Duan and Dahua Lin and Jiaqi Wang", "abstract": "  Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.\n", "link": "http://arxiv.org/abs/2510.27606v1", "date": "2025-10-31", "relevancy": 2.9757, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-SSRL%3A%20Enhancing%20Spatial%20Understanding%20via%20Self-Supervised%0A%20%20Reinforcement%20Learning&body=Title%3A%20Spatial-SSRL%3A%20Enhancing%20Spatial%20Understanding%20via%20Self-Supervised%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Yuhong%20Liu%20and%20Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Long%20Xing%20and%20Xiaoyi%20Dong%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Spatial%20understanding%20remains%20a%20weakness%20of%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Existing%20supervised%20fine-tuning%20%28SFT%29%20and%20recent%20reinforcement%0Alearning%20with%20verifiable%20rewards%20%28RLVR%29%20pipelines%20depend%20on%20costly%20supervision%2C%0Aspecialized%20tools%2C%20or%20constrained%20environments%20that%20limit%20scale.%20We%20introduce%0ASpatial-SSRL%2C%20a%20self-supervised%20RL%20paradigm%20that%20derives%20verifiable%20signals%0Adirectly%20from%20ordinary%20RGB%20or%20RGB-D%20images.%20Spatial-SSRL%20automatically%0Aformulates%20five%20pretext%20tasks%20that%20capture%202D%20and%203D%20spatial%20structure%3A%0Ashuffled%20patch%20reordering%2C%20flipped%20patch%20recognition%2C%20cropped%20patch%20inpainting%2C%0Aregional%20depth%20ordering%2C%20and%20relative%203D%20position%20prediction.%20These%20tasks%0Aprovide%20ground-truth%20answers%20that%20are%20easy%20to%20verify%20and%20require%20no%20human%20or%0ALVLM%20annotation.%20Training%20on%20our%20tasks%20substantially%20improves%20spatial%20reasoning%0Awhile%20preserving%20general%20visual%20capabilities.%20On%20seven%20spatial%20understanding%0Abenchmarks%20in%20both%20image%20and%20video%20settings%2C%20Spatial-SSRL%20delivers%20average%0Aaccuracy%20gains%20of%204.63%25%20%283B%29%20and%203.89%25%20%287B%29%20over%20the%20Qwen2.5-VL%20baselines.%20Our%0Aresults%20show%20that%20simple%2C%20intrinsic%20supervision%20enables%20RLVR%20at%20scale%20and%0Aprovides%20a%20practical%20route%20to%20stronger%20spatial%20intelligence%20in%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-SSRL%253A%2520Enhancing%2520Spatial%2520Understanding%2520via%2520Self-Supervised%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DYuhong%2520Liu%2520and%2520Beichen%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Long%2520Xing%2520and%2520Xiaoyi%2520Dong%2520and%2520Haodong%2520Duan%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Spatial%2520understanding%2520remains%2520a%2520weakness%2520of%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529.%2520Existing%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520recent%2520reinforcement%250Alearning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520pipelines%2520depend%2520on%2520costly%2520supervision%252C%250Aspecialized%2520tools%252C%2520or%2520constrained%2520environments%2520that%2520limit%2520scale.%2520We%2520introduce%250ASpatial-SSRL%252C%2520a%2520self-supervised%2520RL%2520paradigm%2520that%2520derives%2520verifiable%2520signals%250Adirectly%2520from%2520ordinary%2520RGB%2520or%2520RGB-D%2520images.%2520Spatial-SSRL%2520automatically%250Aformulates%2520five%2520pretext%2520tasks%2520that%2520capture%25202D%2520and%25203D%2520spatial%2520structure%253A%250Ashuffled%2520patch%2520reordering%252C%2520flipped%2520patch%2520recognition%252C%2520cropped%2520patch%2520inpainting%252C%250Aregional%2520depth%2520ordering%252C%2520and%2520relative%25203D%2520position%2520prediction.%2520These%2520tasks%250Aprovide%2520ground-truth%2520answers%2520that%2520are%2520easy%2520to%2520verify%2520and%2520require%2520no%2520human%2520or%250ALVLM%2520annotation.%2520Training%2520on%2520our%2520tasks%2520substantially%2520improves%2520spatial%2520reasoning%250Awhile%2520preserving%2520general%2520visual%2520capabilities.%2520On%2520seven%2520spatial%2520understanding%250Abenchmarks%2520in%2520both%2520image%2520and%2520video%2520settings%252C%2520Spatial-SSRL%2520delivers%2520average%250Aaccuracy%2520gains%2520of%25204.63%2525%2520%25283B%2529%2520and%25203.89%2525%2520%25287B%2529%2520over%2520the%2520Qwen2.5-VL%2520baselines.%2520Our%250Aresults%2520show%2520that%2520simple%252C%2520intrinsic%2520supervision%2520enables%2520RLVR%2520at%2520scale%2520and%250Aprovides%2520a%2520practical%2520route%2520to%2520stronger%2520spatial%2520intelligence%2520in%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-SSRL%3A%20Enhancing%20Spatial%20Understanding%20via%20Self-Supervised%0A%20%20Reinforcement%20Learning&entry.906535625=Yuhong%20Liu%20and%20Beichen%20Zhang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Long%20Xing%20and%20Xiaoyi%20Dong%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Spatial%20understanding%20remains%20a%20weakness%20of%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Existing%20supervised%20fine-tuning%20%28SFT%29%20and%20recent%20reinforcement%0Alearning%20with%20verifiable%20rewards%20%28RLVR%29%20pipelines%20depend%20on%20costly%20supervision%2C%0Aspecialized%20tools%2C%20or%20constrained%20environments%20that%20limit%20scale.%20We%20introduce%0ASpatial-SSRL%2C%20a%20self-supervised%20RL%20paradigm%20that%20derives%20verifiable%20signals%0Adirectly%20from%20ordinary%20RGB%20or%20RGB-D%20images.%20Spatial-SSRL%20automatically%0Aformulates%20five%20pretext%20tasks%20that%20capture%202D%20and%203D%20spatial%20structure%3A%0Ashuffled%20patch%20reordering%2C%20flipped%20patch%20recognition%2C%20cropped%20patch%20inpainting%2C%0Aregional%20depth%20ordering%2C%20and%20relative%203D%20position%20prediction.%20These%20tasks%0Aprovide%20ground-truth%20answers%20that%20are%20easy%20to%20verify%20and%20require%20no%20human%20or%0ALVLM%20annotation.%20Training%20on%20our%20tasks%20substantially%20improves%20spatial%20reasoning%0Awhile%20preserving%20general%20visual%20capabilities.%20On%20seven%20spatial%20understanding%0Abenchmarks%20in%20both%20image%20and%20video%20settings%2C%20Spatial-SSRL%20delivers%20average%0Aaccuracy%20gains%20of%204.63%25%20%283B%29%20and%203.89%25%20%287B%29%20over%20the%20Qwen2.5-VL%20baselines.%20Our%0Aresults%20show%20that%20simple%2C%20intrinsic%20supervision%20enables%20RLVR%20at%20scale%20and%0Aprovides%20a%20practical%20route%20to%20stronger%20spatial%20intelligence%20in%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27606v1&entry.124074799=Read"},
{"title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum", "author": "Zhuoning Guo and Mingxin Li and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Xiaowen Chu", "abstract": "  The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.\n", "link": "http://arxiv.org/abs/2510.27571v1", "date": "2025-10-31", "relevancy": 2.8675, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Universal%20Video%20Retrieval%3A%20Generalizing%20Video%20Embedding%20via%0A%20%20Synthesized%20Multimodal%20Pyramid%20Curriculum&body=Title%3A%20Towards%20Universal%20Video%20Retrieval%3A%20Generalizing%20Video%20Embedding%20via%0A%20%20Synthesized%20Multimodal%20Pyramid%20Curriculum%0AAuthor%3A%20Zhuoning%20Guo%20and%20Mingxin%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Xiaowen%20Chu%0AAbstract%3A%20%20%20The%20prevailing%20video%20retrieval%20paradigm%20is%20structurally%20misaligned%2C%20as%20narrow%0Abenchmarks%20incentivize%20correspondingly%20limited%20data%20and%20single-task%20training.%0ATherefore%2C%20universal%20capability%20is%20suppressed%20due%20to%20the%20absence%20of%20a%0Adiagnostic%20evaluation%20that%20defines%20and%20demands%20multi-dimensional%0Ageneralization.%20To%20break%20this%20cycle%2C%20we%20introduce%20a%20framework%20built%20on%20the%0Aco-design%20of%20evaluation%2C%20data%2C%20and%20modeling.%20First%2C%20we%20establish%20the%20Universal%0AVideo%20Retrieval%20Benchmark%20%28UVRB%29%2C%20a%20suite%20of%2016%20datasets%20designed%20not%20only%20to%0Ameasure%20performance%20but%20also%20to%20diagnose%20critical%20capability%20gaps%20across%20tasks%0Aand%20domains.%20Second%2C%20guided%20by%20UVRB%27s%20diagnostics%2C%20we%20introduce%20a%20scalable%0Asynthesis%20workflow%20that%20generates%201.55%20million%20high-quality%20pairs%20to%20populate%0Athe%20semantic%20space%20required%20for%20universality.%20Finally%2C%20we%20devise%20the%20Modality%0APyramid%2C%20a%20curriculum%20that%20trains%20our%20General%20Video%20Embedder%20%28GVE%29%20by%0Aexplicitly%20leveraging%20the%20latent%20interconnections%20within%20our%20diverse%20data.%0AExtensive%20experiments%20show%20GVE%20achieves%20state-of-the-art%20zero-shot%0Ageneralization%20on%20UVRB.%20In%20particular%2C%20our%20analysis%20reveals%20that%20popular%0Abenchmarks%20are%20poor%20predictors%20of%20general%20ability%20and%20that%20partially%20relevant%0Aretrieval%20is%20a%20dominant%20but%20overlooked%20scenario.%20Overall%2C%20our%20co-designed%0Aframework%20provides%20a%20practical%20path%20to%20escape%20the%20limited%20scope%20and%20advance%0Atoward%20truly%20universal%20video%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Universal%2520Video%2520Retrieval%253A%2520Generalizing%2520Video%2520Embedding%2520via%250A%2520%2520Synthesized%2520Multimodal%2520Pyramid%2520Curriculum%26entry.906535625%3DZhuoning%2520Guo%2520and%2520Mingxin%2520Li%2520and%2520Yanzhao%2520Zhang%2520and%2520Dingkun%2520Long%2520and%2520Pengjun%2520Xie%2520and%2520Xiaowen%2520Chu%26entry.1292438233%3D%2520%2520The%2520prevailing%2520video%2520retrieval%2520paradigm%2520is%2520structurally%2520misaligned%252C%2520as%2520narrow%250Abenchmarks%2520incentivize%2520correspondingly%2520limited%2520data%2520and%2520single-task%2520training.%250ATherefore%252C%2520universal%2520capability%2520is%2520suppressed%2520due%2520to%2520the%2520absence%2520of%2520a%250Adiagnostic%2520evaluation%2520that%2520defines%2520and%2520demands%2520multi-dimensional%250Ageneralization.%2520To%2520break%2520this%2520cycle%252C%2520we%2520introduce%2520a%2520framework%2520built%2520on%2520the%250Aco-design%2520of%2520evaluation%252C%2520data%252C%2520and%2520modeling.%2520First%252C%2520we%2520establish%2520the%2520Universal%250AVideo%2520Retrieval%2520Benchmark%2520%2528UVRB%2529%252C%2520a%2520suite%2520of%252016%2520datasets%2520designed%2520not%2520only%2520to%250Ameasure%2520performance%2520but%2520also%2520to%2520diagnose%2520critical%2520capability%2520gaps%2520across%2520tasks%250Aand%2520domains.%2520Second%252C%2520guided%2520by%2520UVRB%2527s%2520diagnostics%252C%2520we%2520introduce%2520a%2520scalable%250Asynthesis%2520workflow%2520that%2520generates%25201.55%2520million%2520high-quality%2520pairs%2520to%2520populate%250Athe%2520semantic%2520space%2520required%2520for%2520universality.%2520Finally%252C%2520we%2520devise%2520the%2520Modality%250APyramid%252C%2520a%2520curriculum%2520that%2520trains%2520our%2520General%2520Video%2520Embedder%2520%2528GVE%2529%2520by%250Aexplicitly%2520leveraging%2520the%2520latent%2520interconnections%2520within%2520our%2520diverse%2520data.%250AExtensive%2520experiments%2520show%2520GVE%2520achieves%2520state-of-the-art%2520zero-shot%250Ageneralization%2520on%2520UVRB.%2520In%2520particular%252C%2520our%2520analysis%2520reveals%2520that%2520popular%250Abenchmarks%2520are%2520poor%2520predictors%2520of%2520general%2520ability%2520and%2520that%2520partially%2520relevant%250Aretrieval%2520is%2520a%2520dominant%2520but%2520overlooked%2520scenario.%2520Overall%252C%2520our%2520co-designed%250Aframework%2520provides%2520a%2520practical%2520path%2520to%2520escape%2520the%2520limited%2520scope%2520and%2520advance%250Atoward%2520truly%2520universal%2520video%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Universal%20Video%20Retrieval%3A%20Generalizing%20Video%20Embedding%20via%0A%20%20Synthesized%20Multimodal%20Pyramid%20Curriculum&entry.906535625=Zhuoning%20Guo%20and%20Mingxin%20Li%20and%20Yanzhao%20Zhang%20and%20Dingkun%20Long%20and%20Pengjun%20Xie%20and%20Xiaowen%20Chu&entry.1292438233=%20%20The%20prevailing%20video%20retrieval%20paradigm%20is%20structurally%20misaligned%2C%20as%20narrow%0Abenchmarks%20incentivize%20correspondingly%20limited%20data%20and%20single-task%20training.%0ATherefore%2C%20universal%20capability%20is%20suppressed%20due%20to%20the%20absence%20of%20a%0Adiagnostic%20evaluation%20that%20defines%20and%20demands%20multi-dimensional%0Ageneralization.%20To%20break%20this%20cycle%2C%20we%20introduce%20a%20framework%20built%20on%20the%0Aco-design%20of%20evaluation%2C%20data%2C%20and%20modeling.%20First%2C%20we%20establish%20the%20Universal%0AVideo%20Retrieval%20Benchmark%20%28UVRB%29%2C%20a%20suite%20of%2016%20datasets%20designed%20not%20only%20to%0Ameasure%20performance%20but%20also%20to%20diagnose%20critical%20capability%20gaps%20across%20tasks%0Aand%20domains.%20Second%2C%20guided%20by%20UVRB%27s%20diagnostics%2C%20we%20introduce%20a%20scalable%0Asynthesis%20workflow%20that%20generates%201.55%20million%20high-quality%20pairs%20to%20populate%0Athe%20semantic%20space%20required%20for%20universality.%20Finally%2C%20we%20devise%20the%20Modality%0APyramid%2C%20a%20curriculum%20that%20trains%20our%20General%20Video%20Embedder%20%28GVE%29%20by%0Aexplicitly%20leveraging%20the%20latent%20interconnections%20within%20our%20diverse%20data.%0AExtensive%20experiments%20show%20GVE%20achieves%20state-of-the-art%20zero-shot%0Ageneralization%20on%20UVRB.%20In%20particular%2C%20our%20analysis%20reveals%20that%20popular%0Abenchmarks%20are%20poor%20predictors%20of%20general%20ability%20and%20that%20partially%20relevant%0Aretrieval%20is%20a%20dominant%20but%20overlooked%20scenario.%20Overall%2C%20our%20co-designed%0Aframework%20provides%20a%20practical%20path%20to%20escape%20the%20limited%20scope%20and%20advance%0Atoward%20truly%20universal%20video%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27571v1&entry.124074799=Read"},
{"title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing", "author": "Zeng Tao and Zheng Ding and Zeyuan Chen and Xiang Zhang and Leizhi Li and Zhuowen Tu", "abstract": "  Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.\n", "link": "http://arxiv.org/abs/2510.04539v2", "date": "2025-10-31", "relevancy": 2.8378, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5734}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3Editor%3A%20Achieving%20Controllable%20Consistency%20in%202D%20Model%20for%203D%20Editing&body=Title%3A%20C3Editor%3A%20Achieving%20Controllable%20Consistency%20in%202D%20Model%20for%203D%20Editing%0AAuthor%3A%20Zeng%20Tao%20and%20Zheng%20Ding%20and%20Zeyuan%20Chen%20and%20Xiang%20Zhang%20and%20Leizhi%20Li%20and%20Zhuowen%20Tu%0AAbstract%3A%20%20%20Existing%202D-lifting-based%203D%20editing%20methods%20often%20encounter%20challenges%0Arelated%20to%20inconsistency%2C%20stemming%20from%20the%20lack%20of%20view-consistent%202D%20editing%0Amodels%20and%20the%20difficulty%20of%20ensuring%20consistent%20editing%20across%20multiple%20views.%0ATo%20address%20these%20issues%2C%20we%20propose%20C3Editor%2C%20a%20controllable%20and%20consistent%0A2D-lifting-based%203D%20editing%20framework.%20Given%20an%20original%203D%20representation%20and%0Aa%20text-based%20editing%20prompt%2C%20our%20method%20selectively%20establishes%20a%0Aview-consistent%202D%20editing%20model%20to%20achieve%20superior%203D%20editing%20results.%20The%0Aprocess%20begins%20with%20the%20controlled%20selection%20of%20a%20ground%20truth%20%28GT%29%20view%20and%0Aits%20corresponding%20edited%20image%20as%20the%20optimization%20target%2C%20allowing%20for%0Auser-defined%20manual%20edits.%20Next%2C%20we%20fine-tune%20the%202D%20editing%20model%20within%20the%0AGT%20view%20and%20across%20multiple%20views%20to%20align%20with%20the%20GT-edited%20image%20while%0Aensuring%20multi-view%20consistency.%20To%20meet%20the%20distinct%20requirements%20of%20GT%20view%0Afitting%20and%20multi-view%20consistency%2C%20we%20introduce%20separate%20LoRA%20modules%20for%0Atargeted%20fine-tuning.%20Our%20approach%20delivers%20more%20consistent%20and%20controllable%202D%0Aand%203D%20editing%20results%20than%20existing%202D-lifting-based%20methods%2C%20outperforming%0Athem%20in%20both%20qualitative%20and%20quantitative%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.04539v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3Editor%253A%2520Achieving%2520Controllable%2520Consistency%2520in%25202D%2520Model%2520for%25203D%2520Editing%26entry.906535625%3DZeng%2520Tao%2520and%2520Zheng%2520Ding%2520and%2520Zeyuan%2520Chen%2520and%2520Xiang%2520Zhang%2520and%2520Leizhi%2520Li%2520and%2520Zhuowen%2520Tu%26entry.1292438233%3D%2520%2520Existing%25202D-lifting-based%25203D%2520editing%2520methods%2520often%2520encounter%2520challenges%250Arelated%2520to%2520inconsistency%252C%2520stemming%2520from%2520the%2520lack%2520of%2520view-consistent%25202D%2520editing%250Amodels%2520and%2520the%2520difficulty%2520of%2520ensuring%2520consistent%2520editing%2520across%2520multiple%2520views.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520C3Editor%252C%2520a%2520controllable%2520and%2520consistent%250A2D-lifting-based%25203D%2520editing%2520framework.%2520Given%2520an%2520original%25203D%2520representation%2520and%250Aa%2520text-based%2520editing%2520prompt%252C%2520our%2520method%2520selectively%2520establishes%2520a%250Aview-consistent%25202D%2520editing%2520model%2520to%2520achieve%2520superior%25203D%2520editing%2520results.%2520The%250Aprocess%2520begins%2520with%2520the%2520controlled%2520selection%2520of%2520a%2520ground%2520truth%2520%2528GT%2529%2520view%2520and%250Aits%2520corresponding%2520edited%2520image%2520as%2520the%2520optimization%2520target%252C%2520allowing%2520for%250Auser-defined%2520manual%2520edits.%2520Next%252C%2520we%2520fine-tune%2520the%25202D%2520editing%2520model%2520within%2520the%250AGT%2520view%2520and%2520across%2520multiple%2520views%2520to%2520align%2520with%2520the%2520GT-edited%2520image%2520while%250Aensuring%2520multi-view%2520consistency.%2520To%2520meet%2520the%2520distinct%2520requirements%2520of%2520GT%2520view%250Afitting%2520and%2520multi-view%2520consistency%252C%2520we%2520introduce%2520separate%2520LoRA%2520modules%2520for%250Atargeted%2520fine-tuning.%2520Our%2520approach%2520delivers%2520more%2520consistent%2520and%2520controllable%25202D%250Aand%25203D%2520editing%2520results%2520than%2520existing%25202D-lifting-based%2520methods%252C%2520outperforming%250Athem%2520in%2520both%2520qualitative%2520and%2520quantitative%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04539v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3Editor%3A%20Achieving%20Controllable%20Consistency%20in%202D%20Model%20for%203D%20Editing&entry.906535625=Zeng%20Tao%20and%20Zheng%20Ding%20and%20Zeyuan%20Chen%20and%20Xiang%20Zhang%20and%20Leizhi%20Li%20and%20Zhuowen%20Tu&entry.1292438233=%20%20Existing%202D-lifting-based%203D%20editing%20methods%20often%20encounter%20challenges%0Arelated%20to%20inconsistency%2C%20stemming%20from%20the%20lack%20of%20view-consistent%202D%20editing%0Amodels%20and%20the%20difficulty%20of%20ensuring%20consistent%20editing%20across%20multiple%20views.%0ATo%20address%20these%20issues%2C%20we%20propose%20C3Editor%2C%20a%20controllable%20and%20consistent%0A2D-lifting-based%203D%20editing%20framework.%20Given%20an%20original%203D%20representation%20and%0Aa%20text-based%20editing%20prompt%2C%20our%20method%20selectively%20establishes%20a%0Aview-consistent%202D%20editing%20model%20to%20achieve%20superior%203D%20editing%20results.%20The%0Aprocess%20begins%20with%20the%20controlled%20selection%20of%20a%20ground%20truth%20%28GT%29%20view%20and%0Aits%20corresponding%20edited%20image%20as%20the%20optimization%20target%2C%20allowing%20for%0Auser-defined%20manual%20edits.%20Next%2C%20we%20fine-tune%20the%202D%20editing%20model%20within%20the%0AGT%20view%20and%20across%20multiple%20views%20to%20align%20with%20the%20GT-edited%20image%20while%0Aensuring%20multi-view%20consistency.%20To%20meet%20the%20distinct%20requirements%20of%20GT%20view%0Afitting%20and%20multi-view%20consistency%2C%20we%20introduce%20separate%20LoRA%20modules%20for%0Atargeted%20fine-tuning.%20Our%20approach%20delivers%20more%20consistent%20and%20controllable%202D%0Aand%203D%20editing%20results%20than%20existing%202D-lifting-based%20methods%2C%20outperforming%0Athem%20in%20both%20qualitative%20and%20quantitative%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.04539v2&entry.124074799=Read"},
{"title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting", "author": "Danyal Maqbool and Changhee Lee and Zachary Huemann and Samuel D. Church and Matthew E. Larson and Scott B. Perlman and Tomas A. Romero and Joshua D. Warner and Meghan Lubner and Xin Tie and Jameson Merkow and Junjie Hu and Steve Y. Cho and Tyler J. Bradshaw", "abstract": "  Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.\n", "link": "http://arxiv.org/abs/2510.27680v1", "date": "2025-10-31", "relevancy": 2.8201, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PETAR%3A%20Localized%20Findings%20Generation%20with%20Mask-Aware%20Vision-Language%0A%20%20Modeling%20for%20PET%20Automated%20Reporting&body=Title%3A%20PETAR%3A%20Localized%20Findings%20Generation%20with%20Mask-Aware%20Vision-Language%0A%20%20Modeling%20for%20PET%20Automated%20Reporting%0AAuthor%3A%20Danyal%20Maqbool%20and%20Changhee%20Lee%20and%20Zachary%20Huemann%20and%20Samuel%20D.%20Church%20and%20Matthew%20E.%20Larson%20and%20Scott%20B.%20Perlman%20and%20Tomas%20A.%20Romero%20and%20Joshua%20D.%20Warner%20and%20Meghan%20Lubner%20and%20Xin%20Tie%20and%20Jameson%20Merkow%20and%20Junjie%20Hu%20and%20Steve%20Y.%20Cho%20and%20Tyler%20J.%20Bradshaw%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20enabled%20impressive%0Amultimodal%20reasoning%2C%20yet%20most%20medical%20applications%20remain%20limited%20to%202D%0Aimaging.%20In%20this%20work%2C%20we%20extend%20VLMs%20to%203D%20positron%20emission%20tomography%20and%0Acomputed%20tomography%20%28PET/CT%29%2C%20a%20domain%20characterized%20by%20large%20volumetric%20data%2C%0Asmall%20and%20dispersed%20lesions%2C%20and%20lengthy%20radiology%20reports.%20We%20introduce%20a%0Alarge-scale%20dataset%20comprising%20over%2011%2C000%20lesion-level%20descriptions%20paired%0Awith%203D%20segmentations%20from%20more%20than%205%2C000%20PET/CT%20exams%2C%20extracted%20via%20a%20hybrid%0Arule-based%20and%20large%20language%20model%20%28LLM%29%20pipeline.%20Building%20upon%20this%20dataset%2C%0Awe%20propose%20PETAR-4B%2C%20a%203D%20mask-aware%20vision-language%20model%20that%20integrates%20PET%2C%0ACT%2C%20and%20lesion%20contours%20for%20spatially%20grounded%20report%20generation.%20PETAR%20bridges%0Aglobal%20contextual%20reasoning%20with%20fine-grained%20lesion%20awareness%2C%20producing%0Aclinically%20coherent%20and%20localized%20findings.%20Comprehensive%20automated%20and%20human%0Aevaluations%20demonstrate%20that%20PETAR%20substantially%20improves%20PET/CT%20report%0Ageneration%20quality%2C%20advancing%203D%20medical%20vision-language%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPETAR%253A%2520Localized%2520Findings%2520Generation%2520with%2520Mask-Aware%2520Vision-Language%250A%2520%2520Modeling%2520for%2520PET%2520Automated%2520Reporting%26entry.906535625%3DDanyal%2520Maqbool%2520and%2520Changhee%2520Lee%2520and%2520Zachary%2520Huemann%2520and%2520Samuel%2520D.%2520Church%2520and%2520Matthew%2520E.%2520Larson%2520and%2520Scott%2520B.%2520Perlman%2520and%2520Tomas%2520A.%2520Romero%2520and%2520Joshua%2520D.%2520Warner%2520and%2520Meghan%2520Lubner%2520and%2520Xin%2520Tie%2520and%2520Jameson%2520Merkow%2520and%2520Junjie%2520Hu%2520and%2520Steve%2520Y.%2520Cho%2520and%2520Tyler%2520J.%2520Bradshaw%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520enabled%2520impressive%250Amultimodal%2520reasoning%252C%2520yet%2520most%2520medical%2520applications%2520remain%2520limited%2520to%25202D%250Aimaging.%2520In%2520this%2520work%252C%2520we%2520extend%2520VLMs%2520to%25203D%2520positron%2520emission%2520tomography%2520and%250Acomputed%2520tomography%2520%2528PET/CT%2529%252C%2520a%2520domain%2520characterized%2520by%2520large%2520volumetric%2520data%252C%250Asmall%2520and%2520dispersed%2520lesions%252C%2520and%2520lengthy%2520radiology%2520reports.%2520We%2520introduce%2520a%250Alarge-scale%2520dataset%2520comprising%2520over%252011%252C000%2520lesion-level%2520descriptions%2520paired%250Awith%25203D%2520segmentations%2520from%2520more%2520than%25205%252C000%2520PET/CT%2520exams%252C%2520extracted%2520via%2520a%2520hybrid%250Arule-based%2520and%2520large%2520language%2520model%2520%2528LLM%2529%2520pipeline.%2520Building%2520upon%2520this%2520dataset%252C%250Awe%2520propose%2520PETAR-4B%252C%2520a%25203D%2520mask-aware%2520vision-language%2520model%2520that%2520integrates%2520PET%252C%250ACT%252C%2520and%2520lesion%2520contours%2520for%2520spatially%2520grounded%2520report%2520generation.%2520PETAR%2520bridges%250Aglobal%2520contextual%2520reasoning%2520with%2520fine-grained%2520lesion%2520awareness%252C%2520producing%250Aclinically%2520coherent%2520and%2520localized%2520findings.%2520Comprehensive%2520automated%2520and%2520human%250Aevaluations%2520demonstrate%2520that%2520PETAR%2520substantially%2520improves%2520PET/CT%2520report%250Ageneration%2520quality%252C%2520advancing%25203D%2520medical%2520vision-language%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PETAR%3A%20Localized%20Findings%20Generation%20with%20Mask-Aware%20Vision-Language%0A%20%20Modeling%20for%20PET%20Automated%20Reporting&entry.906535625=Danyal%20Maqbool%20and%20Changhee%20Lee%20and%20Zachary%20Huemann%20and%20Samuel%20D.%20Church%20and%20Matthew%20E.%20Larson%20and%20Scott%20B.%20Perlman%20and%20Tomas%20A.%20Romero%20and%20Joshua%20D.%20Warner%20and%20Meghan%20Lubner%20and%20Xin%20Tie%20and%20Jameson%20Merkow%20and%20Junjie%20Hu%20and%20Steve%20Y.%20Cho%20and%20Tyler%20J.%20Bradshaw&entry.1292438233=%20%20Recent%20advances%20in%20vision-language%20models%20%28VLMs%29%20have%20enabled%20impressive%0Amultimodal%20reasoning%2C%20yet%20most%20medical%20applications%20remain%20limited%20to%202D%0Aimaging.%20In%20this%20work%2C%20we%20extend%20VLMs%20to%203D%20positron%20emission%20tomography%20and%0Acomputed%20tomography%20%28PET/CT%29%2C%20a%20domain%20characterized%20by%20large%20volumetric%20data%2C%0Asmall%20and%20dispersed%20lesions%2C%20and%20lengthy%20radiology%20reports.%20We%20introduce%20a%0Alarge-scale%20dataset%20comprising%20over%2011%2C000%20lesion-level%20descriptions%20paired%0Awith%203D%20segmentations%20from%20more%20than%205%2C000%20PET/CT%20exams%2C%20extracted%20via%20a%20hybrid%0Arule-based%20and%20large%20language%20model%20%28LLM%29%20pipeline.%20Building%20upon%20this%20dataset%2C%0Awe%20propose%20PETAR-4B%2C%20a%203D%20mask-aware%20vision-language%20model%20that%20integrates%20PET%2C%0ACT%2C%20and%20lesion%20contours%20for%20spatially%20grounded%20report%20generation.%20PETAR%20bridges%0Aglobal%20contextual%20reasoning%20with%20fine-grained%20lesion%20awareness%2C%20producing%0Aclinically%20coherent%20and%20localized%20findings.%20Comprehensive%20automated%20and%20human%0Aevaluations%20demonstrate%20that%20PETAR%20substantially%20improves%20PET/CT%20report%0Ageneration%20quality%2C%20advancing%203D%20medical%20vision-language%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27680v1&entry.124074799=Read"},
{"title": "VRoPE: Rotary Position Embedding for Video Large Language Models", "author": "Zikang Liu and Longteng Guo and Yepeng Tang and Tongtian Yue and Junxian Cai and Kai Ma and Qingbin Liu and Xi Chen and Jing Liu", "abstract": "  Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code is available at\nhttps://github.com/johncaged/VRoPE.\n", "link": "http://arxiv.org/abs/2502.11664v4", "date": "2025-10-31", "relevancy": 2.7905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models&body=Title%3A%20VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models%0AAuthor%3A%20Zikang%20Liu%20and%20Longteng%20Guo%20and%20Yepeng%20Tang%20and%20Tongtian%20Yue%20and%20Junxian%20Cai%20and%20Kai%20Ma%20and%20Qingbin%20Liu%20and%20Xi%20Chen%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20has%20shown%20strong%20performance%20in%20text-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20but%20extending%20it%20to%20video%20remains%20a%20challenge%20due%0Ato%20the%20intricate%20spatiotemporal%20structure%20of%20video%20frames.%20Existing%0Aadaptations%2C%20such%20as%20RoPE-3D%2C%20attempt%20to%20encode%20spatial%20and%20temporal%20dimensions%0Aseparately%20but%20suffer%20from%20two%20major%20limitations%3A%20positional%20bias%20in%20attention%0Adistribution%20and%20disruptions%20in%20video-text%20transitions.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Video%20Rotary%20Position%20Embedding%20%28VRoPE%29%2C%20a%20novel%20positional%0Aencoding%20method%20tailored%20for%20Video-LLMs.%20Specifically%2C%20we%20introduce%20a%20more%0Abalanced%20encoding%20strategy%20that%20mitigates%20attention%20biases%2C%20ensuring%20a%20more%0Auniform%20distribution%20of%20spatial%20focus.%20Additionally%2C%20our%20approach%20restructures%0Apositional%20indices%20to%20ensure%20a%20smooth%20transition%20between%20video%20and%20text%20tokens.%0AExtensive%20experiments%20on%20different%20models%20demonstrate%20that%20VRoPE%20consistently%0Aoutperforms%20previous%20RoPE%20variants%2C%20achieving%20significant%20improvements%20in%20video%0Aunderstanding%2C%20temporal%20reasoning%2C%20and%20retrieval%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/johncaged/VRoPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11664v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRoPE%253A%2520Rotary%2520Position%2520Embedding%2520for%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DZikang%2520Liu%2520and%2520Longteng%2520Guo%2520and%2520Yepeng%2520Tang%2520and%2520Tongtian%2520Yue%2520and%2520Junxian%2520Cai%2520and%2520Kai%2520Ma%2520and%2520Qingbin%2520Liu%2520and%2520Xi%2520Chen%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520has%2520shown%2520strong%2520performance%2520in%2520text-based%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520but%2520extending%2520it%2520to%2520video%2520remains%2520a%2520challenge%2520due%250Ato%2520the%2520intricate%2520spatiotemporal%2520structure%2520of%2520video%2520frames.%2520Existing%250Aadaptations%252C%2520such%2520as%2520RoPE-3D%252C%2520attempt%2520to%2520encode%2520spatial%2520and%2520temporal%2520dimensions%250Aseparately%2520but%2520suffer%2520from%2520two%2520major%2520limitations%253A%2520positional%2520bias%2520in%2520attention%250Adistribution%2520and%2520disruptions%2520in%2520video-text%2520transitions.%2520To%2520overcome%2520these%250Aissues%252C%2520we%2520propose%2520Video%2520Rotary%2520Position%2520Embedding%2520%2528VRoPE%2529%252C%2520a%2520novel%2520positional%250Aencoding%2520method%2520tailored%2520for%2520Video-LLMs.%2520Specifically%252C%2520we%2520introduce%2520a%2520more%250Abalanced%2520encoding%2520strategy%2520that%2520mitigates%2520attention%2520biases%252C%2520ensuring%2520a%2520more%250Auniform%2520distribution%2520of%2520spatial%2520focus.%2520Additionally%252C%2520our%2520approach%2520restructures%250Apositional%2520indices%2520to%2520ensure%2520a%2520smooth%2520transition%2520between%2520video%2520and%2520text%2520tokens.%250AExtensive%2520experiments%2520on%2520different%2520models%2520demonstrate%2520that%2520VRoPE%2520consistently%250Aoutperforms%2520previous%2520RoPE%2520variants%252C%2520achieving%2520significant%2520improvements%2520in%2520video%250Aunderstanding%252C%2520temporal%2520reasoning%252C%2520and%2520retrieval%2520tasks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/johncaged/VRoPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11664v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRoPE%3A%20Rotary%20Position%20Embedding%20for%20Video%20Large%20Language%20Models&entry.906535625=Zikang%20Liu%20and%20Longteng%20Guo%20and%20Yepeng%20Tang%20and%20Tongtian%20Yue%20and%20Junxian%20Cai%20and%20Kai%20Ma%20and%20Qingbin%20Liu%20and%20Xi%20Chen%20and%20Jing%20Liu&entry.1292438233=%20%20Rotary%20Position%20Embedding%20%28RoPE%29%20has%20shown%20strong%20performance%20in%20text-based%0ALarge%20Language%20Models%20%28LLMs%29%2C%20but%20extending%20it%20to%20video%20remains%20a%20challenge%20due%0Ato%20the%20intricate%20spatiotemporal%20structure%20of%20video%20frames.%20Existing%0Aadaptations%2C%20such%20as%20RoPE-3D%2C%20attempt%20to%20encode%20spatial%20and%20temporal%20dimensions%0Aseparately%20but%20suffer%20from%20two%20major%20limitations%3A%20positional%20bias%20in%20attention%0Adistribution%20and%20disruptions%20in%20video-text%20transitions.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20Video%20Rotary%20Position%20Embedding%20%28VRoPE%29%2C%20a%20novel%20positional%0Aencoding%20method%20tailored%20for%20Video-LLMs.%20Specifically%2C%20we%20introduce%20a%20more%0Abalanced%20encoding%20strategy%20that%20mitigates%20attention%20biases%2C%20ensuring%20a%20more%0Auniform%20distribution%20of%20spatial%20focus.%20Additionally%2C%20our%20approach%20restructures%0Apositional%20indices%20to%20ensure%20a%20smooth%20transition%20between%20video%20and%20text%20tokens.%0AExtensive%20experiments%20on%20different%20models%20demonstrate%20that%20VRoPE%20consistently%0Aoutperforms%20previous%20RoPE%20variants%2C%20achieving%20significant%20improvements%20in%20video%0Aunderstanding%2C%20temporal%20reasoning%2C%20and%20retrieval%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/johncaged/VRoPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11664v4&entry.124074799=Read"},
{"title": "InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames", "author": "Haorui Li and Weitao Du and Yuqiang Li and Hongyu Guo and Shengchao Liu", "abstract": "  Transformer-based autoregressive models have emerged as a unifying paradigm\nacross modalities such as text and images, but their extension to 3D molecule\ngeneration remains underexplored. The gap stems from two fundamental\nchallenges: (1) tokenizing molecules into a canonical 1D sequence of tokens\nthat is invariant to both SE(3) transformations and atom index permutations,\nand (2) designing an architecture capable of modeling hybrid atom-based tokens\nthat couple discrete atom types with continuous 3D coordinates. To address\nthese challenges, we introduce InertialAR. InertialAR devises a canonical\ntokenization that aligns molecules to their inertial frames and reorders atoms\nto ensure SE(3) and permutation invariance. Moreover, InertialAR equips the\nattention mechanism with geometric awareness via geometric rotary positional\nencoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive\nparadigm to predict the next atom-based token, predicting the atom type first\nand then its 3D coordinates via Diffusion loss. Experimentally, InertialAR\nachieves state-of-the-art performance on 7 of the 10 evaluation metrics for\nunconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover,\nit significantly outperforms strong baselines in controllable generation for\ntargeted chemical functionality, attaining state-of-the-art results across all\n5 metrics.\n", "link": "http://arxiv.org/abs/2510.27497v1", "date": "2025-10-31", "relevancy": 2.7736, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5851}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5395}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InertialAR%3A%20Autoregressive%203D%20Molecule%20Generation%20with%20Inertial%20Frames&body=Title%3A%20InertialAR%3A%20Autoregressive%203D%20Molecule%20Generation%20with%20Inertial%20Frames%0AAuthor%3A%20Haorui%20Li%20and%20Weitao%20Du%20and%20Yuqiang%20Li%20and%20Hongyu%20Guo%20and%20Shengchao%20Liu%0AAbstract%3A%20%20%20Transformer-based%20autoregressive%20models%20have%20emerged%20as%20a%20unifying%20paradigm%0Aacross%20modalities%20such%20as%20text%20and%20images%2C%20but%20their%20extension%20to%203D%20molecule%0Ageneration%20remains%20underexplored.%20The%20gap%20stems%20from%20two%20fundamental%0Achallenges%3A%20%281%29%20tokenizing%20molecules%20into%20a%20canonical%201D%20sequence%20of%20tokens%0Athat%20is%20invariant%20to%20both%20SE%283%29%20transformations%20and%20atom%20index%20permutations%2C%0Aand%20%282%29%20designing%20an%20architecture%20capable%20of%20modeling%20hybrid%20atom-based%20tokens%0Athat%20couple%20discrete%20atom%20types%20with%20continuous%203D%20coordinates.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20InertialAR.%20InertialAR%20devises%20a%20canonical%0Atokenization%20that%20aligns%20molecules%20to%20their%20inertial%20frames%20and%20reorders%20atoms%0Ato%20ensure%20SE%283%29%20and%20permutation%20invariance.%20Moreover%2C%20InertialAR%20equips%20the%0Aattention%20mechanism%20with%20geometric%20awareness%20via%20geometric%20rotary%20positional%0Aencoding%20%28GeoRoPE%29.%20In%20addition%2C%20it%20utilizes%20a%20hierarchical%20autoregressive%0Aparadigm%20to%20predict%20the%20next%20atom-based%20token%2C%20predicting%20the%20atom%20type%20first%0Aand%20then%20its%203D%20coordinates%20via%20Diffusion%20loss.%20Experimentally%2C%20InertialAR%0Aachieves%20state-of-the-art%20performance%20on%207%20of%20the%2010%20evaluation%20metrics%20for%0Aunconditional%20molecule%20generation%20across%20QM9%2C%20GEOM-Drugs%2C%20and%20B3LYP.%20Moreover%2C%0Ait%20significantly%20outperforms%20strong%20baselines%20in%20controllable%20generation%20for%0Atargeted%20chemical%20functionality%2C%20attaining%20state-of-the-art%20results%20across%20all%0A5%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInertialAR%253A%2520Autoregressive%25203D%2520Molecule%2520Generation%2520with%2520Inertial%2520Frames%26entry.906535625%3DHaorui%2520Li%2520and%2520Weitao%2520Du%2520and%2520Yuqiang%2520Li%2520and%2520Hongyu%2520Guo%2520and%2520Shengchao%2520Liu%26entry.1292438233%3D%2520%2520Transformer-based%2520autoregressive%2520models%2520have%2520emerged%2520as%2520a%2520unifying%2520paradigm%250Aacross%2520modalities%2520such%2520as%2520text%2520and%2520images%252C%2520but%2520their%2520extension%2520to%25203D%2520molecule%250Ageneration%2520remains%2520underexplored.%2520The%2520gap%2520stems%2520from%2520two%2520fundamental%250Achallenges%253A%2520%25281%2529%2520tokenizing%2520molecules%2520into%2520a%2520canonical%25201D%2520sequence%2520of%2520tokens%250Athat%2520is%2520invariant%2520to%2520both%2520SE%25283%2529%2520transformations%2520and%2520atom%2520index%2520permutations%252C%250Aand%2520%25282%2529%2520designing%2520an%2520architecture%2520capable%2520of%2520modeling%2520hybrid%2520atom-based%2520tokens%250Athat%2520couple%2520discrete%2520atom%2520types%2520with%2520continuous%25203D%2520coordinates.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520InertialAR.%2520InertialAR%2520devises%2520a%2520canonical%250Atokenization%2520that%2520aligns%2520molecules%2520to%2520their%2520inertial%2520frames%2520and%2520reorders%2520atoms%250Ato%2520ensure%2520SE%25283%2529%2520and%2520permutation%2520invariance.%2520Moreover%252C%2520InertialAR%2520equips%2520the%250Aattention%2520mechanism%2520with%2520geometric%2520awareness%2520via%2520geometric%2520rotary%2520positional%250Aencoding%2520%2528GeoRoPE%2529.%2520In%2520addition%252C%2520it%2520utilizes%2520a%2520hierarchical%2520autoregressive%250Aparadigm%2520to%2520predict%2520the%2520next%2520atom-based%2520token%252C%2520predicting%2520the%2520atom%2520type%2520first%250Aand%2520then%2520its%25203D%2520coordinates%2520via%2520Diffusion%2520loss.%2520Experimentally%252C%2520InertialAR%250Aachieves%2520state-of-the-art%2520performance%2520on%25207%2520of%2520the%252010%2520evaluation%2520metrics%2520for%250Aunconditional%2520molecule%2520generation%2520across%2520QM9%252C%2520GEOM-Drugs%252C%2520and%2520B3LYP.%2520Moreover%252C%250Ait%2520significantly%2520outperforms%2520strong%2520baselines%2520in%2520controllable%2520generation%2520for%250Atargeted%2520chemical%2520functionality%252C%2520attaining%2520state-of-the-art%2520results%2520across%2520all%250A5%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InertialAR%3A%20Autoregressive%203D%20Molecule%20Generation%20with%20Inertial%20Frames&entry.906535625=Haorui%20Li%20and%20Weitao%20Du%20and%20Yuqiang%20Li%20and%20Hongyu%20Guo%20and%20Shengchao%20Liu&entry.1292438233=%20%20Transformer-based%20autoregressive%20models%20have%20emerged%20as%20a%20unifying%20paradigm%0Aacross%20modalities%20such%20as%20text%20and%20images%2C%20but%20their%20extension%20to%203D%20molecule%0Ageneration%20remains%20underexplored.%20The%20gap%20stems%20from%20two%20fundamental%0Achallenges%3A%20%281%29%20tokenizing%20molecules%20into%20a%20canonical%201D%20sequence%20of%20tokens%0Athat%20is%20invariant%20to%20both%20SE%283%29%20transformations%20and%20atom%20index%20permutations%2C%0Aand%20%282%29%20designing%20an%20architecture%20capable%20of%20modeling%20hybrid%20atom-based%20tokens%0Athat%20couple%20discrete%20atom%20types%20with%20continuous%203D%20coordinates.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20InertialAR.%20InertialAR%20devises%20a%20canonical%0Atokenization%20that%20aligns%20molecules%20to%20their%20inertial%20frames%20and%20reorders%20atoms%0Ato%20ensure%20SE%283%29%20and%20permutation%20invariance.%20Moreover%2C%20InertialAR%20equips%20the%0Aattention%20mechanism%20with%20geometric%20awareness%20via%20geometric%20rotary%20positional%0Aencoding%20%28GeoRoPE%29.%20In%20addition%2C%20it%20utilizes%20a%20hierarchical%20autoregressive%0Aparadigm%20to%20predict%20the%20next%20atom-based%20token%2C%20predicting%20the%20atom%20type%20first%0Aand%20then%20its%203D%20coordinates%20via%20Diffusion%20loss.%20Experimentally%2C%20InertialAR%0Aachieves%20state-of-the-art%20performance%20on%207%20of%20the%2010%20evaluation%20metrics%20for%0Aunconditional%20molecule%20generation%20across%20QM9%2C%20GEOM-Drugs%2C%20and%20B3LYP.%20Moreover%2C%0Ait%20significantly%20outperforms%20strong%20baselines%20in%20controllable%20generation%20for%0Atargeted%20chemical%20functionality%2C%20attaining%20state-of-the-art%20results%20across%20all%0A5%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27497v1&entry.124074799=Read"},
{"title": "Information-Theoretic Greedy Layer-wise Training for Traffic Sign\n  Recognition", "author": "Shuyan Lyu and Zhanzimo Wu and Junliang Du", "abstract": "  Modern deep neural networks (DNNs) are typically trained with a global\ncross-entropy loss in a supervised end-to-end manner: neurons need to store\ntheir outgoing weights; training alternates between a forward pass\n(computation) and a top-down backward pass (learning) which is biologically\nimplausible. Alternatively, greedy layer-wise training eliminates the need for\ncross-entropy loss and backpropagation. By avoiding the computation of\nintermediate gradients and the storage of intermediate outputs, it reduces\nmemory usage and helps mitigate issues such as vanishing or exploding\ngradients. However, most existing layer-wise training approaches have been\nevaluated only on relatively small datasets with simple deep architectures. In\nthis paper, we first systematically analyze the training dynamics of popular\nconvolutional neural networks (CNNs) trained by stochastic gradient descent\n(SGD) through an information-theoretic lens. Our findings reveal that networks\nconverge layer-by-layer from bottom to top and that the flow of information\nadheres to a Markov information bottleneck principle. Building on these\nobservations, we propose a novel layer-wise training approach based on the\nrecently developed deterministic information bottleneck (DIB) and the\nmatrix-based R\\'enyi's $\\alpha$-order entropy functional. Specifically, each\nlayer is trained jointly with an auxiliary classifier that connects directly to\nthe output layer, enabling the learning of minimal sufficient task-relevant\nrepresentations. We empirically validate the effectiveness of our training\nprocedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further\ndemonstrate its applicability to a practical task involving traffic sign\nrecognition. Our approach not only outperforms existing layer-wise training\nbaselines but also achieves performance comparable to SGD.\n", "link": "http://arxiv.org/abs/2510.27651v1", "date": "2025-10-31", "relevancy": 2.7083, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5451}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5408}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Theoretic%20Greedy%20Layer-wise%20Training%20for%20Traffic%20Sign%0A%20%20Recognition&body=Title%3A%20Information-Theoretic%20Greedy%20Layer-wise%20Training%20for%20Traffic%20Sign%0A%20%20Recognition%0AAuthor%3A%20Shuyan%20Lyu%20and%20Zhanzimo%20Wu%20and%20Junliang%20Du%0AAbstract%3A%20%20%20Modern%20deep%20neural%20networks%20%28DNNs%29%20are%20typically%20trained%20with%20a%20global%0Across-entropy%20loss%20in%20a%20supervised%20end-to-end%20manner%3A%20neurons%20need%20to%20store%0Atheir%20outgoing%20weights%3B%20training%20alternates%20between%20a%20forward%20pass%0A%28computation%29%20and%20a%20top-down%20backward%20pass%20%28learning%29%20which%20is%20biologically%0Aimplausible.%20Alternatively%2C%20greedy%20layer-wise%20training%20eliminates%20the%20need%20for%0Across-entropy%20loss%20and%20backpropagation.%20By%20avoiding%20the%20computation%20of%0Aintermediate%20gradients%20and%20the%20storage%20of%20intermediate%20outputs%2C%20it%20reduces%0Amemory%20usage%20and%20helps%20mitigate%20issues%20such%20as%20vanishing%20or%20exploding%0Agradients.%20However%2C%20most%20existing%20layer-wise%20training%20approaches%20have%20been%0Aevaluated%20only%20on%20relatively%20small%20datasets%20with%20simple%20deep%20architectures.%20In%0Athis%20paper%2C%20we%20first%20systematically%20analyze%20the%20training%20dynamics%20of%20popular%0Aconvolutional%20neural%20networks%20%28CNNs%29%20trained%20by%20stochastic%20gradient%20descent%0A%28SGD%29%20through%20an%20information-theoretic%20lens.%20Our%20findings%20reveal%20that%20networks%0Aconverge%20layer-by-layer%20from%20bottom%20to%20top%20and%20that%20the%20flow%20of%20information%0Aadheres%20to%20a%20Markov%20information%20bottleneck%20principle.%20Building%20on%20these%0Aobservations%2C%20we%20propose%20a%20novel%20layer-wise%20training%20approach%20based%20on%20the%0Arecently%20developed%20deterministic%20information%20bottleneck%20%28DIB%29%20and%20the%0Amatrix-based%20R%5C%27enyi%27s%20%24%5Calpha%24-order%20entropy%20functional.%20Specifically%2C%20each%0Alayer%20is%20trained%20jointly%20with%20an%20auxiliary%20classifier%20that%20connects%20directly%20to%0Athe%20output%20layer%2C%20enabling%20the%20learning%20of%20minimal%20sufficient%20task-relevant%0Arepresentations.%20We%20empirically%20validate%20the%20effectiveness%20of%20our%20training%0Aprocedure%20on%20CIFAR-10%20and%20CIFAR-100%20using%20modern%20deep%20CNNs%20and%20further%0Ademonstrate%20its%20applicability%20to%20a%20practical%20task%20involving%20traffic%20sign%0Arecognition.%20Our%20approach%20not%20only%20outperforms%20existing%20layer-wise%20training%0Abaselines%20but%20also%20achieves%20performance%20comparable%20to%20SGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Theoretic%2520Greedy%2520Layer-wise%2520Training%2520for%2520Traffic%2520Sign%250A%2520%2520Recognition%26entry.906535625%3DShuyan%2520Lyu%2520and%2520Zhanzimo%2520Wu%2520and%2520Junliang%2520Du%26entry.1292438233%3D%2520%2520Modern%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520typically%2520trained%2520with%2520a%2520global%250Across-entropy%2520loss%2520in%2520a%2520supervised%2520end-to-end%2520manner%253A%2520neurons%2520need%2520to%2520store%250Atheir%2520outgoing%2520weights%253B%2520training%2520alternates%2520between%2520a%2520forward%2520pass%250A%2528computation%2529%2520and%2520a%2520top-down%2520backward%2520pass%2520%2528learning%2529%2520which%2520is%2520biologically%250Aimplausible.%2520Alternatively%252C%2520greedy%2520layer-wise%2520training%2520eliminates%2520the%2520need%2520for%250Across-entropy%2520loss%2520and%2520backpropagation.%2520By%2520avoiding%2520the%2520computation%2520of%250Aintermediate%2520gradients%2520and%2520the%2520storage%2520of%2520intermediate%2520outputs%252C%2520it%2520reduces%250Amemory%2520usage%2520and%2520helps%2520mitigate%2520issues%2520such%2520as%2520vanishing%2520or%2520exploding%250Agradients.%2520However%252C%2520most%2520existing%2520layer-wise%2520training%2520approaches%2520have%2520been%250Aevaluated%2520only%2520on%2520relatively%2520small%2520datasets%2520with%2520simple%2520deep%2520architectures.%2520In%250Athis%2520paper%252C%2520we%2520first%2520systematically%2520analyze%2520the%2520training%2520dynamics%2520of%2520popular%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520trained%2520by%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529%2520through%2520an%2520information-theoretic%2520lens.%2520Our%2520findings%2520reveal%2520that%2520networks%250Aconverge%2520layer-by-layer%2520from%2520bottom%2520to%2520top%2520and%2520that%2520the%2520flow%2520of%2520information%250Aadheres%2520to%2520a%2520Markov%2520information%2520bottleneck%2520principle.%2520Building%2520on%2520these%250Aobservations%252C%2520we%2520propose%2520a%2520novel%2520layer-wise%2520training%2520approach%2520based%2520on%2520the%250Arecently%2520developed%2520deterministic%2520information%2520bottleneck%2520%2528DIB%2529%2520and%2520the%250Amatrix-based%2520R%255C%2527enyi%2527s%2520%2524%255Calpha%2524-order%2520entropy%2520functional.%2520Specifically%252C%2520each%250Alayer%2520is%2520trained%2520jointly%2520with%2520an%2520auxiliary%2520classifier%2520that%2520connects%2520directly%2520to%250Athe%2520output%2520layer%252C%2520enabling%2520the%2520learning%2520of%2520minimal%2520sufficient%2520task-relevant%250Arepresentations.%2520We%2520empirically%2520validate%2520the%2520effectiveness%2520of%2520our%2520training%250Aprocedure%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520using%2520modern%2520deep%2520CNNs%2520and%2520further%250Ademonstrate%2520its%2520applicability%2520to%2520a%2520practical%2520task%2520involving%2520traffic%2520sign%250Arecognition.%2520Our%2520approach%2520not%2520only%2520outperforms%2520existing%2520layer-wise%2520training%250Abaselines%2520but%2520also%2520achieves%2520performance%2520comparable%2520to%2520SGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Theoretic%20Greedy%20Layer-wise%20Training%20for%20Traffic%20Sign%0A%20%20Recognition&entry.906535625=Shuyan%20Lyu%20and%20Zhanzimo%20Wu%20and%20Junliang%20Du&entry.1292438233=%20%20Modern%20deep%20neural%20networks%20%28DNNs%29%20are%20typically%20trained%20with%20a%20global%0Across-entropy%20loss%20in%20a%20supervised%20end-to-end%20manner%3A%20neurons%20need%20to%20store%0Atheir%20outgoing%20weights%3B%20training%20alternates%20between%20a%20forward%20pass%0A%28computation%29%20and%20a%20top-down%20backward%20pass%20%28learning%29%20which%20is%20biologically%0Aimplausible.%20Alternatively%2C%20greedy%20layer-wise%20training%20eliminates%20the%20need%20for%0Across-entropy%20loss%20and%20backpropagation.%20By%20avoiding%20the%20computation%20of%0Aintermediate%20gradients%20and%20the%20storage%20of%20intermediate%20outputs%2C%20it%20reduces%0Amemory%20usage%20and%20helps%20mitigate%20issues%20such%20as%20vanishing%20or%20exploding%0Agradients.%20However%2C%20most%20existing%20layer-wise%20training%20approaches%20have%20been%0Aevaluated%20only%20on%20relatively%20small%20datasets%20with%20simple%20deep%20architectures.%20In%0Athis%20paper%2C%20we%20first%20systematically%20analyze%20the%20training%20dynamics%20of%20popular%0Aconvolutional%20neural%20networks%20%28CNNs%29%20trained%20by%20stochastic%20gradient%20descent%0A%28SGD%29%20through%20an%20information-theoretic%20lens.%20Our%20findings%20reveal%20that%20networks%0Aconverge%20layer-by-layer%20from%20bottom%20to%20top%20and%20that%20the%20flow%20of%20information%0Aadheres%20to%20a%20Markov%20information%20bottleneck%20principle.%20Building%20on%20these%0Aobservations%2C%20we%20propose%20a%20novel%20layer-wise%20training%20approach%20based%20on%20the%0Arecently%20developed%20deterministic%20information%20bottleneck%20%28DIB%29%20and%20the%0Amatrix-based%20R%5C%27enyi%27s%20%24%5Calpha%24-order%20entropy%20functional.%20Specifically%2C%20each%0Alayer%20is%20trained%20jointly%20with%20an%20auxiliary%20classifier%20that%20connects%20directly%20to%0Athe%20output%20layer%2C%20enabling%20the%20learning%20of%20minimal%20sufficient%20task-relevant%0Arepresentations.%20We%20empirically%20validate%20the%20effectiveness%20of%20our%20training%0Aprocedure%20on%20CIFAR-10%20and%20CIFAR-100%20using%20modern%20deep%20CNNs%20and%20further%0Ademonstrate%20its%20applicability%20to%20a%20practical%20task%20involving%20traffic%20sign%0Arecognition.%20Our%20approach%20not%20only%20outperforms%20existing%20layer-wise%20training%0Abaselines%20but%20also%20achieves%20performance%20comparable%20to%20SGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27651v1&entry.124074799=Read"},
{"title": "ANCHOR: Integrating Adversarial Training with Hard-mined Supervised\n  Contrastive Learning for Robust Representation Learning", "author": "Samarup Bhattacharya and Anubhab Bhattacharya and Abir Chakraborty", "abstract": "  Neural networks have changed the way machines interpret the world. At their\ncore, they learn by following gradients, adjusting their parameters step by\nstep until they identify the most discriminant patterns in the data. This\nprocess gives them their strength, yet it also opens the door to a hidden flaw.\nThe very gradients that help a model learn can also be used to produce small,\nimperceptible tweaks that cause the model to completely alter its decision.\nSuch tweaks are called adversarial attacks. These attacks exploit this\nvulnerability by adding tiny, imperceptible changes to images that, while\nleaving them identical to the human eye, cause the model to make wrong\npredictions. In this work, we propose Adversarially-trained Contrastive\nHard-mining for Optimized Robustness (ANCHOR), a framework that leverages the\npower of supervised contrastive learning with explicit hard positive mining to\nenable the model to learn representations for images such that the embeddings\nfor the images, their augmentations, and their perturbed versions cluster\ntogether in the embedding space along with those for other images of the same\nclass while being separated from images of other classes. This alignment helps\nthe model focus on stable, meaningful patterns rather than fragile gradient\ncues. On CIFAR-10, our approach achieves impressive results for both clean and\nrobust accuracy under PGD-20 (epsilon = 0.031), outperforming standard\nadversarial training methods. Our results indicate that combining adversarial\nguidance with hard-mined contrastive supervision helps models learn more\nstructured and robust representations, narrowing the gap between accuracy and\nrobustness.\n", "link": "http://arxiv.org/abs/2510.27599v1", "date": "2025-10-31", "relevancy": 2.7066, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANCHOR%3A%20Integrating%20Adversarial%20Training%20with%20Hard-mined%20Supervised%0A%20%20Contrastive%20Learning%20for%20Robust%20Representation%20Learning&body=Title%3A%20ANCHOR%3A%20Integrating%20Adversarial%20Training%20with%20Hard-mined%20Supervised%0A%20%20Contrastive%20Learning%20for%20Robust%20Representation%20Learning%0AAuthor%3A%20Samarup%20Bhattacharya%20and%20Anubhab%20Bhattacharya%20and%20Abir%20Chakraborty%0AAbstract%3A%20%20%20Neural%20networks%20have%20changed%20the%20way%20machines%20interpret%20the%20world.%20At%20their%0Acore%2C%20they%20learn%20by%20following%20gradients%2C%20adjusting%20their%20parameters%20step%20by%0Astep%20until%20they%20identify%20the%20most%20discriminant%20patterns%20in%20the%20data.%20This%0Aprocess%20gives%20them%20their%20strength%2C%20yet%20it%20also%20opens%20the%20door%20to%20a%20hidden%20flaw.%0AThe%20very%20gradients%20that%20help%20a%20model%20learn%20can%20also%20be%20used%20to%20produce%20small%2C%0Aimperceptible%20tweaks%20that%20cause%20the%20model%20to%20completely%20alter%20its%20decision.%0ASuch%20tweaks%20are%20called%20adversarial%20attacks.%20These%20attacks%20exploit%20this%0Avulnerability%20by%20adding%20tiny%2C%20imperceptible%20changes%20to%20images%20that%2C%20while%0Aleaving%20them%20identical%20to%20the%20human%20eye%2C%20cause%20the%20model%20to%20make%20wrong%0Apredictions.%20In%20this%20work%2C%20we%20propose%20Adversarially-trained%20Contrastive%0AHard-mining%20for%20Optimized%20Robustness%20%28ANCHOR%29%2C%20a%20framework%20that%20leverages%20the%0Apower%20of%20supervised%20contrastive%20learning%20with%20explicit%20hard%20positive%20mining%20to%0Aenable%20the%20model%20to%20learn%20representations%20for%20images%20such%20that%20the%20embeddings%0Afor%20the%20images%2C%20their%20augmentations%2C%20and%20their%20perturbed%20versions%20cluster%0Atogether%20in%20the%20embedding%20space%20along%20with%20those%20for%20other%20images%20of%20the%20same%0Aclass%20while%20being%20separated%20from%20images%20of%20other%20classes.%20This%20alignment%20helps%0Athe%20model%20focus%20on%20stable%2C%20meaningful%20patterns%20rather%20than%20fragile%20gradient%0Acues.%20On%20CIFAR-10%2C%20our%20approach%20achieves%20impressive%20results%20for%20both%20clean%20and%0Arobust%20accuracy%20under%20PGD-20%20%28epsilon%20%3D%200.031%29%2C%20outperforming%20standard%0Aadversarial%20training%20methods.%20Our%20results%20indicate%20that%20combining%20adversarial%0Aguidance%20with%20hard-mined%20contrastive%20supervision%20helps%20models%20learn%20more%0Astructured%20and%20robust%20representations%2C%20narrowing%20the%20gap%20between%20accuracy%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANCHOR%253A%2520Integrating%2520Adversarial%2520Training%2520with%2520Hard-mined%2520Supervised%250A%2520%2520Contrastive%2520Learning%2520for%2520Robust%2520Representation%2520Learning%26entry.906535625%3DSamarup%2520Bhattacharya%2520and%2520Anubhab%2520Bhattacharya%2520and%2520Abir%2520Chakraborty%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520changed%2520the%2520way%2520machines%2520interpret%2520the%2520world.%2520At%2520their%250Acore%252C%2520they%2520learn%2520by%2520following%2520gradients%252C%2520adjusting%2520their%2520parameters%2520step%2520by%250Astep%2520until%2520they%2520identify%2520the%2520most%2520discriminant%2520patterns%2520in%2520the%2520data.%2520This%250Aprocess%2520gives%2520them%2520their%2520strength%252C%2520yet%2520it%2520also%2520opens%2520the%2520door%2520to%2520a%2520hidden%2520flaw.%250AThe%2520very%2520gradients%2520that%2520help%2520a%2520model%2520learn%2520can%2520also%2520be%2520used%2520to%2520produce%2520small%252C%250Aimperceptible%2520tweaks%2520that%2520cause%2520the%2520model%2520to%2520completely%2520alter%2520its%2520decision.%250ASuch%2520tweaks%2520are%2520called%2520adversarial%2520attacks.%2520These%2520attacks%2520exploit%2520this%250Avulnerability%2520by%2520adding%2520tiny%252C%2520imperceptible%2520changes%2520to%2520images%2520that%252C%2520while%250Aleaving%2520them%2520identical%2520to%2520the%2520human%2520eye%252C%2520cause%2520the%2520model%2520to%2520make%2520wrong%250Apredictions.%2520In%2520this%2520work%252C%2520we%2520propose%2520Adversarially-trained%2520Contrastive%250AHard-mining%2520for%2520Optimized%2520Robustness%2520%2528ANCHOR%2529%252C%2520a%2520framework%2520that%2520leverages%2520the%250Apower%2520of%2520supervised%2520contrastive%2520learning%2520with%2520explicit%2520hard%2520positive%2520mining%2520to%250Aenable%2520the%2520model%2520to%2520learn%2520representations%2520for%2520images%2520such%2520that%2520the%2520embeddings%250Afor%2520the%2520images%252C%2520their%2520augmentations%252C%2520and%2520their%2520perturbed%2520versions%2520cluster%250Atogether%2520in%2520the%2520embedding%2520space%2520along%2520with%2520those%2520for%2520other%2520images%2520of%2520the%2520same%250Aclass%2520while%2520being%2520separated%2520from%2520images%2520of%2520other%2520classes.%2520This%2520alignment%2520helps%250Athe%2520model%2520focus%2520on%2520stable%252C%2520meaningful%2520patterns%2520rather%2520than%2520fragile%2520gradient%250Acues.%2520On%2520CIFAR-10%252C%2520our%2520approach%2520achieves%2520impressive%2520results%2520for%2520both%2520clean%2520and%250Arobust%2520accuracy%2520under%2520PGD-20%2520%2528epsilon%2520%253D%25200.031%2529%252C%2520outperforming%2520standard%250Aadversarial%2520training%2520methods.%2520Our%2520results%2520indicate%2520that%2520combining%2520adversarial%250Aguidance%2520with%2520hard-mined%2520contrastive%2520supervision%2520helps%2520models%2520learn%2520more%250Astructured%2520and%2520robust%2520representations%252C%2520narrowing%2520the%2520gap%2520between%2520accuracy%2520and%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANCHOR%3A%20Integrating%20Adversarial%20Training%20with%20Hard-mined%20Supervised%0A%20%20Contrastive%20Learning%20for%20Robust%20Representation%20Learning&entry.906535625=Samarup%20Bhattacharya%20and%20Anubhab%20Bhattacharya%20and%20Abir%20Chakraborty&entry.1292438233=%20%20Neural%20networks%20have%20changed%20the%20way%20machines%20interpret%20the%20world.%20At%20their%0Acore%2C%20they%20learn%20by%20following%20gradients%2C%20adjusting%20their%20parameters%20step%20by%0Astep%20until%20they%20identify%20the%20most%20discriminant%20patterns%20in%20the%20data.%20This%0Aprocess%20gives%20them%20their%20strength%2C%20yet%20it%20also%20opens%20the%20door%20to%20a%20hidden%20flaw.%0AThe%20very%20gradients%20that%20help%20a%20model%20learn%20can%20also%20be%20used%20to%20produce%20small%2C%0Aimperceptible%20tweaks%20that%20cause%20the%20model%20to%20completely%20alter%20its%20decision.%0ASuch%20tweaks%20are%20called%20adversarial%20attacks.%20These%20attacks%20exploit%20this%0Avulnerability%20by%20adding%20tiny%2C%20imperceptible%20changes%20to%20images%20that%2C%20while%0Aleaving%20them%20identical%20to%20the%20human%20eye%2C%20cause%20the%20model%20to%20make%20wrong%0Apredictions.%20In%20this%20work%2C%20we%20propose%20Adversarially-trained%20Contrastive%0AHard-mining%20for%20Optimized%20Robustness%20%28ANCHOR%29%2C%20a%20framework%20that%20leverages%20the%0Apower%20of%20supervised%20contrastive%20learning%20with%20explicit%20hard%20positive%20mining%20to%0Aenable%20the%20model%20to%20learn%20representations%20for%20images%20such%20that%20the%20embeddings%0Afor%20the%20images%2C%20their%20augmentations%2C%20and%20their%20perturbed%20versions%20cluster%0Atogether%20in%20the%20embedding%20space%20along%20with%20those%20for%20other%20images%20of%20the%20same%0Aclass%20while%20being%20separated%20from%20images%20of%20other%20classes.%20This%20alignment%20helps%0Athe%20model%20focus%20on%20stable%2C%20meaningful%20patterns%20rather%20than%20fragile%20gradient%0Acues.%20On%20CIFAR-10%2C%20our%20approach%20achieves%20impressive%20results%20for%20both%20clean%20and%0Arobust%20accuracy%20under%20PGD-20%20%28epsilon%20%3D%200.031%29%2C%20outperforming%20standard%0Aadversarial%20training%20methods.%20Our%20results%20indicate%20that%20combining%20adversarial%0Aguidance%20with%20hard-mined%20contrastive%20supervision%20helps%20models%20learn%20more%0Astructured%20and%20robust%20representations%2C%20narrowing%20the%20gap%20between%20accuracy%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27599v1&entry.124074799=Read"},
{"title": "Image Hashing via Cross-View Code Alignment in the Age of Foundation\n  Models", "author": "Ilyass Moummad and Kawtar Zaher and Herv\u00e9 Go\u00ebau and Alexis Joly", "abstract": "  Efficient large-scale retrieval requires representations that are both\ncompact and discriminative. Foundation models provide powerful visual and\nmultimodal embeddings, but nearest neighbor search in these high-dimensional\nspaces is computationally expensive. Hashing offers an efficient alternative by\nenabling fast Hamming distance search with binary codes, yet existing\napproaches often rely on complex pipelines, multi-term objectives, designs\nspecialized for a single learning paradigm, and long training times. We\nintroduce CroVCA (Cross-View Code Alignment), a simple and unified principle\nfor learning binary codes that remain consistent across semantically aligned\nviews. A single binary cross-entropy loss enforces alignment, while coding-rate\nmaximization serves as an anti-collapse regularizer to promote balanced and\ndiverse codes. To implement this, we design HashCoder, a lightweight MLP\nhashing network with a final batch normalization layer to enforce balanced\ncodes. HashCoder can be used as a probing head on frozen embeddings or to adapt\nencoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achieves\nstate-of-the-art results in just 5 training epochs. At 16 bits, it particularly\nwell-for instance, unsupervised hashing on COCO completes in under 2 minutes\nand supervised hashing on ImageNet100 in about 3 minutes on a single GPU. These\nresults highlight CroVCA's efficiency, adaptability, and broad applicability.\n", "link": "http://arxiv.org/abs/2510.27584v1", "date": "2025-10-31", "relevancy": 2.6047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Hashing%20via%20Cross-View%20Code%20Alignment%20in%20the%20Age%20of%20Foundation%0A%20%20Models&body=Title%3A%20Image%20Hashing%20via%20Cross-View%20Code%20Alignment%20in%20the%20Age%20of%20Foundation%0A%20%20Models%0AAuthor%3A%20Ilyass%20Moummad%20and%20Kawtar%20Zaher%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Alexis%20Joly%0AAbstract%3A%20%20%20Efficient%20large-scale%20retrieval%20requires%20representations%20that%20are%20both%0Acompact%20and%20discriminative.%20Foundation%20models%20provide%20powerful%20visual%20and%0Amultimodal%20embeddings%2C%20but%20nearest%20neighbor%20search%20in%20these%20high-dimensional%0Aspaces%20is%20computationally%20expensive.%20Hashing%20offers%20an%20efficient%20alternative%20by%0Aenabling%20fast%20Hamming%20distance%20search%20with%20binary%20codes%2C%20yet%20existing%0Aapproaches%20often%20rely%20on%20complex%20pipelines%2C%20multi-term%20objectives%2C%20designs%0Aspecialized%20for%20a%20single%20learning%20paradigm%2C%20and%20long%20training%20times.%20We%0Aintroduce%20CroVCA%20%28Cross-View%20Code%20Alignment%29%2C%20a%20simple%20and%20unified%20principle%0Afor%20learning%20binary%20codes%20that%20remain%20consistent%20across%20semantically%20aligned%0Aviews.%20A%20single%20binary%20cross-entropy%20loss%20enforces%20alignment%2C%20while%20coding-rate%0Amaximization%20serves%20as%20an%20anti-collapse%20regularizer%20to%20promote%20balanced%20and%0Adiverse%20codes.%20To%20implement%20this%2C%20we%20design%20HashCoder%2C%20a%20lightweight%20MLP%0Ahashing%20network%20with%20a%20final%20batch%20normalization%20layer%20to%20enforce%20balanced%0Acodes.%20HashCoder%20can%20be%20used%20as%20a%20probing%20head%20on%20frozen%20embeddings%20or%20to%20adapt%0Aencoders%20efficiently%20via%20LoRA%20fine-tuning.%20Across%20benchmarks%2C%20CroVCA%20achieves%0Astate-of-the-art%20results%20in%20just%205%20training%20epochs.%20At%2016%20bits%2C%20it%20particularly%0Awell-for%20instance%2C%20unsupervised%20hashing%20on%20COCO%20completes%20in%20under%202%20minutes%0Aand%20supervised%20hashing%20on%20ImageNet100%20in%20about%203%20minutes%20on%20a%20single%20GPU.%20These%0Aresults%20highlight%20CroVCA%27s%20efficiency%2C%20adaptability%2C%20and%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Hashing%2520via%2520Cross-View%2520Code%2520Alignment%2520in%2520the%2520Age%2520of%2520Foundation%250A%2520%2520Models%26entry.906535625%3DIlyass%2520Moummad%2520and%2520Kawtar%2520Zaher%2520and%2520Herv%25C3%25A9%2520Go%25C3%25ABau%2520and%2520Alexis%2520Joly%26entry.1292438233%3D%2520%2520Efficient%2520large-scale%2520retrieval%2520requires%2520representations%2520that%2520are%2520both%250Acompact%2520and%2520discriminative.%2520Foundation%2520models%2520provide%2520powerful%2520visual%2520and%250Amultimodal%2520embeddings%252C%2520but%2520nearest%2520neighbor%2520search%2520in%2520these%2520high-dimensional%250Aspaces%2520is%2520computationally%2520expensive.%2520Hashing%2520offers%2520an%2520efficient%2520alternative%2520by%250Aenabling%2520fast%2520Hamming%2520distance%2520search%2520with%2520binary%2520codes%252C%2520yet%2520existing%250Aapproaches%2520often%2520rely%2520on%2520complex%2520pipelines%252C%2520multi-term%2520objectives%252C%2520designs%250Aspecialized%2520for%2520a%2520single%2520learning%2520paradigm%252C%2520and%2520long%2520training%2520times.%2520We%250Aintroduce%2520CroVCA%2520%2528Cross-View%2520Code%2520Alignment%2529%252C%2520a%2520simple%2520and%2520unified%2520principle%250Afor%2520learning%2520binary%2520codes%2520that%2520remain%2520consistent%2520across%2520semantically%2520aligned%250Aviews.%2520A%2520single%2520binary%2520cross-entropy%2520loss%2520enforces%2520alignment%252C%2520while%2520coding-rate%250Amaximization%2520serves%2520as%2520an%2520anti-collapse%2520regularizer%2520to%2520promote%2520balanced%2520and%250Adiverse%2520codes.%2520To%2520implement%2520this%252C%2520we%2520design%2520HashCoder%252C%2520a%2520lightweight%2520MLP%250Ahashing%2520network%2520with%2520a%2520final%2520batch%2520normalization%2520layer%2520to%2520enforce%2520balanced%250Acodes.%2520HashCoder%2520can%2520be%2520used%2520as%2520a%2520probing%2520head%2520on%2520frozen%2520embeddings%2520or%2520to%2520adapt%250Aencoders%2520efficiently%2520via%2520LoRA%2520fine-tuning.%2520Across%2520benchmarks%252C%2520CroVCA%2520achieves%250Astate-of-the-art%2520results%2520in%2520just%25205%2520training%2520epochs.%2520At%252016%2520bits%252C%2520it%2520particularly%250Awell-for%2520instance%252C%2520unsupervised%2520hashing%2520on%2520COCO%2520completes%2520in%2520under%25202%2520minutes%250Aand%2520supervised%2520hashing%2520on%2520ImageNet100%2520in%2520about%25203%2520minutes%2520on%2520a%2520single%2520GPU.%2520These%250Aresults%2520highlight%2520CroVCA%2527s%2520efficiency%252C%2520adaptability%252C%2520and%2520broad%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Hashing%20via%20Cross-View%20Code%20Alignment%20in%20the%20Age%20of%20Foundation%0A%20%20Models&entry.906535625=Ilyass%20Moummad%20and%20Kawtar%20Zaher%20and%20Herv%C3%A9%20Go%C3%ABau%20and%20Alexis%20Joly&entry.1292438233=%20%20Efficient%20large-scale%20retrieval%20requires%20representations%20that%20are%20both%0Acompact%20and%20discriminative.%20Foundation%20models%20provide%20powerful%20visual%20and%0Amultimodal%20embeddings%2C%20but%20nearest%20neighbor%20search%20in%20these%20high-dimensional%0Aspaces%20is%20computationally%20expensive.%20Hashing%20offers%20an%20efficient%20alternative%20by%0Aenabling%20fast%20Hamming%20distance%20search%20with%20binary%20codes%2C%20yet%20existing%0Aapproaches%20often%20rely%20on%20complex%20pipelines%2C%20multi-term%20objectives%2C%20designs%0Aspecialized%20for%20a%20single%20learning%20paradigm%2C%20and%20long%20training%20times.%20We%0Aintroduce%20CroVCA%20%28Cross-View%20Code%20Alignment%29%2C%20a%20simple%20and%20unified%20principle%0Afor%20learning%20binary%20codes%20that%20remain%20consistent%20across%20semantically%20aligned%0Aviews.%20A%20single%20binary%20cross-entropy%20loss%20enforces%20alignment%2C%20while%20coding-rate%0Amaximization%20serves%20as%20an%20anti-collapse%20regularizer%20to%20promote%20balanced%20and%0Adiverse%20codes.%20To%20implement%20this%2C%20we%20design%20HashCoder%2C%20a%20lightweight%20MLP%0Ahashing%20network%20with%20a%20final%20batch%20normalization%20layer%20to%20enforce%20balanced%0Acodes.%20HashCoder%20can%20be%20used%20as%20a%20probing%20head%20on%20frozen%20embeddings%20or%20to%20adapt%0Aencoders%20efficiently%20via%20LoRA%20fine-tuning.%20Across%20benchmarks%2C%20CroVCA%20achieves%0Astate-of-the-art%20results%20in%20just%205%20training%20epochs.%20At%2016%20bits%2C%20it%20particularly%0Awell-for%20instance%2C%20unsupervised%20hashing%20on%20COCO%20completes%20in%20under%202%20minutes%0Aand%20supervised%20hashing%20on%20ImageNet100%20in%20about%203%20minutes%20on%20a%20single%20GPU.%20These%0Aresults%20highlight%20CroVCA%27s%20efficiency%2C%20adaptability%2C%20and%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27584v1&entry.124074799=Read"},
{"title": "MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map\n  Images and Time Series", "author": "Xue Xia and Randall Balestriero and Tao Zhang and Yixin Zhou and Andrew Ding and Dev Saini and Lorenz Hurni", "abstract": "  Historical maps are unique and valuable archives that document geographic\nfeatures across different time periods. However, automated analysis of\nhistorical map images remains a significant challenge due to their wide\nstylistic variability and the scarcity of annotated training data. Constructing\nlinked spatio-temporal datasets from historical map time series is even more\ntime-consuming and labor-intensive, as it requires synthesizing information\nfrom multiple maps. Such datasets are essential for applications such as dating\nbuildings, analyzing the development of road networks and settlements, studying\nenvironmental changes etc. We present MapSAM2, a unified framework for\nautomatically segmenting both historical map images and time series. Built on a\nvisual foundation model, MapSAM2 adapts to diverse segmentation tasks with\nfew-shot fine-tuning. Our key innovation is to treat both historical map images\nand time series as videos. For images, we process a set of tiles as a video,\nenabling the memory attention mechanism to incorporate contextual cues from\nsimilar tiles, leading to improved geometric accuracy, particularly for areal\nfeatures. For time series, we introduce the annotated Siegfried Building Time\nSeries Dataset and, to reduce annotation costs, propose generating pseudo time\nseries from single-year maps by simulating common temporal transformations.\nExperimental results show that MapSAM2 learns temporal associations effectively\nand can accurately segment and link buildings in time series under limited\nsupervision or using pseudo videos. We will release both our dataset and code\nto support future research.\n", "link": "http://arxiv.org/abs/2510.27547v1", "date": "2025-10-31", "relevancy": 2.5773, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5163}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapSAM2%3A%20Adapting%20SAM2%20for%20Automatic%20Segmentation%20of%20Historical%20Map%0A%20%20Images%20and%20Time%20Series&body=Title%3A%20MapSAM2%3A%20Adapting%20SAM2%20for%20Automatic%20Segmentation%20of%20Historical%20Map%0A%20%20Images%20and%20Time%20Series%0AAuthor%3A%20Xue%20Xia%20and%20Randall%20Balestriero%20and%20Tao%20Zhang%20and%20Yixin%20Zhou%20and%20Andrew%20Ding%20and%20Dev%20Saini%20and%20Lorenz%20Hurni%0AAbstract%3A%20%20%20Historical%20maps%20are%20unique%20and%20valuable%20archives%20that%20document%20geographic%0Afeatures%20across%20different%20time%20periods.%20However%2C%20automated%20analysis%20of%0Ahistorical%20map%20images%20remains%20a%20significant%20challenge%20due%20to%20their%20wide%0Astylistic%20variability%20and%20the%20scarcity%20of%20annotated%20training%20data.%20Constructing%0Alinked%20spatio-temporal%20datasets%20from%20historical%20map%20time%20series%20is%20even%20more%0Atime-consuming%20and%20labor-intensive%2C%20as%20it%20requires%20synthesizing%20information%0Afrom%20multiple%20maps.%20Such%20datasets%20are%20essential%20for%20applications%20such%20as%20dating%0Abuildings%2C%20analyzing%20the%20development%20of%20road%20networks%20and%20settlements%2C%20studying%0Aenvironmental%20changes%20etc.%20We%20present%20MapSAM2%2C%20a%20unified%20framework%20for%0Aautomatically%20segmenting%20both%20historical%20map%20images%20and%20time%20series.%20Built%20on%20a%0Avisual%20foundation%20model%2C%20MapSAM2%20adapts%20to%20diverse%20segmentation%20tasks%20with%0Afew-shot%20fine-tuning.%20Our%20key%20innovation%20is%20to%20treat%20both%20historical%20map%20images%0Aand%20time%20series%20as%20videos.%20For%20images%2C%20we%20process%20a%20set%20of%20tiles%20as%20a%20video%2C%0Aenabling%20the%20memory%20attention%20mechanism%20to%20incorporate%20contextual%20cues%20from%0Asimilar%20tiles%2C%20leading%20to%20improved%20geometric%20accuracy%2C%20particularly%20for%20areal%0Afeatures.%20For%20time%20series%2C%20we%20introduce%20the%20annotated%20Siegfried%20Building%20Time%0ASeries%20Dataset%20and%2C%20to%20reduce%20annotation%20costs%2C%20propose%20generating%20pseudo%20time%0Aseries%20from%20single-year%20maps%20by%20simulating%20common%20temporal%20transformations.%0AExperimental%20results%20show%20that%20MapSAM2%20learns%20temporal%20associations%20effectively%0Aand%20can%20accurately%20segment%20and%20link%20buildings%20in%20time%20series%20under%20limited%0Asupervision%20or%20using%20pseudo%20videos.%20We%20will%20release%20both%20our%20dataset%20and%20code%0Ato%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapSAM2%253A%2520Adapting%2520SAM2%2520for%2520Automatic%2520Segmentation%2520of%2520Historical%2520Map%250A%2520%2520Images%2520and%2520Time%2520Series%26entry.906535625%3DXue%2520Xia%2520and%2520Randall%2520Balestriero%2520and%2520Tao%2520Zhang%2520and%2520Yixin%2520Zhou%2520and%2520Andrew%2520Ding%2520and%2520Dev%2520Saini%2520and%2520Lorenz%2520Hurni%26entry.1292438233%3D%2520%2520Historical%2520maps%2520are%2520unique%2520and%2520valuable%2520archives%2520that%2520document%2520geographic%250Afeatures%2520across%2520different%2520time%2520periods.%2520However%252C%2520automated%2520analysis%2520of%250Ahistorical%2520map%2520images%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520their%2520wide%250Astylistic%2520variability%2520and%2520the%2520scarcity%2520of%2520annotated%2520training%2520data.%2520Constructing%250Alinked%2520spatio-temporal%2520datasets%2520from%2520historical%2520map%2520time%2520series%2520is%2520even%2520more%250Atime-consuming%2520and%2520labor-intensive%252C%2520as%2520it%2520requires%2520synthesizing%2520information%250Afrom%2520multiple%2520maps.%2520Such%2520datasets%2520are%2520essential%2520for%2520applications%2520such%2520as%2520dating%250Abuildings%252C%2520analyzing%2520the%2520development%2520of%2520road%2520networks%2520and%2520settlements%252C%2520studying%250Aenvironmental%2520changes%2520etc.%2520We%2520present%2520MapSAM2%252C%2520a%2520unified%2520framework%2520for%250Aautomatically%2520segmenting%2520both%2520historical%2520map%2520images%2520and%2520time%2520series.%2520Built%2520on%2520a%250Avisual%2520foundation%2520model%252C%2520MapSAM2%2520adapts%2520to%2520diverse%2520segmentation%2520tasks%2520with%250Afew-shot%2520fine-tuning.%2520Our%2520key%2520innovation%2520is%2520to%2520treat%2520both%2520historical%2520map%2520images%250Aand%2520time%2520series%2520as%2520videos.%2520For%2520images%252C%2520we%2520process%2520a%2520set%2520of%2520tiles%2520as%2520a%2520video%252C%250Aenabling%2520the%2520memory%2520attention%2520mechanism%2520to%2520incorporate%2520contextual%2520cues%2520from%250Asimilar%2520tiles%252C%2520leading%2520to%2520improved%2520geometric%2520accuracy%252C%2520particularly%2520for%2520areal%250Afeatures.%2520For%2520time%2520series%252C%2520we%2520introduce%2520the%2520annotated%2520Siegfried%2520Building%2520Time%250ASeries%2520Dataset%2520and%252C%2520to%2520reduce%2520annotation%2520costs%252C%2520propose%2520generating%2520pseudo%2520time%250Aseries%2520from%2520single-year%2520maps%2520by%2520simulating%2520common%2520temporal%2520transformations.%250AExperimental%2520results%2520show%2520that%2520MapSAM2%2520learns%2520temporal%2520associations%2520effectively%250Aand%2520can%2520accurately%2520segment%2520and%2520link%2520buildings%2520in%2520time%2520series%2520under%2520limited%250Asupervision%2520or%2520using%2520pseudo%2520videos.%2520We%2520will%2520release%2520both%2520our%2520dataset%2520and%2520code%250Ato%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapSAM2%3A%20Adapting%20SAM2%20for%20Automatic%20Segmentation%20of%20Historical%20Map%0A%20%20Images%20and%20Time%20Series&entry.906535625=Xue%20Xia%20and%20Randall%20Balestriero%20and%20Tao%20Zhang%20and%20Yixin%20Zhou%20and%20Andrew%20Ding%20and%20Dev%20Saini%20and%20Lorenz%20Hurni&entry.1292438233=%20%20Historical%20maps%20are%20unique%20and%20valuable%20archives%20that%20document%20geographic%0Afeatures%20across%20different%20time%20periods.%20However%2C%20automated%20analysis%20of%0Ahistorical%20map%20images%20remains%20a%20significant%20challenge%20due%20to%20their%20wide%0Astylistic%20variability%20and%20the%20scarcity%20of%20annotated%20training%20data.%20Constructing%0Alinked%20spatio-temporal%20datasets%20from%20historical%20map%20time%20series%20is%20even%20more%0Atime-consuming%20and%20labor-intensive%2C%20as%20it%20requires%20synthesizing%20information%0Afrom%20multiple%20maps.%20Such%20datasets%20are%20essential%20for%20applications%20such%20as%20dating%0Abuildings%2C%20analyzing%20the%20development%20of%20road%20networks%20and%20settlements%2C%20studying%0Aenvironmental%20changes%20etc.%20We%20present%20MapSAM2%2C%20a%20unified%20framework%20for%0Aautomatically%20segmenting%20both%20historical%20map%20images%20and%20time%20series.%20Built%20on%20a%0Avisual%20foundation%20model%2C%20MapSAM2%20adapts%20to%20diverse%20segmentation%20tasks%20with%0Afew-shot%20fine-tuning.%20Our%20key%20innovation%20is%20to%20treat%20both%20historical%20map%20images%0Aand%20time%20series%20as%20videos.%20For%20images%2C%20we%20process%20a%20set%20of%20tiles%20as%20a%20video%2C%0Aenabling%20the%20memory%20attention%20mechanism%20to%20incorporate%20contextual%20cues%20from%0Asimilar%20tiles%2C%20leading%20to%20improved%20geometric%20accuracy%2C%20particularly%20for%20areal%0Afeatures.%20For%20time%20series%2C%20we%20introduce%20the%20annotated%20Siegfried%20Building%20Time%0ASeries%20Dataset%20and%2C%20to%20reduce%20annotation%20costs%2C%20propose%20generating%20pseudo%20time%0Aseries%20from%20single-year%20maps%20by%20simulating%20common%20temporal%20transformations.%0AExperimental%20results%20show%20that%20MapSAM2%20learns%20temporal%20associations%20effectively%0Aand%20can%20accurately%20segment%20and%20link%20buildings%20in%20time%20series%20under%20limited%0Asupervision%20or%20using%20pseudo%20videos.%20We%20will%20release%20both%20our%20dataset%20and%20code%0Ato%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27547v1&entry.124074799=Read"},
{"title": "Leveraging Generic Time Series Foundation Models for EEG Classification", "author": "Th\u00e9o Gnassounou and Yessin Moakher and Shifeng Xie and Vasilii Feofanov and Ievgen Redko", "abstract": "  Foundation models for time series are emerging as powerful general-purpose\nbackbones, yet their potential for domain-specific biomedical signals such as\nelectroencephalography (EEG) remains rather unexplored. In this work, we\ninvestigate the applicability a recently proposed time series classification\nfoundation model, to a different EEG tasks such as motor imagery classification\nand sleep stage prediction. We test two pretraining regimes: (a) pretraining on\nheterogeneous real-world time series from multiple domains, and (b) pretraining\non purely synthetic data. We find that both variants yield strong performance,\nconsistently outperforming EEGNet, a widely used convolutional baseline, and\nCBraMod, the most recent EEG-specific foundation model. These results suggest\nthat generalist time series foundation models, even when pretrained on data of\nnon-neural origin or on synthetic signals, can transfer effectively to EEG. Our\nfindings highlight the promise of leveraging cross-domain pretrained models for\nbrain signal analysis, suggesting that EEG may benefit from advances in the\nbroader time series literature.\n", "link": "http://arxiv.org/abs/2510.27522v1", "date": "2025-10-31", "relevancy": 2.5286, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Generic%20Time%20Series%20Foundation%20Models%20for%20EEG%20Classification&body=Title%3A%20Leveraging%20Generic%20Time%20Series%20Foundation%20Models%20for%20EEG%20Classification%0AAuthor%3A%20Th%C3%A9o%20Gnassounou%20and%20Yessin%20Moakher%20and%20Shifeng%20Xie%20and%20Vasilii%20Feofanov%20and%20Ievgen%20Redko%0AAbstract%3A%20%20%20Foundation%20models%20for%20time%20series%20are%20emerging%20as%20powerful%20general-purpose%0Abackbones%2C%20yet%20their%20potential%20for%20domain-specific%20biomedical%20signals%20such%20as%0Aelectroencephalography%20%28EEG%29%20remains%20rather%20unexplored.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20applicability%20a%20recently%20proposed%20time%20series%20classification%0Afoundation%20model%2C%20to%20a%20different%20EEG%20tasks%20such%20as%20motor%20imagery%20classification%0Aand%20sleep%20stage%20prediction.%20We%20test%20two%20pretraining%20regimes%3A%20%28a%29%20pretraining%20on%0Aheterogeneous%20real-world%20time%20series%20from%20multiple%20domains%2C%20and%20%28b%29%20pretraining%0Aon%20purely%20synthetic%20data.%20We%20find%20that%20both%20variants%20yield%20strong%20performance%2C%0Aconsistently%20outperforming%20EEGNet%2C%20a%20widely%20used%20convolutional%20baseline%2C%20and%0ACBraMod%2C%20the%20most%20recent%20EEG-specific%20foundation%20model.%20These%20results%20suggest%0Athat%20generalist%20time%20series%20foundation%20models%2C%20even%20when%20pretrained%20on%20data%20of%0Anon-neural%20origin%20or%20on%20synthetic%20signals%2C%20can%20transfer%20effectively%20to%20EEG.%20Our%0Afindings%20highlight%20the%20promise%20of%20leveraging%20cross-domain%20pretrained%20models%20for%0Abrain%20signal%20analysis%2C%20suggesting%20that%20EEG%20may%20benefit%20from%20advances%20in%20the%0Abroader%20time%20series%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Generic%2520Time%2520Series%2520Foundation%2520Models%2520for%2520EEG%2520Classification%26entry.906535625%3DTh%25C3%25A9o%2520Gnassounou%2520and%2520Yessin%2520Moakher%2520and%2520Shifeng%2520Xie%2520and%2520Vasilii%2520Feofanov%2520and%2520Ievgen%2520Redko%26entry.1292438233%3D%2520%2520Foundation%2520models%2520for%2520time%2520series%2520are%2520emerging%2520as%2520powerful%2520general-purpose%250Abackbones%252C%2520yet%2520their%2520potential%2520for%2520domain-specific%2520biomedical%2520signals%2520such%2520as%250Aelectroencephalography%2520%2528EEG%2529%2520remains%2520rather%2520unexplored.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520applicability%2520a%2520recently%2520proposed%2520time%2520series%2520classification%250Afoundation%2520model%252C%2520to%2520a%2520different%2520EEG%2520tasks%2520such%2520as%2520motor%2520imagery%2520classification%250Aand%2520sleep%2520stage%2520prediction.%2520We%2520test%2520two%2520pretraining%2520regimes%253A%2520%2528a%2529%2520pretraining%2520on%250Aheterogeneous%2520real-world%2520time%2520series%2520from%2520multiple%2520domains%252C%2520and%2520%2528b%2529%2520pretraining%250Aon%2520purely%2520synthetic%2520data.%2520We%2520find%2520that%2520both%2520variants%2520yield%2520strong%2520performance%252C%250Aconsistently%2520outperforming%2520EEGNet%252C%2520a%2520widely%2520used%2520convolutional%2520baseline%252C%2520and%250ACBraMod%252C%2520the%2520most%2520recent%2520EEG-specific%2520foundation%2520model.%2520These%2520results%2520suggest%250Athat%2520generalist%2520time%2520series%2520foundation%2520models%252C%2520even%2520when%2520pretrained%2520on%2520data%2520of%250Anon-neural%2520origin%2520or%2520on%2520synthetic%2520signals%252C%2520can%2520transfer%2520effectively%2520to%2520EEG.%2520Our%250Afindings%2520highlight%2520the%2520promise%2520of%2520leveraging%2520cross-domain%2520pretrained%2520models%2520for%250Abrain%2520signal%2520analysis%252C%2520suggesting%2520that%2520EEG%2520may%2520benefit%2520from%2520advances%2520in%2520the%250Abroader%2520time%2520series%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Generic%20Time%20Series%20Foundation%20Models%20for%20EEG%20Classification&entry.906535625=Th%C3%A9o%20Gnassounou%20and%20Yessin%20Moakher%20and%20Shifeng%20Xie%20and%20Vasilii%20Feofanov%20and%20Ievgen%20Redko&entry.1292438233=%20%20Foundation%20models%20for%20time%20series%20are%20emerging%20as%20powerful%20general-purpose%0Abackbones%2C%20yet%20their%20potential%20for%20domain-specific%20biomedical%20signals%20such%20as%0Aelectroencephalography%20%28EEG%29%20remains%20rather%20unexplored.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20applicability%20a%20recently%20proposed%20time%20series%20classification%0Afoundation%20model%2C%20to%20a%20different%20EEG%20tasks%20such%20as%20motor%20imagery%20classification%0Aand%20sleep%20stage%20prediction.%20We%20test%20two%20pretraining%20regimes%3A%20%28a%29%20pretraining%20on%0Aheterogeneous%20real-world%20time%20series%20from%20multiple%20domains%2C%20and%20%28b%29%20pretraining%0Aon%20purely%20synthetic%20data.%20We%20find%20that%20both%20variants%20yield%20strong%20performance%2C%0Aconsistently%20outperforming%20EEGNet%2C%20a%20widely%20used%20convolutional%20baseline%2C%20and%0ACBraMod%2C%20the%20most%20recent%20EEG-specific%20foundation%20model.%20These%20results%20suggest%0Athat%20generalist%20time%20series%20foundation%20models%2C%20even%20when%20pretrained%20on%20data%20of%0Anon-neural%20origin%20or%20on%20synthetic%20signals%2C%20can%20transfer%20effectively%20to%20EEG.%20Our%0Afindings%20highlight%20the%20promise%20of%20leveraging%20cross-domain%20pretrained%20models%20for%0Abrain%20signal%20analysis%2C%20suggesting%20that%20EEG%20may%20benefit%20from%20advances%20in%20the%0Abroader%20time%20series%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27522v1&entry.124074799=Read"},
{"title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation", "author": "Kai Lion and Liang Zhang and Bingcong Li and Niao He", "abstract": "  We show that low-rank adaptation of large-scale models suffers from a low\nstable rank that is well below the linear algebraic rank of the subspace,\ndegrading fine-tuning performance. To mitigate the underutilization of the\nallocated subspace, we propose PoLAR, a parameterization inspired by the polar\ndecomposition that factorizes the low-rank update into two direction matrices\nconstrained to Stiefel manifolds and an unconstrained scale matrix. Our theory\nshows that PoLAR yields an exponentially faster convergence rate on a canonical\nlow-rank adaptation problem. Pairing the parameterization with Riemannian\noptimization leads to consistent gains on three different benchmarks testing\ngeneral language understanding, commonsense reasoning, and mathematical problem\nsolving with base model sizes ranging from 350M to 27B.\n", "link": "http://arxiv.org/abs/2506.03133v2", "date": "2025-10-31", "relevancy": 2.4845, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation&body=Title%3A%20PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation%0AAuthor%3A%20Kai%20Lion%20and%20Liang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He%0AAbstract%3A%20%20%20We%20show%20that%20low-rank%20adaptation%20of%20large-scale%20models%20suffers%20from%20a%20low%0Astable%20rank%20that%20is%20well%20below%20the%20linear%20algebraic%20rank%20of%20the%20subspace%2C%0Adegrading%20fine-tuning%20performance.%20To%20mitigate%20the%20underutilization%20of%20the%0Aallocated%20subspace%2C%20we%20propose%20PoLAR%2C%20a%20parameterization%20inspired%20by%20the%20polar%0Adecomposition%20that%20factorizes%20the%20low-rank%20update%20into%20two%20direction%20matrices%0Aconstrained%20to%20Stiefel%20manifolds%20and%20an%20unconstrained%20scale%20matrix.%20Our%20theory%0Ashows%20that%20PoLAR%20yields%20an%20exponentially%20faster%20convergence%20rate%20on%20a%20canonical%0Alow-rank%20adaptation%20problem.%20Pairing%20the%20parameterization%20with%20Riemannian%0Aoptimization%20leads%20to%20consistent%20gains%20on%20three%20different%20benchmarks%20testing%0Ageneral%20language%20understanding%2C%20commonsense%20reasoning%2C%20and%20mathematical%20problem%0Asolving%20with%20base%20model%20sizes%20ranging%20from%20350M%20to%2027B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoLAR%253A%2520Polar-Decomposed%2520Low-Rank%2520Adapter%2520Representation%26entry.906535625%3DKai%2520Lion%2520and%2520Liang%2520Zhang%2520and%2520Bingcong%2520Li%2520and%2520Niao%2520He%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520low-rank%2520adaptation%2520of%2520large-scale%2520models%2520suffers%2520from%2520a%2520low%250Astable%2520rank%2520that%2520is%2520well%2520below%2520the%2520linear%2520algebraic%2520rank%2520of%2520the%2520subspace%252C%250Adegrading%2520fine-tuning%2520performance.%2520To%2520mitigate%2520the%2520underutilization%2520of%2520the%250Aallocated%2520subspace%252C%2520we%2520propose%2520PoLAR%252C%2520a%2520parameterization%2520inspired%2520by%2520the%2520polar%250Adecomposition%2520that%2520factorizes%2520the%2520low-rank%2520update%2520into%2520two%2520direction%2520matrices%250Aconstrained%2520to%2520Stiefel%2520manifolds%2520and%2520an%2520unconstrained%2520scale%2520matrix.%2520Our%2520theory%250Ashows%2520that%2520PoLAR%2520yields%2520an%2520exponentially%2520faster%2520convergence%2520rate%2520on%2520a%2520canonical%250Alow-rank%2520adaptation%2520problem.%2520Pairing%2520the%2520parameterization%2520with%2520Riemannian%250Aoptimization%2520leads%2520to%2520consistent%2520gains%2520on%2520three%2520different%2520benchmarks%2520testing%250Ageneral%2520language%2520understanding%252C%2520commonsense%2520reasoning%252C%2520and%2520mathematical%2520problem%250Asolving%2520with%2520base%2520model%2520sizes%2520ranging%2520from%2520350M%2520to%252027B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation&entry.906535625=Kai%20Lion%20and%20Liang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He&entry.1292438233=%20%20We%20show%20that%20low-rank%20adaptation%20of%20large-scale%20models%20suffers%20from%20a%20low%0Astable%20rank%20that%20is%20well%20below%20the%20linear%20algebraic%20rank%20of%20the%20subspace%2C%0Adegrading%20fine-tuning%20performance.%20To%20mitigate%20the%20underutilization%20of%20the%0Aallocated%20subspace%2C%20we%20propose%20PoLAR%2C%20a%20parameterization%20inspired%20by%20the%20polar%0Adecomposition%20that%20factorizes%20the%20low-rank%20update%20into%20two%20direction%20matrices%0Aconstrained%20to%20Stiefel%20manifolds%20and%20an%20unconstrained%20scale%20matrix.%20Our%20theory%0Ashows%20that%20PoLAR%20yields%20an%20exponentially%20faster%20convergence%20rate%20on%20a%20canonical%0Alow-rank%20adaptation%20problem.%20Pairing%20the%20parameterization%20with%20Riemannian%0Aoptimization%20leads%20to%20consistent%20gains%20on%20three%20different%20benchmarks%20testing%0Ageneral%20language%20understanding%2C%20commonsense%20reasoning%2C%20and%20mathematical%20problem%0Asolving%20with%20base%20model%20sizes%20ranging%20from%20350M%20to%2027B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03133v2&entry.124074799=Read"},
{"title": "Generative AI and Firm Productivity: Field Experiments in Online Retail", "author": "Lu Fang and Zhe Yuan and Kaifu Zhang and Dante Donati and Miklos Sarvary", "abstract": "  We quantify the impact of Generative Artificial Intelligence (GenAI) on firm\nproductivity through a series of large-scale randomized field experiments\ninvolving millions of users and products at a leading cross-border online\nretail platform. Over six months in 2023-2024, GenAI-based enhancements were\nintegrated into seven consumer-facing business workflows. We find that GenAI\nadoption significantly increases sales, with treatment effects ranging from\n$0\\%$ to $16.3\\%$, depending on GenAI's marginal contribution relative to\nexisting firm practices. Because inputs and prices were held constant across\nexperimental arms, these gains map directly into total factor productivity\nimprovements. Across the four GenAI applications with positive effects, the\nimplied annual incremental value is approximately $\\$ 5$ per consumer-an\neconomically meaningful impact given the retailer's scale and the early stage\nof GenAI adoption. The primary mechanism operates through higher conversion\nrates, consistent with GenAI reducing frictions in the marketplace and\nimproving consumer experience. We also document substantial heterogeneity:\nsmaller and newer sellers, as well as less experienced consumers, exhibit\ndisproportionately larger gains. Our findings provide novel, large-scale causal\nevidence on the productivity effects of GenAI in online retail, highlighting\nboth its immediate value and broader potential.\n", "link": "http://arxiv.org/abs/2510.12049v2", "date": "2025-10-31", "relevancy": 2.4596, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5152}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4833}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail&body=Title%3A%20Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail%0AAuthor%3A%20Lu%20Fang%20and%20Zhe%20Yuan%20and%20Kaifu%20Zhang%20and%20Dante%20Donati%20and%20Miklos%20Sarvary%0AAbstract%3A%20%20%20We%20quantify%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20on%20firm%0Aproductivity%20through%20a%20series%20of%20large-scale%20randomized%20field%20experiments%0Ainvolving%20millions%20of%20users%20and%20products%20at%20a%20leading%20cross-border%20online%0Aretail%20platform.%20Over%20six%20months%20in%202023-2024%2C%20GenAI-based%20enhancements%20were%0Aintegrated%20into%20seven%20consumer-facing%20business%20workflows.%20We%20find%20that%20GenAI%0Aadoption%20significantly%20increases%20sales%2C%20with%20treatment%20effects%20ranging%20from%0A%240%5C%25%24%20to%20%2416.3%5C%25%24%2C%20depending%20on%20GenAI%27s%20marginal%20contribution%20relative%20to%0Aexisting%20firm%20practices.%20Because%20inputs%20and%20prices%20were%20held%20constant%20across%0Aexperimental%20arms%2C%20these%20gains%20map%20directly%20into%20total%20factor%20productivity%0Aimprovements.%20Across%20the%20four%20GenAI%20applications%20with%20positive%20effects%2C%20the%0Aimplied%20annual%20incremental%20value%20is%20approximately%20%24%5C%24%205%24%20per%20consumer-an%0Aeconomically%20meaningful%20impact%20given%20the%20retailer%27s%20scale%20and%20the%20early%20stage%0Aof%20GenAI%20adoption.%20The%20primary%20mechanism%20operates%20through%20higher%20conversion%0Arates%2C%20consistent%20with%20GenAI%20reducing%20frictions%20in%20the%20marketplace%20and%0Aimproving%20consumer%20experience.%20We%20also%20document%20substantial%20heterogeneity%3A%0Asmaller%20and%20newer%20sellers%2C%20as%20well%20as%20less%20experienced%20consumers%2C%20exhibit%0Adisproportionately%20larger%20gains.%20Our%20findings%20provide%20novel%2C%20large-scale%20causal%0Aevidence%20on%20the%20productivity%20effects%20of%20GenAI%20in%20online%20retail%2C%20highlighting%0Aboth%20its%20immediate%20value%20and%20broader%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.12049v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520and%2520Firm%2520Productivity%253A%2520Field%2520Experiments%2520in%2520Online%2520Retail%26entry.906535625%3DLu%2520Fang%2520and%2520Zhe%2520Yuan%2520and%2520Kaifu%2520Zhang%2520and%2520Dante%2520Donati%2520and%2520Miklos%2520Sarvary%26entry.1292438233%3D%2520%2520We%2520quantify%2520the%2520impact%2520of%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520on%2520firm%250Aproductivity%2520through%2520a%2520series%2520of%2520large-scale%2520randomized%2520field%2520experiments%250Ainvolving%2520millions%2520of%2520users%2520and%2520products%2520at%2520a%2520leading%2520cross-border%2520online%250Aretail%2520platform.%2520Over%2520six%2520months%2520in%25202023-2024%252C%2520GenAI-based%2520enhancements%2520were%250Aintegrated%2520into%2520seven%2520consumer-facing%2520business%2520workflows.%2520We%2520find%2520that%2520GenAI%250Aadoption%2520significantly%2520increases%2520sales%252C%2520with%2520treatment%2520effects%2520ranging%2520from%250A%25240%255C%2525%2524%2520to%2520%252416.3%255C%2525%2524%252C%2520depending%2520on%2520GenAI%2527s%2520marginal%2520contribution%2520relative%2520to%250Aexisting%2520firm%2520practices.%2520Because%2520inputs%2520and%2520prices%2520were%2520held%2520constant%2520across%250Aexperimental%2520arms%252C%2520these%2520gains%2520map%2520directly%2520into%2520total%2520factor%2520productivity%250Aimprovements.%2520Across%2520the%2520four%2520GenAI%2520applications%2520with%2520positive%2520effects%252C%2520the%250Aimplied%2520annual%2520incremental%2520value%2520is%2520approximately%2520%2524%255C%2524%25205%2524%2520per%2520consumer-an%250Aeconomically%2520meaningful%2520impact%2520given%2520the%2520retailer%2527s%2520scale%2520and%2520the%2520early%2520stage%250Aof%2520GenAI%2520adoption.%2520The%2520primary%2520mechanism%2520operates%2520through%2520higher%2520conversion%250Arates%252C%2520consistent%2520with%2520GenAI%2520reducing%2520frictions%2520in%2520the%2520marketplace%2520and%250Aimproving%2520consumer%2520experience.%2520We%2520also%2520document%2520substantial%2520heterogeneity%253A%250Asmaller%2520and%2520newer%2520sellers%252C%2520as%2520well%2520as%2520less%2520experienced%2520consumers%252C%2520exhibit%250Adisproportionately%2520larger%2520gains.%2520Our%2520findings%2520provide%2520novel%252C%2520large-scale%2520causal%250Aevidence%2520on%2520the%2520productivity%2520effects%2520of%2520GenAI%2520in%2520online%2520retail%252C%2520highlighting%250Aboth%2520its%2520immediate%2520value%2520and%2520broader%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12049v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20and%20Firm%20Productivity%3A%20Field%20Experiments%20in%20Online%20Retail&entry.906535625=Lu%20Fang%20and%20Zhe%20Yuan%20and%20Kaifu%20Zhang%20and%20Dante%20Donati%20and%20Miklos%20Sarvary&entry.1292438233=%20%20We%20quantify%20the%20impact%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20on%20firm%0Aproductivity%20through%20a%20series%20of%20large-scale%20randomized%20field%20experiments%0Ainvolving%20millions%20of%20users%20and%20products%20at%20a%20leading%20cross-border%20online%0Aretail%20platform.%20Over%20six%20months%20in%202023-2024%2C%20GenAI-based%20enhancements%20were%0Aintegrated%20into%20seven%20consumer-facing%20business%20workflows.%20We%20find%20that%20GenAI%0Aadoption%20significantly%20increases%20sales%2C%20with%20treatment%20effects%20ranging%20from%0A%240%5C%25%24%20to%20%2416.3%5C%25%24%2C%20depending%20on%20GenAI%27s%20marginal%20contribution%20relative%20to%0Aexisting%20firm%20practices.%20Because%20inputs%20and%20prices%20were%20held%20constant%20across%0Aexperimental%20arms%2C%20these%20gains%20map%20directly%20into%20total%20factor%20productivity%0Aimprovements.%20Across%20the%20four%20GenAI%20applications%20with%20positive%20effects%2C%20the%0Aimplied%20annual%20incremental%20value%20is%20approximately%20%24%5C%24%205%24%20per%20consumer-an%0Aeconomically%20meaningful%20impact%20given%20the%20retailer%27s%20scale%20and%20the%20early%20stage%0Aof%20GenAI%20adoption.%20The%20primary%20mechanism%20operates%20through%20higher%20conversion%0Arates%2C%20consistent%20with%20GenAI%20reducing%20frictions%20in%20the%20marketplace%20and%0Aimproving%20consumer%20experience.%20We%20also%20document%20substantial%20heterogeneity%3A%0Asmaller%20and%20newer%20sellers%2C%20as%20well%20as%20less%20experienced%20consumers%2C%20exhibit%0Adisproportionately%20larger%20gains.%20Our%20findings%20provide%20novel%2C%20large-scale%20causal%0Aevidence%20on%20the%20productivity%20effects%20of%20GenAI%20in%20online%20retail%2C%20highlighting%0Aboth%20its%20immediate%20value%20and%20broader%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.12049v2&entry.124074799=Read"},
{"title": "Augmented Reality-based Guidance with Deformable Registration in Head\n  and Neck Tumor Resection", "author": "Qingyun Yang and Fangjie Li and Jiayi Xu and Zixuan Liu and Sindhura Sridhar and Whitney Jin and Jennifer Du and Jon Heiselman and Michael Miga and Michael Topf and Jie Ying Wu", "abstract": "  Head and neck squamous cell carcinoma (HNSCC) has one of the highest rates of\nrecurrence cases among solid malignancies. Recurrence rates can be reduced by\nimproving positive margins localization. Frozen section analysis (FSA) of\nresected specimens is the gold standard for intraoperative margin assessment.\nHowever, because of the complex 3D anatomy and the significant shrinkage of\nresected specimens, accurate margin relocation from specimen back onto the\nresection site based on FSA results remains challenging. We propose a novel\ndeformable registration framework that uses both the pre-resection upper\nsurface and the post-resection site of the specimen to incorporate thickness\ninformation into the registration process. The proposed method significantly\nimproves target registration error (TRE), demonstrating enhanced adaptability\nto thicker specimens. In tongue specimens, the proposed framework improved TRE\nby up to 33% as compared to prior deformable registration. Notably, tongue\nspecimens exhibit complex 3D anatomies and hold the highest clinical\nsignificance compared to other head and neck specimens from the buccal and\nskin. We analyzed distinct deformation behaviors in different specimens,\nhighlighting the need for tailored deformation strategies. To further aid\nintraoperative visualization, we also integrated this framework with an\naugmented reality-based auto-alignment system. The combined system can\naccurately and automatically overlay the deformed 3D specimen mesh with\npositive margin annotation onto the resection site. With a pilot study of the\nAR guided framework involving two surgeons, the integrated system improved the\nsurgeons' average target relocation error from 9.8 cm to 4.8 cm.\n", "link": "http://arxiv.org/abs/2503.08802v2", "date": "2025-10-31", "relevancy": 2.4267, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4816}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmented%20Reality-based%20Guidance%20with%20Deformable%20Registration%20in%20Head%0A%20%20and%20Neck%20Tumor%20Resection&body=Title%3A%20Augmented%20Reality-based%20Guidance%20with%20Deformable%20Registration%20in%20Head%0A%20%20and%20Neck%20Tumor%20Resection%0AAuthor%3A%20Qingyun%20Yang%20and%20Fangjie%20Li%20and%20Jiayi%20Xu%20and%20Zixuan%20Liu%20and%20Sindhura%20Sridhar%20and%20Whitney%20Jin%20and%20Jennifer%20Du%20and%20Jon%20Heiselman%20and%20Michael%20Miga%20and%20Michael%20Topf%20and%20Jie%20Ying%20Wu%0AAbstract%3A%20%20%20Head%20and%20neck%20squamous%20cell%20carcinoma%20%28HNSCC%29%20has%20one%20of%20the%20highest%20rates%20of%0Arecurrence%20cases%20among%20solid%20malignancies.%20Recurrence%20rates%20can%20be%20reduced%20by%0Aimproving%20positive%20margins%20localization.%20Frozen%20section%20analysis%20%28FSA%29%20of%0Aresected%20specimens%20is%20the%20gold%20standard%20for%20intraoperative%20margin%20assessment.%0AHowever%2C%20because%20of%20the%20complex%203D%20anatomy%20and%20the%20significant%20shrinkage%20of%0Aresected%20specimens%2C%20accurate%20margin%20relocation%20from%20specimen%20back%20onto%20the%0Aresection%20site%20based%20on%20FSA%20results%20remains%20challenging.%20We%20propose%20a%20novel%0Adeformable%20registration%20framework%20that%20uses%20both%20the%20pre-resection%20upper%0Asurface%20and%20the%20post-resection%20site%20of%20the%20specimen%20to%20incorporate%20thickness%0Ainformation%20into%20the%20registration%20process.%20The%20proposed%20method%20significantly%0Aimproves%20target%20registration%20error%20%28TRE%29%2C%20demonstrating%20enhanced%20adaptability%0Ato%20thicker%20specimens.%20In%20tongue%20specimens%2C%20the%20proposed%20framework%20improved%20TRE%0Aby%20up%20to%2033%25%20as%20compared%20to%20prior%20deformable%20registration.%20Notably%2C%20tongue%0Aspecimens%20exhibit%20complex%203D%20anatomies%20and%20hold%20the%20highest%20clinical%0Asignificance%20compared%20to%20other%20head%20and%20neck%20specimens%20from%20the%20buccal%20and%0Askin.%20We%20analyzed%20distinct%20deformation%20behaviors%20in%20different%20specimens%2C%0Ahighlighting%20the%20need%20for%20tailored%20deformation%20strategies.%20To%20further%20aid%0Aintraoperative%20visualization%2C%20we%20also%20integrated%20this%20framework%20with%20an%0Aaugmented%20reality-based%20auto-alignment%20system.%20The%20combined%20system%20can%0Aaccurately%20and%20automatically%20overlay%20the%20deformed%203D%20specimen%20mesh%20with%0Apositive%20margin%20annotation%20onto%20the%20resection%20site.%20With%20a%20pilot%20study%20of%20the%0AAR%20guided%20framework%20involving%20two%20surgeons%2C%20the%20integrated%20system%20improved%20the%0Asurgeons%27%20average%20target%20relocation%20error%20from%209.8%20cm%20to%204.8%20cm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08802v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmented%2520Reality-based%2520Guidance%2520with%2520Deformable%2520Registration%2520in%2520Head%250A%2520%2520and%2520Neck%2520Tumor%2520Resection%26entry.906535625%3DQingyun%2520Yang%2520and%2520Fangjie%2520Li%2520and%2520Jiayi%2520Xu%2520and%2520Zixuan%2520Liu%2520and%2520Sindhura%2520Sridhar%2520and%2520Whitney%2520Jin%2520and%2520Jennifer%2520Du%2520and%2520Jon%2520Heiselman%2520and%2520Michael%2520Miga%2520and%2520Michael%2520Topf%2520and%2520Jie%2520Ying%2520Wu%26entry.1292438233%3D%2520%2520Head%2520and%2520neck%2520squamous%2520cell%2520carcinoma%2520%2528HNSCC%2529%2520has%2520one%2520of%2520the%2520highest%2520rates%2520of%250Arecurrence%2520cases%2520among%2520solid%2520malignancies.%2520Recurrence%2520rates%2520can%2520be%2520reduced%2520by%250Aimproving%2520positive%2520margins%2520localization.%2520Frozen%2520section%2520analysis%2520%2528FSA%2529%2520of%250Aresected%2520specimens%2520is%2520the%2520gold%2520standard%2520for%2520intraoperative%2520margin%2520assessment.%250AHowever%252C%2520because%2520of%2520the%2520complex%25203D%2520anatomy%2520and%2520the%2520significant%2520shrinkage%2520of%250Aresected%2520specimens%252C%2520accurate%2520margin%2520relocation%2520from%2520specimen%2520back%2520onto%2520the%250Aresection%2520site%2520based%2520on%2520FSA%2520results%2520remains%2520challenging.%2520We%2520propose%2520a%2520novel%250Adeformable%2520registration%2520framework%2520that%2520uses%2520both%2520the%2520pre-resection%2520upper%250Asurface%2520and%2520the%2520post-resection%2520site%2520of%2520the%2520specimen%2520to%2520incorporate%2520thickness%250Ainformation%2520into%2520the%2520registration%2520process.%2520The%2520proposed%2520method%2520significantly%250Aimproves%2520target%2520registration%2520error%2520%2528TRE%2529%252C%2520demonstrating%2520enhanced%2520adaptability%250Ato%2520thicker%2520specimens.%2520In%2520tongue%2520specimens%252C%2520the%2520proposed%2520framework%2520improved%2520TRE%250Aby%2520up%2520to%252033%2525%2520as%2520compared%2520to%2520prior%2520deformable%2520registration.%2520Notably%252C%2520tongue%250Aspecimens%2520exhibit%2520complex%25203D%2520anatomies%2520and%2520hold%2520the%2520highest%2520clinical%250Asignificance%2520compared%2520to%2520other%2520head%2520and%2520neck%2520specimens%2520from%2520the%2520buccal%2520and%250Askin.%2520We%2520analyzed%2520distinct%2520deformation%2520behaviors%2520in%2520different%2520specimens%252C%250Ahighlighting%2520the%2520need%2520for%2520tailored%2520deformation%2520strategies.%2520To%2520further%2520aid%250Aintraoperative%2520visualization%252C%2520we%2520also%2520integrated%2520this%2520framework%2520with%2520an%250Aaugmented%2520reality-based%2520auto-alignment%2520system.%2520The%2520combined%2520system%2520can%250Aaccurately%2520and%2520automatically%2520overlay%2520the%2520deformed%25203D%2520specimen%2520mesh%2520with%250Apositive%2520margin%2520annotation%2520onto%2520the%2520resection%2520site.%2520With%2520a%2520pilot%2520study%2520of%2520the%250AAR%2520guided%2520framework%2520involving%2520two%2520surgeons%252C%2520the%2520integrated%2520system%2520improved%2520the%250Asurgeons%2527%2520average%2520target%2520relocation%2520error%2520from%25209.8%2520cm%2520to%25204.8%2520cm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08802v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmented%20Reality-based%20Guidance%20with%20Deformable%20Registration%20in%20Head%0A%20%20and%20Neck%20Tumor%20Resection&entry.906535625=Qingyun%20Yang%20and%20Fangjie%20Li%20and%20Jiayi%20Xu%20and%20Zixuan%20Liu%20and%20Sindhura%20Sridhar%20and%20Whitney%20Jin%20and%20Jennifer%20Du%20and%20Jon%20Heiselman%20and%20Michael%20Miga%20and%20Michael%20Topf%20and%20Jie%20Ying%20Wu&entry.1292438233=%20%20Head%20and%20neck%20squamous%20cell%20carcinoma%20%28HNSCC%29%20has%20one%20of%20the%20highest%20rates%20of%0Arecurrence%20cases%20among%20solid%20malignancies.%20Recurrence%20rates%20can%20be%20reduced%20by%0Aimproving%20positive%20margins%20localization.%20Frozen%20section%20analysis%20%28FSA%29%20of%0Aresected%20specimens%20is%20the%20gold%20standard%20for%20intraoperative%20margin%20assessment.%0AHowever%2C%20because%20of%20the%20complex%203D%20anatomy%20and%20the%20significant%20shrinkage%20of%0Aresected%20specimens%2C%20accurate%20margin%20relocation%20from%20specimen%20back%20onto%20the%0Aresection%20site%20based%20on%20FSA%20results%20remains%20challenging.%20We%20propose%20a%20novel%0Adeformable%20registration%20framework%20that%20uses%20both%20the%20pre-resection%20upper%0Asurface%20and%20the%20post-resection%20site%20of%20the%20specimen%20to%20incorporate%20thickness%0Ainformation%20into%20the%20registration%20process.%20The%20proposed%20method%20significantly%0Aimproves%20target%20registration%20error%20%28TRE%29%2C%20demonstrating%20enhanced%20adaptability%0Ato%20thicker%20specimens.%20In%20tongue%20specimens%2C%20the%20proposed%20framework%20improved%20TRE%0Aby%20up%20to%2033%25%20as%20compared%20to%20prior%20deformable%20registration.%20Notably%2C%20tongue%0Aspecimens%20exhibit%20complex%203D%20anatomies%20and%20hold%20the%20highest%20clinical%0Asignificance%20compared%20to%20other%20head%20and%20neck%20specimens%20from%20the%20buccal%20and%0Askin.%20We%20analyzed%20distinct%20deformation%20behaviors%20in%20different%20specimens%2C%0Ahighlighting%20the%20need%20for%20tailored%20deformation%20strategies.%20To%20further%20aid%0Aintraoperative%20visualization%2C%20we%20also%20integrated%20this%20framework%20with%20an%0Aaugmented%20reality-based%20auto-alignment%20system.%20The%20combined%20system%20can%0Aaccurately%20and%20automatically%20overlay%20the%20deformed%203D%20specimen%20mesh%20with%0Apositive%20margin%20annotation%20onto%20the%20resection%20site.%20With%20a%20pilot%20study%20of%20the%0AAR%20guided%20framework%20involving%20two%20surgeons%2C%20the%20integrated%20system%20improved%20the%0Asurgeons%27%20average%20target%20relocation%20error%20from%209.8%20cm%20to%204.8%20cm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08802v2&entry.124074799=Read"},
{"title": "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm", "author": "Nanyu Luo and Feng Ji", "abstract": "  Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis.\n", "link": "http://arxiv.org/abs/2502.10650v3", "date": "2025-10-31", "relevancy": 2.4077, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4967}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4828}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20for%20High-Dimensional%20Item%20Factor%0A%20%20Analysis%3A%20A%20Deep%20Adversarial%20Learning%20Algorithm&body=Title%3A%20Generative%20Adversarial%20Networks%20for%20High-Dimensional%20Item%20Factor%0A%20%20Analysis%3A%20A%20Deep%20Adversarial%20Learning%20Algorithm%0AAuthor%3A%20Nanyu%20Luo%20and%20Feng%20Ji%0AAbstract%3A%20%20%20Advances%20in%20deep%20learning%20and%20representation%20learning%20have%20transformed%20item%0Afactor%20analysis%20%28IFA%29%20in%20the%20item%20response%20theory%20%28IRT%29%20literature%20by%20enabling%0Amore%20efficient%20and%20accurate%20parameter%20estimation.%20Variational%20Autoencoders%0A%28VAEs%29%20have%20been%20one%20of%20the%20most%20impactful%20techniques%20in%20modeling%0Ahigh-dimensional%20latent%20variables%20in%20this%20context.%20However%2C%20the%20limited%0Aexpressiveness%20of%20the%20inference%20model%20based%20on%20traditional%20VAEs%20can%20still%0Ahinder%20the%20estimation%20performance.%20We%20introduce%20Adversarial%20Variational%20Bayes%0A%28AVB%29%20algorithms%20as%20an%20improvement%20to%20VAEs%20for%20IFA%20with%20improved%20flexibility%0Aand%20accuracy.%20By%20bridging%20the%20strengths%20of%20VAEs%20and%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20AVB%20incorporates%20an%20auxiliary%20discriminator%20network%20to%20reframe%0Athe%20estimation%20process%20as%20a%20two-player%20adversarial%20game%20and%20removes%20the%0Arestrictive%20assumption%20of%20standard%20normal%20distributions%20in%20the%20inference%20model.%0ATheoretically%2C%20AVB%20can%20achieve%20similar%20or%20higher%20likelihood%20compared%20to%20VAEs.%20A%0Afurther%20enhanced%20algorithm%2C%20Importance-weighted%20Adversarial%20Variational%20Bayes%0A%28IWAVB%29%20is%20proposed%20and%20compared%20with%20Importance-weighted%20Autoencoders%20%28IWAE%29.%0AIn%20an%20exploratory%20analysis%20of%20empirical%20data%2C%20IWAVB%20demonstrated%20superior%0Aexpressiveness%20by%20achieving%20a%20higher%20likelihood%20compared%20to%20IWAE.%20In%0Aconfirmatory%20analysis%20with%20simulated%20data%2C%20IWAVB%20achieved%20similar%20mean-square%0Aerror%20results%20to%20IWAE%20while%20consistently%20achieving%20higher%20likelihoods.%20When%0Alatent%20variables%20followed%20a%20multimodal%20distribution%2C%20IWAVB%20outperformed%20IWAE.%0AWith%20its%20innovative%20use%20of%20GANs%2C%20IWAVB%20is%20shown%20to%20have%20the%20potential%20to%20extend%0AIFA%20to%20handle%20large-scale%20data%2C%20facilitating%20the%20potential%20integration%20of%0Apsychometrics%20and%20multimodal%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10650v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520for%2520High-Dimensional%2520Item%2520Factor%250A%2520%2520Analysis%253A%2520A%2520Deep%2520Adversarial%2520Learning%2520Algorithm%26entry.906535625%3DNanyu%2520Luo%2520and%2520Feng%2520Ji%26entry.1292438233%3D%2520%2520Advances%2520in%2520deep%2520learning%2520and%2520representation%2520learning%2520have%2520transformed%2520item%250Afactor%2520analysis%2520%2528IFA%2529%2520in%2520the%2520item%2520response%2520theory%2520%2528IRT%2529%2520literature%2520by%2520enabling%250Amore%2520efficient%2520and%2520accurate%2520parameter%2520estimation.%2520Variational%2520Autoencoders%250A%2528VAEs%2529%2520have%2520been%2520one%2520of%2520the%2520most%2520impactful%2520techniques%2520in%2520modeling%250Ahigh-dimensional%2520latent%2520variables%2520in%2520this%2520context.%2520However%252C%2520the%2520limited%250Aexpressiveness%2520of%2520the%2520inference%2520model%2520based%2520on%2520traditional%2520VAEs%2520can%2520still%250Ahinder%2520the%2520estimation%2520performance.%2520We%2520introduce%2520Adversarial%2520Variational%2520Bayes%250A%2528AVB%2529%2520algorithms%2520as%2520an%2520improvement%2520to%2520VAEs%2520for%2520IFA%2520with%2520improved%2520flexibility%250Aand%2520accuracy.%2520By%2520bridging%2520the%2520strengths%2520of%2520VAEs%2520and%2520Generative%2520Adversarial%250ANetworks%2520%2528GANs%2529%252C%2520AVB%2520incorporates%2520an%2520auxiliary%2520discriminator%2520network%2520to%2520reframe%250Athe%2520estimation%2520process%2520as%2520a%2520two-player%2520adversarial%2520game%2520and%2520removes%2520the%250Arestrictive%2520assumption%2520of%2520standard%2520normal%2520distributions%2520in%2520the%2520inference%2520model.%250ATheoretically%252C%2520AVB%2520can%2520achieve%2520similar%2520or%2520higher%2520likelihood%2520compared%2520to%2520VAEs.%2520A%250Afurther%2520enhanced%2520algorithm%252C%2520Importance-weighted%2520Adversarial%2520Variational%2520Bayes%250A%2528IWAVB%2529%2520is%2520proposed%2520and%2520compared%2520with%2520Importance-weighted%2520Autoencoders%2520%2528IWAE%2529.%250AIn%2520an%2520exploratory%2520analysis%2520of%2520empirical%2520data%252C%2520IWAVB%2520demonstrated%2520superior%250Aexpressiveness%2520by%2520achieving%2520a%2520higher%2520likelihood%2520compared%2520to%2520IWAE.%2520In%250Aconfirmatory%2520analysis%2520with%2520simulated%2520data%252C%2520IWAVB%2520achieved%2520similar%2520mean-square%250Aerror%2520results%2520to%2520IWAE%2520while%2520consistently%2520achieving%2520higher%2520likelihoods.%2520When%250Alatent%2520variables%2520followed%2520a%2520multimodal%2520distribution%252C%2520IWAVB%2520outperformed%2520IWAE.%250AWith%2520its%2520innovative%2520use%2520of%2520GANs%252C%2520IWAVB%2520is%2520shown%2520to%2520have%2520the%2520potential%2520to%2520extend%250AIFA%2520to%2520handle%2520large-scale%2520data%252C%2520facilitating%2520the%2520potential%2520integration%2520of%250Apsychometrics%2520and%2520multimodal%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10650v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20for%20High-Dimensional%20Item%20Factor%0A%20%20Analysis%3A%20A%20Deep%20Adversarial%20Learning%20Algorithm&entry.906535625=Nanyu%20Luo%20and%20Feng%20Ji&entry.1292438233=%20%20Advances%20in%20deep%20learning%20and%20representation%20learning%20have%20transformed%20item%0Afactor%20analysis%20%28IFA%29%20in%20the%20item%20response%20theory%20%28IRT%29%20literature%20by%20enabling%0Amore%20efficient%20and%20accurate%20parameter%20estimation.%20Variational%20Autoencoders%0A%28VAEs%29%20have%20been%20one%20of%20the%20most%20impactful%20techniques%20in%20modeling%0Ahigh-dimensional%20latent%20variables%20in%20this%20context.%20However%2C%20the%20limited%0Aexpressiveness%20of%20the%20inference%20model%20based%20on%20traditional%20VAEs%20can%20still%0Ahinder%20the%20estimation%20performance.%20We%20introduce%20Adversarial%20Variational%20Bayes%0A%28AVB%29%20algorithms%20as%20an%20improvement%20to%20VAEs%20for%20IFA%20with%20improved%20flexibility%0Aand%20accuracy.%20By%20bridging%20the%20strengths%20of%20VAEs%20and%20Generative%20Adversarial%0ANetworks%20%28GANs%29%2C%20AVB%20incorporates%20an%20auxiliary%20discriminator%20network%20to%20reframe%0Athe%20estimation%20process%20as%20a%20two-player%20adversarial%20game%20and%20removes%20the%0Arestrictive%20assumption%20of%20standard%20normal%20distributions%20in%20the%20inference%20model.%0ATheoretically%2C%20AVB%20can%20achieve%20similar%20or%20higher%20likelihood%20compared%20to%20VAEs.%20A%0Afurther%20enhanced%20algorithm%2C%20Importance-weighted%20Adversarial%20Variational%20Bayes%0A%28IWAVB%29%20is%20proposed%20and%20compared%20with%20Importance-weighted%20Autoencoders%20%28IWAE%29.%0AIn%20an%20exploratory%20analysis%20of%20empirical%20data%2C%20IWAVB%20demonstrated%20superior%0Aexpressiveness%20by%20achieving%20a%20higher%20likelihood%20compared%20to%20IWAE.%20In%0Aconfirmatory%20analysis%20with%20simulated%20data%2C%20IWAVB%20achieved%20similar%20mean-square%0Aerror%20results%20to%20IWAE%20while%20consistently%20achieving%20higher%20likelihoods.%20When%0Alatent%20variables%20followed%20a%20multimodal%20distribution%2C%20IWAVB%20outperformed%20IWAE.%0AWith%20its%20innovative%20use%20of%20GANs%2C%20IWAVB%20is%20shown%20to%20have%20the%20potential%20to%20extend%0AIFA%20to%20handle%20large-scale%20data%2C%20facilitating%20the%20potential%20integration%20of%0Apsychometrics%20and%20multimodal%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10650v3&entry.124074799=Read"},
{"title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks", "author": "Fenia Christopoulou and Ronald Cardenas and Gerasimos Lampouras and Haitham Bou-Ammar and Jun Wang", "abstract": "  Direct alignment algorithms have proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants of the Direct\nPreference Optimization objective have focused on a strict setting where all\ntokens are contributing signals of KL divergence and rewards to the loss\nfunction. However, human preference is not affected equally by each word in a\nsequence but is often dependent on specific words or phrases, e.g. existence of\ntoxic terms leads to non-preferred responses. Based on this observation, we\nargue that not all tokens should be weighted equally during PO and propose a\nflexible objective termed SparsePO, that aims to automatically learn to weight\nthe KL divergence and reward corresponding to each token during PO training. We\npropose two different variants of weight-masks that can either be derived from\nthe reference model itself or learned on the fly. Notably, our method induces\nsparsity in the learned masks, allowing the model to learn how to best balance\nreward and KL divergence contributions at the token level, learning an optimal\nlevel of mask sparsity. Extensive experiments illustrate the effectiveness of\nour approach at aligning to preference proxies, including sentiment control,\nhelpfulness and harmlessness, and summary quality. Our method obtains +10% and\n+3% win rate points in summarization and dialogue scenarios, respectively,\nwithout compromising model reasoning or the relevancy and faithfulness of the\nsummary response.\n", "link": "http://arxiv.org/abs/2410.05102v3", "date": "2025-10-31", "relevancy": 2.3794, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparsePO%3A%20Controlling%20Preference%20Alignment%20of%20LLMs%20via%20Sparse%20Token%0A%20%20Masks&body=Title%3A%20SparsePO%3A%20Controlling%20Preference%20Alignment%20of%20LLMs%20via%20Sparse%20Token%0A%20%20Masks%0AAuthor%3A%20Fenia%20Christopoulou%20and%20Ronald%20Cardenas%20and%20Gerasimos%20Lampouras%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Direct%20alignment%20algorithms%20have%20proven%20an%20effective%20step%20for%20aligning%0Alanguage%20models%20to%20human-desired%20behaviors.%20Current%20variants%20of%20the%20Direct%0APreference%20Optimization%20objective%20have%20focused%20on%20a%20strict%20setting%20where%20all%0Atokens%20are%20contributing%20signals%20of%20KL%20divergence%20and%20rewards%20to%20the%20loss%0Afunction.%20However%2C%20human%20preference%20is%20not%20affected%20equally%20by%20each%20word%20in%20a%0Asequence%20but%20is%20often%20dependent%20on%20specific%20words%20or%20phrases%2C%20e.g.%20existence%20of%0Atoxic%20terms%20leads%20to%20non-preferred%20responses.%20Based%20on%20this%20observation%2C%20we%0Aargue%20that%20not%20all%20tokens%20should%20be%20weighted%20equally%20during%20PO%20and%20propose%20a%0Aflexible%20objective%20termed%20SparsePO%2C%20that%20aims%20to%20automatically%20learn%20to%20weight%0Athe%20KL%20divergence%20and%20reward%20corresponding%20to%20each%20token%20during%20PO%20training.%20We%0Apropose%20two%20different%20variants%20of%20weight-masks%20that%20can%20either%20be%20derived%20from%0Athe%20reference%20model%20itself%20or%20learned%20on%20the%20fly.%20Notably%2C%20our%20method%20induces%0Asparsity%20in%20the%20learned%20masks%2C%20allowing%20the%20model%20to%20learn%20how%20to%20best%20balance%0Areward%20and%20KL%20divergence%20contributions%20at%20the%20token%20level%2C%20learning%20an%20optimal%0Alevel%20of%20mask%20sparsity.%20Extensive%20experiments%20illustrate%20the%20effectiveness%20of%0Aour%20approach%20at%20aligning%20to%20preference%20proxies%2C%20including%20sentiment%20control%2C%0Ahelpfulness%20and%20harmlessness%2C%20and%20summary%20quality.%20Our%20method%20obtains%20%2B10%25%20and%0A%2B3%25%20win%20rate%20points%20in%20summarization%20and%20dialogue%20scenarios%2C%20respectively%2C%0Awithout%20compromising%20model%20reasoning%20or%20the%20relevancy%20and%20faithfulness%20of%20the%0Asummary%20response.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05102v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsePO%253A%2520Controlling%2520Preference%2520Alignment%2520of%2520LLMs%2520via%2520Sparse%2520Token%250A%2520%2520Masks%26entry.906535625%3DFenia%2520Christopoulou%2520and%2520Ronald%2520Cardenas%2520and%2520Gerasimos%2520Lampouras%2520and%2520Haitham%2520Bou-Ammar%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Direct%2520alignment%2520algorithms%2520have%2520proven%2520an%2520effective%2520step%2520for%2520aligning%250Alanguage%2520models%2520to%2520human-desired%2520behaviors.%2520Current%2520variants%2520of%2520the%2520Direct%250APreference%2520Optimization%2520objective%2520have%2520focused%2520on%2520a%2520strict%2520setting%2520where%2520all%250Atokens%2520are%2520contributing%2520signals%2520of%2520KL%2520divergence%2520and%2520rewards%2520to%2520the%2520loss%250Afunction.%2520However%252C%2520human%2520preference%2520is%2520not%2520affected%2520equally%2520by%2520each%2520word%2520in%2520a%250Asequence%2520but%2520is%2520often%2520dependent%2520on%2520specific%2520words%2520or%2520phrases%252C%2520e.g.%2520existence%2520of%250Atoxic%2520terms%2520leads%2520to%2520non-preferred%2520responses.%2520Based%2520on%2520this%2520observation%252C%2520we%250Aargue%2520that%2520not%2520all%2520tokens%2520should%2520be%2520weighted%2520equally%2520during%2520PO%2520and%2520propose%2520a%250Aflexible%2520objective%2520termed%2520SparsePO%252C%2520that%2520aims%2520to%2520automatically%2520learn%2520to%2520weight%250Athe%2520KL%2520divergence%2520and%2520reward%2520corresponding%2520to%2520each%2520token%2520during%2520PO%2520training.%2520We%250Apropose%2520two%2520different%2520variants%2520of%2520weight-masks%2520that%2520can%2520either%2520be%2520derived%2520from%250Athe%2520reference%2520model%2520itself%2520or%2520learned%2520on%2520the%2520fly.%2520Notably%252C%2520our%2520method%2520induces%250Asparsity%2520in%2520the%2520learned%2520masks%252C%2520allowing%2520the%2520model%2520to%2520learn%2520how%2520to%2520best%2520balance%250Areward%2520and%2520KL%2520divergence%2520contributions%2520at%2520the%2520token%2520level%252C%2520learning%2520an%2520optimal%250Alevel%2520of%2520mask%2520sparsity.%2520Extensive%2520experiments%2520illustrate%2520the%2520effectiveness%2520of%250Aour%2520approach%2520at%2520aligning%2520to%2520preference%2520proxies%252C%2520including%2520sentiment%2520control%252C%250Ahelpfulness%2520and%2520harmlessness%252C%2520and%2520summary%2520quality.%2520Our%2520method%2520obtains%2520%252B10%2525%2520and%250A%252B3%2525%2520win%2520rate%2520points%2520in%2520summarization%2520and%2520dialogue%2520scenarios%252C%2520respectively%252C%250Awithout%2520compromising%2520model%2520reasoning%2520or%2520the%2520relevancy%2520and%2520faithfulness%2520of%2520the%250Asummary%2520response.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05102v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparsePO%3A%20Controlling%20Preference%20Alignment%20of%20LLMs%20via%20Sparse%20Token%0A%20%20Masks&entry.906535625=Fenia%20Christopoulou%20and%20Ronald%20Cardenas%20and%20Gerasimos%20Lampouras%20and%20Haitham%20Bou-Ammar%20and%20Jun%20Wang&entry.1292438233=%20%20Direct%20alignment%20algorithms%20have%20proven%20an%20effective%20step%20for%20aligning%0Alanguage%20models%20to%20human-desired%20behaviors.%20Current%20variants%20of%20the%20Direct%0APreference%20Optimization%20objective%20have%20focused%20on%20a%20strict%20setting%20where%20all%0Atokens%20are%20contributing%20signals%20of%20KL%20divergence%20and%20rewards%20to%20the%20loss%0Afunction.%20However%2C%20human%20preference%20is%20not%20affected%20equally%20by%20each%20word%20in%20a%0Asequence%20but%20is%20often%20dependent%20on%20specific%20words%20or%20phrases%2C%20e.g.%20existence%20of%0Atoxic%20terms%20leads%20to%20non-preferred%20responses.%20Based%20on%20this%20observation%2C%20we%0Aargue%20that%20not%20all%20tokens%20should%20be%20weighted%20equally%20during%20PO%20and%20propose%20a%0Aflexible%20objective%20termed%20SparsePO%2C%20that%20aims%20to%20automatically%20learn%20to%20weight%0Athe%20KL%20divergence%20and%20reward%20corresponding%20to%20each%20token%20during%20PO%20training.%20We%0Apropose%20two%20different%20variants%20of%20weight-masks%20that%20can%20either%20be%20derived%20from%0Athe%20reference%20model%20itself%20or%20learned%20on%20the%20fly.%20Notably%2C%20our%20method%20induces%0Asparsity%20in%20the%20learned%20masks%2C%20allowing%20the%20model%20to%20learn%20how%20to%20best%20balance%0Areward%20and%20KL%20divergence%20contributions%20at%20the%20token%20level%2C%20learning%20an%20optimal%0Alevel%20of%20mask%20sparsity.%20Extensive%20experiments%20illustrate%20the%20effectiveness%20of%0Aour%20approach%20at%20aligning%20to%20preference%20proxies%2C%20including%20sentiment%20control%2C%0Ahelpfulness%20and%20harmlessness%2C%20and%20summary%20quality.%20Our%20method%20obtains%20%2B10%25%20and%0A%2B3%25%20win%20rate%20points%20in%20summarization%20and%20dialogue%20scenarios%2C%20respectively%2C%0Awithout%20compromising%20model%20reasoning%20or%20the%20relevancy%20and%20faithfulness%20of%20the%0Asummary%20response.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05102v3&entry.124074799=Read"},
{"title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private\n  Federated Learning via Penalizing Gradient Norm", "author": "Junkang Liu and Yuxuan Tian and Fanhua Shang and Yuanyuan Liu and Hongying Liu and Junchao Zhou and Daorui Ding", "abstract": "  To prevent inference attacks in Federated Learning (FL) and reduce the\nleakage of sensitive information, Client-level Differentially Private Federated\nLearning (CL-DPFL) is widely used. However, current CL-DPFL methods usually\nresult in sharper loss landscapes, which leads to a decrease in model\ngeneralization after differential privacy protection. By using Sharpness Aware\nMinimization (SAM), the current popular federated learning methods are to find\na local flat minimum value to alleviate this problem. However, the local\nflatness may not reflect the global flatness in CL-DPFL. Therefore, to address\nthis issue and seek global flat minima of models, we propose a new CL-DPFL\nalgorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to\nthe local loss to find the global flat minimum. Moreover, by using our global\ngradient norm penalty, we not only find a flatter global minimum but also\nreduce the locally updated norm, which means that we further reduce the error\nof gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN\nmitigates the performance degradation caused by DP. Meanwhile, the proposed\nDP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves\nfast convergence. We also use R\\'enyi DP to provide strict privacy guarantees\nand provide sensitivity analysis for local updates. Finally, we conduct\neffectiveness tests on both ResNet and Transformer models, and achieve\nsignificant improvements in six visual and natural language processing tasks\ncompared to existing state-of-the-art algorithms. The code is available at\nhttps://github.com/junkangLiu0/DP-FedPGN\n", "link": "http://arxiv.org/abs/2510.27504v1", "date": "2025-10-31", "relevancy": 2.3761, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4816}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4798}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-FedPGN%3A%20Finding%20Global%20Flat%20Minima%20for%20Differentially%20Private%0A%20%20Federated%20Learning%20via%20Penalizing%20Gradient%20Norm&body=Title%3A%20DP-FedPGN%3A%20Finding%20Global%20Flat%20Minima%20for%20Differentially%20Private%0A%20%20Federated%20Learning%20via%20Penalizing%20Gradient%20Norm%0AAuthor%3A%20Junkang%20Liu%20and%20Yuxuan%20Tian%20and%20Fanhua%20Shang%20and%20Yuanyuan%20Liu%20and%20Hongying%20Liu%20and%20Junchao%20Zhou%20and%20Daorui%20Ding%0AAbstract%3A%20%20%20To%20prevent%20inference%20attacks%20in%20Federated%20Learning%20%28FL%29%20and%20reduce%20the%0Aleakage%20of%20sensitive%20information%2C%20Client-level%20Differentially%20Private%20Federated%0ALearning%20%28CL-DPFL%29%20is%20widely%20used.%20However%2C%20current%20CL-DPFL%20methods%20usually%0Aresult%20in%20sharper%20loss%20landscapes%2C%20which%20leads%20to%20a%20decrease%20in%20model%0Ageneralization%20after%20differential%20privacy%20protection.%20By%20using%20Sharpness%20Aware%0AMinimization%20%28SAM%29%2C%20the%20current%20popular%20federated%20learning%20methods%20are%20to%20find%0Aa%20local%20flat%20minimum%20value%20to%20alleviate%20this%20problem.%20However%2C%20the%20local%0Aflatness%20may%20not%20reflect%20the%20global%20flatness%20in%20CL-DPFL.%20Therefore%2C%20to%20address%0Athis%20issue%20and%20seek%20global%20flat%20minima%20of%20models%2C%20we%20propose%20a%20new%20CL-DPFL%0Aalgorithm%2C%20DP-FedPGN%2C%20in%20which%20we%20introduce%20a%20global%20gradient%20norm%20penalty%20to%0Athe%20local%20loss%20to%20find%20the%20global%20flat%20minimum.%20Moreover%2C%20by%20using%20our%20global%0Agradient%20norm%20penalty%2C%20we%20not%20only%20find%20a%20flatter%20global%20minimum%20but%20also%0Areduce%20the%20locally%20updated%20norm%2C%20which%20means%20that%20we%20further%20reduce%20the%20error%0Aof%20gradient%20clipping.%20From%20a%20theoretical%20perspective%2C%20we%20analyze%20how%20DP-FedPGN%0Amitigates%20the%20performance%20degradation%20caused%20by%20DP.%20Meanwhile%2C%20the%20proposed%0ADP-FedPGN%20algorithm%20eliminates%20the%20impact%20of%20data%20heterogeneity%20and%20achieves%0Afast%20convergence.%20We%20also%20use%20R%5C%27enyi%20DP%20to%20provide%20strict%20privacy%20guarantees%0Aand%20provide%20sensitivity%20analysis%20for%20local%20updates.%20Finally%2C%20we%20conduct%0Aeffectiveness%20tests%20on%20both%20ResNet%20and%20Transformer%20models%2C%20and%20achieve%0Asignificant%20improvements%20in%20six%20visual%20and%20natural%20language%20processing%20tasks%0Acompared%20to%20existing%20state-of-the-art%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/junkangLiu0/DP-FedPGN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-FedPGN%253A%2520Finding%2520Global%2520Flat%2520Minima%2520for%2520Differentially%2520Private%250A%2520%2520Federated%2520Learning%2520via%2520Penalizing%2520Gradient%2520Norm%26entry.906535625%3DJunkang%2520Liu%2520and%2520Yuxuan%2520Tian%2520and%2520Fanhua%2520Shang%2520and%2520Yuanyuan%2520Liu%2520and%2520Hongying%2520Liu%2520and%2520Junchao%2520Zhou%2520and%2520Daorui%2520Ding%26entry.1292438233%3D%2520%2520To%2520prevent%2520inference%2520attacks%2520in%2520Federated%2520Learning%2520%2528FL%2529%2520and%2520reduce%2520the%250Aleakage%2520of%2520sensitive%2520information%252C%2520Client-level%2520Differentially%2520Private%2520Federated%250ALearning%2520%2528CL-DPFL%2529%2520is%2520widely%2520used.%2520However%252C%2520current%2520CL-DPFL%2520methods%2520usually%250Aresult%2520in%2520sharper%2520loss%2520landscapes%252C%2520which%2520leads%2520to%2520a%2520decrease%2520in%2520model%250Ageneralization%2520after%2520differential%2520privacy%2520protection.%2520By%2520using%2520Sharpness%2520Aware%250AMinimization%2520%2528SAM%2529%252C%2520the%2520current%2520popular%2520federated%2520learning%2520methods%2520are%2520to%2520find%250Aa%2520local%2520flat%2520minimum%2520value%2520to%2520alleviate%2520this%2520problem.%2520However%252C%2520the%2520local%250Aflatness%2520may%2520not%2520reflect%2520the%2520global%2520flatness%2520in%2520CL-DPFL.%2520Therefore%252C%2520to%2520address%250Athis%2520issue%2520and%2520seek%2520global%2520flat%2520minima%2520of%2520models%252C%2520we%2520propose%2520a%2520new%2520CL-DPFL%250Aalgorithm%252C%2520DP-FedPGN%252C%2520in%2520which%2520we%2520introduce%2520a%2520global%2520gradient%2520norm%2520penalty%2520to%250Athe%2520local%2520loss%2520to%2520find%2520the%2520global%2520flat%2520minimum.%2520Moreover%252C%2520by%2520using%2520our%2520global%250Agradient%2520norm%2520penalty%252C%2520we%2520not%2520only%2520find%2520a%2520flatter%2520global%2520minimum%2520but%2520also%250Areduce%2520the%2520locally%2520updated%2520norm%252C%2520which%2520means%2520that%2520we%2520further%2520reduce%2520the%2520error%250Aof%2520gradient%2520clipping.%2520From%2520a%2520theoretical%2520perspective%252C%2520we%2520analyze%2520how%2520DP-FedPGN%250Amitigates%2520the%2520performance%2520degradation%2520caused%2520by%2520DP.%2520Meanwhile%252C%2520the%2520proposed%250ADP-FedPGN%2520algorithm%2520eliminates%2520the%2520impact%2520of%2520data%2520heterogeneity%2520and%2520achieves%250Afast%2520convergence.%2520We%2520also%2520use%2520R%255C%2527enyi%2520DP%2520to%2520provide%2520strict%2520privacy%2520guarantees%250Aand%2520provide%2520sensitivity%2520analysis%2520for%2520local%2520updates.%2520Finally%252C%2520we%2520conduct%250Aeffectiveness%2520tests%2520on%2520both%2520ResNet%2520and%2520Transformer%2520models%252C%2520and%2520achieve%250Asignificant%2520improvements%2520in%2520six%2520visual%2520and%2520natural%2520language%2520processing%2520tasks%250Acompared%2520to%2520existing%2520state-of-the-art%2520algorithms.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/junkangLiu0/DP-FedPGN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-FedPGN%3A%20Finding%20Global%20Flat%20Minima%20for%20Differentially%20Private%0A%20%20Federated%20Learning%20via%20Penalizing%20Gradient%20Norm&entry.906535625=Junkang%20Liu%20and%20Yuxuan%20Tian%20and%20Fanhua%20Shang%20and%20Yuanyuan%20Liu%20and%20Hongying%20Liu%20and%20Junchao%20Zhou%20and%20Daorui%20Ding&entry.1292438233=%20%20To%20prevent%20inference%20attacks%20in%20Federated%20Learning%20%28FL%29%20and%20reduce%20the%0Aleakage%20of%20sensitive%20information%2C%20Client-level%20Differentially%20Private%20Federated%0ALearning%20%28CL-DPFL%29%20is%20widely%20used.%20However%2C%20current%20CL-DPFL%20methods%20usually%0Aresult%20in%20sharper%20loss%20landscapes%2C%20which%20leads%20to%20a%20decrease%20in%20model%0Ageneralization%20after%20differential%20privacy%20protection.%20By%20using%20Sharpness%20Aware%0AMinimization%20%28SAM%29%2C%20the%20current%20popular%20federated%20learning%20methods%20are%20to%20find%0Aa%20local%20flat%20minimum%20value%20to%20alleviate%20this%20problem.%20However%2C%20the%20local%0Aflatness%20may%20not%20reflect%20the%20global%20flatness%20in%20CL-DPFL.%20Therefore%2C%20to%20address%0Athis%20issue%20and%20seek%20global%20flat%20minima%20of%20models%2C%20we%20propose%20a%20new%20CL-DPFL%0Aalgorithm%2C%20DP-FedPGN%2C%20in%20which%20we%20introduce%20a%20global%20gradient%20norm%20penalty%20to%0Athe%20local%20loss%20to%20find%20the%20global%20flat%20minimum.%20Moreover%2C%20by%20using%20our%20global%0Agradient%20norm%20penalty%2C%20we%20not%20only%20find%20a%20flatter%20global%20minimum%20but%20also%0Areduce%20the%20locally%20updated%20norm%2C%20which%20means%20that%20we%20further%20reduce%20the%20error%0Aof%20gradient%20clipping.%20From%20a%20theoretical%20perspective%2C%20we%20analyze%20how%20DP-FedPGN%0Amitigates%20the%20performance%20degradation%20caused%20by%20DP.%20Meanwhile%2C%20the%20proposed%0ADP-FedPGN%20algorithm%20eliminates%20the%20impact%20of%20data%20heterogeneity%20and%20achieves%0Afast%20convergence.%20We%20also%20use%20R%5C%27enyi%20DP%20to%20provide%20strict%20privacy%20guarantees%0Aand%20provide%20sensitivity%20analysis%20for%20local%20updates.%20Finally%2C%20we%20conduct%0Aeffectiveness%20tests%20on%20both%20ResNet%20and%20Transformer%20models%2C%20and%20achieve%0Asignificant%20improvements%20in%20six%20visual%20and%20natural%20language%20processing%20tasks%0Acompared%20to%20existing%20state-of-the-art%20algorithms.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/junkangLiu0/DP-FedPGN%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27504v1&entry.124074799=Read"},
{"title": "Supervised Quadratic Feature Analysis: Information Geometry Approach for\n  Dimensionality Reduction", "author": "Daniel Herrera-Esposito and Johannes Burge", "abstract": "  Supervised dimensionality reduction maps labeled data into a low-dimensional\nfeature space while preserving class discriminability. A common approach is to\nmaximize a statistical measure of dissimilarity between classes in the feature\nspace. Information geometry provides an alternative framework for measuring\nclass dissimilarity, with the potential for improved insights and novel\napplications. Information geometry, which is grounded in Riemannian geometry,\nuses the Fisher information metric, a local measure of discriminability that\ninduces the Fisher-Rao distance. Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a linear dimensionality reduction method that maximizes\nFisher-Rao distances between class-conditional distributions, under Gaussian\nassumptions. We motivate the Fisher-Rao distance as a good proxy for\ndiscriminability. We show that SQFA features support good classification\nperformance with Quadratic Discriminant Analysis (QDA) on three real-world\ndatasets. SQFA provides a novel framework for supervised dimensionality\nreduction, motivating future research in applying information geometry to\nmachine learning and neuroscience.\n", "link": "http://arxiv.org/abs/2502.00168v4", "date": "2025-10-31", "relevancy": 2.3553, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4804}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4762}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction&body=Title%3A%20Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction%0AAuthor%3A%20Daniel%20Herrera-Esposito%20and%20Johannes%20Burge%0AAbstract%3A%20%20%20Supervised%20dimensionality%20reduction%20maps%20labeled%20data%20into%20a%20low-dimensional%0Afeature%20space%20while%20preserving%20class%20discriminability.%20A%20common%20approach%20is%20to%0Amaximize%20a%20statistical%20measure%20of%20dissimilarity%20between%20classes%20in%20the%20feature%0Aspace.%20Information%20geometry%20provides%20an%20alternative%20framework%20for%20measuring%0Aclass%20dissimilarity%2C%20with%20the%20potential%20for%20improved%20insights%20and%20novel%0Aapplications.%20Information%20geometry%2C%20which%20is%20grounded%20in%20Riemannian%20geometry%2C%0Auses%20the%20Fisher%20information%20metric%2C%20a%20local%20measure%20of%20discriminability%20that%0Ainduces%20the%20Fisher-Rao%20distance.%20Here%2C%20we%20present%20Supervised%20Quadratic%20Feature%0AAnalysis%20%28SQFA%29%2C%20a%20linear%20dimensionality%20reduction%20method%20that%20maximizes%0AFisher-Rao%20distances%20between%20class-conditional%20distributions%2C%20under%20Gaussian%0Aassumptions.%20We%20motivate%20the%20Fisher-Rao%20distance%20as%20a%20good%20proxy%20for%0Adiscriminability.%20We%20show%20that%20SQFA%20features%20support%20good%20classification%0Aperformance%20with%20Quadratic%20Discriminant%20Analysis%20%28QDA%29%20on%20three%20real-world%0Adatasets.%20SQFA%20provides%20a%20novel%20framework%20for%20supervised%20dimensionality%0Areduction%2C%20motivating%20future%20research%20in%20applying%20information%20geometry%20to%0Amachine%20learning%20and%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00168v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Quadratic%2520Feature%2520Analysis%253A%2520Information%2520Geometry%2520Approach%2520for%250A%2520%2520Dimensionality%2520Reduction%26entry.906535625%3DDaniel%2520Herrera-Esposito%2520and%2520Johannes%2520Burge%26entry.1292438233%3D%2520%2520Supervised%2520dimensionality%2520reduction%2520maps%2520labeled%2520data%2520into%2520a%2520low-dimensional%250Afeature%2520space%2520while%2520preserving%2520class%2520discriminability.%2520A%2520common%2520approach%2520is%2520to%250Amaximize%2520a%2520statistical%2520measure%2520of%2520dissimilarity%2520between%2520classes%2520in%2520the%2520feature%250Aspace.%2520Information%2520geometry%2520provides%2520an%2520alternative%2520framework%2520for%2520measuring%250Aclass%2520dissimilarity%252C%2520with%2520the%2520potential%2520for%2520improved%2520insights%2520and%2520novel%250Aapplications.%2520Information%2520geometry%252C%2520which%2520is%2520grounded%2520in%2520Riemannian%2520geometry%252C%250Auses%2520the%2520Fisher%2520information%2520metric%252C%2520a%2520local%2520measure%2520of%2520discriminability%2520that%250Ainduces%2520the%2520Fisher-Rao%2520distance.%2520Here%252C%2520we%2520present%2520Supervised%2520Quadratic%2520Feature%250AAnalysis%2520%2528SQFA%2529%252C%2520a%2520linear%2520dimensionality%2520reduction%2520method%2520that%2520maximizes%250AFisher-Rao%2520distances%2520between%2520class-conditional%2520distributions%252C%2520under%2520Gaussian%250Aassumptions.%2520We%2520motivate%2520the%2520Fisher-Rao%2520distance%2520as%2520a%2520good%2520proxy%2520for%250Adiscriminability.%2520We%2520show%2520that%2520SQFA%2520features%2520support%2520good%2520classification%250Aperformance%2520with%2520Quadratic%2520Discriminant%2520Analysis%2520%2528QDA%2529%2520on%2520three%2520real-world%250Adatasets.%2520SQFA%2520provides%2520a%2520novel%2520framework%2520for%2520supervised%2520dimensionality%250Areduction%252C%2520motivating%2520future%2520research%2520in%2520applying%2520information%2520geometry%2520to%250Amachine%2520learning%2520and%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00168v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Quadratic%20Feature%20Analysis%3A%20Information%20Geometry%20Approach%20for%0A%20%20Dimensionality%20Reduction&entry.906535625=Daniel%20Herrera-Esposito%20and%20Johannes%20Burge&entry.1292438233=%20%20Supervised%20dimensionality%20reduction%20maps%20labeled%20data%20into%20a%20low-dimensional%0Afeature%20space%20while%20preserving%20class%20discriminability.%20A%20common%20approach%20is%20to%0Amaximize%20a%20statistical%20measure%20of%20dissimilarity%20between%20classes%20in%20the%20feature%0Aspace.%20Information%20geometry%20provides%20an%20alternative%20framework%20for%20measuring%0Aclass%20dissimilarity%2C%20with%20the%20potential%20for%20improved%20insights%20and%20novel%0Aapplications.%20Information%20geometry%2C%20which%20is%20grounded%20in%20Riemannian%20geometry%2C%0Auses%20the%20Fisher%20information%20metric%2C%20a%20local%20measure%20of%20discriminability%20that%0Ainduces%20the%20Fisher-Rao%20distance.%20Here%2C%20we%20present%20Supervised%20Quadratic%20Feature%0AAnalysis%20%28SQFA%29%2C%20a%20linear%20dimensionality%20reduction%20method%20that%20maximizes%0AFisher-Rao%20distances%20between%20class-conditional%20distributions%2C%20under%20Gaussian%0Aassumptions.%20We%20motivate%20the%20Fisher-Rao%20distance%20as%20a%20good%20proxy%20for%0Adiscriminability.%20We%20show%20that%20SQFA%20features%20support%20good%20classification%0Aperformance%20with%20Quadratic%20Discriminant%20Analysis%20%28QDA%29%20on%20three%20real-world%0Adatasets.%20SQFA%20provides%20a%20novel%20framework%20for%20supervised%20dimensionality%0Areduction%2C%20motivating%20future%20research%20in%20applying%20information%20geometry%20to%0Amachine%20learning%20and%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00168v4&entry.124074799=Read"},
{"title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred\n  Code Adjustments", "author": "Forough Mehralian and Ryan Shar and James R. Rae and Alireza Hashemi", "abstract": "  As large language models become increasingly capable of generating code,\nevaluating their performance remains a complex and evolving challenge. Existing\nbenchmarks primarily focus on functional correctness, overlooking the diversity\nof real-world coding tasks and developer expectations. To this end, we\nintroduce a multi-language benchmark that evaluates LLM instruction-following\ncapabilities and is extensible to operate on any set of standalone coding\nproblems. Our benchmark evaluates instruction following in two key settings:\nadherence to pre-defined constraints specified with the initial problem, and\nthe ability to perform refinements based on follow-up instructions. For this\npaper's analysis, we empirically evaluated our benchmarking pipeline with\nprogramming tasks from LiveBench, that are also automatically translated from\nPython into Java and JavaScript. Our automated benchmark reveals that models\nexhibit differing levels of performance across multiple dimensions of\ninstruction-following. Our benchmarking pipeline provides a more comprehensive\nevaluation of code generation models, highlighting their strengths and\nlimitations across languages and generation goals.\n", "link": "http://arxiv.org/abs/2510.27565v1", "date": "2025-10-31", "relevancy": 2.327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeAlignBench%3A%20Assessing%20Code%20Generation%20Models%20on%20Developer-Preferred%0A%20%20Code%20Adjustments&body=Title%3A%20CodeAlignBench%3A%20Assessing%20Code%20Generation%20Models%20on%20Developer-Preferred%0A%20%20Code%20Adjustments%0AAuthor%3A%20Forough%20Mehralian%20and%20Ryan%20Shar%20and%20James%20R.%20Rae%20and%20Alireza%20Hashemi%0AAbstract%3A%20%20%20As%20large%20language%20models%20become%20increasingly%20capable%20of%20generating%20code%2C%0Aevaluating%20their%20performance%20remains%20a%20complex%20and%20evolving%20challenge.%20Existing%0Abenchmarks%20primarily%20focus%20on%20functional%20correctness%2C%20overlooking%20the%20diversity%0Aof%20real-world%20coding%20tasks%20and%20developer%20expectations.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-language%20benchmark%20that%20evaluates%20LLM%20instruction-following%0Acapabilities%20and%20is%20extensible%20to%20operate%20on%20any%20set%20of%20standalone%20coding%0Aproblems.%20Our%20benchmark%20evaluates%20instruction%20following%20in%20two%20key%20settings%3A%0Aadherence%20to%20pre-defined%20constraints%20specified%20with%20the%20initial%20problem%2C%20and%0Athe%20ability%20to%20perform%20refinements%20based%20on%20follow-up%20instructions.%20For%20this%0Apaper%27s%20analysis%2C%20we%20empirically%20evaluated%20our%20benchmarking%20pipeline%20with%0Aprogramming%20tasks%20from%20LiveBench%2C%20that%20are%20also%20automatically%20translated%20from%0APython%20into%20Java%20and%20JavaScript.%20Our%20automated%20benchmark%20reveals%20that%20models%0Aexhibit%20differing%20levels%20of%20performance%20across%20multiple%20dimensions%20of%0Ainstruction-following.%20Our%20benchmarking%20pipeline%20provides%20a%20more%20comprehensive%0Aevaluation%20of%20code%20generation%20models%2C%20highlighting%20their%20strengths%20and%0Alimitations%20across%20languages%20and%20generation%20goals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeAlignBench%253A%2520Assessing%2520Code%2520Generation%2520Models%2520on%2520Developer-Preferred%250A%2520%2520Code%2520Adjustments%26entry.906535625%3DForough%2520Mehralian%2520and%2520Ryan%2520Shar%2520and%2520James%2520R.%2520Rae%2520and%2520Alireza%2520Hashemi%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520become%2520increasingly%2520capable%2520of%2520generating%2520code%252C%250Aevaluating%2520their%2520performance%2520remains%2520a%2520complex%2520and%2520evolving%2520challenge.%2520Existing%250Abenchmarks%2520primarily%2520focus%2520on%2520functional%2520correctness%252C%2520overlooking%2520the%2520diversity%250Aof%2520real-world%2520coding%2520tasks%2520and%2520developer%2520expectations.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520multi-language%2520benchmark%2520that%2520evaluates%2520LLM%2520instruction-following%250Acapabilities%2520and%2520is%2520extensible%2520to%2520operate%2520on%2520any%2520set%2520of%2520standalone%2520coding%250Aproblems.%2520Our%2520benchmark%2520evaluates%2520instruction%2520following%2520in%2520two%2520key%2520settings%253A%250Aadherence%2520to%2520pre-defined%2520constraints%2520specified%2520with%2520the%2520initial%2520problem%252C%2520and%250Athe%2520ability%2520to%2520perform%2520refinements%2520based%2520on%2520follow-up%2520instructions.%2520For%2520this%250Apaper%2527s%2520analysis%252C%2520we%2520empirically%2520evaluated%2520our%2520benchmarking%2520pipeline%2520with%250Aprogramming%2520tasks%2520from%2520LiveBench%252C%2520that%2520are%2520also%2520automatically%2520translated%2520from%250APython%2520into%2520Java%2520and%2520JavaScript.%2520Our%2520automated%2520benchmark%2520reveals%2520that%2520models%250Aexhibit%2520differing%2520levels%2520of%2520performance%2520across%2520multiple%2520dimensions%2520of%250Ainstruction-following.%2520Our%2520benchmarking%2520pipeline%2520provides%2520a%2520more%2520comprehensive%250Aevaluation%2520of%2520code%2520generation%2520models%252C%2520highlighting%2520their%2520strengths%2520and%250Alimitations%2520across%2520languages%2520and%2520generation%2520goals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeAlignBench%3A%20Assessing%20Code%20Generation%20Models%20on%20Developer-Preferred%0A%20%20Code%20Adjustments&entry.906535625=Forough%20Mehralian%20and%20Ryan%20Shar%20and%20James%20R.%20Rae%20and%20Alireza%20Hashemi&entry.1292438233=%20%20As%20large%20language%20models%20become%20increasingly%20capable%20of%20generating%20code%2C%0Aevaluating%20their%20performance%20remains%20a%20complex%20and%20evolving%20challenge.%20Existing%0Abenchmarks%20primarily%20focus%20on%20functional%20correctness%2C%20overlooking%20the%20diversity%0Aof%20real-world%20coding%20tasks%20and%20developer%20expectations.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-language%20benchmark%20that%20evaluates%20LLM%20instruction-following%0Acapabilities%20and%20is%20extensible%20to%20operate%20on%20any%20set%20of%20standalone%20coding%0Aproblems.%20Our%20benchmark%20evaluates%20instruction%20following%20in%20two%20key%20settings%3A%0Aadherence%20to%20pre-defined%20constraints%20specified%20with%20the%20initial%20problem%2C%20and%0Athe%20ability%20to%20perform%20refinements%20based%20on%20follow-up%20instructions.%20For%20this%0Apaper%27s%20analysis%2C%20we%20empirically%20evaluated%20our%20benchmarking%20pipeline%20with%0Aprogramming%20tasks%20from%20LiveBench%2C%20that%20are%20also%20automatically%20translated%20from%0APython%20into%20Java%20and%20JavaScript.%20Our%20automated%20benchmark%20reveals%20that%20models%0Aexhibit%20differing%20levels%20of%20performance%20across%20multiple%20dimensions%20of%0Ainstruction-following.%20Our%20benchmarking%20pipeline%20provides%20a%20more%20comprehensive%0Aevaluation%20of%20code%20generation%20models%2C%20highlighting%20their%20strengths%20and%0Alimitations%20across%20languages%20and%20generation%20goals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27565v1&entry.124074799=Read"},
{"title": "A Regularized Newton Method for Nonconvex Optimization with Global and\n  Local Complexity Guarantees", "author": "Yuhao Zhou and Jintao Xu and Bingrui Li and Chenglong Bao and Chao Ding and Jun Zhu", "abstract": "  Finding an $\\epsilon$-stationary point of a nonconvex function with a\nLipschitz continuous Hessian is a central problem in optimization. Regularized\nNewton methods are a classical tool and have been studied extensively, yet they\nstill face a trade-off between global and local convergence. Whether a\nparameter-free algorithm of this type can simultaneously achieve optimal global\ncomplexity and quadratic local convergence remains an open question. To bridge\nthis long-standing gap, we propose a new class of regularizers constructed from\nthe current and previous gradients, and leverage the conjugate gradient\napproach with a negative curvature monitor to solve the regularized Newton\nequation. The proposed algorithm is adaptive, requiring no prior knowledge of\nthe Hessian Lipschitz constant, and achieves a global complexity of\n$O(\\epsilon^{-3/2})$ in terms of the second-order oracle calls, and\n$\\tilde{O}(\\epsilon^{-7/4})$ for Hessian-vector products, respectively. When\nthe iterates converge to a point where the Hessian is positive definite, the\nmethod exhibits quadratic local convergence. Preliminary numerical results,\nincluding training the physics-informed neural networks, illustrate the\ncompetitiveness of our algorithm.\n", "link": "http://arxiv.org/abs/2502.04799v3", "date": "2025-10-31", "relevancy": 2.2976, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4643}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4643}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&body=Title%3A%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees%0AAuthor%3A%20Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Bingrui%20Li%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20Finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%20nonconvex%20function%20with%20a%0ALipschitz%20continuous%20Hessian%20is%20a%20central%20problem%20in%20optimization.%20Regularized%0ANewton%20methods%20are%20a%20classical%20tool%20and%20have%20been%20studied%20extensively%2C%20yet%20they%0Astill%20face%20a%20trade-off%20between%20global%20and%20local%20convergence.%20Whether%20a%0Aparameter-free%20algorithm%20of%20this%20type%20can%20simultaneously%20achieve%20optimal%20global%0Acomplexity%20and%20quadratic%20local%20convergence%20remains%20an%20open%20question.%20To%20bridge%0Athis%20long-standing%20gap%2C%20we%20propose%20a%20new%20class%20of%20regularizers%20constructed%20from%0Athe%20current%20and%20previous%20gradients%2C%20and%20leverage%20the%20conjugate%20gradient%0Aapproach%20with%20a%20negative%20curvature%20monitor%20to%20solve%20the%20regularized%20Newton%0Aequation.%20The%20proposed%20algorithm%20is%20adaptive%2C%20requiring%20no%20prior%20knowledge%20of%0Athe%20Hessian%20Lipschitz%20constant%2C%20and%20achieves%20a%20global%20complexity%20of%0A%24O%28%5Cepsilon%5E%7B-3/2%7D%29%24%20in%20terms%20of%20the%20second-order%20oracle%20calls%2C%20and%0A%24%5Ctilde%7BO%7D%28%5Cepsilon%5E%7B-7/4%7D%29%24%20for%20Hessian-vector%20products%2C%20respectively.%20When%0Athe%20iterates%20converge%20to%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%0Amethod%20exhibits%20quadratic%20local%20convergence.%20Preliminary%20numerical%20results%2C%0Aincluding%20training%20the%20physics-informed%20neural%20networks%2C%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04799v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Regularized%2520Newton%2520Method%2520for%2520Nonconvex%2520Optimization%2520with%2520Global%2520and%250A%2520%2520Local%2520Complexity%2520Guarantees%26entry.906535625%3DYuhao%2520Zhou%2520and%2520Jintao%2520Xu%2520and%2520Bingrui%2520Li%2520and%2520Chenglong%2520Bao%2520and%2520Chao%2520Ding%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520Finding%2520an%2520%2524%255Cepsilon%2524-stationary%2520point%2520of%2520a%2520nonconvex%2520function%2520with%2520a%250ALipschitz%2520continuous%2520Hessian%2520is%2520a%2520central%2520problem%2520in%2520optimization.%2520Regularized%250ANewton%2520methods%2520are%2520a%2520classical%2520tool%2520and%2520have%2520been%2520studied%2520extensively%252C%2520yet%2520they%250Astill%2520face%2520a%2520trade-off%2520between%2520global%2520and%2520local%2520convergence.%2520Whether%2520a%250Aparameter-free%2520algorithm%2520of%2520this%2520type%2520can%2520simultaneously%2520achieve%2520optimal%2520global%250Acomplexity%2520and%2520quadratic%2520local%2520convergence%2520remains%2520an%2520open%2520question.%2520To%2520bridge%250Athis%2520long-standing%2520gap%252C%2520we%2520propose%2520a%2520new%2520class%2520of%2520regularizers%2520constructed%2520from%250Athe%2520current%2520and%2520previous%2520gradients%252C%2520and%2520leverage%2520the%2520conjugate%2520gradient%250Aapproach%2520with%2520a%2520negative%2520curvature%2520monitor%2520to%2520solve%2520the%2520regularized%2520Newton%250Aequation.%2520The%2520proposed%2520algorithm%2520is%2520adaptive%252C%2520requiring%2520no%2520prior%2520knowledge%2520of%250Athe%2520Hessian%2520Lipschitz%2520constant%252C%2520and%2520achieves%2520a%2520global%2520complexity%2520of%250A%2524O%2528%255Cepsilon%255E%257B-3/2%257D%2529%2524%2520in%2520terms%2520of%2520the%2520second-order%2520oracle%2520calls%252C%2520and%250A%2524%255Ctilde%257BO%257D%2528%255Cepsilon%255E%257B-7/4%257D%2529%2524%2520for%2520Hessian-vector%2520products%252C%2520respectively.%2520When%250Athe%2520iterates%2520converge%2520to%2520a%2520point%2520where%2520the%2520Hessian%2520is%2520positive%2520definite%252C%2520the%250Amethod%2520exhibits%2520quadratic%2520local%2520convergence.%2520Preliminary%2520numerical%2520results%252C%250Aincluding%2520training%2520the%2520physics-informed%2520neural%2520networks%252C%2520illustrate%2520the%250Acompetitiveness%2520of%2520our%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04799v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&entry.906535625=Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Bingrui%20Li%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu&entry.1292438233=%20%20Finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%20nonconvex%20function%20with%20a%0ALipschitz%20continuous%20Hessian%20is%20a%20central%20problem%20in%20optimization.%20Regularized%0ANewton%20methods%20are%20a%20classical%20tool%20and%20have%20been%20studied%20extensively%2C%20yet%20they%0Astill%20face%20a%20trade-off%20between%20global%20and%20local%20convergence.%20Whether%20a%0Aparameter-free%20algorithm%20of%20this%20type%20can%20simultaneously%20achieve%20optimal%20global%0Acomplexity%20and%20quadratic%20local%20convergence%20remains%20an%20open%20question.%20To%20bridge%0Athis%20long-standing%20gap%2C%20we%20propose%20a%20new%20class%20of%20regularizers%20constructed%20from%0Athe%20current%20and%20previous%20gradients%2C%20and%20leverage%20the%20conjugate%20gradient%0Aapproach%20with%20a%20negative%20curvature%20monitor%20to%20solve%20the%20regularized%20Newton%0Aequation.%20The%20proposed%20algorithm%20is%20adaptive%2C%20requiring%20no%20prior%20knowledge%20of%0Athe%20Hessian%20Lipschitz%20constant%2C%20and%20achieves%20a%20global%20complexity%20of%0A%24O%28%5Cepsilon%5E%7B-3/2%7D%29%24%20in%20terms%20of%20the%20second-order%20oracle%20calls%2C%20and%0A%24%5Ctilde%7BO%7D%28%5Cepsilon%5E%7B-7/4%7D%29%24%20for%20Hessian-vector%20products%2C%20respectively.%20When%0Athe%20iterates%20converge%20to%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%0Amethod%20exhibits%20quadratic%20local%20convergence.%20Preliminary%20numerical%20results%2C%0Aincluding%20training%20the%20physics-informed%20neural%20networks%2C%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04799v3&entry.124074799=Read"},
{"title": "Representing Classical Compositions through Implication-Realization\n  Temporal-Gestalt Graphs", "author": "A. V. Bomediano and R. J. Conanan and L. D. Santuyo and A. Coronel", "abstract": "  Understanding the structural and cognitive underpinnings of musical\ncompositions remains a key challenge in music theory and computational\nmusicology. While traditional methods focus on harmony and rhythm, cognitive\nmodels such as the Implication-Realization (I-R) model and Temporal Gestalt\ntheory offer insight into how listeners perceive and anticipate musical\nstructure. This study presents a graph-based computational approach that\noperationalizes these models by segmenting melodies into perceptual units and\nannotating them with I-R patterns. These segments are compared using Dynamic\nTime Warping and organized into k-nearest neighbors graphs to model intra- and\ninter-segment relationships.\n  Each segment is represented as a node in the graph, and nodes are further\nlabeled with melodic expectancy values derived from Schellenberg's two-factor\nI-R model-quantifying pitch proximity and pitch reversal at the segment level.\nThis labeling enables the graphs to encode both structural and cognitive\ninformation, reflecting how listeners experience musical tension and\nresolution.\n  To evaluate the expressiveness of these graphs, we apply the\nWeisfeiler-Lehman graph kernel to measure similarity between and within\ncompositions. Results reveal statistically significant distinctions between\nintra- and inter-graph structures. Segment-level analysis via multidimensional\nscaling confirms that structural similarity at the graph level reflects\nperceptual similarity at the segment level. Graph2vec embeddings and clustering\ndemonstrate that these representations capture stylistic and structural\nfeatures that extend beyond composer identity.\n  These findings highlight the potential of graph-based methods as a\nstructured, cognitively informed framework for computational music analysis,\nenabling a more nuanced understanding of musical structure and style through\nthe lens of listener perception.\n", "link": "http://arxiv.org/abs/2510.27530v1", "date": "2025-10-31", "relevancy": 2.2779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4581}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20Classical%20Compositions%20through%20Implication-Realization%0A%20%20Temporal-Gestalt%20Graphs&body=Title%3A%20Representing%20Classical%20Compositions%20through%20Implication-Realization%0A%20%20Temporal-Gestalt%20Graphs%0AAuthor%3A%20A.%20V.%20Bomediano%20and%20R.%20J.%20Conanan%20and%20L.%20D.%20Santuyo%20and%20A.%20Coronel%0AAbstract%3A%20%20%20Understanding%20the%20structural%20and%20cognitive%20underpinnings%20of%20musical%0Acompositions%20remains%20a%20key%20challenge%20in%20music%20theory%20and%20computational%0Amusicology.%20While%20traditional%20methods%20focus%20on%20harmony%20and%20rhythm%2C%20cognitive%0Amodels%20such%20as%20the%20Implication-Realization%20%28I-R%29%20model%20and%20Temporal%20Gestalt%0Atheory%20offer%20insight%20into%20how%20listeners%20perceive%20and%20anticipate%20musical%0Astructure.%20This%20study%20presents%20a%20graph-based%20computational%20approach%20that%0Aoperationalizes%20these%20models%20by%20segmenting%20melodies%20into%20perceptual%20units%20and%0Aannotating%20them%20with%20I-R%20patterns.%20These%20segments%20are%20compared%20using%20Dynamic%0ATime%20Warping%20and%20organized%20into%20k-nearest%20neighbors%20graphs%20to%20model%20intra-%20and%0Ainter-segment%20relationships.%0A%20%20Each%20segment%20is%20represented%20as%20a%20node%20in%20the%20graph%2C%20and%20nodes%20are%20further%0Alabeled%20with%20melodic%20expectancy%20values%20derived%20from%20Schellenberg%27s%20two-factor%0AI-R%20model-quantifying%20pitch%20proximity%20and%20pitch%20reversal%20at%20the%20segment%20level.%0AThis%20labeling%20enables%20the%20graphs%20to%20encode%20both%20structural%20and%20cognitive%0Ainformation%2C%20reflecting%20how%20listeners%20experience%20musical%20tension%20and%0Aresolution.%0A%20%20To%20evaluate%20the%20expressiveness%20of%20these%20graphs%2C%20we%20apply%20the%0AWeisfeiler-Lehman%20graph%20kernel%20to%20measure%20similarity%20between%20and%20within%0Acompositions.%20Results%20reveal%20statistically%20significant%20distinctions%20between%0Aintra-%20and%20inter-graph%20structures.%20Segment-level%20analysis%20via%20multidimensional%0Ascaling%20confirms%20that%20structural%20similarity%20at%20the%20graph%20level%20reflects%0Aperceptual%20similarity%20at%20the%20segment%20level.%20Graph2vec%20embeddings%20and%20clustering%0Ademonstrate%20that%20these%20representations%20capture%20stylistic%20and%20structural%0Afeatures%20that%20extend%20beyond%20composer%20identity.%0A%20%20These%20findings%20highlight%20the%20potential%20of%20graph-based%20methods%20as%20a%0Astructured%2C%20cognitively%20informed%20framework%20for%20computational%20music%20analysis%2C%0Aenabling%20a%20more%20nuanced%20understanding%20of%20musical%20structure%20and%20style%20through%0Athe%20lens%20of%20listener%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520Classical%2520Compositions%2520through%2520Implication-Realization%250A%2520%2520Temporal-Gestalt%2520Graphs%26entry.906535625%3DA.%2520V.%2520Bomediano%2520and%2520R.%2520J.%2520Conanan%2520and%2520L.%2520D.%2520Santuyo%2520and%2520A.%2520Coronel%26entry.1292438233%3D%2520%2520Understanding%2520the%2520structural%2520and%2520cognitive%2520underpinnings%2520of%2520musical%250Acompositions%2520remains%2520a%2520key%2520challenge%2520in%2520music%2520theory%2520and%2520computational%250Amusicology.%2520While%2520traditional%2520methods%2520focus%2520on%2520harmony%2520and%2520rhythm%252C%2520cognitive%250Amodels%2520such%2520as%2520the%2520Implication-Realization%2520%2528I-R%2529%2520model%2520and%2520Temporal%2520Gestalt%250Atheory%2520offer%2520insight%2520into%2520how%2520listeners%2520perceive%2520and%2520anticipate%2520musical%250Astructure.%2520This%2520study%2520presents%2520a%2520graph-based%2520computational%2520approach%2520that%250Aoperationalizes%2520these%2520models%2520by%2520segmenting%2520melodies%2520into%2520perceptual%2520units%2520and%250Aannotating%2520them%2520with%2520I-R%2520patterns.%2520These%2520segments%2520are%2520compared%2520using%2520Dynamic%250ATime%2520Warping%2520and%2520organized%2520into%2520k-nearest%2520neighbors%2520graphs%2520to%2520model%2520intra-%2520and%250Ainter-segment%2520relationships.%250A%2520%2520Each%2520segment%2520is%2520represented%2520as%2520a%2520node%2520in%2520the%2520graph%252C%2520and%2520nodes%2520are%2520further%250Alabeled%2520with%2520melodic%2520expectancy%2520values%2520derived%2520from%2520Schellenberg%2527s%2520two-factor%250AI-R%2520model-quantifying%2520pitch%2520proximity%2520and%2520pitch%2520reversal%2520at%2520the%2520segment%2520level.%250AThis%2520labeling%2520enables%2520the%2520graphs%2520to%2520encode%2520both%2520structural%2520and%2520cognitive%250Ainformation%252C%2520reflecting%2520how%2520listeners%2520experience%2520musical%2520tension%2520and%250Aresolution.%250A%2520%2520To%2520evaluate%2520the%2520expressiveness%2520of%2520these%2520graphs%252C%2520we%2520apply%2520the%250AWeisfeiler-Lehman%2520graph%2520kernel%2520to%2520measure%2520similarity%2520between%2520and%2520within%250Acompositions.%2520Results%2520reveal%2520statistically%2520significant%2520distinctions%2520between%250Aintra-%2520and%2520inter-graph%2520structures.%2520Segment-level%2520analysis%2520via%2520multidimensional%250Ascaling%2520confirms%2520that%2520structural%2520similarity%2520at%2520the%2520graph%2520level%2520reflects%250Aperceptual%2520similarity%2520at%2520the%2520segment%2520level.%2520Graph2vec%2520embeddings%2520and%2520clustering%250Ademonstrate%2520that%2520these%2520representations%2520capture%2520stylistic%2520and%2520structural%250Afeatures%2520that%2520extend%2520beyond%2520composer%2520identity.%250A%2520%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520graph-based%2520methods%2520as%2520a%250Astructured%252C%2520cognitively%2520informed%2520framework%2520for%2520computational%2520music%2520analysis%252C%250Aenabling%2520a%2520more%2520nuanced%2520understanding%2520of%2520musical%2520structure%2520and%2520style%2520through%250Athe%2520lens%2520of%2520listener%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20Classical%20Compositions%20through%20Implication-Realization%0A%20%20Temporal-Gestalt%20Graphs&entry.906535625=A.%20V.%20Bomediano%20and%20R.%20J.%20Conanan%20and%20L.%20D.%20Santuyo%20and%20A.%20Coronel&entry.1292438233=%20%20Understanding%20the%20structural%20and%20cognitive%20underpinnings%20of%20musical%0Acompositions%20remains%20a%20key%20challenge%20in%20music%20theory%20and%20computational%0Amusicology.%20While%20traditional%20methods%20focus%20on%20harmony%20and%20rhythm%2C%20cognitive%0Amodels%20such%20as%20the%20Implication-Realization%20%28I-R%29%20model%20and%20Temporal%20Gestalt%0Atheory%20offer%20insight%20into%20how%20listeners%20perceive%20and%20anticipate%20musical%0Astructure.%20This%20study%20presents%20a%20graph-based%20computational%20approach%20that%0Aoperationalizes%20these%20models%20by%20segmenting%20melodies%20into%20perceptual%20units%20and%0Aannotating%20them%20with%20I-R%20patterns.%20These%20segments%20are%20compared%20using%20Dynamic%0ATime%20Warping%20and%20organized%20into%20k-nearest%20neighbors%20graphs%20to%20model%20intra-%20and%0Ainter-segment%20relationships.%0A%20%20Each%20segment%20is%20represented%20as%20a%20node%20in%20the%20graph%2C%20and%20nodes%20are%20further%0Alabeled%20with%20melodic%20expectancy%20values%20derived%20from%20Schellenberg%27s%20two-factor%0AI-R%20model-quantifying%20pitch%20proximity%20and%20pitch%20reversal%20at%20the%20segment%20level.%0AThis%20labeling%20enables%20the%20graphs%20to%20encode%20both%20structural%20and%20cognitive%0Ainformation%2C%20reflecting%20how%20listeners%20experience%20musical%20tension%20and%0Aresolution.%0A%20%20To%20evaluate%20the%20expressiveness%20of%20these%20graphs%2C%20we%20apply%20the%0AWeisfeiler-Lehman%20graph%20kernel%20to%20measure%20similarity%20between%20and%20within%0Acompositions.%20Results%20reveal%20statistically%20significant%20distinctions%20between%0Aintra-%20and%20inter-graph%20structures.%20Segment-level%20analysis%20via%20multidimensional%0Ascaling%20confirms%20that%20structural%20similarity%20at%20the%20graph%20level%20reflects%0Aperceptual%20similarity%20at%20the%20segment%20level.%20Graph2vec%20embeddings%20and%20clustering%0Ademonstrate%20that%20these%20representations%20capture%20stylistic%20and%20structural%0Afeatures%20that%20extend%20beyond%20composer%20identity.%0A%20%20These%20findings%20highlight%20the%20potential%20of%20graph-based%20methods%20as%20a%0Astructured%2C%20cognitively%20informed%20framework%20for%20computational%20music%20analysis%2C%0Aenabling%20a%20more%20nuanced%20understanding%20of%20musical%20structure%20and%20style%20through%0Athe%20lens%20of%20listener%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27530v1&entry.124074799=Read"},
{"title": "Vision Transformer for Robust Occluded Person Reidentification in\n  Complex Surveillance Scenes", "author": "Bo Li and Duyuan Zheng and Xinyang Liu and Qingwen Li and Hong Li and Hongyan Cui and Ge Gao and Chen Liu", "abstract": "  Person re-identification (ReID) in surveillance is challenged by occlusion,\nviewpoint distortion, and poor image quality. Most existing methods rely on\ncomplex modules or perform well only on clear frontal images. We propose Sh-ViT\n(Shuffling Vision Transformer), a lightweight and robust model for occluded\nperson ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a\nShuffle module in the final Transformer layer to break spatial correlations and\nenhance robustness to occlusion and blur; Second, scenario-adapted augmentation\n(geometric transforms, erasing, blur, and color adjustment) to simulate\nsurveillance conditions; Third, DeiT-based knowledge distillation to improve\nlearning with limited labels.To support real-world evaluation, we construct the\nMyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base\nstation inspections, with frequent equipment occlusion and camera variations.\nExperiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,\noutperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on\nMarket1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves\nrobustness to occlusion and blur without external modules, offering a practical\nsolution for surveillance-based personnel monitoring.\n", "link": "http://arxiv.org/abs/2510.27677v1", "date": "2025-10-31", "relevancy": 2.2656, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5952}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5493}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformer%20for%20Robust%20Occluded%20Person%20Reidentification%20in%0A%20%20Complex%20Surveillance%20Scenes&body=Title%3A%20Vision%20Transformer%20for%20Robust%20Occluded%20Person%20Reidentification%20in%0A%20%20Complex%20Surveillance%20Scenes%0AAuthor%3A%20Bo%20Li%20and%20Duyuan%20Zheng%20and%20Xinyang%20Liu%20and%20Qingwen%20Li%20and%20Hong%20Li%20and%20Hongyan%20Cui%20and%20Ge%20Gao%20and%20Chen%20Liu%0AAbstract%3A%20%20%20Person%20re-identification%20%28ReID%29%20in%20surveillance%20is%20challenged%20by%20occlusion%2C%0Aviewpoint%20distortion%2C%20and%20poor%20image%20quality.%20Most%20existing%20methods%20rely%20on%0Acomplex%20modules%20or%20perform%20well%20only%20on%20clear%20frontal%20images.%20We%20propose%20Sh-ViT%0A%28Shuffling%20Vision%20Transformer%29%2C%20a%20lightweight%20and%20robust%20model%20for%20occluded%0Aperson%20ReID.%20Built%20on%20ViT-Base%2C%20Sh-ViT%20introduces%20three%20components%3A%20First%2C%20a%0AShuffle%20module%20in%20the%20final%20Transformer%20layer%20to%20break%20spatial%20correlations%20and%0Aenhance%20robustness%20to%20occlusion%20and%20blur%3B%20Second%2C%20scenario-adapted%20augmentation%0A%28geometric%20transforms%2C%20erasing%2C%20blur%2C%20and%20color%20adjustment%29%20to%20simulate%0Asurveillance%20conditions%3B%20Third%2C%20DeiT-based%20knowledge%20distillation%20to%20improve%0Alearning%20with%20limited%20labels.To%20support%20real-world%20evaluation%2C%20we%20construct%20the%0AMyTT%20dataset%2C%20containing%20over%2010%2C000%20pedestrians%20and%2030%2C000%2B%20images%20from%20base%0Astation%20inspections%2C%20with%20frequent%20equipment%20occlusion%20and%20camera%20variations.%0AExperiments%20show%20that%20Sh-ViT%20achieves%2083.2%25%20Rank-1%20and%2080.1%25%20mAP%20on%20MyTT%2C%0Aoutperforming%20CNN%20and%20ViT%20baselines%2C%20and%2094.6%25%20Rank-1%20and%2087.5%25%20mAP%20on%0AMarket1501%2C%20surpassing%20state-of-the-art%20methods.In%20summary%2C%20Sh-ViT%20improves%0Arobustness%20to%20occlusion%20and%20blur%20without%20external%20modules%2C%20offering%20a%20practical%0Asolution%20for%20surveillance-based%20personnel%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformer%2520for%2520Robust%2520Occluded%2520Person%2520Reidentification%2520in%250A%2520%2520Complex%2520Surveillance%2520Scenes%26entry.906535625%3DBo%2520Li%2520and%2520Duyuan%2520Zheng%2520and%2520Xinyang%2520Liu%2520and%2520Qingwen%2520Li%2520and%2520Hong%2520Li%2520and%2520Hongyan%2520Cui%2520and%2520Ge%2520Gao%2520and%2520Chen%2520Liu%26entry.1292438233%3D%2520%2520Person%2520re-identification%2520%2528ReID%2529%2520in%2520surveillance%2520is%2520challenged%2520by%2520occlusion%252C%250Aviewpoint%2520distortion%252C%2520and%2520poor%2520image%2520quality.%2520Most%2520existing%2520methods%2520rely%2520on%250Acomplex%2520modules%2520or%2520perform%2520well%2520only%2520on%2520clear%2520frontal%2520images.%2520We%2520propose%2520Sh-ViT%250A%2528Shuffling%2520Vision%2520Transformer%2529%252C%2520a%2520lightweight%2520and%2520robust%2520model%2520for%2520occluded%250Aperson%2520ReID.%2520Built%2520on%2520ViT-Base%252C%2520Sh-ViT%2520introduces%2520three%2520components%253A%2520First%252C%2520a%250AShuffle%2520module%2520in%2520the%2520final%2520Transformer%2520layer%2520to%2520break%2520spatial%2520correlations%2520and%250Aenhance%2520robustness%2520to%2520occlusion%2520and%2520blur%253B%2520Second%252C%2520scenario-adapted%2520augmentation%250A%2528geometric%2520transforms%252C%2520erasing%252C%2520blur%252C%2520and%2520color%2520adjustment%2529%2520to%2520simulate%250Asurveillance%2520conditions%253B%2520Third%252C%2520DeiT-based%2520knowledge%2520distillation%2520to%2520improve%250Alearning%2520with%2520limited%2520labels.To%2520support%2520real-world%2520evaluation%252C%2520we%2520construct%2520the%250AMyTT%2520dataset%252C%2520containing%2520over%252010%252C000%2520pedestrians%2520and%252030%252C000%252B%2520images%2520from%2520base%250Astation%2520inspections%252C%2520with%2520frequent%2520equipment%2520occlusion%2520and%2520camera%2520variations.%250AExperiments%2520show%2520that%2520Sh-ViT%2520achieves%252083.2%2525%2520Rank-1%2520and%252080.1%2525%2520mAP%2520on%2520MyTT%252C%250Aoutperforming%2520CNN%2520and%2520ViT%2520baselines%252C%2520and%252094.6%2525%2520Rank-1%2520and%252087.5%2525%2520mAP%2520on%250AMarket1501%252C%2520surpassing%2520state-of-the-art%2520methods.In%2520summary%252C%2520Sh-ViT%2520improves%250Arobustness%2520to%2520occlusion%2520and%2520blur%2520without%2520external%2520modules%252C%2520offering%2520a%2520practical%250Asolution%2520for%2520surveillance-based%2520personnel%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformer%20for%20Robust%20Occluded%20Person%20Reidentification%20in%0A%20%20Complex%20Surveillance%20Scenes&entry.906535625=Bo%20Li%20and%20Duyuan%20Zheng%20and%20Xinyang%20Liu%20and%20Qingwen%20Li%20and%20Hong%20Li%20and%20Hongyan%20Cui%20and%20Ge%20Gao%20and%20Chen%20Liu&entry.1292438233=%20%20Person%20re-identification%20%28ReID%29%20in%20surveillance%20is%20challenged%20by%20occlusion%2C%0Aviewpoint%20distortion%2C%20and%20poor%20image%20quality.%20Most%20existing%20methods%20rely%20on%0Acomplex%20modules%20or%20perform%20well%20only%20on%20clear%20frontal%20images.%20We%20propose%20Sh-ViT%0A%28Shuffling%20Vision%20Transformer%29%2C%20a%20lightweight%20and%20robust%20model%20for%20occluded%0Aperson%20ReID.%20Built%20on%20ViT-Base%2C%20Sh-ViT%20introduces%20three%20components%3A%20First%2C%20a%0AShuffle%20module%20in%20the%20final%20Transformer%20layer%20to%20break%20spatial%20correlations%20and%0Aenhance%20robustness%20to%20occlusion%20and%20blur%3B%20Second%2C%20scenario-adapted%20augmentation%0A%28geometric%20transforms%2C%20erasing%2C%20blur%2C%20and%20color%20adjustment%29%20to%20simulate%0Asurveillance%20conditions%3B%20Third%2C%20DeiT-based%20knowledge%20distillation%20to%20improve%0Alearning%20with%20limited%20labels.To%20support%20real-world%20evaluation%2C%20we%20construct%20the%0AMyTT%20dataset%2C%20containing%20over%2010%2C000%20pedestrians%20and%2030%2C000%2B%20images%20from%20base%0Astation%20inspections%2C%20with%20frequent%20equipment%20occlusion%20and%20camera%20variations.%0AExperiments%20show%20that%20Sh-ViT%20achieves%2083.2%25%20Rank-1%20and%2080.1%25%20mAP%20on%20MyTT%2C%0Aoutperforming%20CNN%20and%20ViT%20baselines%2C%20and%2094.6%25%20Rank-1%20and%2087.5%25%20mAP%20on%0AMarket1501%2C%20surpassing%20state-of-the-art%20methods.In%20summary%2C%20Sh-ViT%20improves%0Arobustness%20to%20occlusion%20and%20blur%20without%20external%20modules%2C%20offering%20a%20practical%0Asolution%20for%20surveillance-based%20personnel%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27677v1&entry.124074799=Read"},
{"title": "On Uncertainty Calibration for Equivariant Functions", "author": "Edward Berman and Jacob Ginesin and Marco Pacini and Robin Walters", "abstract": "  Data-sparse settings such as robotic manipulation, molecular physics, and\ngalaxy morphology classification are some of the hardest domains for deep\nlearning. For these problems, equivariant networks can help improve modeling\nacross undersampled parts of the input space, and uncertainty estimation can\nguard against overconfidence. However, until now, the relationships between\nequivariance and model confidence, and more generally equivariance and model\ncalibration, has yet to be studied. Since traditional classification and\nregression error terms show up in the definitions of calibration error, it is\nnatural to suspect that previous work can be used to help understand the\nrelationship between equivariance and calibration error. In this work, we\npresent a theory relating equivariance to uncertainty estimation. By proving\nlower and upper bounds on uncertainty calibration errors (ECE and ENCE) under\nvarious equivariance conditions, we elucidate the generalization limits of\nequivariant models and illustrate how symmetry mismatch can result in\nmiscalibration in both classification and regression. We complement our\ntheoretical framework with numerical experiments that clarify the relationship\nbetween equivariance and uncertainty using a variety of real and simulated\ndatasets, and we comment on trends with symmetry mismatch, group size, and\naleatoric and epistemic uncertainties.\n", "link": "http://arxiv.org/abs/2510.21691v3", "date": "2025-10-31", "relevancy": 2.2052, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5817}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Uncertainty%20Calibration%20for%20Equivariant%20Functions&body=Title%3A%20On%20Uncertainty%20Calibration%20for%20Equivariant%20Functions%0AAuthor%3A%20Edward%20Berman%20and%20Jacob%20Ginesin%20and%20Marco%20Pacini%20and%20Robin%20Walters%0AAbstract%3A%20%20%20Data-sparse%20settings%20such%20as%20robotic%20manipulation%2C%20molecular%20physics%2C%20and%0Agalaxy%20morphology%20classification%20are%20some%20of%20the%20hardest%20domains%20for%20deep%0Alearning.%20For%20these%20problems%2C%20equivariant%20networks%20can%20help%20improve%20modeling%0Aacross%20undersampled%20parts%20of%20the%20input%20space%2C%20and%20uncertainty%20estimation%20can%0Aguard%20against%20overconfidence.%20However%2C%20until%20now%2C%20the%20relationships%20between%0Aequivariance%20and%20model%20confidence%2C%20and%20more%20generally%20equivariance%20and%20model%0Acalibration%2C%20has%20yet%20to%20be%20studied.%20Since%20traditional%20classification%20and%0Aregression%20error%20terms%20show%20up%20in%20the%20definitions%20of%20calibration%20error%2C%20it%20is%0Anatural%20to%20suspect%20that%20previous%20work%20can%20be%20used%20to%20help%20understand%20the%0Arelationship%20between%20equivariance%20and%20calibration%20error.%20In%20this%20work%2C%20we%0Apresent%20a%20theory%20relating%20equivariance%20to%20uncertainty%20estimation.%20By%20proving%0Alower%20and%20upper%20bounds%20on%20uncertainty%20calibration%20errors%20%28ECE%20and%20ENCE%29%20under%0Avarious%20equivariance%20conditions%2C%20we%20elucidate%20the%20generalization%20limits%20of%0Aequivariant%20models%20and%20illustrate%20how%20symmetry%20mismatch%20can%20result%20in%0Amiscalibration%20in%20both%20classification%20and%20regression.%20We%20complement%20our%0Atheoretical%20framework%20with%20numerical%20experiments%20that%20clarify%20the%20relationship%0Abetween%20equivariance%20and%20uncertainty%20using%20a%20variety%20of%20real%20and%20simulated%0Adatasets%2C%20and%20we%20comment%20on%20trends%20with%20symmetry%20mismatch%2C%20group%20size%2C%20and%0Aaleatoric%20and%20epistemic%20uncertainties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.21691v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Uncertainty%2520Calibration%2520for%2520Equivariant%2520Functions%26entry.906535625%3DEdward%2520Berman%2520and%2520Jacob%2520Ginesin%2520and%2520Marco%2520Pacini%2520and%2520Robin%2520Walters%26entry.1292438233%3D%2520%2520Data-sparse%2520settings%2520such%2520as%2520robotic%2520manipulation%252C%2520molecular%2520physics%252C%2520and%250Agalaxy%2520morphology%2520classification%2520are%2520some%2520of%2520the%2520hardest%2520domains%2520for%2520deep%250Alearning.%2520For%2520these%2520problems%252C%2520equivariant%2520networks%2520can%2520help%2520improve%2520modeling%250Aacross%2520undersampled%2520parts%2520of%2520the%2520input%2520space%252C%2520and%2520uncertainty%2520estimation%2520can%250Aguard%2520against%2520overconfidence.%2520However%252C%2520until%2520now%252C%2520the%2520relationships%2520between%250Aequivariance%2520and%2520model%2520confidence%252C%2520and%2520more%2520generally%2520equivariance%2520and%2520model%250Acalibration%252C%2520has%2520yet%2520to%2520be%2520studied.%2520Since%2520traditional%2520classification%2520and%250Aregression%2520error%2520terms%2520show%2520up%2520in%2520the%2520definitions%2520of%2520calibration%2520error%252C%2520it%2520is%250Anatural%2520to%2520suspect%2520that%2520previous%2520work%2520can%2520be%2520used%2520to%2520help%2520understand%2520the%250Arelationship%2520between%2520equivariance%2520and%2520calibration%2520error.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520theory%2520relating%2520equivariance%2520to%2520uncertainty%2520estimation.%2520By%2520proving%250Alower%2520and%2520upper%2520bounds%2520on%2520uncertainty%2520calibration%2520errors%2520%2528ECE%2520and%2520ENCE%2529%2520under%250Avarious%2520equivariance%2520conditions%252C%2520we%2520elucidate%2520the%2520generalization%2520limits%2520of%250Aequivariant%2520models%2520and%2520illustrate%2520how%2520symmetry%2520mismatch%2520can%2520result%2520in%250Amiscalibration%2520in%2520both%2520classification%2520and%2520regression.%2520We%2520complement%2520our%250Atheoretical%2520framework%2520with%2520numerical%2520experiments%2520that%2520clarify%2520the%2520relationship%250Abetween%2520equivariance%2520and%2520uncertainty%2520using%2520a%2520variety%2520of%2520real%2520and%2520simulated%250Adatasets%252C%2520and%2520we%2520comment%2520on%2520trends%2520with%2520symmetry%2520mismatch%252C%2520group%2520size%252C%2520and%250Aaleatoric%2520and%2520epistemic%2520uncertainties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21691v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Uncertainty%20Calibration%20for%20Equivariant%20Functions&entry.906535625=Edward%20Berman%20and%20Jacob%20Ginesin%20and%20Marco%20Pacini%20and%20Robin%20Walters&entry.1292438233=%20%20Data-sparse%20settings%20such%20as%20robotic%20manipulation%2C%20molecular%20physics%2C%20and%0Agalaxy%20morphology%20classification%20are%20some%20of%20the%20hardest%20domains%20for%20deep%0Alearning.%20For%20these%20problems%2C%20equivariant%20networks%20can%20help%20improve%20modeling%0Aacross%20undersampled%20parts%20of%20the%20input%20space%2C%20and%20uncertainty%20estimation%20can%0Aguard%20against%20overconfidence.%20However%2C%20until%20now%2C%20the%20relationships%20between%0Aequivariance%20and%20model%20confidence%2C%20and%20more%20generally%20equivariance%20and%20model%0Acalibration%2C%20has%20yet%20to%20be%20studied.%20Since%20traditional%20classification%20and%0Aregression%20error%20terms%20show%20up%20in%20the%20definitions%20of%20calibration%20error%2C%20it%20is%0Anatural%20to%20suspect%20that%20previous%20work%20can%20be%20used%20to%20help%20understand%20the%0Arelationship%20between%20equivariance%20and%20calibration%20error.%20In%20this%20work%2C%20we%0Apresent%20a%20theory%20relating%20equivariance%20to%20uncertainty%20estimation.%20By%20proving%0Alower%20and%20upper%20bounds%20on%20uncertainty%20calibration%20errors%20%28ECE%20and%20ENCE%29%20under%0Avarious%20equivariance%20conditions%2C%20we%20elucidate%20the%20generalization%20limits%20of%0Aequivariant%20models%20and%20illustrate%20how%20symmetry%20mismatch%20can%20result%20in%0Amiscalibration%20in%20both%20classification%20and%20regression.%20We%20complement%20our%0Atheoretical%20framework%20with%20numerical%20experiments%20that%20clarify%20the%20relationship%0Abetween%20equivariance%20and%20uncertainty%20using%20a%20variety%20of%20real%20and%20simulated%0Adatasets%2C%20and%20we%20comment%20on%20trends%20with%20symmetry%20mismatch%2C%20group%20size%2C%20and%0Aaleatoric%20and%20epistemic%20uncertainties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.21691v3&entry.124074799=Read"},
{"title": "MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design", "author": "Wei Zhang and Zekun Guo and Yingce Xia and Peiran Jin and Shufang Xie and Tao Qin and Xiang-Yang Li", "abstract": "  Structure-based drug design (SBDD), which maps target proteins to candidate\nmolecular ligands, is a fundamental task in drug discovery. Effectively\naligning protein structural representations with molecular representations, and\nensuring alignment between generated drugs and their pharmacological\nproperties, remains a critical challenge. To address these challenges, we\npropose MolChord, which integrates two key techniques: (1) to align protein and\nmolecule structures with their textual descriptions and sequential\nrepresentations (e.g., FASTA for proteins and SMILES for molecules), we\nleverage NatureLM, an autoregressive model unifying text, small molecules, and\nproteins, as the molecule generator, alongside a diffusion-based structure\nencoder; and (2) to guide molecules toward desired properties, we curate a\nproperty-aware dataset by integrating preference data and refine the alignment\nprocess using Direct Preference Optimization (DPO). Experimental results on\nCrossDocked2020 demonstrate that our approach achieves state-of-the-art\nperformance on key evaluation metrics, highlighting its potential as a\npractical tool for SBDD.\n", "link": "http://arxiv.org/abs/2510.27671v1", "date": "2025-10-31", "relevancy": 2.1976, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolChord%3A%20Structure-Sequence%20Alignment%20for%20Protein-Guided%20Drug%20Design&body=Title%3A%20MolChord%3A%20Structure-Sequence%20Alignment%20for%20Protein-Guided%20Drug%20Design%0AAuthor%3A%20Wei%20Zhang%20and%20Zekun%20Guo%20and%20Yingce%20Xia%20and%20Peiran%20Jin%20and%20Shufang%20Xie%20and%20Tao%20Qin%20and%20Xiang-Yang%20Li%0AAbstract%3A%20%20%20Structure-based%20drug%20design%20%28SBDD%29%2C%20which%20maps%20target%20proteins%20to%20candidate%0Amolecular%20ligands%2C%20is%20a%20fundamental%20task%20in%20drug%20discovery.%20Effectively%0Aaligning%20protein%20structural%20representations%20with%20molecular%20representations%2C%20and%0Aensuring%20alignment%20between%20generated%20drugs%20and%20their%20pharmacological%0Aproperties%2C%20remains%20a%20critical%20challenge.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MolChord%2C%20which%20integrates%20two%20key%20techniques%3A%20%281%29%20to%20align%20protein%20and%0Amolecule%20structures%20with%20their%20textual%20descriptions%20and%20sequential%0Arepresentations%20%28e.g.%2C%20FASTA%20for%20proteins%20and%20SMILES%20for%20molecules%29%2C%20we%0Aleverage%20NatureLM%2C%20an%20autoregressive%20model%20unifying%20text%2C%20small%20molecules%2C%20and%0Aproteins%2C%20as%20the%20molecule%20generator%2C%20alongside%20a%20diffusion-based%20structure%0Aencoder%3B%20and%20%282%29%20to%20guide%20molecules%20toward%20desired%20properties%2C%20we%20curate%20a%0Aproperty-aware%20dataset%20by%20integrating%20preference%20data%20and%20refine%20the%20alignment%0Aprocess%20using%20Direct%20Preference%20Optimization%20%28DPO%29.%20Experimental%20results%20on%0ACrossDocked2020%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20key%20evaluation%20metrics%2C%20highlighting%20its%20potential%20as%20a%0Apractical%20tool%20for%20SBDD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolChord%253A%2520Structure-Sequence%2520Alignment%2520for%2520Protein-Guided%2520Drug%2520Design%26entry.906535625%3DWei%2520Zhang%2520and%2520Zekun%2520Guo%2520and%2520Yingce%2520Xia%2520and%2520Peiran%2520Jin%2520and%2520Shufang%2520Xie%2520and%2520Tao%2520Qin%2520and%2520Xiang-Yang%2520Li%26entry.1292438233%3D%2520%2520Structure-based%2520drug%2520design%2520%2528SBDD%2529%252C%2520which%2520maps%2520target%2520proteins%2520to%2520candidate%250Amolecular%2520ligands%252C%2520is%2520a%2520fundamental%2520task%2520in%2520drug%2520discovery.%2520Effectively%250Aaligning%2520protein%2520structural%2520representations%2520with%2520molecular%2520representations%252C%2520and%250Aensuring%2520alignment%2520between%2520generated%2520drugs%2520and%2520their%2520pharmacological%250Aproperties%252C%2520remains%2520a%2520critical%2520challenge.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520MolChord%252C%2520which%2520integrates%2520two%2520key%2520techniques%253A%2520%25281%2529%2520to%2520align%2520protein%2520and%250Amolecule%2520structures%2520with%2520their%2520textual%2520descriptions%2520and%2520sequential%250Arepresentations%2520%2528e.g.%252C%2520FASTA%2520for%2520proteins%2520and%2520SMILES%2520for%2520molecules%2529%252C%2520we%250Aleverage%2520NatureLM%252C%2520an%2520autoregressive%2520model%2520unifying%2520text%252C%2520small%2520molecules%252C%2520and%250Aproteins%252C%2520as%2520the%2520molecule%2520generator%252C%2520alongside%2520a%2520diffusion-based%2520structure%250Aencoder%253B%2520and%2520%25282%2529%2520to%2520guide%2520molecules%2520toward%2520desired%2520properties%252C%2520we%2520curate%2520a%250Aproperty-aware%2520dataset%2520by%2520integrating%2520preference%2520data%2520and%2520refine%2520the%2520alignment%250Aprocess%2520using%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520Experimental%2520results%2520on%250ACrossDocked2020%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520key%2520evaluation%2520metrics%252C%2520highlighting%2520its%2520potential%2520as%2520a%250Apractical%2520tool%2520for%2520SBDD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolChord%3A%20Structure-Sequence%20Alignment%20for%20Protein-Guided%20Drug%20Design&entry.906535625=Wei%20Zhang%20and%20Zekun%20Guo%20and%20Yingce%20Xia%20and%20Peiran%20Jin%20and%20Shufang%20Xie%20and%20Tao%20Qin%20and%20Xiang-Yang%20Li&entry.1292438233=%20%20Structure-based%20drug%20design%20%28SBDD%29%2C%20which%20maps%20target%20proteins%20to%20candidate%0Amolecular%20ligands%2C%20is%20a%20fundamental%20task%20in%20drug%20discovery.%20Effectively%0Aaligning%20protein%20structural%20representations%20with%20molecular%20representations%2C%20and%0Aensuring%20alignment%20between%20generated%20drugs%20and%20their%20pharmacological%0Aproperties%2C%20remains%20a%20critical%20challenge.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MolChord%2C%20which%20integrates%20two%20key%20techniques%3A%20%281%29%20to%20align%20protein%20and%0Amolecule%20structures%20with%20their%20textual%20descriptions%20and%20sequential%0Arepresentations%20%28e.g.%2C%20FASTA%20for%20proteins%20and%20SMILES%20for%20molecules%29%2C%20we%0Aleverage%20NatureLM%2C%20an%20autoregressive%20model%20unifying%20text%2C%20small%20molecules%2C%20and%0Aproteins%2C%20as%20the%20molecule%20generator%2C%20alongside%20a%20diffusion-based%20structure%0Aencoder%3B%20and%20%282%29%20to%20guide%20molecules%20toward%20desired%20properties%2C%20we%20curate%20a%0Aproperty-aware%20dataset%20by%20integrating%20preference%20data%20and%20refine%20the%20alignment%0Aprocess%20using%20Direct%20Preference%20Optimization%20%28DPO%29.%20Experimental%20results%20on%0ACrossDocked2020%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20key%20evaluation%20metrics%2C%20highlighting%20its%20potential%20as%20a%0Apractical%20tool%20for%20SBDD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27671v1&entry.124074799=Read"},
{"title": "Conformal Object Detection by Sequential Risk Control", "author": "L\u00e9o and\u00e9ol and Luca Mossina and Adrien Mazoyer and S\u00e9bastien Gerchinovitz", "abstract": "  Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in safety-critical applications is hindered by\nthe inherent lack of reliability of neural networks and the complex structure\nof object detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc predictive uncertainty quantification procedure with\nstatistical guarantees that are valid for any dataset size, without requiring\nprior knowledge on the model or data distribution. Our contribution is\nmanifold. First, we formally define the problem of Conformal Object Detection\n(COD). We introduce a novel method, Sequential Conformal Risk Control (SeqCRC),\nthat extends the statistical guarantees of Conformal Risk Control to two\nsequential tasks with two parameters, as required in the COD setting. Then, we\npresent old and new loss functions and prediction sets suited to applying\nSeqCRC to different cases and certification requirements. Finally, we present a\nconformal toolkit for replication and further exploration of our method. Using\nthis toolkit, we perform extensive experiments that validate our approach and\nemphasize trade-offs and other practical consequences.\n", "link": "http://arxiv.org/abs/2505.24038v2", "date": "2025-10-31", "relevancy": 2.1683, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5556}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5443}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Object%20Detection%20by%20Sequential%20Risk%20Control&body=Title%3A%20Conformal%20Object%20Detection%20by%20Sequential%20Risk%20Control%0AAuthor%3A%20L%C3%A9o%20and%C3%A9ol%20and%20Luca%20Mossina%20and%20Adrien%20Mazoyer%20and%20S%C3%A9bastien%20Gerchinovitz%0AAbstract%3A%20%20%20Recent%20advances%20in%20object%20detectors%20have%20led%20to%20their%20adoption%20for%20industrial%0Auses.%20However%2C%20their%20deployment%20in%20safety-critical%20applications%20is%20hindered%20by%0Athe%20inherent%20lack%20of%20reliability%20of%20neural%20networks%20and%20the%20complex%20structure%0Aof%20object%20detection%20models.%20To%20address%20these%20challenges%2C%20we%20turn%20to%20Conformal%0APrediction%2C%20a%20post-hoc%20predictive%20uncertainty%20quantification%20procedure%20with%0Astatistical%20guarantees%20that%20are%20valid%20for%20any%20dataset%20size%2C%20without%20requiring%0Aprior%20knowledge%20on%20the%20model%20or%20data%20distribution.%20Our%20contribution%20is%0Amanifold.%20First%2C%20we%20formally%20define%20the%20problem%20of%20Conformal%20Object%20Detection%0A%28COD%29.%20We%20introduce%20a%20novel%20method%2C%20Sequential%20Conformal%20Risk%20Control%20%28SeqCRC%29%2C%0Athat%20extends%20the%20statistical%20guarantees%20of%20Conformal%20Risk%20Control%20to%20two%0Asequential%20tasks%20with%20two%20parameters%2C%20as%20required%20in%20the%20COD%20setting.%20Then%2C%20we%0Apresent%20old%20and%20new%20loss%20functions%20and%20prediction%20sets%20suited%20to%20applying%0ASeqCRC%20to%20different%20cases%20and%20certification%20requirements.%20Finally%2C%20we%20present%20a%0Aconformal%20toolkit%20for%20replication%20and%20further%20exploration%20of%20our%20method.%20Using%0Athis%20toolkit%2C%20we%20perform%20extensive%20experiments%20that%20validate%20our%20approach%20and%0Aemphasize%20trade-offs%20and%20other%20practical%20consequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Object%2520Detection%2520by%2520Sequential%2520Risk%2520Control%26entry.906535625%3DL%25C3%25A9o%2520and%25C3%25A9ol%2520and%2520Luca%2520Mossina%2520and%2520Adrien%2520Mazoyer%2520and%2520S%25C3%25A9bastien%2520Gerchinovitz%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520object%2520detectors%2520have%2520led%2520to%2520their%2520adoption%2520for%2520industrial%250Auses.%2520However%252C%2520their%2520deployment%2520in%2520safety-critical%2520applications%2520is%2520hindered%2520by%250Athe%2520inherent%2520lack%2520of%2520reliability%2520of%2520neural%2520networks%2520and%2520the%2520complex%2520structure%250Aof%2520object%2520detection%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520turn%2520to%2520Conformal%250APrediction%252C%2520a%2520post-hoc%2520predictive%2520uncertainty%2520quantification%2520procedure%2520with%250Astatistical%2520guarantees%2520that%2520are%2520valid%2520for%2520any%2520dataset%2520size%252C%2520without%2520requiring%250Aprior%2520knowledge%2520on%2520the%2520model%2520or%2520data%2520distribution.%2520Our%2520contribution%2520is%250Amanifold.%2520First%252C%2520we%2520formally%2520define%2520the%2520problem%2520of%2520Conformal%2520Object%2520Detection%250A%2528COD%2529.%2520We%2520introduce%2520a%2520novel%2520method%252C%2520Sequential%2520Conformal%2520Risk%2520Control%2520%2528SeqCRC%2529%252C%250Athat%2520extends%2520the%2520statistical%2520guarantees%2520of%2520Conformal%2520Risk%2520Control%2520to%2520two%250Asequential%2520tasks%2520with%2520two%2520parameters%252C%2520as%2520required%2520in%2520the%2520COD%2520setting.%2520Then%252C%2520we%250Apresent%2520old%2520and%2520new%2520loss%2520functions%2520and%2520prediction%2520sets%2520suited%2520to%2520applying%250ASeqCRC%2520to%2520different%2520cases%2520and%2520certification%2520requirements.%2520Finally%252C%2520we%2520present%2520a%250Aconformal%2520toolkit%2520for%2520replication%2520and%2520further%2520exploration%2520of%2520our%2520method.%2520Using%250Athis%2520toolkit%252C%2520we%2520perform%2520extensive%2520experiments%2520that%2520validate%2520our%2520approach%2520and%250Aemphasize%2520trade-offs%2520and%2520other%2520practical%2520consequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Object%20Detection%20by%20Sequential%20Risk%20Control&entry.906535625=L%C3%A9o%20and%C3%A9ol%20and%20Luca%20Mossina%20and%20Adrien%20Mazoyer%20and%20S%C3%A9bastien%20Gerchinovitz&entry.1292438233=%20%20Recent%20advances%20in%20object%20detectors%20have%20led%20to%20their%20adoption%20for%20industrial%0Auses.%20However%2C%20their%20deployment%20in%20safety-critical%20applications%20is%20hindered%20by%0Athe%20inherent%20lack%20of%20reliability%20of%20neural%20networks%20and%20the%20complex%20structure%0Aof%20object%20detection%20models.%20To%20address%20these%20challenges%2C%20we%20turn%20to%20Conformal%0APrediction%2C%20a%20post-hoc%20predictive%20uncertainty%20quantification%20procedure%20with%0Astatistical%20guarantees%20that%20are%20valid%20for%20any%20dataset%20size%2C%20without%20requiring%0Aprior%20knowledge%20on%20the%20model%20or%20data%20distribution.%20Our%20contribution%20is%0Amanifold.%20First%2C%20we%20formally%20define%20the%20problem%20of%20Conformal%20Object%20Detection%0A%28COD%29.%20We%20introduce%20a%20novel%20method%2C%20Sequential%20Conformal%20Risk%20Control%20%28SeqCRC%29%2C%0Athat%20extends%20the%20statistical%20guarantees%20of%20Conformal%20Risk%20Control%20to%20two%0Asequential%20tasks%20with%20two%20parameters%2C%20as%20required%20in%20the%20COD%20setting.%20Then%2C%20we%0Apresent%20old%20and%20new%20loss%20functions%20and%20prediction%20sets%20suited%20to%20applying%0ASeqCRC%20to%20different%20cases%20and%20certification%20requirements.%20Finally%2C%20we%20present%20a%0Aconformal%20toolkit%20for%20replication%20and%20further%20exploration%20of%20our%20method.%20Using%0Athis%20toolkit%2C%20we%20perform%20extensive%20experiments%20that%20validate%20our%20approach%20and%0Aemphasize%20trade-offs%20and%20other%20practical%20consequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24038v2&entry.124074799=Read"},
{"title": "Validity Is What You Need", "author": "Sebastian Benthall and Andrew Clark", "abstract": "  While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it.\n", "link": "http://arxiv.org/abs/2510.27628v1", "date": "2025-10-31", "relevancy": 2.1671, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Validity%20Is%20What%20You%20Need&body=Title%3A%20Validity%20Is%20What%20You%20Need%0AAuthor%3A%20Sebastian%20Benthall%20and%20Andrew%20Clark%0AAbstract%3A%20%20%20While%20AI%20agents%20have%20long%20been%20discussed%20and%20studied%20in%20computer%20science%2C%0Atoday%27s%20Agentic%20AI%20systems%20are%20something%20new.%20We%20consider%20other%20definitions%20of%0AAgentic%20AI%20and%20propose%20a%20new%20realist%20definition.%20Agentic%20AI%20is%20a%20software%0Adelivery%20mechanism%2C%20comparable%20to%20software%20as%20a%20service%20%28SaaS%29%2C%20which%20puts%20an%0Aapplication%20to%20work%20autonomously%20in%20a%20complex%20enterprise%20setting.%20Recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20as%20foundation%20models%20have%20driven%0Aexcitement%20in%20Agentic%20AI.%20We%20note%2C%20however%2C%20that%20Agentic%20AI%20systems%20are%0Aprimarily%20applications%2C%20not%20foundations%2C%20and%20so%20their%20success%20depends%20on%0Avalidation%20by%20end%20users%20and%20principal%20stakeholders.%20The%20tools%20and%20techniques%0Aneeded%20by%20the%20principal%20users%20to%20validate%20their%20applications%20are%20quite%0Adifferent%20from%20the%20tools%20and%20techniques%20used%20to%20evaluate%20foundation%20models.%0AIronically%2C%20with%20good%20validation%20measures%20in%20place%2C%20in%20many%20cases%20the%0Afoundation%20models%20can%20be%20replaced%20with%20much%20simpler%2C%20faster%2C%20and%20more%0Ainterpretable%20models%20that%20handle%20core%20logic.%20When%20it%20comes%20to%20Agentic%20AI%2C%0Avalidity%20is%20what%20you%20need.%20LLMs%20are%20one%20option%20that%20might%20achieve%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValidity%2520Is%2520What%2520You%2520Need%26entry.906535625%3DSebastian%2520Benthall%2520and%2520Andrew%2520Clark%26entry.1292438233%3D%2520%2520While%2520AI%2520agents%2520have%2520long%2520been%2520discussed%2520and%2520studied%2520in%2520computer%2520science%252C%250Atoday%2527s%2520Agentic%2520AI%2520systems%2520are%2520something%2520new.%2520We%2520consider%2520other%2520definitions%2520of%250AAgentic%2520AI%2520and%2520propose%2520a%2520new%2520realist%2520definition.%2520Agentic%2520AI%2520is%2520a%2520software%250Adelivery%2520mechanism%252C%2520comparable%2520to%2520software%2520as%2520a%2520service%2520%2528SaaS%2529%252C%2520which%2520puts%2520an%250Aapplication%2520to%2520work%2520autonomously%2520in%2520a%2520complex%2520enterprise%2520setting.%2520Recent%250Aadvances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520foundation%2520models%2520have%2520driven%250Aexcitement%2520in%2520Agentic%2520AI.%2520We%2520note%252C%2520however%252C%2520that%2520Agentic%2520AI%2520systems%2520are%250Aprimarily%2520applications%252C%2520not%2520foundations%252C%2520and%2520so%2520their%2520success%2520depends%2520on%250Avalidation%2520by%2520end%2520users%2520and%2520principal%2520stakeholders.%2520The%2520tools%2520and%2520techniques%250Aneeded%2520by%2520the%2520principal%2520users%2520to%2520validate%2520their%2520applications%2520are%2520quite%250Adifferent%2520from%2520the%2520tools%2520and%2520techniques%2520used%2520to%2520evaluate%2520foundation%2520models.%250AIronically%252C%2520with%2520good%2520validation%2520measures%2520in%2520place%252C%2520in%2520many%2520cases%2520the%250Afoundation%2520models%2520can%2520be%2520replaced%2520with%2520much%2520simpler%252C%2520faster%252C%2520and%2520more%250Ainterpretable%2520models%2520that%2520handle%2520core%2520logic.%2520When%2520it%2520comes%2520to%2520Agentic%2520AI%252C%250Avalidity%2520is%2520what%2520you%2520need.%2520LLMs%2520are%2520one%2520option%2520that%2520might%2520achieve%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Validity%20Is%20What%20You%20Need&entry.906535625=Sebastian%20Benthall%20and%20Andrew%20Clark&entry.1292438233=%20%20While%20AI%20agents%20have%20long%20been%20discussed%20and%20studied%20in%20computer%20science%2C%0Atoday%27s%20Agentic%20AI%20systems%20are%20something%20new.%20We%20consider%20other%20definitions%20of%0AAgentic%20AI%20and%20propose%20a%20new%20realist%20definition.%20Agentic%20AI%20is%20a%20software%0Adelivery%20mechanism%2C%20comparable%20to%20software%20as%20a%20service%20%28SaaS%29%2C%20which%20puts%20an%0Aapplication%20to%20work%20autonomously%20in%20a%20complex%20enterprise%20setting.%20Recent%0Aadvances%20in%20large%20language%20models%20%28LLMs%29%20as%20foundation%20models%20have%20driven%0Aexcitement%20in%20Agentic%20AI.%20We%20note%2C%20however%2C%20that%20Agentic%20AI%20systems%20are%0Aprimarily%20applications%2C%20not%20foundations%2C%20and%20so%20their%20success%20depends%20on%0Avalidation%20by%20end%20users%20and%20principal%20stakeholders.%20The%20tools%20and%20techniques%0Aneeded%20by%20the%20principal%20users%20to%20validate%20their%20applications%20are%20quite%0Adifferent%20from%20the%20tools%20and%20techniques%20used%20to%20evaluate%20foundation%20models.%0AIronically%2C%20with%20good%20validation%20measures%20in%20place%2C%20in%20many%20cases%20the%0Afoundation%20models%20can%20be%20replaced%20with%20much%20simpler%2C%20faster%2C%20and%20more%0Ainterpretable%20models%20that%20handle%20core%20logic.%20When%20it%20comes%20to%20Agentic%20AI%2C%0Avalidity%20is%20what%20you%20need.%20LLMs%20are%20one%20option%20that%20might%20achieve%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27628v1&entry.124074799=Read"},
{"title": "Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung\n  Tumor Segmentation", "author": "Elena Mulero Ayll\u00f3n and Linlin Shen and Pierangelo Veltri and Fabrizia Gelardi and Arturo Chiti and Paolo Soda and Matteo Tortora", "abstract": "  Accurate lung tumor segmentation is vital for improving diagnosis and\ntreatment planning, and effectively combining anatomical and functional\ninformation from PET and CT remains a major challenge. In this study, we\npropose vMambaX, a lightweight multimodal framework integrating PET and CT scan\nimages through a Context-Gated Cross-Modal Perception Module (CGM). Built on\nthe Visual Mamba architecture, vMambaX adaptively enhances inter-modality\nfeature interaction, emphasizing informative regions while suppressing noise.\nEvaluated on the PCLT20K dataset, the model outperforms baseline models while\nmaintaining lower computational complexity. These results highlight the\neffectiveness of adaptive cross-modal gating for multimodal tumor segmentation\nand demonstrate the potential of vMambaX as an efficient and scalable framework\nfor advanced lung cancer analysis. The code is available at\nhttps://github.com/arco-group/vMambaX.\n", "link": "http://arxiv.org/abs/2510.27508v1", "date": "2025-10-31", "relevancy": 2.1645, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Gated%20Cross-Modal%20Perception%20with%20Visual%20Mamba%20for%20PET-CT%20Lung%0A%20%20Tumor%20Segmentation&body=Title%3A%20Context-Gated%20Cross-Modal%20Perception%20with%20Visual%20Mamba%20for%20PET-CT%20Lung%0A%20%20Tumor%20Segmentation%0AAuthor%3A%20Elena%20Mulero%20Ayll%C3%B3n%20and%20Linlin%20Shen%20and%20Pierangelo%20Veltri%20and%20Fabrizia%20Gelardi%20and%20Arturo%20Chiti%20and%20Paolo%20Soda%20and%20Matteo%20Tortora%0AAbstract%3A%20%20%20Accurate%20lung%20tumor%20segmentation%20is%20vital%20for%20improving%20diagnosis%20and%0Atreatment%20planning%2C%20and%20effectively%20combining%20anatomical%20and%20functional%0Ainformation%20from%20PET%20and%20CT%20remains%20a%20major%20challenge.%20In%20this%20study%2C%20we%0Apropose%20vMambaX%2C%20a%20lightweight%20multimodal%20framework%20integrating%20PET%20and%20CT%20scan%0Aimages%20through%20a%20Context-Gated%20Cross-Modal%20Perception%20Module%20%28CGM%29.%20Built%20on%0Athe%20Visual%20Mamba%20architecture%2C%20vMambaX%20adaptively%20enhances%20inter-modality%0Afeature%20interaction%2C%20emphasizing%20informative%20regions%20while%20suppressing%20noise.%0AEvaluated%20on%20the%20PCLT20K%20dataset%2C%20the%20model%20outperforms%20baseline%20models%20while%0Amaintaining%20lower%20computational%20complexity.%20These%20results%20highlight%20the%0Aeffectiveness%20of%20adaptive%20cross-modal%20gating%20for%20multimodal%20tumor%20segmentation%0Aand%20demonstrate%20the%20potential%20of%20vMambaX%20as%20an%20efficient%20and%20scalable%20framework%0Afor%20advanced%20lung%20cancer%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/arco-group/vMambaX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Gated%2520Cross-Modal%2520Perception%2520with%2520Visual%2520Mamba%2520for%2520PET-CT%2520Lung%250A%2520%2520Tumor%2520Segmentation%26entry.906535625%3DElena%2520Mulero%2520Ayll%25C3%25B3n%2520and%2520Linlin%2520Shen%2520and%2520Pierangelo%2520Veltri%2520and%2520Fabrizia%2520Gelardi%2520and%2520Arturo%2520Chiti%2520and%2520Paolo%2520Soda%2520and%2520Matteo%2520Tortora%26entry.1292438233%3D%2520%2520Accurate%2520lung%2520tumor%2520segmentation%2520is%2520vital%2520for%2520improving%2520diagnosis%2520and%250Atreatment%2520planning%252C%2520and%2520effectively%2520combining%2520anatomical%2520and%2520functional%250Ainformation%2520from%2520PET%2520and%2520CT%2520remains%2520a%2520major%2520challenge.%2520In%2520this%2520study%252C%2520we%250Apropose%2520vMambaX%252C%2520a%2520lightweight%2520multimodal%2520framework%2520integrating%2520PET%2520and%2520CT%2520scan%250Aimages%2520through%2520a%2520Context-Gated%2520Cross-Modal%2520Perception%2520Module%2520%2528CGM%2529.%2520Built%2520on%250Athe%2520Visual%2520Mamba%2520architecture%252C%2520vMambaX%2520adaptively%2520enhances%2520inter-modality%250Afeature%2520interaction%252C%2520emphasizing%2520informative%2520regions%2520while%2520suppressing%2520noise.%250AEvaluated%2520on%2520the%2520PCLT20K%2520dataset%252C%2520the%2520model%2520outperforms%2520baseline%2520models%2520while%250Amaintaining%2520lower%2520computational%2520complexity.%2520These%2520results%2520highlight%2520the%250Aeffectiveness%2520of%2520adaptive%2520cross-modal%2520gating%2520for%2520multimodal%2520tumor%2520segmentation%250Aand%2520demonstrate%2520the%2520potential%2520of%2520vMambaX%2520as%2520an%2520efficient%2520and%2520scalable%2520framework%250Afor%2520advanced%2520lung%2520cancer%2520analysis.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/arco-group/vMambaX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Gated%20Cross-Modal%20Perception%20with%20Visual%20Mamba%20for%20PET-CT%20Lung%0A%20%20Tumor%20Segmentation&entry.906535625=Elena%20Mulero%20Ayll%C3%B3n%20and%20Linlin%20Shen%20and%20Pierangelo%20Veltri%20and%20Fabrizia%20Gelardi%20and%20Arturo%20Chiti%20and%20Paolo%20Soda%20and%20Matteo%20Tortora&entry.1292438233=%20%20Accurate%20lung%20tumor%20segmentation%20is%20vital%20for%20improving%20diagnosis%20and%0Atreatment%20planning%2C%20and%20effectively%20combining%20anatomical%20and%20functional%0Ainformation%20from%20PET%20and%20CT%20remains%20a%20major%20challenge.%20In%20this%20study%2C%20we%0Apropose%20vMambaX%2C%20a%20lightweight%20multimodal%20framework%20integrating%20PET%20and%20CT%20scan%0Aimages%20through%20a%20Context-Gated%20Cross-Modal%20Perception%20Module%20%28CGM%29.%20Built%20on%0Athe%20Visual%20Mamba%20architecture%2C%20vMambaX%20adaptively%20enhances%20inter-modality%0Afeature%20interaction%2C%20emphasizing%20informative%20regions%20while%20suppressing%20noise.%0AEvaluated%20on%20the%20PCLT20K%20dataset%2C%20the%20model%20outperforms%20baseline%20models%20while%0Amaintaining%20lower%20computational%20complexity.%20These%20results%20highlight%20the%0Aeffectiveness%20of%20adaptive%20cross-modal%20gating%20for%20multimodal%20tumor%20segmentation%0Aand%20demonstrate%20the%20potential%20of%20vMambaX%20as%20an%20efficient%20and%20scalable%20framework%0Afor%20advanced%20lung%20cancer%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/arco-group/vMambaX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27508v1&entry.124074799=Read"},
{"title": "SpecAttn: Speculating Sparse Attention", "author": "Harsh Shah", "abstract": "  Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.\n", "link": "http://arxiv.org/abs/2510.27641v1", "date": "2025-10-31", "relevancy": 2.1091, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5753}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5034}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecAttn%3A%20Speculating%20Sparse%20Attention&body=Title%3A%20SpecAttn%3A%20Speculating%20Sparse%20Attention%0AAuthor%3A%20Harsh%20Shah%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20computational%20bottlenecks%0Aduring%20inference%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20mechanisms%2C%0Aparticularly%20as%20context%20lengths%20increase.%20We%20introduce%20SpecAttn%2C%20a%20novel%0Atraining-free%20approach%20that%20seamlessly%20integrates%20with%20existing%20speculative%0Adecoding%20techniques%20to%20enable%20efficient%20sparse%20attention%20in%20pre-trained%0Atransformers.%20Our%20key%20insight%20is%20to%20exploit%20the%20attention%20weights%20already%0Acomputed%20by%20the%20draft%20model%20during%20speculative%20decoding%20to%20identify%20important%0Atokens%20for%20the%20target%20model%2C%20eliminating%20redundant%20computation%20while%0Amaintaining%20output%20quality.%20SpecAttn%20employs%20three%20core%20techniques%3A%20KL%0Adivergence-based%20layer%20alignment%20between%20draft%20and%20target%20models%2C%20a%0AGPU-optimized%20sorting-free%20algorithm%20for%20top-p%20token%20selection%20from%20draft%0Aattention%20patterns%2C%20and%20dynamic%20key-value%20cache%20pruning%20guided%20by%20these%0Apredictions.%20By%20leveraging%20the%20computational%20work%20already%20performed%20in%20standard%0Aspeculative%20decoding%20pipelines%2C%20SpecAttn%20achieves%20over%2075%25%20reduction%20in%0Akey-value%20cache%20accesses%20with%20a%20mere%2015.29%25%20increase%20in%20perplexity%20on%20the%20PG-19%0Adataset%2C%20significantly%20outperforming%20existing%20sparse%20attention%20methods.%20Our%0Aapproach%20demonstrates%20that%20speculative%20execution%20can%20be%20enhanced%20to%20provide%0Aapproximate%20verification%20without%20significant%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecAttn%253A%2520Speculating%2520Sparse%2520Attention%26entry.906535625%3DHarsh%2520Shah%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520significant%2520computational%2520bottlenecks%250Aduring%2520inference%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520self-attention%2520mechanisms%252C%250Aparticularly%2520as%2520context%2520lengths%2520increase.%2520We%2520introduce%2520SpecAttn%252C%2520a%2520novel%250Atraining-free%2520approach%2520that%2520seamlessly%2520integrates%2520with%2520existing%2520speculative%250Adecoding%2520techniques%2520to%2520enable%2520efficient%2520sparse%2520attention%2520in%2520pre-trained%250Atransformers.%2520Our%2520key%2520insight%2520is%2520to%2520exploit%2520the%2520attention%2520weights%2520already%250Acomputed%2520by%2520the%2520draft%2520model%2520during%2520speculative%2520decoding%2520to%2520identify%2520important%250Atokens%2520for%2520the%2520target%2520model%252C%2520eliminating%2520redundant%2520computation%2520while%250Amaintaining%2520output%2520quality.%2520SpecAttn%2520employs%2520three%2520core%2520techniques%253A%2520KL%250Adivergence-based%2520layer%2520alignment%2520between%2520draft%2520and%2520target%2520models%252C%2520a%250AGPU-optimized%2520sorting-free%2520algorithm%2520for%2520top-p%2520token%2520selection%2520from%2520draft%250Aattention%2520patterns%252C%2520and%2520dynamic%2520key-value%2520cache%2520pruning%2520guided%2520by%2520these%250Apredictions.%2520By%2520leveraging%2520the%2520computational%2520work%2520already%2520performed%2520in%2520standard%250Aspeculative%2520decoding%2520pipelines%252C%2520SpecAttn%2520achieves%2520over%252075%2525%2520reduction%2520in%250Akey-value%2520cache%2520accesses%2520with%2520a%2520mere%252015.29%2525%2520increase%2520in%2520perplexity%2520on%2520the%2520PG-19%250Adataset%252C%2520significantly%2520outperforming%2520existing%2520sparse%2520attention%2520methods.%2520Our%250Aapproach%2520demonstrates%2520that%2520speculative%2520execution%2520can%2520be%2520enhanced%2520to%2520provide%250Aapproximate%2520verification%2520without%2520significant%2520performance%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecAttn%3A%20Speculating%20Sparse%20Attention&entry.906535625=Harsh%20Shah&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20computational%20bottlenecks%0Aduring%20inference%20due%20to%20the%20quadratic%20complexity%20of%20self-attention%20mechanisms%2C%0Aparticularly%20as%20context%20lengths%20increase.%20We%20introduce%20SpecAttn%2C%20a%20novel%0Atraining-free%20approach%20that%20seamlessly%20integrates%20with%20existing%20speculative%0Adecoding%20techniques%20to%20enable%20efficient%20sparse%20attention%20in%20pre-trained%0Atransformers.%20Our%20key%20insight%20is%20to%20exploit%20the%20attention%20weights%20already%0Acomputed%20by%20the%20draft%20model%20during%20speculative%20decoding%20to%20identify%20important%0Atokens%20for%20the%20target%20model%2C%20eliminating%20redundant%20computation%20while%0Amaintaining%20output%20quality.%20SpecAttn%20employs%20three%20core%20techniques%3A%20KL%0Adivergence-based%20layer%20alignment%20between%20draft%20and%20target%20models%2C%20a%0AGPU-optimized%20sorting-free%20algorithm%20for%20top-p%20token%20selection%20from%20draft%0Aattention%20patterns%2C%20and%20dynamic%20key-value%20cache%20pruning%20guided%20by%20these%0Apredictions.%20By%20leveraging%20the%20computational%20work%20already%20performed%20in%20standard%0Aspeculative%20decoding%20pipelines%2C%20SpecAttn%20achieves%20over%2075%25%20reduction%20in%0Akey-value%20cache%20accesses%20with%20a%20mere%2015.29%25%20increase%20in%20perplexity%20on%20the%20PG-19%0Adataset%2C%20significantly%20outperforming%20existing%20sparse%20attention%20methods.%20Our%0Aapproach%20demonstrates%20that%20speculative%20execution%20can%20be%20enhanced%20to%20provide%0Aapproximate%20verification%20without%20significant%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27641v1&entry.124074799=Read"},
{"title": "Deep Neural Watermarking for Robust Copyright Protection in 3D Point\n  Clouds", "author": "Khandoker Ashik Uz Zaman and Mohammad Zahangir Alam and Mohammed N. M. Ali and Mahdi H. Miraz", "abstract": "  The protection of intellectual property has become critical due to the rapid\ngrowth of three-dimensional content in digital media. Unlike traditional images\nor videos, 3D point clouds present unique challenges for copyright enforcement,\nas they are especially vulnerable to a range of geometric and non-geometric\nattacks that can easily degrade or remove conventional watermark signals. In\nthis paper, we address these challenges by proposing a robust deep neural\nwatermarking framework for 3D point cloud copyright protection and ownership\nverification. Our approach embeds binary watermarks into the singular values of\n3D point cloud blocks using spectral decomposition, i.e. Singular Value\nDecomposition (SVD), and leverages the extraction capabilities of Deep Learning\nusing PointNet++ neural network architecture. The network is trained to\nreliably extract watermarks even after the data undergoes various attacks such\nas rotation, scaling, noise, cropping and signal distortions. We validated our\nmethod using the publicly available ModelNet40 dataset, demonstrating that deep\nlearning-based extraction significantly outperforms traditional SVD-based\ntechniques under challenging conditions. Our experimental evaluation\ndemonstrates that the deep learning-based extraction approach significantly\noutperforms existing SVD-based methods with deep learning achieving bitwise\naccuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD\nachieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,\nwhich is the most severe geometric distortion in our experiment. This\ndemonstrates our method's ability to achieve superior watermark recovery and\nmaintain high fidelity even under severe distortions.\n", "link": "http://arxiv.org/abs/2510.27533v1", "date": "2025-10-31", "relevancy": 2.1003, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5524}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5211}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Neural%20Watermarking%20for%20Robust%20Copyright%20Protection%20in%203D%20Point%0A%20%20Clouds&body=Title%3A%20Deep%20Neural%20Watermarking%20for%20Robust%20Copyright%20Protection%20in%203D%20Point%0A%20%20Clouds%0AAuthor%3A%20Khandoker%20Ashik%20Uz%20Zaman%20and%20Mohammad%20Zahangir%20Alam%20and%20Mohammed%20N.%20M.%20Ali%20and%20Mahdi%20H.%20Miraz%0AAbstract%3A%20%20%20The%20protection%20of%20intellectual%20property%20has%20become%20critical%20due%20to%20the%20rapid%0Agrowth%20of%20three-dimensional%20content%20in%20digital%20media.%20Unlike%20traditional%20images%0Aor%20videos%2C%203D%20point%20clouds%20present%20unique%20challenges%20for%20copyright%20enforcement%2C%0Aas%20they%20are%20especially%20vulnerable%20to%20a%20range%20of%20geometric%20and%20non-geometric%0Aattacks%20that%20can%20easily%20degrade%20or%20remove%20conventional%20watermark%20signals.%20In%0Athis%20paper%2C%20we%20address%20these%20challenges%20by%20proposing%20a%20robust%20deep%20neural%0Awatermarking%20framework%20for%203D%20point%20cloud%20copyright%20protection%20and%20ownership%0Averification.%20Our%20approach%20embeds%20binary%20watermarks%20into%20the%20singular%20values%20of%0A3D%20point%20cloud%20blocks%20using%20spectral%20decomposition%2C%20i.e.%20Singular%20Value%0ADecomposition%20%28SVD%29%2C%20and%20leverages%20the%20extraction%20capabilities%20of%20Deep%20Learning%0Ausing%20PointNet%2B%2B%20neural%20network%20architecture.%20The%20network%20is%20trained%20to%0Areliably%20extract%20watermarks%20even%20after%20the%20data%20undergoes%20various%20attacks%20such%0Aas%20rotation%2C%20scaling%2C%20noise%2C%20cropping%20and%20signal%20distortions.%20We%20validated%20our%0Amethod%20using%20the%20publicly%20available%20ModelNet40%20dataset%2C%20demonstrating%20that%20deep%0Alearning-based%20extraction%20significantly%20outperforms%20traditional%20SVD-based%0Atechniques%20under%20challenging%20conditions.%20Our%20experimental%20evaluation%0Ademonstrates%20that%20the%20deep%20learning-based%20extraction%20approach%20significantly%0Aoutperforms%20existing%20SVD-based%20methods%20with%20deep%20learning%20achieving%20bitwise%0Aaccuracy%20up%20to%200.83%20and%20Intersection%20over%20Union%20%28IoU%29%20of%200.80%2C%20compared%20to%20SVD%0Aachieving%20a%20bitwise%20accuracy%20of%200.58%20and%20IoU%20of%200.26%20for%20the%20Crop%20%2870%25%29%20attack%2C%0Awhich%20is%20the%20most%20severe%20geometric%20distortion%20in%20our%20experiment.%20This%0Ademonstrates%20our%20method%27s%20ability%20to%20achieve%20superior%20watermark%20recovery%20and%0Amaintain%20high%20fidelity%20even%20under%20severe%20distortions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Neural%2520Watermarking%2520for%2520Robust%2520Copyright%2520Protection%2520in%25203D%2520Point%250A%2520%2520Clouds%26entry.906535625%3DKhandoker%2520Ashik%2520Uz%2520Zaman%2520and%2520Mohammad%2520Zahangir%2520Alam%2520and%2520Mohammed%2520N.%2520M.%2520Ali%2520and%2520Mahdi%2520H.%2520Miraz%26entry.1292438233%3D%2520%2520The%2520protection%2520of%2520intellectual%2520property%2520has%2520become%2520critical%2520due%2520to%2520the%2520rapid%250Agrowth%2520of%2520three-dimensional%2520content%2520in%2520digital%2520media.%2520Unlike%2520traditional%2520images%250Aor%2520videos%252C%25203D%2520point%2520clouds%2520present%2520unique%2520challenges%2520for%2520copyright%2520enforcement%252C%250Aas%2520they%2520are%2520especially%2520vulnerable%2520to%2520a%2520range%2520of%2520geometric%2520and%2520non-geometric%250Aattacks%2520that%2520can%2520easily%2520degrade%2520or%2520remove%2520conventional%2520watermark%2520signals.%2520In%250Athis%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%2520proposing%2520a%2520robust%2520deep%2520neural%250Awatermarking%2520framework%2520for%25203D%2520point%2520cloud%2520copyright%2520protection%2520and%2520ownership%250Averification.%2520Our%2520approach%2520embeds%2520binary%2520watermarks%2520into%2520the%2520singular%2520values%2520of%250A3D%2520point%2520cloud%2520blocks%2520using%2520spectral%2520decomposition%252C%2520i.e.%2520Singular%2520Value%250ADecomposition%2520%2528SVD%2529%252C%2520and%2520leverages%2520the%2520extraction%2520capabilities%2520of%2520Deep%2520Learning%250Ausing%2520PointNet%252B%252B%2520neural%2520network%2520architecture.%2520The%2520network%2520is%2520trained%2520to%250Areliably%2520extract%2520watermarks%2520even%2520after%2520the%2520data%2520undergoes%2520various%2520attacks%2520such%250Aas%2520rotation%252C%2520scaling%252C%2520noise%252C%2520cropping%2520and%2520signal%2520distortions.%2520We%2520validated%2520our%250Amethod%2520using%2520the%2520publicly%2520available%2520ModelNet40%2520dataset%252C%2520demonstrating%2520that%2520deep%250Alearning-based%2520extraction%2520significantly%2520outperforms%2520traditional%2520SVD-based%250Atechniques%2520under%2520challenging%2520conditions.%2520Our%2520experimental%2520evaluation%250Ademonstrates%2520that%2520the%2520deep%2520learning-based%2520extraction%2520approach%2520significantly%250Aoutperforms%2520existing%2520SVD-based%2520methods%2520with%2520deep%2520learning%2520achieving%2520bitwise%250Aaccuracy%2520up%2520to%25200.83%2520and%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520of%25200.80%252C%2520compared%2520to%2520SVD%250Aachieving%2520a%2520bitwise%2520accuracy%2520of%25200.58%2520and%2520IoU%2520of%25200.26%2520for%2520the%2520Crop%2520%252870%2525%2529%2520attack%252C%250Awhich%2520is%2520the%2520most%2520severe%2520geometric%2520distortion%2520in%2520our%2520experiment.%2520This%250Ademonstrates%2520our%2520method%2527s%2520ability%2520to%2520achieve%2520superior%2520watermark%2520recovery%2520and%250Amaintain%2520high%2520fidelity%2520even%2520under%2520severe%2520distortions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Neural%20Watermarking%20for%20Robust%20Copyright%20Protection%20in%203D%20Point%0A%20%20Clouds&entry.906535625=Khandoker%20Ashik%20Uz%20Zaman%20and%20Mohammad%20Zahangir%20Alam%20and%20Mohammed%20N.%20M.%20Ali%20and%20Mahdi%20H.%20Miraz&entry.1292438233=%20%20The%20protection%20of%20intellectual%20property%20has%20become%20critical%20due%20to%20the%20rapid%0Agrowth%20of%20three-dimensional%20content%20in%20digital%20media.%20Unlike%20traditional%20images%0Aor%20videos%2C%203D%20point%20clouds%20present%20unique%20challenges%20for%20copyright%20enforcement%2C%0Aas%20they%20are%20especially%20vulnerable%20to%20a%20range%20of%20geometric%20and%20non-geometric%0Aattacks%20that%20can%20easily%20degrade%20or%20remove%20conventional%20watermark%20signals.%20In%0Athis%20paper%2C%20we%20address%20these%20challenges%20by%20proposing%20a%20robust%20deep%20neural%0Awatermarking%20framework%20for%203D%20point%20cloud%20copyright%20protection%20and%20ownership%0Averification.%20Our%20approach%20embeds%20binary%20watermarks%20into%20the%20singular%20values%20of%0A3D%20point%20cloud%20blocks%20using%20spectral%20decomposition%2C%20i.e.%20Singular%20Value%0ADecomposition%20%28SVD%29%2C%20and%20leverages%20the%20extraction%20capabilities%20of%20Deep%20Learning%0Ausing%20PointNet%2B%2B%20neural%20network%20architecture.%20The%20network%20is%20trained%20to%0Areliably%20extract%20watermarks%20even%20after%20the%20data%20undergoes%20various%20attacks%20such%0Aas%20rotation%2C%20scaling%2C%20noise%2C%20cropping%20and%20signal%20distortions.%20We%20validated%20our%0Amethod%20using%20the%20publicly%20available%20ModelNet40%20dataset%2C%20demonstrating%20that%20deep%0Alearning-based%20extraction%20significantly%20outperforms%20traditional%20SVD-based%0Atechniques%20under%20challenging%20conditions.%20Our%20experimental%20evaluation%0Ademonstrates%20that%20the%20deep%20learning-based%20extraction%20approach%20significantly%0Aoutperforms%20existing%20SVD-based%20methods%20with%20deep%20learning%20achieving%20bitwise%0Aaccuracy%20up%20to%200.83%20and%20Intersection%20over%20Union%20%28IoU%29%20of%200.80%2C%20compared%20to%20SVD%0Aachieving%20a%20bitwise%20accuracy%20of%200.58%20and%20IoU%20of%200.26%20for%20the%20Crop%20%2870%25%29%20attack%2C%0Awhich%20is%20the%20most%20severe%20geometric%20distortion%20in%20our%20experiment.%20This%0Ademonstrates%20our%20method%27s%20ability%20to%20achieve%20superior%20watermark%20recovery%20and%0Amaintain%20high%20fidelity%20even%20under%20severe%20distortions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27533v1&entry.124074799=Read"},
{"title": "Gaussian Combined Distance: A Generic Metric for Object Detection", "author": "Ziqian Guan and Xieyi Fu and Pengjun Huang and Hengyuan Zhang and Hubin Du and Yongtao Liu and Yinglin Wang and Qang Ma", "abstract": "  In object detection, a well-defined similarity metric can significantly\nenhance model performance. Currently, the IoU-based similarity metric is the\nmost commonly preferred choice for detectors. However, detectors using IoU as a\nsimilarity metric often perform poorly when detecting small objects because of\ntheir sensitivity to minor positional deviations. To address this issue, recent\nstudies have proposed the Wasserstein Distance as an alternative to IoU for\nmeasuring the similarity of Gaussian-distributed bounding boxes. However, we\nhave observed that the Wasserstein Distance lacks scale invariance, which\nnegatively impacts the model's generalization capability. Additionally, when\nused as a loss function, its independent optimization of the center attributes\nleads to slow model convergence and unsatisfactory detection precision. To\naddress these challenges, we introduce the Gaussian Combined Distance (GCD).\nThrough analytical examination of GCD and its gradient, we demonstrate that GCD\nnot only possesses scale invariance but also facilitates joint optimization,\nwhich enhances model localization performance. Extensive experiments on the\nAI-TOD-v2 dataset for tiny object detection show that GCD, as a bounding box\nregression loss function and label assignment metric, achieves state-of-the-art\nperformance across various detectors. We further validated the generalizability\nof GCD on the MS-COCO-2017 and Visdrone-2019 datasets, where it outperforms the\nWasserstein Distance across diverse scales of datasets. Code is available at\nhttps://github.com/MArKkwanGuan/mmdet-GCD.\n", "link": "http://arxiv.org/abs/2510.27649v1", "date": "2025-10-31", "relevancy": 2.0927, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5302}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5289}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Combined%20Distance%3A%20A%20Generic%20Metric%20for%20Object%20Detection&body=Title%3A%20Gaussian%20Combined%20Distance%3A%20A%20Generic%20Metric%20for%20Object%20Detection%0AAuthor%3A%20Ziqian%20Guan%20and%20Xieyi%20Fu%20and%20Pengjun%20Huang%20and%20Hengyuan%20Zhang%20and%20Hubin%20Du%20and%20Yongtao%20Liu%20and%20Yinglin%20Wang%20and%20Qang%20Ma%0AAbstract%3A%20%20%20In%20object%20detection%2C%20a%20well-defined%20similarity%20metric%20can%20significantly%0Aenhance%20model%20performance.%20Currently%2C%20the%20IoU-based%20similarity%20metric%20is%20the%0Amost%20commonly%20preferred%20choice%20for%20detectors.%20However%2C%20detectors%20using%20IoU%20as%20a%0Asimilarity%20metric%20often%20perform%20poorly%20when%20detecting%20small%20objects%20because%20of%0Atheir%20sensitivity%20to%20minor%20positional%20deviations.%20To%20address%20this%20issue%2C%20recent%0Astudies%20have%20proposed%20the%20Wasserstein%20Distance%20as%20an%20alternative%20to%20IoU%20for%0Ameasuring%20the%20similarity%20of%20Gaussian-distributed%20bounding%20boxes.%20However%2C%20we%0Ahave%20observed%20that%20the%20Wasserstein%20Distance%20lacks%20scale%20invariance%2C%20which%0Anegatively%20impacts%20the%20model%27s%20generalization%20capability.%20Additionally%2C%20when%0Aused%20as%20a%20loss%20function%2C%20its%20independent%20optimization%20of%20the%20center%20attributes%0Aleads%20to%20slow%20model%20convergence%20and%20unsatisfactory%20detection%20precision.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20Gaussian%20Combined%20Distance%20%28GCD%29.%0AThrough%20analytical%20examination%20of%20GCD%20and%20its%20gradient%2C%20we%20demonstrate%20that%20GCD%0Anot%20only%20possesses%20scale%20invariance%20but%20also%20facilitates%20joint%20optimization%2C%0Awhich%20enhances%20model%20localization%20performance.%20Extensive%20experiments%20on%20the%0AAI-TOD-v2%20dataset%20for%20tiny%20object%20detection%20show%20that%20GCD%2C%20as%20a%20bounding%20box%0Aregression%20loss%20function%20and%20label%20assignment%20metric%2C%20achieves%20state-of-the-art%0Aperformance%20across%20various%20detectors.%20We%20further%20validated%20the%20generalizability%0Aof%20GCD%20on%20the%20MS-COCO-2017%20and%20Visdrone-2019%20datasets%2C%20where%20it%20outperforms%20the%0AWasserstein%20Distance%20across%20diverse%20scales%20of%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MArKkwanGuan/mmdet-GCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Combined%2520Distance%253A%2520A%2520Generic%2520Metric%2520for%2520Object%2520Detection%26entry.906535625%3DZiqian%2520Guan%2520and%2520Xieyi%2520Fu%2520and%2520Pengjun%2520Huang%2520and%2520Hengyuan%2520Zhang%2520and%2520Hubin%2520Du%2520and%2520Yongtao%2520Liu%2520and%2520Yinglin%2520Wang%2520and%2520Qang%2520Ma%26entry.1292438233%3D%2520%2520In%2520object%2520detection%252C%2520a%2520well-defined%2520similarity%2520metric%2520can%2520significantly%250Aenhance%2520model%2520performance.%2520Currently%252C%2520the%2520IoU-based%2520similarity%2520metric%2520is%2520the%250Amost%2520commonly%2520preferred%2520choice%2520for%2520detectors.%2520However%252C%2520detectors%2520using%2520IoU%2520as%2520a%250Asimilarity%2520metric%2520often%2520perform%2520poorly%2520when%2520detecting%2520small%2520objects%2520because%2520of%250Atheir%2520sensitivity%2520to%2520minor%2520positional%2520deviations.%2520To%2520address%2520this%2520issue%252C%2520recent%250Astudies%2520have%2520proposed%2520the%2520Wasserstein%2520Distance%2520as%2520an%2520alternative%2520to%2520IoU%2520for%250Ameasuring%2520the%2520similarity%2520of%2520Gaussian-distributed%2520bounding%2520boxes.%2520However%252C%2520we%250Ahave%2520observed%2520that%2520the%2520Wasserstein%2520Distance%2520lacks%2520scale%2520invariance%252C%2520which%250Anegatively%2520impacts%2520the%2520model%2527s%2520generalization%2520capability.%2520Additionally%252C%2520when%250Aused%2520as%2520a%2520loss%2520function%252C%2520its%2520independent%2520optimization%2520of%2520the%2520center%2520attributes%250Aleads%2520to%2520slow%2520model%2520convergence%2520and%2520unsatisfactory%2520detection%2520precision.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Gaussian%2520Combined%2520Distance%2520%2528GCD%2529.%250AThrough%2520analytical%2520examination%2520of%2520GCD%2520and%2520its%2520gradient%252C%2520we%2520demonstrate%2520that%2520GCD%250Anot%2520only%2520possesses%2520scale%2520invariance%2520but%2520also%2520facilitates%2520joint%2520optimization%252C%250Awhich%2520enhances%2520model%2520localization%2520performance.%2520Extensive%2520experiments%2520on%2520the%250AAI-TOD-v2%2520dataset%2520for%2520tiny%2520object%2520detection%2520show%2520that%2520GCD%252C%2520as%2520a%2520bounding%2520box%250Aregression%2520loss%2520function%2520and%2520label%2520assignment%2520metric%252C%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520various%2520detectors.%2520We%2520further%2520validated%2520the%2520generalizability%250Aof%2520GCD%2520on%2520the%2520MS-COCO-2017%2520and%2520Visdrone-2019%2520datasets%252C%2520where%2520it%2520outperforms%2520the%250AWasserstein%2520Distance%2520across%2520diverse%2520scales%2520of%2520datasets.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/MArKkwanGuan/mmdet-GCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Combined%20Distance%3A%20A%20Generic%20Metric%20for%20Object%20Detection&entry.906535625=Ziqian%20Guan%20and%20Xieyi%20Fu%20and%20Pengjun%20Huang%20and%20Hengyuan%20Zhang%20and%20Hubin%20Du%20and%20Yongtao%20Liu%20and%20Yinglin%20Wang%20and%20Qang%20Ma&entry.1292438233=%20%20In%20object%20detection%2C%20a%20well-defined%20similarity%20metric%20can%20significantly%0Aenhance%20model%20performance.%20Currently%2C%20the%20IoU-based%20similarity%20metric%20is%20the%0Amost%20commonly%20preferred%20choice%20for%20detectors.%20However%2C%20detectors%20using%20IoU%20as%20a%0Asimilarity%20metric%20often%20perform%20poorly%20when%20detecting%20small%20objects%20because%20of%0Atheir%20sensitivity%20to%20minor%20positional%20deviations.%20To%20address%20this%20issue%2C%20recent%0Astudies%20have%20proposed%20the%20Wasserstein%20Distance%20as%20an%20alternative%20to%20IoU%20for%0Ameasuring%20the%20similarity%20of%20Gaussian-distributed%20bounding%20boxes.%20However%2C%20we%0Ahave%20observed%20that%20the%20Wasserstein%20Distance%20lacks%20scale%20invariance%2C%20which%0Anegatively%20impacts%20the%20model%27s%20generalization%20capability.%20Additionally%2C%20when%0Aused%20as%20a%20loss%20function%2C%20its%20independent%20optimization%20of%20the%20center%20attributes%0Aleads%20to%20slow%20model%20convergence%20and%20unsatisfactory%20detection%20precision.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20Gaussian%20Combined%20Distance%20%28GCD%29.%0AThrough%20analytical%20examination%20of%20GCD%20and%20its%20gradient%2C%20we%20demonstrate%20that%20GCD%0Anot%20only%20possesses%20scale%20invariance%20but%20also%20facilitates%20joint%20optimization%2C%0Awhich%20enhances%20model%20localization%20performance.%20Extensive%20experiments%20on%20the%0AAI-TOD-v2%20dataset%20for%20tiny%20object%20detection%20show%20that%20GCD%2C%20as%20a%20bounding%20box%0Aregression%20loss%20function%20and%20label%20assignment%20metric%2C%20achieves%20state-of-the-art%0Aperformance%20across%20various%20detectors.%20We%20further%20validated%20the%20generalizability%0Aof%20GCD%20on%20the%20MS-COCO-2017%20and%20Visdrone-2019%20datasets%2C%20where%20it%20outperforms%20the%0AWasserstein%20Distance%20across%20diverse%20scales%20of%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/MArKkwanGuan/mmdet-GCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27649v1&entry.124074799=Read"},
{"title": "Deep learning denoising unlocks quantitative insights in operando\n  materials microscopy", "author": "Samuel Degnan-Morgenstern and Alexander E. Cohen and Rajeev Gopal and Megan Gober and George J. Nelson and Peng Bai and Martin Z. Bazant", "abstract": "  Operando microscopy provides direct insight into the dynamic chemical and\nphysical processes that govern functional materials, yet measurement noise\nlimits the effective resolution and undermines quantitative analysis. Here, we\npresent a general framework for integrating unsupervised deep learning-based\ndenoising into quantitative microscopy workflows across modalities and length\nscales. Using simulated data, we demonstrate that deep denoising preserves\nphysical fidelity, introduces minimal bias, and reduces uncertainty in model\nlearning with partial differential equation (PDE)-constrained optimization.\nApplied to experiments, denoising reveals nanoscale chemical and structural\nheterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron\nphosphate (LFP), enables automated particle segmentation and phase\nclassification in optical microscopy of graphite electrodes, and reduces\nnoise-induced variability by nearly 80% in neutron radiography to resolve\nheterogeneous lithium transport. Collectively, these results establish deep\ndenoising as a powerful, modality-agnostic enhancement that advances\nquantitative operando imaging and extends the reach of previously noise-limited\ntechniques.\n", "link": "http://arxiv.org/abs/2510.27667v1", "date": "2025-10-31", "relevancy": 2.0897, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5326}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20denoising%20unlocks%20quantitative%20insights%20in%20operando%0A%20%20materials%20microscopy&body=Title%3A%20Deep%20learning%20denoising%20unlocks%20quantitative%20insights%20in%20operando%0A%20%20materials%20microscopy%0AAuthor%3A%20Samuel%20Degnan-Morgenstern%20and%20Alexander%20E.%20Cohen%20and%20Rajeev%20Gopal%20and%20Megan%20Gober%20and%20George%20J.%20Nelson%20and%20Peng%20Bai%20and%20Martin%20Z.%20Bazant%0AAbstract%3A%20%20%20Operando%20microscopy%20provides%20direct%20insight%20into%20the%20dynamic%20chemical%20and%0Aphysical%20processes%20that%20govern%20functional%20materials%2C%20yet%20measurement%20noise%0Alimits%20the%20effective%20resolution%20and%20undermines%20quantitative%20analysis.%20Here%2C%20we%0Apresent%20a%20general%20framework%20for%20integrating%20unsupervised%20deep%20learning-based%0Adenoising%20into%20quantitative%20microscopy%20workflows%20across%20modalities%20and%20length%0Ascales.%20Using%20simulated%20data%2C%20we%20demonstrate%20that%20deep%20denoising%20preserves%0Aphysical%20fidelity%2C%20introduces%20minimal%20bias%2C%20and%20reduces%20uncertainty%20in%20model%0Alearning%20with%20partial%20differential%20equation%20%28PDE%29-constrained%20optimization.%0AApplied%20to%20experiments%2C%20denoising%20reveals%20nanoscale%20chemical%20and%20structural%0Aheterogeneity%20in%20scanning%20transmission%20X-ray%20microscopy%20%28STXM%29%20of%20lithium%20iron%0Aphosphate%20%28LFP%29%2C%20enables%20automated%20particle%20segmentation%20and%20phase%0Aclassification%20in%20optical%20microscopy%20of%20graphite%20electrodes%2C%20and%20reduces%0Anoise-induced%20variability%20by%20nearly%2080%25%20in%20neutron%20radiography%20to%20resolve%0Aheterogeneous%20lithium%20transport.%20Collectively%2C%20these%20results%20establish%20deep%0Adenoising%20as%20a%20powerful%2C%20modality-agnostic%20enhancement%20that%20advances%0Aquantitative%20operando%20imaging%20and%20extends%20the%20reach%20of%20previously%20noise-limited%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520denoising%2520unlocks%2520quantitative%2520insights%2520in%2520operando%250A%2520%2520materials%2520microscopy%26entry.906535625%3DSamuel%2520Degnan-Morgenstern%2520and%2520Alexander%2520E.%2520Cohen%2520and%2520Rajeev%2520Gopal%2520and%2520Megan%2520Gober%2520and%2520George%2520J.%2520Nelson%2520and%2520Peng%2520Bai%2520and%2520Martin%2520Z.%2520Bazant%26entry.1292438233%3D%2520%2520Operando%2520microscopy%2520provides%2520direct%2520insight%2520into%2520the%2520dynamic%2520chemical%2520and%250Aphysical%2520processes%2520that%2520govern%2520functional%2520materials%252C%2520yet%2520measurement%2520noise%250Alimits%2520the%2520effective%2520resolution%2520and%2520undermines%2520quantitative%2520analysis.%2520Here%252C%2520we%250Apresent%2520a%2520general%2520framework%2520for%2520integrating%2520unsupervised%2520deep%2520learning-based%250Adenoising%2520into%2520quantitative%2520microscopy%2520workflows%2520across%2520modalities%2520and%2520length%250Ascales.%2520Using%2520simulated%2520data%252C%2520we%2520demonstrate%2520that%2520deep%2520denoising%2520preserves%250Aphysical%2520fidelity%252C%2520introduces%2520minimal%2520bias%252C%2520and%2520reduces%2520uncertainty%2520in%2520model%250Alearning%2520with%2520partial%2520differential%2520equation%2520%2528PDE%2529-constrained%2520optimization.%250AApplied%2520to%2520experiments%252C%2520denoising%2520reveals%2520nanoscale%2520chemical%2520and%2520structural%250Aheterogeneity%2520in%2520scanning%2520transmission%2520X-ray%2520microscopy%2520%2528STXM%2529%2520of%2520lithium%2520iron%250Aphosphate%2520%2528LFP%2529%252C%2520enables%2520automated%2520particle%2520segmentation%2520and%2520phase%250Aclassification%2520in%2520optical%2520microscopy%2520of%2520graphite%2520electrodes%252C%2520and%2520reduces%250Anoise-induced%2520variability%2520by%2520nearly%252080%2525%2520in%2520neutron%2520radiography%2520to%2520resolve%250Aheterogeneous%2520lithium%2520transport.%2520Collectively%252C%2520these%2520results%2520establish%2520deep%250Adenoising%2520as%2520a%2520powerful%252C%2520modality-agnostic%2520enhancement%2520that%2520advances%250Aquantitative%2520operando%2520imaging%2520and%2520extends%2520the%2520reach%2520of%2520previously%2520noise-limited%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20denoising%20unlocks%20quantitative%20insights%20in%20operando%0A%20%20materials%20microscopy&entry.906535625=Samuel%20Degnan-Morgenstern%20and%20Alexander%20E.%20Cohen%20and%20Rajeev%20Gopal%20and%20Megan%20Gober%20and%20George%20J.%20Nelson%20and%20Peng%20Bai%20and%20Martin%20Z.%20Bazant&entry.1292438233=%20%20Operando%20microscopy%20provides%20direct%20insight%20into%20the%20dynamic%20chemical%20and%0Aphysical%20processes%20that%20govern%20functional%20materials%2C%20yet%20measurement%20noise%0Alimits%20the%20effective%20resolution%20and%20undermines%20quantitative%20analysis.%20Here%2C%20we%0Apresent%20a%20general%20framework%20for%20integrating%20unsupervised%20deep%20learning-based%0Adenoising%20into%20quantitative%20microscopy%20workflows%20across%20modalities%20and%20length%0Ascales.%20Using%20simulated%20data%2C%20we%20demonstrate%20that%20deep%20denoising%20preserves%0Aphysical%20fidelity%2C%20introduces%20minimal%20bias%2C%20and%20reduces%20uncertainty%20in%20model%0Alearning%20with%20partial%20differential%20equation%20%28PDE%29-constrained%20optimization.%0AApplied%20to%20experiments%2C%20denoising%20reveals%20nanoscale%20chemical%20and%20structural%0Aheterogeneity%20in%20scanning%20transmission%20X-ray%20microscopy%20%28STXM%29%20of%20lithium%20iron%0Aphosphate%20%28LFP%29%2C%20enables%20automated%20particle%20segmentation%20and%20phase%0Aclassification%20in%20optical%20microscopy%20of%20graphite%20electrodes%2C%20and%20reduces%0Anoise-induced%20variability%20by%20nearly%2080%25%20in%20neutron%20radiography%20to%20resolve%0Aheterogeneous%20lithium%20transport.%20Collectively%2C%20these%20results%20establish%20deep%0Adenoising%20as%20a%20powerful%2C%20modality-agnostic%20enhancement%20that%20advances%0Aquantitative%20operando%20imaging%20and%20extends%20the%20reach%20of%20previously%20noise-limited%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27667v1&entry.124074799=Read"},
{"title": "VessShape: Few-shot 2D blood vessel segmentation by leveraging shape\n  priors from synthetic images", "author": "Cesar H. Comin and Wesley N. Galv\u00e3o", "abstract": "  Semantic segmentation of blood vessels is an important task in medical image\nanalysis, but its progress is often hindered by the scarcity of large annotated\ndatasets and the poor generalization of models across different imaging\nmodalities. A key aspect is the tendency of Convolutional Neural Networks\n(CNNs) to learn texture-based features, which limits their performance when\napplied to new domains with different visual characteristics. We hypothesize\nthat leveraging geometric priors of vessel shapes, such as their tubular and\nbranching nature, can lead to more robust and data-efficient models. To\ninvestigate this, we introduce VessShape, a methodology for generating\nlarge-scale 2D synthetic datasets designed to instill a shape bias in\nsegmentation models. VessShape images contain procedurally generated tubular\ngeometries combined with a wide variety of foreground and background textures,\nencouraging models to learn shape cues rather than textures. We demonstrate\nthat a model pre-trained on VessShape images achieves strong few-shot\nsegmentation performance on two real-world datasets from different domains,\nrequiring only four to ten samples for fine-tuning. Furthermore, the model\nexhibits notable zero-shot capabilities, effectively segmenting vessels in\nunseen domains without any target-specific training. Our results indicate that\npre-training with a strong shape bias can be an effective strategy to overcome\ndata scarcity and improve model generalization in blood vessel segmentation.\n", "link": "http://arxiv.org/abs/2510.27646v1", "date": "2025-10-31", "relevancy": 2.0762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5165}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VessShape%3A%20Few-shot%202D%20blood%20vessel%20segmentation%20by%20leveraging%20shape%0A%20%20priors%20from%20synthetic%20images&body=Title%3A%20VessShape%3A%20Few-shot%202D%20blood%20vessel%20segmentation%20by%20leveraging%20shape%0A%20%20priors%20from%20synthetic%20images%0AAuthor%3A%20Cesar%20H.%20Comin%20and%20Wesley%20N.%20Galv%C3%A3o%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20blood%20vessels%20is%20an%20important%20task%20in%20medical%20image%0Aanalysis%2C%20but%20its%20progress%20is%20often%20hindered%20by%20the%20scarcity%20of%20large%20annotated%0Adatasets%20and%20the%20poor%20generalization%20of%20models%20across%20different%20imaging%0Amodalities.%20A%20key%20aspect%20is%20the%20tendency%20of%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20to%20learn%20texture-based%20features%2C%20which%20limits%20their%20performance%20when%0Aapplied%20to%20new%20domains%20with%20different%20visual%20characteristics.%20We%20hypothesize%0Athat%20leveraging%20geometric%20priors%20of%20vessel%20shapes%2C%20such%20as%20their%20tubular%20and%0Abranching%20nature%2C%20can%20lead%20to%20more%20robust%20and%20data-efficient%20models.%20To%0Ainvestigate%20this%2C%20we%20introduce%20VessShape%2C%20a%20methodology%20for%20generating%0Alarge-scale%202D%20synthetic%20datasets%20designed%20to%20instill%20a%20shape%20bias%20in%0Asegmentation%20models.%20VessShape%20images%20contain%20procedurally%20generated%20tubular%0Ageometries%20combined%20with%20a%20wide%20variety%20of%20foreground%20and%20background%20textures%2C%0Aencouraging%20models%20to%20learn%20shape%20cues%20rather%20than%20textures.%20We%20demonstrate%0Athat%20a%20model%20pre-trained%20on%20VessShape%20images%20achieves%20strong%20few-shot%0Asegmentation%20performance%20on%20two%20real-world%20datasets%20from%20different%20domains%2C%0Arequiring%20only%20four%20to%20ten%20samples%20for%20fine-tuning.%20Furthermore%2C%20the%20model%0Aexhibits%20notable%20zero-shot%20capabilities%2C%20effectively%20segmenting%20vessels%20in%0Aunseen%20domains%20without%20any%20target-specific%20training.%20Our%20results%20indicate%20that%0Apre-training%20with%20a%20strong%20shape%20bias%20can%20be%20an%20effective%20strategy%20to%20overcome%0Adata%20scarcity%20and%20improve%20model%20generalization%20in%20blood%20vessel%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVessShape%253A%2520Few-shot%25202D%2520blood%2520vessel%2520segmentation%2520by%2520leveraging%2520shape%250A%2520%2520priors%2520from%2520synthetic%2520images%26entry.906535625%3DCesar%2520H.%2520Comin%2520and%2520Wesley%2520N.%2520Galv%25C3%25A3o%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520blood%2520vessels%2520is%2520an%2520important%2520task%2520in%2520medical%2520image%250Aanalysis%252C%2520but%2520its%2520progress%2520is%2520often%2520hindered%2520by%2520the%2520scarcity%2520of%2520large%2520annotated%250Adatasets%2520and%2520the%2520poor%2520generalization%2520of%2520models%2520across%2520different%2520imaging%250Amodalities.%2520A%2520key%2520aspect%2520is%2520the%2520tendency%2520of%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529%2520to%2520learn%2520texture-based%2520features%252C%2520which%2520limits%2520their%2520performance%2520when%250Aapplied%2520to%2520new%2520domains%2520with%2520different%2520visual%2520characteristics.%2520We%2520hypothesize%250Athat%2520leveraging%2520geometric%2520priors%2520of%2520vessel%2520shapes%252C%2520such%2520as%2520their%2520tubular%2520and%250Abranching%2520nature%252C%2520can%2520lead%2520to%2520more%2520robust%2520and%2520data-efficient%2520models.%2520To%250Ainvestigate%2520this%252C%2520we%2520introduce%2520VessShape%252C%2520a%2520methodology%2520for%2520generating%250Alarge-scale%25202D%2520synthetic%2520datasets%2520designed%2520to%2520instill%2520a%2520shape%2520bias%2520in%250Asegmentation%2520models.%2520VessShape%2520images%2520contain%2520procedurally%2520generated%2520tubular%250Ageometries%2520combined%2520with%2520a%2520wide%2520variety%2520of%2520foreground%2520and%2520background%2520textures%252C%250Aencouraging%2520models%2520to%2520learn%2520shape%2520cues%2520rather%2520than%2520textures.%2520We%2520demonstrate%250Athat%2520a%2520model%2520pre-trained%2520on%2520VessShape%2520images%2520achieves%2520strong%2520few-shot%250Asegmentation%2520performance%2520on%2520two%2520real-world%2520datasets%2520from%2520different%2520domains%252C%250Arequiring%2520only%2520four%2520to%2520ten%2520samples%2520for%2520fine-tuning.%2520Furthermore%252C%2520the%2520model%250Aexhibits%2520notable%2520zero-shot%2520capabilities%252C%2520effectively%2520segmenting%2520vessels%2520in%250Aunseen%2520domains%2520without%2520any%2520target-specific%2520training.%2520Our%2520results%2520indicate%2520that%250Apre-training%2520with%2520a%2520strong%2520shape%2520bias%2520can%2520be%2520an%2520effective%2520strategy%2520to%2520overcome%250Adata%2520scarcity%2520and%2520improve%2520model%2520generalization%2520in%2520blood%2520vessel%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VessShape%3A%20Few-shot%202D%20blood%20vessel%20segmentation%20by%20leveraging%20shape%0A%20%20priors%20from%20synthetic%20images&entry.906535625=Cesar%20H.%20Comin%20and%20Wesley%20N.%20Galv%C3%A3o&entry.1292438233=%20%20Semantic%20segmentation%20of%20blood%20vessels%20is%20an%20important%20task%20in%20medical%20image%0Aanalysis%2C%20but%20its%20progress%20is%20often%20hindered%20by%20the%20scarcity%20of%20large%20annotated%0Adatasets%20and%20the%20poor%20generalization%20of%20models%20across%20different%20imaging%0Amodalities.%20A%20key%20aspect%20is%20the%20tendency%20of%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20to%20learn%20texture-based%20features%2C%20which%20limits%20their%20performance%20when%0Aapplied%20to%20new%20domains%20with%20different%20visual%20characteristics.%20We%20hypothesize%0Athat%20leveraging%20geometric%20priors%20of%20vessel%20shapes%2C%20such%20as%20their%20tubular%20and%0Abranching%20nature%2C%20can%20lead%20to%20more%20robust%20and%20data-efficient%20models.%20To%0Ainvestigate%20this%2C%20we%20introduce%20VessShape%2C%20a%20methodology%20for%20generating%0Alarge-scale%202D%20synthetic%20datasets%20designed%20to%20instill%20a%20shape%20bias%20in%0Asegmentation%20models.%20VessShape%20images%20contain%20procedurally%20generated%20tubular%0Ageometries%20combined%20with%20a%20wide%20variety%20of%20foreground%20and%20background%20textures%2C%0Aencouraging%20models%20to%20learn%20shape%20cues%20rather%20than%20textures.%20We%20demonstrate%0Athat%20a%20model%20pre-trained%20on%20VessShape%20images%20achieves%20strong%20few-shot%0Asegmentation%20performance%20on%20two%20real-world%20datasets%20from%20different%20domains%2C%0Arequiring%20only%20four%20to%20ten%20samples%20for%20fine-tuning.%20Furthermore%2C%20the%20model%0Aexhibits%20notable%20zero-shot%20capabilities%2C%20effectively%20segmenting%20vessels%20in%0Aunseen%20domains%20without%20any%20target-specific%20training.%20Our%20results%20indicate%20that%0Apre-training%20with%20a%20strong%20shape%20bias%20can%20be%20an%20effective%20strategy%20to%20overcome%0Adata%20scarcity%20and%20improve%20model%20generalization%20in%20blood%20vessel%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27646v1&entry.124074799=Read"},
{"title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance", "author": "Nikolaus Holzer and William Fishell and Baishakhi Ray and Mark Santolucito", "abstract": "  Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}.\n", "link": "http://arxiv.org/abs/2510.27544v1", "date": "2025-10-31", "relevancy": 2.0754, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanics%20of%20Learned%20Reasoning%201%3A%20TempoBench%2C%20A%20Benchmark%20for%0A%20%20Interpretable%20Deconstruction%20of%20Reasoning%20System%20Performance&body=Title%3A%20Mechanics%20of%20Learned%20Reasoning%201%3A%20TempoBench%2C%20A%20Benchmark%20for%0A%20%20Interpretable%20Deconstruction%20of%20Reasoning%20System%20Performance%0AAuthor%3A%20Nikolaus%20Holzer%20and%20William%20Fishell%20and%20Baishakhi%20Ray%20and%20Mark%20Santolucito%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20excelling%20and%20outpacing%20human%0Aperformance%20on%20many%20tasks.%20However%2C%20to%20improve%20LLM%20reasoning%2C%20researchers%0Aeither%20rely%20on%20ad-hoc%20generated%20datasets%20or%20formal%20mathematical%20proof%20systems%0Asuch%20as%20the%20Lean%20proof%20assistant.%20Whilst%20ad-hoc%20generated%20methods%20can%20capture%0Athe%20decision%20chains%20of%20real-world%20reasoning%20processes%2C%20they%20may%20encode%20some%0Ainadvertent%20bias%20in%20the%20space%20of%20reasoning%20they%20cover%3B%20they%20also%20cannot%20be%0Aformally%20verified.%20On%20the%20other%20hand%2C%20systems%20like%20Lean%20can%20guarantee%0Averifiability%2C%20but%20are%20not%20well-suited%20to%20capture%20the%20nature%20of%20agentic%0Adecision%20chain-based%20tasks.%20This%20creates%20a%20gap%20both%20in%20performance%20for%0Afunctions%20such%20as%20business%20agents%20or%20code%20assistants%2C%20and%20in%20the%20usefulness%20of%0ALLM%20reasoning%20benchmarks%2C%20whereby%20these%20fall%20short%20in%20reasoning%20structure%20or%0Areal-world%20alignment.%20We%20introduce%20TempoBench%2C%20the%20first%20formally%20grounded%20and%0Averifiable%20diagnostic%20benchmark%20that%20parametrizes%20difficulty%20to%20systematically%0Aanalyze%20how%20LLMs%20perform%20reasoning.%20TempoBench%20uses%20two%20evaluation%20benchmarks%0Ato%20break%20down%20reasoning%20ability.%20First%2C%20temporal%20trace%20evaluation%20%28TTE%29%20tests%0Athe%20ability%20of%20an%20LLM%20to%20understand%20and%20simulate%20the%20execution%20of%20a%20given%0Amulti-step%20reasoning%20system.%20Subsequently%2C%20temporal%20causal%20evaluation%20%28TCE%29%0Atests%20an%20LLM%27s%20ability%20to%20perform%20multi-step%20causal%20reasoning%20and%20to%20distill%0Acause-and-effect%20relations%20from%20complex%20systems.%20We%20find%20that%20models%20score%0A65.6%25%20on%20TCE-normal%2C%20and%207.5%25%20on%20TCE-hard.%20This%20shows%20that%20state-of-the-art%0ALLMs%20clearly%20understand%20the%20TCE%20task%20but%20perform%20poorly%20as%20system%20complexity%0Aincreases.%20Our%20code%20is%20available%20at%20our%0A%5Chref%7Bhttps%3A//github.com/nik-hz/tempobench%7D%7BGitHub%20repository%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanics%2520of%2520Learned%2520Reasoning%25201%253A%2520TempoBench%252C%2520A%2520Benchmark%2520for%250A%2520%2520Interpretable%2520Deconstruction%2520of%2520Reasoning%2520System%2520Performance%26entry.906535625%3DNikolaus%2520Holzer%2520and%2520William%2520Fishell%2520and%2520Baishakhi%2520Ray%2520and%2520Mark%2520Santolucito%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520excelling%2520and%2520outpacing%2520human%250Aperformance%2520on%2520many%2520tasks.%2520However%252C%2520to%2520improve%2520LLM%2520reasoning%252C%2520researchers%250Aeither%2520rely%2520on%2520ad-hoc%2520generated%2520datasets%2520or%2520formal%2520mathematical%2520proof%2520systems%250Asuch%2520as%2520the%2520Lean%2520proof%2520assistant.%2520Whilst%2520ad-hoc%2520generated%2520methods%2520can%2520capture%250Athe%2520decision%2520chains%2520of%2520real-world%2520reasoning%2520processes%252C%2520they%2520may%2520encode%2520some%250Ainadvertent%2520bias%2520in%2520the%2520space%2520of%2520reasoning%2520they%2520cover%253B%2520they%2520also%2520cannot%2520be%250Aformally%2520verified.%2520On%2520the%2520other%2520hand%252C%2520systems%2520like%2520Lean%2520can%2520guarantee%250Averifiability%252C%2520but%2520are%2520not%2520well-suited%2520to%2520capture%2520the%2520nature%2520of%2520agentic%250Adecision%2520chain-based%2520tasks.%2520This%2520creates%2520a%2520gap%2520both%2520in%2520performance%2520for%250Afunctions%2520such%2520as%2520business%2520agents%2520or%2520code%2520assistants%252C%2520and%2520in%2520the%2520usefulness%2520of%250ALLM%2520reasoning%2520benchmarks%252C%2520whereby%2520these%2520fall%2520short%2520in%2520reasoning%2520structure%2520or%250Areal-world%2520alignment.%2520We%2520introduce%2520TempoBench%252C%2520the%2520first%2520formally%2520grounded%2520and%250Averifiable%2520diagnostic%2520benchmark%2520that%2520parametrizes%2520difficulty%2520to%2520systematically%250Aanalyze%2520how%2520LLMs%2520perform%2520reasoning.%2520TempoBench%2520uses%2520two%2520evaluation%2520benchmarks%250Ato%2520break%2520down%2520reasoning%2520ability.%2520First%252C%2520temporal%2520trace%2520evaluation%2520%2528TTE%2529%2520tests%250Athe%2520ability%2520of%2520an%2520LLM%2520to%2520understand%2520and%2520simulate%2520the%2520execution%2520of%2520a%2520given%250Amulti-step%2520reasoning%2520system.%2520Subsequently%252C%2520temporal%2520causal%2520evaluation%2520%2528TCE%2529%250Atests%2520an%2520LLM%2527s%2520ability%2520to%2520perform%2520multi-step%2520causal%2520reasoning%2520and%2520to%2520distill%250Acause-and-effect%2520relations%2520from%2520complex%2520systems.%2520We%2520find%2520that%2520models%2520score%250A65.6%2525%2520on%2520TCE-normal%252C%2520and%25207.5%2525%2520on%2520TCE-hard.%2520This%2520shows%2520that%2520state-of-the-art%250ALLMs%2520clearly%2520understand%2520the%2520TCE%2520task%2520but%2520perform%2520poorly%2520as%2520system%2520complexity%250Aincreases.%2520Our%2520code%2520is%2520available%2520at%2520our%250A%255Chref%257Bhttps%253A//github.com/nik-hz/tempobench%257D%257BGitHub%2520repository%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanics%20of%20Learned%20Reasoning%201%3A%20TempoBench%2C%20A%20Benchmark%20for%0A%20%20Interpretable%20Deconstruction%20of%20Reasoning%20System%20Performance&entry.906535625=Nikolaus%20Holzer%20and%20William%20Fishell%20and%20Baishakhi%20Ray%20and%20Mark%20Santolucito&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20excelling%20and%20outpacing%20human%0Aperformance%20on%20many%20tasks.%20However%2C%20to%20improve%20LLM%20reasoning%2C%20researchers%0Aeither%20rely%20on%20ad-hoc%20generated%20datasets%20or%20formal%20mathematical%20proof%20systems%0Asuch%20as%20the%20Lean%20proof%20assistant.%20Whilst%20ad-hoc%20generated%20methods%20can%20capture%0Athe%20decision%20chains%20of%20real-world%20reasoning%20processes%2C%20they%20may%20encode%20some%0Ainadvertent%20bias%20in%20the%20space%20of%20reasoning%20they%20cover%3B%20they%20also%20cannot%20be%0Aformally%20verified.%20On%20the%20other%20hand%2C%20systems%20like%20Lean%20can%20guarantee%0Averifiability%2C%20but%20are%20not%20well-suited%20to%20capture%20the%20nature%20of%20agentic%0Adecision%20chain-based%20tasks.%20This%20creates%20a%20gap%20both%20in%20performance%20for%0Afunctions%20such%20as%20business%20agents%20or%20code%20assistants%2C%20and%20in%20the%20usefulness%20of%0ALLM%20reasoning%20benchmarks%2C%20whereby%20these%20fall%20short%20in%20reasoning%20structure%20or%0Areal-world%20alignment.%20We%20introduce%20TempoBench%2C%20the%20first%20formally%20grounded%20and%0Averifiable%20diagnostic%20benchmark%20that%20parametrizes%20difficulty%20to%20systematically%0Aanalyze%20how%20LLMs%20perform%20reasoning.%20TempoBench%20uses%20two%20evaluation%20benchmarks%0Ato%20break%20down%20reasoning%20ability.%20First%2C%20temporal%20trace%20evaluation%20%28TTE%29%20tests%0Athe%20ability%20of%20an%20LLM%20to%20understand%20and%20simulate%20the%20execution%20of%20a%20given%0Amulti-step%20reasoning%20system.%20Subsequently%2C%20temporal%20causal%20evaluation%20%28TCE%29%0Atests%20an%20LLM%27s%20ability%20to%20perform%20multi-step%20causal%20reasoning%20and%20to%20distill%0Acause-and-effect%20relations%20from%20complex%20systems.%20We%20find%20that%20models%20score%0A65.6%25%20on%20TCE-normal%2C%20and%207.5%25%20on%20TCE-hard.%20This%20shows%20that%20state-of-the-art%0ALLMs%20clearly%20understand%20the%20TCE%20task%20but%20perform%20poorly%20as%20system%20complexity%0Aincreases.%20Our%20code%20is%20available%20at%20our%0A%5Chref%7Bhttps%3A//github.com/nik-hz/tempobench%7D%7BGitHub%20repository%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27544v1&entry.124074799=Read"},
{"title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts\n  Using Large Language Models", "author": "Sungbok Shin and Hyeon Jeon and Sanghyun Hong and Niklas Elmqvist", "abstract": "  Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. Motivated by\ngrowing demands to surface tacit knowledge, we present the Data Therapist, a\nweb-based system that helps domain experts externalize such implicit knowledge\nthrough a mixed-initiative process combining iterative Q&A with interactive\nannotation. Powered by a large language model, the system automatically\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nsupports annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. A\nqualitative study with expert pairs from Accounting, Political Science, and\nComputer Security revealed recurring patterns in how expert reason about their\ndata and highlighted opportunities for AI support to enhance visualization\ndesign.\n", "link": "http://arxiv.org/abs/2505.00455v4", "date": "2025-10-31", "relevancy": 2.0546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models&body=Title%3A%20Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models%0AAuthor%3A%20Sungbok%20Shin%20and%20Hyeon%20Jeon%20and%20Sanghyun%20Hong%20and%20Niklas%20Elmqvist%0AAbstract%3A%20%20%20Effective%20data%20visualization%20requires%20not%20only%20technical%20proficiency%20but%20also%0Aa%20deep%20understanding%20of%20the%20domain-specific%20context%20in%20which%20data%20exists.%20This%0Acontext%20often%20includes%20tacit%20knowledge%20about%20data%20provenance%2C%20quality%2C%20and%0Aintended%20use%2C%20which%20is%20rarely%20explicit%20in%20the%20dataset%20itself.%20Motivated%20by%0Agrowing%20demands%20to%20surface%20tacit%20knowledge%2C%20we%20present%20the%20Data%20Therapist%2C%20a%0Aweb-based%20system%20that%20helps%20domain%20experts%20externalize%20such%20implicit%20knowledge%0Athrough%20a%20mixed-initiative%20process%20combining%20iterative%20Q%26A%20with%20interactive%0Aannotation.%20Powered%20by%20a%20large%20language%20model%2C%20the%20system%20automatically%0Aanalyzes%20user-supplied%20datasets%2C%20prompts%20users%20with%20targeted%20questions%2C%20and%0Asupports%20annotation%20at%20varying%20levels%20of%20granularity.%20The%20resulting%20structured%0Aknowledge%20base%20can%20inform%20both%20human%20and%20automated%20visualization%20design.%20A%0Aqualitative%20study%20with%20expert%20pairs%20from%20Accounting%2C%20Political%20Science%2C%20and%0AComputer%20Security%20revealed%20recurring%20patterns%20in%20how%20expert%20reason%20about%20their%0Adata%20and%20highlighted%20opportunities%20for%20AI%20support%20to%20enhance%20visualization%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00455v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Therapist%253A%2520Eliciting%2520Domain%2520Knowledge%2520from%2520Subject%2520Matter%2520Experts%250A%2520%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DSungbok%2520Shin%2520and%2520Hyeon%2520Jeon%2520and%2520Sanghyun%2520Hong%2520and%2520Niklas%2520Elmqvist%26entry.1292438233%3D%2520%2520Effective%2520data%2520visualization%2520requires%2520not%2520only%2520technical%2520proficiency%2520but%2520also%250Aa%2520deep%2520understanding%2520of%2520the%2520domain-specific%2520context%2520in%2520which%2520data%2520exists.%2520This%250Acontext%2520often%2520includes%2520tacit%2520knowledge%2520about%2520data%2520provenance%252C%2520quality%252C%2520and%250Aintended%2520use%252C%2520which%2520is%2520rarely%2520explicit%2520in%2520the%2520dataset%2520itself.%2520Motivated%2520by%250Agrowing%2520demands%2520to%2520surface%2520tacit%2520knowledge%252C%2520we%2520present%2520the%2520Data%2520Therapist%252C%2520a%250Aweb-based%2520system%2520that%2520helps%2520domain%2520experts%2520externalize%2520such%2520implicit%2520knowledge%250Athrough%2520a%2520mixed-initiative%2520process%2520combining%2520iterative%2520Q%2526A%2520with%2520interactive%250Aannotation.%2520Powered%2520by%2520a%2520large%2520language%2520model%252C%2520the%2520system%2520automatically%250Aanalyzes%2520user-supplied%2520datasets%252C%2520prompts%2520users%2520with%2520targeted%2520questions%252C%2520and%250Asupports%2520annotation%2520at%2520varying%2520levels%2520of%2520granularity.%2520The%2520resulting%2520structured%250Aknowledge%2520base%2520can%2520inform%2520both%2520human%2520and%2520automated%2520visualization%2520design.%2520A%250Aqualitative%2520study%2520with%2520expert%2520pairs%2520from%2520Accounting%252C%2520Political%2520Science%252C%2520and%250AComputer%2520Security%2520revealed%2520recurring%2520patterns%2520in%2520how%2520expert%2520reason%2520about%2520their%250Adata%2520and%2520highlighted%2520opportunities%2520for%2520AI%2520support%2520to%2520enhance%2520visualization%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00455v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Therapist%3A%20Eliciting%20Domain%20Knowledge%20from%20Subject%20Matter%20Experts%0A%20%20Using%20Large%20Language%20Models&entry.906535625=Sungbok%20Shin%20and%20Hyeon%20Jeon%20and%20Sanghyun%20Hong%20and%20Niklas%20Elmqvist&entry.1292438233=%20%20Effective%20data%20visualization%20requires%20not%20only%20technical%20proficiency%20but%20also%0Aa%20deep%20understanding%20of%20the%20domain-specific%20context%20in%20which%20data%20exists.%20This%0Acontext%20often%20includes%20tacit%20knowledge%20about%20data%20provenance%2C%20quality%2C%20and%0Aintended%20use%2C%20which%20is%20rarely%20explicit%20in%20the%20dataset%20itself.%20Motivated%20by%0Agrowing%20demands%20to%20surface%20tacit%20knowledge%2C%20we%20present%20the%20Data%20Therapist%2C%20a%0Aweb-based%20system%20that%20helps%20domain%20experts%20externalize%20such%20implicit%20knowledge%0Athrough%20a%20mixed-initiative%20process%20combining%20iterative%20Q%26A%20with%20interactive%0Aannotation.%20Powered%20by%20a%20large%20language%20model%2C%20the%20system%20automatically%0Aanalyzes%20user-supplied%20datasets%2C%20prompts%20users%20with%20targeted%20questions%2C%20and%0Asupports%20annotation%20at%20varying%20levels%20of%20granularity.%20The%20resulting%20structured%0Aknowledge%20base%20can%20inform%20both%20human%20and%20automated%20visualization%20design.%20A%0Aqualitative%20study%20with%20expert%20pairs%20from%20Accounting%2C%20Political%20Science%2C%20and%0AComputer%20Security%20revealed%20recurring%20patterns%20in%20how%20expert%20reason%20about%20their%0Adata%20and%20highlighted%20opportunities%20for%20AI%20support%20to%20enhance%20visualization%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00455v4&entry.124074799=Read"},
{"title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling", "author": "Zhuohan Wang and Ziwei Zhu and Ziniu Li and Congliang Chen and Yizhou Han and Yufeng Lin and Zhihang Lin and Angyang Gu and Xinglin Hu and Ruoyu Sun and Tian Ding", "abstract": "  Formulating optimization problems for industrial applications demands\nsignificant manual effort and domain expertise. While Large Language Models\n(LLMs) show promise in automating this process, evaluating their performance\nremains difficult due to the absence of robust metrics. Existing solver-based\napproaches often face inconsistency, infeasibility issues, and high\ncomputational costs. To address these issues, we propose ORGEval, a\ngraph-theoretic evaluation framework for assessing LLMs' capabilities in\nformulating linear and mixed-integer linear programs. ORGEval represents\noptimization models as graphs, reducing equivalence detection to graph\nisomorphism testing. We identify and prove a sufficient condition, when the\ntested graphs are symmetric decomposable (SD), under which the\nWeisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.\nBuilding on this, ORGEval integrates a tailored variant of the WL-test with an\nSD detection algorithm to evaluate model equivalence. By focusing on structural\nequivalence rather than instance-level configurations, ORGEval is robust to\nnumerical variations. Experimental results show that our method can\nsuccessfully detect model equivalence and produce 100\\% consistent results\nacross random parameter configurations, while significantly outperforming\nsolver-based methods in runtime, especially on difficult problems. Leveraging\nORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs\non optimization modeling. Our results reveal that although optimization\nmodeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4\nachieve the highest accuracies under direct prompting, outperforming even\nleading reasoning models.\n", "link": "http://arxiv.org/abs/2510.27610v1", "date": "2025-10-31", "relevancy": 2.054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5176}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORGEval%3A%20Graph-Theoretic%20Evaluation%20of%20LLMs%20in%20Optimization%20Modeling&body=Title%3A%20ORGEval%3A%20Graph-Theoretic%20Evaluation%20of%20LLMs%20in%20Optimization%20Modeling%0AAuthor%3A%20Zhuohan%20Wang%20and%20Ziwei%20Zhu%20and%20Ziniu%20Li%20and%20Congliang%20Chen%20and%20Yizhou%20Han%20and%20Yufeng%20Lin%20and%20Zhihang%20Lin%20and%20Angyang%20Gu%20and%20Xinglin%20Hu%20and%20Ruoyu%20Sun%20and%20Tian%20Ding%0AAbstract%3A%20%20%20Formulating%20optimization%20problems%20for%20industrial%20applications%20demands%0Asignificant%20manual%20effort%20and%20domain%20expertise.%20While%20Large%20Language%20Models%0A%28LLMs%29%20show%20promise%20in%20automating%20this%20process%2C%20evaluating%20their%20performance%0Aremains%20difficult%20due%20to%20the%20absence%20of%20robust%20metrics.%20Existing%20solver-based%0Aapproaches%20often%20face%20inconsistency%2C%20infeasibility%20issues%2C%20and%20high%0Acomputational%20costs.%20To%20address%20these%20issues%2C%20we%20propose%20ORGEval%2C%20a%0Agraph-theoretic%20evaluation%20framework%20for%20assessing%20LLMs%27%20capabilities%20in%0Aformulating%20linear%20and%20mixed-integer%20linear%20programs.%20ORGEval%20represents%0Aoptimization%20models%20as%20graphs%2C%20reducing%20equivalence%20detection%20to%20graph%0Aisomorphism%20testing.%20We%20identify%20and%20prove%20a%20sufficient%20condition%2C%20when%20the%0Atested%20graphs%20are%20symmetric%20decomposable%20%28SD%29%2C%20under%20which%20the%0AWeisfeiler-Lehman%20%28WL%29%20test%20is%20guaranteed%20to%20correctly%20detect%20isomorphism.%0ABuilding%20on%20this%2C%20ORGEval%20integrates%20a%20tailored%20variant%20of%20the%20WL-test%20with%20an%0ASD%20detection%20algorithm%20to%20evaluate%20model%20equivalence.%20By%20focusing%20on%20structural%0Aequivalence%20rather%20than%20instance-level%20configurations%2C%20ORGEval%20is%20robust%20to%0Anumerical%20variations.%20Experimental%20results%20show%20that%20our%20method%20can%0Asuccessfully%20detect%20model%20equivalence%20and%20produce%20100%5C%25%20consistent%20results%0Aacross%20random%20parameter%20configurations%2C%20while%20significantly%20outperforming%0Asolver-based%20methods%20in%20runtime%2C%20especially%20on%20difficult%20problems.%20Leveraging%0AORGEval%2C%20we%20construct%20the%20Bench4Opt%20dataset%20and%20benchmark%20state-of-the-art%20LLMs%0Aon%20optimization%20modeling.%20Our%20results%20reveal%20that%20although%20optimization%0Amodeling%20remains%20challenging%20for%20all%20LLMs%2C%20DeepSeek-V3%20and%20Claude-Opus-4%0Aachieve%20the%20highest%20accuracies%20under%20direct%20prompting%2C%20outperforming%20even%0Aleading%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORGEval%253A%2520Graph-Theoretic%2520Evaluation%2520of%2520LLMs%2520in%2520Optimization%2520Modeling%26entry.906535625%3DZhuohan%2520Wang%2520and%2520Ziwei%2520Zhu%2520and%2520Ziniu%2520Li%2520and%2520Congliang%2520Chen%2520and%2520Yizhou%2520Han%2520and%2520Yufeng%2520Lin%2520and%2520Zhihang%2520Lin%2520and%2520Angyang%2520Gu%2520and%2520Xinglin%2520Hu%2520and%2520Ruoyu%2520Sun%2520and%2520Tian%2520Ding%26entry.1292438233%3D%2520%2520Formulating%2520optimization%2520problems%2520for%2520industrial%2520applications%2520demands%250Asignificant%2520manual%2520effort%2520and%2520domain%2520expertise.%2520While%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520show%2520promise%2520in%2520automating%2520this%2520process%252C%2520evaluating%2520their%2520performance%250Aremains%2520difficult%2520due%2520to%2520the%2520absence%2520of%2520robust%2520metrics.%2520Existing%2520solver-based%250Aapproaches%2520often%2520face%2520inconsistency%252C%2520infeasibility%2520issues%252C%2520and%2520high%250Acomputational%2520costs.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520ORGEval%252C%2520a%250Agraph-theoretic%2520evaluation%2520framework%2520for%2520assessing%2520LLMs%2527%2520capabilities%2520in%250Aformulating%2520linear%2520and%2520mixed-integer%2520linear%2520programs.%2520ORGEval%2520represents%250Aoptimization%2520models%2520as%2520graphs%252C%2520reducing%2520equivalence%2520detection%2520to%2520graph%250Aisomorphism%2520testing.%2520We%2520identify%2520and%2520prove%2520a%2520sufficient%2520condition%252C%2520when%2520the%250Atested%2520graphs%2520are%2520symmetric%2520decomposable%2520%2528SD%2529%252C%2520under%2520which%2520the%250AWeisfeiler-Lehman%2520%2528WL%2529%2520test%2520is%2520guaranteed%2520to%2520correctly%2520detect%2520isomorphism.%250ABuilding%2520on%2520this%252C%2520ORGEval%2520integrates%2520a%2520tailored%2520variant%2520of%2520the%2520WL-test%2520with%2520an%250ASD%2520detection%2520algorithm%2520to%2520evaluate%2520model%2520equivalence.%2520By%2520focusing%2520on%2520structural%250Aequivalence%2520rather%2520than%2520instance-level%2520configurations%252C%2520ORGEval%2520is%2520robust%2520to%250Anumerical%2520variations.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520can%250Asuccessfully%2520detect%2520model%2520equivalence%2520and%2520produce%2520100%255C%2525%2520consistent%2520results%250Aacross%2520random%2520parameter%2520configurations%252C%2520while%2520significantly%2520outperforming%250Asolver-based%2520methods%2520in%2520runtime%252C%2520especially%2520on%2520difficult%2520problems.%2520Leveraging%250AORGEval%252C%2520we%2520construct%2520the%2520Bench4Opt%2520dataset%2520and%2520benchmark%2520state-of-the-art%2520LLMs%250Aon%2520optimization%2520modeling.%2520Our%2520results%2520reveal%2520that%2520although%2520optimization%250Amodeling%2520remains%2520challenging%2520for%2520all%2520LLMs%252C%2520DeepSeek-V3%2520and%2520Claude-Opus-4%250Aachieve%2520the%2520highest%2520accuracies%2520under%2520direct%2520prompting%252C%2520outperforming%2520even%250Aleading%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORGEval%3A%20Graph-Theoretic%20Evaluation%20of%20LLMs%20in%20Optimization%20Modeling&entry.906535625=Zhuohan%20Wang%20and%20Ziwei%20Zhu%20and%20Ziniu%20Li%20and%20Congliang%20Chen%20and%20Yizhou%20Han%20and%20Yufeng%20Lin%20and%20Zhihang%20Lin%20and%20Angyang%20Gu%20and%20Xinglin%20Hu%20and%20Ruoyu%20Sun%20and%20Tian%20Ding&entry.1292438233=%20%20Formulating%20optimization%20problems%20for%20industrial%20applications%20demands%0Asignificant%20manual%20effort%20and%20domain%20expertise.%20While%20Large%20Language%20Models%0A%28LLMs%29%20show%20promise%20in%20automating%20this%20process%2C%20evaluating%20their%20performance%0Aremains%20difficult%20due%20to%20the%20absence%20of%20robust%20metrics.%20Existing%20solver-based%0Aapproaches%20often%20face%20inconsistency%2C%20infeasibility%20issues%2C%20and%20high%0Acomputational%20costs.%20To%20address%20these%20issues%2C%20we%20propose%20ORGEval%2C%20a%0Agraph-theoretic%20evaluation%20framework%20for%20assessing%20LLMs%27%20capabilities%20in%0Aformulating%20linear%20and%20mixed-integer%20linear%20programs.%20ORGEval%20represents%0Aoptimization%20models%20as%20graphs%2C%20reducing%20equivalence%20detection%20to%20graph%0Aisomorphism%20testing.%20We%20identify%20and%20prove%20a%20sufficient%20condition%2C%20when%20the%0Atested%20graphs%20are%20symmetric%20decomposable%20%28SD%29%2C%20under%20which%20the%0AWeisfeiler-Lehman%20%28WL%29%20test%20is%20guaranteed%20to%20correctly%20detect%20isomorphism.%0ABuilding%20on%20this%2C%20ORGEval%20integrates%20a%20tailored%20variant%20of%20the%20WL-test%20with%20an%0ASD%20detection%20algorithm%20to%20evaluate%20model%20equivalence.%20By%20focusing%20on%20structural%0Aequivalence%20rather%20than%20instance-level%20configurations%2C%20ORGEval%20is%20robust%20to%0Anumerical%20variations.%20Experimental%20results%20show%20that%20our%20method%20can%0Asuccessfully%20detect%20model%20equivalence%20and%20produce%20100%5C%25%20consistent%20results%0Aacross%20random%20parameter%20configurations%2C%20while%20significantly%20outperforming%0Asolver-based%20methods%20in%20runtime%2C%20especially%20on%20difficult%20problems.%20Leveraging%0AORGEval%2C%20we%20construct%20the%20Bench4Opt%20dataset%20and%20benchmark%20state-of-the-art%20LLMs%0Aon%20optimization%20modeling.%20Our%20results%20reveal%20that%20although%20optimization%0Amodeling%20remains%20challenging%20for%20all%20LLMs%2C%20DeepSeek-V3%20and%20Claude-Opus-4%0Aachieve%20the%20highest%20accuracies%20under%20direct%20prompting%2C%20outperforming%20even%0Aleading%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27610v1&entry.124074799=Read"},
{"title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models", "author": "Giacomo Camposampiero and Michael Hersche and Roger Wattenhofer and Abu Sebastian and Abbas Rahimi", "abstract": "  We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.\n", "link": "http://arxiv.org/abs/2510.17496v2", "date": "2025-10-31", "relevancy": 2.0489, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5188}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I-RAVEN-X%3A%20Benchmarking%20Generalization%20and%20Robustness%20of%20Analogical%20and%0A%20%20Mathematical%20Reasoning%20in%20Large%20Language%20and%20Reasoning%20Models&body=Title%3A%20I-RAVEN-X%3A%20Benchmarking%20Generalization%20and%20Robustness%20of%20Analogical%20and%0A%20%20Mathematical%20Reasoning%20in%20Large%20Language%20and%20Reasoning%20Models%0AAuthor%3A%20Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi%0AAbstract%3A%20%20%20We%20introduce%20I-RAVEN-X%2C%20a%20symbolic%20benchmark%20designed%20to%20evaluate%0Ageneralization%20and%20robustness%20in%20analogical%20and%20mathematical%20reasoning%20for%0ALarge%20Language%20Models%20%28LLMs%29%20and%20Large%20Reasoning%20Models%20%28LRMs%29.%20I-RAVEN-X%0Aextends%20I-RAVEN%20by%20increasing%20operand%20complexity%2C%20attribute%20range%2C%20and%0Aintroducing%20perceptual%20uncertainty.%20Compared%20to%20LLMs%2C%20empirical%20results%20show%0Athat%20LRMs%20achieve%20improved%20productivity%20and%20systematicity%20on%20longer%20reasoning%0Arelations%20and%20wider%20attribute%20ranges%2C%20respectively.%20However%2C%20LRMs%20are%20still%0Asignificantly%20challenged%20by%20reasoning%20under%20uncertainty%20and%20cannot%20effectively%0Aexplore%20multiple%20probabilistic%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI-RAVEN-X%253A%2520Benchmarking%2520Generalization%2520and%2520Robustness%2520of%2520Analogical%2520and%250A%2520%2520Mathematical%2520Reasoning%2520in%2520Large%2520Language%2520and%2520Reasoning%2520Models%26entry.906535625%3DGiacomo%2520Camposampiero%2520and%2520Michael%2520Hersche%2520and%2520Roger%2520Wattenhofer%2520and%2520Abu%2520Sebastian%2520and%2520Abbas%2520Rahimi%26entry.1292438233%3D%2520%2520We%2520introduce%2520I-RAVEN-X%252C%2520a%2520symbolic%2520benchmark%2520designed%2520to%2520evaluate%250Ageneralization%2520and%2520robustness%2520in%2520analogical%2520and%2520mathematical%2520reasoning%2520for%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529.%2520I-RAVEN-X%250Aextends%2520I-RAVEN%2520by%2520increasing%2520operand%2520complexity%252C%2520attribute%2520range%252C%2520and%250Aintroducing%2520perceptual%2520uncertainty.%2520Compared%2520to%2520LLMs%252C%2520empirical%2520results%2520show%250Athat%2520LRMs%2520achieve%2520improved%2520productivity%2520and%2520systematicity%2520on%2520longer%2520reasoning%250Arelations%2520and%2520wider%2520attribute%2520ranges%252C%2520respectively.%2520However%252C%2520LRMs%2520are%2520still%250Asignificantly%2520challenged%2520by%2520reasoning%2520under%2520uncertainty%2520and%2520cannot%2520effectively%250Aexplore%2520multiple%2520probabilistic%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I-RAVEN-X%3A%20Benchmarking%20Generalization%20and%20Robustness%20of%20Analogical%20and%0A%20%20Mathematical%20Reasoning%20in%20Large%20Language%20and%20Reasoning%20Models&entry.906535625=Giacomo%20Camposampiero%20and%20Michael%20Hersche%20and%20Roger%20Wattenhofer%20and%20Abu%20Sebastian%20and%20Abbas%20Rahimi&entry.1292438233=%20%20We%20introduce%20I-RAVEN-X%2C%20a%20symbolic%20benchmark%20designed%20to%20evaluate%0Ageneralization%20and%20robustness%20in%20analogical%20and%20mathematical%20reasoning%20for%0ALarge%20Language%20Models%20%28LLMs%29%20and%20Large%20Reasoning%20Models%20%28LRMs%29.%20I-RAVEN-X%0Aextends%20I-RAVEN%20by%20increasing%20operand%20complexity%2C%20attribute%20range%2C%20and%0Aintroducing%20perceptual%20uncertainty.%20Compared%20to%20LLMs%2C%20empirical%20results%20show%0Athat%20LRMs%20achieve%20improved%20productivity%20and%20systematicity%20on%20longer%20reasoning%0Arelations%20and%20wider%20attribute%20ranges%2C%20respectively.%20However%2C%20LRMs%20are%20still%0Asignificantly%20challenged%20by%20reasoning%20under%20uncertainty%20and%20cannot%20effectively%0Aexplore%20multiple%20probabilistic%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17496v2&entry.124074799=Read"},
{"title": "Active transfer learning for structural health monitoring", "author": "J. Poole and N. Dervilis and K. Worden and P. Gardner and V. Giglioni and R. S. Mills and A. J. Hughes", "abstract": "  Data for training structural health monitoring (SHM) systems are often\nexpensive and/or impractical to obtain, particularly for labelled data.\nPopulation-based SHM (PBSHM) aims to address this limitation by leveraging data\nfrom multiple structures. However, data from different structures will follow\ndistinct distributions, potentially leading to large generalisation errors for\nmodels learnt via conventional machine learning methods. To address this issue,\ntransfer learning -- in the form of domain adaptation (DA) -- can be used to\nalign the data distributions. Most previous approaches have only considered\n\\emph{unsupervised} DA, where no labelled target data are available; they do\nnot consider how to incorporate these technologies in an online framework --\nupdating as labels are obtained throughout the monitoring campaign. This paper\nproposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA\nmappings using a limited quantity of labelled target data. In addition, this\nmodel is integrated into an active sampling strategy to guide inspections to\nselect the most informative observations to label -- leading to further\nreductions in the required labelled data to learn a target classifier. The\neffectiveness of this methodology is evaluated on a population of experimental\nbridges. Specifically, this population includes data corresponding to several\ndamage states, as well as, a comprehensive set of environmental conditions. It\nis found that combining transfer learning and active learning can improve data\nefficiency when learning classification models in label-scarce scenarios. This\nresult has implications for data-informed operation and maintenance of\nstructures, suggesting a reduction in inspections over the operational lifetime\nof a structure -- and therefore a reduction in operational costs -- can be\nachieved.\n", "link": "http://arxiv.org/abs/2510.27525v1", "date": "2025-10-31", "relevancy": 2.0448, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20transfer%20learning%20for%20structural%20health%20monitoring&body=Title%3A%20Active%20transfer%20learning%20for%20structural%20health%20monitoring%0AAuthor%3A%20J.%20Poole%20and%20N.%20Dervilis%20and%20K.%20Worden%20and%20P.%20Gardner%20and%20V.%20Giglioni%20and%20R.%20S.%20Mills%20and%20A.%20J.%20Hughes%0AAbstract%3A%20%20%20Data%20for%20training%20structural%20health%20monitoring%20%28SHM%29%20systems%20are%20often%0Aexpensive%20and/or%20impractical%20to%20obtain%2C%20particularly%20for%20labelled%20data.%0APopulation-based%20SHM%20%28PBSHM%29%20aims%20to%20address%20this%20limitation%20by%20leveraging%20data%0Afrom%20multiple%20structures.%20However%2C%20data%20from%20different%20structures%20will%20follow%0Adistinct%20distributions%2C%20potentially%20leading%20to%20large%20generalisation%20errors%20for%0Amodels%20learnt%20via%20conventional%20machine%20learning%20methods.%20To%20address%20this%20issue%2C%0Atransfer%20learning%20--%20in%20the%20form%20of%20domain%20adaptation%20%28DA%29%20--%20can%20be%20used%20to%0Aalign%20the%20data%20distributions.%20Most%20previous%20approaches%20have%20only%20considered%0A%5Cemph%7Bunsupervised%7D%20DA%2C%20where%20no%20labelled%20target%20data%20are%20available%3B%20they%20do%0Anot%20consider%20how%20to%20incorporate%20these%20technologies%20in%20an%20online%20framework%20--%0Aupdating%20as%20labels%20are%20obtained%20throughout%20the%20monitoring%20campaign.%20This%20paper%0Aproposes%20a%20Bayesian%20framework%20for%20DA%20in%20PBSHM%2C%20that%20can%20improve%20unsupervised%20DA%0Amappings%20using%20a%20limited%20quantity%20of%20labelled%20target%20data.%20In%20addition%2C%20this%0Amodel%20is%20integrated%20into%20an%20active%20sampling%20strategy%20to%20guide%20inspections%20to%0Aselect%20the%20most%20informative%20observations%20to%20label%20--%20leading%20to%20further%0Areductions%20in%20the%20required%20labelled%20data%20to%20learn%20a%20target%20classifier.%20The%0Aeffectiveness%20of%20this%20methodology%20is%20evaluated%20on%20a%20population%20of%20experimental%0Abridges.%20Specifically%2C%20this%20population%20includes%20data%20corresponding%20to%20several%0Adamage%20states%2C%20as%20well%20as%2C%20a%20comprehensive%20set%20of%20environmental%20conditions.%20It%0Ais%20found%20that%20combining%20transfer%20learning%20and%20active%20learning%20can%20improve%20data%0Aefficiency%20when%20learning%20classification%20models%20in%20label-scarce%20scenarios.%20This%0Aresult%20has%20implications%20for%20data-informed%20operation%20and%20maintenance%20of%0Astructures%2C%20suggesting%20a%20reduction%20in%20inspections%20over%20the%20operational%20lifetime%0Aof%20a%20structure%20--%20and%20therefore%20a%20reduction%20in%20operational%20costs%20--%20can%20be%0Aachieved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520transfer%2520learning%2520for%2520structural%2520health%2520monitoring%26entry.906535625%3DJ.%2520Poole%2520and%2520N.%2520Dervilis%2520and%2520K.%2520Worden%2520and%2520P.%2520Gardner%2520and%2520V.%2520Giglioni%2520and%2520R.%2520S.%2520Mills%2520and%2520A.%2520J.%2520Hughes%26entry.1292438233%3D%2520%2520Data%2520for%2520training%2520structural%2520health%2520monitoring%2520%2528SHM%2529%2520systems%2520are%2520often%250Aexpensive%2520and/or%2520impractical%2520to%2520obtain%252C%2520particularly%2520for%2520labelled%2520data.%250APopulation-based%2520SHM%2520%2528PBSHM%2529%2520aims%2520to%2520address%2520this%2520limitation%2520by%2520leveraging%2520data%250Afrom%2520multiple%2520structures.%2520However%252C%2520data%2520from%2520different%2520structures%2520will%2520follow%250Adistinct%2520distributions%252C%2520potentially%2520leading%2520to%2520large%2520generalisation%2520errors%2520for%250Amodels%2520learnt%2520via%2520conventional%2520machine%2520learning%2520methods.%2520To%2520address%2520this%2520issue%252C%250Atransfer%2520learning%2520--%2520in%2520the%2520form%2520of%2520domain%2520adaptation%2520%2528DA%2529%2520--%2520can%2520be%2520used%2520to%250Aalign%2520the%2520data%2520distributions.%2520Most%2520previous%2520approaches%2520have%2520only%2520considered%250A%255Cemph%257Bunsupervised%257D%2520DA%252C%2520where%2520no%2520labelled%2520target%2520data%2520are%2520available%253B%2520they%2520do%250Anot%2520consider%2520how%2520to%2520incorporate%2520these%2520technologies%2520in%2520an%2520online%2520framework%2520--%250Aupdating%2520as%2520labels%2520are%2520obtained%2520throughout%2520the%2520monitoring%2520campaign.%2520This%2520paper%250Aproposes%2520a%2520Bayesian%2520framework%2520for%2520DA%2520in%2520PBSHM%252C%2520that%2520can%2520improve%2520unsupervised%2520DA%250Amappings%2520using%2520a%2520limited%2520quantity%2520of%2520labelled%2520target%2520data.%2520In%2520addition%252C%2520this%250Amodel%2520is%2520integrated%2520into%2520an%2520active%2520sampling%2520strategy%2520to%2520guide%2520inspections%2520to%250Aselect%2520the%2520most%2520informative%2520observations%2520to%2520label%2520--%2520leading%2520to%2520further%250Areductions%2520in%2520the%2520required%2520labelled%2520data%2520to%2520learn%2520a%2520target%2520classifier.%2520The%250Aeffectiveness%2520of%2520this%2520methodology%2520is%2520evaluated%2520on%2520a%2520population%2520of%2520experimental%250Abridges.%2520Specifically%252C%2520this%2520population%2520includes%2520data%2520corresponding%2520to%2520several%250Adamage%2520states%252C%2520as%2520well%2520as%252C%2520a%2520comprehensive%2520set%2520of%2520environmental%2520conditions.%2520It%250Ais%2520found%2520that%2520combining%2520transfer%2520learning%2520and%2520active%2520learning%2520can%2520improve%2520data%250Aefficiency%2520when%2520learning%2520classification%2520models%2520in%2520label-scarce%2520scenarios.%2520This%250Aresult%2520has%2520implications%2520for%2520data-informed%2520operation%2520and%2520maintenance%2520of%250Astructures%252C%2520suggesting%2520a%2520reduction%2520in%2520inspections%2520over%2520the%2520operational%2520lifetime%250Aof%2520a%2520structure%2520--%2520and%2520therefore%2520a%2520reduction%2520in%2520operational%2520costs%2520--%2520can%2520be%250Aachieved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20transfer%20learning%20for%20structural%20health%20monitoring&entry.906535625=J.%20Poole%20and%20N.%20Dervilis%20and%20K.%20Worden%20and%20P.%20Gardner%20and%20V.%20Giglioni%20and%20R.%20S.%20Mills%20and%20A.%20J.%20Hughes&entry.1292438233=%20%20Data%20for%20training%20structural%20health%20monitoring%20%28SHM%29%20systems%20are%20often%0Aexpensive%20and/or%20impractical%20to%20obtain%2C%20particularly%20for%20labelled%20data.%0APopulation-based%20SHM%20%28PBSHM%29%20aims%20to%20address%20this%20limitation%20by%20leveraging%20data%0Afrom%20multiple%20structures.%20However%2C%20data%20from%20different%20structures%20will%20follow%0Adistinct%20distributions%2C%20potentially%20leading%20to%20large%20generalisation%20errors%20for%0Amodels%20learnt%20via%20conventional%20machine%20learning%20methods.%20To%20address%20this%20issue%2C%0Atransfer%20learning%20--%20in%20the%20form%20of%20domain%20adaptation%20%28DA%29%20--%20can%20be%20used%20to%0Aalign%20the%20data%20distributions.%20Most%20previous%20approaches%20have%20only%20considered%0A%5Cemph%7Bunsupervised%7D%20DA%2C%20where%20no%20labelled%20target%20data%20are%20available%3B%20they%20do%0Anot%20consider%20how%20to%20incorporate%20these%20technologies%20in%20an%20online%20framework%20--%0Aupdating%20as%20labels%20are%20obtained%20throughout%20the%20monitoring%20campaign.%20This%20paper%0Aproposes%20a%20Bayesian%20framework%20for%20DA%20in%20PBSHM%2C%20that%20can%20improve%20unsupervised%20DA%0Amappings%20using%20a%20limited%20quantity%20of%20labelled%20target%20data.%20In%20addition%2C%20this%0Amodel%20is%20integrated%20into%20an%20active%20sampling%20strategy%20to%20guide%20inspections%20to%0Aselect%20the%20most%20informative%20observations%20to%20label%20--%20leading%20to%20further%0Areductions%20in%20the%20required%20labelled%20data%20to%20learn%20a%20target%20classifier.%20The%0Aeffectiveness%20of%20this%20methodology%20is%20evaluated%20on%20a%20population%20of%20experimental%0Abridges.%20Specifically%2C%20this%20population%20includes%20data%20corresponding%20to%20several%0Adamage%20states%2C%20as%20well%20as%2C%20a%20comprehensive%20set%20of%20environmental%20conditions.%20It%0Ais%20found%20that%20combining%20transfer%20learning%20and%20active%20learning%20can%20improve%20data%0Aefficiency%20when%20learning%20classification%20models%20in%20label-scarce%20scenarios.%20This%0Aresult%20has%20implications%20for%20data-informed%20operation%20and%20maintenance%20of%0Astructures%2C%20suggesting%20a%20reduction%20in%20inspections%20over%20the%20operational%20lifetime%0Aof%20a%20structure%20--%20and%20therefore%20a%20reduction%20in%20operational%20costs%20--%20can%20be%0Aachieved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27525v1&entry.124074799=Read"},
{"title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models", "author": "Palakorn Achananuparp and Ee-Peng Lim and Yao Lu", "abstract": "  Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs.\n", "link": "http://arxiv.org/abs/2503.12989v3", "date": "2025-10-31", "relevancy": 2.0368, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stage%20Framework%20with%20Taxonomy-Guided%20Reasoning%20for%20Occupation%0A%20%20Classification%20Using%20Large%20Language%20Models&body=Title%3A%20A%20Multi-Stage%20Framework%20with%20Taxonomy-Guided%20Reasoning%20for%20Occupation%0A%20%20Classification%20Using%20Large%20Language%20Models%0AAuthor%3A%20Palakorn%20Achananuparp%20and%20Ee-Peng%20Lim%20and%20Yao%20Lu%0AAbstract%3A%20%20%20Automatically%20annotating%20job%20data%20with%20standardized%20occupations%20from%0Ataxonomies%2C%20known%20as%20occupation%20classification%2C%20is%20crucial%20for%20labor%20market%0Aanalysis.%20However%2C%20this%20task%20is%20often%20hindered%20by%20data%20scarcity%20and%20the%0Achallenges%20of%20manual%20annotations.%20While%20large%20language%20models%20%28LLMs%29%20hold%0Apromise%20due%20to%20their%20extensive%20world%20knowledge%20and%20in-context%20learning%0Acapabilities%2C%20their%20effectiveness%20depends%20on%20their%20knowledge%20of%20occupational%0Ataxonomies%2C%20which%20remains%20unclear.%20In%20this%20study%2C%20we%20assess%20the%20ability%20of%20LLMs%0Ato%20generate%20precise%20taxonomic%20entities%20from%20taxonomy%2C%20highlighting%20their%0Alimitations%2C%20especially%20for%20smaller%20models.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-stage%20framework%20consisting%20of%20inference%2C%20retrieval%2C%20and%0Areranking%20stages%2C%20which%20integrates%20taxonomy-guided%20reasoning%20examples%20to%0Aenhance%20performance%20by%20aligning%20outputs%20with%20taxonomic%20knowledge.%20Evaluations%0Aon%20a%20large-scale%20dataset%20show%20that%20our%20framework%20not%20only%20enhances%20occupation%0Aand%20skill%20classification%20tasks%2C%20but%20also%20provides%20a%20cost-effective%20alternative%0Ato%20frontier%20models%20like%20GPT-4o%2C%20significantly%20reducing%20computational%20costs%0Awhile%20maintaining%20strong%20performance.%20This%20makes%20it%20a%20practical%20and%20scalable%0Asolution%20for%20occupation%20classification%20and%20related%20tasks%20across%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stage%2520Framework%2520with%2520Taxonomy-Guided%2520Reasoning%2520for%2520Occupation%250A%2520%2520Classification%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DPalakorn%2520Achananuparp%2520and%2520Ee-Peng%2520Lim%2520and%2520Yao%2520Lu%26entry.1292438233%3D%2520%2520Automatically%2520annotating%2520job%2520data%2520with%2520standardized%2520occupations%2520from%250Ataxonomies%252C%2520known%2520as%2520occupation%2520classification%252C%2520is%2520crucial%2520for%2520labor%2520market%250Aanalysis.%2520However%252C%2520this%2520task%2520is%2520often%2520hindered%2520by%2520data%2520scarcity%2520and%2520the%250Achallenges%2520of%2520manual%2520annotations.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520hold%250Apromise%2520due%2520to%2520their%2520extensive%2520world%2520knowledge%2520and%2520in-context%2520learning%250Acapabilities%252C%2520their%2520effectiveness%2520depends%2520on%2520their%2520knowledge%2520of%2520occupational%250Ataxonomies%252C%2520which%2520remains%2520unclear.%2520In%2520this%2520study%252C%2520we%2520assess%2520the%2520ability%2520of%2520LLMs%250Ato%2520generate%2520precise%2520taxonomic%2520entities%2520from%2520taxonomy%252C%2520highlighting%2520their%250Alimitations%252C%2520especially%2520for%2520smaller%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520multi-stage%2520framework%2520consisting%2520of%2520inference%252C%2520retrieval%252C%2520and%250Areranking%2520stages%252C%2520which%2520integrates%2520taxonomy-guided%2520reasoning%2520examples%2520to%250Aenhance%2520performance%2520by%2520aligning%2520outputs%2520with%2520taxonomic%2520knowledge.%2520Evaluations%250Aon%2520a%2520large-scale%2520dataset%2520show%2520that%2520our%2520framework%2520not%2520only%2520enhances%2520occupation%250Aand%2520skill%2520classification%2520tasks%252C%2520but%2520also%2520provides%2520a%2520cost-effective%2520alternative%250Ato%2520frontier%2520models%2520like%2520GPT-4o%252C%2520significantly%2520reducing%2520computational%2520costs%250Awhile%2520maintaining%2520strong%2520performance.%2520This%2520makes%2520it%2520a%2520practical%2520and%2520scalable%250Asolution%2520for%2520occupation%2520classification%2520and%2520related%2520tasks%2520across%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stage%20Framework%20with%20Taxonomy-Guided%20Reasoning%20for%20Occupation%0A%20%20Classification%20Using%20Large%20Language%20Models&entry.906535625=Palakorn%20Achananuparp%20and%20Ee-Peng%20Lim%20and%20Yao%20Lu&entry.1292438233=%20%20Automatically%20annotating%20job%20data%20with%20standardized%20occupations%20from%0Ataxonomies%2C%20known%20as%20occupation%20classification%2C%20is%20crucial%20for%20labor%20market%0Aanalysis.%20However%2C%20this%20task%20is%20often%20hindered%20by%20data%20scarcity%20and%20the%0Achallenges%20of%20manual%20annotations.%20While%20large%20language%20models%20%28LLMs%29%20hold%0Apromise%20due%20to%20their%20extensive%20world%20knowledge%20and%20in-context%20learning%0Acapabilities%2C%20their%20effectiveness%20depends%20on%20their%20knowledge%20of%20occupational%0Ataxonomies%2C%20which%20remains%20unclear.%20In%20this%20study%2C%20we%20assess%20the%20ability%20of%20LLMs%0Ato%20generate%20precise%20taxonomic%20entities%20from%20taxonomy%2C%20highlighting%20their%0Alimitations%2C%20especially%20for%20smaller%20models.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-stage%20framework%20consisting%20of%20inference%2C%20retrieval%2C%20and%0Areranking%20stages%2C%20which%20integrates%20taxonomy-guided%20reasoning%20examples%20to%0Aenhance%20performance%20by%20aligning%20outputs%20with%20taxonomic%20knowledge.%20Evaluations%0Aon%20a%20large-scale%20dataset%20show%20that%20our%20framework%20not%20only%20enhances%20occupation%0Aand%20skill%20classification%20tasks%2C%20but%20also%20provides%20a%20cost-effective%20alternative%0Ato%20frontier%20models%20like%20GPT-4o%2C%20significantly%20reducing%20computational%20costs%0Awhile%20maintaining%20strong%20performance.%20This%20makes%20it%20a%20practical%20and%20scalable%0Asolution%20for%20occupation%20classification%20and%20related%20tasks%20across%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12989v3&entry.124074799=Read"},
{"title": "BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for\n  Scalable and Efficient Text Summarization", "author": "Desta Haileselassie Hagos and Legand L. Burge and Anietie Andy and Anis Yazidi and Vladimir Vlassov", "abstract": "  Transformer-based architectures have advanced text summarization, yet their\nquadratic complexity limits scalability on long documents. This paper\nintroduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a\nnovel framework that combines sparse attention, adaptive spans, and bilinear\nattention to address these limitations. Sparse attention reduces computational\ncosts by focusing on the most relevant parts of the input, while adaptive spans\ndynamically adjust the attention ranges. Bilinear attention complements both by\nmodeling complex token interactions within this refined context. BiSparse-AAS\nconsistently outperforms state-of-the-art baselines in both extractive and\nabstractive summarization tasks, achieving average ROUGE improvements of about\n68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance\non OpenWebText and Gigaword datasets. By addressing efficiency, scalability,\nand long-sequence modeling, BiSparse-AAS provides a unified, practical solution\nfor real-world text summarization applications.\n", "link": "http://arxiv.org/abs/2510.27516v1", "date": "2025-10-31", "relevancy": 2.0277, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5076}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiSparse-AAS%3A%20Bilinear%20Sparse%20Attention%20and%20Adaptive%20Spans%20Framework%20for%0A%20%20Scalable%20and%20Efficient%20Text%20Summarization&body=Title%3A%20BiSparse-AAS%3A%20Bilinear%20Sparse%20Attention%20and%20Adaptive%20Spans%20Framework%20for%0A%20%20Scalable%20and%20Efficient%20Text%20Summarization%0AAuthor%3A%20Desta%20Haileselassie%20Hagos%20and%20Legand%20L.%20Burge%20and%20Anietie%20Andy%20and%20Anis%20Yazidi%20and%20Vladimir%20Vlassov%0AAbstract%3A%20%20%20Transformer-based%20architectures%20have%20advanced%20text%20summarization%2C%20yet%20their%0Aquadratic%20complexity%20limits%20scalability%20on%20long%20documents.%20This%20paper%0Aintroduces%20BiSparse-AAS%20%28Bilinear%20Sparse%20Attention%20with%20Adaptive%20Spans%29%2C%20a%0Anovel%20framework%20that%20combines%20sparse%20attention%2C%20adaptive%20spans%2C%20and%20bilinear%0Aattention%20to%20address%20these%20limitations.%20Sparse%20attention%20reduces%20computational%0Acosts%20by%20focusing%20on%20the%20most%20relevant%20parts%20of%20the%20input%2C%20while%20adaptive%20spans%0Adynamically%20adjust%20the%20attention%20ranges.%20Bilinear%20attention%20complements%20both%20by%0Amodeling%20complex%20token%20interactions%20within%20this%20refined%20context.%20BiSparse-AAS%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20in%20both%20extractive%20and%0Aabstractive%20summarization%20tasks%2C%20achieving%20average%20ROUGE%20improvements%20of%20about%0A68.1%25%20on%20CNN/DailyMail%20and%2052.6%25%20on%20XSum%2C%20while%20maintaining%20strong%20performance%0Aon%20OpenWebText%20and%20Gigaword%20datasets.%20By%20addressing%20efficiency%2C%20scalability%2C%0Aand%20long-sequence%20modeling%2C%20BiSparse-AAS%20provides%20a%20unified%2C%20practical%20solution%0Afor%20real-world%20text%20summarization%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiSparse-AAS%253A%2520Bilinear%2520Sparse%2520Attention%2520and%2520Adaptive%2520Spans%2520Framework%2520for%250A%2520%2520Scalable%2520and%2520Efficient%2520Text%2520Summarization%26entry.906535625%3DDesta%2520Haileselassie%2520Hagos%2520and%2520Legand%2520L.%2520Burge%2520and%2520Anietie%2520Andy%2520and%2520Anis%2520Yazidi%2520and%2520Vladimir%2520Vlassov%26entry.1292438233%3D%2520%2520Transformer-based%2520architectures%2520have%2520advanced%2520text%2520summarization%252C%2520yet%2520their%250Aquadratic%2520complexity%2520limits%2520scalability%2520on%2520long%2520documents.%2520This%2520paper%250Aintroduces%2520BiSparse-AAS%2520%2528Bilinear%2520Sparse%2520Attention%2520with%2520Adaptive%2520Spans%2529%252C%2520a%250Anovel%2520framework%2520that%2520combines%2520sparse%2520attention%252C%2520adaptive%2520spans%252C%2520and%2520bilinear%250Aattention%2520to%2520address%2520these%2520limitations.%2520Sparse%2520attention%2520reduces%2520computational%250Acosts%2520by%2520focusing%2520on%2520the%2520most%2520relevant%2520parts%2520of%2520the%2520input%252C%2520while%2520adaptive%2520spans%250Adynamically%2520adjust%2520the%2520attention%2520ranges.%2520Bilinear%2520attention%2520complements%2520both%2520by%250Amodeling%2520complex%2520token%2520interactions%2520within%2520this%2520refined%2520context.%2520BiSparse-AAS%250Aconsistently%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520extractive%2520and%250Aabstractive%2520summarization%2520tasks%252C%2520achieving%2520average%2520ROUGE%2520improvements%2520of%2520about%250A68.1%2525%2520on%2520CNN/DailyMail%2520and%252052.6%2525%2520on%2520XSum%252C%2520while%2520maintaining%2520strong%2520performance%250Aon%2520OpenWebText%2520and%2520Gigaword%2520datasets.%2520By%2520addressing%2520efficiency%252C%2520scalability%252C%250Aand%2520long-sequence%2520modeling%252C%2520BiSparse-AAS%2520provides%2520a%2520unified%252C%2520practical%2520solution%250Afor%2520real-world%2520text%2520summarization%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiSparse-AAS%3A%20Bilinear%20Sparse%20Attention%20and%20Adaptive%20Spans%20Framework%20for%0A%20%20Scalable%20and%20Efficient%20Text%20Summarization&entry.906535625=Desta%20Haileselassie%20Hagos%20and%20Legand%20L.%20Burge%20and%20Anietie%20Andy%20and%20Anis%20Yazidi%20and%20Vladimir%20Vlassov&entry.1292438233=%20%20Transformer-based%20architectures%20have%20advanced%20text%20summarization%2C%20yet%20their%0Aquadratic%20complexity%20limits%20scalability%20on%20long%20documents.%20This%20paper%0Aintroduces%20BiSparse-AAS%20%28Bilinear%20Sparse%20Attention%20with%20Adaptive%20Spans%29%2C%20a%0Anovel%20framework%20that%20combines%20sparse%20attention%2C%20adaptive%20spans%2C%20and%20bilinear%0Aattention%20to%20address%20these%20limitations.%20Sparse%20attention%20reduces%20computational%0Acosts%20by%20focusing%20on%20the%20most%20relevant%20parts%20of%20the%20input%2C%20while%20adaptive%20spans%0Adynamically%20adjust%20the%20attention%20ranges.%20Bilinear%20attention%20complements%20both%20by%0Amodeling%20complex%20token%20interactions%20within%20this%20refined%20context.%20BiSparse-AAS%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20in%20both%20extractive%20and%0Aabstractive%20summarization%20tasks%2C%20achieving%20average%20ROUGE%20improvements%20of%20about%0A68.1%25%20on%20CNN/DailyMail%20and%2052.6%25%20on%20XSum%2C%20while%20maintaining%20strong%20performance%0Aon%20OpenWebText%20and%20Gigaword%20datasets.%20By%20addressing%20efficiency%2C%20scalability%2C%0Aand%20long-sequence%20modeling%2C%20BiSparse-AAS%20provides%20a%20unified%2C%20practical%20solution%0Afor%20real-world%20text%20summarization%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27516v1&entry.124074799=Read"},
{"title": "Continuous Autoregressive Language Models", "author": "Chenze Shao and Darren Li and Fandong Meng and Jie Zhou", "abstract": "  The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.\n", "link": "http://arxiv.org/abs/2510.27688v1", "date": "2025-10-31", "relevancy": 2.021, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5042}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Autoregressive%20Language%20Models&body=Title%3A%20Continuous%20Autoregressive%20Language%20Models%0AAuthor%3A%20Chenze%20Shao%20and%20Darren%20Li%20and%20Fandong%20Meng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20The%20efficiency%20of%20large%20language%20models%20%28LLMs%29%20is%20fundamentally%20limited%20by%0Atheir%20sequential%2C%20token-by-token%20generation%20process.%20We%20argue%20that%20overcoming%0Athis%20bottleneck%20requires%20a%20new%20design%20axis%20for%20LLM%20scaling%3A%20increasing%20the%0Asemantic%20bandwidth%20of%20each%20generative%20step.%20To%20this%20end%2C%20we%20introduce%0AContinuous%20Autoregressive%20Language%20Models%20%28CALM%29%2C%20a%20paradigm%20shift%20from%0Adiscrete%20next-token%20prediction%20to%20continuous%20next-vector%20prediction.%20CALM%20uses%0Aa%20high-fidelity%20autoencoder%20to%20compress%20a%20chunk%20of%20K%20tokens%20into%20a%20single%0Acontinuous%20vector%2C%20from%20which%20the%20original%20tokens%20can%20be%20reconstructed%20with%0Aover%2099.9%5C%25%20accuracy.%20This%20allows%20us%20to%20model%20language%20as%20a%20sequence%20of%0Acontinuous%20vectors%20instead%20of%20discrete%20tokens%2C%20which%20reduces%20the%20number%20of%0Agenerative%20steps%20by%20a%20factor%20of%20K.%20The%20paradigm%20shift%20necessitates%20a%20new%0Amodeling%20toolkit%3B%20therefore%2C%20we%20develop%20a%20comprehensive%20likelihood-free%0Aframework%20that%20enables%20robust%20training%2C%20evaluation%2C%20and%20controllable%20sampling%0Ain%20the%20continuous%20domain.%20Experiments%20show%20that%20CALM%20significantly%20improves%20the%0Aperformance-compute%20trade-off%2C%20achieving%20the%20performance%20of%20strong%20discrete%0Abaselines%20at%20a%20significantly%20lower%20computational%20cost.%20More%20importantly%2C%20these%0Afindings%20establish%20next-vector%20prediction%20as%20a%20powerful%20and%20scalable%20pathway%0Atowards%20ultra-efficient%20language%20models.%20Code%3A%0Ahttps%3A//github.com/shaochenze/calm.%20Project%3A%0Ahttps%3A//shaochenze.github.io/blog/2025/CALM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Autoregressive%2520Language%2520Models%26entry.906535625%3DChenze%2520Shao%2520and%2520Darren%2520Li%2520and%2520Fandong%2520Meng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520The%2520efficiency%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520fundamentally%2520limited%2520by%250Atheir%2520sequential%252C%2520token-by-token%2520generation%2520process.%2520We%2520argue%2520that%2520overcoming%250Athis%2520bottleneck%2520requires%2520a%2520new%2520design%2520axis%2520for%2520LLM%2520scaling%253A%2520increasing%2520the%250Asemantic%2520bandwidth%2520of%2520each%2520generative%2520step.%2520To%2520this%2520end%252C%2520we%2520introduce%250AContinuous%2520Autoregressive%2520Language%2520Models%2520%2528CALM%2529%252C%2520a%2520paradigm%2520shift%2520from%250Adiscrete%2520next-token%2520prediction%2520to%2520continuous%2520next-vector%2520prediction.%2520CALM%2520uses%250Aa%2520high-fidelity%2520autoencoder%2520to%2520compress%2520a%2520chunk%2520of%2520K%2520tokens%2520into%2520a%2520single%250Acontinuous%2520vector%252C%2520from%2520which%2520the%2520original%2520tokens%2520can%2520be%2520reconstructed%2520with%250Aover%252099.9%255C%2525%2520accuracy.%2520This%2520allows%2520us%2520to%2520model%2520language%2520as%2520a%2520sequence%2520of%250Acontinuous%2520vectors%2520instead%2520of%2520discrete%2520tokens%252C%2520which%2520reduces%2520the%2520number%2520of%250Agenerative%2520steps%2520by%2520a%2520factor%2520of%2520K.%2520The%2520paradigm%2520shift%2520necessitates%2520a%2520new%250Amodeling%2520toolkit%253B%2520therefore%252C%2520we%2520develop%2520a%2520comprehensive%2520likelihood-free%250Aframework%2520that%2520enables%2520robust%2520training%252C%2520evaluation%252C%2520and%2520controllable%2520sampling%250Ain%2520the%2520continuous%2520domain.%2520Experiments%2520show%2520that%2520CALM%2520significantly%2520improves%2520the%250Aperformance-compute%2520trade-off%252C%2520achieving%2520the%2520performance%2520of%2520strong%2520discrete%250Abaselines%2520at%2520a%2520significantly%2520lower%2520computational%2520cost.%2520More%2520importantly%252C%2520these%250Afindings%2520establish%2520next-vector%2520prediction%2520as%2520a%2520powerful%2520and%2520scalable%2520pathway%250Atowards%2520ultra-efficient%2520language%2520models.%2520Code%253A%250Ahttps%253A//github.com/shaochenze/calm.%2520Project%253A%250Ahttps%253A//shaochenze.github.io/blog/2025/CALM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Autoregressive%20Language%20Models&entry.906535625=Chenze%20Shao%20and%20Darren%20Li%20and%20Fandong%20Meng%20and%20Jie%20Zhou&entry.1292438233=%20%20The%20efficiency%20of%20large%20language%20models%20%28LLMs%29%20is%20fundamentally%20limited%20by%0Atheir%20sequential%2C%20token-by-token%20generation%20process.%20We%20argue%20that%20overcoming%0Athis%20bottleneck%20requires%20a%20new%20design%20axis%20for%20LLM%20scaling%3A%20increasing%20the%0Asemantic%20bandwidth%20of%20each%20generative%20step.%20To%20this%20end%2C%20we%20introduce%0AContinuous%20Autoregressive%20Language%20Models%20%28CALM%29%2C%20a%20paradigm%20shift%20from%0Adiscrete%20next-token%20prediction%20to%20continuous%20next-vector%20prediction.%20CALM%20uses%0Aa%20high-fidelity%20autoencoder%20to%20compress%20a%20chunk%20of%20K%20tokens%20into%20a%20single%0Acontinuous%20vector%2C%20from%20which%20the%20original%20tokens%20can%20be%20reconstructed%20with%0Aover%2099.9%5C%25%20accuracy.%20This%20allows%20us%20to%20model%20language%20as%20a%20sequence%20of%0Acontinuous%20vectors%20instead%20of%20discrete%20tokens%2C%20which%20reduces%20the%20number%20of%0Agenerative%20steps%20by%20a%20factor%20of%20K.%20The%20paradigm%20shift%20necessitates%20a%20new%0Amodeling%20toolkit%3B%20therefore%2C%20we%20develop%20a%20comprehensive%20likelihood-free%0Aframework%20that%20enables%20robust%20training%2C%20evaluation%2C%20and%20controllable%20sampling%0Ain%20the%20continuous%20domain.%20Experiments%20show%20that%20CALM%20significantly%20improves%20the%0Aperformance-compute%20trade-off%2C%20achieving%20the%20performance%20of%20strong%20discrete%0Abaselines%20at%20a%20significantly%20lower%20computational%20cost.%20More%20importantly%2C%20these%0Afindings%20establish%20next-vector%20prediction%20as%20a%20powerful%20and%20scalable%20pathway%0Atowards%20ultra-efficient%20language%20models.%20Code%3A%0Ahttps%3A//github.com/shaochenze/calm.%20Project%3A%0Ahttps%3A//shaochenze.github.io/blog/2025/CALM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27688v1&entry.124074799=Read"},
{"title": "Intelligent Software System for Low-Cost, Brightfield Segmentation:\n  Algorithmic Implementation for Cytometric Auto-Analysis", "author": "Surajit Das and Pavel Zun", "abstract": "  Bright-field microscopy, a cost-effective solution for live-cell culture, is\noften the only resource available, along with standard CPUs, for many\nlow-budget labs. The inherent challenges of bright-field images -- their\nnoisiness, low contrast, and dynamic morphology -- coupled with a lack of GPU\nresources and complex software interfaces, hinder the desired research output.\nThis article presents a novel microscopy image analysis framework designed for\nlow-budget labs equipped with a standard CPU desktop. The Python-based program\nenables cytometric analysis of live, unstained cells in culture through an\nadvanced computer vision and machine learning pipeline. Crucially, the\nframework operates on label-free data, requiring no manually annotated training\ndata or training phase. It is accessible via a user-friendly, cross-platform\nGUI that requires no programming skills, while also providing a scripting\ninterface for programmatic control and integration by developers. The\nend-to-end workflow performs semantic and instance segmentation, feature\nextraction, analysis, evaluation, and automated report generation. Its modular\narchitecture supports easy maintenance and flexible integration while\nsupporting both single-image and batch processing. Validated on several\nunstained cell types from the public dataset of livecells, the framework\ndemonstrates superior accuracy and reproducibility compared to contemporary\ntools like Cellpose and StarDist. Its competitive segmentation speed on a\nCPU-based platform highlights its significant potential for basic research and\nclinical applications -- particularly in cell transplantation for personalised\nmedicine and muscle regeneration therapies. The access to the application is\navailable for reproducibility\n", "link": "http://arxiv.org/abs/2509.11354v5", "date": "2025-10-31", "relevancy": 2.0187, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5063}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5037}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Software%20System%20for%20Low-Cost%2C%20Brightfield%20Segmentation%3A%0A%20%20Algorithmic%20Implementation%20for%20Cytometric%20Auto-Analysis&body=Title%3A%20Intelligent%20Software%20System%20for%20Low-Cost%2C%20Brightfield%20Segmentation%3A%0A%20%20Algorithmic%20Implementation%20for%20Cytometric%20Auto-Analysis%0AAuthor%3A%20Surajit%20Das%20and%20Pavel%20Zun%0AAbstract%3A%20%20%20Bright-field%20microscopy%2C%20a%20cost-effective%20solution%20for%20live-cell%20culture%2C%20is%0Aoften%20the%20only%20resource%20available%2C%20along%20with%20standard%20CPUs%2C%20for%20many%0Alow-budget%20labs.%20The%20inherent%20challenges%20of%20bright-field%20images%20--%20their%0Anoisiness%2C%20low%20contrast%2C%20and%20dynamic%20morphology%20--%20coupled%20with%20a%20lack%20of%20GPU%0Aresources%20and%20complex%20software%20interfaces%2C%20hinder%20the%20desired%20research%20output.%0AThis%20article%20presents%20a%20novel%20microscopy%20image%20analysis%20framework%20designed%20for%0Alow-budget%20labs%20equipped%20with%20a%20standard%20CPU%20desktop.%20The%20Python-based%20program%0Aenables%20cytometric%20analysis%20of%20live%2C%20unstained%20cells%20in%20culture%20through%20an%0Aadvanced%20computer%20vision%20and%20machine%20learning%20pipeline.%20Crucially%2C%20the%0Aframework%20operates%20on%20label-free%20data%2C%20requiring%20no%20manually%20annotated%20training%0Adata%20or%20training%20phase.%20It%20is%20accessible%20via%20a%20user-friendly%2C%20cross-platform%0AGUI%20that%20requires%20no%20programming%20skills%2C%20while%20also%20providing%20a%20scripting%0Ainterface%20for%20programmatic%20control%20and%20integration%20by%20developers.%20The%0Aend-to-end%20workflow%20performs%20semantic%20and%20instance%20segmentation%2C%20feature%0Aextraction%2C%20analysis%2C%20evaluation%2C%20and%20automated%20report%20generation.%20Its%20modular%0Aarchitecture%20supports%20easy%20maintenance%20and%20flexible%20integration%20while%0Asupporting%20both%20single-image%20and%20batch%20processing.%20Validated%20on%20several%0Aunstained%20cell%20types%20from%20the%20public%20dataset%20of%20livecells%2C%20the%20framework%0Ademonstrates%20superior%20accuracy%20and%20reproducibility%20compared%20to%20contemporary%0Atools%20like%20Cellpose%20and%20StarDist.%20Its%20competitive%20segmentation%20speed%20on%20a%0ACPU-based%20platform%20highlights%20its%20significant%20potential%20for%20basic%20research%20and%0Aclinical%20applications%20--%20particularly%20in%20cell%20transplantation%20for%20personalised%0Amedicine%20and%20muscle%20regeneration%20therapies.%20The%20access%20to%20the%20application%20is%0Aavailable%20for%20reproducibility%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11354v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Software%2520System%2520for%2520Low-Cost%252C%2520Brightfield%2520Segmentation%253A%250A%2520%2520Algorithmic%2520Implementation%2520for%2520Cytometric%2520Auto-Analysis%26entry.906535625%3DSurajit%2520Das%2520and%2520Pavel%2520Zun%26entry.1292438233%3D%2520%2520Bright-field%2520microscopy%252C%2520a%2520cost-effective%2520solution%2520for%2520live-cell%2520culture%252C%2520is%250Aoften%2520the%2520only%2520resource%2520available%252C%2520along%2520with%2520standard%2520CPUs%252C%2520for%2520many%250Alow-budget%2520labs.%2520The%2520inherent%2520challenges%2520of%2520bright-field%2520images%2520--%2520their%250Anoisiness%252C%2520low%2520contrast%252C%2520and%2520dynamic%2520morphology%2520--%2520coupled%2520with%2520a%2520lack%2520of%2520GPU%250Aresources%2520and%2520complex%2520software%2520interfaces%252C%2520hinder%2520the%2520desired%2520research%2520output.%250AThis%2520article%2520presents%2520a%2520novel%2520microscopy%2520image%2520analysis%2520framework%2520designed%2520for%250Alow-budget%2520labs%2520equipped%2520with%2520a%2520standard%2520CPU%2520desktop.%2520The%2520Python-based%2520program%250Aenables%2520cytometric%2520analysis%2520of%2520live%252C%2520unstained%2520cells%2520in%2520culture%2520through%2520an%250Aadvanced%2520computer%2520vision%2520and%2520machine%2520learning%2520pipeline.%2520Crucially%252C%2520the%250Aframework%2520operates%2520on%2520label-free%2520data%252C%2520requiring%2520no%2520manually%2520annotated%2520training%250Adata%2520or%2520training%2520phase.%2520It%2520is%2520accessible%2520via%2520a%2520user-friendly%252C%2520cross-platform%250AGUI%2520that%2520requires%2520no%2520programming%2520skills%252C%2520while%2520also%2520providing%2520a%2520scripting%250Ainterface%2520for%2520programmatic%2520control%2520and%2520integration%2520by%2520developers.%2520The%250Aend-to-end%2520workflow%2520performs%2520semantic%2520and%2520instance%2520segmentation%252C%2520feature%250Aextraction%252C%2520analysis%252C%2520evaluation%252C%2520and%2520automated%2520report%2520generation.%2520Its%2520modular%250Aarchitecture%2520supports%2520easy%2520maintenance%2520and%2520flexible%2520integration%2520while%250Asupporting%2520both%2520single-image%2520and%2520batch%2520processing.%2520Validated%2520on%2520several%250Aunstained%2520cell%2520types%2520from%2520the%2520public%2520dataset%2520of%2520livecells%252C%2520the%2520framework%250Ademonstrates%2520superior%2520accuracy%2520and%2520reproducibility%2520compared%2520to%2520contemporary%250Atools%2520like%2520Cellpose%2520and%2520StarDist.%2520Its%2520competitive%2520segmentation%2520speed%2520on%2520a%250ACPU-based%2520platform%2520highlights%2520its%2520significant%2520potential%2520for%2520basic%2520research%2520and%250Aclinical%2520applications%2520--%2520particularly%2520in%2520cell%2520transplantation%2520for%2520personalised%250Amedicine%2520and%2520muscle%2520regeneration%2520therapies.%2520The%2520access%2520to%2520the%2520application%2520is%250Aavailable%2520for%2520reproducibility%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11354v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Software%20System%20for%20Low-Cost%2C%20Brightfield%20Segmentation%3A%0A%20%20Algorithmic%20Implementation%20for%20Cytometric%20Auto-Analysis&entry.906535625=Surajit%20Das%20and%20Pavel%20Zun&entry.1292438233=%20%20Bright-field%20microscopy%2C%20a%20cost-effective%20solution%20for%20live-cell%20culture%2C%20is%0Aoften%20the%20only%20resource%20available%2C%20along%20with%20standard%20CPUs%2C%20for%20many%0Alow-budget%20labs.%20The%20inherent%20challenges%20of%20bright-field%20images%20--%20their%0Anoisiness%2C%20low%20contrast%2C%20and%20dynamic%20morphology%20--%20coupled%20with%20a%20lack%20of%20GPU%0Aresources%20and%20complex%20software%20interfaces%2C%20hinder%20the%20desired%20research%20output.%0AThis%20article%20presents%20a%20novel%20microscopy%20image%20analysis%20framework%20designed%20for%0Alow-budget%20labs%20equipped%20with%20a%20standard%20CPU%20desktop.%20The%20Python-based%20program%0Aenables%20cytometric%20analysis%20of%20live%2C%20unstained%20cells%20in%20culture%20through%20an%0Aadvanced%20computer%20vision%20and%20machine%20learning%20pipeline.%20Crucially%2C%20the%0Aframework%20operates%20on%20label-free%20data%2C%20requiring%20no%20manually%20annotated%20training%0Adata%20or%20training%20phase.%20It%20is%20accessible%20via%20a%20user-friendly%2C%20cross-platform%0AGUI%20that%20requires%20no%20programming%20skills%2C%20while%20also%20providing%20a%20scripting%0Ainterface%20for%20programmatic%20control%20and%20integration%20by%20developers.%20The%0Aend-to-end%20workflow%20performs%20semantic%20and%20instance%20segmentation%2C%20feature%0Aextraction%2C%20analysis%2C%20evaluation%2C%20and%20automated%20report%20generation.%20Its%20modular%0Aarchitecture%20supports%20easy%20maintenance%20and%20flexible%20integration%20while%0Asupporting%20both%20single-image%20and%20batch%20processing.%20Validated%20on%20several%0Aunstained%20cell%20types%20from%20the%20public%20dataset%20of%20livecells%2C%20the%20framework%0Ademonstrates%20superior%20accuracy%20and%20reproducibility%20compared%20to%20contemporary%0Atools%20like%20Cellpose%20and%20StarDist.%20Its%20competitive%20segmentation%20speed%20on%20a%0ACPU-based%20platform%20highlights%20its%20significant%20potential%20for%20basic%20research%20and%0Aclinical%20applications%20--%20particularly%20in%20cell%20transplantation%20for%20personalised%0Amedicine%20and%20muscle%20regeneration%20therapies.%20The%20access%20to%20the%20application%20is%0Aavailable%20for%20reproducibility%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11354v5&entry.124074799=Read"},
{"title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation\n  Models", "author": "Boyi Wei and Zora Che and Nathaniel Li and Udari Madhushani Sehwag and Jasper G\u00f6tting and Samira Nedungadi and Julian Michael and Summer Yue and Dan Hendrycks and Peter Henderson and Zifan Wang and Seth Donoughe and Mantas Mazeika", "abstract": "  Open-weight bio-foundation models present a dual-use dilemma. While holding\ngreat promise for accelerating scientific research and drug development, they\ncould also enable bad actors to develop more deadly bioweapons. To mitigate the\nrisk posed by these models, current approaches focus on filtering biohazardous\ndata during pre-training. However, the effectiveness of such an approach\nremains unclear, particularly against determined actors who might fine-tune\nthese models for malicious use. To address this gap, we propose \\eval, a\nframework to evaluate the robustness of procedures that are intended to reduce\nthe dual-use capabilities of bio-foundation models. \\eval assesses models'\nvirus understanding through three lenses, including sequence modeling,\nmutational effects prediction, and virulence prediction. Our results show that\ncurrent filtering practices may not be particularly effective: Excluded\nknowledge can be rapidly recovered in some cases via fine-tuning, and exhibits\nbroader generalizability in sequence modeling. Furthermore, dual-use signals\nmay already reside in the pretrained representations, and can be elicited via\nsimple linear probing. These findings highlight the challenges of data\nfiltering as a standalone procedure, underscoring the need for further research\ninto robust safety and security strategies for open-weight bio-foundation\nmodels.\n", "link": "http://arxiv.org/abs/2510.27629v1", "date": "2025-10-31", "relevancy": 2.0136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best%20Practices%20for%20Biorisk%20Evaluations%20on%20Open-Weight%20Bio-Foundation%0A%20%20Models&body=Title%3A%20Best%20Practices%20for%20Biorisk%20Evaluations%20on%20Open-Weight%20Bio-Foundation%0A%20%20Models%0AAuthor%3A%20Boyi%20Wei%20and%20Zora%20Che%20and%20Nathaniel%20Li%20and%20Udari%20Madhushani%20Sehwag%20and%20Jasper%20G%C3%B6tting%20and%20Samira%20Nedungadi%20and%20Julian%20Michael%20and%20Summer%20Yue%20and%20Dan%20Hendrycks%20and%20Peter%20Henderson%20and%20Zifan%20Wang%20and%20Seth%20Donoughe%20and%20Mantas%20Mazeika%0AAbstract%3A%20%20%20Open-weight%20bio-foundation%20models%20present%20a%20dual-use%20dilemma.%20While%20holding%0Agreat%20promise%20for%20accelerating%20scientific%20research%20and%20drug%20development%2C%20they%0Acould%20also%20enable%20bad%20actors%20to%20develop%20more%20deadly%20bioweapons.%20To%20mitigate%20the%0Arisk%20posed%20by%20these%20models%2C%20current%20approaches%20focus%20on%20filtering%20biohazardous%0Adata%20during%20pre-training.%20However%2C%20the%20effectiveness%20of%20such%20an%20approach%0Aremains%20unclear%2C%20particularly%20against%20determined%20actors%20who%20might%20fine-tune%0Athese%20models%20for%20malicious%20use.%20To%20address%20this%20gap%2C%20we%20propose%20%5Ceval%2C%20a%0Aframework%20to%20evaluate%20the%20robustness%20of%20procedures%20that%20are%20intended%20to%20reduce%0Athe%20dual-use%20capabilities%20of%20bio-foundation%20models.%20%5Ceval%20assesses%20models%27%0Avirus%20understanding%20through%20three%20lenses%2C%20including%20sequence%20modeling%2C%0Amutational%20effects%20prediction%2C%20and%20virulence%20prediction.%20Our%20results%20show%20that%0Acurrent%20filtering%20practices%20may%20not%20be%20particularly%20effective%3A%20Excluded%0Aknowledge%20can%20be%20rapidly%20recovered%20in%20some%20cases%20via%20fine-tuning%2C%20and%20exhibits%0Abroader%20generalizability%20in%20sequence%20modeling.%20Furthermore%2C%20dual-use%20signals%0Amay%20already%20reside%20in%20the%20pretrained%20representations%2C%20and%20can%20be%20elicited%20via%0Asimple%20linear%20probing.%20These%20findings%20highlight%20the%20challenges%20of%20data%0Afiltering%20as%20a%20standalone%20procedure%2C%20underscoring%20the%20need%20for%20further%20research%0Ainto%20robust%20safety%20and%20security%20strategies%20for%20open-weight%20bio-foundation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest%2520Practices%2520for%2520Biorisk%2520Evaluations%2520on%2520Open-Weight%2520Bio-Foundation%250A%2520%2520Models%26entry.906535625%3DBoyi%2520Wei%2520and%2520Zora%2520Che%2520and%2520Nathaniel%2520Li%2520and%2520Udari%2520Madhushani%2520Sehwag%2520and%2520Jasper%2520G%25C3%25B6tting%2520and%2520Samira%2520Nedungadi%2520and%2520Julian%2520Michael%2520and%2520Summer%2520Yue%2520and%2520Dan%2520Hendrycks%2520and%2520Peter%2520Henderson%2520and%2520Zifan%2520Wang%2520and%2520Seth%2520Donoughe%2520and%2520Mantas%2520Mazeika%26entry.1292438233%3D%2520%2520Open-weight%2520bio-foundation%2520models%2520present%2520a%2520dual-use%2520dilemma.%2520While%2520holding%250Agreat%2520promise%2520for%2520accelerating%2520scientific%2520research%2520and%2520drug%2520development%252C%2520they%250Acould%2520also%2520enable%2520bad%2520actors%2520to%2520develop%2520more%2520deadly%2520bioweapons.%2520To%2520mitigate%2520the%250Arisk%2520posed%2520by%2520these%2520models%252C%2520current%2520approaches%2520focus%2520on%2520filtering%2520biohazardous%250Adata%2520during%2520pre-training.%2520However%252C%2520the%2520effectiveness%2520of%2520such%2520an%2520approach%250Aremains%2520unclear%252C%2520particularly%2520against%2520determined%2520actors%2520who%2520might%2520fine-tune%250Athese%2520models%2520for%2520malicious%2520use.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520%255Ceval%252C%2520a%250Aframework%2520to%2520evaluate%2520the%2520robustness%2520of%2520procedures%2520that%2520are%2520intended%2520to%2520reduce%250Athe%2520dual-use%2520capabilities%2520of%2520bio-foundation%2520models.%2520%255Ceval%2520assesses%2520models%2527%250Avirus%2520understanding%2520through%2520three%2520lenses%252C%2520including%2520sequence%2520modeling%252C%250Amutational%2520effects%2520prediction%252C%2520and%2520virulence%2520prediction.%2520Our%2520results%2520show%2520that%250Acurrent%2520filtering%2520practices%2520may%2520not%2520be%2520particularly%2520effective%253A%2520Excluded%250Aknowledge%2520can%2520be%2520rapidly%2520recovered%2520in%2520some%2520cases%2520via%2520fine-tuning%252C%2520and%2520exhibits%250Abroader%2520generalizability%2520in%2520sequence%2520modeling.%2520Furthermore%252C%2520dual-use%2520signals%250Amay%2520already%2520reside%2520in%2520the%2520pretrained%2520representations%252C%2520and%2520can%2520be%2520elicited%2520via%250Asimple%2520linear%2520probing.%2520These%2520findings%2520highlight%2520the%2520challenges%2520of%2520data%250Afiltering%2520as%2520a%2520standalone%2520procedure%252C%2520underscoring%2520the%2520need%2520for%2520further%2520research%250Ainto%2520robust%2520safety%2520and%2520security%2520strategies%2520for%2520open-weight%2520bio-foundation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best%20Practices%20for%20Biorisk%20Evaluations%20on%20Open-Weight%20Bio-Foundation%0A%20%20Models&entry.906535625=Boyi%20Wei%20and%20Zora%20Che%20and%20Nathaniel%20Li%20and%20Udari%20Madhushani%20Sehwag%20and%20Jasper%20G%C3%B6tting%20and%20Samira%20Nedungadi%20and%20Julian%20Michael%20and%20Summer%20Yue%20and%20Dan%20Hendrycks%20and%20Peter%20Henderson%20and%20Zifan%20Wang%20and%20Seth%20Donoughe%20and%20Mantas%20Mazeika&entry.1292438233=%20%20Open-weight%20bio-foundation%20models%20present%20a%20dual-use%20dilemma.%20While%20holding%0Agreat%20promise%20for%20accelerating%20scientific%20research%20and%20drug%20development%2C%20they%0Acould%20also%20enable%20bad%20actors%20to%20develop%20more%20deadly%20bioweapons.%20To%20mitigate%20the%0Arisk%20posed%20by%20these%20models%2C%20current%20approaches%20focus%20on%20filtering%20biohazardous%0Adata%20during%20pre-training.%20However%2C%20the%20effectiveness%20of%20such%20an%20approach%0Aremains%20unclear%2C%20particularly%20against%20determined%20actors%20who%20might%20fine-tune%0Athese%20models%20for%20malicious%20use.%20To%20address%20this%20gap%2C%20we%20propose%20%5Ceval%2C%20a%0Aframework%20to%20evaluate%20the%20robustness%20of%20procedures%20that%20are%20intended%20to%20reduce%0Athe%20dual-use%20capabilities%20of%20bio-foundation%20models.%20%5Ceval%20assesses%20models%27%0Avirus%20understanding%20through%20three%20lenses%2C%20including%20sequence%20modeling%2C%0Amutational%20effects%20prediction%2C%20and%20virulence%20prediction.%20Our%20results%20show%20that%0Acurrent%20filtering%20practices%20may%20not%20be%20particularly%20effective%3A%20Excluded%0Aknowledge%20can%20be%20rapidly%20recovered%20in%20some%20cases%20via%20fine-tuning%2C%20and%20exhibits%0Abroader%20generalizability%20in%20sequence%20modeling.%20Furthermore%2C%20dual-use%20signals%0Amay%20already%20reside%20in%20the%20pretrained%20representations%2C%20and%20can%20be%20elicited%20via%0Asimple%20linear%20probing.%20These%20findings%20highlight%20the%20challenges%20of%20data%0Afiltering%20as%20a%20standalone%20procedure%2C%20underscoring%20the%20need%20for%20further%20research%0Ainto%20robust%20safety%20and%20security%20strategies%20for%20open-weight%20bio-foundation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27629v1&entry.124074799=Read"},
{"title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research", "author": "Yunze Wu and Dayuan Fu and Weiye Si and Zhen Huang and Mohan Jiang and Keyu Li and Shijie Xia and Jie Sun and Tianze Xu and Xiangkun Hu and Pengrui Lu and Xiaojie Cai and Lyumanshan Ye and Wenhong Zhu and Yang Xiao and Pengfei Liu", "abstract": "  AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.\n", "link": "http://arxiv.org/abs/2510.27598v1", "date": "2025-10-31", "relevancy": 2.0123, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5107}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5097}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InnovatorBench%3A%20Evaluating%20Agents%27%20Ability%20to%20Conduct%20Innovative%20LLM%0A%20%20Research&body=Title%3A%20InnovatorBench%3A%20Evaluating%20Agents%27%20Ability%20to%20Conduct%20Innovative%20LLM%0A%20%20Research%0AAuthor%3A%20Yunze%20Wu%20and%20Dayuan%20Fu%20and%20Weiye%20Si%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Keyu%20Li%20and%20Shijie%20Xia%20and%20Jie%20Sun%20and%20Tianze%20Xu%20and%20Xiangkun%20Hu%20and%20Pengrui%20Lu%20and%20Xiaojie%20Cai%20and%20Lyumanshan%20Ye%20and%20Wenhong%20Zhu%20and%20Yang%20Xiao%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20AI%20agents%20could%20accelerate%20scientific%20discovery%20by%20automating%20hypothesis%0Aformation%2C%20experiment%20design%2C%20coding%2C%20execution%2C%20and%20analysis%2C%20yet%20existing%0Abenchmarks%20probe%20narrow%20skills%20in%20simplified%20settings.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20InnovatorBench%2C%20a%20benchmark-platform%20pair%20for%20realistic%2C%20end-to-end%0Aassessment%20of%20agents%20performing%20Large%20Language%20Model%20%28LLM%29%20research.%20It%0Acomprises%2020%20tasks%20spanning%20Data%20Construction%2C%20Filtering%2C%20Augmentation%2C%20Loss%0ADesign%2C%20Reward%20Design%2C%20and%20Scaffold%20Construction%2C%20which%20require%20runnable%0Aartifacts%20and%20assessment%20of%20correctness%2C%20performance%2C%20output%20quality%2C%20and%0Auncertainty.%20To%20support%20agent%20operation%2C%20we%20develop%20ResearchGym%2C%20a%20research%0Aenvironment%20offering%20rich%20action%20spaces%2C%20distributed%20and%20long-horizon%0Aexecution%2C%20asynchronous%20monitoring%2C%20and%20snapshot%20saving.%20We%20also%20implement%20a%0Alightweight%20ReAct%20agent%20that%20couples%20explicit%20reasoning%20with%20executable%0Aplanning%20using%20frontier%20models%20such%20as%20Claude-4%2C%20GPT-5%2C%20GLM-4.5%2C%20and%20Kimi-K2.%0AOur%20experiments%20demonstrate%20that%20while%20frontier%20models%20show%20promise%20in%0Acode-driven%20research%20tasks%2C%20they%20struggle%20with%20fragile%20algorithm-related%20tasks%0Aand%20long-horizon%20decision%20making%2C%20such%20as%20impatience%2C%20poor%20resource%20management%2C%0Aand%20overreliance%20on%20template-based%20reasoning.%20Furthermore%2C%20agents%20require%20over%0A11%20hours%20to%20achieve%20their%20best%20performance%20on%20InnovatorBench%2C%20underscoring%20the%0Abenchmark%27s%20difficulty%20and%20showing%20the%20potential%20of%20InnovatorBench%20to%20be%20the%0Anext%20generation%20of%20code-based%20research%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInnovatorBench%253A%2520Evaluating%2520Agents%2527%2520Ability%2520to%2520Conduct%2520Innovative%2520LLM%250A%2520%2520Research%26entry.906535625%3DYunze%2520Wu%2520and%2520Dayuan%2520Fu%2520and%2520Weiye%2520Si%2520and%2520Zhen%2520Huang%2520and%2520Mohan%2520Jiang%2520and%2520Keyu%2520Li%2520and%2520Shijie%2520Xia%2520and%2520Jie%2520Sun%2520and%2520Tianze%2520Xu%2520and%2520Xiangkun%2520Hu%2520and%2520Pengrui%2520Lu%2520and%2520Xiaojie%2520Cai%2520and%2520Lyumanshan%2520Ye%2520and%2520Wenhong%2520Zhu%2520and%2520Yang%2520Xiao%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520AI%2520agents%2520could%2520accelerate%2520scientific%2520discovery%2520by%2520automating%2520hypothesis%250Aformation%252C%2520experiment%2520design%252C%2520coding%252C%2520execution%252C%2520and%2520analysis%252C%2520yet%2520existing%250Abenchmarks%2520probe%2520narrow%2520skills%2520in%2520simplified%2520settings.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520InnovatorBench%252C%2520a%2520benchmark-platform%2520pair%2520for%2520realistic%252C%2520end-to-end%250Aassessment%2520of%2520agents%2520performing%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520research.%2520It%250Acomprises%252020%2520tasks%2520spanning%2520Data%2520Construction%252C%2520Filtering%252C%2520Augmentation%252C%2520Loss%250ADesign%252C%2520Reward%2520Design%252C%2520and%2520Scaffold%2520Construction%252C%2520which%2520require%2520runnable%250Aartifacts%2520and%2520assessment%2520of%2520correctness%252C%2520performance%252C%2520output%2520quality%252C%2520and%250Auncertainty.%2520To%2520support%2520agent%2520operation%252C%2520we%2520develop%2520ResearchGym%252C%2520a%2520research%250Aenvironment%2520offering%2520rich%2520action%2520spaces%252C%2520distributed%2520and%2520long-horizon%250Aexecution%252C%2520asynchronous%2520monitoring%252C%2520and%2520snapshot%2520saving.%2520We%2520also%2520implement%2520a%250Alightweight%2520ReAct%2520agent%2520that%2520couples%2520explicit%2520reasoning%2520with%2520executable%250Aplanning%2520using%2520frontier%2520models%2520such%2520as%2520Claude-4%252C%2520GPT-5%252C%2520GLM-4.5%252C%2520and%2520Kimi-K2.%250AOur%2520experiments%2520demonstrate%2520that%2520while%2520frontier%2520models%2520show%2520promise%2520in%250Acode-driven%2520research%2520tasks%252C%2520they%2520struggle%2520with%2520fragile%2520algorithm-related%2520tasks%250Aand%2520long-horizon%2520decision%2520making%252C%2520such%2520as%2520impatience%252C%2520poor%2520resource%2520management%252C%250Aand%2520overreliance%2520on%2520template-based%2520reasoning.%2520Furthermore%252C%2520agents%2520require%2520over%250A11%2520hours%2520to%2520achieve%2520their%2520best%2520performance%2520on%2520InnovatorBench%252C%2520underscoring%2520the%250Abenchmark%2527s%2520difficulty%2520and%2520showing%2520the%2520potential%2520of%2520InnovatorBench%2520to%2520be%2520the%250Anext%2520generation%2520of%2520code-based%2520research%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InnovatorBench%3A%20Evaluating%20Agents%27%20Ability%20to%20Conduct%20Innovative%20LLM%0A%20%20Research&entry.906535625=Yunze%20Wu%20and%20Dayuan%20Fu%20and%20Weiye%20Si%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Keyu%20Li%20and%20Shijie%20Xia%20and%20Jie%20Sun%20and%20Tianze%20Xu%20and%20Xiangkun%20Hu%20and%20Pengrui%20Lu%20and%20Xiaojie%20Cai%20and%20Lyumanshan%20Ye%20and%20Wenhong%20Zhu%20and%20Yang%20Xiao%20and%20Pengfei%20Liu&entry.1292438233=%20%20AI%20agents%20could%20accelerate%20scientific%20discovery%20by%20automating%20hypothesis%0Aformation%2C%20experiment%20design%2C%20coding%2C%20execution%2C%20and%20analysis%2C%20yet%20existing%0Abenchmarks%20probe%20narrow%20skills%20in%20simplified%20settings.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20InnovatorBench%2C%20a%20benchmark-platform%20pair%20for%20realistic%2C%20end-to-end%0Aassessment%20of%20agents%20performing%20Large%20Language%20Model%20%28LLM%29%20research.%20It%0Acomprises%2020%20tasks%20spanning%20Data%20Construction%2C%20Filtering%2C%20Augmentation%2C%20Loss%0ADesign%2C%20Reward%20Design%2C%20and%20Scaffold%20Construction%2C%20which%20require%20runnable%0Aartifacts%20and%20assessment%20of%20correctness%2C%20performance%2C%20output%20quality%2C%20and%0Auncertainty.%20To%20support%20agent%20operation%2C%20we%20develop%20ResearchGym%2C%20a%20research%0Aenvironment%20offering%20rich%20action%20spaces%2C%20distributed%20and%20long-horizon%0Aexecution%2C%20asynchronous%20monitoring%2C%20and%20snapshot%20saving.%20We%20also%20implement%20a%0Alightweight%20ReAct%20agent%20that%20couples%20explicit%20reasoning%20with%20executable%0Aplanning%20using%20frontier%20models%20such%20as%20Claude-4%2C%20GPT-5%2C%20GLM-4.5%2C%20and%20Kimi-K2.%0AOur%20experiments%20demonstrate%20that%20while%20frontier%20models%20show%20promise%20in%0Acode-driven%20research%20tasks%2C%20they%20struggle%20with%20fragile%20algorithm-related%20tasks%0Aand%20long-horizon%20decision%20making%2C%20such%20as%20impatience%2C%20poor%20resource%20management%2C%0Aand%20overreliance%20on%20template-based%20reasoning.%20Furthermore%2C%20agents%20require%20over%0A11%20hours%20to%20achieve%20their%20best%20performance%20on%20InnovatorBench%2C%20underscoring%20the%0Abenchmark%27s%20difficulty%20and%20showing%20the%20potential%20of%20InnovatorBench%20to%20be%20the%0Anext%20generation%20of%20code-based%20research%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27598v1&entry.124074799=Read"},
{"title": "Learned Static Function Data Structures", "author": "Stefan Hermann and Hans-Peter Lehmann and Giorgio Vinciguerra and Stefan Walzer", "abstract": "  We consider the task of constructing a data structure for associating a\nstatic set of keys with values, while allowing arbitrary output values for\nqueries involving keys outside the set. Compared to hash tables, these\nso-called static function data structures do not need to store the key set and\nthus use significantly less memory. Several techniques are known, with\ncompressed static functions approaching the zero-order empirical entropy of the\nvalue sequence. In this paper, we introduce learned static functions, which use\nmachine learning to capture correlations between keys and values. For each key,\na model predicts a probability distribution over the values, from which we\nderive a key-specific prefix code to compactly encode the true value. The\nresulting codeword is stored in a classic static function data structure. This\ndesign allows learned static functions to break the zero-order entropy barrier\nwhile still supporting point queries. Our experiments show substantial space\nsavings: up to one order of magnitude on real data, and up to three orders of\nmagnitude on synthetic data.\n", "link": "http://arxiv.org/abs/2510.27588v1", "date": "2025-10-31", "relevancy": 1.9979, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4202}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4133}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Static%20Function%20Data%20Structures&body=Title%3A%20Learned%20Static%20Function%20Data%20Structures%0AAuthor%3A%20Stefan%20Hermann%20and%20Hans-Peter%20Lehmann%20and%20Giorgio%20Vinciguerra%20and%20Stefan%20Walzer%0AAbstract%3A%20%20%20We%20consider%20the%20task%20of%20constructing%20a%20data%20structure%20for%20associating%20a%0Astatic%20set%20of%20keys%20with%20values%2C%20while%20allowing%20arbitrary%20output%20values%20for%0Aqueries%20involving%20keys%20outside%20the%20set.%20Compared%20to%20hash%20tables%2C%20these%0Aso-called%20static%20function%20data%20structures%20do%20not%20need%20to%20store%20the%20key%20set%20and%0Athus%20use%20significantly%20less%20memory.%20Several%20techniques%20are%20known%2C%20with%0Acompressed%20static%20functions%20approaching%20the%20zero-order%20empirical%20entropy%20of%20the%0Avalue%20sequence.%20In%20this%20paper%2C%20we%20introduce%20learned%20static%20functions%2C%20which%20use%0Amachine%20learning%20to%20capture%20correlations%20between%20keys%20and%20values.%20For%20each%20key%2C%0Aa%20model%20predicts%20a%20probability%20distribution%20over%20the%20values%2C%20from%20which%20we%0Aderive%20a%20key-specific%20prefix%20code%20to%20compactly%20encode%20the%20true%20value.%20The%0Aresulting%20codeword%20is%20stored%20in%20a%20classic%20static%20function%20data%20structure.%20This%0Adesign%20allows%20learned%20static%20functions%20to%20break%20the%20zero-order%20entropy%20barrier%0Awhile%20still%20supporting%20point%20queries.%20Our%20experiments%20show%20substantial%20space%0Asavings%3A%20up%20to%20one%20order%20of%20magnitude%20on%20real%20data%2C%20and%20up%20to%20three%20orders%20of%0Amagnitude%20on%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Static%2520Function%2520Data%2520Structures%26entry.906535625%3DStefan%2520Hermann%2520and%2520Hans-Peter%2520Lehmann%2520and%2520Giorgio%2520Vinciguerra%2520and%2520Stefan%2520Walzer%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520task%2520of%2520constructing%2520a%2520data%2520structure%2520for%2520associating%2520a%250Astatic%2520set%2520of%2520keys%2520with%2520values%252C%2520while%2520allowing%2520arbitrary%2520output%2520values%2520for%250Aqueries%2520involving%2520keys%2520outside%2520the%2520set.%2520Compared%2520to%2520hash%2520tables%252C%2520these%250Aso-called%2520static%2520function%2520data%2520structures%2520do%2520not%2520need%2520to%2520store%2520the%2520key%2520set%2520and%250Athus%2520use%2520significantly%2520less%2520memory.%2520Several%2520techniques%2520are%2520known%252C%2520with%250Acompressed%2520static%2520functions%2520approaching%2520the%2520zero-order%2520empirical%2520entropy%2520of%2520the%250Avalue%2520sequence.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520learned%2520static%2520functions%252C%2520which%2520use%250Amachine%2520learning%2520to%2520capture%2520correlations%2520between%2520keys%2520and%2520values.%2520For%2520each%2520key%252C%250Aa%2520model%2520predicts%2520a%2520probability%2520distribution%2520over%2520the%2520values%252C%2520from%2520which%2520we%250Aderive%2520a%2520key-specific%2520prefix%2520code%2520to%2520compactly%2520encode%2520the%2520true%2520value.%2520The%250Aresulting%2520codeword%2520is%2520stored%2520in%2520a%2520classic%2520static%2520function%2520data%2520structure.%2520This%250Adesign%2520allows%2520learned%2520static%2520functions%2520to%2520break%2520the%2520zero-order%2520entropy%2520barrier%250Awhile%2520still%2520supporting%2520point%2520queries.%2520Our%2520experiments%2520show%2520substantial%2520space%250Asavings%253A%2520up%2520to%2520one%2520order%2520of%2520magnitude%2520on%2520real%2520data%252C%2520and%2520up%2520to%2520three%2520orders%2520of%250Amagnitude%2520on%2520synthetic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Static%20Function%20Data%20Structures&entry.906535625=Stefan%20Hermann%20and%20Hans-Peter%20Lehmann%20and%20Giorgio%20Vinciguerra%20and%20Stefan%20Walzer&entry.1292438233=%20%20We%20consider%20the%20task%20of%20constructing%20a%20data%20structure%20for%20associating%20a%0Astatic%20set%20of%20keys%20with%20values%2C%20while%20allowing%20arbitrary%20output%20values%20for%0Aqueries%20involving%20keys%20outside%20the%20set.%20Compared%20to%20hash%20tables%2C%20these%0Aso-called%20static%20function%20data%20structures%20do%20not%20need%20to%20store%20the%20key%20set%20and%0Athus%20use%20significantly%20less%20memory.%20Several%20techniques%20are%20known%2C%20with%0Acompressed%20static%20functions%20approaching%20the%20zero-order%20empirical%20entropy%20of%20the%0Avalue%20sequence.%20In%20this%20paper%2C%20we%20introduce%20learned%20static%20functions%2C%20which%20use%0Amachine%20learning%20to%20capture%20correlations%20between%20keys%20and%20values.%20For%20each%20key%2C%0Aa%20model%20predicts%20a%20probability%20distribution%20over%20the%20values%2C%20from%20which%20we%0Aderive%20a%20key-specific%20prefix%20code%20to%20compactly%20encode%20the%20true%20value.%20The%0Aresulting%20codeword%20is%20stored%20in%20a%20classic%20static%20function%20data%20structure.%20This%0Adesign%20allows%20learned%20static%20functions%20to%20break%20the%20zero-order%20entropy%20barrier%0Awhile%20still%20supporting%20point%20queries.%20Our%20experiments%20show%20substantial%20space%0Asavings%3A%20up%20to%20one%20order%20of%20magnitude%20on%20real%20data%2C%20and%20up%20to%20three%20orders%20of%0Amagnitude%20on%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27588v1&entry.124074799=Read"},
{"title": "The End of Manual Decoding: Towards Truly End-to-End Language Models", "author": "Zhichao Wang and Dongyang Ma and Xinting Huang and Deng Cai and Tian Lan and Jiahao Xu and Haitao Mi and Xiaoying Tang and Yan Wang", "abstract": "  The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.\n", "link": "http://arxiv.org/abs/2510.26697v2", "date": "2025-10-31", "relevancy": 1.9969, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models&body=Title%3A%20The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models%0AAuthor%3A%20Zhichao%20Wang%20and%20Dongyang%20Ma%20and%20Xinting%20Huang%20and%20Deng%20Cai%20and%20Tian%20Lan%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Xiaoying%20Tang%20and%20Yan%20Wang%0AAbstract%3A%20%20%20The%20%22end-to-end%22%20label%20for%20LLMs%20is%20a%20misnomer.%20In%20practice%2C%20they%20depend%20on%20a%0Anon-differentiable%20decoding%20process%20that%20requires%20laborious%2C%20hand-tuning%20of%0Ahyperparameters%20like%20temperature%20and%20top-p.%20This%20paper%20introduces%20AutoDeco%2C%20a%0Anovel%20architecture%20that%20enables%20truly%20%22end-to-end%22%20generation%20by%20learning%20to%0Acontrol%20its%20own%20decoding%20strategy.%20We%20augment%20the%20standard%20transformer%20with%0Alightweight%20heads%20that%2C%20at%20each%20step%2C%20dynamically%20predict%20context-specific%0Atemperature%20and%20top-p%20values%20alongside%20the%20next-token%20logits.%20This%20approach%0Atransforms%20decoding%20into%20a%20parametric%2C%20token-level%20process%2C%20allowing%20the%20model%0Ato%20self-regulate%20its%20sampling%20strategy%20within%20a%20single%20forward%20pass.%0A%20%20Through%20extensive%20experiments%20on%20eight%20benchmarks%2C%20we%20demonstrate%20that%0AAutoDeco%20not%20only%20significantly%20outperforms%20default%20decoding%20strategies%20but%0Aalso%20achieves%20performance%20comparable%20to%20an%20oracle-tuned%20baseline%20derived%20from%0A%22hacking%20the%20test%20set%22-a%20practical%20upper%20bound%20for%20any%20static%20method.%0ACrucially%2C%20we%20uncover%20an%20emergent%20capability%20for%20instruction-based%20decoding%0Acontrol%3A%20the%20model%20learns%20to%20interpret%20natural%20language%20commands%20%28e.g.%2C%0A%22generate%20with%20low%20randomness%22%29%20and%20adjusts%20its%20predicted%20temperature%20and%20top-p%0Aon%20a%20token-by-token%20basis%2C%20opening%20a%20new%20paradigm%20for%20steerable%20and%20interactive%0ALLM%20decoding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520End%2520of%2520Manual%2520Decoding%253A%2520Towards%2520Truly%2520End-to-End%2520Language%2520Models%26entry.906535625%3DZhichao%2520Wang%2520and%2520Dongyang%2520Ma%2520and%2520Xinting%2520Huang%2520and%2520Deng%2520Cai%2520and%2520Tian%2520Lan%2520and%2520Jiahao%2520Xu%2520and%2520Haitao%2520Mi%2520and%2520Xiaoying%2520Tang%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520The%2520%2522end-to-end%2522%2520label%2520for%2520LLMs%2520is%2520a%2520misnomer.%2520In%2520practice%252C%2520they%2520depend%2520on%2520a%250Anon-differentiable%2520decoding%2520process%2520that%2520requires%2520laborious%252C%2520hand-tuning%2520of%250Ahyperparameters%2520like%2520temperature%2520and%2520top-p.%2520This%2520paper%2520introduces%2520AutoDeco%252C%2520a%250Anovel%2520architecture%2520that%2520enables%2520truly%2520%2522end-to-end%2522%2520generation%2520by%2520learning%2520to%250Acontrol%2520its%2520own%2520decoding%2520strategy.%2520We%2520augment%2520the%2520standard%2520transformer%2520with%250Alightweight%2520heads%2520that%252C%2520at%2520each%2520step%252C%2520dynamically%2520predict%2520context-specific%250Atemperature%2520and%2520top-p%2520values%2520alongside%2520the%2520next-token%2520logits.%2520This%2520approach%250Atransforms%2520decoding%2520into%2520a%2520parametric%252C%2520token-level%2520process%252C%2520allowing%2520the%2520model%250Ato%2520self-regulate%2520its%2520sampling%2520strategy%2520within%2520a%2520single%2520forward%2520pass.%250A%2520%2520Through%2520extensive%2520experiments%2520on%2520eight%2520benchmarks%252C%2520we%2520demonstrate%2520that%250AAutoDeco%2520not%2520only%2520significantly%2520outperforms%2520default%2520decoding%2520strategies%2520but%250Aalso%2520achieves%2520performance%2520comparable%2520to%2520an%2520oracle-tuned%2520baseline%2520derived%2520from%250A%2522hacking%2520the%2520test%2520set%2522-a%2520practical%2520upper%2520bound%2520for%2520any%2520static%2520method.%250ACrucially%252C%2520we%2520uncover%2520an%2520emergent%2520capability%2520for%2520instruction-based%2520decoding%250Acontrol%253A%2520the%2520model%2520learns%2520to%2520interpret%2520natural%2520language%2520commands%2520%2528e.g.%252C%250A%2522generate%2520with%2520low%2520randomness%2522%2529%2520and%2520adjusts%2520its%2520predicted%2520temperature%2520and%2520top-p%250Aon%2520a%2520token-by-token%2520basis%252C%2520opening%2520a%2520new%2520paradigm%2520for%2520steerable%2520and%2520interactive%250ALLM%2520decoding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20End%20of%20Manual%20Decoding%3A%20Towards%20Truly%20End-to-End%20Language%20Models&entry.906535625=Zhichao%20Wang%20and%20Dongyang%20Ma%20and%20Xinting%20Huang%20and%20Deng%20Cai%20and%20Tian%20Lan%20and%20Jiahao%20Xu%20and%20Haitao%20Mi%20and%20Xiaoying%20Tang%20and%20Yan%20Wang&entry.1292438233=%20%20The%20%22end-to-end%22%20label%20for%20LLMs%20is%20a%20misnomer.%20In%20practice%2C%20they%20depend%20on%20a%0Anon-differentiable%20decoding%20process%20that%20requires%20laborious%2C%20hand-tuning%20of%0Ahyperparameters%20like%20temperature%20and%20top-p.%20This%20paper%20introduces%20AutoDeco%2C%20a%0Anovel%20architecture%20that%20enables%20truly%20%22end-to-end%22%20generation%20by%20learning%20to%0Acontrol%20its%20own%20decoding%20strategy.%20We%20augment%20the%20standard%20transformer%20with%0Alightweight%20heads%20that%2C%20at%20each%20step%2C%20dynamically%20predict%20context-specific%0Atemperature%20and%20top-p%20values%20alongside%20the%20next-token%20logits.%20This%20approach%0Atransforms%20decoding%20into%20a%20parametric%2C%20token-level%20process%2C%20allowing%20the%20model%0Ato%20self-regulate%20its%20sampling%20strategy%20within%20a%20single%20forward%20pass.%0A%20%20Through%20extensive%20experiments%20on%20eight%20benchmarks%2C%20we%20demonstrate%20that%0AAutoDeco%20not%20only%20significantly%20outperforms%20default%20decoding%20strategies%20but%0Aalso%20achieves%20performance%20comparable%20to%20an%20oracle-tuned%20baseline%20derived%20from%0A%22hacking%20the%20test%20set%22-a%20practical%20upper%20bound%20for%20any%20static%20method.%0ACrucially%2C%20we%20uncover%20an%20emergent%20capability%20for%20instruction-based%20decoding%0Acontrol%3A%20the%20model%20learns%20to%20interpret%20natural%20language%20commands%20%28e.g.%2C%0A%22generate%20with%20low%20randomness%22%29%20and%20adjusts%20its%20predicted%20temperature%20and%20top-p%0Aon%20a%20token-by-token%20basis%2C%20opening%20a%20new%20paradigm%20for%20steerable%20and%20interactive%0ALLM%20decoding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26697v2&entry.124074799=Read"},
{"title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting", "author": "Vladyslav Moroshan and Julien Siems and Arber Zela and Timur Carstensen and Frank Hutter", "abstract": "  Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.\n", "link": "http://arxiv.org/abs/2510.25502v2", "date": "2025-10-31", "relevancy": 1.9963, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5186}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Vladyslav%20Moroshan%20and%20Julien%20Siems%20and%20Arber%20Zela%20and%20Timur%20Carstensen%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Foundation%20models%20for%20zero-shot%20time%20series%20forecasting%20face%20challenges%20in%0Aefficient%20long-horizon%20prediction%20and%20reproducibility%2C%20with%20existing%0Asynthetic-only%20approaches%20underperforming%20on%20challenging%20benchmarks.%20This%20paper%0Apresents%20TempoPFN%2C%20a%20univariate%20time%20series%20foundation%20model%20based%20on%20linear%0ARecurrent%20Neural%20Networks%20%28RNNs%29%20pre-trained%20exclusively%20on%20synthetic%20data.%20The%0Amodel%20uses%20a%20GatedDeltaProduct%20architecture%20with%20state-weaving%20for%20fully%0Aparallelizable%20training%20across%20sequence%20lengths%2C%20eliminating%20the%20need%20for%0Awindowing%20or%20summarization%20techniques%20while%20maintaining%20robust%20temporal%0Astate-tracking.%20Our%20comprehensive%20synthetic%20data%20pipeline%20unifies%20diverse%0Agenerators%2C%20including%20stochastic%20differential%20equations%2C%20Gaussian%20processes%2C%0Aand%20audio%20synthesis%2C%20with%20novel%20augmentations.%20In%20zero-shot%20evaluations%20on%20the%0AGift-Eval%20benchmark%2C%20TempoPFN%20achieves%20top-tier%20competitive%20performance%2C%0Aoutperforming%20all%20existing%20synthetic-only%20approaches%20and%20surpassing%20the%20vast%0Amajority%20of%20models%20trained%20on%20real-world%20data%2C%20while%20being%20more%20efficient%20than%0Aexisting%20baselines%20by%20leveraging%20fully%20parallelizable%20training%20and%20inference.%0AWe%20open-source%20our%20complete%20data%20generation%20pipeline%20and%20training%20code%2C%0Aproviding%20a%20reproducible%20foundation%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoPFN%253A%2520Synthetic%2520Pre-training%2520of%2520Linear%2520RNNs%2520for%2520Zero-shot%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DVladyslav%2520Moroshan%2520and%2520Julien%2520Siems%2520and%2520Arber%2520Zela%2520and%2520Timur%2520Carstensen%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Foundation%2520models%2520for%2520zero-shot%2520time%2520series%2520forecasting%2520face%2520challenges%2520in%250Aefficient%2520long-horizon%2520prediction%2520and%2520reproducibility%252C%2520with%2520existing%250Asynthetic-only%2520approaches%2520underperforming%2520on%2520challenging%2520benchmarks.%2520This%2520paper%250Apresents%2520TempoPFN%252C%2520a%2520univariate%2520time%2520series%2520foundation%2520model%2520based%2520on%2520linear%250ARecurrent%2520Neural%2520Networks%2520%2528RNNs%2529%2520pre-trained%2520exclusively%2520on%2520synthetic%2520data.%2520The%250Amodel%2520uses%2520a%2520GatedDeltaProduct%2520architecture%2520with%2520state-weaving%2520for%2520fully%250Aparallelizable%2520training%2520across%2520sequence%2520lengths%252C%2520eliminating%2520the%2520need%2520for%250Awindowing%2520or%2520summarization%2520techniques%2520while%2520maintaining%2520robust%2520temporal%250Astate-tracking.%2520Our%2520comprehensive%2520synthetic%2520data%2520pipeline%2520unifies%2520diverse%250Agenerators%252C%2520including%2520stochastic%2520differential%2520equations%252C%2520Gaussian%2520processes%252C%250Aand%2520audio%2520synthesis%252C%2520with%2520novel%2520augmentations.%2520In%2520zero-shot%2520evaluations%2520on%2520the%250AGift-Eval%2520benchmark%252C%2520TempoPFN%2520achieves%2520top-tier%2520competitive%2520performance%252C%250Aoutperforming%2520all%2520existing%2520synthetic-only%2520approaches%2520and%2520surpassing%2520the%2520vast%250Amajority%2520of%2520models%2520trained%2520on%2520real-world%2520data%252C%2520while%2520being%2520more%2520efficient%2520than%250Aexisting%2520baselines%2520by%2520leveraging%2520fully%2520parallelizable%2520training%2520and%2520inference.%250AWe%2520open-source%2520our%2520complete%2520data%2520generation%2520pipeline%2520and%2520training%2520code%252C%250Aproviding%2520a%2520reproducible%2520foundation%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoPFN%3A%20Synthetic%20Pre-training%20of%20Linear%20RNNs%20for%20Zero-shot%20Time%0A%20%20Series%20Forecasting&entry.906535625=Vladyslav%20Moroshan%20and%20Julien%20Siems%20and%20Arber%20Zela%20and%20Timur%20Carstensen%20and%20Frank%20Hutter&entry.1292438233=%20%20Foundation%20models%20for%20zero-shot%20time%20series%20forecasting%20face%20challenges%20in%0Aefficient%20long-horizon%20prediction%20and%20reproducibility%2C%20with%20existing%0Asynthetic-only%20approaches%20underperforming%20on%20challenging%20benchmarks.%20This%20paper%0Apresents%20TempoPFN%2C%20a%20univariate%20time%20series%20foundation%20model%20based%20on%20linear%0ARecurrent%20Neural%20Networks%20%28RNNs%29%20pre-trained%20exclusively%20on%20synthetic%20data.%20The%0Amodel%20uses%20a%20GatedDeltaProduct%20architecture%20with%20state-weaving%20for%20fully%0Aparallelizable%20training%20across%20sequence%20lengths%2C%20eliminating%20the%20need%20for%0Awindowing%20or%20summarization%20techniques%20while%20maintaining%20robust%20temporal%0Astate-tracking.%20Our%20comprehensive%20synthetic%20data%20pipeline%20unifies%20diverse%0Agenerators%2C%20including%20stochastic%20differential%20equations%2C%20Gaussian%20processes%2C%0Aand%20audio%20synthesis%2C%20with%20novel%20augmentations.%20In%20zero-shot%20evaluations%20on%20the%0AGift-Eval%20benchmark%2C%20TempoPFN%20achieves%20top-tier%20competitive%20performance%2C%0Aoutperforming%20all%20existing%20synthetic-only%20approaches%20and%20surpassing%20the%20vast%0Amajority%20of%20models%20trained%20on%20real-world%20data%2C%20while%20being%20more%20efficient%20than%0Aexisting%20baselines%20by%20leveraging%20fully%20parallelizable%20training%20and%20inference.%0AWe%20open-source%20our%20complete%20data%20generation%20pipeline%20and%20training%20code%2C%0Aproviding%20a%20reproducible%20foundation%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25502v2&entry.124074799=Read"},
{"title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with\n  Oscillation Suppression and Outlier Control", "author": "Yuxiang Chen and Xiaoming Xu and Pengle Zhang and Michael Beyer and Martin Rapp and Jun Zhu and Jianfei Chen", "abstract": "  Large Language Models (LLMs) training is prohibitively expensive, driving\ninterest in low-precision fully-quantized training (FQT). While novel 4-bit\nformats like NVFP4 offer substantial efficiency gains, achieving near-lossless\ntraining at such low precision remains challenging. We introduce TetraJet-v2,\nan end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,\nand gradients in all linear layers. We identify two critical issues hindering\nlow-precision LLM training: weight oscillation and outliers. To address these,\nwe propose: 1) an unbiased double-block quantization method for NVFP4 linear\nlayers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)\nOutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently\noutperforms prior FP4 training methods on pre-training LLMs across varying\nmodel sizes up to 370M and data sizes up to 200B tokens, reducing the\nperformance gap to full-precision training by an average of 51.3%.\n", "link": "http://arxiv.org/abs/2510.27527v1", "date": "2025-10-31", "relevancy": 1.991, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5028}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4983}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TetraJet-v2%3A%20Accurate%20NVFP4%20Training%20for%20Large%20Language%20Models%20with%0A%20%20Oscillation%20Suppression%20and%20Outlier%20Control&body=Title%3A%20TetraJet-v2%3A%20Accurate%20NVFP4%20Training%20for%20Large%20Language%20Models%20with%0A%20%20Oscillation%20Suppression%20and%20Outlier%20Control%0AAuthor%3A%20Yuxiang%20Chen%20and%20Xiaoming%20Xu%20and%20Pengle%20Zhang%20and%20Michael%20Beyer%20and%20Martin%20Rapp%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20training%20is%20prohibitively%20expensive%2C%20driving%0Ainterest%20in%20low-precision%20fully-quantized%20training%20%28FQT%29.%20While%20novel%204-bit%0Aformats%20like%20NVFP4%20offer%20substantial%20efficiency%20gains%2C%20achieving%20near-lossless%0Atraining%20at%20such%20low%20precision%20remains%20challenging.%20We%20introduce%20TetraJet-v2%2C%0Aan%20end-to-end%204-bit%20FQT%20method%20that%20leverages%20NVFP4%20for%20activations%2C%20weights%2C%0Aand%20gradients%20in%20all%20linear%20layers.%20We%20identify%20two%20critical%20issues%20hindering%0Alow-precision%20LLM%20training%3A%20weight%20oscillation%20and%20outliers.%20To%20address%20these%2C%0Awe%20propose%3A%201%29%20an%20unbiased%20double-block%20quantization%20method%20for%20NVFP4%20linear%0Alayers%2C%202%29%20OsciReset%2C%20an%20algorithm%20to%20suppress%20weight%20oscillation%2C%20and%203%29%0AOutControl%2C%20an%20algorithm%20to%20retain%20outlier%20accuracy.%20TetraJet-v2%20consistently%0Aoutperforms%20prior%20FP4%20training%20methods%20on%20pre-training%20LLMs%20across%20varying%0Amodel%20sizes%20up%20to%20370M%20and%20data%20sizes%20up%20to%20200B%20tokens%2C%20reducing%20the%0Aperformance%20gap%20to%20full-precision%20training%20by%20an%20average%20of%2051.3%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetraJet-v2%253A%2520Accurate%2520NVFP4%2520Training%2520for%2520Large%2520Language%2520Models%2520with%250A%2520%2520Oscillation%2520Suppression%2520and%2520Outlier%2520Control%26entry.906535625%3DYuxiang%2520Chen%2520and%2520Xiaoming%2520Xu%2520and%2520Pengle%2520Zhang%2520and%2520Michael%2520Beyer%2520and%2520Martin%2520Rapp%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520training%2520is%2520prohibitively%2520expensive%252C%2520driving%250Ainterest%2520in%2520low-precision%2520fully-quantized%2520training%2520%2528FQT%2529.%2520While%2520novel%25204-bit%250Aformats%2520like%2520NVFP4%2520offer%2520substantial%2520efficiency%2520gains%252C%2520achieving%2520near-lossless%250Atraining%2520at%2520such%2520low%2520precision%2520remains%2520challenging.%2520We%2520introduce%2520TetraJet-v2%252C%250Aan%2520end-to-end%25204-bit%2520FQT%2520method%2520that%2520leverages%2520NVFP4%2520for%2520activations%252C%2520weights%252C%250Aand%2520gradients%2520in%2520all%2520linear%2520layers.%2520We%2520identify%2520two%2520critical%2520issues%2520hindering%250Alow-precision%2520LLM%2520training%253A%2520weight%2520oscillation%2520and%2520outliers.%2520To%2520address%2520these%252C%250Awe%2520propose%253A%25201%2529%2520an%2520unbiased%2520double-block%2520quantization%2520method%2520for%2520NVFP4%2520linear%250Alayers%252C%25202%2529%2520OsciReset%252C%2520an%2520algorithm%2520to%2520suppress%2520weight%2520oscillation%252C%2520and%25203%2529%250AOutControl%252C%2520an%2520algorithm%2520to%2520retain%2520outlier%2520accuracy.%2520TetraJet-v2%2520consistently%250Aoutperforms%2520prior%2520FP4%2520training%2520methods%2520on%2520pre-training%2520LLMs%2520across%2520varying%250Amodel%2520sizes%2520up%2520to%2520370M%2520and%2520data%2520sizes%2520up%2520to%2520200B%2520tokens%252C%2520reducing%2520the%250Aperformance%2520gap%2520to%2520full-precision%2520training%2520by%2520an%2520average%2520of%252051.3%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TetraJet-v2%3A%20Accurate%20NVFP4%20Training%20for%20Large%20Language%20Models%20with%0A%20%20Oscillation%20Suppression%20and%20Outlier%20Control&entry.906535625=Yuxiang%20Chen%20and%20Xiaoming%20Xu%20and%20Pengle%20Zhang%20and%20Michael%20Beyer%20and%20Martin%20Rapp%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20training%20is%20prohibitively%20expensive%2C%20driving%0Ainterest%20in%20low-precision%20fully-quantized%20training%20%28FQT%29.%20While%20novel%204-bit%0Aformats%20like%20NVFP4%20offer%20substantial%20efficiency%20gains%2C%20achieving%20near-lossless%0Atraining%20at%20such%20low%20precision%20remains%20challenging.%20We%20introduce%20TetraJet-v2%2C%0Aan%20end-to-end%204-bit%20FQT%20method%20that%20leverages%20NVFP4%20for%20activations%2C%20weights%2C%0Aand%20gradients%20in%20all%20linear%20layers.%20We%20identify%20two%20critical%20issues%20hindering%0Alow-precision%20LLM%20training%3A%20weight%20oscillation%20and%20outliers.%20To%20address%20these%2C%0Awe%20propose%3A%201%29%20an%20unbiased%20double-block%20quantization%20method%20for%20NVFP4%20linear%0Alayers%2C%202%29%20OsciReset%2C%20an%20algorithm%20to%20suppress%20weight%20oscillation%2C%20and%203%29%0AOutControl%2C%20an%20algorithm%20to%20retain%20outlier%20accuracy.%20TetraJet-v2%20consistently%0Aoutperforms%20prior%20FP4%20training%20methods%20on%20pre-training%20LLMs%20across%20varying%0Amodel%20sizes%20up%20to%20370M%20and%20data%20sizes%20up%20to%20200B%20tokens%2C%20reducing%20the%0Aperformance%20gap%20to%20full-precision%20training%20by%20an%20average%20of%2051.3%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27527v1&entry.124074799=Read"},
{"title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection", "author": "Md Abdul Hannan and Ronghao Ni and Chi Zhang and Limin Jia and Ravi Mangal and Corina S. Pasareanu", "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.\n", "link": "http://arxiv.org/abs/2510.27675v1", "date": "2025-10-31", "relevancy": 1.9741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Selecting%20Few-Shot%20Examples%20for%20LLM-based%20Code%20Vulnerability%0A%20%20Detection&body=Title%3A%20On%20Selecting%20Few-Shot%20Examples%20for%20LLM-based%20Code%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Md%20Abdul%20Hannan%20and%20Ronghao%20Ni%20and%20Chi%20Zhang%20and%20Limin%20Jia%20and%20Ravi%20Mangal%20and%20Corina%20S.%20Pasareanu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20for%0Amany%20coding%20tasks%2C%20including%20summarization%2C%20translation%2C%20completion%2C%20and%20code%0Ageneration.%20However%2C%20detecting%20code%20vulnerabilities%20remains%20a%20challenging%20task%0Afor%20LLMs.%20An%20effective%20way%20to%20improve%20LLM%20performance%20is%20in-context%20learning%0A%28ICL%29%20-%20providing%20few-shot%20examples%20similar%20to%20the%20query%2C%20along%20with%20correct%0Aanswers%2C%20can%20improve%20an%20LLM%27s%20ability%20to%20generate%20correct%20solutions.%20However%2C%0Achoosing%20the%20few-shot%20examples%20appropriately%20is%20crucial%20to%20improving%20model%0Aperformance.%20In%20this%20paper%2C%20we%20explore%20two%20criteria%20for%20choosing%20few-shot%0Aexamples%20for%20ICL%20used%20in%20the%20code%20vulnerability%20detection%20task.%20The%20first%0Acriterion%20considers%20if%20the%20LLM%20%28consistently%29%20makes%20a%20mistake%20or%20not%20on%20a%0Asample%20with%20the%20intuition%20that%20LLM%20performance%20on%20a%20sample%20is%20informative%20about%0Aits%20usefulness%20as%20a%20few-shot%20example.%20The%20other%20criterion%20considers%20similarity%0Aof%20the%20examples%20with%20the%20program%20under%20query%20and%20chooses%20few-shot%20examples%0Abased%20on%20the%20%24k%24-nearest%20neighbors%20to%20the%20given%20sample.%20We%20perform%20evaluations%0Ato%20determine%20the%20benefits%20of%20these%20criteria%20individually%20as%20well%20as%20under%0Avarious%20combinations%2C%20using%20open-source%20models%20on%20multiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Selecting%2520Few-Shot%2520Examples%2520for%2520LLM-based%2520Code%2520Vulnerability%250A%2520%2520Detection%26entry.906535625%3DMd%2520Abdul%2520Hannan%2520and%2520Ronghao%2520Ni%2520and%2520Chi%2520Zhang%2520and%2520Limin%2520Jia%2520and%2520Ravi%2520Mangal%2520and%2520Corina%2520S.%2520Pasareanu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520for%250Amany%2520coding%2520tasks%252C%2520including%2520summarization%252C%2520translation%252C%2520completion%252C%2520and%2520code%250Ageneration.%2520However%252C%2520detecting%2520code%2520vulnerabilities%2520remains%2520a%2520challenging%2520task%250Afor%2520LLMs.%2520An%2520effective%2520way%2520to%2520improve%2520LLM%2520performance%2520is%2520in-context%2520learning%250A%2528ICL%2529%2520-%2520providing%2520few-shot%2520examples%2520similar%2520to%2520the%2520query%252C%2520along%2520with%2520correct%250Aanswers%252C%2520can%2520improve%2520an%2520LLM%2527s%2520ability%2520to%2520generate%2520correct%2520solutions.%2520However%252C%250Achoosing%2520the%2520few-shot%2520examples%2520appropriately%2520is%2520crucial%2520to%2520improving%2520model%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520explore%2520two%2520criteria%2520for%2520choosing%2520few-shot%250Aexamples%2520for%2520ICL%2520used%2520in%2520the%2520code%2520vulnerability%2520detection%2520task.%2520The%2520first%250Acriterion%2520considers%2520if%2520the%2520LLM%2520%2528consistently%2529%2520makes%2520a%2520mistake%2520or%2520not%2520on%2520a%250Asample%2520with%2520the%2520intuition%2520that%2520LLM%2520performance%2520on%2520a%2520sample%2520is%2520informative%2520about%250Aits%2520usefulness%2520as%2520a%2520few-shot%2520example.%2520The%2520other%2520criterion%2520considers%2520similarity%250Aof%2520the%2520examples%2520with%2520the%2520program%2520under%2520query%2520and%2520chooses%2520few-shot%2520examples%250Abased%2520on%2520the%2520%2524k%2524-nearest%2520neighbors%2520to%2520the%2520given%2520sample.%2520We%2520perform%2520evaluations%250Ato%2520determine%2520the%2520benefits%2520of%2520these%2520criteria%2520individually%2520as%2520well%2520as%2520under%250Avarious%2520combinations%252C%2520using%2520open-source%2520models%2520on%2520multiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Selecting%20Few-Shot%20Examples%20for%20LLM-based%20Code%20Vulnerability%0A%20%20Detection&entry.906535625=Md%20Abdul%20Hannan%20and%20Ronghao%20Ni%20and%20Chi%20Zhang%20and%20Limin%20Jia%20and%20Ravi%20Mangal%20and%20Corina%20S.%20Pasareanu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20for%0Amany%20coding%20tasks%2C%20including%20summarization%2C%20translation%2C%20completion%2C%20and%20code%0Ageneration.%20However%2C%20detecting%20code%20vulnerabilities%20remains%20a%20challenging%20task%0Afor%20LLMs.%20An%20effective%20way%20to%20improve%20LLM%20performance%20is%20in-context%20learning%0A%28ICL%29%20-%20providing%20few-shot%20examples%20similar%20to%20the%20query%2C%20along%20with%20correct%0Aanswers%2C%20can%20improve%20an%20LLM%27s%20ability%20to%20generate%20correct%20solutions.%20However%2C%0Achoosing%20the%20few-shot%20examples%20appropriately%20is%20crucial%20to%20improving%20model%0Aperformance.%20In%20this%20paper%2C%20we%20explore%20two%20criteria%20for%20choosing%20few-shot%0Aexamples%20for%20ICL%20used%20in%20the%20code%20vulnerability%20detection%20task.%20The%20first%0Acriterion%20considers%20if%20the%20LLM%20%28consistently%29%20makes%20a%20mistake%20or%20not%20on%20a%0Asample%20with%20the%20intuition%20that%20LLM%20performance%20on%20a%20sample%20is%20informative%20about%0Aits%20usefulness%20as%20a%20few-shot%20example.%20The%20other%20criterion%20considers%20similarity%0Aof%20the%20examples%20with%20the%20program%20under%20query%20and%20chooses%20few-shot%20examples%0Abased%20on%20the%20%24k%24-nearest%20neighbors%20to%20the%20given%20sample.%20We%20perform%20evaluations%0Ato%20determine%20the%20benefits%20of%20these%20criteria%20individually%20as%20well%20as%20under%0Avarious%20combinations%2C%20using%20open-source%20models%20on%20multiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27675v1&entry.124074799=Read"},
{"title": "Resource-Adaptive Successive Doubling for Hyperparameter Optimization\n  with Large Datasets on High-Performance Computing Systems", "author": "Marcel Aach and Rakesh Sarma and Helmut Neukirchen and Morris Riedel and Andreas Lintermann", "abstract": "  On High-Performance Computing (HPC) systems, several hyperparameter\nconfigurations can be evaluated in parallel to speed up the Hyperparameter\nOptimization (HPO) process. State-of-the-art HPO methods follow a bandit-based\napproach and build on top of successive halving, where the final performance of\na combination is estimated based on a lower than fully trained fidelity\nperformance metric and more promising combinations are assigned more resources\nover time. Frequently, the number of epochs is treated as a resource, letting\nmore promising combinations train longer. Another option is to use the number\nof workers as a resource and directly allocate more workers to more promising\nconfigurations via data-parallel training. This article proposes a novel\nResource-Adaptive Successive Doubling Algorithm (RASDA), which combines a\nresource-adaptive successive doubling scheme with the plain Asynchronous\nSuccessive Halving Algorithm (ASHA). Scalability of this approach is shown on\nup to 1,024 Graphics Processing Units (GPUs) on modern HPC systems. It is\napplied to different types of Neural Networks (NNs) and trained on large\ndatasets from the Computer Vision (CV), Computational Fluid Dynamics (CFD), and\nAdditive Manufacturing (AM) domains, where performing more than one full\ntraining run is usually infeasible. Empirical results show that RASDA\noutperforms ASHA by a factor of up to 1.9 with respect to the runtime. At the\nsame time, the solution quality of final ASHA models is maintained or even\nsurpassed by the implicit batch size scheduling of RASDA. With RASDA,\nsystematic HPO is applied to a terabyte-scale scientific dataset for the first\ntime in the literature, enabling efficient optimization of complex models on\nmassive scientific data. The implementation of RASDA is available on\nhttps://github.com/olympiquemarcel/rasda\n", "link": "http://arxiv.org/abs/2412.02729v2", "date": "2025-10-31", "relevancy": 1.9732, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4939}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4933}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resource-Adaptive%20Successive%20Doubling%20for%20Hyperparameter%20Optimization%0A%20%20with%20Large%20Datasets%20on%20High-Performance%20Computing%20Systems&body=Title%3A%20Resource-Adaptive%20Successive%20Doubling%20for%20Hyperparameter%20Optimization%0A%20%20with%20Large%20Datasets%20on%20High-Performance%20Computing%20Systems%0AAuthor%3A%20Marcel%20Aach%20and%20Rakesh%20Sarma%20and%20Helmut%20Neukirchen%20and%20Morris%20Riedel%20and%20Andreas%20Lintermann%0AAbstract%3A%20%20%20On%20High-Performance%20Computing%20%28HPC%29%20systems%2C%20several%20hyperparameter%0Aconfigurations%20can%20be%20evaluated%20in%20parallel%20to%20speed%20up%20the%20Hyperparameter%0AOptimization%20%28HPO%29%20process.%20State-of-the-art%20HPO%20methods%20follow%20a%20bandit-based%0Aapproach%20and%20build%20on%20top%20of%20successive%20halving%2C%20where%20the%20final%20performance%20of%0Aa%20combination%20is%20estimated%20based%20on%20a%20lower%20than%20fully%20trained%20fidelity%0Aperformance%20metric%20and%20more%20promising%20combinations%20are%20assigned%20more%20resources%0Aover%20time.%20Frequently%2C%20the%20number%20of%20epochs%20is%20treated%20as%20a%20resource%2C%20letting%0Amore%20promising%20combinations%20train%20longer.%20Another%20option%20is%20to%20use%20the%20number%0Aof%20workers%20as%20a%20resource%20and%20directly%20allocate%20more%20workers%20to%20more%20promising%0Aconfigurations%20via%20data-parallel%20training.%20This%20article%20proposes%20a%20novel%0AResource-Adaptive%20Successive%20Doubling%20Algorithm%20%28RASDA%29%2C%20which%20combines%20a%0Aresource-adaptive%20successive%20doubling%20scheme%20with%20the%20plain%20Asynchronous%0ASuccessive%20Halving%20Algorithm%20%28ASHA%29.%20Scalability%20of%20this%20approach%20is%20shown%20on%0Aup%20to%201%2C024%20Graphics%20Processing%20Units%20%28GPUs%29%20on%20modern%20HPC%20systems.%20It%20is%0Aapplied%20to%20different%20types%20of%20Neural%20Networks%20%28NNs%29%20and%20trained%20on%20large%0Adatasets%20from%20the%20Computer%20Vision%20%28CV%29%2C%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20and%0AAdditive%20Manufacturing%20%28AM%29%20domains%2C%20where%20performing%20more%20than%20one%20full%0Atraining%20run%20is%20usually%20infeasible.%20Empirical%20results%20show%20that%20RASDA%0Aoutperforms%20ASHA%20by%20a%20factor%20of%20up%20to%201.9%20with%20respect%20to%20the%20runtime.%20At%20the%0Asame%20time%2C%20the%20solution%20quality%20of%20final%20ASHA%20models%20is%20maintained%20or%20even%0Asurpassed%20by%20the%20implicit%20batch%20size%20scheduling%20of%20RASDA.%20With%20RASDA%2C%0Asystematic%20HPO%20is%20applied%20to%20a%20terabyte-scale%20scientific%20dataset%20for%20the%20first%0Atime%20in%20the%20literature%2C%20enabling%20efficient%20optimization%20of%20complex%20models%20on%0Amassive%20scientific%20data.%20The%20implementation%20of%20RASDA%20is%20available%20on%0Ahttps%3A//github.com/olympiquemarcel/rasda%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResource-Adaptive%2520Successive%2520Doubling%2520for%2520Hyperparameter%2520Optimization%250A%2520%2520with%2520Large%2520Datasets%2520on%2520High-Performance%2520Computing%2520Systems%26entry.906535625%3DMarcel%2520Aach%2520and%2520Rakesh%2520Sarma%2520and%2520Helmut%2520Neukirchen%2520and%2520Morris%2520Riedel%2520and%2520Andreas%2520Lintermann%26entry.1292438233%3D%2520%2520On%2520High-Performance%2520Computing%2520%2528HPC%2529%2520systems%252C%2520several%2520hyperparameter%250Aconfigurations%2520can%2520be%2520evaluated%2520in%2520parallel%2520to%2520speed%2520up%2520the%2520Hyperparameter%250AOptimization%2520%2528HPO%2529%2520process.%2520State-of-the-art%2520HPO%2520methods%2520follow%2520a%2520bandit-based%250Aapproach%2520and%2520build%2520on%2520top%2520of%2520successive%2520halving%252C%2520where%2520the%2520final%2520performance%2520of%250Aa%2520combination%2520is%2520estimated%2520based%2520on%2520a%2520lower%2520than%2520fully%2520trained%2520fidelity%250Aperformance%2520metric%2520and%2520more%2520promising%2520combinations%2520are%2520assigned%2520more%2520resources%250Aover%2520time.%2520Frequently%252C%2520the%2520number%2520of%2520epochs%2520is%2520treated%2520as%2520a%2520resource%252C%2520letting%250Amore%2520promising%2520combinations%2520train%2520longer.%2520Another%2520option%2520is%2520to%2520use%2520the%2520number%250Aof%2520workers%2520as%2520a%2520resource%2520and%2520directly%2520allocate%2520more%2520workers%2520to%2520more%2520promising%250Aconfigurations%2520via%2520data-parallel%2520training.%2520This%2520article%2520proposes%2520a%2520novel%250AResource-Adaptive%2520Successive%2520Doubling%2520Algorithm%2520%2528RASDA%2529%252C%2520which%2520combines%2520a%250Aresource-adaptive%2520successive%2520doubling%2520scheme%2520with%2520the%2520plain%2520Asynchronous%250ASuccessive%2520Halving%2520Algorithm%2520%2528ASHA%2529.%2520Scalability%2520of%2520this%2520approach%2520is%2520shown%2520on%250Aup%2520to%25201%252C024%2520Graphics%2520Processing%2520Units%2520%2528GPUs%2529%2520on%2520modern%2520HPC%2520systems.%2520It%2520is%250Aapplied%2520to%2520different%2520types%2520of%2520Neural%2520Networks%2520%2528NNs%2529%2520and%2520trained%2520on%2520large%250Adatasets%2520from%2520the%2520Computer%2520Vision%2520%2528CV%2529%252C%2520Computational%2520Fluid%2520Dynamics%2520%2528CFD%2529%252C%2520and%250AAdditive%2520Manufacturing%2520%2528AM%2529%2520domains%252C%2520where%2520performing%2520more%2520than%2520one%2520full%250Atraining%2520run%2520is%2520usually%2520infeasible.%2520Empirical%2520results%2520show%2520that%2520RASDA%250Aoutperforms%2520ASHA%2520by%2520a%2520factor%2520of%2520up%2520to%25201.9%2520with%2520respect%2520to%2520the%2520runtime.%2520At%2520the%250Asame%2520time%252C%2520the%2520solution%2520quality%2520of%2520final%2520ASHA%2520models%2520is%2520maintained%2520or%2520even%250Asurpassed%2520by%2520the%2520implicit%2520batch%2520size%2520scheduling%2520of%2520RASDA.%2520With%2520RASDA%252C%250Asystematic%2520HPO%2520is%2520applied%2520to%2520a%2520terabyte-scale%2520scientific%2520dataset%2520for%2520the%2520first%250Atime%2520in%2520the%2520literature%252C%2520enabling%2520efficient%2520optimization%2520of%2520complex%2520models%2520on%250Amassive%2520scientific%2520data.%2520The%2520implementation%2520of%2520RASDA%2520is%2520available%2520on%250Ahttps%253A//github.com/olympiquemarcel/rasda%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resource-Adaptive%20Successive%20Doubling%20for%20Hyperparameter%20Optimization%0A%20%20with%20Large%20Datasets%20on%20High-Performance%20Computing%20Systems&entry.906535625=Marcel%20Aach%20and%20Rakesh%20Sarma%20and%20Helmut%20Neukirchen%20and%20Morris%20Riedel%20and%20Andreas%20Lintermann&entry.1292438233=%20%20On%20High-Performance%20Computing%20%28HPC%29%20systems%2C%20several%20hyperparameter%0Aconfigurations%20can%20be%20evaluated%20in%20parallel%20to%20speed%20up%20the%20Hyperparameter%0AOptimization%20%28HPO%29%20process.%20State-of-the-art%20HPO%20methods%20follow%20a%20bandit-based%0Aapproach%20and%20build%20on%20top%20of%20successive%20halving%2C%20where%20the%20final%20performance%20of%0Aa%20combination%20is%20estimated%20based%20on%20a%20lower%20than%20fully%20trained%20fidelity%0Aperformance%20metric%20and%20more%20promising%20combinations%20are%20assigned%20more%20resources%0Aover%20time.%20Frequently%2C%20the%20number%20of%20epochs%20is%20treated%20as%20a%20resource%2C%20letting%0Amore%20promising%20combinations%20train%20longer.%20Another%20option%20is%20to%20use%20the%20number%0Aof%20workers%20as%20a%20resource%20and%20directly%20allocate%20more%20workers%20to%20more%20promising%0Aconfigurations%20via%20data-parallel%20training.%20This%20article%20proposes%20a%20novel%0AResource-Adaptive%20Successive%20Doubling%20Algorithm%20%28RASDA%29%2C%20which%20combines%20a%0Aresource-adaptive%20successive%20doubling%20scheme%20with%20the%20plain%20Asynchronous%0ASuccessive%20Halving%20Algorithm%20%28ASHA%29.%20Scalability%20of%20this%20approach%20is%20shown%20on%0Aup%20to%201%2C024%20Graphics%20Processing%20Units%20%28GPUs%29%20on%20modern%20HPC%20systems.%20It%20is%0Aapplied%20to%20different%20types%20of%20Neural%20Networks%20%28NNs%29%20and%20trained%20on%20large%0Adatasets%20from%20the%20Computer%20Vision%20%28CV%29%2C%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20and%0AAdditive%20Manufacturing%20%28AM%29%20domains%2C%20where%20performing%20more%20than%20one%20full%0Atraining%20run%20is%20usually%20infeasible.%20Empirical%20results%20show%20that%20RASDA%0Aoutperforms%20ASHA%20by%20a%20factor%20of%20up%20to%201.9%20with%20respect%20to%20the%20runtime.%20At%20the%0Asame%20time%2C%20the%20solution%20quality%20of%20final%20ASHA%20models%20is%20maintained%20or%20even%0Asurpassed%20by%20the%20implicit%20batch%20size%20scheduling%20of%20RASDA.%20With%20RASDA%2C%0Asystematic%20HPO%20is%20applied%20to%20a%20terabyte-scale%20scientific%20dataset%20for%20the%20first%0Atime%20in%20the%20literature%2C%20enabling%20efficient%20optimization%20of%20complex%20models%20on%0Amassive%20scientific%20data.%20The%20implementation%20of%20RASDA%20is%20available%20on%0Ahttps%3A//github.com/olympiquemarcel/rasda%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02729v2&entry.124074799=Read"},
{"title": "Optimal Convergence Analysis of DDPM for General Distributions", "author": "Yuchen Jiao and Yuchen Zhou and Gen Li", "abstract": "  Score-based diffusion models have achieved remarkable empirical success in\ngenerating high-quality samples from target data distributions. Among them, the\nDenoising Diffusion Probabilistic Model (DDPM) is one of the most widely used\nsamplers, generating samples via estimated score functions. Despite its\nempirical success, a tight theoretical understanding of DDPM -- especially its\nconvergence properties -- remains limited.\n  In this paper, we provide a refined convergence analysis of the DDPM sampler\nand establish near-optimal convergence rates under general distributional\nassumptions. Specifically, we introduce a relaxed smoothness condition\nparameterized by a constant $L$, which is small for many practical\ndistributions (e.g., Gaussian mixture models). We prove that the DDPM sampler\nwith accurate score estimates achieves a convergence rate of\n$$\\widetilde{O}\\left(\\frac{d\\min\\{d,L^2\\}}{T^2}\\right)~\\text{in\nKullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the\nnumber of iterations, and $\\widetilde{O}$ hides polylogarithmic factors in $T$.\nThis result substantially improves upon the best-known $d^2/T^2$ rate when $L <\n\\sqrt{d}$. By establishing a matching lower bound, we show that our convergence\nanalysis is tight for a wide array of target distributions. Moreover, it\nreveals that DDPM and DDIM share the same dependence on $d$, raising an\ninteresting question of why DDIM often appears empirically faster.\n", "link": "http://arxiv.org/abs/2510.27562v1", "date": "2025-10-31", "relevancy": 1.9679, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5419}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5004}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Convergence%20Analysis%20of%20DDPM%20for%20General%20Distributions&body=Title%3A%20Optimal%20Convergence%20Analysis%20of%20DDPM%20for%20General%20Distributions%0AAuthor%3A%20Yuchen%20Jiao%20and%20Yuchen%20Zhou%20and%20Gen%20Li%0AAbstract%3A%20%20%20Score-based%20diffusion%20models%20have%20achieved%20remarkable%20empirical%20success%20in%0Agenerating%20high-quality%20samples%20from%20target%20data%20distributions.%20Among%20them%2C%20the%0ADenoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%20is%20one%20of%20the%20most%20widely%20used%0Asamplers%2C%20generating%20samples%20via%20estimated%20score%20functions.%20Despite%20its%0Aempirical%20success%2C%20a%20tight%20theoretical%20understanding%20of%20DDPM%20--%20especially%20its%0Aconvergence%20properties%20--%20remains%20limited.%0A%20%20In%20this%20paper%2C%20we%20provide%20a%20refined%20convergence%20analysis%20of%20the%20DDPM%20sampler%0Aand%20establish%20near-optimal%20convergence%20rates%20under%20general%20distributional%0Aassumptions.%20Specifically%2C%20we%20introduce%20a%20relaxed%20smoothness%20condition%0Aparameterized%20by%20a%20constant%20%24L%24%2C%20which%20is%20small%20for%20many%20practical%0Adistributions%20%28e.g.%2C%20Gaussian%20mixture%20models%29.%20We%20prove%20that%20the%20DDPM%20sampler%0Awith%20accurate%20score%20estimates%20achieves%20a%20convergence%20rate%20of%0A%24%24%5Cwidetilde%7BO%7D%5Cleft%28%5Cfrac%7Bd%5Cmin%5C%7Bd%2CL%5E2%5C%7D%7D%7BT%5E2%7D%5Cright%29~%5Ctext%7Bin%0AKullback-Leibler%20divergence%7D%2C%24%24%20where%20%24d%24%20is%20the%20data%20dimension%2C%20%24T%24%20is%20the%0Anumber%20of%20iterations%2C%20and%20%24%5Cwidetilde%7BO%7D%24%20hides%20polylogarithmic%20factors%20in%20%24T%24.%0AThis%20result%20substantially%20improves%20upon%20the%20best-known%20%24d%5E2/T%5E2%24%20rate%20when%20%24L%20%3C%0A%5Csqrt%7Bd%7D%24.%20By%20establishing%20a%20matching%20lower%20bound%2C%20we%20show%20that%20our%20convergence%0Aanalysis%20is%20tight%20for%20a%20wide%20array%20of%20target%20distributions.%20Moreover%2C%20it%0Areveals%20that%20DDPM%20and%20DDIM%20share%20the%20same%20dependence%20on%20%24d%24%2C%20raising%20an%0Ainteresting%20question%20of%20why%20DDIM%20often%20appears%20empirically%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Convergence%2520Analysis%2520of%2520DDPM%2520for%2520General%2520Distributions%26entry.906535625%3DYuchen%2520Jiao%2520and%2520Yuchen%2520Zhou%2520and%2520Gen%2520Li%26entry.1292438233%3D%2520%2520Score-based%2520diffusion%2520models%2520have%2520achieved%2520remarkable%2520empirical%2520success%2520in%250Agenerating%2520high-quality%2520samples%2520from%2520target%2520data%2520distributions.%2520Among%2520them%252C%2520the%250ADenoising%2520Diffusion%2520Probabilistic%2520Model%2520%2528DDPM%2529%2520is%2520one%2520of%2520the%2520most%2520widely%2520used%250Asamplers%252C%2520generating%2520samples%2520via%2520estimated%2520score%2520functions.%2520Despite%2520its%250Aempirical%2520success%252C%2520a%2520tight%2520theoretical%2520understanding%2520of%2520DDPM%2520--%2520especially%2520its%250Aconvergence%2520properties%2520--%2520remains%2520limited.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520refined%2520convergence%2520analysis%2520of%2520the%2520DDPM%2520sampler%250Aand%2520establish%2520near-optimal%2520convergence%2520rates%2520under%2520general%2520distributional%250Aassumptions.%2520Specifically%252C%2520we%2520introduce%2520a%2520relaxed%2520smoothness%2520condition%250Aparameterized%2520by%2520a%2520constant%2520%2524L%2524%252C%2520which%2520is%2520small%2520for%2520many%2520practical%250Adistributions%2520%2528e.g.%252C%2520Gaussian%2520mixture%2520models%2529.%2520We%2520prove%2520that%2520the%2520DDPM%2520sampler%250Awith%2520accurate%2520score%2520estimates%2520achieves%2520a%2520convergence%2520rate%2520of%250A%2524%2524%255Cwidetilde%257BO%257D%255Cleft%2528%255Cfrac%257Bd%255Cmin%255C%257Bd%252CL%255E2%255C%257D%257D%257BT%255E2%257D%255Cright%2529~%255Ctext%257Bin%250AKullback-Leibler%2520divergence%257D%252C%2524%2524%2520where%2520%2524d%2524%2520is%2520the%2520data%2520dimension%252C%2520%2524T%2524%2520is%2520the%250Anumber%2520of%2520iterations%252C%2520and%2520%2524%255Cwidetilde%257BO%257D%2524%2520hides%2520polylogarithmic%2520factors%2520in%2520%2524T%2524.%250AThis%2520result%2520substantially%2520improves%2520upon%2520the%2520best-known%2520%2524d%255E2/T%255E2%2524%2520rate%2520when%2520%2524L%2520%253C%250A%255Csqrt%257Bd%257D%2524.%2520By%2520establishing%2520a%2520matching%2520lower%2520bound%252C%2520we%2520show%2520that%2520our%2520convergence%250Aanalysis%2520is%2520tight%2520for%2520a%2520wide%2520array%2520of%2520target%2520distributions.%2520Moreover%252C%2520it%250Areveals%2520that%2520DDPM%2520and%2520DDIM%2520share%2520the%2520same%2520dependence%2520on%2520%2524d%2524%252C%2520raising%2520an%250Ainteresting%2520question%2520of%2520why%2520DDIM%2520often%2520appears%2520empirically%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Convergence%20Analysis%20of%20DDPM%20for%20General%20Distributions&entry.906535625=Yuchen%20Jiao%20and%20Yuchen%20Zhou%20and%20Gen%20Li&entry.1292438233=%20%20Score-based%20diffusion%20models%20have%20achieved%20remarkable%20empirical%20success%20in%0Agenerating%20high-quality%20samples%20from%20target%20data%20distributions.%20Among%20them%2C%20the%0ADenoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%20is%20one%20of%20the%20most%20widely%20used%0Asamplers%2C%20generating%20samples%20via%20estimated%20score%20functions.%20Despite%20its%0Aempirical%20success%2C%20a%20tight%20theoretical%20understanding%20of%20DDPM%20--%20especially%20its%0Aconvergence%20properties%20--%20remains%20limited.%0A%20%20In%20this%20paper%2C%20we%20provide%20a%20refined%20convergence%20analysis%20of%20the%20DDPM%20sampler%0Aand%20establish%20near-optimal%20convergence%20rates%20under%20general%20distributional%0Aassumptions.%20Specifically%2C%20we%20introduce%20a%20relaxed%20smoothness%20condition%0Aparameterized%20by%20a%20constant%20%24L%24%2C%20which%20is%20small%20for%20many%20practical%0Adistributions%20%28e.g.%2C%20Gaussian%20mixture%20models%29.%20We%20prove%20that%20the%20DDPM%20sampler%0Awith%20accurate%20score%20estimates%20achieves%20a%20convergence%20rate%20of%0A%24%24%5Cwidetilde%7BO%7D%5Cleft%28%5Cfrac%7Bd%5Cmin%5C%7Bd%2CL%5E2%5C%7D%7D%7BT%5E2%7D%5Cright%29~%5Ctext%7Bin%0AKullback-Leibler%20divergence%7D%2C%24%24%20where%20%24d%24%20is%20the%20data%20dimension%2C%20%24T%24%20is%20the%0Anumber%20of%20iterations%2C%20and%20%24%5Cwidetilde%7BO%7D%24%20hides%20polylogarithmic%20factors%20in%20%24T%24.%0AThis%20result%20substantially%20improves%20upon%20the%20best-known%20%24d%5E2/T%5E2%24%20rate%20when%20%24L%20%3C%0A%5Csqrt%7Bd%7D%24.%20By%20establishing%20a%20matching%20lower%20bound%2C%20we%20show%20that%20our%20convergence%0Aanalysis%20is%20tight%20for%20a%20wide%20array%20of%20target%20distributions.%20Moreover%2C%20it%0Areveals%20that%20DDPM%20and%20DDIM%20share%20the%20same%20dependence%20on%20%24d%24%2C%20raising%20an%0Ainteresting%20question%20of%20why%20DDIM%20often%20appears%20empirically%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27562v1&entry.124074799=Read"},
{"title": "Imbalanced Classification through the Lens of Spurious Correlations", "author": "Jakob Hackstein and Sidney Bender", "abstract": "  Class imbalance poses a fundamental challenge in machine learning, frequently\nleading to unreliable classification performance. While prior methods focus on\ndata- or loss-reweighting schemes, we view imbalance as a data condition that\namplifies Clever Hans (CH) effects by underspecification of minority classes.\nIn a counterfactual explanations-based approach, we propose to leverage\nExplainable AI to jointly identify and eliminate CH effects emerging under\nimbalance. Our method achieves competitive classification performance on three\ndatasets and demonstrates how CH effects emerge under imbalance, a perspective\nlargely overlooked by existing approaches.\n", "link": "http://arxiv.org/abs/2510.27650v1", "date": "2025-10-31", "relevancy": 1.9539, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5019}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imbalanced%20Classification%20through%20the%20Lens%20of%20Spurious%20Correlations&body=Title%3A%20Imbalanced%20Classification%20through%20the%20Lens%20of%20Spurious%20Correlations%0AAuthor%3A%20Jakob%20Hackstein%20and%20Sidney%20Bender%0AAbstract%3A%20%20%20Class%20imbalance%20poses%20a%20fundamental%20challenge%20in%20machine%20learning%2C%20frequently%0Aleading%20to%20unreliable%20classification%20performance.%20While%20prior%20methods%20focus%20on%0Adata-%20or%20loss-reweighting%20schemes%2C%20we%20view%20imbalance%20as%20a%20data%20condition%20that%0Aamplifies%20Clever%20Hans%20%28CH%29%20effects%20by%20underspecification%20of%20minority%20classes.%0AIn%20a%20counterfactual%20explanations-based%20approach%2C%20we%20propose%20to%20leverage%0AExplainable%20AI%20to%20jointly%20identify%20and%20eliminate%20CH%20effects%20emerging%20under%0Aimbalance.%20Our%20method%20achieves%20competitive%20classification%20performance%20on%20three%0Adatasets%20and%20demonstrates%20how%20CH%20effects%20emerge%20under%20imbalance%2C%20a%20perspective%0Alargely%20overlooked%20by%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImbalanced%2520Classification%2520through%2520the%2520Lens%2520of%2520Spurious%2520Correlations%26entry.906535625%3DJakob%2520Hackstein%2520and%2520Sidney%2520Bender%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520poses%2520a%2520fundamental%2520challenge%2520in%2520machine%2520learning%252C%2520frequently%250Aleading%2520to%2520unreliable%2520classification%2520performance.%2520While%2520prior%2520methods%2520focus%2520on%250Adata-%2520or%2520loss-reweighting%2520schemes%252C%2520we%2520view%2520imbalance%2520as%2520a%2520data%2520condition%2520that%250Aamplifies%2520Clever%2520Hans%2520%2528CH%2529%2520effects%2520by%2520underspecification%2520of%2520minority%2520classes.%250AIn%2520a%2520counterfactual%2520explanations-based%2520approach%252C%2520we%2520propose%2520to%2520leverage%250AExplainable%2520AI%2520to%2520jointly%2520identify%2520and%2520eliminate%2520CH%2520effects%2520emerging%2520under%250Aimbalance.%2520Our%2520method%2520achieves%2520competitive%2520classification%2520performance%2520on%2520three%250Adatasets%2520and%2520demonstrates%2520how%2520CH%2520effects%2520emerge%2520under%2520imbalance%252C%2520a%2520perspective%250Alargely%2520overlooked%2520by%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imbalanced%20Classification%20through%20the%20Lens%20of%20Spurious%20Correlations&entry.906535625=Jakob%20Hackstein%20and%20Sidney%20Bender&entry.1292438233=%20%20Class%20imbalance%20poses%20a%20fundamental%20challenge%20in%20machine%20learning%2C%20frequently%0Aleading%20to%20unreliable%20classification%20performance.%20While%20prior%20methods%20focus%20on%0Adata-%20or%20loss-reweighting%20schemes%2C%20we%20view%20imbalance%20as%20a%20data%20condition%20that%0Aamplifies%20Clever%20Hans%20%28CH%29%20effects%20by%20underspecification%20of%20minority%20classes.%0AIn%20a%20counterfactual%20explanations-based%20approach%2C%20we%20propose%20to%20leverage%0AExplainable%20AI%20to%20jointly%20identify%20and%20eliminate%20CH%20effects%20emerging%20under%0Aimbalance.%20Our%20method%20achieves%20competitive%20classification%20performance%20on%20three%0Adatasets%20and%20demonstrates%20how%20CH%20effects%20emerge%20under%20imbalance%2C%20a%20perspective%0Alargely%20overlooked%20by%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27650v1&entry.124074799=Read"},
{"title": "Panprediction: Optimal Predictions for Any Downstream Task and Loss", "author": "Sivaraman Balakrishnan and Nika Haghtalab and Daniel Hsu and Brian Lee and Eric Zhao", "abstract": "  Supervised learning is classically formulated as training a model to minimize\na fixed loss function over a fixed distribution, or task. However, an emerging\nparadigm instead views model training as extracting enough information from\ndata so that the model can be used to minimize many losses on many downstream\ntasks. We formalize a mathematical framework for this paradigm, which we call\npanprediction, and study its statistical complexity. Formally, panprediction\ngeneralizes omniprediction and sits upstream from multi-group learning, which\nrespectively focus on predictions that generalize to many downstream losses or\nmany downstream tasks, but not both. Concretely, we design algorithms that\nlearn deterministic and randomized panpredictors with\n$\\tilde{O}(1/\\varepsilon^3)$ and $\\tilde{O}(1/\\varepsilon^2)$ samples,\nrespectively. Our results demonstrate that under mild assumptions,\nsimultaneously minimizing infinitely many losses on infinitely many tasks can\nbe as statistically easy as minimizing one loss on one task. Along the way, we\nimprove the best known sample complexity guarantee of deterministic\nomniprediction by a factor of $1/\\varepsilon$, and match all other known sample\ncomplexity guarantees of omniprediction and multi-group learning. Our key\ntechnical ingredient is a nearly lossless reduction from panprediction to a\nstatistically efficient notion of calibration, called step calibration.\n", "link": "http://arxiv.org/abs/2510.27638v1", "date": "2025-10-31", "relevancy": 1.9347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panprediction%3A%20Optimal%20Predictions%20for%20Any%20Downstream%20Task%20and%20Loss&body=Title%3A%20Panprediction%3A%20Optimal%20Predictions%20for%20Any%20Downstream%20Task%20and%20Loss%0AAuthor%3A%20Sivaraman%20Balakrishnan%20and%20Nika%20Haghtalab%20and%20Daniel%20Hsu%20and%20Brian%20Lee%20and%20Eric%20Zhao%0AAbstract%3A%20%20%20Supervised%20learning%20is%20classically%20formulated%20as%20training%20a%20model%20to%20minimize%0Aa%20fixed%20loss%20function%20over%20a%20fixed%20distribution%2C%20or%20task.%20However%2C%20an%20emerging%0Aparadigm%20instead%20views%20model%20training%20as%20extracting%20enough%20information%20from%0Adata%20so%20that%20the%20model%20can%20be%20used%20to%20minimize%20many%20losses%20on%20many%20downstream%0Atasks.%20We%20formalize%20a%20mathematical%20framework%20for%20this%20paradigm%2C%20which%20we%20call%0Apanprediction%2C%20and%20study%20its%20statistical%20complexity.%20Formally%2C%20panprediction%0Ageneralizes%20omniprediction%20and%20sits%20upstream%20from%20multi-group%20learning%2C%20which%0Arespectively%20focus%20on%20predictions%20that%20generalize%20to%20many%20downstream%20losses%20or%0Amany%20downstream%20tasks%2C%20but%20not%20both.%20Concretely%2C%20we%20design%20algorithms%20that%0Alearn%20deterministic%20and%20randomized%20panpredictors%20with%0A%24%5Ctilde%7BO%7D%281/%5Cvarepsilon%5E3%29%24%20and%20%24%5Ctilde%7BO%7D%281/%5Cvarepsilon%5E2%29%24%20samples%2C%0Arespectively.%20Our%20results%20demonstrate%20that%20under%20mild%20assumptions%2C%0Asimultaneously%20minimizing%20infinitely%20many%20losses%20on%20infinitely%20many%20tasks%20can%0Abe%20as%20statistically%20easy%20as%20minimizing%20one%20loss%20on%20one%20task.%20Along%20the%20way%2C%20we%0Aimprove%20the%20best%20known%20sample%20complexity%20guarantee%20of%20deterministic%0Aomniprediction%20by%20a%20factor%20of%20%241/%5Cvarepsilon%24%2C%20and%20match%20all%20other%20known%20sample%0Acomplexity%20guarantees%20of%20omniprediction%20and%20multi-group%20learning.%20Our%20key%0Atechnical%20ingredient%20is%20a%20nearly%20lossless%20reduction%20from%20panprediction%20to%20a%0Astatistically%20efficient%20notion%20of%20calibration%2C%20called%20step%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanprediction%253A%2520Optimal%2520Predictions%2520for%2520Any%2520Downstream%2520Task%2520and%2520Loss%26entry.906535625%3DSivaraman%2520Balakrishnan%2520and%2520Nika%2520Haghtalab%2520and%2520Daniel%2520Hsu%2520and%2520Brian%2520Lee%2520and%2520Eric%2520Zhao%26entry.1292438233%3D%2520%2520Supervised%2520learning%2520is%2520classically%2520formulated%2520as%2520training%2520a%2520model%2520to%2520minimize%250Aa%2520fixed%2520loss%2520function%2520over%2520a%2520fixed%2520distribution%252C%2520or%2520task.%2520However%252C%2520an%2520emerging%250Aparadigm%2520instead%2520views%2520model%2520training%2520as%2520extracting%2520enough%2520information%2520from%250Adata%2520so%2520that%2520the%2520model%2520can%2520be%2520used%2520to%2520minimize%2520many%2520losses%2520on%2520many%2520downstream%250Atasks.%2520We%2520formalize%2520a%2520mathematical%2520framework%2520for%2520this%2520paradigm%252C%2520which%2520we%2520call%250Apanprediction%252C%2520and%2520study%2520its%2520statistical%2520complexity.%2520Formally%252C%2520panprediction%250Ageneralizes%2520omniprediction%2520and%2520sits%2520upstream%2520from%2520multi-group%2520learning%252C%2520which%250Arespectively%2520focus%2520on%2520predictions%2520that%2520generalize%2520to%2520many%2520downstream%2520losses%2520or%250Amany%2520downstream%2520tasks%252C%2520but%2520not%2520both.%2520Concretely%252C%2520we%2520design%2520algorithms%2520that%250Alearn%2520deterministic%2520and%2520randomized%2520panpredictors%2520with%250A%2524%255Ctilde%257BO%257D%25281/%255Cvarepsilon%255E3%2529%2524%2520and%2520%2524%255Ctilde%257BO%257D%25281/%255Cvarepsilon%255E2%2529%2524%2520samples%252C%250Arespectively.%2520Our%2520results%2520demonstrate%2520that%2520under%2520mild%2520assumptions%252C%250Asimultaneously%2520minimizing%2520infinitely%2520many%2520losses%2520on%2520infinitely%2520many%2520tasks%2520can%250Abe%2520as%2520statistically%2520easy%2520as%2520minimizing%2520one%2520loss%2520on%2520one%2520task.%2520Along%2520the%2520way%252C%2520we%250Aimprove%2520the%2520best%2520known%2520sample%2520complexity%2520guarantee%2520of%2520deterministic%250Aomniprediction%2520by%2520a%2520factor%2520of%2520%25241/%255Cvarepsilon%2524%252C%2520and%2520match%2520all%2520other%2520known%2520sample%250Acomplexity%2520guarantees%2520of%2520omniprediction%2520and%2520multi-group%2520learning.%2520Our%2520key%250Atechnical%2520ingredient%2520is%2520a%2520nearly%2520lossless%2520reduction%2520from%2520panprediction%2520to%2520a%250Astatistically%2520efficient%2520notion%2520of%2520calibration%252C%2520called%2520step%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panprediction%3A%20Optimal%20Predictions%20for%20Any%20Downstream%20Task%20and%20Loss&entry.906535625=Sivaraman%20Balakrishnan%20and%20Nika%20Haghtalab%20and%20Daniel%20Hsu%20and%20Brian%20Lee%20and%20Eric%20Zhao&entry.1292438233=%20%20Supervised%20learning%20is%20classically%20formulated%20as%20training%20a%20model%20to%20minimize%0Aa%20fixed%20loss%20function%20over%20a%20fixed%20distribution%2C%20or%20task.%20However%2C%20an%20emerging%0Aparadigm%20instead%20views%20model%20training%20as%20extracting%20enough%20information%20from%0Adata%20so%20that%20the%20model%20can%20be%20used%20to%20minimize%20many%20losses%20on%20many%20downstream%0Atasks.%20We%20formalize%20a%20mathematical%20framework%20for%20this%20paradigm%2C%20which%20we%20call%0Apanprediction%2C%20and%20study%20its%20statistical%20complexity.%20Formally%2C%20panprediction%0Ageneralizes%20omniprediction%20and%20sits%20upstream%20from%20multi-group%20learning%2C%20which%0Arespectively%20focus%20on%20predictions%20that%20generalize%20to%20many%20downstream%20losses%20or%0Amany%20downstream%20tasks%2C%20but%20not%20both.%20Concretely%2C%20we%20design%20algorithms%20that%0Alearn%20deterministic%20and%20randomized%20panpredictors%20with%0A%24%5Ctilde%7BO%7D%281/%5Cvarepsilon%5E3%29%24%20and%20%24%5Ctilde%7BO%7D%281/%5Cvarepsilon%5E2%29%24%20samples%2C%0Arespectively.%20Our%20results%20demonstrate%20that%20under%20mild%20assumptions%2C%0Asimultaneously%20minimizing%20infinitely%20many%20losses%20on%20infinitely%20many%20tasks%20can%0Abe%20as%20statistically%20easy%20as%20minimizing%20one%20loss%20on%20one%20task.%20Along%20the%20way%2C%20we%0Aimprove%20the%20best%20known%20sample%20complexity%20guarantee%20of%20deterministic%0Aomniprediction%20by%20a%20factor%20of%20%241/%5Cvarepsilon%24%2C%20and%20match%20all%20other%20known%20sample%0Acomplexity%20guarantees%20of%20omniprediction%20and%20multi-group%20learning.%20Our%20key%0Atechnical%20ingredient%20is%20a%20nearly%20lossless%20reduction%20from%20panprediction%20to%20a%0Astatistically%20efficient%20notion%20of%20calibration%2C%20called%20step%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27638v1&entry.124074799=Read"},
{"title": "Data-Driven Stochastic Optimal Control in Reproducing Kernel Hilbert\n  Spaces", "author": "Nicolas Hoischen and Petar Bevanda and Stefan Sosnowski and Sandra Hirche and Boris Houska", "abstract": "  This paper proposes a fully data-driven approach for optimal control of\nnonlinear control-affine systems represented by a stochastic diffusion. The\nfocus is on the scenario where both the nonlinear dynamics and stage cost\nfunctions are unknown, while only a control penalty function and constraints\nare provided. To this end, we embed state probability densities into a\nreproducing kernel Hilbert space (RKHS) to leverage recent advances in operator\nregression, thereby identifying Markov transition operators associated with\ncontrolled diffusion processes. This operator learning approach integrates\nnaturally with convex operator-theoretic Hamilton-Jacobi-Bellman recursions\nthat scale linearly with state dimensionality, effectively solving a wide range\nof nonlinear optimal control problems. Numerical results demonstrate its\nability to address diverse nonlinear control tasks, including the depth\nregulation of an autonomous underwater vehicle.\n", "link": "http://arxiv.org/abs/2407.16407v2", "date": "2025-10-31", "relevancy": 1.9326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4861}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Stochastic%20Optimal%20Control%20in%20Reproducing%20Kernel%20Hilbert%0A%20%20Spaces&body=Title%3A%20Data-Driven%20Stochastic%20Optimal%20Control%20in%20Reproducing%20Kernel%20Hilbert%0A%20%20Spaces%0AAuthor%3A%20Nicolas%20Hoischen%20and%20Petar%20Bevanda%20and%20Stefan%20Sosnowski%20and%20Sandra%20Hirche%20and%20Boris%20Houska%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20fully%20data-driven%20approach%20for%20optimal%20control%20of%0Anonlinear%20control-affine%20systems%20represented%20by%20a%20stochastic%20diffusion.%20The%0Afocus%20is%20on%20the%20scenario%20where%20both%20the%20nonlinear%20dynamics%20and%20stage%20cost%0Afunctions%20are%20unknown%2C%20while%20only%20a%20control%20penalty%20function%20and%20constraints%0Aare%20provided.%20To%20this%20end%2C%20we%20embed%20state%20probability%20densities%20into%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20to%20leverage%20recent%20advances%20in%20operator%0Aregression%2C%20thereby%20identifying%20Markov%20transition%20operators%20associated%20with%0Acontrolled%20diffusion%20processes.%20This%20operator%20learning%20approach%20integrates%0Anaturally%20with%20convex%20operator-theoretic%20Hamilton-Jacobi-Bellman%20recursions%0Athat%20scale%20linearly%20with%20state%20dimensionality%2C%20effectively%20solving%20a%20wide%20range%0Aof%20nonlinear%20optimal%20control%20problems.%20Numerical%20results%20demonstrate%20its%0Aability%20to%20address%20diverse%20nonlinear%20control%20tasks%2C%20including%20the%20depth%0Aregulation%20of%20an%20autonomous%20underwater%20vehicle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Stochastic%2520Optimal%2520Control%2520in%2520Reproducing%2520Kernel%2520Hilbert%250A%2520%2520Spaces%26entry.906535625%3DNicolas%2520Hoischen%2520and%2520Petar%2520Bevanda%2520and%2520Stefan%2520Sosnowski%2520and%2520Sandra%2520Hirche%2520and%2520Boris%2520Houska%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520fully%2520data-driven%2520approach%2520for%2520optimal%2520control%2520of%250Anonlinear%2520control-affine%2520systems%2520represented%2520by%2520a%2520stochastic%2520diffusion.%2520The%250Afocus%2520is%2520on%2520the%2520scenario%2520where%2520both%2520the%2520nonlinear%2520dynamics%2520and%2520stage%2520cost%250Afunctions%2520are%2520unknown%252C%2520while%2520only%2520a%2520control%2520penalty%2520function%2520and%2520constraints%250Aare%2520provided.%2520To%2520this%2520end%252C%2520we%2520embed%2520state%2520probability%2520densities%2520into%2520a%250Areproducing%2520kernel%2520Hilbert%2520space%2520%2528RKHS%2529%2520to%2520leverage%2520recent%2520advances%2520in%2520operator%250Aregression%252C%2520thereby%2520identifying%2520Markov%2520transition%2520operators%2520associated%2520with%250Acontrolled%2520diffusion%2520processes.%2520This%2520operator%2520learning%2520approach%2520integrates%250Anaturally%2520with%2520convex%2520operator-theoretic%2520Hamilton-Jacobi-Bellman%2520recursions%250Athat%2520scale%2520linearly%2520with%2520state%2520dimensionality%252C%2520effectively%2520solving%2520a%2520wide%2520range%250Aof%2520nonlinear%2520optimal%2520control%2520problems.%2520Numerical%2520results%2520demonstrate%2520its%250Aability%2520to%2520address%2520diverse%2520nonlinear%2520control%2520tasks%252C%2520including%2520the%2520depth%250Aregulation%2520of%2520an%2520autonomous%2520underwater%2520vehicle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Stochastic%20Optimal%20Control%20in%20Reproducing%20Kernel%20Hilbert%0A%20%20Spaces&entry.906535625=Nicolas%20Hoischen%20and%20Petar%20Bevanda%20and%20Stefan%20Sosnowski%20and%20Sandra%20Hirche%20and%20Boris%20Houska&entry.1292438233=%20%20This%20paper%20proposes%20a%20fully%20data-driven%20approach%20for%20optimal%20control%20of%0Anonlinear%20control-affine%20systems%20represented%20by%20a%20stochastic%20diffusion.%20The%0Afocus%20is%20on%20the%20scenario%20where%20both%20the%20nonlinear%20dynamics%20and%20stage%20cost%0Afunctions%20are%20unknown%2C%20while%20only%20a%20control%20penalty%20function%20and%20constraints%0Aare%20provided.%20To%20this%20end%2C%20we%20embed%20state%20probability%20densities%20into%20a%0Areproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20to%20leverage%20recent%20advances%20in%20operator%0Aregression%2C%20thereby%20identifying%20Markov%20transition%20operators%20associated%20with%0Acontrolled%20diffusion%20processes.%20This%20operator%20learning%20approach%20integrates%0Anaturally%20with%20convex%20operator-theoretic%20Hamilton-Jacobi-Bellman%20recursions%0Athat%20scale%20linearly%20with%20state%20dimensionality%2C%20effectively%20solving%20a%20wide%20range%0Aof%20nonlinear%20optimal%20control%20problems.%20Numerical%20results%20demonstrate%20its%0Aability%20to%20address%20diverse%20nonlinear%20control%20tasks%2C%20including%20the%20depth%0Aregulation%20of%20an%20autonomous%20underwater%20vehicle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16407v2&entry.124074799=Read"},
{"title": "Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO\n  Satellite Networks", "author": "Ke He and Thang X. Vu and Le He and Lisheng Fan and Symeon Chatzinotas and Bjorn Ottersten", "abstract": "  The rise of ultra-dense LEO constellations creates a complex and asynchronous\nnetwork environment, driven by their massive scale, dynamic topologies, and\nsignificant delays. This unique complexity demands an adaptive packet routing\nalgorithm that is asynchronous, risk-aware, and capable of balancing diverse\nand often conflicting QoS objectives in a decentralized manner. However,\nexisting methods fail to address this need, as they typically rely on\nimpractical synchronous decision-making and/or risk-oblivious approaches. To\ntackle this gap, we introduce PRIMAL, an event-driven multi-agent routing\nframework designed specifically to allow each satellite to act independently on\nits own event-driven timeline, while managing the risk of worst-case\nperformance degradation via a principled primal-dual approach. This is achieved\nby enabling agents to learn the full cost distribution of the targeted QoS\nobjectives and constrain tail-end risks. Extensive simulations on a LEO\nconstellation with 1584 satellites validate its superiority in effectively\noptimizing latency and balancing load. Compared to a recent risk-oblivious\nbaseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms\nend-to-end delay reduction in loaded scenarios. This is accomplished by\nresolving the core conflict between naive shortest-path finding and congestion\navoidance, highlighting such autonomous risk-awareness as a key to robust\nrouting.\n", "link": "http://arxiv.org/abs/2510.27506v1", "date": "2025-10-31", "relevancy": 1.919, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Risk-Aware%20Multi-Agent%20Packet%20Routing%20for%20Ultra-Dense%20LEO%0A%20%20Satellite%20Networks&body=Title%3A%20Asynchronous%20Risk-Aware%20Multi-Agent%20Packet%20Routing%20for%20Ultra-Dense%20LEO%0A%20%20Satellite%20Networks%0AAuthor%3A%20Ke%20He%20and%20Thang%20X.%20Vu%20and%20Le%20He%20and%20Lisheng%20Fan%20and%20Symeon%20Chatzinotas%20and%20Bjorn%20Ottersten%0AAbstract%3A%20%20%20The%20rise%20of%20ultra-dense%20LEO%20constellations%20creates%20a%20complex%20and%20asynchronous%0Anetwork%20environment%2C%20driven%20by%20their%20massive%20scale%2C%20dynamic%20topologies%2C%20and%0Asignificant%20delays.%20This%20unique%20complexity%20demands%20an%20adaptive%20packet%20routing%0Aalgorithm%20that%20is%20asynchronous%2C%20risk-aware%2C%20and%20capable%20of%20balancing%20diverse%0Aand%20often%20conflicting%20QoS%20objectives%20in%20a%20decentralized%20manner.%20However%2C%0Aexisting%20methods%20fail%20to%20address%20this%20need%2C%20as%20they%20typically%20rely%20on%0Aimpractical%20synchronous%20decision-making%20and/or%20risk-oblivious%20approaches.%20To%0Atackle%20this%20gap%2C%20we%20introduce%20PRIMAL%2C%20an%20event-driven%20multi-agent%20routing%0Aframework%20designed%20specifically%20to%20allow%20each%20satellite%20to%20act%20independently%20on%0Aits%20own%20event-driven%20timeline%2C%20while%20managing%20the%20risk%20of%20worst-case%0Aperformance%20degradation%20via%20a%20principled%20primal-dual%20approach.%20This%20is%20achieved%0Aby%20enabling%20agents%20to%20learn%20the%20full%20cost%20distribution%20of%20the%20targeted%20QoS%0Aobjectives%20and%20constrain%20tail-end%20risks.%20Extensive%20simulations%20on%20a%20LEO%0Aconstellation%20with%201584%20satellites%20validate%20its%20superiority%20in%20effectively%0Aoptimizing%20latency%20and%20balancing%20load.%20Compared%20to%20a%20recent%20risk-oblivious%0Abaseline%2C%20it%20reduces%20queuing%20delay%20by%20over%2070%25%2C%20and%20achieves%20a%20nearly%2012%20ms%0Aend-to-end%20delay%20reduction%20in%20loaded%20scenarios.%20This%20is%20accomplished%20by%0Aresolving%20the%20core%20conflict%20between%20naive%20shortest-path%20finding%20and%20congestion%0Aavoidance%2C%20highlighting%20such%20autonomous%20risk-awareness%20as%20a%20key%20to%20robust%0Arouting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Risk-Aware%2520Multi-Agent%2520Packet%2520Routing%2520for%2520Ultra-Dense%2520LEO%250A%2520%2520Satellite%2520Networks%26entry.906535625%3DKe%2520He%2520and%2520Thang%2520X.%2520Vu%2520and%2520Le%2520He%2520and%2520Lisheng%2520Fan%2520and%2520Symeon%2520Chatzinotas%2520and%2520Bjorn%2520Ottersten%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520ultra-dense%2520LEO%2520constellations%2520creates%2520a%2520complex%2520and%2520asynchronous%250Anetwork%2520environment%252C%2520driven%2520by%2520their%2520massive%2520scale%252C%2520dynamic%2520topologies%252C%2520and%250Asignificant%2520delays.%2520This%2520unique%2520complexity%2520demands%2520an%2520adaptive%2520packet%2520routing%250Aalgorithm%2520that%2520is%2520asynchronous%252C%2520risk-aware%252C%2520and%2520capable%2520of%2520balancing%2520diverse%250Aand%2520often%2520conflicting%2520QoS%2520objectives%2520in%2520a%2520decentralized%2520manner.%2520However%252C%250Aexisting%2520methods%2520fail%2520to%2520address%2520this%2520need%252C%2520as%2520they%2520typically%2520rely%2520on%250Aimpractical%2520synchronous%2520decision-making%2520and/or%2520risk-oblivious%2520approaches.%2520To%250Atackle%2520this%2520gap%252C%2520we%2520introduce%2520PRIMAL%252C%2520an%2520event-driven%2520multi-agent%2520routing%250Aframework%2520designed%2520specifically%2520to%2520allow%2520each%2520satellite%2520to%2520act%2520independently%2520on%250Aits%2520own%2520event-driven%2520timeline%252C%2520while%2520managing%2520the%2520risk%2520of%2520worst-case%250Aperformance%2520degradation%2520via%2520a%2520principled%2520primal-dual%2520approach.%2520This%2520is%2520achieved%250Aby%2520enabling%2520agents%2520to%2520learn%2520the%2520full%2520cost%2520distribution%2520of%2520the%2520targeted%2520QoS%250Aobjectives%2520and%2520constrain%2520tail-end%2520risks.%2520Extensive%2520simulations%2520on%2520a%2520LEO%250Aconstellation%2520with%25201584%2520satellites%2520validate%2520its%2520superiority%2520in%2520effectively%250Aoptimizing%2520latency%2520and%2520balancing%2520load.%2520Compared%2520to%2520a%2520recent%2520risk-oblivious%250Abaseline%252C%2520it%2520reduces%2520queuing%2520delay%2520by%2520over%252070%2525%252C%2520and%2520achieves%2520a%2520nearly%252012%2520ms%250Aend-to-end%2520delay%2520reduction%2520in%2520loaded%2520scenarios.%2520This%2520is%2520accomplished%2520by%250Aresolving%2520the%2520core%2520conflict%2520between%2520naive%2520shortest-path%2520finding%2520and%2520congestion%250Aavoidance%252C%2520highlighting%2520such%2520autonomous%2520risk-awareness%2520as%2520a%2520key%2520to%2520robust%250Arouting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Risk-Aware%20Multi-Agent%20Packet%20Routing%20for%20Ultra-Dense%20LEO%0A%20%20Satellite%20Networks&entry.906535625=Ke%20He%20and%20Thang%20X.%20Vu%20and%20Le%20He%20and%20Lisheng%20Fan%20and%20Symeon%20Chatzinotas%20and%20Bjorn%20Ottersten&entry.1292438233=%20%20The%20rise%20of%20ultra-dense%20LEO%20constellations%20creates%20a%20complex%20and%20asynchronous%0Anetwork%20environment%2C%20driven%20by%20their%20massive%20scale%2C%20dynamic%20topologies%2C%20and%0Asignificant%20delays.%20This%20unique%20complexity%20demands%20an%20adaptive%20packet%20routing%0Aalgorithm%20that%20is%20asynchronous%2C%20risk-aware%2C%20and%20capable%20of%20balancing%20diverse%0Aand%20often%20conflicting%20QoS%20objectives%20in%20a%20decentralized%20manner.%20However%2C%0Aexisting%20methods%20fail%20to%20address%20this%20need%2C%20as%20they%20typically%20rely%20on%0Aimpractical%20synchronous%20decision-making%20and/or%20risk-oblivious%20approaches.%20To%0Atackle%20this%20gap%2C%20we%20introduce%20PRIMAL%2C%20an%20event-driven%20multi-agent%20routing%0Aframework%20designed%20specifically%20to%20allow%20each%20satellite%20to%20act%20independently%20on%0Aits%20own%20event-driven%20timeline%2C%20while%20managing%20the%20risk%20of%20worst-case%0Aperformance%20degradation%20via%20a%20principled%20primal-dual%20approach.%20This%20is%20achieved%0Aby%20enabling%20agents%20to%20learn%20the%20full%20cost%20distribution%20of%20the%20targeted%20QoS%0Aobjectives%20and%20constrain%20tail-end%20risks.%20Extensive%20simulations%20on%20a%20LEO%0Aconstellation%20with%201584%20satellites%20validate%20its%20superiority%20in%20effectively%0Aoptimizing%20latency%20and%20balancing%20load.%20Compared%20to%20a%20recent%20risk-oblivious%0Abaseline%2C%20it%20reduces%20queuing%20delay%20by%20over%2070%25%2C%20and%20achieves%20a%20nearly%2012%20ms%0Aend-to-end%20delay%20reduction%20in%20loaded%20scenarios.%20This%20is%20accomplished%20by%0Aresolving%20the%20core%20conflict%20between%20naive%20shortest-path%20finding%20and%20congestion%0Aavoidance%2C%20highlighting%20such%20autonomous%20risk-awareness%20as%20a%20key%20to%20robust%0Arouting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27506v1&entry.124074799=Read"},
{"title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A\n  Bias-Variance Trade-off", "author": "Muhammad Faraz Ul Abrar and Nicol\u00f2 Michelusi", "abstract": "  Over-the-air (OTA) federated learning (FL) has been well recognized as a\nscalable paradigm that exploits the waveform superposition of the wireless\nmultiple-access channel to aggregate model updates in a single use. Existing\nOTA-FL designs largely enforce zero-bias model updates by either assuming\n\\emph{homogeneous} wireless conditions (equal path loss across devices) or\nforcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous}\nwireless scenarios, however, such designs are constrained by the weakest device\nand inflate the update variance. Moreover, prior analyses of biased OTA-FL\nlargely address convex objectives, while most modern AI models are highly\nnon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient\ndescent (SGD) for general smooth non-convex objectives under wireless\nheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,\ntime-invariant model bias while facilitating reduced variance updates. We\nderive a finite-time stationarity bound (expected time average squared gradient\nnorm) that explicitly reveals a bias-variance trade-off. To optimize this\ntrade-off, we pose a non-convex joint OTA power-control design and develop an\nefficient successive convex approximation (SCA) algorithm that requires only\nstatistical CSI at the base station. Experiments on a non-convex image\nclassification task validate the approach: the SCA-based design accelerates\nconvergence via an optimized bias and improves generalization over prior OTA-FL\nbaselines.\n", "link": "http://arxiv.org/abs/2510.26722v2", "date": "2025-10-31", "relevancy": 1.8928, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off&body=Title%3A%20Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off%0AAuthor%3A%20Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi%0AAbstract%3A%20%20%20Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%0Ascalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%0Amultiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%0AOTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%0A%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%0Aforcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%0Awireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%0Aand%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%0Alargely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%0Anon-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%0Adescent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%0Aheterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%0Atime-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%0Aderive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%0Anorm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%0Atrade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%0Aefficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%0Astatistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%0Aclassification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%0Aconvergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Convex%2520Over-the-Air%2520Heterogeneous%2520Federated%2520Learning%253A%2520A%250A%2520%2520Bias-Variance%2520Trade-off%26entry.906535625%3DMuhammad%2520Faraz%2520Ul%2520Abrar%2520and%2520Nicol%25C3%25B2%2520Michelusi%26entry.1292438233%3D%2520%2520Over-the-air%2520%2528OTA%2529%2520federated%2520learning%2520%2528FL%2529%2520has%2520been%2520well%2520recognized%2520as%2520a%250Ascalable%2520paradigm%2520that%2520exploits%2520the%2520waveform%2520superposition%2520of%2520the%2520wireless%250Amultiple-access%2520channel%2520to%2520aggregate%2520model%2520updates%2520in%2520a%2520single%2520use.%2520Existing%250AOTA-FL%2520designs%2520largely%2520enforce%2520zero-bias%2520model%2520updates%2520by%2520either%2520assuming%250A%255Cemph%257Bhomogeneous%257D%2520wireless%2520conditions%2520%2528equal%2520path%2520loss%2520across%2520devices%2529%2520or%250Aforcing%2520zero-bias%2520updates%2520to%2520guarantee%2520convergence.%2520Under%2520%255Cemph%257Bheterogeneous%257D%250Awireless%2520scenarios%252C%2520however%252C%2520such%2520designs%2520are%2520constrained%2520by%2520the%2520weakest%2520device%250Aand%2520inflate%2520the%2520update%2520variance.%2520Moreover%252C%2520prior%2520analyses%2520of%2520biased%2520OTA-FL%250Alargely%2520address%2520convex%2520objectives%252C%2520while%2520most%2520modern%2520AI%2520models%2520are%2520highly%250Anon-convex.%2520Motivated%2520by%2520these%2520gaps%252C%2520we%2520study%2520OTA-FL%2520with%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520for%2520general%2520smooth%2520non-convex%2520objectives%2520under%2520wireless%250Aheterogeneity.%2520We%2520develop%2520novel%2520OTA-FL%2520SGD%2520updates%2520that%2520allow%2520a%2520structured%252C%250Atime-invariant%2520model%2520bias%2520while%2520facilitating%2520reduced%2520variance%2520updates.%2520We%250Aderive%2520a%2520finite-time%2520stationarity%2520bound%2520%2528expected%2520time%2520average%2520squared%2520gradient%250Anorm%2529%2520that%2520explicitly%2520reveals%2520a%2520bias-variance%2520trade-off.%2520To%2520optimize%2520this%250Atrade-off%252C%2520we%2520pose%2520a%2520non-convex%2520joint%2520OTA%2520power-control%2520design%2520and%2520develop%2520an%250Aefficient%2520successive%2520convex%2520approximation%2520%2528SCA%2529%2520algorithm%2520that%2520requires%2520only%250Astatistical%2520CSI%2520at%2520the%2520base%2520station.%2520Experiments%2520on%2520a%2520non-convex%2520image%250Aclassification%2520task%2520validate%2520the%2520approach%253A%2520the%2520SCA-based%2520design%2520accelerates%250Aconvergence%2520via%2520an%2520optimized%2520bias%2520and%2520improves%2520generalization%2520over%2520prior%2520OTA-FL%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Convex%20Over-the-Air%20Heterogeneous%20Federated%20Learning%3A%20A%0A%20%20Bias-Variance%20Trade-off&entry.906535625=Muhammad%20Faraz%20Ul%20Abrar%20and%20Nicol%C3%B2%20Michelusi&entry.1292438233=%20%20Over-the-air%20%28OTA%29%20federated%20learning%20%28FL%29%20has%20been%20well%20recognized%20as%20a%0Ascalable%20paradigm%20that%20exploits%20the%20waveform%20superposition%20of%20the%20wireless%0Amultiple-access%20channel%20to%20aggregate%20model%20updates%20in%20a%20single%20use.%20Existing%0AOTA-FL%20designs%20largely%20enforce%20zero-bias%20model%20updates%20by%20either%20assuming%0A%5Cemph%7Bhomogeneous%7D%20wireless%20conditions%20%28equal%20path%20loss%20across%20devices%29%20or%0Aforcing%20zero-bias%20updates%20to%20guarantee%20convergence.%20Under%20%5Cemph%7Bheterogeneous%7D%0Awireless%20scenarios%2C%20however%2C%20such%20designs%20are%20constrained%20by%20the%20weakest%20device%0Aand%20inflate%20the%20update%20variance.%20Moreover%2C%20prior%20analyses%20of%20biased%20OTA-FL%0Alargely%20address%20convex%20objectives%2C%20while%20most%20modern%20AI%20models%20are%20highly%0Anon-convex.%20Motivated%20by%20these%20gaps%2C%20we%20study%20OTA-FL%20with%20stochastic%20gradient%0Adescent%20%28SGD%29%20for%20general%20smooth%20non-convex%20objectives%20under%20wireless%0Aheterogeneity.%20We%20develop%20novel%20OTA-FL%20SGD%20updates%20that%20allow%20a%20structured%2C%0Atime-invariant%20model%20bias%20while%20facilitating%20reduced%20variance%20updates.%20We%0Aderive%20a%20finite-time%20stationarity%20bound%20%28expected%20time%20average%20squared%20gradient%0Anorm%29%20that%20explicitly%20reveals%20a%20bias-variance%20trade-off.%20To%20optimize%20this%0Atrade-off%2C%20we%20pose%20a%20non-convex%20joint%20OTA%20power-control%20design%20and%20develop%20an%0Aefficient%20successive%20convex%20approximation%20%28SCA%29%20algorithm%20that%20requires%20only%0Astatistical%20CSI%20at%20the%20base%20station.%20Experiments%20on%20a%20non-convex%20image%0Aclassification%20task%20validate%20the%20approach%3A%20the%20SCA-based%20design%20accelerates%0Aconvergence%20via%20an%20optimized%20bias%20and%20improves%20generalization%20over%20prior%20OTA-FL%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26722v2&entry.124074799=Read"},
{"title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive\n  Healthcare", "author": "Pengfei Hu and Xiaoxue Han and Fei Wang and Yue Ning", "abstract": "  Healthcare providers often divide patient populations into cohorts based on\nshared clinical factors, such as medical history, to deliver personalized\nhealthcare services. This idea has also been adopted in clinical prediction\nmodels, where it presents a vital challenge: capturing both global and\ncohort-specific patterns while enabling model generalization to unseen domains.\nAddressing this challenge falls under the scope of domain generalization (DG).\nHowever, conventional DG approaches often struggle in clinical settings due to\nthe absence of explicit domain labels and the inherent gap in medical\nknowledge. To address this, we propose UdonCare, a hierarchy-guided method that\niteratively divides patients into latent domains and decomposes\ndomain-invariant (label) information from patient data. Our method identifies\npatient domains by pruning medical ontologies (e.g. ICD-9-CM hierarchy). On two\npublic datasets, MIMIC-III and MIMIC-IV, UdonCare shows superiority over eight\nbaselines across four clinical prediction tasks with substantial domain gaps,\nhighlighting the untapped potential of medical knowledge in guiding clinical\ndomain generalization problems.\n", "link": "http://arxiv.org/abs/2506.06977v2", "date": "2025-10-31", "relevancy": 1.886, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4671}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UdonCare%3A%20Hierarchy%20Pruning%20for%20Unseen%20Domain%20Discovery%20in%20Predictive%0A%20%20Healthcare&body=Title%3A%20UdonCare%3A%20Hierarchy%20Pruning%20for%20Unseen%20Domain%20Discovery%20in%20Predictive%0A%20%20Healthcare%0AAuthor%3A%20Pengfei%20Hu%20and%20Xiaoxue%20Han%20and%20Fei%20Wang%20and%20Yue%20Ning%0AAbstract%3A%20%20%20Healthcare%20providers%20often%20divide%20patient%20populations%20into%20cohorts%20based%20on%0Ashared%20clinical%20factors%2C%20such%20as%20medical%20history%2C%20to%20deliver%20personalized%0Ahealthcare%20services.%20This%20idea%20has%20also%20been%20adopted%20in%20clinical%20prediction%0Amodels%2C%20where%20it%20presents%20a%20vital%20challenge%3A%20capturing%20both%20global%20and%0Acohort-specific%20patterns%20while%20enabling%20model%20generalization%20to%20unseen%20domains.%0AAddressing%20this%20challenge%20falls%20under%20the%20scope%20of%20domain%20generalization%20%28DG%29.%0AHowever%2C%20conventional%20DG%20approaches%20often%20struggle%20in%20clinical%20settings%20due%20to%0Athe%20absence%20of%20explicit%20domain%20labels%20and%20the%20inherent%20gap%20in%20medical%0Aknowledge.%20To%20address%20this%2C%20we%20propose%20UdonCare%2C%20a%20hierarchy-guided%20method%20that%0Aiteratively%20divides%20patients%20into%20latent%20domains%20and%20decomposes%0Adomain-invariant%20%28label%29%20information%20from%20patient%20data.%20Our%20method%20identifies%0Apatient%20domains%20by%20pruning%20medical%20ontologies%20%28e.g.%20ICD-9-CM%20hierarchy%29.%20On%20two%0Apublic%20datasets%2C%20MIMIC-III%20and%20MIMIC-IV%2C%20UdonCare%20shows%20superiority%20over%20eight%0Abaselines%20across%20four%20clinical%20prediction%20tasks%20with%20substantial%20domain%20gaps%2C%0Ahighlighting%20the%20untapped%20potential%20of%20medical%20knowledge%20in%20guiding%20clinical%0Adomain%20generalization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUdonCare%253A%2520Hierarchy%2520Pruning%2520for%2520Unseen%2520Domain%2520Discovery%2520in%2520Predictive%250A%2520%2520Healthcare%26entry.906535625%3DPengfei%2520Hu%2520and%2520Xiaoxue%2520Han%2520and%2520Fei%2520Wang%2520and%2520Yue%2520Ning%26entry.1292438233%3D%2520%2520Healthcare%2520providers%2520often%2520divide%2520patient%2520populations%2520into%2520cohorts%2520based%2520on%250Ashared%2520clinical%2520factors%252C%2520such%2520as%2520medical%2520history%252C%2520to%2520deliver%2520personalized%250Ahealthcare%2520services.%2520This%2520idea%2520has%2520also%2520been%2520adopted%2520in%2520clinical%2520prediction%250Amodels%252C%2520where%2520it%2520presents%2520a%2520vital%2520challenge%253A%2520capturing%2520both%2520global%2520and%250Acohort-specific%2520patterns%2520while%2520enabling%2520model%2520generalization%2520to%2520unseen%2520domains.%250AAddressing%2520this%2520challenge%2520falls%2520under%2520the%2520scope%2520of%2520domain%2520generalization%2520%2528DG%2529.%250AHowever%252C%2520conventional%2520DG%2520approaches%2520often%2520struggle%2520in%2520clinical%2520settings%2520due%2520to%250Athe%2520absence%2520of%2520explicit%2520domain%2520labels%2520and%2520the%2520inherent%2520gap%2520in%2520medical%250Aknowledge.%2520To%2520address%2520this%252C%2520we%2520propose%2520UdonCare%252C%2520a%2520hierarchy-guided%2520method%2520that%250Aiteratively%2520divides%2520patients%2520into%2520latent%2520domains%2520and%2520decomposes%250Adomain-invariant%2520%2528label%2529%2520information%2520from%2520patient%2520data.%2520Our%2520method%2520identifies%250Apatient%2520domains%2520by%2520pruning%2520medical%2520ontologies%2520%2528e.g.%2520ICD-9-CM%2520hierarchy%2529.%2520On%2520two%250Apublic%2520datasets%252C%2520MIMIC-III%2520and%2520MIMIC-IV%252C%2520UdonCare%2520shows%2520superiority%2520over%2520eight%250Abaselines%2520across%2520four%2520clinical%2520prediction%2520tasks%2520with%2520substantial%2520domain%2520gaps%252C%250Ahighlighting%2520the%2520untapped%2520potential%2520of%2520medical%2520knowledge%2520in%2520guiding%2520clinical%250Adomain%2520generalization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UdonCare%3A%20Hierarchy%20Pruning%20for%20Unseen%20Domain%20Discovery%20in%20Predictive%0A%20%20Healthcare&entry.906535625=Pengfei%20Hu%20and%20Xiaoxue%20Han%20and%20Fei%20Wang%20and%20Yue%20Ning&entry.1292438233=%20%20Healthcare%20providers%20often%20divide%20patient%20populations%20into%20cohorts%20based%20on%0Ashared%20clinical%20factors%2C%20such%20as%20medical%20history%2C%20to%20deliver%20personalized%0Ahealthcare%20services.%20This%20idea%20has%20also%20been%20adopted%20in%20clinical%20prediction%0Amodels%2C%20where%20it%20presents%20a%20vital%20challenge%3A%20capturing%20both%20global%20and%0Acohort-specific%20patterns%20while%20enabling%20model%20generalization%20to%20unseen%20domains.%0AAddressing%20this%20challenge%20falls%20under%20the%20scope%20of%20domain%20generalization%20%28DG%29.%0AHowever%2C%20conventional%20DG%20approaches%20often%20struggle%20in%20clinical%20settings%20due%20to%0Athe%20absence%20of%20explicit%20domain%20labels%20and%20the%20inherent%20gap%20in%20medical%0Aknowledge.%20To%20address%20this%2C%20we%20propose%20UdonCare%2C%20a%20hierarchy-guided%20method%20that%0Aiteratively%20divides%20patients%20into%20latent%20domains%20and%20decomposes%0Adomain-invariant%20%28label%29%20information%20from%20patient%20data.%20Our%20method%20identifies%0Apatient%20domains%20by%20pruning%20medical%20ontologies%20%28e.g.%20ICD-9-CM%20hierarchy%29.%20On%20two%0Apublic%20datasets%2C%20MIMIC-III%20and%20MIMIC-IV%2C%20UdonCare%20shows%20superiority%20over%20eight%0Abaselines%20across%20four%20clinical%20prediction%20tasks%20with%20substantial%20domain%20gaps%2C%0Ahighlighting%20the%20untapped%20potential%20of%20medical%20knowledge%20in%20guiding%20clinical%0Adomain%20generalization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06977v2&entry.124074799=Read"},
{"title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action\n  with Foundation Models via Scene Graphs", "author": "Sushil Samuel Dinesh and Shinkyu Park", "abstract": "  This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models.\n", "link": "http://arxiv.org/abs/2510.27558v1", "date": "2025-10-31", "relevancy": 1.8463, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.699}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Accurate%20Long-Horizon%20Robotic%20Manipulation%3A%20Language-to-Action%0A%20%20with%20Foundation%20Models%20via%20Scene%20Graphs&body=Title%3A%20Toward%20Accurate%20Long-Horizon%20Robotic%20Manipulation%3A%20Language-to-Action%0A%20%20with%20Foundation%20Models%20via%20Scene%20Graphs%0AAuthor%3A%20Sushil%20Samuel%20Dinesh%20and%20Shinkyu%20Park%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20that%20leverages%20pre-trained%20foundation%20models%0Afor%20robotic%20manipulation%20without%20domain-specific%20training.%20The%20framework%0Aintegrates%20off-the-shelf%20models%2C%20combining%20multimodal%20perception%20from%0Afoundation%20models%20with%20a%20general-purpose%20reasoning%20model%20capable%20of%20robust%20task%0Asequencing.%20Scene%20graphs%2C%20dynamically%20maintained%20within%20the%20framework%2C%20provide%0Aspatial%20awareness%20and%20enable%20consistent%20reasoning%20about%20the%20environment.%20The%0Aframework%20is%20evaluated%20through%20a%20series%20of%20tabletop%20robotic%20manipulation%0Aexperiments%2C%20and%20the%20results%20highlight%20its%20potential%20for%20building%20robotic%0Amanipulation%20systems%20directly%20on%20top%20of%20off-the-shelf%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Accurate%2520Long-Horizon%2520Robotic%2520Manipulation%253A%2520Language-to-Action%250A%2520%2520with%2520Foundation%2520Models%2520via%2520Scene%2520Graphs%26entry.906535625%3DSushil%2520Samuel%2520Dinesh%2520and%2520Shinkyu%2520Park%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520that%2520leverages%2520pre-trained%2520foundation%2520models%250Afor%2520robotic%2520manipulation%2520without%2520domain-specific%2520training.%2520The%2520framework%250Aintegrates%2520off-the-shelf%2520models%252C%2520combining%2520multimodal%2520perception%2520from%250Afoundation%2520models%2520with%2520a%2520general-purpose%2520reasoning%2520model%2520capable%2520of%2520robust%2520task%250Asequencing.%2520Scene%2520graphs%252C%2520dynamically%2520maintained%2520within%2520the%2520framework%252C%2520provide%250Aspatial%2520awareness%2520and%2520enable%2520consistent%2520reasoning%2520about%2520the%2520environment.%2520The%250Aframework%2520is%2520evaluated%2520through%2520a%2520series%2520of%2520tabletop%2520robotic%2520manipulation%250Aexperiments%252C%2520and%2520the%2520results%2520highlight%2520its%2520potential%2520for%2520building%2520robotic%250Amanipulation%2520systems%2520directly%2520on%2520top%2520of%2520off-the-shelf%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Accurate%20Long-Horizon%20Robotic%20Manipulation%3A%20Language-to-Action%0A%20%20with%20Foundation%20Models%20via%20Scene%20Graphs&entry.906535625=Sushil%20Samuel%20Dinesh%20and%20Shinkyu%20Park&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20that%20leverages%20pre-trained%20foundation%20models%0Afor%20robotic%20manipulation%20without%20domain-specific%20training.%20The%20framework%0Aintegrates%20off-the-shelf%20models%2C%20combining%20multimodal%20perception%20from%0Afoundation%20models%20with%20a%20general-purpose%20reasoning%20model%20capable%20of%20robust%20task%0Asequencing.%20Scene%20graphs%2C%20dynamically%20maintained%20within%20the%20framework%2C%20provide%0Aspatial%20awareness%20and%20enable%20consistent%20reasoning%20about%20the%20environment.%20The%0Aframework%20is%20evaluated%20through%20a%20series%20of%20tabletop%20robotic%20manipulation%0Aexperiments%2C%20and%20the%20results%20highlight%20its%20potential%20for%20building%20robotic%0Amanipulation%20systems%20directly%20on%20top%20of%20off-the-shelf%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27558v1&entry.124074799=Read"},
{"title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and\n  Multilingual Language Models", "author": "Malik H. Altakrori and Nizar Habash and Abdelhakim Freihat and Younes Samih and Kirill Chirkunov and Muhammed AbuOdeh and Radu Florian and Teresa Lynn and Preslav Nakov and Alham Fikri Aji", "abstract": "  We present DialectalArabicMMLU, a new benchmark for evaluating the\nperformance of large language models (LLMs) across Arabic dialects. While\nrecently developed Arabic and multilingual benchmarks have advanced LLM\nevaluation for Modern Standard Arabic (MSA), dialectal varieties remain\nunderrepresented despite their prevalence in everyday communication.\nDialectalArabicMMLU extends the MMLU-Redux framework through manual translation\nand adaptation of 3K multiple-choice question-answer pairs into five major\ndialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of\n15K QA pairs across 32 academic and professional domains (22K QA pairs when\nalso including English and MSA). The benchmark enables systematic assessment of\nLLM reasoning and comprehension beyond MSA, supporting both task-based and\nlinguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs\n(1B-13B parameters) and report substantial performance variation across\ndialects, revealing persistent gaps in dialectal generalization.\nDialectalArabicMMLU provides the first unified, human-curated resource for\nmeasuring dialectal understanding in Arabic, thus promoting more inclusive\nevaluation and future model development.\n", "link": "http://arxiv.org/abs/2510.27543v1", "date": "2025-10-31", "relevancy": 1.8428, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DialectalArabicMMLU%3A%20Benchmarking%20Dialectal%20Capabilities%20in%20Arabic%20and%0A%20%20Multilingual%20Language%20Models&body=Title%3A%20DialectalArabicMMLU%3A%20Benchmarking%20Dialectal%20Capabilities%20in%20Arabic%20and%0A%20%20Multilingual%20Language%20Models%0AAuthor%3A%20Malik%20H.%20Altakrori%20and%20Nizar%20Habash%20and%20Abdelhakim%20Freihat%20and%20Younes%20Samih%20and%20Kirill%20Chirkunov%20and%20Muhammed%20AbuOdeh%20and%20Radu%20Florian%20and%20Teresa%20Lynn%20and%20Preslav%20Nakov%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20We%20present%20DialectalArabicMMLU%2C%20a%20new%20benchmark%20for%20evaluating%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20across%20Arabic%20dialects.%20While%0Arecently%20developed%20Arabic%20and%20multilingual%20benchmarks%20have%20advanced%20LLM%0Aevaluation%20for%20Modern%20Standard%20Arabic%20%28MSA%29%2C%20dialectal%20varieties%20remain%0Aunderrepresented%20despite%20their%20prevalence%20in%20everyday%20communication.%0ADialectalArabicMMLU%20extends%20the%20MMLU-Redux%20framework%20through%20manual%20translation%0Aand%20adaptation%20of%203K%20multiple-choice%20question-answer%20pairs%20into%20five%20major%0Adialects%20%28Syrian%2C%20Egyptian%2C%20Emirati%2C%20Saudi%2C%20and%20Moroccan%29%2C%20yielding%20a%20total%20of%0A15K%20QA%20pairs%20across%2032%20academic%20and%20professional%20domains%20%2822K%20QA%20pairs%20when%0Aalso%20including%20English%20and%20MSA%29.%20The%20benchmark%20enables%20systematic%20assessment%20of%0ALLM%20reasoning%20and%20comprehension%20beyond%20MSA%2C%20supporting%20both%20task-based%20and%0Alinguistic%20analysis.%20We%20evaluate%2019%20open-weight%20Arabic%20and%20multilingual%20LLMs%0A%281B-13B%20parameters%29%20and%20report%20substantial%20performance%20variation%20across%0Adialects%2C%20revealing%20persistent%20gaps%20in%20dialectal%20generalization.%0ADialectalArabicMMLU%20provides%20the%20first%20unified%2C%20human-curated%20resource%20for%0Ameasuring%20dialectal%20understanding%20in%20Arabic%2C%20thus%20promoting%20more%20inclusive%0Aevaluation%20and%20future%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialectalArabicMMLU%253A%2520Benchmarking%2520Dialectal%2520Capabilities%2520in%2520Arabic%2520and%250A%2520%2520Multilingual%2520Language%2520Models%26entry.906535625%3DMalik%2520H.%2520Altakrori%2520and%2520Nizar%2520Habash%2520and%2520Abdelhakim%2520Freihat%2520and%2520Younes%2520Samih%2520and%2520Kirill%2520Chirkunov%2520and%2520Muhammed%2520AbuOdeh%2520and%2520Radu%2520Florian%2520and%2520Teresa%2520Lynn%2520and%2520Preslav%2520Nakov%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520We%2520present%2520DialectalArabicMMLU%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520the%250Aperformance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520Arabic%2520dialects.%2520While%250Arecently%2520developed%2520Arabic%2520and%2520multilingual%2520benchmarks%2520have%2520advanced%2520LLM%250Aevaluation%2520for%2520Modern%2520Standard%2520Arabic%2520%2528MSA%2529%252C%2520dialectal%2520varieties%2520remain%250Aunderrepresented%2520despite%2520their%2520prevalence%2520in%2520everyday%2520communication.%250ADialectalArabicMMLU%2520extends%2520the%2520MMLU-Redux%2520framework%2520through%2520manual%2520translation%250Aand%2520adaptation%2520of%25203K%2520multiple-choice%2520question-answer%2520pairs%2520into%2520five%2520major%250Adialects%2520%2528Syrian%252C%2520Egyptian%252C%2520Emirati%252C%2520Saudi%252C%2520and%2520Moroccan%2529%252C%2520yielding%2520a%2520total%2520of%250A15K%2520QA%2520pairs%2520across%252032%2520academic%2520and%2520professional%2520domains%2520%252822K%2520QA%2520pairs%2520when%250Aalso%2520including%2520English%2520and%2520MSA%2529.%2520The%2520benchmark%2520enables%2520systematic%2520assessment%2520of%250ALLM%2520reasoning%2520and%2520comprehension%2520beyond%2520MSA%252C%2520supporting%2520both%2520task-based%2520and%250Alinguistic%2520analysis.%2520We%2520evaluate%252019%2520open-weight%2520Arabic%2520and%2520multilingual%2520LLMs%250A%25281B-13B%2520parameters%2529%2520and%2520report%2520substantial%2520performance%2520variation%2520across%250Adialects%252C%2520revealing%2520persistent%2520gaps%2520in%2520dialectal%2520generalization.%250ADialectalArabicMMLU%2520provides%2520the%2520first%2520unified%252C%2520human-curated%2520resource%2520for%250Ameasuring%2520dialectal%2520understanding%2520in%2520Arabic%252C%2520thus%2520promoting%2520more%2520inclusive%250Aevaluation%2520and%2520future%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialectalArabicMMLU%3A%20Benchmarking%20Dialectal%20Capabilities%20in%20Arabic%20and%0A%20%20Multilingual%20Language%20Models&entry.906535625=Malik%20H.%20Altakrori%20and%20Nizar%20Habash%20and%20Abdelhakim%20Freihat%20and%20Younes%20Samih%20and%20Kirill%20Chirkunov%20and%20Muhammed%20AbuOdeh%20and%20Radu%20Florian%20and%20Teresa%20Lynn%20and%20Preslav%20Nakov%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20We%20present%20DialectalArabicMMLU%2C%20a%20new%20benchmark%20for%20evaluating%20the%0Aperformance%20of%20large%20language%20models%20%28LLMs%29%20across%20Arabic%20dialects.%20While%0Arecently%20developed%20Arabic%20and%20multilingual%20benchmarks%20have%20advanced%20LLM%0Aevaluation%20for%20Modern%20Standard%20Arabic%20%28MSA%29%2C%20dialectal%20varieties%20remain%0Aunderrepresented%20despite%20their%20prevalence%20in%20everyday%20communication.%0ADialectalArabicMMLU%20extends%20the%20MMLU-Redux%20framework%20through%20manual%20translation%0Aand%20adaptation%20of%203K%20multiple-choice%20question-answer%20pairs%20into%20five%20major%0Adialects%20%28Syrian%2C%20Egyptian%2C%20Emirati%2C%20Saudi%2C%20and%20Moroccan%29%2C%20yielding%20a%20total%20of%0A15K%20QA%20pairs%20across%2032%20academic%20and%20professional%20domains%20%2822K%20QA%20pairs%20when%0Aalso%20including%20English%20and%20MSA%29.%20The%20benchmark%20enables%20systematic%20assessment%20of%0ALLM%20reasoning%20and%20comprehension%20beyond%20MSA%2C%20supporting%20both%20task-based%20and%0Alinguistic%20analysis.%20We%20evaluate%2019%20open-weight%20Arabic%20and%20multilingual%20LLMs%0A%281B-13B%20parameters%29%20and%20report%20substantial%20performance%20variation%20across%0Adialects%2C%20revealing%20persistent%20gaps%20in%20dialectal%20generalization.%0ADialectalArabicMMLU%20provides%20the%20first%20unified%2C%20human-curated%20resource%20for%0Ameasuring%20dialectal%20understanding%20in%20Arabic%2C%20thus%20promoting%20more%20inclusive%0Aevaluation%20and%20future%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27543v1&entry.124074799=Read"},
{"title": "Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation", "author": "Riccardo Brioschi and Aleksandr Alekseev and Emanuele Nevali and Berkay D\u00f6ner and Omar El Malki and Blagoj Mitrevski and Leandro Kieliger and Mark Collier and Andrii Maksai and Jesse Berent and Claudiu Musat and Efi Kokiopoulou", "abstract": "  Graphic layout generation is a growing research area focusing on generating\naesthetically pleasing layouts ranging from poster designs to documents. While\nrecent research has explored ways to incorporate user constraints to guide the\nlayout generation, these constraints often require complex specifications which\nreduce usability. We introduce an innovative approach exploiting user-provided\nsketches as intuitive constraints and we demonstrate empirically the\neffectiveness of this new guidance method, establishing the sketch-to-layout\nproblem as a promising research direction, which is currently under-explored.\nTo tackle the sketch-to-layout problem, we propose a multimodal\ntransformer-based solution using the sketch and the content assets as inputs to\nproduce high quality layouts. Since collecting sketch training data from human\nannotators to train our model is very costly, we introduce a novel and\nefficient method to synthetically generate training sketches at scale. We train\nand evaluate our model on three publicly available datasets: PubLayNet,\nDocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art\nconstraint-based methods, while offering a more intuitive design experience. In\norder to facilitate future sketch-to-layout research, we release O(200k)\nsynthetically-generated sketches for the public datasets above. The datasets\nare available at https://github.com/google-deepmind/sketch_to_layout.\n", "link": "http://arxiv.org/abs/2510.27632v1", "date": "2025-10-31", "relevancy": 1.8402, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6565}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5647}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-to-Layout%3A%20Sketch-Guided%20Multimodal%20Layout%20Generation&body=Title%3A%20Sketch-to-Layout%3A%20Sketch-Guided%20Multimodal%20Layout%20Generation%0AAuthor%3A%20Riccardo%20Brioschi%20and%20Aleksandr%20Alekseev%20and%20Emanuele%20Nevali%20and%20Berkay%20D%C3%B6ner%20and%20Omar%20El%20Malki%20and%20Blagoj%20Mitrevski%20and%20Leandro%20Kieliger%20and%20Mark%20Collier%20and%20Andrii%20Maksai%20and%20Jesse%20Berent%20and%20Claudiu%20Musat%20and%20Efi%20Kokiopoulou%0AAbstract%3A%20%20%20Graphic%20layout%20generation%20is%20a%20growing%20research%20area%20focusing%20on%20generating%0Aaesthetically%20pleasing%20layouts%20ranging%20from%20poster%20designs%20to%20documents.%20While%0Arecent%20research%20has%20explored%20ways%20to%20incorporate%20user%20constraints%20to%20guide%20the%0Alayout%20generation%2C%20these%20constraints%20often%20require%20complex%20specifications%20which%0Areduce%20usability.%20We%20introduce%20an%20innovative%20approach%20exploiting%20user-provided%0Asketches%20as%20intuitive%20constraints%20and%20we%20demonstrate%20empirically%20the%0Aeffectiveness%20of%20this%20new%20guidance%20method%2C%20establishing%20the%20sketch-to-layout%0Aproblem%20as%20a%20promising%20research%20direction%2C%20which%20is%20currently%20under-explored.%0ATo%20tackle%20the%20sketch-to-layout%20problem%2C%20we%20propose%20a%20multimodal%0Atransformer-based%20solution%20using%20the%20sketch%20and%20the%20content%20assets%20as%20inputs%20to%0Aproduce%20high%20quality%20layouts.%20Since%20collecting%20sketch%20training%20data%20from%20human%0Aannotators%20to%20train%20our%20model%20is%20very%20costly%2C%20we%20introduce%20a%20novel%20and%0Aefficient%20method%20to%20synthetically%20generate%20training%20sketches%20at%20scale.%20We%20train%0Aand%20evaluate%20our%20model%20on%20three%20publicly%20available%20datasets%3A%20PubLayNet%2C%0ADocLayNet%20and%20SlidesVQA%2C%20demonstrating%20that%20it%20outperforms%20state-of-the-art%0Aconstraint-based%20methods%2C%20while%20offering%20a%20more%20intuitive%20design%20experience.%20In%0Aorder%20to%20facilitate%20future%20sketch-to-layout%20research%2C%20we%20release%20O%28200k%29%0Asynthetically-generated%20sketches%20for%20the%20public%20datasets%20above.%20The%20datasets%0Aare%20available%20at%20https%3A//github.com/google-deepmind/sketch_to_layout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-to-Layout%253A%2520Sketch-Guided%2520Multimodal%2520Layout%2520Generation%26entry.906535625%3DRiccardo%2520Brioschi%2520and%2520Aleksandr%2520Alekseev%2520and%2520Emanuele%2520Nevali%2520and%2520Berkay%2520D%25C3%25B6ner%2520and%2520Omar%2520El%2520Malki%2520and%2520Blagoj%2520Mitrevski%2520and%2520Leandro%2520Kieliger%2520and%2520Mark%2520Collier%2520and%2520Andrii%2520Maksai%2520and%2520Jesse%2520Berent%2520and%2520Claudiu%2520Musat%2520and%2520Efi%2520Kokiopoulou%26entry.1292438233%3D%2520%2520Graphic%2520layout%2520generation%2520is%2520a%2520growing%2520research%2520area%2520focusing%2520on%2520generating%250Aaesthetically%2520pleasing%2520layouts%2520ranging%2520from%2520poster%2520designs%2520to%2520documents.%2520While%250Arecent%2520research%2520has%2520explored%2520ways%2520to%2520incorporate%2520user%2520constraints%2520to%2520guide%2520the%250Alayout%2520generation%252C%2520these%2520constraints%2520often%2520require%2520complex%2520specifications%2520which%250Areduce%2520usability.%2520We%2520introduce%2520an%2520innovative%2520approach%2520exploiting%2520user-provided%250Asketches%2520as%2520intuitive%2520constraints%2520and%2520we%2520demonstrate%2520empirically%2520the%250Aeffectiveness%2520of%2520this%2520new%2520guidance%2520method%252C%2520establishing%2520the%2520sketch-to-layout%250Aproblem%2520as%2520a%2520promising%2520research%2520direction%252C%2520which%2520is%2520currently%2520under-explored.%250ATo%2520tackle%2520the%2520sketch-to-layout%2520problem%252C%2520we%2520propose%2520a%2520multimodal%250Atransformer-based%2520solution%2520using%2520the%2520sketch%2520and%2520the%2520content%2520assets%2520as%2520inputs%2520to%250Aproduce%2520high%2520quality%2520layouts.%2520Since%2520collecting%2520sketch%2520training%2520data%2520from%2520human%250Aannotators%2520to%2520train%2520our%2520model%2520is%2520very%2520costly%252C%2520we%2520introduce%2520a%2520novel%2520and%250Aefficient%2520method%2520to%2520synthetically%2520generate%2520training%2520sketches%2520at%2520scale.%2520We%2520train%250Aand%2520evaluate%2520our%2520model%2520on%2520three%2520publicly%2520available%2520datasets%253A%2520PubLayNet%252C%250ADocLayNet%2520and%2520SlidesVQA%252C%2520demonstrating%2520that%2520it%2520outperforms%2520state-of-the-art%250Aconstraint-based%2520methods%252C%2520while%2520offering%2520a%2520more%2520intuitive%2520design%2520experience.%2520In%250Aorder%2520to%2520facilitate%2520future%2520sketch-to-layout%2520research%252C%2520we%2520release%2520O%2528200k%2529%250Asynthetically-generated%2520sketches%2520for%2520the%2520public%2520datasets%2520above.%2520The%2520datasets%250Aare%2520available%2520at%2520https%253A//github.com/google-deepmind/sketch_to_layout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-to-Layout%3A%20Sketch-Guided%20Multimodal%20Layout%20Generation&entry.906535625=Riccardo%20Brioschi%20and%20Aleksandr%20Alekseev%20and%20Emanuele%20Nevali%20and%20Berkay%20D%C3%B6ner%20and%20Omar%20El%20Malki%20and%20Blagoj%20Mitrevski%20and%20Leandro%20Kieliger%20and%20Mark%20Collier%20and%20Andrii%20Maksai%20and%20Jesse%20Berent%20and%20Claudiu%20Musat%20and%20Efi%20Kokiopoulou&entry.1292438233=%20%20Graphic%20layout%20generation%20is%20a%20growing%20research%20area%20focusing%20on%20generating%0Aaesthetically%20pleasing%20layouts%20ranging%20from%20poster%20designs%20to%20documents.%20While%0Arecent%20research%20has%20explored%20ways%20to%20incorporate%20user%20constraints%20to%20guide%20the%0Alayout%20generation%2C%20these%20constraints%20often%20require%20complex%20specifications%20which%0Areduce%20usability.%20We%20introduce%20an%20innovative%20approach%20exploiting%20user-provided%0Asketches%20as%20intuitive%20constraints%20and%20we%20demonstrate%20empirically%20the%0Aeffectiveness%20of%20this%20new%20guidance%20method%2C%20establishing%20the%20sketch-to-layout%0Aproblem%20as%20a%20promising%20research%20direction%2C%20which%20is%20currently%20under-explored.%0ATo%20tackle%20the%20sketch-to-layout%20problem%2C%20we%20propose%20a%20multimodal%0Atransformer-based%20solution%20using%20the%20sketch%20and%20the%20content%20assets%20as%20inputs%20to%0Aproduce%20high%20quality%20layouts.%20Since%20collecting%20sketch%20training%20data%20from%20human%0Aannotators%20to%20train%20our%20model%20is%20very%20costly%2C%20we%20introduce%20a%20novel%20and%0Aefficient%20method%20to%20synthetically%20generate%20training%20sketches%20at%20scale.%20We%20train%0Aand%20evaluate%20our%20model%20on%20three%20publicly%20available%20datasets%3A%20PubLayNet%2C%0ADocLayNet%20and%20SlidesVQA%2C%20demonstrating%20that%20it%20outperforms%20state-of-the-art%0Aconstraint-based%20methods%2C%20while%20offering%20a%20more%20intuitive%20design%20experience.%20In%0Aorder%20to%20facilitate%20future%20sketch-to-layout%20research%2C%20we%20release%20O%28200k%29%0Asynthetically-generated%20sketches%20for%20the%20public%20datasets%20above.%20The%20datasets%0Aare%20available%20at%20https%3A//github.com/google-deepmind/sketch_to_layout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27632v1&entry.124074799=Read"},
{"title": "Bayesian Optimization on Networks", "author": "Wenwen Li and Daniel Sanz-Alonso and Ruiyi Yang", "abstract": "  This paper studies optimization on networks modeled as metric graphs.\nMotivated by applications where the objective function is expensive to evaluate\nor only available as a black box, we develop Bayesian optimization algorithms\nthat sequentially update a Gaussian process surrogate model of the objective to\nguide the acquisition of query points. To ensure that the surrogates are\ntailored to the network's geometry, we adopt Whittle-Mat\\'ern Gaussian process\nprior models defined via stochastic partial differential equations on metric\ngraphs. In addition to establishing regret bounds for optimizing sufficiently\nsmooth objective functions, we analyze the practical case in which the\nsmoothness of the objective is unknown and the Whittle-Mat\\'ern prior is\nrepresented using finite elements. Numerical results demonstrate the\neffectiveness of our algorithms for optimizing benchmark objective functions on\na synthetic metric graph and for Bayesian inversion via maximum a posteriori\nestimation on a telecommunication network.\n", "link": "http://arxiv.org/abs/2510.27643v1", "date": "2025-10-31", "relevancy": 1.8401, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20on%20Networks&body=Title%3A%20Bayesian%20Optimization%20on%20Networks%0AAuthor%3A%20Wenwen%20Li%20and%20Daniel%20Sanz-Alonso%20and%20Ruiyi%20Yang%0AAbstract%3A%20%20%20This%20paper%20studies%20optimization%20on%20networks%20modeled%20as%20metric%20graphs.%0AMotivated%20by%20applications%20where%20the%20objective%20function%20is%20expensive%20to%20evaluate%0Aor%20only%20available%20as%20a%20black%20box%2C%20we%20develop%20Bayesian%20optimization%20algorithms%0Athat%20sequentially%20update%20a%20Gaussian%20process%20surrogate%20model%20of%20the%20objective%20to%0Aguide%20the%20acquisition%20of%20query%20points.%20To%20ensure%20that%20the%20surrogates%20are%0Atailored%20to%20the%20network%27s%20geometry%2C%20we%20adopt%20Whittle-Mat%5C%27ern%20Gaussian%20process%0Aprior%20models%20defined%20via%20stochastic%20partial%20differential%20equations%20on%20metric%0Agraphs.%20In%20addition%20to%20establishing%20regret%20bounds%20for%20optimizing%20sufficiently%0Asmooth%20objective%20functions%2C%20we%20analyze%20the%20practical%20case%20in%20which%20the%0Asmoothness%20of%20the%20objective%20is%20unknown%20and%20the%20Whittle-Mat%5C%27ern%20prior%20is%0Arepresented%20using%20finite%20elements.%20Numerical%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithms%20for%20optimizing%20benchmark%20objective%20functions%20on%0Aa%20synthetic%20metric%20graph%20and%20for%20Bayesian%20inversion%20via%20maximum%20a%20posteriori%0Aestimation%20on%20a%20telecommunication%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520on%2520Networks%26entry.906535625%3DWenwen%2520Li%2520and%2520Daniel%2520Sanz-Alonso%2520and%2520Ruiyi%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520optimization%2520on%2520networks%2520modeled%2520as%2520metric%2520graphs.%250AMotivated%2520by%2520applications%2520where%2520the%2520objective%2520function%2520is%2520expensive%2520to%2520evaluate%250Aor%2520only%2520available%2520as%2520a%2520black%2520box%252C%2520we%2520develop%2520Bayesian%2520optimization%2520algorithms%250Athat%2520sequentially%2520update%2520a%2520Gaussian%2520process%2520surrogate%2520model%2520of%2520the%2520objective%2520to%250Aguide%2520the%2520acquisition%2520of%2520query%2520points.%2520To%2520ensure%2520that%2520the%2520surrogates%2520are%250Atailored%2520to%2520the%2520network%2527s%2520geometry%252C%2520we%2520adopt%2520Whittle-Mat%255C%2527ern%2520Gaussian%2520process%250Aprior%2520models%2520defined%2520via%2520stochastic%2520partial%2520differential%2520equations%2520on%2520metric%250Agraphs.%2520In%2520addition%2520to%2520establishing%2520regret%2520bounds%2520for%2520optimizing%2520sufficiently%250Asmooth%2520objective%2520functions%252C%2520we%2520analyze%2520the%2520practical%2520case%2520in%2520which%2520the%250Asmoothness%2520of%2520the%2520objective%2520is%2520unknown%2520and%2520the%2520Whittle-Mat%255C%2527ern%2520prior%2520is%250Arepresented%2520using%2520finite%2520elements.%2520Numerical%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520algorithms%2520for%2520optimizing%2520benchmark%2520objective%2520functions%2520on%250Aa%2520synthetic%2520metric%2520graph%2520and%2520for%2520Bayesian%2520inversion%2520via%2520maximum%2520a%2520posteriori%250Aestimation%2520on%2520a%2520telecommunication%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20on%20Networks&entry.906535625=Wenwen%20Li%20and%20Daniel%20Sanz-Alonso%20and%20Ruiyi%20Yang&entry.1292438233=%20%20This%20paper%20studies%20optimization%20on%20networks%20modeled%20as%20metric%20graphs.%0AMotivated%20by%20applications%20where%20the%20objective%20function%20is%20expensive%20to%20evaluate%0Aor%20only%20available%20as%20a%20black%20box%2C%20we%20develop%20Bayesian%20optimization%20algorithms%0Athat%20sequentially%20update%20a%20Gaussian%20process%20surrogate%20model%20of%20the%20objective%20to%0Aguide%20the%20acquisition%20of%20query%20points.%20To%20ensure%20that%20the%20surrogates%20are%0Atailored%20to%20the%20network%27s%20geometry%2C%20we%20adopt%20Whittle-Mat%5C%27ern%20Gaussian%20process%0Aprior%20models%20defined%20via%20stochastic%20partial%20differential%20equations%20on%20metric%0Agraphs.%20In%20addition%20to%20establishing%20regret%20bounds%20for%20optimizing%20sufficiently%0Asmooth%20objective%20functions%2C%20we%20analyze%20the%20practical%20case%20in%20which%20the%0Asmoothness%20of%20the%20objective%20is%20unknown%20and%20the%20Whittle-Mat%5C%27ern%20prior%20is%0Arepresented%20using%20finite%20elements.%20Numerical%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20algorithms%20for%20optimizing%20benchmark%20objective%20functions%20on%0Aa%20synthetic%20metric%20graph%20and%20for%20Bayesian%20inversion%20via%20maximum%20a%20posteriori%0Aestimation%20on%20a%20telecommunication%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27643v1&entry.124074799=Read"},
{"title": "DO-IQS: Dynamics-Aware Offline Inverse Q-Learning for Optimal Stopping\n  with Unknown Gain Functions", "author": "Anna Kuchko", "abstract": "  We consider the Inverse Optimal Stopping (IOS) problem where, based on\nstopped expert trajectories, one aims to recover the optimal stopping region\nthrough the continuation and stopping gain functions approximation. The\nuniqueness of the stopping region allows the use of IOS in real-world\napplications with safety concerns. Although current state-of-the-art inverse\nreinforcement learning methods recover both a Q-function and the corresponding\noptimal policy, they fail to account for specific challenges posed by optimal\nstopping problems. These include data sparsity near the stopping region, the\nnon-Markovian nature of the continuation gain, a proper treatment of boundary\nconditions, the need for a stable offline approach for risk-sensitive\napplications, and a lack of a quality evaluation metric. These challenges are\naddressed with the proposed Dynamics-Aware Offline Inverse Q-Learning for\nOptimal Stopping (DO-IQS), which incorporates temporal information by\napproximating the cumulative continuation gain together with the world dynamics\nand the Q-function without querying to the environment. In addition, a\nconfidence-based oversampling approach is proposed to treat the data sparsity\nproblem. We demonstrate the performance of our models on real and artificial\ndata including an optimal intervention for the critical events problem.\n", "link": "http://arxiv.org/abs/2503.03515v2", "date": "2025-10-31", "relevancy": 1.8124, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DO-IQS%3A%20Dynamics-Aware%20Offline%20Inverse%20Q-Learning%20for%20Optimal%20Stopping%0A%20%20with%20Unknown%20Gain%20Functions&body=Title%3A%20DO-IQS%3A%20Dynamics-Aware%20Offline%20Inverse%20Q-Learning%20for%20Optimal%20Stopping%0A%20%20with%20Unknown%20Gain%20Functions%0AAuthor%3A%20Anna%20Kuchko%0AAbstract%3A%20%20%20We%20consider%20the%20Inverse%20Optimal%20Stopping%20%28IOS%29%20problem%20where%2C%20based%20on%0Astopped%20expert%20trajectories%2C%20one%20aims%20to%20recover%20the%20optimal%20stopping%20region%0Athrough%20the%20continuation%20and%20stopping%20gain%20functions%20approximation.%20The%0Auniqueness%20of%20the%20stopping%20region%20allows%20the%20use%20of%20IOS%20in%20real-world%0Aapplications%20with%20safety%20concerns.%20Although%20current%20state-of-the-art%20inverse%0Areinforcement%20learning%20methods%20recover%20both%20a%20Q-function%20and%20the%20corresponding%0Aoptimal%20policy%2C%20they%20fail%20to%20account%20for%20specific%20challenges%20posed%20by%20optimal%0Astopping%20problems.%20These%20include%20data%20sparsity%20near%20the%20stopping%20region%2C%20the%0Anon-Markovian%20nature%20of%20the%20continuation%20gain%2C%20a%20proper%20treatment%20of%20boundary%0Aconditions%2C%20the%20need%20for%20a%20stable%20offline%20approach%20for%20risk-sensitive%0Aapplications%2C%20and%20a%20lack%20of%20a%20quality%20evaluation%20metric.%20These%20challenges%20are%0Aaddressed%20with%20the%20proposed%20Dynamics-Aware%20Offline%20Inverse%20Q-Learning%20for%0AOptimal%20Stopping%20%28DO-IQS%29%2C%20which%20incorporates%20temporal%20information%20by%0Aapproximating%20the%20cumulative%20continuation%20gain%20together%20with%20the%20world%20dynamics%0Aand%20the%20Q-function%20without%20querying%20to%20the%20environment.%20In%20addition%2C%20a%0Aconfidence-based%20oversampling%20approach%20is%20proposed%20to%20treat%20the%20data%20sparsity%0Aproblem.%20We%20demonstrate%20the%20performance%20of%20our%20models%20on%20real%20and%20artificial%0Adata%20including%20an%20optimal%20intervention%20for%20the%20critical%20events%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDO-IQS%253A%2520Dynamics-Aware%2520Offline%2520Inverse%2520Q-Learning%2520for%2520Optimal%2520Stopping%250A%2520%2520with%2520Unknown%2520Gain%2520Functions%26entry.906535625%3DAnna%2520Kuchko%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520Inverse%2520Optimal%2520Stopping%2520%2528IOS%2529%2520problem%2520where%252C%2520based%2520on%250Astopped%2520expert%2520trajectories%252C%2520one%2520aims%2520to%2520recover%2520the%2520optimal%2520stopping%2520region%250Athrough%2520the%2520continuation%2520and%2520stopping%2520gain%2520functions%2520approximation.%2520The%250Auniqueness%2520of%2520the%2520stopping%2520region%2520allows%2520the%2520use%2520of%2520IOS%2520in%2520real-world%250Aapplications%2520with%2520safety%2520concerns.%2520Although%2520current%2520state-of-the-art%2520inverse%250Areinforcement%2520learning%2520methods%2520recover%2520both%2520a%2520Q-function%2520and%2520the%2520corresponding%250Aoptimal%2520policy%252C%2520they%2520fail%2520to%2520account%2520for%2520specific%2520challenges%2520posed%2520by%2520optimal%250Astopping%2520problems.%2520These%2520include%2520data%2520sparsity%2520near%2520the%2520stopping%2520region%252C%2520the%250Anon-Markovian%2520nature%2520of%2520the%2520continuation%2520gain%252C%2520a%2520proper%2520treatment%2520of%2520boundary%250Aconditions%252C%2520the%2520need%2520for%2520a%2520stable%2520offline%2520approach%2520for%2520risk-sensitive%250Aapplications%252C%2520and%2520a%2520lack%2520of%2520a%2520quality%2520evaluation%2520metric.%2520These%2520challenges%2520are%250Aaddressed%2520with%2520the%2520proposed%2520Dynamics-Aware%2520Offline%2520Inverse%2520Q-Learning%2520for%250AOptimal%2520Stopping%2520%2528DO-IQS%2529%252C%2520which%2520incorporates%2520temporal%2520information%2520by%250Aapproximating%2520the%2520cumulative%2520continuation%2520gain%2520together%2520with%2520the%2520world%2520dynamics%250Aand%2520the%2520Q-function%2520without%2520querying%2520to%2520the%2520environment.%2520In%2520addition%252C%2520a%250Aconfidence-based%2520oversampling%2520approach%2520is%2520proposed%2520to%2520treat%2520the%2520data%2520sparsity%250Aproblem.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%2520models%2520on%2520real%2520and%2520artificial%250Adata%2520including%2520an%2520optimal%2520intervention%2520for%2520the%2520critical%2520events%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DO-IQS%3A%20Dynamics-Aware%20Offline%20Inverse%20Q-Learning%20for%20Optimal%20Stopping%0A%20%20with%20Unknown%20Gain%20Functions&entry.906535625=Anna%20Kuchko&entry.1292438233=%20%20We%20consider%20the%20Inverse%20Optimal%20Stopping%20%28IOS%29%20problem%20where%2C%20based%20on%0Astopped%20expert%20trajectories%2C%20one%20aims%20to%20recover%20the%20optimal%20stopping%20region%0Athrough%20the%20continuation%20and%20stopping%20gain%20functions%20approximation.%20The%0Auniqueness%20of%20the%20stopping%20region%20allows%20the%20use%20of%20IOS%20in%20real-world%0Aapplications%20with%20safety%20concerns.%20Although%20current%20state-of-the-art%20inverse%0Areinforcement%20learning%20methods%20recover%20both%20a%20Q-function%20and%20the%20corresponding%0Aoptimal%20policy%2C%20they%20fail%20to%20account%20for%20specific%20challenges%20posed%20by%20optimal%0Astopping%20problems.%20These%20include%20data%20sparsity%20near%20the%20stopping%20region%2C%20the%0Anon-Markovian%20nature%20of%20the%20continuation%20gain%2C%20a%20proper%20treatment%20of%20boundary%0Aconditions%2C%20the%20need%20for%20a%20stable%20offline%20approach%20for%20risk-sensitive%0Aapplications%2C%20and%20a%20lack%20of%20a%20quality%20evaluation%20metric.%20These%20challenges%20are%0Aaddressed%20with%20the%20proposed%20Dynamics-Aware%20Offline%20Inverse%20Q-Learning%20for%0AOptimal%20Stopping%20%28DO-IQS%29%2C%20which%20incorporates%20temporal%20information%20by%0Aapproximating%20the%20cumulative%20continuation%20gain%20together%20with%20the%20world%20dynamics%0Aand%20the%20Q-function%20without%20querying%20to%20the%20environment.%20In%20addition%2C%20a%0Aconfidence-based%20oversampling%20approach%20is%20proposed%20to%20treat%20the%20data%20sparsity%0Aproblem.%20We%20demonstrate%20the%20performance%20of%20our%20models%20on%20real%20and%20artificial%0Adata%20including%20an%20optimal%20intervention%20for%20the%20critical%20events%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03515v2&entry.124074799=Read"},
{"title": "LifWavNet: Lifting Wavelet-based Network for Non-contact ECG\n  Reconstruction from Radar", "author": "Soumitra Kundu and Gargi Panda and Saumik Bhattacharya and Aurobinda Routray and Rajlakshmi Guha", "abstract": "  Non-contact electrocardiogram (ECG) reconstruction from radar signals offers\na promising approach for unobtrusive cardiac monitoring. We present LifWavNet,\na lifting wavelet network based on a multi-resolution analysis and synthesis\n(MRAS) model for radar-to-ECG reconstruction. Unlike prior models that use\nfixed wavelet approaches, LifWavNet employs learnable lifting wavelets with\nlifting and inverse lifting units to adaptively capture radar signal features\nand synthesize physiologically meaningful ECG waveforms. To improve\nreconstruction fidelity, we introduce a multi-resolution short-time Fourier\ntransform (STFT) loss, that enforces consistency with the ground-truth ECG in\nboth temporal and spectral domains. Evaluations on two public datasets\ndemonstrate that LifWavNet outperforms state-of-the-art methods in ECG\nreconstruction and downstream vital sign estimation (heart rate and heart rate\nvariability). Furthermore, intermediate feature visualization highlights the\ninterpretability of multi-resolution decomposition and synthesis in\nradar-to-ECG reconstruction. These results establish LifWavNet as a robust\nframework for radar-based non-contact ECG measurement.\n", "link": "http://arxiv.org/abs/2510.27692v1", "date": "2025-10-31", "relevancy": 1.806, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.446}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LifWavNet%3A%20Lifting%20Wavelet-based%20Network%20for%20Non-contact%20ECG%0A%20%20Reconstruction%20from%20Radar&body=Title%3A%20LifWavNet%3A%20Lifting%20Wavelet-based%20Network%20for%20Non-contact%20ECG%0A%20%20Reconstruction%20from%20Radar%0AAuthor%3A%20Soumitra%20Kundu%20and%20Gargi%20Panda%20and%20Saumik%20Bhattacharya%20and%20Aurobinda%20Routray%20and%20Rajlakshmi%20Guha%0AAbstract%3A%20%20%20Non-contact%20electrocardiogram%20%28ECG%29%20reconstruction%20from%20radar%20signals%20offers%0Aa%20promising%20approach%20for%20unobtrusive%20cardiac%20monitoring.%20We%20present%20LifWavNet%2C%0Aa%20lifting%20wavelet%20network%20based%20on%20a%20multi-resolution%20analysis%20and%20synthesis%0A%28MRAS%29%20model%20for%20radar-to-ECG%20reconstruction.%20Unlike%20prior%20models%20that%20use%0Afixed%20wavelet%20approaches%2C%20LifWavNet%20employs%20learnable%20lifting%20wavelets%20with%0Alifting%20and%20inverse%20lifting%20units%20to%20adaptively%20capture%20radar%20signal%20features%0Aand%20synthesize%20physiologically%20meaningful%20ECG%20waveforms.%20To%20improve%0Areconstruction%20fidelity%2C%20we%20introduce%20a%20multi-resolution%20short-time%20Fourier%0Atransform%20%28STFT%29%20loss%2C%20that%20enforces%20consistency%20with%20the%20ground-truth%20ECG%20in%0Aboth%20temporal%20and%20spectral%20domains.%20Evaluations%20on%20two%20public%20datasets%0Ademonstrate%20that%20LifWavNet%20outperforms%20state-of-the-art%20methods%20in%20ECG%0Areconstruction%20and%20downstream%20vital%20sign%20estimation%20%28heart%20rate%20and%20heart%20rate%0Avariability%29.%20Furthermore%2C%20intermediate%20feature%20visualization%20highlights%20the%0Ainterpretability%20of%20multi-resolution%20decomposition%20and%20synthesis%20in%0Aradar-to-ECG%20reconstruction.%20These%20results%20establish%20LifWavNet%20as%20a%20robust%0Aframework%20for%20radar-based%20non-contact%20ECG%20measurement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifWavNet%253A%2520Lifting%2520Wavelet-based%2520Network%2520for%2520Non-contact%2520ECG%250A%2520%2520Reconstruction%2520from%2520Radar%26entry.906535625%3DSoumitra%2520Kundu%2520and%2520Gargi%2520Panda%2520and%2520Saumik%2520Bhattacharya%2520and%2520Aurobinda%2520Routray%2520and%2520Rajlakshmi%2520Guha%26entry.1292438233%3D%2520%2520Non-contact%2520electrocardiogram%2520%2528ECG%2529%2520reconstruction%2520from%2520radar%2520signals%2520offers%250Aa%2520promising%2520approach%2520for%2520unobtrusive%2520cardiac%2520monitoring.%2520We%2520present%2520LifWavNet%252C%250Aa%2520lifting%2520wavelet%2520network%2520based%2520on%2520a%2520multi-resolution%2520analysis%2520and%2520synthesis%250A%2528MRAS%2529%2520model%2520for%2520radar-to-ECG%2520reconstruction.%2520Unlike%2520prior%2520models%2520that%2520use%250Afixed%2520wavelet%2520approaches%252C%2520LifWavNet%2520employs%2520learnable%2520lifting%2520wavelets%2520with%250Alifting%2520and%2520inverse%2520lifting%2520units%2520to%2520adaptively%2520capture%2520radar%2520signal%2520features%250Aand%2520synthesize%2520physiologically%2520meaningful%2520ECG%2520waveforms.%2520To%2520improve%250Areconstruction%2520fidelity%252C%2520we%2520introduce%2520a%2520multi-resolution%2520short-time%2520Fourier%250Atransform%2520%2528STFT%2529%2520loss%252C%2520that%2520enforces%2520consistency%2520with%2520the%2520ground-truth%2520ECG%2520in%250Aboth%2520temporal%2520and%2520spectral%2520domains.%2520Evaluations%2520on%2520two%2520public%2520datasets%250Ademonstrate%2520that%2520LifWavNet%2520outperforms%2520state-of-the-art%2520methods%2520in%2520ECG%250Areconstruction%2520and%2520downstream%2520vital%2520sign%2520estimation%2520%2528heart%2520rate%2520and%2520heart%2520rate%250Avariability%2529.%2520Furthermore%252C%2520intermediate%2520feature%2520visualization%2520highlights%2520the%250Ainterpretability%2520of%2520multi-resolution%2520decomposition%2520and%2520synthesis%2520in%250Aradar-to-ECG%2520reconstruction.%2520These%2520results%2520establish%2520LifWavNet%2520as%2520a%2520robust%250Aframework%2520for%2520radar-based%2520non-contact%2520ECG%2520measurement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LifWavNet%3A%20Lifting%20Wavelet-based%20Network%20for%20Non-contact%20ECG%0A%20%20Reconstruction%20from%20Radar&entry.906535625=Soumitra%20Kundu%20and%20Gargi%20Panda%20and%20Saumik%20Bhattacharya%20and%20Aurobinda%20Routray%20and%20Rajlakshmi%20Guha&entry.1292438233=%20%20Non-contact%20electrocardiogram%20%28ECG%29%20reconstruction%20from%20radar%20signals%20offers%0Aa%20promising%20approach%20for%20unobtrusive%20cardiac%20monitoring.%20We%20present%20LifWavNet%2C%0Aa%20lifting%20wavelet%20network%20based%20on%20a%20multi-resolution%20analysis%20and%20synthesis%0A%28MRAS%29%20model%20for%20radar-to-ECG%20reconstruction.%20Unlike%20prior%20models%20that%20use%0Afixed%20wavelet%20approaches%2C%20LifWavNet%20employs%20learnable%20lifting%20wavelets%20with%0Alifting%20and%20inverse%20lifting%20units%20to%20adaptively%20capture%20radar%20signal%20features%0Aand%20synthesize%20physiologically%20meaningful%20ECG%20waveforms.%20To%20improve%0Areconstruction%20fidelity%2C%20we%20introduce%20a%20multi-resolution%20short-time%20Fourier%0Atransform%20%28STFT%29%20loss%2C%20that%20enforces%20consistency%20with%20the%20ground-truth%20ECG%20in%0Aboth%20temporal%20and%20spectral%20domains.%20Evaluations%20on%20two%20public%20datasets%0Ademonstrate%20that%20LifWavNet%20outperforms%20state-of-the-art%20methods%20in%20ECG%0Areconstruction%20and%20downstream%20vital%20sign%20estimation%20%28heart%20rate%20and%20heart%20rate%0Avariability%29.%20Furthermore%2C%20intermediate%20feature%20visualization%20highlights%20the%0Ainterpretability%20of%20multi-resolution%20decomposition%20and%20synthesis%20in%0Aradar-to-ECG%20reconstruction.%20These%20results%20establish%20LifWavNet%20as%20a%20robust%0Aframework%20for%20radar-based%20non-contact%20ECG%20measurement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27692v1&entry.124074799=Read"},
{"title": "Challenges learning from imbalanced data using tree-based models:\n  Prevalence estimates systematically depend on hyperparameters and can be\n  upwardly biased", "author": "Nathan Phelps and Daniel J. Lizotte and Douglas G. Woolford", "abstract": "  When using machine learning for imbalanced binary classification problems, it\nis common to subsample the majority class to create a (more) balanced training\ndataset. This biases the model's predictions because the model learns from data\nwhose data generating process differs from new data. One way of accounting for\nthis bias is analytically mapping the resulting predictions to new values based\non the sampling rate for the majority class. We show that calibrating a random\nforest this way has negative consequences, including prevalence estimates that\ndepend on both the number of predictors considered at each split in the random\nforest and the sampling rate used. We explain the former using known properties\nof random forests and analytical calibration. Through investigating the latter\nissue, we made a surprising discovery - contrary to the widespread belief that\ndecision trees are biased towards the majority class, they actually can be\nbiased towards the minority class.\n", "link": "http://arxiv.org/abs/2412.16209v4", "date": "2025-10-31", "relevancy": 1.8032, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.453}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20learning%20from%20imbalanced%20data%20using%20tree-based%20models%3A%0A%20%20Prevalence%20estimates%20systematically%20depend%20on%20hyperparameters%20and%20can%20be%0A%20%20upwardly%20biased&body=Title%3A%20Challenges%20learning%20from%20imbalanced%20data%20using%20tree-based%20models%3A%0A%20%20Prevalence%20estimates%20systematically%20depend%20on%20hyperparameters%20and%20can%20be%0A%20%20upwardly%20biased%0AAuthor%3A%20Nathan%20Phelps%20and%20Daniel%20J.%20Lizotte%20and%20Douglas%20G.%20Woolford%0AAbstract%3A%20%20%20When%20using%20machine%20learning%20for%20imbalanced%20binary%20classification%20problems%2C%20it%0Ais%20common%20to%20subsample%20the%20majority%20class%20to%20create%20a%20%28more%29%20balanced%20training%0Adataset.%20This%20biases%20the%20model%27s%20predictions%20because%20the%20model%20learns%20from%20data%0Awhose%20data%20generating%20process%20differs%20from%20new%20data.%20One%20way%20of%20accounting%20for%0Athis%20bias%20is%20analytically%20mapping%20the%20resulting%20predictions%20to%20new%20values%20based%0Aon%20the%20sampling%20rate%20for%20the%20majority%20class.%20We%20show%20that%20calibrating%20a%20random%0Aforest%20this%20way%20has%20negative%20consequences%2C%20including%20prevalence%20estimates%20that%0Adepend%20on%20both%20the%20number%20of%20predictors%20considered%20at%20each%20split%20in%20the%20random%0Aforest%20and%20the%20sampling%20rate%20used.%20We%20explain%20the%20former%20using%20known%20properties%0Aof%20random%20forests%20and%20analytical%20calibration.%20Through%20investigating%20the%20latter%0Aissue%2C%20we%20made%20a%20surprising%20discovery%20-%20contrary%20to%20the%20widespread%20belief%20that%0Adecision%20trees%20are%20biased%20towards%20the%20majority%20class%2C%20they%20actually%20can%20be%0Abiased%20towards%20the%20minority%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16209v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520learning%2520from%2520imbalanced%2520data%2520using%2520tree-based%2520models%253A%250A%2520%2520Prevalence%2520estimates%2520systematically%2520depend%2520on%2520hyperparameters%2520and%2520can%2520be%250A%2520%2520upwardly%2520biased%26entry.906535625%3DNathan%2520Phelps%2520and%2520Daniel%2520J.%2520Lizotte%2520and%2520Douglas%2520G.%2520Woolford%26entry.1292438233%3D%2520%2520When%2520using%2520machine%2520learning%2520for%2520imbalanced%2520binary%2520classification%2520problems%252C%2520it%250Ais%2520common%2520to%2520subsample%2520the%2520majority%2520class%2520to%2520create%2520a%2520%2528more%2529%2520balanced%2520training%250Adataset.%2520This%2520biases%2520the%2520model%2527s%2520predictions%2520because%2520the%2520model%2520learns%2520from%2520data%250Awhose%2520data%2520generating%2520process%2520differs%2520from%2520new%2520data.%2520One%2520way%2520of%2520accounting%2520for%250Athis%2520bias%2520is%2520analytically%2520mapping%2520the%2520resulting%2520predictions%2520to%2520new%2520values%2520based%250Aon%2520the%2520sampling%2520rate%2520for%2520the%2520majority%2520class.%2520We%2520show%2520that%2520calibrating%2520a%2520random%250Aforest%2520this%2520way%2520has%2520negative%2520consequences%252C%2520including%2520prevalence%2520estimates%2520that%250Adepend%2520on%2520both%2520the%2520number%2520of%2520predictors%2520considered%2520at%2520each%2520split%2520in%2520the%2520random%250Aforest%2520and%2520the%2520sampling%2520rate%2520used.%2520We%2520explain%2520the%2520former%2520using%2520known%2520properties%250Aof%2520random%2520forests%2520and%2520analytical%2520calibration.%2520Through%2520investigating%2520the%2520latter%250Aissue%252C%2520we%2520made%2520a%2520surprising%2520discovery%2520-%2520contrary%2520to%2520the%2520widespread%2520belief%2520that%250Adecision%2520trees%2520are%2520biased%2520towards%2520the%2520majority%2520class%252C%2520they%2520actually%2520can%2520be%250Abiased%2520towards%2520the%2520minority%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16209v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20learning%20from%20imbalanced%20data%20using%20tree-based%20models%3A%0A%20%20Prevalence%20estimates%20systematically%20depend%20on%20hyperparameters%20and%20can%20be%0A%20%20upwardly%20biased&entry.906535625=Nathan%20Phelps%20and%20Daniel%20J.%20Lizotte%20and%20Douglas%20G.%20Woolford&entry.1292438233=%20%20When%20using%20machine%20learning%20for%20imbalanced%20binary%20classification%20problems%2C%20it%0Ais%20common%20to%20subsample%20the%20majority%20class%20to%20create%20a%20%28more%29%20balanced%20training%0Adataset.%20This%20biases%20the%20model%27s%20predictions%20because%20the%20model%20learns%20from%20data%0Awhose%20data%20generating%20process%20differs%20from%20new%20data.%20One%20way%20of%20accounting%20for%0Athis%20bias%20is%20analytically%20mapping%20the%20resulting%20predictions%20to%20new%20values%20based%0Aon%20the%20sampling%20rate%20for%20the%20majority%20class.%20We%20show%20that%20calibrating%20a%20random%0Aforest%20this%20way%20has%20negative%20consequences%2C%20including%20prevalence%20estimates%20that%0Adepend%20on%20both%20the%20number%20of%20predictors%20considered%20at%20each%20split%20in%20the%20random%0Aforest%20and%20the%20sampling%20rate%20used.%20We%20explain%20the%20former%20using%20known%20properties%0Aof%20random%20forests%20and%20analytical%20calibration.%20Through%20investigating%20the%20latter%0Aissue%2C%20we%20made%20a%20surprising%20discovery%20-%20contrary%20to%20the%20widespread%20belief%20that%0Adecision%20trees%20are%20biased%20towards%20the%20majority%20class%2C%20they%20actually%20can%20be%0Abiased%20towards%20the%20minority%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16209v4&entry.124074799=Read"},
{"title": "Enhancing software product lines with machine learning components", "author": "Luz-Viviana Cobaleda and Juli\u00e1n Carvajal and Paola Vallejo and Andr\u00e9s L\u00f3pez and Ra\u00fal Mazo", "abstract": "  Modern software systems increasingly integrate machine learning (ML) due to\nits advancements and ability to enhance data-driven decision-making. However,\nthis integration introduces significant challenges for software engineering,\nespecially in software product lines (SPLs), where managing variability and\nreuse becomes more complex with the inclusion of ML components. Although\nexisting approaches have addressed variability management in SPLs and the\nintegration of ML components in isolated systems, few have explored the\nintersection of both domains. Specifically, there is limited support for\nmodeling and managing variability in SPLs that incorporate ML components. To\nbridge this gap, this article proposes a structured framework designed to\nextend Software Product Line engineering, facilitating the integration of ML\ncomponents. It facilitates the design of SPLs with ML capabilities by enabling\nsystematic modeling of variability and reuse. The proposal has been partially\nimplemented with the VariaMos tool.\n", "link": "http://arxiv.org/abs/2510.27640v1", "date": "2025-10-31", "relevancy": 1.8025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20software%20product%20lines%20with%20machine%20learning%20components&body=Title%3A%20Enhancing%20software%20product%20lines%20with%20machine%20learning%20components%0AAuthor%3A%20Luz-Viviana%20Cobaleda%20and%20Juli%C3%A1n%20Carvajal%20and%20Paola%20Vallejo%20and%20Andr%C3%A9s%20L%C3%B3pez%20and%20Ra%C3%BAl%20Mazo%0AAbstract%3A%20%20%20Modern%20software%20systems%20increasingly%20integrate%20machine%20learning%20%28ML%29%20due%20to%0Aits%20advancements%20and%20ability%20to%20enhance%20data-driven%20decision-making.%20However%2C%0Athis%20integration%20introduces%20significant%20challenges%20for%20software%20engineering%2C%0Aespecially%20in%20software%20product%20lines%20%28SPLs%29%2C%20where%20managing%20variability%20and%0Areuse%20becomes%20more%20complex%20with%20the%20inclusion%20of%20ML%20components.%20Although%0Aexisting%20approaches%20have%20addressed%20variability%20management%20in%20SPLs%20and%20the%0Aintegration%20of%20ML%20components%20in%20isolated%20systems%2C%20few%20have%20explored%20the%0Aintersection%20of%20both%20domains.%20Specifically%2C%20there%20is%20limited%20support%20for%0Amodeling%20and%20managing%20variability%20in%20SPLs%20that%20incorporate%20ML%20components.%20To%0Abridge%20this%20gap%2C%20this%20article%20proposes%20a%20structured%20framework%20designed%20to%0Aextend%20Software%20Product%20Line%20engineering%2C%20facilitating%20the%20integration%20of%20ML%0Acomponents.%20It%20facilitates%20the%20design%20of%20SPLs%20with%20ML%20capabilities%20by%20enabling%0Asystematic%20modeling%20of%20variability%20and%20reuse.%20The%20proposal%20has%20been%20partially%0Aimplemented%20with%20the%20VariaMos%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520software%2520product%2520lines%2520with%2520machine%2520learning%2520components%26entry.906535625%3DLuz-Viviana%2520Cobaleda%2520and%2520Juli%25C3%25A1n%2520Carvajal%2520and%2520Paola%2520Vallejo%2520and%2520Andr%25C3%25A9s%2520L%25C3%25B3pez%2520and%2520Ra%25C3%25BAl%2520Mazo%26entry.1292438233%3D%2520%2520Modern%2520software%2520systems%2520increasingly%2520integrate%2520machine%2520learning%2520%2528ML%2529%2520due%2520to%250Aits%2520advancements%2520and%2520ability%2520to%2520enhance%2520data-driven%2520decision-making.%2520However%252C%250Athis%2520integration%2520introduces%2520significant%2520challenges%2520for%2520software%2520engineering%252C%250Aespecially%2520in%2520software%2520product%2520lines%2520%2528SPLs%2529%252C%2520where%2520managing%2520variability%2520and%250Areuse%2520becomes%2520more%2520complex%2520with%2520the%2520inclusion%2520of%2520ML%2520components.%2520Although%250Aexisting%2520approaches%2520have%2520addressed%2520variability%2520management%2520in%2520SPLs%2520and%2520the%250Aintegration%2520of%2520ML%2520components%2520in%2520isolated%2520systems%252C%2520few%2520have%2520explored%2520the%250Aintersection%2520of%2520both%2520domains.%2520Specifically%252C%2520there%2520is%2520limited%2520support%2520for%250Amodeling%2520and%2520managing%2520variability%2520in%2520SPLs%2520that%2520incorporate%2520ML%2520components.%2520To%250Abridge%2520this%2520gap%252C%2520this%2520article%2520proposes%2520a%2520structured%2520framework%2520designed%2520to%250Aextend%2520Software%2520Product%2520Line%2520engineering%252C%2520facilitating%2520the%2520integration%2520of%2520ML%250Acomponents.%2520It%2520facilitates%2520the%2520design%2520of%2520SPLs%2520with%2520ML%2520capabilities%2520by%2520enabling%250Asystematic%2520modeling%2520of%2520variability%2520and%2520reuse.%2520The%2520proposal%2520has%2520been%2520partially%250Aimplemented%2520with%2520the%2520VariaMos%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20software%20product%20lines%20with%20machine%20learning%20components&entry.906535625=Luz-Viviana%20Cobaleda%20and%20Juli%C3%A1n%20Carvajal%20and%20Paola%20Vallejo%20and%20Andr%C3%A9s%20L%C3%B3pez%20and%20Ra%C3%BAl%20Mazo&entry.1292438233=%20%20Modern%20software%20systems%20increasingly%20integrate%20machine%20learning%20%28ML%29%20due%20to%0Aits%20advancements%20and%20ability%20to%20enhance%20data-driven%20decision-making.%20However%2C%0Athis%20integration%20introduces%20significant%20challenges%20for%20software%20engineering%2C%0Aespecially%20in%20software%20product%20lines%20%28SPLs%29%2C%20where%20managing%20variability%20and%0Areuse%20becomes%20more%20complex%20with%20the%20inclusion%20of%20ML%20components.%20Although%0Aexisting%20approaches%20have%20addressed%20variability%20management%20in%20SPLs%20and%20the%0Aintegration%20of%20ML%20components%20in%20isolated%20systems%2C%20few%20have%20explored%20the%0Aintersection%20of%20both%20domains.%20Specifically%2C%20there%20is%20limited%20support%20for%0Amodeling%20and%20managing%20variability%20in%20SPLs%20that%20incorporate%20ML%20components.%20To%0Abridge%20this%20gap%2C%20this%20article%20proposes%20a%20structured%20framework%20designed%20to%0Aextend%20Software%20Product%20Line%20engineering%2C%20facilitating%20the%20integration%20of%20ML%0Acomponents.%20It%20facilitates%20the%20design%20of%20SPLs%20with%20ML%20capabilities%20by%20enabling%0Asystematic%20modeling%20of%20variability%20and%20reuse.%20The%20proposal%20has%20been%20partially%0Aimplemented%20with%20the%20VariaMos%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27640v1&entry.124074799=Read"},
{"title": "Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust\n  Cross-Scale Grasping", "author": "Dong Heon Han and Xiaohao Xu and Yuxi Chen and Yusheng Zhou and Xinqi Zhang and Jiaqi Wang and Daniel Bruder and Xiaonan Huang", "abstract": "  Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation.\n", "link": "http://arxiv.org/abs/2510.27666v1", "date": "2025-10-31", "relevancy": 1.7948, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6132}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole-Body%20Proprioceptive%20Morphing%3A%20A%20Modular%20Soft%20Gripper%20for%20Robust%0A%20%20Cross-Scale%20Grasping&body=Title%3A%20Whole-Body%20Proprioceptive%20Morphing%3A%20A%20Modular%20Soft%20Gripper%20for%20Robust%0A%20%20Cross-Scale%20Grasping%0AAuthor%3A%20Dong%20Heon%20Han%20and%20Xiaohao%20Xu%20and%20Yuxi%20Chen%20and%20Yusheng%20Zhou%20and%20Xinqi%20Zhang%20and%20Jiaqi%20Wang%20and%20Daniel%20Bruder%20and%20Xiaonan%20Huang%0AAbstract%3A%20%20%20Biological%20systems%2C%20such%20as%20the%20octopus%2C%20exhibit%20masterful%20cross-scale%0Amanipulation%20by%20adaptively%20reconfiguring%20their%20entire%20form%2C%20a%20capability%20that%0Aremains%20elusive%20in%20robotics.%20Conventional%20soft%20grippers%2C%20while%20compliant%2C%20are%0Amostly%20constrained%20by%20a%20fixed%20global%20morphology%2C%20and%20prior%20shape-morphing%0Aefforts%20have%20been%20largely%20confined%20to%20localized%20deformations%2C%20failing%20to%0Areplicate%20this%20biological%20dexterity.%20Inspired%20by%20this%20natural%20exemplar%2C%20we%0Aintroduce%20the%20paradigm%20of%20collaborative%2C%20whole-body%20proprioceptive%20morphing%2C%0Arealized%20in%20a%20modular%20soft%20gripper%20architecture.%20Our%20design%20is%20a%20distributed%0Anetwork%20of%20modular%20self-sensing%20pneumatic%20actuators%20that%20enables%20the%20gripper%20to%0Aintelligently%20reconfigure%20its%20entire%20topology%2C%20achieving%20multiple%20morphing%0Astates%20that%20are%20controllable%20to%20form%20diverse%20polygonal%20shapes.%20By%20integrating%0Arich%20proprioceptive%20feedback%20from%20embedded%20sensors%2C%20our%20system%20can%20seamlessly%0Atransition%20from%20a%20precise%20pinch%20to%20a%20large%20envelope%20grasp.%20We%20experimentally%0Ademonstrate%20that%20this%20approach%20expands%20the%20grasping%20envelope%20and%20enhances%0Ageneralization%20across%20diverse%20object%20geometries%20%28standard%20and%20irregular%29%20and%0Ascales%20%28up%20to%2010%24%5Ctimes%24%29%2C%20while%20also%20unlocking%20novel%20manipulation%20modalities%0Asuch%20as%20multi-object%20and%20internal%20hook%20grasping.%20This%20work%20presents%20a%20low-cost%2C%0Aeasy-to-fabricate%2C%20and%20scalable%20framework%20that%20fuses%20distributed%20actuation%20with%0Aintegrated%20sensing%2C%20offering%20a%20new%20pathway%20toward%20achieving%20biological%20levels%0Aof%20dexterity%20in%20robotic%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole-Body%2520Proprioceptive%2520Morphing%253A%2520A%2520Modular%2520Soft%2520Gripper%2520for%2520Robust%250A%2520%2520Cross-Scale%2520Grasping%26entry.906535625%3DDong%2520Heon%2520Han%2520and%2520Xiaohao%2520Xu%2520and%2520Yuxi%2520Chen%2520and%2520Yusheng%2520Zhou%2520and%2520Xinqi%2520Zhang%2520and%2520Jiaqi%2520Wang%2520and%2520Daniel%2520Bruder%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3D%2520%2520Biological%2520systems%252C%2520such%2520as%2520the%2520octopus%252C%2520exhibit%2520masterful%2520cross-scale%250Amanipulation%2520by%2520adaptively%2520reconfiguring%2520their%2520entire%2520form%252C%2520a%2520capability%2520that%250Aremains%2520elusive%2520in%2520robotics.%2520Conventional%2520soft%2520grippers%252C%2520while%2520compliant%252C%2520are%250Amostly%2520constrained%2520by%2520a%2520fixed%2520global%2520morphology%252C%2520and%2520prior%2520shape-morphing%250Aefforts%2520have%2520been%2520largely%2520confined%2520to%2520localized%2520deformations%252C%2520failing%2520to%250Areplicate%2520this%2520biological%2520dexterity.%2520Inspired%2520by%2520this%2520natural%2520exemplar%252C%2520we%250Aintroduce%2520the%2520paradigm%2520of%2520collaborative%252C%2520whole-body%2520proprioceptive%2520morphing%252C%250Arealized%2520in%2520a%2520modular%2520soft%2520gripper%2520architecture.%2520Our%2520design%2520is%2520a%2520distributed%250Anetwork%2520of%2520modular%2520self-sensing%2520pneumatic%2520actuators%2520that%2520enables%2520the%2520gripper%2520to%250Aintelligently%2520reconfigure%2520its%2520entire%2520topology%252C%2520achieving%2520multiple%2520morphing%250Astates%2520that%2520are%2520controllable%2520to%2520form%2520diverse%2520polygonal%2520shapes.%2520By%2520integrating%250Arich%2520proprioceptive%2520feedback%2520from%2520embedded%2520sensors%252C%2520our%2520system%2520can%2520seamlessly%250Atransition%2520from%2520a%2520precise%2520pinch%2520to%2520a%2520large%2520envelope%2520grasp.%2520We%2520experimentally%250Ademonstrate%2520that%2520this%2520approach%2520expands%2520the%2520grasping%2520envelope%2520and%2520enhances%250Ageneralization%2520across%2520diverse%2520object%2520geometries%2520%2528standard%2520and%2520irregular%2529%2520and%250Ascales%2520%2528up%2520to%252010%2524%255Ctimes%2524%2529%252C%2520while%2520also%2520unlocking%2520novel%2520manipulation%2520modalities%250Asuch%2520as%2520multi-object%2520and%2520internal%2520hook%2520grasping.%2520This%2520work%2520presents%2520a%2520low-cost%252C%250Aeasy-to-fabricate%252C%2520and%2520scalable%2520framework%2520that%2520fuses%2520distributed%2520actuation%2520with%250Aintegrated%2520sensing%252C%2520offering%2520a%2520new%2520pathway%2520toward%2520achieving%2520biological%2520levels%250Aof%2520dexterity%2520in%2520robotic%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole-Body%20Proprioceptive%20Morphing%3A%20A%20Modular%20Soft%20Gripper%20for%20Robust%0A%20%20Cross-Scale%20Grasping&entry.906535625=Dong%20Heon%20Han%20and%20Xiaohao%20Xu%20and%20Yuxi%20Chen%20and%20Yusheng%20Zhou%20and%20Xinqi%20Zhang%20and%20Jiaqi%20Wang%20and%20Daniel%20Bruder%20and%20Xiaonan%20Huang&entry.1292438233=%20%20Biological%20systems%2C%20such%20as%20the%20octopus%2C%20exhibit%20masterful%20cross-scale%0Amanipulation%20by%20adaptively%20reconfiguring%20their%20entire%20form%2C%20a%20capability%20that%0Aremains%20elusive%20in%20robotics.%20Conventional%20soft%20grippers%2C%20while%20compliant%2C%20are%0Amostly%20constrained%20by%20a%20fixed%20global%20morphology%2C%20and%20prior%20shape-morphing%0Aefforts%20have%20been%20largely%20confined%20to%20localized%20deformations%2C%20failing%20to%0Areplicate%20this%20biological%20dexterity.%20Inspired%20by%20this%20natural%20exemplar%2C%20we%0Aintroduce%20the%20paradigm%20of%20collaborative%2C%20whole-body%20proprioceptive%20morphing%2C%0Arealized%20in%20a%20modular%20soft%20gripper%20architecture.%20Our%20design%20is%20a%20distributed%0Anetwork%20of%20modular%20self-sensing%20pneumatic%20actuators%20that%20enables%20the%20gripper%20to%0Aintelligently%20reconfigure%20its%20entire%20topology%2C%20achieving%20multiple%20morphing%0Astates%20that%20are%20controllable%20to%20form%20diverse%20polygonal%20shapes.%20By%20integrating%0Arich%20proprioceptive%20feedback%20from%20embedded%20sensors%2C%20our%20system%20can%20seamlessly%0Atransition%20from%20a%20precise%20pinch%20to%20a%20large%20envelope%20grasp.%20We%20experimentally%0Ademonstrate%20that%20this%20approach%20expands%20the%20grasping%20envelope%20and%20enhances%0Ageneralization%20across%20diverse%20object%20geometries%20%28standard%20and%20irregular%29%20and%0Ascales%20%28up%20to%2010%24%5Ctimes%24%29%2C%20while%20also%20unlocking%20novel%20manipulation%20modalities%0Asuch%20as%20multi-object%20and%20internal%20hook%20grasping.%20This%20work%20presents%20a%20low-cost%2C%0Aeasy-to-fabricate%2C%20and%20scalable%20framework%20that%20fuses%20distributed%20actuation%20with%0Aintegrated%20sensing%2C%20offering%20a%20new%20pathway%20toward%20achieving%20biological%20levels%0Aof%20dexterity%20in%20robotic%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27666v1&entry.124074799=Read"},
{"title": "Persistent Homology for Structural Characterization in Disordered\n  Systems", "author": "An Wang and Li Zou", "abstract": "  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. We also\ndemonstrated that using a single variable enables a linear SVM to achieve\nnearly perfect three-phase classification. Inspired by this discovery, we\ndefine a non-parametric metric, the Separation Index (SI), which not only\nachieves this classification without sacrificing significant performance but\nalso establishes a connection between particle environments and the global\nphase structure. Our methods provide an effective framework for understanding\nand analyzing the properties of disordered materials, with broad potential\napplications in materials science and even wider studies of complex systems.\n", "link": "http://arxiv.org/abs/2411.14390v9", "date": "2025-10-31", "relevancy": 1.7425, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4372}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems&body=Title%3A%20Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems%0AAuthor%3A%20An%20Wang%20and%20Li%20Zou%0AAbstract%3A%20%20%20We%20propose%20a%20unified%20framework%20based%20on%20persistent%20homology%20%28PH%29%20to%0Acharacterize%20both%20local%20and%20global%20structures%20in%20disordered%20systems.%20It%20can%0Asimultaneously%20generate%20local%20and%20global%20descriptors%20using%20the%20same%20algorithm%0Aand%20data%20structure%2C%20and%20has%20shown%20to%20be%20highly%20effective%20and%20interpretable%20in%0Apredicting%20particle%20rearrangements%20and%20classifying%20global%20phases.%20We%20also%0Ademonstrated%20that%20using%20a%20single%20variable%20enables%20a%20linear%20SVM%20to%20achieve%0Anearly%20perfect%20three-phase%20classification.%20Inspired%20by%20this%20discovery%2C%20we%0Adefine%20a%20non-parametric%20metric%2C%20the%20Separation%20Index%20%28SI%29%2C%20which%20not%20only%0Aachieves%20this%20classification%20without%20sacrificing%20significant%20performance%20but%0Aalso%20establishes%20a%20connection%20between%20particle%20environments%20and%20the%20global%0Aphase%20structure.%20Our%20methods%20provide%20an%20effective%20framework%20for%20understanding%0Aand%20analyzing%20the%20properties%20of%20disordered%20materials%2C%20with%20broad%20potential%0Aapplications%20in%20materials%20science%20and%20even%20wider%20studies%20of%20complex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14390v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Homology%2520for%2520Structural%2520Characterization%2520in%2520Disordered%250A%2520%2520Systems%26entry.906535625%3DAn%2520Wang%2520and%2520Li%2520Zou%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520unified%2520framework%2520based%2520on%2520persistent%2520homology%2520%2528PH%2529%2520to%250Acharacterize%2520both%2520local%2520and%2520global%2520structures%2520in%2520disordered%2520systems.%2520It%2520can%250Asimultaneously%2520generate%2520local%2520and%2520global%2520descriptors%2520using%2520the%2520same%2520algorithm%250Aand%2520data%2520structure%252C%2520and%2520has%2520shown%2520to%2520be%2520highly%2520effective%2520and%2520interpretable%2520in%250Apredicting%2520particle%2520rearrangements%2520and%2520classifying%2520global%2520phases.%2520We%2520also%250Ademonstrated%2520that%2520using%2520a%2520single%2520variable%2520enables%2520a%2520linear%2520SVM%2520to%2520achieve%250Anearly%2520perfect%2520three-phase%2520classification.%2520Inspired%2520by%2520this%2520discovery%252C%2520we%250Adefine%2520a%2520non-parametric%2520metric%252C%2520the%2520Separation%2520Index%2520%2528SI%2529%252C%2520which%2520not%2520only%250Aachieves%2520this%2520classification%2520without%2520sacrificing%2520significant%2520performance%2520but%250Aalso%2520establishes%2520a%2520connection%2520between%2520particle%2520environments%2520and%2520the%2520global%250Aphase%2520structure.%2520Our%2520methods%2520provide%2520an%2520effective%2520framework%2520for%2520understanding%250Aand%2520analyzing%2520the%2520properties%2520of%2520disordered%2520materials%252C%2520with%2520broad%2520potential%250Aapplications%2520in%2520materials%2520science%2520and%2520even%2520wider%2520studies%2520of%2520complex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14390v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems&entry.906535625=An%20Wang%20and%20Li%20Zou&entry.1292438233=%20%20We%20propose%20a%20unified%20framework%20based%20on%20persistent%20homology%20%28PH%29%20to%0Acharacterize%20both%20local%20and%20global%20structures%20in%20disordered%20systems.%20It%20can%0Asimultaneously%20generate%20local%20and%20global%20descriptors%20using%20the%20same%20algorithm%0Aand%20data%20structure%2C%20and%20has%20shown%20to%20be%20highly%20effective%20and%20interpretable%20in%0Apredicting%20particle%20rearrangements%20and%20classifying%20global%20phases.%20We%20also%0Ademonstrated%20that%20using%20a%20single%20variable%20enables%20a%20linear%20SVM%20to%20achieve%0Anearly%20perfect%20three-phase%20classification.%20Inspired%20by%20this%20discovery%2C%20we%0Adefine%20a%20non-parametric%20metric%2C%20the%20Separation%20Index%20%28SI%29%2C%20which%20not%20only%0Aachieves%20this%20classification%20without%20sacrificing%20significant%20performance%20but%0Aalso%20establishes%20a%20connection%20between%20particle%20environments%20and%20the%20global%0Aphase%20structure.%20Our%20methods%20provide%20an%20effective%20framework%20for%20understanding%0Aand%20analyzing%20the%20properties%20of%20disordered%20materials%2C%20with%20broad%20potential%0Aapplications%20in%20materials%20science%20and%20even%20wider%20studies%20of%20complex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14390v9&entry.124074799=Read"},
{"title": "Kernel conditional tests from learning-theoretic bounds", "author": "Pierre-Fran\u00e7ois Massiani and Christian Fiedler and Lukas Haverbeck and Friedrich Solowjow and Sebastian Trimpe", "abstract": "  We propose a framework for hypothesis testing on conditional probability\ndistributions, which we then use to construct statistical tests of functionals\nof conditional distributions. These tests identify the inputs where the\nfunctionals differ with high probability, and include tests of conditional\nmoments or two-sample tests. Our key idea is to transform confidence bounds of\na learning method into a test of conditional expectations. We instantiate this\nprinciple for kernel ridge regression (KRR) with subgaussian noise. An\nintermediate data embedding then enables more general tests -- including\nconditional two-sample tests -- via kernel mean embeddings of distributions. To\nhave guarantees in this setting, we generalize existing pointwise-in-time or\ntime-uniform confidence bounds for KRR to previously-inaccessible yet essential\ncases such as infinite-dimensional outputs with non-trace-class kernels. These\nbounds also circumvent the need for independent data, allowing for instance\nonline sampling. To make our tests readily applicable in practice, we introduce\nbootstrapping schemes leveraging the parametric form of testing thresholds\nidentified in theory to avoid tuning inaccessible parameters. We illustrate the\ntests on examples, including one in process monitoring and comparison of\ndynamical systems. Overall, our results establish a comprehensive foundation\nfor conditional testing on functionals, from theoretical guarantees to an\nalgorithmic implementation, and advance the state of the art on confidence\nbounds for vector-valued least squares estimation.\n", "link": "http://arxiv.org/abs/2506.03898v2", "date": "2025-10-31", "relevancy": 1.7289, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4212}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20conditional%20tests%20from%20learning-theoretic%20bounds&body=Title%3A%20Kernel%20conditional%20tests%20from%20learning-theoretic%20bounds%0AAuthor%3A%20Pierre-Fran%C3%A7ois%20Massiani%20and%20Christian%20Fiedler%20and%20Lukas%20Haverbeck%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20We%20propose%20a%20framework%20for%20hypothesis%20testing%20on%20conditional%20probability%0Adistributions%2C%20which%20we%20then%20use%20to%20construct%20statistical%20tests%20of%20functionals%0Aof%20conditional%20distributions.%20These%20tests%20identify%20the%20inputs%20where%20the%0Afunctionals%20differ%20with%20high%20probability%2C%20and%20include%20tests%20of%20conditional%0Amoments%20or%20two-sample%20tests.%20Our%20key%20idea%20is%20to%20transform%20confidence%20bounds%20of%0Aa%20learning%20method%20into%20a%20test%20of%20conditional%20expectations.%20We%20instantiate%20this%0Aprinciple%20for%20kernel%20ridge%20regression%20%28KRR%29%20with%20subgaussian%20noise.%20An%0Aintermediate%20data%20embedding%20then%20enables%20more%20general%20tests%20--%20including%0Aconditional%20two-sample%20tests%20--%20via%20kernel%20mean%20embeddings%20of%20distributions.%20To%0Ahave%20guarantees%20in%20this%20setting%2C%20we%20generalize%20existing%20pointwise-in-time%20or%0Atime-uniform%20confidence%20bounds%20for%20KRR%20to%20previously-inaccessible%20yet%20essential%0Acases%20such%20as%20infinite-dimensional%20outputs%20with%20non-trace-class%20kernels.%20These%0Abounds%20also%20circumvent%20the%20need%20for%20independent%20data%2C%20allowing%20for%20instance%0Aonline%20sampling.%20To%20make%20our%20tests%20readily%20applicable%20in%20practice%2C%20we%20introduce%0Abootstrapping%20schemes%20leveraging%20the%20parametric%20form%20of%20testing%20thresholds%0Aidentified%20in%20theory%20to%20avoid%20tuning%20inaccessible%20parameters.%20We%20illustrate%20the%0Atests%20on%20examples%2C%20including%20one%20in%20process%20monitoring%20and%20comparison%20of%0Adynamical%20systems.%20Overall%2C%20our%20results%20establish%20a%20comprehensive%20foundation%0Afor%20conditional%20testing%20on%20functionals%2C%20from%20theoretical%20guarantees%20to%20an%0Aalgorithmic%20implementation%2C%20and%20advance%20the%20state%20of%20the%20art%20on%20confidence%0Abounds%20for%20vector-valued%20least%20squares%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03898v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520conditional%2520tests%2520from%2520learning-theoretic%2520bounds%26entry.906535625%3DPierre-Fran%25C3%25A7ois%2520Massiani%2520and%2520Christian%2520Fiedler%2520and%2520Lukas%2520Haverbeck%2520and%2520Friedrich%2520Solowjow%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520framework%2520for%2520hypothesis%2520testing%2520on%2520conditional%2520probability%250Adistributions%252C%2520which%2520we%2520then%2520use%2520to%2520construct%2520statistical%2520tests%2520of%2520functionals%250Aof%2520conditional%2520distributions.%2520These%2520tests%2520identify%2520the%2520inputs%2520where%2520the%250Afunctionals%2520differ%2520with%2520high%2520probability%252C%2520and%2520include%2520tests%2520of%2520conditional%250Amoments%2520or%2520two-sample%2520tests.%2520Our%2520key%2520idea%2520is%2520to%2520transform%2520confidence%2520bounds%2520of%250Aa%2520learning%2520method%2520into%2520a%2520test%2520of%2520conditional%2520expectations.%2520We%2520instantiate%2520this%250Aprinciple%2520for%2520kernel%2520ridge%2520regression%2520%2528KRR%2529%2520with%2520subgaussian%2520noise.%2520An%250Aintermediate%2520data%2520embedding%2520then%2520enables%2520more%2520general%2520tests%2520--%2520including%250Aconditional%2520two-sample%2520tests%2520--%2520via%2520kernel%2520mean%2520embeddings%2520of%2520distributions.%2520To%250Ahave%2520guarantees%2520in%2520this%2520setting%252C%2520we%2520generalize%2520existing%2520pointwise-in-time%2520or%250Atime-uniform%2520confidence%2520bounds%2520for%2520KRR%2520to%2520previously-inaccessible%2520yet%2520essential%250Acases%2520such%2520as%2520infinite-dimensional%2520outputs%2520with%2520non-trace-class%2520kernels.%2520These%250Abounds%2520also%2520circumvent%2520the%2520need%2520for%2520independent%2520data%252C%2520allowing%2520for%2520instance%250Aonline%2520sampling.%2520To%2520make%2520our%2520tests%2520readily%2520applicable%2520in%2520practice%252C%2520we%2520introduce%250Abootstrapping%2520schemes%2520leveraging%2520the%2520parametric%2520form%2520of%2520testing%2520thresholds%250Aidentified%2520in%2520theory%2520to%2520avoid%2520tuning%2520inaccessible%2520parameters.%2520We%2520illustrate%2520the%250Atests%2520on%2520examples%252C%2520including%2520one%2520in%2520process%2520monitoring%2520and%2520comparison%2520of%250Adynamical%2520systems.%2520Overall%252C%2520our%2520results%2520establish%2520a%2520comprehensive%2520foundation%250Afor%2520conditional%2520testing%2520on%2520functionals%252C%2520from%2520theoretical%2520guarantees%2520to%2520an%250Aalgorithmic%2520implementation%252C%2520and%2520advance%2520the%2520state%2520of%2520the%2520art%2520on%2520confidence%250Abounds%2520for%2520vector-valued%2520least%2520squares%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03898v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20conditional%20tests%20from%20learning-theoretic%20bounds&entry.906535625=Pierre-Fran%C3%A7ois%20Massiani%20and%20Christian%20Fiedler%20and%20Lukas%20Haverbeck%20and%20Friedrich%20Solowjow%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20We%20propose%20a%20framework%20for%20hypothesis%20testing%20on%20conditional%20probability%0Adistributions%2C%20which%20we%20then%20use%20to%20construct%20statistical%20tests%20of%20functionals%0Aof%20conditional%20distributions.%20These%20tests%20identify%20the%20inputs%20where%20the%0Afunctionals%20differ%20with%20high%20probability%2C%20and%20include%20tests%20of%20conditional%0Amoments%20or%20two-sample%20tests.%20Our%20key%20idea%20is%20to%20transform%20confidence%20bounds%20of%0Aa%20learning%20method%20into%20a%20test%20of%20conditional%20expectations.%20We%20instantiate%20this%0Aprinciple%20for%20kernel%20ridge%20regression%20%28KRR%29%20with%20subgaussian%20noise.%20An%0Aintermediate%20data%20embedding%20then%20enables%20more%20general%20tests%20--%20including%0Aconditional%20two-sample%20tests%20--%20via%20kernel%20mean%20embeddings%20of%20distributions.%20To%0Ahave%20guarantees%20in%20this%20setting%2C%20we%20generalize%20existing%20pointwise-in-time%20or%0Atime-uniform%20confidence%20bounds%20for%20KRR%20to%20previously-inaccessible%20yet%20essential%0Acases%20such%20as%20infinite-dimensional%20outputs%20with%20non-trace-class%20kernels.%20These%0Abounds%20also%20circumvent%20the%20need%20for%20independent%20data%2C%20allowing%20for%20instance%0Aonline%20sampling.%20To%20make%20our%20tests%20readily%20applicable%20in%20practice%2C%20we%20introduce%0Abootstrapping%20schemes%20leveraging%20the%20parametric%20form%20of%20testing%20thresholds%0Aidentified%20in%20theory%20to%20avoid%20tuning%20inaccessible%20parameters.%20We%20illustrate%20the%0Atests%20on%20examples%2C%20including%20one%20in%20process%20monitoring%20and%20comparison%20of%0Adynamical%20systems.%20Overall%2C%20our%20results%20establish%20a%20comprehensive%20foundation%0Afor%20conditional%20testing%20on%20functionals%2C%20from%20theoretical%20guarantees%20to%20an%0Aalgorithmic%20implementation%2C%20and%20advance%20the%20state%20of%20the%20art%20on%20confidence%0Abounds%20for%20vector-valued%20least%20squares%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03898v2&entry.124074799=Read"},
{"title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model", "author": "John Won and Kyungmin Lee and Huiwon Jang and Dongyoung Kim and Jinwoo Shin", "abstract": "  Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.\n", "link": "http://arxiv.org/abs/2510.27607v1", "date": "2025-10-31", "relevancy": 1.7281, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.612}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Stream%20Diffusion%20for%20World-Model%20Augmented%20Vision-Language-Action%0A%20%20Model&body=Title%3A%20Dual-Stream%20Diffusion%20for%20World-Model%20Augmented%20Vision-Language-Action%0A%20%20Model%0AAuthor%3A%20John%20Won%20and%20Kyungmin%20Lee%20and%20Huiwon%20Jang%20and%20Dongyoung%20Kim%20and%20Jinwoo%20Shin%0AAbstract%3A%20%20%20Recently%2C%20augmenting%20Vision-Language-Action%20models%20%28VLAs%29%20with%20world%20modeling%0Ahas%20shown%20promise%20in%20improving%20robotic%20policy%20learning.%20However%2C%20it%20remains%0Achallenging%20to%20jointly%20predict%20next-state%20observations%20and%20action%20sequences%0Abecause%20of%20the%20inherent%20difference%20between%20the%20two%20modalities.%20To%20address%20this%2C%0Awe%20propose%20DUal-STream%20diffusion%20%28DUST%29%2C%20a%20world-model%20augmented%20VLA%20framework%0Athat%20handles%20the%20modality%20conflict%20and%20enhances%20the%20performance%20of%20VLAs%20across%0Adiverse%20tasks.%20Specifically%2C%20we%20propose%20a%20multimodal%20diffusion%20transformer%0Aarchitecture%20that%20explicitly%20maintains%20separate%20modality%20streams%20while%20still%0Aenabling%20cross-modal%20knowledge%20sharing.%20In%20addition%2C%20we%20introduce%20independent%0Anoise%20perturbations%20for%20each%20modality%20and%20a%20decoupled%20flow-matching%20loss.%20This%0Adesign%20enables%20the%20model%20to%20learn%20the%20joint%20distribution%20in%20a%20bidirectional%0Amanner%20while%20avoiding%20the%20need%20for%20a%20unified%20latent%20space.%20Based%20on%20the%0Adecoupling%20of%20modalities%20during%20training%2C%20we%20also%20introduce%20a%20joint%20sampling%0Amethod%20that%20supports%20test-time%20scaling%2C%20where%20action%20and%20vision%20tokens%20evolve%0Aasynchronously%20at%20different%20rates.%20Through%20experiments%20on%20simulated%20benchmarks%0Asuch%20as%20RoboCasa%20and%20GR-1%2C%20DUST%20achieves%20up%20to%206%25%20gains%20over%20baseline%20methods%2C%0Awhile%20our%20test-time%20scaling%20approach%20provides%20an%20additional%202-5%25%20boost.%20On%0Areal-world%20tasks%20with%20the%20Franka%20Research%203%2C%20DUST%20improves%20success%20rates%20by%0A13%25%2C%20confirming%20its%20effectiveness%20beyond%20simulation.%20Furthermore%2C%20pre-training%0Aon%20action-free%20videos%20from%20BridgeV2%20yields%20significant%20transfer%20gains%20on%0ARoboCasa%2C%20underscoring%20DUST%27s%20potential%20for%20large-scale%20VLA%20pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Stream%2520Diffusion%2520for%2520World-Model%2520Augmented%2520Vision-Language-Action%250A%2520%2520Model%26entry.906535625%3DJohn%2520Won%2520and%2520Kyungmin%2520Lee%2520and%2520Huiwon%2520Jang%2520and%2520Dongyoung%2520Kim%2520and%2520Jinwoo%2520Shin%26entry.1292438233%3D%2520%2520Recently%252C%2520augmenting%2520Vision-Language-Action%2520models%2520%2528VLAs%2529%2520with%2520world%2520modeling%250Ahas%2520shown%2520promise%2520in%2520improving%2520robotic%2520policy%2520learning.%2520However%252C%2520it%2520remains%250Achallenging%2520to%2520jointly%2520predict%2520next-state%2520observations%2520and%2520action%2520sequences%250Abecause%2520of%2520the%2520inherent%2520difference%2520between%2520the%2520two%2520modalities.%2520To%2520address%2520this%252C%250Awe%2520propose%2520DUal-STream%2520diffusion%2520%2528DUST%2529%252C%2520a%2520world-model%2520augmented%2520VLA%2520framework%250Athat%2520handles%2520the%2520modality%2520conflict%2520and%2520enhances%2520the%2520performance%2520of%2520VLAs%2520across%250Adiverse%2520tasks.%2520Specifically%252C%2520we%2520propose%2520a%2520multimodal%2520diffusion%2520transformer%250Aarchitecture%2520that%2520explicitly%2520maintains%2520separate%2520modality%2520streams%2520while%2520still%250Aenabling%2520cross-modal%2520knowledge%2520sharing.%2520In%2520addition%252C%2520we%2520introduce%2520independent%250Anoise%2520perturbations%2520for%2520each%2520modality%2520and%2520a%2520decoupled%2520flow-matching%2520loss.%2520This%250Adesign%2520enables%2520the%2520model%2520to%2520learn%2520the%2520joint%2520distribution%2520in%2520a%2520bidirectional%250Amanner%2520while%2520avoiding%2520the%2520need%2520for%2520a%2520unified%2520latent%2520space.%2520Based%2520on%2520the%250Adecoupling%2520of%2520modalities%2520during%2520training%252C%2520we%2520also%2520introduce%2520a%2520joint%2520sampling%250Amethod%2520that%2520supports%2520test-time%2520scaling%252C%2520where%2520action%2520and%2520vision%2520tokens%2520evolve%250Aasynchronously%2520at%2520different%2520rates.%2520Through%2520experiments%2520on%2520simulated%2520benchmarks%250Asuch%2520as%2520RoboCasa%2520and%2520GR-1%252C%2520DUST%2520achieves%2520up%2520to%25206%2525%2520gains%2520over%2520baseline%2520methods%252C%250Awhile%2520our%2520test-time%2520scaling%2520approach%2520provides%2520an%2520additional%25202-5%2525%2520boost.%2520On%250Areal-world%2520tasks%2520with%2520the%2520Franka%2520Research%25203%252C%2520DUST%2520improves%2520success%2520rates%2520by%250A13%2525%252C%2520confirming%2520its%2520effectiveness%2520beyond%2520simulation.%2520Furthermore%252C%2520pre-training%250Aon%2520action-free%2520videos%2520from%2520BridgeV2%2520yields%2520significant%2520transfer%2520gains%2520on%250ARoboCasa%252C%2520underscoring%2520DUST%2527s%2520potential%2520for%2520large-scale%2520VLA%2520pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Stream%20Diffusion%20for%20World-Model%20Augmented%20Vision-Language-Action%0A%20%20Model&entry.906535625=John%20Won%20and%20Kyungmin%20Lee%20and%20Huiwon%20Jang%20and%20Dongyoung%20Kim%20and%20Jinwoo%20Shin&entry.1292438233=%20%20Recently%2C%20augmenting%20Vision-Language-Action%20models%20%28VLAs%29%20with%20world%20modeling%0Ahas%20shown%20promise%20in%20improving%20robotic%20policy%20learning.%20However%2C%20it%20remains%0Achallenging%20to%20jointly%20predict%20next-state%20observations%20and%20action%20sequences%0Abecause%20of%20the%20inherent%20difference%20between%20the%20two%20modalities.%20To%20address%20this%2C%0Awe%20propose%20DUal-STream%20diffusion%20%28DUST%29%2C%20a%20world-model%20augmented%20VLA%20framework%0Athat%20handles%20the%20modality%20conflict%20and%20enhances%20the%20performance%20of%20VLAs%20across%0Adiverse%20tasks.%20Specifically%2C%20we%20propose%20a%20multimodal%20diffusion%20transformer%0Aarchitecture%20that%20explicitly%20maintains%20separate%20modality%20streams%20while%20still%0Aenabling%20cross-modal%20knowledge%20sharing.%20In%20addition%2C%20we%20introduce%20independent%0Anoise%20perturbations%20for%20each%20modality%20and%20a%20decoupled%20flow-matching%20loss.%20This%0Adesign%20enables%20the%20model%20to%20learn%20the%20joint%20distribution%20in%20a%20bidirectional%0Amanner%20while%20avoiding%20the%20need%20for%20a%20unified%20latent%20space.%20Based%20on%20the%0Adecoupling%20of%20modalities%20during%20training%2C%20we%20also%20introduce%20a%20joint%20sampling%0Amethod%20that%20supports%20test-time%20scaling%2C%20where%20action%20and%20vision%20tokens%20evolve%0Aasynchronously%20at%20different%20rates.%20Through%20experiments%20on%20simulated%20benchmarks%0Asuch%20as%20RoboCasa%20and%20GR-1%2C%20DUST%20achieves%20up%20to%206%25%20gains%20over%20baseline%20methods%2C%0Awhile%20our%20test-time%20scaling%20approach%20provides%20an%20additional%202-5%25%20boost.%20On%0Areal-world%20tasks%20with%20the%20Franka%20Research%203%2C%20DUST%20improves%20success%20rates%20by%0A13%25%2C%20confirming%20its%20effectiveness%20beyond%20simulation.%20Furthermore%2C%20pre-training%0Aon%20action-free%20videos%20from%20BridgeV2%20yields%20significant%20transfer%20gains%20on%0ARoboCasa%2C%20underscoring%20DUST%27s%20potential%20for%20large-scale%20VLA%20pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27607v1&entry.124074799=Read"},
{"title": "Who Made This? Fake Detection and Source Attribution with Diffusion\n  Features", "author": "Simone Bonechi and Paolo Andreini and Barbara Toniella Corradini", "abstract": "  The rapid progress of generative diffusion models has enabled the creation of\nsynthetic images that are increasingly difficult to distinguish from real ones,\nraising concerns about authenticity, copyright, and misinformation. Existing\nsupervised detectors often struggle to generalize across unseen generators,\nrequiring extensive labeled data and frequent retraining. We introduce FRIDA\n(Fake-image Recognition and source Identification via Diffusion-features\nAnalysis), a lightweight framework that leverages internal activations from a\npre-trained diffusion model for deepfake detection and source generator\nattribution. A k-nearest-neighbor classifier applied to diffusion features\nachieves state-of-the-art cross-generator performance without fine-tuning,\nwhile a compact neural model enables accurate source attribution. These results\nshow that diffusion representations inherently encode generator-specific\npatterns, providing a simple and interpretable foundation for synthetic image\nforensics.\n", "link": "http://arxiv.org/abs/2510.27602v1", "date": "2025-10-31", "relevancy": 1.726, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5801}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5771}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20Made%20This%3F%20Fake%20Detection%20and%20Source%20Attribution%20with%20Diffusion%0A%20%20Features&body=Title%3A%20Who%20Made%20This%3F%20Fake%20Detection%20and%20Source%20Attribution%20with%20Diffusion%0A%20%20Features%0AAuthor%3A%20Simone%20Bonechi%20and%20Paolo%20Andreini%20and%20Barbara%20Toniella%20Corradini%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20generative%20diffusion%20models%20has%20enabled%20the%20creation%20of%0Asynthetic%20images%20that%20are%20increasingly%20difficult%20to%20distinguish%20from%20real%20ones%2C%0Araising%20concerns%20about%20authenticity%2C%20copyright%2C%20and%20misinformation.%20Existing%0Asupervised%20detectors%20often%20struggle%20to%20generalize%20across%20unseen%20generators%2C%0Arequiring%20extensive%20labeled%20data%20and%20frequent%20retraining.%20We%20introduce%20FRIDA%0A%28Fake-image%20Recognition%20and%20source%20Identification%20via%20Diffusion-features%0AAnalysis%29%2C%20a%20lightweight%20framework%20that%20leverages%20internal%20activations%20from%20a%0Apre-trained%20diffusion%20model%20for%20deepfake%20detection%20and%20source%20generator%0Aattribution.%20A%20k-nearest-neighbor%20classifier%20applied%20to%20diffusion%20features%0Aachieves%20state-of-the-art%20cross-generator%20performance%20without%20fine-tuning%2C%0Awhile%20a%20compact%20neural%20model%20enables%20accurate%20source%20attribution.%20These%20results%0Ashow%20that%20diffusion%20representations%20inherently%20encode%20generator-specific%0Apatterns%2C%20providing%20a%20simple%20and%20interpretable%20foundation%20for%20synthetic%20image%0Aforensics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520Made%2520This%253F%2520Fake%2520Detection%2520and%2520Source%2520Attribution%2520with%2520Diffusion%250A%2520%2520Features%26entry.906535625%3DSimone%2520Bonechi%2520and%2520Paolo%2520Andreini%2520and%2520Barbara%2520Toniella%2520Corradini%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520generative%2520diffusion%2520models%2520has%2520enabled%2520the%2520creation%2520of%250Asynthetic%2520images%2520that%2520are%2520increasingly%2520difficult%2520to%2520distinguish%2520from%2520real%2520ones%252C%250Araising%2520concerns%2520about%2520authenticity%252C%2520copyright%252C%2520and%2520misinformation.%2520Existing%250Asupervised%2520detectors%2520often%2520struggle%2520to%2520generalize%2520across%2520unseen%2520generators%252C%250Arequiring%2520extensive%2520labeled%2520data%2520and%2520frequent%2520retraining.%2520We%2520introduce%2520FRIDA%250A%2528Fake-image%2520Recognition%2520and%2520source%2520Identification%2520via%2520Diffusion-features%250AAnalysis%2529%252C%2520a%2520lightweight%2520framework%2520that%2520leverages%2520internal%2520activations%2520from%2520a%250Apre-trained%2520diffusion%2520model%2520for%2520deepfake%2520detection%2520and%2520source%2520generator%250Aattribution.%2520A%2520k-nearest-neighbor%2520classifier%2520applied%2520to%2520diffusion%2520features%250Aachieves%2520state-of-the-art%2520cross-generator%2520performance%2520without%2520fine-tuning%252C%250Awhile%2520a%2520compact%2520neural%2520model%2520enables%2520accurate%2520source%2520attribution.%2520These%2520results%250Ashow%2520that%2520diffusion%2520representations%2520inherently%2520encode%2520generator-specific%250Apatterns%252C%2520providing%2520a%2520simple%2520and%2520interpretable%2520foundation%2520for%2520synthetic%2520image%250Aforensics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Made%20This%3F%20Fake%20Detection%20and%20Source%20Attribution%20with%20Diffusion%0A%20%20Features&entry.906535625=Simone%20Bonechi%20and%20Paolo%20Andreini%20and%20Barbara%20Toniella%20Corradini&entry.1292438233=%20%20The%20rapid%20progress%20of%20generative%20diffusion%20models%20has%20enabled%20the%20creation%20of%0Asynthetic%20images%20that%20are%20increasingly%20difficult%20to%20distinguish%20from%20real%20ones%2C%0Araising%20concerns%20about%20authenticity%2C%20copyright%2C%20and%20misinformation.%20Existing%0Asupervised%20detectors%20often%20struggle%20to%20generalize%20across%20unseen%20generators%2C%0Arequiring%20extensive%20labeled%20data%20and%20frequent%20retraining.%20We%20introduce%20FRIDA%0A%28Fake-image%20Recognition%20and%20source%20Identification%20via%20Diffusion-features%0AAnalysis%29%2C%20a%20lightweight%20framework%20that%20leverages%20internal%20activations%20from%20a%0Apre-trained%20diffusion%20model%20for%20deepfake%20detection%20and%20source%20generator%0Aattribution.%20A%20k-nearest-neighbor%20classifier%20applied%20to%20diffusion%20features%0Aachieves%20state-of-the-art%20cross-generator%20performance%20without%20fine-tuning%2C%0Awhile%20a%20compact%20neural%20model%20enables%20accurate%20source%20attribution.%20These%20results%0Ashow%20that%20diffusion%20representations%20inherently%20encode%20generator-specific%0Apatterns%2C%20providing%20a%20simple%20and%20interpretable%20foundation%20for%20synthetic%20image%0Aforensics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27602v1&entry.124074799=Read"},
{"title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training", "author": "Dayuan Fu and Yunze Wu and Xiaojie Cai and Lyumanshan Ye and Shijie Xia and Zhen Huang and Weiye Si and Tianze Xu and Jie Sun and Keyu Li and Mohan Jiang and Junfei Wang and Qishuo Hua and Pengrui Lu and Yang Xiao and Pengfei Liu", "abstract": "  Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.\n", "link": "http://arxiv.org/abs/2510.27630v1", "date": "2025-10-31", "relevancy": 1.7179, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6015}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interaction%20as%20Intelligence%20Part%20II%3A%20Asynchronous%20Human-Agent%20Rollout%0A%20%20for%20Long-Horizon%20Task%20Training&body=Title%3A%20Interaction%20as%20Intelligence%20Part%20II%3A%20Asynchronous%20Human-Agent%20Rollout%0A%20%20for%20Long-Horizon%20Task%20Training%0AAuthor%3A%20Dayuan%20Fu%20and%20Yunze%20Wu%20and%20Xiaojie%20Cai%20and%20Lyumanshan%20Ye%20and%20Shijie%20Xia%20and%20Zhen%20Huang%20and%20Weiye%20Si%20and%20Tianze%20Xu%20and%20Jie%20Sun%20and%20Keyu%20Li%20and%20Mohan%20Jiang%20and%20Junfei%20Wang%20and%20Qishuo%20Hua%20and%20Pengrui%20Lu%20and%20Yang%20Xiao%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20recently%20shown%20strong%20potential%20in%0Adomains%20such%20as%20automated%20coding%2C%20deep%20research%2C%20and%20graphical%20user%20interface%0Amanipulation.%20However%2C%20training%20them%20to%20succeed%20on%20long-horizon%2C%0Adomain-specialized%20tasks%20remains%20challenging.%20Current%20methods%20primarily%20fall%0Ainto%20two%20categories.%20The%20first%20relies%20on%20dense%20human%20annotations%20through%0Abehavior%20cloning%2C%20which%20is%20prohibitively%20expensive%20for%20long-horizon%20tasks%20that%0Acan%20take%20days%20or%20months.%20The%20second%20depends%20on%20outcome-driven%20sampling%2C%20which%0Aoften%20collapses%20due%20to%20the%20rarity%20of%20valid%20positive%20trajectories%20on%0Adomain-specialized%20tasks.%20We%20introduce%20Apollo%2C%20a%20sampling%20framework%20that%0Aintegrates%20asynchronous%20human%20guidance%20with%20action-level%20data%20filtering.%0AInstead%20of%20requiring%20annotators%20to%20shadow%20every%20step%2C%20Apollo%20allows%20them%20to%0Aintervene%20only%20when%20the%20agent%20drifts%20from%20a%20promising%20trajectory%2C%20by%20providing%0Aprior%20knowledge%2C%20strategic%20advice%2C%20etc.%20This%20lightweight%20design%20makes%20it%0Apossible%20to%20sustain%20interactions%20for%20over%2030%20hours%20and%20produces%20valuable%0Atrajectories%20at%20a%20lower%20cost.%20Apollo%20then%20applies%20supervision%20control%20to%20filter%0Aout%20sub-optimal%20actions%20and%20prevent%20error%20propagation.%20Together%2C%20these%0Acomponents%20enable%20reliable%20and%20effective%20data%20collection%20in%20long-horizon%0Aenvironments.%20To%20demonstrate%20the%20effectiveness%20of%20Apollo%2C%20we%20evaluate%20it%20using%0AInnovatorBench.%20Our%20experiments%20show%20that%20when%20applied%20to%20train%20the%20GLM-4.5%0Amodel%20on%20InnovatorBench%2C%20Apollo%20achieves%20more%20than%20a%2050%25%20improvement%20over%20the%0Auntrained%20baseline%20and%20a%2028%25%20improvement%20over%20a%20variant%20trained%20without%20human%0Ainteraction.%20These%20results%20highlight%20the%20critical%20role%20of%20human-in-the-loop%0Asampling%20and%20the%20robustness%20of%20Apollo%27s%20design%20in%20handling%20long-horizon%2C%0Adomain-specialized%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteraction%2520as%2520Intelligence%2520Part%2520II%253A%2520Asynchronous%2520Human-Agent%2520Rollout%250A%2520%2520for%2520Long-Horizon%2520Task%2520Training%26entry.906535625%3DDayuan%2520Fu%2520and%2520Yunze%2520Wu%2520and%2520Xiaojie%2520Cai%2520and%2520Lyumanshan%2520Ye%2520and%2520Shijie%2520Xia%2520and%2520Zhen%2520Huang%2520and%2520Weiye%2520Si%2520and%2520Tianze%2520Xu%2520and%2520Jie%2520Sun%2520and%2520Keyu%2520Li%2520and%2520Mohan%2520Jiang%2520and%2520Junfei%2520Wang%2520and%2520Qishuo%2520Hua%2520and%2520Pengrui%2520Lu%2520and%2520Yang%2520Xiao%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520recently%2520shown%2520strong%2520potential%2520in%250Adomains%2520such%2520as%2520automated%2520coding%252C%2520deep%2520research%252C%2520and%2520graphical%2520user%2520interface%250Amanipulation.%2520However%252C%2520training%2520them%2520to%2520succeed%2520on%2520long-horizon%252C%250Adomain-specialized%2520tasks%2520remains%2520challenging.%2520Current%2520methods%2520primarily%2520fall%250Ainto%2520two%2520categories.%2520The%2520first%2520relies%2520on%2520dense%2520human%2520annotations%2520through%250Abehavior%2520cloning%252C%2520which%2520is%2520prohibitively%2520expensive%2520for%2520long-horizon%2520tasks%2520that%250Acan%2520take%2520days%2520or%2520months.%2520The%2520second%2520depends%2520on%2520outcome-driven%2520sampling%252C%2520which%250Aoften%2520collapses%2520due%2520to%2520the%2520rarity%2520of%2520valid%2520positive%2520trajectories%2520on%250Adomain-specialized%2520tasks.%2520We%2520introduce%2520Apollo%252C%2520a%2520sampling%2520framework%2520that%250Aintegrates%2520asynchronous%2520human%2520guidance%2520with%2520action-level%2520data%2520filtering.%250AInstead%2520of%2520requiring%2520annotators%2520to%2520shadow%2520every%2520step%252C%2520Apollo%2520allows%2520them%2520to%250Aintervene%2520only%2520when%2520the%2520agent%2520drifts%2520from%2520a%2520promising%2520trajectory%252C%2520by%2520providing%250Aprior%2520knowledge%252C%2520strategic%2520advice%252C%2520etc.%2520This%2520lightweight%2520design%2520makes%2520it%250Apossible%2520to%2520sustain%2520interactions%2520for%2520over%252030%2520hours%2520and%2520produces%2520valuable%250Atrajectories%2520at%2520a%2520lower%2520cost.%2520Apollo%2520then%2520applies%2520supervision%2520control%2520to%2520filter%250Aout%2520sub-optimal%2520actions%2520and%2520prevent%2520error%2520propagation.%2520Together%252C%2520these%250Acomponents%2520enable%2520reliable%2520and%2520effective%2520data%2520collection%2520in%2520long-horizon%250Aenvironments.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520Apollo%252C%2520we%2520evaluate%2520it%2520using%250AInnovatorBench.%2520Our%2520experiments%2520show%2520that%2520when%2520applied%2520to%2520train%2520the%2520GLM-4.5%250Amodel%2520on%2520InnovatorBench%252C%2520Apollo%2520achieves%2520more%2520than%2520a%252050%2525%2520improvement%2520over%2520the%250Auntrained%2520baseline%2520and%2520a%252028%2525%2520improvement%2520over%2520a%2520variant%2520trained%2520without%2520human%250Ainteraction.%2520These%2520results%2520highlight%2520the%2520critical%2520role%2520of%2520human-in-the-loop%250Asampling%2520and%2520the%2520robustness%2520of%2520Apollo%2527s%2520design%2520in%2520handling%2520long-horizon%252C%250Adomain-specialized%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interaction%20as%20Intelligence%20Part%20II%3A%20Asynchronous%20Human-Agent%20Rollout%0A%20%20for%20Long-Horizon%20Task%20Training&entry.906535625=Dayuan%20Fu%20and%20Yunze%20Wu%20and%20Xiaojie%20Cai%20and%20Lyumanshan%20Ye%20and%20Shijie%20Xia%20and%20Zhen%20Huang%20and%20Weiye%20Si%20and%20Tianze%20Xu%20and%20Jie%20Sun%20and%20Keyu%20Li%20and%20Mohan%20Jiang%20and%20Junfei%20Wang%20and%20Qishuo%20Hua%20and%20Pengrui%20Lu%20and%20Yang%20Xiao%20and%20Pengfei%20Liu&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20recently%20shown%20strong%20potential%20in%0Adomains%20such%20as%20automated%20coding%2C%20deep%20research%2C%20and%20graphical%20user%20interface%0Amanipulation.%20However%2C%20training%20them%20to%20succeed%20on%20long-horizon%2C%0Adomain-specialized%20tasks%20remains%20challenging.%20Current%20methods%20primarily%20fall%0Ainto%20two%20categories.%20The%20first%20relies%20on%20dense%20human%20annotations%20through%0Abehavior%20cloning%2C%20which%20is%20prohibitively%20expensive%20for%20long-horizon%20tasks%20that%0Acan%20take%20days%20or%20months.%20The%20second%20depends%20on%20outcome-driven%20sampling%2C%20which%0Aoften%20collapses%20due%20to%20the%20rarity%20of%20valid%20positive%20trajectories%20on%0Adomain-specialized%20tasks.%20We%20introduce%20Apollo%2C%20a%20sampling%20framework%20that%0Aintegrates%20asynchronous%20human%20guidance%20with%20action-level%20data%20filtering.%0AInstead%20of%20requiring%20annotators%20to%20shadow%20every%20step%2C%20Apollo%20allows%20them%20to%0Aintervene%20only%20when%20the%20agent%20drifts%20from%20a%20promising%20trajectory%2C%20by%20providing%0Aprior%20knowledge%2C%20strategic%20advice%2C%20etc.%20This%20lightweight%20design%20makes%20it%0Apossible%20to%20sustain%20interactions%20for%20over%2030%20hours%20and%20produces%20valuable%0Atrajectories%20at%20a%20lower%20cost.%20Apollo%20then%20applies%20supervision%20control%20to%20filter%0Aout%20sub-optimal%20actions%20and%20prevent%20error%20propagation.%20Together%2C%20these%0Acomponents%20enable%20reliable%20and%20effective%20data%20collection%20in%20long-horizon%0Aenvironments.%20To%20demonstrate%20the%20effectiveness%20of%20Apollo%2C%20we%20evaluate%20it%20using%0AInnovatorBench.%20Our%20experiments%20show%20that%20when%20applied%20to%20train%20the%20GLM-4.5%0Amodel%20on%20InnovatorBench%2C%20Apollo%20achieves%20more%20than%20a%2050%25%20improvement%20over%20the%0Auntrained%20baseline%20and%20a%2028%25%20improvement%20over%20a%20variant%20trained%20without%20human%0Ainteraction.%20These%20results%20highlight%20the%20critical%20role%20of%20human-in-the-loop%0Asampling%20and%20the%20robustness%20of%20Apollo%27s%20design%20in%20handling%20long-horizon%2C%0Adomain-specialized%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27630v1&entry.124074799=Read"},
{"title": "A Process Mining-Based System For The Analysis and Prediction of\n  Software Development Workflows", "author": "Ant\u00eda Dorado and Iv\u00e1n Folgueira and Sof\u00eda Mart\u00edn and Gonzalo Mart\u00edn and \u00c1lvaro Porto and Alejandro Ramos and John Wallace", "abstract": "  CodeSight is an end-to-end system designed to anticipate deadline compliance\nin software development workflows. It captures development and deployment data\ndirectly from GitHub, transforming it into process mining logs for detailed\nanalysis. From these logs, the system generates metrics and dashboards that\nprovide actionable insights into PR activity patterns and workflow efficiency.\nBuilding on this structured representation, CodeSight employs an LSTM model\nthat predicts remaining PR resolution times based on sequential activity traces\nand static features, enabling early identification of potential deadline\nbreaches. In tests, the system demonstrates high precision and F1 scores in\npredicting deadline compliance, illustrating the value of integrating process\nmining with machine learning for proactive software project management.\n", "link": "http://arxiv.org/abs/2510.25935v2", "date": "2025-10-31", "relevancy": 1.6942, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4492}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4188}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Process%20Mining-Based%20System%20For%20The%20Analysis%20and%20Prediction%20of%0A%20%20Software%20Development%20Workflows&body=Title%3A%20A%20Process%20Mining-Based%20System%20For%20The%20Analysis%20and%20Prediction%20of%0A%20%20Software%20Development%20Workflows%0AAuthor%3A%20Ant%C3%ADa%20Dorado%20and%20Iv%C3%A1n%20Folgueira%20and%20Sof%C3%ADa%20Mart%C3%ADn%20and%20Gonzalo%20Mart%C3%ADn%20and%20%C3%81lvaro%20Porto%20and%20Alejandro%20Ramos%20and%20John%20Wallace%0AAbstract%3A%20%20%20CodeSight%20is%20an%20end-to-end%20system%20designed%20to%20anticipate%20deadline%20compliance%0Ain%20software%20development%20workflows.%20It%20captures%20development%20and%20deployment%20data%0Adirectly%20from%20GitHub%2C%20transforming%20it%20into%20process%20mining%20logs%20for%20detailed%0Aanalysis.%20From%20these%20logs%2C%20the%20system%20generates%20metrics%20and%20dashboards%20that%0Aprovide%20actionable%20insights%20into%20PR%20activity%20patterns%20and%20workflow%20efficiency.%0ABuilding%20on%20this%20structured%20representation%2C%20CodeSight%20employs%20an%20LSTM%20model%0Athat%20predicts%20remaining%20PR%20resolution%20times%20based%20on%20sequential%20activity%20traces%0Aand%20static%20features%2C%20enabling%20early%20identification%20of%20potential%20deadline%0Abreaches.%20In%20tests%2C%20the%20system%20demonstrates%20high%20precision%20and%20F1%20scores%20in%0Apredicting%20deadline%20compliance%2C%20illustrating%20the%20value%20of%20integrating%20process%0Amining%20with%20machine%20learning%20for%20proactive%20software%20project%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.25935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Process%2520Mining-Based%2520System%2520For%2520The%2520Analysis%2520and%2520Prediction%2520of%250A%2520%2520Software%2520Development%2520Workflows%26entry.906535625%3DAnt%25C3%25ADa%2520Dorado%2520and%2520Iv%25C3%25A1n%2520Folgueira%2520and%2520Sof%25C3%25ADa%2520Mart%25C3%25ADn%2520and%2520Gonzalo%2520Mart%25C3%25ADn%2520and%2520%25C3%2581lvaro%2520Porto%2520and%2520Alejandro%2520Ramos%2520and%2520John%2520Wallace%26entry.1292438233%3D%2520%2520CodeSight%2520is%2520an%2520end-to-end%2520system%2520designed%2520to%2520anticipate%2520deadline%2520compliance%250Ain%2520software%2520development%2520workflows.%2520It%2520captures%2520development%2520and%2520deployment%2520data%250Adirectly%2520from%2520GitHub%252C%2520transforming%2520it%2520into%2520process%2520mining%2520logs%2520for%2520detailed%250Aanalysis.%2520From%2520these%2520logs%252C%2520the%2520system%2520generates%2520metrics%2520and%2520dashboards%2520that%250Aprovide%2520actionable%2520insights%2520into%2520PR%2520activity%2520patterns%2520and%2520workflow%2520efficiency.%250ABuilding%2520on%2520this%2520structured%2520representation%252C%2520CodeSight%2520employs%2520an%2520LSTM%2520model%250Athat%2520predicts%2520remaining%2520PR%2520resolution%2520times%2520based%2520on%2520sequential%2520activity%2520traces%250Aand%2520static%2520features%252C%2520enabling%2520early%2520identification%2520of%2520potential%2520deadline%250Abreaches.%2520In%2520tests%252C%2520the%2520system%2520demonstrates%2520high%2520precision%2520and%2520F1%2520scores%2520in%250Apredicting%2520deadline%2520compliance%252C%2520illustrating%2520the%2520value%2520of%2520integrating%2520process%250Amining%2520with%2520machine%2520learning%2520for%2520proactive%2520software%2520project%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Process%20Mining-Based%20System%20For%20The%20Analysis%20and%20Prediction%20of%0A%20%20Software%20Development%20Workflows&entry.906535625=Ant%C3%ADa%20Dorado%20and%20Iv%C3%A1n%20Folgueira%20and%20Sof%C3%ADa%20Mart%C3%ADn%20and%20Gonzalo%20Mart%C3%ADn%20and%20%C3%81lvaro%20Porto%20and%20Alejandro%20Ramos%20and%20John%20Wallace&entry.1292438233=%20%20CodeSight%20is%20an%20end-to-end%20system%20designed%20to%20anticipate%20deadline%20compliance%0Ain%20software%20development%20workflows.%20It%20captures%20development%20and%20deployment%20data%0Adirectly%20from%20GitHub%2C%20transforming%20it%20into%20process%20mining%20logs%20for%20detailed%0Aanalysis.%20From%20these%20logs%2C%20the%20system%20generates%20metrics%20and%20dashboards%20that%0Aprovide%20actionable%20insights%20into%20PR%20activity%20patterns%20and%20workflow%20efficiency.%0ABuilding%20on%20this%20structured%20representation%2C%20CodeSight%20employs%20an%20LSTM%20model%0Athat%20predicts%20remaining%20PR%20resolution%20times%20based%20on%20sequential%20activity%20traces%0Aand%20static%20features%2C%20enabling%20early%20identification%20of%20potential%20deadline%0Abreaches.%20In%20tests%2C%20the%20system%20demonstrates%20high%20precision%20and%20F1%20scores%20in%0Apredicting%20deadline%20compliance%2C%20illustrating%20the%20value%20of%20integrating%20process%0Amining%20with%20machine%20learning%20for%20proactive%20software%20project%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.25935v2&entry.124074799=Read"},
{"title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals", "author": "Xiangyu Fan and Zesong Qiu and Zhuguanyu Wu and Fanzhou Wang and Zhiqian Lin and Tianxiang Ren and Dahua Lin and Ruihao Gong and Lei Yang", "abstract": "  Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.\n", "link": "http://arxiv.org/abs/2510.27684v1", "date": "2025-10-31", "relevancy": 1.6878, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.558}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phased%20DMD%3A%20Few-step%20Distribution%20Matching%20Distillation%20via%20Score%0A%20%20Matching%20within%20Subintervals&body=Title%3A%20Phased%20DMD%3A%20Few-step%20Distribution%20Matching%20Distillation%20via%20Score%0A%20%20Matching%20within%20Subintervals%0AAuthor%3A%20Xiangyu%20Fan%20and%20Zesong%20Qiu%20and%20Zhuguanyu%20Wu%20and%20Fanzhou%20Wang%20and%20Zhiqian%20Lin%20and%20Tianxiang%20Ren%20and%20Dahua%20Lin%20and%20Ruihao%20Gong%20and%20Lei%20Yang%0AAbstract%3A%20%20%20Distribution%20Matching%20Distillation%20%28DMD%29%20distills%20score-based%20generative%0Amodels%20into%20efficient%20one-step%20generators%2C%20without%20requiring%20a%20one-to-one%0Acorrespondence%20with%20the%20sampling%20trajectories%20of%20their%20teachers.%20However%2C%0Alimited%20model%20capacity%20causes%20one-step%20distilled%20models%20underperform%20on%20complex%0Agenerative%20tasks%2C%20e.g.%2C%20synthesizing%20intricate%20object%20motions%20in%20text-to-video%0Ageneration.%20Directly%20extending%20DMD%20to%20multi-step%20distillation%20increases%20memory%0Ausage%20and%20computational%20depth%2C%20leading%20to%20instability%20and%20reduced%20efficiency.%0AWhile%20prior%20works%20propose%20stochastic%20gradient%20truncation%20as%20a%20potential%0Asolution%2C%20we%20observe%20that%20it%20substantially%20reduces%20the%20generation%20diversity%20of%0Amulti-step%20distilled%20models%2C%20bringing%20it%20down%20to%20the%20level%20of%20their%20one-step%0Acounterparts.%20To%20address%20these%20limitations%2C%20we%20propose%20Phased%20DMD%2C%20a%20multi-step%0Adistillation%20framework%20that%20bridges%20the%20idea%20of%20phase-wise%20distillation%20with%0AMixture-of-Experts%20%28MoE%29%2C%20reducing%20learning%20difficulty%20while%20enhancing%20model%0Acapacity.%20Phased%20DMD%20is%20built%20upon%20two%20key%20ideas%3A%20progressive%20distribution%0Amatching%20and%20score%20matching%20within%20subintervals.%20First%2C%20our%20model%20divides%20the%0ASNR%20range%20into%20subintervals%2C%20progressively%20refining%20the%20model%20to%20higher%20SNR%0Alevels%2C%20to%20better%20capture%20complex%20distributions.%20Next%2C%20to%20ensure%20the%20training%0Aobjective%20within%20each%20subinterval%20is%20accurate%2C%20we%20have%20conducted%20rigorous%0Amathematical%20derivations.%20We%20validate%20Phased%20DMD%20by%20distilling%20state-of-the-art%0Aimage%20and%20video%20generation%20models%2C%20including%20Qwen-Image%20%2820B%20parameters%29%20and%0AWan2.2%20%2828B%20parameters%29.%20Experimental%20results%20demonstrate%20that%20Phased%20DMD%0Apreserves%20output%20diversity%20better%20than%20DMD%20while%20retaining%20key%20generative%0Acapabilities.%20We%20will%20release%20our%20code%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhased%2520DMD%253A%2520Few-step%2520Distribution%2520Matching%2520Distillation%2520via%2520Score%250A%2520%2520Matching%2520within%2520Subintervals%26entry.906535625%3DXiangyu%2520Fan%2520and%2520Zesong%2520Qiu%2520and%2520Zhuguanyu%2520Wu%2520and%2520Fanzhou%2520Wang%2520and%2520Zhiqian%2520Lin%2520and%2520Tianxiang%2520Ren%2520and%2520Dahua%2520Lin%2520and%2520Ruihao%2520Gong%2520and%2520Lei%2520Yang%26entry.1292438233%3D%2520%2520Distribution%2520Matching%2520Distillation%2520%2528DMD%2529%2520distills%2520score-based%2520generative%250Amodels%2520into%2520efficient%2520one-step%2520generators%252C%2520without%2520requiring%2520a%2520one-to-one%250Acorrespondence%2520with%2520the%2520sampling%2520trajectories%2520of%2520their%2520teachers.%2520However%252C%250Alimited%2520model%2520capacity%2520causes%2520one-step%2520distilled%2520models%2520underperform%2520on%2520complex%250Agenerative%2520tasks%252C%2520e.g.%252C%2520synthesizing%2520intricate%2520object%2520motions%2520in%2520text-to-video%250Ageneration.%2520Directly%2520extending%2520DMD%2520to%2520multi-step%2520distillation%2520increases%2520memory%250Ausage%2520and%2520computational%2520depth%252C%2520leading%2520to%2520instability%2520and%2520reduced%2520efficiency.%250AWhile%2520prior%2520works%2520propose%2520stochastic%2520gradient%2520truncation%2520as%2520a%2520potential%250Asolution%252C%2520we%2520observe%2520that%2520it%2520substantially%2520reduces%2520the%2520generation%2520diversity%2520of%250Amulti-step%2520distilled%2520models%252C%2520bringing%2520it%2520down%2520to%2520the%2520level%2520of%2520their%2520one-step%250Acounterparts.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Phased%2520DMD%252C%2520a%2520multi-step%250Adistillation%2520framework%2520that%2520bridges%2520the%2520idea%2520of%2520phase-wise%2520distillation%2520with%250AMixture-of-Experts%2520%2528MoE%2529%252C%2520reducing%2520learning%2520difficulty%2520while%2520enhancing%2520model%250Acapacity.%2520Phased%2520DMD%2520is%2520built%2520upon%2520two%2520key%2520ideas%253A%2520progressive%2520distribution%250Amatching%2520and%2520score%2520matching%2520within%2520subintervals.%2520First%252C%2520our%2520model%2520divides%2520the%250ASNR%2520range%2520into%2520subintervals%252C%2520progressively%2520refining%2520the%2520model%2520to%2520higher%2520SNR%250Alevels%252C%2520to%2520better%2520capture%2520complex%2520distributions.%2520Next%252C%2520to%2520ensure%2520the%2520training%250Aobjective%2520within%2520each%2520subinterval%2520is%2520accurate%252C%2520we%2520have%2520conducted%2520rigorous%250Amathematical%2520derivations.%2520We%2520validate%2520Phased%2520DMD%2520by%2520distilling%2520state-of-the-art%250Aimage%2520and%2520video%2520generation%2520models%252C%2520including%2520Qwen-Image%2520%252820B%2520parameters%2529%2520and%250AWan2.2%2520%252828B%2520parameters%2529.%2520Experimental%2520results%2520demonstrate%2520that%2520Phased%2520DMD%250Apreserves%2520output%2520diversity%2520better%2520than%2520DMD%2520while%2520retaining%2520key%2520generative%250Acapabilities.%2520We%2520will%2520release%2520our%2520code%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phased%20DMD%3A%20Few-step%20Distribution%20Matching%20Distillation%20via%20Score%0A%20%20Matching%20within%20Subintervals&entry.906535625=Xiangyu%20Fan%20and%20Zesong%20Qiu%20and%20Zhuguanyu%20Wu%20and%20Fanzhou%20Wang%20and%20Zhiqian%20Lin%20and%20Tianxiang%20Ren%20and%20Dahua%20Lin%20and%20Ruihao%20Gong%20and%20Lei%20Yang&entry.1292438233=%20%20Distribution%20Matching%20Distillation%20%28DMD%29%20distills%20score-based%20generative%0Amodels%20into%20efficient%20one-step%20generators%2C%20without%20requiring%20a%20one-to-one%0Acorrespondence%20with%20the%20sampling%20trajectories%20of%20their%20teachers.%20However%2C%0Alimited%20model%20capacity%20causes%20one-step%20distilled%20models%20underperform%20on%20complex%0Agenerative%20tasks%2C%20e.g.%2C%20synthesizing%20intricate%20object%20motions%20in%20text-to-video%0Ageneration.%20Directly%20extending%20DMD%20to%20multi-step%20distillation%20increases%20memory%0Ausage%20and%20computational%20depth%2C%20leading%20to%20instability%20and%20reduced%20efficiency.%0AWhile%20prior%20works%20propose%20stochastic%20gradient%20truncation%20as%20a%20potential%0Asolution%2C%20we%20observe%20that%20it%20substantially%20reduces%20the%20generation%20diversity%20of%0Amulti-step%20distilled%20models%2C%20bringing%20it%20down%20to%20the%20level%20of%20their%20one-step%0Acounterparts.%20To%20address%20these%20limitations%2C%20we%20propose%20Phased%20DMD%2C%20a%20multi-step%0Adistillation%20framework%20that%20bridges%20the%20idea%20of%20phase-wise%20distillation%20with%0AMixture-of-Experts%20%28MoE%29%2C%20reducing%20learning%20difficulty%20while%20enhancing%20model%0Acapacity.%20Phased%20DMD%20is%20built%20upon%20two%20key%20ideas%3A%20progressive%20distribution%0Amatching%20and%20score%20matching%20within%20subintervals.%20First%2C%20our%20model%20divides%20the%0ASNR%20range%20into%20subintervals%2C%20progressively%20refining%20the%20model%20to%20higher%20SNR%0Alevels%2C%20to%20better%20capture%20complex%20distributions.%20Next%2C%20to%20ensure%20the%20training%0Aobjective%20within%20each%20subinterval%20is%20accurate%2C%20we%20have%20conducted%20rigorous%0Amathematical%20derivations.%20We%20validate%20Phased%20DMD%20by%20distilling%20state-of-the-art%0Aimage%20and%20video%20generation%20models%2C%20including%20Qwen-Image%20%2820B%20parameters%29%20and%0AWan2.2%20%2828B%20parameters%29.%20Experimental%20results%20demonstrate%20that%20Phased%20DMD%0Apreserves%20output%20diversity%20better%20than%20DMD%20while%20retaining%20key%20generative%0Acapabilities.%20We%20will%20release%20our%20code%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27684v1&entry.124074799=Read"},
{"title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing\n  Discrete Diffusion Models", "author": "Yuchen Liang and Renxiang Huang and Lifeng Lai and Ness Shroff and Yingbin Liang", "abstract": "  Discrete state space diffusion models have shown significant advantages in\napplications involving discrete data, such as text and image generation. It has\nalso been observed that their performance is highly sensitive to the choice of\nrate matrices, particularly between uniform and absorbing rate matrices. While\nempirical results suggest that absorbing rate matrices often yield better\ngeneration quality compared to uniform rate matrices, existing theoretical\nworks have largely focused on the uniform rate matrices case. Notably,\nconvergence guarantees and error analyses for absorbing diffusion models are\nstill missing. In this work, we provide the first finite-time error bounds and\nconvergence rate analysis for discrete diffusion models using absorbing rate\nmatrices. We begin by deriving an upper bound on the KL divergence of the\nforward process, introducing a surrogate initialization distribution to address\nthe challenge posed by the absorbing stationary distribution, which is a\nsingleton and causes the KL divergence to be ill-defined. We then establish the\nfirst convergence guarantees for both the $\\tau$-leaping and uniformization\nsamplers under absorbing rate matrices, demonstrating improved rates over their\ncounterparts using uniform rate matrices. Furthermore, under suitable\nassumptions, we provide convergence guarantees without early stopping. Our\nanalysis introduces several new technical tools to address challenges unique to\nabsorbing rate matrices. These include a Jensen-type argument for bounding\nforward process convergence, novel techniques for bounding absorbing score\nfunctions, and a non-divergent upper bound on the score near initialization\nthat removes the need of early-stopping.\n", "link": "http://arxiv.org/abs/2506.02318v3", "date": "2025-10-31", "relevancy": 1.6506, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5834}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5408}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Absorb%20and%20Converge%3A%20Provable%20Convergence%20Guarantee%20for%20Absorbing%0A%20%20Discrete%20Diffusion%20Models&body=Title%3A%20Absorb%20and%20Converge%3A%20Provable%20Convergence%20Guarantee%20for%20Absorbing%0A%20%20Discrete%20Diffusion%20Models%0AAuthor%3A%20Yuchen%20Liang%20and%20Renxiang%20Huang%20and%20Lifeng%20Lai%20and%20Ness%20Shroff%20and%20Yingbin%20Liang%0AAbstract%3A%20%20%20Discrete%20state%20space%20diffusion%20models%20have%20shown%20significant%20advantages%20in%0Aapplications%20involving%20discrete%20data%2C%20such%20as%20text%20and%20image%20generation.%20It%20has%0Aalso%20been%20observed%20that%20their%20performance%20is%20highly%20sensitive%20to%20the%20choice%20of%0Arate%20matrices%2C%20particularly%20between%20uniform%20and%20absorbing%20rate%20matrices.%20While%0Aempirical%20results%20suggest%20that%20absorbing%20rate%20matrices%20often%20yield%20better%0Ageneration%20quality%20compared%20to%20uniform%20rate%20matrices%2C%20existing%20theoretical%0Aworks%20have%20largely%20focused%20on%20the%20uniform%20rate%20matrices%20case.%20Notably%2C%0Aconvergence%20guarantees%20and%20error%20analyses%20for%20absorbing%20diffusion%20models%20are%0Astill%20missing.%20In%20this%20work%2C%20we%20provide%20the%20first%20finite-time%20error%20bounds%20and%0Aconvergence%20rate%20analysis%20for%20discrete%20diffusion%20models%20using%20absorbing%20rate%0Amatrices.%20We%20begin%20by%20deriving%20an%20upper%20bound%20on%20the%20KL%20divergence%20of%20the%0Aforward%20process%2C%20introducing%20a%20surrogate%20initialization%20distribution%20to%20address%0Athe%20challenge%20posed%20by%20the%20absorbing%20stationary%20distribution%2C%20which%20is%20a%0Asingleton%20and%20causes%20the%20KL%20divergence%20to%20be%20ill-defined.%20We%20then%20establish%20the%0Afirst%20convergence%20guarantees%20for%20both%20the%20%24%5Ctau%24-leaping%20and%20uniformization%0Asamplers%20under%20absorbing%20rate%20matrices%2C%20demonstrating%20improved%20rates%20over%20their%0Acounterparts%20using%20uniform%20rate%20matrices.%20Furthermore%2C%20under%20suitable%0Aassumptions%2C%20we%20provide%20convergence%20guarantees%20without%20early%20stopping.%20Our%0Aanalysis%20introduces%20several%20new%20technical%20tools%20to%20address%20challenges%20unique%20to%0Aabsorbing%20rate%20matrices.%20These%20include%20a%20Jensen-type%20argument%20for%20bounding%0Aforward%20process%20convergence%2C%20novel%20techniques%20for%20bounding%20absorbing%20score%0Afunctions%2C%20and%20a%20non-divergent%20upper%20bound%20on%20the%20score%20near%20initialization%0Athat%20removes%20the%20need%20of%20early-stopping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02318v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbsorb%2520and%2520Converge%253A%2520Provable%2520Convergence%2520Guarantee%2520for%2520Absorbing%250A%2520%2520Discrete%2520Diffusion%2520Models%26entry.906535625%3DYuchen%2520Liang%2520and%2520Renxiang%2520Huang%2520and%2520Lifeng%2520Lai%2520and%2520Ness%2520Shroff%2520and%2520Yingbin%2520Liang%26entry.1292438233%3D%2520%2520Discrete%2520state%2520space%2520diffusion%2520models%2520have%2520shown%2520significant%2520advantages%2520in%250Aapplications%2520involving%2520discrete%2520data%252C%2520such%2520as%2520text%2520and%2520image%2520generation.%2520It%2520has%250Aalso%2520been%2520observed%2520that%2520their%2520performance%2520is%2520highly%2520sensitive%2520to%2520the%2520choice%2520of%250Arate%2520matrices%252C%2520particularly%2520between%2520uniform%2520and%2520absorbing%2520rate%2520matrices.%2520While%250Aempirical%2520results%2520suggest%2520that%2520absorbing%2520rate%2520matrices%2520often%2520yield%2520better%250Ageneration%2520quality%2520compared%2520to%2520uniform%2520rate%2520matrices%252C%2520existing%2520theoretical%250Aworks%2520have%2520largely%2520focused%2520on%2520the%2520uniform%2520rate%2520matrices%2520case.%2520Notably%252C%250Aconvergence%2520guarantees%2520and%2520error%2520analyses%2520for%2520absorbing%2520diffusion%2520models%2520are%250Astill%2520missing.%2520In%2520this%2520work%252C%2520we%2520provide%2520the%2520first%2520finite-time%2520error%2520bounds%2520and%250Aconvergence%2520rate%2520analysis%2520for%2520discrete%2520diffusion%2520models%2520using%2520absorbing%2520rate%250Amatrices.%2520We%2520begin%2520by%2520deriving%2520an%2520upper%2520bound%2520on%2520the%2520KL%2520divergence%2520of%2520the%250Aforward%2520process%252C%2520introducing%2520a%2520surrogate%2520initialization%2520distribution%2520to%2520address%250Athe%2520challenge%2520posed%2520by%2520the%2520absorbing%2520stationary%2520distribution%252C%2520which%2520is%2520a%250Asingleton%2520and%2520causes%2520the%2520KL%2520divergence%2520to%2520be%2520ill-defined.%2520We%2520then%2520establish%2520the%250Afirst%2520convergence%2520guarantees%2520for%2520both%2520the%2520%2524%255Ctau%2524-leaping%2520and%2520uniformization%250Asamplers%2520under%2520absorbing%2520rate%2520matrices%252C%2520demonstrating%2520improved%2520rates%2520over%2520their%250Acounterparts%2520using%2520uniform%2520rate%2520matrices.%2520Furthermore%252C%2520under%2520suitable%250Aassumptions%252C%2520we%2520provide%2520convergence%2520guarantees%2520without%2520early%2520stopping.%2520Our%250Aanalysis%2520introduces%2520several%2520new%2520technical%2520tools%2520to%2520address%2520challenges%2520unique%2520to%250Aabsorbing%2520rate%2520matrices.%2520These%2520include%2520a%2520Jensen-type%2520argument%2520for%2520bounding%250Aforward%2520process%2520convergence%252C%2520novel%2520techniques%2520for%2520bounding%2520absorbing%2520score%250Afunctions%252C%2520and%2520a%2520non-divergent%2520upper%2520bound%2520on%2520the%2520score%2520near%2520initialization%250Athat%2520removes%2520the%2520need%2520of%2520early-stopping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02318v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Absorb%20and%20Converge%3A%20Provable%20Convergence%20Guarantee%20for%20Absorbing%0A%20%20Discrete%20Diffusion%20Models&entry.906535625=Yuchen%20Liang%20and%20Renxiang%20Huang%20and%20Lifeng%20Lai%20and%20Ness%20Shroff%20and%20Yingbin%20Liang&entry.1292438233=%20%20Discrete%20state%20space%20diffusion%20models%20have%20shown%20significant%20advantages%20in%0Aapplications%20involving%20discrete%20data%2C%20such%20as%20text%20and%20image%20generation.%20It%20has%0Aalso%20been%20observed%20that%20their%20performance%20is%20highly%20sensitive%20to%20the%20choice%20of%0Arate%20matrices%2C%20particularly%20between%20uniform%20and%20absorbing%20rate%20matrices.%20While%0Aempirical%20results%20suggest%20that%20absorbing%20rate%20matrices%20often%20yield%20better%0Ageneration%20quality%20compared%20to%20uniform%20rate%20matrices%2C%20existing%20theoretical%0Aworks%20have%20largely%20focused%20on%20the%20uniform%20rate%20matrices%20case.%20Notably%2C%0Aconvergence%20guarantees%20and%20error%20analyses%20for%20absorbing%20diffusion%20models%20are%0Astill%20missing.%20In%20this%20work%2C%20we%20provide%20the%20first%20finite-time%20error%20bounds%20and%0Aconvergence%20rate%20analysis%20for%20discrete%20diffusion%20models%20using%20absorbing%20rate%0Amatrices.%20We%20begin%20by%20deriving%20an%20upper%20bound%20on%20the%20KL%20divergence%20of%20the%0Aforward%20process%2C%20introducing%20a%20surrogate%20initialization%20distribution%20to%20address%0Athe%20challenge%20posed%20by%20the%20absorbing%20stationary%20distribution%2C%20which%20is%20a%0Asingleton%20and%20causes%20the%20KL%20divergence%20to%20be%20ill-defined.%20We%20then%20establish%20the%0Afirst%20convergence%20guarantees%20for%20both%20the%20%24%5Ctau%24-leaping%20and%20uniformization%0Asamplers%20under%20absorbing%20rate%20matrices%2C%20demonstrating%20improved%20rates%20over%20their%0Acounterparts%20using%20uniform%20rate%20matrices.%20Furthermore%2C%20under%20suitable%0Aassumptions%2C%20we%20provide%20convergence%20guarantees%20without%20early%20stopping.%20Our%0Aanalysis%20introduces%20several%20new%20technical%20tools%20to%20address%20challenges%20unique%20to%0Aabsorbing%20rate%20matrices.%20These%20include%20a%20Jensen-type%20argument%20for%20bounding%0Aforward%20process%20convergence%2C%20novel%20techniques%20for%20bounding%20absorbing%20score%0Afunctions%2C%20and%20a%20non-divergent%20upper%20bound%20on%20the%20score%20near%20initialization%0Athat%20removes%20the%20need%20of%20early-stopping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02318v3&entry.124074799=Read"},
{"title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation", "author": "Juntao Jiang and Mengmeng Wang and Huizhong Tian and Lingbo Cheng and Yong Liu", "abstract": "  While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at https://github.com/juntaoJianggavin/LV-UNet.\n", "link": "http://arxiv.org/abs/2408.16886v4", "date": "2025-10-31", "relevancy": 1.6135, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5624}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5383}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation&body=Title%3A%20LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Juntao%20Jiang%20and%20Mengmeng%20Wang%20and%20Huizhong%20Tian%20and%20Lingbo%20Cheng%20and%20Yong%20Liu%0AAbstract%3A%20%20%20While%20large%20models%20have%20achieved%20significant%20progress%20in%20computer%20vision%2C%0Achallenges%20such%20as%20optimization%20complexity%2C%20the%20intricacy%20of%20transformer%0Aarchitectures%2C%20computational%20constraints%2C%20and%20practical%20application%20demands%0Ahighlight%20the%20importance%20of%20simpler%20model%20designs%20in%20medical%20image%0Asegmentation.%20This%20need%20is%20particularly%20pronounced%20in%20mobile%20medical%20devices%2C%0Awhich%20require%20lightweight%2C%20deployable%20models%20with%20real-time%20performance.%0AHowever%2C%20existing%20lightweight%20models%20often%20suffer%20from%20poor%20robustness%20across%0Adatasets%2C%20limiting%20their%20widespread%20adoption.%20To%20address%20these%20challenges%2C%20this%0Apaper%20introduces%20LV-UNet%2C%20a%20lightweight%20and%20vanilla%20model%20that%20leverages%0Apre-trained%20MobileNetv3-Large%20backbones%20and%20incorporates%20fusible%20modules.%0ALV-UNet%20employs%20an%20enhanced%20deep%20training%20strategy%20and%20switches%20to%20a%20deployment%0Amode%20during%20inference%20by%20re-parametrization%2C%20significantly%20reducing%20parameter%0Acount%20and%20computational%20overhead.%20Experimental%20results%20on%20ISIC%202016%2C%20BUSI%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20and%20Kvair-SEG%20datasets%20demonstrate%20a%20better%0Atrade-off%20between%20performance%20and%20the%20computational%20load.%20The%20code%20will%20be%0Areleased%20at%20https%3A//github.com/juntaoJianggavin/LV-UNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16886v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLV-UNet%253A%2520A%2520Lightweight%2520and%2520Vanilla%2520Model%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJuntao%2520Jiang%2520and%2520Mengmeng%2520Wang%2520and%2520Huizhong%2520Tian%2520and%2520Lingbo%2520Cheng%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520While%2520large%2520models%2520have%2520achieved%2520significant%2520progress%2520in%2520computer%2520vision%252C%250Achallenges%2520such%2520as%2520optimization%2520complexity%252C%2520the%2520intricacy%2520of%2520transformer%250Aarchitectures%252C%2520computational%2520constraints%252C%2520and%2520practical%2520application%2520demands%250Ahighlight%2520the%2520importance%2520of%2520simpler%2520model%2520designs%2520in%2520medical%2520image%250Asegmentation.%2520This%2520need%2520is%2520particularly%2520pronounced%2520in%2520mobile%2520medical%2520devices%252C%250Awhich%2520require%2520lightweight%252C%2520deployable%2520models%2520with%2520real-time%2520performance.%250AHowever%252C%2520existing%2520lightweight%2520models%2520often%2520suffer%2520from%2520poor%2520robustness%2520across%250Adatasets%252C%2520limiting%2520their%2520widespread%2520adoption.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520introduces%2520LV-UNet%252C%2520a%2520lightweight%2520and%2520vanilla%2520model%2520that%2520leverages%250Apre-trained%2520MobileNetv3-Large%2520backbones%2520and%2520incorporates%2520fusible%2520modules.%250ALV-UNet%2520employs%2520an%2520enhanced%2520deep%2520training%2520strategy%2520and%2520switches%2520to%2520a%2520deployment%250Amode%2520during%2520inference%2520by%2520re-parametrization%252C%2520significantly%2520reducing%2520parameter%250Acount%2520and%2520computational%2520overhead.%2520Experimental%2520results%2520on%2520ISIC%25202016%252C%2520BUSI%252C%250ACVC-ClinicDB%252C%2520CVC-ColonDB%252C%2520and%2520Kvair-SEG%2520datasets%2520demonstrate%2520a%2520better%250Atrade-off%2520between%2520performance%2520and%2520the%2520computational%2520load.%2520The%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/juntaoJianggavin/LV-UNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16886v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LV-UNet%3A%20A%20Lightweight%20and%20Vanilla%20Model%20for%20Medical%20Image%20Segmentation&entry.906535625=Juntao%20Jiang%20and%20Mengmeng%20Wang%20and%20Huizhong%20Tian%20and%20Lingbo%20Cheng%20and%20Yong%20Liu&entry.1292438233=%20%20While%20large%20models%20have%20achieved%20significant%20progress%20in%20computer%20vision%2C%0Achallenges%20such%20as%20optimization%20complexity%2C%20the%20intricacy%20of%20transformer%0Aarchitectures%2C%20computational%20constraints%2C%20and%20practical%20application%20demands%0Ahighlight%20the%20importance%20of%20simpler%20model%20designs%20in%20medical%20image%0Asegmentation.%20This%20need%20is%20particularly%20pronounced%20in%20mobile%20medical%20devices%2C%0Awhich%20require%20lightweight%2C%20deployable%20models%20with%20real-time%20performance.%0AHowever%2C%20existing%20lightweight%20models%20often%20suffer%20from%20poor%20robustness%20across%0Adatasets%2C%20limiting%20their%20widespread%20adoption.%20To%20address%20these%20challenges%2C%20this%0Apaper%20introduces%20LV-UNet%2C%20a%20lightweight%20and%20vanilla%20model%20that%20leverages%0Apre-trained%20MobileNetv3-Large%20backbones%20and%20incorporates%20fusible%20modules.%0ALV-UNet%20employs%20an%20enhanced%20deep%20training%20strategy%20and%20switches%20to%20a%20deployment%0Amode%20during%20inference%20by%20re-parametrization%2C%20significantly%20reducing%20parameter%0Acount%20and%20computational%20overhead.%20Experimental%20results%20on%20ISIC%202016%2C%20BUSI%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20and%20Kvair-SEG%20datasets%20demonstrate%20a%20better%0Atrade-off%20between%20performance%20and%20the%20computational%20load.%20The%20code%20will%20be%0Areleased%20at%20https%3A//github.com/juntaoJianggavin/LV-UNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16886v4&entry.124074799=Read"},
{"title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities", "author": "Travis Davies and Yiqi Huang and Alexi Gladstone and Yunxin Liu and Xiang Chen and Heng Ji and Huxian Liu and Luhui Hu", "abstract": "  Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.\n", "link": "http://arxiv.org/abs/2510.27545v1", "date": "2025-10-31", "relevancy": 1.6086, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5332}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EBT-Policy%3A%20Energy%20Unlocks%20Emergent%20Physical%20Reasoning%20Capabilities&body=Title%3A%20EBT-Policy%3A%20Energy%20Unlocks%20Emergent%20Physical%20Reasoning%20Capabilities%0AAuthor%3A%20Travis%20Davies%20and%20Yiqi%20Huang%20and%20Alexi%20Gladstone%20and%20Yunxin%20Liu%20and%20Xiang%20Chen%20and%20Heng%20Ji%20and%20Huxian%20Liu%20and%20Luhui%20Hu%0AAbstract%3A%20%20%20Implicit%20policies%20parameterized%20by%20generative%20models%2C%20such%20as%20Diffusion%0APolicy%2C%20have%20become%20the%20standard%20for%20policy%20learning%20and%20Vision-Language-Action%0A%28VLA%29%20models%20in%20robotics.%20However%2C%20these%20approaches%20often%20suffer%20from%20high%0Acomputational%20cost%2C%20exposure%20bias%2C%20and%20unstable%20inference%20dynamics%2C%20which%20lead%0Ato%20divergence%20under%20distribution%20shifts.%20Energy-Based%20Models%20%28EBMs%29%20address%0Athese%20issues%20by%20learning%20energy%20landscapes%20end-to-end%20and%20modeling%20equilibrium%0Adynamics%2C%20offering%20improved%20robustness%20and%20reduced%20exposure%20bias.%20Yet%2C%20policies%0Aparameterized%20by%20EBMs%20have%20historically%20struggled%20to%20scale%20effectively.%20Recent%0Awork%20on%20Energy-Based%20Transformers%20%28EBTs%29%20demonstrates%20the%20scalability%20of%20EBMs%0Ato%20high-dimensional%20spaces%2C%20but%20their%20potential%20for%20solving%20core%20challenges%20in%0Aphysically%20embodied%20models%20remains%20underexplored.%20We%20introduce%20a%20new%0Aenergy-based%20architecture%2C%20EBT-Policy%2C%20that%20solves%20core%20issues%20in%20robotic%20and%0Areal-world%20settings.%20Across%20simulated%20and%20real-world%20tasks%2C%20EBT-Policy%0Aconsistently%20outperforms%20diffusion-based%20policies%2C%20while%20requiring%20less%0Atraining%20and%20inference%20computation.%20Remarkably%2C%20on%20some%20tasks%20it%20converges%0Awithin%20just%20two%20inference%20steps%2C%20a%2050x%20reduction%20compared%20to%20Diffusion%20Policy%27s%0A100.%20Moreover%2C%20EBT-Policy%20exhibits%20emergent%20capabilities%20not%20seen%20in%20prior%0Amodels%2C%20such%20as%20zero-shot%20recovery%20from%20failed%20action%20sequences%20using%20only%0Abehavior%20cloning%20and%20without%20explicit%20retry%20training.%20By%20leveraging%20its%20scalar%0Aenergy%20for%20uncertainty-aware%20inference%20and%20dynamic%20compute%20allocation%2C%0AEBT-Policy%20offers%20a%20promising%20path%20toward%20robust%2C%20generalizable%20robot%20behavior%0Aunder%20distribution%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEBT-Policy%253A%2520Energy%2520Unlocks%2520Emergent%2520Physical%2520Reasoning%2520Capabilities%26entry.906535625%3DTravis%2520Davies%2520and%2520Yiqi%2520Huang%2520and%2520Alexi%2520Gladstone%2520and%2520Yunxin%2520Liu%2520and%2520Xiang%2520Chen%2520and%2520Heng%2520Ji%2520and%2520Huxian%2520Liu%2520and%2520Luhui%2520Hu%26entry.1292438233%3D%2520%2520Implicit%2520policies%2520parameterized%2520by%2520generative%2520models%252C%2520such%2520as%2520Diffusion%250APolicy%252C%2520have%2520become%2520the%2520standard%2520for%2520policy%2520learning%2520and%2520Vision-Language-Action%250A%2528VLA%2529%2520models%2520in%2520robotics.%2520However%252C%2520these%2520approaches%2520often%2520suffer%2520from%2520high%250Acomputational%2520cost%252C%2520exposure%2520bias%252C%2520and%2520unstable%2520inference%2520dynamics%252C%2520which%2520lead%250Ato%2520divergence%2520under%2520distribution%2520shifts.%2520Energy-Based%2520Models%2520%2528EBMs%2529%2520address%250Athese%2520issues%2520by%2520learning%2520energy%2520landscapes%2520end-to-end%2520and%2520modeling%2520equilibrium%250Adynamics%252C%2520offering%2520improved%2520robustness%2520and%2520reduced%2520exposure%2520bias.%2520Yet%252C%2520policies%250Aparameterized%2520by%2520EBMs%2520have%2520historically%2520struggled%2520to%2520scale%2520effectively.%2520Recent%250Awork%2520on%2520Energy-Based%2520Transformers%2520%2528EBTs%2529%2520demonstrates%2520the%2520scalability%2520of%2520EBMs%250Ato%2520high-dimensional%2520spaces%252C%2520but%2520their%2520potential%2520for%2520solving%2520core%2520challenges%2520in%250Aphysically%2520embodied%2520models%2520remains%2520underexplored.%2520We%2520introduce%2520a%2520new%250Aenergy-based%2520architecture%252C%2520EBT-Policy%252C%2520that%2520solves%2520core%2520issues%2520in%2520robotic%2520and%250Areal-world%2520settings.%2520Across%2520simulated%2520and%2520real-world%2520tasks%252C%2520EBT-Policy%250Aconsistently%2520outperforms%2520diffusion-based%2520policies%252C%2520while%2520requiring%2520less%250Atraining%2520and%2520inference%2520computation.%2520Remarkably%252C%2520on%2520some%2520tasks%2520it%2520converges%250Awithin%2520just%2520two%2520inference%2520steps%252C%2520a%252050x%2520reduction%2520compared%2520to%2520Diffusion%2520Policy%2527s%250A100.%2520Moreover%252C%2520EBT-Policy%2520exhibits%2520emergent%2520capabilities%2520not%2520seen%2520in%2520prior%250Amodels%252C%2520such%2520as%2520zero-shot%2520recovery%2520from%2520failed%2520action%2520sequences%2520using%2520only%250Abehavior%2520cloning%2520and%2520without%2520explicit%2520retry%2520training.%2520By%2520leveraging%2520its%2520scalar%250Aenergy%2520for%2520uncertainty-aware%2520inference%2520and%2520dynamic%2520compute%2520allocation%252C%250AEBT-Policy%2520offers%2520a%2520promising%2520path%2520toward%2520robust%252C%2520generalizable%2520robot%2520behavior%250Aunder%2520distribution%2520shifts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EBT-Policy%3A%20Energy%20Unlocks%20Emergent%20Physical%20Reasoning%20Capabilities&entry.906535625=Travis%20Davies%20and%20Yiqi%20Huang%20and%20Alexi%20Gladstone%20and%20Yunxin%20Liu%20and%20Xiang%20Chen%20and%20Heng%20Ji%20and%20Huxian%20Liu%20and%20Luhui%20Hu&entry.1292438233=%20%20Implicit%20policies%20parameterized%20by%20generative%20models%2C%20such%20as%20Diffusion%0APolicy%2C%20have%20become%20the%20standard%20for%20policy%20learning%20and%20Vision-Language-Action%0A%28VLA%29%20models%20in%20robotics.%20However%2C%20these%20approaches%20often%20suffer%20from%20high%0Acomputational%20cost%2C%20exposure%20bias%2C%20and%20unstable%20inference%20dynamics%2C%20which%20lead%0Ato%20divergence%20under%20distribution%20shifts.%20Energy-Based%20Models%20%28EBMs%29%20address%0Athese%20issues%20by%20learning%20energy%20landscapes%20end-to-end%20and%20modeling%20equilibrium%0Adynamics%2C%20offering%20improved%20robustness%20and%20reduced%20exposure%20bias.%20Yet%2C%20policies%0Aparameterized%20by%20EBMs%20have%20historically%20struggled%20to%20scale%20effectively.%20Recent%0Awork%20on%20Energy-Based%20Transformers%20%28EBTs%29%20demonstrates%20the%20scalability%20of%20EBMs%0Ato%20high-dimensional%20spaces%2C%20but%20their%20potential%20for%20solving%20core%20challenges%20in%0Aphysically%20embodied%20models%20remains%20underexplored.%20We%20introduce%20a%20new%0Aenergy-based%20architecture%2C%20EBT-Policy%2C%20that%20solves%20core%20issues%20in%20robotic%20and%0Areal-world%20settings.%20Across%20simulated%20and%20real-world%20tasks%2C%20EBT-Policy%0Aconsistently%20outperforms%20diffusion-based%20policies%2C%20while%20requiring%20less%0Atraining%20and%20inference%20computation.%20Remarkably%2C%20on%20some%20tasks%20it%20converges%0Awithin%20just%20two%20inference%20steps%2C%20a%2050x%20reduction%20compared%20to%20Diffusion%20Policy%27s%0A100.%20Moreover%2C%20EBT-Policy%20exhibits%20emergent%20capabilities%20not%20seen%20in%20prior%0Amodels%2C%20such%20as%20zero-shot%20recovery%20from%20failed%20action%20sequences%20using%20only%0Abehavior%20cloning%20and%20without%20explicit%20retry%20training.%20By%20leveraging%20its%20scalar%0Aenergy%20for%20uncertainty-aware%20inference%20and%20dynamic%20compute%20allocation%2C%0AEBT-Policy%20offers%20a%20promising%20path%20toward%20robust%2C%20generalizable%20robot%20behavior%0Aunder%20distribution%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27545v1&entry.124074799=Read"},
{"title": "RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for\n  Solving Complex Sequential Tasks", "author": "Eleftherios Triantafyllidis and Fernando Acero and Zhaocheng Liu and Zhibin Li", "abstract": "  Solving long sequential tasks poses a significant challenge in embodied\nartificial intelligence. Enabling a robotic system to perform diverse\nsequential tasks with a broad range of manipulation skills is an active area of\nresearch. In this work, we present a Hybrid Hierarchical Learning framework,\nthe Robotic Manipulation Network (ROMAN), to address the challenge of solving\nmultiple complex tasks over long time horizons in robotic manipulation. ROMAN\nachieves task versatility and robust failure recovery by integrating\nbehavioural cloning, imitation learning, and reinforcement learning. It\nconsists of a central manipulation network that coordinates an ensemble of\nvarious neural networks, each specialising in distinct re-combinable sub-tasks\nto generate their correct in-sequence actions for solving complex long-horizon\nmanipulation tasks. Experimental results show that by orchestrating and\nactivating these specialised manipulation experts, ROMAN generates correct\nsequential activations for accomplishing long sequences of sophisticated\nmanipulation tasks and achieving adaptive behaviours beyond demonstrations,\nwhile exhibiting robustness to various sensory noises. These results\ndemonstrate the significance and versatility of ROMAN's dynamic adaptability\nfeaturing autonomous failure recovery capabilities, and highlight its potential\nfor various autonomous manipulation tasks that demand adaptive motor skills.\n", "link": "http://arxiv.org/abs/2307.00125v3", "date": "2025-10-31", "relevancy": 1.6061, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RObotic%20MAnipulation%20Network%20%28ROMAN%29%20--%20Hybrid%20Hierarchical%20Learning%20for%0A%20%20Solving%20Complex%20Sequential%20Tasks&body=Title%3A%20RObotic%20MAnipulation%20Network%20%28ROMAN%29%20--%20Hybrid%20Hierarchical%20Learning%20for%0A%20%20Solving%20Complex%20Sequential%20Tasks%0AAuthor%3A%20Eleftherios%20Triantafyllidis%20and%20Fernando%20Acero%20and%20Zhaocheng%20Liu%20and%20Zhibin%20Li%0AAbstract%3A%20%20%20Solving%20long%20sequential%20tasks%20poses%20a%20significant%20challenge%20in%20embodied%0Aartificial%20intelligence.%20Enabling%20a%20robotic%20system%20to%20perform%20diverse%0Asequential%20tasks%20with%20a%20broad%20range%20of%20manipulation%20skills%20is%20an%20active%20area%20of%0Aresearch.%20In%20this%20work%2C%20we%20present%20a%20Hybrid%20Hierarchical%20Learning%20framework%2C%0Athe%20Robotic%20Manipulation%20Network%20%28ROMAN%29%2C%20to%20address%20the%20challenge%20of%20solving%0Amultiple%20complex%20tasks%20over%20long%20time%20horizons%20in%20robotic%20manipulation.%20ROMAN%0Aachieves%20task%20versatility%20and%20robust%20failure%20recovery%20by%20integrating%0Abehavioural%20cloning%2C%20imitation%20learning%2C%20and%20reinforcement%20learning.%20It%0Aconsists%20of%20a%20central%20manipulation%20network%20that%20coordinates%20an%20ensemble%20of%0Avarious%20neural%20networks%2C%20each%20specialising%20in%20distinct%20re-combinable%20sub-tasks%0Ato%20generate%20their%20correct%20in-sequence%20actions%20for%20solving%20complex%20long-horizon%0Amanipulation%20tasks.%20Experimental%20results%20show%20that%20by%20orchestrating%20and%0Aactivating%20these%20specialised%20manipulation%20experts%2C%20ROMAN%20generates%20correct%0Asequential%20activations%20for%20accomplishing%20long%20sequences%20of%20sophisticated%0Amanipulation%20tasks%20and%20achieving%20adaptive%20behaviours%20beyond%20demonstrations%2C%0Awhile%20exhibiting%20robustness%20to%20various%20sensory%20noises.%20These%20results%0Ademonstrate%20the%20significance%20and%20versatility%20of%20ROMAN%27s%20dynamic%20adaptability%0Afeaturing%20autonomous%20failure%20recovery%20capabilities%2C%20and%20highlight%20its%20potential%0Afor%20various%20autonomous%20manipulation%20tasks%20that%20demand%20adaptive%20motor%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00125v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRObotic%2520MAnipulation%2520Network%2520%2528ROMAN%2529%2520--%2520Hybrid%2520Hierarchical%2520Learning%2520for%250A%2520%2520Solving%2520Complex%2520Sequential%2520Tasks%26entry.906535625%3DEleftherios%2520Triantafyllidis%2520and%2520Fernando%2520Acero%2520and%2520Zhaocheng%2520Liu%2520and%2520Zhibin%2520Li%26entry.1292438233%3D%2520%2520Solving%2520long%2520sequential%2520tasks%2520poses%2520a%2520significant%2520challenge%2520in%2520embodied%250Aartificial%2520intelligence.%2520Enabling%2520a%2520robotic%2520system%2520to%2520perform%2520diverse%250Asequential%2520tasks%2520with%2520a%2520broad%2520range%2520of%2520manipulation%2520skills%2520is%2520an%2520active%2520area%2520of%250Aresearch.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520Hybrid%2520Hierarchical%2520Learning%2520framework%252C%250Athe%2520Robotic%2520Manipulation%2520Network%2520%2528ROMAN%2529%252C%2520to%2520address%2520the%2520challenge%2520of%2520solving%250Amultiple%2520complex%2520tasks%2520over%2520long%2520time%2520horizons%2520in%2520robotic%2520manipulation.%2520ROMAN%250Aachieves%2520task%2520versatility%2520and%2520robust%2520failure%2520recovery%2520by%2520integrating%250Abehavioural%2520cloning%252C%2520imitation%2520learning%252C%2520and%2520reinforcement%2520learning.%2520It%250Aconsists%2520of%2520a%2520central%2520manipulation%2520network%2520that%2520coordinates%2520an%2520ensemble%2520of%250Avarious%2520neural%2520networks%252C%2520each%2520specialising%2520in%2520distinct%2520re-combinable%2520sub-tasks%250Ato%2520generate%2520their%2520correct%2520in-sequence%2520actions%2520for%2520solving%2520complex%2520long-horizon%250Amanipulation%2520tasks.%2520Experimental%2520results%2520show%2520that%2520by%2520orchestrating%2520and%250Aactivating%2520these%2520specialised%2520manipulation%2520experts%252C%2520ROMAN%2520generates%2520correct%250Asequential%2520activations%2520for%2520accomplishing%2520long%2520sequences%2520of%2520sophisticated%250Amanipulation%2520tasks%2520and%2520achieving%2520adaptive%2520behaviours%2520beyond%2520demonstrations%252C%250Awhile%2520exhibiting%2520robustness%2520to%2520various%2520sensory%2520noises.%2520These%2520results%250Ademonstrate%2520the%2520significance%2520and%2520versatility%2520of%2520ROMAN%2527s%2520dynamic%2520adaptability%250Afeaturing%2520autonomous%2520failure%2520recovery%2520capabilities%252C%2520and%2520highlight%2520its%2520potential%250Afor%2520various%2520autonomous%2520manipulation%2520tasks%2520that%2520demand%2520adaptive%2520motor%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.00125v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RObotic%20MAnipulation%20Network%20%28ROMAN%29%20--%20Hybrid%20Hierarchical%20Learning%20for%0A%20%20Solving%20Complex%20Sequential%20Tasks&entry.906535625=Eleftherios%20Triantafyllidis%20and%20Fernando%20Acero%20and%20Zhaocheng%20Liu%20and%20Zhibin%20Li&entry.1292438233=%20%20Solving%20long%20sequential%20tasks%20poses%20a%20significant%20challenge%20in%20embodied%0Aartificial%20intelligence.%20Enabling%20a%20robotic%20system%20to%20perform%20diverse%0Asequential%20tasks%20with%20a%20broad%20range%20of%20manipulation%20skills%20is%20an%20active%20area%20of%0Aresearch.%20In%20this%20work%2C%20we%20present%20a%20Hybrid%20Hierarchical%20Learning%20framework%2C%0Athe%20Robotic%20Manipulation%20Network%20%28ROMAN%29%2C%20to%20address%20the%20challenge%20of%20solving%0Amultiple%20complex%20tasks%20over%20long%20time%20horizons%20in%20robotic%20manipulation.%20ROMAN%0Aachieves%20task%20versatility%20and%20robust%20failure%20recovery%20by%20integrating%0Abehavioural%20cloning%2C%20imitation%20learning%2C%20and%20reinforcement%20learning.%20It%0Aconsists%20of%20a%20central%20manipulation%20network%20that%20coordinates%20an%20ensemble%20of%0Avarious%20neural%20networks%2C%20each%20specialising%20in%20distinct%20re-combinable%20sub-tasks%0Ato%20generate%20their%20correct%20in-sequence%20actions%20for%20solving%20complex%20long-horizon%0Amanipulation%20tasks.%20Experimental%20results%20show%20that%20by%20orchestrating%20and%0Aactivating%20these%20specialised%20manipulation%20experts%2C%20ROMAN%20generates%20correct%0Asequential%20activations%20for%20accomplishing%20long%20sequences%20of%20sophisticated%0Amanipulation%20tasks%20and%20achieving%20adaptive%20behaviours%20beyond%20demonstrations%2C%0Awhile%20exhibiting%20robustness%20to%20various%20sensory%20noises.%20These%20results%0Ademonstrate%20the%20significance%20and%20versatility%20of%20ROMAN%27s%20dynamic%20adaptability%0Afeaturing%20autonomous%20failure%20recovery%20capabilities%2C%20and%20highlight%20its%20potential%0Afor%20various%20autonomous%20manipulation%20tasks%20that%20demand%20adaptive%20motor%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00125v3&entry.124074799=Read"},
{"title": "Minimax-Optimal Two-Sample Test with Sliced Wasserstein", "author": "Binh Thuan Tran and Nicolas Schreuder", "abstract": "  We study the problem of nonparametric two-sample testing using the sliced\nWasserstein (SW) distance. While prior theoretical and empirical work indicates\nthat the SW distance offers a promising balance between strong statistical\nguarantees and computational efficiency, its theoretical foundations for\nhypothesis testing remain limited. We address this gap by proposing a\npermutation-based SW test and analyzing its performance. The test inherits\nfinite-sample Type I error control from the permutation principle. Moreover, we\nestablish non-asymptotic power bounds and show that the procedure achieves the\nminimax separation rate $n^{-1/2}$ over multinomial and bounded-support\nalternatives, matching the optimal guarantees of kernel-based tests while\nbuilding on the geometric foundations of Wasserstein distances. Our analysis\nfurther quantifies the trade-off between the number of projections and\nstatistical power. Finally, numerical experiments demonstrate that the test\ncombines finite-sample validity with competitive power and scalability, and --\nunlike kernel-based tests, which require careful kernel tuning -- it performs\nconsistently well across all scenarios we consider.\n", "link": "http://arxiv.org/abs/2510.27498v1", "date": "2025-10-31", "relevancy": 1.6038, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4167}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4018}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimax-Optimal%20Two-Sample%20Test%20with%20Sliced%20Wasserstein&body=Title%3A%20Minimax-Optimal%20Two-Sample%20Test%20with%20Sliced%20Wasserstein%0AAuthor%3A%20Binh%20Thuan%20Tran%20and%20Nicolas%20Schreuder%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20nonparametric%20two-sample%20testing%20using%20the%20sliced%0AWasserstein%20%28SW%29%20distance.%20While%20prior%20theoretical%20and%20empirical%20work%20indicates%0Athat%20the%20SW%20distance%20offers%20a%20promising%20balance%20between%20strong%20statistical%0Aguarantees%20and%20computational%20efficiency%2C%20its%20theoretical%20foundations%20for%0Ahypothesis%20testing%20remain%20limited.%20We%20address%20this%20gap%20by%20proposing%20a%0Apermutation-based%20SW%20test%20and%20analyzing%20its%20performance.%20The%20test%20inherits%0Afinite-sample%20Type%20I%20error%20control%20from%20the%20permutation%20principle.%20Moreover%2C%20we%0Aestablish%20non-asymptotic%20power%20bounds%20and%20show%20that%20the%20procedure%20achieves%20the%0Aminimax%20separation%20rate%20%24n%5E%7B-1/2%7D%24%20over%20multinomial%20and%20bounded-support%0Aalternatives%2C%20matching%20the%20optimal%20guarantees%20of%20kernel-based%20tests%20while%0Abuilding%20on%20the%20geometric%20foundations%20of%20Wasserstein%20distances.%20Our%20analysis%0Afurther%20quantifies%20the%20trade-off%20between%20the%20number%20of%20projections%20and%0Astatistical%20power.%20Finally%2C%20numerical%20experiments%20demonstrate%20that%20the%20test%0Acombines%20finite-sample%20validity%20with%20competitive%20power%20and%20scalability%2C%20and%20--%0Aunlike%20kernel-based%20tests%2C%20which%20require%20careful%20kernel%20tuning%20--%20it%20performs%0Aconsistently%20well%20across%20all%20scenarios%20we%20consider.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimax-Optimal%2520Two-Sample%2520Test%2520with%2520Sliced%2520Wasserstein%26entry.906535625%3DBinh%2520Thuan%2520Tran%2520and%2520Nicolas%2520Schreuder%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520nonparametric%2520two-sample%2520testing%2520using%2520the%2520sliced%250AWasserstein%2520%2528SW%2529%2520distance.%2520While%2520prior%2520theoretical%2520and%2520empirical%2520work%2520indicates%250Athat%2520the%2520SW%2520distance%2520offers%2520a%2520promising%2520balance%2520between%2520strong%2520statistical%250Aguarantees%2520and%2520computational%2520efficiency%252C%2520its%2520theoretical%2520foundations%2520for%250Ahypothesis%2520testing%2520remain%2520limited.%2520We%2520address%2520this%2520gap%2520by%2520proposing%2520a%250Apermutation-based%2520SW%2520test%2520and%2520analyzing%2520its%2520performance.%2520The%2520test%2520inherits%250Afinite-sample%2520Type%2520I%2520error%2520control%2520from%2520the%2520permutation%2520principle.%2520Moreover%252C%2520we%250Aestablish%2520non-asymptotic%2520power%2520bounds%2520and%2520show%2520that%2520the%2520procedure%2520achieves%2520the%250Aminimax%2520separation%2520rate%2520%2524n%255E%257B-1/2%257D%2524%2520over%2520multinomial%2520and%2520bounded-support%250Aalternatives%252C%2520matching%2520the%2520optimal%2520guarantees%2520of%2520kernel-based%2520tests%2520while%250Abuilding%2520on%2520the%2520geometric%2520foundations%2520of%2520Wasserstein%2520distances.%2520Our%2520analysis%250Afurther%2520quantifies%2520the%2520trade-off%2520between%2520the%2520number%2520of%2520projections%2520and%250Astatistical%2520power.%2520Finally%252C%2520numerical%2520experiments%2520demonstrate%2520that%2520the%2520test%250Acombines%2520finite-sample%2520validity%2520with%2520competitive%2520power%2520and%2520scalability%252C%2520and%2520--%250Aunlike%2520kernel-based%2520tests%252C%2520which%2520require%2520careful%2520kernel%2520tuning%2520--%2520it%2520performs%250Aconsistently%2520well%2520across%2520all%2520scenarios%2520we%2520consider.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimax-Optimal%20Two-Sample%20Test%20with%20Sliced%20Wasserstein&entry.906535625=Binh%20Thuan%20Tran%20and%20Nicolas%20Schreuder&entry.1292438233=%20%20We%20study%20the%20problem%20of%20nonparametric%20two-sample%20testing%20using%20the%20sliced%0AWasserstein%20%28SW%29%20distance.%20While%20prior%20theoretical%20and%20empirical%20work%20indicates%0Athat%20the%20SW%20distance%20offers%20a%20promising%20balance%20between%20strong%20statistical%0Aguarantees%20and%20computational%20efficiency%2C%20its%20theoretical%20foundations%20for%0Ahypothesis%20testing%20remain%20limited.%20We%20address%20this%20gap%20by%20proposing%20a%0Apermutation-based%20SW%20test%20and%20analyzing%20its%20performance.%20The%20test%20inherits%0Afinite-sample%20Type%20I%20error%20control%20from%20the%20permutation%20principle.%20Moreover%2C%20we%0Aestablish%20non-asymptotic%20power%20bounds%20and%20show%20that%20the%20procedure%20achieves%20the%0Aminimax%20separation%20rate%20%24n%5E%7B-1/2%7D%24%20over%20multinomial%20and%20bounded-support%0Aalternatives%2C%20matching%20the%20optimal%20guarantees%20of%20kernel-based%20tests%20while%0Abuilding%20on%20the%20geometric%20foundations%20of%20Wasserstein%20distances.%20Our%20analysis%0Afurther%20quantifies%20the%20trade-off%20between%20the%20number%20of%20projections%20and%0Astatistical%20power.%20Finally%2C%20numerical%20experiments%20demonstrate%20that%20the%20test%0Acombines%20finite-sample%20validity%20with%20competitive%20power%20and%20scalability%2C%20and%20--%0Aunlike%20kernel-based%20tests%2C%20which%20require%20careful%20kernel%20tuning%20--%20it%20performs%0Aconsistently%20well%20across%20all%20scenarios%20we%20consider.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27498v1&entry.124074799=Read"},
{"title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees", "author": "Yuchen Liang and Yingbin Liang and Lifeng Lai and Ness Shroff", "abstract": "  Discrete diffusion models have recently gained significant prominence in\napplications involving natural language and graph data. A key factor\ninfluencing their effectiveness is the efficiency of discretized samplers.\nAmong these, $\\tau$-leaping samplers have become particularly popular due to\ntheir theoretical and empirical success. However, existing theoretical analyses\nof $\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify\nregularity assumptions, and their convergence bounds contain quadratic\ndependence on the vocabulary size. In this work, we introduce a new analytical\napproach for discrete diffusion models that removes the need for such\nassumptions. For the standard $\\tau$-leaping method, we establish convergence\nguarantees in KL divergence that scale linearly with vocabulary size, improving\nupon prior results with quadratic dependence. Our approach is also more broadly\napplicable: it provides the first convergence guarantees for other widely used\nsamplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our\napproach is a novel technique based on differential inequalities, offering a\nmore flexible alternative to the traditional Girsanov change-of-measure\nmethods. This technique may also be of independent interest for the analysis of\nother stochastic processes.\n", "link": "http://arxiv.org/abs/2509.16756v2", "date": "2025-10-31", "relevancy": 1.5839, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Diffusion%20Models%3A%20Novel%20Analysis%20and%20New%20Sampler%20Guarantees&body=Title%3A%20Discrete%20Diffusion%20Models%3A%20Novel%20Analysis%20and%20New%20Sampler%20Guarantees%0AAuthor%3A%20Yuchen%20Liang%20and%20Yingbin%20Liang%20and%20Lifeng%20Lai%20and%20Ness%20Shroff%0AAbstract%3A%20%20%20Discrete%20diffusion%20models%20have%20recently%20gained%20significant%20prominence%20in%0Aapplications%20involving%20natural%20language%20and%20graph%20data.%20A%20key%20factor%0Ainfluencing%20their%20effectiveness%20is%20the%20efficiency%20of%20discretized%20samplers.%0AAmong%20these%2C%20%24%5Ctau%24-leaping%20samplers%20have%20become%20particularly%20popular%20due%20to%0Atheir%20theoretical%20and%20empirical%20success.%20However%2C%20existing%20theoretical%20analyses%0Aof%20%24%5Ctau%24-leaping%20often%20rely%20on%20somewhat%20restrictive%20and%20difficult-to-verify%0Aregularity%20assumptions%2C%20and%20their%20convergence%20bounds%20contain%20quadratic%0Adependence%20on%20the%20vocabulary%20size.%20In%20this%20work%2C%20we%20introduce%20a%20new%20analytical%0Aapproach%20for%20discrete%20diffusion%20models%20that%20removes%20the%20need%20for%20such%0Aassumptions.%20For%20the%20standard%20%24%5Ctau%24-leaping%20method%2C%20we%20establish%20convergence%0Aguarantees%20in%20KL%20divergence%20that%20scale%20linearly%20with%20vocabulary%20size%2C%20improving%0Aupon%20prior%20results%20with%20quadratic%20dependence.%20Our%20approach%20is%20also%20more%20broadly%0Aapplicable%3A%20it%20provides%20the%20first%20convergence%20guarantees%20for%20other%20widely%20used%0Asamplers%2C%20including%20the%20Euler%20method%20and%20Tweedie%20%24%5Ctau%24-leaping.%20Central%20to%20our%0Aapproach%20is%20a%20novel%20technique%20based%20on%20differential%20inequalities%2C%20offering%20a%0Amore%20flexible%20alternative%20to%20the%20traditional%20Girsanov%20change-of-measure%0Amethods.%20This%20technique%20may%20also%20be%20of%20independent%20interest%20for%20the%20analysis%20of%0Aother%20stochastic%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Diffusion%2520Models%253A%2520Novel%2520Analysis%2520and%2520New%2520Sampler%2520Guarantees%26entry.906535625%3DYuchen%2520Liang%2520and%2520Yingbin%2520Liang%2520and%2520Lifeng%2520Lai%2520and%2520Ness%2520Shroff%26entry.1292438233%3D%2520%2520Discrete%2520diffusion%2520models%2520have%2520recently%2520gained%2520significant%2520prominence%2520in%250Aapplications%2520involving%2520natural%2520language%2520and%2520graph%2520data.%2520A%2520key%2520factor%250Ainfluencing%2520their%2520effectiveness%2520is%2520the%2520efficiency%2520of%2520discretized%2520samplers.%250AAmong%2520these%252C%2520%2524%255Ctau%2524-leaping%2520samplers%2520have%2520become%2520particularly%2520popular%2520due%2520to%250Atheir%2520theoretical%2520and%2520empirical%2520success.%2520However%252C%2520existing%2520theoretical%2520analyses%250Aof%2520%2524%255Ctau%2524-leaping%2520often%2520rely%2520on%2520somewhat%2520restrictive%2520and%2520difficult-to-verify%250Aregularity%2520assumptions%252C%2520and%2520their%2520convergence%2520bounds%2520contain%2520quadratic%250Adependence%2520on%2520the%2520vocabulary%2520size.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520analytical%250Aapproach%2520for%2520discrete%2520diffusion%2520models%2520that%2520removes%2520the%2520need%2520for%2520such%250Aassumptions.%2520For%2520the%2520standard%2520%2524%255Ctau%2524-leaping%2520method%252C%2520we%2520establish%2520convergence%250Aguarantees%2520in%2520KL%2520divergence%2520that%2520scale%2520linearly%2520with%2520vocabulary%2520size%252C%2520improving%250Aupon%2520prior%2520results%2520with%2520quadratic%2520dependence.%2520Our%2520approach%2520is%2520also%2520more%2520broadly%250Aapplicable%253A%2520it%2520provides%2520the%2520first%2520convergence%2520guarantees%2520for%2520other%2520widely%2520used%250Asamplers%252C%2520including%2520the%2520Euler%2520method%2520and%2520Tweedie%2520%2524%255Ctau%2524-leaping.%2520Central%2520to%2520our%250Aapproach%2520is%2520a%2520novel%2520technique%2520based%2520on%2520differential%2520inequalities%252C%2520offering%2520a%250Amore%2520flexible%2520alternative%2520to%2520the%2520traditional%2520Girsanov%2520change-of-measure%250Amethods.%2520This%2520technique%2520may%2520also%2520be%2520of%2520independent%2520interest%2520for%2520the%2520analysis%2520of%250Aother%2520stochastic%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Diffusion%20Models%3A%20Novel%20Analysis%20and%20New%20Sampler%20Guarantees&entry.906535625=Yuchen%20Liang%20and%20Yingbin%20Liang%20and%20Lifeng%20Lai%20and%20Ness%20Shroff&entry.1292438233=%20%20Discrete%20diffusion%20models%20have%20recently%20gained%20significant%20prominence%20in%0Aapplications%20involving%20natural%20language%20and%20graph%20data.%20A%20key%20factor%0Ainfluencing%20their%20effectiveness%20is%20the%20efficiency%20of%20discretized%20samplers.%0AAmong%20these%2C%20%24%5Ctau%24-leaping%20samplers%20have%20become%20particularly%20popular%20due%20to%0Atheir%20theoretical%20and%20empirical%20success.%20However%2C%20existing%20theoretical%20analyses%0Aof%20%24%5Ctau%24-leaping%20often%20rely%20on%20somewhat%20restrictive%20and%20difficult-to-verify%0Aregularity%20assumptions%2C%20and%20their%20convergence%20bounds%20contain%20quadratic%0Adependence%20on%20the%20vocabulary%20size.%20In%20this%20work%2C%20we%20introduce%20a%20new%20analytical%0Aapproach%20for%20discrete%20diffusion%20models%20that%20removes%20the%20need%20for%20such%0Aassumptions.%20For%20the%20standard%20%24%5Ctau%24-leaping%20method%2C%20we%20establish%20convergence%0Aguarantees%20in%20KL%20divergence%20that%20scale%20linearly%20with%20vocabulary%20size%2C%20improving%0Aupon%20prior%20results%20with%20quadratic%20dependence.%20Our%20approach%20is%20also%20more%20broadly%0Aapplicable%3A%20it%20provides%20the%20first%20convergence%20guarantees%20for%20other%20widely%20used%0Asamplers%2C%20including%20the%20Euler%20method%20and%20Tweedie%20%24%5Ctau%24-leaping.%20Central%20to%20our%0Aapproach%20is%20a%20novel%20technique%20based%20on%20differential%20inequalities%2C%20offering%20a%0Amore%20flexible%20alternative%20to%20the%20traditional%20Girsanov%20change-of-measure%0Amethods.%20This%20technique%20may%20also%20be%20of%20independent%20interest%20for%20the%20analysis%20of%0Aother%20stochastic%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16756v2&entry.124074799=Read"},
{"title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning", "author": "Qiusi Zhan and Hyeonjeong Ha and Rui Yang and Sirui Xu and Hanyang Chen and Liang-Yan Gui and Yu-Xiong Wang and Huan Zhang and Heng Ji and Daniel Kang", "abstract": "  Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.\n", "link": "http://arxiv.org/abs/2510.27623v1", "date": "2025-10-31", "relevancy": 1.5775, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Backdoor%20Attacks%20on%20MLLM%20Embodied%20Decision%20Making%20via%20Contrastive%0A%20%20Trigger%20Learning&body=Title%3A%20Visual%20Backdoor%20Attacks%20on%20MLLM%20Embodied%20Decision%20Making%20via%20Contrastive%0A%20%20Trigger%20Learning%0AAuthor%3A%20Qiusi%20Zhan%20and%20Hyeonjeong%20Ha%20and%20Rui%20Yang%20and%20Sirui%20Xu%20and%20Hanyang%20Chen%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%20and%20Huan%20Zhang%20and%20Heng%20Ji%20and%20Daniel%20Kang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20embodied%20agents%20by%0Aenabling%20direct%20perception%2C%20reasoning%2C%20and%20planning%20task-oriented%20actions%20from%0Avisual%20inputs.%20However%2C%20such%20vision%20driven%20embodied%20agents%20open%20a%20new%20attack%0Asurface%3A%20visual%20backdoor%20attacks%2C%20where%20the%20agent%20behaves%20normally%20until%20a%0Avisual%20trigger%20appears%20in%20the%20scene%2C%20then%20persistently%20executes%20an%0Aattacker-specified%20multi-step%20policy.%20We%20introduce%20BEAT%2C%20the%20first%20framework%20to%0Ainject%20such%20visual%20backdoors%20into%20MLLM-based%20embodied%20agents%20using%20objects%20in%0Athe%20environments%20as%20triggers.%20Unlike%20textual%20triggers%2C%20object%20triggers%20exhibit%0Awide%20variation%20across%20viewpoints%20and%20lighting%2C%20making%20them%20difficult%20to%20implant%0Areliably.%20BEAT%20addresses%20this%20challenge%20by%20%281%29%20constructing%20a%20training%20set%20that%0Aspans%20diverse%20scenes%2C%20tasks%2C%20and%20trigger%20placements%20to%20expose%20agents%20to%20trigger%0Avariability%2C%20and%20%282%29%20introducing%20a%20two-stage%20training%20scheme%20that%20first%20applies%0Asupervised%20fine-tuning%20%28SFT%29%20and%20then%20our%20novel%20Contrastive%20Trigger%20Learning%0A%28CTL%29.%20CTL%20formulates%20trigger%20discrimination%20as%20preference%20learning%20between%0Atrigger-present%20and%20trigger-free%20inputs%2C%20explicitly%20sharpening%20the%20decision%0Aboundaries%20to%20ensure%20precise%20backdoor%20activation.%20Across%20various%20embodied%20agent%0Abenchmarks%20and%20MLLMs%2C%20BEAT%20achieves%20attack%20success%20rates%20up%20to%2080%25%2C%20while%0Amaintaining%20strong%20benign%20task%20performance%2C%20and%20generalizes%20reliably%20to%0Aout-of-distribution%20trigger%20placements.%20Notably%2C%20compared%20to%20naive%20SFT%2C%20CTL%0Aboosts%20backdoor%20activation%20accuracy%20up%20to%2039%25%20under%20limited%20backdoor%20data.%0AThese%20findings%20expose%20a%20critical%20yet%20unexplored%20security%20risk%20in%20MLLM-based%0Aembodied%20agents%2C%20underscoring%20the%20need%20for%20robust%20defenses%20before%20real-world%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Backdoor%2520Attacks%2520on%2520MLLM%2520Embodied%2520Decision%2520Making%2520via%2520Contrastive%250A%2520%2520Trigger%2520Learning%26entry.906535625%3DQiusi%2520Zhan%2520and%2520Hyeonjeong%2520Ha%2520and%2520Rui%2520Yang%2520and%2520Sirui%2520Xu%2520and%2520Hanyang%2520Chen%2520and%2520Liang-Yan%2520Gui%2520and%2520Yu-Xiong%2520Wang%2520and%2520Huan%2520Zhang%2520and%2520Heng%2520Ji%2520and%2520Daniel%2520Kang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520advanced%2520embodied%2520agents%2520by%250Aenabling%2520direct%2520perception%252C%2520reasoning%252C%2520and%2520planning%2520task-oriented%2520actions%2520from%250Avisual%2520inputs.%2520However%252C%2520such%2520vision%2520driven%2520embodied%2520agents%2520open%2520a%2520new%2520attack%250Asurface%253A%2520visual%2520backdoor%2520attacks%252C%2520where%2520the%2520agent%2520behaves%2520normally%2520until%2520a%250Avisual%2520trigger%2520appears%2520in%2520the%2520scene%252C%2520then%2520persistently%2520executes%2520an%250Aattacker-specified%2520multi-step%2520policy.%2520We%2520introduce%2520BEAT%252C%2520the%2520first%2520framework%2520to%250Ainject%2520such%2520visual%2520backdoors%2520into%2520MLLM-based%2520embodied%2520agents%2520using%2520objects%2520in%250Athe%2520environments%2520as%2520triggers.%2520Unlike%2520textual%2520triggers%252C%2520object%2520triggers%2520exhibit%250Awide%2520variation%2520across%2520viewpoints%2520and%2520lighting%252C%2520making%2520them%2520difficult%2520to%2520implant%250Areliably.%2520BEAT%2520addresses%2520this%2520challenge%2520by%2520%25281%2529%2520constructing%2520a%2520training%2520set%2520that%250Aspans%2520diverse%2520scenes%252C%2520tasks%252C%2520and%2520trigger%2520placements%2520to%2520expose%2520agents%2520to%2520trigger%250Avariability%252C%2520and%2520%25282%2529%2520introducing%2520a%2520two-stage%2520training%2520scheme%2520that%2520first%2520applies%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520then%2520our%2520novel%2520Contrastive%2520Trigger%2520Learning%250A%2528CTL%2529.%2520CTL%2520formulates%2520trigger%2520discrimination%2520as%2520preference%2520learning%2520between%250Atrigger-present%2520and%2520trigger-free%2520inputs%252C%2520explicitly%2520sharpening%2520the%2520decision%250Aboundaries%2520to%2520ensure%2520precise%2520backdoor%2520activation.%2520Across%2520various%2520embodied%2520agent%250Abenchmarks%2520and%2520MLLMs%252C%2520BEAT%2520achieves%2520attack%2520success%2520rates%2520up%2520to%252080%2525%252C%2520while%250Amaintaining%2520strong%2520benign%2520task%2520performance%252C%2520and%2520generalizes%2520reliably%2520to%250Aout-of-distribution%2520trigger%2520placements.%2520Notably%252C%2520compared%2520to%2520naive%2520SFT%252C%2520CTL%250Aboosts%2520backdoor%2520activation%2520accuracy%2520up%2520to%252039%2525%2520under%2520limited%2520backdoor%2520data.%250AThese%2520findings%2520expose%2520a%2520critical%2520yet%2520unexplored%2520security%2520risk%2520in%2520MLLM-based%250Aembodied%2520agents%252C%2520underscoring%2520the%2520need%2520for%2520robust%2520defenses%2520before%2520real-world%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Backdoor%20Attacks%20on%20MLLM%20Embodied%20Decision%20Making%20via%20Contrastive%0A%20%20Trigger%20Learning&entry.906535625=Qiusi%20Zhan%20and%20Hyeonjeong%20Ha%20and%20Rui%20Yang%20and%20Sirui%20Xu%20and%20Hanyang%20Chen%20and%20Liang-Yan%20Gui%20and%20Yu-Xiong%20Wang%20and%20Huan%20Zhang%20and%20Heng%20Ji%20and%20Daniel%20Kang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20embodied%20agents%20by%0Aenabling%20direct%20perception%2C%20reasoning%2C%20and%20planning%20task-oriented%20actions%20from%0Avisual%20inputs.%20However%2C%20such%20vision%20driven%20embodied%20agents%20open%20a%20new%20attack%0Asurface%3A%20visual%20backdoor%20attacks%2C%20where%20the%20agent%20behaves%20normally%20until%20a%0Avisual%20trigger%20appears%20in%20the%20scene%2C%20then%20persistently%20executes%20an%0Aattacker-specified%20multi-step%20policy.%20We%20introduce%20BEAT%2C%20the%20first%20framework%20to%0Ainject%20such%20visual%20backdoors%20into%20MLLM-based%20embodied%20agents%20using%20objects%20in%0Athe%20environments%20as%20triggers.%20Unlike%20textual%20triggers%2C%20object%20triggers%20exhibit%0Awide%20variation%20across%20viewpoints%20and%20lighting%2C%20making%20them%20difficult%20to%20implant%0Areliably.%20BEAT%20addresses%20this%20challenge%20by%20%281%29%20constructing%20a%20training%20set%20that%0Aspans%20diverse%20scenes%2C%20tasks%2C%20and%20trigger%20placements%20to%20expose%20agents%20to%20trigger%0Avariability%2C%20and%20%282%29%20introducing%20a%20two-stage%20training%20scheme%20that%20first%20applies%0Asupervised%20fine-tuning%20%28SFT%29%20and%20then%20our%20novel%20Contrastive%20Trigger%20Learning%0A%28CTL%29.%20CTL%20formulates%20trigger%20discrimination%20as%20preference%20learning%20between%0Atrigger-present%20and%20trigger-free%20inputs%2C%20explicitly%20sharpening%20the%20decision%0Aboundaries%20to%20ensure%20precise%20backdoor%20activation.%20Across%20various%20embodied%20agent%0Abenchmarks%20and%20MLLMs%2C%20BEAT%20achieves%20attack%20success%20rates%20up%20to%2080%25%2C%20while%0Amaintaining%20strong%20benign%20task%20performance%2C%20and%20generalizes%20reliably%20to%0Aout-of-distribution%20trigger%20placements.%20Notably%2C%20compared%20to%20naive%20SFT%2C%20CTL%0Aboosts%20backdoor%20activation%20accuracy%20up%20to%2039%25%20under%20limited%20backdoor%20data.%0AThese%20findings%20expose%20a%20critical%20yet%20unexplored%20security%20risk%20in%20MLLM-based%0Aembodied%20agents%2C%20underscoring%20the%20need%20for%20robust%20defenses%20before%20real-world%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27623v1&entry.124074799=Read"},
{"title": "SRAGAN: Saliency Regularized and Attended Generative Adversarial Network\n  for Chinese Ink-wash Painting Style Transfer", "author": "Xiang Gao and Yuqi Zhang", "abstract": "  Recent style transfer problems are still largely dominated by Generative\nAdversarial Network (GAN) from the perspective of cross-domain image-to-image\n(I2I) translation, where the pivotal issue is to learn and transfer\ntarget-domain style patterns onto source-domain content images. This paper\nhandles the problem of translating real pictures into traditional Chinese\nink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though a\nwide range of I2I models tackle this problem, a notable challenge is that the\ncontent details of the source image could be easily erased or corrupted due to\nthe transfer of ink-wash style elements. To remedy this issue, we propose to\nincorporate saliency detection into the unpaired I2I framework to regularize\nimage content, where the detected saliency map is utilized from two aspects:\n(\\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize\nobject content structure by enforcing saliency consistency before and after\nimage stylization; (\\romannumeral2) we propose saliency adaptive normalization\n(SANorm) which implicitly enhances object structure integrity of the generated\npaintings by dynamically injecting image saliency information into the\ngenerator to guide stylization process. Besides, we also propose saliency\nattended discriminator which harnesses image saliency information to focus\ngenerative adversarial attention onto the drawn objects, contributing to\ngenerating more vivid and delicate brush strokes and ink-wash textures.\nExtensive qualitative and quantitative experiments demonstrate superiority of\nour approach over related advanced image stylization methods in both GAN and\ndiffusion model paradigms.\n", "link": "http://arxiv.org/abs/2404.15743v3", "date": "2025-10-31", "relevancy": 1.5638, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.53}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5245}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRAGAN%3A%20Saliency%20Regularized%20and%20Attended%20Generative%20Adversarial%20Network%0A%20%20for%20Chinese%20Ink-wash%20Painting%20Style%20Transfer&body=Title%3A%20SRAGAN%3A%20Saliency%20Regularized%20and%20Attended%20Generative%20Adversarial%20Network%0A%20%20for%20Chinese%20Ink-wash%20Painting%20Style%20Transfer%0AAuthor%3A%20Xiang%20Gao%20and%20Yuqi%20Zhang%0AAbstract%3A%20%20%20Recent%20style%20transfer%20problems%20are%20still%20largely%20dominated%20by%20Generative%0AAdversarial%20Network%20%28GAN%29%20from%20the%20perspective%20of%20cross-domain%20image-to-image%0A%28I2I%29%20translation%2C%20where%20the%20pivotal%20issue%20is%20to%20learn%20and%20transfer%0Atarget-domain%20style%20patterns%20onto%20source-domain%20content%20images.%20This%20paper%0Ahandles%20the%20problem%20of%20translating%20real%20pictures%20into%20traditional%20Chinese%0Aink-wash%20paintings%2C%20i.e.%2C%20Chinese%20ink-wash%20painting%20style%20transfer.%20Though%20a%0Awide%20range%20of%20I2I%20models%20tackle%20this%20problem%2C%20a%20notable%20challenge%20is%20that%20the%0Acontent%20details%20of%20the%20source%20image%20could%20be%20easily%20erased%20or%20corrupted%20due%20to%0Athe%20transfer%20of%20ink-wash%20style%20elements.%20To%20remedy%20this%20issue%2C%20we%20propose%20to%0Aincorporate%20saliency%20detection%20into%20the%20unpaired%20I2I%20framework%20to%20regularize%0Aimage%20content%2C%20where%20the%20detected%20saliency%20map%20is%20utilized%20from%20two%20aspects%3A%0A%28%5Cromannumeral1%29%20we%20propose%20saliency%20IOU%20%28SIOU%29%20loss%20to%20explicitly%20regularize%0Aobject%20content%20structure%20by%20enforcing%20saliency%20consistency%20before%20and%20after%0Aimage%20stylization%3B%20%28%5Cromannumeral2%29%20we%20propose%20saliency%20adaptive%20normalization%0A%28SANorm%29%20which%20implicitly%20enhances%20object%20structure%20integrity%20of%20the%20generated%0Apaintings%20by%20dynamically%20injecting%20image%20saliency%20information%20into%20the%0Agenerator%20to%20guide%20stylization%20process.%20Besides%2C%20we%20also%20propose%20saliency%0Aattended%20discriminator%20which%20harnesses%20image%20saliency%20information%20to%20focus%0Agenerative%20adversarial%20attention%20onto%20the%20drawn%20objects%2C%20contributing%20to%0Agenerating%20more%20vivid%20and%20delicate%20brush%20strokes%20and%20ink-wash%20textures.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20superiority%20of%0Aour%20approach%20over%20related%20advanced%20image%20stylization%20methods%20in%20both%20GAN%20and%0Adiffusion%20model%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRAGAN%253A%2520Saliency%2520Regularized%2520and%2520Attended%2520Generative%2520Adversarial%2520Network%250A%2520%2520for%2520Chinese%2520Ink-wash%2520Painting%2520Style%2520Transfer%26entry.906535625%3DXiang%2520Gao%2520and%2520Yuqi%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520style%2520transfer%2520problems%2520are%2520still%2520largely%2520dominated%2520by%2520Generative%250AAdversarial%2520Network%2520%2528GAN%2529%2520from%2520the%2520perspective%2520of%2520cross-domain%2520image-to-image%250A%2528I2I%2529%2520translation%252C%2520where%2520the%2520pivotal%2520issue%2520is%2520to%2520learn%2520and%2520transfer%250Atarget-domain%2520style%2520patterns%2520onto%2520source-domain%2520content%2520images.%2520This%2520paper%250Ahandles%2520the%2520problem%2520of%2520translating%2520real%2520pictures%2520into%2520traditional%2520Chinese%250Aink-wash%2520paintings%252C%2520i.e.%252C%2520Chinese%2520ink-wash%2520painting%2520style%2520transfer.%2520Though%2520a%250Awide%2520range%2520of%2520I2I%2520models%2520tackle%2520this%2520problem%252C%2520a%2520notable%2520challenge%2520is%2520that%2520the%250Acontent%2520details%2520of%2520the%2520source%2520image%2520could%2520be%2520easily%2520erased%2520or%2520corrupted%2520due%2520to%250Athe%2520transfer%2520of%2520ink-wash%2520style%2520elements.%2520To%2520remedy%2520this%2520issue%252C%2520we%2520propose%2520to%250Aincorporate%2520saliency%2520detection%2520into%2520the%2520unpaired%2520I2I%2520framework%2520to%2520regularize%250Aimage%2520content%252C%2520where%2520the%2520detected%2520saliency%2520map%2520is%2520utilized%2520from%2520two%2520aspects%253A%250A%2528%255Cromannumeral1%2529%2520we%2520propose%2520saliency%2520IOU%2520%2528SIOU%2529%2520loss%2520to%2520explicitly%2520regularize%250Aobject%2520content%2520structure%2520by%2520enforcing%2520saliency%2520consistency%2520before%2520and%2520after%250Aimage%2520stylization%253B%2520%2528%255Cromannumeral2%2529%2520we%2520propose%2520saliency%2520adaptive%2520normalization%250A%2528SANorm%2529%2520which%2520implicitly%2520enhances%2520object%2520structure%2520integrity%2520of%2520the%2520generated%250Apaintings%2520by%2520dynamically%2520injecting%2520image%2520saliency%2520information%2520into%2520the%250Agenerator%2520to%2520guide%2520stylization%2520process.%2520Besides%252C%2520we%2520also%2520propose%2520saliency%250Aattended%2520discriminator%2520which%2520harnesses%2520image%2520saliency%2520information%2520to%2520focus%250Agenerative%2520adversarial%2520attention%2520onto%2520the%2520drawn%2520objects%252C%2520contributing%2520to%250Agenerating%2520more%2520vivid%2520and%2520delicate%2520brush%2520strokes%2520and%2520ink-wash%2520textures.%250AExtensive%2520qualitative%2520and%2520quantitative%2520experiments%2520demonstrate%2520superiority%2520of%250Aour%2520approach%2520over%2520related%2520advanced%2520image%2520stylization%2520methods%2520in%2520both%2520GAN%2520and%250Adiffusion%2520model%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRAGAN%3A%20Saliency%20Regularized%20and%20Attended%20Generative%20Adversarial%20Network%0A%20%20for%20Chinese%20Ink-wash%20Painting%20Style%20Transfer&entry.906535625=Xiang%20Gao%20and%20Yuqi%20Zhang&entry.1292438233=%20%20Recent%20style%20transfer%20problems%20are%20still%20largely%20dominated%20by%20Generative%0AAdversarial%20Network%20%28GAN%29%20from%20the%20perspective%20of%20cross-domain%20image-to-image%0A%28I2I%29%20translation%2C%20where%20the%20pivotal%20issue%20is%20to%20learn%20and%20transfer%0Atarget-domain%20style%20patterns%20onto%20source-domain%20content%20images.%20This%20paper%0Ahandles%20the%20problem%20of%20translating%20real%20pictures%20into%20traditional%20Chinese%0Aink-wash%20paintings%2C%20i.e.%2C%20Chinese%20ink-wash%20painting%20style%20transfer.%20Though%20a%0Awide%20range%20of%20I2I%20models%20tackle%20this%20problem%2C%20a%20notable%20challenge%20is%20that%20the%0Acontent%20details%20of%20the%20source%20image%20could%20be%20easily%20erased%20or%20corrupted%20due%20to%0Athe%20transfer%20of%20ink-wash%20style%20elements.%20To%20remedy%20this%20issue%2C%20we%20propose%20to%0Aincorporate%20saliency%20detection%20into%20the%20unpaired%20I2I%20framework%20to%20regularize%0Aimage%20content%2C%20where%20the%20detected%20saliency%20map%20is%20utilized%20from%20two%20aspects%3A%0A%28%5Cromannumeral1%29%20we%20propose%20saliency%20IOU%20%28SIOU%29%20loss%20to%20explicitly%20regularize%0Aobject%20content%20structure%20by%20enforcing%20saliency%20consistency%20before%20and%20after%0Aimage%20stylization%3B%20%28%5Cromannumeral2%29%20we%20propose%20saliency%20adaptive%20normalization%0A%28SANorm%29%20which%20implicitly%20enhances%20object%20structure%20integrity%20of%20the%20generated%0Apaintings%20by%20dynamically%20injecting%20image%20saliency%20information%20into%20the%0Agenerator%20to%20guide%20stylization%20process.%20Besides%2C%20we%20also%20propose%20saliency%0Aattended%20discriminator%20which%20harnesses%20image%20saliency%20information%20to%20focus%0Agenerative%20adversarial%20attention%20onto%20the%20drawn%20objects%2C%20contributing%20to%0Agenerating%20more%20vivid%20and%20delicate%20brush%20strokes%20and%20ink-wash%20textures.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20demonstrate%20superiority%20of%0Aour%20approach%20over%20related%20advanced%20image%20stylization%20methods%20in%20both%20GAN%20and%0Adiffusion%20model%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15743v3&entry.124074799=Read"},
{"title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "author": "Xiangyan Liu and Jinjie Ni and Zijian Wu and Chao Du and Longxu Dou and Haonan Wang and Tianyu Pang and Michael Qizhe Shieh", "abstract": "  Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to better scale test-time compute remains largely\nunderexplored. In addition, VLMs continue to struggle with imperfect visual\nperception, which in turn affects the subsequent reasoning process. We\nintroduce NoisyRollout, a simple yet effective data augmentation method that\naddresses these issues by mixing training trajectories from both clean and\nmoderately distorted images. This approach injects perceptual diversity,\nencouraging better policy exploration and leading to more robust reasoning. A\nnoise annealing schedule gradually reduces distortion strength, aiding\nexploration early in training while ensuring later stability. Crucially, our\nmethod is easy-to-adopt--requiring no additional training cost and no\nmodifications to the RL objective. Extensive experiments on 2 distinct training\ndatasets demonstrate that NoisyRollout achieves state-of-the-art performance\namong open-source RL-tuned models across 5 out-of-domain reasoning and\nperception benchmarks. Furthermore, we validate the effectiveness of\nNoisyRollout across model sizes (7B and 32B), data scales (from 1K to 6K) and\nimage augmentation types (Gaussion noise and rotation), highlighting its\ngeneralizability and scalability.\n", "link": "http://arxiv.org/abs/2504.13055v4", "date": "2025-10-31", "relevancy": 1.5367, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.525}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5125}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoisyRollout%3A%20Reinforcing%20Visual%20Reasoning%20with%20Data%20Augmentation&body=Title%3A%20NoisyRollout%3A%20Reinforcing%20Visual%20Reasoning%20with%20Data%20Augmentation%0AAuthor%3A%20Xiangyan%20Liu%20and%20Jinjie%20Ni%20and%20Zijian%20Wu%20and%20Chao%20Du%20and%20Longxu%20Dou%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Michael%20Qizhe%20Shieh%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20strengthened%20the%0Areasoning%20capabilities%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20enhancing%0Apolicy%20exploration%20to%20better%20scale%20test-time%20compute%20remains%20largely%0Aunderexplored.%20In%20addition%2C%20VLMs%20continue%20to%20struggle%20with%20imperfect%20visual%0Aperception%2C%20which%20in%20turn%20affects%20the%20subsequent%20reasoning%20process.%20We%0Aintroduce%20NoisyRollout%2C%20a%20simple%20yet%20effective%20data%20augmentation%20method%20that%0Aaddresses%20these%20issues%20by%20mixing%20training%20trajectories%20from%20both%20clean%20and%0Amoderately%20distorted%20images.%20This%20approach%20injects%20perceptual%20diversity%2C%0Aencouraging%20better%20policy%20exploration%20and%20leading%20to%20more%20robust%20reasoning.%20A%0Anoise%20annealing%20schedule%20gradually%20reduces%20distortion%20strength%2C%20aiding%0Aexploration%20early%20in%20training%20while%20ensuring%20later%20stability.%20Crucially%2C%20our%0Amethod%20is%20easy-to-adopt--requiring%20no%20additional%20training%20cost%20and%20no%0Amodifications%20to%20the%20RL%20objective.%20Extensive%20experiments%20on%202%20distinct%20training%0Adatasets%20demonstrate%20that%20NoisyRollout%20achieves%20state-of-the-art%20performance%0Aamong%20open-source%20RL-tuned%20models%20across%205%20out-of-domain%20reasoning%20and%0Aperception%20benchmarks.%20Furthermore%2C%20we%20validate%20the%20effectiveness%20of%0ANoisyRollout%20across%20model%20sizes%20%287B%20and%2032B%29%2C%20data%20scales%20%28from%201K%20to%206K%29%20and%0Aimage%20augmentation%20types%20%28Gaussion%20noise%20and%20rotation%29%2C%20highlighting%20its%0Ageneralizability%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13055v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisyRollout%253A%2520Reinforcing%2520Visual%2520Reasoning%2520with%2520Data%2520Augmentation%26entry.906535625%3DXiangyan%2520Liu%2520and%2520Jinjie%2520Ni%2520and%2520Zijian%2520Wu%2520and%2520Chao%2520Du%2520and%2520Longxu%2520Dou%2520and%2520Haonan%2520Wang%2520and%2520Tianyu%2520Pang%2520and%2520Michael%2520Qizhe%2520Shieh%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520strengthened%2520the%250Areasoning%2520capabilities%2520of%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520enhancing%250Apolicy%2520exploration%2520to%2520better%2520scale%2520test-time%2520compute%2520remains%2520largely%250Aunderexplored.%2520In%2520addition%252C%2520VLMs%2520continue%2520to%2520struggle%2520with%2520imperfect%2520visual%250Aperception%252C%2520which%2520in%2520turn%2520affects%2520the%2520subsequent%2520reasoning%2520process.%2520We%250Aintroduce%2520NoisyRollout%252C%2520a%2520simple%2520yet%2520effective%2520data%2520augmentation%2520method%2520that%250Aaddresses%2520these%2520issues%2520by%2520mixing%2520training%2520trajectories%2520from%2520both%2520clean%2520and%250Amoderately%2520distorted%2520images.%2520This%2520approach%2520injects%2520perceptual%2520diversity%252C%250Aencouraging%2520better%2520policy%2520exploration%2520and%2520leading%2520to%2520more%2520robust%2520reasoning.%2520A%250Anoise%2520annealing%2520schedule%2520gradually%2520reduces%2520distortion%2520strength%252C%2520aiding%250Aexploration%2520early%2520in%2520training%2520while%2520ensuring%2520later%2520stability.%2520Crucially%252C%2520our%250Amethod%2520is%2520easy-to-adopt--requiring%2520no%2520additional%2520training%2520cost%2520and%2520no%250Amodifications%2520to%2520the%2520RL%2520objective.%2520Extensive%2520experiments%2520on%25202%2520distinct%2520training%250Adatasets%2520demonstrate%2520that%2520NoisyRollout%2520achieves%2520state-of-the-art%2520performance%250Aamong%2520open-source%2520RL-tuned%2520models%2520across%25205%2520out-of-domain%2520reasoning%2520and%250Aperception%2520benchmarks.%2520Furthermore%252C%2520we%2520validate%2520the%2520effectiveness%2520of%250ANoisyRollout%2520across%2520model%2520sizes%2520%25287B%2520and%252032B%2529%252C%2520data%2520scales%2520%2528from%25201K%2520to%25206K%2529%2520and%250Aimage%2520augmentation%2520types%2520%2528Gaussion%2520noise%2520and%2520rotation%2529%252C%2520highlighting%2520its%250Ageneralizability%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13055v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoisyRollout%3A%20Reinforcing%20Visual%20Reasoning%20with%20Data%20Augmentation&entry.906535625=Xiangyan%20Liu%20and%20Jinjie%20Ni%20and%20Zijian%20Wu%20and%20Chao%20Du%20and%20Longxu%20Dou%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Michael%20Qizhe%20Shieh&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20strengthened%20the%0Areasoning%20capabilities%20of%20vision-language%20models%20%28VLMs%29.%20However%2C%20enhancing%0Apolicy%20exploration%20to%20better%20scale%20test-time%20compute%20remains%20largely%0Aunderexplored.%20In%20addition%2C%20VLMs%20continue%20to%20struggle%20with%20imperfect%20visual%0Aperception%2C%20which%20in%20turn%20affects%20the%20subsequent%20reasoning%20process.%20We%0Aintroduce%20NoisyRollout%2C%20a%20simple%20yet%20effective%20data%20augmentation%20method%20that%0Aaddresses%20these%20issues%20by%20mixing%20training%20trajectories%20from%20both%20clean%20and%0Amoderately%20distorted%20images.%20This%20approach%20injects%20perceptual%20diversity%2C%0Aencouraging%20better%20policy%20exploration%20and%20leading%20to%20more%20robust%20reasoning.%20A%0Anoise%20annealing%20schedule%20gradually%20reduces%20distortion%20strength%2C%20aiding%0Aexploration%20early%20in%20training%20while%20ensuring%20later%20stability.%20Crucially%2C%20our%0Amethod%20is%20easy-to-adopt--requiring%20no%20additional%20training%20cost%20and%20no%0Amodifications%20to%20the%20RL%20objective.%20Extensive%20experiments%20on%202%20distinct%20training%0Adatasets%20demonstrate%20that%20NoisyRollout%20achieves%20state-of-the-art%20performance%0Aamong%20open-source%20RL-tuned%20models%20across%205%20out-of-domain%20reasoning%20and%0Aperception%20benchmarks.%20Furthermore%2C%20we%20validate%20the%20effectiveness%20of%0ANoisyRollout%20across%20model%20sizes%20%287B%20and%2032B%29%2C%20data%20scales%20%28from%201K%20to%206K%29%20and%0Aimage%20augmentation%20types%20%28Gaussion%20noise%20and%20rotation%29%2C%20highlighting%20its%0Ageneralizability%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13055v4&entry.124074799=Read"},
{"title": "Learning Sparse Approximate Inverse Preconditioners for Conjugate\n  Gradient Solvers on GPUs", "author": "Zherui Yang and Zhehao Li and Kangbo Lyu and Yixuan Li and Tao Du and Ligang Liu", "abstract": "  The conjugate gradient solver (CG) is a prevalent method for solving\nsymmetric and positive definite linear systems Ax=b, where effective\npreconditioners are crucial for fast convergence. Traditional preconditioners\nrely on prescribed algorithms to offer rigorous theoretical guarantees, while\nlimiting their ability to exploit optimization from data. Existing\nlearning-based methods often utilize Graph Neural Networks (GNNs) to improve\nthe performance and speed up the construction. However, their reliance on\nincomplete factorization leads to significant challenges: the associated\ntriangular solve hinders GPU parallelization in practice, and introduces\nlong-range dependencies which are difficult for GNNs to model. To address these\nissues, we propose a learning-based method to generate GPU-friendly\npreconditioners, particularly using GNNs to construct Sparse Approximate\nInverse (SPAI) preconditioners, which avoids triangular solves and requires\nonly two matrix-vector products at each CG step. The locality of matrix-vector\nproduct is compatible with the local propagation mechanism of GNNs. The\nflexibility of GNNs also allows our approach to be applied in a wide range of\nscenarios. Furthermore, we introduce a statistics-based scale-invariant loss\nfunction. Its design matches CG's property that the convergence rate depends on\nthe condition number, rather than the absolute scale of A, leading to improved\nperformance of the learned preconditioner. Evaluations on three PDE-derived\ndatasets and one synthetic dataset demonstrate that our method outperforms\nstandard preconditioners (Diagonal, IC, and traditional SPAI) and previous\nlearning-based preconditioners on GPUs. We reduce solution time on GPUs by\n40%-53% (68%-113% faster), along with better condition numbers and superior\ngeneralization performance. Source code available at\nhttps://github.com/Adversarr/LearningSparsePreconditioner4GPU\n", "link": "http://arxiv.org/abs/2510.27517v1", "date": "2025-10-31", "relevancy": 1.5356, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5195}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Sparse%20Approximate%20Inverse%20Preconditioners%20for%20Conjugate%0A%20%20Gradient%20Solvers%20on%20GPUs&body=Title%3A%20Learning%20Sparse%20Approximate%20Inverse%20Preconditioners%20for%20Conjugate%0A%20%20Gradient%20Solvers%20on%20GPUs%0AAuthor%3A%20Zherui%20Yang%20and%20Zhehao%20Li%20and%20Kangbo%20Lyu%20and%20Yixuan%20Li%20and%20Tao%20Du%20and%20Ligang%20Liu%0AAbstract%3A%20%20%20The%20conjugate%20gradient%20solver%20%28CG%29%20is%20a%20prevalent%20method%20for%20solving%0Asymmetric%20and%20positive%20definite%20linear%20systems%20Ax%3Db%2C%20where%20effective%0Apreconditioners%20are%20crucial%20for%20fast%20convergence.%20Traditional%20preconditioners%0Arely%20on%20prescribed%20algorithms%20to%20offer%20rigorous%20theoretical%20guarantees%2C%20while%0Alimiting%20their%20ability%20to%20exploit%20optimization%20from%20data.%20Existing%0Alearning-based%20methods%20often%20utilize%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20improve%0Athe%20performance%20and%20speed%20up%20the%20construction.%20However%2C%20their%20reliance%20on%0Aincomplete%20factorization%20leads%20to%20significant%20challenges%3A%20the%20associated%0Atriangular%20solve%20hinders%20GPU%20parallelization%20in%20practice%2C%20and%20introduces%0Along-range%20dependencies%20which%20are%20difficult%20for%20GNNs%20to%20model.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20learning-based%20method%20to%20generate%20GPU-friendly%0Apreconditioners%2C%20particularly%20using%20GNNs%20to%20construct%20Sparse%20Approximate%0AInverse%20%28SPAI%29%20preconditioners%2C%20which%20avoids%20triangular%20solves%20and%20requires%0Aonly%20two%20matrix-vector%20products%20at%20each%20CG%20step.%20The%20locality%20of%20matrix-vector%0Aproduct%20is%20compatible%20with%20the%20local%20propagation%20mechanism%20of%20GNNs.%20The%0Aflexibility%20of%20GNNs%20also%20allows%20our%20approach%20to%20be%20applied%20in%20a%20wide%20range%20of%0Ascenarios.%20Furthermore%2C%20we%20introduce%20a%20statistics-based%20scale-invariant%20loss%0Afunction.%20Its%20design%20matches%20CG%27s%20property%20that%20the%20convergence%20rate%20depends%20on%0Athe%20condition%20number%2C%20rather%20than%20the%20absolute%20scale%20of%20A%2C%20leading%20to%20improved%0Aperformance%20of%20the%20learned%20preconditioner.%20Evaluations%20on%20three%20PDE-derived%0Adatasets%20and%20one%20synthetic%20dataset%20demonstrate%20that%20our%20method%20outperforms%0Astandard%20preconditioners%20%28Diagonal%2C%20IC%2C%20and%20traditional%20SPAI%29%20and%20previous%0Alearning-based%20preconditioners%20on%20GPUs.%20We%20reduce%20solution%20time%20on%20GPUs%20by%0A40%25-53%25%20%2868%25-113%25%20faster%29%2C%20along%20with%20better%20condition%20numbers%20and%20superior%0Ageneralization%20performance.%20Source%20code%20available%20at%0Ahttps%3A//github.com/Adversarr/LearningSparsePreconditioner4GPU%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Sparse%2520Approximate%2520Inverse%2520Preconditioners%2520for%2520Conjugate%250A%2520%2520Gradient%2520Solvers%2520on%2520GPUs%26entry.906535625%3DZherui%2520Yang%2520and%2520Zhehao%2520Li%2520and%2520Kangbo%2520Lyu%2520and%2520Yixuan%2520Li%2520and%2520Tao%2520Du%2520and%2520Ligang%2520Liu%26entry.1292438233%3D%2520%2520The%2520conjugate%2520gradient%2520solver%2520%2528CG%2529%2520is%2520a%2520prevalent%2520method%2520for%2520solving%250Asymmetric%2520and%2520positive%2520definite%2520linear%2520systems%2520Ax%253Db%252C%2520where%2520effective%250Apreconditioners%2520are%2520crucial%2520for%2520fast%2520convergence.%2520Traditional%2520preconditioners%250Arely%2520on%2520prescribed%2520algorithms%2520to%2520offer%2520rigorous%2520theoretical%2520guarantees%252C%2520while%250Alimiting%2520their%2520ability%2520to%2520exploit%2520optimization%2520from%2520data.%2520Existing%250Alearning-based%2520methods%2520often%2520utilize%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520improve%250Athe%2520performance%2520and%2520speed%2520up%2520the%2520construction.%2520However%252C%2520their%2520reliance%2520on%250Aincomplete%2520factorization%2520leads%2520to%2520significant%2520challenges%253A%2520the%2520associated%250Atriangular%2520solve%2520hinders%2520GPU%2520parallelization%2520in%2520practice%252C%2520and%2520introduces%250Along-range%2520dependencies%2520which%2520are%2520difficult%2520for%2520GNNs%2520to%2520model.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520learning-based%2520method%2520to%2520generate%2520GPU-friendly%250Apreconditioners%252C%2520particularly%2520using%2520GNNs%2520to%2520construct%2520Sparse%2520Approximate%250AInverse%2520%2528SPAI%2529%2520preconditioners%252C%2520which%2520avoids%2520triangular%2520solves%2520and%2520requires%250Aonly%2520two%2520matrix-vector%2520products%2520at%2520each%2520CG%2520step.%2520The%2520locality%2520of%2520matrix-vector%250Aproduct%2520is%2520compatible%2520with%2520the%2520local%2520propagation%2520mechanism%2520of%2520GNNs.%2520The%250Aflexibility%2520of%2520GNNs%2520also%2520allows%2520our%2520approach%2520to%2520be%2520applied%2520in%2520a%2520wide%2520range%2520of%250Ascenarios.%2520Furthermore%252C%2520we%2520introduce%2520a%2520statistics-based%2520scale-invariant%2520loss%250Afunction.%2520Its%2520design%2520matches%2520CG%2527s%2520property%2520that%2520the%2520convergence%2520rate%2520depends%2520on%250Athe%2520condition%2520number%252C%2520rather%2520than%2520the%2520absolute%2520scale%2520of%2520A%252C%2520leading%2520to%2520improved%250Aperformance%2520of%2520the%2520learned%2520preconditioner.%2520Evaluations%2520on%2520three%2520PDE-derived%250Adatasets%2520and%2520one%2520synthetic%2520dataset%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Astandard%2520preconditioners%2520%2528Diagonal%252C%2520IC%252C%2520and%2520traditional%2520SPAI%2529%2520and%2520previous%250Alearning-based%2520preconditioners%2520on%2520GPUs.%2520We%2520reduce%2520solution%2520time%2520on%2520GPUs%2520by%250A40%2525-53%2525%2520%252868%2525-113%2525%2520faster%2529%252C%2520along%2520with%2520better%2520condition%2520numbers%2520and%2520superior%250Ageneralization%2520performance.%2520Source%2520code%2520available%2520at%250Ahttps%253A//github.com/Adversarr/LearningSparsePreconditioner4GPU%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sparse%20Approximate%20Inverse%20Preconditioners%20for%20Conjugate%0A%20%20Gradient%20Solvers%20on%20GPUs&entry.906535625=Zherui%20Yang%20and%20Zhehao%20Li%20and%20Kangbo%20Lyu%20and%20Yixuan%20Li%20and%20Tao%20Du%20and%20Ligang%20Liu&entry.1292438233=%20%20The%20conjugate%20gradient%20solver%20%28CG%29%20is%20a%20prevalent%20method%20for%20solving%0Asymmetric%20and%20positive%20definite%20linear%20systems%20Ax%3Db%2C%20where%20effective%0Apreconditioners%20are%20crucial%20for%20fast%20convergence.%20Traditional%20preconditioners%0Arely%20on%20prescribed%20algorithms%20to%20offer%20rigorous%20theoretical%20guarantees%2C%20while%0Alimiting%20their%20ability%20to%20exploit%20optimization%20from%20data.%20Existing%0Alearning-based%20methods%20often%20utilize%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20improve%0Athe%20performance%20and%20speed%20up%20the%20construction.%20However%2C%20their%20reliance%20on%0Aincomplete%20factorization%20leads%20to%20significant%20challenges%3A%20the%20associated%0Atriangular%20solve%20hinders%20GPU%20parallelization%20in%20practice%2C%20and%20introduces%0Along-range%20dependencies%20which%20are%20difficult%20for%20GNNs%20to%20model.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20learning-based%20method%20to%20generate%20GPU-friendly%0Apreconditioners%2C%20particularly%20using%20GNNs%20to%20construct%20Sparse%20Approximate%0AInverse%20%28SPAI%29%20preconditioners%2C%20which%20avoids%20triangular%20solves%20and%20requires%0Aonly%20two%20matrix-vector%20products%20at%20each%20CG%20step.%20The%20locality%20of%20matrix-vector%0Aproduct%20is%20compatible%20with%20the%20local%20propagation%20mechanism%20of%20GNNs.%20The%0Aflexibility%20of%20GNNs%20also%20allows%20our%20approach%20to%20be%20applied%20in%20a%20wide%20range%20of%0Ascenarios.%20Furthermore%2C%20we%20introduce%20a%20statistics-based%20scale-invariant%20loss%0Afunction.%20Its%20design%20matches%20CG%27s%20property%20that%20the%20convergence%20rate%20depends%20on%0Athe%20condition%20number%2C%20rather%20than%20the%20absolute%20scale%20of%20A%2C%20leading%20to%20improved%0Aperformance%20of%20the%20learned%20preconditioner.%20Evaluations%20on%20three%20PDE-derived%0Adatasets%20and%20one%20synthetic%20dataset%20demonstrate%20that%20our%20method%20outperforms%0Astandard%20preconditioners%20%28Diagonal%2C%20IC%2C%20and%20traditional%20SPAI%29%20and%20previous%0Alearning-based%20preconditioners%20on%20GPUs.%20We%20reduce%20solution%20time%20on%20GPUs%20by%0A40%25-53%25%20%2868%25-113%25%20faster%29%2C%20along%20with%20better%20condition%20numbers%20and%20superior%0Ageneralization%20performance.%20Source%20code%20available%20at%0Ahttps%3A//github.com/Adversarr/LearningSparsePreconditioner4GPU%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27517v1&entry.124074799=Read"},
{"title": "NegoCollab: A Common Representation Negotiation Approach for\n  Heterogeneous Collaborative Perception", "author": "Congzhang Shao and Quan Yuan and Guiyang Luo and Yue Hu and Danni Wang and Yilin Liu and Rui Pan and Bo Chen and Jinglin Li", "abstract": "  Collaborative perception improves task performance by expanding the\nperception range through information sharing among agents. . Immutable\nheterogeneity poses a significant challenge in collaborative perception, as\nparticipating agents may employ different and fixed perception models. This\nleads to domain gaps in the intermediate features shared among agents,\nconsequently degrading collaborative performance. Aligning the features of all\nagents to a common representation can eliminate domain gaps with low training\ncost. However, in existing methods, the common representation is designated as\nthe representation of a specific agent, making it difficult for agents with\nsignificant domain discrepancies from this specific agent to achieve proper\nalignment. This paper proposes NegoCollab, a heterogeneous collaboration method\nbased on the negotiated common representation. It introduces a negotiator\nduring training to derive the common representation from the local\nrepresentations of each modality's agent, effectively reducing the inherent\ndomain gap with the various local representations. In NegoCollab, the mutual\ntransformation of features between the local representation space and the\ncommon representation space is achieved by a pair of sender and receiver. To\nbetter align local representations to the common representation containing\nmultimodal information, we introduce structural alignment loss and pragmatic\nalignment loss in addition to the distribution alignment loss to supervise the\ntraining. This enables the knowledge in the common representation to be fully\ndistilled into the sender.\n", "link": "http://arxiv.org/abs/2510.27647v1", "date": "2025-10-31", "relevancy": 1.5245, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NegoCollab%3A%20A%20Common%20Representation%20Negotiation%20Approach%20for%0A%20%20Heterogeneous%20Collaborative%20Perception&body=Title%3A%20NegoCollab%3A%20A%20Common%20Representation%20Negotiation%20Approach%20for%0A%20%20Heterogeneous%20Collaborative%20Perception%0AAuthor%3A%20Congzhang%20Shao%20and%20Quan%20Yuan%20and%20Guiyang%20Luo%20and%20Yue%20Hu%20and%20Danni%20Wang%20and%20Yilin%20Liu%20and%20Rui%20Pan%20and%20Bo%20Chen%20and%20Jinglin%20Li%0AAbstract%3A%20%20%20Collaborative%20perception%20improves%20task%20performance%20by%20expanding%20the%0Aperception%20range%20through%20information%20sharing%20among%20agents.%20.%20Immutable%0Aheterogeneity%20poses%20a%20significant%20challenge%20in%20collaborative%20perception%2C%20as%0Aparticipating%20agents%20may%20employ%20different%20and%20fixed%20perception%20models.%20This%0Aleads%20to%20domain%20gaps%20in%20the%20intermediate%20features%20shared%20among%20agents%2C%0Aconsequently%20degrading%20collaborative%20performance.%20Aligning%20the%20features%20of%20all%0Aagents%20to%20a%20common%20representation%20can%20eliminate%20domain%20gaps%20with%20low%20training%0Acost.%20However%2C%20in%20existing%20methods%2C%20the%20common%20representation%20is%20designated%20as%0Athe%20representation%20of%20a%20specific%20agent%2C%20making%20it%20difficult%20for%20agents%20with%0Asignificant%20domain%20discrepancies%20from%20this%20specific%20agent%20to%20achieve%20proper%0Aalignment.%20This%20paper%20proposes%20NegoCollab%2C%20a%20heterogeneous%20collaboration%20method%0Abased%20on%20the%20negotiated%20common%20representation.%20It%20introduces%20a%20negotiator%0Aduring%20training%20to%20derive%20the%20common%20representation%20from%20the%20local%0Arepresentations%20of%20each%20modality%27s%20agent%2C%20effectively%20reducing%20the%20inherent%0Adomain%20gap%20with%20the%20various%20local%20representations.%20In%20NegoCollab%2C%20the%20mutual%0Atransformation%20of%20features%20between%20the%20local%20representation%20space%20and%20the%0Acommon%20representation%20space%20is%20achieved%20by%20a%20pair%20of%20sender%20and%20receiver.%20To%0Abetter%20align%20local%20representations%20to%20the%20common%20representation%20containing%0Amultimodal%20information%2C%20we%20introduce%20structural%20alignment%20loss%20and%20pragmatic%0Aalignment%20loss%20in%20addition%20to%20the%20distribution%20alignment%20loss%20to%20supervise%20the%0Atraining.%20This%20enables%20the%20knowledge%20in%20the%20common%20representation%20to%20be%20fully%0Adistilled%20into%20the%20sender.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNegoCollab%253A%2520A%2520Common%2520Representation%2520Negotiation%2520Approach%2520for%250A%2520%2520Heterogeneous%2520Collaborative%2520Perception%26entry.906535625%3DCongzhang%2520Shao%2520and%2520Quan%2520Yuan%2520and%2520Guiyang%2520Luo%2520and%2520Yue%2520Hu%2520and%2520Danni%2520Wang%2520and%2520Yilin%2520Liu%2520and%2520Rui%2520Pan%2520and%2520Bo%2520Chen%2520and%2520Jinglin%2520Li%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520improves%2520task%2520performance%2520by%2520expanding%2520the%250Aperception%2520range%2520through%2520information%2520sharing%2520among%2520agents.%2520.%2520Immutable%250Aheterogeneity%2520poses%2520a%2520significant%2520challenge%2520in%2520collaborative%2520perception%252C%2520as%250Aparticipating%2520agents%2520may%2520employ%2520different%2520and%2520fixed%2520perception%2520models.%2520This%250Aleads%2520to%2520domain%2520gaps%2520in%2520the%2520intermediate%2520features%2520shared%2520among%2520agents%252C%250Aconsequently%2520degrading%2520collaborative%2520performance.%2520Aligning%2520the%2520features%2520of%2520all%250Aagents%2520to%2520a%2520common%2520representation%2520can%2520eliminate%2520domain%2520gaps%2520with%2520low%2520training%250Acost.%2520However%252C%2520in%2520existing%2520methods%252C%2520the%2520common%2520representation%2520is%2520designated%2520as%250Athe%2520representation%2520of%2520a%2520specific%2520agent%252C%2520making%2520it%2520difficult%2520for%2520agents%2520with%250Asignificant%2520domain%2520discrepancies%2520from%2520this%2520specific%2520agent%2520to%2520achieve%2520proper%250Aalignment.%2520This%2520paper%2520proposes%2520NegoCollab%252C%2520a%2520heterogeneous%2520collaboration%2520method%250Abased%2520on%2520the%2520negotiated%2520common%2520representation.%2520It%2520introduces%2520a%2520negotiator%250Aduring%2520training%2520to%2520derive%2520the%2520common%2520representation%2520from%2520the%2520local%250Arepresentations%2520of%2520each%2520modality%2527s%2520agent%252C%2520effectively%2520reducing%2520the%2520inherent%250Adomain%2520gap%2520with%2520the%2520various%2520local%2520representations.%2520In%2520NegoCollab%252C%2520the%2520mutual%250Atransformation%2520of%2520features%2520between%2520the%2520local%2520representation%2520space%2520and%2520the%250Acommon%2520representation%2520space%2520is%2520achieved%2520by%2520a%2520pair%2520of%2520sender%2520and%2520receiver.%2520To%250Abetter%2520align%2520local%2520representations%2520to%2520the%2520common%2520representation%2520containing%250Amultimodal%2520information%252C%2520we%2520introduce%2520structural%2520alignment%2520loss%2520and%2520pragmatic%250Aalignment%2520loss%2520in%2520addition%2520to%2520the%2520distribution%2520alignment%2520loss%2520to%2520supervise%2520the%250Atraining.%2520This%2520enables%2520the%2520knowledge%2520in%2520the%2520common%2520representation%2520to%2520be%2520fully%250Adistilled%2520into%2520the%2520sender.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NegoCollab%3A%20A%20Common%20Representation%20Negotiation%20Approach%20for%0A%20%20Heterogeneous%20Collaborative%20Perception&entry.906535625=Congzhang%20Shao%20and%20Quan%20Yuan%20and%20Guiyang%20Luo%20and%20Yue%20Hu%20and%20Danni%20Wang%20and%20Yilin%20Liu%20and%20Rui%20Pan%20and%20Bo%20Chen%20and%20Jinglin%20Li&entry.1292438233=%20%20Collaborative%20perception%20improves%20task%20performance%20by%20expanding%20the%0Aperception%20range%20through%20information%20sharing%20among%20agents.%20.%20Immutable%0Aheterogeneity%20poses%20a%20significant%20challenge%20in%20collaborative%20perception%2C%20as%0Aparticipating%20agents%20may%20employ%20different%20and%20fixed%20perception%20models.%20This%0Aleads%20to%20domain%20gaps%20in%20the%20intermediate%20features%20shared%20among%20agents%2C%0Aconsequently%20degrading%20collaborative%20performance.%20Aligning%20the%20features%20of%20all%0Aagents%20to%20a%20common%20representation%20can%20eliminate%20domain%20gaps%20with%20low%20training%0Acost.%20However%2C%20in%20existing%20methods%2C%20the%20common%20representation%20is%20designated%20as%0Athe%20representation%20of%20a%20specific%20agent%2C%20making%20it%20difficult%20for%20agents%20with%0Asignificant%20domain%20discrepancies%20from%20this%20specific%20agent%20to%20achieve%20proper%0Aalignment.%20This%20paper%20proposes%20NegoCollab%2C%20a%20heterogeneous%20collaboration%20method%0Abased%20on%20the%20negotiated%20common%20representation.%20It%20introduces%20a%20negotiator%0Aduring%20training%20to%20derive%20the%20common%20representation%20from%20the%20local%0Arepresentations%20of%20each%20modality%27s%20agent%2C%20effectively%20reducing%20the%20inherent%0Adomain%20gap%20with%20the%20various%20local%20representations.%20In%20NegoCollab%2C%20the%20mutual%0Atransformation%20of%20features%20between%20the%20local%20representation%20space%20and%20the%0Acommon%20representation%20space%20is%20achieved%20by%20a%20pair%20of%20sender%20and%20receiver.%20To%0Abetter%20align%20local%20representations%20to%20the%20common%20representation%20containing%0Amultimodal%20information%2C%20we%20introduce%20structural%20alignment%20loss%20and%20pragmatic%0Aalignment%20loss%20in%20addition%20to%20the%20distribution%20alignment%20loss%20to%20supervise%20the%0Atraining.%20This%20enables%20the%20knowledge%20in%20the%20common%20representation%20to%20be%20fully%0Adistilled%20into%20the%20sender.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27647v1&entry.124074799=Read"},
{"title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation", "author": "Heng Ping and Arijit Bhattacharjee and Peiyu Zhang and Shixuan Li and Wei Yang and Anzhe Cheng and Xiaole Zhang and Jesse Thomason and Ali Jannesari and Nesreen Ahmed and Paul Bogdan", "abstract": "  Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.\n", "link": "http://arxiv.org/abs/2510.27617v1", "date": "2025-10-31", "relevancy": 1.4858, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5167}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeriMoA%3A%20A%20Mixture-of-Agents%20Framework%20for%20Spec-to-HDL%20Generation&body=Title%3A%20VeriMoA%3A%20A%20Mixture-of-Agents%20Framework%20for%20Spec-to-HDL%20Generation%0AAuthor%3A%20Heng%20Ping%20and%20Arijit%20Bhattacharjee%20and%20Peiyu%20Zhang%20and%20Shixuan%20Li%20and%20Wei%20Yang%20and%20Anzhe%20Cheng%20and%20Xiaole%20Zhang%20and%20Jesse%20Thomason%20and%20Ali%20Jannesari%20and%20Nesreen%20Ahmed%20and%20Paul%20Bogdan%0AAbstract%3A%20%20%20Automation%20of%20Register%20Transfer%20Level%20%28RTL%29%20design%20can%20help%20developers%20meet%0Aincreasing%20computational%20demands.%20Large%20Language%20Models%20%28LLMs%29%20show%20promise%20for%0AHardware%20Description%20Language%20%28HDL%29%20generation%2C%20but%20face%20challenges%20due%20to%0Alimited%20parametric%20knowledge%20and%20domain-specific%20constraints.%20While%20prompt%0Aengineering%20and%20fine-tuning%20have%20limitations%20in%20knowledge%20coverage%20and%20training%0Acosts%2C%20multi-agent%20architectures%20offer%20a%20training-free%20paradigm%20to%20enhance%0Areasoning%20through%20collaborative%20generation.%20However%2C%20current%20multi-agent%0Aapproaches%20suffer%20from%20two%20critical%20deficiencies%3A%20susceptibility%20to%20noise%0Apropagation%20and%20constrained%20reasoning%20space%20exploration.%20We%20propose%20VeriMoA%2C%20a%0Atraining-free%20mixture-of-agents%20%28MoA%29%20framework%20with%20two%20synergistic%0Ainnovations.%20First%2C%20a%20quality-guided%20caching%20mechanism%20to%20maintain%20all%0Aintermediate%20HDL%20outputs%20and%20enables%20quality-based%20ranking%20and%20selection%20across%0Athe%20entire%20generation%20process%2C%20encouraging%20knowledge%20accumulation%20over%20layers%0Aof%20reasoning.%20Second%2C%20a%20multi-path%20generation%20strategy%20that%20leverages%20C%2B%2B%20and%0APython%20as%20intermediate%20representations%2C%20decomposing%20specification-to-HDL%0Atranslation%20into%20two-stage%20processes%20that%20exploit%20LLM%20fluency%20in%20high-resource%0Alanguages%20while%20promoting%20solution%20diversity.%20Comprehensive%20experiments%20on%0AVerilogEval%202.0%20and%20RTLLM%202.0%20benchmarks%20demonstrate%20that%20VeriMoA%20achieves%0A15--30%25%20improvements%20in%20Pass%401%20across%20diverse%20LLM%20backbones%2C%20especially%0Aenabling%20smaller%20models%20to%20match%20larger%20models%20and%20fine-tuned%20alternatives%0Awithout%20requiring%20costly%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeriMoA%253A%2520A%2520Mixture-of-Agents%2520Framework%2520for%2520Spec-to-HDL%2520Generation%26entry.906535625%3DHeng%2520Ping%2520and%2520Arijit%2520Bhattacharjee%2520and%2520Peiyu%2520Zhang%2520and%2520Shixuan%2520Li%2520and%2520Wei%2520Yang%2520and%2520Anzhe%2520Cheng%2520and%2520Xiaole%2520Zhang%2520and%2520Jesse%2520Thomason%2520and%2520Ali%2520Jannesari%2520and%2520Nesreen%2520Ahmed%2520and%2520Paul%2520Bogdan%26entry.1292438233%3D%2520%2520Automation%2520of%2520Register%2520Transfer%2520Level%2520%2528RTL%2529%2520design%2520can%2520help%2520developers%2520meet%250Aincreasing%2520computational%2520demands.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520promise%2520for%250AHardware%2520Description%2520Language%2520%2528HDL%2529%2520generation%252C%2520but%2520face%2520challenges%2520due%2520to%250Alimited%2520parametric%2520knowledge%2520and%2520domain-specific%2520constraints.%2520While%2520prompt%250Aengineering%2520and%2520fine-tuning%2520have%2520limitations%2520in%2520knowledge%2520coverage%2520and%2520training%250Acosts%252C%2520multi-agent%2520architectures%2520offer%2520a%2520training-free%2520paradigm%2520to%2520enhance%250Areasoning%2520through%2520collaborative%2520generation.%2520However%252C%2520current%2520multi-agent%250Aapproaches%2520suffer%2520from%2520two%2520critical%2520deficiencies%253A%2520susceptibility%2520to%2520noise%250Apropagation%2520and%2520constrained%2520reasoning%2520space%2520exploration.%2520We%2520propose%2520VeriMoA%252C%2520a%250Atraining-free%2520mixture-of-agents%2520%2528MoA%2529%2520framework%2520with%2520two%2520synergistic%250Ainnovations.%2520First%252C%2520a%2520quality-guided%2520caching%2520mechanism%2520to%2520maintain%2520all%250Aintermediate%2520HDL%2520outputs%2520and%2520enables%2520quality-based%2520ranking%2520and%2520selection%2520across%250Athe%2520entire%2520generation%2520process%252C%2520encouraging%2520knowledge%2520accumulation%2520over%2520layers%250Aof%2520reasoning.%2520Second%252C%2520a%2520multi-path%2520generation%2520strategy%2520that%2520leverages%2520C%252B%252B%2520and%250APython%2520as%2520intermediate%2520representations%252C%2520decomposing%2520specification-to-HDL%250Atranslation%2520into%2520two-stage%2520processes%2520that%2520exploit%2520LLM%2520fluency%2520in%2520high-resource%250Alanguages%2520while%2520promoting%2520solution%2520diversity.%2520Comprehensive%2520experiments%2520on%250AVerilogEval%25202.0%2520and%2520RTLLM%25202.0%2520benchmarks%2520demonstrate%2520that%2520VeriMoA%2520achieves%250A15--30%2525%2520improvements%2520in%2520Pass%25401%2520across%2520diverse%2520LLM%2520backbones%252C%2520especially%250Aenabling%2520smaller%2520models%2520to%2520match%2520larger%2520models%2520and%2520fine-tuned%2520alternatives%250Awithout%2520requiring%2520costly%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeriMoA%3A%20A%20Mixture-of-Agents%20Framework%20for%20Spec-to-HDL%20Generation&entry.906535625=Heng%20Ping%20and%20Arijit%20Bhattacharjee%20and%20Peiyu%20Zhang%20and%20Shixuan%20Li%20and%20Wei%20Yang%20and%20Anzhe%20Cheng%20and%20Xiaole%20Zhang%20and%20Jesse%20Thomason%20and%20Ali%20Jannesari%20and%20Nesreen%20Ahmed%20and%20Paul%20Bogdan&entry.1292438233=%20%20Automation%20of%20Register%20Transfer%20Level%20%28RTL%29%20design%20can%20help%20developers%20meet%0Aincreasing%20computational%20demands.%20Large%20Language%20Models%20%28LLMs%29%20show%20promise%20for%0AHardware%20Description%20Language%20%28HDL%29%20generation%2C%20but%20face%20challenges%20due%20to%0Alimited%20parametric%20knowledge%20and%20domain-specific%20constraints.%20While%20prompt%0Aengineering%20and%20fine-tuning%20have%20limitations%20in%20knowledge%20coverage%20and%20training%0Acosts%2C%20multi-agent%20architectures%20offer%20a%20training-free%20paradigm%20to%20enhance%0Areasoning%20through%20collaborative%20generation.%20However%2C%20current%20multi-agent%0Aapproaches%20suffer%20from%20two%20critical%20deficiencies%3A%20susceptibility%20to%20noise%0Apropagation%20and%20constrained%20reasoning%20space%20exploration.%20We%20propose%20VeriMoA%2C%20a%0Atraining-free%20mixture-of-agents%20%28MoA%29%20framework%20with%20two%20synergistic%0Ainnovations.%20First%2C%20a%20quality-guided%20caching%20mechanism%20to%20maintain%20all%0Aintermediate%20HDL%20outputs%20and%20enables%20quality-based%20ranking%20and%20selection%20across%0Athe%20entire%20generation%20process%2C%20encouraging%20knowledge%20accumulation%20over%20layers%0Aof%20reasoning.%20Second%2C%20a%20multi-path%20generation%20strategy%20that%20leverages%20C%2B%2B%20and%0APython%20as%20intermediate%20representations%2C%20decomposing%20specification-to-HDL%0Atranslation%20into%20two-stage%20processes%20that%20exploit%20LLM%20fluency%20in%20high-resource%0Alanguages%20while%20promoting%20solution%20diversity.%20Comprehensive%20experiments%20on%0AVerilogEval%202.0%20and%20RTLLM%202.0%20benchmarks%20demonstrate%20that%20VeriMoA%20achieves%0A15--30%25%20improvements%20in%20Pass%401%20across%20diverse%20LLM%20backbones%2C%20especially%0Aenabling%20smaller%20models%20to%20match%20larger%20models%20and%20fine-tuned%20alternatives%0Awithout%20requiring%20costly%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27617v1&entry.124074799=Read"},
{"title": "SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic\n  Mathematical Reasoning", "author": "Ali Asgarov and Umid Suleymanov and Aadyant Khatri", "abstract": "  Solving mathematical reasoning problems requires not only accurate access to\nrelevant knowledge but also careful, multi-step thinking. However, current\nretrieval-augmented models often rely on a single perspective, follow\ninflexible search strategies, and struggle to effectively combine information\nfrom multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge\nIntegration for AGentic Mathematical reAsoning), a unified framework that\norchestrates specialized agents to independently reason, perform targeted\nsearches, and synthesize findings through a moderator mechanism. Each agent\ngenerates hypothetical passages to optimize retrieval for its analytic\nperspective, ensuring knowledge integration is both context-sensitive and\ncomputation-efficient. When evaluated on challenging benchmarks such as\nMATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms\nboth open- and closed-source systems, achieving an absolute performance\nimprovement of 7.4%. Our results demonstrate that multi-agent, on-demand\nknowledge integration significantly enhances both reasoning accuracy and\nefficiency, offering a scalable approach for complex, knowledge-intensive\nproblem-solving. We will release the code upon publication.\n", "link": "http://arxiv.org/abs/2510.27568v1", "date": "2025-10-31", "relevancy": 1.4846, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5181}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGMA%3A%20Search-Augmented%20On-Demand%20Knowledge%20Integration%20for%20Agentic%0A%20%20Mathematical%20Reasoning&body=Title%3A%20SIGMA%3A%20Search-Augmented%20On-Demand%20Knowledge%20Integration%20for%20Agentic%0A%20%20Mathematical%20Reasoning%0AAuthor%3A%20Ali%20Asgarov%20and%20Umid%20Suleymanov%20and%20Aadyant%20Khatri%0AAbstract%3A%20%20%20Solving%20mathematical%20reasoning%20problems%20requires%20not%20only%20accurate%20access%20to%0Arelevant%20knowledge%20but%20also%20careful%2C%20multi-step%20thinking.%20However%2C%20current%0Aretrieval-augmented%20models%20often%20rely%20on%20a%20single%20perspective%2C%20follow%0Ainflexible%20search%20strategies%2C%20and%20struggle%20to%20effectively%20combine%20information%0Afrom%20multiple%20sources.%20We%20introduce%20SIGMA%20%28Search-Augmented%20On-Demand%20Knowledge%0AIntegration%20for%20AGentic%20Mathematical%20reAsoning%29%2C%20a%20unified%20framework%20that%0Aorchestrates%20specialized%20agents%20to%20independently%20reason%2C%20perform%20targeted%0Asearches%2C%20and%20synthesize%20findings%20through%20a%20moderator%20mechanism.%20Each%20agent%0Agenerates%20hypothetical%20passages%20to%20optimize%20retrieval%20for%20its%20analytic%0Aperspective%2C%20ensuring%20knowledge%20integration%20is%20both%20context-sensitive%20and%0Acomputation-efficient.%20When%20evaluated%20on%20challenging%20benchmarks%20such%20as%0AMATH500%2C%20AIME%2C%20and%20PhD-level%20science%20QA%20GPQA%2C%20SIGMA%20consistently%20outperforms%0Aboth%20open-%20and%20closed-source%20systems%2C%20achieving%20an%20absolute%20performance%0Aimprovement%20of%207.4%25.%20Our%20results%20demonstrate%20that%20multi-agent%2C%20on-demand%0Aknowledge%20integration%20significantly%20enhances%20both%20reasoning%20accuracy%20and%0Aefficiency%2C%20offering%20a%20scalable%20approach%20for%20complex%2C%20knowledge-intensive%0Aproblem-solving.%20We%20will%20release%20the%20code%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGMA%253A%2520Search-Augmented%2520On-Demand%2520Knowledge%2520Integration%2520for%2520Agentic%250A%2520%2520Mathematical%2520Reasoning%26entry.906535625%3DAli%2520Asgarov%2520and%2520Umid%2520Suleymanov%2520and%2520Aadyant%2520Khatri%26entry.1292438233%3D%2520%2520Solving%2520mathematical%2520reasoning%2520problems%2520requires%2520not%2520only%2520accurate%2520access%2520to%250Arelevant%2520knowledge%2520but%2520also%2520careful%252C%2520multi-step%2520thinking.%2520However%252C%2520current%250Aretrieval-augmented%2520models%2520often%2520rely%2520on%2520a%2520single%2520perspective%252C%2520follow%250Ainflexible%2520search%2520strategies%252C%2520and%2520struggle%2520to%2520effectively%2520combine%2520information%250Afrom%2520multiple%2520sources.%2520We%2520introduce%2520SIGMA%2520%2528Search-Augmented%2520On-Demand%2520Knowledge%250AIntegration%2520for%2520AGentic%2520Mathematical%2520reAsoning%2529%252C%2520a%2520unified%2520framework%2520that%250Aorchestrates%2520specialized%2520agents%2520to%2520independently%2520reason%252C%2520perform%2520targeted%250Asearches%252C%2520and%2520synthesize%2520findings%2520through%2520a%2520moderator%2520mechanism.%2520Each%2520agent%250Agenerates%2520hypothetical%2520passages%2520to%2520optimize%2520retrieval%2520for%2520its%2520analytic%250Aperspective%252C%2520ensuring%2520knowledge%2520integration%2520is%2520both%2520context-sensitive%2520and%250Acomputation-efficient.%2520When%2520evaluated%2520on%2520challenging%2520benchmarks%2520such%2520as%250AMATH500%252C%2520AIME%252C%2520and%2520PhD-level%2520science%2520QA%2520GPQA%252C%2520SIGMA%2520consistently%2520outperforms%250Aboth%2520open-%2520and%2520closed-source%2520systems%252C%2520achieving%2520an%2520absolute%2520performance%250Aimprovement%2520of%25207.4%2525.%2520Our%2520results%2520demonstrate%2520that%2520multi-agent%252C%2520on-demand%250Aknowledge%2520integration%2520significantly%2520enhances%2520both%2520reasoning%2520accuracy%2520and%250Aefficiency%252C%2520offering%2520a%2520scalable%2520approach%2520for%2520complex%252C%2520knowledge-intensive%250Aproblem-solving.%2520We%2520will%2520release%2520the%2520code%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMA%3A%20Search-Augmented%20On-Demand%20Knowledge%20Integration%20for%20Agentic%0A%20%20Mathematical%20Reasoning&entry.906535625=Ali%20Asgarov%20and%20Umid%20Suleymanov%20and%20Aadyant%20Khatri&entry.1292438233=%20%20Solving%20mathematical%20reasoning%20problems%20requires%20not%20only%20accurate%20access%20to%0Arelevant%20knowledge%20but%20also%20careful%2C%20multi-step%20thinking.%20However%2C%20current%0Aretrieval-augmented%20models%20often%20rely%20on%20a%20single%20perspective%2C%20follow%0Ainflexible%20search%20strategies%2C%20and%20struggle%20to%20effectively%20combine%20information%0Afrom%20multiple%20sources.%20We%20introduce%20SIGMA%20%28Search-Augmented%20On-Demand%20Knowledge%0AIntegration%20for%20AGentic%20Mathematical%20reAsoning%29%2C%20a%20unified%20framework%20that%0Aorchestrates%20specialized%20agents%20to%20independently%20reason%2C%20perform%20targeted%0Asearches%2C%20and%20synthesize%20findings%20through%20a%20moderator%20mechanism.%20Each%20agent%0Agenerates%20hypothetical%20passages%20to%20optimize%20retrieval%20for%20its%20analytic%0Aperspective%2C%20ensuring%20knowledge%20integration%20is%20both%20context-sensitive%20and%0Acomputation-efficient.%20When%20evaluated%20on%20challenging%20benchmarks%20such%20as%0AMATH500%2C%20AIME%2C%20and%20PhD-level%20science%20QA%20GPQA%2C%20SIGMA%20consistently%20outperforms%0Aboth%20open-%20and%20closed-source%20systems%2C%20achieving%20an%20absolute%20performance%0Aimprovement%20of%207.4%25.%20Our%20results%20demonstrate%20that%20multi-agent%2C%20on-demand%0Aknowledge%20integration%20significantly%20enhances%20both%20reasoning%20accuracy%20and%0Aefficiency%2C%20offering%20a%20scalable%20approach%20for%20complex%2C%20knowledge-intensive%0Aproblem-solving.%20We%20will%20release%20the%20code%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27568v1&entry.124074799=Read"},
{"title": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving", "author": "Avinash Kumar and Shashank Nag and Jason Clemons and Lizy John and Poulami Das", "abstract": "  Early-Exit Large Language Models (EE-LLMs) enable high throughput inference\nby allowing tokens to exit early at intermediate layers. However, their\nthroughput is limited by the computational and memory savings. Existing EE-LLM\nframeworks rely on a single model and therefore, their token generation\nlatencies are bottlenecked by tokens that do not exit early and traverse\nadditional layers. Moreover, early exits are only known at runtime and depend\non the request. Therefore, these frameworks load the weights of all model\nlayers even though large portions remain unused when tokens exit early. The\nlack of memory savings limit us from scaling the batch sizes.\n  We propose $\\textit{HELIOS}$, a framework that improves both token generation\nlatency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits\ntwo insights. $\\textit{First}$, early exits are often complimentary across\nmodels, tokens that do not exit early on one model often take an early-exit on\nanother. HELIOS employs multiple models and dynamically switches between them\nto collectively maximize the number of tokens that exit early, and minimize\ntoken generation latencies. $\\textit{Second}$, even when a predicted token does\nnot exit early due to poor confidence, it often remains unchanged even after\nadditional layer traversal. HELIOS greedily allows such tokens to exit early\nand only loads the weights of the most likely to be used layers, yielding\nmemory savings which is then re-purposed to increase batch sizes. HELIOS\nemploys real-time profiling to accurately identify the early-exit\ndistributions, and adaptively switches between models by tracking tokens in\nreal-time to minimize the performance degradation caused by greedy model\nloading and exiting. Our evaluations show that HELIOS achieves $1.48\\times$\nhigher throughput and $15.14\\times$ larger batch size compared to existing\nEE-LLM frameworks.\n", "link": "http://arxiv.org/abs/2504.10724v2", "date": "2025-10-31", "relevancy": 1.4829, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5003}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4883}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HELIOS%3A%20Adaptive%20Model%20And%20Early-Exit%20Selection%20for%20Efficient%20LLM%0A%20%20Inference%20Serving&body=Title%3A%20HELIOS%3A%20Adaptive%20Model%20And%20Early-Exit%20Selection%20for%20Efficient%20LLM%0A%20%20Inference%20Serving%0AAuthor%3A%20Avinash%20Kumar%20and%20Shashank%20Nag%20and%20Jason%20Clemons%20and%20Lizy%20John%20and%20Poulami%20Das%0AAbstract%3A%20%20%20Early-Exit%20Large%20Language%20Models%20%28EE-LLMs%29%20enable%20high%20throughput%20inference%0Aby%20allowing%20tokens%20to%20exit%20early%20at%20intermediate%20layers.%20However%2C%20their%0Athroughput%20is%20limited%20by%20the%20computational%20and%20memory%20savings.%20Existing%20EE-LLM%0Aframeworks%20rely%20on%20a%20single%20model%20and%20therefore%2C%20their%20token%20generation%0Alatencies%20are%20bottlenecked%20by%20tokens%20that%20do%20not%20exit%20early%20and%20traverse%0Aadditional%20layers.%20Moreover%2C%20early%20exits%20are%20only%20known%20at%20runtime%20and%20depend%0Aon%20the%20request.%20Therefore%2C%20these%20frameworks%20load%20the%20weights%20of%20all%20model%0Alayers%20even%20though%20large%20portions%20remain%20unused%20when%20tokens%20exit%20early.%20The%0Alack%20of%20memory%20savings%20limit%20us%20from%20scaling%20the%20batch%20sizes.%0A%20%20We%20propose%20%24%5Ctextit%7BHELIOS%7D%24%2C%20a%20framework%20that%20improves%20both%20token%20generation%0Alatency%20and%20batch%20sizes%20to%20enable%20high-throughput%20in%20EE-LLMs.%20HELIOS%20exploits%0Atwo%20insights.%20%24%5Ctextit%7BFirst%7D%24%2C%20early%20exits%20are%20often%20complimentary%20across%0Amodels%2C%20tokens%20that%20do%20not%20exit%20early%20on%20one%20model%20often%20take%20an%20early-exit%20on%0Aanother.%20HELIOS%20employs%20multiple%20models%20and%20dynamically%20switches%20between%20them%0Ato%20collectively%20maximize%20the%20number%20of%20tokens%20that%20exit%20early%2C%20and%20minimize%0Atoken%20generation%20latencies.%20%24%5Ctextit%7BSecond%7D%24%2C%20even%20when%20a%20predicted%20token%20does%0Anot%20exit%20early%20due%20to%20poor%20confidence%2C%20it%20often%20remains%20unchanged%20even%20after%0Aadditional%20layer%20traversal.%20HELIOS%20greedily%20allows%20such%20tokens%20to%20exit%20early%0Aand%20only%20loads%20the%20weights%20of%20the%20most%20likely%20to%20be%20used%20layers%2C%20yielding%0Amemory%20savings%20which%20is%20then%20re-purposed%20to%20increase%20batch%20sizes.%20HELIOS%0Aemploys%20real-time%20profiling%20to%20accurately%20identify%20the%20early-exit%0Adistributions%2C%20and%20adaptively%20switches%20between%20models%20by%20tracking%20tokens%20in%0Areal-time%20to%20minimize%20the%20performance%20degradation%20caused%20by%20greedy%20model%0Aloading%20and%20exiting.%20Our%20evaluations%20show%20that%20HELIOS%20achieves%20%241.48%5Ctimes%24%0Ahigher%20throughput%20and%20%2415.14%5Ctimes%24%20larger%20batch%20size%20compared%20to%20existing%0AEE-LLM%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHELIOS%253A%2520Adaptive%2520Model%2520And%2520Early-Exit%2520Selection%2520for%2520Efficient%2520LLM%250A%2520%2520Inference%2520Serving%26entry.906535625%3DAvinash%2520Kumar%2520and%2520Shashank%2520Nag%2520and%2520Jason%2520Clemons%2520and%2520Lizy%2520John%2520and%2520Poulami%2520Das%26entry.1292438233%3D%2520%2520Early-Exit%2520Large%2520Language%2520Models%2520%2528EE-LLMs%2529%2520enable%2520high%2520throughput%2520inference%250Aby%2520allowing%2520tokens%2520to%2520exit%2520early%2520at%2520intermediate%2520layers.%2520However%252C%2520their%250Athroughput%2520is%2520limited%2520by%2520the%2520computational%2520and%2520memory%2520savings.%2520Existing%2520EE-LLM%250Aframeworks%2520rely%2520on%2520a%2520single%2520model%2520and%2520therefore%252C%2520their%2520token%2520generation%250Alatencies%2520are%2520bottlenecked%2520by%2520tokens%2520that%2520do%2520not%2520exit%2520early%2520and%2520traverse%250Aadditional%2520layers.%2520Moreover%252C%2520early%2520exits%2520are%2520only%2520known%2520at%2520runtime%2520and%2520depend%250Aon%2520the%2520request.%2520Therefore%252C%2520these%2520frameworks%2520load%2520the%2520weights%2520of%2520all%2520model%250Alayers%2520even%2520though%2520large%2520portions%2520remain%2520unused%2520when%2520tokens%2520exit%2520early.%2520The%250Alack%2520of%2520memory%2520savings%2520limit%2520us%2520from%2520scaling%2520the%2520batch%2520sizes.%250A%2520%2520We%2520propose%2520%2524%255Ctextit%257BHELIOS%257D%2524%252C%2520a%2520framework%2520that%2520improves%2520both%2520token%2520generation%250Alatency%2520and%2520batch%2520sizes%2520to%2520enable%2520high-throughput%2520in%2520EE-LLMs.%2520HELIOS%2520exploits%250Atwo%2520insights.%2520%2524%255Ctextit%257BFirst%257D%2524%252C%2520early%2520exits%2520are%2520often%2520complimentary%2520across%250Amodels%252C%2520tokens%2520that%2520do%2520not%2520exit%2520early%2520on%2520one%2520model%2520often%2520take%2520an%2520early-exit%2520on%250Aanother.%2520HELIOS%2520employs%2520multiple%2520models%2520and%2520dynamically%2520switches%2520between%2520them%250Ato%2520collectively%2520maximize%2520the%2520number%2520of%2520tokens%2520that%2520exit%2520early%252C%2520and%2520minimize%250Atoken%2520generation%2520latencies.%2520%2524%255Ctextit%257BSecond%257D%2524%252C%2520even%2520when%2520a%2520predicted%2520token%2520does%250Anot%2520exit%2520early%2520due%2520to%2520poor%2520confidence%252C%2520it%2520often%2520remains%2520unchanged%2520even%2520after%250Aadditional%2520layer%2520traversal.%2520HELIOS%2520greedily%2520allows%2520such%2520tokens%2520to%2520exit%2520early%250Aand%2520only%2520loads%2520the%2520weights%2520of%2520the%2520most%2520likely%2520to%2520be%2520used%2520layers%252C%2520yielding%250Amemory%2520savings%2520which%2520is%2520then%2520re-purposed%2520to%2520increase%2520batch%2520sizes.%2520HELIOS%250Aemploys%2520real-time%2520profiling%2520to%2520accurately%2520identify%2520the%2520early-exit%250Adistributions%252C%2520and%2520adaptively%2520switches%2520between%2520models%2520by%2520tracking%2520tokens%2520in%250Areal-time%2520to%2520minimize%2520the%2520performance%2520degradation%2520caused%2520by%2520greedy%2520model%250Aloading%2520and%2520exiting.%2520Our%2520evaluations%2520show%2520that%2520HELIOS%2520achieves%2520%25241.48%255Ctimes%2524%250Ahigher%2520throughput%2520and%2520%252415.14%255Ctimes%2524%2520larger%2520batch%2520size%2520compared%2520to%2520existing%250AEE-LLM%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HELIOS%3A%20Adaptive%20Model%20And%20Early-Exit%20Selection%20for%20Efficient%20LLM%0A%20%20Inference%20Serving&entry.906535625=Avinash%20Kumar%20and%20Shashank%20Nag%20and%20Jason%20Clemons%20and%20Lizy%20John%20and%20Poulami%20Das&entry.1292438233=%20%20Early-Exit%20Large%20Language%20Models%20%28EE-LLMs%29%20enable%20high%20throughput%20inference%0Aby%20allowing%20tokens%20to%20exit%20early%20at%20intermediate%20layers.%20However%2C%20their%0Athroughput%20is%20limited%20by%20the%20computational%20and%20memory%20savings.%20Existing%20EE-LLM%0Aframeworks%20rely%20on%20a%20single%20model%20and%20therefore%2C%20their%20token%20generation%0Alatencies%20are%20bottlenecked%20by%20tokens%20that%20do%20not%20exit%20early%20and%20traverse%0Aadditional%20layers.%20Moreover%2C%20early%20exits%20are%20only%20known%20at%20runtime%20and%20depend%0Aon%20the%20request.%20Therefore%2C%20these%20frameworks%20load%20the%20weights%20of%20all%20model%0Alayers%20even%20though%20large%20portions%20remain%20unused%20when%20tokens%20exit%20early.%20The%0Alack%20of%20memory%20savings%20limit%20us%20from%20scaling%20the%20batch%20sizes.%0A%20%20We%20propose%20%24%5Ctextit%7BHELIOS%7D%24%2C%20a%20framework%20that%20improves%20both%20token%20generation%0Alatency%20and%20batch%20sizes%20to%20enable%20high-throughput%20in%20EE-LLMs.%20HELIOS%20exploits%0Atwo%20insights.%20%24%5Ctextit%7BFirst%7D%24%2C%20early%20exits%20are%20often%20complimentary%20across%0Amodels%2C%20tokens%20that%20do%20not%20exit%20early%20on%20one%20model%20often%20take%20an%20early-exit%20on%0Aanother.%20HELIOS%20employs%20multiple%20models%20and%20dynamically%20switches%20between%20them%0Ato%20collectively%20maximize%20the%20number%20of%20tokens%20that%20exit%20early%2C%20and%20minimize%0Atoken%20generation%20latencies.%20%24%5Ctextit%7BSecond%7D%24%2C%20even%20when%20a%20predicted%20token%20does%0Anot%20exit%20early%20due%20to%20poor%20confidence%2C%20it%20often%20remains%20unchanged%20even%20after%0Aadditional%20layer%20traversal.%20HELIOS%20greedily%20allows%20such%20tokens%20to%20exit%20early%0Aand%20only%20loads%20the%20weights%20of%20the%20most%20likely%20to%20be%20used%20layers%2C%20yielding%0Amemory%20savings%20which%20is%20then%20re-purposed%20to%20increase%20batch%20sizes.%20HELIOS%0Aemploys%20real-time%20profiling%20to%20accurately%20identify%20the%20early-exit%0Adistributions%2C%20and%20adaptively%20switches%20between%20models%20by%20tracking%20tokens%20in%0Areal-time%20to%20minimize%20the%20performance%20degradation%20caused%20by%20greedy%20model%0Aloading%20and%20exiting.%20Our%20evaluations%20show%20that%20HELIOS%20achieves%20%241.48%5Ctimes%24%0Ahigher%20throughput%20and%20%2415.14%5Ctimes%24%20larger%20batch%20size%20compared%20to%20existing%0AEE-LLM%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10724v2&entry.124074799=Read"},
{"title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning\n  in Open Agent Systems", "author": "Alireza Saleh Abadi and Leen-Kiat Soh", "abstract": "  In the rapidly evolving field of multi-agent reinforcement learning (MARL),\nunderstanding the dynamics of open systems is crucial. Openness in MARL refers\nto the dynam-ic nature of agent populations, tasks, and agent types with-in a\nsystem. Specifically, there are three types of openness as reported in (Eck et\nal. 2023) [2]: agent openness, where agents can enter or leave the system at\nany time; task openness, where new tasks emerge, and existing ones evolve or\ndisappear; and type openness, where the capabil-ities and behaviors of agents\nchange over time. This report provides a conceptual and empirical review,\nfocusing on the interplay between openness and the credit assignment problem\n(CAP). CAP involves determining the contribution of individual agents to the\noverall system performance, a task that becomes increasingly complex in open\nenviron-ments. Traditional credit assignment (CA) methods often assume static\nagent populations, fixed and pre-defined tasks, and stationary types, making\nthem inadequate for open systems. We first conduct a conceptual analysis,\nin-troducing new sub-categories of openness to detail how events like agent\nturnover or task cancellation break the assumptions of environmental\nstationarity and fixed team composition that underpin existing CAP methods. We\nthen present an empirical study using representative temporal and structural\nalgorithms in an open environment. The results demonstrate that openness\ndirectly causes credit misattribution, evidenced by unstable loss functions and\nsignificant performance degradation.\n", "link": "http://arxiv.org/abs/2510.27659v1", "date": "2025-10-31", "relevancy": 1.4811, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4861}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20in%20Credit%20Assignment%20for%20Multi-Agent%20Reinforcement%20Learning%0A%20%20in%20Open%20Agent%20Systems&body=Title%3A%20Challenges%20in%20Credit%20Assignment%20for%20Multi-Agent%20Reinforcement%20Learning%0A%20%20in%20Open%20Agent%20Systems%0AAuthor%3A%20Alireza%20Saleh%20Abadi%20and%20Leen-Kiat%20Soh%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%0Aunderstanding%20the%20dynamics%20of%20open%20systems%20is%20crucial.%20Openness%20in%20MARL%20refers%0Ato%20the%20dynam-ic%20nature%20of%20agent%20populations%2C%20tasks%2C%20and%20agent%20types%20with-in%20a%0Asystem.%20Specifically%2C%20there%20are%20three%20types%20of%20openness%20as%20reported%20in%20%28Eck%20et%0Aal.%202023%29%20%5B2%5D%3A%20agent%20openness%2C%20where%20agents%20can%20enter%20or%20leave%20the%20system%20at%0Aany%20time%3B%20task%20openness%2C%20where%20new%20tasks%20emerge%2C%20and%20existing%20ones%20evolve%20or%0Adisappear%3B%20and%20type%20openness%2C%20where%20the%20capabil-ities%20and%20behaviors%20of%20agents%0Achange%20over%20time.%20This%20report%20provides%20a%20conceptual%20and%20empirical%20review%2C%0Afocusing%20on%20the%20interplay%20between%20openness%20and%20the%20credit%20assignment%20problem%0A%28CAP%29.%20CAP%20involves%20determining%20the%20contribution%20of%20individual%20agents%20to%20the%0Aoverall%20system%20performance%2C%20a%20task%20that%20becomes%20increasingly%20complex%20in%20open%0Aenviron-ments.%20Traditional%20credit%20assignment%20%28CA%29%20methods%20often%20assume%20static%0Aagent%20populations%2C%20fixed%20and%20pre-defined%20tasks%2C%20and%20stationary%20types%2C%20making%0Athem%20inadequate%20for%20open%20systems.%20We%20first%20conduct%20a%20conceptual%20analysis%2C%0Ain-troducing%20new%20sub-categories%20of%20openness%20to%20detail%20how%20events%20like%20agent%0Aturnover%20or%20task%20cancellation%20break%20the%20assumptions%20of%20environmental%0Astationarity%20and%20fixed%20team%20composition%20that%20underpin%20existing%20CAP%20methods.%20We%0Athen%20present%20an%20empirical%20study%20using%20representative%20temporal%20and%20structural%0Aalgorithms%20in%20an%20open%20environment.%20The%20results%20demonstrate%20that%20openness%0Adirectly%20causes%20credit%20misattribution%2C%20evidenced%20by%20unstable%20loss%20functions%20and%0Asignificant%20performance%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520in%2520Credit%2520Assignment%2520for%2520Multi-Agent%2520Reinforcement%2520Learning%250A%2520%2520in%2520Open%2520Agent%2520Systems%26entry.906535625%3DAlireza%2520Saleh%2520Abadi%2520and%2520Leen-Kiat%2520Soh%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%252C%250Aunderstanding%2520the%2520dynamics%2520of%2520open%2520systems%2520is%2520crucial.%2520Openness%2520in%2520MARL%2520refers%250Ato%2520the%2520dynam-ic%2520nature%2520of%2520agent%2520populations%252C%2520tasks%252C%2520and%2520agent%2520types%2520with-in%2520a%250Asystem.%2520Specifically%252C%2520there%2520are%2520three%2520types%2520of%2520openness%2520as%2520reported%2520in%2520%2528Eck%2520et%250Aal.%25202023%2529%2520%255B2%255D%253A%2520agent%2520openness%252C%2520where%2520agents%2520can%2520enter%2520or%2520leave%2520the%2520system%2520at%250Aany%2520time%253B%2520task%2520openness%252C%2520where%2520new%2520tasks%2520emerge%252C%2520and%2520existing%2520ones%2520evolve%2520or%250Adisappear%253B%2520and%2520type%2520openness%252C%2520where%2520the%2520capabil-ities%2520and%2520behaviors%2520of%2520agents%250Achange%2520over%2520time.%2520This%2520report%2520provides%2520a%2520conceptual%2520and%2520empirical%2520review%252C%250Afocusing%2520on%2520the%2520interplay%2520between%2520openness%2520and%2520the%2520credit%2520assignment%2520problem%250A%2528CAP%2529.%2520CAP%2520involves%2520determining%2520the%2520contribution%2520of%2520individual%2520agents%2520to%2520the%250Aoverall%2520system%2520performance%252C%2520a%2520task%2520that%2520becomes%2520increasingly%2520complex%2520in%2520open%250Aenviron-ments.%2520Traditional%2520credit%2520assignment%2520%2528CA%2529%2520methods%2520often%2520assume%2520static%250Aagent%2520populations%252C%2520fixed%2520and%2520pre-defined%2520tasks%252C%2520and%2520stationary%2520types%252C%2520making%250Athem%2520inadequate%2520for%2520open%2520systems.%2520We%2520first%2520conduct%2520a%2520conceptual%2520analysis%252C%250Ain-troducing%2520new%2520sub-categories%2520of%2520openness%2520to%2520detail%2520how%2520events%2520like%2520agent%250Aturnover%2520or%2520task%2520cancellation%2520break%2520the%2520assumptions%2520of%2520environmental%250Astationarity%2520and%2520fixed%2520team%2520composition%2520that%2520underpin%2520existing%2520CAP%2520methods.%2520We%250Athen%2520present%2520an%2520empirical%2520study%2520using%2520representative%2520temporal%2520and%2520structural%250Aalgorithms%2520in%2520an%2520open%2520environment.%2520The%2520results%2520demonstrate%2520that%2520openness%250Adirectly%2520causes%2520credit%2520misattribution%252C%2520evidenced%2520by%2520unstable%2520loss%2520functions%2520and%250Asignificant%2520performance%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20in%20Credit%20Assignment%20for%20Multi-Agent%20Reinforcement%20Learning%0A%20%20in%20Open%20Agent%20Systems&entry.906535625=Alireza%20Saleh%20Abadi%20and%20Leen-Kiat%20Soh&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20multi-agent%20reinforcement%20learning%20%28MARL%29%2C%0Aunderstanding%20the%20dynamics%20of%20open%20systems%20is%20crucial.%20Openness%20in%20MARL%20refers%0Ato%20the%20dynam-ic%20nature%20of%20agent%20populations%2C%20tasks%2C%20and%20agent%20types%20with-in%20a%0Asystem.%20Specifically%2C%20there%20are%20three%20types%20of%20openness%20as%20reported%20in%20%28Eck%20et%0Aal.%202023%29%20%5B2%5D%3A%20agent%20openness%2C%20where%20agents%20can%20enter%20or%20leave%20the%20system%20at%0Aany%20time%3B%20task%20openness%2C%20where%20new%20tasks%20emerge%2C%20and%20existing%20ones%20evolve%20or%0Adisappear%3B%20and%20type%20openness%2C%20where%20the%20capabil-ities%20and%20behaviors%20of%20agents%0Achange%20over%20time.%20This%20report%20provides%20a%20conceptual%20and%20empirical%20review%2C%0Afocusing%20on%20the%20interplay%20between%20openness%20and%20the%20credit%20assignment%20problem%0A%28CAP%29.%20CAP%20involves%20determining%20the%20contribution%20of%20individual%20agents%20to%20the%0Aoverall%20system%20performance%2C%20a%20task%20that%20becomes%20increasingly%20complex%20in%20open%0Aenviron-ments.%20Traditional%20credit%20assignment%20%28CA%29%20methods%20often%20assume%20static%0Aagent%20populations%2C%20fixed%20and%20pre-defined%20tasks%2C%20and%20stationary%20types%2C%20making%0Athem%20inadequate%20for%20open%20systems.%20We%20first%20conduct%20a%20conceptual%20analysis%2C%0Ain-troducing%20new%20sub-categories%20of%20openness%20to%20detail%20how%20events%20like%20agent%0Aturnover%20or%20task%20cancellation%20break%20the%20assumptions%20of%20environmental%0Astationarity%20and%20fixed%20team%20composition%20that%20underpin%20existing%20CAP%20methods.%20We%0Athen%20present%20an%20empirical%20study%20using%20representative%20temporal%20and%20structural%0Aalgorithms%20in%20an%20open%20environment.%20The%20results%20demonstrate%20that%20openness%0Adirectly%20causes%20credit%20misattribution%2C%20evidenced%20by%20unstable%20loss%20functions%20and%0Asignificant%20performance%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27659v1&entry.124074799=Read"},
{"title": "pDANSE: Particle-based Data-driven Nonlinear State Estimation from\n  Nonlinear Measurements", "author": "Anubhab Ghosh and Yonina C. Eldar and Saikat Chatterjee", "abstract": "  We consider the problem of designing a data-driven nonlinear state estimation\n(DANSE) method that uses (noisy) nonlinear measurements of a process whose\nunderlying state transition model (STM) is unknown. Such a process is referred\nto as a model-free process. A recurrent neural network (RNN) provides\nparameters of a Gaussian prior that characterize the state of the model-free\nprocess, using all previous measurements at a given time point. In the case of\nDANSE, the measurement system was linear, leading to a closed-form solution for\nthe state posterior. However, the presence of a nonlinear measurement system\nrenders a closed-form solution infeasible. Instead, the second-order statistics\nof the state posterior are computed using the nonlinear measurements observed\nat the time point. We address the nonlinear measurements using a\nreparameterization trick-based particle sampling approach, and estimate the\nsecond-order statistics of the state posterior. The proposed method is referred\nto as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequential\nmeasurements efficiently and avoids the use of computationally intensive\nsequential Monte-Carlo (SMC) and/or ancestral sampling. We describe the\nsemi-supervised learning method for pDANSE, which transitions to unsupervised\nlearning in the absence of labeled data. Using a stochastic Lorenz-$63$ system\nas a benchmark process, we experimentally demonstrate the state estimation\nperformance for four nonlinear measurement systems. We explore cubic\nnonlinearity and a camera-model nonlinearity where unsupervised learning is\nused; then we explore half-wave rectification nonlinearity and\nCartesian-to-spherical nonlinearity where semi-supervised learning is used. The\nperformance of state estimation is shown to be competitive vis-\\`a-vis particle\nfilters that have complete knowledge of the STM of the Lorenz-$63$ system.\n", "link": "http://arxiv.org/abs/2510.27503v1", "date": "2025-10-31", "relevancy": 1.4807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20pDANSE%3A%20Particle-based%20Data-driven%20Nonlinear%20State%20Estimation%20from%0A%20%20Nonlinear%20Measurements&body=Title%3A%20pDANSE%3A%20Particle-based%20Data-driven%20Nonlinear%20State%20Estimation%20from%0A%20%20Nonlinear%20Measurements%0AAuthor%3A%20Anubhab%20Ghosh%20and%20Yonina%20C.%20Eldar%20and%20Saikat%20Chatterjee%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20designing%20a%20data-driven%20nonlinear%20state%20estimation%0A%28DANSE%29%20method%20that%20uses%20%28noisy%29%20nonlinear%20measurements%20of%20a%20process%20whose%0Aunderlying%20state%20transition%20model%20%28STM%29%20is%20unknown.%20Such%20a%20process%20is%20referred%0Ato%20as%20a%20model-free%20process.%20A%20recurrent%20neural%20network%20%28RNN%29%20provides%0Aparameters%20of%20a%20Gaussian%20prior%20that%20characterize%20the%20state%20of%20the%20model-free%0Aprocess%2C%20using%20all%20previous%20measurements%20at%20a%20given%20time%20point.%20In%20the%20case%20of%0ADANSE%2C%20the%20measurement%20system%20was%20linear%2C%20leading%20to%20a%20closed-form%20solution%20for%0Athe%20state%20posterior.%20However%2C%20the%20presence%20of%20a%20nonlinear%20measurement%20system%0Arenders%20a%20closed-form%20solution%20infeasible.%20Instead%2C%20the%20second-order%20statistics%0Aof%20the%20state%20posterior%20are%20computed%20using%20the%20nonlinear%20measurements%20observed%0Aat%20the%20time%20point.%20We%20address%20the%20nonlinear%20measurements%20using%20a%0Areparameterization%20trick-based%20particle%20sampling%20approach%2C%20and%20estimate%20the%0Asecond-order%20statistics%20of%20the%20state%20posterior.%20The%20proposed%20method%20is%20referred%0Ato%20as%20particle-based%20DANSE%20%28pDANSE%29.%20The%20RNN%20of%20pDANSE%20uses%20sequential%0Ameasurements%20efficiently%20and%20avoids%20the%20use%20of%20computationally%20intensive%0Asequential%20Monte-Carlo%20%28SMC%29%20and/or%20ancestral%20sampling.%20We%20describe%20the%0Asemi-supervised%20learning%20method%20for%20pDANSE%2C%20which%20transitions%20to%20unsupervised%0Alearning%20in%20the%20absence%20of%20labeled%20data.%20Using%20a%20stochastic%20Lorenz-%2463%24%20system%0Aas%20a%20benchmark%20process%2C%20we%20experimentally%20demonstrate%20the%20state%20estimation%0Aperformance%20for%20four%20nonlinear%20measurement%20systems.%20We%20explore%20cubic%0Anonlinearity%20and%20a%20camera-model%20nonlinearity%20where%20unsupervised%20learning%20is%0Aused%3B%20then%20we%20explore%20half-wave%20rectification%20nonlinearity%20and%0ACartesian-to-spherical%20nonlinearity%20where%20semi-supervised%20learning%20is%20used.%20The%0Aperformance%20of%20state%20estimation%20is%20shown%20to%20be%20competitive%20vis-%5C%60a-vis%20particle%0Afilters%20that%20have%20complete%20knowledge%20of%20the%20STM%20of%20the%20Lorenz-%2463%24%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpDANSE%253A%2520Particle-based%2520Data-driven%2520Nonlinear%2520State%2520Estimation%2520from%250A%2520%2520Nonlinear%2520Measurements%26entry.906535625%3DAnubhab%2520Ghosh%2520and%2520Yonina%2520C.%2520Eldar%2520and%2520Saikat%2520Chatterjee%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520designing%2520a%2520data-driven%2520nonlinear%2520state%2520estimation%250A%2528DANSE%2529%2520method%2520that%2520uses%2520%2528noisy%2529%2520nonlinear%2520measurements%2520of%2520a%2520process%2520whose%250Aunderlying%2520state%2520transition%2520model%2520%2528STM%2529%2520is%2520unknown.%2520Such%2520a%2520process%2520is%2520referred%250Ato%2520as%2520a%2520model-free%2520process.%2520A%2520recurrent%2520neural%2520network%2520%2528RNN%2529%2520provides%250Aparameters%2520of%2520a%2520Gaussian%2520prior%2520that%2520characterize%2520the%2520state%2520of%2520the%2520model-free%250Aprocess%252C%2520using%2520all%2520previous%2520measurements%2520at%2520a%2520given%2520time%2520point.%2520In%2520the%2520case%2520of%250ADANSE%252C%2520the%2520measurement%2520system%2520was%2520linear%252C%2520leading%2520to%2520a%2520closed-form%2520solution%2520for%250Athe%2520state%2520posterior.%2520However%252C%2520the%2520presence%2520of%2520a%2520nonlinear%2520measurement%2520system%250Arenders%2520a%2520closed-form%2520solution%2520infeasible.%2520Instead%252C%2520the%2520second-order%2520statistics%250Aof%2520the%2520state%2520posterior%2520are%2520computed%2520using%2520the%2520nonlinear%2520measurements%2520observed%250Aat%2520the%2520time%2520point.%2520We%2520address%2520the%2520nonlinear%2520measurements%2520using%2520a%250Areparameterization%2520trick-based%2520particle%2520sampling%2520approach%252C%2520and%2520estimate%2520the%250Asecond-order%2520statistics%2520of%2520the%2520state%2520posterior.%2520The%2520proposed%2520method%2520is%2520referred%250Ato%2520as%2520particle-based%2520DANSE%2520%2528pDANSE%2529.%2520The%2520RNN%2520of%2520pDANSE%2520uses%2520sequential%250Ameasurements%2520efficiently%2520and%2520avoids%2520the%2520use%2520of%2520computationally%2520intensive%250Asequential%2520Monte-Carlo%2520%2528SMC%2529%2520and/or%2520ancestral%2520sampling.%2520We%2520describe%2520the%250Asemi-supervised%2520learning%2520method%2520for%2520pDANSE%252C%2520which%2520transitions%2520to%2520unsupervised%250Alearning%2520in%2520the%2520absence%2520of%2520labeled%2520data.%2520Using%2520a%2520stochastic%2520Lorenz-%252463%2524%2520system%250Aas%2520a%2520benchmark%2520process%252C%2520we%2520experimentally%2520demonstrate%2520the%2520state%2520estimation%250Aperformance%2520for%2520four%2520nonlinear%2520measurement%2520systems.%2520We%2520explore%2520cubic%250Anonlinearity%2520and%2520a%2520camera-model%2520nonlinearity%2520where%2520unsupervised%2520learning%2520is%250Aused%253B%2520then%2520we%2520explore%2520half-wave%2520rectification%2520nonlinearity%2520and%250ACartesian-to-spherical%2520nonlinearity%2520where%2520semi-supervised%2520learning%2520is%2520used.%2520The%250Aperformance%2520of%2520state%2520estimation%2520is%2520shown%2520to%2520be%2520competitive%2520vis-%255C%2560a-vis%2520particle%250Afilters%2520that%2520have%2520complete%2520knowledge%2520of%2520the%2520STM%2520of%2520the%2520Lorenz-%252463%2524%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pDANSE%3A%20Particle-based%20Data-driven%20Nonlinear%20State%20Estimation%20from%0A%20%20Nonlinear%20Measurements&entry.906535625=Anubhab%20Ghosh%20and%20Yonina%20C.%20Eldar%20and%20Saikat%20Chatterjee&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20designing%20a%20data-driven%20nonlinear%20state%20estimation%0A%28DANSE%29%20method%20that%20uses%20%28noisy%29%20nonlinear%20measurements%20of%20a%20process%20whose%0Aunderlying%20state%20transition%20model%20%28STM%29%20is%20unknown.%20Such%20a%20process%20is%20referred%0Ato%20as%20a%20model-free%20process.%20A%20recurrent%20neural%20network%20%28RNN%29%20provides%0Aparameters%20of%20a%20Gaussian%20prior%20that%20characterize%20the%20state%20of%20the%20model-free%0Aprocess%2C%20using%20all%20previous%20measurements%20at%20a%20given%20time%20point.%20In%20the%20case%20of%0ADANSE%2C%20the%20measurement%20system%20was%20linear%2C%20leading%20to%20a%20closed-form%20solution%20for%0Athe%20state%20posterior.%20However%2C%20the%20presence%20of%20a%20nonlinear%20measurement%20system%0Arenders%20a%20closed-form%20solution%20infeasible.%20Instead%2C%20the%20second-order%20statistics%0Aof%20the%20state%20posterior%20are%20computed%20using%20the%20nonlinear%20measurements%20observed%0Aat%20the%20time%20point.%20We%20address%20the%20nonlinear%20measurements%20using%20a%0Areparameterization%20trick-based%20particle%20sampling%20approach%2C%20and%20estimate%20the%0Asecond-order%20statistics%20of%20the%20state%20posterior.%20The%20proposed%20method%20is%20referred%0Ato%20as%20particle-based%20DANSE%20%28pDANSE%29.%20The%20RNN%20of%20pDANSE%20uses%20sequential%0Ameasurements%20efficiently%20and%20avoids%20the%20use%20of%20computationally%20intensive%0Asequential%20Monte-Carlo%20%28SMC%29%20and/or%20ancestral%20sampling.%20We%20describe%20the%0Asemi-supervised%20learning%20method%20for%20pDANSE%2C%20which%20transitions%20to%20unsupervised%0Alearning%20in%20the%20absence%20of%20labeled%20data.%20Using%20a%20stochastic%20Lorenz-%2463%24%20system%0Aas%20a%20benchmark%20process%2C%20we%20experimentally%20demonstrate%20the%20state%20estimation%0Aperformance%20for%20four%20nonlinear%20measurement%20systems.%20We%20explore%20cubic%0Anonlinearity%20and%20a%20camera-model%20nonlinearity%20where%20unsupervised%20learning%20is%0Aused%3B%20then%20we%20explore%20half-wave%20rectification%20nonlinearity%20and%0ACartesian-to-spherical%20nonlinearity%20where%20semi-supervised%20learning%20is%20used.%20The%0Aperformance%20of%20state%20estimation%20is%20shown%20to%20be%20competitive%20vis-%5C%60a-vis%20particle%0Afilters%20that%20have%20complete%20knowledge%20of%20the%20STM%20of%20the%20Lorenz-%2463%24%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27503v1&entry.124074799=Read"},
{"title": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale\n  Orchestration", "author": "Yan Jiang and Hao Zhou and LiZhong GU and Ai Han and TianLong Li", "abstract": "  Large language models (LLMs) have recently demonstrated the ability to act as\nfunction call agents by invoking external tools, enabling them to solve tasks\nbeyond their static knowledge. However, existing agents typically call tools\nstep by step at a time without a global view of task structure. As tools depend\non each other, this leads to error accumulation and limited scalability,\nparticularly when scaling to thousands of tools. To address these limitations,\nwe propose NaviAgent, a novel bilevel architecture that decouples task planning\nfrom tool execution through graph-based modeling of the tool ecosystem. At the\ntask-planning level, the LLM-based agent decides whether to respond directly,\nclarify user intent, invoke a toolchain, or execute tool outputs, ensuring\nbroad coverage of interaction scenarios independent of inter-tool complexity.\nAt the execution level, a continuously evolving Tool World Navigation Model\n(TWNM) encodes structural and behavioral relations among tools, guiding the\nagent to generate scalable and robust invocation sequences. By incorporating\nfeedback from real tool interactions, NaviAgent supports closed-loop\noptimization of planning and execution, moving beyond tool calling toward\nadaptive navigation of large-scale tool ecosystems. Experiments show that\nNaviAgent achieves the best task success rates across models and tasks, and\nintegrating TWMN further boosts performance by up to 17 points on complex\ntasks, underscoring its key role in toolchain orchestration.\n", "link": "http://arxiv.org/abs/2506.19500v2", "date": "2025-10-31", "relevancy": 1.4709, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.538}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4788}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NaviAgent%3A%20Bilevel%20Planning%20on%20Tool%20Navigation%20Graph%20for%20Large-Scale%0A%20%20Orchestration&body=Title%3A%20NaviAgent%3A%20Bilevel%20Planning%20on%20Tool%20Navigation%20Graph%20for%20Large-Scale%0A%20%20Orchestration%0AAuthor%3A%20Yan%20Jiang%20and%20Hao%20Zhou%20and%20LiZhong%20GU%20and%20Ai%20Han%20and%20TianLong%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20the%20ability%20to%20act%20as%0Afunction%20call%20agents%20by%20invoking%20external%20tools%2C%20enabling%20them%20to%20solve%20tasks%0Abeyond%20their%20static%20knowledge.%20However%2C%20existing%20agents%20typically%20call%20tools%0Astep%20by%20step%20at%20a%20time%20without%20a%20global%20view%20of%20task%20structure.%20As%20tools%20depend%0Aon%20each%20other%2C%20this%20leads%20to%20error%20accumulation%20and%20limited%20scalability%2C%0Aparticularly%20when%20scaling%20to%20thousands%20of%20tools.%20To%20address%20these%20limitations%2C%0Awe%20propose%20NaviAgent%2C%20a%20novel%20bilevel%20architecture%20that%20decouples%20task%20planning%0Afrom%20tool%20execution%20through%20graph-based%20modeling%20of%20the%20tool%20ecosystem.%20At%20the%0Atask-planning%20level%2C%20the%20LLM-based%20agent%20decides%20whether%20to%20respond%20directly%2C%0Aclarify%20user%20intent%2C%20invoke%20a%20toolchain%2C%20or%20execute%20tool%20outputs%2C%20ensuring%0Abroad%20coverage%20of%20interaction%20scenarios%20independent%20of%20inter-tool%20complexity.%0AAt%20the%20execution%20level%2C%20a%20continuously%20evolving%20Tool%20World%20Navigation%20Model%0A%28TWNM%29%20encodes%20structural%20and%20behavioral%20relations%20among%20tools%2C%20guiding%20the%0Aagent%20to%20generate%20scalable%20and%20robust%20invocation%20sequences.%20By%20incorporating%0Afeedback%20from%20real%20tool%20interactions%2C%20NaviAgent%20supports%20closed-loop%0Aoptimization%20of%20planning%20and%20execution%2C%20moving%20beyond%20tool%20calling%20toward%0Aadaptive%20navigation%20of%20large-scale%20tool%20ecosystems.%20Experiments%20show%20that%0ANaviAgent%20achieves%20the%20best%20task%20success%20rates%20across%20models%20and%20tasks%2C%20and%0Aintegrating%20TWMN%20further%20boosts%20performance%20by%20up%20to%2017%20points%20on%20complex%0Atasks%2C%20underscoring%20its%20key%20role%20in%20toolchain%20orchestration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNaviAgent%253A%2520Bilevel%2520Planning%2520on%2520Tool%2520Navigation%2520Graph%2520for%2520Large-Scale%250A%2520%2520Orchestration%26entry.906535625%3DYan%2520Jiang%2520and%2520Hao%2520Zhou%2520and%2520LiZhong%2520GU%2520and%2520Ai%2520Han%2520and%2520TianLong%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520the%2520ability%2520to%2520act%2520as%250Afunction%2520call%2520agents%2520by%2520invoking%2520external%2520tools%252C%2520enabling%2520them%2520to%2520solve%2520tasks%250Abeyond%2520their%2520static%2520knowledge.%2520However%252C%2520existing%2520agents%2520typically%2520call%2520tools%250Astep%2520by%2520step%2520at%2520a%2520time%2520without%2520a%2520global%2520view%2520of%2520task%2520structure.%2520As%2520tools%2520depend%250Aon%2520each%2520other%252C%2520this%2520leads%2520to%2520error%2520accumulation%2520and%2520limited%2520scalability%252C%250Aparticularly%2520when%2520scaling%2520to%2520thousands%2520of%2520tools.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520NaviAgent%252C%2520a%2520novel%2520bilevel%2520architecture%2520that%2520decouples%2520task%2520planning%250Afrom%2520tool%2520execution%2520through%2520graph-based%2520modeling%2520of%2520the%2520tool%2520ecosystem.%2520At%2520the%250Atask-planning%2520level%252C%2520the%2520LLM-based%2520agent%2520decides%2520whether%2520to%2520respond%2520directly%252C%250Aclarify%2520user%2520intent%252C%2520invoke%2520a%2520toolchain%252C%2520or%2520execute%2520tool%2520outputs%252C%2520ensuring%250Abroad%2520coverage%2520of%2520interaction%2520scenarios%2520independent%2520of%2520inter-tool%2520complexity.%250AAt%2520the%2520execution%2520level%252C%2520a%2520continuously%2520evolving%2520Tool%2520World%2520Navigation%2520Model%250A%2528TWNM%2529%2520encodes%2520structural%2520and%2520behavioral%2520relations%2520among%2520tools%252C%2520guiding%2520the%250Aagent%2520to%2520generate%2520scalable%2520and%2520robust%2520invocation%2520sequences.%2520By%2520incorporating%250Afeedback%2520from%2520real%2520tool%2520interactions%252C%2520NaviAgent%2520supports%2520closed-loop%250Aoptimization%2520of%2520planning%2520and%2520execution%252C%2520moving%2520beyond%2520tool%2520calling%2520toward%250Aadaptive%2520navigation%2520of%2520large-scale%2520tool%2520ecosystems.%2520Experiments%2520show%2520that%250ANaviAgent%2520achieves%2520the%2520best%2520task%2520success%2520rates%2520across%2520models%2520and%2520tasks%252C%2520and%250Aintegrating%2520TWMN%2520further%2520boosts%2520performance%2520by%2520up%2520to%252017%2520points%2520on%2520complex%250Atasks%252C%2520underscoring%2520its%2520key%2520role%2520in%2520toolchain%2520orchestration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NaviAgent%3A%20Bilevel%20Planning%20on%20Tool%20Navigation%20Graph%20for%20Large-Scale%0A%20%20Orchestration&entry.906535625=Yan%20Jiang%20and%20Hao%20Zhou%20and%20LiZhong%20GU%20and%20Ai%20Han%20and%20TianLong%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20demonstrated%20the%20ability%20to%20act%20as%0Afunction%20call%20agents%20by%20invoking%20external%20tools%2C%20enabling%20them%20to%20solve%20tasks%0Abeyond%20their%20static%20knowledge.%20However%2C%20existing%20agents%20typically%20call%20tools%0Astep%20by%20step%20at%20a%20time%20without%20a%20global%20view%20of%20task%20structure.%20As%20tools%20depend%0Aon%20each%20other%2C%20this%20leads%20to%20error%20accumulation%20and%20limited%20scalability%2C%0Aparticularly%20when%20scaling%20to%20thousands%20of%20tools.%20To%20address%20these%20limitations%2C%0Awe%20propose%20NaviAgent%2C%20a%20novel%20bilevel%20architecture%20that%20decouples%20task%20planning%0Afrom%20tool%20execution%20through%20graph-based%20modeling%20of%20the%20tool%20ecosystem.%20At%20the%0Atask-planning%20level%2C%20the%20LLM-based%20agent%20decides%20whether%20to%20respond%20directly%2C%0Aclarify%20user%20intent%2C%20invoke%20a%20toolchain%2C%20or%20execute%20tool%20outputs%2C%20ensuring%0Abroad%20coverage%20of%20interaction%20scenarios%20independent%20of%20inter-tool%20complexity.%0AAt%20the%20execution%20level%2C%20a%20continuously%20evolving%20Tool%20World%20Navigation%20Model%0A%28TWNM%29%20encodes%20structural%20and%20behavioral%20relations%20among%20tools%2C%20guiding%20the%0Aagent%20to%20generate%20scalable%20and%20robust%20invocation%20sequences.%20By%20incorporating%0Afeedback%20from%20real%20tool%20interactions%2C%20NaviAgent%20supports%20closed-loop%0Aoptimization%20of%20planning%20and%20execution%2C%20moving%20beyond%20tool%20calling%20toward%0Aadaptive%20navigation%20of%20large-scale%20tool%20ecosystems.%20Experiments%20show%20that%0ANaviAgent%20achieves%20the%20best%20task%20success%20rates%20across%20models%20and%20tasks%2C%20and%0Aintegrating%20TWMN%20further%20boosts%20performance%20by%20up%20to%2017%20points%20on%20complex%0Atasks%2C%20underscoring%20its%20key%20role%20in%20toolchain%20orchestration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19500v2&entry.124074799=Read"},
{"title": "Sybil-Resistant Service Discovery for Agent Economies", "author": "David Shi and Kevin Joo", "abstract": "  x402 enables Hypertext Transfer Protocol (HTTP) services like application\nprogramming interfaces (APIs), data feeds, and inference providers to accept\ncryptocurrency payments for access. As agents increasingly consume these\nservices, discovery becomes critical: which swap interface should an agent\ntrust? Which data provider is the most reliable? We introduce TraceRank, a\nreputation-weighted ranking algorithm where payment transactions serve as\nendorsements. TraceRank seeds addresses with precomputed reputation metrics and\npropagates reputation through payment flows weighted by transaction value and\ntemporal recency. Applied to x402's payment graph, this surfaces services\npreferred by high-reputation users rather than those with high transaction\nvolume. Our system combines TraceRank with semantic search to respond to\nnatural language queries with high quality results. We argue that reputation\npropagation resists Sybil attacks by making spam services with many\nlow-reputation payers rank below legitimate services with few high-reputation\npayers. Ultimately, we aim to construct a search method for x402 enabled\nservices that avoids infrastructure bias and has better performance than purely\nvolume based or semantic methods.\n", "link": "http://arxiv.org/abs/2510.27554v1", "date": "2025-10-31", "relevancy": 0.7867, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3946}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sybil-Resistant%20Service%20Discovery%20for%20Agent%20Economies&body=Title%3A%20Sybil-Resistant%20Service%20Discovery%20for%20Agent%20Economies%0AAuthor%3A%20David%20Shi%20and%20Kevin%20Joo%0AAbstract%3A%20%20%20x402%20enables%20Hypertext%20Transfer%20Protocol%20%28HTTP%29%20services%20like%20application%0Aprogramming%20interfaces%20%28APIs%29%2C%20data%20feeds%2C%20and%20inference%20providers%20to%20accept%0Acryptocurrency%20payments%20for%20access.%20As%20agents%20increasingly%20consume%20these%0Aservices%2C%20discovery%20becomes%20critical%3A%20which%20swap%20interface%20should%20an%20agent%0Atrust%3F%20Which%20data%20provider%20is%20the%20most%20reliable%3F%20We%20introduce%20TraceRank%2C%20a%0Areputation-weighted%20ranking%20algorithm%20where%20payment%20transactions%20serve%20as%0Aendorsements.%20TraceRank%20seeds%20addresses%20with%20precomputed%20reputation%20metrics%20and%0Apropagates%20reputation%20through%20payment%20flows%20weighted%20by%20transaction%20value%20and%0Atemporal%20recency.%20Applied%20to%20x402%27s%20payment%20graph%2C%20this%20surfaces%20services%0Apreferred%20by%20high-reputation%20users%20rather%20than%20those%20with%20high%20transaction%0Avolume.%20Our%20system%20combines%20TraceRank%20with%20semantic%20search%20to%20respond%20to%0Anatural%20language%20queries%20with%20high%20quality%20results.%20We%20argue%20that%20reputation%0Apropagation%20resists%20Sybil%20attacks%20by%20making%20spam%20services%20with%20many%0Alow-reputation%20payers%20rank%20below%20legitimate%20services%20with%20few%20high-reputation%0Apayers.%20Ultimately%2C%20we%20aim%20to%20construct%20a%20search%20method%20for%20x402%20enabled%0Aservices%20that%20avoids%20infrastructure%20bias%20and%20has%20better%20performance%20than%20purely%0Avolume%20based%20or%20semantic%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSybil-Resistant%2520Service%2520Discovery%2520for%2520Agent%2520Economies%26entry.906535625%3DDavid%2520Shi%2520and%2520Kevin%2520Joo%26entry.1292438233%3D%2520%2520x402%2520enables%2520Hypertext%2520Transfer%2520Protocol%2520%2528HTTP%2529%2520services%2520like%2520application%250Aprogramming%2520interfaces%2520%2528APIs%2529%252C%2520data%2520feeds%252C%2520and%2520inference%2520providers%2520to%2520accept%250Acryptocurrency%2520payments%2520for%2520access.%2520As%2520agents%2520increasingly%2520consume%2520these%250Aservices%252C%2520discovery%2520becomes%2520critical%253A%2520which%2520swap%2520interface%2520should%2520an%2520agent%250Atrust%253F%2520Which%2520data%2520provider%2520is%2520the%2520most%2520reliable%253F%2520We%2520introduce%2520TraceRank%252C%2520a%250Areputation-weighted%2520ranking%2520algorithm%2520where%2520payment%2520transactions%2520serve%2520as%250Aendorsements.%2520TraceRank%2520seeds%2520addresses%2520with%2520precomputed%2520reputation%2520metrics%2520and%250Apropagates%2520reputation%2520through%2520payment%2520flows%2520weighted%2520by%2520transaction%2520value%2520and%250Atemporal%2520recency.%2520Applied%2520to%2520x402%2527s%2520payment%2520graph%252C%2520this%2520surfaces%2520services%250Apreferred%2520by%2520high-reputation%2520users%2520rather%2520than%2520those%2520with%2520high%2520transaction%250Avolume.%2520Our%2520system%2520combines%2520TraceRank%2520with%2520semantic%2520search%2520to%2520respond%2520to%250Anatural%2520language%2520queries%2520with%2520high%2520quality%2520results.%2520We%2520argue%2520that%2520reputation%250Apropagation%2520resists%2520Sybil%2520attacks%2520by%2520making%2520spam%2520services%2520with%2520many%250Alow-reputation%2520payers%2520rank%2520below%2520legitimate%2520services%2520with%2520few%2520high-reputation%250Apayers.%2520Ultimately%252C%2520we%2520aim%2520to%2520construct%2520a%2520search%2520method%2520for%2520x402%2520enabled%250Aservices%2520that%2520avoids%2520infrastructure%2520bias%2520and%2520has%2520better%2520performance%2520than%2520purely%250Avolume%2520based%2520or%2520semantic%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sybil-Resistant%20Service%20Discovery%20for%20Agent%20Economies&entry.906535625=David%20Shi%20and%20Kevin%20Joo&entry.1292438233=%20%20x402%20enables%20Hypertext%20Transfer%20Protocol%20%28HTTP%29%20services%20like%20application%0Aprogramming%20interfaces%20%28APIs%29%2C%20data%20feeds%2C%20and%20inference%20providers%20to%20accept%0Acryptocurrency%20payments%20for%20access.%20As%20agents%20increasingly%20consume%20these%0Aservices%2C%20discovery%20becomes%20critical%3A%20which%20swap%20interface%20should%20an%20agent%0Atrust%3F%20Which%20data%20provider%20is%20the%20most%20reliable%3F%20We%20introduce%20TraceRank%2C%20a%0Areputation-weighted%20ranking%20algorithm%20where%20payment%20transactions%20serve%20as%0Aendorsements.%20TraceRank%20seeds%20addresses%20with%20precomputed%20reputation%20metrics%20and%0Apropagates%20reputation%20through%20payment%20flows%20weighted%20by%20transaction%20value%20and%0Atemporal%20recency.%20Applied%20to%20x402%27s%20payment%20graph%2C%20this%20surfaces%20services%0Apreferred%20by%20high-reputation%20users%20rather%20than%20those%20with%20high%20transaction%0Avolume.%20Our%20system%20combines%20TraceRank%20with%20semantic%20search%20to%20respond%20to%0Anatural%20language%20queries%20with%20high%20quality%20results.%20We%20argue%20that%20reputation%0Apropagation%20resists%20Sybil%20attacks%20by%20making%20spam%20services%20with%20many%0Alow-reputation%20payers%20rank%20below%20legitimate%20services%20with%20few%20high-reputation%0Apayers.%20Ultimately%2C%20we%20aim%20to%20construct%20a%20search%20method%20for%20x402%20enabled%0Aservices%20that%20avoids%20infrastructure%20bias%20and%20has%20better%20performance%20than%20purely%0Avolume%20based%20or%20semantic%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27554v1&entry.124074799=Read"},
{"title": "Community Detection on Model Explanation Graphs for Explainable AI", "author": "Ehsan Moradi", "abstract": "  Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions\nbut often miss higher-order structure: sets of features that act in concert. We\npropose Modules of Influence (MoI), a framework that (i) constructs a model\nexplanation graph from per-instance attributions, (ii) applies community\ndetection to find feature modules that jointly affect predictions, and (iii)\nquantifies how these modules relate to bias, redundancy, and causality\npatterns. Across synthetic and real datasets, MoI uncovers correlated feature\ngroups, improves model debugging via module-level ablations, and localizes bias\nexposure to specific modules. We release stability and synergy metrics, a\nreference implementation, and evaluation protocols to benchmark module\ndiscovery in XAI.\n", "link": "http://arxiv.org/abs/2510.27655v1", "date": "2025-10-31", "relevancy": 1.4699, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5002}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Community%20Detection%20on%20Model%20Explanation%20Graphs%20for%20Explainable%20AI&body=Title%3A%20Community%20Detection%20on%20Model%20Explanation%20Graphs%20for%20Explainable%20AI%0AAuthor%3A%20Ehsan%20Moradi%0AAbstract%3A%20%20%20Feature-attribution%20methods%20%28e.g.%2C%20SHAP%2C%20LIME%29%20explain%20individual%20predictions%0Abut%20often%20miss%20higher-order%20structure%3A%20sets%20of%20features%20that%20act%20in%20concert.%20We%0Apropose%20Modules%20of%20Influence%20%28MoI%29%2C%20a%20framework%20that%20%28i%29%20constructs%20a%20model%0Aexplanation%20graph%20from%20per-instance%20attributions%2C%20%28ii%29%20applies%20community%0Adetection%20to%20find%20feature%20modules%20that%20jointly%20affect%20predictions%2C%20and%20%28iii%29%0Aquantifies%20how%20these%20modules%20relate%20to%20bias%2C%20redundancy%2C%20and%20causality%0Apatterns.%20Across%20synthetic%20and%20real%20datasets%2C%20MoI%20uncovers%20correlated%20feature%0Agroups%2C%20improves%20model%20debugging%20via%20module-level%20ablations%2C%20and%20localizes%20bias%0Aexposure%20to%20specific%20modules.%20We%20release%20stability%20and%20synergy%20metrics%2C%20a%0Areference%20implementation%2C%20and%20evaluation%20protocols%20to%20benchmark%20module%0Adiscovery%20in%20XAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunity%2520Detection%2520on%2520Model%2520Explanation%2520Graphs%2520for%2520Explainable%2520AI%26entry.906535625%3DEhsan%2520Moradi%26entry.1292438233%3D%2520%2520Feature-attribution%2520methods%2520%2528e.g.%252C%2520SHAP%252C%2520LIME%2529%2520explain%2520individual%2520predictions%250Abut%2520often%2520miss%2520higher-order%2520structure%253A%2520sets%2520of%2520features%2520that%2520act%2520in%2520concert.%2520We%250Apropose%2520Modules%2520of%2520Influence%2520%2528MoI%2529%252C%2520a%2520framework%2520that%2520%2528i%2529%2520constructs%2520a%2520model%250Aexplanation%2520graph%2520from%2520per-instance%2520attributions%252C%2520%2528ii%2529%2520applies%2520community%250Adetection%2520to%2520find%2520feature%2520modules%2520that%2520jointly%2520affect%2520predictions%252C%2520and%2520%2528iii%2529%250Aquantifies%2520how%2520these%2520modules%2520relate%2520to%2520bias%252C%2520redundancy%252C%2520and%2520causality%250Apatterns.%2520Across%2520synthetic%2520and%2520real%2520datasets%252C%2520MoI%2520uncovers%2520correlated%2520feature%250Agroups%252C%2520improves%2520model%2520debugging%2520via%2520module-level%2520ablations%252C%2520and%2520localizes%2520bias%250Aexposure%2520to%2520specific%2520modules.%2520We%2520release%2520stability%2520and%2520synergy%2520metrics%252C%2520a%250Areference%2520implementation%252C%2520and%2520evaluation%2520protocols%2520to%2520benchmark%2520module%250Adiscovery%2520in%2520XAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Community%20Detection%20on%20Model%20Explanation%20Graphs%20for%20Explainable%20AI&entry.906535625=Ehsan%20Moradi&entry.1292438233=%20%20Feature-attribution%20methods%20%28e.g.%2C%20SHAP%2C%20LIME%29%20explain%20individual%20predictions%0Abut%20often%20miss%20higher-order%20structure%3A%20sets%20of%20features%20that%20act%20in%20concert.%20We%0Apropose%20Modules%20of%20Influence%20%28MoI%29%2C%20a%20framework%20that%20%28i%29%20constructs%20a%20model%0Aexplanation%20graph%20from%20per-instance%20attributions%2C%20%28ii%29%20applies%20community%0Adetection%20to%20find%20feature%20modules%20that%20jointly%20affect%20predictions%2C%20and%20%28iii%29%0Aquantifies%20how%20these%20modules%20relate%20to%20bias%2C%20redundancy%2C%20and%20causality%0Apatterns.%20Across%20synthetic%20and%20real%20datasets%2C%20MoI%20uncovers%20correlated%20feature%0Agroups%2C%20improves%20model%20debugging%20via%20module-level%20ablations%2C%20and%20localizes%20bias%0Aexposure%20to%20specific%20modules.%20We%20release%20stability%20and%20synergy%20metrics%2C%20a%0Areference%20implementation%2C%20and%20evaluation%20protocols%20to%20benchmark%20module%0Adiscovery%20in%20XAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27655v1&entry.124074799=Read"},
{"title": "Smooth Flow Matching", "author": "Jianbin Tan and Anru R. Zhang", "abstract": "  Functional data, i.e., smooth random functions observed over a continuous\ndomain, are increasingly available in areas such as biomedical research, health\ninformatics, and epidemiology. However, effective statistical analysis for\nfunctional data is often hindered by challenges such as privacy constraints,\nsparse and irregular sampling, infinite dimensionality, and non-Gaussian\nstructures. To address these challenges, we introduce a novel framework named\nSmooth Flow Matching (SFM), tailored for generative modeling of functional data\nto enable statistical analysis without exposing sensitive real data. Built upon\nflow-matching ideas, SFM constructs a semiparametric copula flow to generate\ninfinite-dimensional functional data, free from Gaussianity or low-rank\nassumptions. It is computationally efficient, handles irregular observations,\nand guarantees the smoothness of the generated functions, offering a practical\nand flexible solution in scenarios where existing deep generative methods are\nnot applicable. Through extensive simulation studies, we demonstrate the\nadvantages of SFM in terms of both synthetic data quality and computational\nefficiency. We then apply SFM to generate clinical trajectory data from the\nMIMIC-IV patient electronic health records (EHR) longitudinal database. Our\nanalysis showcases the ability of SFM to produce high-quality surrogate data\nfor downstream statistical tasks, highlighting its potential to boost the\nutility of EHR data for clinical applications.\n", "link": "http://arxiv.org/abs/2508.13831v2", "date": "2025-10-31", "relevancy": 1.4626, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5128}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Flow%20Matching&body=Title%3A%20Smooth%20Flow%20Matching%0AAuthor%3A%20Jianbin%20Tan%20and%20Anru%20R.%20Zhang%0AAbstract%3A%20%20%20Functional%20data%2C%20i.e.%2C%20smooth%20random%20functions%20observed%20over%20a%20continuous%0Adomain%2C%20are%20increasingly%20available%20in%20areas%20such%20as%20biomedical%20research%2C%20health%0Ainformatics%2C%20and%20epidemiology.%20However%2C%20effective%20statistical%20analysis%20for%0Afunctional%20data%20is%20often%20hindered%20by%20challenges%20such%20as%20privacy%20constraints%2C%0Asparse%20and%20irregular%20sampling%2C%20infinite%20dimensionality%2C%20and%20non-Gaussian%0Astructures.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20framework%20named%0ASmooth%20Flow%20Matching%20%28SFM%29%2C%20tailored%20for%20generative%20modeling%20of%20functional%20data%0Ato%20enable%20statistical%20analysis%20without%20exposing%20sensitive%20real%20data.%20Built%20upon%0Aflow-matching%20ideas%2C%20SFM%20constructs%20a%20semiparametric%20copula%20flow%20to%20generate%0Ainfinite-dimensional%20functional%20data%2C%20free%20from%20Gaussianity%20or%20low-rank%0Aassumptions.%20It%20is%20computationally%20efficient%2C%20handles%20irregular%20observations%2C%0Aand%20guarantees%20the%20smoothness%20of%20the%20generated%20functions%2C%20offering%20a%20practical%0Aand%20flexible%20solution%20in%20scenarios%20where%20existing%20deep%20generative%20methods%20are%0Anot%20applicable.%20Through%20extensive%20simulation%20studies%2C%20we%20demonstrate%20the%0Aadvantages%20of%20SFM%20in%20terms%20of%20both%20synthetic%20data%20quality%20and%20computational%0Aefficiency.%20We%20then%20apply%20SFM%20to%20generate%20clinical%20trajectory%20data%20from%20the%0AMIMIC-IV%20patient%20electronic%20health%20records%20%28EHR%29%20longitudinal%20database.%20Our%0Aanalysis%20showcases%20the%20ability%20of%20SFM%20to%20produce%20high-quality%20surrogate%20data%0Afor%20downstream%20statistical%20tasks%2C%20highlighting%20its%20potential%20to%20boost%20the%0Autility%20of%20EHR%20data%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Flow%2520Matching%26entry.906535625%3DJianbin%2520Tan%2520and%2520Anru%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520Functional%2520data%252C%2520i.e.%252C%2520smooth%2520random%2520functions%2520observed%2520over%2520a%2520continuous%250Adomain%252C%2520are%2520increasingly%2520available%2520in%2520areas%2520such%2520as%2520biomedical%2520research%252C%2520health%250Ainformatics%252C%2520and%2520epidemiology.%2520However%252C%2520effective%2520statistical%2520analysis%2520for%250Afunctional%2520data%2520is%2520often%2520hindered%2520by%2520challenges%2520such%2520as%2520privacy%2520constraints%252C%250Asparse%2520and%2520irregular%2520sampling%252C%2520infinite%2520dimensionality%252C%2520and%2520non-Gaussian%250Astructures.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520named%250ASmooth%2520Flow%2520Matching%2520%2528SFM%2529%252C%2520tailored%2520for%2520generative%2520modeling%2520of%2520functional%2520data%250Ato%2520enable%2520statistical%2520analysis%2520without%2520exposing%2520sensitive%2520real%2520data.%2520Built%2520upon%250Aflow-matching%2520ideas%252C%2520SFM%2520constructs%2520a%2520semiparametric%2520copula%2520flow%2520to%2520generate%250Ainfinite-dimensional%2520functional%2520data%252C%2520free%2520from%2520Gaussianity%2520or%2520low-rank%250Aassumptions.%2520It%2520is%2520computationally%2520efficient%252C%2520handles%2520irregular%2520observations%252C%250Aand%2520guarantees%2520the%2520smoothness%2520of%2520the%2520generated%2520functions%252C%2520offering%2520a%2520practical%250Aand%2520flexible%2520solution%2520in%2520scenarios%2520where%2520existing%2520deep%2520generative%2520methods%2520are%250Anot%2520applicable.%2520Through%2520extensive%2520simulation%2520studies%252C%2520we%2520demonstrate%2520the%250Aadvantages%2520of%2520SFM%2520in%2520terms%2520of%2520both%2520synthetic%2520data%2520quality%2520and%2520computational%250Aefficiency.%2520We%2520then%2520apply%2520SFM%2520to%2520generate%2520clinical%2520trajectory%2520data%2520from%2520the%250AMIMIC-IV%2520patient%2520electronic%2520health%2520records%2520%2528EHR%2529%2520longitudinal%2520database.%2520Our%250Aanalysis%2520showcases%2520the%2520ability%2520of%2520SFM%2520to%2520produce%2520high-quality%2520surrogate%2520data%250Afor%2520downstream%2520statistical%2520tasks%252C%2520highlighting%2520its%2520potential%2520to%2520boost%2520the%250Autility%2520of%2520EHR%2520data%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Flow%20Matching&entry.906535625=Jianbin%20Tan%20and%20Anru%20R.%20Zhang&entry.1292438233=%20%20Functional%20data%2C%20i.e.%2C%20smooth%20random%20functions%20observed%20over%20a%20continuous%0Adomain%2C%20are%20increasingly%20available%20in%20areas%20such%20as%20biomedical%20research%2C%20health%0Ainformatics%2C%20and%20epidemiology.%20However%2C%20effective%20statistical%20analysis%20for%0Afunctional%20data%20is%20often%20hindered%20by%20challenges%20such%20as%20privacy%20constraints%2C%0Asparse%20and%20irregular%20sampling%2C%20infinite%20dimensionality%2C%20and%20non-Gaussian%0Astructures.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20framework%20named%0ASmooth%20Flow%20Matching%20%28SFM%29%2C%20tailored%20for%20generative%20modeling%20of%20functional%20data%0Ato%20enable%20statistical%20analysis%20without%20exposing%20sensitive%20real%20data.%20Built%20upon%0Aflow-matching%20ideas%2C%20SFM%20constructs%20a%20semiparametric%20copula%20flow%20to%20generate%0Ainfinite-dimensional%20functional%20data%2C%20free%20from%20Gaussianity%20or%20low-rank%0Aassumptions.%20It%20is%20computationally%20efficient%2C%20handles%20irregular%20observations%2C%0Aand%20guarantees%20the%20smoothness%20of%20the%20generated%20functions%2C%20offering%20a%20practical%0Aand%20flexible%20solution%20in%20scenarios%20where%20existing%20deep%20generative%20methods%20are%0Anot%20applicable.%20Through%20extensive%20simulation%20studies%2C%20we%20demonstrate%20the%0Aadvantages%20of%20SFM%20in%20terms%20of%20both%20synthetic%20data%20quality%20and%20computational%0Aefficiency.%20We%20then%20apply%20SFM%20to%20generate%20clinical%20trajectory%20data%20from%20the%0AMIMIC-IV%20patient%20electronic%20health%20records%20%28EHR%29%20longitudinal%20database.%20Our%0Aanalysis%20showcases%20the%20ability%20of%20SFM%20to%20produce%20high-quality%20surrogate%20data%0Afor%20downstream%20statistical%20tasks%2C%20highlighting%20its%20potential%20to%20boost%20the%0Autility%20of%20EHR%20data%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13831v2&entry.124074799=Read"},
{"title": "On the limitation of evaluating machine unlearning using only a single\n  training seed", "author": "Jamie Lanyon and Axel Finke and Petros Andreou and Georgina Cosma", "abstract": "  Machine unlearning (MU) aims to remove the influence of certain data points\nfrom a trained model without costly retraining. Most practical MU algorithms\nare only approximate and their performance can only be assessed empirically.\nCare must therefore be taken to make empirical comparisons as representative as\npossible. A common practice is to run the MU algorithm multiple times\nindependently starting from the same trained model. In this work, we\ndemonstrate that this practice can give highly non-representative results\nbecause -- even for the same architecture and same dataset -- some MU methods\ncan be highly sensitive to the choice of random number seed used for model\ntraining. We therefore recommend that empirical comparisons of MU algorithms\nshould also reflect the variability across different model training seeds.\n", "link": "http://arxiv.org/abs/2510.26714v2", "date": "2025-10-31", "relevancy": 1.3217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4859}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4319}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed&body=Title%3A%20On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed%0AAuthor%3A%20Jamie%20Lanyon%20and%20Axel%20Finke%20and%20Petros%20Andreou%20and%20Georgina%20Cosma%0AAbstract%3A%20%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20certain%20data%20points%0Afrom%20a%20trained%20model%20without%20costly%20retraining.%20Most%20practical%20MU%20algorithms%0Aare%20only%20approximate%20and%20their%20performance%20can%20only%20be%20assessed%20empirically.%0ACare%20must%20therefore%20be%20taken%20to%20make%20empirical%20comparisons%20as%20representative%20as%0Apossible.%20A%20common%20practice%20is%20to%20run%20the%20MU%20algorithm%20multiple%20times%0Aindependently%20starting%20from%20the%20same%20trained%20model.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20this%20practice%20can%20give%20highly%20non-representative%20results%0Abecause%20--%20even%20for%20the%20same%20architecture%20and%20same%20dataset%20--%20some%20MU%20methods%0Acan%20be%20highly%20sensitive%20to%20the%20choice%20of%20random%20number%20seed%20used%20for%20model%0Atraining.%20We%20therefore%20recommend%20that%20empirical%20comparisons%20of%20MU%20algorithms%0Ashould%20also%20reflect%20the%20variability%20across%20different%20model%20training%20seeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.26714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520limitation%2520of%2520evaluating%2520machine%2520unlearning%2520using%2520only%2520a%2520single%250A%2520%2520training%2520seed%26entry.906535625%3DJamie%2520Lanyon%2520and%2520Axel%2520Finke%2520and%2520Petros%2520Andreou%2520and%2520Georgina%2520Cosma%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520%2528MU%2529%2520aims%2520to%2520remove%2520the%2520influence%2520of%2520certain%2520data%2520points%250Afrom%2520a%2520trained%2520model%2520without%2520costly%2520retraining.%2520Most%2520practical%2520MU%2520algorithms%250Aare%2520only%2520approximate%2520and%2520their%2520performance%2520can%2520only%2520be%2520assessed%2520empirically.%250ACare%2520must%2520therefore%2520be%2520taken%2520to%2520make%2520empirical%2520comparisons%2520as%2520representative%2520as%250Apossible.%2520A%2520common%2520practice%2520is%2520to%2520run%2520the%2520MU%2520algorithm%2520multiple%2520times%250Aindependently%2520starting%2520from%2520the%2520same%2520trained%2520model.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520this%2520practice%2520can%2520give%2520highly%2520non-representative%2520results%250Abecause%2520--%2520even%2520for%2520the%2520same%2520architecture%2520and%2520same%2520dataset%2520--%2520some%2520MU%2520methods%250Acan%2520be%2520highly%2520sensitive%2520to%2520the%2520choice%2520of%2520random%2520number%2520seed%2520used%2520for%2520model%250Atraining.%2520We%2520therefore%2520recommend%2520that%2520empirical%2520comparisons%2520of%2520MU%2520algorithms%250Ashould%2520also%2520reflect%2520the%2520variability%2520across%2520different%2520model%2520training%2520seeds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.26714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20limitation%20of%20evaluating%20machine%20unlearning%20using%20only%20a%20single%0A%20%20training%20seed&entry.906535625=Jamie%20Lanyon%20and%20Axel%20Finke%20and%20Petros%20Andreou%20and%20Georgina%20Cosma&entry.1292438233=%20%20Machine%20unlearning%20%28MU%29%20aims%20to%20remove%20the%20influence%20of%20certain%20data%20points%0Afrom%20a%20trained%20model%20without%20costly%20retraining.%20Most%20practical%20MU%20algorithms%0Aare%20only%20approximate%20and%20their%20performance%20can%20only%20be%20assessed%20empirically.%0ACare%20must%20therefore%20be%20taken%20to%20make%20empirical%20comparisons%20as%20representative%20as%0Apossible.%20A%20common%20practice%20is%20to%20run%20the%20MU%20algorithm%20multiple%20times%0Aindependently%20starting%20from%20the%20same%20trained%20model.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20this%20practice%20can%20give%20highly%20non-representative%20results%0Abecause%20--%20even%20for%20the%20same%20architecture%20and%20same%20dataset%20--%20some%20MU%20methods%0Acan%20be%20highly%20sensitive%20to%20the%20choice%20of%20random%20number%20seed%20used%20for%20model%0Atraining.%20We%20therefore%20recommend%20that%20empirical%20comparisons%20of%20MU%20algorithms%0Ashould%20also%20reflect%20the%20variability%20across%20different%20model%20training%20seeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.26714v2&entry.124074799=Read"},
{"title": "A tutorial on discovering and quantifying the effect of latent causal\n  sources of multimodal EHR data", "author": "Marco Barbero-Mota and Eric V. Strobl and John M. Still and William W. Stead and Thomas A. Lasko", "abstract": "  We provide an accessible description of a peer-reviewed generalizable causal\nmachine learning pipeline to (i) discover latent causal sources of large-scale\nelectronic health records observations, and (ii) quantify the source causal\neffects on clinical outcomes. We illustrate how imperfect multimodal clinical\ndata can be processed, decomposed into probabilistic independent latent\nsources, and used to train taskspecific causal models from which individual\ncausal effects can be estimated. We summarize the findings of the two\nreal-world applications of the approach to date as a demonstration of its\nversatility and utility for medical discovery at scale.\n", "link": "http://arxiv.org/abs/2510.16026v2", "date": "2025-10-31", "relevancy": 0.9724, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4886}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20tutorial%20on%20discovering%20and%20quantifying%20the%20effect%20of%20latent%20causal%0A%20%20sources%20of%20multimodal%20EHR%20data&body=Title%3A%20A%20tutorial%20on%20discovering%20and%20quantifying%20the%20effect%20of%20latent%20causal%0A%20%20sources%20of%20multimodal%20EHR%20data%0AAuthor%3A%20Marco%20Barbero-Mota%20and%20Eric%20V.%20Strobl%20and%20John%20M.%20Still%20and%20William%20W.%20Stead%20and%20Thomas%20A.%20Lasko%0AAbstract%3A%20%20%20We%20provide%20an%20accessible%20description%20of%20a%20peer-reviewed%20generalizable%20causal%0Amachine%20learning%20pipeline%20to%20%28i%29%20discover%20latent%20causal%20sources%20of%20large-scale%0Aelectronic%20health%20records%20observations%2C%20and%20%28ii%29%20quantify%20the%20source%20causal%0Aeffects%20on%20clinical%20outcomes.%20We%20illustrate%20how%20imperfect%20multimodal%20clinical%0Adata%20can%20be%20processed%2C%20decomposed%20into%20probabilistic%20independent%20latent%0Asources%2C%20and%20used%20to%20train%20taskspecific%20causal%20models%20from%20which%20individual%0Acausal%20effects%20can%20be%20estimated.%20We%20summarize%20the%20findings%20of%20the%20two%0Areal-world%20applications%20of%20the%20approach%20to%20date%20as%20a%20demonstration%20of%20its%0Aversatility%20and%20utility%20for%20medical%20discovery%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520tutorial%2520on%2520discovering%2520and%2520quantifying%2520the%2520effect%2520of%2520latent%2520causal%250A%2520%2520sources%2520of%2520multimodal%2520EHR%2520data%26entry.906535625%3DMarco%2520Barbero-Mota%2520and%2520Eric%2520V.%2520Strobl%2520and%2520John%2520M.%2520Still%2520and%2520William%2520W.%2520Stead%2520and%2520Thomas%2520A.%2520Lasko%26entry.1292438233%3D%2520%2520We%2520provide%2520an%2520accessible%2520description%2520of%2520a%2520peer-reviewed%2520generalizable%2520causal%250Amachine%2520learning%2520pipeline%2520to%2520%2528i%2529%2520discover%2520latent%2520causal%2520sources%2520of%2520large-scale%250Aelectronic%2520health%2520records%2520observations%252C%2520and%2520%2528ii%2529%2520quantify%2520the%2520source%2520causal%250Aeffects%2520on%2520clinical%2520outcomes.%2520We%2520illustrate%2520how%2520imperfect%2520multimodal%2520clinical%250Adata%2520can%2520be%2520processed%252C%2520decomposed%2520into%2520probabilistic%2520independent%2520latent%250Asources%252C%2520and%2520used%2520to%2520train%2520taskspecific%2520causal%2520models%2520from%2520which%2520individual%250Acausal%2520effects%2520can%2520be%2520estimated.%2520We%2520summarize%2520the%2520findings%2520of%2520the%2520two%250Areal-world%2520applications%2520of%2520the%2520approach%2520to%2520date%2520as%2520a%2520demonstration%2520of%2520its%250Aversatility%2520and%2520utility%2520for%2520medical%2520discovery%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20tutorial%20on%20discovering%20and%20quantifying%20the%20effect%20of%20latent%20causal%0A%20%20sources%20of%20multimodal%20EHR%20data&entry.906535625=Marco%20Barbero-Mota%20and%20Eric%20V.%20Strobl%20and%20John%20M.%20Still%20and%20William%20W.%20Stead%20and%20Thomas%20A.%20Lasko&entry.1292438233=%20%20We%20provide%20an%20accessible%20description%20of%20a%20peer-reviewed%20generalizable%20causal%0Amachine%20learning%20pipeline%20to%20%28i%29%20discover%20latent%20causal%20sources%20of%20large-scale%0Aelectronic%20health%20records%20observations%2C%20and%20%28ii%29%20quantify%20the%20source%20causal%0Aeffects%20on%20clinical%20outcomes.%20We%20illustrate%20how%20imperfect%20multimodal%20clinical%0Adata%20can%20be%20processed%2C%20decomposed%20into%20probabilistic%20independent%20latent%0Asources%2C%20and%20used%20to%20train%20taskspecific%20causal%20models%20from%20which%20individual%0Acausal%20effects%20can%20be%20estimated.%20We%20summarize%20the%20findings%20of%20the%20two%0Areal-world%20applications%20of%20the%20approach%20to%20date%20as%20a%20demonstration%20of%20its%0Aversatility%20and%20utility%20for%20medical%20discovery%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16026v2&entry.124074799=Read"},
{"title": "A Survey of AI Scientists", "author": "Guiyao Tie and Pan Zhou and Lichao Sun", "abstract": "  Artificial intelligence is undergoing a profound transition from a\ncomputational instrument to an autonomous originator of scientific knowledge.\nThis emerging paradigm, the AI scientist, is architected to emulate the\ncomplete scientific workflow-from initial hypothesis generation to the final\nsynthesis of publishable findings-thereby promising to fundamentally reshape\nthe pace and scale of discovery. However, the rapid and unstructured\nproliferation of these systems has created a fragmented research landscape,\nobscuring overarching methodological principles and developmental trends. This\nsurvey provides a systematic and comprehensive synthesis of this domain by\nintroducing a unified, six-stage methodological framework that deconstructs the\nend-to-end scientific process into: Literature Review, Idea Generation,\nExperimental Preparation, Experimental Execution, Scientific Writing, and Paper\nGeneration. Through this analytical lens, we chart the field's evolution from\nearly Foundational Modules (2022-2023) to integrated Closed-Loop Systems\n(2024), and finally to the current frontier of Scalability, Impact, and\nHuman-AI Collaboration (2025-present). By rigorously synthesizing these\ndevelopments, this survey not only clarifies the current state of autonomous\nscience but also provides a critical roadmap for overcoming remaining\nchallenges in robustness and governance, ultimately guiding the next generation\nof systems toward becoming trustworthy and indispensable partners in human\nscientific inquiry.\n", "link": "http://arxiv.org/abs/2510.23045v3", "date": "2025-10-31", "relevancy": 1.314, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4342}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20AI%20Scientists&body=Title%3A%20A%20Survey%20of%20AI%20Scientists%0AAuthor%3A%20Guiyao%20Tie%20and%20Pan%20Zhou%20and%20Lichao%20Sun%0AAbstract%3A%20%20%20Artificial%20intelligence%20is%20undergoing%20a%20profound%20transition%20from%20a%0Acomputational%20instrument%20to%20an%20autonomous%20originator%20of%20scientific%20knowledge.%0AThis%20emerging%20paradigm%2C%20the%20AI%20scientist%2C%20is%20architected%20to%20emulate%20the%0Acomplete%20scientific%20workflow-from%20initial%20hypothesis%20generation%20to%20the%20final%0Asynthesis%20of%20publishable%20findings-thereby%20promising%20to%20fundamentally%20reshape%0Athe%20pace%20and%20scale%20of%20discovery.%20However%2C%20the%20rapid%20and%20unstructured%0Aproliferation%20of%20these%20systems%20has%20created%20a%20fragmented%20research%20landscape%2C%0Aobscuring%20overarching%20methodological%20principles%20and%20developmental%20trends.%20This%0Asurvey%20provides%20a%20systematic%20and%20comprehensive%20synthesis%20of%20this%20domain%20by%0Aintroducing%20a%20unified%2C%20six-stage%20methodological%20framework%20that%20deconstructs%20the%0Aend-to-end%20scientific%20process%20into%3A%20Literature%20Review%2C%20Idea%20Generation%2C%0AExperimental%20Preparation%2C%20Experimental%20Execution%2C%20Scientific%20Writing%2C%20and%20Paper%0AGeneration.%20Through%20this%20analytical%20lens%2C%20we%20chart%20the%20field%27s%20evolution%20from%0Aearly%20Foundational%20Modules%20%282022-2023%29%20to%20integrated%20Closed-Loop%20Systems%0A%282024%29%2C%20and%20finally%20to%20the%20current%20frontier%20of%20Scalability%2C%20Impact%2C%20and%0AHuman-AI%20Collaboration%20%282025-present%29.%20By%20rigorously%20synthesizing%20these%0Adevelopments%2C%20this%20survey%20not%20only%20clarifies%20the%20current%20state%20of%20autonomous%0Ascience%20but%20also%20provides%20a%20critical%20roadmap%20for%20overcoming%20remaining%0Achallenges%20in%20robustness%20and%20governance%2C%20ultimately%20guiding%20the%20next%20generation%0Aof%20systems%20toward%20becoming%20trustworthy%20and%20indispensable%20partners%20in%20human%0Ascientific%20inquiry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.23045v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520AI%2520Scientists%26entry.906535625%3DGuiyao%2520Tie%2520and%2520Pan%2520Zhou%2520and%2520Lichao%2520Sun%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520is%2520undergoing%2520a%2520profound%2520transition%2520from%2520a%250Acomputational%2520instrument%2520to%2520an%2520autonomous%2520originator%2520of%2520scientific%2520knowledge.%250AThis%2520emerging%2520paradigm%252C%2520the%2520AI%2520scientist%252C%2520is%2520architected%2520to%2520emulate%2520the%250Acomplete%2520scientific%2520workflow-from%2520initial%2520hypothesis%2520generation%2520to%2520the%2520final%250Asynthesis%2520of%2520publishable%2520findings-thereby%2520promising%2520to%2520fundamentally%2520reshape%250Athe%2520pace%2520and%2520scale%2520of%2520discovery.%2520However%252C%2520the%2520rapid%2520and%2520unstructured%250Aproliferation%2520of%2520these%2520systems%2520has%2520created%2520a%2520fragmented%2520research%2520landscape%252C%250Aobscuring%2520overarching%2520methodological%2520principles%2520and%2520developmental%2520trends.%2520This%250Asurvey%2520provides%2520a%2520systematic%2520and%2520comprehensive%2520synthesis%2520of%2520this%2520domain%2520by%250Aintroducing%2520a%2520unified%252C%2520six-stage%2520methodological%2520framework%2520that%2520deconstructs%2520the%250Aend-to-end%2520scientific%2520process%2520into%253A%2520Literature%2520Review%252C%2520Idea%2520Generation%252C%250AExperimental%2520Preparation%252C%2520Experimental%2520Execution%252C%2520Scientific%2520Writing%252C%2520and%2520Paper%250AGeneration.%2520Through%2520this%2520analytical%2520lens%252C%2520we%2520chart%2520the%2520field%2527s%2520evolution%2520from%250Aearly%2520Foundational%2520Modules%2520%25282022-2023%2529%2520to%2520integrated%2520Closed-Loop%2520Systems%250A%25282024%2529%252C%2520and%2520finally%2520to%2520the%2520current%2520frontier%2520of%2520Scalability%252C%2520Impact%252C%2520and%250AHuman-AI%2520Collaboration%2520%25282025-present%2529.%2520By%2520rigorously%2520synthesizing%2520these%250Adevelopments%252C%2520this%2520survey%2520not%2520only%2520clarifies%2520the%2520current%2520state%2520of%2520autonomous%250Ascience%2520but%2520also%2520provides%2520a%2520critical%2520roadmap%2520for%2520overcoming%2520remaining%250Achallenges%2520in%2520robustness%2520and%2520governance%252C%2520ultimately%2520guiding%2520the%2520next%2520generation%250Aof%2520systems%2520toward%2520becoming%2520trustworthy%2520and%2520indispensable%2520partners%2520in%2520human%250Ascientific%2520inquiry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.23045v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20AI%20Scientists&entry.906535625=Guiyao%20Tie%20and%20Pan%20Zhou%20and%20Lichao%20Sun&entry.1292438233=%20%20Artificial%20intelligence%20is%20undergoing%20a%20profound%20transition%20from%20a%0Acomputational%20instrument%20to%20an%20autonomous%20originator%20of%20scientific%20knowledge.%0AThis%20emerging%20paradigm%2C%20the%20AI%20scientist%2C%20is%20architected%20to%20emulate%20the%0Acomplete%20scientific%20workflow-from%20initial%20hypothesis%20generation%20to%20the%20final%0Asynthesis%20of%20publishable%20findings-thereby%20promising%20to%20fundamentally%20reshape%0Athe%20pace%20and%20scale%20of%20discovery.%20However%2C%20the%20rapid%20and%20unstructured%0Aproliferation%20of%20these%20systems%20has%20created%20a%20fragmented%20research%20landscape%2C%0Aobscuring%20overarching%20methodological%20principles%20and%20developmental%20trends.%20This%0Asurvey%20provides%20a%20systematic%20and%20comprehensive%20synthesis%20of%20this%20domain%20by%0Aintroducing%20a%20unified%2C%20six-stage%20methodological%20framework%20that%20deconstructs%20the%0Aend-to-end%20scientific%20process%20into%3A%20Literature%20Review%2C%20Idea%20Generation%2C%0AExperimental%20Preparation%2C%20Experimental%20Execution%2C%20Scientific%20Writing%2C%20and%20Paper%0AGeneration.%20Through%20this%20analytical%20lens%2C%20we%20chart%20the%20field%27s%20evolution%20from%0Aearly%20Foundational%20Modules%20%282022-2023%29%20to%20integrated%20Closed-Loop%20Systems%0A%282024%29%2C%20and%20finally%20to%20the%20current%20frontier%20of%20Scalability%2C%20Impact%2C%20and%0AHuman-AI%20Collaboration%20%282025-present%29.%20By%20rigorously%20synthesizing%20these%0Adevelopments%2C%20this%20survey%20not%20only%20clarifies%20the%20current%20state%20of%20autonomous%0Ascience%20but%20also%20provides%20a%20critical%20roadmap%20for%20overcoming%20remaining%0Achallenges%20in%20robustness%20and%20governance%2C%20ultimately%20guiding%20the%20next%20generation%0Aof%20systems%20toward%20becoming%20trustworthy%20and%20indispensable%20partners%20in%20human%0Ascientific%20inquiry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.23045v3&entry.124074799=Read"},
{"title": "AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for\n  Proprietary Data Challenges in Financial Question Answering", "author": "Mohammad Zahangir Alam and Khandoker Ashik Uz Zaman and Mahdi H. Miraz", "abstract": "  Retrieval-Augmented Generation (RAG) shows significant promise in\nknowledge-intensive tasks by improving domain specificity, enhancing temporal\nrelevance, and reducing hallucinations. However, applying RAG to finance\nencounters critical challenges: restricted access to proprietary datasets,\nlimited retrieval accuracy, regulatory constraints, and sensitive data\ninterpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored\nfor Financial Question Answering (FQA), leveraging task-aware prompt\nengineering to address these challenges. The framework uses a hybrid retrieval\nstrategy integrating both open-source and proprietary financial data while\nmaintaining strict security protocols and regulatory compliance. A dynamic\nprompt framework adapts in real time to query complexity, improving precision\nand contextual relevance. To systematically address diverse financial queries,\nwe propose a four-tier task classification: explicit factual, implicit factual,\ninterpretable rationale, and hidden rationale involving implicit causal\nreasoning. For each category, we identify key challenges, datasets, and\noptimization techniques within the retrieval and generation process. The\nframework incorporates multi-layered security mechanisms including differential\nprivacy, data anonymization, and role-based access controls to protect\nsensitive financial information. Additionally, AstuteRAG-FQA implements\nreal-time compliance monitoring through automated regulatory validation systems\nthat verify responses against industry standards and legal obligations. We\nevaluate three data integration techniques - contextual embedding, small model\naugmentation, and targeted fine-tuning - analyzing their efficiency and\nfeasibility across varied financial environments.\n", "link": "http://arxiv.org/abs/2510.27537v1", "date": "2025-10-31", "relevancy": 1.3476, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4669}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstuteRAG-FQA%3A%20Task-Aware%20Retrieval-Augmented%20Generation%20Framework%20for%0A%20%20Proprietary%20Data%20Challenges%20in%20Financial%20Question%20Answering&body=Title%3A%20AstuteRAG-FQA%3A%20Task-Aware%20Retrieval-Augmented%20Generation%20Framework%20for%0A%20%20Proprietary%20Data%20Challenges%20in%20Financial%20Question%20Answering%0AAuthor%3A%20Mohammad%20Zahangir%20Alam%20and%20Khandoker%20Ashik%20Uz%20Zaman%20and%20Mahdi%20H.%20Miraz%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20shows%20significant%20promise%20in%0Aknowledge-intensive%20tasks%20by%20improving%20domain%20specificity%2C%20enhancing%20temporal%0Arelevance%2C%20and%20reducing%20hallucinations.%20However%2C%20applying%20RAG%20to%20finance%0Aencounters%20critical%20challenges%3A%20restricted%20access%20to%20proprietary%20datasets%2C%0Alimited%20retrieval%20accuracy%2C%20regulatory%20constraints%2C%20and%20sensitive%20data%0Ainterpretation.%20We%20introduce%20AstuteRAG-FQA%2C%20an%20adaptive%20RAG%20framework%20tailored%0Afor%20Financial%20Question%20Answering%20%28FQA%29%2C%20leveraging%20task-aware%20prompt%0Aengineering%20to%20address%20these%20challenges.%20The%20framework%20uses%20a%20hybrid%20retrieval%0Astrategy%20integrating%20both%20open-source%20and%20proprietary%20financial%20data%20while%0Amaintaining%20strict%20security%20protocols%20and%20regulatory%20compliance.%20A%20dynamic%0Aprompt%20framework%20adapts%20in%20real%20time%20to%20query%20complexity%2C%20improving%20precision%0Aand%20contextual%20relevance.%20To%20systematically%20address%20diverse%20financial%20queries%2C%0Awe%20propose%20a%20four-tier%20task%20classification%3A%20explicit%20factual%2C%20implicit%20factual%2C%0Ainterpretable%20rationale%2C%20and%20hidden%20rationale%20involving%20implicit%20causal%0Areasoning.%20For%20each%20category%2C%20we%20identify%20key%20challenges%2C%20datasets%2C%20and%0Aoptimization%20techniques%20within%20the%20retrieval%20and%20generation%20process.%20The%0Aframework%20incorporates%20multi-layered%20security%20mechanisms%20including%20differential%0Aprivacy%2C%20data%20anonymization%2C%20and%20role-based%20access%20controls%20to%20protect%0Asensitive%20financial%20information.%20Additionally%2C%20AstuteRAG-FQA%20implements%0Areal-time%20compliance%20monitoring%20through%20automated%20regulatory%20validation%20systems%0Athat%20verify%20responses%20against%20industry%20standards%20and%20legal%20obligations.%20We%0Aevaluate%20three%20data%20integration%20techniques%20-%20contextual%20embedding%2C%20small%20model%0Aaugmentation%2C%20and%20targeted%20fine-tuning%20-%20analyzing%20their%20efficiency%20and%0Afeasibility%20across%20varied%20financial%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstuteRAG-FQA%253A%2520Task-Aware%2520Retrieval-Augmented%2520Generation%2520Framework%2520for%250A%2520%2520Proprietary%2520Data%2520Challenges%2520in%2520Financial%2520Question%2520Answering%26entry.906535625%3DMohammad%2520Zahangir%2520Alam%2520and%2520Khandoker%2520Ashik%2520Uz%2520Zaman%2520and%2520Mahdi%2520H.%2520Miraz%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520shows%2520significant%2520promise%2520in%250Aknowledge-intensive%2520tasks%2520by%2520improving%2520domain%2520specificity%252C%2520enhancing%2520temporal%250Arelevance%252C%2520and%2520reducing%2520hallucinations.%2520However%252C%2520applying%2520RAG%2520to%2520finance%250Aencounters%2520critical%2520challenges%253A%2520restricted%2520access%2520to%2520proprietary%2520datasets%252C%250Alimited%2520retrieval%2520accuracy%252C%2520regulatory%2520constraints%252C%2520and%2520sensitive%2520data%250Ainterpretation.%2520We%2520introduce%2520AstuteRAG-FQA%252C%2520an%2520adaptive%2520RAG%2520framework%2520tailored%250Afor%2520Financial%2520Question%2520Answering%2520%2528FQA%2529%252C%2520leveraging%2520task-aware%2520prompt%250Aengineering%2520to%2520address%2520these%2520challenges.%2520The%2520framework%2520uses%2520a%2520hybrid%2520retrieval%250Astrategy%2520integrating%2520both%2520open-source%2520and%2520proprietary%2520financial%2520data%2520while%250Amaintaining%2520strict%2520security%2520protocols%2520and%2520regulatory%2520compliance.%2520A%2520dynamic%250Aprompt%2520framework%2520adapts%2520in%2520real%2520time%2520to%2520query%2520complexity%252C%2520improving%2520precision%250Aand%2520contextual%2520relevance.%2520To%2520systematically%2520address%2520diverse%2520financial%2520queries%252C%250Awe%2520propose%2520a%2520four-tier%2520task%2520classification%253A%2520explicit%2520factual%252C%2520implicit%2520factual%252C%250Ainterpretable%2520rationale%252C%2520and%2520hidden%2520rationale%2520involving%2520implicit%2520causal%250Areasoning.%2520For%2520each%2520category%252C%2520we%2520identify%2520key%2520challenges%252C%2520datasets%252C%2520and%250Aoptimization%2520techniques%2520within%2520the%2520retrieval%2520and%2520generation%2520process.%2520The%250Aframework%2520incorporates%2520multi-layered%2520security%2520mechanisms%2520including%2520differential%250Aprivacy%252C%2520data%2520anonymization%252C%2520and%2520role-based%2520access%2520controls%2520to%2520protect%250Asensitive%2520financial%2520information.%2520Additionally%252C%2520AstuteRAG-FQA%2520implements%250Areal-time%2520compliance%2520monitoring%2520through%2520automated%2520regulatory%2520validation%2520systems%250Athat%2520verify%2520responses%2520against%2520industry%2520standards%2520and%2520legal%2520obligations.%2520We%250Aevaluate%2520three%2520data%2520integration%2520techniques%2520-%2520contextual%2520embedding%252C%2520small%2520model%250Aaugmentation%252C%2520and%2520targeted%2520fine-tuning%2520-%2520analyzing%2520their%2520efficiency%2520and%250Afeasibility%2520across%2520varied%2520financial%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstuteRAG-FQA%3A%20Task-Aware%20Retrieval-Augmented%20Generation%20Framework%20for%0A%20%20Proprietary%20Data%20Challenges%20in%20Financial%20Question%20Answering&entry.906535625=Mohammad%20Zahangir%20Alam%20and%20Khandoker%20Ashik%20Uz%20Zaman%20and%20Mahdi%20H.%20Miraz&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20shows%20significant%20promise%20in%0Aknowledge-intensive%20tasks%20by%20improving%20domain%20specificity%2C%20enhancing%20temporal%0Arelevance%2C%20and%20reducing%20hallucinations.%20However%2C%20applying%20RAG%20to%20finance%0Aencounters%20critical%20challenges%3A%20restricted%20access%20to%20proprietary%20datasets%2C%0Alimited%20retrieval%20accuracy%2C%20regulatory%20constraints%2C%20and%20sensitive%20data%0Ainterpretation.%20We%20introduce%20AstuteRAG-FQA%2C%20an%20adaptive%20RAG%20framework%20tailored%0Afor%20Financial%20Question%20Answering%20%28FQA%29%2C%20leveraging%20task-aware%20prompt%0Aengineering%20to%20address%20these%20challenges.%20The%20framework%20uses%20a%20hybrid%20retrieval%0Astrategy%20integrating%20both%20open-source%20and%20proprietary%20financial%20data%20while%0Amaintaining%20strict%20security%20protocols%20and%20regulatory%20compliance.%20A%20dynamic%0Aprompt%20framework%20adapts%20in%20real%20time%20to%20query%20complexity%2C%20improving%20precision%0Aand%20contextual%20relevance.%20To%20systematically%20address%20diverse%20financial%20queries%2C%0Awe%20propose%20a%20four-tier%20task%20classification%3A%20explicit%20factual%2C%20implicit%20factual%2C%0Ainterpretable%20rationale%2C%20and%20hidden%20rationale%20involving%20implicit%20causal%0Areasoning.%20For%20each%20category%2C%20we%20identify%20key%20challenges%2C%20datasets%2C%20and%0Aoptimization%20techniques%20within%20the%20retrieval%20and%20generation%20process.%20The%0Aframework%20incorporates%20multi-layered%20security%20mechanisms%20including%20differential%0Aprivacy%2C%20data%20anonymization%2C%20and%20role-based%20access%20controls%20to%20protect%0Asensitive%20financial%20information.%20Additionally%2C%20AstuteRAG-FQA%20implements%0Areal-time%20compliance%20monitoring%20through%20automated%20regulatory%20validation%20systems%0Athat%20verify%20responses%20against%20industry%20standards%20and%20legal%20obligations.%20We%0Aevaluate%20three%20data%20integration%20techniques%20-%20contextual%20embedding%2C%20small%20model%0Aaugmentation%2C%20and%20targeted%20fine-tuning%20-%20analyzing%20their%20efficiency%20and%0Afeasibility%20across%20varied%20financial%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27537v1&entry.124074799=Read"},
{"title": "Dark-Field X-Ray Imaging Significantly Improves Deep-Learning based\n  Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models", "author": "Joyoni Dey and Hunter C. Meyer and Murtuza S. Taqi", "abstract": "  Low-dose computed tomography (LDCT) is the current standard for lung cancer\nscreening, yet its adoption and accessibility remain limited. Many regions lack\nLDCT infrastructure, and even among those screened, early-stage cancer\ndetection often yield false positives, as shown in the National Lung Screening\nTrial (NLST) with a sensitivity of 93.8 percent and a false-positive rate of\n26.6 percent. We aim to investigate whether X-ray dark-field imaging (DFI)\nradiograph, a technique sensitive to small-angle scatter from alveolar\nmicrostructure and less susceptible to organ shadowing, can significantly\nimprove early-stage lung tumor detection when coupled with deep-learning\nsegmentation. Using paired attenuation (ATTN) and DFI radiograph images of\neuthanized mouse lungs, we generated realistic synthetic tumors with irregular\nboundaries and intensity profiles consistent with physical lung contrast. A\nU-Net segmentation network was trained on small patches using either ATTN, DFI,\nor a combination of ATTN and DFI channels. Results show that the DFI-only model\nachieved a true-positive detection rate of 83.7 percent, compared with 51\npercent for ATTN-only, while maintaining comparable specificity (90.5 versus\n92.9 percent). The combined ATTN and DFI input achieved 79.6 percent\nsensitivity and 97.6 percent specificity. In conclusion, DFI substantially\nimproves early-tumor detectability in comparison to standard attenuation\nradiography and shows potential as an accessible, low-cost, low-dose\nalternative for pre-clinical or limited-resource screening where LDCT is\nunavailable.\n", "link": "http://arxiv.org/abs/2510.27679v1", "date": "2025-10-31", "relevancy": 1.4282, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5091}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4787}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dark-Field%20X-Ray%20Imaging%20Significantly%20Improves%20Deep-Learning%20based%0A%20%20Detection%20of%20Synthetic%20Early-Stage%20Lung%20Tumors%20in%20Preclinical%20Models&body=Title%3A%20Dark-Field%20X-Ray%20Imaging%20Significantly%20Improves%20Deep-Learning%20based%0A%20%20Detection%20of%20Synthetic%20Early-Stage%20Lung%20Tumors%20in%20Preclinical%20Models%0AAuthor%3A%20Joyoni%20Dey%20and%20Hunter%20C.%20Meyer%20and%20Murtuza%20S.%20Taqi%0AAbstract%3A%20%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20is%20the%20current%20standard%20for%20lung%20cancer%0Ascreening%2C%20yet%20its%20adoption%20and%20accessibility%20remain%20limited.%20Many%20regions%20lack%0ALDCT%20infrastructure%2C%20and%20even%20among%20those%20screened%2C%20early-stage%20cancer%0Adetection%20often%20yield%20false%20positives%2C%20as%20shown%20in%20the%20National%20Lung%20Screening%0ATrial%20%28NLST%29%20with%20a%20sensitivity%20of%2093.8%20percent%20and%20a%20false-positive%20rate%20of%0A26.6%20percent.%20We%20aim%20to%20investigate%20whether%20X-ray%20dark-field%20imaging%20%28DFI%29%0Aradiograph%2C%20a%20technique%20sensitive%20to%20small-angle%20scatter%20from%20alveolar%0Amicrostructure%20and%20less%20susceptible%20to%20organ%20shadowing%2C%20can%20significantly%0Aimprove%20early-stage%20lung%20tumor%20detection%20when%20coupled%20with%20deep-learning%0Asegmentation.%20Using%20paired%20attenuation%20%28ATTN%29%20and%20DFI%20radiograph%20images%20of%0Aeuthanized%20mouse%20lungs%2C%20we%20generated%20realistic%20synthetic%20tumors%20with%20irregular%0Aboundaries%20and%20intensity%20profiles%20consistent%20with%20physical%20lung%20contrast.%20A%0AU-Net%20segmentation%20network%20was%20trained%20on%20small%20patches%20using%20either%20ATTN%2C%20DFI%2C%0Aor%20a%20combination%20of%20ATTN%20and%20DFI%20channels.%20Results%20show%20that%20the%20DFI-only%20model%0Aachieved%20a%20true-positive%20detection%20rate%20of%2083.7%20percent%2C%20compared%20with%2051%0Apercent%20for%20ATTN-only%2C%20while%20maintaining%20comparable%20specificity%20%2890.5%20versus%0A92.9%20percent%29.%20The%20combined%20ATTN%20and%20DFI%20input%20achieved%2079.6%20percent%0Asensitivity%20and%2097.6%20percent%20specificity.%20In%20conclusion%2C%20DFI%20substantially%0Aimproves%20early-tumor%20detectability%20in%20comparison%20to%20standard%20attenuation%0Aradiography%20and%20shows%20potential%20as%20an%20accessible%2C%20low-cost%2C%20low-dose%0Aalternative%20for%20pre-clinical%20or%20limited-resource%20screening%20where%20LDCT%20is%0Aunavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDark-Field%2520X-Ray%2520Imaging%2520Significantly%2520Improves%2520Deep-Learning%2520based%250A%2520%2520Detection%2520of%2520Synthetic%2520Early-Stage%2520Lung%2520Tumors%2520in%2520Preclinical%2520Models%26entry.906535625%3DJoyoni%2520Dey%2520and%2520Hunter%2520C.%2520Meyer%2520and%2520Murtuza%2520S.%2520Taqi%26entry.1292438233%3D%2520%2520Low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520is%2520the%2520current%2520standard%2520for%2520lung%2520cancer%250Ascreening%252C%2520yet%2520its%2520adoption%2520and%2520accessibility%2520remain%2520limited.%2520Many%2520regions%2520lack%250ALDCT%2520infrastructure%252C%2520and%2520even%2520among%2520those%2520screened%252C%2520early-stage%2520cancer%250Adetection%2520often%2520yield%2520false%2520positives%252C%2520as%2520shown%2520in%2520the%2520National%2520Lung%2520Screening%250ATrial%2520%2528NLST%2529%2520with%2520a%2520sensitivity%2520of%252093.8%2520percent%2520and%2520a%2520false-positive%2520rate%2520of%250A26.6%2520percent.%2520We%2520aim%2520to%2520investigate%2520whether%2520X-ray%2520dark-field%2520imaging%2520%2528DFI%2529%250Aradiograph%252C%2520a%2520technique%2520sensitive%2520to%2520small-angle%2520scatter%2520from%2520alveolar%250Amicrostructure%2520and%2520less%2520susceptible%2520to%2520organ%2520shadowing%252C%2520can%2520significantly%250Aimprove%2520early-stage%2520lung%2520tumor%2520detection%2520when%2520coupled%2520with%2520deep-learning%250Asegmentation.%2520Using%2520paired%2520attenuation%2520%2528ATTN%2529%2520and%2520DFI%2520radiograph%2520images%2520of%250Aeuthanized%2520mouse%2520lungs%252C%2520we%2520generated%2520realistic%2520synthetic%2520tumors%2520with%2520irregular%250Aboundaries%2520and%2520intensity%2520profiles%2520consistent%2520with%2520physical%2520lung%2520contrast.%2520A%250AU-Net%2520segmentation%2520network%2520was%2520trained%2520on%2520small%2520patches%2520using%2520either%2520ATTN%252C%2520DFI%252C%250Aor%2520a%2520combination%2520of%2520ATTN%2520and%2520DFI%2520channels.%2520Results%2520show%2520that%2520the%2520DFI-only%2520model%250Aachieved%2520a%2520true-positive%2520detection%2520rate%2520of%252083.7%2520percent%252C%2520compared%2520with%252051%250Apercent%2520for%2520ATTN-only%252C%2520while%2520maintaining%2520comparable%2520specificity%2520%252890.5%2520versus%250A92.9%2520percent%2529.%2520The%2520combined%2520ATTN%2520and%2520DFI%2520input%2520achieved%252079.6%2520percent%250Asensitivity%2520and%252097.6%2520percent%2520specificity.%2520In%2520conclusion%252C%2520DFI%2520substantially%250Aimproves%2520early-tumor%2520detectability%2520in%2520comparison%2520to%2520standard%2520attenuation%250Aradiography%2520and%2520shows%2520potential%2520as%2520an%2520accessible%252C%2520low-cost%252C%2520low-dose%250Aalternative%2520for%2520pre-clinical%2520or%2520limited-resource%2520screening%2520where%2520LDCT%2520is%250Aunavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dark-Field%20X-Ray%20Imaging%20Significantly%20Improves%20Deep-Learning%20based%0A%20%20Detection%20of%20Synthetic%20Early-Stage%20Lung%20Tumors%20in%20Preclinical%20Models&entry.906535625=Joyoni%20Dey%20and%20Hunter%20C.%20Meyer%20and%20Murtuza%20S.%20Taqi&entry.1292438233=%20%20Low-dose%20computed%20tomography%20%28LDCT%29%20is%20the%20current%20standard%20for%20lung%20cancer%0Ascreening%2C%20yet%20its%20adoption%20and%20accessibility%20remain%20limited.%20Many%20regions%20lack%0ALDCT%20infrastructure%2C%20and%20even%20among%20those%20screened%2C%20early-stage%20cancer%0Adetection%20often%20yield%20false%20positives%2C%20as%20shown%20in%20the%20National%20Lung%20Screening%0ATrial%20%28NLST%29%20with%20a%20sensitivity%20of%2093.8%20percent%20and%20a%20false-positive%20rate%20of%0A26.6%20percent.%20We%20aim%20to%20investigate%20whether%20X-ray%20dark-field%20imaging%20%28DFI%29%0Aradiograph%2C%20a%20technique%20sensitive%20to%20small-angle%20scatter%20from%20alveolar%0Amicrostructure%20and%20less%20susceptible%20to%20organ%20shadowing%2C%20can%20significantly%0Aimprove%20early-stage%20lung%20tumor%20detection%20when%20coupled%20with%20deep-learning%0Asegmentation.%20Using%20paired%20attenuation%20%28ATTN%29%20and%20DFI%20radiograph%20images%20of%0Aeuthanized%20mouse%20lungs%2C%20we%20generated%20realistic%20synthetic%20tumors%20with%20irregular%0Aboundaries%20and%20intensity%20profiles%20consistent%20with%20physical%20lung%20contrast.%20A%0AU-Net%20segmentation%20network%20was%20trained%20on%20small%20patches%20using%20either%20ATTN%2C%20DFI%2C%0Aor%20a%20combination%20of%20ATTN%20and%20DFI%20channels.%20Results%20show%20that%20the%20DFI-only%20model%0Aachieved%20a%20true-positive%20detection%20rate%20of%2083.7%20percent%2C%20compared%20with%2051%0Apercent%20for%20ATTN-only%2C%20while%20maintaining%20comparable%20specificity%20%2890.5%20versus%0A92.9%20percent%29.%20The%20combined%20ATTN%20and%20DFI%20input%20achieved%2079.6%20percent%0Asensitivity%20and%2097.6%20percent%20specificity.%20In%20conclusion%2C%20DFI%20substantially%0Aimproves%20early-tumor%20detectability%20in%20comparison%20to%20standard%20attenuation%0Aradiography%20and%20shows%20potential%20as%20an%20accessible%2C%20low-cost%2C%20low-dose%0Aalternative%20for%20pre-clinical%20or%20limited-resource%20screening%20where%20LDCT%20is%0Aunavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27679v1&entry.124074799=Read"},
{"title": "Bayesian model selection and misspecification testing in imaging inverse\n  problems only from noisy and partial measurements", "author": "Tom Sprunck and Marcelo Pereyra and Tobias Liaudat", "abstract": "  Modern imaging techniques heavily rely on Bayesian statistical models to\naddress difficult image reconstruction and restoration tasks. This paper\naddresses the objective evaluation of such models in settings where ground\ntruth is unavailable, with a focus on model selection and misspecification\ndiagnosis. Existing unsupervised model evaluation methods are often unsuitable\nfor computational imaging due to their high computational cost and\nincompatibility with modern image priors defined implicitly via machine\nlearning models. We herein propose a general methodology for unsupervised model\nselection and misspecification detection in Bayesian imaging sciences, based on\na novel combination of Bayesian cross-validation and data fission, a randomized\nmeasurement splitting technique. The approach is compatible with any Bayesian\nimaging sampler, including diffusion and plug-and-play samplers. We demonstrate\nthe methodology through experiments involving various scoring rules and types\nof model misspecification, where we achieve excellent selection and detection\naccuracy with a low computational cost.\n", "link": "http://arxiv.org/abs/2510.27663v1", "date": "2025-10-31", "relevancy": 1.0606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5402}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.54}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20model%20selection%20and%20misspecification%20testing%20in%20imaging%20inverse%0A%20%20problems%20only%20from%20noisy%20and%20partial%20measurements&body=Title%3A%20Bayesian%20model%20selection%20and%20misspecification%20testing%20in%20imaging%20inverse%0A%20%20problems%20only%20from%20noisy%20and%20partial%20measurements%0AAuthor%3A%20Tom%20Sprunck%20and%20Marcelo%20Pereyra%20and%20Tobias%20Liaudat%0AAbstract%3A%20%20%20Modern%20imaging%20techniques%20heavily%20rely%20on%20Bayesian%20statistical%20models%20to%0Aaddress%20difficult%20image%20reconstruction%20and%20restoration%20tasks.%20This%20paper%0Aaddresses%20the%20objective%20evaluation%20of%20such%20models%20in%20settings%20where%20ground%0Atruth%20is%20unavailable%2C%20with%20a%20focus%20on%20model%20selection%20and%20misspecification%0Adiagnosis.%20Existing%20unsupervised%20model%20evaluation%20methods%20are%20often%20unsuitable%0Afor%20computational%20imaging%20due%20to%20their%20high%20computational%20cost%20and%0Aincompatibility%20with%20modern%20image%20priors%20defined%20implicitly%20via%20machine%0Alearning%20models.%20We%20herein%20propose%20a%20general%20methodology%20for%20unsupervised%20model%0Aselection%20and%20misspecification%20detection%20in%20Bayesian%20imaging%20sciences%2C%20based%20on%0Aa%20novel%20combination%20of%20Bayesian%20cross-validation%20and%20data%20fission%2C%20a%20randomized%0Ameasurement%20splitting%20technique.%20The%20approach%20is%20compatible%20with%20any%20Bayesian%0Aimaging%20sampler%2C%20including%20diffusion%20and%20plug-and-play%20samplers.%20We%20demonstrate%0Athe%20methodology%20through%20experiments%20involving%20various%20scoring%20rules%20and%20types%0Aof%20model%20misspecification%2C%20where%20we%20achieve%20excellent%20selection%20and%20detection%0Aaccuracy%20with%20a%20low%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.27663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520model%2520selection%2520and%2520misspecification%2520testing%2520in%2520imaging%2520inverse%250A%2520%2520problems%2520only%2520from%2520noisy%2520and%2520partial%2520measurements%26entry.906535625%3DTom%2520Sprunck%2520and%2520Marcelo%2520Pereyra%2520and%2520Tobias%2520Liaudat%26entry.1292438233%3D%2520%2520Modern%2520imaging%2520techniques%2520heavily%2520rely%2520on%2520Bayesian%2520statistical%2520models%2520to%250Aaddress%2520difficult%2520image%2520reconstruction%2520and%2520restoration%2520tasks.%2520This%2520paper%250Aaddresses%2520the%2520objective%2520evaluation%2520of%2520such%2520models%2520in%2520settings%2520where%2520ground%250Atruth%2520is%2520unavailable%252C%2520with%2520a%2520focus%2520on%2520model%2520selection%2520and%2520misspecification%250Adiagnosis.%2520Existing%2520unsupervised%2520model%2520evaluation%2520methods%2520are%2520often%2520unsuitable%250Afor%2520computational%2520imaging%2520due%2520to%2520their%2520high%2520computational%2520cost%2520and%250Aincompatibility%2520with%2520modern%2520image%2520priors%2520defined%2520implicitly%2520via%2520machine%250Alearning%2520models.%2520We%2520herein%2520propose%2520a%2520general%2520methodology%2520for%2520unsupervised%2520model%250Aselection%2520and%2520misspecification%2520detection%2520in%2520Bayesian%2520imaging%2520sciences%252C%2520based%2520on%250Aa%2520novel%2520combination%2520of%2520Bayesian%2520cross-validation%2520and%2520data%2520fission%252C%2520a%2520randomized%250Ameasurement%2520splitting%2520technique.%2520The%2520approach%2520is%2520compatible%2520with%2520any%2520Bayesian%250Aimaging%2520sampler%252C%2520including%2520diffusion%2520and%2520plug-and-play%2520samplers.%2520We%2520demonstrate%250Athe%2520methodology%2520through%2520experiments%2520involving%2520various%2520scoring%2520rules%2520and%2520types%250Aof%2520model%2520misspecification%252C%2520where%2520we%2520achieve%2520excellent%2520selection%2520and%2520detection%250Aaccuracy%2520with%2520a%2520low%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.27663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20model%20selection%20and%20misspecification%20testing%20in%20imaging%20inverse%0A%20%20problems%20only%20from%20noisy%20and%20partial%20measurements&entry.906535625=Tom%20Sprunck%20and%20Marcelo%20Pereyra%20and%20Tobias%20Liaudat&entry.1292438233=%20%20Modern%20imaging%20techniques%20heavily%20rely%20on%20Bayesian%20statistical%20models%20to%0Aaddress%20difficult%20image%20reconstruction%20and%20restoration%20tasks.%20This%20paper%0Aaddresses%20the%20objective%20evaluation%20of%20such%20models%20in%20settings%20where%20ground%0Atruth%20is%20unavailable%2C%20with%20a%20focus%20on%20model%20selection%20and%20misspecification%0Adiagnosis.%20Existing%20unsupervised%20model%20evaluation%20methods%20are%20often%20unsuitable%0Afor%20computational%20imaging%20due%20to%20their%20high%20computational%20cost%20and%0Aincompatibility%20with%20modern%20image%20priors%20defined%20implicitly%20via%20machine%0Alearning%20models.%20We%20herein%20propose%20a%20general%20methodology%20for%20unsupervised%20model%0Aselection%20and%20misspecification%20detection%20in%20Bayesian%20imaging%20sciences%2C%20based%20on%0Aa%20novel%20combination%20of%20Bayesian%20cross-validation%20and%20data%20fission%2C%20a%20randomized%0Ameasurement%20splitting%20technique.%20The%20approach%20is%20compatible%20with%20any%20Bayesian%0Aimaging%20sampler%2C%20including%20diffusion%20and%20plug-and-play%20samplers.%20We%20demonstrate%0Athe%20methodology%20through%20experiments%20involving%20various%20scoring%20rules%20and%20types%0Aof%20model%20misspecification%2C%20where%20we%20achieve%20excellent%20selection%20and%20detection%0Aaccuracy%20with%20a%20low%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.27663v1&entry.124074799=Read"},
{"title": "A Practical Introduction to Kernel Discrepancies: MMD, HSIC & KSD", "author": "Antonin Schrab", "abstract": "  This article provides a practical introduction to kernel discrepancies,\nfocusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt\nIndependence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various\nestimators for these discrepancies are presented, including the commonly-used\nV-statistics and U-statistics, as well as several forms of the more\ncomputationally-efficient incomplete U-statistics. The importance of the choice\nof kernel bandwidth is stressed, showing how it affects the behaviour of the\ndiscrepancy estimation. Adaptive estimators are introduced, which combine\nmultiple estimators with various kernels, addressing the problem of kernel\nselection.\n", "link": "http://arxiv.org/abs/2503.04820v3", "date": "2025-10-31", "relevancy": 1.2239, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4195}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4053}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practical%20Introduction%20to%20Kernel%20Discrepancies%3A%20MMD%2C%20HSIC%20%26%20KSD&body=Title%3A%20A%20Practical%20Introduction%20to%20Kernel%20Discrepancies%3A%20MMD%2C%20HSIC%20%26%20KSD%0AAuthor%3A%20Antonin%20Schrab%0AAbstract%3A%20%20%20This%20article%20provides%20a%20practical%20introduction%20to%20kernel%20discrepancies%2C%0Afocusing%20on%20the%20Maximum%20Mean%20Discrepancy%20%28MMD%29%2C%20the%20Hilbert-Schmidt%0AIndependence%20Criterion%20%28HSIC%29%2C%20and%20the%20Kernel%20Stein%20Discrepancy%20%28KSD%29.%20Various%0Aestimators%20for%20these%20discrepancies%20are%20presented%2C%20including%20the%20commonly-used%0AV-statistics%20and%20U-statistics%2C%20as%20well%20as%20several%20forms%20of%20the%20more%0Acomputationally-efficient%20incomplete%20U-statistics.%20The%20importance%20of%20the%20choice%0Aof%20kernel%20bandwidth%20is%20stressed%2C%20showing%20how%20it%20affects%20the%20behaviour%20of%20the%0Adiscrepancy%20estimation.%20Adaptive%20estimators%20are%20introduced%2C%20which%20combine%0Amultiple%20estimators%20with%20various%20kernels%2C%20addressing%20the%20problem%20of%20kernel%0Aselection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04820v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practical%2520Introduction%2520to%2520Kernel%2520Discrepancies%253A%2520MMD%252C%2520HSIC%2520%2526%2520KSD%26entry.906535625%3DAntonin%2520Schrab%26entry.1292438233%3D%2520%2520This%2520article%2520provides%2520a%2520practical%2520introduction%2520to%2520kernel%2520discrepancies%252C%250Afocusing%2520on%2520the%2520Maximum%2520Mean%2520Discrepancy%2520%2528MMD%2529%252C%2520the%2520Hilbert-Schmidt%250AIndependence%2520Criterion%2520%2528HSIC%2529%252C%2520and%2520the%2520Kernel%2520Stein%2520Discrepancy%2520%2528KSD%2529.%2520Various%250Aestimators%2520for%2520these%2520discrepancies%2520are%2520presented%252C%2520including%2520the%2520commonly-used%250AV-statistics%2520and%2520U-statistics%252C%2520as%2520well%2520as%2520several%2520forms%2520of%2520the%2520more%250Acomputationally-efficient%2520incomplete%2520U-statistics.%2520The%2520importance%2520of%2520the%2520choice%250Aof%2520kernel%2520bandwidth%2520is%2520stressed%252C%2520showing%2520how%2520it%2520affects%2520the%2520behaviour%2520of%2520the%250Adiscrepancy%2520estimation.%2520Adaptive%2520estimators%2520are%2520introduced%252C%2520which%2520combine%250Amultiple%2520estimators%2520with%2520various%2520kernels%252C%2520addressing%2520the%2520problem%2520of%2520kernel%250Aselection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04820v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practical%20Introduction%20to%20Kernel%20Discrepancies%3A%20MMD%2C%20HSIC%20%26%20KSD&entry.906535625=Antonin%20Schrab&entry.1292438233=%20%20This%20article%20provides%20a%20practical%20introduction%20to%20kernel%20discrepancies%2C%0Afocusing%20on%20the%20Maximum%20Mean%20Discrepancy%20%28MMD%29%2C%20the%20Hilbert-Schmidt%0AIndependence%20Criterion%20%28HSIC%29%2C%20and%20the%20Kernel%20Stein%20Discrepancy%20%28KSD%29.%20Various%0Aestimators%20for%20these%20discrepancies%20are%20presented%2C%20including%20the%20commonly-used%0AV-statistics%20and%20U-statistics%2C%20as%20well%20as%20several%20forms%20of%20the%20more%0Acomputationally-efficient%20incomplete%20U-statistics.%20The%20importance%20of%20the%20choice%0Aof%20kernel%20bandwidth%20is%20stressed%2C%20showing%20how%20it%20affects%20the%20behaviour%20of%20the%0Adiscrepancy%20estimation.%20Adaptive%20estimators%20are%20introduced%2C%20which%20combine%0Amultiple%20estimators%20with%20various%20kernels%2C%20addressing%20the%20problem%20of%20kernel%0Aselection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04820v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


