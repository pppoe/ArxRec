<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Driver Attention Tracking and Analysis", "author": "Dat Viet Thanh Nguyen and Anh Tran and Nam Vu and Cuong Pham and Minh Hoai", "abstract": "  We propose a novel method to estimate a driver's points-of-gaze using a pair\nof ordinary cameras mounted on the windshield and dashboard of a car. This is a\nchallenging problem due to the dynamics of traffic environments with 3D scenes\nof unknown depths. This problem is further complicated by the volatile distance\nbetween the driver and the camera system. To tackle these challenges, we\ndevelop a novel convolutional network that simultaneously analyzes the image of\nthe scene and the image of the driver's face. This network has a camera\ncalibration module that can compute an embedding vector that represents the\nspatial configuration between the driver and the camera system. This\ncalibration module improves the overall network's performance, which can be\njointly trained end to end.\n  We also address the lack of annotated data for training and evaluation by\nintroducing a large-scale driving dataset with point-of-gaze annotations. This\nis an in situ dataset of real driving sessions in an urban city, containing\nsynchronized images of the driving scene as well as the face and gaze of the\ndriver. Experiments on this dataset show that the proposed method outperforms\nvarious baseline methods, having the mean prediction error of 29.69 pixels,\nwhich is relatively small compared to the $1280{\\times}720$ resolution of the\nscene camera.\n", "link": "http://arxiv.org/abs/2404.07122v1", "date": "2024-04-10", "relevancy": 2.8067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5905}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5658}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Driver%20Attention%20Tracking%20and%20Analysis&body=Title%3A%20Driver%20Attention%20Tracking%20and%20Analysis%0AAuthor%3A%20Dat%20Viet%20Thanh%20Nguyen%20and%20Anh%20Tran%20and%20Nam%20Vu%20and%20Cuong%20Pham%20and%20Minh%20Hoai%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20to%20estimate%20a%20driver%27s%20points-of-gaze%20using%20a%20pair%0Aof%20ordinary%20cameras%20mounted%20on%20the%20windshield%20and%20dashboard%20of%20a%20car.%20This%20is%20a%0Achallenging%20problem%20due%20to%20the%20dynamics%20of%20traffic%20environments%20with%203D%20scenes%0Aof%20unknown%20depths.%20This%20problem%20is%20further%20complicated%20by%20the%20volatile%20distance%0Abetween%20the%20driver%20and%20the%20camera%20system.%20To%20tackle%20these%20challenges%2C%20we%0Adevelop%20a%20novel%20convolutional%20network%20that%20simultaneously%20analyzes%20the%20image%20of%0Athe%20scene%20and%20the%20image%20of%20the%20driver%27s%20face.%20This%20network%20has%20a%20camera%0Acalibration%20module%20that%20can%20compute%20an%20embedding%20vector%20that%20represents%20the%0Aspatial%20configuration%20between%20the%20driver%20and%20the%20camera%20system.%20This%0Acalibration%20module%20improves%20the%20overall%20network%27s%20performance%2C%20which%20can%20be%0Ajointly%20trained%20end%20to%20end.%0A%20%20We%20also%20address%20the%20lack%20of%20annotated%20data%20for%20training%20and%20evaluation%20by%0Aintroducing%20a%20large-scale%20driving%20dataset%20with%20point-of-gaze%20annotations.%20This%0Ais%20an%20in%20situ%20dataset%20of%20real%20driving%20sessions%20in%20an%20urban%20city%2C%20containing%0Asynchronized%20images%20of%20the%20driving%20scene%20as%20well%20as%20the%20face%20and%20gaze%20of%20the%0Adriver.%20Experiments%20on%20this%20dataset%20show%20that%20the%20proposed%20method%20outperforms%0Avarious%20baseline%20methods%2C%20having%20the%20mean%20prediction%20error%20of%2029.69%20pixels%2C%0Awhich%20is%20relatively%20small%20compared%20to%20the%20%241280%7B%5Ctimes%7D720%24%20resolution%20of%20the%0Ascene%20camera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07122v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driver%20Attention%20Tracking%20and%20Analysis&entry.906535625=Dat%20Viet%20Thanh%20Nguyen%20and%20Anh%20Tran%20and%20Nam%20Vu%20and%20Cuong%20Pham%20and%20Minh%20Hoai&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20to%20estimate%20a%20driver%27s%20points-of-gaze%20using%20a%20pair%0Aof%20ordinary%20cameras%20mounted%20on%20the%20windshield%20and%20dashboard%20of%20a%20car.%20This%20is%20a%0Achallenging%20problem%20due%20to%20the%20dynamics%20of%20traffic%20environments%20with%203D%20scenes%0Aof%20unknown%20depths.%20This%20problem%20is%20further%20complicated%20by%20the%20volatile%20distance%0Abetween%20the%20driver%20and%20the%20camera%20system.%20To%20tackle%20these%20challenges%2C%20we%0Adevelop%20a%20novel%20convolutional%20network%20that%20simultaneously%20analyzes%20the%20image%20of%0Athe%20scene%20and%20the%20image%20of%20the%20driver%27s%20face.%20This%20network%20has%20a%20camera%0Acalibration%20module%20that%20can%20compute%20an%20embedding%20vector%20that%20represents%20the%0Aspatial%20configuration%20between%20the%20driver%20and%20the%20camera%20system.%20This%0Acalibration%20module%20improves%20the%20overall%20network%27s%20performance%2C%20which%20can%20be%0Ajointly%20trained%20end%20to%20end.%0A%20%20We%20also%20address%20the%20lack%20of%20annotated%20data%20for%20training%20and%20evaluation%20by%0Aintroducing%20a%20large-scale%20driving%20dataset%20with%20point-of-gaze%20annotations.%20This%0Ais%20an%20in%20situ%20dataset%20of%20real%20driving%20sessions%20in%20an%20urban%20city%2C%20containing%0Asynchronized%20images%20of%20the%20driving%20scene%20as%20well%20as%20the%20face%20and%20gaze%20of%20the%0Adriver.%20Experiments%20on%20this%20dataset%20show%20that%20the%20proposed%20method%20outperforms%0Avarious%20baseline%20methods%2C%20having%20the%20mean%20prediction%20error%20of%2029.69%20pixels%2C%0Awhich%20is%20relatively%20small%20compared%20to%20the%20%241280%7B%5Ctimes%7D720%24%20resolution%20of%20the%0Ascene%20camera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07122v1&entry.124074799=Read"},
{"title": "GLiDR: Topologically Regularized Graph Generative Network for Sparse\n  LiDAR Point Clouds", "author": "Prashant Kumar and Kshitij Madhav Bhat and Vedang Bhupesh Shenvi Nadkarni and Prem Kalra", "abstract": "  Sparse LiDAR point clouds cause severe loss of detail of static structures\nand reduce the density of static points available for navigation. Reduced\ndensity can be detrimental to navigation under several scenarios. We observe\nthat despite high sparsity, in most cases, the global topology of LiDAR\noutlining the static structures can be inferred. We utilize this property to\nobtain a backbone skeleton of a LiDAR scan in the form of a single connected\ncomponent that is a proxy to its global topology. We utilize the backbone to\naugment new points along static structures to overcome sparsity. Newly\nintroduced points could correspond to existing static structures or to static\npoints that were earlier obstructed by dynamic objects. To the best of our\nknowledge, we are the first to use such a strategy for sparse LiDAR point\nclouds. Existing solutions close to our approach fail to identify and preserve\nthe global static LiDAR topology and generate sub-optimal points. We propose\nGLiDR, a Graph Generative network that is topologically regularized using\n0-dimensional Persistent Homology ($\\mathcal{PH}$) constraints. This enables\nGLiDR to introduce newer static points along a topologically consistent global\nstatic LiDAR backbone. GLiDR generates precise static points using $32\\times$\nsparser dynamic scans and performs better than the baselines across three\ndatasets. GLiDR generates a valuable byproduct - an accurate binary\nsegmentation mask of static and dynamic objects that are helpful for navigation\nplanning and safety in constrained environments. The newly introduced static\npoints allow GLiDR to outperform LiDAR-based navigation using SLAM in several\nsettings. Source code is available at\n$\\texttt{https://github.com/GLiDR-CVPR2024/GLiDR}$.\n", "link": "http://arxiv.org/abs/2312.00068v2", "date": "2024-04-10", "relevancy": 2.7594, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.563}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5474}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5452}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds&body=Title%3A%20GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Prashant%20Kumar%20and%20Kshitij%20Madhav%20Bhat%20and%20Vedang%20Bhupesh%20Shenvi%20Nadkarni%20and%20Prem%20Kalra%0AAbstract%3A%20%20%20Sparse%20LiDAR%20point%20clouds%20cause%20severe%20loss%20of%20detail%20of%20static%20structures%0Aand%20reduce%20the%20density%20of%20static%20points%20available%20for%20navigation.%20Reduced%0Adensity%20can%20be%20detrimental%20to%20navigation%20under%20several%20scenarios.%20We%20observe%0Athat%20despite%20high%20sparsity%2C%20in%20most%20cases%2C%20the%20global%20topology%20of%20LiDAR%0Aoutlining%20the%20static%20structures%20can%20be%20inferred.%20We%20utilize%20this%20property%20to%0Aobtain%20a%20backbone%20skeleton%20of%20a%20LiDAR%20scan%20in%20the%20form%20of%20a%20single%20connected%0Acomponent%20that%20is%20a%20proxy%20to%20its%20global%20topology.%20We%20utilize%20the%20backbone%20to%0Aaugment%20new%20points%20along%20static%20structures%20to%20overcome%20sparsity.%20Newly%0Aintroduced%20points%20could%20correspond%20to%20existing%20static%20structures%20or%20to%20static%0Apoints%20that%20were%20earlier%20obstructed%20by%20dynamic%20objects.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20such%20a%20strategy%20for%20sparse%20LiDAR%20point%0Aclouds.%20Existing%20solutions%20close%20to%20our%20approach%20fail%20to%20identify%20and%20preserve%0Athe%20global%20static%20LiDAR%20topology%20and%20generate%20sub-optimal%20points.%20We%20propose%0AGLiDR%2C%20a%20Graph%20Generative%20network%20that%20is%20topologically%20regularized%20using%0A0-dimensional%20Persistent%20Homology%20%28%24%5Cmathcal%7BPH%7D%24%29%20constraints.%20This%20enables%0AGLiDR%20to%20introduce%20newer%20static%20points%20along%20a%20topologically%20consistent%20global%0Astatic%20LiDAR%20backbone.%20GLiDR%20generates%20precise%20static%20points%20using%20%2432%5Ctimes%24%0Asparser%20dynamic%20scans%20and%20performs%20better%20than%20the%20baselines%20across%20three%0Adatasets.%20GLiDR%20generates%20a%20valuable%20byproduct%20-%20an%20accurate%20binary%0Asegmentation%20mask%20of%20static%20and%20dynamic%20objects%20that%20are%20helpful%20for%20navigation%0Aplanning%20and%20safety%20in%20constrained%20environments.%20The%20newly%20introduced%20static%0Apoints%20allow%20GLiDR%20to%20outperform%20LiDAR-based%20navigation%20using%20SLAM%20in%20several%0Asettings.%20Source%20code%20is%20available%20at%0A%24%5Ctexttt%7Bhttps%3A//github.com/GLiDR-CVPR2024/GLiDR%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00068v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLiDR%3A%20Topologically%20Regularized%20Graph%20Generative%20Network%20for%20Sparse%0A%20%20LiDAR%20Point%20Clouds&entry.906535625=Prashant%20Kumar%20and%20Kshitij%20Madhav%20Bhat%20and%20Vedang%20Bhupesh%20Shenvi%20Nadkarni%20and%20Prem%20Kalra&entry.1292438233=%20%20Sparse%20LiDAR%20point%20clouds%20cause%20severe%20loss%20of%20detail%20of%20static%20structures%0Aand%20reduce%20the%20density%20of%20static%20points%20available%20for%20navigation.%20Reduced%0Adensity%20can%20be%20detrimental%20to%20navigation%20under%20several%20scenarios.%20We%20observe%0Athat%20despite%20high%20sparsity%2C%20in%20most%20cases%2C%20the%20global%20topology%20of%20LiDAR%0Aoutlining%20the%20static%20structures%20can%20be%20inferred.%20We%20utilize%20this%20property%20to%0Aobtain%20a%20backbone%20skeleton%20of%20a%20LiDAR%20scan%20in%20the%20form%20of%20a%20single%20connected%0Acomponent%20that%20is%20a%20proxy%20to%20its%20global%20topology.%20We%20utilize%20the%20backbone%20to%0Aaugment%20new%20points%20along%20static%20structures%20to%20overcome%20sparsity.%20Newly%0Aintroduced%20points%20could%20correspond%20to%20existing%20static%20structures%20or%20to%20static%0Apoints%20that%20were%20earlier%20obstructed%20by%20dynamic%20objects.%20To%20the%20best%20of%20our%0Aknowledge%2C%20we%20are%20the%20first%20to%20use%20such%20a%20strategy%20for%20sparse%20LiDAR%20point%0Aclouds.%20Existing%20solutions%20close%20to%20our%20approach%20fail%20to%20identify%20and%20preserve%0Athe%20global%20static%20LiDAR%20topology%20and%20generate%20sub-optimal%20points.%20We%20propose%0AGLiDR%2C%20a%20Graph%20Generative%20network%20that%20is%20topologically%20regularized%20using%0A0-dimensional%20Persistent%20Homology%20%28%24%5Cmathcal%7BPH%7D%24%29%20constraints.%20This%20enables%0AGLiDR%20to%20introduce%20newer%20static%20points%20along%20a%20topologically%20consistent%20global%0Astatic%20LiDAR%20backbone.%20GLiDR%20generates%20precise%20static%20points%20using%20%2432%5Ctimes%24%0Asparser%20dynamic%20scans%20and%20performs%20better%20than%20the%20baselines%20across%20three%0Adatasets.%20GLiDR%20generates%20a%20valuable%20byproduct%20-%20an%20accurate%20binary%0Asegmentation%20mask%20of%20static%20and%20dynamic%20objects%20that%20are%20helpful%20for%20navigation%0Aplanning%20and%20safety%20in%20constrained%20environments.%20The%20newly%20introduced%20static%0Apoints%20allow%20GLiDR%20to%20outperform%20LiDAR-based%20navigation%20using%20SLAM%20in%20several%0Asettings.%20Source%20code%20is%20available%20at%0A%24%5Ctexttt%7Bhttps%3A//github.com/GLiDR-CVPR2024/GLiDR%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00068v2&entry.124074799=Read"},
{"title": "Efficient and Generic Point Model for Lossless Point Cloud Attribute\n  Compression", "author": "Kang You and Pan Gao and Zhan Ma", "abstract": "  The past several years have witnessed the emergence of learned point cloud\ncompression (PCC) techniques. However, current learning-based lossless point\ncloud attribute compression (PCAC) methods either suffer from high\ncomputational complexity or deteriorated compression performance. Moreover, the\nsignificant variations in point cloud scale and sparsity encountered in\nreal-world applications make developing an all-in-one neural model a\nchallenging task. In this paper, we propose PoLoPCAC, an efficient and generic\nlossless PCAC method that achieves high compression efficiency and strong\ngeneralizability simultaneously. We formulate lossless PCAC as the task of\ninferring explicit distributions of attributes from group-wise autoregressive\npriors. A progressive random grouping strategy is first devised to efficiently\nresolve the point cloud into groups, and then the attributes of each group are\nmodeled sequentially from accumulated antecedents. A locality-aware attention\nmechanism is utilized to exploit prior knowledge from context windows in\nparallel. Since our method directly operates on points, it can naturally avoids\ndistortion caused by voxelization, and can be executed on point clouds with\narbitrary scale and density. Experiments show that our method can be instantly\ndeployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying\ncontinuous bit-rate reduction over the latest G-PCCv23 on various datasets\n(ShapeNet, ScanNet, MVUB, 8iVFB). Meanwhile, our method reports shorter coding\ntime than G-PCCv23 on the majority of sequences with a lightweight model size\n(2.6MB), which is highly attractive for practical applications. Dataset, code\nand trained model are available at\nhttps://github.com/I2-Multimedia-Lab/PoLoPCAC.\n", "link": "http://arxiv.org/abs/2404.06936v1", "date": "2024-04-10", "relevancy": 2.6928, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Generic%20Point%20Model%20for%20Lossless%20Point%20Cloud%20Attribute%0A%20%20Compression&body=Title%3A%20Efficient%20and%20Generic%20Point%20Model%20for%20Lossless%20Point%20Cloud%20Attribute%0A%20%20Compression%0AAuthor%3A%20Kang%20You%20and%20Pan%20Gao%20and%20Zhan%20Ma%0AAbstract%3A%20%20%20The%20past%20several%20years%20have%20witnessed%20the%20emergence%20of%20learned%20point%20cloud%0Acompression%20%28PCC%29%20techniques.%20However%2C%20current%20learning-based%20lossless%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20methods%20either%20suffer%20from%20high%0Acomputational%20complexity%20or%20deteriorated%20compression%20performance.%20Moreover%2C%20the%0Asignificant%20variations%20in%20point%20cloud%20scale%20and%20sparsity%20encountered%20in%0Areal-world%20applications%20make%20developing%20an%20all-in-one%20neural%20model%20a%0Achallenging%20task.%20In%20this%20paper%2C%20we%20propose%20PoLoPCAC%2C%20an%20efficient%20and%20generic%0Alossless%20PCAC%20method%20that%20achieves%20high%20compression%20efficiency%20and%20strong%0Ageneralizability%20simultaneously.%20We%20formulate%20lossless%20PCAC%20as%20the%20task%20of%0Ainferring%20explicit%20distributions%20of%20attributes%20from%20group-wise%20autoregressive%0Apriors.%20A%20progressive%20random%20grouping%20strategy%20is%20first%20devised%20to%20efficiently%0Aresolve%20the%20point%20cloud%20into%20groups%2C%20and%20then%20the%20attributes%20of%20each%20group%20are%0Amodeled%20sequentially%20from%20accumulated%20antecedents.%20A%20locality-aware%20attention%0Amechanism%20is%20utilized%20to%20exploit%20prior%20knowledge%20from%20context%20windows%20in%0Aparallel.%20Since%20our%20method%20directly%20operates%20on%20points%2C%20it%20can%20naturally%20avoids%0Adistortion%20caused%20by%20voxelization%2C%20and%20can%20be%20executed%20on%20point%20clouds%20with%0Aarbitrary%20scale%20and%20density.%20Experiments%20show%20that%20our%20method%20can%20be%20instantly%0Adeployed%20once%20trained%20on%20a%20Synthetic%202k-ShapeNet%20dataset%20while%20enjoying%0Acontinuous%20bit-rate%20reduction%20over%20the%20latest%20G-PCCv23%20on%20various%20datasets%0A%28ShapeNet%2C%20ScanNet%2C%20MVUB%2C%208iVFB%29.%20Meanwhile%2C%20our%20method%20reports%20shorter%20coding%0Atime%20than%20G-PCCv23%20on%20the%20majority%20of%20sequences%20with%20a%20lightweight%20model%20size%0A%282.6MB%29%2C%20which%20is%20highly%20attractive%20for%20practical%20applications.%20Dataset%2C%20code%0Aand%20trained%20model%20are%20available%20at%0Ahttps%3A//github.com/I2-Multimedia-Lab/PoLoPCAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06936v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Generic%20Point%20Model%20for%20Lossless%20Point%20Cloud%20Attribute%0A%20%20Compression&entry.906535625=Kang%20You%20and%20Pan%20Gao%20and%20Zhan%20Ma&entry.1292438233=%20%20The%20past%20several%20years%20have%20witnessed%20the%20emergence%20of%20learned%20point%20cloud%0Acompression%20%28PCC%29%20techniques.%20However%2C%20current%20learning-based%20lossless%20point%0Acloud%20attribute%20compression%20%28PCAC%29%20methods%20either%20suffer%20from%20high%0Acomputational%20complexity%20or%20deteriorated%20compression%20performance.%20Moreover%2C%20the%0Asignificant%20variations%20in%20point%20cloud%20scale%20and%20sparsity%20encountered%20in%0Areal-world%20applications%20make%20developing%20an%20all-in-one%20neural%20model%20a%0Achallenging%20task.%20In%20this%20paper%2C%20we%20propose%20PoLoPCAC%2C%20an%20efficient%20and%20generic%0Alossless%20PCAC%20method%20that%20achieves%20high%20compression%20efficiency%20and%20strong%0Ageneralizability%20simultaneously.%20We%20formulate%20lossless%20PCAC%20as%20the%20task%20of%0Ainferring%20explicit%20distributions%20of%20attributes%20from%20group-wise%20autoregressive%0Apriors.%20A%20progressive%20random%20grouping%20strategy%20is%20first%20devised%20to%20efficiently%0Aresolve%20the%20point%20cloud%20into%20groups%2C%20and%20then%20the%20attributes%20of%20each%20group%20are%0Amodeled%20sequentially%20from%20accumulated%20antecedents.%20A%20locality-aware%20attention%0Amechanism%20is%20utilized%20to%20exploit%20prior%20knowledge%20from%20context%20windows%20in%0Aparallel.%20Since%20our%20method%20directly%20operates%20on%20points%2C%20it%20can%20naturally%20avoids%0Adistortion%20caused%20by%20voxelization%2C%20and%20can%20be%20executed%20on%20point%20clouds%20with%0Aarbitrary%20scale%20and%20density.%20Experiments%20show%20that%20our%20method%20can%20be%20instantly%0Adeployed%20once%20trained%20on%20a%20Synthetic%202k-ShapeNet%20dataset%20while%20enjoying%0Acontinuous%20bit-rate%20reduction%20over%20the%20latest%20G-PCCv23%20on%20various%20datasets%0A%28ShapeNet%2C%20ScanNet%2C%20MVUB%2C%208iVFB%29.%20Meanwhile%2C%20our%20method%20reports%20shorter%20coding%0Atime%20than%20G-PCCv23%20on%20the%20majority%20of%20sequences%20with%20a%20lightweight%20model%20size%0A%282.6MB%29%2C%20which%20is%20highly%20attractive%20for%20practical%20applications.%20Dataset%2C%20code%0Aand%20trained%20model%20are%20available%20at%0Ahttps%3A//github.com/I2-Multimedia-Lab/PoLoPCAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06936v1&entry.124074799=Read"},
{"title": "Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for\n  Multi-exposure Image Fusion", "author": "Pan Mu and Zhiying Du and Jinyuan Liu and Cong Bai", "abstract": "  In recent years, deep learning networks have made remarkable strides in the\ndomain of multi-exposure image fusion. Nonetheless, prevailing approaches often\ninvolve directly feeding over-exposed and under-exposed images into the\nnetwork, which leads to the under-utilization of inherent information present\nin the source images. Additionally, unsupervised techniques predominantly\nemploy rudimentary weighted summation for color channel processing, culminating\nin an overall desaturated final image tone. To partially mitigate these issues,\nthis study proposes a gamma correction module specifically designed to fully\nleverage latent information embedded within source images. Furthermore, a\nmodified transformer block, embracing with self-attention mechanisms, is\nintroduced to optimize the fusion process. Ultimately, a novel color\nenhancement algorithm is presented to augment image saturation while preserving\nintricate details. The source code is available at\nhttps://github.com/ZhiyingDu/BHFMEF.\n", "link": "http://arxiv.org/abs/2404.06033v2", "date": "2024-04-10", "relevancy": 2.6851, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5404}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5369}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Little%20Strokes%20Fell%20Great%20Oaks%3A%20Boosting%20the%20Hierarchical%20Features%20for%0A%20%20Multi-exposure%20Image%20Fusion&body=Title%3A%20Little%20Strokes%20Fell%20Great%20Oaks%3A%20Boosting%20the%20Hierarchical%20Features%20for%0A%20%20Multi-exposure%20Image%20Fusion%0AAuthor%3A%20Pan%20Mu%20and%20Zhiying%20Du%20and%20Jinyuan%20Liu%20and%20Cong%20Bai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning%20networks%20have%20made%20remarkable%20strides%20in%20the%0Adomain%20of%20multi-exposure%20image%20fusion.%20Nonetheless%2C%20prevailing%20approaches%20often%0Ainvolve%20directly%20feeding%20over-exposed%20and%20under-exposed%20images%20into%20the%0Anetwork%2C%20which%20leads%20to%20the%20under-utilization%20of%20inherent%20information%20present%0Ain%20the%20source%20images.%20Additionally%2C%20unsupervised%20techniques%20predominantly%0Aemploy%20rudimentary%20weighted%20summation%20for%20color%20channel%20processing%2C%20culminating%0Ain%20an%20overall%20desaturated%20final%20image%20tone.%20To%20partially%20mitigate%20these%20issues%2C%0Athis%20study%20proposes%20a%20gamma%20correction%20module%20specifically%20designed%20to%20fully%0Aleverage%20latent%20information%20embedded%20within%20source%20images.%20Furthermore%2C%20a%0Amodified%20transformer%20block%2C%20embracing%20with%20self-attention%20mechanisms%2C%20is%0Aintroduced%20to%20optimize%20the%20fusion%20process.%20Ultimately%2C%20a%20novel%20color%0Aenhancement%20algorithm%20is%20presented%20to%20augment%20image%20saturation%20while%20preserving%0Aintricate%20details.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhiyingDu/BHFMEF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06033v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Little%20Strokes%20Fell%20Great%20Oaks%3A%20Boosting%20the%20Hierarchical%20Features%20for%0A%20%20Multi-exposure%20Image%20Fusion&entry.906535625=Pan%20Mu%20and%20Zhiying%20Du%20and%20Jinyuan%20Liu%20and%20Cong%20Bai&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning%20networks%20have%20made%20remarkable%20strides%20in%20the%0Adomain%20of%20multi-exposure%20image%20fusion.%20Nonetheless%2C%20prevailing%20approaches%20often%0Ainvolve%20directly%20feeding%20over-exposed%20and%20under-exposed%20images%20into%20the%0Anetwork%2C%20which%20leads%20to%20the%20under-utilization%20of%20inherent%20information%20present%0Ain%20the%20source%20images.%20Additionally%2C%20unsupervised%20techniques%20predominantly%0Aemploy%20rudimentary%20weighted%20summation%20for%20color%20channel%20processing%2C%20culminating%0Ain%20an%20overall%20desaturated%20final%20image%20tone.%20To%20partially%20mitigate%20these%20issues%2C%0Athis%20study%20proposes%20a%20gamma%20correction%20module%20specifically%20designed%20to%20fully%0Aleverage%20latent%20information%20embedded%20within%20source%20images.%20Furthermore%2C%20a%0Amodified%20transformer%20block%2C%20embracing%20with%20self-attention%20mechanisms%2C%20is%0Aintroduced%20to%20optimize%20the%20fusion%20process.%20Ultimately%2C%20a%20novel%20color%0Aenhancement%20algorithm%20is%20presented%20to%20augment%20image%20saturation%20while%20preserving%0Aintricate%20details.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhiyingDu/BHFMEF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06033v2&entry.124074799=Read"},
{"title": "A Gauss-Newton Approach for Min-Max Optimization in Generative\n  Adversarial Networks", "author": "Neel Mishra and Bamdev Mishra and Pratik Jawanpuria and Pawan Kumar", "abstract": "  A novel first-order method is proposed for training generative adversarial\nnetworks (GANs). It modifies the Gauss-Newton method to approximate the min-max\nHessian and uses the Sherman-Morrison inversion formula to calculate the\ninverse. The method corresponds to a fixed-point method that ensures necessary\ncontraction. To evaluate its effectiveness, numerical experiments are conducted\non various datasets commonly used in image generation tasks, such as MNIST,\nFashion MNIST, CIFAR10, FFHQ, and LSUN. Our method is capable of generating\nhigh-fidelity images with greater diversity across multiple datasets. It also\nachieves the highest inception score for CIFAR10 among all compared methods,\nincluding state-of-the-art second-order methods. Additionally, its execution\ntime is comparable to that of first-order min-max methods.\n", "link": "http://arxiv.org/abs/2404.07172v1", "date": "2024-04-10", "relevancy": 2.6497, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5339}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5315}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5243}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Gauss-Newton%20Approach%20for%20Min-Max%20Optimization%20in%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20A%20Gauss-Newton%20Approach%20for%20Min-Max%20Optimization%20in%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Neel%20Mishra%20and%20Bamdev%20Mishra%20and%20Pratik%20Jawanpuria%20and%20Pawan%20Kumar%0AAbstract%3A%20%20%20A%20novel%20first-order%20method%20is%20proposed%20for%20training%20generative%20adversarial%0Anetworks%20%28GANs%29.%20It%20modifies%20the%20Gauss-Newton%20method%20to%20approximate%20the%20min-max%0AHessian%20and%20uses%20the%20Sherman-Morrison%20inversion%20formula%20to%20calculate%20the%0Ainverse.%20The%20method%20corresponds%20to%20a%20fixed-point%20method%20that%20ensures%20necessary%0Acontraction.%20To%20evaluate%20its%20effectiveness%2C%20numerical%20experiments%20are%20conducted%0Aon%20various%20datasets%20commonly%20used%20in%20image%20generation%20tasks%2C%20such%20as%20MNIST%2C%0AFashion%20MNIST%2C%20CIFAR10%2C%20FFHQ%2C%20and%20LSUN.%20Our%20method%20is%20capable%20of%20generating%0Ahigh-fidelity%20images%20with%20greater%20diversity%20across%20multiple%20datasets.%20It%20also%0Aachieves%20the%20highest%20inception%20score%20for%20CIFAR10%20among%20all%20compared%20methods%2C%0Aincluding%20state-of-the-art%20second-order%20methods.%20Additionally%2C%20its%20execution%0Atime%20is%20comparable%20to%20that%20of%20first-order%20min-max%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07172v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Gauss-Newton%20Approach%20for%20Min-Max%20Optimization%20in%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Neel%20Mishra%20and%20Bamdev%20Mishra%20and%20Pratik%20Jawanpuria%20and%20Pawan%20Kumar&entry.1292438233=%20%20A%20novel%20first-order%20method%20is%20proposed%20for%20training%20generative%20adversarial%0Anetworks%20%28GANs%29.%20It%20modifies%20the%20Gauss-Newton%20method%20to%20approximate%20the%20min-max%0AHessian%20and%20uses%20the%20Sherman-Morrison%20inversion%20formula%20to%20calculate%20the%0Ainverse.%20The%20method%20corresponds%20to%20a%20fixed-point%20method%20that%20ensures%20necessary%0Acontraction.%20To%20evaluate%20its%20effectiveness%2C%20numerical%20experiments%20are%20conducted%0Aon%20various%20datasets%20commonly%20used%20in%20image%20generation%20tasks%2C%20such%20as%20MNIST%2C%0AFashion%20MNIST%2C%20CIFAR10%2C%20FFHQ%2C%20and%20LSUN.%20Our%20method%20is%20capable%20of%20generating%0Ahigh-fidelity%20images%20with%20greater%20diversity%20across%20multiple%20datasets.%20It%20also%0Aachieves%20the%20highest%20inception%20score%20for%20CIFAR10%20among%20all%20compared%20methods%2C%0Aincluding%20state-of-the-art%20second-order%20methods.%20Additionally%2C%20its%20execution%0Atime%20is%20comparable%20to%20that%20of%20first-order%20min-max%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07172v1&entry.124074799=Read"},
{"title": "Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial\n  Robustness", "author": "Sibo Wang and Jie Zhang and Zheng Yuan and Shiguang Shan", "abstract": "  Large-scale pre-trained vision-language models like CLIP have demonstrated\nimpressive performance across various tasks, and exhibit remarkable zero-shot\ngeneralization capability, while they are also vulnerable to imperceptible\nadversarial examples. Existing works typically employ adversarial training\n(fine-tuning) as a defense method against adversarial examples. However, direct\napplication to the CLIP model may result in overfitting, compromising the\nmodel's capacity for generalization. In this paper, we propose Pre-trained\nModel Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages\nsupervision from the original pre-trained model by carefully designing an\nauxiliary branch, to enhance the model's zero-shot adversarial robustness.\nSpecifically, PMG-AFT minimizes the distance between the features of\nadversarial examples in the target model and those in the pre-trained model,\naiming to preserve the generalization features already captured by the\npre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate\nthat PMG-AFT significantly outperforms the state-of-the-art method, improving\nthe top-1 robust accuracy by an average of 4.99%. Furthermore, our approach\nconsistently improves clean accuracy by an average of 8.72%. Our code is\navailable at\nhttps://github.com/serendipity1122/Pre-trained-Model-Guided-Fine-Tuning-for-Zero-Shot-Adversarial-Robustness.\n", "link": "http://arxiv.org/abs/2401.04350v3", "date": "2024-04-10", "relevancy": 2.6463, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5662}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5158}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5058}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Model%20Guided%20Fine-Tuning%20for%20Zero-Shot%20Adversarial%0A%20%20Robustness&body=Title%3A%20Pre-trained%20Model%20Guided%20Fine-Tuning%20for%20Zero-Shot%20Adversarial%0A%20%20Robustness%0AAuthor%3A%20Sibo%20Wang%20and%20Jie%20Zhang%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20vision-language%20models%20like%20CLIP%20have%20demonstrated%0Aimpressive%20performance%20across%20various%20tasks%2C%20and%20exhibit%20remarkable%20zero-shot%0Ageneralization%20capability%2C%20while%20they%20are%20also%20vulnerable%20to%20imperceptible%0Aadversarial%20examples.%20Existing%20works%20typically%20employ%20adversarial%20training%0A%28fine-tuning%29%20as%20a%20defense%20method%20against%20adversarial%20examples.%20However%2C%20direct%0Aapplication%20to%20the%20CLIP%20model%20may%20result%20in%20overfitting%2C%20compromising%20the%0Amodel%27s%20capacity%20for%20generalization.%20In%20this%20paper%2C%20we%20propose%20Pre-trained%0AModel%20Guided%20Adversarial%20Fine-Tuning%20%28PMG-AFT%29%20method%2C%20which%20leverages%0Asupervision%20from%20the%20original%20pre-trained%20model%20by%20carefully%20designing%20an%0Aauxiliary%20branch%2C%20to%20enhance%20the%20model%27s%20zero-shot%20adversarial%20robustness.%0ASpecifically%2C%20PMG-AFT%20minimizes%20the%20distance%20between%20the%20features%20of%0Aadversarial%20examples%20in%20the%20target%20model%20and%20those%20in%20the%20pre-trained%20model%2C%0Aaiming%20to%20preserve%20the%20generalization%20features%20already%20captured%20by%20the%0Apre-trained%20model.%20Extensive%20Experiments%20on%2015%20zero-shot%20datasets%20demonstrate%0Athat%20PMG-AFT%20significantly%20outperforms%20the%20state-of-the-art%20method%2C%20improving%0Athe%20top-1%20robust%20accuracy%20by%20an%20average%20of%204.99%25.%20Furthermore%2C%20our%20approach%0Aconsistently%20improves%20clean%20accuracy%20by%20an%20average%20of%208.72%25.%20Our%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/serendipity1122/Pre-trained-Model-Guided-Fine-Tuning-for-Zero-Shot-Adversarial-Robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04350v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Model%20Guided%20Fine-Tuning%20for%20Zero-Shot%20Adversarial%0A%20%20Robustness&entry.906535625=Sibo%20Wang%20and%20Jie%20Zhang%20and%20Zheng%20Yuan%20and%20Shiguang%20Shan&entry.1292438233=%20%20Large-scale%20pre-trained%20vision-language%20models%20like%20CLIP%20have%20demonstrated%0Aimpressive%20performance%20across%20various%20tasks%2C%20and%20exhibit%20remarkable%20zero-shot%0Ageneralization%20capability%2C%20while%20they%20are%20also%20vulnerable%20to%20imperceptible%0Aadversarial%20examples.%20Existing%20works%20typically%20employ%20adversarial%20training%0A%28fine-tuning%29%20as%20a%20defense%20method%20against%20adversarial%20examples.%20However%2C%20direct%0Aapplication%20to%20the%20CLIP%20model%20may%20result%20in%20overfitting%2C%20compromising%20the%0Amodel%27s%20capacity%20for%20generalization.%20In%20this%20paper%2C%20we%20propose%20Pre-trained%0AModel%20Guided%20Adversarial%20Fine-Tuning%20%28PMG-AFT%29%20method%2C%20which%20leverages%0Asupervision%20from%20the%20original%20pre-trained%20model%20by%20carefully%20designing%20an%0Aauxiliary%20branch%2C%20to%20enhance%20the%20model%27s%20zero-shot%20adversarial%20robustness.%0ASpecifically%2C%20PMG-AFT%20minimizes%20the%20distance%20between%20the%20features%20of%0Aadversarial%20examples%20in%20the%20target%20model%20and%20those%20in%20the%20pre-trained%20model%2C%0Aaiming%20to%20preserve%20the%20generalization%20features%20already%20captured%20by%20the%0Apre-trained%20model.%20Extensive%20Experiments%20on%2015%20zero-shot%20datasets%20demonstrate%0Athat%20PMG-AFT%20significantly%20outperforms%20the%20state-of-the-art%20method%2C%20improving%0Athe%20top-1%20robust%20accuracy%20by%20an%20average%20of%204.99%25.%20Furthermore%2C%20our%20approach%0Aconsistently%20improves%20clean%20accuracy%20by%20an%20average%20of%208.72%25.%20Our%20code%20is%0Aavailable%20at%0Ahttps%3A//github.com/serendipity1122/Pre-trained-Model-Guided-Fine-Tuning-for-Zero-Shot-Adversarial-Robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04350v3&entry.124074799=Read"},
{"title": "3DMambaComplete: Exploring Structured State Space Model for Point Cloud\n  Completion", "author": "Yixuan Li and Weidong Yang and Ben Fei", "abstract": "  Point cloud completion aims to generate a complete and high-fidelity point\ncloud from an initially incomplete and low-quality input. A prevalent strategy\ninvolves leveraging Transformer-based models to encode global features and\nfacilitate the reconstruction process. However, the adoption of pooling\noperations to obtain global feature representations often results in the loss\nof local details within the point cloud. Moreover, the attention mechanism\ninherent in Transformers introduces additional computational complexity,\nrendering it challenging to handle long sequences effectively. To address these\nissues, we propose 3DMambaComplete, a point cloud completion network built on\nthe novel Mamba framework. It comprises three modules: HyperPoint Generation\nencodes point cloud features using Mamba's selection mechanism and predicts a\nset of Hyperpoints. A specific offset is estimated, and the down-sampled points\nbecome HyperPoints. The HyperPoint Spread module disperses these HyperPoints\nacross different spatial locations to avoid concentration. Finally, a\ndeformation method transforms the 2D mesh representation of HyperPoints into a\nfine-grained 3D structure for point cloud reconstruction. Extensive experiments\nconducted on various established benchmarks demonstrate that 3DMambaComplete\nsurpasses state-of-the-art point cloud completion methods, as confirmed by\nqualitative and quantitative analyses.\n", "link": "http://arxiv.org/abs/2404.07106v1", "date": "2024-04-10", "relevancy": 2.5997, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203DMambaComplete%3A%20Exploring%20Structured%20State%20Space%20Model%20for%20Point%20Cloud%0A%20%20Completion&body=Title%3A%203DMambaComplete%3A%20Exploring%20Structured%20State%20Space%20Model%20for%20Point%20Cloud%0A%20%20Completion%0AAuthor%3A%20Yixuan%20Li%20and%20Weidong%20Yang%20and%20Ben%20Fei%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20generate%20a%20complete%20and%20high-fidelity%20point%0Acloud%20from%20an%20initially%20incomplete%20and%20low-quality%20input.%20A%20prevalent%20strategy%0Ainvolves%20leveraging%20Transformer-based%20models%20to%20encode%20global%20features%20and%0Afacilitate%20the%20reconstruction%20process.%20However%2C%20the%20adoption%20of%20pooling%0Aoperations%20to%20obtain%20global%20feature%20representations%20often%20results%20in%20the%20loss%0Aof%20local%20details%20within%20the%20point%20cloud.%20Moreover%2C%20the%20attention%20mechanism%0Ainherent%20in%20Transformers%20introduces%20additional%20computational%20complexity%2C%0Arendering%20it%20challenging%20to%20handle%20long%20sequences%20effectively.%20To%20address%20these%0Aissues%2C%20we%20propose%203DMambaComplete%2C%20a%20point%20cloud%20completion%20network%20built%20on%0Athe%20novel%20Mamba%20framework.%20It%20comprises%20three%20modules%3A%20HyperPoint%20Generation%0Aencodes%20point%20cloud%20features%20using%20Mamba%27s%20selection%20mechanism%20and%20predicts%20a%0Aset%20of%20Hyperpoints.%20A%20specific%20offset%20is%20estimated%2C%20and%20the%20down-sampled%20points%0Abecome%20HyperPoints.%20The%20HyperPoint%20Spread%20module%20disperses%20these%20HyperPoints%0Aacross%20different%20spatial%20locations%20to%20avoid%20concentration.%20Finally%2C%20a%0Adeformation%20method%20transforms%20the%202D%20mesh%20representation%20of%20HyperPoints%20into%20a%0Afine-grained%203D%20structure%20for%20point%20cloud%20reconstruction.%20Extensive%20experiments%0Aconducted%20on%20various%20established%20benchmarks%20demonstrate%20that%203DMambaComplete%0Asurpasses%20state-of-the-art%20point%20cloud%20completion%20methods%2C%20as%20confirmed%20by%0Aqualitative%20and%20quantitative%20analyses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07106v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DMambaComplete%3A%20Exploring%20Structured%20State%20Space%20Model%20for%20Point%20Cloud%0A%20%20Completion&entry.906535625=Yixuan%20Li%20and%20Weidong%20Yang%20and%20Ben%20Fei&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20generate%20a%20complete%20and%20high-fidelity%20point%0Acloud%20from%20an%20initially%20incomplete%20and%20low-quality%20input.%20A%20prevalent%20strategy%0Ainvolves%20leveraging%20Transformer-based%20models%20to%20encode%20global%20features%20and%0Afacilitate%20the%20reconstruction%20process.%20However%2C%20the%20adoption%20of%20pooling%0Aoperations%20to%20obtain%20global%20feature%20representations%20often%20results%20in%20the%20loss%0Aof%20local%20details%20within%20the%20point%20cloud.%20Moreover%2C%20the%20attention%20mechanism%0Ainherent%20in%20Transformers%20introduces%20additional%20computational%20complexity%2C%0Arendering%20it%20challenging%20to%20handle%20long%20sequences%20effectively.%20To%20address%20these%0Aissues%2C%20we%20propose%203DMambaComplete%2C%20a%20point%20cloud%20completion%20network%20built%20on%0Athe%20novel%20Mamba%20framework.%20It%20comprises%20three%20modules%3A%20HyperPoint%20Generation%0Aencodes%20point%20cloud%20features%20using%20Mamba%27s%20selection%20mechanism%20and%20predicts%20a%0Aset%20of%20Hyperpoints.%20A%20specific%20offset%20is%20estimated%2C%20and%20the%20down-sampled%20points%0Abecome%20HyperPoints.%20The%20HyperPoint%20Spread%20module%20disperses%20these%20HyperPoints%0Aacross%20different%20spatial%20locations%20to%20avoid%20concentration.%20Finally%2C%20a%0Adeformation%20method%20transforms%20the%202D%20mesh%20representation%20of%20HyperPoints%20into%20a%0Afine-grained%203D%20structure%20for%20point%20cloud%20reconstruction.%20Extensive%20experiments%0Aconducted%20on%20various%20established%20benchmarks%20demonstrate%20that%203DMambaComplete%0Asurpasses%20state-of-the-art%20point%20cloud%20completion%20methods%2C%20as%20confirmed%20by%0Aqualitative%20and%20quantitative%20analyses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07106v1&entry.124074799=Read"},
{"title": "BRAVE: Broadening the visual encoding of vision-language models", "author": "O\u011fuzhan Fatih Kar and Alessio Tonioni and Petra Poklukar and Achin Kulshrestha and Amir Zamir and Federico Tombari", "abstract": "  Vision-language models (VLMs) are typically composed of a vision encoder,\ne.g. CLIP, and a language model (LM) that interprets the encoded features to\nsolve downstream tasks. Despite remarkable progress, VLMs are subject to\nseveral shortcomings due to the limited capabilities of vision encoders, e.g.\n\"blindness\" to certain image features, visual hallucination, etc. To address\nthese issues, we study broadening the visual encoding capabilities of VLMs. We\nfirst comprehensively benchmark several vision encoders with different\ninductive biases for solving VLM tasks. We observe that there is no single\nencoding configuration that consistently achieves top performance across\ndifferent tasks, and encoders with different biases can perform surprisingly\nsimilarly. Motivated by this, we introduce a method, named BRAVE, that\nconsolidates features from multiple frozen encoders into a more versatile\nrepresentation that can be directly fed as the input to a frozen LM. BRAVE\nachieves state-of-the-art performance on a broad range of captioning and VQA\nbenchmarks and significantly reduces the aforementioned issues of VLMs, while\nrequiring a smaller number of trainable parameters than existing methods and\nhaving a more compressed representation. Our results highlight the potential of\nincorporating different visual biases for a more broad and contextualized\nvisual understanding of VLMs.\n", "link": "http://arxiv.org/abs/2404.07204v1", "date": "2024-04-10", "relevancy": 2.5916, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BRAVE%3A%20Broadening%20the%20visual%20encoding%20of%20vision-language%20models&body=Title%3A%20BRAVE%3A%20Broadening%20the%20visual%20encoding%20of%20vision-language%20models%0AAuthor%3A%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Alessio%20Tonioni%20and%20Petra%20Poklukar%20and%20Achin%20Kulshrestha%20and%20Amir%20Zamir%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20typically%20composed%20of%20a%20vision%20encoder%2C%0Ae.g.%20CLIP%2C%20and%20a%20language%20model%20%28LM%29%20that%20interprets%20the%20encoded%20features%20to%0Asolve%20downstream%20tasks.%20Despite%20remarkable%20progress%2C%20VLMs%20are%20subject%20to%0Aseveral%20shortcomings%20due%20to%20the%20limited%20capabilities%20of%20vision%20encoders%2C%20e.g.%0A%22blindness%22%20to%20certain%20image%20features%2C%20visual%20hallucination%2C%20etc.%20To%20address%0Athese%20issues%2C%20we%20study%20broadening%20the%20visual%20encoding%20capabilities%20of%20VLMs.%20We%0Afirst%20comprehensively%20benchmark%20several%20vision%20encoders%20with%20different%0Ainductive%20biases%20for%20solving%20VLM%20tasks.%20We%20observe%20that%20there%20is%20no%20single%0Aencoding%20configuration%20that%20consistently%20achieves%20top%20performance%20across%0Adifferent%20tasks%2C%20and%20encoders%20with%20different%20biases%20can%20perform%20surprisingly%0Asimilarly.%20Motivated%20by%20this%2C%20we%20introduce%20a%20method%2C%20named%20BRAVE%2C%20that%0Aconsolidates%20features%20from%20multiple%20frozen%20encoders%20into%20a%20more%20versatile%0Arepresentation%20that%20can%20be%20directly%20fed%20as%20the%20input%20to%20a%20frozen%20LM.%20BRAVE%0Aachieves%20state-of-the-art%20performance%20on%20a%20broad%20range%20of%20captioning%20and%20VQA%0Abenchmarks%20and%20significantly%20reduces%20the%20aforementioned%20issues%20of%20VLMs%2C%20while%0Arequiring%20a%20smaller%20number%20of%20trainable%20parameters%20than%20existing%20methods%20and%0Ahaving%20a%20more%20compressed%20representation.%20Our%20results%20highlight%20the%20potential%20of%0Aincorporating%20different%20visual%20biases%20for%20a%20more%20broad%20and%20contextualized%0Avisual%20understanding%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07204v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRAVE%3A%20Broadening%20the%20visual%20encoding%20of%20vision-language%20models&entry.906535625=O%C4%9Fuzhan%20Fatih%20Kar%20and%20Alessio%20Tonioni%20and%20Petra%20Poklukar%20and%20Achin%20Kulshrestha%20and%20Amir%20Zamir%20and%20Federico%20Tombari&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20typically%20composed%20of%20a%20vision%20encoder%2C%0Ae.g.%20CLIP%2C%20and%20a%20language%20model%20%28LM%29%20that%20interprets%20the%20encoded%20features%20to%0Asolve%20downstream%20tasks.%20Despite%20remarkable%20progress%2C%20VLMs%20are%20subject%20to%0Aseveral%20shortcomings%20due%20to%20the%20limited%20capabilities%20of%20vision%20encoders%2C%20e.g.%0A%22blindness%22%20to%20certain%20image%20features%2C%20visual%20hallucination%2C%20etc.%20To%20address%0Athese%20issues%2C%20we%20study%20broadening%20the%20visual%20encoding%20capabilities%20of%20VLMs.%20We%0Afirst%20comprehensively%20benchmark%20several%20vision%20encoders%20with%20different%0Ainductive%20biases%20for%20solving%20VLM%20tasks.%20We%20observe%20that%20there%20is%20no%20single%0Aencoding%20configuration%20that%20consistently%20achieves%20top%20performance%20across%0Adifferent%20tasks%2C%20and%20encoders%20with%20different%20biases%20can%20perform%20surprisingly%0Asimilarly.%20Motivated%20by%20this%2C%20we%20introduce%20a%20method%2C%20named%20BRAVE%2C%20that%0Aconsolidates%20features%20from%20multiple%20frozen%20encoders%20into%20a%20more%20versatile%0Arepresentation%20that%20can%20be%20directly%20fed%20as%20the%20input%20to%20a%20frozen%20LM.%20BRAVE%0Aachieves%20state-of-the-art%20performance%20on%20a%20broad%20range%20of%20captioning%20and%20VQA%0Abenchmarks%20and%20significantly%20reduces%20the%20aforementioned%20issues%20of%20VLMs%2C%20while%0Arequiring%20a%20smaller%20number%20of%20trainable%20parameters%20than%20existing%20methods%20and%0Ahaving%20a%20more%20compressed%20representation.%20Our%20results%20highlight%20the%20potential%20of%0Aincorporating%20different%20visual%20biases%20for%20a%20more%20broad%20and%20contextualized%0Avisual%20understanding%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07204v1&entry.124074799=Read"},
{"title": "Visual Concept Connectome (VCC): Open World Concept Discovery and their\n  Interlayer Connections in Deep Models", "author": "Matthew Kowal and Richard P. Wildes and Konstantinos G. Derpanis", "abstract": "  Understanding what deep network models capture in their learned\nrepresentations is a fundamental challenge in computer vision. We present a new\nmethodology to understanding such vision models, the Visual Concept Connectome\n(VCC), which discovers human interpretable concepts and their interlayer\nconnections in a fully unsupervised manner. Our approach simultaneously reveals\nfine-grained concepts at a layer, connection weightings across all layers and\nis amendable to global analysis of network structure (e.g., branching pattern\nof hierarchical concept assemblies). Previous work yielded ways to extract\ninterpretable concepts from single layers and examine their impact on\nclassification, but did not afford multilayer concept analysis across an entire\nnetwork architecture. Quantitative and qualitative empirical results show the\neffectiveness of VCCs in the domain of image classification. Also, we leverage\nVCCs for the application of failure mode debugging to reveal where mistakes\narise in deep networks.\n", "link": "http://arxiv.org/abs/2404.02233v2", "date": "2024-04-10", "relevancy": 2.5161, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visual%20Concept%20Connectome%20%28VCC%29%3A%20Open%20World%20Concept%20Discovery%20and%20their%0A%20%20Interlayer%20Connections%20in%20Deep%20Models&body=Title%3A%20Visual%20Concept%20Connectome%20%28VCC%29%3A%20Open%20World%20Concept%20Discovery%20and%20their%0A%20%20Interlayer%20Connections%20in%20Deep%20Models%0AAuthor%3A%20Matthew%20Kowal%20and%20Richard%20P.%20Wildes%20and%20Konstantinos%20G.%20Derpanis%0AAbstract%3A%20%20%20Understanding%20what%20deep%20network%20models%20capture%20in%20their%20learned%0Arepresentations%20is%20a%20fundamental%20challenge%20in%20computer%20vision.%20We%20present%20a%20new%0Amethodology%20to%20understanding%20such%20vision%20models%2C%20the%20Visual%20Concept%20Connectome%0A%28VCC%29%2C%20which%20discovers%20human%20interpretable%20concepts%20and%20their%20interlayer%0Aconnections%20in%20a%20fully%20unsupervised%20manner.%20Our%20approach%20simultaneously%20reveals%0Afine-grained%20concepts%20at%20a%20layer%2C%20connection%20weightings%20across%20all%20layers%20and%0Ais%20amendable%20to%20global%20analysis%20of%20network%20structure%20%28e.g.%2C%20branching%20pattern%0Aof%20hierarchical%20concept%20assemblies%29.%20Previous%20work%20yielded%20ways%20to%20extract%0Ainterpretable%20concepts%20from%20single%20layers%20and%20examine%20their%20impact%20on%0Aclassification%2C%20but%20did%20not%20afford%20multilayer%20concept%20analysis%20across%20an%20entire%0Anetwork%20architecture.%20Quantitative%20and%20qualitative%20empirical%20results%20show%20the%0Aeffectiveness%20of%20VCCs%20in%20the%20domain%20of%20image%20classification.%20Also%2C%20we%20leverage%0AVCCs%20for%20the%20application%20of%20failure%20mode%20debugging%20to%20reveal%20where%20mistakes%0Aarise%20in%20deep%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02233v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Concept%20Connectome%20%28VCC%29%3A%20Open%20World%20Concept%20Discovery%20and%20their%0A%20%20Interlayer%20Connections%20in%20Deep%20Models&entry.906535625=Matthew%20Kowal%20and%20Richard%20P.%20Wildes%20and%20Konstantinos%20G.%20Derpanis&entry.1292438233=%20%20Understanding%20what%20deep%20network%20models%20capture%20in%20their%20learned%0Arepresentations%20is%20a%20fundamental%20challenge%20in%20computer%20vision.%20We%20present%20a%20new%0Amethodology%20to%20understanding%20such%20vision%20models%2C%20the%20Visual%20Concept%20Connectome%0A%28VCC%29%2C%20which%20discovers%20human%20interpretable%20concepts%20and%20their%20interlayer%0Aconnections%20in%20a%20fully%20unsupervised%20manner.%20Our%20approach%20simultaneously%20reveals%0Afine-grained%20concepts%20at%20a%20layer%2C%20connection%20weightings%20across%20all%20layers%20and%0Ais%20amendable%20to%20global%20analysis%20of%20network%20structure%20%28e.g.%2C%20branching%20pattern%0Aof%20hierarchical%20concept%20assemblies%29.%20Previous%20work%20yielded%20ways%20to%20extract%0Ainterpretable%20concepts%20from%20single%20layers%20and%20examine%20their%20impact%20on%0Aclassification%2C%20but%20did%20not%20afford%20multilayer%20concept%20analysis%20across%20an%20entire%0Anetwork%20architecture.%20Quantitative%20and%20qualitative%20empirical%20results%20show%20the%0Aeffectiveness%20of%20VCCs%20in%20the%20domain%20of%20image%20classification.%20Also%2C%20we%20leverage%0AVCCs%20for%20the%20application%20of%20failure%20mode%20debugging%20to%20reveal%20where%20mistakes%0Aarise%20in%20deep%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02233v2&entry.124074799=Read"},
{"title": "Local Observability of VINS and LINS", "author": "Xinran Li", "abstract": "  This work analyzes unobservable directions of Vision-aided Inertial\nNavigation System (VINS) and Lidar-aided Inertial Navigation System (LINS)\nnonlinear model. Under the assumption that there exist two features observed by\nthe camera without occlusion, the unobservable directions of VINS are uniformly\nglobally translation and global rotations about the gravity vector. The\nunobservable directions of LINS are same as VINS, while only one feature need\nto be observed. Also, a constraint in Observability-Constrained VINS (OC-VINS)\nis proved.\n", "link": "http://arxiv.org/abs/2404.00066v2", "date": "2024-04-10", "relevancy": 2.4816, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5434}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4729}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Local%20Observability%20of%20VINS%20and%20LINS&body=Title%3A%20Local%20Observability%20of%20VINS%20and%20LINS%0AAuthor%3A%20Xinran%20Li%0AAbstract%3A%20%20%20This%20work%20analyzes%20unobservable%20directions%20of%20Vision-aided%20Inertial%0ANavigation%20System%20%28VINS%29%20and%20Lidar-aided%20Inertial%20Navigation%20System%20%28LINS%29%0Anonlinear%20model.%20Under%20the%20assumption%20that%20there%20exist%20two%20features%20observed%20by%0Athe%20camera%20without%20occlusion%2C%20the%20unobservable%20directions%20of%20VINS%20are%20uniformly%0Aglobally%20translation%20and%20global%20rotations%20about%20the%20gravity%20vector.%20The%0Aunobservable%20directions%20of%20LINS%20are%20same%20as%20VINS%2C%20while%20only%20one%20feature%20need%0Ato%20be%20observed.%20Also%2C%20a%20constraint%20in%20Observability-Constrained%20VINS%20%28OC-VINS%29%0Ais%20proved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00066v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Observability%20of%20VINS%20and%20LINS&entry.906535625=Xinran%20Li&entry.1292438233=%20%20This%20work%20analyzes%20unobservable%20directions%20of%20Vision-aided%20Inertial%0ANavigation%20System%20%28VINS%29%20and%20Lidar-aided%20Inertial%20Navigation%20System%20%28LINS%29%0Anonlinear%20model.%20Under%20the%20assumption%20that%20there%20exist%20two%20features%20observed%20by%0Athe%20camera%20without%20occlusion%2C%20the%20unobservable%20directions%20of%20VINS%20are%20uniformly%0Aglobally%20translation%20and%20global%20rotations%20about%20the%20gravity%20vector.%20The%0Aunobservable%20directions%20of%20LINS%20are%20same%20as%20VINS%2C%20while%20only%20one%20feature%20need%0Ato%20be%20observed.%20Also%2C%20a%20constraint%20in%20Observability-Constrained%20VINS%20%28OC-VINS%29%0Ais%20proved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00066v2&entry.124074799=Read"},
{"title": "Equivariant Networks for Zero-Shot Coordination", "author": "Darius Muglich and Christian Schroeder de Witt and Elise van der Pol and Shimon Whiteson and Jakob Foerster", "abstract": "  Successful coordination in Dec-POMDPs requires agents to adopt robust\nstrategies and interpretable styles of play for their partner. A common failure\nmode is symmetry breaking, when agents arbitrarily converge on one out of many\nequivalent but mutually incompatible policies. Commonly these examples include\npartial observability, e.g. waving your right hand vs. left hand to convey a\ncovert message. In this paper, we present a novel equivariant network\narchitecture for use in Dec-POMDPs that effectively leverages environmental\nsymmetry for improving zero-shot coordination, doing so more effectively than\nprior methods. Our method also acts as a ``coordination-improvement operator''\nfor generic, pre-trained policies, and thus may be applied at test-time in\nconjunction with any self-play algorithm. We provide theoretical guarantees of\nour work and test on the AI benchmark task of Hanabi, where we demonstrate our\nmethods outperforming other symmetry-aware baselines in zero-shot coordination,\nas well as able to improve the coordination ability of a variety of pre-trained\npolicies. In particular, we show our method can be used to improve on the state\nof the art for zero-shot coordination on the Hanabi benchmark.\n", "link": "http://arxiv.org/abs/2210.12124v2", "date": "2024-04-10", "relevancy": 2.4046, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.493}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4715}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Networks%20for%20Zero-Shot%20Coordination&body=Title%3A%20Equivariant%20Networks%20for%20Zero-Shot%20Coordination%0AAuthor%3A%20Darius%20Muglich%20and%20Christian%20Schroeder%20de%20Witt%20and%20Elise%20van%20der%20Pol%20and%20Shimon%20Whiteson%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Successful%20coordination%20in%20Dec-POMDPs%20requires%20agents%20to%20adopt%20robust%0Astrategies%20and%20interpretable%20styles%20of%20play%20for%20their%20partner.%20A%20common%20failure%0Amode%20is%20symmetry%20breaking%2C%20when%20agents%20arbitrarily%20converge%20on%20one%20out%20of%20many%0Aequivalent%20but%20mutually%20incompatible%20policies.%20Commonly%20these%20examples%20include%0Apartial%20observability%2C%20e.g.%20waving%20your%20right%20hand%20vs.%20left%20hand%20to%20convey%20a%0Acovert%20message.%20In%20this%20paper%2C%20we%20present%20a%20novel%20equivariant%20network%0Aarchitecture%20for%20use%20in%20Dec-POMDPs%20that%20effectively%20leverages%20environmental%0Asymmetry%20for%20improving%20zero-shot%20coordination%2C%20doing%20so%20more%20effectively%20than%0Aprior%20methods.%20Our%20method%20also%20acts%20as%20a%20%60%60coordination-improvement%20operator%27%27%0Afor%20generic%2C%20pre-trained%20policies%2C%20and%20thus%20may%20be%20applied%20at%20test-time%20in%0Aconjunction%20with%20any%20self-play%20algorithm.%20We%20provide%20theoretical%20guarantees%20of%0Aour%20work%20and%20test%20on%20the%20AI%20benchmark%20task%20of%20Hanabi%2C%20where%20we%20demonstrate%20our%0Amethods%20outperforming%20other%20symmetry-aware%20baselines%20in%20zero-shot%20coordination%2C%0Aas%20well%20as%20able%20to%20improve%20the%20coordination%20ability%20of%20a%20variety%20of%20pre-trained%0Apolicies.%20In%20particular%2C%20we%20show%20our%20method%20can%20be%20used%20to%20improve%20on%20the%20state%0Aof%20the%20art%20for%20zero-shot%20coordination%20on%20the%20Hanabi%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.12124v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Networks%20for%20Zero-Shot%20Coordination&entry.906535625=Darius%20Muglich%20and%20Christian%20Schroeder%20de%20Witt%20and%20Elise%20van%20der%20Pol%20and%20Shimon%20Whiteson%20and%20Jakob%20Foerster&entry.1292438233=%20%20Successful%20coordination%20in%20Dec-POMDPs%20requires%20agents%20to%20adopt%20robust%0Astrategies%20and%20interpretable%20styles%20of%20play%20for%20their%20partner.%20A%20common%20failure%0Amode%20is%20symmetry%20breaking%2C%20when%20agents%20arbitrarily%20converge%20on%20one%20out%20of%20many%0Aequivalent%20but%20mutually%20incompatible%20policies.%20Commonly%20these%20examples%20include%0Apartial%20observability%2C%20e.g.%20waving%20your%20right%20hand%20vs.%20left%20hand%20to%20convey%20a%0Acovert%20message.%20In%20this%20paper%2C%20we%20present%20a%20novel%20equivariant%20network%0Aarchitecture%20for%20use%20in%20Dec-POMDPs%20that%20effectively%20leverages%20environmental%0Asymmetry%20for%20improving%20zero-shot%20coordination%2C%20doing%20so%20more%20effectively%20than%0Aprior%20methods.%20Our%20method%20also%20acts%20as%20a%20%60%60coordination-improvement%20operator%27%27%0Afor%20generic%2C%20pre-trained%20policies%2C%20and%20thus%20may%20be%20applied%20at%20test-time%20in%0Aconjunction%20with%20any%20self-play%20algorithm.%20We%20provide%20theoretical%20guarantees%20of%0Aour%20work%20and%20test%20on%20the%20AI%20benchmark%20task%20of%20Hanabi%2C%20where%20we%20demonstrate%20our%0Amethods%20outperforming%20other%20symmetry-aware%20baselines%20in%20zero-shot%20coordination%2C%0Aas%20well%20as%20able%20to%20improve%20the%20coordination%20ability%20of%20a%20variety%20of%20pre-trained%0Apolicies.%20In%20particular%2C%20we%20show%20our%20method%20can%20be%20used%20to%20improve%20on%20the%20state%0Aof%20the%20art%20for%20zero-shot%20coordination%20on%20the%20Hanabi%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.12124v2&entry.124074799=Read"},
{"title": "Dynamic Generation of Personalities with Large Language Models", "author": "Jianzhi Liu and Hexiang Gu and Tianyu Zheng and Liuyu Xiang and Huijia Wu and Jie Fu and Zhaofeng He", "abstract": "  In the realm of mimicking human deliberation, large language models (LLMs)\nshow promising performance, thereby amplifying the importance of this research\narea. Deliberation is influenced by both logic and personality. However,\nprevious studies predominantly focused on the logic of LLMs, neglecting the\nexploration of personality aspects. In this work, we introduce Dynamic\nPersonality Generation (DPG), a dynamic personality generation method based on\nHypernetworks. Initially, we embed the Big Five personality theory into GPT-4\nto form a personality assessment machine, enabling it to evaluate characters'\npersonality traits from dialogues automatically. We propose a new metric to\nassess personality generation capability based on this evaluation method. Then,\nwe use this personality assessment machine to evaluate dialogues in script\ndata, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on\nthe personality-dialogue dataset. Experiments prove that DPG's personality\ngeneration capability is stronger after fine-tuning on this dataset than\ntraditional fine-tuning methods, surpassing prompt-based GPT-4.\n", "link": "http://arxiv.org/abs/2404.07084v1", "date": "2024-04-10", "relevancy": 2.3977, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5144}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4644}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4598}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Generation%20of%20Personalities%20with%20Large%20Language%20Models&body=Title%3A%20Dynamic%20Generation%20of%20Personalities%20with%20Large%20Language%20Models%0AAuthor%3A%20Jianzhi%20Liu%20and%20Hexiang%20Gu%20and%20Tianyu%20Zheng%20and%20Liuyu%20Xiang%20and%20Huijia%20Wu%20and%20Jie%20Fu%20and%20Zhaofeng%20He%0AAbstract%3A%20%20%20In%20the%20realm%20of%20mimicking%20human%20deliberation%2C%20large%20language%20models%20%28LLMs%29%0Ashow%20promising%20performance%2C%20thereby%20amplifying%20the%20importance%20of%20this%20research%0Aarea.%20Deliberation%20is%20influenced%20by%20both%20logic%20and%20personality.%20However%2C%0Aprevious%20studies%20predominantly%20focused%20on%20the%20logic%20of%20LLMs%2C%20neglecting%20the%0Aexploration%20of%20personality%20aspects.%20In%20this%20work%2C%20we%20introduce%20Dynamic%0APersonality%20Generation%20%28DPG%29%2C%20a%20dynamic%20personality%20generation%20method%20based%20on%0AHypernetworks.%20Initially%2C%20we%20embed%20the%20Big%20Five%20personality%20theory%20into%20GPT-4%0Ato%20form%20a%20personality%20assessment%20machine%2C%20enabling%20it%20to%20evaluate%20characters%27%0Apersonality%20traits%20from%20dialogues%20automatically.%20We%20propose%20a%20new%20metric%20to%0Aassess%20personality%20generation%20capability%20based%20on%20this%20evaluation%20method.%20Then%2C%0Awe%20use%20this%20personality%20assessment%20machine%20to%20evaluate%20dialogues%20in%20script%0Adata%2C%20resulting%20in%20a%20personality-dialogue%20dataset.%20Finally%2C%20we%20fine-tune%20DPG%20on%0Athe%20personality-dialogue%20dataset.%20Experiments%20prove%20that%20DPG%27s%20personality%0Ageneration%20capability%20is%20stronger%20after%20fine-tuning%20on%20this%20dataset%20than%0Atraditional%20fine-tuning%20methods%2C%20surpassing%20prompt-based%20GPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07084v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Generation%20of%20Personalities%20with%20Large%20Language%20Models&entry.906535625=Jianzhi%20Liu%20and%20Hexiang%20Gu%20and%20Tianyu%20Zheng%20and%20Liuyu%20Xiang%20and%20Huijia%20Wu%20and%20Jie%20Fu%20and%20Zhaofeng%20He&entry.1292438233=%20%20In%20the%20realm%20of%20mimicking%20human%20deliberation%2C%20large%20language%20models%20%28LLMs%29%0Ashow%20promising%20performance%2C%20thereby%20amplifying%20the%20importance%20of%20this%20research%0Aarea.%20Deliberation%20is%20influenced%20by%20both%20logic%20and%20personality.%20However%2C%0Aprevious%20studies%20predominantly%20focused%20on%20the%20logic%20of%20LLMs%2C%20neglecting%20the%0Aexploration%20of%20personality%20aspects.%20In%20this%20work%2C%20we%20introduce%20Dynamic%0APersonality%20Generation%20%28DPG%29%2C%20a%20dynamic%20personality%20generation%20method%20based%20on%0AHypernetworks.%20Initially%2C%20we%20embed%20the%20Big%20Five%20personality%20theory%20into%20GPT-4%0Ato%20form%20a%20personality%20assessment%20machine%2C%20enabling%20it%20to%20evaluate%20characters%27%0Apersonality%20traits%20from%20dialogues%20automatically.%20We%20propose%20a%20new%20metric%20to%0Aassess%20personality%20generation%20capability%20based%20on%20this%20evaluation%20method.%20Then%2C%0Awe%20use%20this%20personality%20assessment%20machine%20to%20evaluate%20dialogues%20in%20script%0Adata%2C%20resulting%20in%20a%20personality-dialogue%20dataset.%20Finally%2C%20we%20fine-tune%20DPG%20on%0Athe%20personality-dialogue%20dataset.%20Experiments%20prove%20that%20DPG%27s%20personality%0Ageneration%20capability%20is%20stronger%20after%20fine-tuning%20on%20this%20dataset%20than%0Atraditional%20fine-tuning%20methods%2C%20surpassing%20prompt-based%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07084v1&entry.124074799=Read"},
{"title": "VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes\n  Enhance Protein Binding Site Identification", "author": "Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G\u00fcnter Klambauer", "abstract": "  Being able to identify regions within or around proteins, to which ligands\ncan potentially bind, is an essential step to develop new drugs. Binding site\nidentification methods can now profit from the availability of large amounts of\n3D structures in protein structure databases or from AlphaFold predictions.\nCurrent binding site identification methods heavily rely on graph neural\nnetworks (GNNs), usually designed to output E(3)-equivariant predictions. Such\nmethods turned out to be very beneficial for physics-related tasks like binding\nenergy or motion trajectory prediction. However, the performance of GNNs at\nbinding site identification is still limited potentially due to the lack of\ndedicated nodes that model hidden geometric entities, such as binding pockets.\nIn this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by\nadding virtual nodes and applying an extended message passing scheme. The\nvirtual nodes in these graphs are dedicated quantities to learn representations\nof binding sites, which leads to improved predictive performance. In our\nexperiments, we show that our proposed method VN-EGNN sets a new\nstate-of-the-art at locating binding site centers on COACH420, HOLO4K and\nPDBbind2020.\n", "link": "http://arxiv.org/abs/2404.07194v1", "date": "2024-04-10", "relevancy": 2.3872, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.501}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4671}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4642}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VN-EGNN%3A%20E%283%29-Equivariant%20Graph%20Neural%20Networks%20with%20Virtual%20Nodes%0A%20%20Enhance%20Protein%20Binding%20Site%20Identification&body=Title%3A%20VN-EGNN%3A%20E%283%29-Equivariant%20Graph%20Neural%20Networks%20with%20Virtual%20Nodes%0A%20%20Enhance%20Protein%20Binding%20Site%20Identification%0AAuthor%3A%20Florian%20Sestak%20and%20Lisa%20Schneckenreiter%20and%20Johannes%20Brandstetter%20and%20Sepp%20Hochreiter%20and%20Andreas%20Mayr%20and%20G%C3%BCnter%20Klambauer%0AAbstract%3A%20%20%20Being%20able%20to%20identify%20regions%20within%20or%20around%20proteins%2C%20to%20which%20ligands%0Acan%20potentially%20bind%2C%20is%20an%20essential%20step%20to%20develop%20new%20drugs.%20Binding%20site%0Aidentification%20methods%20can%20now%20profit%20from%20the%20availability%20of%20large%20amounts%20of%0A3D%20structures%20in%20protein%20structure%20databases%20or%20from%20AlphaFold%20predictions.%0ACurrent%20binding%20site%20identification%20methods%20heavily%20rely%20on%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20usually%20designed%20to%20output%20E%283%29-equivariant%20predictions.%20Such%0Amethods%20turned%20out%20to%20be%20very%20beneficial%20for%20physics-related%20tasks%20like%20binding%0Aenergy%20or%20motion%20trajectory%20prediction.%20However%2C%20the%20performance%20of%20GNNs%20at%0Abinding%20site%20identification%20is%20still%20limited%20potentially%20due%20to%20the%20lack%20of%0Adedicated%20nodes%20that%20model%20hidden%20geometric%20entities%2C%20such%20as%20binding%20pockets.%0AIn%20this%20work%2C%20we%20extend%20E%28n%29-Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20by%0Aadding%20virtual%20nodes%20and%20applying%20an%20extended%20message%20passing%20scheme.%20The%0Avirtual%20nodes%20in%20these%20graphs%20are%20dedicated%20quantities%20to%20learn%20representations%0Aof%20binding%20sites%2C%20which%20leads%20to%20improved%20predictive%20performance.%20In%20our%0Aexperiments%2C%20we%20show%20that%20our%20proposed%20method%20VN-EGNN%20sets%20a%20new%0Astate-of-the-art%20at%20locating%20binding%20site%20centers%20on%20COACH420%2C%20HOLO4K%20and%0APDBbind2020.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07194v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VN-EGNN%3A%20E%283%29-Equivariant%20Graph%20Neural%20Networks%20with%20Virtual%20Nodes%0A%20%20Enhance%20Protein%20Binding%20Site%20Identification&entry.906535625=Florian%20Sestak%20and%20Lisa%20Schneckenreiter%20and%20Johannes%20Brandstetter%20and%20Sepp%20Hochreiter%20and%20Andreas%20Mayr%20and%20G%C3%BCnter%20Klambauer&entry.1292438233=%20%20Being%20able%20to%20identify%20regions%20within%20or%20around%20proteins%2C%20to%20which%20ligands%0Acan%20potentially%20bind%2C%20is%20an%20essential%20step%20to%20develop%20new%20drugs.%20Binding%20site%0Aidentification%20methods%20can%20now%20profit%20from%20the%20availability%20of%20large%20amounts%20of%0A3D%20structures%20in%20protein%20structure%20databases%20or%20from%20AlphaFold%20predictions.%0ACurrent%20binding%20site%20identification%20methods%20heavily%20rely%20on%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20usually%20designed%20to%20output%20E%283%29-equivariant%20predictions.%20Such%0Amethods%20turned%20out%20to%20be%20very%20beneficial%20for%20physics-related%20tasks%20like%20binding%0Aenergy%20or%20motion%20trajectory%20prediction.%20However%2C%20the%20performance%20of%20GNNs%20at%0Abinding%20site%20identification%20is%20still%20limited%20potentially%20due%20to%20the%20lack%20of%0Adedicated%20nodes%20that%20model%20hidden%20geometric%20entities%2C%20such%20as%20binding%20pockets.%0AIn%20this%20work%2C%20we%20extend%20E%28n%29-Equivariant%20Graph%20Neural%20Networks%20%28EGNNs%29%20by%0Aadding%20virtual%20nodes%20and%20applying%20an%20extended%20message%20passing%20scheme.%20The%0Avirtual%20nodes%20in%20these%20graphs%20are%20dedicated%20quantities%20to%20learn%20representations%0Aof%20binding%20sites%2C%20which%20leads%20to%20improved%20predictive%20performance.%20In%20our%0Aexperiments%2C%20we%20show%20that%20our%20proposed%20method%20VN-EGNN%20sets%20a%20new%0Astate-of-the-art%20at%20locating%20binding%20site%20centers%20on%20COACH420%2C%20HOLO4K%20and%0APDBbind2020.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07194v1&entry.124074799=Read"},
{"title": "GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on\n  FPGA", "author": "Bingyi Zhang and Rajgopal Kannan and Carl Busart and Viktor Prasanna", "abstract": "  Graph neural networks (GNNs) have recently empowered various novel computer\nvision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN\nlayers or only GNN layers are employed. This paper introduces GCV-Turbo, a\ndomain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV\ntasks. GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware\narchitecture optimized for the computation kernels in both CNNs and GNNs using\nthe same set of computation resources. (2) a PyTorch-compatible compiler that\ntakes a user-defined model as input, performs end-to-end optimization for the\ncomputation graph of a given GNN-based CV task, and produces optimized code for\nhardware execution. The hardware architecture and the compiler work\nsynergistically to support a variety of GNN-based CV tasks. We implement\nGCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six\nrepresentative GNN-based CV tasks with diverse input data modalities (e.g.,\nimage, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU)\nimplementations, GCV-Turbo achieves an average latency reduction of\n$68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo\nsupports the execution of the standalone CNNs or GNNs, achieving performance\ncomparable to that of state-of-the-art CNN (GNN) accelerators for widely used\nCNN-only (GNN-only) models.\n", "link": "http://arxiv.org/abs/2404.07188v1", "date": "2024-04-10", "relevancy": 2.386, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4859}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4676}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GCV-Turbo%3A%20End-to-end%20Acceleration%20of%20GNN-based%20Computer%20Vision%20Tasks%20on%0A%20%20FPGA&body=Title%3A%20GCV-Turbo%3A%20End-to-end%20Acceleration%20of%20GNN-based%20Computer%20Vision%20Tasks%20on%0A%20%20FPGA%0AAuthor%3A%20Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Carl%20Busart%20and%20Viktor%20Prasanna%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20recently%20empowered%20various%20novel%20computer%0Avision%20%28CV%29%20tasks.%20In%20GNN-based%20CV%20tasks%2C%20a%20combination%20of%20CNN%20layers%20and%20GNN%0Alayers%20or%20only%20GNN%20layers%20are%20employed.%20This%20paper%20introduces%20GCV-Turbo%2C%20a%0Adomain-specific%20accelerator%20on%20FPGA%20for%20end-to-end%20acceleration%20of%20GNN-based%20CV%0Atasks.%20GCV-Turbo%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20%5Cemph%7Bnovel%7D%20hardware%0Aarchitecture%20optimized%20for%20the%20computation%20kernels%20in%20both%20CNNs%20and%20GNNs%20using%0Athe%20same%20set%20of%20computation%20resources.%20%282%29%20a%20PyTorch-compatible%20compiler%20that%0Atakes%20a%20user-defined%20model%20as%20input%2C%20performs%20end-to-end%20optimization%20for%20the%0Acomputation%20graph%20of%20a%20given%20GNN-based%20CV%20task%2C%20and%20produces%20optimized%20code%20for%0Ahardware%20execution.%20The%20hardware%20architecture%20and%20the%20compiler%20work%0Asynergistically%20to%20support%20a%20variety%20of%20GNN-based%20CV%20tasks.%20We%20implement%0AGCV-Turbo%20on%20a%20state-of-the-art%20FPGA%20and%20evaluate%20its%20performance%20across%20six%0Arepresentative%20GNN-based%20CV%20tasks%20with%20diverse%20input%20data%20modalities%20%28e.g.%2C%0Aimage%2C%20human%20skeleton%2C%20point%20cloud%29.%20Compared%20with%20state-of-the-art%20CPU%20%28GPU%29%0Aimplementations%2C%20GCV-Turbo%20achieves%20an%20average%20latency%20reduction%20of%0A%2468.4%5Ctimes%24%20%28%244.1%5Ctimes%24%29%20on%20these%20six%20GNN-based%20CV%20tasks.%20Moreover%2C%20GCV-Turbo%0Asupports%20the%20execution%20of%20the%20standalone%20CNNs%20or%20GNNs%2C%20achieving%20performance%0Acomparable%20to%20that%20of%20state-of-the-art%20CNN%20%28GNN%29%20accelerators%20for%20widely%20used%0ACNN-only%20%28GNN-only%29%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07188v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCV-Turbo%3A%20End-to-end%20Acceleration%20of%20GNN-based%20Computer%20Vision%20Tasks%20on%0A%20%20FPGA&entry.906535625=Bingyi%20Zhang%20and%20Rajgopal%20Kannan%20and%20Carl%20Busart%20and%20Viktor%20Prasanna&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20recently%20empowered%20various%20novel%20computer%0Avision%20%28CV%29%20tasks.%20In%20GNN-based%20CV%20tasks%2C%20a%20combination%20of%20CNN%20layers%20and%20GNN%0Alayers%20or%20only%20GNN%20layers%20are%20employed.%20This%20paper%20introduces%20GCV-Turbo%2C%20a%0Adomain-specific%20accelerator%20on%20FPGA%20for%20end-to-end%20acceleration%20of%20GNN-based%20CV%0Atasks.%20GCV-Turbo%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20%5Cemph%7Bnovel%7D%20hardware%0Aarchitecture%20optimized%20for%20the%20computation%20kernels%20in%20both%20CNNs%20and%20GNNs%20using%0Athe%20same%20set%20of%20computation%20resources.%20%282%29%20a%20PyTorch-compatible%20compiler%20that%0Atakes%20a%20user-defined%20model%20as%20input%2C%20performs%20end-to-end%20optimization%20for%20the%0Acomputation%20graph%20of%20a%20given%20GNN-based%20CV%20task%2C%20and%20produces%20optimized%20code%20for%0Ahardware%20execution.%20The%20hardware%20architecture%20and%20the%20compiler%20work%0Asynergistically%20to%20support%20a%20variety%20of%20GNN-based%20CV%20tasks.%20We%20implement%0AGCV-Turbo%20on%20a%20state-of-the-art%20FPGA%20and%20evaluate%20its%20performance%20across%20six%0Arepresentative%20GNN-based%20CV%20tasks%20with%20diverse%20input%20data%20modalities%20%28e.g.%2C%0Aimage%2C%20human%20skeleton%2C%20point%20cloud%29.%20Compared%20with%20state-of-the-art%20CPU%20%28GPU%29%0Aimplementations%2C%20GCV-Turbo%20achieves%20an%20average%20latency%20reduction%20of%0A%2468.4%5Ctimes%24%20%28%244.1%5Ctimes%24%29%20on%20these%20six%20GNN-based%20CV%20tasks.%20Moreover%2C%20GCV-Turbo%0Asupports%20the%20execution%20of%20the%20standalone%20CNNs%20or%20GNNs%2C%20achieving%20performance%0Acomparable%20to%20that%20of%20state-of-the-art%20CNN%20%28GNN%29%20accelerators%20for%20widely%20used%0ACNN-only%20%28GNN-only%29%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07188v1&entry.124074799=Read"},
{"title": "DG-TTA: Out-of-domain medical image segmentation through Domain\n  Generalization and Test-Time Adaptation", "author": "Christian Weihsbach and Christian N. Kruse and Alexander Bigalke and Mattias P. Heinrich", "abstract": "  Applying pre-trained medical segmentation models on out-of-domain images\noften yields predictions of insufficient quality. Several strategies have been\nproposed to maintain model performance, such as finetuning or unsupervised- and\nsource-free domain adaptation. These strategies set restrictive requirements\nfor data availability. In this study, we propose to combine domain\ngeneralization and test-time adaptation to create a highly effective approach\nfor reusing pre-trained models in unseen target domains. Domain-generalized\npre-training on source data is used to obtain the best initial performance in\nthe target domain. We introduce the MIND descriptor previously used in image\nregistration tasks as a further technique to achieve generalization and present\nsuperior performance for small-scale datasets compared to existing approaches.\nAt test-time, high-quality segmentation for every single unseen scan is ensured\nby optimizing the model weights for consistency given different image\naugmentations. That way, our method enables separate use of source and target\ndata and thus removes current data availability barriers. Moreover, the\npresented method is highly modular as it does not require specific model\narchitectures or prior knowledge of involved domains and labels. We demonstrate\nthis by integrating it into the nnUNet, which is currently the most popular and\naccurate framework for medical image segmentation. We employ multiple datasets\ncovering abdominal, cardiac, and lumbar spine scans and compose several\nout-of-domain scenarios in this study. We demonstrate that our method, combined\nwith pre-trained whole-body CT models, can effectively segment MR images with\nhigh accuracy in all of the aforementioned scenarios. Open-source code can be\nfound here: https://github.com/multimodallearning/DG-TTA\n", "link": "http://arxiv.org/abs/2312.06275v3", "date": "2024-04-10", "relevancy": 2.3196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6142}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5624}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DG-TTA%3A%20Out-of-domain%20medical%20image%20segmentation%20through%20Domain%0A%20%20Generalization%20and%20Test-Time%20Adaptation&body=Title%3A%20DG-TTA%3A%20Out-of-domain%20medical%20image%20segmentation%20through%20Domain%0A%20%20Generalization%20and%20Test-Time%20Adaptation%0AAuthor%3A%20Christian%20Weihsbach%20and%20Christian%20N.%20Kruse%20and%20Alexander%20Bigalke%20and%20Mattias%20P.%20Heinrich%0AAbstract%3A%20%20%20Applying%20pre-trained%20medical%20segmentation%20models%20on%20out-of-domain%20images%0Aoften%20yields%20predictions%20of%20insufficient%20quality.%20Several%20strategies%20have%20been%0Aproposed%20to%20maintain%20model%20performance%2C%20such%20as%20finetuning%20or%20unsupervised-%20and%0Asource-free%20domain%20adaptation.%20These%20strategies%20set%20restrictive%20requirements%0Afor%20data%20availability.%20In%20this%20study%2C%20we%20propose%20to%20combine%20domain%0Ageneralization%20and%20test-time%20adaptation%20to%20create%20a%20highly%20effective%20approach%0Afor%20reusing%20pre-trained%20models%20in%20unseen%20target%20domains.%20Domain-generalized%0Apre-training%20on%20source%20data%20is%20used%20to%20obtain%20the%20best%20initial%20performance%20in%0Athe%20target%20domain.%20We%20introduce%20the%20MIND%20descriptor%20previously%20used%20in%20image%0Aregistration%20tasks%20as%20a%20further%20technique%20to%20achieve%20generalization%20and%20present%0Asuperior%20performance%20for%20small-scale%20datasets%20compared%20to%20existing%20approaches.%0AAt%20test-time%2C%20high-quality%20segmentation%20for%20every%20single%20unseen%20scan%20is%20ensured%0Aby%20optimizing%20the%20model%20weights%20for%20consistency%20given%20different%20image%0Aaugmentations.%20That%20way%2C%20our%20method%20enables%20separate%20use%20of%20source%20and%20target%0Adata%20and%20thus%20removes%20current%20data%20availability%20barriers.%20Moreover%2C%20the%0Apresented%20method%20is%20highly%20modular%20as%20it%20does%20not%20require%20specific%20model%0Aarchitectures%20or%20prior%20knowledge%20of%20involved%20domains%20and%20labels.%20We%20demonstrate%0Athis%20by%20integrating%20it%20into%20the%20nnUNet%2C%20which%20is%20currently%20the%20most%20popular%20and%0Aaccurate%20framework%20for%20medical%20image%20segmentation.%20We%20employ%20multiple%20datasets%0Acovering%20abdominal%2C%20cardiac%2C%20and%20lumbar%20spine%20scans%20and%20compose%20several%0Aout-of-domain%20scenarios%20in%20this%20study.%20We%20demonstrate%20that%20our%20method%2C%20combined%0Awith%20pre-trained%20whole-body%20CT%20models%2C%20can%20effectively%20segment%20MR%20images%20with%0Ahigh%20accuracy%20in%20all%20of%20the%20aforementioned%20scenarios.%20Open-source%20code%20can%20be%0Afound%20here%3A%20https%3A//github.com/multimodallearning/DG-TTA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06275v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DG-TTA%3A%20Out-of-domain%20medical%20image%20segmentation%20through%20Domain%0A%20%20Generalization%20and%20Test-Time%20Adaptation&entry.906535625=Christian%20Weihsbach%20and%20Christian%20N.%20Kruse%20and%20Alexander%20Bigalke%20and%20Mattias%20P.%20Heinrich&entry.1292438233=%20%20Applying%20pre-trained%20medical%20segmentation%20models%20on%20out-of-domain%20images%0Aoften%20yields%20predictions%20of%20insufficient%20quality.%20Several%20strategies%20have%20been%0Aproposed%20to%20maintain%20model%20performance%2C%20such%20as%20finetuning%20or%20unsupervised-%20and%0Asource-free%20domain%20adaptation.%20These%20strategies%20set%20restrictive%20requirements%0Afor%20data%20availability.%20In%20this%20study%2C%20we%20propose%20to%20combine%20domain%0Ageneralization%20and%20test-time%20adaptation%20to%20create%20a%20highly%20effective%20approach%0Afor%20reusing%20pre-trained%20models%20in%20unseen%20target%20domains.%20Domain-generalized%0Apre-training%20on%20source%20data%20is%20used%20to%20obtain%20the%20best%20initial%20performance%20in%0Athe%20target%20domain.%20We%20introduce%20the%20MIND%20descriptor%20previously%20used%20in%20image%0Aregistration%20tasks%20as%20a%20further%20technique%20to%20achieve%20generalization%20and%20present%0Asuperior%20performance%20for%20small-scale%20datasets%20compared%20to%20existing%20approaches.%0AAt%20test-time%2C%20high-quality%20segmentation%20for%20every%20single%20unseen%20scan%20is%20ensured%0Aby%20optimizing%20the%20model%20weights%20for%20consistency%20given%20different%20image%0Aaugmentations.%20That%20way%2C%20our%20method%20enables%20separate%20use%20of%20source%20and%20target%0Adata%20and%20thus%20removes%20current%20data%20availability%20barriers.%20Moreover%2C%20the%0Apresented%20method%20is%20highly%20modular%20as%20it%20does%20not%20require%20specific%20model%0Aarchitectures%20or%20prior%20knowledge%20of%20involved%20domains%20and%20labels.%20We%20demonstrate%0Athis%20by%20integrating%20it%20into%20the%20nnUNet%2C%20which%20is%20currently%20the%20most%20popular%20and%0Aaccurate%20framework%20for%20medical%20image%20segmentation.%20We%20employ%20multiple%20datasets%0Acovering%20abdominal%2C%20cardiac%2C%20and%20lumbar%20spine%20scans%20and%20compose%20several%0Aout-of-domain%20scenarios%20in%20this%20study.%20We%20demonstrate%20that%20our%20method%2C%20combined%0Awith%20pre-trained%20whole-body%20CT%20models%2C%20can%20effectively%20segment%20MR%20images%20with%0Ahigh%20accuracy%20in%20all%20of%20the%20aforementioned%20scenarios.%20Open-source%20code%20can%20be%0Afound%20here%3A%20https%3A//github.com/multimodallearning/DG-TTA%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06275v3&entry.124074799=Read"},
{"title": "Toward a Better Understanding of Fourier Neural Operators: Analysis and\n  Improvement from a Spectral Perspective", "author": "Shaoxiang Qin and Fuyuan Lyu and Wenhui Peng and Dingyang Geng and Ju Wang and Naiping Gao and Xue Liu and Liangzhu Leon Wang", "abstract": "  In solving partial differential equations (PDEs), Fourier Neural Operators\n(FNOs) have exhibited notable effectiveness compared to Convolutional Neural\nNetworks (CNNs). This paper presents clear empirical evidence through spectral\nanalysis to elucidate the superiority of FNO over CNNs: FNO is significantly\nmore capable of learning low-frequencies. This empirical evidence also unveils\nFNO's distinct low-frequency bias, which limits FNO's effectiveness in learning\nhigh-frequency information from PDE data. To tackle this challenge, we\nintroduce SpecBoost, an ensemble learning framework that employs multiple FNOs\nto better capture high-frequency information. Specifically, a secondary FNO is\nutilized to learn the overlooked high-frequency information from the prediction\nresidual of the initial FNO. Experiments demonstrate that SpecBoost noticeably\nenhances FNO's prediction accuracy on diverse PDE applications, achieving an up\nto 71% improvement.\n", "link": "http://arxiv.org/abs/2404.07200v1", "date": "2024-04-10", "relevancy": 2.3071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4753}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4731}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4359}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20a%20Better%20Understanding%20of%20Fourier%20Neural%20Operators%3A%20Analysis%20and%0A%20%20Improvement%20from%20a%20Spectral%20Perspective&body=Title%3A%20Toward%20a%20Better%20Understanding%20of%20Fourier%20Neural%20Operators%3A%20Analysis%20and%0A%20%20Improvement%20from%20a%20Spectral%20Perspective%0AAuthor%3A%20Shaoxiang%20Qin%20and%20Fuyuan%20Lyu%20and%20Wenhui%20Peng%20and%20Dingyang%20Geng%20and%20Ju%20Wang%20and%20Naiping%20Gao%20and%20Xue%20Liu%20and%20Liangzhu%20Leon%20Wang%0AAbstract%3A%20%20%20In%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20Fourier%20Neural%20Operators%0A%28FNOs%29%20have%20exhibited%20notable%20effectiveness%20compared%20to%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20This%20paper%20presents%20clear%20empirical%20evidence%20through%20spectral%0Aanalysis%20to%20elucidate%20the%20superiority%20of%20FNO%20over%20CNNs%3A%20FNO%20is%20significantly%0Amore%20capable%20of%20learning%20low-frequencies.%20This%20empirical%20evidence%20also%20unveils%0AFNO%27s%20distinct%20low-frequency%20bias%2C%20which%20limits%20FNO%27s%20effectiveness%20in%20learning%0Ahigh-frequency%20information%20from%20PDE%20data.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20SpecBoost%2C%20an%20ensemble%20learning%20framework%20that%20employs%20multiple%20FNOs%0Ato%20better%20capture%20high-frequency%20information.%20Specifically%2C%20a%20secondary%20FNO%20is%0Autilized%20to%20learn%20the%20overlooked%20high-frequency%20information%20from%20the%20prediction%0Aresidual%20of%20the%20initial%20FNO.%20Experiments%20demonstrate%20that%20SpecBoost%20noticeably%0Aenhances%20FNO%27s%20prediction%20accuracy%20on%20diverse%20PDE%20applications%2C%20achieving%20an%20up%0Ato%2071%25%20improvement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07200v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20a%20Better%20Understanding%20of%20Fourier%20Neural%20Operators%3A%20Analysis%20and%0A%20%20Improvement%20from%20a%20Spectral%20Perspective&entry.906535625=Shaoxiang%20Qin%20and%20Fuyuan%20Lyu%20and%20Wenhui%20Peng%20and%20Dingyang%20Geng%20and%20Ju%20Wang%20and%20Naiping%20Gao%20and%20Xue%20Liu%20and%20Liangzhu%20Leon%20Wang&entry.1292438233=%20%20In%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20Fourier%20Neural%20Operators%0A%28FNOs%29%20have%20exhibited%20notable%20effectiveness%20compared%20to%20Convolutional%20Neural%0ANetworks%20%28CNNs%29.%20This%20paper%20presents%20clear%20empirical%20evidence%20through%20spectral%0Aanalysis%20to%20elucidate%20the%20superiority%20of%20FNO%20over%20CNNs%3A%20FNO%20is%20significantly%0Amore%20capable%20of%20learning%20low-frequencies.%20This%20empirical%20evidence%20also%20unveils%0AFNO%27s%20distinct%20low-frequency%20bias%2C%20which%20limits%20FNO%27s%20effectiveness%20in%20learning%0Ahigh-frequency%20information%20from%20PDE%20data.%20To%20tackle%20this%20challenge%2C%20we%0Aintroduce%20SpecBoost%2C%20an%20ensemble%20learning%20framework%20that%20employs%20multiple%20FNOs%0Ato%20better%20capture%20high-frequency%20information.%20Specifically%2C%20a%20secondary%20FNO%20is%0Autilized%20to%20learn%20the%20overlooked%20high-frequency%20information%20from%20the%20prediction%0Aresidual%20of%20the%20initial%20FNO.%20Experiments%20demonstrate%20that%20SpecBoost%20noticeably%0Aenhances%20FNO%27s%20prediction%20accuracy%20on%20diverse%20PDE%20applications%2C%20achieving%20an%20up%0Ato%2071%25%20improvement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07200v1&entry.124074799=Read"},
{"title": "Gemma: Open Models Based on Gemini Research and Technology", "author": " Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and L\u00e9onard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am\u00e9lie H\u00e9liou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl\u00e9ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Miku\u0142a and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl\u00e9ment Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy", "abstract": "  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n", "link": "http://arxiv.org/abs/2403.08295v2", "date": "2024-04-10", "relevancy": 2.3049, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4649}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&body=Title%3A%20Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology%0AAuthor%3A%20%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%0AAbstract%3A%20%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08295v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemma%3A%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology&entry.906535625=%20Gemma%20Team%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20L%C3%A9onard%20Hussenot%20and%20Pier%20Giuseppe%20Sessa%20and%20Aakanksha%20Chowdhery%20and%20Adam%20Roberts%20and%20Aditya%20Barua%20and%20Alex%20Botev%20and%20Alex%20Castro-Ros%20and%20Ambrose%20Slone%20and%20Am%C3%A9lie%20H%C3%A9liou%20and%20Andrea%20Tacchetti%20and%20Anna%20Bulanova%20and%20Antonia%20Paterson%20and%20Beth%20Tsai%20and%20Bobak%20Shahriari%20and%20Charline%20Le%20Lan%20and%20Christopher%20A.%20Choquette-Choo%20and%20Cl%C3%A9ment%20Crepy%20and%20Daniel%20Cer%20and%20Daphne%20Ippolito%20and%20David%20Reid%20and%20Elena%20Buchatskaya%20and%20Eric%20Ni%20and%20Eric%20Noland%20and%20Geng%20Yan%20and%20George%20Tucker%20and%20George-Christian%20Muraru%20and%20Grigory%20Rozhdestvenskiy%20and%20Henryk%20Michalewski%20and%20Ian%20Tenney%20and%20Ivan%20Grishchenko%20and%20Jacob%20Austin%20and%20James%20Keeling%20and%20Jane%20Labanowski%20and%20Jean-Baptiste%20Lespiau%20and%20Jeff%20Stanway%20and%20Jenny%20Brennan%20and%20Jeremy%20Chen%20and%20Johan%20Ferret%20and%20Justin%20Chiu%20and%20Justin%20Mao-Jones%20and%20Katherine%20Lee%20and%20Kathy%20Yu%20and%20Katie%20Millican%20and%20Lars%20Lowe%20Sjoesund%20and%20Lisa%20Lee%20and%20Lucas%20Dixon%20and%20Machel%20Reid%20and%20Maciej%20Miku%C5%82a%20and%20Mateo%20Wirth%20and%20Michael%20Sharman%20and%20Nikolai%20Chinaev%20and%20Nithum%20Thain%20and%20Olivier%20Bachem%20and%20Oscar%20Chang%20and%20Oscar%20Wahltinez%20and%20Paige%20Bailey%20and%20Paul%20Michel%20and%20Petko%20Yotov%20and%20Rahma%20Chaabouni%20and%20Ramona%20Comanescu%20and%20Reena%20Jana%20and%20Rohan%20Anil%20and%20Ross%20McIlroy%20and%20Ruibo%20Liu%20and%20Ryan%20Mullins%20and%20Samuel%20L%20Smith%20and%20Sebastian%20Borgeaud%20and%20Sertan%20Girgin%20and%20Sholto%20Douglas%20and%20Shree%20Pandya%20and%20Siamak%20Shakeri%20and%20Soham%20De%20and%20Ted%20Klimenko%20and%20Tom%20Hennigan%20and%20Vlad%20Feinberg%20and%20Wojciech%20Stokowiec%20and%20Yu-hui%20Chen%20and%20Zafarali%20Ahmed%20and%20Zhitao%20Gong%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Cl%C3%A9ment%20Farabet%20and%20Oriol%20Vinyals%20and%20Jeff%20Dean%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Zoubin%20Ghahramani%20and%20Douglas%20Eck%20and%20Joelle%20Barral%20and%20Fernando%20Pereira%20and%20Eli%20Collins%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy&entry.1292438233=%20%20This%20work%20introduces%20Gemma%2C%20a%20family%20of%20lightweight%2C%20state-of-the%20art%20open%0Amodels%20built%20from%20the%20research%20and%20technology%20used%20to%20create%20Gemini%20models.%0AGemma%20models%20demonstrate%20strong%20performance%20across%20academic%20benchmarks%20for%0Alanguage%20understanding%2C%20reasoning%2C%20and%20safety.%20We%20release%20two%20sizes%20of%20models%0A%282%20billion%20and%207%20billion%20parameters%29%2C%20and%20provide%20both%20pretrained%20and%0Afine-tuned%20checkpoints.%20Gemma%20outperforms%20similarly%20sized%20open%20models%20on%2011%20out%0Aof%2018%20text-based%20tasks%2C%20and%20we%20present%20comprehensive%20evaluations%20of%20safety%20and%0Aresponsibility%20aspects%20of%20the%20models%2C%20alongside%20a%20detailed%20description%20of%20model%0Adevelopment.%20We%20believe%20the%20responsible%20release%20of%20LLMs%20is%20critical%20for%0Aimproving%20the%20safety%20of%20frontier%20models%2C%20and%20for%20enabling%20the%20next%20wave%20of%20LLM%0Ainnovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08295v2&entry.124074799=Read"},
{"title": "Learning Priors for Non Rigid SfM from Casual Videos", "author": "Yoni Kasten and Wuyue Lu and Haggai Maron", "abstract": "  We tackle the long-standing challenge of reconstructing 3D structures and\ncamera positions from videos. The problem is particularly hard when objects are\ntransformed in a non-rigid way. Current approaches to this problem make\nunrealistic assumptions or require a long optimization time.\n  We present TracksTo4D, a novel deep learning-based approach that enables\ninferring 3D structure and camera positions from dynamic content originating\nfrom in-the-wild videos using a single feed-forward pass on a sparse point\ntrack matrix. To achieve this, we leverage recent advances in 2D point tracking\nand design an equivariant neural architecture tailored for directly processing\n2D point tracks by leveraging their symmetries. TracksTo4D is trained on a\ndataset of in-the-wild videos utilizing only the 2D point tracks extracted from\nthe videos, without any 3D supervision. Our experiments demonstrate that\nTracksTo4D generalizes well to unseen videos of unseen semantic categories at\ninference time, producing equivalent results to state-of-the-art methods while\nsignificantly reducing the runtime compared to other baselines.\n", "link": "http://arxiv.org/abs/2404.07097v1", "date": "2024-04-10", "relevancy": 2.3024, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Priors%20for%20Non%20Rigid%20SfM%20from%20Casual%20Videos&body=Title%3A%20Learning%20Priors%20for%20Non%20Rigid%20SfM%20from%20Casual%20Videos%0AAuthor%3A%20Yoni%20Kasten%20and%20Wuyue%20Lu%20and%20Haggai%20Maron%0AAbstract%3A%20%20%20We%20tackle%20the%20long-standing%20challenge%20of%20reconstructing%203D%20structures%20and%0Acamera%20positions%20from%20videos.%20The%20problem%20is%20particularly%20hard%20when%20objects%20are%0Atransformed%20in%20a%20non-rigid%20way.%20Current%20approaches%20to%20this%20problem%20make%0Aunrealistic%20assumptions%20or%20require%20a%20long%20optimization%20time.%0A%20%20We%20present%20TracksTo4D%2C%20a%20novel%20deep%20learning-based%20approach%20that%20enables%0Ainferring%203D%20structure%20and%20camera%20positions%20from%20dynamic%20content%20originating%0Afrom%20in-the-wild%20videos%20using%20a%20single%20feed-forward%20pass%20on%20a%20sparse%20point%0Atrack%20matrix.%20To%20achieve%20this%2C%20we%20leverage%20recent%20advances%20in%202D%20point%20tracking%0Aand%20design%20an%20equivariant%20neural%20architecture%20tailored%20for%20directly%20processing%0A2D%20point%20tracks%20by%20leveraging%20their%20symmetries.%20TracksTo4D%20is%20trained%20on%20a%0Adataset%20of%20in-the-wild%20videos%20utilizing%20only%20the%202D%20point%20tracks%20extracted%20from%0Athe%20videos%2C%20without%20any%203D%20supervision.%20Our%20experiments%20demonstrate%20that%0ATracksTo4D%20generalizes%20well%20to%20unseen%20videos%20of%20unseen%20semantic%20categories%20at%0Ainference%20time%2C%20producing%20equivalent%20results%20to%20state-of-the-art%20methods%20while%0Asignificantly%20reducing%20the%20runtime%20compared%20to%20other%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07097v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Priors%20for%20Non%20Rigid%20SfM%20from%20Casual%20Videos&entry.906535625=Yoni%20Kasten%20and%20Wuyue%20Lu%20and%20Haggai%20Maron&entry.1292438233=%20%20We%20tackle%20the%20long-standing%20challenge%20of%20reconstructing%203D%20structures%20and%0Acamera%20positions%20from%20videos.%20The%20problem%20is%20particularly%20hard%20when%20objects%20are%0Atransformed%20in%20a%20non-rigid%20way.%20Current%20approaches%20to%20this%20problem%20make%0Aunrealistic%20assumptions%20or%20require%20a%20long%20optimization%20time.%0A%20%20We%20present%20TracksTo4D%2C%20a%20novel%20deep%20learning-based%20approach%20that%20enables%0Ainferring%203D%20structure%20and%20camera%20positions%20from%20dynamic%20content%20originating%0Afrom%20in-the-wild%20videos%20using%20a%20single%20feed-forward%20pass%20on%20a%20sparse%20point%0Atrack%20matrix.%20To%20achieve%20this%2C%20we%20leverage%20recent%20advances%20in%202D%20point%20tracking%0Aand%20design%20an%20equivariant%20neural%20architecture%20tailored%20for%20directly%20processing%0A2D%20point%20tracks%20by%20leveraging%20their%20symmetries.%20TracksTo4D%20is%20trained%20on%20a%0Adataset%20of%20in-the-wild%20videos%20utilizing%20only%20the%202D%20point%20tracks%20extracted%20from%0Athe%20videos%2C%20without%20any%203D%20supervision.%20Our%20experiments%20demonstrate%20that%0ATracksTo4D%20generalizes%20well%20to%20unseen%20videos%20of%20unseen%20semantic%20categories%20at%0Ainference%20time%2C%20producing%20equivalent%20results%20to%20state-of-the-art%20methods%20while%0Asignificantly%20reducing%20the%20runtime%20compared%20to%20other%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07097v1&entry.124074799=Read"},
{"title": "ExpPoint-MAE: Better interpretability and performance for\n  self-supervised point cloud transformers", "author": "Ioannis Romanelis and Vlassis Fotis and Konstantinos Moustakas and Adrian Munteanu", "abstract": "  In this paper we delve into the properties of transformers, attained through\nself-supervision, in the point cloud domain. Specifically, we evaluate the\neffectiveness of Masked Autoencoding as a pretraining scheme, and explore\nMomentum Contrast as an alternative. In our study we investigate the impact of\ndata quantity on the learned features, and uncover similarities in the\ntransformer's behavior across domains. Through comprehensive visualiations, we\nobserve that the transformer learns to attend to semantically meaningful\nregions, indicating that pretraining leads to a better understanding of the\nunderlying geometry. Moreover, we examine the finetuning process and its effect\non the learned representations. Based on that, we devise an unfreezing strategy\nwhich consistently outperforms our baseline without introducing any other\nmodifications to the model or the training pipeline, and achieve\nstate-of-the-art results in the classification task among transformer models.\n", "link": "http://arxiv.org/abs/2306.10798v3", "date": "2024-04-10", "relevancy": 2.2846, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.603}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ExpPoint-MAE%3A%20Better%20interpretability%20and%20performance%20for%0A%20%20self-supervised%20point%20cloud%20transformers&body=Title%3A%20ExpPoint-MAE%3A%20Better%20interpretability%20and%20performance%20for%0A%20%20self-supervised%20point%20cloud%20transformers%0AAuthor%3A%20Ioannis%20Romanelis%20and%20Vlassis%20Fotis%20and%20Konstantinos%20Moustakas%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20In%20this%20paper%20we%20delve%20into%20the%20properties%20of%20transformers%2C%20attained%20through%0Aself-supervision%2C%20in%20the%20point%20cloud%20domain.%20Specifically%2C%20we%20evaluate%20the%0Aeffectiveness%20of%20Masked%20Autoencoding%20as%20a%20pretraining%20scheme%2C%20and%20explore%0AMomentum%20Contrast%20as%20an%20alternative.%20In%20our%20study%20we%20investigate%20the%20impact%20of%0Adata%20quantity%20on%20the%20learned%20features%2C%20and%20uncover%20similarities%20in%20the%0Atransformer%27s%20behavior%20across%20domains.%20Through%20comprehensive%20visualiations%2C%20we%0Aobserve%20that%20the%20transformer%20learns%20to%20attend%20to%20semantically%20meaningful%0Aregions%2C%20indicating%20that%20pretraining%20leads%20to%20a%20better%20understanding%20of%20the%0Aunderlying%20geometry.%20Moreover%2C%20we%20examine%20the%20finetuning%20process%20and%20its%20effect%0Aon%20the%20learned%20representations.%20Based%20on%20that%2C%20we%20devise%20an%20unfreezing%20strategy%0Awhich%20consistently%20outperforms%20our%20baseline%20without%20introducing%20any%20other%0Amodifications%20to%20the%20model%20or%20the%20training%20pipeline%2C%20and%20achieve%0Astate-of-the-art%20results%20in%20the%20classification%20task%20among%20transformer%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10798v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpPoint-MAE%3A%20Better%20interpretability%20and%20performance%20for%0A%20%20self-supervised%20point%20cloud%20transformers&entry.906535625=Ioannis%20Romanelis%20and%20Vlassis%20Fotis%20and%20Konstantinos%20Moustakas%20and%20Adrian%20Munteanu&entry.1292438233=%20%20In%20this%20paper%20we%20delve%20into%20the%20properties%20of%20transformers%2C%20attained%20through%0Aself-supervision%2C%20in%20the%20point%20cloud%20domain.%20Specifically%2C%20we%20evaluate%20the%0Aeffectiveness%20of%20Masked%20Autoencoding%20as%20a%20pretraining%20scheme%2C%20and%20explore%0AMomentum%20Contrast%20as%20an%20alternative.%20In%20our%20study%20we%20investigate%20the%20impact%20of%0Adata%20quantity%20on%20the%20learned%20features%2C%20and%20uncover%20similarities%20in%20the%0Atransformer%27s%20behavior%20across%20domains.%20Through%20comprehensive%20visualiations%2C%20we%0Aobserve%20that%20the%20transformer%20learns%20to%20attend%20to%20semantically%20meaningful%0Aregions%2C%20indicating%20that%20pretraining%20leads%20to%20a%20better%20understanding%20of%20the%0Aunderlying%20geometry.%20Moreover%2C%20we%20examine%20the%20finetuning%20process%20and%20its%20effect%0Aon%20the%20learned%20representations.%20Based%20on%20that%2C%20we%20devise%20an%20unfreezing%20strategy%0Awhich%20consistently%20outperforms%20our%20baseline%20without%20introducing%20any%20other%0Amodifications%20to%20the%20model%20or%20the%20training%20pipeline%2C%20and%20achieve%0Astate-of-the-art%20results%20in%20the%20classification%20task%20among%20transformer%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10798v3&entry.124074799=Read"},
{"title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation", "author": "Yuanwen Yue and Sabarinath Mahadevan and Jonas Schult and Francis Engelmann and Bastian Leibe and Konrad Schindler and Theodora Kontogianni", "abstract": "  During interactive segmentation, a model and a user work together to\ndelineate objects of interest in a 3D point cloud. In an iterative process, the\nmodel assigns each data point to an object (or the background), while the user\ncorrects errors in the resulting segmentation and feeds them back into the\nmodel. The current best practice formulates the problem as binary\nclassification and segments objects one at a time. The model expects the user\nto provide positive clicks to indicate regions wrongly assigned to the\nbackground and negative clicks on regions wrongly assigned to the object.\nSequentially visiting objects is wasteful since it disregards synergies between\nobjects: a positive click for a given object can, by definition, serve as a\nnegative click for nearby objects. Moreover, a direct competition between\nadjacent objects can speed up the identification of their common boundary. We\nintroduce AGILE3D, an efficient, attention-based model that (1) supports\nsimultaneous segmentation of multiple 3D objects, (2) yields more accurate\nsegmentation masks with fewer user clicks, and (3) offers faster inference. Our\ncore idea is to encode user clicks as spatial-temporal queries and enable\nexplicit interactions between click queries as well as between them and the 3D\nscene through a click attention module. Every time new clicks are added, we\nonly need to run a lightweight decoder that produces updated segmentation\nmasks. In experiments with four different 3D point cloud datasets, AGILE3D sets\na new state-of-the-art. Moreover, we also verify its practicality in real-world\nsetups with real user studies.\n", "link": "http://arxiv.org/abs/2306.00977v4", "date": "2024-04-10", "relevancy": 2.2401, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AGILE3D%3A%20Attention%20Guided%20Interactive%20Multi-object%203D%20Segmentation&body=Title%3A%20AGILE3D%3A%20Attention%20Guided%20Interactive%20Multi-object%203D%20Segmentation%0AAuthor%3A%20Yuanwen%20Yue%20and%20Sabarinath%20Mahadevan%20and%20Jonas%20Schult%20and%20Francis%20Engelmann%20and%20Bastian%20Leibe%20and%20Konrad%20Schindler%20and%20Theodora%20Kontogianni%0AAbstract%3A%20%20%20During%20interactive%20segmentation%2C%20a%20model%20and%20a%20user%20work%20together%20to%0Adelineate%20objects%20of%20interest%20in%20a%203D%20point%20cloud.%20In%20an%20iterative%20process%2C%20the%0Amodel%20assigns%20each%20data%20point%20to%20an%20object%20%28or%20the%20background%29%2C%20while%20the%20user%0Acorrects%20errors%20in%20the%20resulting%20segmentation%20and%20feeds%20them%20back%20into%20the%0Amodel.%20The%20current%20best%20practice%20formulates%20the%20problem%20as%20binary%0Aclassification%20and%20segments%20objects%20one%20at%20a%20time.%20The%20model%20expects%20the%20user%0Ato%20provide%20positive%20clicks%20to%20indicate%20regions%20wrongly%20assigned%20to%20the%0Abackground%20and%20negative%20clicks%20on%20regions%20wrongly%20assigned%20to%20the%20object.%0ASequentially%20visiting%20objects%20is%20wasteful%20since%20it%20disregards%20synergies%20between%0Aobjects%3A%20a%20positive%20click%20for%20a%20given%20object%20can%2C%20by%20definition%2C%20serve%20as%20a%0Anegative%20click%20for%20nearby%20objects.%20Moreover%2C%20a%20direct%20competition%20between%0Aadjacent%20objects%20can%20speed%20up%20the%20identification%20of%20their%20common%20boundary.%20We%0Aintroduce%20AGILE3D%2C%20an%20efficient%2C%20attention-based%20model%20that%20%281%29%20supports%0Asimultaneous%20segmentation%20of%20multiple%203D%20objects%2C%20%282%29%20yields%20more%20accurate%0Asegmentation%20masks%20with%20fewer%20user%20clicks%2C%20and%20%283%29%20offers%20faster%20inference.%20Our%0Acore%20idea%20is%20to%20encode%20user%20clicks%20as%20spatial-temporal%20queries%20and%20enable%0Aexplicit%20interactions%20between%20click%20queries%20as%20well%20as%20between%20them%20and%20the%203D%0Ascene%20through%20a%20click%20attention%20module.%20Every%20time%20new%20clicks%20are%20added%2C%20we%0Aonly%20need%20to%20run%20a%20lightweight%20decoder%20that%20produces%20updated%20segmentation%0Amasks.%20In%20experiments%20with%20four%20different%203D%20point%20cloud%20datasets%2C%20AGILE3D%20sets%0Aa%20new%20state-of-the-art.%20Moreover%2C%20we%20also%20verify%20its%20practicality%20in%20real-world%0Asetups%20with%20real%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00977v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGILE3D%3A%20Attention%20Guided%20Interactive%20Multi-object%203D%20Segmentation&entry.906535625=Yuanwen%20Yue%20and%20Sabarinath%20Mahadevan%20and%20Jonas%20Schult%20and%20Francis%20Engelmann%20and%20Bastian%20Leibe%20and%20Konrad%20Schindler%20and%20Theodora%20Kontogianni&entry.1292438233=%20%20During%20interactive%20segmentation%2C%20a%20model%20and%20a%20user%20work%20together%20to%0Adelineate%20objects%20of%20interest%20in%20a%203D%20point%20cloud.%20In%20an%20iterative%20process%2C%20the%0Amodel%20assigns%20each%20data%20point%20to%20an%20object%20%28or%20the%20background%29%2C%20while%20the%20user%0Acorrects%20errors%20in%20the%20resulting%20segmentation%20and%20feeds%20them%20back%20into%20the%0Amodel.%20The%20current%20best%20practice%20formulates%20the%20problem%20as%20binary%0Aclassification%20and%20segments%20objects%20one%20at%20a%20time.%20The%20model%20expects%20the%20user%0Ato%20provide%20positive%20clicks%20to%20indicate%20regions%20wrongly%20assigned%20to%20the%0Abackground%20and%20negative%20clicks%20on%20regions%20wrongly%20assigned%20to%20the%20object.%0ASequentially%20visiting%20objects%20is%20wasteful%20since%20it%20disregards%20synergies%20between%0Aobjects%3A%20a%20positive%20click%20for%20a%20given%20object%20can%2C%20by%20definition%2C%20serve%20as%20a%0Anegative%20click%20for%20nearby%20objects.%20Moreover%2C%20a%20direct%20competition%20between%0Aadjacent%20objects%20can%20speed%20up%20the%20identification%20of%20their%20common%20boundary.%20We%0Aintroduce%20AGILE3D%2C%20an%20efficient%2C%20attention-based%20model%20that%20%281%29%20supports%0Asimultaneous%20segmentation%20of%20multiple%203D%20objects%2C%20%282%29%20yields%20more%20accurate%0Asegmentation%20masks%20with%20fewer%20user%20clicks%2C%20and%20%283%29%20offers%20faster%20inference.%20Our%0Acore%20idea%20is%20to%20encode%20user%20clicks%20as%20spatial-temporal%20queries%20and%20enable%0Aexplicit%20interactions%20between%20click%20queries%20as%20well%20as%20between%20them%20and%20the%203D%0Ascene%20through%20a%20click%20attention%20module.%20Every%20time%20new%20clicks%20are%20added%2C%20we%0Aonly%20need%20to%20run%20a%20lightweight%20decoder%20that%20produces%20updated%20segmentation%0Amasks.%20In%20experiments%20with%20four%20different%203D%20point%20cloud%20datasets%2C%20AGILE3D%20sets%0Aa%20new%20state-of-the-art.%20Moreover%2C%20we%20also%20verify%20its%20practicality%20in%20real-world%0Asetups%20with%20real%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00977v4&entry.124074799=Read"},
{"title": "MaskClustering: View Consensus based Mask Graph Clustering for\n  Open-Vocabulary 3D Instance Segmentation", "author": "Mi Yan and Jiazhao Zhang and Yan Zhu and He Wang", "abstract": "  Open-vocabulary 3D instance segmentation is cutting-edge for its ability to\nsegment 3D instances without predefined categories. However, progress in 3D\nlags behind its 2D counterpart due to limited annotated 3D data. To address\nthis, recent works first generate 2D open-vocabulary masks through 2D models\nand then merge them into 3D instances based on metrics calculated between two\nneighboring frames. In contrast to these local metrics, we propose a novel\nmetric, view consensus rate, to enhance the utilization of multi-view\nobservations. The key insight is that two 2D masks should be deemed part of the\nsame 3D instance if a significant number of other 2D masks from different views\ncontain both these two masks. Using this metric as edge weight, we construct a\nglobal mask graph where each mask is a node. Through iterative clustering of\nmasks showing high view consensus, we generate a series of clusters, each\nrepresenting a distinct 3D instance. Notably, our model is training-free.\nThrough extensive experiments on publicly available datasets, including\nScanNet++, ScanNet200 and MatterPort3D, we demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary 3D instance segmentation. Our\nproject page is at https://pku-epic.github.io/MaskClustering.\n", "link": "http://arxiv.org/abs/2401.07745v2", "date": "2024-04-10", "relevancy": 2.2274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.58}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5448}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MaskClustering%3A%20View%20Consensus%20based%20Mask%20Graph%20Clustering%20for%0A%20%20Open-Vocabulary%203D%20Instance%20Segmentation&body=Title%3A%20MaskClustering%3A%20View%20Consensus%20based%20Mask%20Graph%20Clustering%20for%0A%20%20Open-Vocabulary%203D%20Instance%20Segmentation%0AAuthor%3A%20Mi%20Yan%20and%20Jiazhao%20Zhang%20and%20Yan%20Zhu%20and%20He%20Wang%0AAbstract%3A%20%20%20Open-vocabulary%203D%20instance%20segmentation%20is%20cutting-edge%20for%20its%20ability%20to%0Asegment%203D%20instances%20without%20predefined%20categories.%20However%2C%20progress%20in%203D%0Alags%20behind%20its%202D%20counterpart%20due%20to%20limited%20annotated%203D%20data.%20To%20address%0Athis%2C%20recent%20works%20first%20generate%202D%20open-vocabulary%20masks%20through%202D%20models%0Aand%20then%20merge%20them%20into%203D%20instances%20based%20on%20metrics%20calculated%20between%20two%0Aneighboring%20frames.%20In%20contrast%20to%20these%20local%20metrics%2C%20we%20propose%20a%20novel%0Ametric%2C%20view%20consensus%20rate%2C%20to%20enhance%20the%20utilization%20of%20multi-view%0Aobservations.%20The%20key%20insight%20is%20that%20two%202D%20masks%20should%20be%20deemed%20part%20of%20the%0Asame%203D%20instance%20if%20a%20significant%20number%20of%20other%202D%20masks%20from%20different%20views%0Acontain%20both%20these%20two%20masks.%20Using%20this%20metric%20as%20edge%20weight%2C%20we%20construct%20a%0Aglobal%20mask%20graph%20where%20each%20mask%20is%20a%20node.%20Through%20iterative%20clustering%20of%0Amasks%20showing%20high%20view%20consensus%2C%20we%20generate%20a%20series%20of%20clusters%2C%20each%0Arepresenting%20a%20distinct%203D%20instance.%20Notably%2C%20our%20model%20is%20training-free.%0AThrough%20extensive%20experiments%20on%20publicly%20available%20datasets%2C%20including%0AScanNet%2B%2B%2C%20ScanNet200%20and%20MatterPort3D%2C%20we%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%203D%20instance%20segmentation.%20Our%0Aproject%20page%20is%20at%20https%3A//pku-epic.github.io/MaskClustering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07745v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskClustering%3A%20View%20Consensus%20based%20Mask%20Graph%20Clustering%20for%0A%20%20Open-Vocabulary%203D%20Instance%20Segmentation&entry.906535625=Mi%20Yan%20and%20Jiazhao%20Zhang%20and%20Yan%20Zhu%20and%20He%20Wang&entry.1292438233=%20%20Open-vocabulary%203D%20instance%20segmentation%20is%20cutting-edge%20for%20its%20ability%20to%0Asegment%203D%20instances%20without%20predefined%20categories.%20However%2C%20progress%20in%203D%0Alags%20behind%20its%202D%20counterpart%20due%20to%20limited%20annotated%203D%20data.%20To%20address%0Athis%2C%20recent%20works%20first%20generate%202D%20open-vocabulary%20masks%20through%202D%20models%0Aand%20then%20merge%20them%20into%203D%20instances%20based%20on%20metrics%20calculated%20between%20two%0Aneighboring%20frames.%20In%20contrast%20to%20these%20local%20metrics%2C%20we%20propose%20a%20novel%0Ametric%2C%20view%20consensus%20rate%2C%20to%20enhance%20the%20utilization%20of%20multi-view%0Aobservations.%20The%20key%20insight%20is%20that%20two%202D%20masks%20should%20be%20deemed%20part%20of%20the%0Asame%203D%20instance%20if%20a%20significant%20number%20of%20other%202D%20masks%20from%20different%20views%0Acontain%20both%20these%20two%20masks.%20Using%20this%20metric%20as%20edge%20weight%2C%20we%20construct%20a%0Aglobal%20mask%20graph%20where%20each%20mask%20is%20a%20node.%20Through%20iterative%20clustering%20of%0Amasks%20showing%20high%20view%20consensus%2C%20we%20generate%20a%20series%20of%20clusters%2C%20each%0Arepresenting%20a%20distinct%203D%20instance.%20Notably%2C%20our%20model%20is%20training-free.%0AThrough%20extensive%20experiments%20on%20publicly%20available%20datasets%2C%20including%0AScanNet%2B%2B%2C%20ScanNet200%20and%20MatterPort3D%2C%20we%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20open-vocabulary%203D%20instance%20segmentation.%20Our%0Aproject%20page%20is%20at%20https%3A//pku-epic.github.io/MaskClustering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07745v2&entry.124074799=Read"},
{"title": "Towards a Safe Real-Time Motion Planning Framework for Autonomous\n  Driving Systems: An MPPI Approach", "author": "Mehdi Testouri and Gamal Elghazaly and Raphael Frank", "abstract": "  Planning safe trajectories in Autonomous Driving Systems (ADS) is a complex\nproblem to solve in real-time. The main challenge to solve this problem arises\nfrom the various conditions and constraints imposed by road geometry, semantics\nand traffic rules, as well as the presence of dynamic agents. Recently, Model\nPredictive Path Integral (MPPI) has shown to be an effective framework for\noptimal motion planning and control in robot navigation in unstructured and\nhighly uncertain environments. In this paper, we formulate the motion planning\nproblem in ADS as a nonlinear stochastic dynamic optimization problem that can\nbe solved using an MPPI strategy. The main technical contribution of this work\nis a method to handle obstacles within the MPPI formulation safely. In this\nmethod, obstacles are approximated by circles that can be easily integrated\ninto the MPPI cost formulation while considering safety margins. The proposed\nMPPI framework has been efficiently implemented in our autonomous vehicle and\nexperimentally validated using three different primitive scenarios.\nExperimental results show that generated trajectories are safe, feasible and\nperfectly achieve the planning objective. The video results as well as the\nopen-source implementation are available at:\nhttps://gitlab.uni.lu/360lab-public/mppi\n", "link": "http://arxiv.org/abs/2308.01654v3", "date": "2024-04-10", "relevancy": 2.2237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5631}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5446}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach&body=Title%3A%20Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach%0AAuthor%3A%20Mehdi%20Testouri%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank%0AAbstract%3A%20%20%20Planning%20safe%20trajectories%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%20is%20a%20complex%0Aproblem%20to%20solve%20in%20real-time.%20The%20main%20challenge%20to%20solve%20this%20problem%20arises%0Afrom%20the%20various%20conditions%20and%20constraints%20imposed%20by%20road%20geometry%2C%20semantics%0Aand%20traffic%20rules%2C%20as%20well%20as%20the%20presence%20of%20dynamic%20agents.%20Recently%2C%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20has%20shown%20to%20be%20an%20effective%20framework%20for%0Aoptimal%20motion%20planning%20and%20control%20in%20robot%20navigation%20in%20unstructured%20and%0Ahighly%20uncertain%20environments.%20In%20this%20paper%2C%20we%20formulate%20the%20motion%20planning%0Aproblem%20in%20ADS%20as%20a%20nonlinear%20stochastic%20dynamic%20optimization%20problem%20that%20can%0Abe%20solved%20using%20an%20MPPI%20strategy.%20The%20main%20technical%20contribution%20of%20this%20work%0Ais%20a%20method%20to%20handle%20obstacles%20within%20the%20MPPI%20formulation%20safely.%20In%20this%0Amethod%2C%20obstacles%20are%20approximated%20by%20circles%20that%20can%20be%20easily%20integrated%0Ainto%20the%20MPPI%20cost%20formulation%20while%20considering%20safety%20margins.%20The%20proposed%0AMPPI%20framework%20has%20been%20efficiently%20implemented%20in%20our%20autonomous%20vehicle%20and%0Aexperimentally%20validated%20using%20three%20different%20primitive%20scenarios.%0AExperimental%20results%20show%20that%20generated%20trajectories%20are%20safe%2C%20feasible%20and%0Aperfectly%20achieve%20the%20planning%20objective.%20The%20video%20results%20as%20well%20as%20the%0Aopen-source%20implementation%20are%20available%20at%3A%0Ahttps%3A//gitlab.uni.lu/360lab-public/mppi%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01654v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Safe%20Real-Time%20Motion%20Planning%20Framework%20for%20Autonomous%0A%20%20Driving%20Systems%3A%20An%20MPPI%20Approach&entry.906535625=Mehdi%20Testouri%20and%20Gamal%20Elghazaly%20and%20Raphael%20Frank&entry.1292438233=%20%20Planning%20safe%20trajectories%20in%20Autonomous%20Driving%20Systems%20%28ADS%29%20is%20a%20complex%0Aproblem%20to%20solve%20in%20real-time.%20The%20main%20challenge%20to%20solve%20this%20problem%20arises%0Afrom%20the%20various%20conditions%20and%20constraints%20imposed%20by%20road%20geometry%2C%20semantics%0Aand%20traffic%20rules%2C%20as%20well%20as%20the%20presence%20of%20dynamic%20agents.%20Recently%2C%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20has%20shown%20to%20be%20an%20effective%20framework%20for%0Aoptimal%20motion%20planning%20and%20control%20in%20robot%20navigation%20in%20unstructured%20and%0Ahighly%20uncertain%20environments.%20In%20this%20paper%2C%20we%20formulate%20the%20motion%20planning%0Aproblem%20in%20ADS%20as%20a%20nonlinear%20stochastic%20dynamic%20optimization%20problem%20that%20can%0Abe%20solved%20using%20an%20MPPI%20strategy.%20The%20main%20technical%20contribution%20of%20this%20work%0Ais%20a%20method%20to%20handle%20obstacles%20within%20the%20MPPI%20formulation%20safely.%20In%20this%0Amethod%2C%20obstacles%20are%20approximated%20by%20circles%20that%20can%20be%20easily%20integrated%0Ainto%20the%20MPPI%20cost%20formulation%20while%20considering%20safety%20margins.%20The%20proposed%0AMPPI%20framework%20has%20been%20efficiently%20implemented%20in%20our%20autonomous%20vehicle%20and%0Aexperimentally%20validated%20using%20three%20different%20primitive%20scenarios.%0AExperimental%20results%20show%20that%20generated%20trajectories%20are%20safe%2C%20feasible%20and%0Aperfectly%20achieve%20the%20planning%20objective.%20The%20video%20results%20as%20well%20as%20the%0Aopen-source%20implementation%20are%20available%20at%3A%0Ahttps%3A//gitlab.uni.lu/360lab-public/mppi%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01654v3&entry.124074799=Read"},
{"title": "MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation\n  from 2D Keypoints", "author": "Bedirhan Uguz and Ozhan Suat and Batuhan Karagoz and Emre Akbas", "abstract": "  This paper presents Key2Mesh, a model that takes a set of 2D human pose\nkeypoints as input and estimates the corresponding body mesh. Since this\nprocess does not involve any visual (i.e. RGB image) data, the model can be\ntrained on large-scale motion capture (MoCap) datasets, thereby overcoming the\nscarcity of image datasets with 3D labels. To enable the model's application on\nRGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D\nkeypoints, and then feed these 2D keypoints to Key2Mesh. To improve the\nperformance of our model on RGB images, we apply an adversarial domain\nadaptation (DA) method to bridge the gap between the MoCap and visual domains.\nCrucially, our DA method does not require 3D labels for visual data, which\nenables adaptation to target sets without the need for costly labels. We\nevaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints,\nin the absence of RGB and mesh label pairs. Our results on widely used H3.6M\nand 3DPW datasets show that Key2Mesh sets the new state-of-the-art by\noutperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE\nfor the 3DPW dataset. Thanks to our model's simple architecture, it operates at\nleast 12x faster than the prior state-of-the-art model, LGD. Additional\nqualitative samples and code are available on the project website:\nhttps://key2mesh.github.io/.\n", "link": "http://arxiv.org/abs/2404.07094v1", "date": "2024-04-10", "relevancy": 2.2231, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5676}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoCap-to-Visual%20Domain%20Adaptation%20for%20Efficient%20Human%20Mesh%20Estimation%0A%20%20from%202D%20Keypoints&body=Title%3A%20MoCap-to-Visual%20Domain%20Adaptation%20for%20Efficient%20Human%20Mesh%20Estimation%0A%20%20from%202D%20Keypoints%0AAuthor%3A%20Bedirhan%20Uguz%20and%20Ozhan%20Suat%20and%20Batuhan%20Karagoz%20and%20Emre%20Akbas%0AAbstract%3A%20%20%20This%20paper%20presents%20Key2Mesh%2C%20a%20model%20that%20takes%20a%20set%20of%202D%20human%20pose%0Akeypoints%20as%20input%20and%20estimates%20the%20corresponding%20body%20mesh.%20Since%20this%0Aprocess%20does%20not%20involve%20any%20visual%20%28i.e.%20RGB%20image%29%20data%2C%20the%20model%20can%20be%0Atrained%20on%20large-scale%20motion%20capture%20%28MoCap%29%20datasets%2C%20thereby%20overcoming%20the%0Ascarcity%20of%20image%20datasets%20with%203D%20labels.%20To%20enable%20the%20model%27s%20application%20on%0ARGB%20images%2C%20we%20first%20run%20an%20off-the-shelf%202D%20pose%20estimator%20to%20obtain%20the%202D%0Akeypoints%2C%20and%20then%20feed%20these%202D%20keypoints%20to%20Key2Mesh.%20To%20improve%20the%0Aperformance%20of%20our%20model%20on%20RGB%20images%2C%20we%20apply%20an%20adversarial%20domain%0Aadaptation%20%28DA%29%20method%20to%20bridge%20the%20gap%20between%20the%20MoCap%20and%20visual%20domains.%0ACrucially%2C%20our%20DA%20method%20does%20not%20require%203D%20labels%20for%20visual%20data%2C%20which%0Aenables%20adaptation%20to%20target%20sets%20without%20the%20need%20for%20costly%20labels.%20We%0Aevaluate%20Key2Mesh%20for%20the%20task%20of%20estimating%203D%20human%20meshes%20from%202D%20keypoints%2C%0Ain%20the%20absence%20of%20RGB%20and%20mesh%20label%20pairs.%20Our%20results%20on%20widely%20used%20H3.6M%0Aand%203DPW%20datasets%20show%20that%20Key2Mesh%20sets%20the%20new%20state-of-the-art%20by%0Aoutperforming%20other%20models%20in%20PA-MPJPE%20for%20both%20datasets%2C%20and%20in%20MPJPE%20and%20PVE%0Afor%20the%203DPW%20dataset.%20Thanks%20to%20our%20model%27s%20simple%20architecture%2C%20it%20operates%20at%0Aleast%2012x%20faster%20than%20the%20prior%20state-of-the-art%20model%2C%20LGD.%20Additional%0Aqualitative%20samples%20and%20code%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//key2mesh.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07094v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCap-to-Visual%20Domain%20Adaptation%20for%20Efficient%20Human%20Mesh%20Estimation%0A%20%20from%202D%20Keypoints&entry.906535625=Bedirhan%20Uguz%20and%20Ozhan%20Suat%20and%20Batuhan%20Karagoz%20and%20Emre%20Akbas&entry.1292438233=%20%20This%20paper%20presents%20Key2Mesh%2C%20a%20model%20that%20takes%20a%20set%20of%202D%20human%20pose%0Akeypoints%20as%20input%20and%20estimates%20the%20corresponding%20body%20mesh.%20Since%20this%0Aprocess%20does%20not%20involve%20any%20visual%20%28i.e.%20RGB%20image%29%20data%2C%20the%20model%20can%20be%0Atrained%20on%20large-scale%20motion%20capture%20%28MoCap%29%20datasets%2C%20thereby%20overcoming%20the%0Ascarcity%20of%20image%20datasets%20with%203D%20labels.%20To%20enable%20the%20model%27s%20application%20on%0ARGB%20images%2C%20we%20first%20run%20an%20off-the-shelf%202D%20pose%20estimator%20to%20obtain%20the%202D%0Akeypoints%2C%20and%20then%20feed%20these%202D%20keypoints%20to%20Key2Mesh.%20To%20improve%20the%0Aperformance%20of%20our%20model%20on%20RGB%20images%2C%20we%20apply%20an%20adversarial%20domain%0Aadaptation%20%28DA%29%20method%20to%20bridge%20the%20gap%20between%20the%20MoCap%20and%20visual%20domains.%0ACrucially%2C%20our%20DA%20method%20does%20not%20require%203D%20labels%20for%20visual%20data%2C%20which%0Aenables%20adaptation%20to%20target%20sets%20without%20the%20need%20for%20costly%20labels.%20We%0Aevaluate%20Key2Mesh%20for%20the%20task%20of%20estimating%203D%20human%20meshes%20from%202D%20keypoints%2C%0Ain%20the%20absence%20of%20RGB%20and%20mesh%20label%20pairs.%20Our%20results%20on%20widely%20used%20H3.6M%0Aand%203DPW%20datasets%20show%20that%20Key2Mesh%20sets%20the%20new%20state-of-the-art%20by%0Aoutperforming%20other%20models%20in%20PA-MPJPE%20for%20both%20datasets%2C%20and%20in%20MPJPE%20and%20PVE%0Afor%20the%203DPW%20dataset.%20Thanks%20to%20our%20model%27s%20simple%20architecture%2C%20it%20operates%20at%0Aleast%2012x%20faster%20than%20the%20prior%20state-of-the-art%20model%2C%20LGD.%20Additional%0Aqualitative%20samples%20and%20code%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//key2mesh.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07094v1&entry.124074799=Read"},
{"title": "UMBRAE: Unified Multimodal Decoding of Brain Signals", "author": "Weihao Xia and Raoul de Charette and Cengiz \u00d6ztireli and Jing-Hao Xue", "abstract": "  We address prevailing challenges of the brain-powered research, departing\nfrom the observation that the literature hardly recover accurate spatial\ninformation and require subject-specific models. To address these challenges,\nwe propose UMBRAE, a unified multimodal decoding of brain signals. First, to\nextract instance-level conceptual and spatial details from neural signals, we\nintroduce an efficient universal brain encoder for multimodal-brain alignment\nand recover object descriptions at multiple levels of granularity from\nsubsequent multimodal large language model (MLLM). Second, we introduce a\ncross-subject training strategy mapping subject-specific features to a common\nfeature space. This allows a model to be trained on multiple subjects without\nextra resources, even yielding superior results compared to subject-specific\nmodels. Further, we demonstrate this supports weakly-supervised adaptation to\nnew subjects, with only a fraction of the total training data. Experiments\ndemonstrate that UMBRAE not only achieves superior results in the newly\nintroduced tasks but also outperforms methods in well established tasks. To\nassess our method, we construct and share with the community a comprehensive\nbrain understanding benchmark BrainHub. Our code and benchmark are available at\nhttps://weihaox.github.io/UMBRAE.\n", "link": "http://arxiv.org/abs/2404.07202v1", "date": "2024-04-10", "relevancy": 2.2096, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UMBRAE%3A%20Unified%20Multimodal%20Decoding%20of%20Brain%20Signals&body=Title%3A%20UMBRAE%3A%20Unified%20Multimodal%20Decoding%20of%20Brain%20Signals%0AAuthor%3A%20Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20We%20address%20prevailing%20challenges%20of%20the%20brain-powered%20research%2C%20departing%0Afrom%20the%20observation%20that%20the%20literature%20hardly%20recover%20accurate%20spatial%0Ainformation%20and%20require%20subject-specific%20models.%20To%20address%20these%20challenges%2C%0Awe%20propose%20UMBRAE%2C%20a%20unified%20multimodal%20decoding%20of%20brain%20signals.%20First%2C%20to%0Aextract%20instance-level%20conceptual%20and%20spatial%20details%20from%20neural%20signals%2C%20we%0Aintroduce%20an%20efficient%20universal%20brain%20encoder%20for%20multimodal-brain%20alignment%0Aand%20recover%20object%20descriptions%20at%20multiple%20levels%20of%20granularity%20from%0Asubsequent%20multimodal%20large%20language%20model%20%28MLLM%29.%20Second%2C%20we%20introduce%20a%0Across-subject%20training%20strategy%20mapping%20subject-specific%20features%20to%20a%20common%0Afeature%20space.%20This%20allows%20a%20model%20to%20be%20trained%20on%20multiple%20subjects%20without%0Aextra%20resources%2C%20even%20yielding%20superior%20results%20compared%20to%20subject-specific%0Amodels.%20Further%2C%20we%20demonstrate%20this%20supports%20weakly-supervised%20adaptation%20to%0Anew%20subjects%2C%20with%20only%20a%20fraction%20of%20the%20total%20training%20data.%20Experiments%0Ademonstrate%20that%20UMBRAE%20not%20only%20achieves%20superior%20results%20in%20the%20newly%0Aintroduced%20tasks%20but%20also%20outperforms%20methods%20in%20well%20established%20tasks.%20To%0Aassess%20our%20method%2C%20we%20construct%20and%20share%20with%20the%20community%20a%20comprehensive%0Abrain%20understanding%20benchmark%20BrainHub.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//weihaox.github.io/UMBRAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07202v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMBRAE%3A%20Unified%20Multimodal%20Decoding%20of%20Brain%20Signals&entry.906535625=Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20We%20address%20prevailing%20challenges%20of%20the%20brain-powered%20research%2C%20departing%0Afrom%20the%20observation%20that%20the%20literature%20hardly%20recover%20accurate%20spatial%0Ainformation%20and%20require%20subject-specific%20models.%20To%20address%20these%20challenges%2C%0Awe%20propose%20UMBRAE%2C%20a%20unified%20multimodal%20decoding%20of%20brain%20signals.%20First%2C%20to%0Aextract%20instance-level%20conceptual%20and%20spatial%20details%20from%20neural%20signals%2C%20we%0Aintroduce%20an%20efficient%20universal%20brain%20encoder%20for%20multimodal-brain%20alignment%0Aand%20recover%20object%20descriptions%20at%20multiple%20levels%20of%20granularity%20from%0Asubsequent%20multimodal%20large%20language%20model%20%28MLLM%29.%20Second%2C%20we%20introduce%20a%0Across-subject%20training%20strategy%20mapping%20subject-specific%20features%20to%20a%20common%0Afeature%20space.%20This%20allows%20a%20model%20to%20be%20trained%20on%20multiple%20subjects%20without%0Aextra%20resources%2C%20even%20yielding%20superior%20results%20compared%20to%20subject-specific%0Amodels.%20Further%2C%20we%20demonstrate%20this%20supports%20weakly-supervised%20adaptation%20to%0Anew%20subjects%2C%20with%20only%20a%20fraction%20of%20the%20total%20training%20data.%20Experiments%0Ademonstrate%20that%20UMBRAE%20not%20only%20achieves%20superior%20results%20in%20the%20newly%0Aintroduced%20tasks%20but%20also%20outperforms%20methods%20in%20well%20established%20tasks.%20To%0Aassess%20our%20method%2C%20we%20construct%20and%20share%20with%20the%20community%20a%20comprehensive%0Abrain%20understanding%20benchmark%20BrainHub.%20Our%20code%20and%20benchmark%20are%20available%20at%0Ahttps%3A//weihaox.github.io/UMBRAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07202v1&entry.124074799=Read"},
{"title": "An Evidential-enhanced Tri-Branch Consistency Learning Method for\n  Semi-supervised Medical Image Segmentation", "author": "Zhenxi Zhang and Heng Zhou and Xiaoran Shi and Ran Ran and Chunna Tian and Feng Zhou", "abstract": "  Semi-supervised segmentation presents a promising approach for large-scale\nmedical image analysis, effectively reducing annotation burdens while achieving\ncomparable performance. This methodology holds substantial potential for\nstreamlining the segmentation process and enhancing its feasibility within\nclinical settings for translational investigations. While cross-supervised\ntraining, based on distinct co-training sub-networks, has become a prevalent\nparadigm for this task, addressing critical issues such as predication\ndisagreement and label-noise suppression requires further attention and\nprogress in cross-supervised training. In this paper, we introduce an\nEvidential Tri-Branch Consistency learning framework (ETC-Net) for\nsemi-supervised medical image segmentation. ETC-Net employs three branches: an\nevidential conservative branch, an evidential progressive branch, and an\nevidential fusion branch. The first two branches exhibit complementary\ncharacteristics, allowing them to address prediction diversity and enhance\ntraining stability. We also integrate uncertainty estimation from the\nevidential learning into cross-supervised training, mitigating the negative\nimpact of erroneous supervision signals. Additionally, the evidential fusion\nbranch capitalizes on the complementary attributes of the first two branches\nand leverages an evidence-based Dempster-Shafer fusion strategy, supervised by\nmore reliable and accurate pseudo-labels of unlabeled data. Extensive\nexperiments conducted on LA, Pancreas-CT, and ACDC datasets demonstrate that\nETC-Net surpasses other state-of-the-art methods for semi-supervised\nsegmentation. The code will be made available in the near future at\nhttps://github.com/Medsemiseg.\n", "link": "http://arxiv.org/abs/2404.07032v1", "date": "2024-04-10", "relevancy": 2.2053, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6458}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Evidential-enhanced%20Tri-Branch%20Consistency%20Learning%20Method%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&body=Title%3A%20An%20Evidential-enhanced%20Tri-Branch%20Consistency%20Learning%20Method%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Zhenxi%20Zhang%20and%20Heng%20Zhou%20and%20Xiaoran%20Shi%20and%20Ran%20Ran%20and%20Chunna%20Tian%20and%20Feng%20Zhou%0AAbstract%3A%20%20%20Semi-supervised%20segmentation%20presents%20a%20promising%20approach%20for%20large-scale%0Amedical%20image%20analysis%2C%20effectively%20reducing%20annotation%20burdens%20while%20achieving%0Acomparable%20performance.%20This%20methodology%20holds%20substantial%20potential%20for%0Astreamlining%20the%20segmentation%20process%20and%20enhancing%20its%20feasibility%20within%0Aclinical%20settings%20for%20translational%20investigations.%20While%20cross-supervised%0Atraining%2C%20based%20on%20distinct%20co-training%20sub-networks%2C%20has%20become%20a%20prevalent%0Aparadigm%20for%20this%20task%2C%20addressing%20critical%20issues%20such%20as%20predication%0Adisagreement%20and%20label-noise%20suppression%20requires%20further%20attention%20and%0Aprogress%20in%20cross-supervised%20training.%20In%20this%20paper%2C%20we%20introduce%20an%0AEvidential%20Tri-Branch%20Consistency%20learning%20framework%20%28ETC-Net%29%20for%0Asemi-supervised%20medical%20image%20segmentation.%20ETC-Net%20employs%20three%20branches%3A%20an%0Aevidential%20conservative%20branch%2C%20an%20evidential%20progressive%20branch%2C%20and%20an%0Aevidential%20fusion%20branch.%20The%20first%20two%20branches%20exhibit%20complementary%0Acharacteristics%2C%20allowing%20them%20to%20address%20prediction%20diversity%20and%20enhance%0Atraining%20stability.%20We%20also%20integrate%20uncertainty%20estimation%20from%20the%0Aevidential%20learning%20into%20cross-supervised%20training%2C%20mitigating%20the%20negative%0Aimpact%20of%20erroneous%20supervision%20signals.%20Additionally%2C%20the%20evidential%20fusion%0Abranch%20capitalizes%20on%20the%20complementary%20attributes%20of%20the%20first%20two%20branches%0Aand%20leverages%20an%20evidence-based%20Dempster-Shafer%20fusion%20strategy%2C%20supervised%20by%0Amore%20reliable%20and%20accurate%20pseudo-labels%20of%20unlabeled%20data.%20Extensive%0Aexperiments%20conducted%20on%20LA%2C%20Pancreas-CT%2C%20and%20ACDC%20datasets%20demonstrate%20that%0AETC-Net%20surpasses%20other%20state-of-the-art%20methods%20for%20semi-supervised%0Asegmentation.%20The%20code%20will%20be%20made%20available%20in%20the%20near%20future%20at%0Ahttps%3A//github.com/Medsemiseg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07032v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Evidential-enhanced%20Tri-Branch%20Consistency%20Learning%20Method%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&entry.906535625=Zhenxi%20Zhang%20and%20Heng%20Zhou%20and%20Xiaoran%20Shi%20and%20Ran%20Ran%20and%20Chunna%20Tian%20and%20Feng%20Zhou&entry.1292438233=%20%20Semi-supervised%20segmentation%20presents%20a%20promising%20approach%20for%20large-scale%0Amedical%20image%20analysis%2C%20effectively%20reducing%20annotation%20burdens%20while%20achieving%0Acomparable%20performance.%20This%20methodology%20holds%20substantial%20potential%20for%0Astreamlining%20the%20segmentation%20process%20and%20enhancing%20its%20feasibility%20within%0Aclinical%20settings%20for%20translational%20investigations.%20While%20cross-supervised%0Atraining%2C%20based%20on%20distinct%20co-training%20sub-networks%2C%20has%20become%20a%20prevalent%0Aparadigm%20for%20this%20task%2C%20addressing%20critical%20issues%20such%20as%20predication%0Adisagreement%20and%20label-noise%20suppression%20requires%20further%20attention%20and%0Aprogress%20in%20cross-supervised%20training.%20In%20this%20paper%2C%20we%20introduce%20an%0AEvidential%20Tri-Branch%20Consistency%20learning%20framework%20%28ETC-Net%29%20for%0Asemi-supervised%20medical%20image%20segmentation.%20ETC-Net%20employs%20three%20branches%3A%20an%0Aevidential%20conservative%20branch%2C%20an%20evidential%20progressive%20branch%2C%20and%20an%0Aevidential%20fusion%20branch.%20The%20first%20two%20branches%20exhibit%20complementary%0Acharacteristics%2C%20allowing%20them%20to%20address%20prediction%20diversity%20and%20enhance%0Atraining%20stability.%20We%20also%20integrate%20uncertainty%20estimation%20from%20the%0Aevidential%20learning%20into%20cross-supervised%20training%2C%20mitigating%20the%20negative%0Aimpact%20of%20erroneous%20supervision%20signals.%20Additionally%2C%20the%20evidential%20fusion%0Abranch%20capitalizes%20on%20the%20complementary%20attributes%20of%20the%20first%20two%20branches%0Aand%20leverages%20an%20evidence-based%20Dempster-Shafer%20fusion%20strategy%2C%20supervised%20by%0Amore%20reliable%20and%20accurate%20pseudo-labels%20of%20unlabeled%20data.%20Extensive%0Aexperiments%20conducted%20on%20LA%2C%20Pancreas-CT%2C%20and%20ACDC%20datasets%20demonstrate%20that%0AETC-Net%20surpasses%20other%20state-of-the-art%20methods%20for%20semi-supervised%0Asegmentation.%20The%20code%20will%20be%20made%20available%20in%20the%20near%20future%20at%0Ahttps%3A//github.com/Medsemiseg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07032v1&entry.124074799=Read"},
{"title": "Expediting Building Footprint Extraction from High-resolution Remote\n  Sensing Images via progressive lenient supervision", "author": "Haonan Guo and Bo Du and Chen Wu and Xin Su and Liangpei Zhang", "abstract": "  The efficacy of building footprint segmentation from remotely sensed images\nhas been hindered by model transfer effectiveness. Many existing building\nsegmentation methods were developed upon the encoder-decoder architecture of\nU-Net, in which the encoder is finetuned from the newly developed backbone\nnetworks that are pre-trained on ImageNet. However, the heavy computational\nburden of the existing decoder designs hampers the successful transfer of these\nmodern encoder networks to remote sensing tasks. Even the widely-adopted deep\nsupervision strategy fails to mitigate these challenges due to its invalid loss\nin hybrid regions where foreground and background pixels are intermixed. In\nthis paper, we conduct a comprehensive evaluation of existing decoder network\ndesigns for building footprint segmentation and propose an efficient framework\ndenoted as BFSeg to enhance learning efficiency and effectiveness.\nSpecifically, a densely-connected coarse-to-fine feature fusion decoder network\nthat facilitates easy and fast feature fusion across scales is proposed.\nMoreover, considering the invalidity of hybrid regions in the down-sampled\nground truth during the deep supervision process, we present a lenient deep\nsupervision and distillation strategy that enables the network to learn proper\nknowledge from deep supervision. Building upon these advancements, we have\ndeveloped a new family of building segmentation networks, which consistently\nsurpass prior works with outstanding performance and efficiency across a wide\nrange of newly developed encoder networks.\n", "link": "http://arxiv.org/abs/2307.12220v2", "date": "2024-04-10", "relevancy": 2.1979, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5591}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5531}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5384}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Expediting%20Building%20Footprint%20Extraction%20from%20High-resolution%20Remote%0A%20%20Sensing%20Images%20via%20progressive%20lenient%20supervision&body=Title%3A%20Expediting%20Building%20Footprint%20Extraction%20from%20High-resolution%20Remote%0A%20%20Sensing%20Images%20via%20progressive%20lenient%20supervision%0AAuthor%3A%20Haonan%20Guo%20and%20Bo%20Du%20and%20Chen%20Wu%20and%20Xin%20Su%20and%20Liangpei%20Zhang%0AAbstract%3A%20%20%20The%20efficacy%20of%20building%20footprint%20segmentation%20from%20remotely%20sensed%20images%0Ahas%20been%20hindered%20by%20model%20transfer%20effectiveness.%20Many%20existing%20building%0Asegmentation%20methods%20were%20developed%20upon%20the%20encoder-decoder%20architecture%20of%0AU-Net%2C%20in%20which%20the%20encoder%20is%20finetuned%20from%20the%20newly%20developed%20backbone%0Anetworks%20that%20are%20pre-trained%20on%20ImageNet.%20However%2C%20the%20heavy%20computational%0Aburden%20of%20the%20existing%20decoder%20designs%20hampers%20the%20successful%20transfer%20of%20these%0Amodern%20encoder%20networks%20to%20remote%20sensing%20tasks.%20Even%20the%20widely-adopted%20deep%0Asupervision%20strategy%20fails%20to%20mitigate%20these%20challenges%20due%20to%20its%20invalid%20loss%0Ain%20hybrid%20regions%20where%20foreground%20and%20background%20pixels%20are%20intermixed.%20In%0Athis%20paper%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20existing%20decoder%20network%0Adesigns%20for%20building%20footprint%20segmentation%20and%20propose%20an%20efficient%20framework%0Adenoted%20as%20BFSeg%20to%20enhance%20learning%20efficiency%20and%20effectiveness.%0ASpecifically%2C%20a%20densely-connected%20coarse-to-fine%20feature%20fusion%20decoder%20network%0Athat%20facilitates%20easy%20and%20fast%20feature%20fusion%20across%20scales%20is%20proposed.%0AMoreover%2C%20considering%20the%20invalidity%20of%20hybrid%20regions%20in%20the%20down-sampled%0Aground%20truth%20during%20the%20deep%20supervision%20process%2C%20we%20present%20a%20lenient%20deep%0Asupervision%20and%20distillation%20strategy%20that%20enables%20the%20network%20to%20learn%20proper%0Aknowledge%20from%20deep%20supervision.%20Building%20upon%20these%20advancements%2C%20we%20have%0Adeveloped%20a%20new%20family%20of%20building%20segmentation%20networks%2C%20which%20consistently%0Asurpass%20prior%20works%20with%20outstanding%20performance%20and%20efficiency%20across%20a%20wide%0Arange%20of%20newly%20developed%20encoder%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.12220v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expediting%20Building%20Footprint%20Extraction%20from%20High-resolution%20Remote%0A%20%20Sensing%20Images%20via%20progressive%20lenient%20supervision&entry.906535625=Haonan%20Guo%20and%20Bo%20Du%20and%20Chen%20Wu%20and%20Xin%20Su%20and%20Liangpei%20Zhang&entry.1292438233=%20%20The%20efficacy%20of%20building%20footprint%20segmentation%20from%20remotely%20sensed%20images%0Ahas%20been%20hindered%20by%20model%20transfer%20effectiveness.%20Many%20existing%20building%0Asegmentation%20methods%20were%20developed%20upon%20the%20encoder-decoder%20architecture%20of%0AU-Net%2C%20in%20which%20the%20encoder%20is%20finetuned%20from%20the%20newly%20developed%20backbone%0Anetworks%20that%20are%20pre-trained%20on%20ImageNet.%20However%2C%20the%20heavy%20computational%0Aburden%20of%20the%20existing%20decoder%20designs%20hampers%20the%20successful%20transfer%20of%20these%0Amodern%20encoder%20networks%20to%20remote%20sensing%20tasks.%20Even%20the%20widely-adopted%20deep%0Asupervision%20strategy%20fails%20to%20mitigate%20these%20challenges%20due%20to%20its%20invalid%20loss%0Ain%20hybrid%20regions%20where%20foreground%20and%20background%20pixels%20are%20intermixed.%20In%0Athis%20paper%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20existing%20decoder%20network%0Adesigns%20for%20building%20footprint%20segmentation%20and%20propose%20an%20efficient%20framework%0Adenoted%20as%20BFSeg%20to%20enhance%20learning%20efficiency%20and%20effectiveness.%0ASpecifically%2C%20a%20densely-connected%20coarse-to-fine%20feature%20fusion%20decoder%20network%0Athat%20facilitates%20easy%20and%20fast%20feature%20fusion%20across%20scales%20is%20proposed.%0AMoreover%2C%20considering%20the%20invalidity%20of%20hybrid%20regions%20in%20the%20down-sampled%0Aground%20truth%20during%20the%20deep%20supervision%20process%2C%20we%20present%20a%20lenient%20deep%0Asupervision%20and%20distillation%20strategy%20that%20enables%20the%20network%20to%20learn%20proper%0Aknowledge%20from%20deep%20supervision.%20Building%20upon%20these%20advancements%2C%20we%20have%0Adeveloped%20a%20new%20family%20of%20building%20segmentation%20networks%2C%20which%20consistently%0Asurpass%20prior%20works%20with%20outstanding%20performance%20and%20efficiency%20across%20a%20wide%0Arange%20of%20newly%20developed%20encoder%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.12220v2&entry.124074799=Read"},
{"title": "Accelerating Cardiac MRI Reconstruction with CMRatt: An Attention-Driven\n  Approach", "author": "Anam Hashmi and Julia Dietlmeier and Kathleen M. Curran and Noel E. O'Connor", "abstract": "  Cine cardiac magnetic resonance (CMR) imaging is recognised as the benchmark\nmodality for the comprehensive assessment of cardiac function. Nevertheless,\nthe acquisition process of cine CMR is considered as an impediment due to its\nprolonged scanning time. One commonly used strategy to expedite the acquisition\nprocess is through k-space undersampling, though it comes with a drawback of\nintroducing aliasing effects in the reconstructed image. Lately, deep\nlearning-based methods have shown remarkable results over traditional\napproaches in rapidly achieving precise CMR reconstructed images. This study\naims to explore the untapped potential of attention mechanisms incorporated\nwith a deep learning model within the context of the CMR reconstruction\nproblem. We are motivated by the fact that attention has proven beneficial in\ndownstream tasks such as image classification and segmentation, but has not\nbeen systematically analysed in the context of CMR reconstruction. Our primary\ngoal is to identify the strengths and potential limitations of attention\nalgorithms when integrated with a convolutional backbone model such as a U-Net.\nTo achieve this, we benchmark different state-of-the-art spatial and channel\nattention mechanisms on the CMRxRecon dataset and quantitatively evaluate the\nquality of reconstruction using objective metrics. Furthermore, inspired by the\nbest performing attention mechanism, we propose a new, simple yet effective,\nattention pipeline specifically optimised for the task of cardiac image\nreconstruction that outperforms other state-of-the-art attention methods. The\nlayer and model code will be made publicly available.\n", "link": "http://arxiv.org/abs/2404.06941v1", "date": "2024-04-10", "relevancy": 2.1912, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5698}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Cardiac%20MRI%20Reconstruction%20with%20CMRatt%3A%20An%20Attention-Driven%0A%20%20Approach&body=Title%3A%20Accelerating%20Cardiac%20MRI%20Reconstruction%20with%20CMRatt%3A%20An%20Attention-Driven%0A%20%20Approach%0AAuthor%3A%20Anam%20Hashmi%20and%20Julia%20Dietlmeier%20and%20Kathleen%20M.%20Curran%20and%20Noel%20E.%20O%27Connor%0AAbstract%3A%20%20%20Cine%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging%20is%20recognised%20as%20the%20benchmark%0Amodality%20for%20the%20comprehensive%20assessment%20of%20cardiac%20function.%20Nevertheless%2C%0Athe%20acquisition%20process%20of%20cine%20CMR%20is%20considered%20as%20an%20impediment%20due%20to%20its%0Aprolonged%20scanning%20time.%20One%20commonly%20used%20strategy%20to%20expedite%20the%20acquisition%0Aprocess%20is%20through%20k-space%20undersampling%2C%20though%20it%20comes%20with%20a%20drawback%20of%0Aintroducing%20aliasing%20effects%20in%20the%20reconstructed%20image.%20Lately%2C%20deep%0Alearning-based%20methods%20have%20shown%20remarkable%20results%20over%20traditional%0Aapproaches%20in%20rapidly%20achieving%20precise%20CMR%20reconstructed%20images.%20This%20study%0Aaims%20to%20explore%20the%20untapped%20potential%20of%20attention%20mechanisms%20incorporated%0Awith%20a%20deep%20learning%20model%20within%20the%20context%20of%20the%20CMR%20reconstruction%0Aproblem.%20We%20are%20motivated%20by%20the%20fact%20that%20attention%20has%20proven%20beneficial%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20segmentation%2C%20but%20has%20not%0Abeen%20systematically%20analysed%20in%20the%20context%20of%20CMR%20reconstruction.%20Our%20primary%0Agoal%20is%20to%20identify%20the%20strengths%20and%20potential%20limitations%20of%20attention%0Aalgorithms%20when%20integrated%20with%20a%20convolutional%20backbone%20model%20such%20as%20a%20U-Net.%0ATo%20achieve%20this%2C%20we%20benchmark%20different%20state-of-the-art%20spatial%20and%20channel%0Aattention%20mechanisms%20on%20the%20CMRxRecon%20dataset%20and%20quantitatively%20evaluate%20the%0Aquality%20of%20reconstruction%20using%20objective%20metrics.%20Furthermore%2C%20inspired%20by%20the%0Abest%20performing%20attention%20mechanism%2C%20we%20propose%20a%20new%2C%20simple%20yet%20effective%2C%0Aattention%20pipeline%20specifically%20optimised%20for%20the%20task%20of%20cardiac%20image%0Areconstruction%20that%20outperforms%20other%20state-of-the-art%20attention%20methods.%20The%0Alayer%20and%20model%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06941v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Cardiac%20MRI%20Reconstruction%20with%20CMRatt%3A%20An%20Attention-Driven%0A%20%20Approach&entry.906535625=Anam%20Hashmi%20and%20Julia%20Dietlmeier%20and%20Kathleen%20M.%20Curran%20and%20Noel%20E.%20O%27Connor&entry.1292438233=%20%20Cine%20cardiac%20magnetic%20resonance%20%28CMR%29%20imaging%20is%20recognised%20as%20the%20benchmark%0Amodality%20for%20the%20comprehensive%20assessment%20of%20cardiac%20function.%20Nevertheless%2C%0Athe%20acquisition%20process%20of%20cine%20CMR%20is%20considered%20as%20an%20impediment%20due%20to%20its%0Aprolonged%20scanning%20time.%20One%20commonly%20used%20strategy%20to%20expedite%20the%20acquisition%0Aprocess%20is%20through%20k-space%20undersampling%2C%20though%20it%20comes%20with%20a%20drawback%20of%0Aintroducing%20aliasing%20effects%20in%20the%20reconstructed%20image.%20Lately%2C%20deep%0Alearning-based%20methods%20have%20shown%20remarkable%20results%20over%20traditional%0Aapproaches%20in%20rapidly%20achieving%20precise%20CMR%20reconstructed%20images.%20This%20study%0Aaims%20to%20explore%20the%20untapped%20potential%20of%20attention%20mechanisms%20incorporated%0Awith%20a%20deep%20learning%20model%20within%20the%20context%20of%20the%20CMR%20reconstruction%0Aproblem.%20We%20are%20motivated%20by%20the%20fact%20that%20attention%20has%20proven%20beneficial%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20segmentation%2C%20but%20has%20not%0Abeen%20systematically%20analysed%20in%20the%20context%20of%20CMR%20reconstruction.%20Our%20primary%0Agoal%20is%20to%20identify%20the%20strengths%20and%20potential%20limitations%20of%20attention%0Aalgorithms%20when%20integrated%20with%20a%20convolutional%20backbone%20model%20such%20as%20a%20U-Net.%0ATo%20achieve%20this%2C%20we%20benchmark%20different%20state-of-the-art%20spatial%20and%20channel%0Aattention%20mechanisms%20on%20the%20CMRxRecon%20dataset%20and%20quantitatively%20evaluate%20the%0Aquality%20of%20reconstruction%20using%20objective%20metrics.%20Furthermore%2C%20inspired%20by%20the%0Abest%20performing%20attention%20mechanism%2C%20we%20propose%20a%20new%2C%20simple%20yet%20effective%2C%0Aattention%20pipeline%20specifically%20optimised%20for%20the%20task%20of%20cardiac%20image%0Areconstruction%20that%20outperforms%20other%20state-of-the-art%20attention%20methods.%20The%0Alayer%20and%20model%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06941v1&entry.124074799=Read"},
{"title": "Unified Language-driven Zero-shot Domain Adaptation", "author": "Senqiao Yang and Zhuotao Tian and Li Jiang and Jiaya Jia", "abstract": "  This paper introduces Unified Language-driven Zero-shot Domain Adaptation\n(ULDA), a novel task setting that enables a single model to adapt to diverse\ntarget domains without explicit domain-ID knowledge. We identify the\nconstraints in the existing language-driven zero-shot domain adaptation task,\nparticularly the requirement for domain IDs and domain-specific models, which\nmay restrict flexibility and scalability. To overcome these issues, we propose\na new framework for ULDA, consisting of Hierarchical Context Alignment (HCA),\nDomain Consistent Representation Learning (DCRL), and Text-Driven Rectifier\n(TDR). These components work synergistically to align simulated features with\ntarget text across multiple visual levels, retain semantic correlations between\ndifferent regional representations, and rectify biases between simulated and\nreal target visual features, respectively. Our extensive empirical evaluations\ndemonstrate that this framework achieves competitive performance in both\nsettings, surpassing even the model that requires domain-ID, showcasing its\nsuperiority and generalization ability. The proposed method is not only\neffective but also maintains practicality and efficiency, as it does not\nintroduce additional computational costs during inference. Our project page is\nhttps://senqiaoyang.com/project/ULDA .\n", "link": "http://arxiv.org/abs/2404.07155v1", "date": "2024-04-10", "relevancy": 2.1867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5388}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unified%20Language-driven%20Zero-shot%20Domain%20Adaptation&body=Title%3A%20Unified%20Language-driven%20Zero-shot%20Domain%20Adaptation%0AAuthor%3A%20Senqiao%20Yang%20and%20Zhuotao%20Tian%20and%20Li%20Jiang%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20This%20paper%20introduces%20Unified%20Language-driven%20Zero-shot%20Domain%20Adaptation%0A%28ULDA%29%2C%20a%20novel%20task%20setting%20that%20enables%20a%20single%20model%20to%20adapt%20to%20diverse%0Atarget%20domains%20without%20explicit%20domain-ID%20knowledge.%20We%20identify%20the%0Aconstraints%20in%20the%20existing%20language-driven%20zero-shot%20domain%20adaptation%20task%2C%0Aparticularly%20the%20requirement%20for%20domain%20IDs%20and%20domain-specific%20models%2C%20which%0Amay%20restrict%20flexibility%20and%20scalability.%20To%20overcome%20these%20issues%2C%20we%20propose%0Aa%20new%20framework%20for%20ULDA%2C%20consisting%20of%20Hierarchical%20Context%20Alignment%20%28HCA%29%2C%0ADomain%20Consistent%20Representation%20Learning%20%28DCRL%29%2C%20and%20Text-Driven%20Rectifier%0A%28TDR%29.%20These%20components%20work%20synergistically%20to%20align%20simulated%20features%20with%0Atarget%20text%20across%20multiple%20visual%20levels%2C%20retain%20semantic%20correlations%20between%0Adifferent%20regional%20representations%2C%20and%20rectify%20biases%20between%20simulated%20and%0Areal%20target%20visual%20features%2C%20respectively.%20Our%20extensive%20empirical%20evaluations%0Ademonstrate%20that%20this%20framework%20achieves%20competitive%20performance%20in%20both%0Asettings%2C%20surpassing%20even%20the%20model%20that%20requires%20domain-ID%2C%20showcasing%20its%0Asuperiority%20and%20generalization%20ability.%20The%20proposed%20method%20is%20not%20only%0Aeffective%20but%20also%20maintains%20practicality%20and%20efficiency%2C%20as%20it%20does%20not%0Aintroduce%20additional%20computational%20costs%20during%20inference.%20Our%20project%20page%20is%0Ahttps%3A//senqiaoyang.com/project/ULDA%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07155v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Language-driven%20Zero-shot%20Domain%20Adaptation&entry.906535625=Senqiao%20Yang%20and%20Zhuotao%20Tian%20and%20Li%20Jiang%20and%20Jiaya%20Jia&entry.1292438233=%20%20This%20paper%20introduces%20Unified%20Language-driven%20Zero-shot%20Domain%20Adaptation%0A%28ULDA%29%2C%20a%20novel%20task%20setting%20that%20enables%20a%20single%20model%20to%20adapt%20to%20diverse%0Atarget%20domains%20without%20explicit%20domain-ID%20knowledge.%20We%20identify%20the%0Aconstraints%20in%20the%20existing%20language-driven%20zero-shot%20domain%20adaptation%20task%2C%0Aparticularly%20the%20requirement%20for%20domain%20IDs%20and%20domain-specific%20models%2C%20which%0Amay%20restrict%20flexibility%20and%20scalability.%20To%20overcome%20these%20issues%2C%20we%20propose%0Aa%20new%20framework%20for%20ULDA%2C%20consisting%20of%20Hierarchical%20Context%20Alignment%20%28HCA%29%2C%0ADomain%20Consistent%20Representation%20Learning%20%28DCRL%29%2C%20and%20Text-Driven%20Rectifier%0A%28TDR%29.%20These%20components%20work%20synergistically%20to%20align%20simulated%20features%20with%0Atarget%20text%20across%20multiple%20visual%20levels%2C%20retain%20semantic%20correlations%20between%0Adifferent%20regional%20representations%2C%20and%20rectify%20biases%20between%20simulated%20and%0Areal%20target%20visual%20features%2C%20respectively.%20Our%20extensive%20empirical%20evaluations%0Ademonstrate%20that%20this%20framework%20achieves%20competitive%20performance%20in%20both%0Asettings%2C%20surpassing%20even%20the%20model%20that%20requires%20domain-ID%2C%20showcasing%20its%0Asuperiority%20and%20generalization%20ability.%20The%20proposed%20method%20is%20not%20only%0Aeffective%20but%20also%20maintains%20practicality%20and%20efficiency%2C%20as%20it%20does%20not%0Aintroduce%20additional%20computational%20costs%20during%20inference.%20Our%20project%20page%20is%0Ahttps%3A//senqiaoyang.com/project/ULDA%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07155v1&entry.124074799=Read"},
{"title": "Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D\n  Gaussian Splatting", "author": "Xiaolei Lang and Laijian Li and Hang Zhang and Feng Xiong and Mu Xu and Yong Liu and Xingxing Zuo and Jiajun Lv", "abstract": "  We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian\nSplatting as the mapping backend. Leveraging robust pose estimates from our\nLiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic\nmapping system is proposed in this paper. We initialize 3D Gaussians from\ncolorized LiDAR points and optimize them using differentiable rendering powered\nby 3D Gaussian Splatting. Meticulously designed strategies are employed to\nincrementally expand the Gaussian map and adaptively control its density,\nensuring high-quality mapping with real-time capability. Experiments conducted\nin diverse scenarios demonstrate the superior performance of our method\ncompared to existing radiance-field-based SLAM systems.\n", "link": "http://arxiv.org/abs/2404.06926v1", "date": "2024-04-10", "relevancy": 2.1654, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5041}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gaussian-LIC%3A%20Photo-realistic%20LiDAR-Inertial-Camera%20SLAM%20with%203D%0A%20%20Gaussian%20Splatting&body=Title%3A%20Gaussian-LIC%3A%20Photo-realistic%20LiDAR-Inertial-Camera%20SLAM%20with%203D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Hang%20Zhang%20and%20Feng%20Xiong%20and%20Mu%20Xu%20and%20Yong%20Liu%20and%20Xingxing%20Zuo%20and%20Jiajun%20Lv%0AAbstract%3A%20%20%20We%20present%20a%20real-time%20LiDAR-Inertial-Camera%20SLAM%20system%20with%203D%20Gaussian%0ASplatting%20as%20the%20mapping%20backend.%20Leveraging%20robust%20pose%20estimates%20from%20our%0ALiDAR-Inertial-Camera%20odometry%2C%20Coco-LIC%2C%20an%20incremental%20photo-realistic%0Amapping%20system%20is%20proposed%20in%20this%20paper.%20We%20initialize%203D%20Gaussians%20from%0Acolorized%20LiDAR%20points%20and%20optimize%20them%20using%20differentiable%20rendering%20powered%0Aby%203D%20Gaussian%20Splatting.%20Meticulously%20designed%20strategies%20are%20employed%20to%0Aincrementally%20expand%20the%20Gaussian%20map%20and%20adaptively%20control%20its%20density%2C%0Aensuring%20high-quality%20mapping%20with%20real-time%20capability.%20Experiments%20conducted%0Ain%20diverse%20scenarios%20demonstrate%20the%20superior%20performance%20of%20our%20method%0Acompared%20to%20existing%20radiance-field-based%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06926v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian-LIC%3A%20Photo-realistic%20LiDAR-Inertial-Camera%20SLAM%20with%203D%0A%20%20Gaussian%20Splatting&entry.906535625=Xiaolei%20Lang%20and%20Laijian%20Li%20and%20Hang%20Zhang%20and%20Feng%20Xiong%20and%20Mu%20Xu%20and%20Yong%20Liu%20and%20Xingxing%20Zuo%20and%20Jiajun%20Lv&entry.1292438233=%20%20We%20present%20a%20real-time%20LiDAR-Inertial-Camera%20SLAM%20system%20with%203D%20Gaussian%0ASplatting%20as%20the%20mapping%20backend.%20Leveraging%20robust%20pose%20estimates%20from%20our%0ALiDAR-Inertial-Camera%20odometry%2C%20Coco-LIC%2C%20an%20incremental%20photo-realistic%0Amapping%20system%20is%20proposed%20in%20this%20paper.%20We%20initialize%203D%20Gaussians%20from%0Acolorized%20LiDAR%20points%20and%20optimize%20them%20using%20differentiable%20rendering%20powered%0Aby%203D%20Gaussian%20Splatting.%20Meticulously%20designed%20strategies%20are%20employed%20to%0Aincrementally%20expand%20the%20Gaussian%20map%20and%20adaptively%20control%20its%20density%2C%0Aensuring%20high-quality%20mapping%20with%20real-time%20capability.%20Experiments%20conducted%0Ain%20diverse%20scenarios%20demonstrate%20the%20superior%20performance%20of%20our%20method%0Acompared%20to%20existing%20radiance-field-based%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06926v1&entry.124074799=Read"},
{"title": "VLLMs Provide Better Context for Emotion Understanding Through Common\n  Sense Reasoning", "author": "Alexandros Xenos and Niki Maria Foteinopoulou and Ioanna Ntinou and Ioannis Patras and Georgios Tzimiropoulos", "abstract": "  Recognising emotions in context involves identifying the apparent emotions of\nan individual, taking into account contextual cues from the surrounding scene.\nPrevious approaches to this task have involved the design of explicit\nscene-encoding architectures or the incorporation of external scene-related\ninformation, such as captions. However, these methods often utilise limited\ncontextual information or rely on intricate training pipelines. In this work,\nwe leverage the groundbreaking capabilities of Vision-and-Large-Language Models\n(VLLMs) to enhance in-context emotion classification without introducing\ncomplexity to the training process in a two-stage approach. In the first stage,\nwe propose prompting VLLMs to generate descriptions in natural language of the\nsubject's apparent emotion relative to the visual context. In the second stage,\nthe descriptions are used as contextual information and, along with the image\ninput, are used to train a transformer-based architecture that fuses text and\nvisual features before the final classification task. Our experimental results\nshow that the text and image features have complementary information, and our\nfused architecture significantly outperforms the individual modalities without\nany complex training methods. We evaluate our approach on three different\ndatasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art or\ncomparable accuracy across all datasets and metrics compared to much more\ncomplex approaches. The code will be made publicly available on github:\nhttps://github.com/NickyFot/EmoCommonSense.git\n", "link": "http://arxiv.org/abs/2404.07078v1", "date": "2024-04-10", "relevancy": 2.1648, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.549}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5385}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VLLMs%20Provide%20Better%20Context%20for%20Emotion%20Understanding%20Through%20Common%0A%20%20Sense%20Reasoning&body=Title%3A%20VLLMs%20Provide%20Better%20Context%20for%20Emotion%20Understanding%20Through%20Common%0A%20%20Sense%20Reasoning%0AAuthor%3A%20Alexandros%20Xenos%20and%20Niki%20Maria%20Foteinopoulou%20and%20Ioanna%20Ntinou%20and%20Ioannis%20Patras%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20Recognising%20emotions%20in%20context%20involves%20identifying%20the%20apparent%20emotions%20of%0Aan%20individual%2C%20taking%20into%20account%20contextual%20cues%20from%20the%20surrounding%20scene.%0APrevious%20approaches%20to%20this%20task%20have%20involved%20the%20design%20of%20explicit%0Ascene-encoding%20architectures%20or%20the%20incorporation%20of%20external%20scene-related%0Ainformation%2C%20such%20as%20captions.%20However%2C%20these%20methods%20often%20utilise%20limited%0Acontextual%20information%20or%20rely%20on%20intricate%20training%20pipelines.%20In%20this%20work%2C%0Awe%20leverage%20the%20groundbreaking%20capabilities%20of%20Vision-and-Large-Language%20Models%0A%28VLLMs%29%20to%20enhance%20in-context%20emotion%20classification%20without%20introducing%0Acomplexity%20to%20the%20training%20process%20in%20a%20two-stage%20approach.%20In%20the%20first%20stage%2C%0Awe%20propose%20prompting%20VLLMs%20to%20generate%20descriptions%20in%20natural%20language%20of%20the%0Asubject%27s%20apparent%20emotion%20relative%20to%20the%20visual%20context.%20In%20the%20second%20stage%2C%0Athe%20descriptions%20are%20used%20as%20contextual%20information%20and%2C%20along%20with%20the%20image%0Ainput%2C%20are%20used%20to%20train%20a%20transformer-based%20architecture%20that%20fuses%20text%20and%0Avisual%20features%20before%20the%20final%20classification%20task.%20Our%20experimental%20results%0Ashow%20that%20the%20text%20and%20image%20features%20have%20complementary%20information%2C%20and%20our%0Afused%20architecture%20significantly%20outperforms%20the%20individual%20modalities%20without%0Aany%20complex%20training%20methods.%20We%20evaluate%20our%20approach%20on%20three%20different%0Adatasets%2C%20namely%2C%20EMOTIC%2C%20CAER-S%2C%20and%20BoLD%2C%20and%20achieve%20state-of-the-art%20or%0Acomparable%20accuracy%20across%20all%20datasets%20and%20metrics%20compared%20to%20much%20more%0Acomplex%20approaches.%20The%20code%20will%20be%20made%20publicly%20available%20on%20github%3A%0Ahttps%3A//github.com/NickyFot/EmoCommonSense.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07078v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLLMs%20Provide%20Better%20Context%20for%20Emotion%20Understanding%20Through%20Common%0A%20%20Sense%20Reasoning&entry.906535625=Alexandros%20Xenos%20and%20Niki%20Maria%20Foteinopoulou%20and%20Ioanna%20Ntinou%20and%20Ioannis%20Patras%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20Recognising%20emotions%20in%20context%20involves%20identifying%20the%20apparent%20emotions%20of%0Aan%20individual%2C%20taking%20into%20account%20contextual%20cues%20from%20the%20surrounding%20scene.%0APrevious%20approaches%20to%20this%20task%20have%20involved%20the%20design%20of%20explicit%0Ascene-encoding%20architectures%20or%20the%20incorporation%20of%20external%20scene-related%0Ainformation%2C%20such%20as%20captions.%20However%2C%20these%20methods%20often%20utilise%20limited%0Acontextual%20information%20or%20rely%20on%20intricate%20training%20pipelines.%20In%20this%20work%2C%0Awe%20leverage%20the%20groundbreaking%20capabilities%20of%20Vision-and-Large-Language%20Models%0A%28VLLMs%29%20to%20enhance%20in-context%20emotion%20classification%20without%20introducing%0Acomplexity%20to%20the%20training%20process%20in%20a%20two-stage%20approach.%20In%20the%20first%20stage%2C%0Awe%20propose%20prompting%20VLLMs%20to%20generate%20descriptions%20in%20natural%20language%20of%20the%0Asubject%27s%20apparent%20emotion%20relative%20to%20the%20visual%20context.%20In%20the%20second%20stage%2C%0Athe%20descriptions%20are%20used%20as%20contextual%20information%20and%2C%20along%20with%20the%20image%0Ainput%2C%20are%20used%20to%20train%20a%20transformer-based%20architecture%20that%20fuses%20text%20and%0Avisual%20features%20before%20the%20final%20classification%20task.%20Our%20experimental%20results%0Ashow%20that%20the%20text%20and%20image%20features%20have%20complementary%20information%2C%20and%20our%0Afused%20architecture%20significantly%20outperforms%20the%20individual%20modalities%20without%0Aany%20complex%20training%20methods.%20We%20evaluate%20our%20approach%20on%20three%20different%0Adatasets%2C%20namely%2C%20EMOTIC%2C%20CAER-S%2C%20and%20BoLD%2C%20and%20achieve%20state-of-the-art%20or%0Acomparable%20accuracy%20across%20all%20datasets%20and%20metrics%20compared%20to%20much%20more%0Acomplex%20approaches.%20The%20code%20will%20be%20made%20publicly%20available%20on%20github%3A%0Ahttps%3A//github.com/NickyFot/EmoCommonSense.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07078v1&entry.124074799=Read"},
{"title": "Location-guided Head Pose Estimation for Fisheye Image", "author": "Bing Li and Dong Zhang and Cheng Huang and Yun Xian and Ming Li and Dah-Jye Lee", "abstract": "  Camera with a fisheye or ultra-wide lens covers a wide field of view that\ncannot be modeled by the perspective projection. Serious fisheye lens\ndistortion in the peripheral region of the image leads to degraded performance\nof the existing head pose estimation models trained on undistorted images. This\npaper presents a new approach for head pose estimation that uses the knowledge\nof head location in the image to reduce the negative effect of fisheye\ndistortion. We develop an end-to-end convolutional neural network to estimate\nthe head pose with the multi-task learning of head pose and head location. Our\nproposed network estimates the head pose directly from the fisheye image\nwithout the operation of rectification or calibration. We also created a\nfisheye-distorted version of the three popular head pose estimation datasets,\nBIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that\nour network remarkably improves the accuracy of head pose estimation compared\nwith other state-of-the-art one-stage and two-stage methods.\n", "link": "http://arxiv.org/abs/2402.18320v2", "date": "2024-04-10", "relevancy": 2.128, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5762}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5079}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Location-guided%20Head%20Pose%20Estimation%20for%20Fisheye%20Image&body=Title%3A%20Location-guided%20Head%20Pose%20Estimation%20for%20Fisheye%20Image%0AAuthor%3A%20Bing%20Li%20and%20Dong%20Zhang%20and%20Cheng%20Huang%20and%20Yun%20Xian%20and%20Ming%20Li%20and%20Dah-Jye%20Lee%0AAbstract%3A%20%20%20Camera%20with%20a%20fisheye%20or%20ultra-wide%20lens%20covers%20a%20wide%20field%20of%20view%20that%0Acannot%20be%20modeled%20by%20the%20perspective%20projection.%20Serious%20fisheye%20lens%0Adistortion%20in%20the%20peripheral%20region%20of%20the%20image%20leads%20to%20degraded%20performance%0Aof%20the%20existing%20head%20pose%20estimation%20models%20trained%20on%20undistorted%20images.%20This%0Apaper%20presents%20a%20new%20approach%20for%20head%20pose%20estimation%20that%20uses%20the%20knowledge%0Aof%20head%20location%20in%20the%20image%20to%20reduce%20the%20negative%20effect%20of%20fisheye%0Adistortion.%20We%20develop%20an%20end-to-end%20convolutional%20neural%20network%20to%20estimate%0Athe%20head%20pose%20with%20the%20multi-task%20learning%20of%20head%20pose%20and%20head%20location.%20Our%0Aproposed%20network%20estimates%20the%20head%20pose%20directly%20from%20the%20fisheye%20image%0Awithout%20the%20operation%20of%20rectification%20or%20calibration.%20We%20also%20created%20a%0Afisheye-distorted%20version%20of%20the%20three%20popular%20head%20pose%20estimation%20datasets%2C%0ABIWI%2C%20300W-LP%2C%20and%20AFLW2000%20for%20our%20experiments.%20Experiments%20results%20show%20that%0Aour%20network%20remarkably%20improves%20the%20accuracy%20of%20head%20pose%20estimation%20compared%0Awith%20other%20state-of-the-art%20one-stage%20and%20two-stage%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18320v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Location-guided%20Head%20Pose%20Estimation%20for%20Fisheye%20Image&entry.906535625=Bing%20Li%20and%20Dong%20Zhang%20and%20Cheng%20Huang%20and%20Yun%20Xian%20and%20Ming%20Li%20and%20Dah-Jye%20Lee&entry.1292438233=%20%20Camera%20with%20a%20fisheye%20or%20ultra-wide%20lens%20covers%20a%20wide%20field%20of%20view%20that%0Acannot%20be%20modeled%20by%20the%20perspective%20projection.%20Serious%20fisheye%20lens%0Adistortion%20in%20the%20peripheral%20region%20of%20the%20image%20leads%20to%20degraded%20performance%0Aof%20the%20existing%20head%20pose%20estimation%20models%20trained%20on%20undistorted%20images.%20This%0Apaper%20presents%20a%20new%20approach%20for%20head%20pose%20estimation%20that%20uses%20the%20knowledge%0Aof%20head%20location%20in%20the%20image%20to%20reduce%20the%20negative%20effect%20of%20fisheye%0Adistortion.%20We%20develop%20an%20end-to-end%20convolutional%20neural%20network%20to%20estimate%0Athe%20head%20pose%20with%20the%20multi-task%20learning%20of%20head%20pose%20and%20head%20location.%20Our%0Aproposed%20network%20estimates%20the%20head%20pose%20directly%20from%20the%20fisheye%20image%0Awithout%20the%20operation%20of%20rectification%20or%20calibration.%20We%20also%20created%20a%0Afisheye-distorted%20version%20of%20the%20three%20popular%20head%20pose%20estimation%20datasets%2C%0ABIWI%2C%20300W-LP%2C%20and%20AFLW2000%20for%20our%20experiments.%20Experiments%20results%20show%20that%0Aour%20network%20remarkably%20improves%20the%20accuracy%20of%20head%20pose%20estimation%20compared%0Awith%20other%20state-of-the-art%20one-stage%20and%20two-stage%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18320v2&entry.124074799=Read"},
{"title": "TrajPRed: Trajectory Prediction with Region-based Relation Learning", "author": "Chen Zhou and Ghassan AlRegib and Armin Parchami and Kunjan Singh", "abstract": "  Forecasting human trajectories in traffic scenes is critical for safety\nwithin mixed or fully autonomous systems. Human future trajectories are driven\nby two major stimuli, social interactions, and stochastic goals. Thus, reliable\nforecasting needs to capture these two stimuli. Edge-based relation modeling\nrepresents social interactions using pairwise correlations from precise\nindividual states. Nevertheless, edge-based relations can be vulnerable under\nperturbations. To alleviate these issues, we propose a region-based relation\nlearning paradigm that models social interactions via region-wise dynamics of\njoint states, i.e., the changes in the density of crowds. In particular,\nregion-wise agent joint information is encoded within convolutional feature\ngrids. Social relations are modeled by relating the temporal changes of local\njoint information from a global perspective. We show that region-based\nrelations are less susceptible to perturbations. In order to account for the\nstochastic individual goals, we exploit a conditional variational autoencoder\nto realize multi-goal estimation and diverse future prediction. Specifically,\nwe perform variational inference via the latent distribution, which is\nconditioned on the correlation between input states and associated target\ngoals. Sampling from the latent distribution enables the framework to reliably\ncapture the stochastic behavior in test data. We integrate multi-goal\nestimation and region-based relation learning to model the two stimuli, social\ninteractions, and stochastic goals, in a prediction framework. We evaluate our\nframework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that\nthe diverse prediction better fits the ground truth when incorporating the\nrelation module. Our framework outperforms the state-of-the-art models on SDD\nby $27.61\\%$/$18.20\\%$ of ADE/FDE metrics.\n", "link": "http://arxiv.org/abs/2404.06971v1", "date": "2024-04-10", "relevancy": 2.1261, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5979}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5209}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5156}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TrajPRed%3A%20Trajectory%20Prediction%20with%20Region-based%20Relation%20Learning&body=Title%3A%20TrajPRed%3A%20Trajectory%20Prediction%20with%20Region-based%20Relation%20Learning%0AAuthor%3A%20Chen%20Zhou%20and%20Ghassan%20AlRegib%20and%20Armin%20Parchami%20and%20Kunjan%20Singh%0AAbstract%3A%20%20%20Forecasting%20human%20trajectories%20in%20traffic%20scenes%20is%20critical%20for%20safety%0Awithin%20mixed%20or%20fully%20autonomous%20systems.%20Human%20future%20trajectories%20are%20driven%0Aby%20two%20major%20stimuli%2C%20social%20interactions%2C%20and%20stochastic%20goals.%20Thus%2C%20reliable%0Aforecasting%20needs%20to%20capture%20these%20two%20stimuli.%20Edge-based%20relation%20modeling%0Arepresents%20social%20interactions%20using%20pairwise%20correlations%20from%20precise%0Aindividual%20states.%20Nevertheless%2C%20edge-based%20relations%20can%20be%20vulnerable%20under%0Aperturbations.%20To%20alleviate%20these%20issues%2C%20we%20propose%20a%20region-based%20relation%0Alearning%20paradigm%20that%20models%20social%20interactions%20via%20region-wise%20dynamics%20of%0Ajoint%20states%2C%20i.e.%2C%20the%20changes%20in%20the%20density%20of%20crowds.%20In%20particular%2C%0Aregion-wise%20agent%20joint%20information%20is%20encoded%20within%20convolutional%20feature%0Agrids.%20Social%20relations%20are%20modeled%20by%20relating%20the%20temporal%20changes%20of%20local%0Ajoint%20information%20from%20a%20global%20perspective.%20We%20show%20that%20region-based%0Arelations%20are%20less%20susceptible%20to%20perturbations.%20In%20order%20to%20account%20for%20the%0Astochastic%20individual%20goals%2C%20we%20exploit%20a%20conditional%20variational%20autoencoder%0Ato%20realize%20multi-goal%20estimation%20and%20diverse%20future%20prediction.%20Specifically%2C%0Awe%20perform%20variational%20inference%20via%20the%20latent%20distribution%2C%20which%20is%0Aconditioned%20on%20the%20correlation%20between%20input%20states%20and%20associated%20target%0Agoals.%20Sampling%20from%20the%20latent%20distribution%20enables%20the%20framework%20to%20reliably%0Acapture%20the%20stochastic%20behavior%20in%20test%20data.%20We%20integrate%20multi-goal%0Aestimation%20and%20region-based%20relation%20learning%20to%20model%20the%20two%20stimuli%2C%20social%0Ainteractions%2C%20and%20stochastic%20goals%2C%20in%20a%20prediction%20framework.%20We%20evaluate%20our%0Aframework%20on%20the%20ETH-UCY%20dataset%20and%20Stanford%20Drone%20Dataset%20%28SDD%29.%20We%20show%20that%0Athe%20diverse%20prediction%20better%20fits%20the%20ground%20truth%20when%20incorporating%20the%0Arelation%20module.%20Our%20framework%20outperforms%20the%20state-of-the-art%20models%20on%20SDD%0Aby%20%2427.61%5C%25%24/%2418.20%5C%25%24%20of%20ADE/FDE%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06971v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrajPRed%3A%20Trajectory%20Prediction%20with%20Region-based%20Relation%20Learning&entry.906535625=Chen%20Zhou%20and%20Ghassan%20AlRegib%20and%20Armin%20Parchami%20and%20Kunjan%20Singh&entry.1292438233=%20%20Forecasting%20human%20trajectories%20in%20traffic%20scenes%20is%20critical%20for%20safety%0Awithin%20mixed%20or%20fully%20autonomous%20systems.%20Human%20future%20trajectories%20are%20driven%0Aby%20two%20major%20stimuli%2C%20social%20interactions%2C%20and%20stochastic%20goals.%20Thus%2C%20reliable%0Aforecasting%20needs%20to%20capture%20these%20two%20stimuli.%20Edge-based%20relation%20modeling%0Arepresents%20social%20interactions%20using%20pairwise%20correlations%20from%20precise%0Aindividual%20states.%20Nevertheless%2C%20edge-based%20relations%20can%20be%20vulnerable%20under%0Aperturbations.%20To%20alleviate%20these%20issues%2C%20we%20propose%20a%20region-based%20relation%0Alearning%20paradigm%20that%20models%20social%20interactions%20via%20region-wise%20dynamics%20of%0Ajoint%20states%2C%20i.e.%2C%20the%20changes%20in%20the%20density%20of%20crowds.%20In%20particular%2C%0Aregion-wise%20agent%20joint%20information%20is%20encoded%20within%20convolutional%20feature%0Agrids.%20Social%20relations%20are%20modeled%20by%20relating%20the%20temporal%20changes%20of%20local%0Ajoint%20information%20from%20a%20global%20perspective.%20We%20show%20that%20region-based%0Arelations%20are%20less%20susceptible%20to%20perturbations.%20In%20order%20to%20account%20for%20the%0Astochastic%20individual%20goals%2C%20we%20exploit%20a%20conditional%20variational%20autoencoder%0Ato%20realize%20multi-goal%20estimation%20and%20diverse%20future%20prediction.%20Specifically%2C%0Awe%20perform%20variational%20inference%20via%20the%20latent%20distribution%2C%20which%20is%0Aconditioned%20on%20the%20correlation%20between%20input%20states%20and%20associated%20target%0Agoals.%20Sampling%20from%20the%20latent%20distribution%20enables%20the%20framework%20to%20reliably%0Acapture%20the%20stochastic%20behavior%20in%20test%20data.%20We%20integrate%20multi-goal%0Aestimation%20and%20region-based%20relation%20learning%20to%20model%20the%20two%20stimuli%2C%20social%0Ainteractions%2C%20and%20stochastic%20goals%2C%20in%20a%20prediction%20framework.%20We%20evaluate%20our%0Aframework%20on%20the%20ETH-UCY%20dataset%20and%20Stanford%20Drone%20Dataset%20%28SDD%29.%20We%20show%20that%0Athe%20diverse%20prediction%20better%20fits%20the%20ground%20truth%20when%20incorporating%20the%0Arelation%20module.%20Our%20framework%20outperforms%20the%20state-of-the-art%20models%20on%20SDD%0Aby%20%2427.61%5C%25%24/%2418.20%5C%25%24%20of%20ADE/FDE%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06971v1&entry.124074799=Read"},
{"title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update", "author": "Zhi Gao and Yuntao Du and Xintong Zhang and Xiaojian Ma and Wenjuan Han and Song-Chun Zhu and Qing Li", "abstract": "  Utilizing large language models (LLMs) to compose off-the-shelf visual tools\nrepresents a promising avenue of research for developing robust visual\nassistants capable of addressing diverse visual tasks. However, these methods\noften overlook the potential for continual learning, typically by freezing the\nutilized tools, thus limiting their adaptation to environments requiring new\nknowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual\nAssistant, which operates within a framework encompassing inference,\nreflection, and learning phases. During the inference phase, LLMs generate\nprograms and execute corresponding tools to complete assigned tasks. In the\nreflection phase, a multimodal global-local reflection scheme analyzes human\nfeedback to determine which tools require updating. Lastly, the learning phase\nemploys three flexible approaches to automatically gather training data and\nintroduces a novel prompt tuning scheme to update the tools, allowing CLOVA to\nefficiently acquire new knowledge. Experimental findings demonstrate that CLOVA\nsurpasses existing tool-usage methods by 5% in visual question answering and\nmultiple-image reasoning, by 10% in knowledge tagging, and by 20% in image\nediting. These results underscore the significance of the continual learning\ncapability in general visual assistants.\n", "link": "http://arxiv.org/abs/2312.10908v3", "date": "2024-04-10", "relevancy": 2.1257, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5578}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5181}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5104}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLOVA%3A%20A%20Closed-Loop%20Visual%20Assistant%20with%20Tool%20Usage%20and%20Update&body=Title%3A%20CLOVA%3A%20A%20Closed-Loop%20Visual%20Assistant%20with%20Tool%20Usage%20and%20Update%0AAuthor%3A%20Zhi%20Gao%20and%20Yuntao%20Du%20and%20Xintong%20Zhang%20and%20Xiaojian%20Ma%20and%20Wenjuan%20Han%20and%20Song-Chun%20Zhu%20and%20Qing%20Li%0AAbstract%3A%20%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20to%20compose%20off-the-shelf%20visual%20tools%0Arepresents%20a%20promising%20avenue%20of%20research%20for%20developing%20robust%20visual%0Aassistants%20capable%20of%20addressing%20diverse%20visual%20tasks.%20However%2C%20these%20methods%0Aoften%20overlook%20the%20potential%20for%20continual%20learning%2C%20typically%20by%20freezing%20the%0Autilized%20tools%2C%20thus%20limiting%20their%20adaptation%20to%20environments%20requiring%20new%0Aknowledge.%20To%20tackle%20this%20challenge%2C%20we%20propose%20CLOVA%2C%20a%20Closed-Loop%20Visual%0AAssistant%2C%20which%20operates%20within%20a%20framework%20encompassing%20inference%2C%0Areflection%2C%20and%20learning%20phases.%20During%20the%20inference%20phase%2C%20LLMs%20generate%0Aprograms%20and%20execute%20corresponding%20tools%20to%20complete%20assigned%20tasks.%20In%20the%0Areflection%20phase%2C%20a%20multimodal%20global-local%20reflection%20scheme%20analyzes%20human%0Afeedback%20to%20determine%20which%20tools%20require%20updating.%20Lastly%2C%20the%20learning%20phase%0Aemploys%20three%20flexible%20approaches%20to%20automatically%20gather%20training%20data%20and%0Aintroduces%20a%20novel%20prompt%20tuning%20scheme%20to%20update%20the%20tools%2C%20allowing%20CLOVA%20to%0Aefficiently%20acquire%20new%20knowledge.%20Experimental%20findings%20demonstrate%20that%20CLOVA%0Asurpasses%20existing%20tool-usage%20methods%20by%205%25%20in%20visual%20question%20answering%20and%0Amultiple-image%20reasoning%2C%20by%2010%25%20in%20knowledge%20tagging%2C%20and%20by%2020%25%20in%20image%0Aediting.%20These%20results%20underscore%20the%20significance%20of%20the%20continual%20learning%0Acapability%20in%20general%20visual%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10908v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOVA%3A%20A%20Closed-Loop%20Visual%20Assistant%20with%20Tool%20Usage%20and%20Update&entry.906535625=Zhi%20Gao%20and%20Yuntao%20Du%20and%20Xintong%20Zhang%20and%20Xiaojian%20Ma%20and%20Wenjuan%20Han%20and%20Song-Chun%20Zhu%20and%20Qing%20Li&entry.1292438233=%20%20Utilizing%20large%20language%20models%20%28LLMs%29%20to%20compose%20off-the-shelf%20visual%20tools%0Arepresents%20a%20promising%20avenue%20of%20research%20for%20developing%20robust%20visual%0Aassistants%20capable%20of%20addressing%20diverse%20visual%20tasks.%20However%2C%20these%20methods%0Aoften%20overlook%20the%20potential%20for%20continual%20learning%2C%20typically%20by%20freezing%20the%0Autilized%20tools%2C%20thus%20limiting%20their%20adaptation%20to%20environments%20requiring%20new%0Aknowledge.%20To%20tackle%20this%20challenge%2C%20we%20propose%20CLOVA%2C%20a%20Closed-Loop%20Visual%0AAssistant%2C%20which%20operates%20within%20a%20framework%20encompassing%20inference%2C%0Areflection%2C%20and%20learning%20phases.%20During%20the%20inference%20phase%2C%20LLMs%20generate%0Aprograms%20and%20execute%20corresponding%20tools%20to%20complete%20assigned%20tasks.%20In%20the%0Areflection%20phase%2C%20a%20multimodal%20global-local%20reflection%20scheme%20analyzes%20human%0Afeedback%20to%20determine%20which%20tools%20require%20updating.%20Lastly%2C%20the%20learning%20phase%0Aemploys%20three%20flexible%20approaches%20to%20automatically%20gather%20training%20data%20and%0Aintroduces%20a%20novel%20prompt%20tuning%20scheme%20to%20update%20the%20tools%2C%20allowing%20CLOVA%20to%0Aefficiently%20acquire%20new%20knowledge.%20Experimental%20findings%20demonstrate%20that%20CLOVA%0Asurpasses%20existing%20tool-usage%20methods%20by%205%25%20in%20visual%20question%20answering%20and%0Amultiple-image%20reasoning%2C%20by%2010%25%20in%20knowledge%20tagging%2C%20and%20by%2020%25%20in%20image%0Aediting.%20These%20results%20underscore%20the%20significance%20of%20the%20continual%20learning%0Acapability%20in%20general%20visual%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10908v3&entry.124074799=Read"},
{"title": "A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain\n  Text Classification", "author": "Yunlong Feng and Bohan Li and Libo Qin and Xiao Xu and Wanxiang Che", "abstract": "  Cross-domain text classification aims to adapt models to a target domain that\nlacks labeled data. It leverages or reuses rich labeled data from the different\nbut related source domain(s) and unlabeled data from the target domain. To this\nend, previous work focuses on either extracting domain-invariant features or\ntask-agnostic features, ignoring domain-aware features that may be present in\nthe target domain and could be useful for the downstream task. In this paper,\nwe propose a two-stage framework for cross-domain text classification. In the\nfirst stage, we finetune the model with mask language modeling (MLM) and\nlabeled data from the source domain. In the second stage, we further fine-tune\nthe model with self-supervised distillation (SSD) and unlabeled data from the\ntarget domain. We evaluate its performance on a public cross-domain text\nclassification benchmark and the experiment results show that our method\nachieves new state-of-the-art results for both single-source domain adaptations\n(94.17% $\\uparrow$1.03%) and multi-source domain adaptations (95.09%\n$\\uparrow$1.34%).\n", "link": "http://arxiv.org/abs/2304.09820v2", "date": "2024-04-10", "relevancy": 2.1144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5241}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5189}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Two-Stage%20Framework%20with%20Self-Supervised%20Distillation%20For%20Cross-Domain%0A%20%20Text%20Classification&body=Title%3A%20A%20Two-Stage%20Framework%20with%20Self-Supervised%20Distillation%20For%20Cross-Domain%0A%20%20Text%20Classification%0AAuthor%3A%20Yunlong%20Feng%20and%20Bohan%20Li%20and%20Libo%20Qin%20and%20Xiao%20Xu%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Cross-domain%20text%20classification%20aims%20to%20adapt%20models%20to%20a%20target%20domain%20that%0Alacks%20labeled%20data.%20It%20leverages%20or%20reuses%20rich%20labeled%20data%20from%20the%20different%0Abut%20related%20source%20domain%28s%29%20and%20unlabeled%20data%20from%20the%20target%20domain.%20To%20this%0Aend%2C%20previous%20work%20focuses%20on%20either%20extracting%20domain-invariant%20features%20or%0Atask-agnostic%20features%2C%20ignoring%20domain-aware%20features%20that%20may%20be%20present%20in%0Athe%20target%20domain%20and%20could%20be%20useful%20for%20the%20downstream%20task.%20In%20this%20paper%2C%0Awe%20propose%20a%20two-stage%20framework%20for%20cross-domain%20text%20classification.%20In%20the%0Afirst%20stage%2C%20we%20finetune%20the%20model%20with%20mask%20language%20modeling%20%28MLM%29%20and%0Alabeled%20data%20from%20the%20source%20domain.%20In%20the%20second%20stage%2C%20we%20further%20fine-tune%0Athe%20model%20with%20self-supervised%20distillation%20%28SSD%29%20and%20unlabeled%20data%20from%20the%0Atarget%20domain.%20We%20evaluate%20its%20performance%20on%20a%20public%20cross-domain%20text%0Aclassification%20benchmark%20and%20the%20experiment%20results%20show%20that%20our%20method%0Aachieves%20new%20state-of-the-art%20results%20for%20both%20single-source%20domain%20adaptations%0A%2894.17%25%20%24%5Cuparrow%241.03%25%29%20and%20multi-source%20domain%20adaptations%20%2895.09%25%0A%24%5Cuparrow%241.34%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09820v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Two-Stage%20Framework%20with%20Self-Supervised%20Distillation%20For%20Cross-Domain%0A%20%20Text%20Classification&entry.906535625=Yunlong%20Feng%20and%20Bohan%20Li%20and%20Libo%20Qin%20and%20Xiao%20Xu%20and%20Wanxiang%20Che&entry.1292438233=%20%20Cross-domain%20text%20classification%20aims%20to%20adapt%20models%20to%20a%20target%20domain%20that%0Alacks%20labeled%20data.%20It%20leverages%20or%20reuses%20rich%20labeled%20data%20from%20the%20different%0Abut%20related%20source%20domain%28s%29%20and%20unlabeled%20data%20from%20the%20target%20domain.%20To%20this%0Aend%2C%20previous%20work%20focuses%20on%20either%20extracting%20domain-invariant%20features%20or%0Atask-agnostic%20features%2C%20ignoring%20domain-aware%20features%20that%20may%20be%20present%20in%0Athe%20target%20domain%20and%20could%20be%20useful%20for%20the%20downstream%20task.%20In%20this%20paper%2C%0Awe%20propose%20a%20two-stage%20framework%20for%20cross-domain%20text%20classification.%20In%20the%0Afirst%20stage%2C%20we%20finetune%20the%20model%20with%20mask%20language%20modeling%20%28MLM%29%20and%0Alabeled%20data%20from%20the%20source%20domain.%20In%20the%20second%20stage%2C%20we%20further%20fine-tune%0Athe%20model%20with%20self-supervised%20distillation%20%28SSD%29%20and%20unlabeled%20data%20from%20the%0Atarget%20domain.%20We%20evaluate%20its%20performance%20on%20a%20public%20cross-domain%20text%0Aclassification%20benchmark%20and%20the%20experiment%20results%20show%20that%20our%20method%0Aachieves%20new%20state-of-the-art%20results%20for%20both%20single-source%20domain%20adaptations%0A%2894.17%25%20%24%5Cuparrow%241.03%25%29%20and%20multi-source%20domain%20adaptations%20%2895.09%25%0A%24%5Cuparrow%241.34%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09820v2&entry.124074799=Read"},
{"title": "Sparse Global Matching for Video Frame Interpolation with Large Motion", "author": "Chunxu Liu and Guozhen Zhang and Rui Zhao and Limin Wang", "abstract": "  Large motion poses a critical challenge in Video Frame Interpolation (VFI)\ntask. Existing methods are often constrained by limited receptive fields,\nresulting in sub-optimal performance when handling scenarios with large motion.\nIn this paper, we introduce a new pipeline for VFI, which can effectively\nintegrate global-level information to alleviate issues associated with large\nmotion. Specifically, we first estimate a pair of initial intermediate flows\nusing a high-resolution feature map for extracting local details. Then, we\nincorporate a sparse global matching branch to compensate for flow estimation,\nwhich consists of identifying flaws in initial flows and generating sparse flow\ncompensation with a global receptive field. Finally, we adaptively merge the\ninitial flow estimation with global flow compensation, yielding a more accurate\nintermediate flow. To evaluate the effectiveness of our method in handling\nlarge motion, we carefully curate a more challenging subset from commonly used\nbenchmarks. Our method demonstrates the state-of-the-art performance on these\nVFI subsets with large motion.\n", "link": "http://arxiv.org/abs/2404.06913v1", "date": "2024-04-10", "relevancy": 2.108, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5455}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&body=Title%3A%20Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion%0AAuthor%3A%20Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06913v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Global%20Matching%20for%20Video%20Frame%20Interpolation%20with%20Large%20Motion&entry.906535625=Chunxu%20Liu%20and%20Guozhen%20Zhang%20and%20Rui%20Zhao%20and%20Limin%20Wang&entry.1292438233=%20%20Large%20motion%20poses%20a%20critical%20challenge%20in%20Video%20Frame%20Interpolation%20%28VFI%29%0Atask.%20Existing%20methods%20are%20often%20constrained%20by%20limited%20receptive%20fields%2C%0Aresulting%20in%20sub-optimal%20performance%20when%20handling%20scenarios%20with%20large%20motion.%0AIn%20this%20paper%2C%20we%20introduce%20a%20new%20pipeline%20for%20VFI%2C%20which%20can%20effectively%0Aintegrate%20global-level%20information%20to%20alleviate%20issues%20associated%20with%20large%0Amotion.%20Specifically%2C%20we%20first%20estimate%20a%20pair%20of%20initial%20intermediate%20flows%0Ausing%20a%20high-resolution%20feature%20map%20for%20extracting%20local%20details.%20Then%2C%20we%0Aincorporate%20a%20sparse%20global%20matching%20branch%20to%20compensate%20for%20flow%20estimation%2C%0Awhich%20consists%20of%20identifying%20flaws%20in%20initial%20flows%20and%20generating%20sparse%20flow%0Acompensation%20with%20a%20global%20receptive%20field.%20Finally%2C%20we%20adaptively%20merge%20the%0Ainitial%20flow%20estimation%20with%20global%20flow%20compensation%2C%20yielding%20a%20more%20accurate%0Aintermediate%20flow.%20To%20evaluate%20the%20effectiveness%20of%20our%20method%20in%20handling%0Alarge%20motion%2C%20we%20carefully%20curate%20a%20more%20challenging%20subset%20from%20commonly%20used%0Abenchmarks.%20Our%20method%20demonstrates%20the%20state-of-the-art%20performance%20on%20these%0AVFI%20subsets%20with%20large%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06913v1&entry.124074799=Read"},
{"title": "HRVDA: High-Resolution Visual Document Assistant", "author": "Chaohu Liu and Kun Yin and Haoyu Cao and Xinghua Jiang and Xin Li and Yinsong Liu and Deqiang Jiang and Xing Sun and Linli Xu", "abstract": "  Leveraging vast training data, multimodal large language models (MLLMs) have\ndemonstrated formidable general visual comprehension capabilities and achieved\nremarkable performance across various tasks. However, their performance in\nvisual document understanding still leaves much room for improvement. This\ndiscrepancy is primarily attributed to the fact that visual document\nunderstanding is a fine-grained prediction task. In natural scenes, MLLMs\ntypically use low-resolution images, leading to a substantial loss of visual\ninformation. Furthermore, general-purpose MLLMs do not excel in handling\ndocument-oriented instructions. In this paper, we propose a High-Resolution\nVisual Document Assistant (HRVDA), which bridges the gap between MLLMs and\nvisual document understanding. This model employs a content filtering mechanism\nand an instruction filtering module to separately filter out the\ncontent-agnostic visual tokens and instruction-agnostic visual tokens, thereby\nachieving efficient model training and inference for high-resolution images. In\naddition, we construct a document-oriented visual instruction tuning dataset\nand apply a multi-stage training strategy to enhance the model's document\nmodeling capabilities. Extensive experiments demonstrate that our model\nachieves state-of-the-art performance across multiple document understanding\ndatasets, while maintaining training efficiency and inference speed comparable\nto low-resolution models.\n", "link": "http://arxiv.org/abs/2404.06918v1", "date": "2024-04-10", "relevancy": 2.0926, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HRVDA%3A%20High-Resolution%20Visual%20Document%20Assistant&body=Title%3A%20HRVDA%3A%20High-Resolution%20Visual%20Document%20Assistant%0AAuthor%3A%20Chaohu%20Liu%20and%20Kun%20Yin%20and%20Haoyu%20Cao%20and%20Xinghua%20Jiang%20and%20Xin%20Li%20and%20Yinsong%20Liu%20and%20Deqiang%20Jiang%20and%20Xing%20Sun%20and%20Linli%20Xu%0AAbstract%3A%20%20%20Leveraging%20vast%20training%20data%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20formidable%20general%20visual%20comprehension%20capabilities%20and%20achieved%0Aremarkable%20performance%20across%20various%20tasks.%20However%2C%20their%20performance%20in%0Avisual%20document%20understanding%20still%20leaves%20much%20room%20for%20improvement.%20This%0Adiscrepancy%20is%20primarily%20attributed%20to%20the%20fact%20that%20visual%20document%0Aunderstanding%20is%20a%20fine-grained%20prediction%20task.%20In%20natural%20scenes%2C%20MLLMs%0Atypically%20use%20low-resolution%20images%2C%20leading%20to%20a%20substantial%20loss%20of%20visual%0Ainformation.%20Furthermore%2C%20general-purpose%20MLLMs%20do%20not%20excel%20in%20handling%0Adocument-oriented%20instructions.%20In%20this%20paper%2C%20we%20propose%20a%20High-Resolution%0AVisual%20Document%20Assistant%20%28HRVDA%29%2C%20which%20bridges%20the%20gap%20between%20MLLMs%20and%0Avisual%20document%20understanding.%20This%20model%20employs%20a%20content%20filtering%20mechanism%0Aand%20an%20instruction%20filtering%20module%20to%20separately%20filter%20out%20the%0Acontent-agnostic%20visual%20tokens%20and%20instruction-agnostic%20visual%20tokens%2C%20thereby%0Aachieving%20efficient%20model%20training%20and%20inference%20for%20high-resolution%20images.%20In%0Aaddition%2C%20we%20construct%20a%20document-oriented%20visual%20instruction%20tuning%20dataset%0Aand%20apply%20a%20multi-stage%20training%20strategy%20to%20enhance%20the%20model%27s%20document%0Amodeling%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20across%20multiple%20document%20understanding%0Adatasets%2C%20while%20maintaining%20training%20efficiency%20and%20inference%20speed%20comparable%0Ato%20low-resolution%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06918v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HRVDA%3A%20High-Resolution%20Visual%20Document%20Assistant&entry.906535625=Chaohu%20Liu%20and%20Kun%20Yin%20and%20Haoyu%20Cao%20and%20Xinghua%20Jiang%20and%20Xin%20Li%20and%20Yinsong%20Liu%20and%20Deqiang%20Jiang%20and%20Xing%20Sun%20and%20Linli%20Xu&entry.1292438233=%20%20Leveraging%20vast%20training%20data%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%0Ademonstrated%20formidable%20general%20visual%20comprehension%20capabilities%20and%20achieved%0Aremarkable%20performance%20across%20various%20tasks.%20However%2C%20their%20performance%20in%0Avisual%20document%20understanding%20still%20leaves%20much%20room%20for%20improvement.%20This%0Adiscrepancy%20is%20primarily%20attributed%20to%20the%20fact%20that%20visual%20document%0Aunderstanding%20is%20a%20fine-grained%20prediction%20task.%20In%20natural%20scenes%2C%20MLLMs%0Atypically%20use%20low-resolution%20images%2C%20leading%20to%20a%20substantial%20loss%20of%20visual%0Ainformation.%20Furthermore%2C%20general-purpose%20MLLMs%20do%20not%20excel%20in%20handling%0Adocument-oriented%20instructions.%20In%20this%20paper%2C%20we%20propose%20a%20High-Resolution%0AVisual%20Document%20Assistant%20%28HRVDA%29%2C%20which%20bridges%20the%20gap%20between%20MLLMs%20and%0Avisual%20document%20understanding.%20This%20model%20employs%20a%20content%20filtering%20mechanism%0Aand%20an%20instruction%20filtering%20module%20to%20separately%20filter%20out%20the%0Acontent-agnostic%20visual%20tokens%20and%20instruction-agnostic%20visual%20tokens%2C%20thereby%0Aachieving%20efficient%20model%20training%20and%20inference%20for%20high-resolution%20images.%20In%0Aaddition%2C%20we%20construct%20a%20document-oriented%20visual%20instruction%20tuning%20dataset%0Aand%20apply%20a%20multi-stage%20training%20strategy%20to%20enhance%20the%20model%27s%20document%0Amodeling%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Aachieves%20state-of-the-art%20performance%20across%20multiple%20document%20understanding%0Adatasets%2C%20while%20maintaining%20training%20efficiency%20and%20inference%20speed%20comparable%0Ato%20low-resolution%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06918v1&entry.124074799=Read"},
{"title": "Dual Prompt Tuning for Domain-Aware Federated Learning", "author": "Guoyizhe Wei and Feng Wang and Anshul Shah and Rama Chellappa", "abstract": "  Federated learning is a distributed machine learning paradigm that allows\nmultiple clients to collaboratively train a shared model with their local data.\nNonetheless, conventional federated learning algorithms often struggle to\ngeneralize well due to the ubiquitous domain shift across clients. In this\nwork, we consider a challenging yet realistic federated learning scenario where\nthe training data of each client originates from different domains. We address\nthe challenges of domain shift by leveraging the technique of prompt learning,\nand propose a novel method called Federated Dual Prompt Tuning (Fed-DPT).\nSpecifically, Fed-DPT employs a pre-trained vision-language model and then\napplies both visual and textual prompt tuning to facilitate domain adaptation\nover decentralized data. Extensive experiments of Fed-DPT demonstrate its\nsignificant effectiveness in domain-aware federated learning. With a\npre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT\nattains 68.4% average accuracy over six domains in the DomainNet dataset, which\nimproves the original CLIP by a large margin of 14.8%.\n", "link": "http://arxiv.org/abs/2310.03103v4", "date": "2024-04-10", "relevancy": 2.0744, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.501}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Prompt%20Tuning%20for%20Domain-Aware%20Federated%20Learning&body=Title%3A%20Dual%20Prompt%20Tuning%20for%20Domain-Aware%20Federated%20Learning%0AAuthor%3A%20Guoyizhe%20Wei%20and%20Feng%20Wang%20and%20Anshul%20Shah%20and%20Rama%20Chellappa%0AAbstract%3A%20%20%20Federated%20learning%20is%20a%20distributed%20machine%20learning%20paradigm%20that%20allows%0Amultiple%20clients%20to%20collaboratively%20train%20a%20shared%20model%20with%20their%20local%20data.%0ANonetheless%2C%20conventional%20federated%20learning%20algorithms%20often%20struggle%20to%0Ageneralize%20well%20due%20to%20the%20ubiquitous%20domain%20shift%20across%20clients.%20In%20this%0Awork%2C%20we%20consider%20a%20challenging%20yet%20realistic%20federated%20learning%20scenario%20where%0Athe%20training%20data%20of%20each%20client%20originates%20from%20different%20domains.%20We%20address%0Athe%20challenges%20of%20domain%20shift%20by%20leveraging%20the%20technique%20of%20prompt%20learning%2C%0Aand%20propose%20a%20novel%20method%20called%20Federated%20Dual%20Prompt%20Tuning%20%28Fed-DPT%29.%0ASpecifically%2C%20Fed-DPT%20employs%20a%20pre-trained%20vision-language%20model%20and%20then%0Aapplies%20both%20visual%20and%20textual%20prompt%20tuning%20to%20facilitate%20domain%20adaptation%0Aover%20decentralized%20data.%20Extensive%20experiments%20of%20Fed-DPT%20demonstrate%20its%0Asignificant%20effectiveness%20in%20domain-aware%20federated%20learning.%20With%20a%0Apre-trained%20CLIP%20model%20%28ViT-Base%20as%20image%20encoder%29%2C%20the%20proposed%20Fed-DPT%0Aattains%2068.4%25%20average%20accuracy%20over%20six%20domains%20in%20the%20DomainNet%20dataset%2C%20which%0Aimproves%20the%20original%20CLIP%20by%20a%20large%20margin%20of%2014.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03103v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Prompt%20Tuning%20for%20Domain-Aware%20Federated%20Learning&entry.906535625=Guoyizhe%20Wei%20and%20Feng%20Wang%20and%20Anshul%20Shah%20and%20Rama%20Chellappa&entry.1292438233=%20%20Federated%20learning%20is%20a%20distributed%20machine%20learning%20paradigm%20that%20allows%0Amultiple%20clients%20to%20collaboratively%20train%20a%20shared%20model%20with%20their%20local%20data.%0ANonetheless%2C%20conventional%20federated%20learning%20algorithms%20often%20struggle%20to%0Ageneralize%20well%20due%20to%20the%20ubiquitous%20domain%20shift%20across%20clients.%20In%20this%0Awork%2C%20we%20consider%20a%20challenging%20yet%20realistic%20federated%20learning%20scenario%20where%0Athe%20training%20data%20of%20each%20client%20originates%20from%20different%20domains.%20We%20address%0Athe%20challenges%20of%20domain%20shift%20by%20leveraging%20the%20technique%20of%20prompt%20learning%2C%0Aand%20propose%20a%20novel%20method%20called%20Federated%20Dual%20Prompt%20Tuning%20%28Fed-DPT%29.%0ASpecifically%2C%20Fed-DPT%20employs%20a%20pre-trained%20vision-language%20model%20and%20then%0Aapplies%20both%20visual%20and%20textual%20prompt%20tuning%20to%20facilitate%20domain%20adaptation%0Aover%20decentralized%20data.%20Extensive%20experiments%20of%20Fed-DPT%20demonstrate%20its%0Asignificant%20effectiveness%20in%20domain-aware%20federated%20learning.%20With%20a%0Apre-trained%20CLIP%20model%20%28ViT-Base%20as%20image%20encoder%29%2C%20the%20proposed%20Fed-DPT%0Aattains%2068.4%25%20average%20accuracy%20over%20six%20domains%20in%20the%20DomainNet%20dataset%2C%20which%0Aimproves%20the%20original%20CLIP%20by%20a%20large%20margin%20of%2014.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03103v4&entry.124074799=Read"},
{"title": "VMamba: Visual State Space Model", "author": "Yue Liu and Yunjie Tian and Yuzhong Zhao and Hongtian Yu and Lingxi Xie and Yaowei Wang and Qixiang Ye and Yunfan Liu", "abstract": "  Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have long\nbeen the predominant backbone networks for visual representation learning.\nWhile ViTs have recently gained prominence over CNNs due to their superior\nfitting capabilities, their scalability is largely constrained by the quadratic\ncomplexity of attention computation. Inspired by the capability of Mamba in\nefficiently modeling long sequences, we propose VMamba, a generic vision\nbackbone model aiming to reduce the computational complexity to linear while\nretaining ViTs' advantageous features. To enhance VMamba's adaptability in\nprocessing vision data, we introduce the Cross-Scan Module (CSM) to enable 1D\nselective scanning in 2D image space with global receptive fields.\nAdditionally, we make further improvements in implementation details and\narchitectural designs to enhance VMamba's performance and boost its inference\nspeed. Extensive experimental results demonstrate VMamba's promising\nperformance across various visual perception tasks, highlighting its pronounced\nadvantages in input scaling efficiency compared to existing benchmark models.\nSource code is available at https://github.com/MzeroMiko/VMamba.\n", "link": "http://arxiv.org/abs/2401.10166v2", "date": "2024-04-10", "relevancy": 2.0696, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.534}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5029}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VMamba%3A%20Visual%20State%20Space%20Model&body=Title%3A%20VMamba%3A%20Visual%20State%20Space%20Model%0AAuthor%3A%20Yue%20Liu%20and%20Yunjie%20Tian%20and%20Yuzhong%20Zhao%20and%20Hongtian%20Yu%20and%20Lingxi%20Xie%20and%20Yaowei%20Wang%20and%20Qixiang%20Ye%20and%20Yunfan%20Liu%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20have%20long%0Abeen%20the%20predominant%20backbone%20networks%20for%20visual%20representation%20learning.%0AWhile%20ViTs%20have%20recently%20gained%20prominence%20over%20CNNs%20due%20to%20their%20superior%0Afitting%20capabilities%2C%20their%20scalability%20is%20largely%20constrained%20by%20the%20quadratic%0Acomplexity%20of%20attention%20computation.%20Inspired%20by%20the%20capability%20of%20Mamba%20in%0Aefficiently%20modeling%20long%20sequences%2C%20we%20propose%20VMamba%2C%20a%20generic%20vision%0Abackbone%20model%20aiming%20to%20reduce%20the%20computational%20complexity%20to%20linear%20while%0Aretaining%20ViTs%27%20advantageous%20features.%20To%20enhance%20VMamba%27s%20adaptability%20in%0Aprocessing%20vision%20data%2C%20we%20introduce%20the%20Cross-Scan%20Module%20%28CSM%29%20to%20enable%201D%0Aselective%20scanning%20in%202D%20image%20space%20with%20global%20receptive%20fields.%0AAdditionally%2C%20we%20make%20further%20improvements%20in%20implementation%20details%20and%0Aarchitectural%20designs%20to%20enhance%20VMamba%27s%20performance%20and%20boost%20its%20inference%0Aspeed.%20Extensive%20experimental%20results%20demonstrate%20VMamba%27s%20promising%0Aperformance%20across%20various%20visual%20perception%20tasks%2C%20highlighting%20its%20pronounced%0Aadvantages%20in%20input%20scaling%20efficiency%20compared%20to%20existing%20benchmark%20models.%0ASource%20code%20is%20available%20at%20https%3A//github.com/MzeroMiko/VMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10166v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VMamba%3A%20Visual%20State%20Space%20Model&entry.906535625=Yue%20Liu%20and%20Yunjie%20Tian%20and%20Yuzhong%20Zhao%20and%20Hongtian%20Yu%20and%20Lingxi%20Xie%20and%20Yaowei%20Wang%20and%20Qixiang%20Ye%20and%20Yunfan%20Liu&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20have%20long%0Abeen%20the%20predominant%20backbone%20networks%20for%20visual%20representation%20learning.%0AWhile%20ViTs%20have%20recently%20gained%20prominence%20over%20CNNs%20due%20to%20their%20superior%0Afitting%20capabilities%2C%20their%20scalability%20is%20largely%20constrained%20by%20the%20quadratic%0Acomplexity%20of%20attention%20computation.%20Inspired%20by%20the%20capability%20of%20Mamba%20in%0Aefficiently%20modeling%20long%20sequences%2C%20we%20propose%20VMamba%2C%20a%20generic%20vision%0Abackbone%20model%20aiming%20to%20reduce%20the%20computational%20complexity%20to%20linear%20while%0Aretaining%20ViTs%27%20advantageous%20features.%20To%20enhance%20VMamba%27s%20adaptability%20in%0Aprocessing%20vision%20data%2C%20we%20introduce%20the%20Cross-Scan%20Module%20%28CSM%29%20to%20enable%201D%0Aselective%20scanning%20in%202D%20image%20space%20with%20global%20receptive%20fields.%0AAdditionally%2C%20we%20make%20further%20improvements%20in%20implementation%20details%20and%0Aarchitectural%20designs%20to%20enhance%20VMamba%27s%20performance%20and%20boost%20its%20inference%0Aspeed.%20Extensive%20experimental%20results%20demonstrate%20VMamba%27s%20promising%0Aperformance%20across%20various%20visual%20perception%20tasks%2C%20highlighting%20its%20pronounced%0Aadvantages%20in%20input%20scaling%20efficiency%20compared%20to%20existing%20benchmark%20models.%0ASource%20code%20is%20available%20at%20https%3A//github.com/MzeroMiko/VMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10166v2&entry.124074799=Read"},
{"title": "Quiver Laplacians and Feature Selection", "author": "Otto Sumray and Heather A. Harrington and Vidit Nanda", "abstract": "  The challenge of selecting the most relevant features of a given dataset\narises ubiquitously in data analysis and dimensionality reduction. However,\nfeatures found to be of high importance for the entire dataset may not be\nrelevant to subsets of interest, and vice versa. Given a feature selector and a\nfixed decomposition of the data into subsets, we describe a method for\nidentifying selected features which are compatible with the decomposition into\nsubsets. We achieve this by re-framing the problem of finding compatible\nfeatures to one of finding sections of a suitable quiver representation. In\norder to approximate such sections, we then introduce a Laplacian operator for\nquiver representations valued in Hilbert spaces. We provide explicit bounds on\nhow the spectrum of a quiver Laplacian changes when the representation and the\nunderlying quiver are modified in certain natural ways. Finally, we apply this\nmachinery to the study of peak-calling algorithms which measure chromatin\naccessibility in single-cell data. We demonstrate that eigenvectors of the\nassociated quiver Laplacian yield locally and globally compatible features.\n", "link": "http://arxiv.org/abs/2404.06993v1", "date": "2024-04-10", "relevancy": 2.0649, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4059}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quiver%20Laplacians%20and%20Feature%20Selection&body=Title%3A%20Quiver%20Laplacians%20and%20Feature%20Selection%0AAuthor%3A%20Otto%20Sumray%20and%20Heather%20A.%20Harrington%20and%20Vidit%20Nanda%0AAbstract%3A%20%20%20The%20challenge%20of%20selecting%20the%20most%20relevant%20features%20of%20a%20given%20dataset%0Aarises%20ubiquitously%20in%20data%20analysis%20and%20dimensionality%20reduction.%20However%2C%0Afeatures%20found%20to%20be%20of%20high%20importance%20for%20the%20entire%20dataset%20may%20not%20be%0Arelevant%20to%20subsets%20of%20interest%2C%20and%20vice%20versa.%20Given%20a%20feature%20selector%20and%20a%0Afixed%20decomposition%20of%20the%20data%20into%20subsets%2C%20we%20describe%20a%20method%20for%0Aidentifying%20selected%20features%20which%20are%20compatible%20with%20the%20decomposition%20into%0Asubsets.%20We%20achieve%20this%20by%20re-framing%20the%20problem%20of%20finding%20compatible%0Afeatures%20to%20one%20of%20finding%20sections%20of%20a%20suitable%20quiver%20representation.%20In%0Aorder%20to%20approximate%20such%20sections%2C%20we%20then%20introduce%20a%20Laplacian%20operator%20for%0Aquiver%20representations%20valued%20in%20Hilbert%20spaces.%20We%20provide%20explicit%20bounds%20on%0Ahow%20the%20spectrum%20of%20a%20quiver%20Laplacian%20changes%20when%20the%20representation%20and%20the%0Aunderlying%20quiver%20are%20modified%20in%20certain%20natural%20ways.%20Finally%2C%20we%20apply%20this%0Amachinery%20to%20the%20study%20of%20peak-calling%20algorithms%20which%20measure%20chromatin%0Aaccessibility%20in%20single-cell%20data.%20We%20demonstrate%20that%20eigenvectors%20of%20the%0Aassociated%20quiver%20Laplacian%20yield%20locally%20and%20globally%20compatible%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06993v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quiver%20Laplacians%20and%20Feature%20Selection&entry.906535625=Otto%20Sumray%20and%20Heather%20A.%20Harrington%20and%20Vidit%20Nanda&entry.1292438233=%20%20The%20challenge%20of%20selecting%20the%20most%20relevant%20features%20of%20a%20given%20dataset%0Aarises%20ubiquitously%20in%20data%20analysis%20and%20dimensionality%20reduction.%20However%2C%0Afeatures%20found%20to%20be%20of%20high%20importance%20for%20the%20entire%20dataset%20may%20not%20be%0Arelevant%20to%20subsets%20of%20interest%2C%20and%20vice%20versa.%20Given%20a%20feature%20selector%20and%20a%0Afixed%20decomposition%20of%20the%20data%20into%20subsets%2C%20we%20describe%20a%20method%20for%0Aidentifying%20selected%20features%20which%20are%20compatible%20with%20the%20decomposition%20into%0Asubsets.%20We%20achieve%20this%20by%20re-framing%20the%20problem%20of%20finding%20compatible%0Afeatures%20to%20one%20of%20finding%20sections%20of%20a%20suitable%20quiver%20representation.%20In%0Aorder%20to%20approximate%20such%20sections%2C%20we%20then%20introduce%20a%20Laplacian%20operator%20for%0Aquiver%20representations%20valued%20in%20Hilbert%20spaces.%20We%20provide%20explicit%20bounds%20on%0Ahow%20the%20spectrum%20of%20a%20quiver%20Laplacian%20changes%20when%20the%20representation%20and%20the%0Aunderlying%20quiver%20are%20modified%20in%20certain%20natural%20ways.%20Finally%2C%20we%20apply%20this%0Amachinery%20to%20the%20study%20of%20peak-calling%20algorithms%20which%20measure%20chromatin%0Aaccessibility%20in%20single-cell%20data.%20We%20demonstrate%20that%20eigenvectors%20of%20the%0Aassociated%20quiver%20Laplacian%20yield%20locally%20and%20globally%20compatible%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06993v1&entry.124074799=Read"},
{"title": "LaTiM: Longitudinal representation learning in continuous-time models to\n  predict disease progression", "author": "Rachid Zeghlache and Pierre-Henri Conze and Mostafa El Habib Daho and Yihao Li and Hugo Le Boit\u00e9 and Ramin Tadayoni and Pascal Massin and B\u00e9atrice Cochener and Alireza Rezaei and Ikram Brahim and Gwenol\u00e9 Quellec and Mathieu Lamard", "abstract": "  This work proposes a novel framework for analyzing disease progression using\ntime-aware neural ordinary differential equations (NODE). We introduce a\n\"time-aware head\" in a framework trained through self-supervised learning (SSL)\nto leverage temporal information in latent space for data augmentation. This\napproach effectively integrates NODEs with SSL, offering significant\nperformance improvements compared to traditional methods that lack explicit\ntemporal integration. We demonstrate the effectiveness of our strategy for\ndiabetic retinopathy progression prediction using the OPHDIAT database.\nCompared to the baseline, all NODE architectures achieve statistically\nsignificant improvements in area under the ROC curve (AUC) and Kappa metrics,\nhighlighting the efficacy of pre-training with SSL-inspired approaches.\nAdditionally, our framework promotes stable training for NODEs, a commonly\nencountered challenge in time-aware modeling.\n", "link": "http://arxiv.org/abs/2404.07091v1", "date": "2024-04-10", "relevancy": 2.0607, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5413}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5072}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaTiM%3A%20Longitudinal%20representation%20learning%20in%20continuous-time%20models%20to%0A%20%20predict%20disease%20progression&body=Title%3A%20LaTiM%3A%20Longitudinal%20representation%20learning%20in%20continuous-time%20models%20to%0A%20%20predict%20disease%20progression%0AAuthor%3A%20Rachid%20Zeghlache%20and%20Pierre-Henri%20Conze%20and%20Mostafa%20El%20Habib%20Daho%20and%20Yihao%20Li%20and%20Hugo%20Le%20Boit%C3%A9%20and%20Ramin%20Tadayoni%20and%20Pascal%20Massin%20and%20B%C3%A9atrice%20Cochener%20and%20Alireza%20Rezaei%20and%20Ikram%20Brahim%20and%20Gwenol%C3%A9%20Quellec%20and%20Mathieu%20Lamard%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20novel%20framework%20for%20analyzing%20disease%20progression%20using%0Atime-aware%20neural%20ordinary%20differential%20equations%20%28NODE%29.%20We%20introduce%20a%0A%22time-aware%20head%22%20in%20a%20framework%20trained%20through%20self-supervised%20learning%20%28SSL%29%0Ato%20leverage%20temporal%20information%20in%20latent%20space%20for%20data%20augmentation.%20This%0Aapproach%20effectively%20integrates%20NODEs%20with%20SSL%2C%20offering%20significant%0Aperformance%20improvements%20compared%20to%20traditional%20methods%20that%20lack%20explicit%0Atemporal%20integration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20strategy%20for%0Adiabetic%20retinopathy%20progression%20prediction%20using%20the%20OPHDIAT%20database.%0ACompared%20to%20the%20baseline%2C%20all%20NODE%20architectures%20achieve%20statistically%0Asignificant%20improvements%20in%20area%20under%20the%20ROC%20curve%20%28AUC%29%20and%20Kappa%20metrics%2C%0Ahighlighting%20the%20efficacy%20of%20pre-training%20with%20SSL-inspired%20approaches.%0AAdditionally%2C%20our%20framework%20promotes%20stable%20training%20for%20NODEs%2C%20a%20commonly%0Aencountered%20challenge%20in%20time-aware%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07091v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaTiM%3A%20Longitudinal%20representation%20learning%20in%20continuous-time%20models%20to%0A%20%20predict%20disease%20progression&entry.906535625=Rachid%20Zeghlache%20and%20Pierre-Henri%20Conze%20and%20Mostafa%20El%20Habib%20Daho%20and%20Yihao%20Li%20and%20Hugo%20Le%20Boit%C3%A9%20and%20Ramin%20Tadayoni%20and%20Pascal%20Massin%20and%20B%C3%A9atrice%20Cochener%20and%20Alireza%20Rezaei%20and%20Ikram%20Brahim%20and%20Gwenol%C3%A9%20Quellec%20and%20Mathieu%20Lamard&entry.1292438233=%20%20This%20work%20proposes%20a%20novel%20framework%20for%20analyzing%20disease%20progression%20using%0Atime-aware%20neural%20ordinary%20differential%20equations%20%28NODE%29.%20We%20introduce%20a%0A%22time-aware%20head%22%20in%20a%20framework%20trained%20through%20self-supervised%20learning%20%28SSL%29%0Ato%20leverage%20temporal%20information%20in%20latent%20space%20for%20data%20augmentation.%20This%0Aapproach%20effectively%20integrates%20NODEs%20with%20SSL%2C%20offering%20significant%0Aperformance%20improvements%20compared%20to%20traditional%20methods%20that%20lack%20explicit%0Atemporal%20integration.%20We%20demonstrate%20the%20effectiveness%20of%20our%20strategy%20for%0Adiabetic%20retinopathy%20progression%20prediction%20using%20the%20OPHDIAT%20database.%0ACompared%20to%20the%20baseline%2C%20all%20NODE%20architectures%20achieve%20statistically%0Asignificant%20improvements%20in%20area%20under%20the%20ROC%20curve%20%28AUC%29%20and%20Kappa%20metrics%2C%0Ahighlighting%20the%20efficacy%20of%20pre-training%20with%20SSL-inspired%20approaches.%0AAdditionally%2C%20our%20framework%20promotes%20stable%20training%20for%20NODEs%2C%20a%20commonly%0Aencountered%20challenge%20in%20time-aware%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07091v1&entry.124074799=Read"},
{"title": "Implicit Neural Representation for MRI Parallel Imaging Reconstruction", "author": "Hao Li and Yusheng Zhou and Jianan Liu and Xiling Liu and Tao Huang and Zhihan Lv and Weidong Cai", "abstract": "  Magnetic resonance imaging (MRI) usually faces lengthy acquisition times,\nprompting the exploration of strategies such as parallel imaging (PI) to\nalleviate this problem by periodically skipping specific K-space lines and\nsubsequently reconstructing high-quality images from the undersampled K-space.\nImplicit neural representation (INR) has recently emerged as a promising deep\nlearning technique, characterizing objects as continuous functions of spatial\ncoordinates typically parameterized by a multilayer perceptron (MLP). In this\nstudy, we propose a novel MRI PI reconstruction method that uses INR. Our\napproach represents reconstructed fully-sampled images as functions of voxel\ncoordinates and prior feature vectors from undersampled images, addressing the\ngeneralization challenges of INR. Specifically, we introduce a scale-embedded\nencoder to generate scale-independent, voxel-specific features from MR images\nacross various undersampling scales. These features are then concatenated with\ncoordinate vectors to reconstruct fully-sampled MR images, facilitating\nmultiple-scale reconstructions. To evaluate our method's performance, we\nconducted experiments using publicly available MRI datasets, comparing it with\nalternative reconstruction techniques. Our quantitative assessment demonstrates\nthe superiority of our proposed method.\n", "link": "http://arxiv.org/abs/2309.06067v6", "date": "2024-04-10", "relevancy": 2.058, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5565}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4734}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Implicit%20Neural%20Representation%20for%20MRI%20Parallel%20Imaging%20Reconstruction&body=Title%3A%20Implicit%20Neural%20Representation%20for%20MRI%20Parallel%20Imaging%20Reconstruction%0AAuthor%3A%20Hao%20Li%20and%20Yusheng%20Zhou%20and%20Jianan%20Liu%20and%20Xiling%20Liu%20and%20Tao%20Huang%20and%20Zhihan%20Lv%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20usually%20faces%20lengthy%20acquisition%20times%2C%0Aprompting%20the%20exploration%20of%20strategies%20such%20as%20parallel%20imaging%20%28PI%29%20to%0Aalleviate%20this%20problem%20by%20periodically%20skipping%20specific%20K-space%20lines%20and%0Asubsequently%20reconstructing%20high-quality%20images%20from%20the%20undersampled%20K-space.%0AImplicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%20deep%0Alearning%20technique%2C%20characterizing%20objects%20as%20continuous%20functions%20of%20spatial%0Acoordinates%20typically%20parameterized%20by%20a%20multilayer%20perceptron%20%28MLP%29.%20In%20this%0Astudy%2C%20we%20propose%20a%20novel%20MRI%20PI%20reconstruction%20method%20that%20uses%20INR.%20Our%0Aapproach%20represents%20reconstructed%20fully-sampled%20images%20as%20functions%20of%20voxel%0Acoordinates%20and%20prior%20feature%20vectors%20from%20undersampled%20images%2C%20addressing%20the%0Ageneralization%20challenges%20of%20INR.%20Specifically%2C%20we%20introduce%20a%20scale-embedded%0Aencoder%20to%20generate%20scale-independent%2C%20voxel-specific%20features%20from%20MR%20images%0Aacross%20various%20undersampling%20scales.%20These%20features%20are%20then%20concatenated%20with%0Acoordinate%20vectors%20to%20reconstruct%20fully-sampled%20MR%20images%2C%20facilitating%0Amultiple-scale%20reconstructions.%20To%20evaluate%20our%20method%27s%20performance%2C%20we%0Aconducted%20experiments%20using%20publicly%20available%20MRI%20datasets%2C%20comparing%20it%20with%0Aalternative%20reconstruction%20techniques.%20Our%20quantitative%20assessment%20demonstrates%0Athe%20superiority%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06067v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Neural%20Representation%20for%20MRI%20Parallel%20Imaging%20Reconstruction&entry.906535625=Hao%20Li%20and%20Yusheng%20Zhou%20and%20Jianan%20Liu%20and%20Xiling%20Liu%20and%20Tao%20Huang%20and%20Zhihan%20Lv%20and%20Weidong%20Cai&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20usually%20faces%20lengthy%20acquisition%20times%2C%0Aprompting%20the%20exploration%20of%20strategies%20such%20as%20parallel%20imaging%20%28PI%29%20to%0Aalleviate%20this%20problem%20by%20periodically%20skipping%20specific%20K-space%20lines%20and%0Asubsequently%20reconstructing%20high-quality%20images%20from%20the%20undersampled%20K-space.%0AImplicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%20deep%0Alearning%20technique%2C%20characterizing%20objects%20as%20continuous%20functions%20of%20spatial%0Acoordinates%20typically%20parameterized%20by%20a%20multilayer%20perceptron%20%28MLP%29.%20In%20this%0Astudy%2C%20we%20propose%20a%20novel%20MRI%20PI%20reconstruction%20method%20that%20uses%20INR.%20Our%0Aapproach%20represents%20reconstructed%20fully-sampled%20images%20as%20functions%20of%20voxel%0Acoordinates%20and%20prior%20feature%20vectors%20from%20undersampled%20images%2C%20addressing%20the%0Ageneralization%20challenges%20of%20INR.%20Specifically%2C%20we%20introduce%20a%20scale-embedded%0Aencoder%20to%20generate%20scale-independent%2C%20voxel-specific%20features%20from%20MR%20images%0Aacross%20various%20undersampling%20scales.%20These%20features%20are%20then%20concatenated%20with%0Acoordinate%20vectors%20to%20reconstruct%20fully-sampled%20MR%20images%2C%20facilitating%0Amultiple-scale%20reconstructions.%20To%20evaluate%20our%20method%27s%20performance%2C%20we%0Aconducted%20experiments%20using%20publicly%20available%20MRI%20datasets%2C%20comparing%20it%20with%0Aalternative%20reconstruction%20techniques.%20Our%20quantitative%20assessment%20demonstrates%0Athe%20superiority%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06067v6&entry.124074799=Read"},
{"title": "Two-Phase Multi-Dose-Level PET Image Reconstruction with Dose Level\n  Awareness", "author": "Yuchen Fei and Yanmei Luo and Yan Wang and Jiaqi Cui and Yuanyuan Xu and Jiliu Zhou and Dinggang Shen", "abstract": "  To obtain high-quality positron emission tomography (PET) while minimizing\nradiation exposure, a range of methods have been designed to reconstruct\nstandard-dose PET (SPET) from corresponding low-dose PET (LPET) images.\nHowever, most current methods merely learn the mapping between\nsingle-dose-level LPET and SPET images, but omit the dose disparity of LPET\nimages in clinical scenarios. In this paper, to reconstruct high-quality SPET\nimages from multi-dose-level LPET images, we design a novel two-phase\nmulti-dose-level PET reconstruction algorithm with dose level awareness,\ncontaining a pre-training phase and a SPET prediction phase. Specifically, the\npre-training phase is devised to explore both fine-grained discriminative\nfeatures and effective semantic representation. The SPET prediction phase\nadopts a coarse prediction network utilizing pre-learned dose level prior to\ngenerate preliminary result, and a refinement network to precisely preserve the\ndetails. Experiments on MICCAI 2022 Ultra-low Dose PET Imaging Challenge\nDataset have demonstrated the superiority of our method.\n", "link": "http://arxiv.org/abs/2404.01563v2", "date": "2024-04-10", "relevancy": 2.0517, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5106}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Two-Phase%20Multi-Dose-Level%20PET%20Image%20Reconstruction%20with%20Dose%20Level%0A%20%20Awareness&body=Title%3A%20Two-Phase%20Multi-Dose-Level%20PET%20Image%20Reconstruction%20with%20Dose%20Level%0A%20%20Awareness%0AAuthor%3A%20Yuchen%20Fei%20and%20Yanmei%20Luo%20and%20Yan%20Wang%20and%20Jiaqi%20Cui%20and%20Yuanyuan%20Xu%20and%20Jiliu%20Zhou%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20To%20obtain%20high-quality%20positron%20emission%20tomography%20%28PET%29%20while%20minimizing%0Aradiation%20exposure%2C%20a%20range%20of%20methods%20have%20been%20designed%20to%20reconstruct%0Astandard-dose%20PET%20%28SPET%29%20from%20corresponding%20low-dose%20PET%20%28LPET%29%20images.%0AHowever%2C%20most%20current%20methods%20merely%20learn%20the%20mapping%20between%0Asingle-dose-level%20LPET%20and%20SPET%20images%2C%20but%20omit%20the%20dose%20disparity%20of%20LPET%0Aimages%20in%20clinical%20scenarios.%20In%20this%20paper%2C%20to%20reconstruct%20high-quality%20SPET%0Aimages%20from%20multi-dose-level%20LPET%20images%2C%20we%20design%20a%20novel%20two-phase%0Amulti-dose-level%20PET%20reconstruction%20algorithm%20with%20dose%20level%20awareness%2C%0Acontaining%20a%20pre-training%20phase%20and%20a%20SPET%20prediction%20phase.%20Specifically%2C%20the%0Apre-training%20phase%20is%20devised%20to%20explore%20both%20fine-grained%20discriminative%0Afeatures%20and%20effective%20semantic%20representation.%20The%20SPET%20prediction%20phase%0Aadopts%20a%20coarse%20prediction%20network%20utilizing%20pre-learned%20dose%20level%20prior%20to%0Agenerate%20preliminary%20result%2C%20and%20a%20refinement%20network%20to%20precisely%20preserve%20the%0Adetails.%20Experiments%20on%20MICCAI%202022%20Ultra-low%20Dose%20PET%20Imaging%20Challenge%0ADataset%20have%20demonstrated%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01563v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Phase%20Multi-Dose-Level%20PET%20Image%20Reconstruction%20with%20Dose%20Level%0A%20%20Awareness&entry.906535625=Yuchen%20Fei%20and%20Yanmei%20Luo%20and%20Yan%20Wang%20and%20Jiaqi%20Cui%20and%20Yuanyuan%20Xu%20and%20Jiliu%20Zhou%20and%20Dinggang%20Shen&entry.1292438233=%20%20To%20obtain%20high-quality%20positron%20emission%20tomography%20%28PET%29%20while%20minimizing%0Aradiation%20exposure%2C%20a%20range%20of%20methods%20have%20been%20designed%20to%20reconstruct%0Astandard-dose%20PET%20%28SPET%29%20from%20corresponding%20low-dose%20PET%20%28LPET%29%20images.%0AHowever%2C%20most%20current%20methods%20merely%20learn%20the%20mapping%20between%0Asingle-dose-level%20LPET%20and%20SPET%20images%2C%20but%20omit%20the%20dose%20disparity%20of%20LPET%0Aimages%20in%20clinical%20scenarios.%20In%20this%20paper%2C%20to%20reconstruct%20high-quality%20SPET%0Aimages%20from%20multi-dose-level%20LPET%20images%2C%20we%20design%20a%20novel%20two-phase%0Amulti-dose-level%20PET%20reconstruction%20algorithm%20with%20dose%20level%20awareness%2C%0Acontaining%20a%20pre-training%20phase%20and%20a%20SPET%20prediction%20phase.%20Specifically%2C%20the%0Apre-training%20phase%20is%20devised%20to%20explore%20both%20fine-grained%20discriminative%0Afeatures%20and%20effective%20semantic%20representation.%20The%20SPET%20prediction%20phase%0Aadopts%20a%20coarse%20prediction%20network%20utilizing%20pre-learned%20dose%20level%20prior%20to%0Agenerate%20preliminary%20result%2C%20and%20a%20refinement%20network%20to%20precisely%20preserve%20the%0Adetails.%20Experiments%20on%20MICCAI%202022%20Ultra-low%20Dose%20PET%20Imaging%20Challenge%0ADataset%20have%20demonstrated%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01563v2&entry.124074799=Read"},
{"title": "Multi-Agent Soft Actor-Critic with Global Loss for Autonomous\n  Mobility-on-Demand Fleet Control", "author": "Zeno Woywood and Jasper I. Wiltfang and Julius Luy and Tobias Enders and Maximilian Schiffer", "abstract": "  We study a sequential decision-making problem for a profit-maximizing\noperator of an Autonomous Mobility-on-Demand system. Optimizing a central\noperator's vehicle-to-request dispatching policy requires efficient and\neffective fleet control strategies. To this end, we employ a multi-agent Soft\nActor-Critic algorithm combined with weighted bipartite matching. We propose a\nnovel vehicle-based algorithm architecture and adapt the critic's loss function\nto appropriately consider global actions. Furthermore, we extend our algorithm\nto incorporate rebalancing capabilities. Through numerical experiments, we show\nthat our approach outperforms state-of-the-art benchmarks by up to 12.9% for\ndispatching and up to 38.9% with integrated rebalancing.\n", "link": "http://arxiv.org/abs/2404.06975v1", "date": "2024-04-10", "relevancy": 2.048, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4976}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Soft%20Actor-Critic%20with%20Global%20Loss%20for%20Autonomous%0A%20%20Mobility-on-Demand%20Fleet%20Control&body=Title%3A%20Multi-Agent%20Soft%20Actor-Critic%20with%20Global%20Loss%20for%20Autonomous%0A%20%20Mobility-on-Demand%20Fleet%20Control%0AAuthor%3A%20Zeno%20Woywood%20and%20Jasper%20I.%20Wiltfang%20and%20Julius%20Luy%20and%20Tobias%20Enders%20and%20Maximilian%20Schiffer%0AAbstract%3A%20%20%20We%20study%20a%20sequential%20decision-making%20problem%20for%20a%20profit-maximizing%0Aoperator%20of%20an%20Autonomous%20Mobility-on-Demand%20system.%20Optimizing%20a%20central%0Aoperator%27s%20vehicle-to-request%20dispatching%20policy%20requires%20efficient%20and%0Aeffective%20fleet%20control%20strategies.%20To%20this%20end%2C%20we%20employ%20a%20multi-agent%20Soft%0AActor-Critic%20algorithm%20combined%20with%20weighted%20bipartite%20matching.%20We%20propose%20a%0Anovel%20vehicle-based%20algorithm%20architecture%20and%20adapt%20the%20critic%27s%20loss%20function%0Ato%20appropriately%20consider%20global%20actions.%20Furthermore%2C%20we%20extend%20our%20algorithm%0Ato%20incorporate%20rebalancing%20capabilities.%20Through%20numerical%20experiments%2C%20we%20show%0Athat%20our%20approach%20outperforms%20state-of-the-art%20benchmarks%20by%20up%20to%2012.9%25%20for%0Adispatching%20and%20up%20to%2038.9%25%20with%20integrated%20rebalancing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06975v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Soft%20Actor-Critic%20with%20Global%20Loss%20for%20Autonomous%0A%20%20Mobility-on-Demand%20Fleet%20Control&entry.906535625=Zeno%20Woywood%20and%20Jasper%20I.%20Wiltfang%20and%20Julius%20Luy%20and%20Tobias%20Enders%20and%20Maximilian%20Schiffer&entry.1292438233=%20%20We%20study%20a%20sequential%20decision-making%20problem%20for%20a%20profit-maximizing%0Aoperator%20of%20an%20Autonomous%20Mobility-on-Demand%20system.%20Optimizing%20a%20central%0Aoperator%27s%20vehicle-to-request%20dispatching%20policy%20requires%20efficient%20and%0Aeffective%20fleet%20control%20strategies.%20To%20this%20end%2C%20we%20employ%20a%20multi-agent%20Soft%0AActor-Critic%20algorithm%20combined%20with%20weighted%20bipartite%20matching.%20We%20propose%20a%0Anovel%20vehicle-based%20algorithm%20architecture%20and%20adapt%20the%20critic%27s%20loss%20function%0Ato%20appropriately%20consider%20global%20actions.%20Furthermore%2C%20we%20extend%20our%20algorithm%0Ato%20incorporate%20rebalancing%20capabilities.%20Through%20numerical%20experiments%2C%20we%20show%0Athat%20our%20approach%20outperforms%20state-of-the-art%20benchmarks%20by%20up%20to%2012.9%25%20for%0Adispatching%20and%20up%20to%2038.9%25%20with%20integrated%20rebalancing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06975v1&entry.124074799=Read"},
{"title": "V-MAD: Video-based Morphing Attack Detection in Operational Scenarios", "author": "Guido Borghi and Annalisa Franco and Nicol\u00f2 Di Domenico and Matteo Ferrara and Davide Maltoni", "abstract": "  In response to the rising threat of the face morphing attack, this paper\nintroduces and explores the potential of Video-based Morphing Attack Detection\n(V-MAD) systems in real-world operational scenarios. While current morphing\nattack detection methods primarily focus on a single or a pair of images, V-MAD\nis based on video sequences, exploiting the video streams often acquired by\nface verification tools available, for instance, at airport gates. Through this\nstudy, we show for the first time the advantages that the availability of\nmultiple probe frames can bring to the morphing attack detection task,\nespecially in scenarios where the quality of probe images is varied and might\nbe affected, for instance, by pose or illumination variations. Experimental\nresults on a real operational database demonstrate that video sequences\nrepresent valuable information for increasing the robustness and performance of\nmorphing attack detection systems.\n", "link": "http://arxiv.org/abs/2404.06963v1", "date": "2024-04-10", "relevancy": 2.0366, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5324}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4993}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4898}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20V-MAD%3A%20Video-based%20Morphing%20Attack%20Detection%20in%20Operational%20Scenarios&body=Title%3A%20V-MAD%3A%20Video-based%20Morphing%20Attack%20Detection%20in%20Operational%20Scenarios%0AAuthor%3A%20Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Nicol%C3%B2%20Di%20Domenico%20and%20Matteo%20Ferrara%20and%20Davide%20Maltoni%0AAbstract%3A%20%20%20In%20response%20to%20the%20rising%20threat%20of%20the%20face%20morphing%20attack%2C%20this%20paper%0Aintroduces%20and%20explores%20the%20potential%20of%20Video-based%20Morphing%20Attack%20Detection%0A%28V-MAD%29%20systems%20in%20real-world%20operational%20scenarios.%20While%20current%20morphing%0Aattack%20detection%20methods%20primarily%20focus%20on%20a%20single%20or%20a%20pair%20of%20images%2C%20V-MAD%0Ais%20based%20on%20video%20sequences%2C%20exploiting%20the%20video%20streams%20often%20acquired%20by%0Aface%20verification%20tools%20available%2C%20for%20instance%2C%20at%20airport%20gates.%20Through%20this%0Astudy%2C%20we%20show%20for%20the%20first%20time%20the%20advantages%20that%20the%20availability%20of%0Amultiple%20probe%20frames%20can%20bring%20to%20the%20morphing%20attack%20detection%20task%2C%0Aespecially%20in%20scenarios%20where%20the%20quality%20of%20probe%20images%20is%20varied%20and%20might%0Abe%20affected%2C%20for%20instance%2C%20by%20pose%20or%20illumination%20variations.%20Experimental%0Aresults%20on%20a%20real%20operational%20database%20demonstrate%20that%20video%20sequences%0Arepresent%20valuable%20information%20for%20increasing%20the%20robustness%20and%20performance%20of%0Amorphing%20attack%20detection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06963v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-MAD%3A%20Video-based%20Morphing%20Attack%20Detection%20in%20Operational%20Scenarios&entry.906535625=Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Nicol%C3%B2%20Di%20Domenico%20and%20Matteo%20Ferrara%20and%20Davide%20Maltoni&entry.1292438233=%20%20In%20response%20to%20the%20rising%20threat%20of%20the%20face%20morphing%20attack%2C%20this%20paper%0Aintroduces%20and%20explores%20the%20potential%20of%20Video-based%20Morphing%20Attack%20Detection%0A%28V-MAD%29%20systems%20in%20real-world%20operational%20scenarios.%20While%20current%20morphing%0Aattack%20detection%20methods%20primarily%20focus%20on%20a%20single%20or%20a%20pair%20of%20images%2C%20V-MAD%0Ais%20based%20on%20video%20sequences%2C%20exploiting%20the%20video%20streams%20often%20acquired%20by%0Aface%20verification%20tools%20available%2C%20for%20instance%2C%20at%20airport%20gates.%20Through%20this%0Astudy%2C%20we%20show%20for%20the%20first%20time%20the%20advantages%20that%20the%20availability%20of%0Amultiple%20probe%20frames%20can%20bring%20to%20the%20morphing%20attack%20detection%20task%2C%0Aespecially%20in%20scenarios%20where%20the%20quality%20of%20probe%20images%20is%20varied%20and%20might%0Abe%20affected%2C%20for%20instance%2C%20by%20pose%20or%20illumination%20variations.%20Experimental%0Aresults%20on%20a%20real%20operational%20database%20demonstrate%20that%20video%20sequences%0Arepresent%20valuable%20information%20for%20increasing%20the%20robustness%20and%20performance%20of%0Amorphing%20attack%20detection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06963v1&entry.124074799=Read"},
{"title": "Deep Learning for Inertial Sensor Alignment", "author": "Maxim Freydin and Niv Sfaradi and Nimrod Segol and Areej Eweida and Barak Or", "abstract": "  Accurate alignment of a fixed mobile device equipped with inertial sensors\ninside a moving vehicle is important for navigation, activity recognition, and\nother applications. Accurate estimation of the device mounting angle is\nrequired to rotate the inertial measurement from the sensor frame to the moving\nplatform frame to standardize measurements and improve the performance of the\ntarget task. In this work, a data-driven approach using deep neural networks\n(DNNs) is proposed to learn the yaw mounting angle of a smartphone equipped\nwith an inertial measurement unit (IMU) and strapped to a car. The proposed\nmodel uses only the accelerometer and gyroscope readings from an IMU as input\nand, in contrast to existing solutions, does not require global position inputs\nfrom global navigation satellite systems (GNSS). To train the model in a\nsupervised manner, IMU data is collected for training and validation with the\nsensor mounted at a known yaw mounting angle, and a range of ground truth\nlabels is generated by applying a random rotation in a bounded range to the\nmeasurements. The trained model is tested on data with real rotations showing\nsimilar performance as with synthetic rotations. The trained model is deployed\non an Android device and evaluated in real-time to test the accuracy of the\nestimated yaw mounting angle. The model is shown to find the mounting angle at\nan accuracy of 8 degrees within 5 seconds, and 4 degrees within 27 seconds. An\nexperiment is conducted to compare the proposed model with an existing\noff-the-shelf solution.\n", "link": "http://arxiv.org/abs/2212.11120v2", "date": "2024-04-10", "relevancy": 2.0257, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Inertial%20Sensor%20Alignment&body=Title%3A%20Deep%20Learning%20for%20Inertial%20Sensor%20Alignment%0AAuthor%3A%20Maxim%20Freydin%20and%20Niv%20Sfaradi%20and%20Nimrod%20Segol%20and%20Areej%20Eweida%20and%20Barak%20Or%0AAbstract%3A%20%20%20Accurate%20alignment%20of%20a%20fixed%20mobile%20device%20equipped%20with%20inertial%20sensors%0Ainside%20a%20moving%20vehicle%20is%20important%20for%20navigation%2C%20activity%20recognition%2C%20and%0Aother%20applications.%20Accurate%20estimation%20of%20the%20device%20mounting%20angle%20is%0Arequired%20to%20rotate%20the%20inertial%20measurement%20from%20the%20sensor%20frame%20to%20the%20moving%0Aplatform%20frame%20to%20standardize%20measurements%20and%20improve%20the%20performance%20of%20the%0Atarget%20task.%20In%20this%20work%2C%20a%20data-driven%20approach%20using%20deep%20neural%20networks%0A%28DNNs%29%20is%20proposed%20to%20learn%20the%20yaw%20mounting%20angle%20of%20a%20smartphone%20equipped%0Awith%20an%20inertial%20measurement%20unit%20%28IMU%29%20and%20strapped%20to%20a%20car.%20The%20proposed%0Amodel%20uses%20only%20the%20accelerometer%20and%20gyroscope%20readings%20from%20an%20IMU%20as%20input%0Aand%2C%20in%20contrast%20to%20existing%20solutions%2C%20does%20not%20require%20global%20position%20inputs%0Afrom%20global%20navigation%20satellite%20systems%20%28GNSS%29.%20To%20train%20the%20model%20in%20a%0Asupervised%20manner%2C%20IMU%20data%20is%20collected%20for%20training%20and%20validation%20with%20the%0Asensor%20mounted%20at%20a%20known%20yaw%20mounting%20angle%2C%20and%20a%20range%20of%20ground%20truth%0Alabels%20is%20generated%20by%20applying%20a%20random%20rotation%20in%20a%20bounded%20range%20to%20the%0Ameasurements.%20The%20trained%20model%20is%20tested%20on%20data%20with%20real%20rotations%20showing%0Asimilar%20performance%20as%20with%20synthetic%20rotations.%20The%20trained%20model%20is%20deployed%0Aon%20an%20Android%20device%20and%20evaluated%20in%20real-time%20to%20test%20the%20accuracy%20of%20the%0Aestimated%20yaw%20mounting%20angle.%20The%20model%20is%20shown%20to%20find%20the%20mounting%20angle%20at%0Aan%20accuracy%20of%208%20degrees%20within%205%20seconds%2C%20and%204%20degrees%20within%2027%20seconds.%20An%0Aexperiment%20is%20conducted%20to%20compare%20the%20proposed%20model%20with%20an%20existing%0Aoff-the-shelf%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.11120v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Inertial%20Sensor%20Alignment&entry.906535625=Maxim%20Freydin%20and%20Niv%20Sfaradi%20and%20Nimrod%20Segol%20and%20Areej%20Eweida%20and%20Barak%20Or&entry.1292438233=%20%20Accurate%20alignment%20of%20a%20fixed%20mobile%20device%20equipped%20with%20inertial%20sensors%0Ainside%20a%20moving%20vehicle%20is%20important%20for%20navigation%2C%20activity%20recognition%2C%20and%0Aother%20applications.%20Accurate%20estimation%20of%20the%20device%20mounting%20angle%20is%0Arequired%20to%20rotate%20the%20inertial%20measurement%20from%20the%20sensor%20frame%20to%20the%20moving%0Aplatform%20frame%20to%20standardize%20measurements%20and%20improve%20the%20performance%20of%20the%0Atarget%20task.%20In%20this%20work%2C%20a%20data-driven%20approach%20using%20deep%20neural%20networks%0A%28DNNs%29%20is%20proposed%20to%20learn%20the%20yaw%20mounting%20angle%20of%20a%20smartphone%20equipped%0Awith%20an%20inertial%20measurement%20unit%20%28IMU%29%20and%20strapped%20to%20a%20car.%20The%20proposed%0Amodel%20uses%20only%20the%20accelerometer%20and%20gyroscope%20readings%20from%20an%20IMU%20as%20input%0Aand%2C%20in%20contrast%20to%20existing%20solutions%2C%20does%20not%20require%20global%20position%20inputs%0Afrom%20global%20navigation%20satellite%20systems%20%28GNSS%29.%20To%20train%20the%20model%20in%20a%0Asupervised%20manner%2C%20IMU%20data%20is%20collected%20for%20training%20and%20validation%20with%20the%0Asensor%20mounted%20at%20a%20known%20yaw%20mounting%20angle%2C%20and%20a%20range%20of%20ground%20truth%0Alabels%20is%20generated%20by%20applying%20a%20random%20rotation%20in%20a%20bounded%20range%20to%20the%0Ameasurements.%20The%20trained%20model%20is%20tested%20on%20data%20with%20real%20rotations%20showing%0Asimilar%20performance%20as%20with%20synthetic%20rotations.%20The%20trained%20model%20is%20deployed%0Aon%20an%20Android%20device%20and%20evaluated%20in%20real-time%20to%20test%20the%20accuracy%20of%20the%0Aestimated%20yaw%20mounting%20angle.%20The%20model%20is%20shown%20to%20find%20the%20mounting%20angle%20at%0Aan%20accuracy%20of%208%20degrees%20within%205%20seconds%2C%20and%204%20degrees%20within%2027%20seconds.%20An%0Aexperiment%20is%20conducted%20to%20compare%20the%20proposed%20model%20with%20an%20existing%0Aoff-the-shelf%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.11120v2&entry.124074799=Read"},
{"title": "LLMatic: Neural Architecture Search via Large Language Models and\n  Quality Diversity Optimization", "author": "Muhammad U. Nasir and Sam Earle and Julian Togelius and Steven James and Christopher Cleghorn", "abstract": "  Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. Here, we propose using the coding abilities of LLMs to introduce\nmeaningful variations to code defining neural networks. Meanwhile,\nQuality-Diversity (QD) algorithms are known to discover diverse and robust\nsolutions. By merging the code-generating abilities of LLMs with the diversity\nand robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, \\texttt{LLMatic} uses a procedural approach,\nleveraging QD for prompts and network architecture to create diverse and\nhigh-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and\nNAS-bench-201 benchmarks, demonstrating that it can produce competitive\nnetworks while evaluating just $2,000$ candidates, even without prior knowledge\nof the benchmark domain or exposure to any previous top-performing models for\nthe benchmark. The open-sourced code is available in\n\\url{https://github.com/umair-nasir14/LLMatic}.\n", "link": "http://arxiv.org/abs/2306.01102v7", "date": "2024-04-10", "relevancy": 2.0216, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5056}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5036}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLMatic%3A%20Neural%20Architecture%20Search%20via%20Large%20Language%20Models%20and%0A%20%20Quality%20Diversity%20Optimization&body=Title%3A%20LLMatic%3A%20Neural%20Architecture%20Search%20via%20Large%20Language%20Models%20and%0A%20%20Quality%20Diversity%20Optimization%0AAuthor%3A%20Muhammad%20U.%20Nasir%20and%20Sam%20Earle%20and%20Julian%20Togelius%20and%20Steven%20James%20and%20Christopher%20Cleghorn%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20capable%20of%0Aaccomplishing%20a%20broad%20spectrum%20of%20tasks.%20Their%20abilities%20span%20numerous%20areas%2C%0Aand%20one%20area%20where%20they%20have%20made%20a%20significant%20impact%20is%20in%20the%20domain%20of%20code%0Ageneration.%20Here%2C%20we%20propose%20using%20the%20coding%20abilities%20of%20LLMs%20to%20introduce%0Ameaningful%20variations%20to%20code%20defining%20neural%20networks.%20Meanwhile%2C%0AQuality-Diversity%20%28QD%29%20algorithms%20are%20known%20to%20discover%20diverse%20and%20robust%0Asolutions.%20By%20merging%20the%20code-generating%20abilities%20of%20LLMs%20with%20the%20diversity%0Aand%20robustness%20of%20QD%20solutions%2C%20we%20introduce%20%5Ctexttt%7BLLMatic%7D%2C%20a%20Neural%0AArchitecture%20Search%20%28NAS%29%20algorithm.%20While%20LLMs%20struggle%20to%20conduct%20NAS%0Adirectly%20through%20prompts%2C%20%5Ctexttt%7BLLMatic%7D%20uses%20a%20procedural%20approach%2C%0Aleveraging%20QD%20for%20prompts%20and%20network%20architecture%20to%20create%20diverse%20and%0Ahigh-performing%20networks.%20We%20test%20%5Ctexttt%7BLLMatic%7D%20on%20the%20CIFAR-10%20and%0ANAS-bench-201%20benchmarks%2C%20demonstrating%20that%20it%20can%20produce%20competitive%0Anetworks%20while%20evaluating%20just%20%242%2C000%24%20candidates%2C%20even%20without%20prior%20knowledge%0Aof%20the%20benchmark%20domain%20or%20exposure%20to%20any%20previous%20top-performing%20models%20for%0Athe%20benchmark.%20The%20open-sourced%20code%20is%20available%20in%0A%5Curl%7Bhttps%3A//github.com/umair-nasir14/LLMatic%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.01102v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMatic%3A%20Neural%20Architecture%20Search%20via%20Large%20Language%20Models%20and%0A%20%20Quality%20Diversity%20Optimization&entry.906535625=Muhammad%20U.%20Nasir%20and%20Sam%20Earle%20and%20Julian%20Togelius%20and%20Steven%20James%20and%20Christopher%20Cleghorn&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20tools%20capable%20of%0Aaccomplishing%20a%20broad%20spectrum%20of%20tasks.%20Their%20abilities%20span%20numerous%20areas%2C%0Aand%20one%20area%20where%20they%20have%20made%20a%20significant%20impact%20is%20in%20the%20domain%20of%20code%0Ageneration.%20Here%2C%20we%20propose%20using%20the%20coding%20abilities%20of%20LLMs%20to%20introduce%0Ameaningful%20variations%20to%20code%20defining%20neural%20networks.%20Meanwhile%2C%0AQuality-Diversity%20%28QD%29%20algorithms%20are%20known%20to%20discover%20diverse%20and%20robust%0Asolutions.%20By%20merging%20the%20code-generating%20abilities%20of%20LLMs%20with%20the%20diversity%0Aand%20robustness%20of%20QD%20solutions%2C%20we%20introduce%20%5Ctexttt%7BLLMatic%7D%2C%20a%20Neural%0AArchitecture%20Search%20%28NAS%29%20algorithm.%20While%20LLMs%20struggle%20to%20conduct%20NAS%0Adirectly%20through%20prompts%2C%20%5Ctexttt%7BLLMatic%7D%20uses%20a%20procedural%20approach%2C%0Aleveraging%20QD%20for%20prompts%20and%20network%20architecture%20to%20create%20diverse%20and%0Ahigh-performing%20networks.%20We%20test%20%5Ctexttt%7BLLMatic%7D%20on%20the%20CIFAR-10%20and%0ANAS-bench-201%20benchmarks%2C%20demonstrating%20that%20it%20can%20produce%20competitive%0Anetworks%20while%20evaluating%20just%20%242%2C000%24%20candidates%2C%20even%20without%20prior%20knowledge%0Aof%20the%20benchmark%20domain%20or%20exposure%20to%20any%20previous%20top-performing%20models%20for%0Athe%20benchmark.%20The%20open-sourced%20code%20is%20available%20in%0A%5Curl%7Bhttps%3A//github.com/umair-nasir14/LLMatic%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.01102v7&entry.124074799=Read"},
{"title": "DREAM: Visual Decoding from Reversing Human Visual System", "author": "Weihao Xia and Raoul de Charette and Cengiz \u00d6ztireli and Jing-Hao Xue", "abstract": "  In this work we present DREAM, an fMRI-to-image method for reconstructing\nviewed images from brain activities, grounded on fundamental knowledge of the\nhuman visual system. We craft reverse pathways that emulate the hierarchical\nand parallel nature of how humans perceive the visual world. These tailored\npathways are specialized to decipher semantics, color, and depth cues from fMRI\ndata, mirroring the forward pathways from visual stimuli to fMRI recordings. To\ndo so, two components mimic the inverse processes within the human visual\nsystem: the Reverse Visual Association Cortex (R-VAC) which reverses pathways\nof this brain region, extracting semantics from fMRI data; the Reverse Parallel\nPKM (R-PKM) component simultaneously predicting color and depth from fMRI\nsignals. The experiments indicate that our method outperforms the current\nstate-of-the-art models in terms of the consistency of appearance, structure,\nand semantics. Code will be made publicly available to facilitate further\nresearch in this field.\n", "link": "http://arxiv.org/abs/2310.02265v2", "date": "2024-04-10", "relevancy": 2.0152, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5142}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4883}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DREAM%3A%20Visual%20Decoding%20from%20Reversing%20Human%20Visual%20System&body=Title%3A%20DREAM%3A%20Visual%20Decoding%20from%20Reversing%20Human%20Visual%20System%0AAuthor%3A%20Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20In%20this%20work%20we%20present%20DREAM%2C%20an%20fMRI-to-image%20method%20for%20reconstructing%0Aviewed%20images%20from%20brain%20activities%2C%20grounded%20on%20fundamental%20knowledge%20of%20the%0Ahuman%20visual%20system.%20We%20craft%20reverse%20pathways%20that%20emulate%20the%20hierarchical%0Aand%20parallel%20nature%20of%20how%20humans%20perceive%20the%20visual%20world.%20These%20tailored%0Apathways%20are%20specialized%20to%20decipher%20semantics%2C%20color%2C%20and%20depth%20cues%20from%20fMRI%0Adata%2C%20mirroring%20the%20forward%20pathways%20from%20visual%20stimuli%20to%20fMRI%20recordings.%20To%0Ado%20so%2C%20two%20components%20mimic%20the%20inverse%20processes%20within%20the%20human%20visual%0Asystem%3A%20the%20Reverse%20Visual%20Association%20Cortex%20%28R-VAC%29%20which%20reverses%20pathways%0Aof%20this%20brain%20region%2C%20extracting%20semantics%20from%20fMRI%20data%3B%20the%20Reverse%20Parallel%0APKM%20%28R-PKM%29%20component%20simultaneously%20predicting%20color%20and%20depth%20from%20fMRI%0Asignals.%20The%20experiments%20indicate%20that%20our%20method%20outperforms%20the%20current%0Astate-of-the-art%20models%20in%20terms%20of%20the%20consistency%20of%20appearance%2C%20structure%2C%0Aand%20semantics.%20Code%20will%20be%20made%20publicly%20available%20to%20facilitate%20further%0Aresearch%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02265v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DREAM%3A%20Visual%20Decoding%20from%20Reversing%20Human%20Visual%20System&entry.906535625=Weihao%20Xia%20and%20Raoul%20de%20Charette%20and%20Cengiz%20%C3%96ztireli%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20In%20this%20work%20we%20present%20DREAM%2C%20an%20fMRI-to-image%20method%20for%20reconstructing%0Aviewed%20images%20from%20brain%20activities%2C%20grounded%20on%20fundamental%20knowledge%20of%20the%0Ahuman%20visual%20system.%20We%20craft%20reverse%20pathways%20that%20emulate%20the%20hierarchical%0Aand%20parallel%20nature%20of%20how%20humans%20perceive%20the%20visual%20world.%20These%20tailored%0Apathways%20are%20specialized%20to%20decipher%20semantics%2C%20color%2C%20and%20depth%20cues%20from%20fMRI%0Adata%2C%20mirroring%20the%20forward%20pathways%20from%20visual%20stimuli%20to%20fMRI%20recordings.%20To%0Ado%20so%2C%20two%20components%20mimic%20the%20inverse%20processes%20within%20the%20human%20visual%0Asystem%3A%20the%20Reverse%20Visual%20Association%20Cortex%20%28R-VAC%29%20which%20reverses%20pathways%0Aof%20this%20brain%20region%2C%20extracting%20semantics%20from%20fMRI%20data%3B%20the%20Reverse%20Parallel%0APKM%20%28R-PKM%29%20component%20simultaneously%20predicting%20color%20and%20depth%20from%20fMRI%0Asignals.%20The%20experiments%20indicate%20that%20our%20method%20outperforms%20the%20current%0Astate-of-the-art%20models%20in%20terms%20of%20the%20consistency%20of%20appearance%2C%20structure%2C%0Aand%20semantics.%20Code%20will%20be%20made%20publicly%20available%20to%20facilitate%20further%0Aresearch%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02265v2&entry.124074799=Read"},
{"title": "Improving Language Model Reasoning with Self-motivated Learning", "author": "Yunlong Feng and Yang Xu and Libo Qin and Yasheng Wang and Wanxiang Che", "abstract": "  Large-scale high-quality training data is important for improving the\nperformance of models. After trained with data that has rationales (reasoning\nsteps), models gain reasoning capability. However, the dataset with\nhigh-quality rationales is relatively scarce due to the high annotation cost.\nTo address this issue, we propose \\textit{Self-motivated Learning} framework.\nThe framework motivates the model itself to automatically generate rationales\non existing datasets. Based on the inherent rank from correctness across\nmultiple rationales, the model learns to generate better rationales, leading to\nhigher reasoning capability. Specifically, we train a reward model with the\nrank to evaluate the quality of rationales, and improve the performance of\nreasoning through reinforcement learning. Experiment results of Llama2 7B on\nmultiple reasoning datasets show that our method significantly improves the\nreasoning ability of models, even outperforming text-davinci-002 in some\ndatasets.\n", "link": "http://arxiv.org/abs/2404.07017v1", "date": "2024-04-10", "relevancy": 2.0131, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5325}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning&body=Title%3A%20Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning%0AAuthor%3A%20Yunlong%20Feng%20and%20Yang%20Xu%20and%20Libo%20Qin%20and%20Yasheng%20Wang%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Large-scale%20high-quality%20training%20data%20is%20important%20for%20improving%20the%0Aperformance%20of%20models.%20After%20trained%20with%20data%20that%20has%20rationales%20%28reasoning%0Asteps%29%2C%20models%20gain%20reasoning%20capability.%20However%2C%20the%20dataset%20with%0Ahigh-quality%20rationales%20is%20relatively%20scarce%20due%20to%20the%20high%20annotation%20cost.%0ATo%20address%20this%20issue%2C%20we%20propose%20%5Ctextit%7BSelf-motivated%20Learning%7D%20framework.%0AThe%20framework%20motivates%20the%20model%20itself%20to%20automatically%20generate%20rationales%0Aon%20existing%20datasets.%20Based%20on%20the%20inherent%20rank%20from%20correctness%20across%0Amultiple%20rationales%2C%20the%20model%20learns%20to%20generate%20better%20rationales%2C%20leading%20to%0Ahigher%20reasoning%20capability.%20Specifically%2C%20we%20train%20a%20reward%20model%20with%20the%0Arank%20to%20evaluate%20the%20quality%20of%20rationales%2C%20and%20improve%20the%20performance%20of%0Areasoning%20through%20reinforcement%20learning.%20Experiment%20results%20of%20Llama2%207B%20on%0Amultiple%20reasoning%20datasets%20show%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%20models%2C%20even%20outperforming%20text-davinci-002%20in%20some%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07017v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Language%20Model%20Reasoning%20with%20Self-motivated%20Learning&entry.906535625=Yunlong%20Feng%20and%20Yang%20Xu%20and%20Libo%20Qin%20and%20Yasheng%20Wang%20and%20Wanxiang%20Che&entry.1292438233=%20%20Large-scale%20high-quality%20training%20data%20is%20important%20for%20improving%20the%0Aperformance%20of%20models.%20After%20trained%20with%20data%20that%20has%20rationales%20%28reasoning%0Asteps%29%2C%20models%20gain%20reasoning%20capability.%20However%2C%20the%20dataset%20with%0Ahigh-quality%20rationales%20is%20relatively%20scarce%20due%20to%20the%20high%20annotation%20cost.%0ATo%20address%20this%20issue%2C%20we%20propose%20%5Ctextit%7BSelf-motivated%20Learning%7D%20framework.%0AThe%20framework%20motivates%20the%20model%20itself%20to%20automatically%20generate%20rationales%0Aon%20existing%20datasets.%20Based%20on%20the%20inherent%20rank%20from%20correctness%20across%0Amultiple%20rationales%2C%20the%20model%20learns%20to%20generate%20better%20rationales%2C%20leading%20to%0Ahigher%20reasoning%20capability.%20Specifically%2C%20we%20train%20a%20reward%20model%20with%20the%0Arank%20to%20evaluate%20the%20quality%20of%20rationales%2C%20and%20improve%20the%20performance%20of%0Areasoning%20through%20reinforcement%20learning.%20Experiment%20results%20of%20Llama2%207B%20on%0Amultiple%20reasoning%20datasets%20show%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%20models%2C%20even%20outperforming%20text-davinci-002%20in%20some%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07017v1&entry.124074799=Read"},
{"title": "M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via\n  Multiplier Induced Loss Landscape Scheduling", "author": "Xudong Sun and Nutan Chen and Alexej Gossmann and Yu Xing and Carla Feistner and Emilio Dorigatt and Felix Drost and Daniele Scarcella and Lisa Beer and Carsten Marr", "abstract": "  We address the online combinatorial choice of weight multipliers for\nmulti-objective optimization of many loss terms parameterized by neural works\nvia a probabilistic graphical model (PGM) for the joint model parameter and\nmultiplier evolution process, with a hypervolume based likelihood promoting\nmulti-objective descent. The corresponding parameter and multiplier estimation\nas a sequential decision process is then cast into an optimal control problem,\nwhere the multi-objective descent goal is dispatched hierarchically into a\nseries of constraint optimization sub-problems. The subproblem constraint\nautomatically adapts itself according to Pareto dominance and serves as the\nsetpoint for the low level multiplier controller to schedule loss landscapes\nvia output feedback of each loss term. Our method is multiplier-free and\noperates at the timescale of epochs, thus saves tremendous computational\nresources compared to full training cycle multiplier tuning. It also\ncircumvents the excessive memory requirements and heavy computational burden of\nexisting multi-objective deep learning methods. We applied it to domain\ninvariant variational auto-encoding with 6 loss terms on the PACS domain\ngeneralization task, and observed robust performance across a range of\ncontroller hyperparameters, as well as different multiplier initial conditions,\noutperforming other multiplier scheduling methods. We offered modular\nimplementation of our method, admitting extension to custom definition of many\nloss terms.\n", "link": "http://arxiv.org/abs/2403.13728v2", "date": "2024-04-10", "relevancy": 1.9957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5153}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4688}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling&body=Title%3A%20M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling%0AAuthor%3A%20Xudong%20Sun%20and%20Nutan%20Chen%20and%20Alexej%20Gossmann%20and%20Yu%20Xing%20and%20Carla%20Feistner%20and%20Emilio%20Dorigatt%20and%20Felix%20Drost%20and%20Daniele%20Scarcella%20and%20Lisa%20Beer%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20We%20address%20the%20online%20combinatorial%20choice%20of%20weight%20multipliers%20for%0Amulti-objective%20optimization%20of%20many%20loss%20terms%20parameterized%20by%20neural%20works%0Avia%20a%20probabilistic%20graphical%20model%20%28PGM%29%20for%20the%20joint%20model%20parameter%20and%0Amultiplier%20evolution%20process%2C%20with%20a%20hypervolume%20based%20likelihood%20promoting%0Amulti-objective%20descent.%20The%20corresponding%20parameter%20and%20multiplier%20estimation%0Aas%20a%20sequential%20decision%20process%20is%20then%20cast%20into%20an%20optimal%20control%20problem%2C%0Awhere%20the%20multi-objective%20descent%20goal%20is%20dispatched%20hierarchically%20into%20a%0Aseries%20of%20constraint%20optimization%20sub-problems.%20The%20subproblem%20constraint%0Aautomatically%20adapts%20itself%20according%20to%20Pareto%20dominance%20and%20serves%20as%20the%0Asetpoint%20for%20the%20low%20level%20multiplier%20controller%20to%20schedule%20loss%20landscapes%0Avia%20output%20feedback%20of%20each%20loss%20term.%20Our%20method%20is%20multiplier-free%20and%0Aoperates%20at%20the%20timescale%20of%20epochs%2C%20thus%20saves%20tremendous%20computational%0Aresources%20compared%20to%20full%20training%20cycle%20multiplier%20tuning.%20It%20also%0Acircumvents%20the%20excessive%20memory%20requirements%20and%20heavy%20computational%20burden%20of%0Aexisting%20multi-objective%20deep%20learning%20methods.%20We%20applied%20it%20to%20domain%0Ainvariant%20variational%20auto-encoding%20with%206%20loss%20terms%20on%20the%20PACS%20domain%0Ageneralization%20task%2C%20and%20observed%20robust%20performance%20across%20a%20range%20of%0Acontroller%20hyperparameters%2C%20as%20well%20as%20different%20multiplier%20initial%20conditions%2C%0Aoutperforming%20other%20multiplier%20scheduling%20methods.%20We%20offered%20modular%0Aimplementation%20of%20our%20method%2C%20admitting%20extension%20to%20custom%20definition%20of%20many%0Aloss%20terms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13728v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-HOF-Opt%3A%20Multi-Objective%20Hierarchical%20Output%20Feedback%20Optimization%20via%0A%20%20Multiplier%20Induced%20Loss%20Landscape%20Scheduling&entry.906535625=Xudong%20Sun%20and%20Nutan%20Chen%20and%20Alexej%20Gossmann%20and%20Yu%20Xing%20and%20Carla%20Feistner%20and%20Emilio%20Dorigatt%20and%20Felix%20Drost%20and%20Daniele%20Scarcella%20and%20Lisa%20Beer%20and%20Carsten%20Marr&entry.1292438233=%20%20We%20address%20the%20online%20combinatorial%20choice%20of%20weight%20multipliers%20for%0Amulti-objective%20optimization%20of%20many%20loss%20terms%20parameterized%20by%20neural%20works%0Avia%20a%20probabilistic%20graphical%20model%20%28PGM%29%20for%20the%20joint%20model%20parameter%20and%0Amultiplier%20evolution%20process%2C%20with%20a%20hypervolume%20based%20likelihood%20promoting%0Amulti-objective%20descent.%20The%20corresponding%20parameter%20and%20multiplier%20estimation%0Aas%20a%20sequential%20decision%20process%20is%20then%20cast%20into%20an%20optimal%20control%20problem%2C%0Awhere%20the%20multi-objective%20descent%20goal%20is%20dispatched%20hierarchically%20into%20a%0Aseries%20of%20constraint%20optimization%20sub-problems.%20The%20subproblem%20constraint%0Aautomatically%20adapts%20itself%20according%20to%20Pareto%20dominance%20and%20serves%20as%20the%0Asetpoint%20for%20the%20low%20level%20multiplier%20controller%20to%20schedule%20loss%20landscapes%0Avia%20output%20feedback%20of%20each%20loss%20term.%20Our%20method%20is%20multiplier-free%20and%0Aoperates%20at%20the%20timescale%20of%20epochs%2C%20thus%20saves%20tremendous%20computational%0Aresources%20compared%20to%20full%20training%20cycle%20multiplier%20tuning.%20It%20also%0Acircumvents%20the%20excessive%20memory%20requirements%20and%20heavy%20computational%20burden%20of%0Aexisting%20multi-objective%20deep%20learning%20methods.%20We%20applied%20it%20to%20domain%0Ainvariant%20variational%20auto-encoding%20with%206%20loss%20terms%20on%20the%20PACS%20domain%0Ageneralization%20task%2C%20and%20observed%20robust%20performance%20across%20a%20range%20of%0Acontroller%20hyperparameters%2C%20as%20well%20as%20different%20multiplier%20initial%20conditions%2C%0Aoutperforming%20other%20multiplier%20scheduling%20methods.%20We%20offered%20modular%0Aimplementation%20of%20our%20method%2C%20admitting%20extension%20to%20custom%20definition%20of%20many%0Aloss%20terms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13728v2&entry.124074799=Read"},
{"title": "Enhancing Efficiency in Multidevice Federated Learning through Data\n  Selection", "author": "Fan Mo and Mohammad Malekzadeh and Soumyajit Chatterjee and Fahim Kawsar and Akhil Mathur", "abstract": "  Federated learning (FL) in multidevice environments creates new opportunities\nto learn from a vast and diverse amount of private data. Although personal\ndevices capture valuable data, their memory, computing, connectivity, and\nbattery resources are often limited. Since deep neural networks (DNNs) are the\ntypical machine learning models employed in FL, there are demands for\nintegrating ubiquitous constrained devices into the training process of DNNs.\nIn this paper, we develop an FL framework to incorporate on-device data\nselection on such constrained devices, which allows partition-based training of\na DNN through collaboration between constrained devices and resourceful devices\nof the same client. Evaluations on five benchmark DNNs and six benchmark\ndatasets across different modalities show that, on average, our framework\nachieves ~19% higher accuracy and ~58% lower latency; compared to the baseline\nFL without our implemented strategies. We demonstrate the effectiveness of our\nFL framework when dealing with imbalanced data, client participation\nheterogeneity, and various mobility patterns. As a benchmark for the community,\nour code is available at\nhttps://github.com/dr-bell/data-centric-federated-learning\n", "link": "http://arxiv.org/abs/2211.04175v4", "date": "2024-04-10", "relevancy": 1.9896, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4938}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4845}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection&body=Title%3A%20Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection%0AAuthor%3A%20Fan%20Mo%20and%20Mohammad%20Malekzadeh%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Akhil%20Mathur%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20in%20multidevice%20environments%20creates%20new%20opportunities%0Ato%20learn%20from%20a%20vast%20and%20diverse%20amount%20of%20private%20data.%20Although%20personal%0Adevices%20capture%20valuable%20data%2C%20their%20memory%2C%20computing%2C%20connectivity%2C%20and%0Abattery%20resources%20are%20often%20limited.%20Since%20deep%20neural%20networks%20%28DNNs%29%20are%20the%0Atypical%20machine%20learning%20models%20employed%20in%20FL%2C%20there%20are%20demands%20for%0Aintegrating%20ubiquitous%20constrained%20devices%20into%20the%20training%20process%20of%20DNNs.%0AIn%20this%20paper%2C%20we%20develop%20an%20FL%20framework%20to%20incorporate%20on-device%20data%0Aselection%20on%20such%20constrained%20devices%2C%20which%20allows%20partition-based%20training%20of%0Aa%20DNN%20through%20collaboration%20between%20constrained%20devices%20and%20resourceful%20devices%0Aof%20the%20same%20client.%20Evaluations%20on%20five%20benchmark%20DNNs%20and%20six%20benchmark%0Adatasets%20across%20different%20modalities%20show%20that%2C%20on%20average%2C%20our%20framework%0Aachieves%20~19%25%20higher%20accuracy%20and%20~58%25%20lower%20latency%3B%20compared%20to%20the%20baseline%0AFL%20without%20our%20implemented%20strategies.%20We%20demonstrate%20the%20effectiveness%20of%20our%0AFL%20framework%20when%20dealing%20with%20imbalanced%20data%2C%20client%20participation%0Aheterogeneity%2C%20and%20various%20mobility%20patterns.%20As%20a%20benchmark%20for%20the%20community%2C%0Aour%20code%20is%20available%20at%0Ahttps%3A//github.com/dr-bell/data-centric-federated-learning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.04175v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection&entry.906535625=Fan%20Mo%20and%20Mohammad%20Malekzadeh%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Akhil%20Mathur&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20in%20multidevice%20environments%20creates%20new%20opportunities%0Ato%20learn%20from%20a%20vast%20and%20diverse%20amount%20of%20private%20data.%20Although%20personal%0Adevices%20capture%20valuable%20data%2C%20their%20memory%2C%20computing%2C%20connectivity%2C%20and%0Abattery%20resources%20are%20often%20limited.%20Since%20deep%20neural%20networks%20%28DNNs%29%20are%20the%0Atypical%20machine%20learning%20models%20employed%20in%20FL%2C%20there%20are%20demands%20for%0Aintegrating%20ubiquitous%20constrained%20devices%20into%20the%20training%20process%20of%20DNNs.%0AIn%20this%20paper%2C%20we%20develop%20an%20FL%20framework%20to%20incorporate%20on-device%20data%0Aselection%20on%20such%20constrained%20devices%2C%20which%20allows%20partition-based%20training%20of%0Aa%20DNN%20through%20collaboration%20between%20constrained%20devices%20and%20resourceful%20devices%0Aof%20the%20same%20client.%20Evaluations%20on%20five%20benchmark%20DNNs%20and%20six%20benchmark%0Adatasets%20across%20different%20modalities%20show%20that%2C%20on%20average%2C%20our%20framework%0Aachieves%20~19%25%20higher%20accuracy%20and%20~58%25%20lower%20latency%3B%20compared%20to%20the%20baseline%0AFL%20without%20our%20implemented%20strategies.%20We%20demonstrate%20the%20effectiveness%20of%20our%0AFL%20framework%20when%20dealing%20with%20imbalanced%20data%2C%20client%20participation%0Aheterogeneity%2C%20and%20various%20mobility%20patterns.%20As%20a%20benchmark%20for%20the%20community%2C%0Aour%20code%20is%20available%20at%0Ahttps%3A//github.com/dr-bell/data-centric-federated-learning%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.04175v4&entry.124074799=Read"},
{"title": "FiP: a Fixed-Point Approach for Causal Generative Modeling", "author": "Meyer Scetbon and Joel Jennings and Agrin Hilmkil and Cheng Zhang and Chao Ma", "abstract": "  Modeling true world data-generating processes lies at the heart of empirical\nscience. Structural Causal Models (SCMs) and their associated Directed Acyclic\nGraphs (DAGs) provide an increasingly popular answer to such problems by\ndefining the causal generative process that transforms random noise into\nobservations. However, learning them from observational data poses an ill-posed\nand NP-hard inverse problem in general. In this work, we propose a new and\nequivalent formalism that do not require DAGs to describe them, viewed as\nfixed-point problems on the causally ordered variables, and show three\nimportant cases where they can be uniquely recovered given the topological\nordering (TO). To the best of our knowledge, we obtain the most general\nrecovery results when the TO is known. Based on our theoretical findings, we\ndesign a two-stage causal generative model that first infers the causal order\nfrom observations in a zero-shot manner, thus by-passing the search, and then\nlearns the generative fixed-point SCM on the ordered variables. To infer TOs\nfrom observations, we propose to amortize the learning of TOs on generated\ndatasets by sequentially predicting the leaves of graphs seen during training.\nTo learn fixed-point SCMs, we design a transformer-based architecture that\nexploits a new attention mechanism enabling the modeling of causal structures,\nand show that this parameterization is consistent with our formalism. Finally,\nwe conduct an extensive evaluation of each method individually, and show that\nwhen combined, our model outperforms various baselines on generated\nout-of-distribution problems.\n", "link": "http://arxiv.org/abs/2404.06969v1", "date": "2024-04-10", "relevancy": 1.9887, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5101}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4743}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FiP%3A%20a%20Fixed-Point%20Approach%20for%20Causal%20Generative%20Modeling&body=Title%3A%20FiP%3A%20a%20Fixed-Point%20Approach%20for%20Causal%20Generative%20Modeling%0AAuthor%3A%20Meyer%20Scetbon%20and%20Joel%20Jennings%20and%20Agrin%20Hilmkil%20and%20Cheng%20Zhang%20and%20Chao%20Ma%0AAbstract%3A%20%20%20Modeling%20true%20world%20data-generating%20processes%20lies%20at%20the%20heart%20of%20empirical%0Ascience.%20Structural%20Causal%20Models%20%28SCMs%29%20and%20their%20associated%20Directed%20Acyclic%0AGraphs%20%28DAGs%29%20provide%20an%20increasingly%20popular%20answer%20to%20such%20problems%20by%0Adefining%20the%20causal%20generative%20process%20that%20transforms%20random%20noise%20into%0Aobservations.%20However%2C%20learning%20them%20from%20observational%20data%20poses%20an%20ill-posed%0Aand%20NP-hard%20inverse%20problem%20in%20general.%20In%20this%20work%2C%20we%20propose%20a%20new%20and%0Aequivalent%20formalism%20that%20do%20not%20require%20DAGs%20to%20describe%20them%2C%20viewed%20as%0Afixed-point%20problems%20on%20the%20causally%20ordered%20variables%2C%20and%20show%20three%0Aimportant%20cases%20where%20they%20can%20be%20uniquely%20recovered%20given%20the%20topological%0Aordering%20%28TO%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20obtain%20the%20most%20general%0Arecovery%20results%20when%20the%20TO%20is%20known.%20Based%20on%20our%20theoretical%20findings%2C%20we%0Adesign%20a%20two-stage%20causal%20generative%20model%20that%20first%20infers%20the%20causal%20order%0Afrom%20observations%20in%20a%20zero-shot%20manner%2C%20thus%20by-passing%20the%20search%2C%20and%20then%0Alearns%20the%20generative%20fixed-point%20SCM%20on%20the%20ordered%20variables.%20To%20infer%20TOs%0Afrom%20observations%2C%20we%20propose%20to%20amortize%20the%20learning%20of%20TOs%20on%20generated%0Adatasets%20by%20sequentially%20predicting%20the%20leaves%20of%20graphs%20seen%20during%20training.%0ATo%20learn%20fixed-point%20SCMs%2C%20we%20design%20a%20transformer-based%20architecture%20that%0Aexploits%20a%20new%20attention%20mechanism%20enabling%20the%20modeling%20of%20causal%20structures%2C%0Aand%20show%20that%20this%20parameterization%20is%20consistent%20with%20our%20formalism.%20Finally%2C%0Awe%20conduct%20an%20extensive%20evaluation%20of%20each%20method%20individually%2C%20and%20show%20that%0Awhen%20combined%2C%20our%20model%20outperforms%20various%20baselines%20on%20generated%0Aout-of-distribution%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06969v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FiP%3A%20a%20Fixed-Point%20Approach%20for%20Causal%20Generative%20Modeling&entry.906535625=Meyer%20Scetbon%20and%20Joel%20Jennings%20and%20Agrin%20Hilmkil%20and%20Cheng%20Zhang%20and%20Chao%20Ma&entry.1292438233=%20%20Modeling%20true%20world%20data-generating%20processes%20lies%20at%20the%20heart%20of%20empirical%0Ascience.%20Structural%20Causal%20Models%20%28SCMs%29%20and%20their%20associated%20Directed%20Acyclic%0AGraphs%20%28DAGs%29%20provide%20an%20increasingly%20popular%20answer%20to%20such%20problems%20by%0Adefining%20the%20causal%20generative%20process%20that%20transforms%20random%20noise%20into%0Aobservations.%20However%2C%20learning%20them%20from%20observational%20data%20poses%20an%20ill-posed%0Aand%20NP-hard%20inverse%20problem%20in%20general.%20In%20this%20work%2C%20we%20propose%20a%20new%20and%0Aequivalent%20formalism%20that%20do%20not%20require%20DAGs%20to%20describe%20them%2C%20viewed%20as%0Afixed-point%20problems%20on%20the%20causally%20ordered%20variables%2C%20and%20show%20three%0Aimportant%20cases%20where%20they%20can%20be%20uniquely%20recovered%20given%20the%20topological%0Aordering%20%28TO%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20obtain%20the%20most%20general%0Arecovery%20results%20when%20the%20TO%20is%20known.%20Based%20on%20our%20theoretical%20findings%2C%20we%0Adesign%20a%20two-stage%20causal%20generative%20model%20that%20first%20infers%20the%20causal%20order%0Afrom%20observations%20in%20a%20zero-shot%20manner%2C%20thus%20by-passing%20the%20search%2C%20and%20then%0Alearns%20the%20generative%20fixed-point%20SCM%20on%20the%20ordered%20variables.%20To%20infer%20TOs%0Afrom%20observations%2C%20we%20propose%20to%20amortize%20the%20learning%20of%20TOs%20on%20generated%0Adatasets%20by%20sequentially%20predicting%20the%20leaves%20of%20graphs%20seen%20during%20training.%0ATo%20learn%20fixed-point%20SCMs%2C%20we%20design%20a%20transformer-based%20architecture%20that%0Aexploits%20a%20new%20attention%20mechanism%20enabling%20the%20modeling%20of%20causal%20structures%2C%0Aand%20show%20that%20this%20parameterization%20is%20consistent%20with%20our%20formalism.%20Finally%2C%0Awe%20conduct%20an%20extensive%20evaluation%20of%20each%20method%20individually%2C%20and%20show%20that%0Awhen%20combined%2C%20our%20model%20outperforms%20various%20baselines%20on%20generated%0Aout-of-distribution%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06969v1&entry.124074799=Read"},
{"title": "A Mathematical Theory for Learning Semantic Languages by Abstract\n  Learners", "author": "Kuo-Yu Liao and Cheng-Shang Chang and Y. -W. Peter Hong", "abstract": "  Recent advances in Large Language Models (LLMs) have demonstrated the\nemergence of capabilities (learned skills) when the number of system parameters\nand the size of training data surpass certain thresholds. The exact mechanisms\nbehind such phenomena are not fully understood and remain a topic of active\nresearch. Inspired by the skill-text bipartite graph model presented in [1] for\nmodeling semantic language, we develop a mathematical theory to explain the\nemergence of learned skills, taking the learning (or training) process into\naccount. Our approach models the learning process for skills in the skill-text\nbipartite graph as an iterative decoding process in Low-Density Parity Check\n(LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density\nevolution analysis, we demonstrate the emergence of learned skills when the\nratio of the size of training texts to the number of skills exceeds a certain\nthreshold. Our analysis also yields a scaling law for testing errors relative\nto the size of training texts. Upon completion of the training, we propose a\nmethod for semantic compression and discuss its application in semantic\ncommunication.\n", "link": "http://arxiv.org/abs/2404.07009v1", "date": "2024-04-10", "relevancy": 1.9811, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Mathematical%20Theory%20for%20Learning%20Semantic%20Languages%20by%20Abstract%0A%20%20Learners&body=Title%3A%20A%20Mathematical%20Theory%20for%20Learning%20Semantic%20Languages%20by%20Abstract%0A%20%20Learners%0AAuthor%3A%20Kuo-Yu%20Liao%20and%20Cheng-Shang%20Chang%20and%20Y.%20-W.%20Peter%20Hong%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20the%0Aemergence%20of%20capabilities%20%28learned%20skills%29%20when%20the%20number%20of%20system%20parameters%0Aand%20the%20size%20of%20training%20data%20surpass%20certain%20thresholds.%20The%20exact%20mechanisms%0Abehind%20such%20phenomena%20are%20not%20fully%20understood%20and%20remain%20a%20topic%20of%20active%0Aresearch.%20Inspired%20by%20the%20skill-text%20bipartite%20graph%20model%20presented%20in%20%5B1%5D%20for%0Amodeling%20semantic%20language%2C%20we%20develop%20a%20mathematical%20theory%20to%20explain%20the%0Aemergence%20of%20learned%20skills%2C%20taking%20the%20learning%20%28or%20training%29%20process%20into%0Aaccount.%20Our%20approach%20models%20the%20learning%20process%20for%20skills%20in%20the%20skill-text%0Abipartite%20graph%20as%20an%20iterative%20decoding%20process%20in%20Low-Density%20Parity%20Check%0A%28LDPC%29%20codes%20and%20Irregular%20Repetition%20Slotted%20ALOHA%20%28IRSA%29.%20Using%20density%0Aevolution%20analysis%2C%20we%20demonstrate%20the%20emergence%20of%20learned%20skills%20when%20the%0Aratio%20of%20the%20size%20of%20training%20texts%20to%20the%20number%20of%20skills%20exceeds%20a%20certain%0Athreshold.%20Our%20analysis%20also%20yields%20a%20scaling%20law%20for%20testing%20errors%20relative%0Ato%20the%20size%20of%20training%20texts.%20Upon%20completion%20of%20the%20training%2C%20we%20propose%20a%0Amethod%20for%20semantic%20compression%20and%20discuss%20its%20application%20in%20semantic%0Acommunication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07009v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mathematical%20Theory%20for%20Learning%20Semantic%20Languages%20by%20Abstract%0A%20%20Learners&entry.906535625=Kuo-Yu%20Liao%20and%20Cheng-Shang%20Chang%20and%20Y.%20-W.%20Peter%20Hong&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20the%0Aemergence%20of%20capabilities%20%28learned%20skills%29%20when%20the%20number%20of%20system%20parameters%0Aand%20the%20size%20of%20training%20data%20surpass%20certain%20thresholds.%20The%20exact%20mechanisms%0Abehind%20such%20phenomena%20are%20not%20fully%20understood%20and%20remain%20a%20topic%20of%20active%0Aresearch.%20Inspired%20by%20the%20skill-text%20bipartite%20graph%20model%20presented%20in%20%5B1%5D%20for%0Amodeling%20semantic%20language%2C%20we%20develop%20a%20mathematical%20theory%20to%20explain%20the%0Aemergence%20of%20learned%20skills%2C%20taking%20the%20learning%20%28or%20training%29%20process%20into%0Aaccount.%20Our%20approach%20models%20the%20learning%20process%20for%20skills%20in%20the%20skill-text%0Abipartite%20graph%20as%20an%20iterative%20decoding%20process%20in%20Low-Density%20Parity%20Check%0A%28LDPC%29%20codes%20and%20Irregular%20Repetition%20Slotted%20ALOHA%20%28IRSA%29.%20Using%20density%0Aevolution%20analysis%2C%20we%20demonstrate%20the%20emergence%20of%20learned%20skills%20when%20the%0Aratio%20of%20the%20size%20of%20training%20texts%20to%20the%20number%20of%20skills%20exceeds%20a%20certain%0Athreshold.%20Our%20analysis%20also%20yields%20a%20scaling%20law%20for%20testing%20errors%20relative%0Ato%20the%20size%20of%20training%20texts.%20Upon%20completion%20of%20the%20training%2C%20we%20propose%20a%0Amethod%20for%20semantic%20compression%20and%20discuss%20its%20application%20in%20semantic%0Acommunication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07009v1&entry.124074799=Read"},
{"title": "GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM\n  Applications", "author": "Shishir G. Patil and Tianjun Zhang and Vivian Fang and Noppapon C. and Roy Huang and Aaron Hao and Martin Casado and Joseph E. Gonzalez and Raluca Ada Popa and Ion Stoica", "abstract": "  Large Language Models (LLMs) are evolving beyond their classical role of\nproviding information within dialogue systems to actively engaging with tools\nand performing actions on real-world applications and services. Today, humans\nverify the correctness and appropriateness of the LLM-generated outputs (e.g.,\ncode, functions, or actions) before putting them into real-world execution.\nThis poses significant challenges as code comprehension is well known to be\nnotoriously difficult. In this paper, we study how humans can efficiently\ncollaborate with, delegate to, and supervise autonomous LLMs in the future. We\nargue that in many cases, \"post-facto validation\" - verifying the correctness\nof a proposed action after seeing the output - is much easier than the\naforementioned \"pre-facto validation\" setting. The core concept behind enabling\na post-facto validation system is the integration of an intuitive undo feature,\nand establishing a damage confinement for the LLM-generated actions as\neffective strategies to mitigate the associated risks. Using this, a human can\nnow either revert the effect of an LLM-generated output or be confident that\nthe potential risk is bounded. We believe this is critical to unlock the\npotential for LLM agents to interact with applications and services with\nlimited (post-facto) human involvement. We describe the design and\nimplementation of our open-source runtime for executing LLM actions, Gorilla\nExecution Engine (GoEX), and present open research questions towards realizing\nthe goal of LLMs and applications interacting with each other with minimal\nhuman supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.\n", "link": "http://arxiv.org/abs/2404.06921v1", "date": "2024-04-10", "relevancy": 1.9696, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5612}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4789}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4783}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GoEX%3A%20Perspectives%20and%20Designs%20Towards%20a%20Runtime%20for%20Autonomous%20LLM%0A%20%20Applications&body=Title%3A%20GoEX%3A%20Perspectives%20and%20Designs%20Towards%20a%20Runtime%20for%20Autonomous%20LLM%0A%20%20Applications%0AAuthor%3A%20Shishir%20G.%20Patil%20and%20Tianjun%20Zhang%20and%20Vivian%20Fang%20and%20Noppapon%20C.%20and%20Roy%20Huang%20and%20Aaron%20Hao%20and%20Martin%20Casado%20and%20Joseph%20E.%20Gonzalez%20and%20Raluca%20Ada%20Popa%20and%20Ion%20Stoica%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20evolving%20beyond%20their%20classical%20role%20of%0Aproviding%20information%20within%20dialogue%20systems%20to%20actively%20engaging%20with%20tools%0Aand%20performing%20actions%20on%20real-world%20applications%20and%20services.%20Today%2C%20humans%0Averify%20the%20correctness%20and%20appropriateness%20of%20the%20LLM-generated%20outputs%20%28e.g.%2C%0Acode%2C%20functions%2C%20or%20actions%29%20before%20putting%20them%20into%20real-world%20execution.%0AThis%20poses%20significant%20challenges%20as%20code%20comprehension%20is%20well%20known%20to%20be%0Anotoriously%20difficult.%20In%20this%20paper%2C%20we%20study%20how%20humans%20can%20efficiently%0Acollaborate%20with%2C%20delegate%20to%2C%20and%20supervise%20autonomous%20LLMs%20in%20the%20future.%20We%0Aargue%20that%20in%20many%20cases%2C%20%22post-facto%20validation%22%20-%20verifying%20the%20correctness%0Aof%20a%20proposed%20action%20after%20seeing%20the%20output%20-%20is%20much%20easier%20than%20the%0Aaforementioned%20%22pre-facto%20validation%22%20setting.%20The%20core%20concept%20behind%20enabling%0Aa%20post-facto%20validation%20system%20is%20the%20integration%20of%20an%20intuitive%20undo%20feature%2C%0Aand%20establishing%20a%20damage%20confinement%20for%20the%20LLM-generated%20actions%20as%0Aeffective%20strategies%20to%20mitigate%20the%20associated%20risks.%20Using%20this%2C%20a%20human%20can%0Anow%20either%20revert%20the%20effect%20of%20an%20LLM-generated%20output%20or%20be%20confident%20that%0Athe%20potential%20risk%20is%20bounded.%20We%20believe%20this%20is%20critical%20to%20unlock%20the%0Apotential%20for%20LLM%20agents%20to%20interact%20with%20applications%20and%20services%20with%0Alimited%20%28post-facto%29%20human%20involvement.%20We%20describe%20the%20design%20and%0Aimplementation%20of%20our%20open-source%20runtime%20for%20executing%20LLM%20actions%2C%20Gorilla%0AExecution%20Engine%20%28GoEX%29%2C%20and%20present%20open%20research%20questions%20towards%20realizing%0Athe%20goal%20of%20LLMs%20and%20applications%20interacting%20with%20each%20other%20with%20minimal%0Ahuman%20supervision.%20We%20release%20GoEX%20at%20https%3A//github.com/ShishirPatil/gorilla/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06921v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoEX%3A%20Perspectives%20and%20Designs%20Towards%20a%20Runtime%20for%20Autonomous%20LLM%0A%20%20Applications&entry.906535625=Shishir%20G.%20Patil%20and%20Tianjun%20Zhang%20and%20Vivian%20Fang%20and%20Noppapon%20C.%20and%20Roy%20Huang%20and%20Aaron%20Hao%20and%20Martin%20Casado%20and%20Joseph%20E.%20Gonzalez%20and%20Raluca%20Ada%20Popa%20and%20Ion%20Stoica&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20evolving%20beyond%20their%20classical%20role%20of%0Aproviding%20information%20within%20dialogue%20systems%20to%20actively%20engaging%20with%20tools%0Aand%20performing%20actions%20on%20real-world%20applications%20and%20services.%20Today%2C%20humans%0Averify%20the%20correctness%20and%20appropriateness%20of%20the%20LLM-generated%20outputs%20%28e.g.%2C%0Acode%2C%20functions%2C%20or%20actions%29%20before%20putting%20them%20into%20real-world%20execution.%0AThis%20poses%20significant%20challenges%20as%20code%20comprehension%20is%20well%20known%20to%20be%0Anotoriously%20difficult.%20In%20this%20paper%2C%20we%20study%20how%20humans%20can%20efficiently%0Acollaborate%20with%2C%20delegate%20to%2C%20and%20supervise%20autonomous%20LLMs%20in%20the%20future.%20We%0Aargue%20that%20in%20many%20cases%2C%20%22post-facto%20validation%22%20-%20verifying%20the%20correctness%0Aof%20a%20proposed%20action%20after%20seeing%20the%20output%20-%20is%20much%20easier%20than%20the%0Aaforementioned%20%22pre-facto%20validation%22%20setting.%20The%20core%20concept%20behind%20enabling%0Aa%20post-facto%20validation%20system%20is%20the%20integration%20of%20an%20intuitive%20undo%20feature%2C%0Aand%20establishing%20a%20damage%20confinement%20for%20the%20LLM-generated%20actions%20as%0Aeffective%20strategies%20to%20mitigate%20the%20associated%20risks.%20Using%20this%2C%20a%20human%20can%0Anow%20either%20revert%20the%20effect%20of%20an%20LLM-generated%20output%20or%20be%20confident%20that%0Athe%20potential%20risk%20is%20bounded.%20We%20believe%20this%20is%20critical%20to%20unlock%20the%0Apotential%20for%20LLM%20agents%20to%20interact%20with%20applications%20and%20services%20with%0Alimited%20%28post-facto%29%20human%20involvement.%20We%20describe%20the%20design%20and%0Aimplementation%20of%20our%20open-source%20runtime%20for%20executing%20LLM%20actions%2C%20Gorilla%0AExecution%20Engine%20%28GoEX%29%2C%20and%20present%20open%20research%20questions%20towards%20realizing%0Athe%20goal%20of%20LLMs%20and%20applications%20interacting%20with%20each%20other%20with%20minimal%0Ahuman%20supervision.%20We%20release%20GoEX%20at%20https%3A//github.com/ShishirPatil/gorilla/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06921v1&entry.124074799=Read"},
{"title": "Leave No Context Behind: Efficient Infinite Context Transformers with\n  Infini-attention", "author": "Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal", "abstract": "  This work introduces an efficient method to scale Transformer-based Large\nLanguage Models (LLMs) to infinitely long inputs with bounded memory and\ncomputation. A key component in our proposed approach is a new attention\ntechnique dubbed Infini-attention. The Infini-attention incorporates a\ncompressive memory into the vanilla attention mechanism and builds in both\nmasked local attention and long-term linear attention mechanisms in a single\nTransformer block. We demonstrate the effectiveness of our approach on\nlong-context language modeling benchmarks, 1M sequence length passkey context\nblock retrieval and 500K length book summarization tasks with 1B and 8B LLMs.\nOur approach introduces minimal bounded memory parameters and enables fast\nstreaming inference for LLMs.\n", "link": "http://arxiv.org/abs/2404.07143v1", "date": "2024-04-10", "relevancy": 1.9585, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leave%20No%20Context%20Behind%3A%20Efficient%20Infinite%20Context%20Transformers%20with%0A%20%20Infini-attention&body=Title%3A%20Leave%20No%20Context%20Behind%3A%20Efficient%20Infinite%20Context%20Transformers%20with%0A%20%20Infini-attention%0AAuthor%3A%20Tsendsuren%20Munkhdalai%20and%20Manaal%20Faruqui%20and%20Siddharth%20Gopal%0AAbstract%3A%20%20%20This%20work%20introduces%20an%20efficient%20method%20to%20scale%20Transformer-based%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20infinitely%20long%20inputs%20with%20bounded%20memory%20and%0Acomputation.%20A%20key%20component%20in%20our%20proposed%20approach%20is%20a%20new%20attention%0Atechnique%20dubbed%20Infini-attention.%20The%20Infini-attention%20incorporates%20a%0Acompressive%20memory%20into%20the%20vanilla%20attention%20mechanism%20and%20builds%20in%20both%0Amasked%20local%20attention%20and%20long-term%20linear%20attention%20mechanisms%20in%20a%20single%0ATransformer%20block.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Along-context%20language%20modeling%20benchmarks%2C%201M%20sequence%20length%20passkey%20context%0Ablock%20retrieval%20and%20500K%20length%20book%20summarization%20tasks%20with%201B%20and%208B%20LLMs.%0AOur%20approach%20introduces%20minimal%20bounded%20memory%20parameters%20and%20enables%20fast%0Astreaming%20inference%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07143v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leave%20No%20Context%20Behind%3A%20Efficient%20Infinite%20Context%20Transformers%20with%0A%20%20Infini-attention&entry.906535625=Tsendsuren%20Munkhdalai%20and%20Manaal%20Faruqui%20and%20Siddharth%20Gopal&entry.1292438233=%20%20This%20work%20introduces%20an%20efficient%20method%20to%20scale%20Transformer-based%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20infinitely%20long%20inputs%20with%20bounded%20memory%20and%0Acomputation.%20A%20key%20component%20in%20our%20proposed%20approach%20is%20a%20new%20attention%0Atechnique%20dubbed%20Infini-attention.%20The%20Infini-attention%20incorporates%20a%0Acompressive%20memory%20into%20the%20vanilla%20attention%20mechanism%20and%20builds%20in%20both%0Amasked%20local%20attention%20and%20long-term%20linear%20attention%20mechanisms%20in%20a%20single%0ATransformer%20block.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Along-context%20language%20modeling%20benchmarks%2C%201M%20sequence%20length%20passkey%20context%0Ablock%20retrieval%20and%20500K%20length%20book%20summarization%20tasks%20with%201B%20and%208B%20LLMs.%0AOur%20approach%20introduces%20minimal%20bounded%20memory%20parameters%20and%20enables%20fast%0Astreaming%20inference%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07143v1&entry.124074799=Read"},
{"title": "L2MAC: Large Language Model Automatic Computer for Extensive Code\n  Generation", "author": "Samuel Holt and Max Ruiz Luyten and Mihaela van der Schaar", "abstract": "  Transformer-based large language models (LLMs) are constrained by the fixed\ncontext window of the underlying transformer architecture, hindering their\nability to produce long and coherent outputs. Memory-augmented LLMs are a\npromising solution, but current approaches cannot handle long output generation\ntasks since they (1) only focus on reading memory and reduce its evolution to\nthe concatenation of new memories or (2) use very specialized memories that\ncannot adapt to other domains. This paper presents L2MAC, the first practical\nLLM-based general-purpose stored-program automatic computer (von Neumann\narchitecture) framework, an LLM-based multi-agent system, for long and\nconsistent output generation. Its memory has two components: the instruction\nregistry, which is populated with a prompt program to solve the user-given\ntask, and a file store, which will contain the final and intermediate outputs.\nEach instruction in turn is executed by a separate LLM agent, whose context is\nmanaged by a control unit capable of precise memory reading and writing to\nensure effective interaction with the file store. These components enable L2MAC\nto generate extensive outputs, bypassing the constraints of the finite context\nwindow while producing outputs that fulfill a complex user-specified task. We\nempirically demonstrate that L2MAC achieves state-of-the-art performance in\ngenerating large codebases for system design tasks, significantly outperforming\nother coding methods in implementing the detailed user-specified task; we show\nthat L2MAC works for general-purpose extensive text-based tasks, such as\nwriting an entire book; and we provide valuable insights into L2MAC's\nperformance improvement over existing methods.\n", "link": "http://arxiv.org/abs/2310.02003v5", "date": "2024-04-10", "relevancy": 1.9568, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4893}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4869}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20L2MAC%3A%20Large%20Language%20Model%20Automatic%20Computer%20for%20Extensive%20Code%0A%20%20Generation&body=Title%3A%20L2MAC%3A%20Large%20Language%20Model%20Automatic%20Computer%20for%20Extensive%20Code%0A%20%20Generation%0AAuthor%3A%20Samuel%20Holt%20and%20Max%20Ruiz%20Luyten%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20are%20constrained%20by%20the%20fixed%0Acontext%20window%20of%20the%20underlying%20transformer%20architecture%2C%20hindering%20their%0Aability%20to%20produce%20long%20and%20coherent%20outputs.%20Memory-augmented%20LLMs%20are%20a%0Apromising%20solution%2C%20but%20current%20approaches%20cannot%20handle%20long%20output%20generation%0Atasks%20since%20they%20%281%29%20only%20focus%20on%20reading%20memory%20and%20reduce%20its%20evolution%20to%0Athe%20concatenation%20of%20new%20memories%20or%20%282%29%20use%20very%20specialized%20memories%20that%0Acannot%20adapt%20to%20other%20domains.%20This%20paper%20presents%20L2MAC%2C%20the%20first%20practical%0ALLM-based%20general-purpose%20stored-program%20automatic%20computer%20%28von%20Neumann%0Aarchitecture%29%20framework%2C%20an%20LLM-based%20multi-agent%20system%2C%20for%20long%20and%0Aconsistent%20output%20generation.%20Its%20memory%20has%20two%20components%3A%20the%20instruction%0Aregistry%2C%20which%20is%20populated%20with%20a%20prompt%20program%20to%20solve%20the%20user-given%0Atask%2C%20and%20a%20file%20store%2C%20which%20will%20contain%20the%20final%20and%20intermediate%20outputs.%0AEach%20instruction%20in%20turn%20is%20executed%20by%20a%20separate%20LLM%20agent%2C%20whose%20context%20is%0Amanaged%20by%20a%20control%20unit%20capable%20of%20precise%20memory%20reading%20and%20writing%20to%0Aensure%20effective%20interaction%20with%20the%20file%20store.%20These%20components%20enable%20L2MAC%0Ato%20generate%20extensive%20outputs%2C%20bypassing%20the%20constraints%20of%20the%20finite%20context%0Awindow%20while%20producing%20outputs%20that%20fulfill%20a%20complex%20user-specified%20task.%20We%0Aempirically%20demonstrate%20that%20L2MAC%20achieves%20state-of-the-art%20performance%20in%0Agenerating%20large%20codebases%20for%20system%20design%20tasks%2C%20significantly%20outperforming%0Aother%20coding%20methods%20in%20implementing%20the%20detailed%20user-specified%20task%3B%20we%20show%0Athat%20L2MAC%20works%20for%20general-purpose%20extensive%20text-based%20tasks%2C%20such%20as%0Awriting%20an%20entire%20book%3B%20and%20we%20provide%20valuable%20insights%20into%20L2MAC%27s%0Aperformance%20improvement%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02003v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L2MAC%3A%20Large%20Language%20Model%20Automatic%20Computer%20for%20Extensive%20Code%0A%20%20Generation&entry.906535625=Samuel%20Holt%20and%20Max%20Ruiz%20Luyten%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Transformer-based%20large%20language%20models%20%28LLMs%29%20are%20constrained%20by%20the%20fixed%0Acontext%20window%20of%20the%20underlying%20transformer%20architecture%2C%20hindering%20their%0Aability%20to%20produce%20long%20and%20coherent%20outputs.%20Memory-augmented%20LLMs%20are%20a%0Apromising%20solution%2C%20but%20current%20approaches%20cannot%20handle%20long%20output%20generation%0Atasks%20since%20they%20%281%29%20only%20focus%20on%20reading%20memory%20and%20reduce%20its%20evolution%20to%0Athe%20concatenation%20of%20new%20memories%20or%20%282%29%20use%20very%20specialized%20memories%20that%0Acannot%20adapt%20to%20other%20domains.%20This%20paper%20presents%20L2MAC%2C%20the%20first%20practical%0ALLM-based%20general-purpose%20stored-program%20automatic%20computer%20%28von%20Neumann%0Aarchitecture%29%20framework%2C%20an%20LLM-based%20multi-agent%20system%2C%20for%20long%20and%0Aconsistent%20output%20generation.%20Its%20memory%20has%20two%20components%3A%20the%20instruction%0Aregistry%2C%20which%20is%20populated%20with%20a%20prompt%20program%20to%20solve%20the%20user-given%0Atask%2C%20and%20a%20file%20store%2C%20which%20will%20contain%20the%20final%20and%20intermediate%20outputs.%0AEach%20instruction%20in%20turn%20is%20executed%20by%20a%20separate%20LLM%20agent%2C%20whose%20context%20is%0Amanaged%20by%20a%20control%20unit%20capable%20of%20precise%20memory%20reading%20and%20writing%20to%0Aensure%20effective%20interaction%20with%20the%20file%20store.%20These%20components%20enable%20L2MAC%0Ato%20generate%20extensive%20outputs%2C%20bypassing%20the%20constraints%20of%20the%20finite%20context%0Awindow%20while%20producing%20outputs%20that%20fulfill%20a%20complex%20user-specified%20task.%20We%0Aempirically%20demonstrate%20that%20L2MAC%20achieves%20state-of-the-art%20performance%20in%0Agenerating%20large%20codebases%20for%20system%20design%20tasks%2C%20significantly%20outperforming%0Aother%20coding%20methods%20in%20implementing%20the%20detailed%20user-specified%20task%3B%20we%20show%0Athat%20L2MAC%20works%20for%20general-purpose%20extensive%20text-based%20tasks%2C%20such%20as%0Awriting%20an%20entire%20book%3B%20and%20we%20provide%20valuable%20insights%20into%20L2MAC%27s%0Aperformance%20improvement%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02003v5&entry.124074799=Read"},
{"title": "Disentangled Explanations of Neural Network Predictions by Finding\n  Relevant Subspaces", "author": "Pattarawat Chormai and Jan Herrmann and Klaus-Robert M\u00fcller and Gr\u00e9goire Montavon", "abstract": "  Explainable AI aims to overcome the black-box nature of complex ML models\nlike neural networks by generating explanations for their predictions.\nExplanations often take the form of a heatmap identifying input features (e.g.\npixels) that are relevant to the model's decision. These explanations, however,\nentangle the potentially multiple factors that enter into the overall complex\ndecision strategy. We propose to disentangle explanations by extracting at some\nintermediate layer of a neural network, subspaces that capture the multiple and\ndistinct activation patterns (e.g. visual concepts) that are relevant to the\nprediction. To automatically extract these subspaces, we propose two new\nanalyses, extending principles found in PCA or ICA to explanations. These novel\nanalyses, which we call principal relevant component analysis (PRCA) and\ndisentangled relevant subspace analysis (DRSA), maximize relevance instead of\ne.g. variance or kurtosis. This allows for a much stronger focus of the\nanalysis on what the ML model actually uses for predicting, ignoring\nactivations or concepts to which the model is invariant. Our approach is\ngeneral enough to work alongside common attribution techniques such as Shapley\nValue, Integrated Gradients, or LRP. Our proposed methods show to be\npractically useful and compare favorably to the state of the art as\ndemonstrated on benchmarks and three use cases.\n", "link": "http://arxiv.org/abs/2212.14855v2", "date": "2024-04-10", "relevancy": 1.9545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5165}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4872}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.479}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Explanations%20of%20Neural%20Network%20Predictions%20by%20Finding%0A%20%20Relevant%20Subspaces&body=Title%3A%20Disentangled%20Explanations%20of%20Neural%20Network%20Predictions%20by%20Finding%0A%20%20Relevant%20Subspaces%0AAuthor%3A%20Pattarawat%20Chormai%20and%20Jan%20Herrmann%20and%20Klaus-Robert%20M%C3%BCller%20and%20Gr%C3%A9goire%20Montavon%0AAbstract%3A%20%20%20Explainable%20AI%20aims%20to%20overcome%20the%20black-box%20nature%20of%20complex%20ML%20models%0Alike%20neural%20networks%20by%20generating%20explanations%20for%20their%20predictions.%0AExplanations%20often%20take%20the%20form%20of%20a%20heatmap%20identifying%20input%20features%20%28e.g.%0Apixels%29%20that%20are%20relevant%20to%20the%20model%27s%20decision.%20These%20explanations%2C%20however%2C%0Aentangle%20the%20potentially%20multiple%20factors%20that%20enter%20into%20the%20overall%20complex%0Adecision%20strategy.%20We%20propose%20to%20disentangle%20explanations%20by%20extracting%20at%20some%0Aintermediate%20layer%20of%20a%20neural%20network%2C%20subspaces%20that%20capture%20the%20multiple%20and%0Adistinct%20activation%20patterns%20%28e.g.%20visual%20concepts%29%20that%20are%20relevant%20to%20the%0Aprediction.%20To%20automatically%20extract%20these%20subspaces%2C%20we%20propose%20two%20new%0Aanalyses%2C%20extending%20principles%20found%20in%20PCA%20or%20ICA%20to%20explanations.%20These%20novel%0Aanalyses%2C%20which%20we%20call%20principal%20relevant%20component%20analysis%20%28PRCA%29%20and%0Adisentangled%20relevant%20subspace%20analysis%20%28DRSA%29%2C%20maximize%20relevance%20instead%20of%0Ae.g.%20variance%20or%20kurtosis.%20This%20allows%20for%20a%20much%20stronger%20focus%20of%20the%0Aanalysis%20on%20what%20the%20ML%20model%20actually%20uses%20for%20predicting%2C%20ignoring%0Aactivations%20or%20concepts%20to%20which%20the%20model%20is%20invariant.%20Our%20approach%20is%0Ageneral%20enough%20to%20work%20alongside%20common%20attribution%20techniques%20such%20as%20Shapley%0AValue%2C%20Integrated%20Gradients%2C%20or%20LRP.%20Our%20proposed%20methods%20show%20to%20be%0Apractically%20useful%20and%20compare%20favorably%20to%20the%20state%20of%20the%20art%20as%0Ademonstrated%20on%20benchmarks%20and%20three%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.14855v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Explanations%20of%20Neural%20Network%20Predictions%20by%20Finding%0A%20%20Relevant%20Subspaces&entry.906535625=Pattarawat%20Chormai%20and%20Jan%20Herrmann%20and%20Klaus-Robert%20M%C3%BCller%20and%20Gr%C3%A9goire%20Montavon&entry.1292438233=%20%20Explainable%20AI%20aims%20to%20overcome%20the%20black-box%20nature%20of%20complex%20ML%20models%0Alike%20neural%20networks%20by%20generating%20explanations%20for%20their%20predictions.%0AExplanations%20often%20take%20the%20form%20of%20a%20heatmap%20identifying%20input%20features%20%28e.g.%0Apixels%29%20that%20are%20relevant%20to%20the%20model%27s%20decision.%20These%20explanations%2C%20however%2C%0Aentangle%20the%20potentially%20multiple%20factors%20that%20enter%20into%20the%20overall%20complex%0Adecision%20strategy.%20We%20propose%20to%20disentangle%20explanations%20by%20extracting%20at%20some%0Aintermediate%20layer%20of%20a%20neural%20network%2C%20subspaces%20that%20capture%20the%20multiple%20and%0Adistinct%20activation%20patterns%20%28e.g.%20visual%20concepts%29%20that%20are%20relevant%20to%20the%0Aprediction.%20To%20automatically%20extract%20these%20subspaces%2C%20we%20propose%20two%20new%0Aanalyses%2C%20extending%20principles%20found%20in%20PCA%20or%20ICA%20to%20explanations.%20These%20novel%0Aanalyses%2C%20which%20we%20call%20principal%20relevant%20component%20analysis%20%28PRCA%29%20and%0Adisentangled%20relevant%20subspace%20analysis%20%28DRSA%29%2C%20maximize%20relevance%20instead%20of%0Ae.g.%20variance%20or%20kurtosis.%20This%20allows%20for%20a%20much%20stronger%20focus%20of%20the%0Aanalysis%20on%20what%20the%20ML%20model%20actually%20uses%20for%20predicting%2C%20ignoring%0Aactivations%20or%20concepts%20to%20which%20the%20model%20is%20invariant.%20Our%20approach%20is%0Ageneral%20enough%20to%20work%20alongside%20common%20attribution%20techniques%20such%20as%20Shapley%0AValue%2C%20Integrated%20Gradients%2C%20or%20LRP.%20Our%20proposed%20methods%20show%20to%20be%0Apractically%20useful%20and%20compare%20favorably%20to%20the%20state%20of%20the%20art%20as%0Ademonstrated%20on%20benchmarks%20and%20three%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.14855v2&entry.124074799=Read"},
{"title": "Measuring proximity to standard planes during fetal brain ultrasound\n  scanning", "author": "Chiara Di Vece and Antonio Cirigliano and Meala Le Lous and Raffaele Napolitano and Anna L. David and Donald Peebles and Pierre Jannin and Francisco Vasconcelos and Danail Stoyanov", "abstract": "  This paper introduces a novel pipeline designed to bring ultrasound (US)\nplane pose estimation closer to clinical use for more effective navigation to\nthe standard planes (SPs) in the fetal brain. We propose a semi-supervised\nsegmentation model utilizing both labeled SPs and unlabeled 3D US volume\nslices. Our model enables reliable segmentation across a diverse set of fetal\nbrain images. Furthermore, the model incorporates a classification mechanism to\nidentify the fetal brain precisely. Our model not only filters out frames\nlacking the brain but also generates masks for those containing it, enhancing\nthe relevance of plane pose regression in clinical settings. We focus on fetal\nbrain navigation from 2D ultrasound (US) video analysis and combine this model\nwith a US plane pose regression network to provide sensorless proximity\ndetection to SPs and non-SPs planes; we emphasize the importance of proximity\ndetection to SPs for guiding sonographers, offering a substantial advantage\nover traditional methods by allowing earlier and more precise adjustments\nduring scanning. We demonstrate the practical applicability of our approach\nthrough validation on real fetal scan videos obtained from sonographers of\nvarying expertise levels. Our findings demonstrate the potential of our\napproach to complement existing fetal US technologies and advance prenatal\ndiagnostic practices.\n", "link": "http://arxiv.org/abs/2404.07124v1", "date": "2024-04-10", "relevancy": 1.9518, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4809}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4595}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Measuring%20proximity%20to%20standard%20planes%20during%20fetal%20brain%20ultrasound%0A%20%20scanning&body=Title%3A%20Measuring%20proximity%20to%20standard%20planes%20during%20fetal%20brain%20ultrasound%0A%20%20scanning%0AAuthor%3A%20Chiara%20Di%20Vece%20and%20Antonio%20Cirigliano%20and%20Meala%20Le%20Lous%20and%20Raffaele%20Napolitano%20and%20Anna%20L.%20David%20and%20Donald%20Peebles%20and%20Pierre%20Jannin%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20pipeline%20designed%20to%20bring%20ultrasound%20%28US%29%0Aplane%20pose%20estimation%20closer%20to%20clinical%20use%20for%20more%20effective%20navigation%20to%0Athe%20standard%20planes%20%28SPs%29%20in%20the%20fetal%20brain.%20We%20propose%20a%20semi-supervised%0Asegmentation%20model%20utilizing%20both%20labeled%20SPs%20and%20unlabeled%203D%20US%20volume%0Aslices.%20Our%20model%20enables%20reliable%20segmentation%20across%20a%20diverse%20set%20of%20fetal%0Abrain%20images.%20Furthermore%2C%20the%20model%20incorporates%20a%20classification%20mechanism%20to%0Aidentify%20the%20fetal%20brain%20precisely.%20Our%20model%20not%20only%20filters%20out%20frames%0Alacking%20the%20brain%20but%20also%20generates%20masks%20for%20those%20containing%20it%2C%20enhancing%0Athe%20relevance%20of%20plane%20pose%20regression%20in%20clinical%20settings.%20We%20focus%20on%20fetal%0Abrain%20navigation%20from%202D%20ultrasound%20%28US%29%20video%20analysis%20and%20combine%20this%20model%0Awith%20a%20US%20plane%20pose%20regression%20network%20to%20provide%20sensorless%20proximity%0Adetection%20to%20SPs%20and%20non-SPs%20planes%3B%20we%20emphasize%20the%20importance%20of%20proximity%0Adetection%20to%20SPs%20for%20guiding%20sonographers%2C%20offering%20a%20substantial%20advantage%0Aover%20traditional%20methods%20by%20allowing%20earlier%20and%20more%20precise%20adjustments%0Aduring%20scanning.%20We%20demonstrate%20the%20practical%20applicability%20of%20our%20approach%0Athrough%20validation%20on%20real%20fetal%20scan%20videos%20obtained%20from%20sonographers%20of%0Avarying%20expertise%20levels.%20Our%20findings%20demonstrate%20the%20potential%20of%20our%0Aapproach%20to%20complement%20existing%20fetal%20US%20technologies%20and%20advance%20prenatal%0Adiagnostic%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07124v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20proximity%20to%20standard%20planes%20during%20fetal%20brain%20ultrasound%0A%20%20scanning&entry.906535625=Chiara%20Di%20Vece%20and%20Antonio%20Cirigliano%20and%20Meala%20Le%20Lous%20and%20Raffaele%20Napolitano%20and%20Anna%20L.%20David%20and%20Donald%20Peebles%20and%20Pierre%20Jannin%20and%20Francisco%20Vasconcelos%20and%20Danail%20Stoyanov&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20pipeline%20designed%20to%20bring%20ultrasound%20%28US%29%0Aplane%20pose%20estimation%20closer%20to%20clinical%20use%20for%20more%20effective%20navigation%20to%0Athe%20standard%20planes%20%28SPs%29%20in%20the%20fetal%20brain.%20We%20propose%20a%20semi-supervised%0Asegmentation%20model%20utilizing%20both%20labeled%20SPs%20and%20unlabeled%203D%20US%20volume%0Aslices.%20Our%20model%20enables%20reliable%20segmentation%20across%20a%20diverse%20set%20of%20fetal%0Abrain%20images.%20Furthermore%2C%20the%20model%20incorporates%20a%20classification%20mechanism%20to%0Aidentify%20the%20fetal%20brain%20precisely.%20Our%20model%20not%20only%20filters%20out%20frames%0Alacking%20the%20brain%20but%20also%20generates%20masks%20for%20those%20containing%20it%2C%20enhancing%0Athe%20relevance%20of%20plane%20pose%20regression%20in%20clinical%20settings.%20We%20focus%20on%20fetal%0Abrain%20navigation%20from%202D%20ultrasound%20%28US%29%20video%20analysis%20and%20combine%20this%20model%0Awith%20a%20US%20plane%20pose%20regression%20network%20to%20provide%20sensorless%20proximity%0Adetection%20to%20SPs%20and%20non-SPs%20planes%3B%20we%20emphasize%20the%20importance%20of%20proximity%0Adetection%20to%20SPs%20for%20guiding%20sonographers%2C%20offering%20a%20substantial%20advantage%0Aover%20traditional%20methods%20by%20allowing%20earlier%20and%20more%20precise%20adjustments%0Aduring%20scanning.%20We%20demonstrate%20the%20practical%20applicability%20of%20our%20approach%0Athrough%20validation%20on%20real%20fetal%20scan%20videos%20obtained%20from%20sonographers%20of%0Avarying%20expertise%20levels.%20Our%20findings%20demonstrate%20the%20potential%20of%20our%0Aapproach%20to%20complement%20existing%20fetal%20US%20technologies%20and%20advance%20prenatal%0Adiagnostic%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07124v1&entry.124074799=Read"},
{"title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs", "author": "Bowen Jin and Chulin Xie and Jiawei Zhang and Kashob Kumar Roy and Yu Zhang and Suhang Wang and Yu Meng and Jiawei Han", "abstract": "  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n", "link": "http://arxiv.org/abs/2404.07103v1", "date": "2024-04-10", "relevancy": 1.9269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5187}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4549}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs&body=Title%3A%20Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs%0AAuthor%3A%20Bowen%20Jin%20and%20Chulin%20Xie%20and%20Jiawei%20Zhang%20and%20Kashob%20Kumar%20Roy%20and%20Yu%20Zhang%20and%20Suhang%20Wang%20and%20Yu%20Meng%20and%20Jiawei%20Han%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20exhibiting%20exceptional%20performance%2C%0Asuffer%20from%20hallucinations%2C%20especially%20on%20knowledge-intensive%20tasks.%20Existing%0Aworks%20propose%20to%20augment%20LLMs%20with%20individual%20text%20units%20retrieved%20from%0Aexternal%20knowledge%20corpora%20to%20alleviate%20the%20issue.%20However%2C%20in%20many%20domains%2C%0Atexts%20are%20interconnected%20%28e.g.%2C%20academic%20papers%20in%20a%20bibliographic%20graph%20are%0Alinked%20by%20citations%20and%20co-authorships%29%20which%20form%20a%20%28text-attributed%29%20graph.%0AThe%20knowledge%20in%20such%20graphs%20is%20encoded%20not%20only%20in%20single%20texts/nodes%20but%20also%0Ain%20their%20associated%20connections.%20To%20facilitate%20the%20research%20of%20augmenting%20LLMs%0Awith%20graphs%2C%20we%20manually%20construct%20a%20Graph%20Reasoning%20Benchmark%20dataset%20called%0AGRBench%2C%20containing%201%2C740%20questions%20that%20can%20be%20answered%20with%20the%20knowledge%0Afrom%2010%20domain%20graphs.%20Then%2C%20we%20propose%20a%20simple%20and%20effective%20framework%20called%0AGraph%20Chain-of-thought%20%28Graph-CoT%29%20to%20augment%20LLMs%20with%20graphs%20by%20encouraging%0ALLMs%20to%20reason%20on%20the%20graph%20iteratively.%20Each%20Graph-CoT%20iteration%20consists%20of%0Athree%20sub-steps%3A%20LLM%20reasoning%2C%20LLM-graph%20interaction%2C%20and%20graph%20execution.%20We%0Aconduct%20systematic%20experiments%20with%20three%20LLM%20backbones%20on%20GRBench%2C%20where%0AGraph-CoT%20outperforms%20the%20baselines%20consistently.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/Graph-CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07103v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs&entry.906535625=Bowen%20Jin%20and%20Chulin%20Xie%20and%20Jiawei%20Zhang%20and%20Kashob%20Kumar%20Roy%20and%20Yu%20Zhang%20and%20Suhang%20Wang%20and%20Yu%20Meng%20and%20Jiawei%20Han&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20exhibiting%20exceptional%20performance%2C%0Asuffer%20from%20hallucinations%2C%20especially%20on%20knowledge-intensive%20tasks.%20Existing%0Aworks%20propose%20to%20augment%20LLMs%20with%20individual%20text%20units%20retrieved%20from%0Aexternal%20knowledge%20corpora%20to%20alleviate%20the%20issue.%20However%2C%20in%20many%20domains%2C%0Atexts%20are%20interconnected%20%28e.g.%2C%20academic%20papers%20in%20a%20bibliographic%20graph%20are%0Alinked%20by%20citations%20and%20co-authorships%29%20which%20form%20a%20%28text-attributed%29%20graph.%0AThe%20knowledge%20in%20such%20graphs%20is%20encoded%20not%20only%20in%20single%20texts/nodes%20but%20also%0Ain%20their%20associated%20connections.%20To%20facilitate%20the%20research%20of%20augmenting%20LLMs%0Awith%20graphs%2C%20we%20manually%20construct%20a%20Graph%20Reasoning%20Benchmark%20dataset%20called%0AGRBench%2C%20containing%201%2C740%20questions%20that%20can%20be%20answered%20with%20the%20knowledge%0Afrom%2010%20domain%20graphs.%20Then%2C%20we%20propose%20a%20simple%20and%20effective%20framework%20called%0AGraph%20Chain-of-thought%20%28Graph-CoT%29%20to%20augment%20LLMs%20with%20graphs%20by%20encouraging%0ALLMs%20to%20reason%20on%20the%20graph%20iteratively.%20Each%20Graph-CoT%20iteration%20consists%20of%0Athree%20sub-steps%3A%20LLM%20reasoning%2C%20LLM-graph%20interaction%2C%20and%20graph%20execution.%20We%0Aconduct%20systematic%20experiments%20with%20three%20LLM%20backbones%20on%20GRBench%2C%20where%0AGraph-CoT%20outperforms%20the%20baselines%20consistently.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/Graph-CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07103v1&entry.124074799=Read"},
{"title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs", "author": "Bowen Jin and Chulin Xie and Jiawei Zhang and Kashob Kumar Roy and Yu Zhang and Suhang Wang and Yu Meng and Jiawei Han", "abstract": "  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n", "link": "http://arxiv.org/abs/2404.07103v1", "date": "2024-04-10", "relevancy": 1.9269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5187}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4937}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4549}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs&body=Title%3A%20Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs%0AAuthor%3A%20Bowen%20Jin%20and%20Chulin%20Xie%20and%20Jiawei%20Zhang%20and%20Kashob%20Kumar%20Roy%20and%20Yu%20Zhang%20and%20Suhang%20Wang%20and%20Yu%20Meng%20and%20Jiawei%20Han%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20exhibiting%20exceptional%20performance%2C%0Asuffer%20from%20hallucinations%2C%20especially%20on%20knowledge-intensive%20tasks.%20Existing%0Aworks%20propose%20to%20augment%20LLMs%20with%20individual%20text%20units%20retrieved%20from%0Aexternal%20knowledge%20corpora%20to%20alleviate%20the%20issue.%20However%2C%20in%20many%20domains%2C%0Atexts%20are%20interconnected%20%28e.g.%2C%20academic%20papers%20in%20a%20bibliographic%20graph%20are%0Alinked%20by%20citations%20and%20co-authorships%29%20which%20form%20a%20%28text-attributed%29%20graph.%0AThe%20knowledge%20in%20such%20graphs%20is%20encoded%20not%20only%20in%20single%20texts/nodes%20but%20also%0Ain%20their%20associated%20connections.%20To%20facilitate%20the%20research%20of%20augmenting%20LLMs%0Awith%20graphs%2C%20we%20manually%20construct%20a%20Graph%20Reasoning%20Benchmark%20dataset%20called%0AGRBench%2C%20containing%201%2C740%20questions%20that%20can%20be%20answered%20with%20the%20knowledge%0Afrom%2010%20domain%20graphs.%20Then%2C%20we%20propose%20a%20simple%20and%20effective%20framework%20called%0AGraph%20Chain-of-thought%20%28Graph-CoT%29%20to%20augment%20LLMs%20with%20graphs%20by%20encouraging%0ALLMs%20to%20reason%20on%20the%20graph%20iteratively.%20Each%20Graph-CoT%20iteration%20consists%20of%0Athree%20sub-steps%3A%20LLM%20reasoning%2C%20LLM-graph%20interaction%2C%20and%20graph%20execution.%20We%0Aconduct%20systematic%20experiments%20with%20three%20LLM%20backbones%20on%20GRBench%2C%20where%0AGraph-CoT%20outperforms%20the%20baselines%20consistently.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/Graph-CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07103v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Chain-of-Thought%3A%20Augmenting%20Large%20Language%20Models%20by%20Reasoning%20on%0A%20%20Graphs&entry.906535625=Bowen%20Jin%20and%20Chulin%20Xie%20and%20Jiawei%20Zhang%20and%20Kashob%20Kumar%20Roy%20and%20Yu%20Zhang%20and%20Suhang%20Wang%20and%20Yu%20Meng%20and%20Jiawei%20Han&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20while%20exhibiting%20exceptional%20performance%2C%0Asuffer%20from%20hallucinations%2C%20especially%20on%20knowledge-intensive%20tasks.%20Existing%0Aworks%20propose%20to%20augment%20LLMs%20with%20individual%20text%20units%20retrieved%20from%0Aexternal%20knowledge%20corpora%20to%20alleviate%20the%20issue.%20However%2C%20in%20many%20domains%2C%0Atexts%20are%20interconnected%20%28e.g.%2C%20academic%20papers%20in%20a%20bibliographic%20graph%20are%0Alinked%20by%20citations%20and%20co-authorships%29%20which%20form%20a%20%28text-attributed%29%20graph.%0AThe%20knowledge%20in%20such%20graphs%20is%20encoded%20not%20only%20in%20single%20texts/nodes%20but%20also%0Ain%20their%20associated%20connections.%20To%20facilitate%20the%20research%20of%20augmenting%20LLMs%0Awith%20graphs%2C%20we%20manually%20construct%20a%20Graph%20Reasoning%20Benchmark%20dataset%20called%0AGRBench%2C%20containing%201%2C740%20questions%20that%20can%20be%20answered%20with%20the%20knowledge%0Afrom%2010%20domain%20graphs.%20Then%2C%20we%20propose%20a%20simple%20and%20effective%20framework%20called%0AGraph%20Chain-of-thought%20%28Graph-CoT%29%20to%20augment%20LLMs%20with%20graphs%20by%20encouraging%0ALLMs%20to%20reason%20on%20the%20graph%20iteratively.%20Each%20Graph-CoT%20iteration%20consists%20of%0Athree%20sub-steps%3A%20LLM%20reasoning%2C%20LLM-graph%20interaction%2C%20and%20graph%20execution.%20We%0Aconduct%20systematic%20experiments%20with%20three%20LLM%20backbones%20on%20GRBench%2C%20where%0AGraph-CoT%20outperforms%20the%20baselines%20consistently.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PeterGriffinJin/Graph-CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07103v1&entry.124074799=Read"},
{"title": "Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated\n  Continuum Robots", "author": "Yuan Wang and Max McCandless and Abdulhamit Donder and Giovanni Pittiglio and Behnam Moradkhani and Yash Chitalia and Pierre E. Dupont", "abstract": "  The ability to accurately model mechanical hysteretic behavior in\ntendon-actuated continuum robots using deep learning approaches is a growing\narea of interest. In this paper, we investigate the hysteretic response of two\ntypes of tendon-actuated continuum robots and, ultimately, compare three types\nof neural network modeling approaches with both forward and inverse kinematic\nmappings: feedforward neural network (FNN), FNN with a history input buffer,\nand long short-term memory (LSTM) network. We seek to determine which model\nbest captures temporal dependent behavior. We find that, depending on the\nrobot's design, choosing different kinematic inputs can alter whether\nhysteresis is exhibited by the system. Furthermore, we present the results of\nthe model fittings, revealing that, in contrast to the standard FNN, both FNN\nwith a history input buffer and the LSTM model exhibit the capacity to model\nhistorical dependence with comparable performance in capturing rate-dependent\nhysteresis.\n", "link": "http://arxiv.org/abs/2404.07168v1", "date": "2024-04-10", "relevancy": 1.9207, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5393}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20Neural%20Networks%20to%20Model%20Hysteretic%20Kinematics%20in%20Tendon-Actuated%0A%20%20Continuum%20Robots&body=Title%3A%20Using%20Neural%20Networks%20to%20Model%20Hysteretic%20Kinematics%20in%20Tendon-Actuated%0A%20%20Continuum%20Robots%0AAuthor%3A%20Yuan%20Wang%20and%20Max%20McCandless%20and%20Abdulhamit%20Donder%20and%20Giovanni%20Pittiglio%20and%20Behnam%20Moradkhani%20and%20Yash%20Chitalia%20and%20Pierre%20E.%20Dupont%0AAbstract%3A%20%20%20The%20ability%20to%20accurately%20model%20mechanical%20hysteretic%20behavior%20in%0Atendon-actuated%20continuum%20robots%20using%20deep%20learning%20approaches%20is%20a%20growing%0Aarea%20of%20interest.%20In%20this%20paper%2C%20we%20investigate%20the%20hysteretic%20response%20of%20two%0Atypes%20of%20tendon-actuated%20continuum%20robots%20and%2C%20ultimately%2C%20compare%20three%20types%0Aof%20neural%20network%20modeling%20approaches%20with%20both%20forward%20and%20inverse%20kinematic%0Amappings%3A%20feedforward%20neural%20network%20%28FNN%29%2C%20FNN%20with%20a%20history%20input%20buffer%2C%0Aand%20long%20short-term%20memory%20%28LSTM%29%20network.%20We%20seek%20to%20determine%20which%20model%0Abest%20captures%20temporal%20dependent%20behavior.%20We%20find%20that%2C%20depending%20on%20the%0Arobot%27s%20design%2C%20choosing%20different%20kinematic%20inputs%20can%20alter%20whether%0Ahysteresis%20is%20exhibited%20by%20the%20system.%20Furthermore%2C%20we%20present%20the%20results%20of%0Athe%20model%20fittings%2C%20revealing%20that%2C%20in%20contrast%20to%20the%20standard%20FNN%2C%20both%20FNN%0Awith%20a%20history%20input%20buffer%20and%20the%20LSTM%20model%20exhibit%20the%20capacity%20to%20model%0Ahistorical%20dependence%20with%20comparable%20performance%20in%20capturing%20rate-dependent%0Ahysteresis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07168v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Neural%20Networks%20to%20Model%20Hysteretic%20Kinematics%20in%20Tendon-Actuated%0A%20%20Continuum%20Robots&entry.906535625=Yuan%20Wang%20and%20Max%20McCandless%20and%20Abdulhamit%20Donder%20and%20Giovanni%20Pittiglio%20and%20Behnam%20Moradkhani%20and%20Yash%20Chitalia%20and%20Pierre%20E.%20Dupont&entry.1292438233=%20%20The%20ability%20to%20accurately%20model%20mechanical%20hysteretic%20behavior%20in%0Atendon-actuated%20continuum%20robots%20using%20deep%20learning%20approaches%20is%20a%20growing%0Aarea%20of%20interest.%20In%20this%20paper%2C%20we%20investigate%20the%20hysteretic%20response%20of%20two%0Atypes%20of%20tendon-actuated%20continuum%20robots%20and%2C%20ultimately%2C%20compare%20three%20types%0Aof%20neural%20network%20modeling%20approaches%20with%20both%20forward%20and%20inverse%20kinematic%0Amappings%3A%20feedforward%20neural%20network%20%28FNN%29%2C%20FNN%20with%20a%20history%20input%20buffer%2C%0Aand%20long%20short-term%20memory%20%28LSTM%29%20network.%20We%20seek%20to%20determine%20which%20model%0Abest%20captures%20temporal%20dependent%20behavior.%20We%20find%20that%2C%20depending%20on%20the%0Arobot%27s%20design%2C%20choosing%20different%20kinematic%20inputs%20can%20alter%20whether%0Ahysteresis%20is%20exhibited%20by%20the%20system.%20Furthermore%2C%20we%20present%20the%20results%20of%0Athe%20model%20fittings%2C%20revealing%20that%2C%20in%20contrast%20to%20the%20standard%20FNN%2C%20both%20FNN%0Awith%20a%20history%20input%20buffer%20and%20the%20LSTM%20model%20exhibit%20the%20capacity%20to%20model%0Ahistorical%20dependence%20with%20comparable%20performance%20in%20capturing%20rate-dependent%0Ahysteresis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07168v1&entry.124074799=Read"},
{"title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?", "author": "Mingyu Jin and Qinkai Yu and Jingyuan Huang and Qingcheng Zeng and Zhenting Wang and Wenyue Hua and Haiyan Zhao and Kai Mei and Yanda Meng and Kaize Ding and Fan Yang and Mengnan Du and Yongfeng Zhang", "abstract": "  This paper studies the phenomenon that different concepts are learned in\ndifferent layers of large language models, i.e. more difficult concepts are\nfully acquired with deeper layers. We define the difficulty of concepts by the\nlevel of abstraction, and here it is crudely categorized by factual, emotional,\nand inferential. Each category contains a spectrum of tasks, arranged from\nsimple to complex. For example, within the factual dimension, tasks range from\nlie detection to categorizing mathematical problems. We employ a probing\ntechnique to extract representations from different layers of the model and\napply these to classification tasks. Our findings reveal that models tend to\nefficiently classify simpler tasks, indicating that these concepts are learned\nin shallower layers. Conversely, more complex tasks may only be discernible at\ndeeper layers, if at all. This paper explores the implications of these\nfindings for our understanding of model learning processes and internal\nrepresentations. Our implementation is available at\n\\url{https://github.com/Luckfort/CD}.\n", "link": "http://arxiv.org/abs/2404.07066v1", "date": "2024-04-10", "relevancy": 1.9203, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4879}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Concept%20Depth%3A%20How%20Large%20Language%20Models%20Acquire%20Knowledge%20at%0A%20%20Different%20Layers%3F&body=Title%3A%20Exploring%20Concept%20Depth%3A%20How%20Large%20Language%20Models%20Acquire%20Knowledge%20at%0A%20%20Different%20Layers%3F%0AAuthor%3A%20Mingyu%20Jin%20and%20Qinkai%20Yu%20and%20Jingyuan%20Huang%20and%20Qingcheng%20Zeng%20and%20Zhenting%20Wang%20and%20Wenyue%20Hua%20and%20Haiyan%20Zhao%20and%20Kai%20Mei%20and%20Yanda%20Meng%20and%20Kaize%20Ding%20and%20Fan%20Yang%20and%20Mengnan%20Du%20and%20Yongfeng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20phenomenon%20that%20different%20concepts%20are%20learned%20in%0Adifferent%20layers%20of%20large%20language%20models%2C%20i.e.%20more%20difficult%20concepts%20are%0Afully%20acquired%20with%20deeper%20layers.%20We%20define%20the%20difficulty%20of%20concepts%20by%20the%0Alevel%20of%20abstraction%2C%20and%20here%20it%20is%20crudely%20categorized%20by%20factual%2C%20emotional%2C%0Aand%20inferential.%20Each%20category%20contains%20a%20spectrum%20of%20tasks%2C%20arranged%20from%0Asimple%20to%20complex.%20For%20example%2C%20within%20the%20factual%20dimension%2C%20tasks%20range%20from%0Alie%20detection%20to%20categorizing%20mathematical%20problems.%20We%20employ%20a%20probing%0Atechnique%20to%20extract%20representations%20from%20different%20layers%20of%20the%20model%20and%0Aapply%20these%20to%20classification%20tasks.%20Our%20findings%20reveal%20that%20models%20tend%20to%0Aefficiently%20classify%20simpler%20tasks%2C%20indicating%20that%20these%20concepts%20are%20learned%0Ain%20shallower%20layers.%20Conversely%2C%20more%20complex%20tasks%20may%20only%20be%20discernible%20at%0Adeeper%20layers%2C%20if%20at%20all.%20This%20paper%20explores%20the%20implications%20of%20these%0Afindings%20for%20our%20understanding%20of%20model%20learning%20processes%20and%20internal%0Arepresentations.%20Our%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Luckfort/CD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07066v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Concept%20Depth%3A%20How%20Large%20Language%20Models%20Acquire%20Knowledge%20at%0A%20%20Different%20Layers%3F&entry.906535625=Mingyu%20Jin%20and%20Qinkai%20Yu%20and%20Jingyuan%20Huang%20and%20Qingcheng%20Zeng%20and%20Zhenting%20Wang%20and%20Wenyue%20Hua%20and%20Haiyan%20Zhao%20and%20Kai%20Mei%20and%20Yanda%20Meng%20and%20Kaize%20Ding%20and%20Fan%20Yang%20and%20Mengnan%20Du%20and%20Yongfeng%20Zhang&entry.1292438233=%20%20This%20paper%20studies%20the%20phenomenon%20that%20different%20concepts%20are%20learned%20in%0Adifferent%20layers%20of%20large%20language%20models%2C%20i.e.%20more%20difficult%20concepts%20are%0Afully%20acquired%20with%20deeper%20layers.%20We%20define%20the%20difficulty%20of%20concepts%20by%20the%0Alevel%20of%20abstraction%2C%20and%20here%20it%20is%20crudely%20categorized%20by%20factual%2C%20emotional%2C%0Aand%20inferential.%20Each%20category%20contains%20a%20spectrum%20of%20tasks%2C%20arranged%20from%0Asimple%20to%20complex.%20For%20example%2C%20within%20the%20factual%20dimension%2C%20tasks%20range%20from%0Alie%20detection%20to%20categorizing%20mathematical%20problems.%20We%20employ%20a%20probing%0Atechnique%20to%20extract%20representations%20from%20different%20layers%20of%20the%20model%20and%0Aapply%20these%20to%20classification%20tasks.%20Our%20findings%20reveal%20that%20models%20tend%20to%0Aefficiently%20classify%20simpler%20tasks%2C%20indicating%20that%20these%20concepts%20are%20learned%0Ain%20shallower%20layers.%20Conversely%2C%20more%20complex%20tasks%20may%20only%20be%20discernible%20at%0Adeeper%20layers%2C%20if%20at%20all.%20This%20paper%20explores%20the%20implications%20of%20these%0Afindings%20for%20our%20understanding%20of%20model%20learning%20processes%20and%20internal%0Arepresentations.%20Our%20implementation%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Luckfort/CD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07066v1&entry.124074799=Read"},
{"title": "TransTARec: Time-Adaptive Translating Embedding Model for Next POI\n  Recommendation", "author": "Yiping Sun", "abstract": "  The rapid growth of location acquisition technologies makes\nPoint-of-Interest(POI) recommendation possible due to redundant user check-in\nrecords. In this paper, we focus on next POI recommendation in which next POI\nis based on previous POI. We observe that time plays an important role in next\nPOI recommendation but is neglected in the recent proposed translating\nembedding methods. To tackle this shortage, we propose a time-adaptive\ntranslating embedding model (TransTARec) for next POI recommendation that\nnaturally incorporates temporal influence, sequential dynamics, and user\npreference within a single component. Methodologically, we treat a (previous\ntimestamp, user, next timestamp) triplet as a union translation vector and\ndevelop a neural-based fusion operation to fuse user preference and temporal\ninfluence. The superiority of TransTARec, which is confirmed by extensive\nexperiments on real-world datasets, comes from not only the introduction of\ntemporal influence but also the direct unification with user preference and\nsequential dynamics.\n", "link": "http://arxiv.org/abs/2404.07096v1", "date": "2024-04-10", "relevancy": 1.912, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4978}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4682}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TransTARec%3A%20Time-Adaptive%20Translating%20Embedding%20Model%20for%20Next%20POI%0A%20%20Recommendation&body=Title%3A%20TransTARec%3A%20Time-Adaptive%20Translating%20Embedding%20Model%20for%20Next%20POI%0A%20%20Recommendation%0AAuthor%3A%20Yiping%20Sun%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20location%20acquisition%20technologies%20makes%0APoint-of-Interest%28POI%29%20recommendation%20possible%20due%20to%20redundant%20user%20check-in%0Arecords.%20In%20this%20paper%2C%20we%20focus%20on%20next%20POI%20recommendation%20in%20which%20next%20POI%0Ais%20based%20on%20previous%20POI.%20We%20observe%20that%20time%20plays%20an%20important%20role%20in%20next%0APOI%20recommendation%20but%20is%20neglected%20in%20the%20recent%20proposed%20translating%0Aembedding%20methods.%20To%20tackle%20this%20shortage%2C%20we%20propose%20a%20time-adaptive%0Atranslating%20embedding%20model%20%28TransTARec%29%20for%20next%20POI%20recommendation%20that%0Anaturally%20incorporates%20temporal%20influence%2C%20sequential%20dynamics%2C%20and%20user%0Apreference%20within%20a%20single%20component.%20Methodologically%2C%20we%20treat%20a%20%28previous%0Atimestamp%2C%20user%2C%20next%20timestamp%29%20triplet%20as%20a%20union%20translation%20vector%20and%0Adevelop%20a%20neural-based%20fusion%20operation%20to%20fuse%20user%20preference%20and%20temporal%0Ainfluence.%20The%20superiority%20of%20TransTARec%2C%20which%20is%20confirmed%20by%20extensive%0Aexperiments%20on%20real-world%20datasets%2C%20comes%20from%20not%20only%20the%20introduction%20of%0Atemporal%20influence%20but%20also%20the%20direct%20unification%20with%20user%20preference%20and%0Asequential%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07096v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransTARec%3A%20Time-Adaptive%20Translating%20Embedding%20Model%20for%20Next%20POI%0A%20%20Recommendation&entry.906535625=Yiping%20Sun&entry.1292438233=%20%20The%20rapid%20growth%20of%20location%20acquisition%20technologies%20makes%0APoint-of-Interest%28POI%29%20recommendation%20possible%20due%20to%20redundant%20user%20check-in%0Arecords.%20In%20this%20paper%2C%20we%20focus%20on%20next%20POI%20recommendation%20in%20which%20next%20POI%0Ais%20based%20on%20previous%20POI.%20We%20observe%20that%20time%20plays%20an%20important%20role%20in%20next%0APOI%20recommendation%20but%20is%20neglected%20in%20the%20recent%20proposed%20translating%0Aembedding%20methods.%20To%20tackle%20this%20shortage%2C%20we%20propose%20a%20time-adaptive%0Atranslating%20embedding%20model%20%28TransTARec%29%20for%20next%20POI%20recommendation%20that%0Anaturally%20incorporates%20temporal%20influence%2C%20sequential%20dynamics%2C%20and%20user%0Apreference%20within%20a%20single%20component.%20Methodologically%2C%20we%20treat%20a%20%28previous%0Atimestamp%2C%20user%2C%20next%20timestamp%29%20triplet%20as%20a%20union%20translation%20vector%20and%0Adevelop%20a%20neural-based%20fusion%20operation%20to%20fuse%20user%20preference%20and%20temporal%0Ainfluence.%20The%20superiority%20of%20TransTARec%2C%20which%20is%20confirmed%20by%20extensive%0Aexperiments%20on%20real-world%20datasets%2C%20comes%20from%20not%20only%20the%20introduction%20of%0Atemporal%20influence%20but%20also%20the%20direct%20unification%20with%20user%20preference%20and%0Asequential%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07096v1&entry.124074799=Read"},
{"title": "Groundedness in Retrieval-augmented Long-form Generation: An Empirical\n  Study", "author": "Alessandro Stolfo", "abstract": "  We present an empirical study of groundedness in long-form question answering\n(LFQA) by retrieval-augmented large language models (LLMs). In particular, we\nevaluate whether every generated sentence is grounded in the retrieved\ndocuments or the model's pre-training data. Across 3 datasets and 4 model\nfamilies, our findings reveal that a significant fraction of generated\nsentences are consistently ungrounded, even when those sentences contain\ncorrect ground-truth answers. Additionally, we examine the impacts of factors\nsuch as model size, decoding strategy, and instruction tuning on groundedness.\nOur results show that while larger models tend to ground their outputs more\neffectively, a significant portion of correct answers remains compromised by\nhallucinations. This study provides novel insights into the groundedness\nchallenges in LFQA and underscores the necessity for more robust mechanisms in\nLLMs to mitigate the generation of ungrounded content.\n", "link": "http://arxiv.org/abs/2404.07060v1", "date": "2024-04-10", "relevancy": 1.9079, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4832}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4762}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Groundedness%20in%20Retrieval-augmented%20Long-form%20Generation%3A%20An%20Empirical%0A%20%20Study&body=Title%3A%20Groundedness%20in%20Retrieval-augmented%20Long-form%20Generation%3A%20An%20Empirical%0A%20%20Study%0AAuthor%3A%20Alessandro%20Stolfo%0AAbstract%3A%20%20%20We%20present%20an%20empirical%20study%20of%20groundedness%20in%20long-form%20question%20answering%0A%28LFQA%29%20by%20retrieval-augmented%20large%20language%20models%20%28LLMs%29.%20In%20particular%2C%20we%0Aevaluate%20whether%20every%20generated%20sentence%20is%20grounded%20in%20the%20retrieved%0Adocuments%20or%20the%20model%27s%20pre-training%20data.%20Across%203%20datasets%20and%204%20model%0Afamilies%2C%20our%20findings%20reveal%20that%20a%20significant%20fraction%20of%20generated%0Asentences%20are%20consistently%20ungrounded%2C%20even%20when%20those%20sentences%20contain%0Acorrect%20ground-truth%20answers.%20Additionally%2C%20we%20examine%20the%20impacts%20of%20factors%0Asuch%20as%20model%20size%2C%20decoding%20strategy%2C%20and%20instruction%20tuning%20on%20groundedness.%0AOur%20results%20show%20that%20while%20larger%20models%20tend%20to%20ground%20their%20outputs%20more%0Aeffectively%2C%20a%20significant%20portion%20of%20correct%20answers%20remains%20compromised%20by%0Ahallucinations.%20This%20study%20provides%20novel%20insights%20into%20the%20groundedness%0Achallenges%20in%20LFQA%20and%20underscores%20the%20necessity%20for%20more%20robust%20mechanisms%20in%0ALLMs%20to%20mitigate%20the%20generation%20of%20ungrounded%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07060v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Groundedness%20in%20Retrieval-augmented%20Long-form%20Generation%3A%20An%20Empirical%0A%20%20Study&entry.906535625=Alessandro%20Stolfo&entry.1292438233=%20%20We%20present%20an%20empirical%20study%20of%20groundedness%20in%20long-form%20question%20answering%0A%28LFQA%29%20by%20retrieval-augmented%20large%20language%20models%20%28LLMs%29.%20In%20particular%2C%20we%0Aevaluate%20whether%20every%20generated%20sentence%20is%20grounded%20in%20the%20retrieved%0Adocuments%20or%20the%20model%27s%20pre-training%20data.%20Across%203%20datasets%20and%204%20model%0Afamilies%2C%20our%20findings%20reveal%20that%20a%20significant%20fraction%20of%20generated%0Asentences%20are%20consistently%20ungrounded%2C%20even%20when%20those%20sentences%20contain%0Acorrect%20ground-truth%20answers.%20Additionally%2C%20we%20examine%20the%20impacts%20of%20factors%0Asuch%20as%20model%20size%2C%20decoding%20strategy%2C%20and%20instruction%20tuning%20on%20groundedness.%0AOur%20results%20show%20that%20while%20larger%20models%20tend%20to%20ground%20their%20outputs%20more%0Aeffectively%2C%20a%20significant%20portion%20of%20correct%20answers%20remains%20compromised%20by%0Ahallucinations.%20This%20study%20provides%20novel%20insights%20into%20the%20groundedness%0Achallenges%20in%20LFQA%20and%20underscores%20the%20necessity%20for%20more%20robust%20mechanisms%20in%0ALLMs%20to%20mitigate%20the%20generation%20of%20ungrounded%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07060v1&entry.124074799=Read"},
{"title": "Prediction Horizon Requirements for Automated Driving: Optimizing\n  Safety, Comfort, and Efficiency", "author": "Manuel Mu\u00f1oz S\u00e1nchez and Chris van der Ploeg and Robin Smit and Jos Elfring and Emilia Silvas and Ren\u00e9 van de Molengraft", "abstract": "  Predicting the movement of other road users is beneficial for improving\nautomated vehicle (AV) performance. However, the relationship between the time\nhorizon associated with these predictions and AV performance remains unclear.\nDespite the existence of numerous trajectory prediction algorithms, no studies\nhave been conducted on how varying prediction lengths affect AV safety and\nother vehicle performance metrics, resulting in undefined horizon requirements\nfor prediction methods. Our study addresses this gap by examining the effects\nof different prediction horizons on AV performance, focusing on safety,\ncomfort, and efficiency. Through multiple experiments using a state-of-the-art,\nrisk-based predictive trajectory planner, we simulated predictions with\nhorizons up to 20 seconds. Based on our simulations, we propose a framework for\nspecifying the minimum required and optimal prediction horizons based on\nspecific AV performance criteria and application needs. Our results indicate\nthat a horizon of 1.6 seconds is required to prevent collisions with crossing\npedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up\nto 15 seconds improve passenger comfort. We conclude that prediction horizon\nrequirements are application-dependent, and recommend aiming for a prediction\nhorizon of 11.8 seconds as a general guideline for applications involving\ncrossing pedestrians.\n", "link": "http://arxiv.org/abs/2402.03893v2", "date": "2024-04-10", "relevancy": 1.905, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5064}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prediction%20Horizon%20Requirements%20for%20Automated%20Driving%3A%20Optimizing%0A%20%20Safety%2C%20Comfort%2C%20and%20Efficiency&body=Title%3A%20Prediction%20Horizon%20Requirements%20for%20Automated%20Driving%3A%20Optimizing%0A%20%20Safety%2C%20Comfort%2C%20and%20Efficiency%0AAuthor%3A%20Manuel%20Mu%C3%B1oz%20S%C3%A1nchez%20and%20Chris%20van%20der%20Ploeg%20and%20Robin%20Smit%20and%20Jos%20Elfring%20and%20Emilia%20Silvas%20and%20Ren%C3%A9%20van%20de%20Molengraft%0AAbstract%3A%20%20%20Predicting%20the%20movement%20of%20other%20road%20users%20is%20beneficial%20for%20improving%0Aautomated%20vehicle%20%28AV%29%20performance.%20However%2C%20the%20relationship%20between%20the%20time%0Ahorizon%20associated%20with%20these%20predictions%20and%20AV%20performance%20remains%20unclear.%0ADespite%20the%20existence%20of%20numerous%20trajectory%20prediction%20algorithms%2C%20no%20studies%0Ahave%20been%20conducted%20on%20how%20varying%20prediction%20lengths%20affect%20AV%20safety%20and%0Aother%20vehicle%20performance%20metrics%2C%20resulting%20in%20undefined%20horizon%20requirements%0Afor%20prediction%20methods.%20Our%20study%20addresses%20this%20gap%20by%20examining%20the%20effects%0Aof%20different%20prediction%20horizons%20on%20AV%20performance%2C%20focusing%20on%20safety%2C%0Acomfort%2C%20and%20efficiency.%20Through%20multiple%20experiments%20using%20a%20state-of-the-art%2C%0Arisk-based%20predictive%20trajectory%20planner%2C%20we%20simulated%20predictions%20with%0Ahorizons%20up%20to%2020%20seconds.%20Based%20on%20our%20simulations%2C%20we%20propose%20a%20framework%20for%0Aspecifying%20the%20minimum%20required%20and%20optimal%20prediction%20horizons%20based%20on%0Aspecific%20AV%20performance%20criteria%20and%20application%20needs.%20Our%20results%20indicate%0Athat%20a%20horizon%20of%201.6%20seconds%20is%20required%20to%20prevent%20collisions%20with%20crossing%0Apedestrians%2C%20horizons%20of%207-8%20seconds%20yield%20the%20best%20efficiency%2C%20and%20horizons%20up%0Ato%2015%20seconds%20improve%20passenger%20comfort.%20We%20conclude%20that%20prediction%20horizon%0Arequirements%20are%20application-dependent%2C%20and%20recommend%20aiming%20for%20a%20prediction%0Ahorizon%20of%2011.8%20seconds%20as%20a%20general%20guideline%20for%20applications%20involving%0Acrossing%20pedestrians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03893v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20Horizon%20Requirements%20for%20Automated%20Driving%3A%20Optimizing%0A%20%20Safety%2C%20Comfort%2C%20and%20Efficiency&entry.906535625=Manuel%20Mu%C3%B1oz%20S%C3%A1nchez%20and%20Chris%20van%20der%20Ploeg%20and%20Robin%20Smit%20and%20Jos%20Elfring%20and%20Emilia%20Silvas%20and%20Ren%C3%A9%20van%20de%20Molengraft&entry.1292438233=%20%20Predicting%20the%20movement%20of%20other%20road%20users%20is%20beneficial%20for%20improving%0Aautomated%20vehicle%20%28AV%29%20performance.%20However%2C%20the%20relationship%20between%20the%20time%0Ahorizon%20associated%20with%20these%20predictions%20and%20AV%20performance%20remains%20unclear.%0ADespite%20the%20existence%20of%20numerous%20trajectory%20prediction%20algorithms%2C%20no%20studies%0Ahave%20been%20conducted%20on%20how%20varying%20prediction%20lengths%20affect%20AV%20safety%20and%0Aother%20vehicle%20performance%20metrics%2C%20resulting%20in%20undefined%20horizon%20requirements%0Afor%20prediction%20methods.%20Our%20study%20addresses%20this%20gap%20by%20examining%20the%20effects%0Aof%20different%20prediction%20horizons%20on%20AV%20performance%2C%20focusing%20on%20safety%2C%0Acomfort%2C%20and%20efficiency.%20Through%20multiple%20experiments%20using%20a%20state-of-the-art%2C%0Arisk-based%20predictive%20trajectory%20planner%2C%20we%20simulated%20predictions%20with%0Ahorizons%20up%20to%2020%20seconds.%20Based%20on%20our%20simulations%2C%20we%20propose%20a%20framework%20for%0Aspecifying%20the%20minimum%20required%20and%20optimal%20prediction%20horizons%20based%20on%0Aspecific%20AV%20performance%20criteria%20and%20application%20needs.%20Our%20results%20indicate%0Athat%20a%20horizon%20of%201.6%20seconds%20is%20required%20to%20prevent%20collisions%20with%20crossing%0Apedestrians%2C%20horizons%20of%207-8%20seconds%20yield%20the%20best%20efficiency%2C%20and%20horizons%20up%0Ato%2015%20seconds%20improve%20passenger%20comfort.%20We%20conclude%20that%20prediction%20horizon%0Arequirements%20are%20application-dependent%2C%20and%20recommend%20aiming%20for%20a%20prediction%0Ahorizon%20of%2011.8%20seconds%20as%20a%20general%20guideline%20for%20applications%20involving%0Acrossing%20pedestrians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03893v2&entry.124074799=Read"},
{"title": "Event Grounded Criminal Court View Generation withCooperative (Large)\n  Language Models", "author": "Linan Yue and Qi Liu and Lili Zhao and Li Wang and Weibo Gao and Yanqing An", "abstract": "  With the development of legal intelligence, Criminal Court View Generation\nhas attracted much attention as a crucial task of legal intelligence, which\naims to generate concise and coherent texts that summarize case facts and\nprovide explanations for verdicts. Existing researches explore the key\ninformation in case facts to yield the court views. Most of them employ a\ncoarse-grained approach that partitions the facts into broad segments (e.g.,\nverdict-related sentences) to make predictions. However, this approach fails to\ncapture the complex details present in the case facts, such as various criminal\nelements and legal events. To this end, in this paper, we propose an Event\nGrounded Generation (EGG) method for criminal court view generation with\ncooperative (Large) Language Models, which introduces the fine-grained event\ninformation into the generation. Specifically, we first design a LLMs-based\nextraction method that can extract events in case facts without massive\nannotated events. Then, we incorporate the extracted events into court view\ngeneration by merging case facts and events. Besides, considering the\ncomputational burden posed by the use of LLMs in the extraction phase of EGG,\nwe propose a LLMs-free EGG method that can eliminate the requirement for event\nextraction using LLMs in the inference phase. Extensive experimental results on\na real-world dataset clearly validate the effectiveness of our proposed method.\n", "link": "http://arxiv.org/abs/2404.07001v1", "date": "2024-04-10", "relevancy": 1.9004, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4814}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.477}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4546}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Event%20Grounded%20Criminal%20Court%20View%20Generation%20withCooperative%20%28Large%29%0A%20%20Language%20Models&body=Title%3A%20Event%20Grounded%20Criminal%20Court%20View%20Generation%20withCooperative%20%28Large%29%0A%20%20Language%20Models%0AAuthor%3A%20Linan%20Yue%20and%20Qi%20Liu%20and%20Lili%20Zhao%20and%20Li%20Wang%20and%20Weibo%20Gao%20and%20Yanqing%20An%0AAbstract%3A%20%20%20With%20the%20development%20of%20legal%20intelligence%2C%20Criminal%20Court%20View%20Generation%0Ahas%20attracted%20much%20attention%20as%20a%20crucial%20task%20of%20legal%20intelligence%2C%20which%0Aaims%20to%20generate%20concise%20and%20coherent%20texts%20that%20summarize%20case%20facts%20and%0Aprovide%20explanations%20for%20verdicts.%20Existing%20researches%20explore%20the%20key%0Ainformation%20in%20case%20facts%20to%20yield%20the%20court%20views.%20Most%20of%20them%20employ%20a%0Acoarse-grained%20approach%20that%20partitions%20the%20facts%20into%20broad%20segments%20%28e.g.%2C%0Averdict-related%20sentences%29%20to%20make%20predictions.%20However%2C%20this%20approach%20fails%20to%0Acapture%20the%20complex%20details%20present%20in%20the%20case%20facts%2C%20such%20as%20various%20criminal%0Aelements%20and%20legal%20events.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20an%20Event%0AGrounded%20Generation%20%28EGG%29%20method%20for%20criminal%20court%20view%20generation%20with%0Acooperative%20%28Large%29%20Language%20Models%2C%20which%20introduces%20the%20fine-grained%20event%0Ainformation%20into%20the%20generation.%20Specifically%2C%20we%20first%20design%20a%20LLMs-based%0Aextraction%20method%20that%20can%20extract%20events%20in%20case%20facts%20without%20massive%0Aannotated%20events.%20Then%2C%20we%20incorporate%20the%20extracted%20events%20into%20court%20view%0Ageneration%20by%20merging%20case%20facts%20and%20events.%20Besides%2C%20considering%20the%0Acomputational%20burden%20posed%20by%20the%20use%20of%20LLMs%20in%20the%20extraction%20phase%20of%20EGG%2C%0Awe%20propose%20a%20LLMs-free%20EGG%20method%20that%20can%20eliminate%20the%20requirement%20for%20event%0Aextraction%20using%20LLMs%20in%20the%20inference%20phase.%20Extensive%20experimental%20results%20on%0Aa%20real-world%20dataset%20clearly%20validate%20the%20effectiveness%20of%20our%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07001v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event%20Grounded%20Criminal%20Court%20View%20Generation%20withCooperative%20%28Large%29%0A%20%20Language%20Models&entry.906535625=Linan%20Yue%20and%20Qi%20Liu%20and%20Lili%20Zhao%20and%20Li%20Wang%20and%20Weibo%20Gao%20and%20Yanqing%20An&entry.1292438233=%20%20With%20the%20development%20of%20legal%20intelligence%2C%20Criminal%20Court%20View%20Generation%0Ahas%20attracted%20much%20attention%20as%20a%20crucial%20task%20of%20legal%20intelligence%2C%20which%0Aaims%20to%20generate%20concise%20and%20coherent%20texts%20that%20summarize%20case%20facts%20and%0Aprovide%20explanations%20for%20verdicts.%20Existing%20researches%20explore%20the%20key%0Ainformation%20in%20case%20facts%20to%20yield%20the%20court%20views.%20Most%20of%20them%20employ%20a%0Acoarse-grained%20approach%20that%20partitions%20the%20facts%20into%20broad%20segments%20%28e.g.%2C%0Averdict-related%20sentences%29%20to%20make%20predictions.%20However%2C%20this%20approach%20fails%20to%0Acapture%20the%20complex%20details%20present%20in%20the%20case%20facts%2C%20such%20as%20various%20criminal%0Aelements%20and%20legal%20events.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20propose%20an%20Event%0AGrounded%20Generation%20%28EGG%29%20method%20for%20criminal%20court%20view%20generation%20with%0Acooperative%20%28Large%29%20Language%20Models%2C%20which%20introduces%20the%20fine-grained%20event%0Ainformation%20into%20the%20generation.%20Specifically%2C%20we%20first%20design%20a%20LLMs-based%0Aextraction%20method%20that%20can%20extract%20events%20in%20case%20facts%20without%20massive%0Aannotated%20events.%20Then%2C%20we%20incorporate%20the%20extracted%20events%20into%20court%20view%0Ageneration%20by%20merging%20case%20facts%20and%20events.%20Besides%2C%20considering%20the%0Acomputational%20burden%20posed%20by%20the%20use%20of%20LLMs%20in%20the%20extraction%20phase%20of%20EGG%2C%0Awe%20propose%20a%20LLMs-free%20EGG%20method%20that%20can%20eliminate%20the%20requirement%20for%20event%0Aextraction%20using%20LLMs%20in%20the%20inference%20phase.%20Extensive%20experimental%20results%20on%0Aa%20real-world%20dataset%20clearly%20validate%20the%20effectiveness%20of%20our%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07001v1&entry.124074799=Read"},
{"title": "Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A\n  Multiobjective Hyperparameter and Architecture Optimization Approach", "author": "Yixuan Sun and Ololade Sowunmi and Romain Egele and Sri Hari Krishna Narayanan and Luke Van Roekel and Prasanna Balaprakash", "abstract": "  Training an effective deep learning model to learn ocean processes involves\ncareful choices of various hyperparameters. We leverage the advanced search\nalgorithms for multiobjective optimization in DeepHyper, a scalable\nhyperparameter optimization software, to streamline the development of neural\nnetworks tailored for ocean modeling. The focus is on optimizing Fourier neural\noperators (FNOs), a data-driven model capable of simulating complex ocean\nbehaviors. Selecting the correct model and tuning the hyperparameters are\nchallenging tasks, requiring much effort to ensure model accuracy. DeepHyper\nallows efficient exploration of hyperparameters associated with data\npreprocessing, FNO architecture-related hyperparameters, and various model\ntraining strategies. We aim to obtain an optimal set of hyperparameters leading\nto the most performant model. Moreover, on top of the commonly used mean\nsquared error for model training, we propose adopting the negative anomaly\ncorrelation coefficient as the additional loss term to improve model\nperformance and investigate the potential trade-off between the two terms. The\nexperimental results show that the optimal set of hyperparameters enhanced\nmodel performance in single timestepping forecasting and greatly exceeded the\nbaseline configuration in the autoregressive rollout for long-horizon\nforecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to\nenhance the use of FNOs in ocean dynamics forecasting, offering a scalable\nsolution with improved precision.\n", "link": "http://arxiv.org/abs/2404.05768v2", "date": "2024-04-10", "relevancy": 1.8978, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4797}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4716}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Streamlining%20Ocean%20Dynamics%20Modeling%20with%20Fourier%20Neural%20Operators%3A%20A%0A%20%20Multiobjective%20Hyperparameter%20and%20Architecture%20Optimization%20Approach&body=Title%3A%20Streamlining%20Ocean%20Dynamics%20Modeling%20with%20Fourier%20Neural%20Operators%3A%20A%0A%20%20Multiobjective%20Hyperparameter%20and%20Architecture%20Optimization%20Approach%0AAuthor%3A%20Yixuan%20Sun%20and%20Ololade%20Sowunmi%20and%20Romain%20Egele%20and%20Sri%20Hari%20Krishna%20Narayanan%20and%20Luke%20Van%20Roekel%20and%20Prasanna%20Balaprakash%0AAbstract%3A%20%20%20Training%20an%20effective%20deep%20learning%20model%20to%20learn%20ocean%20processes%20involves%0Acareful%20choices%20of%20various%20hyperparameters.%20We%20leverage%20the%20advanced%20search%0Aalgorithms%20for%20multiobjective%20optimization%20in%20DeepHyper%2C%20a%20scalable%0Ahyperparameter%20optimization%20software%2C%20to%20streamline%20the%20development%20of%20neural%0Anetworks%20tailored%20for%20ocean%20modeling.%20The%20focus%20is%20on%20optimizing%20Fourier%20neural%0Aoperators%20%28FNOs%29%2C%20a%20data-driven%20model%20capable%20of%20simulating%20complex%20ocean%0Abehaviors.%20Selecting%20the%20correct%20model%20and%20tuning%20the%20hyperparameters%20are%0Achallenging%20tasks%2C%20requiring%20much%20effort%20to%20ensure%20model%20accuracy.%20DeepHyper%0Aallows%20efficient%20exploration%20of%20hyperparameters%20associated%20with%20data%0Apreprocessing%2C%20FNO%20architecture-related%20hyperparameters%2C%20and%20various%20model%0Atraining%20strategies.%20We%20aim%20to%20obtain%20an%20optimal%20set%20of%20hyperparameters%20leading%0Ato%20the%20most%20performant%20model.%20Moreover%2C%20on%20top%20of%20the%20commonly%20used%20mean%0Asquared%20error%20for%20model%20training%2C%20we%20propose%20adopting%20the%20negative%20anomaly%0Acorrelation%20coefficient%20as%20the%20additional%20loss%20term%20to%20improve%20model%0Aperformance%20and%20investigate%20the%20potential%20trade-off%20between%20the%20two%20terms.%20The%0Aexperimental%20results%20show%20that%20the%20optimal%20set%20of%20hyperparameters%20enhanced%0Amodel%20performance%20in%20single%20timestepping%20forecasting%20and%20greatly%20exceeded%20the%0Abaseline%20configuration%20in%20the%20autoregressive%20rollout%20for%20long-horizon%0Aforecasting%20up%20to%2030%20days.%20Utilizing%20DeepHyper%2C%20we%20demonstrate%20an%20approach%20to%0Aenhance%20the%20use%20of%20FNOs%20in%20ocean%20dynamics%20forecasting%2C%20offering%20a%20scalable%0Asolution%20with%20improved%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05768v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamlining%20Ocean%20Dynamics%20Modeling%20with%20Fourier%20Neural%20Operators%3A%20A%0A%20%20Multiobjective%20Hyperparameter%20and%20Architecture%20Optimization%20Approach&entry.906535625=Yixuan%20Sun%20and%20Ololade%20Sowunmi%20and%20Romain%20Egele%20and%20Sri%20Hari%20Krishna%20Narayanan%20and%20Luke%20Van%20Roekel%20and%20Prasanna%20Balaprakash&entry.1292438233=%20%20Training%20an%20effective%20deep%20learning%20model%20to%20learn%20ocean%20processes%20involves%0Acareful%20choices%20of%20various%20hyperparameters.%20We%20leverage%20the%20advanced%20search%0Aalgorithms%20for%20multiobjective%20optimization%20in%20DeepHyper%2C%20a%20scalable%0Ahyperparameter%20optimization%20software%2C%20to%20streamline%20the%20development%20of%20neural%0Anetworks%20tailored%20for%20ocean%20modeling.%20The%20focus%20is%20on%20optimizing%20Fourier%20neural%0Aoperators%20%28FNOs%29%2C%20a%20data-driven%20model%20capable%20of%20simulating%20complex%20ocean%0Abehaviors.%20Selecting%20the%20correct%20model%20and%20tuning%20the%20hyperparameters%20are%0Achallenging%20tasks%2C%20requiring%20much%20effort%20to%20ensure%20model%20accuracy.%20DeepHyper%0Aallows%20efficient%20exploration%20of%20hyperparameters%20associated%20with%20data%0Apreprocessing%2C%20FNO%20architecture-related%20hyperparameters%2C%20and%20various%20model%0Atraining%20strategies.%20We%20aim%20to%20obtain%20an%20optimal%20set%20of%20hyperparameters%20leading%0Ato%20the%20most%20performant%20model.%20Moreover%2C%20on%20top%20of%20the%20commonly%20used%20mean%0Asquared%20error%20for%20model%20training%2C%20we%20propose%20adopting%20the%20negative%20anomaly%0Acorrelation%20coefficient%20as%20the%20additional%20loss%20term%20to%20improve%20model%0Aperformance%20and%20investigate%20the%20potential%20trade-off%20between%20the%20two%20terms.%20The%0Aexperimental%20results%20show%20that%20the%20optimal%20set%20of%20hyperparameters%20enhanced%0Amodel%20performance%20in%20single%20timestepping%20forecasting%20and%20greatly%20exceeded%20the%0Abaseline%20configuration%20in%20the%20autoregressive%20rollout%20for%20long-horizon%0Aforecasting%20up%20to%2030%20days.%20Utilizing%20DeepHyper%2C%20we%20demonstrate%20an%20approach%20to%0Aenhance%20the%20use%20of%20FNOs%20in%20ocean%20dynamics%20forecasting%2C%20offering%20a%20scalable%0Asolution%20with%20improved%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05768v2&entry.124074799=Read"},
{"title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n  Generation", "author": "Thomas Merth and Qichen Fu and Mohammad Rastegari and Mahyar Najibi", "abstract": "  Despite the successes of large language models (LLMs), they exhibit\nsignificant drawbacks, particularly when processing long contexts. Their\ninference cost scales quadratically with respect to sequence length, making it\nexpensive for deployment in some real-world text processing applications, such\nas retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\n\"distraction phenomenon,\" where irrelevant context in the prompt degrades\noutput quality. To address these drawbacks, we propose a novel RAG prompting\nmethodology, superposition prompting, which can be directly applied to\npre-trained transformer-based LLMs without the need for fine-tuning. At a high\nlevel, superposition prompting allows the LLM to process input documents in\nparallel prompt paths, discarding paths once they are deemed irrelevant. We\ndemonstrate the capability of our method to simultaneously enhance time\nefficiency across a variety of question-answering benchmarks using multiple\npre-trained LLMs. Furthermore, our technique significantly improves accuracy\nwhen the retrieved context is large relative the context the model was trained\non. For example, our approach facilitates an 93x reduction in compute time\nwhile improving accuracy by 43\\% on the NaturalQuestions-Open dataset with the\nMPT-7B instruction-tuned model over naive RAG.\n", "link": "http://arxiv.org/abs/2404.06910v1", "date": "2024-04-10", "relevancy": 1.8776, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4798}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4766}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4561}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation&body=Title%3A%20Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation%0AAuthor%3A%20Thomas%20Merth%20and%20Qichen%20Fu%20and%20Mohammad%20Rastegari%20and%20Mahyar%20Najibi%0AAbstract%3A%20%20%20Despite%20the%20successes%20of%20large%20language%20models%20%28LLMs%29%2C%20they%20exhibit%0Asignificant%20drawbacks%2C%20particularly%20when%20processing%20long%20contexts.%20Their%0Ainference%20cost%20scales%20quadratically%20with%20respect%20to%20sequence%20length%2C%20making%20it%0Aexpensive%20for%20deployment%20in%20some%20real-world%20text%20processing%20applications%2C%20such%0Aas%20retrieval-augmented%20generation%20%28RAG%29.%20Additionally%2C%20LLMs%20also%20exhibit%20the%0A%22distraction%20phenomenon%2C%22%20where%20irrelevant%20context%20in%20the%20prompt%20degrades%0Aoutput%20quality.%20To%20address%20these%20drawbacks%2C%20we%20propose%20a%20novel%20RAG%20prompting%0Amethodology%2C%20superposition%20prompting%2C%20which%20can%20be%20directly%20applied%20to%0Apre-trained%20transformer-based%20LLMs%20without%20the%20need%20for%20fine-tuning.%20At%20a%20high%0Alevel%2C%20superposition%20prompting%20allows%20the%20LLM%20to%20process%20input%20documents%20in%0Aparallel%20prompt%20paths%2C%20discarding%20paths%20once%20they%20are%20deemed%20irrelevant.%20We%0Ademonstrate%20the%20capability%20of%20our%20method%20to%20simultaneously%20enhance%20time%0Aefficiency%20across%20a%20variety%20of%20question-answering%20benchmarks%20using%20multiple%0Apre-trained%20LLMs.%20Furthermore%2C%20our%20technique%20significantly%20improves%20accuracy%0Awhen%20the%20retrieved%20context%20is%20large%20relative%20the%20context%20the%20model%20was%20trained%0Aon.%20For%20example%2C%20our%20approach%20facilitates%20an%2093x%20reduction%20in%20compute%20time%0Awhile%20improving%20accuracy%20by%2043%5C%25%20on%20the%20NaturalQuestions-Open%20dataset%20with%20the%0AMPT-7B%20instruction-tuned%20model%20over%20naive%20RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06910v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superposition%20Prompting%3A%20Improving%20and%20Accelerating%20Retrieval-Augmented%0A%20%20Generation&entry.906535625=Thomas%20Merth%20and%20Qichen%20Fu%20and%20Mohammad%20Rastegari%20and%20Mahyar%20Najibi&entry.1292438233=%20%20Despite%20the%20successes%20of%20large%20language%20models%20%28LLMs%29%2C%20they%20exhibit%0Asignificant%20drawbacks%2C%20particularly%20when%20processing%20long%20contexts.%20Their%0Ainference%20cost%20scales%20quadratically%20with%20respect%20to%20sequence%20length%2C%20making%20it%0Aexpensive%20for%20deployment%20in%20some%20real-world%20text%20processing%20applications%2C%20such%0Aas%20retrieval-augmented%20generation%20%28RAG%29.%20Additionally%2C%20LLMs%20also%20exhibit%20the%0A%22distraction%20phenomenon%2C%22%20where%20irrelevant%20context%20in%20the%20prompt%20degrades%0Aoutput%20quality.%20To%20address%20these%20drawbacks%2C%20we%20propose%20a%20novel%20RAG%20prompting%0Amethodology%2C%20superposition%20prompting%2C%20which%20can%20be%20directly%20applied%20to%0Apre-trained%20transformer-based%20LLMs%20without%20the%20need%20for%20fine-tuning.%20At%20a%20high%0Alevel%2C%20superposition%20prompting%20allows%20the%20LLM%20to%20process%20input%20documents%20in%0Aparallel%20prompt%20paths%2C%20discarding%20paths%20once%20they%20are%20deemed%20irrelevant.%20We%0Ademonstrate%20the%20capability%20of%20our%20method%20to%20simultaneously%20enhance%20time%0Aefficiency%20across%20a%20variety%20of%20question-answering%20benchmarks%20using%20multiple%0Apre-trained%20LLMs.%20Furthermore%2C%20our%20technique%20significantly%20improves%20accuracy%0Awhen%20the%20retrieved%20context%20is%20large%20relative%20the%20context%20the%20model%20was%20trained%0Aon.%20For%20example%2C%20our%20approach%20facilitates%20an%2093x%20reduction%20in%20compute%20time%0Awhile%20improving%20accuracy%20by%2043%5C%25%20on%20the%20NaturalQuestions-Open%20dataset%20with%20the%0AMPT-7B%20instruction-tuned%20model%20over%20naive%20RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06910v1&entry.124074799=Read"},
{"title": "Semantically-correlated memories in a dense associative model", "author": "Thomas F Burns", "abstract": "  I introduce a novel associative memory model named Correlated Dense\nAssociative Memory (CDAM), which integrates both auto- and hetero-association\nin a unified framework for continuous-valued memory patterns. Employing an\narbitrary graph structure to semantically link memory patterns, CDAM is\ntheoretically and numerically analysed, revealing four distinct dynamical\nmodes: auto-association, narrow hetero-association, wide hetero-association,\nand neutral quiescence. Drawing inspiration from inhibitory modulation studies,\nI employ anti-Hebbian learning rules to control the range of\nhetero-association, extract multi-scale representations of community structures\nin graphs, and stabilise the recall of temporal sequences. Experimental\ndemonstrations showcase CDAM's efficacy in handling real-world data,\nreplicating a classical neuroscience experiment, performing image retrieval,\nand simulating arbitrary finite automata.\n", "link": "http://arxiv.org/abs/2404.07123v1", "date": "2024-04-10", "relevancy": 1.8771, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4873}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4402}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantically-correlated%20memories%20in%20a%20dense%20associative%20model&body=Title%3A%20Semantically-correlated%20memories%20in%20a%20dense%20associative%20model%0AAuthor%3A%20Thomas%20F%20Burns%0AAbstract%3A%20%20%20I%20introduce%20a%20novel%20associative%20memory%20model%20named%20Correlated%20Dense%0AAssociative%20Memory%20%28CDAM%29%2C%20which%20integrates%20both%20auto-%20and%20hetero-association%0Ain%20a%20unified%20framework%20for%20continuous-valued%20memory%20patterns.%20Employing%20an%0Aarbitrary%20graph%20structure%20to%20semantically%20link%20memory%20patterns%2C%20CDAM%20is%0Atheoretically%20and%20numerically%20analysed%2C%20revealing%20four%20distinct%20dynamical%0Amodes%3A%20auto-association%2C%20narrow%20hetero-association%2C%20wide%20hetero-association%2C%0Aand%20neutral%20quiescence.%20Drawing%20inspiration%20from%20inhibitory%20modulation%20studies%2C%0AI%20employ%20anti-Hebbian%20learning%20rules%20to%20control%20the%20range%20of%0Ahetero-association%2C%20extract%20multi-scale%20representations%20of%20community%20structures%0Ain%20graphs%2C%20and%20stabilise%20the%20recall%20of%20temporal%20sequences.%20Experimental%0Ademonstrations%20showcase%20CDAM%27s%20efficacy%20in%20handling%20real-world%20data%2C%0Areplicating%20a%20classical%20neuroscience%20experiment%2C%20performing%20image%20retrieval%2C%0Aand%20simulating%20arbitrary%20finite%20automata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07123v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically-correlated%20memories%20in%20a%20dense%20associative%20model&entry.906535625=Thomas%20F%20Burns&entry.1292438233=%20%20I%20introduce%20a%20novel%20associative%20memory%20model%20named%20Correlated%20Dense%0AAssociative%20Memory%20%28CDAM%29%2C%20which%20integrates%20both%20auto-%20and%20hetero-association%0Ain%20a%20unified%20framework%20for%20continuous-valued%20memory%20patterns.%20Employing%20an%0Aarbitrary%20graph%20structure%20to%20semantically%20link%20memory%20patterns%2C%20CDAM%20is%0Atheoretically%20and%20numerically%20analysed%2C%20revealing%20four%20distinct%20dynamical%0Amodes%3A%20auto-association%2C%20narrow%20hetero-association%2C%20wide%20hetero-association%2C%0Aand%20neutral%20quiescence.%20Drawing%20inspiration%20from%20inhibitory%20modulation%20studies%2C%0AI%20employ%20anti-Hebbian%20learning%20rules%20to%20control%20the%20range%20of%0Ahetero-association%2C%20extract%20multi-scale%20representations%20of%20community%20structures%0Ain%20graphs%2C%20and%20stabilise%20the%20recall%20of%20temporal%20sequences.%20Experimental%0Ademonstrations%20showcase%20CDAM%27s%20efficacy%20in%20handling%20real-world%20data%2C%0Areplicating%20a%20classical%20neuroscience%20experiment%2C%20performing%20image%20retrieval%2C%0Aand%20simulating%20arbitrary%20finite%20automata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07123v1&entry.124074799=Read"},
{"title": "Fast System Technology Co-Optimization Framework for Emerging Technology\n  Based on Graph Neural Networks", "author": "Tianliang Ma and Guangxi Fan and Xuguang Sun and Zhihui Deng and Kainlu Low and Leilai Shao", "abstract": "  This paper proposes a fast system technology co-optimization (STCO) framework\nthat optimizes power, performance, and area (PPA) for next-generation IC\ndesign, addressing the challenges and opportunities presented by novel\nmaterials and device architectures. We focus on accelerating the technology\nlevel of STCO using AI techniques, by employing graph neural network\n(GNN)-based approaches for both TCAD simulation and cell library\ncharacterization, which are interconnected through a unified compact model,\ncollectively achieving over a 100X speedup over traditional methods. These\nadvancements enable comprehensive STCO iterations with runtime speedups ranging\nfrom 1.9X to 14.1X and supports both emerging and traditional technologies.\n", "link": "http://arxiv.org/abs/2404.06939v1", "date": "2024-04-10", "relevancy": 1.8771, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4524}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4512}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20System%20Technology%20Co-Optimization%20Framework%20for%20Emerging%20Technology%0A%20%20Based%20on%20Graph%20Neural%20Networks&body=Title%3A%20Fast%20System%20Technology%20Co-Optimization%20Framework%20for%20Emerging%20Technology%0A%20%20Based%20on%20Graph%20Neural%20Networks%0AAuthor%3A%20Tianliang%20Ma%20and%20Guangxi%20Fan%20and%20Xuguang%20Sun%20and%20Zhihui%20Deng%20and%20Kainlu%20Low%20and%20Leilai%20Shao%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20fast%20system%20technology%20co-optimization%20%28STCO%29%20framework%0Athat%20optimizes%20power%2C%20performance%2C%20and%20area%20%28PPA%29%20for%20next-generation%20IC%0Adesign%2C%20addressing%20the%20challenges%20and%20opportunities%20presented%20by%20novel%0Amaterials%20and%20device%20architectures.%20We%20focus%20on%20accelerating%20the%20technology%0Alevel%20of%20STCO%20using%20AI%20techniques%2C%20by%20employing%20graph%20neural%20network%0A%28GNN%29-based%20approaches%20for%20both%20TCAD%20simulation%20and%20cell%20library%0Acharacterization%2C%20which%20are%20interconnected%20through%20a%20unified%20compact%20model%2C%0Acollectively%20achieving%20over%20a%20100X%20speedup%20over%20traditional%20methods.%20These%0Aadvancements%20enable%20comprehensive%20STCO%20iterations%20with%20runtime%20speedups%20ranging%0Afrom%201.9X%20to%2014.1X%20and%20supports%20both%20emerging%20and%20traditional%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06939v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20System%20Technology%20Co-Optimization%20Framework%20for%20Emerging%20Technology%0A%20%20Based%20on%20Graph%20Neural%20Networks&entry.906535625=Tianliang%20Ma%20and%20Guangxi%20Fan%20and%20Xuguang%20Sun%20and%20Zhihui%20Deng%20and%20Kainlu%20Low%20and%20Leilai%20Shao&entry.1292438233=%20%20This%20paper%20proposes%20a%20fast%20system%20technology%20co-optimization%20%28STCO%29%20framework%0Athat%20optimizes%20power%2C%20performance%2C%20and%20area%20%28PPA%29%20for%20next-generation%20IC%0Adesign%2C%20addressing%20the%20challenges%20and%20opportunities%20presented%20by%20novel%0Amaterials%20and%20device%20architectures.%20We%20focus%20on%20accelerating%20the%20technology%0Alevel%20of%20STCO%20using%20AI%20techniques%2C%20by%20employing%20graph%20neural%20network%0A%28GNN%29-based%20approaches%20for%20both%20TCAD%20simulation%20and%20cell%20library%0Acharacterization%2C%20which%20are%20interconnected%20through%20a%20unified%20compact%20model%2C%0Acollectively%20achieving%20over%20a%20100X%20speedup%20over%20traditional%20methods.%20These%0Aadvancements%20enable%20comprehensive%20STCO%20iterations%20with%20runtime%20speedups%20ranging%0Afrom%201.9X%20to%2014.1X%20and%20supports%20both%20emerging%20and%20traditional%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06939v1&entry.124074799=Read"},
{"title": "Towards Learning Stochastic Population Models by Gradient Descent", "author": "Justin N. Kreikemeyer and Philipp Andelfinger and Adelinde M. Uhrmacher", "abstract": "  Increasing effort is put into the development of methods for learning\nmechanistic models from data. This task entails not only the accurate\nestimation of parameters, but also a suitable model structure. Recent work on\nthe discovery of dynamical systems formulates this problem as a linear equation\nsystem. Here, we explore several simulation-based optimization approaches,\nwhich allow much greater freedom in the objective formulation and weaker\nconditions on the available data. We show that even for relatively small\nstochastic population models, simultaneous estimation of parameters and\nstructure poses major challenges for optimization procedures. Particularly, we\ninvestigate the application of the local stochastic gradient descent method,\ncommonly used for training machine learning models. We demonstrate accurate\nestimation of models but find that enforcing the inference of parsimonious,\ninterpretable models drastically increases the difficulty. We give an outlook\non how this challenge can be overcome.\n", "link": "http://arxiv.org/abs/2404.07049v1", "date": "2024-04-10", "relevancy": 1.8762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4748}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.468}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4678}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent&body=Title%3A%20Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent%0AAuthor%3A%20Justin%20N.%20Kreikemeyer%20and%20Philipp%20Andelfinger%20and%20Adelinde%20M.%20Uhrmacher%0AAbstract%3A%20%20%20Increasing%20effort%20is%20put%20into%20the%20development%20of%20methods%20for%20learning%0Amechanistic%20models%20from%20data.%20This%20task%20entails%20not%20only%20the%20accurate%0Aestimation%20of%20parameters%2C%20but%20also%20a%20suitable%20model%20structure.%20Recent%20work%20on%0Athe%20discovery%20of%20dynamical%20systems%20formulates%20this%20problem%20as%20a%20linear%20equation%0Asystem.%20Here%2C%20we%20explore%20several%20simulation-based%20optimization%20approaches%2C%0Awhich%20allow%20much%20greater%20freedom%20in%20the%20objective%20formulation%20and%20weaker%0Aconditions%20on%20the%20available%20data.%20We%20show%20that%20even%20for%20relatively%20small%0Astochastic%20population%20models%2C%20simultaneous%20estimation%20of%20parameters%20and%0Astructure%20poses%20major%20challenges%20for%20optimization%20procedures.%20Particularly%2C%20we%0Ainvestigate%20the%20application%20of%20the%20local%20stochastic%20gradient%20descent%20method%2C%0Acommonly%20used%20for%20training%20machine%20learning%20models.%20We%20demonstrate%20accurate%0Aestimation%20of%20models%20but%20find%20that%20enforcing%20the%20inference%20of%20parsimonious%2C%0Ainterpretable%20models%20drastically%20increases%20the%20difficulty.%20We%20give%20an%20outlook%0Aon%20how%20this%20challenge%20can%20be%20overcome.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07049v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Learning%20Stochastic%20Population%20Models%20by%20Gradient%20Descent&entry.906535625=Justin%20N.%20Kreikemeyer%20and%20Philipp%20Andelfinger%20and%20Adelinde%20M.%20Uhrmacher&entry.1292438233=%20%20Increasing%20effort%20is%20put%20into%20the%20development%20of%20methods%20for%20learning%0Amechanistic%20models%20from%20data.%20This%20task%20entails%20not%20only%20the%20accurate%0Aestimation%20of%20parameters%2C%20but%20also%20a%20suitable%20model%20structure.%20Recent%20work%20on%0Athe%20discovery%20of%20dynamical%20systems%20formulates%20this%20problem%20as%20a%20linear%20equation%0Asystem.%20Here%2C%20we%20explore%20several%20simulation-based%20optimization%20approaches%2C%0Awhich%20allow%20much%20greater%20freedom%20in%20the%20objective%20formulation%20and%20weaker%0Aconditions%20on%20the%20available%20data.%20We%20show%20that%20even%20for%20relatively%20small%0Astochastic%20population%20models%2C%20simultaneous%20estimation%20of%20parameters%20and%0Astructure%20poses%20major%20challenges%20for%20optimization%20procedures.%20Particularly%2C%20we%0Ainvestigate%20the%20application%20of%20the%20local%20stochastic%20gradient%20descent%20method%2C%0Acommonly%20used%20for%20training%20machine%20learning%20models.%20We%20demonstrate%20accurate%0Aestimation%20of%20models%20but%20find%20that%20enforcing%20the%20inference%20of%20parsimonious%2C%0Ainterpretable%20models%20drastically%20increases%20the%20difficulty.%20We%20give%20an%20outlook%0Aon%20how%20this%20challenge%20can%20be%20overcome.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07049v1&entry.124074799=Read"},
{"title": "The CAST package for training and assessment of spatial prediction\n  models in R", "author": "Hanna Meyer and Marvin Ludwig and Carles Mil\u00e0 and Jan Linnenbrink and Fabian Schumacher", "abstract": "  One key task in environmental science is to map environmental variables\ncontinuously in space or even in space and time. Machine learning algorithms\nare frequently used to learn from local field observations to make spatial\npredictions by estimating the value of the variable of interest in places where\nit has not been measured. However, the application of machine learning\nstrategies for spatial mapping involves additional challenges compared to\n\"non-spatial\" prediction tasks that often originate from spatial\nautocorrelation and from training data that are not independent and identically\ndistributed.\n  In the past few years, we developed a number of methods to support the\napplication of machine learning for spatial data which involves the development\nof suitable cross-validation strategies for performance assessment and model\nselection, spatial feature selection, and methods to assess the area of\napplicability of the trained models. The intention of the CAST package is to\nsupport the application of machine learning strategies for predictive mapping\nby implementing such methods and making them available for easy integration\ninto modelling workflows.\n  Here we introduce the CAST package and its core functionalities. At the case\nstudy of mapping plant species richness, we will go through the different steps\nof the modelling workflow and show how CAST can be used to support more\nreliable spatial predictions.\n", "link": "http://arxiv.org/abs/2404.06978v1", "date": "2024-04-10", "relevancy": 1.857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4591}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20CAST%20package%20for%20training%20and%20assessment%20of%20spatial%20prediction%0A%20%20models%20in%20R&body=Title%3A%20The%20CAST%20package%20for%20training%20and%20assessment%20of%20spatial%20prediction%0A%20%20models%20in%20R%0AAuthor%3A%20Hanna%20Meyer%20and%20Marvin%20Ludwig%20and%20Carles%20Mil%C3%A0%20and%20Jan%20Linnenbrink%20and%20Fabian%20Schumacher%0AAbstract%3A%20%20%20One%20key%20task%20in%20environmental%20science%20is%20to%20map%20environmental%20variables%0Acontinuously%20in%20space%20or%20even%20in%20space%20and%20time.%20Machine%20learning%20algorithms%0Aare%20frequently%20used%20to%20learn%20from%20local%20field%20observations%20to%20make%20spatial%0Apredictions%20by%20estimating%20the%20value%20of%20the%20variable%20of%20interest%20in%20places%20where%0Ait%20has%20not%20been%20measured.%20However%2C%20the%20application%20of%20machine%20learning%0Astrategies%20for%20spatial%20mapping%20involves%20additional%20challenges%20compared%20to%0A%22non-spatial%22%20prediction%20tasks%20that%20often%20originate%20from%20spatial%0Aautocorrelation%20and%20from%20training%20data%20that%20are%20not%20independent%20and%20identically%0Adistributed.%0A%20%20In%20the%20past%20few%20years%2C%20we%20developed%20a%20number%20of%20methods%20to%20support%20the%0Aapplication%20of%20machine%20learning%20for%20spatial%20data%20which%20involves%20the%20development%0Aof%20suitable%20cross-validation%20strategies%20for%20performance%20assessment%20and%20model%0Aselection%2C%20spatial%20feature%20selection%2C%20and%20methods%20to%20assess%20the%20area%20of%0Aapplicability%20of%20the%20trained%20models.%20The%20intention%20of%20the%20CAST%20package%20is%20to%0Asupport%20the%20application%20of%20machine%20learning%20strategies%20for%20predictive%20mapping%0Aby%20implementing%20such%20methods%20and%20making%20them%20available%20for%20easy%20integration%0Ainto%20modelling%20workflows.%0A%20%20Here%20we%20introduce%20the%20CAST%20package%20and%20its%20core%20functionalities.%20At%20the%20case%0Astudy%20of%20mapping%20plant%20species%20richness%2C%20we%20will%20go%20through%20the%20different%20steps%0Aof%20the%20modelling%20workflow%20and%20show%20how%20CAST%20can%20be%20used%20to%20support%20more%0Areliable%20spatial%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06978v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20CAST%20package%20for%20training%20and%20assessment%20of%20spatial%20prediction%0A%20%20models%20in%20R&entry.906535625=Hanna%20Meyer%20and%20Marvin%20Ludwig%20and%20Carles%20Mil%C3%A0%20and%20Jan%20Linnenbrink%20and%20Fabian%20Schumacher&entry.1292438233=%20%20One%20key%20task%20in%20environmental%20science%20is%20to%20map%20environmental%20variables%0Acontinuously%20in%20space%20or%20even%20in%20space%20and%20time.%20Machine%20learning%20algorithms%0Aare%20frequently%20used%20to%20learn%20from%20local%20field%20observations%20to%20make%20spatial%0Apredictions%20by%20estimating%20the%20value%20of%20the%20variable%20of%20interest%20in%20places%20where%0Ait%20has%20not%20been%20measured.%20However%2C%20the%20application%20of%20machine%20learning%0Astrategies%20for%20spatial%20mapping%20involves%20additional%20challenges%20compared%20to%0A%22non-spatial%22%20prediction%20tasks%20that%20often%20originate%20from%20spatial%0Aautocorrelation%20and%20from%20training%20data%20that%20are%20not%20independent%20and%20identically%0Adistributed.%0A%20%20In%20the%20past%20few%20years%2C%20we%20developed%20a%20number%20of%20methods%20to%20support%20the%0Aapplication%20of%20machine%20learning%20for%20spatial%20data%20which%20involves%20the%20development%0Aof%20suitable%20cross-validation%20strategies%20for%20performance%20assessment%20and%20model%0Aselection%2C%20spatial%20feature%20selection%2C%20and%20methods%20to%20assess%20the%20area%20of%0Aapplicability%20of%20the%20trained%20models.%20The%20intention%20of%20the%20CAST%20package%20is%20to%0Asupport%20the%20application%20of%20machine%20learning%20strategies%20for%20predictive%20mapping%0Aby%20implementing%20such%20methods%20and%20making%20them%20available%20for%20easy%20integration%0Ainto%20modelling%20workflows.%0A%20%20Here%20we%20introduce%20the%20CAST%20package%20and%20its%20core%20functionalities.%20At%20the%20case%0Astudy%20of%20mapping%20plant%20species%20richness%2C%20we%20will%20go%20through%20the%20different%20steps%0Aof%20the%20modelling%20workflow%20and%20show%20how%20CAST%20can%20be%20used%20to%20support%20more%0Areliable%20spatial%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06978v1&entry.124074799=Read"},
{"title": "Is Learning in Biological Neural Networks based on Stochastic Gradient\n  Descent? An analysis using stochastic processes", "author": "S\u00f6ren Christensen and Jan Kallsen", "abstract": "  In recent years, there has been an intense debate about how learning in\nbiological neural networks (BNNs) differs from learning in artificial neural\nnetworks. It is often argued that the updating of connections in the brain\nrelies only on local information, and therefore a stochastic gradient-descent\ntype optimization method cannot be used. In this paper, we study a stochastic\nmodel for supervised learning in BNNs. We show that a (continuous) gradient\nstep occurs approximately when each learning opportunity is processed by many\nlocal updates. This result suggests that stochastic gradient descent may indeed\nplay a role in optimizing BNNs.\n", "link": "http://arxiv.org/abs/2309.05102v3", "date": "2024-04-10", "relevancy": 1.8451, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4688}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4204}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Is%20Learning%20in%20Biological%20Neural%20Networks%20based%20on%20Stochastic%20Gradient%0A%20%20Descent%3F%20An%20analysis%20using%20stochastic%20processes&body=Title%3A%20Is%20Learning%20in%20Biological%20Neural%20Networks%20based%20on%20Stochastic%20Gradient%0A%20%20Descent%3F%20An%20analysis%20using%20stochastic%20processes%0AAuthor%3A%20S%C3%B6ren%20Christensen%20and%20Jan%20Kallsen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20an%20intense%20debate%20about%20how%20learning%20in%0Abiological%20neural%20networks%20%28BNNs%29%20differs%20from%20learning%20in%20artificial%20neural%0Anetworks.%20It%20is%20often%20argued%20that%20the%20updating%20of%20connections%20in%20the%20brain%0Arelies%20only%20on%20local%20information%2C%20and%20therefore%20a%20stochastic%20gradient-descent%0Atype%20optimization%20method%20cannot%20be%20used.%20In%20this%20paper%2C%20we%20study%20a%20stochastic%0Amodel%20for%20supervised%20learning%20in%20BNNs.%20We%20show%20that%20a%20%28continuous%29%20gradient%0Astep%20occurs%20approximately%20when%20each%20learning%20opportunity%20is%20processed%20by%20many%0Alocal%20updates.%20This%20result%20suggests%20that%20stochastic%20gradient%20descent%20may%20indeed%0Aplay%20a%20role%20in%20optimizing%20BNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05102v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Learning%20in%20Biological%20Neural%20Networks%20based%20on%20Stochastic%20Gradient%0A%20%20Descent%3F%20An%20analysis%20using%20stochastic%20processes&entry.906535625=S%C3%B6ren%20Christensen%20and%20Jan%20Kallsen&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20an%20intense%20debate%20about%20how%20learning%20in%0Abiological%20neural%20networks%20%28BNNs%29%20differs%20from%20learning%20in%20artificial%20neural%0Anetworks.%20It%20is%20often%20argued%20that%20the%20updating%20of%20connections%20in%20the%20brain%0Arelies%20only%20on%20local%20information%2C%20and%20therefore%20a%20stochastic%20gradient-descent%0Atype%20optimization%20method%20cannot%20be%20used.%20In%20this%20paper%2C%20we%20study%20a%20stochastic%0Amodel%20for%20supervised%20learning%20in%20BNNs.%20We%20show%20that%20a%20%28continuous%29%20gradient%0Astep%20occurs%20approximately%20when%20each%20learning%20opportunity%20is%20processed%20by%20many%0Alocal%20updates.%20This%20result%20suggests%20that%20stochastic%20gradient%20descent%20may%20indeed%0Aplay%20a%20role%20in%20optimizing%20BNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05102v3&entry.124074799=Read"},
{"title": "Global $\\mathcal{L}^2$ minimization at uniform exponential rate via\n  geometrically adapted gradient descent in Deep Learning", "author": "Thomas Chen", "abstract": "  We consider the scenario of supervised learning in Deep Learning (DL)\nnetworks, and exploit the arbitrariness of choice in the Riemannian metric\nrelative to which the gradient descent flow can be defined (a general fact of\ndifferential geometry). In the standard approach to DL, the gradient flow on\nthe space of parameters (weights and biases) is defined with respect to the\nEuclidean metric. Here instead, we choose the gradient flow with respect to the\nEuclidean metric in the output layer of the DL network. This naturally induces\ntwo modified versions of the gradient descent flow in the parameter space, one\nadapted for the overparametrized setting, and the other for the\nunderparametrized setting. In the overparametrized case, we prove that,\nprovided that a rank condition holds, all orbits of the modified gradient\ndescent drive the ${\\mathcal L}^2$ cost to its global minimum at a uniform\nexponential convergence rate; one thereby obtains an a priori stopping time for\nany prescribed proximity to the global minimum. We point out relations of the\nlatter to sub-Riemannian geometry. Moreover, we generalize the above framework\nto the situation in which the rank condition does not hold; in particular, we\nshow that local equilibria can only exist if a rank loss occurs, and that\ngenerically, they are not isolated points, but elements of a critical\nsubmanifold of parameter space.\n", "link": "http://arxiv.org/abs/2311.15487v4", "date": "2024-04-10", "relevancy": 1.842, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4781}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4482}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4479}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Global%20%24%5Cmathcal%7BL%7D%5E2%24%20minimization%20at%20uniform%20exponential%20rate%20via%0A%20%20geometrically%20adapted%20gradient%20descent%20in%20Deep%20Learning&body=Title%3A%20Global%20%24%5Cmathcal%7BL%7D%5E2%24%20minimization%20at%20uniform%20exponential%20rate%20via%0A%20%20geometrically%20adapted%20gradient%20descent%20in%20Deep%20Learning%0AAuthor%3A%20Thomas%20Chen%0AAbstract%3A%20%20%20We%20consider%20the%20scenario%20of%20supervised%20learning%20in%20Deep%20Learning%20%28DL%29%0Anetworks%2C%20and%20exploit%20the%20arbitrariness%20of%20choice%20in%20the%20Riemannian%20metric%0Arelative%20to%20which%20the%20gradient%20descent%20flow%20can%20be%20defined%20%28a%20general%20fact%20of%0Adifferential%20geometry%29.%20In%20the%20standard%20approach%20to%20DL%2C%20the%20gradient%20flow%20on%0Athe%20space%20of%20parameters%20%28weights%20and%20biases%29%20is%20defined%20with%20respect%20to%20the%0AEuclidean%20metric.%20Here%20instead%2C%20we%20choose%20the%20gradient%20flow%20with%20respect%20to%20the%0AEuclidean%20metric%20in%20the%20output%20layer%20of%20the%20DL%20network.%20This%20naturally%20induces%0Atwo%20modified%20versions%20of%20the%20gradient%20descent%20flow%20in%20the%20parameter%20space%2C%20one%0Aadapted%20for%20the%20overparametrized%20setting%2C%20and%20the%20other%20for%20the%0Aunderparametrized%20setting.%20In%20the%20overparametrized%20case%2C%20we%20prove%20that%2C%0Aprovided%20that%20a%20rank%20condition%20holds%2C%20all%20orbits%20of%20the%20modified%20gradient%0Adescent%20drive%20the%20%24%7B%5Cmathcal%20L%7D%5E2%24%20cost%20to%20its%20global%20minimum%20at%20a%20uniform%0Aexponential%20convergence%20rate%3B%20one%20thereby%20obtains%20an%20a%20priori%20stopping%20time%20for%0Aany%20prescribed%20proximity%20to%20the%20global%20minimum.%20We%20point%20out%20relations%20of%20the%0Alatter%20to%20sub-Riemannian%20geometry.%20Moreover%2C%20we%20generalize%20the%20above%20framework%0Ato%20the%20situation%20in%20which%20the%20rank%20condition%20does%20not%20hold%3B%20in%20particular%2C%20we%0Ashow%20that%20local%20equilibria%20can%20only%20exist%20if%20a%20rank%20loss%20occurs%2C%20and%20that%0Agenerically%2C%20they%20are%20not%20isolated%20points%2C%20but%20elements%20of%20a%20critical%0Asubmanifold%20of%20parameter%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15487v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20%24%5Cmathcal%7BL%7D%5E2%24%20minimization%20at%20uniform%20exponential%20rate%20via%0A%20%20geometrically%20adapted%20gradient%20descent%20in%20Deep%20Learning&entry.906535625=Thomas%20Chen&entry.1292438233=%20%20We%20consider%20the%20scenario%20of%20supervised%20learning%20in%20Deep%20Learning%20%28DL%29%0Anetworks%2C%20and%20exploit%20the%20arbitrariness%20of%20choice%20in%20the%20Riemannian%20metric%0Arelative%20to%20which%20the%20gradient%20descent%20flow%20can%20be%20defined%20%28a%20general%20fact%20of%0Adifferential%20geometry%29.%20In%20the%20standard%20approach%20to%20DL%2C%20the%20gradient%20flow%20on%0Athe%20space%20of%20parameters%20%28weights%20and%20biases%29%20is%20defined%20with%20respect%20to%20the%0AEuclidean%20metric.%20Here%20instead%2C%20we%20choose%20the%20gradient%20flow%20with%20respect%20to%20the%0AEuclidean%20metric%20in%20the%20output%20layer%20of%20the%20DL%20network.%20This%20naturally%20induces%0Atwo%20modified%20versions%20of%20the%20gradient%20descent%20flow%20in%20the%20parameter%20space%2C%20one%0Aadapted%20for%20the%20overparametrized%20setting%2C%20and%20the%20other%20for%20the%0Aunderparametrized%20setting.%20In%20the%20overparametrized%20case%2C%20we%20prove%20that%2C%0Aprovided%20that%20a%20rank%20condition%20holds%2C%20all%20orbits%20of%20the%20modified%20gradient%0Adescent%20drive%20the%20%24%7B%5Cmathcal%20L%7D%5E2%24%20cost%20to%20its%20global%20minimum%20at%20a%20uniform%0Aexponential%20convergence%20rate%3B%20one%20thereby%20obtains%20an%20a%20priori%20stopping%20time%20for%0Aany%20prescribed%20proximity%20to%20the%20global%20minimum.%20We%20point%20out%20relations%20of%20the%0Alatter%20to%20sub-Riemannian%20geometry.%20Moreover%2C%20we%20generalize%20the%20above%20framework%0Ato%20the%20situation%20in%20which%20the%20rank%20condition%20does%20not%20hold%3B%20in%20particular%2C%20we%0Ashow%20that%20local%20equilibria%20can%20only%20exist%20if%20a%20rank%20loss%20occurs%2C%20and%20that%0Agenerically%2C%20they%20are%20not%20isolated%20points%2C%20but%20elements%20of%20a%20critical%0Asubmanifold%20of%20parameter%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15487v4&entry.124074799=Read"},
{"title": "Are EEG Sequences Time Series? EEG Classification with Time Series\n  Models and Joint Subject Training", "author": "Johannes Burchert and Thorben Werner and Vijaya Krishna Yalavarthi and Diego Coello de Portugal and Maximilian Stubbemann and Lars Schmidt-Thieme", "abstract": "  As with most other data domains, EEG data analysis relies on rich\ndomain-specific preprocessing. Beyond such preprocessing, machine learners\nwould hope to deal with such data as with any other time series data. For EEG\nclassification many models have been developed with layer types and\narchitectures we typically do not see in time series classification.\nFurthermore, typically separate models for each individual subject are learned,\nnot one model for all of them. In this paper, we systematically study the\ndifferences between EEG classification models and generic time series\nclassification models. We describe three different model setups to deal with\nEEG data from different subjects, subject-specific models (most EEG\nliterature), subject-agnostic models and subject-conditional models. In\nexperiments on three datasets, we demonstrate that off-the-shelf time series\nclassification models trained per subject perform close to EEG classification\nmodels, but that do not quite reach the performance of domain-specific\nmodeling. Additionally, we combine time-series models with subject embeddings\nto train one joint subject-conditional classifier on all subjects. The\nresulting models are competitive with dedicated EEG models in 2 out of 3\ndatasets, even outperforming all EEG methods on one of them.\n", "link": "http://arxiv.org/abs/2404.06966v1", "date": "2024-04-10", "relevancy": 1.841, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4743}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.454}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4407}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20EEG%20Sequences%20Time%20Series%3F%20EEG%20Classification%20with%20Time%20Series%0A%20%20Models%20and%20Joint%20Subject%20Training&body=Title%3A%20Are%20EEG%20Sequences%20Time%20Series%3F%20EEG%20Classification%20with%20Time%20Series%0A%20%20Models%20and%20Joint%20Subject%20Training%0AAuthor%3A%20Johannes%20Burchert%20and%20Thorben%20Werner%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Diego%20Coello%20de%20Portugal%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20%20%20As%20with%20most%20other%20data%20domains%2C%20EEG%20data%20analysis%20relies%20on%20rich%0Adomain-specific%20preprocessing.%20Beyond%20such%20preprocessing%2C%20machine%20learners%0Awould%20hope%20to%20deal%20with%20such%20data%20as%20with%20any%20other%20time%20series%20data.%20For%20EEG%0Aclassification%20many%20models%20have%20been%20developed%20with%20layer%20types%20and%0Aarchitectures%20we%20typically%20do%20not%20see%20in%20time%20series%20classification.%0AFurthermore%2C%20typically%20separate%20models%20for%20each%20individual%20subject%20are%20learned%2C%0Anot%20one%20model%20for%20all%20of%20them.%20In%20this%20paper%2C%20we%20systematically%20study%20the%0Adifferences%20between%20EEG%20classification%20models%20and%20generic%20time%20series%0Aclassification%20models.%20We%20describe%20three%20different%20model%20setups%20to%20deal%20with%0AEEG%20data%20from%20different%20subjects%2C%20subject-specific%20models%20%28most%20EEG%0Aliterature%29%2C%20subject-agnostic%20models%20and%20subject-conditional%20models.%20In%0Aexperiments%20on%20three%20datasets%2C%20we%20demonstrate%20that%20off-the-shelf%20time%20series%0Aclassification%20models%20trained%20per%20subject%20perform%20close%20to%20EEG%20classification%0Amodels%2C%20but%20that%20do%20not%20quite%20reach%20the%20performance%20of%20domain-specific%0Amodeling.%20Additionally%2C%20we%20combine%20time-series%20models%20with%20subject%20embeddings%0Ato%20train%20one%20joint%20subject-conditional%20classifier%20on%20all%20subjects.%20The%0Aresulting%20models%20are%20competitive%20with%20dedicated%20EEG%20models%20in%202%20out%20of%203%0Adatasets%2C%20even%20outperforming%20all%20EEG%20methods%20on%20one%20of%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06966v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20EEG%20Sequences%20Time%20Series%3F%20EEG%20Classification%20with%20Time%20Series%0A%20%20Models%20and%20Joint%20Subject%20Training&entry.906535625=Johannes%20Burchert%20and%20Thorben%20Werner%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Diego%20Coello%20de%20Portugal%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme&entry.1292438233=%20%20As%20with%20most%20other%20data%20domains%2C%20EEG%20data%20analysis%20relies%20on%20rich%0Adomain-specific%20preprocessing.%20Beyond%20such%20preprocessing%2C%20machine%20learners%0Awould%20hope%20to%20deal%20with%20such%20data%20as%20with%20any%20other%20time%20series%20data.%20For%20EEG%0Aclassification%20many%20models%20have%20been%20developed%20with%20layer%20types%20and%0Aarchitectures%20we%20typically%20do%20not%20see%20in%20time%20series%20classification.%0AFurthermore%2C%20typically%20separate%20models%20for%20each%20individual%20subject%20are%20learned%2C%0Anot%20one%20model%20for%20all%20of%20them.%20In%20this%20paper%2C%20we%20systematically%20study%20the%0Adifferences%20between%20EEG%20classification%20models%20and%20generic%20time%20series%0Aclassification%20models.%20We%20describe%20three%20different%20model%20setups%20to%20deal%20with%0AEEG%20data%20from%20different%20subjects%2C%20subject-specific%20models%20%28most%20EEG%0Aliterature%29%2C%20subject-agnostic%20models%20and%20subject-conditional%20models.%20In%0Aexperiments%20on%20three%20datasets%2C%20we%20demonstrate%20that%20off-the-shelf%20time%20series%0Aclassification%20models%20trained%20per%20subject%20perform%20close%20to%20EEG%20classification%0Amodels%2C%20but%20that%20do%20not%20quite%20reach%20the%20performance%20of%20domain-specific%0Amodeling.%20Additionally%2C%20we%20combine%20time-series%20models%20with%20subject%20embeddings%0Ato%20train%20one%20joint%20subject-conditional%20classifier%20on%20all%20subjects.%20The%0Aresulting%20models%20are%20competitive%20with%20dedicated%20EEG%20models%20in%202%20out%20of%203%0Adatasets%2C%20even%20outperforming%20all%20EEG%20methods%20on%20one%20of%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06966v1&entry.124074799=Read"},
{"title": "Characterizing and Classifying Developer Forum Posts with their\n  Intentions", "author": "Xingfang Wu and Eric Laufer and Heng Li and Foutse Khomh and Santhosh Srinivasan and Jayden Luo", "abstract": "  With the rapid growth of the developer community, the amount of posts on\nonline technical forums has been growing rapidly, which poses difficulties for\nusers to filter useful posts and find important information. Tags provide a\nconcise feature dimension for users to locate their interested posts and for\nsearch engines to index the most relevant posts according to the queries.\nHowever, most tags are only focused on the technical perspective (e.g., program\nlanguage, platform, tool). In most cases, forum posts in online developer\ncommunities reveal the author's intentions to solve a problem, ask for advice,\nshare information, etc. The modeling of the intentions of posts can provide an\nextra dimension to the current tag taxonomy. By referencing previous studies\nand learning from industrial perspectives, we create a refined taxonomy for the\nintentions of technical forum posts. Through manual labeling and analysis on a\nsampled post dataset extracted from online forums, we understand the relevance\nbetween the constitution of posts (code, error messages) and their intentions.\nFurthermore, inspired by our manual study, we design a pre-trained\ntransformer-based model to automatically predict post intentions. The best\nvariant of our intention prediction framework, which achieves a Micro F1-score\nof 0.589, Top 1-3 accuracy of 62.6% to 87.8%, and an average AUC of 0.787,\noutperforms the state-of-the-art baseline approach. Our characterization and\nautomated classification of forum posts regarding their intentions may help\nforum maintainers or third-party tool developers improve the organization and\nretrieval of posts on technical forums. We have released our annotated dataset\nand codes in our supplementary material package.\n", "link": "http://arxiv.org/abs/2312.14279v2", "date": "2024-04-10", "relevancy": 1.8304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4498}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Characterizing%20and%20Classifying%20Developer%20Forum%20Posts%20with%20their%0A%20%20Intentions&body=Title%3A%20Characterizing%20and%20Classifying%20Developer%20Forum%20Posts%20with%20their%0A%20%20Intentions%0AAuthor%3A%20Xingfang%20Wu%20and%20Eric%20Laufer%20and%20Heng%20Li%20and%20Foutse%20Khomh%20and%20Santhosh%20Srinivasan%20and%20Jayden%20Luo%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20the%20developer%20community%2C%20the%20amount%20of%20posts%20on%0Aonline%20technical%20forums%20has%20been%20growing%20rapidly%2C%20which%20poses%20difficulties%20for%0Ausers%20to%20filter%20useful%20posts%20and%20find%20important%20information.%20Tags%20provide%20a%0Aconcise%20feature%20dimension%20for%20users%20to%20locate%20their%20interested%20posts%20and%20for%0Asearch%20engines%20to%20index%20the%20most%20relevant%20posts%20according%20to%20the%20queries.%0AHowever%2C%20most%20tags%20are%20only%20focused%20on%20the%20technical%20perspective%20%28e.g.%2C%20program%0Alanguage%2C%20platform%2C%20tool%29.%20In%20most%20cases%2C%20forum%20posts%20in%20online%20developer%0Acommunities%20reveal%20the%20author%27s%20intentions%20to%20solve%20a%20problem%2C%20ask%20for%20advice%2C%0Ashare%20information%2C%20etc.%20The%20modeling%20of%20the%20intentions%20of%20posts%20can%20provide%20an%0Aextra%20dimension%20to%20the%20current%20tag%20taxonomy.%20By%20referencing%20previous%20studies%0Aand%20learning%20from%20industrial%20perspectives%2C%20we%20create%20a%20refined%20taxonomy%20for%20the%0Aintentions%20of%20technical%20forum%20posts.%20Through%20manual%20labeling%20and%20analysis%20on%20a%0Asampled%20post%20dataset%20extracted%20from%20online%20forums%2C%20we%20understand%20the%20relevance%0Abetween%20the%20constitution%20of%20posts%20%28code%2C%20error%20messages%29%20and%20their%20intentions.%0AFurthermore%2C%20inspired%20by%20our%20manual%20study%2C%20we%20design%20a%20pre-trained%0Atransformer-based%20model%20to%20automatically%20predict%20post%20intentions.%20The%20best%0Avariant%20of%20our%20intention%20prediction%20framework%2C%20which%20achieves%20a%20Micro%20F1-score%0Aof%200.589%2C%20Top%201-3%20accuracy%20of%2062.6%25%20to%2087.8%25%2C%20and%20an%20average%20AUC%20of%200.787%2C%0Aoutperforms%20the%20state-of-the-art%20baseline%20approach.%20Our%20characterization%20and%0Aautomated%20classification%20of%20forum%20posts%20regarding%20their%20intentions%20may%20help%0Aforum%20maintainers%20or%20third-party%20tool%20developers%20improve%20the%20organization%20and%0Aretrieval%20of%20posts%20on%20technical%20forums.%20We%20have%20released%20our%20annotated%20dataset%0Aand%20codes%20in%20our%20supplementary%20material%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14279v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20and%20Classifying%20Developer%20Forum%20Posts%20with%20their%0A%20%20Intentions&entry.906535625=Xingfang%20Wu%20and%20Eric%20Laufer%20and%20Heng%20Li%20and%20Foutse%20Khomh%20and%20Santhosh%20Srinivasan%20and%20Jayden%20Luo&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20the%20developer%20community%2C%20the%20amount%20of%20posts%20on%0Aonline%20technical%20forums%20has%20been%20growing%20rapidly%2C%20which%20poses%20difficulties%20for%0Ausers%20to%20filter%20useful%20posts%20and%20find%20important%20information.%20Tags%20provide%20a%0Aconcise%20feature%20dimension%20for%20users%20to%20locate%20their%20interested%20posts%20and%20for%0Asearch%20engines%20to%20index%20the%20most%20relevant%20posts%20according%20to%20the%20queries.%0AHowever%2C%20most%20tags%20are%20only%20focused%20on%20the%20technical%20perspective%20%28e.g.%2C%20program%0Alanguage%2C%20platform%2C%20tool%29.%20In%20most%20cases%2C%20forum%20posts%20in%20online%20developer%0Acommunities%20reveal%20the%20author%27s%20intentions%20to%20solve%20a%20problem%2C%20ask%20for%20advice%2C%0Ashare%20information%2C%20etc.%20The%20modeling%20of%20the%20intentions%20of%20posts%20can%20provide%20an%0Aextra%20dimension%20to%20the%20current%20tag%20taxonomy.%20By%20referencing%20previous%20studies%0Aand%20learning%20from%20industrial%20perspectives%2C%20we%20create%20a%20refined%20taxonomy%20for%20the%0Aintentions%20of%20technical%20forum%20posts.%20Through%20manual%20labeling%20and%20analysis%20on%20a%0Asampled%20post%20dataset%20extracted%20from%20online%20forums%2C%20we%20understand%20the%20relevance%0Abetween%20the%20constitution%20of%20posts%20%28code%2C%20error%20messages%29%20and%20their%20intentions.%0AFurthermore%2C%20inspired%20by%20our%20manual%20study%2C%20we%20design%20a%20pre-trained%0Atransformer-based%20model%20to%20automatically%20predict%20post%20intentions.%20The%20best%0Avariant%20of%20our%20intention%20prediction%20framework%2C%20which%20achieves%20a%20Micro%20F1-score%0Aof%200.589%2C%20Top%201-3%20accuracy%20of%2062.6%25%20to%2087.8%25%2C%20and%20an%20average%20AUC%20of%200.787%2C%0Aoutperforms%20the%20state-of-the-art%20baseline%20approach.%20Our%20characterization%20and%0Aautomated%20classification%20of%20forum%20posts%20regarding%20their%20intentions%20may%20help%0Aforum%20maintainers%20or%20third-party%20tool%20developers%20improve%20the%20organization%20and%0Aretrieval%20of%20posts%20on%20technical%20forums.%20We%20have%20released%20our%20annotated%20dataset%0Aand%20codes%20in%20our%20supplementary%20material%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14279v2&entry.124074799=Read"},
{"title": "Bias-Reduced Neural Networks for Parameter Estimation in Quantitative\n  MRI", "author": "Andrew Mao and Sebastian Flassbeck and Jakob Assl\u00e4nder", "abstract": "  Purpose: To develop neural network (NN)-based quantitative MRI parameter\nestimators with minimal bias and a variance close to the Cram\\'er-Rao bound.\n  Theory and Methods: We generalize the mean squared error loss to control the\nbias and variance of the NN's estimates, which involves averaging over multiple\nnoise realizations of the same measurements during training. Bias and variance\nproperties of the resulting NNs are studied for two neuroimaging applications.\n  Results: In simulations, the proposed strategy reduces the estimates' bias\nthroughout parameter space and achieves a variance close to the Cram\\'er-Rao\nbound. In vivo, we observe good concordance between parameter maps estimated\nwith the proposed NNs and traditional estimators, such as non-linear\nleast-squares fitting, while state-of-the-art NNs show larger deviations.\n  Conclusion: The proposed NNs have greatly reduced bias compared to those\ntrained using the mean squared error and offer significantly improved\ncomputational efficiency over traditional estimators with comparable or better\naccuracy.\n", "link": "http://arxiv.org/abs/2312.11468v3", "date": "2024-04-10", "relevancy": 1.8289, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bias-Reduced%20Neural%20Networks%20for%20Parameter%20Estimation%20in%20Quantitative%0A%20%20MRI&body=Title%3A%20Bias-Reduced%20Neural%20Networks%20for%20Parameter%20Estimation%20in%20Quantitative%0A%20%20MRI%0AAuthor%3A%20Andrew%20Mao%20and%20Sebastian%20Flassbeck%20and%20Jakob%20Assl%C3%A4nder%0AAbstract%3A%20%20%20Purpose%3A%20To%20develop%20neural%20network%20%28NN%29-based%20quantitative%20MRI%20parameter%0Aestimators%20with%20minimal%20bias%20and%20a%20variance%20close%20to%20the%20Cram%5C%27er-Rao%20bound.%0A%20%20Theory%20and%20Methods%3A%20We%20generalize%20the%20mean%20squared%20error%20loss%20to%20control%20the%0Abias%20and%20variance%20of%20the%20NN%27s%20estimates%2C%20which%20involves%20averaging%20over%20multiple%0Anoise%20realizations%20of%20the%20same%20measurements%20during%20training.%20Bias%20and%20variance%0Aproperties%20of%20the%20resulting%20NNs%20are%20studied%20for%20two%20neuroimaging%20applications.%0A%20%20Results%3A%20In%20simulations%2C%20the%20proposed%20strategy%20reduces%20the%20estimates%27%20bias%0Athroughout%20parameter%20space%20and%20achieves%20a%20variance%20close%20to%20the%20Cram%5C%27er-Rao%0Abound.%20In%20vivo%2C%20we%20observe%20good%20concordance%20between%20parameter%20maps%20estimated%0Awith%20the%20proposed%20NNs%20and%20traditional%20estimators%2C%20such%20as%20non-linear%0Aleast-squares%20fitting%2C%20while%20state-of-the-art%20NNs%20show%20larger%20deviations.%0A%20%20Conclusion%3A%20The%20proposed%20NNs%20have%20greatly%20reduced%20bias%20compared%20to%20those%0Atrained%20using%20the%20mean%20squared%20error%20and%20offer%20significantly%20improved%0Acomputational%20efficiency%20over%20traditional%20estimators%20with%20comparable%20or%20better%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11468v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias-Reduced%20Neural%20Networks%20for%20Parameter%20Estimation%20in%20Quantitative%0A%20%20MRI&entry.906535625=Andrew%20Mao%20and%20Sebastian%20Flassbeck%20and%20Jakob%20Assl%C3%A4nder&entry.1292438233=%20%20Purpose%3A%20To%20develop%20neural%20network%20%28NN%29-based%20quantitative%20MRI%20parameter%0Aestimators%20with%20minimal%20bias%20and%20a%20variance%20close%20to%20the%20Cram%5C%27er-Rao%20bound.%0A%20%20Theory%20and%20Methods%3A%20We%20generalize%20the%20mean%20squared%20error%20loss%20to%20control%20the%0Abias%20and%20variance%20of%20the%20NN%27s%20estimates%2C%20which%20involves%20averaging%20over%20multiple%0Anoise%20realizations%20of%20the%20same%20measurements%20during%20training.%20Bias%20and%20variance%0Aproperties%20of%20the%20resulting%20NNs%20are%20studied%20for%20two%20neuroimaging%20applications.%0A%20%20Results%3A%20In%20simulations%2C%20the%20proposed%20strategy%20reduces%20the%20estimates%27%20bias%0Athroughout%20parameter%20space%20and%20achieves%20a%20variance%20close%20to%20the%20Cram%5C%27er-Rao%0Abound.%20In%20vivo%2C%20we%20observe%20good%20concordance%20between%20parameter%20maps%20estimated%0Awith%20the%20proposed%20NNs%20and%20traditional%20estimators%2C%20such%20as%20non-linear%0Aleast-squares%20fitting%2C%20while%20state-of-the-art%20NNs%20show%20larger%20deviations.%0A%20%20Conclusion%3A%20The%20proposed%20NNs%20have%20greatly%20reduced%20bias%20compared%20to%20those%0Atrained%20using%20the%20mean%20squared%20error%20and%20offer%20significantly%20improved%0Acomputational%20efficiency%20over%20traditional%20estimators%20with%20comparable%20or%20better%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11468v3&entry.124074799=Read"},
{"title": "Knowledge graphs for empirical concept retrieval", "author": "Lenka T\u011btkov\u00e1 and Teresa Karen Scheidt and Maria Mandrup Fogh and Ellen Marie Gaunby J\u00f8rgensen and Finn \u00c5rup Nielsen and Lars Kai Hansen", "abstract": "  Concept-based explainable AI is promising as a tool to improve the\nunderstanding of complex models at the premises of a given user, viz.\\ as a\ntool for personalized explainability. An important class of concept-based\nexplainability methods is constructed with empirically defined concepts,\nindirectly defined through a set of positive and negative examples, as in the\nTCAV approach (Kim et al., 2018). While it is appealing to the user to avoid\nformal definitions of concepts and their operationalization, it can be\nchallenging to establish relevant concept datasets. Here, we address this\nchallenge using general knowledge graphs (such as, e.g., Wikidata or WordNet)\nfor comprehensive concept definition and present a workflow for user-driven\ndata collection in both text and image domains. The concepts derived from\nknowledge graphs are defined interactively, providing an opportunity for\npersonalization and ensuring that the concepts reflect the user's intentions.\nWe test the retrieved concept datasets on two concept-based explainability\nmethods, namely concept activation vectors (CAVs) and concept activation\nregions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs\nbased on these empirical concept datasets provide robust and accurate\nexplanations. Importantly, we also find good alignment between the models'\nrepresentations of concepts and the structure of knowledge graphs, i.e., human\nrepresentations. This supports our conclusion that knowledge graph-based\nconcepts are relevant for XAI.\n", "link": "http://arxiv.org/abs/2404.07008v1", "date": "2024-04-10", "relevancy": 1.8227, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4496}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Knowledge%20graphs%20for%20empirical%20concept%20retrieval&body=Title%3A%20Knowledge%20graphs%20for%20empirical%20concept%20retrieval%0AAuthor%3A%20Lenka%20T%C4%9Btkov%C3%A1%20and%20Teresa%20Karen%20Scheidt%20and%20Maria%20Mandrup%20Fogh%20and%20Ellen%20Marie%20Gaunby%20J%C3%B8rgensen%20and%20Finn%20%C3%85rup%20Nielsen%20and%20Lars%20Kai%20Hansen%0AAbstract%3A%20%20%20Concept-based%20explainable%20AI%20is%20promising%20as%20a%20tool%20to%20improve%20the%0Aunderstanding%20of%20complex%20models%20at%20the%20premises%20of%20a%20given%20user%2C%20viz.%5C%20as%20a%0Atool%20for%20personalized%20explainability.%20An%20important%20class%20of%20concept-based%0Aexplainability%20methods%20is%20constructed%20with%20empirically%20defined%20concepts%2C%0Aindirectly%20defined%20through%20a%20set%20of%20positive%20and%20negative%20examples%2C%20as%20in%20the%0ATCAV%20approach%20%28Kim%20et%20al.%2C%202018%29.%20While%20it%20is%20appealing%20to%20the%20user%20to%20avoid%0Aformal%20definitions%20of%20concepts%20and%20their%20operationalization%2C%20it%20can%20be%0Achallenging%20to%20establish%20relevant%20concept%20datasets.%20Here%2C%20we%20address%20this%0Achallenge%20using%20general%20knowledge%20graphs%20%28such%20as%2C%20e.g.%2C%20Wikidata%20or%20WordNet%29%0Afor%20comprehensive%20concept%20definition%20and%20present%20a%20workflow%20for%20user-driven%0Adata%20collection%20in%20both%20text%20and%20image%20domains.%20The%20concepts%20derived%20from%0Aknowledge%20graphs%20are%20defined%20interactively%2C%20providing%20an%20opportunity%20for%0Apersonalization%20and%20ensuring%20that%20the%20concepts%20reflect%20the%20user%27s%20intentions.%0AWe%20test%20the%20retrieved%20concept%20datasets%20on%20two%20concept-based%20explainability%0Amethods%2C%20namely%20concept%20activation%20vectors%20%28CAVs%29%20and%20concept%20activation%0Aregions%20%28CARs%29%20%28Crabbe%20and%20van%20der%20Schaar%2C%202022%29.%20We%20show%20that%20CAVs%20and%20CARs%0Abased%20on%20these%20empirical%20concept%20datasets%20provide%20robust%20and%20accurate%0Aexplanations.%20Importantly%2C%20we%20also%20find%20good%20alignment%20between%20the%20models%27%0Arepresentations%20of%20concepts%20and%20the%20structure%20of%20knowledge%20graphs%2C%20i.e.%2C%20human%0Arepresentations.%20This%20supports%20our%20conclusion%20that%20knowledge%20graph-based%0Aconcepts%20are%20relevant%20for%20XAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07008v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20graphs%20for%20empirical%20concept%20retrieval&entry.906535625=Lenka%20T%C4%9Btkov%C3%A1%20and%20Teresa%20Karen%20Scheidt%20and%20Maria%20Mandrup%20Fogh%20and%20Ellen%20Marie%20Gaunby%20J%C3%B8rgensen%20and%20Finn%20%C3%85rup%20Nielsen%20and%20Lars%20Kai%20Hansen&entry.1292438233=%20%20Concept-based%20explainable%20AI%20is%20promising%20as%20a%20tool%20to%20improve%20the%0Aunderstanding%20of%20complex%20models%20at%20the%20premises%20of%20a%20given%20user%2C%20viz.%5C%20as%20a%0Atool%20for%20personalized%20explainability.%20An%20important%20class%20of%20concept-based%0Aexplainability%20methods%20is%20constructed%20with%20empirically%20defined%20concepts%2C%0Aindirectly%20defined%20through%20a%20set%20of%20positive%20and%20negative%20examples%2C%20as%20in%20the%0ATCAV%20approach%20%28Kim%20et%20al.%2C%202018%29.%20While%20it%20is%20appealing%20to%20the%20user%20to%20avoid%0Aformal%20definitions%20of%20concepts%20and%20their%20operationalization%2C%20it%20can%20be%0Achallenging%20to%20establish%20relevant%20concept%20datasets.%20Here%2C%20we%20address%20this%0Achallenge%20using%20general%20knowledge%20graphs%20%28such%20as%2C%20e.g.%2C%20Wikidata%20or%20WordNet%29%0Afor%20comprehensive%20concept%20definition%20and%20present%20a%20workflow%20for%20user-driven%0Adata%20collection%20in%20both%20text%20and%20image%20domains.%20The%20concepts%20derived%20from%0Aknowledge%20graphs%20are%20defined%20interactively%2C%20providing%20an%20opportunity%20for%0Apersonalization%20and%20ensuring%20that%20the%20concepts%20reflect%20the%20user%27s%20intentions.%0AWe%20test%20the%20retrieved%20concept%20datasets%20on%20two%20concept-based%20explainability%0Amethods%2C%20namely%20concept%20activation%20vectors%20%28CAVs%29%20and%20concept%20activation%0Aregions%20%28CARs%29%20%28Crabbe%20and%20van%20der%20Schaar%2C%202022%29.%20We%20show%20that%20CAVs%20and%20CARs%0Abased%20on%20these%20empirical%20concept%20datasets%20provide%20robust%20and%20accurate%0Aexplanations.%20Importantly%2C%20we%20also%20find%20good%20alignment%20between%20the%20models%27%0Arepresentations%20of%20concepts%20and%20the%20structure%20of%20knowledge%20graphs%2C%20i.e.%2C%20human%0Arepresentations.%20This%20supports%20our%20conclusion%20that%20knowledge%20graph-based%0Aconcepts%20are%20relevant%20for%20XAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07008v1&entry.124074799=Read"},
{"title": "A Survey on the Integration of Generative AI for Critical Thinking in\n  Mobile Networks", "author": "Athanasios Karapantelakis and Alexandros Nikou and Ajay Kattepur and Jean Martins and Leonid Mokrushin and Swarup Kumar Mohalik and Marin Orlic and Aneta Vulgarakis Feljan", "abstract": "  In the near future, mobile networks are expected to broaden their services\nand coverage to accommodate a larger user base and diverse user needs. Thus,\nthey will increasingly rely on artificial intelligence (AI) to manage network\noperation and control costs, undertaking complex decision-making roles. This\nshift will necessitate the application of techniques that incorporate critical\nthinking abilities, including reasoning and planning. Symbolic AI techniques\nalready facilitate critical thinking based on existing knowledge. Yet, their\nuse in telecommunications is hindered by the high cost of mostly manual\ncuration of this knowledge and high computational complexity of reasoning\ntasks. At the same time, there is a spurt of innovations in industries such as\ntelecommunications due to Generative AI (GenAI) technologies, operating\nindependently of human-curated knowledge. However, their capacity for critical\nthinking remains uncertain. This paper aims to address this gap by examining\nthe current status of GenAI algorithms with critical thinking capabilities and\ninvestigating their potential applications in telecom networks. Specifically,\nthe aim of this study is to offer an introduction to the potential utilization\nof GenAI for critical thinking techniques in mobile networks, while also\nestablishing a foundation for future research.\n", "link": "http://arxiv.org/abs/2404.06946v1", "date": "2024-04-10", "relevancy": 1.8159, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4914}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4324}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4144}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20the%20Integration%20of%20Generative%20AI%20for%20Critical%20Thinking%20in%0A%20%20Mobile%20Networks&body=Title%3A%20A%20Survey%20on%20the%20Integration%20of%20Generative%20AI%20for%20Critical%20Thinking%20in%0A%20%20Mobile%20Networks%0AAuthor%3A%20Athanasios%20Karapantelakis%20and%20Alexandros%20Nikou%20and%20Ajay%20Kattepur%20and%20Jean%20Martins%20and%20Leonid%20Mokrushin%20and%20Swarup%20Kumar%20Mohalik%20and%20Marin%20Orlic%20and%20Aneta%20Vulgarakis%20Feljan%0AAbstract%3A%20%20%20In%20the%20near%20future%2C%20mobile%20networks%20are%20expected%20to%20broaden%20their%20services%0Aand%20coverage%20to%20accommodate%20a%20larger%20user%20base%20and%20diverse%20user%20needs.%20Thus%2C%0Athey%20will%20increasingly%20rely%20on%20artificial%20intelligence%20%28AI%29%20to%20manage%20network%0Aoperation%20and%20control%20costs%2C%20undertaking%20complex%20decision-making%20roles.%20This%0Ashift%20will%20necessitate%20the%20application%20of%20techniques%20that%20incorporate%20critical%0Athinking%20abilities%2C%20including%20reasoning%20and%20planning.%20Symbolic%20AI%20techniques%0Aalready%20facilitate%20critical%20thinking%20based%20on%20existing%20knowledge.%20Yet%2C%20their%0Ause%20in%20telecommunications%20is%20hindered%20by%20the%20high%20cost%20of%20mostly%20manual%0Acuration%20of%20this%20knowledge%20and%20high%20computational%20complexity%20of%20reasoning%0Atasks.%20At%20the%20same%20time%2C%20there%20is%20a%20spurt%20of%20innovations%20in%20industries%20such%20as%0Atelecommunications%20due%20to%20Generative%20AI%20%28GenAI%29%20technologies%2C%20operating%0Aindependently%20of%20human-curated%20knowledge.%20However%2C%20their%20capacity%20for%20critical%0Athinking%20remains%20uncertain.%20This%20paper%20aims%20to%20address%20this%20gap%20by%20examining%0Athe%20current%20status%20of%20GenAI%20algorithms%20with%20critical%20thinking%20capabilities%20and%0Ainvestigating%20their%20potential%20applications%20in%20telecom%20networks.%20Specifically%2C%0Athe%20aim%20of%20this%20study%20is%20to%20offer%20an%20introduction%20to%20the%20potential%20utilization%0Aof%20GenAI%20for%20critical%20thinking%20techniques%20in%20mobile%20networks%2C%20while%20also%0Aestablishing%20a%20foundation%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06946v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20the%20Integration%20of%20Generative%20AI%20for%20Critical%20Thinking%20in%0A%20%20Mobile%20Networks&entry.906535625=Athanasios%20Karapantelakis%20and%20Alexandros%20Nikou%20and%20Ajay%20Kattepur%20and%20Jean%20Martins%20and%20Leonid%20Mokrushin%20and%20Swarup%20Kumar%20Mohalik%20and%20Marin%20Orlic%20and%20Aneta%20Vulgarakis%20Feljan&entry.1292438233=%20%20In%20the%20near%20future%2C%20mobile%20networks%20are%20expected%20to%20broaden%20their%20services%0Aand%20coverage%20to%20accommodate%20a%20larger%20user%20base%20and%20diverse%20user%20needs.%20Thus%2C%0Athey%20will%20increasingly%20rely%20on%20artificial%20intelligence%20%28AI%29%20to%20manage%20network%0Aoperation%20and%20control%20costs%2C%20undertaking%20complex%20decision-making%20roles.%20This%0Ashift%20will%20necessitate%20the%20application%20of%20techniques%20that%20incorporate%20critical%0Athinking%20abilities%2C%20including%20reasoning%20and%20planning.%20Symbolic%20AI%20techniques%0Aalready%20facilitate%20critical%20thinking%20based%20on%20existing%20knowledge.%20Yet%2C%20their%0Ause%20in%20telecommunications%20is%20hindered%20by%20the%20high%20cost%20of%20mostly%20manual%0Acuration%20of%20this%20knowledge%20and%20high%20computational%20complexity%20of%20reasoning%0Atasks.%20At%20the%20same%20time%2C%20there%20is%20a%20spurt%20of%20innovations%20in%20industries%20such%20as%0Atelecommunications%20due%20to%20Generative%20AI%20%28GenAI%29%20technologies%2C%20operating%0Aindependently%20of%20human-curated%20knowledge.%20However%2C%20their%20capacity%20for%20critical%0Athinking%20remains%20uncertain.%20This%20paper%20aims%20to%20address%20this%20gap%20by%20examining%0Athe%20current%20status%20of%20GenAI%20algorithms%20with%20critical%20thinking%20capabilities%20and%0Ainvestigating%20their%20potential%20applications%20in%20telecom%20networks.%20Specifically%2C%0Athe%20aim%20of%20this%20study%20is%20to%20offer%20an%20introduction%20to%20the%20potential%20utilization%0Aof%20GenAI%20for%20critical%20thinking%20techniques%20in%20mobile%20networks%2C%20while%20also%0Aestablishing%20a%20foundation%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06946v1&entry.124074799=Read"},
{"title": "Visibility into AI Agents", "author": "Alan Chan and Carson Ezell and Max Kaufmann and Kevin Wei and Lewis Hammond and Herbie Bradley and Emma Bluemke and Nitarshan Rajkumar and David Krueger and Noam Kolt and Lennart Heim and Markus Anderljung", "abstract": "  Increased delegation of commercial, scientific, governmental, and personal\nactivities to AI agents -- systems capable of pursuing complex goals with\nlimited supervision -- may exacerbate existing societal risks and introduce new\nrisks. Understanding and mitigating these risks involves critically evaluating\nexisting governance structures, revising and adapting these structures where\nneeded, and ensuring accountability of key stakeholders. Information about\nwhere, why, how, and by whom certain AI agents are used, which we refer to as\nvisibility, is critical to these objectives. In this paper, we assess three\ncategories of measures to increase visibility into AI agents: agent\nidentifiers, real-time monitoring, and activity logging. For each, we outline\npotential implementations that vary in intrusiveness and informativeness. We\nanalyze how the measures apply across a spectrum of centralized through\ndecentralized deployment contexts, accounting for various actors in the supply\nchain including hardware and software service providers. Finally, we discuss\nthe implications of our measures for privacy and concentration of power.\nFurther work into understanding the measures and mitigating their negative\nimpacts can help to build a foundation for the governance of AI agents.\n", "link": "http://arxiv.org/abs/2401.13138v4", "date": "2024-04-10", "relevancy": 1.8041, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4651}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4406}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visibility%20into%20AI%20Agents&body=Title%3A%20Visibility%20into%20AI%20Agents%0AAuthor%3A%20Alan%20Chan%20and%20Carson%20Ezell%20and%20Max%20Kaufmann%20and%20Kevin%20Wei%20and%20Lewis%20Hammond%20and%20Herbie%20Bradley%20and%20Emma%20Bluemke%20and%20Nitarshan%20Rajkumar%20and%20David%20Krueger%20and%20Noam%20Kolt%20and%20Lennart%20Heim%20and%20Markus%20Anderljung%0AAbstract%3A%20%20%20Increased%20delegation%20of%20commercial%2C%20scientific%2C%20governmental%2C%20and%20personal%0Aactivities%20to%20AI%20agents%20--%20systems%20capable%20of%20pursuing%20complex%20goals%20with%0Alimited%20supervision%20--%20may%20exacerbate%20existing%20societal%20risks%20and%20introduce%20new%0Arisks.%20Understanding%20and%20mitigating%20these%20risks%20involves%20critically%20evaluating%0Aexisting%20governance%20structures%2C%20revising%20and%20adapting%20these%20structures%20where%0Aneeded%2C%20and%20ensuring%20accountability%20of%20key%20stakeholders.%20Information%20about%0Awhere%2C%20why%2C%20how%2C%20and%20by%20whom%20certain%20AI%20agents%20are%20used%2C%20which%20we%20refer%20to%20as%0Avisibility%2C%20is%20critical%20to%20these%20objectives.%20In%20this%20paper%2C%20we%20assess%20three%0Acategories%20of%20measures%20to%20increase%20visibility%20into%20AI%20agents%3A%20agent%0Aidentifiers%2C%20real-time%20monitoring%2C%20and%20activity%20logging.%20For%20each%2C%20we%20outline%0Apotential%20implementations%20that%20vary%20in%20intrusiveness%20and%20informativeness.%20We%0Aanalyze%20how%20the%20measures%20apply%20across%20a%20spectrum%20of%20centralized%20through%0Adecentralized%20deployment%20contexts%2C%20accounting%20for%20various%20actors%20in%20the%20supply%0Achain%20including%20hardware%20and%20software%20service%20providers.%20Finally%2C%20we%20discuss%0Athe%20implications%20of%20our%20measures%20for%20privacy%20and%20concentration%20of%20power.%0AFurther%20work%20into%20understanding%20the%20measures%20and%20mitigating%20their%20negative%0Aimpacts%20can%20help%20to%20build%20a%20foundation%20for%20the%20governance%20of%20AI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13138v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visibility%20into%20AI%20Agents&entry.906535625=Alan%20Chan%20and%20Carson%20Ezell%20and%20Max%20Kaufmann%20and%20Kevin%20Wei%20and%20Lewis%20Hammond%20and%20Herbie%20Bradley%20and%20Emma%20Bluemke%20and%20Nitarshan%20Rajkumar%20and%20David%20Krueger%20and%20Noam%20Kolt%20and%20Lennart%20Heim%20and%20Markus%20Anderljung&entry.1292438233=%20%20Increased%20delegation%20of%20commercial%2C%20scientific%2C%20governmental%2C%20and%20personal%0Aactivities%20to%20AI%20agents%20--%20systems%20capable%20of%20pursuing%20complex%20goals%20with%0Alimited%20supervision%20--%20may%20exacerbate%20existing%20societal%20risks%20and%20introduce%20new%0Arisks.%20Understanding%20and%20mitigating%20these%20risks%20involves%20critically%20evaluating%0Aexisting%20governance%20structures%2C%20revising%20and%20adapting%20these%20structures%20where%0Aneeded%2C%20and%20ensuring%20accountability%20of%20key%20stakeholders.%20Information%20about%0Awhere%2C%20why%2C%20how%2C%20and%20by%20whom%20certain%20AI%20agents%20are%20used%2C%20which%20we%20refer%20to%20as%0Avisibility%2C%20is%20critical%20to%20these%20objectives.%20In%20this%20paper%2C%20we%20assess%20three%0Acategories%20of%20measures%20to%20increase%20visibility%20into%20AI%20agents%3A%20agent%0Aidentifiers%2C%20real-time%20monitoring%2C%20and%20activity%20logging.%20For%20each%2C%20we%20outline%0Apotential%20implementations%20that%20vary%20in%20intrusiveness%20and%20informativeness.%20We%0Aanalyze%20how%20the%20measures%20apply%20across%20a%20spectrum%20of%20centralized%20through%0Adecentralized%20deployment%20contexts%2C%20accounting%20for%20various%20actors%20in%20the%20supply%0Achain%20including%20hardware%20and%20software%20service%20providers.%20Finally%2C%20we%20discuss%0Athe%20implications%20of%20our%20measures%20for%20privacy%20and%20concentration%20of%20power.%0AFurther%20work%20into%20understanding%20the%20measures%20and%20mitigating%20their%20negative%0Aimpacts%20can%20help%20to%20build%20a%20foundation%20for%20the%20governance%20of%20AI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13138v4&entry.124074799=Read"},
{"title": "fairret: a Framework for Differentiable Fairness Regularization Terms", "author": "Maarten Buyl and MaryBeth Defrance and Tijl De Bie", "abstract": "  Current fairness toolkits in machine learning only admit a limited range of\nfairness definitions and have seen little integration with automatic\ndifferentiation libraries, despite the central role these libraries play in\nmodern machine learning pipelines.\n  We introduce a framework of fairness regularization terms (fairrets) which\nquantify bias as modular, flexible objectives that are easily integrated in\nautomatic differentiation pipelines. By employing a general definition of\nfairness in terms of linear-fractional statistics, a wide class of fairrets can\nbe computed efficiently. Experiments show the behavior of their gradients and\ntheir utility in enforcing fairness with minimal loss of predictive power\ncompared to baselines. Our contribution includes a PyTorch implementation of\nthe fairret framework.\n", "link": "http://arxiv.org/abs/2310.17256v2", "date": "2024-04-10", "relevancy": 1.8027, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4694}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4545}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4394}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20fairret%3A%20a%20Framework%20for%20Differentiable%20Fairness%20Regularization%20Terms&body=Title%3A%20fairret%3A%20a%20Framework%20for%20Differentiable%20Fairness%20Regularization%20Terms%0AAuthor%3A%20Maarten%20Buyl%20and%20MaryBeth%20Defrance%20and%20Tijl%20De%20Bie%0AAbstract%3A%20%20%20Current%20fairness%20toolkits%20in%20machine%20learning%20only%20admit%20a%20limited%20range%20of%0Afairness%20definitions%20and%20have%20seen%20little%20integration%20with%20automatic%0Adifferentiation%20libraries%2C%20despite%20the%20central%20role%20these%20libraries%20play%20in%0Amodern%20machine%20learning%20pipelines.%0A%20%20We%20introduce%20a%20framework%20of%20fairness%20regularization%20terms%20%28fairrets%29%20which%0Aquantify%20bias%20as%20modular%2C%20flexible%20objectives%20that%20are%20easily%20integrated%20in%0Aautomatic%20differentiation%20pipelines.%20By%20employing%20a%20general%20definition%20of%0Afairness%20in%20terms%20of%20linear-fractional%20statistics%2C%20a%20wide%20class%20of%20fairrets%20can%0Abe%20computed%20efficiently.%20Experiments%20show%20the%20behavior%20of%20their%20gradients%20and%0Atheir%20utility%20in%20enforcing%20fairness%20with%20minimal%20loss%20of%20predictive%20power%0Acompared%20to%20baselines.%20Our%20contribution%20includes%20a%20PyTorch%20implementation%20of%0Athe%20fairret%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17256v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=fairret%3A%20a%20Framework%20for%20Differentiable%20Fairness%20Regularization%20Terms&entry.906535625=Maarten%20Buyl%20and%20MaryBeth%20Defrance%20and%20Tijl%20De%20Bie&entry.1292438233=%20%20Current%20fairness%20toolkits%20in%20machine%20learning%20only%20admit%20a%20limited%20range%20of%0Afairness%20definitions%20and%20have%20seen%20little%20integration%20with%20automatic%0Adifferentiation%20libraries%2C%20despite%20the%20central%20role%20these%20libraries%20play%20in%0Amodern%20machine%20learning%20pipelines.%0A%20%20We%20introduce%20a%20framework%20of%20fairness%20regularization%20terms%20%28fairrets%29%20which%0Aquantify%20bias%20as%20modular%2C%20flexible%20objectives%20that%20are%20easily%20integrated%20in%0Aautomatic%20differentiation%20pipelines.%20By%20employing%20a%20general%20definition%20of%0Afairness%20in%20terms%20of%20linear-fractional%20statistics%2C%20a%20wide%20class%20of%20fairrets%20can%0Abe%20computed%20efficiently.%20Experiments%20show%20the%20behavior%20of%20their%20gradients%20and%0Atheir%20utility%20in%20enforcing%20fairness%20with%20minimal%20loss%20of%20predictive%20power%0Acompared%20to%20baselines.%20Our%20contribution%20includes%20a%20PyTorch%20implementation%20of%0Athe%20fairret%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17256v2&entry.124074799=Read"},
{"title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and\n  Interpretation", "author": "Elisa Sanchez-Bayona and Rodrigo Agerri", "abstract": "  Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.\n", "link": "http://arxiv.org/abs/2404.07053v1", "date": "2024-04-10", "relevancy": 1.7965, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4373}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4312}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation&body=Title%3A%20Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation%0AAuthor%3A%20Elisa%20Sanchez-Bayona%20and%20Rodrigo%20Agerri%0AAbstract%3A%20%20%20Metaphors%2C%20although%20occasionally%20unperceived%2C%20are%20ubiquitous%20in%20our%20everyday%0Alanguage.%20Thus%2C%20it%20is%20crucial%20for%20Language%20Models%20to%20be%20able%20to%20grasp%20the%0Aunderlying%20meaning%20of%20this%20kind%20of%20figurative%20language.%20In%20this%20work%2C%20we%0Apresent%20Meta4XNLI%2C%20a%20novel%20parallel%20dataset%20for%20the%20tasks%20of%20metaphor%20detection%0Aand%20interpretation%20that%20contains%20metaphor%20annotations%20in%20both%20Spanish%20and%0AEnglish.%20We%20investigate%20language%20models%27%20metaphor%20identification%20and%0Aunderstanding%20abilities%20through%20a%20series%20of%20monolingual%20and%20cross-lingual%0Aexperiments%20by%20leveraging%20our%20proposed%20corpus.%20In%20order%20to%20comprehend%20how%20these%0Anon-literal%20expressions%20affect%20models%27%20performance%2C%20we%20look%20over%20the%20results%0Aand%20perform%20an%20error%20analysis.%20Additionally%2C%20parallel%20data%20offers%20many%0Apotential%20opportunities%20to%20investigate%20metaphor%20transferability%20between%20these%0Alanguages%20and%20the%20impact%20of%20translation%20on%20the%20development%20of%20multilingual%0Aannotated%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07053v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation&entry.906535625=Elisa%20Sanchez-Bayona%20and%20Rodrigo%20Agerri&entry.1292438233=%20%20Metaphors%2C%20although%20occasionally%20unperceived%2C%20are%20ubiquitous%20in%20our%20everyday%0Alanguage.%20Thus%2C%20it%20is%20crucial%20for%20Language%20Models%20to%20be%20able%20to%20grasp%20the%0Aunderlying%20meaning%20of%20this%20kind%20of%20figurative%20language.%20In%20this%20work%2C%20we%0Apresent%20Meta4XNLI%2C%20a%20novel%20parallel%20dataset%20for%20the%20tasks%20of%20metaphor%20detection%0Aand%20interpretation%20that%20contains%20metaphor%20annotations%20in%20both%20Spanish%20and%0AEnglish.%20We%20investigate%20language%20models%27%20metaphor%20identification%20and%0Aunderstanding%20abilities%20through%20a%20series%20of%20monolingual%20and%20cross-lingual%0Aexperiments%20by%20leveraging%20our%20proposed%20corpus.%20In%20order%20to%20comprehend%20how%20these%0Anon-literal%20expressions%20affect%20models%27%20performance%2C%20we%20look%20over%20the%20results%0Aand%20perform%20an%20error%20analysis.%20Additionally%2C%20parallel%20data%20offers%20many%0Apotential%20opportunities%20to%20investigate%20metaphor%20transferability%20between%20these%0Alanguages%20and%20the%20impact%20of%20translation%20on%20the%20development%20of%20multilingual%0Aannotated%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07053v1&entry.124074799=Read"},
{"title": "Accurate Tennis Court Line Detection on Amateur Recorded Matches", "author": "Sameer Agrawal and Ragoth Sundararajan and Vishak Sagar", "abstract": "  Typically, tennis court line detection is done by running\nHough-Line-Detection to find straight lines in the image, and then computing a\ntransformation matrix from the detected lines to create the final court\nstructure. We propose numerous improvements and enhancements to this algorithm,\nincluding using pretrained State-of-the-Art shadow-removal and object-detection\nML models to make our line-detection more robust. Compared to the original\nalgorithm, our method can accurately detect lines on amateur, dirty courts.\nWhen combined with a robust ball-tracking system, our method will enable\naccurate, automatic refereeing for amateur and professional tennis matches\nalike.\n", "link": "http://arxiv.org/abs/2404.06977v1", "date": "2024-04-10", "relevancy": 1.788, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4691}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4336}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4252}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Accurate%20Tennis%20Court%20Line%20Detection%20on%20Amateur%20Recorded%20Matches&body=Title%3A%20Accurate%20Tennis%20Court%20Line%20Detection%20on%20Amateur%20Recorded%20Matches%0AAuthor%3A%20Sameer%20Agrawal%20and%20Ragoth%20Sundararajan%20and%20Vishak%20Sagar%0AAbstract%3A%20%20%20Typically%2C%20tennis%20court%20line%20detection%20is%20done%20by%20running%0AHough-Line-Detection%20to%20find%20straight%20lines%20in%20the%20image%2C%20and%20then%20computing%20a%0Atransformation%20matrix%20from%20the%20detected%20lines%20to%20create%20the%20final%20court%0Astructure.%20We%20propose%20numerous%20improvements%20and%20enhancements%20to%20this%20algorithm%2C%0Aincluding%20using%20pretrained%20State-of-the-Art%20shadow-removal%20and%20object-detection%0AML%20models%20to%20make%20our%20line-detection%20more%20robust.%20Compared%20to%20the%20original%0Aalgorithm%2C%20our%20method%20can%20accurately%20detect%20lines%20on%20amateur%2C%20dirty%20courts.%0AWhen%20combined%20with%20a%20robust%20ball-tracking%20system%2C%20our%20method%20will%20enable%0Aaccurate%2C%20automatic%20refereeing%20for%20amateur%20and%20professional%20tennis%20matches%0Aalike.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06977v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20Tennis%20Court%20Line%20Detection%20on%20Amateur%20Recorded%20Matches&entry.906535625=Sameer%20Agrawal%20and%20Ragoth%20Sundararajan%20and%20Vishak%20Sagar&entry.1292438233=%20%20Typically%2C%20tennis%20court%20line%20detection%20is%20done%20by%20running%0AHough-Line-Detection%20to%20find%20straight%20lines%20in%20the%20image%2C%20and%20then%20computing%20a%0Atransformation%20matrix%20from%20the%20detected%20lines%20to%20create%20the%20final%20court%0Astructure.%20We%20propose%20numerous%20improvements%20and%20enhancements%20to%20this%20algorithm%2C%0Aincluding%20using%20pretrained%20State-of-the-Art%20shadow-removal%20and%20object-detection%0AML%20models%20to%20make%20our%20line-detection%20more%20robust.%20Compared%20to%20the%20original%0Aalgorithm%2C%20our%20method%20can%20accurately%20detect%20lines%20on%20amateur%2C%20dirty%20courts.%0AWhen%20combined%20with%20a%20robust%20ball-tracking%20system%2C%20our%20method%20will%20enable%0Aaccurate%2C%20automatic%20refereeing%20for%20amateur%20and%20professional%20tennis%20matches%0Aalike.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06977v1&entry.124074799=Read"},
{"title": "On Regression in Extreme Regions", "author": "Nathan Huet and Stephan Cl\u00e9men\u00e7on and Anne Sabourin", "abstract": "  The statistical learning problem consists in building a predictive function\n$\\hat{f}$ based on independent copies of $(X,Y)$ so that $Y$ is approximated by\n$\\hat{f}(X)$ with minimum (squared) error. Motivated by various applications,\nspecial attention is paid here to the case of extreme (i.e. very large)\nobservations $X$. Because of their rarity, the contributions of such\nobservations to the (empirical) error is negligible, and the predictive\nperformance of empirical risk minimizers can be consequently very poor in\nextreme regions. In this paper, we develop a general framework for regression\non extremes. Under appropriate regular variation assumptions regarding the pair\n$(X,Y)$, we show that an asymptotic notion of risk can be tailored to summarize\nappropriately predictive performance in extreme regions. It is also proved that\nminimization of an empirical and nonasymptotic version of this 'extreme risk',\nbased on a fraction of the largest observations solely, yields good\ngeneralization capacity. In addition, numerical results providing strong\nempirical evidence of the relevance of the approach proposed are displayed.\n", "link": "http://arxiv.org/abs/2303.03084v2", "date": "2024-04-10", "relevancy": 1.7736, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4446}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4394}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Regression%20in%20Extreme%20Regions&body=Title%3A%20On%20Regression%20in%20Extreme%20Regions%0AAuthor%3A%20Nathan%20Huet%20and%20Stephan%20Cl%C3%A9men%C3%A7on%20and%20Anne%20Sabourin%0AAbstract%3A%20%20%20The%20statistical%20learning%20problem%20consists%20in%20building%20a%20predictive%20function%0A%24%5Chat%7Bf%7D%24%20based%20on%20independent%20copies%20of%20%24%28X%2CY%29%24%20so%20that%20%24Y%24%20is%20approximated%20by%0A%24%5Chat%7Bf%7D%28X%29%24%20with%20minimum%20%28squared%29%20error.%20Motivated%20by%20various%20applications%2C%0Aspecial%20attention%20is%20paid%20here%20to%20the%20case%20of%20extreme%20%28i.e.%20very%20large%29%0Aobservations%20%24X%24.%20Because%20of%20their%20rarity%2C%20the%20contributions%20of%20such%0Aobservations%20to%20the%20%28empirical%29%20error%20is%20negligible%2C%20and%20the%20predictive%0Aperformance%20of%20empirical%20risk%20minimizers%20can%20be%20consequently%20very%20poor%20in%0Aextreme%20regions.%20In%20this%20paper%2C%20we%20develop%20a%20general%20framework%20for%20regression%0Aon%20extremes.%20Under%20appropriate%20regular%20variation%20assumptions%20regarding%20the%20pair%0A%24%28X%2CY%29%24%2C%20we%20show%20that%20an%20asymptotic%20notion%20of%20risk%20can%20be%20tailored%20to%20summarize%0Aappropriately%20predictive%20performance%20in%20extreme%20regions.%20It%20is%20also%20proved%20that%0Aminimization%20of%20an%20empirical%20and%20nonasymptotic%20version%20of%20this%20%27extreme%20risk%27%2C%0Abased%20on%20a%20fraction%20of%20the%20largest%20observations%20solely%2C%20yields%20good%0Ageneralization%20capacity.%20In%20addition%2C%20numerical%20results%20providing%20strong%0Aempirical%20evidence%20of%20the%20relevance%20of%20the%20approach%20proposed%20are%20displayed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.03084v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Regression%20in%20Extreme%20Regions&entry.906535625=Nathan%20Huet%20and%20Stephan%20Cl%C3%A9men%C3%A7on%20and%20Anne%20Sabourin&entry.1292438233=%20%20The%20statistical%20learning%20problem%20consists%20in%20building%20a%20predictive%20function%0A%24%5Chat%7Bf%7D%24%20based%20on%20independent%20copies%20of%20%24%28X%2CY%29%24%20so%20that%20%24Y%24%20is%20approximated%20by%0A%24%5Chat%7Bf%7D%28X%29%24%20with%20minimum%20%28squared%29%20error.%20Motivated%20by%20various%20applications%2C%0Aspecial%20attention%20is%20paid%20here%20to%20the%20case%20of%20extreme%20%28i.e.%20very%20large%29%0Aobservations%20%24X%24.%20Because%20of%20their%20rarity%2C%20the%20contributions%20of%20such%0Aobservations%20to%20the%20%28empirical%29%20error%20is%20negligible%2C%20and%20the%20predictive%0Aperformance%20of%20empirical%20risk%20minimizers%20can%20be%20consequently%20very%20poor%20in%0Aextreme%20regions.%20In%20this%20paper%2C%20we%20develop%20a%20general%20framework%20for%20regression%0Aon%20extremes.%20Under%20appropriate%20regular%20variation%20assumptions%20regarding%20the%20pair%0A%24%28X%2CY%29%24%2C%20we%20show%20that%20an%20asymptotic%20notion%20of%20risk%20can%20be%20tailored%20to%20summarize%0Aappropriately%20predictive%20performance%20in%20extreme%20regions.%20It%20is%20also%20proved%20that%0Aminimization%20of%20an%20empirical%20and%20nonasymptotic%20version%20of%20this%20%27extreme%20risk%27%2C%0Abased%20on%20a%20fraction%20of%20the%20largest%20observations%20solely%2C%20yields%20good%0Ageneralization%20capacity.%20In%20addition%2C%20numerical%20results%20providing%20strong%0Aempirical%20evidence%20of%20the%20relevance%20of%20the%20approach%20proposed%20are%20displayed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.03084v2&entry.124074799=Read"},
{"title": "Bridging Algorithmic Information Theory and Machine Learning: A New\n  Approach to Kernel Learning", "author": "Boumediene Hamzi and Marcus Hutter and Houman Owhadi", "abstract": "  Machine Learning (ML) and Algorithmic Information Theory (AIT) look at\nComplexity from different points of view. We explore the interface between AIT\nand Kernel Methods (that are prevalent in ML) by adopting an AIT perspective on\nthe problem of learning kernels from data, in kernel ridge regression, through\nthe method of Sparse Kernel Flows. In particular, by looking at the differences\nand commonalities between Minimal Description Length (MDL) and Regularization\nin Machine Learning (RML), we prove that the method of Sparse Kernel Flows is\nthe natural approach to adopt to learn kernels from data. This approach aligns\nnaturally with the MDL principle, offering a more robust theoretical basis than\nthe existing reliance on cross-validation. The study reveals that deriving\nSparse Kernel Flows does not require a statistical approach; instead, one can\ndirectly engage with code-lengths and complexities, concepts central to AIT.\nThereby, this approach opens the door to reformulating algorithms in machine\nlearning using tools from AIT, with the aim of providing them a more solid\ntheoretical foundation.\n", "link": "http://arxiv.org/abs/2311.12624v3", "date": "2024-04-10", "relevancy": 1.7518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4343}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20Algorithmic%20Information%20Theory%20and%20Machine%20Learning%3A%20A%20New%0A%20%20Approach%20to%20Kernel%20Learning&body=Title%3A%20Bridging%20Algorithmic%20Information%20Theory%20and%20Machine%20Learning%3A%20A%20New%0A%20%20Approach%20to%20Kernel%20Learning%0AAuthor%3A%20Boumediene%20Hamzi%20and%20Marcus%20Hutter%20and%20Houman%20Owhadi%0AAbstract%3A%20%20%20Machine%20Learning%20%28ML%29%20and%20Algorithmic%20Information%20Theory%20%28AIT%29%20look%20at%0AComplexity%20from%20different%20points%20of%20view.%20We%20explore%20the%20interface%20between%20AIT%0Aand%20Kernel%20Methods%20%28that%20are%20prevalent%20in%20ML%29%20by%20adopting%20an%20AIT%20perspective%20on%0Athe%20problem%20of%20learning%20kernels%20from%20data%2C%20in%20kernel%20ridge%20regression%2C%20through%0Athe%20method%20of%20Sparse%20Kernel%20Flows.%20In%20particular%2C%20by%20looking%20at%20the%20differences%0Aand%20commonalities%20between%20Minimal%20Description%20Length%20%28MDL%29%20and%20Regularization%0Ain%20Machine%20Learning%20%28RML%29%2C%20we%20prove%20that%20the%20method%20of%20Sparse%20Kernel%20Flows%20is%0Athe%20natural%20approach%20to%20adopt%20to%20learn%20kernels%20from%20data.%20This%20approach%20aligns%0Anaturally%20with%20the%20MDL%20principle%2C%20offering%20a%20more%20robust%20theoretical%20basis%20than%0Athe%20existing%20reliance%20on%20cross-validation.%20The%20study%20reveals%20that%20deriving%0ASparse%20Kernel%20Flows%20does%20not%20require%20a%20statistical%20approach%3B%20instead%2C%20one%20can%0Adirectly%20engage%20with%20code-lengths%20and%20complexities%2C%20concepts%20central%20to%20AIT.%0AThereby%2C%20this%20approach%20opens%20the%20door%20to%20reformulating%20algorithms%20in%20machine%0Alearning%20using%20tools%20from%20AIT%2C%20with%20the%20aim%20of%20providing%20them%20a%20more%20solid%0Atheoretical%20foundation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12624v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Algorithmic%20Information%20Theory%20and%20Machine%20Learning%3A%20A%20New%0A%20%20Approach%20to%20Kernel%20Learning&entry.906535625=Boumediene%20Hamzi%20and%20Marcus%20Hutter%20and%20Houman%20Owhadi&entry.1292438233=%20%20Machine%20Learning%20%28ML%29%20and%20Algorithmic%20Information%20Theory%20%28AIT%29%20look%20at%0AComplexity%20from%20different%20points%20of%20view.%20We%20explore%20the%20interface%20between%20AIT%0Aand%20Kernel%20Methods%20%28that%20are%20prevalent%20in%20ML%29%20by%20adopting%20an%20AIT%20perspective%20on%0Athe%20problem%20of%20learning%20kernels%20from%20data%2C%20in%20kernel%20ridge%20regression%2C%20through%0Athe%20method%20of%20Sparse%20Kernel%20Flows.%20In%20particular%2C%20by%20looking%20at%20the%20differences%0Aand%20commonalities%20between%20Minimal%20Description%20Length%20%28MDL%29%20and%20Regularization%0Ain%20Machine%20Learning%20%28RML%29%2C%20we%20prove%20that%20the%20method%20of%20Sparse%20Kernel%20Flows%20is%0Athe%20natural%20approach%20to%20adopt%20to%20learn%20kernels%20from%20data.%20This%20approach%20aligns%0Anaturally%20with%20the%20MDL%20principle%2C%20offering%20a%20more%20robust%20theoretical%20basis%20than%0Athe%20existing%20reliance%20on%20cross-validation.%20The%20study%20reveals%20that%20deriving%0ASparse%20Kernel%20Flows%20does%20not%20require%20a%20statistical%20approach%3B%20instead%2C%20one%20can%0Adirectly%20engage%20with%20code-lengths%20and%20complexities%2C%20concepts%20central%20to%20AIT.%0AThereby%2C%20this%20approach%20opens%20the%20door%20to%20reformulating%20algorithms%20in%20machine%0Alearning%20using%20tools%20from%20AIT%2C%20with%20the%20aim%20of%20providing%20them%20a%20more%20solid%0Atheoretical%20foundation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12624v3&entry.124074799=Read"},
{"title": "Wild Visual Navigation: Fast Traversability Learning via Pre-Trained\n  Models and Online Self-Supervision", "author": "Mat\u00edas Mattamala and Jonas Frey and Piotr Libera and Nived Chebrolu and Georg Martius and Cesar Cadena and Marco Hutter and Maurice Fallon", "abstract": "  Natural environments such as forests and grasslands are challenging for\nrobotic navigation because of the false perception of rigid obstacles from high\ngrass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN),\nan online self-supervised learning system for visual traversability estimation.\nThe system is able to continuously adapt from a short human demonstration in\nthe field, only using onboard sensing and computing. One of the key ideas to\nachieve this is the use of high-dimensional features from pre-trained\nself-supervised models, which implicitly encode semantic information that\nmassively simplifies the learning task. Further, the development of an online\nscheme for supervision generator enables concurrent training and inference of\nthe learned model in the wild. We demonstrate our approach through diverse\nreal-world deployments in forests, parks, and grasslands. Our system is able to\nbootstrap the traversable terrain segmentation in less than 5 min of in-field\ntraining time, enabling the robot to navigate in complex, previously unseen\noutdoor terrains. Code: https://bit.ly/498b0CV - Project\npage:https://bit.ly/3M6nMHH\n", "link": "http://arxiv.org/abs/2404.07110v1", "date": "2024-04-10", "relevancy": 1.7429, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5644}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Wild%20Visual%20Navigation%3A%20Fast%20Traversability%20Learning%20via%20Pre-Trained%0A%20%20Models%20and%20Online%20Self-Supervision&body=Title%3A%20Wild%20Visual%20Navigation%3A%20Fast%20Traversability%20Learning%20via%20Pre-Trained%0A%20%20Models%20and%20Online%20Self-Supervision%0AAuthor%3A%20Mat%C3%ADas%20Mattamala%20and%20Jonas%20Frey%20and%20Piotr%20Libera%20and%20Nived%20Chebrolu%20and%20Georg%20Martius%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%20and%20Maurice%20Fallon%0AAbstract%3A%20%20%20Natural%20environments%20such%20as%20forests%20and%20grasslands%20are%20challenging%20for%0Arobotic%20navigation%20because%20of%20the%20false%20perception%20of%20rigid%20obstacles%20from%20high%0Agrass%2C%20twigs%2C%20or%20bushes.%20In%20this%20work%2C%20we%20present%20Wild%20Visual%20Navigation%20%28WVN%29%2C%0Aan%20online%20self-supervised%20learning%20system%20for%20visual%20traversability%20estimation.%0AThe%20system%20is%20able%20to%20continuously%20adapt%20from%20a%20short%20human%20demonstration%20in%0Athe%20field%2C%20only%20using%20onboard%20sensing%20and%20computing.%20One%20of%20the%20key%20ideas%20to%0Aachieve%20this%20is%20the%20use%20of%20high-dimensional%20features%20from%20pre-trained%0Aself-supervised%20models%2C%20which%20implicitly%20encode%20semantic%20information%20that%0Amassively%20simplifies%20the%20learning%20task.%20Further%2C%20the%20development%20of%20an%20online%0Ascheme%20for%20supervision%20generator%20enables%20concurrent%20training%20and%20inference%20of%0Athe%20learned%20model%20in%20the%20wild.%20We%20demonstrate%20our%20approach%20through%20diverse%0Areal-world%20deployments%20in%20forests%2C%20parks%2C%20and%20grasslands.%20Our%20system%20is%20able%20to%0Abootstrap%20the%20traversable%20terrain%20segmentation%20in%20less%20than%205%20min%20of%20in-field%0Atraining%20time%2C%20enabling%20the%20robot%20to%20navigate%20in%20complex%2C%20previously%20unseen%0Aoutdoor%20terrains.%20Code%3A%20https%3A//bit.ly/498b0CV%20-%20Project%0Apage%3Ahttps%3A//bit.ly/3M6nMHH%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07110v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wild%20Visual%20Navigation%3A%20Fast%20Traversability%20Learning%20via%20Pre-Trained%0A%20%20Models%20and%20Online%20Self-Supervision&entry.906535625=Mat%C3%ADas%20Mattamala%20and%20Jonas%20Frey%20and%20Piotr%20Libera%20and%20Nived%20Chebrolu%20and%20Georg%20Martius%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%20and%20Maurice%20Fallon&entry.1292438233=%20%20Natural%20environments%20such%20as%20forests%20and%20grasslands%20are%20challenging%20for%0Arobotic%20navigation%20because%20of%20the%20false%20perception%20of%20rigid%20obstacles%20from%20high%0Agrass%2C%20twigs%2C%20or%20bushes.%20In%20this%20work%2C%20we%20present%20Wild%20Visual%20Navigation%20%28WVN%29%2C%0Aan%20online%20self-supervised%20learning%20system%20for%20visual%20traversability%20estimation.%0AThe%20system%20is%20able%20to%20continuously%20adapt%20from%20a%20short%20human%20demonstration%20in%0Athe%20field%2C%20only%20using%20onboard%20sensing%20and%20computing.%20One%20of%20the%20key%20ideas%20to%0Aachieve%20this%20is%20the%20use%20of%20high-dimensional%20features%20from%20pre-trained%0Aself-supervised%20models%2C%20which%20implicitly%20encode%20semantic%20information%20that%0Amassively%20simplifies%20the%20learning%20task.%20Further%2C%20the%20development%20of%20an%20online%0Ascheme%20for%20supervision%20generator%20enables%20concurrent%20training%20and%20inference%20of%0Athe%20learned%20model%20in%20the%20wild.%20We%20demonstrate%20our%20approach%20through%20diverse%0Areal-world%20deployments%20in%20forests%2C%20parks%2C%20and%20grasslands.%20Our%20system%20is%20able%20to%0Abootstrap%20the%20traversable%20terrain%20segmentation%20in%20less%20than%205%20min%20of%20in-field%0Atraining%20time%2C%20enabling%20the%20robot%20to%20navigate%20in%20complex%2C%20previously%20unseen%0Aoutdoor%20terrains.%20Code%3A%20https%3A//bit.ly/498b0CV%20-%20Project%0Apage%3Ahttps%3A//bit.ly/3M6nMHH%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07110v1&entry.124074799=Read"},
{"title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth\n  Diffusion", "author": "Jaidev Shriram and Alex Trevithick and Lingjie Liu and Ravi Ramamoorthi", "abstract": "  We introduce RealmDreamer, a technique for generation of general\nforward-facing 3D scenes from text descriptions. Our technique optimizes a 3D\nGaussian Splatting representation to match complex text prompts. We initialize\nthese splats by utilizing the state-of-the-art text-to-image generators,\nlifting their samples into 3D, and computing the occlusion volume. We then\noptimize this representation across multiple views as a 3D inpainting task with\nimage-conditional diffusion models. To learn correct geometric structure, we\nincorporate a depth diffusion model by conditioning on the samples from the\ninpainting model, giving rich geometric structure. Finally, we finetune the\nmodel using sharpened samples from image generators. Notably, our technique\ndoes not require video or multi-view data and can synthesize a variety of\nhigh-quality 3D scenes in different styles, consisting of multiple objects. Its\ngenerality additionally allows 3D synthesis from a single image.\n", "link": "http://arxiv.org/abs/2404.07199v1", "date": "2024-04-10", "relevancy": 1.7225, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5784}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5733}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RealmDreamer%3A%20Text-Driven%203D%20Scene%20Generation%20with%20Inpainting%20and%20Depth%0A%20%20Diffusion&body=Title%3A%20RealmDreamer%3A%20Text-Driven%203D%20Scene%20Generation%20with%20Inpainting%20and%20Depth%0A%20%20Diffusion%0AAuthor%3A%20Jaidev%20Shriram%20and%20Alex%20Trevithick%20and%20Lingjie%20Liu%20and%20Ravi%20Ramamoorthi%0AAbstract%3A%20%20%20We%20introduce%20RealmDreamer%2C%20a%20technique%20for%20generation%20of%20general%0Aforward-facing%203D%20scenes%20from%20text%20descriptions.%20Our%20technique%20optimizes%20a%203D%0AGaussian%20Splatting%20representation%20to%20match%20complex%20text%20prompts.%20We%20initialize%0Athese%20splats%20by%20utilizing%20the%20state-of-the-art%20text-to-image%20generators%2C%0Alifting%20their%20samples%20into%203D%2C%20and%20computing%20the%20occlusion%20volume.%20We%20then%0Aoptimize%20this%20representation%20across%20multiple%20views%20as%20a%203D%20inpainting%20task%20with%0Aimage-conditional%20diffusion%20models.%20To%20learn%20correct%20geometric%20structure%2C%20we%0Aincorporate%20a%20depth%20diffusion%20model%20by%20conditioning%20on%20the%20samples%20from%20the%0Ainpainting%20model%2C%20giving%20rich%20geometric%20structure.%20Finally%2C%20we%20finetune%20the%0Amodel%20using%20sharpened%20samples%20from%20image%20generators.%20Notably%2C%20our%20technique%0Adoes%20not%20require%20video%20or%20multi-view%20data%20and%20can%20synthesize%20a%20variety%20of%0Ahigh-quality%203D%20scenes%20in%20different%20styles%2C%20consisting%20of%20multiple%20objects.%20Its%0Agenerality%20additionally%20allows%203D%20synthesis%20from%20a%20single%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07199v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealmDreamer%3A%20Text-Driven%203D%20Scene%20Generation%20with%20Inpainting%20and%20Depth%0A%20%20Diffusion&entry.906535625=Jaidev%20Shriram%20and%20Alex%20Trevithick%20and%20Lingjie%20Liu%20and%20Ravi%20Ramamoorthi&entry.1292438233=%20%20We%20introduce%20RealmDreamer%2C%20a%20technique%20for%20generation%20of%20general%0Aforward-facing%203D%20scenes%20from%20text%20descriptions.%20Our%20technique%20optimizes%20a%203D%0AGaussian%20Splatting%20representation%20to%20match%20complex%20text%20prompts.%20We%20initialize%0Athese%20splats%20by%20utilizing%20the%20state-of-the-art%20text-to-image%20generators%2C%0Alifting%20their%20samples%20into%203D%2C%20and%20computing%20the%20occlusion%20volume.%20We%20then%0Aoptimize%20this%20representation%20across%20multiple%20views%20as%20a%203D%20inpainting%20task%20with%0Aimage-conditional%20diffusion%20models.%20To%20learn%20correct%20geometric%20structure%2C%20we%0Aincorporate%20a%20depth%20diffusion%20model%20by%20conditioning%20on%20the%20samples%20from%20the%0Ainpainting%20model%2C%20giving%20rich%20geometric%20structure.%20Finally%2C%20we%20finetune%20the%0Amodel%20using%20sharpened%20samples%20from%20image%20generators.%20Notably%2C%20our%20technique%0Adoes%20not%20require%20video%20or%20multi-view%20data%20and%20can%20synthesize%20a%20variety%20of%0Ahigh-quality%203D%20scenes%20in%20different%20styles%2C%20consisting%20of%20multiple%20objects.%20Its%0Agenerality%20additionally%20allows%203D%20synthesis%20from%20a%20single%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07199v1&entry.124074799=Read"},
{"title": "Understanding Video Transformers via Universal Concept Discovery", "author": "Matthew Kowal and Achal Dave and Rares Ambrus and Adrien Gaidon and Konstantinos G. Derpanis and Pavel Tokmakov", "abstract": "  This paper studies the problem of concept-based interpretability of\ntransformer representations for videos. Concretely, we seek to explain the\ndecision-making process of video transformers based on high-level,\nspatiotemporal concepts that are automatically discovered. Prior research on\nconcept-based interpretability has concentrated solely on image-level tasks.\nComparatively, video models deal with the added temporal dimension, increasing\ncomplexity and posing challenges in identifying dynamic concepts over time. In\nthis work, we systematically address these challenges by introducing the first\nVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we propose\nan efficient approach for unsupervised identification of units of video\ntransformer representations - concepts, and ranking their importance to the\noutput of a model. The resulting concepts are highly interpretable, revealing\nspatio-temporal reasoning mechanisms and object-centric representations in\nunstructured video models. Performing this analysis jointly over a diverse set\nof supervised and self-supervised representations, we discover that some of\nthese mechanism are universal in video transformers. Finally, we show that VTCD\ncan be used for fine-grained action recognition and video object segmentation.\n", "link": "http://arxiv.org/abs/2401.10831v3", "date": "2024-04-10", "relevancy": 1.7202, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5825}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5735}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Understanding%20Video%20Transformers%20via%20Universal%20Concept%20Discovery&body=Title%3A%20Understanding%20Video%20Transformers%20via%20Universal%20Concept%20Discovery%0AAuthor%3A%20Matthew%20Kowal%20and%20Achal%20Dave%20and%20Rares%20Ambrus%20and%20Adrien%20Gaidon%20and%20Konstantinos%20G.%20Derpanis%20and%20Pavel%20Tokmakov%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20concept-based%20interpretability%20of%0Atransformer%20representations%20for%20videos.%20Concretely%2C%20we%20seek%20to%20explain%20the%0Adecision-making%20process%20of%20video%20transformers%20based%20on%20high-level%2C%0Aspatiotemporal%20concepts%20that%20are%20automatically%20discovered.%20Prior%20research%20on%0Aconcept-based%20interpretability%20has%20concentrated%20solely%20on%20image-level%20tasks.%0AComparatively%2C%20video%20models%20deal%20with%20the%20added%20temporal%20dimension%2C%20increasing%0Acomplexity%20and%20posing%20challenges%20in%20identifying%20dynamic%20concepts%20over%20time.%20In%0Athis%20work%2C%20we%20systematically%20address%20these%20challenges%20by%20introducing%20the%20first%0AVideo%20Transformer%20Concept%20Discovery%20%28VTCD%29%20algorithm.%20To%20this%20end%2C%20we%20propose%0Aan%20efficient%20approach%20for%20unsupervised%20identification%20of%20units%20of%20video%0Atransformer%20representations%20-%20concepts%2C%20and%20ranking%20their%20importance%20to%20the%0Aoutput%20of%20a%20model.%20The%20resulting%20concepts%20are%20highly%20interpretable%2C%20revealing%0Aspatio-temporal%20reasoning%20mechanisms%20and%20object-centric%20representations%20in%0Aunstructured%20video%20models.%20Performing%20this%20analysis%20jointly%20over%20a%20diverse%20set%0Aof%20supervised%20and%20self-supervised%20representations%2C%20we%20discover%20that%20some%20of%0Athese%20mechanism%20are%20universal%20in%20video%20transformers.%20Finally%2C%20we%20show%20that%20VTCD%0Acan%20be%20used%20for%20fine-grained%20action%20recognition%20and%20video%20object%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10831v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Video%20Transformers%20via%20Universal%20Concept%20Discovery&entry.906535625=Matthew%20Kowal%20and%20Achal%20Dave%20and%20Rares%20Ambrus%20and%20Adrien%20Gaidon%20and%20Konstantinos%20G.%20Derpanis%20and%20Pavel%20Tokmakov&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20concept-based%20interpretability%20of%0Atransformer%20representations%20for%20videos.%20Concretely%2C%20we%20seek%20to%20explain%20the%0Adecision-making%20process%20of%20video%20transformers%20based%20on%20high-level%2C%0Aspatiotemporal%20concepts%20that%20are%20automatically%20discovered.%20Prior%20research%20on%0Aconcept-based%20interpretability%20has%20concentrated%20solely%20on%20image-level%20tasks.%0AComparatively%2C%20video%20models%20deal%20with%20the%20added%20temporal%20dimension%2C%20increasing%0Acomplexity%20and%20posing%20challenges%20in%20identifying%20dynamic%20concepts%20over%20time.%20In%0Athis%20work%2C%20we%20systematically%20address%20these%20challenges%20by%20introducing%20the%20first%0AVideo%20Transformer%20Concept%20Discovery%20%28VTCD%29%20algorithm.%20To%20this%20end%2C%20we%20propose%0Aan%20efficient%20approach%20for%20unsupervised%20identification%20of%20units%20of%20video%0Atransformer%20representations%20-%20concepts%2C%20and%20ranking%20their%20importance%20to%20the%0Aoutput%20of%20a%20model.%20The%20resulting%20concepts%20are%20highly%20interpretable%2C%20revealing%0Aspatio-temporal%20reasoning%20mechanisms%20and%20object-centric%20representations%20in%0Aunstructured%20video%20models.%20Performing%20this%20analysis%20jointly%20over%20a%20diverse%20set%0Aof%20supervised%20and%20self-supervised%20representations%2C%20we%20discover%20that%20some%20of%0Athese%20mechanism%20are%20universal%20in%20video%20transformers.%20Finally%2C%20we%20show%20that%20VTCD%0Acan%20be%20used%20for%20fine-grained%20action%20recognition%20and%20video%20object%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10831v3&entry.124074799=Read"},
{"title": "Lost in Translation: Modern Neural Networks Still Struggle With Small\n  Realistic Image Transformations", "author": "Ofir Shifman and Yair Weiss", "abstract": "  Deep neural networks that achieve remarkable performance in image\nclassification have previously been shown to be easily fooled by tiny\ntransformations such as a one pixel translation of the input image. In order to\naddress this problem, two approaches have been proposed in recent years. The\nfirst approach suggests using huge datasets together with data augmentation in\nthe hope that a highly varied training set will teach the network to learn to\nbe invariant. The second approach suggests using architectural modifications\nbased on sampling theory to deal explicitly with image translations. In this\npaper, we show that these approaches still fall short in robustly handling\n'natural' image translations that simulate a subtle change in camera\norientation. Our findings reveal that a mere one-pixel translation can result\nin a significant change in the predicted image representation for approximately\n40% of the test images in state-of-the-art models (e.g. open-CLIP trained on\nLAION-2B or DINO-v2) , while models that are explicitly constructed to be\nrobust to cyclic translations can still be fooled with 1 pixel realistic\n(non-cyclic) translations 11% of the time. We present Robust Inference by Crop\nSelection: a simple method that can be proven to achieve any desired level of\nconsistency, although with a modest tradeoff with the model's accuracy.\nImportantly, we demonstrate how employing this method reduces the ability to\nfool state-of-the-art models with a 1 pixel translation to less than 5% while\nsuffering from only a 1% drop in classification accuracy. Additionally, we show\nthat our method can be easy adjusted to deal with circular shifts as well. In\nsuch case we achieve 100% robustness to integer shifts with state-of-the-art\naccuracy, and with no need for any further training.\n", "link": "http://arxiv.org/abs/2404.07153v1", "date": "2024-04-10", "relevancy": 1.7146, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5937}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.583}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Translation%3A%20Modern%20Neural%20Networks%20Still%20Struggle%20With%20Small%0A%20%20Realistic%20Image%20Transformations&body=Title%3A%20Lost%20in%20Translation%3A%20Modern%20Neural%20Networks%20Still%20Struggle%20With%20Small%0A%20%20Realistic%20Image%20Transformations%0AAuthor%3A%20Ofir%20Shifman%20and%20Yair%20Weiss%0AAbstract%3A%20%20%20Deep%20neural%20networks%20that%20achieve%20remarkable%20performance%20in%20image%0Aclassification%20have%20previously%20been%20shown%20to%20be%20easily%20fooled%20by%20tiny%0Atransformations%20such%20as%20a%20one%20pixel%20translation%20of%20the%20input%20image.%20In%20order%20to%0Aaddress%20this%20problem%2C%20two%20approaches%20have%20been%20proposed%20in%20recent%20years.%20The%0Afirst%20approach%20suggests%20using%20huge%20datasets%20together%20with%20data%20augmentation%20in%0Athe%20hope%20that%20a%20highly%20varied%20training%20set%20will%20teach%20the%20network%20to%20learn%20to%0Abe%20invariant.%20The%20second%20approach%20suggests%20using%20architectural%20modifications%0Abased%20on%20sampling%20theory%20to%20deal%20explicitly%20with%20image%20translations.%20In%20this%0Apaper%2C%20we%20show%20that%20these%20approaches%20still%20fall%20short%20in%20robustly%20handling%0A%27natural%27%20image%20translations%20that%20simulate%20a%20subtle%20change%20in%20camera%0Aorientation.%20Our%20findings%20reveal%20that%20a%20mere%20one-pixel%20translation%20can%20result%0Ain%20a%20significant%20change%20in%20the%20predicted%20image%20representation%20for%20approximately%0A40%25%20of%20the%20test%20images%20in%20state-of-the-art%20models%20%28e.g.%20open-CLIP%20trained%20on%0ALAION-2B%20or%20DINO-v2%29%20%2C%20while%20models%20that%20are%20explicitly%20constructed%20to%20be%0Arobust%20to%20cyclic%20translations%20can%20still%20be%20fooled%20with%201%20pixel%20realistic%0A%28non-cyclic%29%20translations%2011%25%20of%20the%20time.%20We%20present%20Robust%20Inference%20by%20Crop%0ASelection%3A%20a%20simple%20method%20that%20can%20be%20proven%20to%20achieve%20any%20desired%20level%20of%0Aconsistency%2C%20although%20with%20a%20modest%20tradeoff%20with%20the%20model%27s%20accuracy.%0AImportantly%2C%20we%20demonstrate%20how%20employing%20this%20method%20reduces%20the%20ability%20to%0Afool%20state-of-the-art%20models%20with%20a%201%20pixel%20translation%20to%20less%20than%205%25%20while%0Asuffering%20from%20only%20a%201%25%20drop%20in%20classification%20accuracy.%20Additionally%2C%20we%20show%0Athat%20our%20method%20can%20be%20easy%20adjusted%20to%20deal%20with%20circular%20shifts%20as%20well.%20In%0Asuch%20case%20we%20achieve%20100%25%20robustness%20to%20integer%20shifts%20with%20state-of-the-art%0Aaccuracy%2C%20and%20with%20no%20need%20for%20any%20further%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07153v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Translation%3A%20Modern%20Neural%20Networks%20Still%20Struggle%20With%20Small%0A%20%20Realistic%20Image%20Transformations&entry.906535625=Ofir%20Shifman%20and%20Yair%20Weiss&entry.1292438233=%20%20Deep%20neural%20networks%20that%20achieve%20remarkable%20performance%20in%20image%0Aclassification%20have%20previously%20been%20shown%20to%20be%20easily%20fooled%20by%20tiny%0Atransformations%20such%20as%20a%20one%20pixel%20translation%20of%20the%20input%20image.%20In%20order%20to%0Aaddress%20this%20problem%2C%20two%20approaches%20have%20been%20proposed%20in%20recent%20years.%20The%0Afirst%20approach%20suggests%20using%20huge%20datasets%20together%20with%20data%20augmentation%20in%0Athe%20hope%20that%20a%20highly%20varied%20training%20set%20will%20teach%20the%20network%20to%20learn%20to%0Abe%20invariant.%20The%20second%20approach%20suggests%20using%20architectural%20modifications%0Abased%20on%20sampling%20theory%20to%20deal%20explicitly%20with%20image%20translations.%20In%20this%0Apaper%2C%20we%20show%20that%20these%20approaches%20still%20fall%20short%20in%20robustly%20handling%0A%27natural%27%20image%20translations%20that%20simulate%20a%20subtle%20change%20in%20camera%0Aorientation.%20Our%20findings%20reveal%20that%20a%20mere%20one-pixel%20translation%20can%20result%0Ain%20a%20significant%20change%20in%20the%20predicted%20image%20representation%20for%20approximately%0A40%25%20of%20the%20test%20images%20in%20state-of-the-art%20models%20%28e.g.%20open-CLIP%20trained%20on%0ALAION-2B%20or%20DINO-v2%29%20%2C%20while%20models%20that%20are%20explicitly%20constructed%20to%20be%0Arobust%20to%20cyclic%20translations%20can%20still%20be%20fooled%20with%201%20pixel%20realistic%0A%28non-cyclic%29%20translations%2011%25%20of%20the%20time.%20We%20present%20Robust%20Inference%20by%20Crop%0ASelection%3A%20a%20simple%20method%20that%20can%20be%20proven%20to%20achieve%20any%20desired%20level%20of%0Aconsistency%2C%20although%20with%20a%20modest%20tradeoff%20with%20the%20model%27s%20accuracy.%0AImportantly%2C%20we%20demonstrate%20how%20employing%20this%20method%20reduces%20the%20ability%20to%0Afool%20state-of-the-art%20models%20with%20a%201%20pixel%20translation%20to%20less%20than%205%25%20while%0Asuffering%20from%20only%20a%201%25%20drop%20in%20classification%20accuracy.%20Additionally%2C%20we%20show%0Athat%20our%20method%20can%20be%20easy%20adjusted%20to%20deal%20with%20circular%20shifts%20as%20well.%20In%0Asuch%20case%20we%20achieve%20100%25%20robustness%20to%20integer%20shifts%20with%20state-of-the-art%0Aaccuracy%2C%20and%20with%20no%20need%20for%20any%20further%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07153v1&entry.124074799=Read"},
{"title": "XNLIeu: a dataset for cross-lingual NLI in Basque", "author": "Maite Heredia and Julen Etxaniz and Muitze Zulaika and Xabier Saralegi and Jeremy Barnes and Aitor Soroa", "abstract": "  XNLI is a popular Natural Language Inference (NLI) benchmark widely used to\nevaluate cross-lingual Natural Language Understanding (NLU) capabilities across\nlanguages. In this paper, we expand XNLI to include Basque, a low-resource\nlanguage that can greatly benefit from transfer-learning approaches. The new\ndataset, dubbed XNLIeu, has been developed by first machine-translating the\nEnglish XNLI corpus into Basque, followed by a manual post-edition step. We\nhave conducted a series of experiments using mono- and multilingual LLMs to\nassess a) the effect of professional post-edition on the MT system; b) the best\ncross-lingual strategy for NLI in Basque; and c) whether the choice of the best\ncross-lingual strategy is influenced by the fact that the dataset is built by\ntranslation. The results show that post-edition is necessary and that the\ntranslate-train cross-lingual strategy obtains better results overall, although\nthe gain is lower when tested in a dataset that has been built natively from\nscratch. Our code and datasets are publicly available under open licenses.\n", "link": "http://arxiv.org/abs/2404.06996v1", "date": "2024-04-10", "relevancy": 1.7073, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4502}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XNLIeu%3A%20a%20dataset%20for%20cross-lingual%20NLI%20in%20Basque&body=Title%3A%20XNLIeu%3A%20a%20dataset%20for%20cross-lingual%20NLI%20in%20Basque%0AAuthor%3A%20Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Muitze%20Zulaika%20and%20Xabier%20Saralegi%20and%20Jeremy%20Barnes%20and%20Aitor%20Soroa%0AAbstract%3A%20%20%20XNLI%20is%20a%20popular%20Natural%20Language%20Inference%20%28NLI%29%20benchmark%20widely%20used%20to%0Aevaluate%20cross-lingual%20Natural%20Language%20Understanding%20%28NLU%29%20capabilities%20across%0Alanguages.%20In%20this%20paper%2C%20we%20expand%20XNLI%20to%20include%20Basque%2C%20a%20low-resource%0Alanguage%20that%20can%20greatly%20benefit%20from%20transfer-learning%20approaches.%20The%20new%0Adataset%2C%20dubbed%20XNLIeu%2C%20has%20been%20developed%20by%20first%20machine-translating%20the%0AEnglish%20XNLI%20corpus%20into%20Basque%2C%20followed%20by%20a%20manual%20post-edition%20step.%20We%0Ahave%20conducted%20a%20series%20of%20experiments%20using%20mono-%20and%20multilingual%20LLMs%20to%0Aassess%20a%29%20the%20effect%20of%20professional%20post-edition%20on%20the%20MT%20system%3B%20b%29%20the%20best%0Across-lingual%20strategy%20for%20NLI%20in%20Basque%3B%20and%20c%29%20whether%20the%20choice%20of%20the%20best%0Across-lingual%20strategy%20is%20influenced%20by%20the%20fact%20that%20the%20dataset%20is%20built%20by%0Atranslation.%20The%20results%20show%20that%20post-edition%20is%20necessary%20and%20that%20the%0Atranslate-train%20cross-lingual%20strategy%20obtains%20better%20results%20overall%2C%20although%0Athe%20gain%20is%20lower%20when%20tested%20in%20a%20dataset%20that%20has%20been%20built%20natively%20from%0Ascratch.%20Our%20code%20and%20datasets%20are%20publicly%20available%20under%20open%20licenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06996v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XNLIeu%3A%20a%20dataset%20for%20cross-lingual%20NLI%20in%20Basque&entry.906535625=Maite%20Heredia%20and%20Julen%20Etxaniz%20and%20Muitze%20Zulaika%20and%20Xabier%20Saralegi%20and%20Jeremy%20Barnes%20and%20Aitor%20Soroa&entry.1292438233=%20%20XNLI%20is%20a%20popular%20Natural%20Language%20Inference%20%28NLI%29%20benchmark%20widely%20used%20to%0Aevaluate%20cross-lingual%20Natural%20Language%20Understanding%20%28NLU%29%20capabilities%20across%0Alanguages.%20In%20this%20paper%2C%20we%20expand%20XNLI%20to%20include%20Basque%2C%20a%20low-resource%0Alanguage%20that%20can%20greatly%20benefit%20from%20transfer-learning%20approaches.%20The%20new%0Adataset%2C%20dubbed%20XNLIeu%2C%20has%20been%20developed%20by%20first%20machine-translating%20the%0AEnglish%20XNLI%20corpus%20into%20Basque%2C%20followed%20by%20a%20manual%20post-edition%20step.%20We%0Ahave%20conducted%20a%20series%20of%20experiments%20using%20mono-%20and%20multilingual%20LLMs%20to%0Aassess%20a%29%20the%20effect%20of%20professional%20post-edition%20on%20the%20MT%20system%3B%20b%29%20the%20best%0Across-lingual%20strategy%20for%20NLI%20in%20Basque%3B%20and%20c%29%20whether%20the%20choice%20of%20the%20best%0Across-lingual%20strategy%20is%20influenced%20by%20the%20fact%20that%20the%20dataset%20is%20built%20by%0Atranslation.%20The%20results%20show%20that%20post-edition%20is%20necessary%20and%20that%20the%0Atranslate-train%20cross-lingual%20strategy%20obtains%20better%20results%20overall%2C%20although%0Athe%20gain%20is%20lower%20when%20tested%20in%20a%20dataset%20that%20has%20been%20built%20natively%20from%0Ascratch.%20Our%20code%20and%20datasets%20are%20publicly%20available%20under%20open%20licenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06996v1&entry.124074799=Read"},
{"title": "Toward industrial use of continual learning : new metrics proposal for\n  class incremental learning", "author": "Konat\u00e9 Mohamed Abbas and Anne-Fran\u00e7oise Yao and Thierry Chateau and Pierre Bouges", "abstract": "  In this paper, we investigate continual learning performance metrics used in\nclass incremental learning strategies for continual learning (CL) using some\nhigh performing methods. We investigate especially mean task accuracy. First,\nwe show that it lacks of expressiveness through some simple experiments to\ncapture performance. We show that monitoring average tasks performance is over\noptimistic and can lead to misleading conclusions for future real life\nindustrial uses. Then, we propose first a simple metric, Minimal Incremental\nClass Accuracy (MICA) which gives a fair and more useful evaluation of\ndifferent continual learning methods. Moreover, in order to provide a simple\nway to easily compare different methods performance in continual learning, we\nderive another single scalar metric that take into account the learning\nperformance variation as well as our newly introduced metric.\n", "link": "http://arxiv.org/abs/2404.06972v1", "date": "2024-04-10", "relevancy": 1.6951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4248}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4109}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20industrial%20use%20of%20continual%20learning%20%3A%20new%20metrics%20proposal%20for%0A%20%20class%20incremental%20learning&body=Title%3A%20Toward%20industrial%20use%20of%20continual%20learning%20%3A%20new%20metrics%20proposal%20for%0A%20%20class%20incremental%20learning%0AAuthor%3A%20Konat%C3%A9%20Mohamed%20Abbas%20and%20Anne-Fran%C3%A7oise%20Yao%20and%20Thierry%20Chateau%20and%20Pierre%20Bouges%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20continual%20learning%20performance%20metrics%20used%20in%0Aclass%20incremental%20learning%20strategies%20for%20continual%20learning%20%28CL%29%20using%20some%0Ahigh%20performing%20methods.%20We%20investigate%20especially%20mean%20task%20accuracy.%20First%2C%0Awe%20show%20that%20it%20lacks%20of%20expressiveness%20through%20some%20simple%20experiments%20to%0Acapture%20performance.%20We%20show%20that%20monitoring%20average%20tasks%20performance%20is%20over%0Aoptimistic%20and%20can%20lead%20to%20misleading%20conclusions%20for%20future%20real%20life%0Aindustrial%20uses.%20Then%2C%20we%20propose%20first%20a%20simple%20metric%2C%20Minimal%20Incremental%0AClass%20Accuracy%20%28MICA%29%20which%20gives%20a%20fair%20and%20more%20useful%20evaluation%20of%0Adifferent%20continual%20learning%20methods.%20Moreover%2C%20in%20order%20to%20provide%20a%20simple%0Away%20to%20easily%20compare%20different%20methods%20performance%20in%20continual%20learning%2C%20we%0Aderive%20another%20single%20scalar%20metric%20that%20take%20into%20account%20the%20learning%0Aperformance%20variation%20as%20well%20as%20our%20newly%20introduced%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06972v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20industrial%20use%20of%20continual%20learning%20%3A%20new%20metrics%20proposal%20for%0A%20%20class%20incremental%20learning&entry.906535625=Konat%C3%A9%20Mohamed%20Abbas%20and%20Anne-Fran%C3%A7oise%20Yao%20and%20Thierry%20Chateau%20and%20Pierre%20Bouges&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20continual%20learning%20performance%20metrics%20used%20in%0Aclass%20incremental%20learning%20strategies%20for%20continual%20learning%20%28CL%29%20using%20some%0Ahigh%20performing%20methods.%20We%20investigate%20especially%20mean%20task%20accuracy.%20First%2C%0Awe%20show%20that%20it%20lacks%20of%20expressiveness%20through%20some%20simple%20experiments%20to%0Acapture%20performance.%20We%20show%20that%20monitoring%20average%20tasks%20performance%20is%20over%0Aoptimistic%20and%20can%20lead%20to%20misleading%20conclusions%20for%20future%20real%20life%0Aindustrial%20uses.%20Then%2C%20we%20propose%20first%20a%20simple%20metric%2C%20Minimal%20Incremental%0AClass%20Accuracy%20%28MICA%29%20which%20gives%20a%20fair%20and%20more%20useful%20evaluation%20of%0Adifferent%20continual%20learning%20methods.%20Moreover%2C%20in%20order%20to%20provide%20a%20simple%0Away%20to%20easily%20compare%20different%20methods%20performance%20in%20continual%20learning%2C%20we%0Aderive%20another%20single%20scalar%20metric%20that%20take%20into%20account%20the%20learning%0Aperformance%20variation%20as%20well%20as%20our%20newly%20introduced%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06972v1&entry.124074799=Read"},
{"title": "LaPlaSS: Latent Space Planning for Stochastic Systems", "author": "Marlyse Reeves and Brian C. Williams", "abstract": "  Autonomous mobile agents often operate in hazardous environments,\nnecessitating an awareness of safety. These agents can have non-linear,\nstochastic dynamics that must be considered during planning to guarantee\nbounded risk. Most state of the art methods require closed-form dynamics to\nverify plan correctness and safety however modern robotic systems often have\ndynamics that are learned from data. Thus, there is a need to perform efficient\ntrajectory planning with guarantees on risk for agents without known dynamics\nmodels. We propose a \"generate-and-test\" approach to risk-bounded planning in\nwhich a planner generates a candidate trajectory using an approximate linear\ndynamics model and a validator assesses the risk of the trajectory, computing\nadditional safety constraints for the planner if the candidate does not satisfy\nthe desired risk bound. To acquire the approximate model, we use a variational\nautoencoder to learn a latent linear dynamics model and encode the planning\nproblem into the latent space to generate the candidate trajectory. The VAE\nalso serves to sample trajectories around the candidate to use in the\nvalidator. We demonstrate that our algorithm, LaPlaSS, is able to generate\ntrajectory plans with bounded risk for a real-world agent with learned dynamics\nand is an order of magnitude more efficient than the state of the art.\n", "link": "http://arxiv.org/abs/2404.07063v1", "date": "2024-04-10", "relevancy": 1.6393, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5524}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5221}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LaPlaSS%3A%20Latent%20Space%20Planning%20for%20Stochastic%20Systems&body=Title%3A%20LaPlaSS%3A%20Latent%20Space%20Planning%20for%20Stochastic%20Systems%0AAuthor%3A%20Marlyse%20Reeves%20and%20Brian%20C.%20Williams%0AAbstract%3A%20%20%20Autonomous%20mobile%20agents%20often%20operate%20in%20hazardous%20environments%2C%0Anecessitating%20an%20awareness%20of%20safety.%20These%20agents%20can%20have%20non-linear%2C%0Astochastic%20dynamics%20that%20must%20be%20considered%20during%20planning%20to%20guarantee%0Abounded%20risk.%20Most%20state%20of%20the%20art%20methods%20require%20closed-form%20dynamics%20to%0Averify%20plan%20correctness%20and%20safety%20however%20modern%20robotic%20systems%20often%20have%0Adynamics%20that%20are%20learned%20from%20data.%20Thus%2C%20there%20is%20a%20need%20to%20perform%20efficient%0Atrajectory%20planning%20with%20guarantees%20on%20risk%20for%20agents%20without%20known%20dynamics%0Amodels.%20We%20propose%20a%20%22generate-and-test%22%20approach%20to%20risk-bounded%20planning%20in%0Awhich%20a%20planner%20generates%20a%20candidate%20trajectory%20using%20an%20approximate%20linear%0Adynamics%20model%20and%20a%20validator%20assesses%20the%20risk%20of%20the%20trajectory%2C%20computing%0Aadditional%20safety%20constraints%20for%20the%20planner%20if%20the%20candidate%20does%20not%20satisfy%0Athe%20desired%20risk%20bound.%20To%20acquire%20the%20approximate%20model%2C%20we%20use%20a%20variational%0Aautoencoder%20to%20learn%20a%20latent%20linear%20dynamics%20model%20and%20encode%20the%20planning%0Aproblem%20into%20the%20latent%20space%20to%20generate%20the%20candidate%20trajectory.%20The%20VAE%0Aalso%20serves%20to%20sample%20trajectories%20around%20the%20candidate%20to%20use%20in%20the%0Avalidator.%20We%20demonstrate%20that%20our%20algorithm%2C%20LaPlaSS%2C%20is%20able%20to%20generate%0Atrajectory%20plans%20with%20bounded%20risk%20for%20a%20real-world%20agent%20with%20learned%20dynamics%0Aand%20is%20an%20order%20of%20magnitude%20more%20efficient%20than%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07063v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaPlaSS%3A%20Latent%20Space%20Planning%20for%20Stochastic%20Systems&entry.906535625=Marlyse%20Reeves%20and%20Brian%20C.%20Williams&entry.1292438233=%20%20Autonomous%20mobile%20agents%20often%20operate%20in%20hazardous%20environments%2C%0Anecessitating%20an%20awareness%20of%20safety.%20These%20agents%20can%20have%20non-linear%2C%0Astochastic%20dynamics%20that%20must%20be%20considered%20during%20planning%20to%20guarantee%0Abounded%20risk.%20Most%20state%20of%20the%20art%20methods%20require%20closed-form%20dynamics%20to%0Averify%20plan%20correctness%20and%20safety%20however%20modern%20robotic%20systems%20often%20have%0Adynamics%20that%20are%20learned%20from%20data.%20Thus%2C%20there%20is%20a%20need%20to%20perform%20efficient%0Atrajectory%20planning%20with%20guarantees%20on%20risk%20for%20agents%20without%20known%20dynamics%0Amodels.%20We%20propose%20a%20%22generate-and-test%22%20approach%20to%20risk-bounded%20planning%20in%0Awhich%20a%20planner%20generates%20a%20candidate%20trajectory%20using%20an%20approximate%20linear%0Adynamics%20model%20and%20a%20validator%20assesses%20the%20risk%20of%20the%20trajectory%2C%20computing%0Aadditional%20safety%20constraints%20for%20the%20planner%20if%20the%20candidate%20does%20not%20satisfy%0Athe%20desired%20risk%20bound.%20To%20acquire%20the%20approximate%20model%2C%20we%20use%20a%20variational%0Aautoencoder%20to%20learn%20a%20latent%20linear%20dynamics%20model%20and%20encode%20the%20planning%0Aproblem%20into%20the%20latent%20space%20to%20generate%20the%20candidate%20trajectory.%20The%20VAE%0Aalso%20serves%20to%20sample%20trajectories%20around%20the%20candidate%20to%20use%20in%20the%0Avalidator.%20We%20demonstrate%20that%20our%20algorithm%2C%20LaPlaSS%2C%20is%20able%20to%20generate%0Atrajectory%20plans%20with%20bounded%20risk%20for%20a%20real-world%20agent%20with%20learned%20dynamics%0Aand%20is%20an%20order%20of%20magnitude%20more%20efficient%20than%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07063v1&entry.124074799=Read"},
{"title": "Scaling Laws for Data Filtering -- Data Curation cannot be Compute\n  Agnostic", "author": "Sachin Goyal and Pratyush Maini and Zachary C. Lipton and Aditi Raghunathan and J. Zico Kolter", "abstract": "  Vision-language models (VLMs) are trained for thousands of GPU hours on\ncarefully curated web datasets. In recent times, data curation has gained\nprominence with several works developing strategies to retain 'high-quality'\nsubsets of 'raw' scraped data. For instance, the LAION public dataset retained\nonly 10% of the total crawled data. However, these strategies are typically\ndeveloped agnostic of the available compute for training. In this paper, we\nfirst demonstrate that making filtering decisions independent of training\ncompute is often suboptimal: the limited high-quality data rapidly loses its\nutility when repeated, eventually requiring the inclusion of 'unseen' but\n'lower-quality' data. To address this quality-quantity tradeoff\n($\\texttt{QQT}$), we introduce neural scaling laws that account for the\nnon-homogeneous nature of web data, an angle ignored in existing literature.\nOur scaling laws (i) characterize the $\\textit{differing}$ 'utility' of various\nquality subsets of web data; (ii) account for how utility diminishes for a data\npoint at its 'nth' repetition; and (iii) formulate the mutual interaction of\nvarious data pools when combined, enabling the estimation of model performance\non a combination of multiple data pools without ever jointly training on them.\nOur key message is that data curation $\\textit{cannot}$ be agnostic of the\ntotal compute that a model will be trained for. Our scaling laws allow us to\ncurate the best possible pool for achieving top performance on Datacomp at\nvarious compute budgets, carving out a pareto-frontier for data curation. Code\nis available at https://github.com/locuslab/scaling_laws_data_filtering.\n", "link": "http://arxiv.org/abs/2404.07177v1", "date": "2024-04-10", "relevancy": 1.4595, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5393}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.475}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scaling%20Laws%20for%20Data%20Filtering%20--%20Data%20Curation%20cannot%20be%20Compute%0A%20%20Agnostic&body=Title%3A%20Scaling%20Laws%20for%20Data%20Filtering%20--%20Data%20Curation%20cannot%20be%20Compute%0A%20%20Agnostic%0AAuthor%3A%20Sachin%20Goyal%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20Aditi%20Raghunathan%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20are%20trained%20for%20thousands%20of%20GPU%20hours%20on%0Acarefully%20curated%20web%20datasets.%20In%20recent%20times%2C%20data%20curation%20has%20gained%0Aprominence%20with%20several%20works%20developing%20strategies%20to%20retain%20%27high-quality%27%0Asubsets%20of%20%27raw%27%20scraped%20data.%20For%20instance%2C%20the%20LAION%20public%20dataset%20retained%0Aonly%2010%25%20of%20the%20total%20crawled%20data.%20However%2C%20these%20strategies%20are%20typically%0Adeveloped%20agnostic%20of%20the%20available%20compute%20for%20training.%20In%20this%20paper%2C%20we%0Afirst%20demonstrate%20that%20making%20filtering%20decisions%20independent%20of%20training%0Acompute%20is%20often%20suboptimal%3A%20the%20limited%20high-quality%20data%20rapidly%20loses%20its%0Autility%20when%20repeated%2C%20eventually%20requiring%20the%20inclusion%20of%20%27unseen%27%20but%0A%27lower-quality%27%20data.%20To%20address%20this%20quality-quantity%20tradeoff%0A%28%24%5Ctexttt%7BQQT%7D%24%29%2C%20we%20introduce%20neural%20scaling%20laws%20that%20account%20for%20the%0Anon-homogeneous%20nature%20of%20web%20data%2C%20an%20angle%20ignored%20in%20existing%20literature.%0AOur%20scaling%20laws%20%28i%29%20characterize%20the%20%24%5Ctextit%7Bdiffering%7D%24%20%27utility%27%20of%20various%0Aquality%20subsets%20of%20web%20data%3B%20%28ii%29%20account%20for%20how%20utility%20diminishes%20for%20a%20data%0Apoint%20at%20its%20%27nth%27%20repetition%3B%20and%20%28iii%29%20formulate%20the%20mutual%20interaction%20of%0Avarious%20data%20pools%20when%20combined%2C%20enabling%20the%20estimation%20of%20model%20performance%0Aon%20a%20combination%20of%20multiple%20data%20pools%20without%20ever%20jointly%20training%20on%20them.%0AOur%20key%20message%20is%20that%20data%20curation%20%24%5Ctextit%7Bcannot%7D%24%20be%20agnostic%20of%20the%0Atotal%20compute%20that%20a%20model%20will%20be%20trained%20for.%20Our%20scaling%20laws%20allow%20us%20to%0Acurate%20the%20best%20possible%20pool%20for%20achieving%20top%20performance%20on%20Datacomp%20at%0Avarious%20compute%20budgets%2C%20carving%20out%20a%20pareto-frontier%20for%20data%20curation.%20Code%0Ais%20available%20at%20https%3A//github.com/locuslab/scaling_laws_data_filtering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07177v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Laws%20for%20Data%20Filtering%20--%20Data%20Curation%20cannot%20be%20Compute%0A%20%20Agnostic&entry.906535625=Sachin%20Goyal%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20Aditi%20Raghunathan%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20are%20trained%20for%20thousands%20of%20GPU%20hours%20on%0Acarefully%20curated%20web%20datasets.%20In%20recent%20times%2C%20data%20curation%20has%20gained%0Aprominence%20with%20several%20works%20developing%20strategies%20to%20retain%20%27high-quality%27%0Asubsets%20of%20%27raw%27%20scraped%20data.%20For%20instance%2C%20the%20LAION%20public%20dataset%20retained%0Aonly%2010%25%20of%20the%20total%20crawled%20data.%20However%2C%20these%20strategies%20are%20typically%0Adeveloped%20agnostic%20of%20the%20available%20compute%20for%20training.%20In%20this%20paper%2C%20we%0Afirst%20demonstrate%20that%20making%20filtering%20decisions%20independent%20of%20training%0Acompute%20is%20often%20suboptimal%3A%20the%20limited%20high-quality%20data%20rapidly%20loses%20its%0Autility%20when%20repeated%2C%20eventually%20requiring%20the%20inclusion%20of%20%27unseen%27%20but%0A%27lower-quality%27%20data.%20To%20address%20this%20quality-quantity%20tradeoff%0A%28%24%5Ctexttt%7BQQT%7D%24%29%2C%20we%20introduce%20neural%20scaling%20laws%20that%20account%20for%20the%0Anon-homogeneous%20nature%20of%20web%20data%2C%20an%20angle%20ignored%20in%20existing%20literature.%0AOur%20scaling%20laws%20%28i%29%20characterize%20the%20%24%5Ctextit%7Bdiffering%7D%24%20%27utility%27%20of%20various%0Aquality%20subsets%20of%20web%20data%3B%20%28ii%29%20account%20for%20how%20utility%20diminishes%20for%20a%20data%0Apoint%20at%20its%20%27nth%27%20repetition%3B%20and%20%28iii%29%20formulate%20the%20mutual%20interaction%20of%0Avarious%20data%20pools%20when%20combined%2C%20enabling%20the%20estimation%20of%20model%20performance%0Aon%20a%20combination%20of%20multiple%20data%20pools%20without%20ever%20jointly%20training%20on%20them.%0AOur%20key%20message%20is%20that%20data%20curation%20%24%5Ctextit%7Bcannot%7D%24%20be%20agnostic%20of%20the%0Atotal%20compute%20that%20a%20model%20will%20be%20trained%20for.%20Our%20scaling%20laws%20allow%20us%20to%0Acurate%20the%20best%20possible%20pool%20for%20achieving%20top%20performance%20on%20Datacomp%20at%0Avarious%20compute%20budgets%2C%20carving%20out%20a%20pareto-frontier%20for%20data%20curation.%20Code%0Ais%20available%20at%20https%3A//github.com/locuslab/scaling_laws_data_filtering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07177v1&entry.124074799=Read"},
{"title": "Policy Optimization with Smooth Guidance Learned from State-Only\n  Demonstrations", "author": "Guojian Wang and Faguo Wu and Xiao Zhang and Tianyuan Chen and Zhiming Zheng", "abstract": "  The sparsity of reward feedback remains a challenging problem in online deep\nreinforcement learning (DRL). Previous approaches have utilized offline\ndemonstrations to achieve impressive results in multiple hard tasks. However,\nthese approaches place high demands on demonstration quality, and obtaining\nexpert-like actions is often costly and unrealistic. To tackle these problems,\nwe propose a simple and efficient algorithm called Policy Optimization with\nSmooth Guidance (POSG), which leverages a small set of state-only\ndemonstrations (where only state information is included in demonstrations) to\nindirectly make approximate and feasible long-term credit assignments and\nfacilitate exploration. Specifically, we first design a trajectory-importance\nevaluation mechanism to determine the quality of the current trajectory against\ndemonstrations. Then, we introduce a guidance reward computation technology\nbased on trajectory importance to measure the impact of each state-action pair.\nWe theoretically analyze the performance improvement caused by smooth guidance\nrewards and derive a new worst-case lower bound on the performance improvement.\nExtensive results demonstrate POSG's significant advantages in control\nperformance and convergence speed in four sparse-reward environments, including\nthe grid-world maze, Hopper-v4, HalfCheetah-v4, and Ant maze. Notably, the\nspecific metrics and quantifiable results are investigated to demonstrate the\nsuperiority of POSG.\n", "link": "http://arxiv.org/abs/2401.00162v2", "date": "2024-04-10", "relevancy": 1.5996, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5587}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5429}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Policy%20Optimization%20with%20Smooth%20Guidance%20Learned%20from%20State-Only%0A%20%20Demonstrations&body=Title%3A%20Policy%20Optimization%20with%20Smooth%20Guidance%20Learned%20from%20State-Only%0A%20%20Demonstrations%0AAuthor%3A%20Guojian%20Wang%20and%20Faguo%20Wu%20and%20Xiao%20Zhang%20and%20Tianyuan%20Chen%20and%20Zhiming%20Zheng%0AAbstract%3A%20%20%20The%20sparsity%20of%20reward%20feedback%20remains%20a%20challenging%20problem%20in%20online%20deep%0Areinforcement%20learning%20%28DRL%29.%20Previous%20approaches%20have%20utilized%20offline%0Ademonstrations%20to%20achieve%20impressive%20results%20in%20multiple%20hard%20tasks.%20However%2C%0Athese%20approaches%20place%20high%20demands%20on%20demonstration%20quality%2C%20and%20obtaining%0Aexpert-like%20actions%20is%20often%20costly%20and%20unrealistic.%20To%20tackle%20these%20problems%2C%0Awe%20propose%20a%20simple%20and%20efficient%20algorithm%20called%20Policy%20Optimization%20with%0ASmooth%20Guidance%20%28POSG%29%2C%20which%20leverages%20a%20small%20set%20of%20state-only%0Ademonstrations%20%28where%20only%20state%20information%20is%20included%20in%20demonstrations%29%20to%0Aindirectly%20make%20approximate%20and%20feasible%20long-term%20credit%20assignments%20and%0Afacilitate%20exploration.%20Specifically%2C%20we%20first%20design%20a%20trajectory-importance%0Aevaluation%20mechanism%20to%20determine%20the%20quality%20of%20the%20current%20trajectory%20against%0Ademonstrations.%20Then%2C%20we%20introduce%20a%20guidance%20reward%20computation%20technology%0Abased%20on%20trajectory%20importance%20to%20measure%20the%20impact%20of%20each%20state-action%20pair.%0AWe%20theoretically%20analyze%20the%20performance%20improvement%20caused%20by%20smooth%20guidance%0Arewards%20and%20derive%20a%20new%20worst-case%20lower%20bound%20on%20the%20performance%20improvement.%0AExtensive%20results%20demonstrate%20POSG%27s%20significant%20advantages%20in%20control%0Aperformance%20and%20convergence%20speed%20in%20four%20sparse-reward%20environments%2C%20including%0Athe%20grid-world%20maze%2C%20Hopper-v4%2C%20HalfCheetah-v4%2C%20and%20Ant%20maze.%20Notably%2C%20the%0Aspecific%20metrics%20and%20quantifiable%20results%20are%20investigated%20to%20demonstrate%20the%0Asuperiority%20of%20POSG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00162v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Optimization%20with%20Smooth%20Guidance%20Learned%20from%20State-Only%0A%20%20Demonstrations&entry.906535625=Guojian%20Wang%20and%20Faguo%20Wu%20and%20Xiao%20Zhang%20and%20Tianyuan%20Chen%20and%20Zhiming%20Zheng&entry.1292438233=%20%20The%20sparsity%20of%20reward%20feedback%20remains%20a%20challenging%20problem%20in%20online%20deep%0Areinforcement%20learning%20%28DRL%29.%20Previous%20approaches%20have%20utilized%20offline%0Ademonstrations%20to%20achieve%20impressive%20results%20in%20multiple%20hard%20tasks.%20However%2C%0Athese%20approaches%20place%20high%20demands%20on%20demonstration%20quality%2C%20and%20obtaining%0Aexpert-like%20actions%20is%20often%20costly%20and%20unrealistic.%20To%20tackle%20these%20problems%2C%0Awe%20propose%20a%20simple%20and%20efficient%20algorithm%20called%20Policy%20Optimization%20with%0ASmooth%20Guidance%20%28POSG%29%2C%20which%20leverages%20a%20small%20set%20of%20state-only%0Ademonstrations%20%28where%20only%20state%20information%20is%20included%20in%20demonstrations%29%20to%0Aindirectly%20make%20approximate%20and%20feasible%20long-term%20credit%20assignments%20and%0Afacilitate%20exploration.%20Specifically%2C%20we%20first%20design%20a%20trajectory-importance%0Aevaluation%20mechanism%20to%20determine%20the%20quality%20of%20the%20current%20trajectory%20against%0Ademonstrations.%20Then%2C%20we%20introduce%20a%20guidance%20reward%20computation%20technology%0Abased%20on%20trajectory%20importance%20to%20measure%20the%20impact%20of%20each%20state-action%20pair.%0AWe%20theoretically%20analyze%20the%20performance%20improvement%20caused%20by%20smooth%20guidance%0Arewards%20and%20derive%20a%20new%20worst-case%20lower%20bound%20on%20the%20performance%20improvement.%0AExtensive%20results%20demonstrate%20POSG%27s%20significant%20advantages%20in%20control%0Aperformance%20and%20convergence%20speed%20in%20four%20sparse-reward%20environments%2C%20including%0Athe%20grid-world%20maze%2C%20Hopper-v4%2C%20HalfCheetah-v4%2C%20and%20Ant%20maze.%20Notably%2C%20the%0Aspecific%20metrics%20and%20quantifiable%20results%20are%20investigated%20to%20demonstrate%20the%0Asuperiority%20of%20POSG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00162v2&entry.124074799=Read"},
{"title": "PLAN: Variance-Aware Private Mean Estimation", "author": "Martin Aum\u00fcller and Christian Janos Lebeda and Boel Nelson and Rasmus Pagh", "abstract": "  Differentially private mean estimation is an important building block in\nprivacy-preserving algorithms for data analysis and machine learning. Though\nthe trade-off between privacy and utility is well understood in the worst case,\nmany datasets exhibit structure that could potentially be exploited to yield\nbetter algorithms. In this paper we present $\\textit{Private Limit Adapted\nNoise}$ (PLAN), a family of differentially private algorithms for mean\nestimation in the setting where inputs are independently sampled from a\ndistribution $\\mathcal{D}$ over $\\mathbf{R}^d$, with coordinate-wise standard\ndeviations $\\boldsymbol{\\sigma} \\in \\mathbf{R}^d$. Similar to mean estimation\nunder Mahalanobis distance, PLAN tailors the shape of the noise to the shape of\nthe data, but unlike previous algorithms the privacy budget is spent\nnon-uniformly over the coordinates. Under a concentration assumption on\n$\\mathcal{D}$, we show how to exploit skew in the vector $\\boldsymbol{\\sigma}$,\nobtaining a (zero-concentrated) differentially private mean estimate with\n$\\ell_2$ error proportional to $\\|\\boldsymbol{\\sigma}\\|_1$. Previous work has\neither not taken $\\boldsymbol{\\sigma}$ into account, or measured error in\nMahalanobis distance $\\unicode{x2013}$ in both cases resulting in $\\ell_2$\nerror proportional to $\\sqrt{d}\\|\\boldsymbol{\\sigma}\\|_2$, which can be up to a\nfactor $\\sqrt{d}$ larger. To verify the effectiveness of PLAN, we empirically\nevaluate accuracy on both synthetic and real world data.\n", "link": "http://arxiv.org/abs/2306.08745v3", "date": "2024-04-10", "relevancy": 1.3063, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.436}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4315}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PLAN%3A%20Variance-Aware%20Private%20Mean%20Estimation&body=Title%3A%20PLAN%3A%20Variance-Aware%20Private%20Mean%20Estimation%0AAuthor%3A%20Martin%20Aum%C3%BCller%20and%20Christian%20Janos%20Lebeda%20and%20Boel%20Nelson%20and%20Rasmus%20Pagh%0AAbstract%3A%20%20%20Differentially%20private%20mean%20estimation%20is%20an%20important%20building%20block%20in%0Aprivacy-preserving%20algorithms%20for%20data%20analysis%20and%20machine%20learning.%20Though%0Athe%20trade-off%20between%20privacy%20and%20utility%20is%20well%20understood%20in%20the%20worst%20case%2C%0Amany%20datasets%20exhibit%20structure%20that%20could%20potentially%20be%20exploited%20to%20yield%0Abetter%20algorithms.%20In%20this%20paper%20we%20present%20%24%5Ctextit%7BPrivate%20Limit%20Adapted%0ANoise%7D%24%20%28PLAN%29%2C%20a%20family%20of%20differentially%20private%20algorithms%20for%20mean%0Aestimation%20in%20the%20setting%20where%20inputs%20are%20independently%20sampled%20from%20a%0Adistribution%20%24%5Cmathcal%7BD%7D%24%20over%20%24%5Cmathbf%7BR%7D%5Ed%24%2C%20with%20coordinate-wise%20standard%0Adeviations%20%24%5Cboldsymbol%7B%5Csigma%7D%20%5Cin%20%5Cmathbf%7BR%7D%5Ed%24.%20Similar%20to%20mean%20estimation%0Aunder%20Mahalanobis%20distance%2C%20PLAN%20tailors%20the%20shape%20of%20the%20noise%20to%20the%20shape%20of%0Athe%20data%2C%20but%20unlike%20previous%20algorithms%20the%20privacy%20budget%20is%20spent%0Anon-uniformly%20over%20the%20coordinates.%20Under%20a%20concentration%20assumption%20on%0A%24%5Cmathcal%7BD%7D%24%2C%20we%20show%20how%20to%20exploit%20skew%20in%20the%20vector%20%24%5Cboldsymbol%7B%5Csigma%7D%24%2C%0Aobtaining%20a%20%28zero-concentrated%29%20differentially%20private%20mean%20estimate%20with%0A%24%5Cell_2%24%20error%20proportional%20to%20%24%5C%7C%5Cboldsymbol%7B%5Csigma%7D%5C%7C_1%24.%20Previous%20work%20has%0Aeither%20not%20taken%20%24%5Cboldsymbol%7B%5Csigma%7D%24%20into%20account%2C%20or%20measured%20error%20in%0AMahalanobis%20distance%20%24%5Cunicode%7Bx2013%7D%24%20in%20both%20cases%20resulting%20in%20%24%5Cell_2%24%0Aerror%20proportional%20to%20%24%5Csqrt%7Bd%7D%5C%7C%5Cboldsymbol%7B%5Csigma%7D%5C%7C_2%24%2C%20which%20can%20be%20up%20to%20a%0Afactor%20%24%5Csqrt%7Bd%7D%24%20larger.%20To%20verify%20the%20effectiveness%20of%20PLAN%2C%20we%20empirically%0Aevaluate%20accuracy%20on%20both%20synthetic%20and%20real%20world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08745v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLAN%3A%20Variance-Aware%20Private%20Mean%20Estimation&entry.906535625=Martin%20Aum%C3%BCller%20and%20Christian%20Janos%20Lebeda%20and%20Boel%20Nelson%20and%20Rasmus%20Pagh&entry.1292438233=%20%20Differentially%20private%20mean%20estimation%20is%20an%20important%20building%20block%20in%0Aprivacy-preserving%20algorithms%20for%20data%20analysis%20and%20machine%20learning.%20Though%0Athe%20trade-off%20between%20privacy%20and%20utility%20is%20well%20understood%20in%20the%20worst%20case%2C%0Amany%20datasets%20exhibit%20structure%20that%20could%20potentially%20be%20exploited%20to%20yield%0Abetter%20algorithms.%20In%20this%20paper%20we%20present%20%24%5Ctextit%7BPrivate%20Limit%20Adapted%0ANoise%7D%24%20%28PLAN%29%2C%20a%20family%20of%20differentially%20private%20algorithms%20for%20mean%0Aestimation%20in%20the%20setting%20where%20inputs%20are%20independently%20sampled%20from%20a%0Adistribution%20%24%5Cmathcal%7BD%7D%24%20over%20%24%5Cmathbf%7BR%7D%5Ed%24%2C%20with%20coordinate-wise%20standard%0Adeviations%20%24%5Cboldsymbol%7B%5Csigma%7D%20%5Cin%20%5Cmathbf%7BR%7D%5Ed%24.%20Similar%20to%20mean%20estimation%0Aunder%20Mahalanobis%20distance%2C%20PLAN%20tailors%20the%20shape%20of%20the%20noise%20to%20the%20shape%20of%0Athe%20data%2C%20but%20unlike%20previous%20algorithms%20the%20privacy%20budget%20is%20spent%0Anon-uniformly%20over%20the%20coordinates.%20Under%20a%20concentration%20assumption%20on%0A%24%5Cmathcal%7BD%7D%24%2C%20we%20show%20how%20to%20exploit%20skew%20in%20the%20vector%20%24%5Cboldsymbol%7B%5Csigma%7D%24%2C%0Aobtaining%20a%20%28zero-concentrated%29%20differentially%20private%20mean%20estimate%20with%0A%24%5Cell_2%24%20error%20proportional%20to%20%24%5C%7C%5Cboldsymbol%7B%5Csigma%7D%5C%7C_1%24.%20Previous%20work%20has%0Aeither%20not%20taken%20%24%5Cboldsymbol%7B%5Csigma%7D%24%20into%20account%2C%20or%20measured%20error%20in%0AMahalanobis%20distance%20%24%5Cunicode%7Bx2013%7D%24%20in%20both%20cases%20resulting%20in%20%24%5Cell_2%24%0Aerror%20proportional%20to%20%24%5Csqrt%7Bd%7D%5C%7C%5Cboldsymbol%7B%5Csigma%7D%5C%7C_2%24%2C%20which%20can%20be%20up%20to%20a%0Afactor%20%24%5Csqrt%7Bd%7D%24%20larger.%20To%20verify%20the%20effectiveness%20of%20PLAN%2C%20we%20empirically%0Aevaluate%20accuracy%20on%20both%20synthetic%20and%20real%20world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08745v3&entry.124074799=Read"},
{"title": "Ray-driven Spectral CT Reconstruction Based on Neural Base-Material\n  Fields", "author": "Ligen Shi and Chang Liu and Ping Yang and Jun Qiu and Xing Zhao", "abstract": "  In spectral CT reconstruction, the basis materials decomposition involves\nsolving a large-scale nonlinear system of integral equations, which is highly\nill-posed mathematically. This paper proposes a model that parameterizes the\nattenuation coefficients of the object using a neural field representation,\nthereby avoiding the complex calculations of pixel-driven projection\ncoefficient matrices during the discretization process of line integrals. It\nintroduces a lightweight discretization method for line integrals based on a\nray-driven neural field, enhancing the accuracy of the integral approximation\nduring the discretization process. The basis materials are represented as\ncontinuous vector-valued implicit functions to establish a neural field\nparameterization model for the basis materials. The auto-differentiation\nframework of deep learning is then used to solve the implicit continuous\nfunction of the neural base-material fields. This method is not limited by the\nspatial resolution of reconstructed images, and the network has compact and\nregular properties. Experimental validation shows that our method performs\nexceptionally well in addressing the spectral CT reconstruction. Additionally,\nit fulfils the requirements for the generation of high-resolution\nreconstruction images.\n", "link": "http://arxiv.org/abs/2404.06991v1", "date": "2024-04-10", "relevancy": 1.4833, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5212}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.487}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4862}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ray-driven%20Spectral%20CT%20Reconstruction%20Based%20on%20Neural%20Base-Material%0A%20%20Fields&body=Title%3A%20Ray-driven%20Spectral%20CT%20Reconstruction%20Based%20on%20Neural%20Base-Material%0A%20%20Fields%0AAuthor%3A%20Ligen%20Shi%20and%20Chang%20Liu%20and%20Ping%20Yang%20and%20Jun%20Qiu%20and%20Xing%20Zhao%0AAbstract%3A%20%20%20In%20spectral%20CT%20reconstruction%2C%20the%20basis%20materials%20decomposition%20involves%0Asolving%20a%20large-scale%20nonlinear%20system%20of%20integral%20equations%2C%20which%20is%20highly%0Aill-posed%20mathematically.%20This%20paper%20proposes%20a%20model%20that%20parameterizes%20the%0Aattenuation%20coefficients%20of%20the%20object%20using%20a%20neural%20field%20representation%2C%0Athereby%20avoiding%20the%20complex%20calculations%20of%20pixel-driven%20projection%0Acoefficient%20matrices%20during%20the%20discretization%20process%20of%20line%20integrals.%20It%0Aintroduces%20a%20lightweight%20discretization%20method%20for%20line%20integrals%20based%20on%20a%0Aray-driven%20neural%20field%2C%20enhancing%20the%20accuracy%20of%20the%20integral%20approximation%0Aduring%20the%20discretization%20process.%20The%20basis%20materials%20are%20represented%20as%0Acontinuous%20vector-valued%20implicit%20functions%20to%20establish%20a%20neural%20field%0Aparameterization%20model%20for%20the%20basis%20materials.%20The%20auto-differentiation%0Aframework%20of%20deep%20learning%20is%20then%20used%20to%20solve%20the%20implicit%20continuous%0Afunction%20of%20the%20neural%20base-material%20fields.%20This%20method%20is%20not%20limited%20by%20the%0Aspatial%20resolution%20of%20reconstructed%20images%2C%20and%20the%20network%20has%20compact%20and%0Aregular%20properties.%20Experimental%20validation%20shows%20that%20our%20method%20performs%0Aexceptionally%20well%20in%20addressing%20the%20spectral%20CT%20reconstruction.%20Additionally%2C%0Ait%20fulfils%20the%20requirements%20for%20the%20generation%20of%20high-resolution%0Areconstruction%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06991v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ray-driven%20Spectral%20CT%20Reconstruction%20Based%20on%20Neural%20Base-Material%0A%20%20Fields&entry.906535625=Ligen%20Shi%20and%20Chang%20Liu%20and%20Ping%20Yang%20and%20Jun%20Qiu%20and%20Xing%20Zhao&entry.1292438233=%20%20In%20spectral%20CT%20reconstruction%2C%20the%20basis%20materials%20decomposition%20involves%0Asolving%20a%20large-scale%20nonlinear%20system%20of%20integral%20equations%2C%20which%20is%20highly%0Aill-posed%20mathematically.%20This%20paper%20proposes%20a%20model%20that%20parameterizes%20the%0Aattenuation%20coefficients%20of%20the%20object%20using%20a%20neural%20field%20representation%2C%0Athereby%20avoiding%20the%20complex%20calculations%20of%20pixel-driven%20projection%0Acoefficient%20matrices%20during%20the%20discretization%20process%20of%20line%20integrals.%20It%0Aintroduces%20a%20lightweight%20discretization%20method%20for%20line%20integrals%20based%20on%20a%0Aray-driven%20neural%20field%2C%20enhancing%20the%20accuracy%20of%20the%20integral%20approximation%0Aduring%20the%20discretization%20process.%20The%20basis%20materials%20are%20represented%20as%0Acontinuous%20vector-valued%20implicit%20functions%20to%20establish%20a%20neural%20field%0Aparameterization%20model%20for%20the%20basis%20materials.%20The%20auto-differentiation%0Aframework%20of%20deep%20learning%20is%20then%20used%20to%20solve%20the%20implicit%20continuous%0Afunction%20of%20the%20neural%20base-material%20fields.%20This%20method%20is%20not%20limited%20by%20the%0Aspatial%20resolution%20of%20reconstructed%20images%2C%20and%20the%20network%20has%20compact%20and%0Aregular%20properties.%20Experimental%20validation%20shows%20that%20our%20method%20performs%0Aexceptionally%20well%20in%20addressing%20the%20spectral%20CT%20reconstruction.%20Additionally%2C%0Ait%20fulfils%20the%20requirements%20for%20the%20generation%20of%20high-resolution%0Areconstruction%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06991v1&entry.124074799=Read"},
{"title": "Designing Interpretable ML System to Enhance Trust in Healthcare: A\n  Systematic Review to Proposed Responsible Clinician-AI-Collaboration\n  Framework", "author": "Elham Nasarian and Roohallah Alizadehsani and U. Rajendra Acharya and Kwok-Leung Tsui", "abstract": "  This paper explores the significant impact of AI-based medical devices,\nincluding wearables, telemedicine, large language models, and digital twins, on\nclinical decision support systems. It emphasizes the importance of producing\noutcomes that are not only accurate but also interpretable and understandable\nto clinicians, addressing the risk that lack of interpretability poses in terms\nof mistrust and reluctance to adopt these technologies in healthcare. The paper\nreviews interpretable AI processes, methods, applications, and the challenges\nof implementation in healthcare, focusing on quality control to facilitate\nresponsible communication between AI systems and clinicians. It breaks down the\ninterpretability process into data pre-processing, model selection, and\npost-processing, aiming to foster a comprehensive understanding of the crucial\nrole of a robust interpretability approach in healthcare and to guide future\nresearch in this area. with insights for creating responsible clinician-AI\ntools for healthcare, as well as to offer a deeper understanding of the\nchallenges they might face. Our research questions, eligibility criteria and\nprimary goals were identified using Preferred Reporting Items for Systematic\nreviews and Meta-Analyses guideline and PICO method; PubMed, Scopus and Web of\nScience databases were systematically searched using sensitive and specific\nsearch strings. In the end, 52 publications were selected for data extraction\nwhich included 8 existing reviews and 44 related experimental studies. The\npaper offers general concepts of interpretable AI in healthcare and discuss\nthree-levels interpretability process. Additionally, it provides a\ncomprehensive discussion of evaluating robust interpretability AI in\nhealthcare. Moreover, this survey introduces a step-by-step roadmap for\nimplementing responsible AI in healthcare.\n", "link": "http://arxiv.org/abs/2311.11055v2", "date": "2024-04-10", "relevancy": 1.2871, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4642}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4207}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Designing%20Interpretable%20ML%20System%20to%20Enhance%20Trust%20in%20Healthcare%3A%20A%0A%20%20Systematic%20Review%20to%20Proposed%20Responsible%20Clinician-AI-Collaboration%0A%20%20Framework&body=Title%3A%20Designing%20Interpretable%20ML%20System%20to%20Enhance%20Trust%20in%20Healthcare%3A%20A%0A%20%20Systematic%20Review%20to%20Proposed%20Responsible%20Clinician-AI-Collaboration%0A%20%20Framework%0AAuthor%3A%20Elham%20Nasarian%20and%20Roohallah%20Alizadehsani%20and%20U.%20Rajendra%20Acharya%20and%20Kwok-Leung%20Tsui%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20significant%20impact%20of%20AI-based%20medical%20devices%2C%0Aincluding%20wearables%2C%20telemedicine%2C%20large%20language%20models%2C%20and%20digital%20twins%2C%20on%0Aclinical%20decision%20support%20systems.%20It%20emphasizes%20the%20importance%20of%20producing%0Aoutcomes%20that%20are%20not%20only%20accurate%20but%20also%20interpretable%20and%20understandable%0Ato%20clinicians%2C%20addressing%20the%20risk%20that%20lack%20of%20interpretability%20poses%20in%20terms%0Aof%20mistrust%20and%20reluctance%20to%20adopt%20these%20technologies%20in%20healthcare.%20The%20paper%0Areviews%20interpretable%20AI%20processes%2C%20methods%2C%20applications%2C%20and%20the%20challenges%0Aof%20implementation%20in%20healthcare%2C%20focusing%20on%20quality%20control%20to%20facilitate%0Aresponsible%20communication%20between%20AI%20systems%20and%20clinicians.%20It%20breaks%20down%20the%0Ainterpretability%20process%20into%20data%20pre-processing%2C%20model%20selection%2C%20and%0Apost-processing%2C%20aiming%20to%20foster%20a%20comprehensive%20understanding%20of%20the%20crucial%0Arole%20of%20a%20robust%20interpretability%20approach%20in%20healthcare%20and%20to%20guide%20future%0Aresearch%20in%20this%20area.%20with%20insights%20for%20creating%20responsible%20clinician-AI%0Atools%20for%20healthcare%2C%20as%20well%20as%20to%20offer%20a%20deeper%20understanding%20of%20the%0Achallenges%20they%20might%20face.%20Our%20research%20questions%2C%20eligibility%20criteria%20and%0Aprimary%20goals%20were%20identified%20using%20Preferred%20Reporting%20Items%20for%20Systematic%0Areviews%20and%20Meta-Analyses%20guideline%20and%20PICO%20method%3B%20PubMed%2C%20Scopus%20and%20Web%20of%0AScience%20databases%20were%20systematically%20searched%20using%20sensitive%20and%20specific%0Asearch%20strings.%20In%20the%20end%2C%2052%20publications%20were%20selected%20for%20data%20extraction%0Awhich%20included%208%20existing%20reviews%20and%2044%20related%20experimental%20studies.%20The%0Apaper%20offers%20general%20concepts%20of%20interpretable%20AI%20in%20healthcare%20and%20discuss%0Athree-levels%20interpretability%20process.%20Additionally%2C%20it%20provides%20a%0Acomprehensive%20discussion%20of%20evaluating%20robust%20interpretability%20AI%20in%0Ahealthcare.%20Moreover%2C%20this%20survey%20introduces%20a%20step-by-step%20roadmap%20for%0Aimplementing%20responsible%20AI%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11055v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Interpretable%20ML%20System%20to%20Enhance%20Trust%20in%20Healthcare%3A%20A%0A%20%20Systematic%20Review%20to%20Proposed%20Responsible%20Clinician-AI-Collaboration%0A%20%20Framework&entry.906535625=Elham%20Nasarian%20and%20Roohallah%20Alizadehsani%20and%20U.%20Rajendra%20Acharya%20and%20Kwok-Leung%20Tsui&entry.1292438233=%20%20This%20paper%20explores%20the%20significant%20impact%20of%20AI-based%20medical%20devices%2C%0Aincluding%20wearables%2C%20telemedicine%2C%20large%20language%20models%2C%20and%20digital%20twins%2C%20on%0Aclinical%20decision%20support%20systems.%20It%20emphasizes%20the%20importance%20of%20producing%0Aoutcomes%20that%20are%20not%20only%20accurate%20but%20also%20interpretable%20and%20understandable%0Ato%20clinicians%2C%20addressing%20the%20risk%20that%20lack%20of%20interpretability%20poses%20in%20terms%0Aof%20mistrust%20and%20reluctance%20to%20adopt%20these%20technologies%20in%20healthcare.%20The%20paper%0Areviews%20interpretable%20AI%20processes%2C%20methods%2C%20applications%2C%20and%20the%20challenges%0Aof%20implementation%20in%20healthcare%2C%20focusing%20on%20quality%20control%20to%20facilitate%0Aresponsible%20communication%20between%20AI%20systems%20and%20clinicians.%20It%20breaks%20down%20the%0Ainterpretability%20process%20into%20data%20pre-processing%2C%20model%20selection%2C%20and%0Apost-processing%2C%20aiming%20to%20foster%20a%20comprehensive%20understanding%20of%20the%20crucial%0Arole%20of%20a%20robust%20interpretability%20approach%20in%20healthcare%20and%20to%20guide%20future%0Aresearch%20in%20this%20area.%20with%20insights%20for%20creating%20responsible%20clinician-AI%0Atools%20for%20healthcare%2C%20as%20well%20as%20to%20offer%20a%20deeper%20understanding%20of%20the%0Achallenges%20they%20might%20face.%20Our%20research%20questions%2C%20eligibility%20criteria%20and%0Aprimary%20goals%20were%20identified%20using%20Preferred%20Reporting%20Items%20for%20Systematic%0Areviews%20and%20Meta-Analyses%20guideline%20and%20PICO%20method%3B%20PubMed%2C%20Scopus%20and%20Web%20of%0AScience%20databases%20were%20systematically%20searched%20using%20sensitive%20and%20specific%0Asearch%20strings.%20In%20the%20end%2C%2052%20publications%20were%20selected%20for%20data%20extraction%0Awhich%20included%208%20existing%20reviews%20and%2044%20related%20experimental%20studies.%20The%0Apaper%20offers%20general%20concepts%20of%20interpretable%20AI%20in%20healthcare%20and%20discuss%0Athree-levels%20interpretability%20process.%20Additionally%2C%20it%20provides%20a%0Acomprehensive%20discussion%20of%20evaluating%20robust%20interpretability%20AI%20in%0Ahealthcare.%20Moreover%2C%20this%20survey%20introduces%20a%20step-by-step%20roadmap%20for%0Aimplementing%20responsible%20AI%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11055v2&entry.124074799=Read"},
{"title": "Algorithms for Caching and MTS with reduced number of predictions", "author": "Karim Abdel Sadek and Marek Elias", "abstract": "  ML-augmented algorithms utilize predictions to achieve performance beyond\ntheir worst-case bounds. Producing these predictions might be a costly\noperation -- this motivated Im et al. '22 to introduce the study of algorithms\nwhich use predictions parsimoniously. We design parsimonious algorithms for\ncaching and MTS with action predictions, proposed by Antoniadis et al. '20,\nfocusing on the parameters of consistency (performance with perfect\npredictions) and smoothness (dependence of their performance on the prediction\nerror). Our algorithm for caching is 1-consistent, robust, and its smoothness\ndeteriorates with the decreasing number of available predictions. We propose an\nalgorithm for general MTS whose consistency and smoothness both scale linearly\nwith the decreasing number of predictions. Without the restriction on the\nnumber of available predictions, both algorithms match the earlier guarantees\nachieved by Antoniadis et al. '20.\n", "link": "http://arxiv.org/abs/2404.06280v2", "date": "2024-04-10", "relevancy": 1.411, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4761}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4606}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Algorithms%20for%20Caching%20and%20MTS%20with%20reduced%20number%20of%20predictions&body=Title%3A%20Algorithms%20for%20Caching%20and%20MTS%20with%20reduced%20number%20of%20predictions%0AAuthor%3A%20Karim%20Abdel%20Sadek%20and%20Marek%20Elias%0AAbstract%3A%20%20%20ML-augmented%20algorithms%20utilize%20predictions%20to%20achieve%20performance%20beyond%0Atheir%20worst-case%20bounds.%20Producing%20these%20predictions%20might%20be%20a%20costly%0Aoperation%20--%20this%20motivated%20Im%20et%20al.%20%2722%20to%20introduce%20the%20study%20of%20algorithms%0Awhich%20use%20predictions%20parsimoniously.%20We%20design%20parsimonious%20algorithms%20for%0Acaching%20and%20MTS%20with%20action%20predictions%2C%20proposed%20by%20Antoniadis%20et%20al.%20%2720%2C%0Afocusing%20on%20the%20parameters%20of%20consistency%20%28performance%20with%20perfect%0Apredictions%29%20and%20smoothness%20%28dependence%20of%20their%20performance%20on%20the%20prediction%0Aerror%29.%20Our%20algorithm%20for%20caching%20is%201-consistent%2C%20robust%2C%20and%20its%20smoothness%0Adeteriorates%20with%20the%20decreasing%20number%20of%20available%20predictions.%20We%20propose%20an%0Aalgorithm%20for%20general%20MTS%20whose%20consistency%20and%20smoothness%20both%20scale%20linearly%0Awith%20the%20decreasing%20number%20of%20predictions.%20Without%20the%20restriction%20on%20the%0Anumber%20of%20available%20predictions%2C%20both%20algorithms%20match%20the%20earlier%20guarantees%0Aachieved%20by%20Antoniadis%20et%20al.%20%2720.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06280v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithms%20for%20Caching%20and%20MTS%20with%20reduced%20number%20of%20predictions&entry.906535625=Karim%20Abdel%20Sadek%20and%20Marek%20Elias&entry.1292438233=%20%20ML-augmented%20algorithms%20utilize%20predictions%20to%20achieve%20performance%20beyond%0Atheir%20worst-case%20bounds.%20Producing%20these%20predictions%20might%20be%20a%20costly%0Aoperation%20--%20this%20motivated%20Im%20et%20al.%20%2722%20to%20introduce%20the%20study%20of%20algorithms%0Awhich%20use%20predictions%20parsimoniously.%20We%20design%20parsimonious%20algorithms%20for%0Acaching%20and%20MTS%20with%20action%20predictions%2C%20proposed%20by%20Antoniadis%20et%20al.%20%2720%2C%0Afocusing%20on%20the%20parameters%20of%20consistency%20%28performance%20with%20perfect%0Apredictions%29%20and%20smoothness%20%28dependence%20of%20their%20performance%20on%20the%20prediction%0Aerror%29.%20Our%20algorithm%20for%20caching%20is%201-consistent%2C%20robust%2C%20and%20its%20smoothness%0Adeteriorates%20with%20the%20decreasing%20number%20of%20available%20predictions.%20We%20propose%20an%0Aalgorithm%20for%20general%20MTS%20whose%20consistency%20and%20smoothness%20both%20scale%20linearly%0Awith%20the%20decreasing%20number%20of%20predictions.%20Without%20the%20restriction%20on%20the%0Anumber%20of%20available%20predictions%2C%20both%20algorithms%20match%20the%20earlier%20guarantees%0Aachieved%20by%20Antoniadis%20et%20al.%20%2720.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06280v2&entry.124074799=Read"},
{"title": "InstantMesh: Efficient 3D Mesh Generation from a Single Image with\n  Sparse-view Large Reconstruction Models", "author": "Jiale Xu and Weihao Cheng and Yiming Gao and Xintao Wang and Shenghua Gao and Ying Shan", "abstract": "  We present InstantMesh, a feed-forward framework for instant 3D mesh\ngeneration from a single image, featuring state-of-the-art generation quality\nand significant training scalability. By synergizing the strengths of an\noff-the-shelf multiview diffusion model and a sparse-view reconstruction model\nbased on the LRM architecture, InstantMesh is able to create diverse 3D assets\nwithin 10 seconds. To enhance the training efficiency and exploit more\ngeometric supervisions, e.g, depths and normals, we integrate a differentiable\niso-surface extraction module into our framework and directly optimize on the\nmesh representation. Experimental results on public datasets demonstrate that\nInstantMesh significantly outperforms other latest image-to-3D baselines, both\nqualitatively and quantitatively. We release all the code, weights, and demo of\nInstantMesh, with the intention that it can make substantial contributions to\nthe community of 3D generative AI and empower both researchers and content\ncreators.\n", "link": "http://arxiv.org/abs/2404.07191v1", "date": "2024-04-10", "relevancy": 1.6273, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5707}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5416}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5314}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstantMesh%3A%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%20Image%20with%0A%20%20Sparse-view%20Large%20Reconstruction%20Models&body=Title%3A%20InstantMesh%3A%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%20Image%20with%0A%20%20Sparse-view%20Large%20Reconstruction%20Models%0AAuthor%3A%20Jiale%20Xu%20and%20Weihao%20Cheng%20and%20Yiming%20Gao%20and%20Xintao%20Wang%20and%20Shenghua%20Gao%20and%20Ying%20Shan%0AAbstract%3A%20%20%20We%20present%20InstantMesh%2C%20a%20feed-forward%20framework%20for%20instant%203D%20mesh%0Ageneration%20from%20a%20single%20image%2C%20featuring%20state-of-the-art%20generation%20quality%0Aand%20significant%20training%20scalability.%20By%20synergizing%20the%20strengths%20of%20an%0Aoff-the-shelf%20multiview%20diffusion%20model%20and%20a%20sparse-view%20reconstruction%20model%0Abased%20on%20the%20LRM%20architecture%2C%20InstantMesh%20is%20able%20to%20create%20diverse%203D%20assets%0Awithin%2010%20seconds.%20To%20enhance%20the%20training%20efficiency%20and%20exploit%20more%0Ageometric%20supervisions%2C%20e.g%2C%20depths%20and%20normals%2C%20we%20integrate%20a%20differentiable%0Aiso-surface%20extraction%20module%20into%20our%20framework%20and%20directly%20optimize%20on%20the%0Amesh%20representation.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%0AInstantMesh%20significantly%20outperforms%20other%20latest%20image-to-3D%20baselines%2C%20both%0Aqualitatively%20and%20quantitatively.%20We%20release%20all%20the%20code%2C%20weights%2C%20and%20demo%20of%0AInstantMesh%2C%20with%20the%20intention%20that%20it%20can%20make%20substantial%20contributions%20to%0Athe%20community%20of%203D%20generative%20AI%20and%20empower%20both%20researchers%20and%20content%0Acreators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07191v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantMesh%3A%20Efficient%203D%20Mesh%20Generation%20from%20a%20Single%20Image%20with%0A%20%20Sparse-view%20Large%20Reconstruction%20Models&entry.906535625=Jiale%20Xu%20and%20Weihao%20Cheng%20and%20Yiming%20Gao%20and%20Xintao%20Wang%20and%20Shenghua%20Gao%20and%20Ying%20Shan&entry.1292438233=%20%20We%20present%20InstantMesh%2C%20a%20feed-forward%20framework%20for%20instant%203D%20mesh%0Ageneration%20from%20a%20single%20image%2C%20featuring%20state-of-the-art%20generation%20quality%0Aand%20significant%20training%20scalability.%20By%20synergizing%20the%20strengths%20of%20an%0Aoff-the-shelf%20multiview%20diffusion%20model%20and%20a%20sparse-view%20reconstruction%20model%0Abased%20on%20the%20LRM%20architecture%2C%20InstantMesh%20is%20able%20to%20create%20diverse%203D%20assets%0Awithin%2010%20seconds.%20To%20enhance%20the%20training%20efficiency%20and%20exploit%20more%0Ageometric%20supervisions%2C%20e.g%2C%20depths%20and%20normals%2C%20we%20integrate%20a%20differentiable%0Aiso-surface%20extraction%20module%20into%20our%20framework%20and%20directly%20optimize%20on%20the%0Amesh%20representation.%20Experimental%20results%20on%20public%20datasets%20demonstrate%20that%0AInstantMesh%20significantly%20outperforms%20other%20latest%20image-to-3D%20baselines%2C%20both%0Aqualitatively%20and%20quantitatively.%20We%20release%20all%20the%20code%2C%20weights%2C%20and%20demo%20of%0AInstantMesh%2C%20with%20the%20intention%20that%20it%20can%20make%20substantial%20contributions%20to%0Athe%20community%20of%203D%20generative%20AI%20and%20empower%20both%20researchers%20and%20content%0Acreators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07191v1&entry.124074799=Read"},
{"title": "Reward Learning from Suboptimal Demonstrations with Applications in\n  Surgical Electrocautery", "author": "Zohre Karimi and Shing-Hei Ho and Bao Thach and Alan Kuntz and Daniel S. Brown", "abstract": "  Automating robotic surgery via learning from demonstration (LfD) techniques\nis extremely challenging. This is because surgical tasks often involve\nsequential decision-making processes with complex interactions of physical\nobjects and have low tolerance for mistakes. Prior works assume that all\ndemonstrations are fully observable and optimal, which might not be practical\nin the real world. This paper introduces a sample-efficient method that learns\na robust reward function from a limited amount of ranked suboptimal\ndemonstrations consisting of partial-view point cloud observations. The method\nthen learns a policy by optimizing the learned reward function using\nreinforcement learning (RL). We show that using a learned reward function to\nobtain a policy is more robust than pure imitation learning. We apply our\napproach on a physical surgical electrocautery task and demonstrate that our\nmethod can perform well even when the provided demonstrations are suboptimal\nand the observations are high-dimensional point clouds.\n", "link": "http://arxiv.org/abs/2404.07185v1", "date": "2024-04-10", "relevancy": 1.6035, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5573}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5196}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reward%20Learning%20from%20Suboptimal%20Demonstrations%20with%20Applications%20in%0A%20%20Surgical%20Electrocautery&body=Title%3A%20Reward%20Learning%20from%20Suboptimal%20Demonstrations%20with%20Applications%20in%0A%20%20Surgical%20Electrocautery%0AAuthor%3A%20Zohre%20Karimi%20and%20Shing-Hei%20Ho%20and%20Bao%20Thach%20and%20Alan%20Kuntz%20and%20Daniel%20S.%20Brown%0AAbstract%3A%20%20%20Automating%20robotic%20surgery%20via%20learning%20from%20demonstration%20%28LfD%29%20techniques%0Ais%20extremely%20challenging.%20This%20is%20because%20surgical%20tasks%20often%20involve%0Asequential%20decision-making%20processes%20with%20complex%20interactions%20of%20physical%0Aobjects%20and%20have%20low%20tolerance%20for%20mistakes.%20Prior%20works%20assume%20that%20all%0Ademonstrations%20are%20fully%20observable%20and%20optimal%2C%20which%20might%20not%20be%20practical%0Ain%20the%20real%20world.%20This%20paper%20introduces%20a%20sample-efficient%20method%20that%20learns%0Aa%20robust%20reward%20function%20from%20a%20limited%20amount%20of%20ranked%20suboptimal%0Ademonstrations%20consisting%20of%20partial-view%20point%20cloud%20observations.%20The%20method%0Athen%20learns%20a%20policy%20by%20optimizing%20the%20learned%20reward%20function%20using%0Areinforcement%20learning%20%28RL%29.%20We%20show%20that%20using%20a%20learned%20reward%20function%20to%0Aobtain%20a%20policy%20is%20more%20robust%20than%20pure%20imitation%20learning.%20We%20apply%20our%0Aapproach%20on%20a%20physical%20surgical%20electrocautery%20task%20and%20demonstrate%20that%20our%0Amethod%20can%20perform%20well%20even%20when%20the%20provided%20demonstrations%20are%20suboptimal%0Aand%20the%20observations%20are%20high-dimensional%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07185v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Learning%20from%20Suboptimal%20Demonstrations%20with%20Applications%20in%0A%20%20Surgical%20Electrocautery&entry.906535625=Zohre%20Karimi%20and%20Shing-Hei%20Ho%20and%20Bao%20Thach%20and%20Alan%20Kuntz%20and%20Daniel%20S.%20Brown&entry.1292438233=%20%20Automating%20robotic%20surgery%20via%20learning%20from%20demonstration%20%28LfD%29%20techniques%0Ais%20extremely%20challenging.%20This%20is%20because%20surgical%20tasks%20often%20involve%0Asequential%20decision-making%20processes%20with%20complex%20interactions%20of%20physical%0Aobjects%20and%20have%20low%20tolerance%20for%20mistakes.%20Prior%20works%20assume%20that%20all%0Ademonstrations%20are%20fully%20observable%20and%20optimal%2C%20which%20might%20not%20be%20practical%0Ain%20the%20real%20world.%20This%20paper%20introduces%20a%20sample-efficient%20method%20that%20learns%0Aa%20robust%20reward%20function%20from%20a%20limited%20amount%20of%20ranked%20suboptimal%0Ademonstrations%20consisting%20of%20partial-view%20point%20cloud%20observations.%20The%20method%0Athen%20learns%20a%20policy%20by%20optimizing%20the%20learned%20reward%20function%20using%0Areinforcement%20learning%20%28RL%29.%20We%20show%20that%20using%20a%20learned%20reward%20function%20to%0Aobtain%20a%20policy%20is%20more%20robust%20than%20pure%20imitation%20learning.%20We%20apply%20our%0Aapproach%20on%20a%20physical%20surgical%20electrocautery%20task%20and%20demonstrate%20that%20our%0Amethod%20can%20perform%20well%20even%20when%20the%20provided%20demonstrations%20are%20suboptimal%0Aand%20the%20observations%20are%20high-dimensional%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07185v1&entry.124074799=Read"},
{"title": "Model-based deep reinforcement learning for accelerated learning from\n  flow simulations", "author": "Andre Weiner and Janis Geise", "abstract": "  In recent years, deep reinforcement learning has emerged as a technique to\nsolve closed-loop flow control problems. Employing simulation-based\nenvironments in reinforcement learning enables a priori end-to-end optimization\nof the control system, provides a virtual testbed for safety-critical control\napplications, and allows to gain a deep understanding of the control\nmechanisms. While reinforcement learning has been applied successfully in a\nnumber of rather simple flow control benchmarks, a major bottleneck toward\nreal-world applications is the high computational cost and turnaround time of\nflow simulations. In this contribution, we demonstrate the benefits of\nmodel-based reinforcement learning for flow control applications. Specifically,\nwe optimize the policy by alternating between trajectories sampled from flow\nsimulations and trajectories sampled from an ensemble of environment models.\nThe model-based learning reduces the overall training time by up to $85\\%$ for\nthe fluidic pinball test case. Even larger savings are expected for more\ndemanding flow simulations.\n", "link": "http://arxiv.org/abs/2402.16543v2", "date": "2024-04-10", "relevancy": 1.5595, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5086}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Model-based%20deep%20reinforcement%20learning%20for%20accelerated%20learning%20from%0A%20%20flow%20simulations&body=Title%3A%20Model-based%20deep%20reinforcement%20learning%20for%20accelerated%20learning%20from%0A%20%20flow%20simulations%0AAuthor%3A%20Andre%20Weiner%20and%20Janis%20Geise%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20reinforcement%20learning%20has%20emerged%20as%20a%20technique%20to%0Asolve%20closed-loop%20flow%20control%20problems.%20Employing%20simulation-based%0Aenvironments%20in%20reinforcement%20learning%20enables%20a%20priori%20end-to-end%20optimization%0Aof%20the%20control%20system%2C%20provides%20a%20virtual%20testbed%20for%20safety-critical%20control%0Aapplications%2C%20and%20allows%20to%20gain%20a%20deep%20understanding%20of%20the%20control%0Amechanisms.%20While%20reinforcement%20learning%20has%20been%20applied%20successfully%20in%20a%0Anumber%20of%20rather%20simple%20flow%20control%20benchmarks%2C%20a%20major%20bottleneck%20toward%0Areal-world%20applications%20is%20the%20high%20computational%20cost%20and%20turnaround%20time%20of%0Aflow%20simulations.%20In%20this%20contribution%2C%20we%20demonstrate%20the%20benefits%20of%0Amodel-based%20reinforcement%20learning%20for%20flow%20control%20applications.%20Specifically%2C%0Awe%20optimize%20the%20policy%20by%20alternating%20between%20trajectories%20sampled%20from%20flow%0Asimulations%20and%20trajectories%20sampled%20from%20an%20ensemble%20of%20environment%20models.%0AThe%20model-based%20learning%20reduces%20the%20overall%20training%20time%20by%20up%20to%20%2485%5C%25%24%20for%0Athe%20fluidic%20pinball%20test%20case.%20Even%20larger%20savings%20are%20expected%20for%20more%0Ademanding%20flow%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16543v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-based%20deep%20reinforcement%20learning%20for%20accelerated%20learning%20from%0A%20%20flow%20simulations&entry.906535625=Andre%20Weiner%20and%20Janis%20Geise&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20reinforcement%20learning%20has%20emerged%20as%20a%20technique%20to%0Asolve%20closed-loop%20flow%20control%20problems.%20Employing%20simulation-based%0Aenvironments%20in%20reinforcement%20learning%20enables%20a%20priori%20end-to-end%20optimization%0Aof%20the%20control%20system%2C%20provides%20a%20virtual%20testbed%20for%20safety-critical%20control%0Aapplications%2C%20and%20allows%20to%20gain%20a%20deep%20understanding%20of%20the%20control%0Amechanisms.%20While%20reinforcement%20learning%20has%20been%20applied%20successfully%20in%20a%0Anumber%20of%20rather%20simple%20flow%20control%20benchmarks%2C%20a%20major%20bottleneck%20toward%0Areal-world%20applications%20is%20the%20high%20computational%20cost%20and%20turnaround%20time%20of%0Aflow%20simulations.%20In%20this%20contribution%2C%20we%20demonstrate%20the%20benefits%20of%0Amodel-based%20reinforcement%20learning%20for%20flow%20control%20applications.%20Specifically%2C%0Awe%20optimize%20the%20policy%20by%20alternating%20between%20trajectories%20sampled%20from%20flow%0Asimulations%20and%20trajectories%20sampled%20from%20an%20ensemble%20of%20environment%20models.%0AThe%20model-based%20learning%20reduces%20the%20overall%20training%20time%20by%20up%20to%20%2485%5C%25%24%20for%0Athe%20fluidic%20pinball%20test%20case.%20Even%20larger%20savings%20are%20expected%20for%20more%0Ademanding%20flow%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16543v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


