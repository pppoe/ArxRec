<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251109.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Visual Spatial Tuning", "author": "Rui Yang and Ziyu Zhu and Yanwei Li and Jingjia Huang and Shen Yan and Siyuan Zhou and Zhe Liu and Xiangtai Li and Shuangye Li and Wenqian Wang and Yi Lin and Hengshuang Zhao", "abstract": "  Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including $34.8\\%$ on\nMMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.\n", "link": "http://arxiv.org/abs/2511.05491v1", "date": "2025-11-07", "relevancy": 2.8902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5921}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Spatial%20Tuning&body=Title%3A%20Visual%20Spatial%20Tuning%0AAuthor%3A%20Rui%20Yang%20and%20Ziyu%20Zhu%20and%20Yanwei%20Li%20and%20Jingjia%20Huang%20and%20Shen%20Yan%20and%20Siyuan%20Zhou%20and%20Zhe%20Liu%20and%20Xiangtai%20Li%20and%20Shuangye%20Li%20and%20Wenqian%20Wang%20and%20Yi%20Lin%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20Capturing%20spatial%20relationships%20from%20visual%20inputs%20is%20a%20cornerstone%20of%0Ahuman-like%20general%20intelligence.%20Several%20previous%20studies%20have%20tried%20to%20enhance%0Athe%20spatial%20awareness%20of%20Vision-Language%20Models%20%28VLMs%29%20by%20adding%20extra%20expert%0Aencoders%2C%20which%20brings%20extra%20overhead%20and%20usually%20harms%20general%20capabilities.%0ATo%20enhance%20the%20spatial%20ability%20in%20general%20architectures%2C%20we%20introduce%20Visual%0ASpatial%20Tuning%20%28VST%29%2C%20a%20comprehensive%20framework%20to%20cultivate%20VLMs%20with%0Ahuman-like%20visuospatial%20abilities%2C%20from%20spatial%20perception%20to%20reasoning.%20We%0Afirst%20attempt%20to%20enhance%20spatial%20perception%20in%20VLMs%20by%20constructing%20a%0Alarge-scale%20dataset%20termed%20VST-P%2C%20which%20comprises%204.1%20million%20samples%20spanning%0A19%20skills%20across%20single%20views%2C%20multiple%20images%2C%20and%20videos.%20Then%2C%20we%20present%0AVST-R%2C%20a%20curated%20dataset%20with%20135K%20samples%20that%20instruct%20models%20to%20reason%20in%0Aspace.%20In%20particular%2C%20we%20adopt%20a%20progressive%20training%20pipeline%3A%20supervised%0Afine-tuning%20to%20build%20foundational%20spatial%20knowledge%2C%20followed%20by%20reinforcement%0Alearning%20to%20further%20improve%20spatial%20reasoning%20abilities.%20Without%20the%0Aside-effect%20to%20general%20capabilities%2C%20the%20proposed%20VST%20consistently%20achieves%0Astate-of-the-art%20results%20on%20several%20spatial%20benchmarks%2C%20including%20%2434.8%5C%25%24%20on%0AMMSI-Bench%20and%20%2461.2%5C%25%24%20on%20VSIBench.%20It%20turns%20out%20that%20the%0AVision-Language-Action%20models%20can%20be%20significantly%20enhanced%20with%20the%20proposed%0Aspatial%20tuning%20paradigm%2C%20paving%20the%20way%20for%20more%20physically%20grounded%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Spatial%2520Tuning%26entry.906535625%3DRui%2520Yang%2520and%2520Ziyu%2520Zhu%2520and%2520Yanwei%2520Li%2520and%2520Jingjia%2520Huang%2520and%2520Shen%2520Yan%2520and%2520Siyuan%2520Zhou%2520and%2520Zhe%2520Liu%2520and%2520Xiangtai%2520Li%2520and%2520Shuangye%2520Li%2520and%2520Wenqian%2520Wang%2520and%2520Yi%2520Lin%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520Capturing%2520spatial%2520relationships%2520from%2520visual%2520inputs%2520is%2520a%2520cornerstone%2520of%250Ahuman-like%2520general%2520intelligence.%2520Several%2520previous%2520studies%2520have%2520tried%2520to%2520enhance%250Athe%2520spatial%2520awareness%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520by%2520adding%2520extra%2520expert%250Aencoders%252C%2520which%2520brings%2520extra%2520overhead%2520and%2520usually%2520harms%2520general%2520capabilities.%250ATo%2520enhance%2520the%2520spatial%2520ability%2520in%2520general%2520architectures%252C%2520we%2520introduce%2520Visual%250ASpatial%2520Tuning%2520%2528VST%2529%252C%2520a%2520comprehensive%2520framework%2520to%2520cultivate%2520VLMs%2520with%250Ahuman-like%2520visuospatial%2520abilities%252C%2520from%2520spatial%2520perception%2520to%2520reasoning.%2520We%250Afirst%2520attempt%2520to%2520enhance%2520spatial%2520perception%2520in%2520VLMs%2520by%2520constructing%2520a%250Alarge-scale%2520dataset%2520termed%2520VST-P%252C%2520which%2520comprises%25204.1%2520million%2520samples%2520spanning%250A19%2520skills%2520across%2520single%2520views%252C%2520multiple%2520images%252C%2520and%2520videos.%2520Then%252C%2520we%2520present%250AVST-R%252C%2520a%2520curated%2520dataset%2520with%2520135K%2520samples%2520that%2520instruct%2520models%2520to%2520reason%2520in%250Aspace.%2520In%2520particular%252C%2520we%2520adopt%2520a%2520progressive%2520training%2520pipeline%253A%2520supervised%250Afine-tuning%2520to%2520build%2520foundational%2520spatial%2520knowledge%252C%2520followed%2520by%2520reinforcement%250Alearning%2520to%2520further%2520improve%2520spatial%2520reasoning%2520abilities.%2520Without%2520the%250Aside-effect%2520to%2520general%2520capabilities%252C%2520the%2520proposed%2520VST%2520consistently%2520achieves%250Astate-of-the-art%2520results%2520on%2520several%2520spatial%2520benchmarks%252C%2520including%2520%252434.8%255C%2525%2524%2520on%250AMMSI-Bench%2520and%2520%252461.2%255C%2525%2524%2520on%2520VSIBench.%2520It%2520turns%2520out%2520that%2520the%250AVision-Language-Action%2520models%2520can%2520be%2520significantly%2520enhanced%2520with%2520the%2520proposed%250Aspatial%2520tuning%2520paradigm%252C%2520paving%2520the%2520way%2520for%2520more%2520physically%2520grounded%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Spatial%20Tuning&entry.906535625=Rui%20Yang%20and%20Ziyu%20Zhu%20and%20Yanwei%20Li%20and%20Jingjia%20Huang%20and%20Shen%20Yan%20and%20Siyuan%20Zhou%20and%20Zhe%20Liu%20and%20Xiangtai%20Li%20and%20Shuangye%20Li%20and%20Wenqian%20Wang%20and%20Yi%20Lin%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20Capturing%20spatial%20relationships%20from%20visual%20inputs%20is%20a%20cornerstone%20of%0Ahuman-like%20general%20intelligence.%20Several%20previous%20studies%20have%20tried%20to%20enhance%0Athe%20spatial%20awareness%20of%20Vision-Language%20Models%20%28VLMs%29%20by%20adding%20extra%20expert%0Aencoders%2C%20which%20brings%20extra%20overhead%20and%20usually%20harms%20general%20capabilities.%0ATo%20enhance%20the%20spatial%20ability%20in%20general%20architectures%2C%20we%20introduce%20Visual%0ASpatial%20Tuning%20%28VST%29%2C%20a%20comprehensive%20framework%20to%20cultivate%20VLMs%20with%0Ahuman-like%20visuospatial%20abilities%2C%20from%20spatial%20perception%20to%20reasoning.%20We%0Afirst%20attempt%20to%20enhance%20spatial%20perception%20in%20VLMs%20by%20constructing%20a%0Alarge-scale%20dataset%20termed%20VST-P%2C%20which%20comprises%204.1%20million%20samples%20spanning%0A19%20skills%20across%20single%20views%2C%20multiple%20images%2C%20and%20videos.%20Then%2C%20we%20present%0AVST-R%2C%20a%20curated%20dataset%20with%20135K%20samples%20that%20instruct%20models%20to%20reason%20in%0Aspace.%20In%20particular%2C%20we%20adopt%20a%20progressive%20training%20pipeline%3A%20supervised%0Afine-tuning%20to%20build%20foundational%20spatial%20knowledge%2C%20followed%20by%20reinforcement%0Alearning%20to%20further%20improve%20spatial%20reasoning%20abilities.%20Without%20the%0Aside-effect%20to%20general%20capabilities%2C%20the%20proposed%20VST%20consistently%20achieves%0Astate-of-the-art%20results%20on%20several%20spatial%20benchmarks%2C%20including%20%2434.8%5C%25%24%20on%0AMMSI-Bench%20and%20%2461.2%5C%25%24%20on%20VSIBench.%20It%20turns%20out%20that%20the%0AVision-Language-Action%20models%20can%20be%20significantly%20enhanced%20with%20the%20proposed%0Aspatial%20tuning%20paradigm%2C%20paving%20the%20way%20for%20more%20physically%20grounded%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05491v1&entry.124074799=Read"},
{"title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence", "author": "Xingxuan Zhang and Gang Ren and Han Yu and Hao Yuan and Hui Wang and Jiansheng Li and Jiayun Wu and Lang Mo and Li Mao and Mingchao Hao and Ningbo Dai and Renzhe Xu and Shuyang Li and Tianyang Zhang and Yue He and Yuanrui Wang and Yunjia Zhang and Zijing Xu and Dongzhe Li and Fang Gao and Hao Zou and Jiandong Liu and Jiashuo Liu and Jiawei Xu and Kaijie Cheng and Kehan Li and Linjun Zhou and Qing Li and Shaohua Fan and Xiaoyu Lin and Xinyan Han and Xuanyue Li and Yan Lu and Yuan Xue and Yuanyuan Jiang and Zimu Wang and Zhenlei Wang and Peng Cui", "abstract": "  We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX-16M and LimiX-2M, two instantiations of our\nlarge structured-data models (LDMs). Both models treat structured data as a\njoint distribution over variables and missingness, thus capable of addressing a\nwide range of tabular tasks through query-based conditional prediction via a\nsingle model. They are pretrained using masked joint-distribution modeling with\nan episodic, context-conditional objective, supporting rapid, training-free\nadaptation at inference. We evaluate LimiX models across 11 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. LimiX-16M consistently surpasses\nstrong baselines, as shown in Figure 1 and Figure 2. The superiority holds\nacross a wide range of tasks, such as classification, regression, missing value\nimputation, and data generation, often by substantial margins, while avoiding\ntask-specific architectures or bespoke training per task. Notably, LimiX-2M\ndelivers strong results under tight compute and memory budgets. We also present\nthe first scaling law study for LDMs, revealing how data and model scaling\njointly influence downstream performance and offering quantitative guidance for\ntabular foundation modeling. All LimiX models are publicly accessible under\nApache 2.0.\n", "link": "http://arxiv.org/abs/2509.03505v2", "date": "2025-11-07", "relevancy": 2.7786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence&body=Title%3A%20LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence%0AAuthor%3A%20Xingxuan%20Zhang%20and%20Gang%20Ren%20and%20Han%20Yu%20and%20Hao%20Yuan%20and%20Hui%20Wang%20and%20Jiansheng%20Li%20and%20Jiayun%20Wu%20and%20Lang%20Mo%20and%20Li%20Mao%20and%20Mingchao%20Hao%20and%20Ningbo%20Dai%20and%20Renzhe%20Xu%20and%20Shuyang%20Li%20and%20Tianyang%20Zhang%20and%20Yue%20He%20and%20Yuanrui%20Wang%20and%20Yunjia%20Zhang%20and%20Zijing%20Xu%20and%20Dongzhe%20Li%20and%20Fang%20Gao%20and%20Hao%20Zou%20and%20Jiandong%20Liu%20and%20Jiashuo%20Liu%20and%20Jiawei%20Xu%20and%20Kaijie%20Cheng%20and%20Kehan%20Li%20and%20Linjun%20Zhou%20and%20Qing%20Li%20and%20Shaohua%20Fan%20and%20Xiaoyu%20Lin%20and%20Xinyan%20Han%20and%20Xuanyue%20Li%20and%20Yan%20Lu%20and%20Yuan%20Xue%20and%20Yuanyuan%20Jiang%20and%20Zimu%20Wang%20and%20Zhenlei%20Wang%20and%20Peng%20Cui%0AAbstract%3A%20%20%20We%20argue%20that%20progress%20toward%20general%20intelligence%20requires%20complementary%0Afoundation%20models%20grounded%20in%20language%2C%20the%20physical%20world%2C%20and%20structured%0Adata.%20This%20report%20presents%20LimiX-16M%20and%20LimiX-2M%2C%20two%20instantiations%20of%20our%0Alarge%20structured-data%20models%20%28LDMs%29.%20Both%20models%20treat%20structured%20data%20as%20a%0Ajoint%20distribution%20over%20variables%20and%20missingness%2C%20thus%20capable%20of%20addressing%20a%0Awide%20range%20of%20tabular%20tasks%20through%20query-based%20conditional%20prediction%20via%20a%0Asingle%20model.%20They%20are%20pretrained%20using%20masked%20joint-distribution%20modeling%20with%0Aan%20episodic%2C%20context-conditional%20objective%2C%20supporting%20rapid%2C%20training-free%0Aadaptation%20at%20inference.%20We%20evaluate%20LimiX%20models%20across%2011%20large%0Astructured-data%20benchmarks%20with%20broad%20regimes%20of%20sample%20size%2C%20feature%0Adimensionality%2C%20class%20number%2C%20categorical-to-numerical%20feature%20ratio%2C%0Amissingness%2C%20and%20sample-to-feature%20ratios.%20LimiX-16M%20consistently%20surpasses%0Astrong%20baselines%2C%20as%20shown%20in%20Figure%201%20and%20Figure%202.%20The%20superiority%20holds%0Aacross%20a%20wide%20range%20of%20tasks%2C%20such%20as%20classification%2C%20regression%2C%20missing%20value%0Aimputation%2C%20and%20data%20generation%2C%20often%20by%20substantial%20margins%2C%20while%20avoiding%0Atask-specific%20architectures%20or%20bespoke%20training%20per%20task.%20Notably%2C%20LimiX-2M%0Adelivers%20strong%20results%20under%20tight%20compute%20and%20memory%20budgets.%20We%20also%20present%0Athe%20first%20scaling%20law%20study%20for%20LDMs%2C%20revealing%20how%20data%20and%20model%20scaling%0Ajointly%20influence%20downstream%20performance%20and%20offering%20quantitative%20guidance%20for%0Atabular%20foundation%20modeling.%20All%20LimiX%20models%20are%20publicly%20accessible%20under%0AApache%202.0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimiX%253A%2520Unleashing%2520Structured-Data%2520Modeling%2520Capability%2520for%2520Generalist%250A%2520%2520Intelligence%26entry.906535625%3DXingxuan%2520Zhang%2520and%2520Gang%2520Ren%2520and%2520Han%2520Yu%2520and%2520Hao%2520Yuan%2520and%2520Hui%2520Wang%2520and%2520Jiansheng%2520Li%2520and%2520Jiayun%2520Wu%2520and%2520Lang%2520Mo%2520and%2520Li%2520Mao%2520and%2520Mingchao%2520Hao%2520and%2520Ningbo%2520Dai%2520and%2520Renzhe%2520Xu%2520and%2520Shuyang%2520Li%2520and%2520Tianyang%2520Zhang%2520and%2520Yue%2520He%2520and%2520Yuanrui%2520Wang%2520and%2520Yunjia%2520Zhang%2520and%2520Zijing%2520Xu%2520and%2520Dongzhe%2520Li%2520and%2520Fang%2520Gao%2520and%2520Hao%2520Zou%2520and%2520Jiandong%2520Liu%2520and%2520Jiashuo%2520Liu%2520and%2520Jiawei%2520Xu%2520and%2520Kaijie%2520Cheng%2520and%2520Kehan%2520Li%2520and%2520Linjun%2520Zhou%2520and%2520Qing%2520Li%2520and%2520Shaohua%2520Fan%2520and%2520Xiaoyu%2520Lin%2520and%2520Xinyan%2520Han%2520and%2520Xuanyue%2520Li%2520and%2520Yan%2520Lu%2520and%2520Yuan%2520Xue%2520and%2520Yuanyuan%2520Jiang%2520and%2520Zimu%2520Wang%2520and%2520Zhenlei%2520Wang%2520and%2520Peng%2520Cui%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520progress%2520toward%2520general%2520intelligence%2520requires%2520complementary%250Afoundation%2520models%2520grounded%2520in%2520language%252C%2520the%2520physical%2520world%252C%2520and%2520structured%250Adata.%2520This%2520report%2520presents%2520LimiX-16M%2520and%2520LimiX-2M%252C%2520two%2520instantiations%2520of%2520our%250Alarge%2520structured-data%2520models%2520%2528LDMs%2529.%2520Both%2520models%2520treat%2520structured%2520data%2520as%2520a%250Ajoint%2520distribution%2520over%2520variables%2520and%2520missingness%252C%2520thus%2520capable%2520of%2520addressing%2520a%250Awide%2520range%2520of%2520tabular%2520tasks%2520through%2520query-based%2520conditional%2520prediction%2520via%2520a%250Asingle%2520model.%2520They%2520are%2520pretrained%2520using%2520masked%2520joint-distribution%2520modeling%2520with%250Aan%2520episodic%252C%2520context-conditional%2520objective%252C%2520supporting%2520rapid%252C%2520training-free%250Aadaptation%2520at%2520inference.%2520We%2520evaluate%2520LimiX%2520models%2520across%252011%2520large%250Astructured-data%2520benchmarks%2520with%2520broad%2520regimes%2520of%2520sample%2520size%252C%2520feature%250Adimensionality%252C%2520class%2520number%252C%2520categorical-to-numerical%2520feature%2520ratio%252C%250Amissingness%252C%2520and%2520sample-to-feature%2520ratios.%2520LimiX-16M%2520consistently%2520surpasses%250Astrong%2520baselines%252C%2520as%2520shown%2520in%2520Figure%25201%2520and%2520Figure%25202.%2520The%2520superiority%2520holds%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%252C%2520such%2520as%2520classification%252C%2520regression%252C%2520missing%2520value%250Aimputation%252C%2520and%2520data%2520generation%252C%2520often%2520by%2520substantial%2520margins%252C%2520while%2520avoiding%250Atask-specific%2520architectures%2520or%2520bespoke%2520training%2520per%2520task.%2520Notably%252C%2520LimiX-2M%250Adelivers%2520strong%2520results%2520under%2520tight%2520compute%2520and%2520memory%2520budgets.%2520We%2520also%2520present%250Athe%2520first%2520scaling%2520law%2520study%2520for%2520LDMs%252C%2520revealing%2520how%2520data%2520and%2520model%2520scaling%250Ajointly%2520influence%2520downstream%2520performance%2520and%2520offering%2520quantitative%2520guidance%2520for%250Atabular%2520foundation%2520modeling.%2520All%2520LimiX%2520models%2520are%2520publicly%2520accessible%2520under%250AApache%25202.0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LimiX%3A%20Unleashing%20Structured-Data%20Modeling%20Capability%20for%20Generalist%0A%20%20Intelligence&entry.906535625=Xingxuan%20Zhang%20and%20Gang%20Ren%20and%20Han%20Yu%20and%20Hao%20Yuan%20and%20Hui%20Wang%20and%20Jiansheng%20Li%20and%20Jiayun%20Wu%20and%20Lang%20Mo%20and%20Li%20Mao%20and%20Mingchao%20Hao%20and%20Ningbo%20Dai%20and%20Renzhe%20Xu%20and%20Shuyang%20Li%20and%20Tianyang%20Zhang%20and%20Yue%20He%20and%20Yuanrui%20Wang%20and%20Yunjia%20Zhang%20and%20Zijing%20Xu%20and%20Dongzhe%20Li%20and%20Fang%20Gao%20and%20Hao%20Zou%20and%20Jiandong%20Liu%20and%20Jiashuo%20Liu%20and%20Jiawei%20Xu%20and%20Kaijie%20Cheng%20and%20Kehan%20Li%20and%20Linjun%20Zhou%20and%20Qing%20Li%20and%20Shaohua%20Fan%20and%20Xiaoyu%20Lin%20and%20Xinyan%20Han%20and%20Xuanyue%20Li%20and%20Yan%20Lu%20and%20Yuan%20Xue%20and%20Yuanyuan%20Jiang%20and%20Zimu%20Wang%20and%20Zhenlei%20Wang%20and%20Peng%20Cui&entry.1292438233=%20%20We%20argue%20that%20progress%20toward%20general%20intelligence%20requires%20complementary%0Afoundation%20models%20grounded%20in%20language%2C%20the%20physical%20world%2C%20and%20structured%0Adata.%20This%20report%20presents%20LimiX-16M%20and%20LimiX-2M%2C%20two%20instantiations%20of%20our%0Alarge%20structured-data%20models%20%28LDMs%29.%20Both%20models%20treat%20structured%20data%20as%20a%0Ajoint%20distribution%20over%20variables%20and%20missingness%2C%20thus%20capable%20of%20addressing%20a%0Awide%20range%20of%20tabular%20tasks%20through%20query-based%20conditional%20prediction%20via%20a%0Asingle%20model.%20They%20are%20pretrained%20using%20masked%20joint-distribution%20modeling%20with%0Aan%20episodic%2C%20context-conditional%20objective%2C%20supporting%20rapid%2C%20training-free%0Aadaptation%20at%20inference.%20We%20evaluate%20LimiX%20models%20across%2011%20large%0Astructured-data%20benchmarks%20with%20broad%20regimes%20of%20sample%20size%2C%20feature%0Adimensionality%2C%20class%20number%2C%20categorical-to-numerical%20feature%20ratio%2C%0Amissingness%2C%20and%20sample-to-feature%20ratios.%20LimiX-16M%20consistently%20surpasses%0Astrong%20baselines%2C%20as%20shown%20in%20Figure%201%20and%20Figure%202.%20The%20superiority%20holds%0Aacross%20a%20wide%20range%20of%20tasks%2C%20such%20as%20classification%2C%20regression%2C%20missing%20value%0Aimputation%2C%20and%20data%20generation%2C%20often%20by%20substantial%20margins%2C%20while%20avoiding%0Atask-specific%20architectures%20or%20bespoke%20training%20per%20task.%20Notably%2C%20LimiX-2M%0Adelivers%20strong%20results%20under%20tight%20compute%20and%20memory%20budgets.%20We%20also%20present%0Athe%20first%20scaling%20law%20study%20for%20LDMs%2C%20revealing%20how%20data%20and%20model%20scaling%0Ajointly%20influence%20downstream%20performance%20and%20offering%20quantitative%20guidance%20for%0Atabular%20foundation%20modeling.%20All%20LimiX%20models%20are%20publicly%20accessible%20under%0AApache%202.0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03505v2&entry.124074799=Read"},
{"title": "On the Brittleness of CLIP Text Encoders", "author": "Allie Tran and Luca Rossetto", "abstract": "  Multimodal co-embedding models, especially CLIP, have advanced the state of\nthe art in zero-shot classification and multimedia information retrieval in\nrecent years by aligning images and text in a shared representation space.\nHowever, such modals trained on a contrastive alignment can lack stability\ntowards small input perturbations. Especially when dealing with manually\nexpressed queries, minor variations in the query can cause large differences in\nthe ranking of the best-matching results. In this paper, we present a\nsystematic analysis of the effect of multiple classes of non-semantic query\nperturbations in an multimedia information retrieval scenario. We evaluate a\ndiverse set of lexical, syntactic, and semantic perturbations across multiple\nCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video\ncollection. Across models, we find that syntactic and semantic perturbations\ndrive the largest instabilities, while brittleness is concentrated in trivial\nsurface edits such as punctuation and case. Our results highlight robustness as\na critical dimension for evaluating vision-language models beyond benchmark\naccuracy.\n", "link": "http://arxiv.org/abs/2511.04247v2", "date": "2025-11-07", "relevancy": 2.7575, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Brittleness%20of%20CLIP%20Text%20Encoders&body=Title%3A%20On%20the%20Brittleness%20of%20CLIP%20Text%20Encoders%0AAuthor%3A%20Allie%20Tran%20and%20Luca%20Rossetto%0AAbstract%3A%20%20%20Multimodal%20co-embedding%20models%2C%20especially%20CLIP%2C%20have%20advanced%20the%20state%20of%0Athe%20art%20in%20zero-shot%20classification%20and%20multimedia%20information%20retrieval%20in%0Arecent%20years%20by%20aligning%20images%20and%20text%20in%20a%20shared%20representation%20space.%0AHowever%2C%20such%20modals%20trained%20on%20a%20contrastive%20alignment%20can%20lack%20stability%0Atowards%20small%20input%20perturbations.%20Especially%20when%20dealing%20with%20manually%0Aexpressed%20queries%2C%20minor%20variations%20in%20the%20query%20can%20cause%20large%20differences%20in%0Athe%20ranking%20of%20the%20best-matching%20results.%20In%20this%20paper%2C%20we%20present%20a%0Asystematic%20analysis%20of%20the%20effect%20of%20multiple%20classes%20of%20non-semantic%20query%0Aperturbations%20in%20an%20multimedia%20information%20retrieval%20scenario.%20We%20evaluate%20a%0Adiverse%20set%20of%20lexical%2C%20syntactic%2C%20and%20semantic%20perturbations%20across%20multiple%0ACLIP%20variants%20using%20the%20TRECVID%20Ad-Hoc%20Video%20Search%20queries%20and%20the%20V3C1%20video%0Acollection.%20Across%20models%2C%20we%20find%20that%20syntactic%20and%20semantic%20perturbations%0Adrive%20the%20largest%20instabilities%2C%20while%20brittleness%20is%20concentrated%20in%20trivial%0Asurface%20edits%20such%20as%20punctuation%20and%20case.%20Our%20results%20highlight%20robustness%20as%0Aa%20critical%20dimension%20for%20evaluating%20vision-language%20models%20beyond%20benchmark%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.04247v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Brittleness%2520of%2520CLIP%2520Text%2520Encoders%26entry.906535625%3DAllie%2520Tran%2520and%2520Luca%2520Rossetto%26entry.1292438233%3D%2520%2520Multimodal%2520co-embedding%2520models%252C%2520especially%2520CLIP%252C%2520have%2520advanced%2520the%2520state%2520of%250Athe%2520art%2520in%2520zero-shot%2520classification%2520and%2520multimedia%2520information%2520retrieval%2520in%250Arecent%2520years%2520by%2520aligning%2520images%2520and%2520text%2520in%2520a%2520shared%2520representation%2520space.%250AHowever%252C%2520such%2520modals%2520trained%2520on%2520a%2520contrastive%2520alignment%2520can%2520lack%2520stability%250Atowards%2520small%2520input%2520perturbations.%2520Especially%2520when%2520dealing%2520with%2520manually%250Aexpressed%2520queries%252C%2520minor%2520variations%2520in%2520the%2520query%2520can%2520cause%2520large%2520differences%2520in%250Athe%2520ranking%2520of%2520the%2520best-matching%2520results.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Asystematic%2520analysis%2520of%2520the%2520effect%2520of%2520multiple%2520classes%2520of%2520non-semantic%2520query%250Aperturbations%2520in%2520an%2520multimedia%2520information%2520retrieval%2520scenario.%2520We%2520evaluate%2520a%250Adiverse%2520set%2520of%2520lexical%252C%2520syntactic%252C%2520and%2520semantic%2520perturbations%2520across%2520multiple%250ACLIP%2520variants%2520using%2520the%2520TRECVID%2520Ad-Hoc%2520Video%2520Search%2520queries%2520and%2520the%2520V3C1%2520video%250Acollection.%2520Across%2520models%252C%2520we%2520find%2520that%2520syntactic%2520and%2520semantic%2520perturbations%250Adrive%2520the%2520largest%2520instabilities%252C%2520while%2520brittleness%2520is%2520concentrated%2520in%2520trivial%250Asurface%2520edits%2520such%2520as%2520punctuation%2520and%2520case.%2520Our%2520results%2520highlight%2520robustness%2520as%250Aa%2520critical%2520dimension%2520for%2520evaluating%2520vision-language%2520models%2520beyond%2520benchmark%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04247v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Brittleness%20of%20CLIP%20Text%20Encoders&entry.906535625=Allie%20Tran%20and%20Luca%20Rossetto&entry.1292438233=%20%20Multimodal%20co-embedding%20models%2C%20especially%20CLIP%2C%20have%20advanced%20the%20state%20of%0Athe%20art%20in%20zero-shot%20classification%20and%20multimedia%20information%20retrieval%20in%0Arecent%20years%20by%20aligning%20images%20and%20text%20in%20a%20shared%20representation%20space.%0AHowever%2C%20such%20modals%20trained%20on%20a%20contrastive%20alignment%20can%20lack%20stability%0Atowards%20small%20input%20perturbations.%20Especially%20when%20dealing%20with%20manually%0Aexpressed%20queries%2C%20minor%20variations%20in%20the%20query%20can%20cause%20large%20differences%20in%0Athe%20ranking%20of%20the%20best-matching%20results.%20In%20this%20paper%2C%20we%20present%20a%0Asystematic%20analysis%20of%20the%20effect%20of%20multiple%20classes%20of%20non-semantic%20query%0Aperturbations%20in%20an%20multimedia%20information%20retrieval%20scenario.%20We%20evaluate%20a%0Adiverse%20set%20of%20lexical%2C%20syntactic%2C%20and%20semantic%20perturbations%20across%20multiple%0ACLIP%20variants%20using%20the%20TRECVID%20Ad-Hoc%20Video%20Search%20queries%20and%20the%20V3C1%20video%0Acollection.%20Across%20models%2C%20we%20find%20that%20syntactic%20and%20semantic%20perturbations%0Adrive%20the%20largest%20instabilities%2C%20while%20brittleness%20is%20concentrated%20in%20trivial%0Asurface%20edits%20such%20as%20punctuation%20and%20case.%20Our%20results%20highlight%20robustness%20as%0Aa%20critical%20dimension%20for%20evaluating%20vision-language%20models%20beyond%20benchmark%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.04247v2&entry.124074799=Read"},
{"title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding\n  via Self-Verification Reinforcement Learning", "author": "Junwen Pan and Qizhe Zhang and Rui Zhang and Ming Lu and Xin Wan and Yuan Zhang and Chang Liu and Qi She", "abstract": "  Temporal search aims to identify a minimal set of relevant frames from tens\nof thousands based on a given query, serving as a foundation for accurate\nlong-form video understanding. Existing works attempt to progressively narrow\nthe search space. However, these approaches typically rely on a hand-crafted\nsearch process, lacking end-to-end optimization for learning optimal search\nstrategies. In this paper, we propose TimeSearch-R, which reformulates temporal\nsearch as interleaved text-video thinking, seamlessly integrating searching\nvideo clips into the reasoning process through reinforcement learning (RL).\nHowever, applying RL training methods, such as Group Relative Policy\nOptimization (GRPO), to video reasoning can result in unsupervised intermediate\nsearch decisions. This leads to insufficient exploration of the video content\nand inconsistent logical reasoning. To address these issues, we introduce GRPO\nwith Completeness Self-Verification (GRPO-CSV), which gathers searched video\nframes from the interleaved reasoning process and utilizes the same policy\nmodel to verify the adequacy of searched frames, thereby improving the\ncompleteness of video reasoning. Additionally, we construct datasets\nspecifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task\ndifficulty and improve temporal search capabilities. Extensive experiments\ndemonstrate that TimeSearch-R achieves significant improvements on temporal\nsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as\nlong-form video understanding benchmarks like VideoMME and MLVU. Notably,\nTimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%\nimprovement over the base model Qwen2.5-VL and 2.0% over the advanced video\nreasoning model Video-R1. Our code is available at\nhttps://github.com/Time-Search/TimeSearch-R.\n", "link": "http://arxiv.org/abs/2511.05489v1", "date": "2025-11-07", "relevancy": 2.6916, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeSearch-R%3A%20Adaptive%20Temporal%20Search%20for%20Long-Form%20Video%20Understanding%0A%20%20via%20Self-Verification%20Reinforcement%20Learning&body=Title%3A%20TimeSearch-R%3A%20Adaptive%20Temporal%20Search%20for%20Long-Form%20Video%20Understanding%0A%20%20via%20Self-Verification%20Reinforcement%20Learning%0AAuthor%3A%20Junwen%20Pan%20and%20Qizhe%20Zhang%20and%20Rui%20Zhang%20and%20Ming%20Lu%20and%20Xin%20Wan%20and%20Yuan%20Zhang%20and%20Chang%20Liu%20and%20Qi%20She%0AAbstract%3A%20%20%20Temporal%20search%20aims%20to%20identify%20a%20minimal%20set%20of%20relevant%20frames%20from%20tens%0Aof%20thousands%20based%20on%20a%20given%20query%2C%20serving%20as%20a%20foundation%20for%20accurate%0Along-form%20video%20understanding.%20Existing%20works%20attempt%20to%20progressively%20narrow%0Athe%20search%20space.%20However%2C%20these%20approaches%20typically%20rely%20on%20a%20hand-crafted%0Asearch%20process%2C%20lacking%20end-to-end%20optimization%20for%20learning%20optimal%20search%0Astrategies.%20In%20this%20paper%2C%20we%20propose%20TimeSearch-R%2C%20which%20reformulates%20temporal%0Asearch%20as%20interleaved%20text-video%20thinking%2C%20seamlessly%20integrating%20searching%0Avideo%20clips%20into%20the%20reasoning%20process%20through%20reinforcement%20learning%20%28RL%29.%0AHowever%2C%20applying%20RL%20training%20methods%2C%20such%20as%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%2C%20to%20video%20reasoning%20can%20result%20in%20unsupervised%20intermediate%0Asearch%20decisions.%20This%20leads%20to%20insufficient%20exploration%20of%20the%20video%20content%0Aand%20inconsistent%20logical%20reasoning.%20To%20address%20these%20issues%2C%20we%20introduce%20GRPO%0Awith%20Completeness%20Self-Verification%20%28GRPO-CSV%29%2C%20which%20gathers%20searched%20video%0Aframes%20from%20the%20interleaved%20reasoning%20process%20and%20utilizes%20the%20same%20policy%0Amodel%20to%20verify%20the%20adequacy%20of%20searched%20frames%2C%20thereby%20improving%20the%0Acompleteness%20of%20video%20reasoning.%20Additionally%2C%20we%20construct%20datasets%0Aspecifically%20designed%20for%20the%20SFT%20cold-start%20and%20RL%20training%20of%20GRPO-CSV%2C%0Afiltering%20out%20samples%20with%20weak%20temporal%20dependencies%20to%20enhance%20task%0Adifficulty%20and%20improve%20temporal%20search%20capabilities.%20Extensive%20experiments%0Ademonstrate%20that%20TimeSearch-R%20achieves%20significant%20improvements%20on%20temporal%0Asearch%20benchmarks%20such%20as%20Haystack-LVBench%20and%20Haystack-Ego4D%2C%20as%20well%20as%0Along-form%20video%20understanding%20benchmarks%20like%20VideoMME%20and%20MLVU.%20Notably%2C%0ATimeSearch-R%20establishes%20a%20new%20state-of-the-art%20on%20LongVideoBench%20with%204.1%25%0Aimprovement%20over%20the%20base%20model%20Qwen2.5-VL%20and%202.0%25%20over%20the%20advanced%20video%0Areasoning%20model%20Video-R1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Time-Search/TimeSearch-R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeSearch-R%253A%2520Adaptive%2520Temporal%2520Search%2520for%2520Long-Form%2520Video%2520Understanding%250A%2520%2520via%2520Self-Verification%2520Reinforcement%2520Learning%26entry.906535625%3DJunwen%2520Pan%2520and%2520Qizhe%2520Zhang%2520and%2520Rui%2520Zhang%2520and%2520Ming%2520Lu%2520and%2520Xin%2520Wan%2520and%2520Yuan%2520Zhang%2520and%2520Chang%2520Liu%2520and%2520Qi%2520She%26entry.1292438233%3D%2520%2520Temporal%2520search%2520aims%2520to%2520identify%2520a%2520minimal%2520set%2520of%2520relevant%2520frames%2520from%2520tens%250Aof%2520thousands%2520based%2520on%2520a%2520given%2520query%252C%2520serving%2520as%2520a%2520foundation%2520for%2520accurate%250Along-form%2520video%2520understanding.%2520Existing%2520works%2520attempt%2520to%2520progressively%2520narrow%250Athe%2520search%2520space.%2520However%252C%2520these%2520approaches%2520typically%2520rely%2520on%2520a%2520hand-crafted%250Asearch%2520process%252C%2520lacking%2520end-to-end%2520optimization%2520for%2520learning%2520optimal%2520search%250Astrategies.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TimeSearch-R%252C%2520which%2520reformulates%2520temporal%250Asearch%2520as%2520interleaved%2520text-video%2520thinking%252C%2520seamlessly%2520integrating%2520searching%250Avideo%2520clips%2520into%2520the%2520reasoning%2520process%2520through%2520reinforcement%2520learning%2520%2528RL%2529.%250AHowever%252C%2520applying%2520RL%2520training%2520methods%252C%2520such%2520as%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528GRPO%2529%252C%2520to%2520video%2520reasoning%2520can%2520result%2520in%2520unsupervised%2520intermediate%250Asearch%2520decisions.%2520This%2520leads%2520to%2520insufficient%2520exploration%2520of%2520the%2520video%2520content%250Aand%2520inconsistent%2520logical%2520reasoning.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520GRPO%250Awith%2520Completeness%2520Self-Verification%2520%2528GRPO-CSV%2529%252C%2520which%2520gathers%2520searched%2520video%250Aframes%2520from%2520the%2520interleaved%2520reasoning%2520process%2520and%2520utilizes%2520the%2520same%2520policy%250Amodel%2520to%2520verify%2520the%2520adequacy%2520of%2520searched%2520frames%252C%2520thereby%2520improving%2520the%250Acompleteness%2520of%2520video%2520reasoning.%2520Additionally%252C%2520we%2520construct%2520datasets%250Aspecifically%2520designed%2520for%2520the%2520SFT%2520cold-start%2520and%2520RL%2520training%2520of%2520GRPO-CSV%252C%250Afiltering%2520out%2520samples%2520with%2520weak%2520temporal%2520dependencies%2520to%2520enhance%2520task%250Adifficulty%2520and%2520improve%2520temporal%2520search%2520capabilities.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520TimeSearch-R%2520achieves%2520significant%2520improvements%2520on%2520temporal%250Asearch%2520benchmarks%2520such%2520as%2520Haystack-LVBench%2520and%2520Haystack-Ego4D%252C%2520as%2520well%2520as%250Along-form%2520video%2520understanding%2520benchmarks%2520like%2520VideoMME%2520and%2520MLVU.%2520Notably%252C%250ATimeSearch-R%2520establishes%2520a%2520new%2520state-of-the-art%2520on%2520LongVideoBench%2520with%25204.1%2525%250Aimprovement%2520over%2520the%2520base%2520model%2520Qwen2.5-VL%2520and%25202.0%2525%2520over%2520the%2520advanced%2520video%250Areasoning%2520model%2520Video-R1.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Time-Search/TimeSearch-R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeSearch-R%3A%20Adaptive%20Temporal%20Search%20for%20Long-Form%20Video%20Understanding%0A%20%20via%20Self-Verification%20Reinforcement%20Learning&entry.906535625=Junwen%20Pan%20and%20Qizhe%20Zhang%20and%20Rui%20Zhang%20and%20Ming%20Lu%20and%20Xin%20Wan%20and%20Yuan%20Zhang%20and%20Chang%20Liu%20and%20Qi%20She&entry.1292438233=%20%20Temporal%20search%20aims%20to%20identify%20a%20minimal%20set%20of%20relevant%20frames%20from%20tens%0Aof%20thousands%20based%20on%20a%20given%20query%2C%20serving%20as%20a%20foundation%20for%20accurate%0Along-form%20video%20understanding.%20Existing%20works%20attempt%20to%20progressively%20narrow%0Athe%20search%20space.%20However%2C%20these%20approaches%20typically%20rely%20on%20a%20hand-crafted%0Asearch%20process%2C%20lacking%20end-to-end%20optimization%20for%20learning%20optimal%20search%0Astrategies.%20In%20this%20paper%2C%20we%20propose%20TimeSearch-R%2C%20which%20reformulates%20temporal%0Asearch%20as%20interleaved%20text-video%20thinking%2C%20seamlessly%20integrating%20searching%0Avideo%20clips%20into%20the%20reasoning%20process%20through%20reinforcement%20learning%20%28RL%29.%0AHowever%2C%20applying%20RL%20training%20methods%2C%20such%20as%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%2C%20to%20video%20reasoning%20can%20result%20in%20unsupervised%20intermediate%0Asearch%20decisions.%20This%20leads%20to%20insufficient%20exploration%20of%20the%20video%20content%0Aand%20inconsistent%20logical%20reasoning.%20To%20address%20these%20issues%2C%20we%20introduce%20GRPO%0Awith%20Completeness%20Self-Verification%20%28GRPO-CSV%29%2C%20which%20gathers%20searched%20video%0Aframes%20from%20the%20interleaved%20reasoning%20process%20and%20utilizes%20the%20same%20policy%0Amodel%20to%20verify%20the%20adequacy%20of%20searched%20frames%2C%20thereby%20improving%20the%0Acompleteness%20of%20video%20reasoning.%20Additionally%2C%20we%20construct%20datasets%0Aspecifically%20designed%20for%20the%20SFT%20cold-start%20and%20RL%20training%20of%20GRPO-CSV%2C%0Afiltering%20out%20samples%20with%20weak%20temporal%20dependencies%20to%20enhance%20task%0Adifficulty%20and%20improve%20temporal%20search%20capabilities.%20Extensive%20experiments%0Ademonstrate%20that%20TimeSearch-R%20achieves%20significant%20improvements%20on%20temporal%0Asearch%20benchmarks%20such%20as%20Haystack-LVBench%20and%20Haystack-Ego4D%2C%20as%20well%20as%0Along-form%20video%20understanding%20benchmarks%20like%20VideoMME%20and%20MLVU.%20Notably%2C%0ATimeSearch-R%20establishes%20a%20new%20state-of-the-art%20on%20LongVideoBench%20with%204.1%25%0Aimprovement%20over%20the%20base%20model%20Qwen2.5-VL%20and%202.0%25%20over%20the%20advanced%20video%0Areasoning%20model%20Video-R1.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Time-Search/TimeSearch-R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05489v1&entry.124074799=Read"},
{"title": "SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive\n  Cross-Component Learning", "author": "Kang Yang and Yuanlin Yang and Yuning Chen and Sikai Yang and Xinyu Zhang and Wan Du", "abstract": "  Precision agriculture demands continuous and accurate monitoring of soil\nmoisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),\nand potassium (K), to optimize yields and conserve resources. Wireless soil\nsensing has been explored to measure these four components; however, current\nsolutions require recalibration (i.e., retraining the data processing model) to\nhandle variations in soil texture, characterized by aluminosilicates (Al) and\norganic carbon (C), limiting their practicality. To address this, we introduce\nSoilX, a calibration-free soil sensing system that jointly measures six key\ncomponents: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX\neliminates texture- and carbon-dependent recalibration. SoilX incorporates\nContrastive Cross-Component Learning (3CL), with two customized terms: the\nOrthogonality Regularizer and the Separation Loss, to effectively disentangle\ncross-component interference. Additionally, we design a novel tetrahedral\nantenna array with an antenna-switching mechanism, which can robustly measure\nsoil dielectric permittivity independent of device placement. Extensive\nexperiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%\nover baselines and generalizes well to unseen fields.\n", "link": "http://arxiv.org/abs/2511.05482v1", "date": "2025-11-07", "relevancy": 2.4942, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoilX%3A%20Calibration-Free%20Comprehensive%20Soil%20Sensing%20Through%20Contrastive%0A%20%20Cross-Component%20Learning&body=Title%3A%20SoilX%3A%20Calibration-Free%20Comprehensive%20Soil%20Sensing%20Through%20Contrastive%0A%20%20Cross-Component%20Learning%0AAuthor%3A%20Kang%20Yang%20and%20Yuanlin%20Yang%20and%20Yuning%20Chen%20and%20Sikai%20Yang%20and%20Xinyu%20Zhang%20and%20Wan%20Du%0AAbstract%3A%20%20%20Precision%20agriculture%20demands%20continuous%20and%20accurate%20monitoring%20of%20soil%0Amoisture%20%28M%29%20and%20key%20macronutrients%2C%20including%20nitrogen%20%28N%29%2C%20phosphorus%20%28P%29%2C%0Aand%20potassium%20%28K%29%2C%20to%20optimize%20yields%20and%20conserve%20resources.%20Wireless%20soil%0Asensing%20has%20been%20explored%20to%20measure%20these%20four%20components%3B%20however%2C%20current%0Asolutions%20require%20recalibration%20%28i.e.%2C%20retraining%20the%20data%20processing%20model%29%20to%0Ahandle%20variations%20in%20soil%20texture%2C%20characterized%20by%20aluminosilicates%20%28Al%29%20and%0Aorganic%20carbon%20%28C%29%2C%20limiting%20their%20practicality.%20To%20address%20this%2C%20we%20introduce%0ASoilX%2C%20a%20calibration-free%20soil%20sensing%20system%20that%20jointly%20measures%20six%20key%0Acomponents%3A%20%7BM%2C%20N%2C%20P%2C%20K%2C%20C%2C%20Al%7D.%20By%20explicitly%20modeling%20C%20and%20Al%2C%20SoilX%0Aeliminates%20texture-%20and%20carbon-dependent%20recalibration.%20SoilX%20incorporates%0AContrastive%20Cross-Component%20Learning%20%283CL%29%2C%20with%20two%20customized%20terms%3A%20the%0AOrthogonality%20Regularizer%20and%20the%20Separation%20Loss%2C%20to%20effectively%20disentangle%0Across-component%20interference.%20Additionally%2C%20we%20design%20a%20novel%20tetrahedral%0Aantenna%20array%20with%20an%20antenna-switching%20mechanism%2C%20which%20can%20robustly%20measure%0Asoil%20dielectric%20permittivity%20independent%20of%20device%20placement.%20Extensive%0Aexperiments%20demonstrate%20that%20SoilX%20reduces%20estimation%20errors%20by%2023.8%25%20to%2031.5%25%0Aover%20baselines%20and%20generalizes%20well%20to%20unseen%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoilX%253A%2520Calibration-Free%2520Comprehensive%2520Soil%2520Sensing%2520Through%2520Contrastive%250A%2520%2520Cross-Component%2520Learning%26entry.906535625%3DKang%2520Yang%2520and%2520Yuanlin%2520Yang%2520and%2520Yuning%2520Chen%2520and%2520Sikai%2520Yang%2520and%2520Xinyu%2520Zhang%2520and%2520Wan%2520Du%26entry.1292438233%3D%2520%2520Precision%2520agriculture%2520demands%2520continuous%2520and%2520accurate%2520monitoring%2520of%2520soil%250Amoisture%2520%2528M%2529%2520and%2520key%2520macronutrients%252C%2520including%2520nitrogen%2520%2528N%2529%252C%2520phosphorus%2520%2528P%2529%252C%250Aand%2520potassium%2520%2528K%2529%252C%2520to%2520optimize%2520yields%2520and%2520conserve%2520resources.%2520Wireless%2520soil%250Asensing%2520has%2520been%2520explored%2520to%2520measure%2520these%2520four%2520components%253B%2520however%252C%2520current%250Asolutions%2520require%2520recalibration%2520%2528i.e.%252C%2520retraining%2520the%2520data%2520processing%2520model%2529%2520to%250Ahandle%2520variations%2520in%2520soil%2520texture%252C%2520characterized%2520by%2520aluminosilicates%2520%2528Al%2529%2520and%250Aorganic%2520carbon%2520%2528C%2529%252C%2520limiting%2520their%2520practicality.%2520To%2520address%2520this%252C%2520we%2520introduce%250ASoilX%252C%2520a%2520calibration-free%2520soil%2520sensing%2520system%2520that%2520jointly%2520measures%2520six%2520key%250Acomponents%253A%2520%257BM%252C%2520N%252C%2520P%252C%2520K%252C%2520C%252C%2520Al%257D.%2520By%2520explicitly%2520modeling%2520C%2520and%2520Al%252C%2520SoilX%250Aeliminates%2520texture-%2520and%2520carbon-dependent%2520recalibration.%2520SoilX%2520incorporates%250AContrastive%2520Cross-Component%2520Learning%2520%25283CL%2529%252C%2520with%2520two%2520customized%2520terms%253A%2520the%250AOrthogonality%2520Regularizer%2520and%2520the%2520Separation%2520Loss%252C%2520to%2520effectively%2520disentangle%250Across-component%2520interference.%2520Additionally%252C%2520we%2520design%2520a%2520novel%2520tetrahedral%250Aantenna%2520array%2520with%2520an%2520antenna-switching%2520mechanism%252C%2520which%2520can%2520robustly%2520measure%250Asoil%2520dielectric%2520permittivity%2520independent%2520of%2520device%2520placement.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520SoilX%2520reduces%2520estimation%2520errors%2520by%252023.8%2525%2520to%252031.5%2525%250Aover%2520baselines%2520and%2520generalizes%2520well%2520to%2520unseen%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoilX%3A%20Calibration-Free%20Comprehensive%20Soil%20Sensing%20Through%20Contrastive%0A%20%20Cross-Component%20Learning&entry.906535625=Kang%20Yang%20and%20Yuanlin%20Yang%20and%20Yuning%20Chen%20and%20Sikai%20Yang%20and%20Xinyu%20Zhang%20and%20Wan%20Du&entry.1292438233=%20%20Precision%20agriculture%20demands%20continuous%20and%20accurate%20monitoring%20of%20soil%0Amoisture%20%28M%29%20and%20key%20macronutrients%2C%20including%20nitrogen%20%28N%29%2C%20phosphorus%20%28P%29%2C%0Aand%20potassium%20%28K%29%2C%20to%20optimize%20yields%20and%20conserve%20resources.%20Wireless%20soil%0Asensing%20has%20been%20explored%20to%20measure%20these%20four%20components%3B%20however%2C%20current%0Asolutions%20require%20recalibration%20%28i.e.%2C%20retraining%20the%20data%20processing%20model%29%20to%0Ahandle%20variations%20in%20soil%20texture%2C%20characterized%20by%20aluminosilicates%20%28Al%29%20and%0Aorganic%20carbon%20%28C%29%2C%20limiting%20their%20practicality.%20To%20address%20this%2C%20we%20introduce%0ASoilX%2C%20a%20calibration-free%20soil%20sensing%20system%20that%20jointly%20measures%20six%20key%0Acomponents%3A%20%7BM%2C%20N%2C%20P%2C%20K%2C%20C%2C%20Al%7D.%20By%20explicitly%20modeling%20C%20and%20Al%2C%20SoilX%0Aeliminates%20texture-%20and%20carbon-dependent%20recalibration.%20SoilX%20incorporates%0AContrastive%20Cross-Component%20Learning%20%283CL%29%2C%20with%20two%20customized%20terms%3A%20the%0AOrthogonality%20Regularizer%20and%20the%20Separation%20Loss%2C%20to%20effectively%20disentangle%0Across-component%20interference.%20Additionally%2C%20we%20design%20a%20novel%20tetrahedral%0Aantenna%20array%20with%20an%20antenna-switching%20mechanism%2C%20which%20can%20robustly%20measure%0Asoil%20dielectric%20permittivity%20independent%20of%20device%20placement.%20Extensive%0Aexperiments%20demonstrate%20that%20SoilX%20reduces%20estimation%20errors%20by%2023.8%25%20to%2031.5%25%0Aover%20baselines%20and%20generalizes%20well%20to%20unseen%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05482v1&entry.124074799=Read"},
{"title": "SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning", "author": "Xiaodong Wang and Jing Huang and Kevin J Liang", "abstract": "  Recent studies have demonstrated the effectiveness of clustering-based\napproaches for self-supervised and unsupervised learning. However, the\napplication of clustering is often heuristic, and the optimal methodology\nremains unclear. In this work, we establish connections between these\nunsupervised clustering methods and classical mixture models from statistics.\nThrough this framework, we demonstrate significant enhancements to these\nclustering methods, leading to the development of a novel model named SiamMM.\nOur method attains state-of-the-art performance across various self-supervised\nlearning benchmarks. Inspection of the learned clusters reveals a strong\nresemblance to unseen ground truth labels, uncovering potential instances of\nmislabeling.\n", "link": "http://arxiv.org/abs/2511.05462v1", "date": "2025-11-07", "relevancy": 2.4361, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiamMM%3A%20A%20Mixture%20Model%20Perspective%20on%20Deep%20Unsupervised%20Learning&body=Title%3A%20SiamMM%3A%20A%20Mixture%20Model%20Perspective%20on%20Deep%20Unsupervised%20Learning%0AAuthor%3A%20Xiaodong%20Wang%20and%20Jing%20Huang%20and%20Kevin%20J%20Liang%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20clustering-based%0Aapproaches%20for%20self-supervised%20and%20unsupervised%20learning.%20However%2C%20the%0Aapplication%20of%20clustering%20is%20often%20heuristic%2C%20and%20the%20optimal%20methodology%0Aremains%20unclear.%20In%20this%20work%2C%20we%20establish%20connections%20between%20these%0Aunsupervised%20clustering%20methods%20and%20classical%20mixture%20models%20from%20statistics.%0AThrough%20this%20framework%2C%20we%20demonstrate%20significant%20enhancements%20to%20these%0Aclustering%20methods%2C%20leading%20to%20the%20development%20of%20a%20novel%20model%20named%20SiamMM.%0AOur%20method%20attains%20state-of-the-art%20performance%20across%20various%20self-supervised%0Alearning%20benchmarks.%20Inspection%20of%20the%20learned%20clusters%20reveals%20a%20strong%0Aresemblance%20to%20unseen%20ground%20truth%20labels%2C%20uncovering%20potential%20instances%20of%0Amislabeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiamMM%253A%2520A%2520Mixture%2520Model%2520Perspective%2520on%2520Deep%2520Unsupervised%2520Learning%26entry.906535625%3DXiaodong%2520Wang%2520and%2520Jing%2520Huang%2520and%2520Kevin%2520J%2520Liang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520clustering-based%250Aapproaches%2520for%2520self-supervised%2520and%2520unsupervised%2520learning.%2520However%252C%2520the%250Aapplication%2520of%2520clustering%2520is%2520often%2520heuristic%252C%2520and%2520the%2520optimal%2520methodology%250Aremains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520establish%2520connections%2520between%2520these%250Aunsupervised%2520clustering%2520methods%2520and%2520classical%2520mixture%2520models%2520from%2520statistics.%250AThrough%2520this%2520framework%252C%2520we%2520demonstrate%2520significant%2520enhancements%2520to%2520these%250Aclustering%2520methods%252C%2520leading%2520to%2520the%2520development%2520of%2520a%2520novel%2520model%2520named%2520SiamMM.%250AOur%2520method%2520attains%2520state-of-the-art%2520performance%2520across%2520various%2520self-supervised%250Alearning%2520benchmarks.%2520Inspection%2520of%2520the%2520learned%2520clusters%2520reveals%2520a%2520strong%250Aresemblance%2520to%2520unseen%2520ground%2520truth%2520labels%252C%2520uncovering%2520potential%2520instances%2520of%250Amislabeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiamMM%3A%20A%20Mixture%20Model%20Perspective%20on%20Deep%20Unsupervised%20Learning&entry.906535625=Xiaodong%20Wang%20and%20Jing%20Huang%20and%20Kevin%20J%20Liang&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20effectiveness%20of%20clustering-based%0Aapproaches%20for%20self-supervised%20and%20unsupervised%20learning.%20However%2C%20the%0Aapplication%20of%20clustering%20is%20often%20heuristic%2C%20and%20the%20optimal%20methodology%0Aremains%20unclear.%20In%20this%20work%2C%20we%20establish%20connections%20between%20these%0Aunsupervised%20clustering%20methods%20and%20classical%20mixture%20models%20from%20statistics.%0AThrough%20this%20framework%2C%20we%20demonstrate%20significant%20enhancements%20to%20these%0Aclustering%20methods%2C%20leading%20to%20the%20development%20of%20a%20novel%20model%20named%20SiamMM.%0AOur%20method%20attains%20state-of-the-art%20performance%20across%20various%20self-supervised%0Alearning%20benchmarks.%20Inspection%20of%20the%20learned%20clusters%20reveals%20a%20strong%0Aresemblance%20to%20unseen%20ground%20truth%20labels%2C%20uncovering%20potential%20instances%20of%0Amislabeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05462v1&entry.124074799=Read"},
{"title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?", "author": "Tuan Anh Tran and Duy M. H. Nguyen and Hoai-Chau Tran and Michael Barz and Khoa D. Doan and Roger Wattenhofer and Ngo Anh Vien and Mathias Niepert and Daniel Sonntag and Paul Swoboda", "abstract": "  Recent advances in 3D point cloud transformers have led to state-of-the-art\nresults in tasks such as semantic segmentation and reconstruction. However,\nthese models typically rely on dense token representations, incurring high\ncomputational and memory costs during training and inference. In this work, we\npresent the finding that tokens are remarkably redundant, leading to\nsubstantial inefficiency. We introduce gitmerge3D, a globally informed graph\ntoken merging method that can reduce the token count by up to 90-95% while\nmaintaining competitive performance. This finding challenges the prevailing\nassumption that more tokens inherently yield better performance and highlights\nthat many current models are over-tokenized and under-optimized for\nscalability. We validate our method across multiple 3D vision tasks and show\nconsistent improvements in computational efficiency. This work is the first to\nassess redundancy in large-scale 3D transformer models, providing insights into\nthe development of more efficient 3D foundation architectures. Our code and\ncheckpoints are publicly available at https://gitmerge3d.github.io\n", "link": "http://arxiv.org/abs/2511.05449v1", "date": "2025-11-07", "relevancy": 2.2924, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6162}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Many%20Tokens%20Do%203D%20Point%20Cloud%20Transformer%20Architectures%20Really%20Need%3F&body=Title%3A%20How%20Many%20Tokens%20Do%203D%20Point%20Cloud%20Transformer%20Architectures%20Really%20Need%3F%0AAuthor%3A%20Tuan%20Anh%20Tran%20and%20Duy%20M.%20H.%20Nguyen%20and%20Hoai-Chau%20Tran%20and%20Michael%20Barz%20and%20Khoa%20D.%20Doan%20and%20Roger%20Wattenhofer%20and%20Ngo%20Anh%20Vien%20and%20Mathias%20Niepert%20and%20Daniel%20Sonntag%20and%20Paul%20Swoboda%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20point%20cloud%20transformers%20have%20led%20to%20state-of-the-art%0Aresults%20in%20tasks%20such%20as%20semantic%20segmentation%20and%20reconstruction.%20However%2C%0Athese%20models%20typically%20rely%20on%20dense%20token%20representations%2C%20incurring%20high%0Acomputational%20and%20memory%20costs%20during%20training%20and%20inference.%20In%20this%20work%2C%20we%0Apresent%20the%20finding%20that%20tokens%20are%20remarkably%20redundant%2C%20leading%20to%0Asubstantial%20inefficiency.%20We%20introduce%20gitmerge3D%2C%20a%20globally%20informed%20graph%0Atoken%20merging%20method%20that%20can%20reduce%20the%20token%20count%20by%20up%20to%2090-95%25%20while%0Amaintaining%20competitive%20performance.%20This%20finding%20challenges%20the%20prevailing%0Aassumption%20that%20more%20tokens%20inherently%20yield%20better%20performance%20and%20highlights%0Athat%20many%20current%20models%20are%20over-tokenized%20and%20under-optimized%20for%0Ascalability.%20We%20validate%20our%20method%20across%20multiple%203D%20vision%20tasks%20and%20show%0Aconsistent%20improvements%20in%20computational%20efficiency.%20This%20work%20is%20the%20first%20to%0Aassess%20redundancy%20in%20large-scale%203D%20transformer%20models%2C%20providing%20insights%20into%0Athe%20development%20of%20more%20efficient%203D%20foundation%20architectures.%20Our%20code%20and%0Acheckpoints%20are%20publicly%20available%20at%20https%3A//gitmerge3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Many%2520Tokens%2520Do%25203D%2520Point%2520Cloud%2520Transformer%2520Architectures%2520Really%2520Need%253F%26entry.906535625%3DTuan%2520Anh%2520Tran%2520and%2520Duy%2520M.%2520H.%2520Nguyen%2520and%2520Hoai-Chau%2520Tran%2520and%2520Michael%2520Barz%2520and%2520Khoa%2520D.%2520Doan%2520and%2520Roger%2520Wattenhofer%2520and%2520Ngo%2520Anh%2520Vien%2520and%2520Mathias%2520Niepert%2520and%2520Daniel%2520Sonntag%2520and%2520Paul%2520Swoboda%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520point%2520cloud%2520transformers%2520have%2520led%2520to%2520state-of-the-art%250Aresults%2520in%2520tasks%2520such%2520as%2520semantic%2520segmentation%2520and%2520reconstruction.%2520However%252C%250Athese%2520models%2520typically%2520rely%2520on%2520dense%2520token%2520representations%252C%2520incurring%2520high%250Acomputational%2520and%2520memory%2520costs%2520during%2520training%2520and%2520inference.%2520In%2520this%2520work%252C%2520we%250Apresent%2520the%2520finding%2520that%2520tokens%2520are%2520remarkably%2520redundant%252C%2520leading%2520to%250Asubstantial%2520inefficiency.%2520We%2520introduce%2520gitmerge3D%252C%2520a%2520globally%2520informed%2520graph%250Atoken%2520merging%2520method%2520that%2520can%2520reduce%2520the%2520token%2520count%2520by%2520up%2520to%252090-95%2525%2520while%250Amaintaining%2520competitive%2520performance.%2520This%2520finding%2520challenges%2520the%2520prevailing%250Aassumption%2520that%2520more%2520tokens%2520inherently%2520yield%2520better%2520performance%2520and%2520highlights%250Athat%2520many%2520current%2520models%2520are%2520over-tokenized%2520and%2520under-optimized%2520for%250Ascalability.%2520We%2520validate%2520our%2520method%2520across%2520multiple%25203D%2520vision%2520tasks%2520and%2520show%250Aconsistent%2520improvements%2520in%2520computational%2520efficiency.%2520This%2520work%2520is%2520the%2520first%2520to%250Aassess%2520redundancy%2520in%2520large-scale%25203D%2520transformer%2520models%252C%2520providing%2520insights%2520into%250Athe%2520development%2520of%2520more%2520efficient%25203D%2520foundation%2520architectures.%2520Our%2520code%2520and%250Acheckpoints%2520are%2520publicly%2520available%2520at%2520https%253A//gitmerge3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Many%20Tokens%20Do%203D%20Point%20Cloud%20Transformer%20Architectures%20Really%20Need%3F&entry.906535625=Tuan%20Anh%20Tran%20and%20Duy%20M.%20H.%20Nguyen%20and%20Hoai-Chau%20Tran%20and%20Michael%20Barz%20and%20Khoa%20D.%20Doan%20and%20Roger%20Wattenhofer%20and%20Ngo%20Anh%20Vien%20and%20Mathias%20Niepert%20and%20Daniel%20Sonntag%20and%20Paul%20Swoboda&entry.1292438233=%20%20Recent%20advances%20in%203D%20point%20cloud%20transformers%20have%20led%20to%20state-of-the-art%0Aresults%20in%20tasks%20such%20as%20semantic%20segmentation%20and%20reconstruction.%20However%2C%0Athese%20models%20typically%20rely%20on%20dense%20token%20representations%2C%20incurring%20high%0Acomputational%20and%20memory%20costs%20during%20training%20and%20inference.%20In%20this%20work%2C%20we%0Apresent%20the%20finding%20that%20tokens%20are%20remarkably%20redundant%2C%20leading%20to%0Asubstantial%20inefficiency.%20We%20introduce%20gitmerge3D%2C%20a%20globally%20informed%20graph%0Atoken%20merging%20method%20that%20can%20reduce%20the%20token%20count%20by%20up%20to%2090-95%25%20while%0Amaintaining%20competitive%20performance.%20This%20finding%20challenges%20the%20prevailing%0Aassumption%20that%20more%20tokens%20inherently%20yield%20better%20performance%20and%20highlights%0Athat%20many%20current%20models%20are%20over-tokenized%20and%20under-optimized%20for%0Ascalability.%20We%20validate%20our%20method%20across%20multiple%203D%20vision%20tasks%20and%20show%0Aconsistent%20improvements%20in%20computational%20efficiency.%20This%20work%20is%20the%20first%20to%0Aassess%20redundancy%20in%20large-scale%203D%20transformer%20models%2C%20providing%20insights%20into%0Athe%20development%20of%20more%20efficient%203D%20foundation%20architectures.%20Our%20code%20and%0Acheckpoints%20are%20publicly%20available%20at%20https%3A//gitmerge3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05449v1&entry.124074799=Read"},
{"title": "APP: Accelerated Path Patching with Task-Specific Pruning", "author": "Frauke Andersen and William Rudman and Ruochen Zhang and Carsten Eickhoff", "abstract": "  Circuit discovery is a key step in many mechanistic interpretability\npipelines. Current methods, such as Path Patching, are computationally\nexpensive and have limited in-depth circuit analysis for smaller models. In\nthis study, we propose Accelerated Path Patching (APP), a hybrid approach\nleveraging our novel contrastive attention head pruning method to drastically\nreduce the search space of circuit discovery methods. Our Contrastive-FLAP\npruning algorithm uses techniques from causal mediation analysis to assign\nhigher pruning scores to task-specific attention heads, leading to higher\nperforming sparse models compared to traditional pruning techniques. Although\nContrastive-FLAP is successful at preserving task-specific heads that existing\npruning algorithms remove at low sparsity ratios, the circuits found by\nContrastive-FLAP alone are too large to satisfy the minimality constraint\nrequired in circuit analysis. APP first applies Contrastive-FLAP to reduce the\nsearch space on required for circuit discovery algorithms by, on average, 56\\%.\nNext, APP, applies traditional Path Patching on the remaining attention heads,\nleading to a speed up of 59.63\\%-93.27\\% compared to Path Patching applied to\nthe dense model. Despite the substantial computational saving that APP\nprovides, circuits obtained from APP exhibit substantial overlap and similar\nperformance to previously established Path Patching circuits\n", "link": "http://arxiv.org/abs/2511.05442v1", "date": "2025-11-07", "relevancy": 2.2804, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APP%3A%20Accelerated%20Path%20Patching%20with%20Task-Specific%20Pruning&body=Title%3A%20APP%3A%20Accelerated%20Path%20Patching%20with%20Task-Specific%20Pruning%0AAuthor%3A%20Frauke%20Andersen%20and%20William%20Rudman%20and%20Ruochen%20Zhang%20and%20Carsten%20Eickhoff%0AAbstract%3A%20%20%20Circuit%20discovery%20is%20a%20key%20step%20in%20many%20mechanistic%20interpretability%0Apipelines.%20Current%20methods%2C%20such%20as%20Path%20Patching%2C%20are%20computationally%0Aexpensive%20and%20have%20limited%20in-depth%20circuit%20analysis%20for%20smaller%20models.%20In%0Athis%20study%2C%20we%20propose%20Accelerated%20Path%20Patching%20%28APP%29%2C%20a%20hybrid%20approach%0Aleveraging%20our%20novel%20contrastive%20attention%20head%20pruning%20method%20to%20drastically%0Areduce%20the%20search%20space%20of%20circuit%20discovery%20methods.%20Our%20Contrastive-FLAP%0Apruning%20algorithm%20uses%20techniques%20from%20causal%20mediation%20analysis%20to%20assign%0Ahigher%20pruning%20scores%20to%20task-specific%20attention%20heads%2C%20leading%20to%20higher%0Aperforming%20sparse%20models%20compared%20to%20traditional%20pruning%20techniques.%20Although%0AContrastive-FLAP%20is%20successful%20at%20preserving%20task-specific%20heads%20that%20existing%0Apruning%20algorithms%20remove%20at%20low%20sparsity%20ratios%2C%20the%20circuits%20found%20by%0AContrastive-FLAP%20alone%20are%20too%20large%20to%20satisfy%20the%20minimality%20constraint%0Arequired%20in%20circuit%20analysis.%20APP%20first%20applies%20Contrastive-FLAP%20to%20reduce%20the%0Asearch%20space%20on%20required%20for%20circuit%20discovery%20algorithms%20by%2C%20on%20average%2C%2056%5C%25.%0ANext%2C%20APP%2C%20applies%20traditional%20Path%20Patching%20on%20the%20remaining%20attention%20heads%2C%0Aleading%20to%20a%20speed%20up%20of%2059.63%5C%25-93.27%5C%25%20compared%20to%20Path%20Patching%20applied%20to%0Athe%20dense%20model.%20Despite%20the%20substantial%20computational%20saving%20that%20APP%0Aprovides%2C%20circuits%20obtained%20from%20APP%20exhibit%20substantial%20overlap%20and%20similar%0Aperformance%20to%20previously%20established%20Path%20Patching%20circuits%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPP%253A%2520Accelerated%2520Path%2520Patching%2520with%2520Task-Specific%2520Pruning%26entry.906535625%3DFrauke%2520Andersen%2520and%2520William%2520Rudman%2520and%2520Ruochen%2520Zhang%2520and%2520Carsten%2520Eickhoff%26entry.1292438233%3D%2520%2520Circuit%2520discovery%2520is%2520a%2520key%2520step%2520in%2520many%2520mechanistic%2520interpretability%250Apipelines.%2520Current%2520methods%252C%2520such%2520as%2520Path%2520Patching%252C%2520are%2520computationally%250Aexpensive%2520and%2520have%2520limited%2520in-depth%2520circuit%2520analysis%2520for%2520smaller%2520models.%2520In%250Athis%2520study%252C%2520we%2520propose%2520Accelerated%2520Path%2520Patching%2520%2528APP%2529%252C%2520a%2520hybrid%2520approach%250Aleveraging%2520our%2520novel%2520contrastive%2520attention%2520head%2520pruning%2520method%2520to%2520drastically%250Areduce%2520the%2520search%2520space%2520of%2520circuit%2520discovery%2520methods.%2520Our%2520Contrastive-FLAP%250Apruning%2520algorithm%2520uses%2520techniques%2520from%2520causal%2520mediation%2520analysis%2520to%2520assign%250Ahigher%2520pruning%2520scores%2520to%2520task-specific%2520attention%2520heads%252C%2520leading%2520to%2520higher%250Aperforming%2520sparse%2520models%2520compared%2520to%2520traditional%2520pruning%2520techniques.%2520Although%250AContrastive-FLAP%2520is%2520successful%2520at%2520preserving%2520task-specific%2520heads%2520that%2520existing%250Apruning%2520algorithms%2520remove%2520at%2520low%2520sparsity%2520ratios%252C%2520the%2520circuits%2520found%2520by%250AContrastive-FLAP%2520alone%2520are%2520too%2520large%2520to%2520satisfy%2520the%2520minimality%2520constraint%250Arequired%2520in%2520circuit%2520analysis.%2520APP%2520first%2520applies%2520Contrastive-FLAP%2520to%2520reduce%2520the%250Asearch%2520space%2520on%2520required%2520for%2520circuit%2520discovery%2520algorithms%2520by%252C%2520on%2520average%252C%252056%255C%2525.%250ANext%252C%2520APP%252C%2520applies%2520traditional%2520Path%2520Patching%2520on%2520the%2520remaining%2520attention%2520heads%252C%250Aleading%2520to%2520a%2520speed%2520up%2520of%252059.63%255C%2525-93.27%255C%2525%2520compared%2520to%2520Path%2520Patching%2520applied%2520to%250Athe%2520dense%2520model.%2520Despite%2520the%2520substantial%2520computational%2520saving%2520that%2520APP%250Aprovides%252C%2520circuits%2520obtained%2520from%2520APP%2520exhibit%2520substantial%2520overlap%2520and%2520similar%250Aperformance%2520to%2520previously%2520established%2520Path%2520Patching%2520circuits%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APP%3A%20Accelerated%20Path%20Patching%20with%20Task-Specific%20Pruning&entry.906535625=Frauke%20Andersen%20and%20William%20Rudman%20and%20Ruochen%20Zhang%20and%20Carsten%20Eickhoff&entry.1292438233=%20%20Circuit%20discovery%20is%20a%20key%20step%20in%20many%20mechanistic%20interpretability%0Apipelines.%20Current%20methods%2C%20such%20as%20Path%20Patching%2C%20are%20computationally%0Aexpensive%20and%20have%20limited%20in-depth%20circuit%20analysis%20for%20smaller%20models.%20In%0Athis%20study%2C%20we%20propose%20Accelerated%20Path%20Patching%20%28APP%29%2C%20a%20hybrid%20approach%0Aleveraging%20our%20novel%20contrastive%20attention%20head%20pruning%20method%20to%20drastically%0Areduce%20the%20search%20space%20of%20circuit%20discovery%20methods.%20Our%20Contrastive-FLAP%0Apruning%20algorithm%20uses%20techniques%20from%20causal%20mediation%20analysis%20to%20assign%0Ahigher%20pruning%20scores%20to%20task-specific%20attention%20heads%2C%20leading%20to%20higher%0Aperforming%20sparse%20models%20compared%20to%20traditional%20pruning%20techniques.%20Although%0AContrastive-FLAP%20is%20successful%20at%20preserving%20task-specific%20heads%20that%20existing%0Apruning%20algorithms%20remove%20at%20low%20sparsity%20ratios%2C%20the%20circuits%20found%20by%0AContrastive-FLAP%20alone%20are%20too%20large%20to%20satisfy%20the%20minimality%20constraint%0Arequired%20in%20circuit%20analysis.%20APP%20first%20applies%20Contrastive-FLAP%20to%20reduce%20the%0Asearch%20space%20on%20required%20for%20circuit%20discovery%20algorithms%20by%2C%20on%20average%2C%2056%5C%25.%0ANext%2C%20APP%2C%20applies%20traditional%20Path%20Patching%20on%20the%20remaining%20attention%20heads%2C%0Aleading%20to%20a%20speed%20up%20of%2059.63%5C%25-93.27%5C%25%20compared%20to%20Path%20Patching%20applied%20to%0Athe%20dense%20model.%20Despite%20the%20substantial%20computational%20saving%20that%20APP%0Aprovides%2C%20circuits%20obtained%20from%20APP%20exhibit%20substantial%20overlap%20and%20similar%0Aperformance%20to%20previously%20established%20Path%20Patching%20circuits%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05442v1&entry.124074799=Read"},
{"title": "Semantic-Guided Natural Language and Visual Fusion for Cross-Modal\n  Interaction Based on Tiny Object Detection", "author": "Xian-Hong Huang and Hui-Kai Su and Chi-Chia Sun and Jun-Wei Hsieh", "abstract": "  This paper introduces a cutting-edge approach to cross-modal interaction for\ntiny object detection by combining semantic-guided natural language processing\nwith advanced visual recognition backbones. The proposed method integrates the\nBERT language model with the CNN-based Parallel Residual Bi-Fusion Feature\nPyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures\nsuch as ELAN, MSP, and CSP to optimize feature extraction and fusion. By\nemploying lemmatization and fine-tuning techniques, the system aligns semantic\ncues from textual inputs with visual features, enhancing detection precision\nfor small and complex objects. Experimental validation using the COCO and\nObjects365 datasets demonstrates that the model achieves superior performance.\nOn the COCO2017 validation set, it attains a 52.6% average precision (AP),\noutperforming YOLO-World significantly while maintaining half the parameter\nconsumption of Transformer-based models like GLIP. Several test on different of\nbackbones such ELAN, MSP, and CSP further enable efficient handling of\nmulti-scale objects, ensuring scalability and robustness in\nresource-constrained environments. This study underscores the potential of\nintegrating natural language understanding with advanced backbone\narchitectures, setting new benchmarks in object detection accuracy, efficiency,\nand adaptability to real-world challenges.\n", "link": "http://arxiv.org/abs/2511.05474v1", "date": "2025-11-07", "relevancy": 2.2684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Guided%20Natural%20Language%20and%20Visual%20Fusion%20for%20Cross-Modal%0A%20%20Interaction%20Based%20on%20Tiny%20Object%20Detection&body=Title%3A%20Semantic-Guided%20Natural%20Language%20and%20Visual%20Fusion%20for%20Cross-Modal%0A%20%20Interaction%20Based%20on%20Tiny%20Object%20Detection%0AAuthor%3A%20Xian-Hong%20Huang%20and%20Hui-Kai%20Su%20and%20Chi-Chia%20Sun%20and%20Jun-Wei%20Hsieh%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20cutting-edge%20approach%20to%20cross-modal%20interaction%20for%0Atiny%20object%20detection%20by%20combining%20semantic-guided%20natural%20language%20processing%0Awith%20advanced%20visual%20recognition%20backbones.%20The%20proposed%20method%20integrates%20the%0ABERT%20language%20model%20with%20the%20CNN-based%20Parallel%20Residual%20Bi-Fusion%20Feature%0APyramid%20Network%20%28PRB-FPN-Net%29%2C%20incorporating%20innovative%20backbone%20architectures%0Asuch%20as%20ELAN%2C%20MSP%2C%20and%20CSP%20to%20optimize%20feature%20extraction%20and%20fusion.%20By%0Aemploying%20lemmatization%20and%20fine-tuning%20techniques%2C%20the%20system%20aligns%20semantic%0Acues%20from%20textual%20inputs%20with%20visual%20features%2C%20enhancing%20detection%20precision%0Afor%20small%20and%20complex%20objects.%20Experimental%20validation%20using%20the%20COCO%20and%0AObjects365%20datasets%20demonstrates%20that%20the%20model%20achieves%20superior%20performance.%0AOn%20the%20COCO2017%20validation%20set%2C%20it%20attains%20a%2052.6%25%20average%20precision%20%28AP%29%2C%0Aoutperforming%20YOLO-World%20significantly%20while%20maintaining%20half%20the%20parameter%0Aconsumption%20of%20Transformer-based%20models%20like%20GLIP.%20Several%20test%20on%20different%20of%0Abackbones%20such%20ELAN%2C%20MSP%2C%20and%20CSP%20further%20enable%20efficient%20handling%20of%0Amulti-scale%20objects%2C%20ensuring%20scalability%20and%20robustness%20in%0Aresource-constrained%20environments.%20This%20study%20underscores%20the%20potential%20of%0Aintegrating%20natural%20language%20understanding%20with%20advanced%20backbone%0Aarchitectures%2C%20setting%20new%20benchmarks%20in%20object%20detection%20accuracy%2C%20efficiency%2C%0Aand%20adaptability%20to%20real-world%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Guided%2520Natural%2520Language%2520and%2520Visual%2520Fusion%2520for%2520Cross-Modal%250A%2520%2520Interaction%2520Based%2520on%2520Tiny%2520Object%2520Detection%26entry.906535625%3DXian-Hong%2520Huang%2520and%2520Hui-Kai%2520Su%2520and%2520Chi-Chia%2520Sun%2520and%2520Jun-Wei%2520Hsieh%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520cutting-edge%2520approach%2520to%2520cross-modal%2520interaction%2520for%250Atiny%2520object%2520detection%2520by%2520combining%2520semantic-guided%2520natural%2520language%2520processing%250Awith%2520advanced%2520visual%2520recognition%2520backbones.%2520The%2520proposed%2520method%2520integrates%2520the%250ABERT%2520language%2520model%2520with%2520the%2520CNN-based%2520Parallel%2520Residual%2520Bi-Fusion%2520Feature%250APyramid%2520Network%2520%2528PRB-FPN-Net%2529%252C%2520incorporating%2520innovative%2520backbone%2520architectures%250Asuch%2520as%2520ELAN%252C%2520MSP%252C%2520and%2520CSP%2520to%2520optimize%2520feature%2520extraction%2520and%2520fusion.%2520By%250Aemploying%2520lemmatization%2520and%2520fine-tuning%2520techniques%252C%2520the%2520system%2520aligns%2520semantic%250Acues%2520from%2520textual%2520inputs%2520with%2520visual%2520features%252C%2520enhancing%2520detection%2520precision%250Afor%2520small%2520and%2520complex%2520objects.%2520Experimental%2520validation%2520using%2520the%2520COCO%2520and%250AObjects365%2520datasets%2520demonstrates%2520that%2520the%2520model%2520achieves%2520superior%2520performance.%250AOn%2520the%2520COCO2017%2520validation%2520set%252C%2520it%2520attains%2520a%252052.6%2525%2520average%2520precision%2520%2528AP%2529%252C%250Aoutperforming%2520YOLO-World%2520significantly%2520while%2520maintaining%2520half%2520the%2520parameter%250Aconsumption%2520of%2520Transformer-based%2520models%2520like%2520GLIP.%2520Several%2520test%2520on%2520different%2520of%250Abackbones%2520such%2520ELAN%252C%2520MSP%252C%2520and%2520CSP%2520further%2520enable%2520efficient%2520handling%2520of%250Amulti-scale%2520objects%252C%2520ensuring%2520scalability%2520and%2520robustness%2520in%250Aresource-constrained%2520environments.%2520This%2520study%2520underscores%2520the%2520potential%2520of%250Aintegrating%2520natural%2520language%2520understanding%2520with%2520advanced%2520backbone%250Aarchitectures%252C%2520setting%2520new%2520benchmarks%2520in%2520object%2520detection%2520accuracy%252C%2520efficiency%252C%250Aand%2520adaptability%2520to%2520real-world%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Guided%20Natural%20Language%20and%20Visual%20Fusion%20for%20Cross-Modal%0A%20%20Interaction%20Based%20on%20Tiny%20Object%20Detection&entry.906535625=Xian-Hong%20Huang%20and%20Hui-Kai%20Su%20and%20Chi-Chia%20Sun%20and%20Jun-Wei%20Hsieh&entry.1292438233=%20%20This%20paper%20introduces%20a%20cutting-edge%20approach%20to%20cross-modal%20interaction%20for%0Atiny%20object%20detection%20by%20combining%20semantic-guided%20natural%20language%20processing%0Awith%20advanced%20visual%20recognition%20backbones.%20The%20proposed%20method%20integrates%20the%0ABERT%20language%20model%20with%20the%20CNN-based%20Parallel%20Residual%20Bi-Fusion%20Feature%0APyramid%20Network%20%28PRB-FPN-Net%29%2C%20incorporating%20innovative%20backbone%20architectures%0Asuch%20as%20ELAN%2C%20MSP%2C%20and%20CSP%20to%20optimize%20feature%20extraction%20and%20fusion.%20By%0Aemploying%20lemmatization%20and%20fine-tuning%20techniques%2C%20the%20system%20aligns%20semantic%0Acues%20from%20textual%20inputs%20with%20visual%20features%2C%20enhancing%20detection%20precision%0Afor%20small%20and%20complex%20objects.%20Experimental%20validation%20using%20the%20COCO%20and%0AObjects365%20datasets%20demonstrates%20that%20the%20model%20achieves%20superior%20performance.%0AOn%20the%20COCO2017%20validation%20set%2C%20it%20attains%20a%2052.6%25%20average%20precision%20%28AP%29%2C%0Aoutperforming%20YOLO-World%20significantly%20while%20maintaining%20half%20the%20parameter%0Aconsumption%20of%20Transformer-based%20models%20like%20GLIP.%20Several%20test%20on%20different%20of%0Abackbones%20such%20ELAN%2C%20MSP%2C%20and%20CSP%20further%20enable%20efficient%20handling%20of%0Amulti-scale%20objects%2C%20ensuring%20scalability%20and%20robustness%20in%0Aresource-constrained%20environments.%20This%20study%20underscores%20the%20potential%20of%0Aintegrating%20natural%20language%20understanding%20with%20advanced%20backbone%0Aarchitectures%2C%20setting%20new%20benchmarks%20in%20object%20detection%20accuracy%2C%20efficiency%2C%0Aand%20adaptability%20to%20real-world%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05474v1&entry.124074799=Read"},
{"title": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis", "author": "Dogucan Yaman and Seymanur Akti and Fevziye Irem Eyiokur and Alexander Waibel", "abstract": "  We propose a text-to-talking-face synthesis framework leveraging latent\nspeech representations from HierSpeech++. A Text-to-Vec module generates\nWav2Vec2 embeddings from text, which jointly condition speech and face\ngeneration. To handle distribution shifts between clean and TTS-predicted\nfeatures, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and\nfinetuning on TTS outputs. This enables tight audio-visual alignment, preserves\nspeaker identity, and produces natural, expressive speech and synchronized\nfacial motion without ground-truth audio at inference. Experiments show that\nconditioning on TTS-predicted latent features outperforms cascaded pipelines,\nimproving both lip-sync and visual realism.\n", "link": "http://arxiv.org/abs/2511.05432v1", "date": "2025-11-07", "relevancy": 2.207, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5711}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5524}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shared%20Latent%20Representation%20for%20Joint%20Text-to-Audio-Visual%20Synthesis&body=Title%3A%20Shared%20Latent%20Representation%20for%20Joint%20Text-to-Audio-Visual%20Synthesis%0AAuthor%3A%20Dogucan%20Yaman%20and%20Seymanur%20Akti%20and%20Fevziye%20Irem%20Eyiokur%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20We%20propose%20a%20text-to-talking-face%20synthesis%20framework%20leveraging%20latent%0Aspeech%20representations%20from%20HierSpeech%2B%2B.%20A%20Text-to-Vec%20module%20generates%0AWav2Vec2%20embeddings%20from%20text%2C%20which%20jointly%20condition%20speech%20and%20face%0Ageneration.%20To%20handle%20distribution%20shifts%20between%20clean%20and%20TTS-predicted%0Afeatures%2C%20we%20adopt%20a%20two-stage%20training%3A%20pretraining%20on%20Wav2Vec2%20embeddings%20and%0Afinetuning%20on%20TTS%20outputs.%20This%20enables%20tight%20audio-visual%20alignment%2C%20preserves%0Aspeaker%20identity%2C%20and%20produces%20natural%2C%20expressive%20speech%20and%20synchronized%0Afacial%20motion%20without%20ground-truth%20audio%20at%20inference.%20Experiments%20show%20that%0Aconditioning%20on%20TTS-predicted%20latent%20features%20outperforms%20cascaded%20pipelines%2C%0Aimproving%20both%20lip-sync%20and%20visual%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShared%2520Latent%2520Representation%2520for%2520Joint%2520Text-to-Audio-Visual%2520Synthesis%26entry.906535625%3DDogucan%2520Yaman%2520and%2520Seymanur%2520Akti%2520and%2520Fevziye%2520Irem%2520Eyiokur%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520text-to-talking-face%2520synthesis%2520framework%2520leveraging%2520latent%250Aspeech%2520representations%2520from%2520HierSpeech%252B%252B.%2520A%2520Text-to-Vec%2520module%2520generates%250AWav2Vec2%2520embeddings%2520from%2520text%252C%2520which%2520jointly%2520condition%2520speech%2520and%2520face%250Ageneration.%2520To%2520handle%2520distribution%2520shifts%2520between%2520clean%2520and%2520TTS-predicted%250Afeatures%252C%2520we%2520adopt%2520a%2520two-stage%2520training%253A%2520pretraining%2520on%2520Wav2Vec2%2520embeddings%2520and%250Afinetuning%2520on%2520TTS%2520outputs.%2520This%2520enables%2520tight%2520audio-visual%2520alignment%252C%2520preserves%250Aspeaker%2520identity%252C%2520and%2520produces%2520natural%252C%2520expressive%2520speech%2520and%2520synchronized%250Afacial%2520motion%2520without%2520ground-truth%2520audio%2520at%2520inference.%2520Experiments%2520show%2520that%250Aconditioning%2520on%2520TTS-predicted%2520latent%2520features%2520outperforms%2520cascaded%2520pipelines%252C%250Aimproving%2520both%2520lip-sync%2520and%2520visual%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shared%20Latent%20Representation%20for%20Joint%20Text-to-Audio-Visual%20Synthesis&entry.906535625=Dogucan%20Yaman%20and%20Seymanur%20Akti%20and%20Fevziye%20Irem%20Eyiokur%20and%20Alexander%20Waibel&entry.1292438233=%20%20We%20propose%20a%20text-to-talking-face%20synthesis%20framework%20leveraging%20latent%0Aspeech%20representations%20from%20HierSpeech%2B%2B.%20A%20Text-to-Vec%20module%20generates%0AWav2Vec2%20embeddings%20from%20text%2C%20which%20jointly%20condition%20speech%20and%20face%0Ageneration.%20To%20handle%20distribution%20shifts%20between%20clean%20and%20TTS-predicted%0Afeatures%2C%20we%20adopt%20a%20two-stage%20training%3A%20pretraining%20on%20Wav2Vec2%20embeddings%20and%0Afinetuning%20on%20TTS%20outputs.%20This%20enables%20tight%20audio-visual%20alignment%2C%20preserves%0Aspeaker%20identity%2C%20and%20produces%20natural%2C%20expressive%20speech%20and%20synchronized%0Afacial%20motion%20without%20ground-truth%20audio%20at%20inference.%20Experiments%20show%20that%0Aconditioning%20on%20TTS-predicted%20latent%20features%20outperforms%20cascaded%20pipelines%2C%0Aimproving%20both%20lip-sync%20and%20visual%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05432v1&entry.124074799=Read"},
{"title": "TRACE: Textual Relevance Augmentation and Contextual Encoding for\n  Multimodal Hate Detection", "author": "Girish A. Koushik and Helen Treharne and Aditya Joshi and Diptesh Kanojia", "abstract": "  Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. To tackle\nthese challenges, we introduce TRACE, a hierarchical multimodal framework that\nleverages visually grounded context augmentation, along with a novel\ncaption-scoring network to emphasize hate-relevant content, and\nparameter-efficient fine-tuning of CLIP's text encoder. Our experiments\ndemonstrate that selectively fine-tuning deeper text encoder layers\nsignificantly enhances performance compared to simpler projection-layer\nfine-tuning methods. Specifically, our framework achieves state-of-the-art\naccuracy (0.807) and F1-score (0.806) on the widely-used Hateful Memes dataset,\nmatching the performance of considerably larger models while maintaining\nefficiency. Moreover, it achieves superior generalization on the MultiOFF\noffensive meme dataset (F1-score 0.673), highlighting robustness across meme\ncategories. Additional analyses confirm that robust visual grounding and\nnuanced text representations significantly reduce errors caused by benign\nconfounders. We publicly release our code to facilitate future research.\n", "link": "http://arxiv.org/abs/2504.17902v2", "date": "2025-11-07", "relevancy": 2.1642, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5959}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20Textual%20Relevance%20Augmentation%20and%20Contextual%20Encoding%20for%0A%20%20Multimodal%20Hate%20Detection&body=Title%3A%20TRACE%3A%20Textual%20Relevance%20Augmentation%20and%20Contextual%20Encoding%20for%0A%20%20Multimodal%20Hate%20Detection%0AAuthor%3A%20Girish%20A.%20Koushik%20and%20Helen%20Treharne%20and%20Aditya%20Joshi%20and%20Diptesh%20Kanojia%0AAbstract%3A%20%20%20Social%20media%20memes%20are%20a%20challenging%20domain%20for%20hate%20detection%20because%20they%0Aintertwine%20visual%20and%20textual%20cues%20into%20culturally%20nuanced%20messages.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20TRACE%2C%20a%20hierarchical%20multimodal%20framework%20that%0Aleverages%20visually%20grounded%20context%20augmentation%2C%20along%20with%20a%20novel%0Acaption-scoring%20network%20to%20emphasize%20hate-relevant%20content%2C%20and%0Aparameter-efficient%20fine-tuning%20of%20CLIP%27s%20text%20encoder.%20Our%20experiments%0Ademonstrate%20that%20selectively%20fine-tuning%20deeper%20text%20encoder%20layers%0Asignificantly%20enhances%20performance%20compared%20to%20simpler%20projection-layer%0Afine-tuning%20methods.%20Specifically%2C%20our%20framework%20achieves%20state-of-the-art%0Aaccuracy%20%280.807%29%20and%20F1-score%20%280.806%29%20on%20the%20widely-used%20Hateful%20Memes%20dataset%2C%0Amatching%20the%20performance%20of%20considerably%20larger%20models%20while%20maintaining%0Aefficiency.%20Moreover%2C%20it%20achieves%20superior%20generalization%20on%20the%20MultiOFF%0Aoffensive%20meme%20dataset%20%28F1-score%200.673%29%2C%20highlighting%20robustness%20across%20meme%0Acategories.%20Additional%20analyses%20confirm%20that%20robust%20visual%20grounding%20and%0Anuanced%20text%20representations%20significantly%20reduce%20errors%20caused%20by%20benign%0Aconfounders.%20We%20publicly%20release%20our%20code%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17902v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520Textual%2520Relevance%2520Augmentation%2520and%2520Contextual%2520Encoding%2520for%250A%2520%2520Multimodal%2520Hate%2520Detection%26entry.906535625%3DGirish%2520A.%2520Koushik%2520and%2520Helen%2520Treharne%2520and%2520Aditya%2520Joshi%2520and%2520Diptesh%2520Kanojia%26entry.1292438233%3D%2520%2520Social%2520media%2520memes%2520are%2520a%2520challenging%2520domain%2520for%2520hate%2520detection%2520because%2520they%250Aintertwine%2520visual%2520and%2520textual%2520cues%2520into%2520culturally%2520nuanced%2520messages.%2520To%2520tackle%250Athese%2520challenges%252C%2520we%2520introduce%2520TRACE%252C%2520a%2520hierarchical%2520multimodal%2520framework%2520that%250Aleverages%2520visually%2520grounded%2520context%2520augmentation%252C%2520along%2520with%2520a%2520novel%250Acaption-scoring%2520network%2520to%2520emphasize%2520hate-relevant%2520content%252C%2520and%250Aparameter-efficient%2520fine-tuning%2520of%2520CLIP%2527s%2520text%2520encoder.%2520Our%2520experiments%250Ademonstrate%2520that%2520selectively%2520fine-tuning%2520deeper%2520text%2520encoder%2520layers%250Asignificantly%2520enhances%2520performance%2520compared%2520to%2520simpler%2520projection-layer%250Afine-tuning%2520methods.%2520Specifically%252C%2520our%2520framework%2520achieves%2520state-of-the-art%250Aaccuracy%2520%25280.807%2529%2520and%2520F1-score%2520%25280.806%2529%2520on%2520the%2520widely-used%2520Hateful%2520Memes%2520dataset%252C%250Amatching%2520the%2520performance%2520of%2520considerably%2520larger%2520models%2520while%2520maintaining%250Aefficiency.%2520Moreover%252C%2520it%2520achieves%2520superior%2520generalization%2520on%2520the%2520MultiOFF%250Aoffensive%2520meme%2520dataset%2520%2528F1-score%25200.673%2529%252C%2520highlighting%2520robustness%2520across%2520meme%250Acategories.%2520Additional%2520analyses%2520confirm%2520that%2520robust%2520visual%2520grounding%2520and%250Anuanced%2520text%2520representations%2520significantly%2520reduce%2520errors%2520caused%2520by%2520benign%250Aconfounders.%2520We%2520publicly%2520release%2520our%2520code%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17902v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20Textual%20Relevance%20Augmentation%20and%20Contextual%20Encoding%20for%0A%20%20Multimodal%20Hate%20Detection&entry.906535625=Girish%20A.%20Koushik%20and%20Helen%20Treharne%20and%20Aditya%20Joshi%20and%20Diptesh%20Kanojia&entry.1292438233=%20%20Social%20media%20memes%20are%20a%20challenging%20domain%20for%20hate%20detection%20because%20they%0Aintertwine%20visual%20and%20textual%20cues%20into%20culturally%20nuanced%20messages.%20To%20tackle%0Athese%20challenges%2C%20we%20introduce%20TRACE%2C%20a%20hierarchical%20multimodal%20framework%20that%0Aleverages%20visually%20grounded%20context%20augmentation%2C%20along%20with%20a%20novel%0Acaption-scoring%20network%20to%20emphasize%20hate-relevant%20content%2C%20and%0Aparameter-efficient%20fine-tuning%20of%20CLIP%27s%20text%20encoder.%20Our%20experiments%0Ademonstrate%20that%20selectively%20fine-tuning%20deeper%20text%20encoder%20layers%0Asignificantly%20enhances%20performance%20compared%20to%20simpler%20projection-layer%0Afine-tuning%20methods.%20Specifically%2C%20our%20framework%20achieves%20state-of-the-art%0Aaccuracy%20%280.807%29%20and%20F1-score%20%280.806%29%20on%20the%20widely-used%20Hateful%20Memes%20dataset%2C%0Amatching%20the%20performance%20of%20considerably%20larger%20models%20while%20maintaining%0Aefficiency.%20Moreover%2C%20it%20achieves%20superior%20generalization%20on%20the%20MultiOFF%0Aoffensive%20meme%20dataset%20%28F1-score%200.673%29%2C%20highlighting%20robustness%20across%20meme%0Acategories.%20Additional%20analyses%20confirm%20that%20robust%20visual%20grounding%20and%0Anuanced%20text%20representations%20significantly%20reduce%20errors%20caused%20by%20benign%0Aconfounders.%20We%20publicly%20release%20our%20code%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17902v2&entry.124074799=Read"},
{"title": "Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in\n  Interactive Urban Driving", "author": "Dianzhao Li and Ostap Okhrin", "abstract": "  Autonomous vehicles hold great promise for reducing traffic fatalities and\nimproving transportation efficiency, yet their widespread adoption hinges on\nembedding credible and transparent ethical reasoning into routine and emergency\nmaneuvers, particularly to protect vulnerable road users (VRUs) such as\npedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement\nLearning (Safe RL) framework that augments standard driving objectives with\nethics-aware cost signals. At the decision level, a Safe RL agent is trained\nusing a composite ethical risk cost, combining collision probability and harm\nseverity, to generate high-level motion targets. A dynamic, risk-sensitive\nPrioritized Experience Replay mechanism amplifies learning from rare but\ncritical, high-risk events. At the execution level, polynomial path planning\ncoupled with Proportional-Integral-Derivative (PID) and Stanley controllers\ntranslates these targets into smooth, feasible trajectories, ensuring both\naccuracy and comfort. We train and validate our approach on closed-loop\nsimulation environments derived from large-scale, real-world traffic datasets\nencompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that\nit outperforms baseline methods in reducing risk to others while maintaining\nego performance and comfort. This work provides a reproducible benchmark for\nSafe RL with explicitly ethics-aware objectives in human-mixed traffic\nscenarios. Our results highlight the potential of combining formal control\ntheory and data-driven learning to advance ethically accountable autonomy that\nexplicitly protects those most at risk in urban traffic environments. Across\ntwo interactive benchmarks and five random seeds, our policy decreases conflict\nfrequency by 25-45% compared to matched task successes while maintaining\ncomfort metrics within 5%.\n", "link": "http://arxiv.org/abs/2508.14926v3", "date": "2025-11-07", "relevancy": 2.1405, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5409}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ethics-Aware%20Safe%20Reinforcement%20Learning%20for%20Rare-Event%20Risk%20Control%20in%0A%20%20Interactive%20Urban%20Driving&body=Title%3A%20Ethics-Aware%20Safe%20Reinforcement%20Learning%20for%20Rare-Event%20Risk%20Control%20in%0A%20%20Interactive%20Urban%20Driving%0AAuthor%3A%20Dianzhao%20Li%20and%20Ostap%20Okhrin%0AAbstract%3A%20%20%20Autonomous%20vehicles%20hold%20great%20promise%20for%20reducing%20traffic%20fatalities%20and%0Aimproving%20transportation%20efficiency%2C%20yet%20their%20widespread%20adoption%20hinges%20on%0Aembedding%20credible%20and%20transparent%20ethical%20reasoning%20into%20routine%20and%20emergency%0Amaneuvers%2C%20particularly%20to%20protect%20vulnerable%20road%20users%20%28VRUs%29%20such%20as%0Apedestrians%20and%20cyclists.%20Here%2C%20we%20present%20a%20hierarchical%20Safe%20Reinforcement%0ALearning%20%28Safe%20RL%29%20framework%20that%20augments%20standard%20driving%20objectives%20with%0Aethics-aware%20cost%20signals.%20At%20the%20decision%20level%2C%20a%20Safe%20RL%20agent%20is%20trained%0Ausing%20a%20composite%20ethical%20risk%20cost%2C%20combining%20collision%20probability%20and%20harm%0Aseverity%2C%20to%20generate%20high-level%20motion%20targets.%20A%20dynamic%2C%20risk-sensitive%0APrioritized%20Experience%20Replay%20mechanism%20amplifies%20learning%20from%20rare%20but%0Acritical%2C%20high-risk%20events.%20At%20the%20execution%20level%2C%20polynomial%20path%20planning%0Acoupled%20with%20Proportional-Integral-Derivative%20%28PID%29%20and%20Stanley%20controllers%0Atranslates%20these%20targets%20into%20smooth%2C%20feasible%20trajectories%2C%20ensuring%20both%0Aaccuracy%20and%20comfort.%20We%20train%20and%20validate%20our%20approach%20on%20closed-loop%0Asimulation%20environments%20derived%20from%20large-scale%2C%20real-world%20traffic%20datasets%0Aencompassing%20diverse%20vehicles%2C%20cyclists%2C%20and%20pedestrians%2C%20and%20demonstrate%20that%0Ait%20outperforms%20baseline%20methods%20in%20reducing%20risk%20to%20others%20while%20maintaining%0Aego%20performance%20and%20comfort.%20This%20work%20provides%20a%20reproducible%20benchmark%20for%0ASafe%20RL%20with%20explicitly%20ethics-aware%20objectives%20in%20human-mixed%20traffic%0Ascenarios.%20Our%20results%20highlight%20the%20potential%20of%20combining%20formal%20control%0Atheory%20and%20data-driven%20learning%20to%20advance%20ethically%20accountable%20autonomy%20that%0Aexplicitly%20protects%20those%20most%20at%20risk%20in%20urban%20traffic%20environments.%20Across%0Atwo%20interactive%20benchmarks%20and%20five%20random%20seeds%2C%20our%20policy%20decreases%20conflict%0Afrequency%20by%2025-45%25%20compared%20to%20matched%20task%20successes%20while%20maintaining%0Acomfort%20metrics%20within%205%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14926v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEthics-Aware%2520Safe%2520Reinforcement%2520Learning%2520for%2520Rare-Event%2520Risk%2520Control%2520in%250A%2520%2520Interactive%2520Urban%2520Driving%26entry.906535625%3DDianzhao%2520Li%2520and%2520Ostap%2520Okhrin%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520hold%2520great%2520promise%2520for%2520reducing%2520traffic%2520fatalities%2520and%250Aimproving%2520transportation%2520efficiency%252C%2520yet%2520their%2520widespread%2520adoption%2520hinges%2520on%250Aembedding%2520credible%2520and%2520transparent%2520ethical%2520reasoning%2520into%2520routine%2520and%2520emergency%250Amaneuvers%252C%2520particularly%2520to%2520protect%2520vulnerable%2520road%2520users%2520%2528VRUs%2529%2520such%2520as%250Apedestrians%2520and%2520cyclists.%2520Here%252C%2520we%2520present%2520a%2520hierarchical%2520Safe%2520Reinforcement%250ALearning%2520%2528Safe%2520RL%2529%2520framework%2520that%2520augments%2520standard%2520driving%2520objectives%2520with%250Aethics-aware%2520cost%2520signals.%2520At%2520the%2520decision%2520level%252C%2520a%2520Safe%2520RL%2520agent%2520is%2520trained%250Ausing%2520a%2520composite%2520ethical%2520risk%2520cost%252C%2520combining%2520collision%2520probability%2520and%2520harm%250Aseverity%252C%2520to%2520generate%2520high-level%2520motion%2520targets.%2520A%2520dynamic%252C%2520risk-sensitive%250APrioritized%2520Experience%2520Replay%2520mechanism%2520amplifies%2520learning%2520from%2520rare%2520but%250Acritical%252C%2520high-risk%2520events.%2520At%2520the%2520execution%2520level%252C%2520polynomial%2520path%2520planning%250Acoupled%2520with%2520Proportional-Integral-Derivative%2520%2528PID%2529%2520and%2520Stanley%2520controllers%250Atranslates%2520these%2520targets%2520into%2520smooth%252C%2520feasible%2520trajectories%252C%2520ensuring%2520both%250Aaccuracy%2520and%2520comfort.%2520We%2520train%2520and%2520validate%2520our%2520approach%2520on%2520closed-loop%250Asimulation%2520environments%2520derived%2520from%2520large-scale%252C%2520real-world%2520traffic%2520datasets%250Aencompassing%2520diverse%2520vehicles%252C%2520cyclists%252C%2520and%2520pedestrians%252C%2520and%2520demonstrate%2520that%250Ait%2520outperforms%2520baseline%2520methods%2520in%2520reducing%2520risk%2520to%2520others%2520while%2520maintaining%250Aego%2520performance%2520and%2520comfort.%2520This%2520work%2520provides%2520a%2520reproducible%2520benchmark%2520for%250ASafe%2520RL%2520with%2520explicitly%2520ethics-aware%2520objectives%2520in%2520human-mixed%2520traffic%250Ascenarios.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520combining%2520formal%2520control%250Atheory%2520and%2520data-driven%2520learning%2520to%2520advance%2520ethically%2520accountable%2520autonomy%2520that%250Aexplicitly%2520protects%2520those%2520most%2520at%2520risk%2520in%2520urban%2520traffic%2520environments.%2520Across%250Atwo%2520interactive%2520benchmarks%2520and%2520five%2520random%2520seeds%252C%2520our%2520policy%2520decreases%2520conflict%250Afrequency%2520by%252025-45%2525%2520compared%2520to%2520matched%2520task%2520successes%2520while%2520maintaining%250Acomfort%2520metrics%2520within%25205%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14926v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ethics-Aware%20Safe%20Reinforcement%20Learning%20for%20Rare-Event%20Risk%20Control%20in%0A%20%20Interactive%20Urban%20Driving&entry.906535625=Dianzhao%20Li%20and%20Ostap%20Okhrin&entry.1292438233=%20%20Autonomous%20vehicles%20hold%20great%20promise%20for%20reducing%20traffic%20fatalities%20and%0Aimproving%20transportation%20efficiency%2C%20yet%20their%20widespread%20adoption%20hinges%20on%0Aembedding%20credible%20and%20transparent%20ethical%20reasoning%20into%20routine%20and%20emergency%0Amaneuvers%2C%20particularly%20to%20protect%20vulnerable%20road%20users%20%28VRUs%29%20such%20as%0Apedestrians%20and%20cyclists.%20Here%2C%20we%20present%20a%20hierarchical%20Safe%20Reinforcement%0ALearning%20%28Safe%20RL%29%20framework%20that%20augments%20standard%20driving%20objectives%20with%0Aethics-aware%20cost%20signals.%20At%20the%20decision%20level%2C%20a%20Safe%20RL%20agent%20is%20trained%0Ausing%20a%20composite%20ethical%20risk%20cost%2C%20combining%20collision%20probability%20and%20harm%0Aseverity%2C%20to%20generate%20high-level%20motion%20targets.%20A%20dynamic%2C%20risk-sensitive%0APrioritized%20Experience%20Replay%20mechanism%20amplifies%20learning%20from%20rare%20but%0Acritical%2C%20high-risk%20events.%20At%20the%20execution%20level%2C%20polynomial%20path%20planning%0Acoupled%20with%20Proportional-Integral-Derivative%20%28PID%29%20and%20Stanley%20controllers%0Atranslates%20these%20targets%20into%20smooth%2C%20feasible%20trajectories%2C%20ensuring%20both%0Aaccuracy%20and%20comfort.%20We%20train%20and%20validate%20our%20approach%20on%20closed-loop%0Asimulation%20environments%20derived%20from%20large-scale%2C%20real-world%20traffic%20datasets%0Aencompassing%20diverse%20vehicles%2C%20cyclists%2C%20and%20pedestrians%2C%20and%20demonstrate%20that%0Ait%20outperforms%20baseline%20methods%20in%20reducing%20risk%20to%20others%20while%20maintaining%0Aego%20performance%20and%20comfort.%20This%20work%20provides%20a%20reproducible%20benchmark%20for%0ASafe%20RL%20with%20explicitly%20ethics-aware%20objectives%20in%20human-mixed%20traffic%0Ascenarios.%20Our%20results%20highlight%20the%20potential%20of%20combining%20formal%20control%0Atheory%20and%20data-driven%20learning%20to%20advance%20ethically%20accountable%20autonomy%20that%0Aexplicitly%20protects%20those%20most%20at%20risk%20in%20urban%20traffic%20environments.%20Across%0Atwo%20interactive%20benchmarks%20and%20five%20random%20seeds%2C%20our%20policy%20decreases%20conflict%0Afrequency%20by%2025-45%25%20compared%20to%20matched%20task%20successes%20while%20maintaining%0Acomfort%20metrics%20within%205%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14926v3&entry.124074799=Read"},
{"title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?", "author": "Md. Abdul Awal and Mrigank Rochan and Chanchal K. Roy", "abstract": "  Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.\n", "link": "http://arxiv.org/abs/2511.05476v1", "date": "2025-11-07", "relevancy": 2.1252, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5352}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Metamorphic%20Testing%20Perspective%20on%20Knowledge%20Distillation%20for%20Language%0A%20%20Models%20of%20Code%3A%20Does%20the%20Student%20Deeply%20Mimic%20the%20Teacher%3F&body=Title%3A%20A%20Metamorphic%20Testing%20Perspective%20on%20Knowledge%20Distillation%20for%20Language%0A%20%20Models%20of%20Code%3A%20Does%20the%20Student%20Deeply%20Mimic%20the%20Teacher%3F%0AAuthor%3A%20Md.%20Abdul%20Awal%20and%20Mrigank%20Rochan%20and%20Chanchal%20K.%20Roy%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20of%20code%20have%20achieved%20state-of-the-art%0Aperformance%20across%20a%20wide%20range%20of%20software%20analytics%20tasks%2C%20but%20their%0Apractical%20deployment%20remains%20limited%20due%20to%20high%20computational%20costs%2C%20slow%0Ainference%20speeds%2C%20and%20significant%20environmental%20impact.%20To%20address%20these%0Achallenges%2C%20recent%20research%20has%20increasingly%20explored%20knowledge%20distillation%20as%0Aa%20method%20for%20compressing%20a%20large%20language%20model%20of%20code%20%28the%20teacher%29%20into%20a%0Asmaller%20model%20%28the%20student%29%20while%20maintaining%20performance.%20However%2C%20the%20degree%0Ato%20which%20a%20student%20model%20deeply%20mimics%20the%20predictive%20behavior%20and%20internal%0Arepresentations%20of%20its%20teacher%20remains%20largely%20unexplored%2C%20as%20current%0Aaccuracy-based%20evaluation%20provides%20only%20a%20surface-level%20view%20of%20model%20quality%0Aand%20often%20fails%20to%20capture%20more%20profound%20discrepancies%20in%20behavioral%20fidelity%0Abetween%20the%20teacher%20and%20student%20models.%20To%20address%20this%20gap%2C%20we%20empirically%0Ashow%20that%20the%20student%20model%20often%20fails%20to%20deeply%20mimic%20the%20teacher%20model%2C%0Aresulting%20in%20up%20to%20285%25%20greater%20performance%20drop%20under%20adversarial%20attacks%2C%0Awhich%20is%20not%20captured%20by%20traditional%20accuracy-based%20evaluation.%20Therefore%2C%20we%0Apropose%20MetaCompress%2C%20a%20metamorphic%20testing%20framework%20that%20systematically%0Aevaluates%20behavioral%20fidelity%20by%20comparing%20the%20outputs%20of%20teacher%20and%20student%0Amodels%20under%20a%20set%20of%20behavior-preserving%20metamorphic%20relations.%20We%20evaluate%0AMetaCompress%20on%20two%20widely%20studied%20tasks%2C%20using%20compressed%20versions%20of%20popular%0Alanguage%20models%20of%20code%2C%20obtained%20via%20three%20different%20knowledge%20distillation%0Atechniques%3A%20Compressor%2C%20AVATAR%2C%20and%20MORPH.%20The%20results%20show%20that%20MetaCompress%0Aidentifies%20up%20to%2062%25%20behavioral%20discrepancies%20in%20student%20models%2C%20underscoring%0Athe%20need%20for%20behavioral%20fidelity%20evaluation%20within%20the%20knowledge%20distillation%0Apipeline%20and%20establishing%20MetaCompress%20as%20a%20practical%20framework%20for%20testing%0Acompressed%20language%20models%20of%20code%20derived%20through%20knowledge%20distillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Metamorphic%2520Testing%2520Perspective%2520on%2520Knowledge%2520Distillation%2520for%2520Language%250A%2520%2520Models%2520of%2520Code%253A%2520Does%2520the%2520Student%2520Deeply%2520Mimic%2520the%2520Teacher%253F%26entry.906535625%3DMd.%2520Abdul%2520Awal%2520and%2520Mrigank%2520Rochan%2520and%2520Chanchal%2520K.%2520Roy%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520of%2520code%2520have%2520achieved%2520state-of-the-art%250Aperformance%2520across%2520a%2520wide%2520range%2520of%2520software%2520analytics%2520tasks%252C%2520but%2520their%250Apractical%2520deployment%2520remains%2520limited%2520due%2520to%2520high%2520computational%2520costs%252C%2520slow%250Ainference%2520speeds%252C%2520and%2520significant%2520environmental%2520impact.%2520To%2520address%2520these%250Achallenges%252C%2520recent%2520research%2520has%2520increasingly%2520explored%2520knowledge%2520distillation%2520as%250Aa%2520method%2520for%2520compressing%2520a%2520large%2520language%2520model%2520of%2520code%2520%2528the%2520teacher%2529%2520into%2520a%250Asmaller%2520model%2520%2528the%2520student%2529%2520while%2520maintaining%2520performance.%2520However%252C%2520the%2520degree%250Ato%2520which%2520a%2520student%2520model%2520deeply%2520mimics%2520the%2520predictive%2520behavior%2520and%2520internal%250Arepresentations%2520of%2520its%2520teacher%2520remains%2520largely%2520unexplored%252C%2520as%2520current%250Aaccuracy-based%2520evaluation%2520provides%2520only%2520a%2520surface-level%2520view%2520of%2520model%2520quality%250Aand%2520often%2520fails%2520to%2520capture%2520more%2520profound%2520discrepancies%2520in%2520behavioral%2520fidelity%250Abetween%2520the%2520teacher%2520and%2520student%2520models.%2520To%2520address%2520this%2520gap%252C%2520we%2520empirically%250Ashow%2520that%2520the%2520student%2520model%2520often%2520fails%2520to%2520deeply%2520mimic%2520the%2520teacher%2520model%252C%250Aresulting%2520in%2520up%2520to%2520285%2525%2520greater%2520performance%2520drop%2520under%2520adversarial%2520attacks%252C%250Awhich%2520is%2520not%2520captured%2520by%2520traditional%2520accuracy-based%2520evaluation.%2520Therefore%252C%2520we%250Apropose%2520MetaCompress%252C%2520a%2520metamorphic%2520testing%2520framework%2520that%2520systematically%250Aevaluates%2520behavioral%2520fidelity%2520by%2520comparing%2520the%2520outputs%2520of%2520teacher%2520and%2520student%250Amodels%2520under%2520a%2520set%2520of%2520behavior-preserving%2520metamorphic%2520relations.%2520We%2520evaluate%250AMetaCompress%2520on%2520two%2520widely%2520studied%2520tasks%252C%2520using%2520compressed%2520versions%2520of%2520popular%250Alanguage%2520models%2520of%2520code%252C%2520obtained%2520via%2520three%2520different%2520knowledge%2520distillation%250Atechniques%253A%2520Compressor%252C%2520AVATAR%252C%2520and%2520MORPH.%2520The%2520results%2520show%2520that%2520MetaCompress%250Aidentifies%2520up%2520to%252062%2525%2520behavioral%2520discrepancies%2520in%2520student%2520models%252C%2520underscoring%250Athe%2520need%2520for%2520behavioral%2520fidelity%2520evaluation%2520within%2520the%2520knowledge%2520distillation%250Apipeline%2520and%2520establishing%2520MetaCompress%2520as%2520a%2520practical%2520framework%2520for%2520testing%250Acompressed%2520language%2520models%2520of%2520code%2520derived%2520through%2520knowledge%2520distillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Metamorphic%20Testing%20Perspective%20on%20Knowledge%20Distillation%20for%20Language%0A%20%20Models%20of%20Code%3A%20Does%20the%20Student%20Deeply%20Mimic%20the%20Teacher%3F&entry.906535625=Md.%20Abdul%20Awal%20and%20Mrigank%20Rochan%20and%20Chanchal%20K.%20Roy&entry.1292438233=%20%20Transformer-based%20language%20models%20of%20code%20have%20achieved%20state-of-the-art%0Aperformance%20across%20a%20wide%20range%20of%20software%20analytics%20tasks%2C%20but%20their%0Apractical%20deployment%20remains%20limited%20due%20to%20high%20computational%20costs%2C%20slow%0Ainference%20speeds%2C%20and%20significant%20environmental%20impact.%20To%20address%20these%0Achallenges%2C%20recent%20research%20has%20increasingly%20explored%20knowledge%20distillation%20as%0Aa%20method%20for%20compressing%20a%20large%20language%20model%20of%20code%20%28the%20teacher%29%20into%20a%0Asmaller%20model%20%28the%20student%29%20while%20maintaining%20performance.%20However%2C%20the%20degree%0Ato%20which%20a%20student%20model%20deeply%20mimics%20the%20predictive%20behavior%20and%20internal%0Arepresentations%20of%20its%20teacher%20remains%20largely%20unexplored%2C%20as%20current%0Aaccuracy-based%20evaluation%20provides%20only%20a%20surface-level%20view%20of%20model%20quality%0Aand%20often%20fails%20to%20capture%20more%20profound%20discrepancies%20in%20behavioral%20fidelity%0Abetween%20the%20teacher%20and%20student%20models.%20To%20address%20this%20gap%2C%20we%20empirically%0Ashow%20that%20the%20student%20model%20often%20fails%20to%20deeply%20mimic%20the%20teacher%20model%2C%0Aresulting%20in%20up%20to%20285%25%20greater%20performance%20drop%20under%20adversarial%20attacks%2C%0Awhich%20is%20not%20captured%20by%20traditional%20accuracy-based%20evaluation.%20Therefore%2C%20we%0Apropose%20MetaCompress%2C%20a%20metamorphic%20testing%20framework%20that%20systematically%0Aevaluates%20behavioral%20fidelity%20by%20comparing%20the%20outputs%20of%20teacher%20and%20student%0Amodels%20under%20a%20set%20of%20behavior-preserving%20metamorphic%20relations.%20We%20evaluate%0AMetaCompress%20on%20two%20widely%20studied%20tasks%2C%20using%20compressed%20versions%20of%20popular%0Alanguage%20models%20of%20code%2C%20obtained%20via%20three%20different%20knowledge%20distillation%0Atechniques%3A%20Compressor%2C%20AVATAR%2C%20and%20MORPH.%20The%20results%20show%20that%20MetaCompress%0Aidentifies%20up%20to%2062%25%20behavioral%20discrepancies%20in%20student%20models%2C%20underscoring%0Athe%20need%20for%20behavioral%20fidelity%20evaluation%20within%20the%20knowledge%20distillation%0Apipeline%20and%20establishing%20MetaCompress%20as%20a%20practical%20framework%20for%20testing%0Acompressed%20language%20models%20of%20code%20derived%20through%20knowledge%20distillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05476v1&entry.124074799=Read"},
{"title": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter\n  Parameters for Continual Image Restoration", "author": "Aupendu Kar and Krishnendu Ghosh and Prabir Kumar Biswas", "abstract": "  Continual learning is an emerging topic in the field of deep learning, where\na model is expected to learn continuously for new upcoming tasks without\nforgetting previous experiences. This field has witnessed numerous\nadvancements, but few works have been attempted in the direction of image\nrestoration. Handling large image sizes and the divergent nature of various\ndegradation poses a unique challenge in the restoration domain. However,\nexisting works require heavily engineered architectural modifications for new\ntask adaptation, resulting in significant computational overhead.\nRegularization-based methods are unsuitable for restoration, as different\nrestoration challenges require different kinds of feature processing. In this\ndirection, we propose a simple modification of the convolution layer to adapt\nthe knowledge from previous restoration tasks without touching the main\nbackbone architecture. Therefore, it can be seamlessly applied to any deep\narchitecture without any structural modifications. Unlike other approaches, we\ndemonstrate that our model can increase the number of trainable parameters\nwithout significantly increasing computational overhead or inference time.\nExperimental validation demonstrates that new restoration tasks can be\nintroduced without compromising the performance of existing tasks. We also show\nthat performance on new restoration tasks improves by adapting the knowledge\nfrom the knowledge base created by previous restoration tasks. The code is\navailable at https://github.com/aupendu/continual-restore.\n", "link": "http://arxiv.org/abs/2511.05421v1", "date": "2025-11-07", "relevancy": 2.1199, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5606}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5308}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20the%20Learned%20Knowledge-base%20to%20Estimate%20Convolutional%20Filter%0A%20%20Parameters%20for%20Continual%20Image%20Restoration&body=Title%3A%20Sharing%20the%20Learned%20Knowledge-base%20to%20Estimate%20Convolutional%20Filter%0A%20%20Parameters%20for%20Continual%20Image%20Restoration%0AAuthor%3A%20Aupendu%20Kar%20and%20Krishnendu%20Ghosh%20and%20Prabir%20Kumar%20Biswas%0AAbstract%3A%20%20%20Continual%20learning%20is%20an%20emerging%20topic%20in%20the%20field%20of%20deep%20learning%2C%20where%0Aa%20model%20is%20expected%20to%20learn%20continuously%20for%20new%20upcoming%20tasks%20without%0Aforgetting%20previous%20experiences.%20This%20field%20has%20witnessed%20numerous%0Aadvancements%2C%20but%20few%20works%20have%20been%20attempted%20in%20the%20direction%20of%20image%0Arestoration.%20Handling%20large%20image%20sizes%20and%20the%20divergent%20nature%20of%20various%0Adegradation%20poses%20a%20unique%20challenge%20in%20the%20restoration%20domain.%20However%2C%0Aexisting%20works%20require%20heavily%20engineered%20architectural%20modifications%20for%20new%0Atask%20adaptation%2C%20resulting%20in%20significant%20computational%20overhead.%0ARegularization-based%20methods%20are%20unsuitable%20for%20restoration%2C%20as%20different%0Arestoration%20challenges%20require%20different%20kinds%20of%20feature%20processing.%20In%20this%0Adirection%2C%20we%20propose%20a%20simple%20modification%20of%20the%20convolution%20layer%20to%20adapt%0Athe%20knowledge%20from%20previous%20restoration%20tasks%20without%20touching%20the%20main%0Abackbone%20architecture.%20Therefore%2C%20it%20can%20be%20seamlessly%20applied%20to%20any%20deep%0Aarchitecture%20without%20any%20structural%20modifications.%20Unlike%20other%20approaches%2C%20we%0Ademonstrate%20that%20our%20model%20can%20increase%20the%20number%20of%20trainable%20parameters%0Awithout%20significantly%20increasing%20computational%20overhead%20or%20inference%20time.%0AExperimental%20validation%20demonstrates%20that%20new%20restoration%20tasks%20can%20be%0Aintroduced%20without%20compromising%20the%20performance%20of%20existing%20tasks.%20We%20also%20show%0Athat%20performance%20on%20new%20restoration%20tasks%20improves%20by%20adapting%20the%20knowledge%0Afrom%20the%20knowledge%20base%20created%20by%20previous%20restoration%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/aupendu/continual-restore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520the%2520Learned%2520Knowledge-base%2520to%2520Estimate%2520Convolutional%2520Filter%250A%2520%2520Parameters%2520for%2520Continual%2520Image%2520Restoration%26entry.906535625%3DAupendu%2520Kar%2520and%2520Krishnendu%2520Ghosh%2520and%2520Prabir%2520Kumar%2520Biswas%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520an%2520emerging%2520topic%2520in%2520the%2520field%2520of%2520deep%2520learning%252C%2520where%250Aa%2520model%2520is%2520expected%2520to%2520learn%2520continuously%2520for%2520new%2520upcoming%2520tasks%2520without%250Aforgetting%2520previous%2520experiences.%2520This%2520field%2520has%2520witnessed%2520numerous%250Aadvancements%252C%2520but%2520few%2520works%2520have%2520been%2520attempted%2520in%2520the%2520direction%2520of%2520image%250Arestoration.%2520Handling%2520large%2520image%2520sizes%2520and%2520the%2520divergent%2520nature%2520of%2520various%250Adegradation%2520poses%2520a%2520unique%2520challenge%2520in%2520the%2520restoration%2520domain.%2520However%252C%250Aexisting%2520works%2520require%2520heavily%2520engineered%2520architectural%2520modifications%2520for%2520new%250Atask%2520adaptation%252C%2520resulting%2520in%2520significant%2520computational%2520overhead.%250ARegularization-based%2520methods%2520are%2520unsuitable%2520for%2520restoration%252C%2520as%2520different%250Arestoration%2520challenges%2520require%2520different%2520kinds%2520of%2520feature%2520processing.%2520In%2520this%250Adirection%252C%2520we%2520propose%2520a%2520simple%2520modification%2520of%2520the%2520convolution%2520layer%2520to%2520adapt%250Athe%2520knowledge%2520from%2520previous%2520restoration%2520tasks%2520without%2520touching%2520the%2520main%250Abackbone%2520architecture.%2520Therefore%252C%2520it%2520can%2520be%2520seamlessly%2520applied%2520to%2520any%2520deep%250Aarchitecture%2520without%2520any%2520structural%2520modifications.%2520Unlike%2520other%2520approaches%252C%2520we%250Ademonstrate%2520that%2520our%2520model%2520can%2520increase%2520the%2520number%2520of%2520trainable%2520parameters%250Awithout%2520significantly%2520increasing%2520computational%2520overhead%2520or%2520inference%2520time.%250AExperimental%2520validation%2520demonstrates%2520that%2520new%2520restoration%2520tasks%2520can%2520be%250Aintroduced%2520without%2520compromising%2520the%2520performance%2520of%2520existing%2520tasks.%2520We%2520also%2520show%250Athat%2520performance%2520on%2520new%2520restoration%2520tasks%2520improves%2520by%2520adapting%2520the%2520knowledge%250Afrom%2520the%2520knowledge%2520base%2520created%2520by%2520previous%2520restoration%2520tasks.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/aupendu/continual-restore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20the%20Learned%20Knowledge-base%20to%20Estimate%20Convolutional%20Filter%0A%20%20Parameters%20for%20Continual%20Image%20Restoration&entry.906535625=Aupendu%20Kar%20and%20Krishnendu%20Ghosh%20and%20Prabir%20Kumar%20Biswas&entry.1292438233=%20%20Continual%20learning%20is%20an%20emerging%20topic%20in%20the%20field%20of%20deep%20learning%2C%20where%0Aa%20model%20is%20expected%20to%20learn%20continuously%20for%20new%20upcoming%20tasks%20without%0Aforgetting%20previous%20experiences.%20This%20field%20has%20witnessed%20numerous%0Aadvancements%2C%20but%20few%20works%20have%20been%20attempted%20in%20the%20direction%20of%20image%0Arestoration.%20Handling%20large%20image%20sizes%20and%20the%20divergent%20nature%20of%20various%0Adegradation%20poses%20a%20unique%20challenge%20in%20the%20restoration%20domain.%20However%2C%0Aexisting%20works%20require%20heavily%20engineered%20architectural%20modifications%20for%20new%0Atask%20adaptation%2C%20resulting%20in%20significant%20computational%20overhead.%0ARegularization-based%20methods%20are%20unsuitable%20for%20restoration%2C%20as%20different%0Arestoration%20challenges%20require%20different%20kinds%20of%20feature%20processing.%20In%20this%0Adirection%2C%20we%20propose%20a%20simple%20modification%20of%20the%20convolution%20layer%20to%20adapt%0Athe%20knowledge%20from%20previous%20restoration%20tasks%20without%20touching%20the%20main%0Abackbone%20architecture.%20Therefore%2C%20it%20can%20be%20seamlessly%20applied%20to%20any%20deep%0Aarchitecture%20without%20any%20structural%20modifications.%20Unlike%20other%20approaches%2C%20we%0Ademonstrate%20that%20our%20model%20can%20increase%20the%20number%20of%20trainable%20parameters%0Awithout%20significantly%20increasing%20computational%20overhead%20or%20inference%20time.%0AExperimental%20validation%20demonstrates%20that%20new%20restoration%20tasks%20can%20be%0Aintroduced%20without%20compromising%20the%20performance%20of%20existing%20tasks.%20We%20also%20show%0Athat%20performance%20on%20new%20restoration%20tasks%20improves%20by%20adapting%20the%20knowledge%0Afrom%20the%20knowledge%20base%20created%20by%20previous%20restoration%20tasks.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/aupendu/continual-restore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05421v1&entry.124074799=Read"},
{"title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models", "author": "Jingxuan Xu and Ken Deng and Weihao Li and Songwei Yu and Huaixi Tang and Haoyang Huang and Zhiyi Lai and Zizheng Zhan and Yanan Wu and Chenchen Zhang and Kepeng Lei and Yifan Yao and Xinping Lei and Wenqiang Zhu and Zongxian Feng and Han Li and Junqi Xiong and Dailin Li and Zuchen Gao and Kun Wu and Wen Xiang and Ziqi Zhan and Yuanxing Zhang and Wuxuan Gong and Ziyuan Gao and Guanxiang Wang and Yirong Xue and Xiaojiang Zhang and Jinghui Wang and Huiming Wang and Wenhao Zhuang and Zhaoxiang Zhang and Yuqun Zhang and Haotian Zhang and Bin Chen and Jiaheng Liu", "abstract": "  Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.\n", "link": "http://arxiv.org/abs/2511.05459v1", "date": "2025-11-07", "relevancy": 2.0692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWE-Compass%3A%20Towards%20Unified%20Evaluation%20of%20Agentic%20Coding%20Abilities%20for%0A%20%20Large%20Language%20Models&body=Title%3A%20SWE-Compass%3A%20Towards%20Unified%20Evaluation%20of%20Agentic%20Coding%20Abilities%20for%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jingxuan%20Xu%20and%20Ken%20Deng%20and%20Weihao%20Li%20and%20Songwei%20Yu%20and%20Huaixi%20Tang%20and%20Haoyang%20Huang%20and%20Zhiyi%20Lai%20and%20Zizheng%20Zhan%20and%20Yanan%20Wu%20and%20Chenchen%20Zhang%20and%20Kepeng%20Lei%20and%20Yifan%20Yao%20and%20Xinping%20Lei%20and%20Wenqiang%20Zhu%20and%20Zongxian%20Feng%20and%20Han%20Li%20and%20Junqi%20Xiong%20and%20Dailin%20Li%20and%20Zuchen%20Gao%20and%20Kun%20Wu%20and%20Wen%20Xiang%20and%20Ziqi%20Zhan%20and%20Yuanxing%20Zhang%20and%20Wuxuan%20Gong%20and%20Ziyuan%20Gao%20and%20Guanxiang%20Wang%20and%20Yirong%20Xue%20and%20Xiaojiang%20Zhang%20and%20Jinghui%20Wang%20and%20Huiming%20Wang%20and%20Wenhao%20Zhuang%20and%20Zhaoxiang%20Zhang%20and%20Yuqun%20Zhang%20and%20Haotian%20Zhang%20and%20Bin%20Chen%20and%20Jiaheng%20Liu%0AAbstract%3A%20%20%20Evaluating%20large%20language%20models%20%28LLMs%29%20for%20software%20engineering%20has%20been%0Alimited%20by%20narrow%20task%20coverage%2C%20language%20bias%2C%20and%20insufficient%20alignment%20with%0Areal-world%20developer%20workflows.%20Existing%20benchmarks%20often%20focus%20on%20algorithmic%0Aproblems%20or%20Python-centric%20bug%20fixing%2C%20leaving%20critical%20dimensions%20of%20software%0Aengineering%20underexplored.%20To%20address%20these%20gaps%2C%20we%20introduce%20SWE-Compass1%2C%20a%0Acomprehensive%20benchmark%20that%20unifies%20heterogeneous%20code-related%20evaluations%0Ainto%20a%20structured%20and%20production-aligned%20framework.%20SWE-Compass%20spans%208%20task%0Atypes%2C%208%20programming%20scenarios%2C%20and%2010%20programming%20languages%2C%20with%202000%0Ahigh-quality%20instances%20curated%20from%20authentic%20GitHub%20pull%20requests%20and%20refined%0Athrough%20systematic%20filtering%20and%20validation.%20We%20benchmark%20ten%20state-of-the-art%0ALLMs%20under%20two%20agentic%20frameworks%2C%20SWE-Agent%20and%20Claude%20Code%2C%20revealing%20a%20clear%0Ahierarchy%20of%20difficulty%20across%20task%20types%2C%20languages%2C%20and%20scenarios.%20Moreover%2C%0Aby%20aligning%20evaluation%20with%20real-world%20developer%20practices%2C%20SWE-Compass%0Aprovides%20a%20rigorous%20and%20reproducible%20foundation%20for%20diagnosing%20and%20advancing%0Aagentic%20coding%20capabilities%20in%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWE-Compass%253A%2520Towards%2520Unified%2520Evaluation%2520of%2520Agentic%2520Coding%2520Abilities%2520for%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJingxuan%2520Xu%2520and%2520Ken%2520Deng%2520and%2520Weihao%2520Li%2520and%2520Songwei%2520Yu%2520and%2520Huaixi%2520Tang%2520and%2520Haoyang%2520Huang%2520and%2520Zhiyi%2520Lai%2520and%2520Zizheng%2520Zhan%2520and%2520Yanan%2520Wu%2520and%2520Chenchen%2520Zhang%2520and%2520Kepeng%2520Lei%2520and%2520Yifan%2520Yao%2520and%2520Xinping%2520Lei%2520and%2520Wenqiang%2520Zhu%2520and%2520Zongxian%2520Feng%2520and%2520Han%2520Li%2520and%2520Junqi%2520Xiong%2520and%2520Dailin%2520Li%2520and%2520Zuchen%2520Gao%2520and%2520Kun%2520Wu%2520and%2520Wen%2520Xiang%2520and%2520Ziqi%2520Zhan%2520and%2520Yuanxing%2520Zhang%2520and%2520Wuxuan%2520Gong%2520and%2520Ziyuan%2520Gao%2520and%2520Guanxiang%2520Wang%2520and%2520Yirong%2520Xue%2520and%2520Xiaojiang%2520Zhang%2520and%2520Jinghui%2520Wang%2520and%2520Huiming%2520Wang%2520and%2520Wenhao%2520Zhuang%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Yuqun%2520Zhang%2520and%2520Haotian%2520Zhang%2520and%2520Bin%2520Chen%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3D%2520%2520Evaluating%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520software%2520engineering%2520has%2520been%250Alimited%2520by%2520narrow%2520task%2520coverage%252C%2520language%2520bias%252C%2520and%2520insufficient%2520alignment%2520with%250Areal-world%2520developer%2520workflows.%2520Existing%2520benchmarks%2520often%2520focus%2520on%2520algorithmic%250Aproblems%2520or%2520Python-centric%2520bug%2520fixing%252C%2520leaving%2520critical%2520dimensions%2520of%2520software%250Aengineering%2520underexplored.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520SWE-Compass1%252C%2520a%250Acomprehensive%2520benchmark%2520that%2520unifies%2520heterogeneous%2520code-related%2520evaluations%250Ainto%2520a%2520structured%2520and%2520production-aligned%2520framework.%2520SWE-Compass%2520spans%25208%2520task%250Atypes%252C%25208%2520programming%2520scenarios%252C%2520and%252010%2520programming%2520languages%252C%2520with%25202000%250Ahigh-quality%2520instances%2520curated%2520from%2520authentic%2520GitHub%2520pull%2520requests%2520and%2520refined%250Athrough%2520systematic%2520filtering%2520and%2520validation.%2520We%2520benchmark%2520ten%2520state-of-the-art%250ALLMs%2520under%2520two%2520agentic%2520frameworks%252C%2520SWE-Agent%2520and%2520Claude%2520Code%252C%2520revealing%2520a%2520clear%250Ahierarchy%2520of%2520difficulty%2520across%2520task%2520types%252C%2520languages%252C%2520and%2520scenarios.%2520Moreover%252C%250Aby%2520aligning%2520evaluation%2520with%2520real-world%2520developer%2520practices%252C%2520SWE-Compass%250Aprovides%2520a%2520rigorous%2520and%2520reproducible%2520foundation%2520for%2520diagnosing%2520and%2520advancing%250Aagentic%2520coding%2520capabilities%2520in%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWE-Compass%3A%20Towards%20Unified%20Evaluation%20of%20Agentic%20Coding%20Abilities%20for%0A%20%20Large%20Language%20Models&entry.906535625=Jingxuan%20Xu%20and%20Ken%20Deng%20and%20Weihao%20Li%20and%20Songwei%20Yu%20and%20Huaixi%20Tang%20and%20Haoyang%20Huang%20and%20Zhiyi%20Lai%20and%20Zizheng%20Zhan%20and%20Yanan%20Wu%20and%20Chenchen%20Zhang%20and%20Kepeng%20Lei%20and%20Yifan%20Yao%20and%20Xinping%20Lei%20and%20Wenqiang%20Zhu%20and%20Zongxian%20Feng%20and%20Han%20Li%20and%20Junqi%20Xiong%20and%20Dailin%20Li%20and%20Zuchen%20Gao%20and%20Kun%20Wu%20and%20Wen%20Xiang%20and%20Ziqi%20Zhan%20and%20Yuanxing%20Zhang%20and%20Wuxuan%20Gong%20and%20Ziyuan%20Gao%20and%20Guanxiang%20Wang%20and%20Yirong%20Xue%20and%20Xiaojiang%20Zhang%20and%20Jinghui%20Wang%20and%20Huiming%20Wang%20and%20Wenhao%20Zhuang%20and%20Zhaoxiang%20Zhang%20and%20Yuqun%20Zhang%20and%20Haotian%20Zhang%20and%20Bin%20Chen%20and%20Jiaheng%20Liu&entry.1292438233=%20%20Evaluating%20large%20language%20models%20%28LLMs%29%20for%20software%20engineering%20has%20been%0Alimited%20by%20narrow%20task%20coverage%2C%20language%20bias%2C%20and%20insufficient%20alignment%20with%0Areal-world%20developer%20workflows.%20Existing%20benchmarks%20often%20focus%20on%20algorithmic%0Aproblems%20or%20Python-centric%20bug%20fixing%2C%20leaving%20critical%20dimensions%20of%20software%0Aengineering%20underexplored.%20To%20address%20these%20gaps%2C%20we%20introduce%20SWE-Compass1%2C%20a%0Acomprehensive%20benchmark%20that%20unifies%20heterogeneous%20code-related%20evaluations%0Ainto%20a%20structured%20and%20production-aligned%20framework.%20SWE-Compass%20spans%208%20task%0Atypes%2C%208%20programming%20scenarios%2C%20and%2010%20programming%20languages%2C%20with%202000%0Ahigh-quality%20instances%20curated%20from%20authentic%20GitHub%20pull%20requests%20and%20refined%0Athrough%20systematic%20filtering%20and%20validation.%20We%20benchmark%20ten%20state-of-the-art%0ALLMs%20under%20two%20agentic%20frameworks%2C%20SWE-Agent%20and%20Claude%20Code%2C%20revealing%20a%20clear%0Ahierarchy%20of%20difficulty%20across%20task%20types%2C%20languages%2C%20and%20scenarios.%20Moreover%2C%0Aby%20aligning%20evaluation%20with%20real-world%20developer%20practices%2C%20SWE-Compass%0Aprovides%20a%20rigorous%20and%20reproducible%20foundation%20for%20diagnosing%20and%20advancing%0Aagentic%20coding%20capabilities%20in%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05459v1&entry.124074799=Read"},
{"title": "Self-adaptive weighting and sampling for physics-informed neural\n  networks", "author": "Wenqian Chen and Amanda Howard and Panos Stinis", "abstract": "  Physics-informed deep learning has emerged as a promising framework for\nsolving partial differential equations (PDEs). Nevertheless, training these\nmodels on complex problems remains challenging, often leading to limited\naccuracy and efficiency. In this work, we introduce a hybrid adaptive sampling\nand weighting method to enhance the performance of physics-informed neural\nnetworks (PINNs). The adaptive sampling component identifies training points in\nregions where the solution exhibits rapid variation, while the adaptive\nweighting component balances the convergence rate across training points.\nNumerical experiments show that applying only adaptive sampling or only\nadaptive weighting is insufficient to consistently achieve accurate\npredictions, particularly when training points are scarce. Since each method\nemphasizes different aspects of the solution, their effectiveness is problem\ndependent. By combining both strategies, the proposed framework consistently\nimproves prediction accuracy and training efficiency, offering a more robust\napproach for solving PDEs with PINNs.\n", "link": "http://arxiv.org/abs/2511.05452v1", "date": "2025-11-07", "relevancy": 2.0519, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-adaptive%20weighting%20and%20sampling%20for%20physics-informed%20neural%0A%20%20networks&body=Title%3A%20Self-adaptive%20weighting%20and%20sampling%20for%20physics-informed%20neural%0A%20%20networks%0AAuthor%3A%20Wenqian%20Chen%20and%20Amanda%20Howard%20and%20Panos%20Stinis%0AAbstract%3A%20%20%20Physics-informed%20deep%20learning%20has%20emerged%20as%20a%20promising%20framework%20for%0Asolving%20partial%20differential%20equations%20%28PDEs%29.%20Nevertheless%2C%20training%20these%0Amodels%20on%20complex%20problems%20remains%20challenging%2C%20often%20leading%20to%20limited%0Aaccuracy%20and%20efficiency.%20In%20this%20work%2C%20we%20introduce%20a%20hybrid%20adaptive%20sampling%0Aand%20weighting%20method%20to%20enhance%20the%20performance%20of%20physics-informed%20neural%0Anetworks%20%28PINNs%29.%20The%20adaptive%20sampling%20component%20identifies%20training%20points%20in%0Aregions%20where%20the%20solution%20exhibits%20rapid%20variation%2C%20while%20the%20adaptive%0Aweighting%20component%20balances%20the%20convergence%20rate%20across%20training%20points.%0ANumerical%20experiments%20show%20that%20applying%20only%20adaptive%20sampling%20or%20only%0Aadaptive%20weighting%20is%20insufficient%20to%20consistently%20achieve%20accurate%0Apredictions%2C%20particularly%20when%20training%20points%20are%20scarce.%20Since%20each%20method%0Aemphasizes%20different%20aspects%20of%20the%20solution%2C%20their%20effectiveness%20is%20problem%0Adependent.%20By%20combining%20both%20strategies%2C%20the%20proposed%20framework%20consistently%0Aimproves%20prediction%20accuracy%20and%20training%20efficiency%2C%20offering%20a%20more%20robust%0Aapproach%20for%20solving%20PDEs%20with%20PINNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-adaptive%2520weighting%2520and%2520sampling%2520for%2520physics-informed%2520neural%250A%2520%2520networks%26entry.906535625%3DWenqian%2520Chen%2520and%2520Amanda%2520Howard%2520and%2520Panos%2520Stinis%26entry.1292438233%3D%2520%2520Physics-informed%2520deep%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520for%250Asolving%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520Nevertheless%252C%2520training%2520these%250Amodels%2520on%2520complex%2520problems%2520remains%2520challenging%252C%2520often%2520leading%2520to%2520limited%250Aaccuracy%2520and%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520hybrid%2520adaptive%2520sampling%250Aand%2520weighting%2520method%2520to%2520enhance%2520the%2520performance%2520of%2520physics-informed%2520neural%250Anetworks%2520%2528PINNs%2529.%2520The%2520adaptive%2520sampling%2520component%2520identifies%2520training%2520points%2520in%250Aregions%2520where%2520the%2520solution%2520exhibits%2520rapid%2520variation%252C%2520while%2520the%2520adaptive%250Aweighting%2520component%2520balances%2520the%2520convergence%2520rate%2520across%2520training%2520points.%250ANumerical%2520experiments%2520show%2520that%2520applying%2520only%2520adaptive%2520sampling%2520or%2520only%250Aadaptive%2520weighting%2520is%2520insufficient%2520to%2520consistently%2520achieve%2520accurate%250Apredictions%252C%2520particularly%2520when%2520training%2520points%2520are%2520scarce.%2520Since%2520each%2520method%250Aemphasizes%2520different%2520aspects%2520of%2520the%2520solution%252C%2520their%2520effectiveness%2520is%2520problem%250Adependent.%2520By%2520combining%2520both%2520strategies%252C%2520the%2520proposed%2520framework%2520consistently%250Aimproves%2520prediction%2520accuracy%2520and%2520training%2520efficiency%252C%2520offering%2520a%2520more%2520robust%250Aapproach%2520for%2520solving%2520PDEs%2520with%2520PINNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-adaptive%20weighting%20and%20sampling%20for%20physics-informed%20neural%0A%20%20networks&entry.906535625=Wenqian%20Chen%20and%20Amanda%20Howard%20and%20Panos%20Stinis&entry.1292438233=%20%20Physics-informed%20deep%20learning%20has%20emerged%20as%20a%20promising%20framework%20for%0Asolving%20partial%20differential%20equations%20%28PDEs%29.%20Nevertheless%2C%20training%20these%0Amodels%20on%20complex%20problems%20remains%20challenging%2C%20often%20leading%20to%20limited%0Aaccuracy%20and%20efficiency.%20In%20this%20work%2C%20we%20introduce%20a%20hybrid%20adaptive%20sampling%0Aand%20weighting%20method%20to%20enhance%20the%20performance%20of%20physics-informed%20neural%0Anetworks%20%28PINNs%29.%20The%20adaptive%20sampling%20component%20identifies%20training%20points%20in%0Aregions%20where%20the%20solution%20exhibits%20rapid%20variation%2C%20while%20the%20adaptive%0Aweighting%20component%20balances%20the%20convergence%20rate%20across%20training%20points.%0ANumerical%20experiments%20show%20that%20applying%20only%20adaptive%20sampling%20or%20only%0Aadaptive%20weighting%20is%20insufficient%20to%20consistently%20achieve%20accurate%0Apredictions%2C%20particularly%20when%20training%20points%20are%20scarce.%20Since%20each%20method%0Aemphasizes%20different%20aspects%20of%20the%20solution%2C%20their%20effectiveness%20is%20problem%0Adependent.%20By%20combining%20both%20strategies%2C%20the%20proposed%20framework%20consistently%0Aimproves%20prediction%20accuracy%20and%20training%20efficiency%2C%20offering%20a%20more%20robust%0Aapproach%20for%20solving%20PDEs%20with%20PINNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05452v1&entry.124074799=Read"},
{"title": "GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling\n  for Efficient Medical Image Segmentation", "author": "Guojie Li and Anwar P. P. Abdul Majeed and Muhammad Ateeq and Anh Nguyen and Fan Zhang", "abstract": "  Medical image segmentation requires models that are accurate, lightweight,\nand interpretable. Convolutional architectures lack adaptive nonlinearity and\ntransparent decision-making, whereas Transformer architectures are hindered by\nquadratic complexity and opaque attention mechanisms. U-KAN addresses these\nchallenges using Kolmogorov-Arnold Networks, achieving higher accuracy than\nboth convolutional and attention-based methods, fewer parameters than\nTransformer variants, and improved interpretability compared to conventional\napproaches. However, its O(C^2) complexity due to full-channel transformations\nlimits its scalability as the number of channels increases. To overcome this,\nwe introduce GroupKAN, a lightweight segmentation network that incorporates two\nnovel, structured functional modules: (1) Grouped KAN Transform, which\npartitions channels into G groups for multivariate spline mappings, reducing\ncomplexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared\nspline-based mappings within each channel group for efficient, token-wise\nnonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC),\nGroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11\npercent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M),\nand shows improved interpretability.\n", "link": "http://arxiv.org/abs/2511.05477v1", "date": "2025-11-07", "relevancy": 2.0282, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5263}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroupKAN%3A%20Rethinking%20Nonlinearity%20with%20Grouped%20Spline-based%20KAN%20Modeling%0A%20%20for%20Efficient%20Medical%20Image%20Segmentation&body=Title%3A%20GroupKAN%3A%20Rethinking%20Nonlinearity%20with%20Grouped%20Spline-based%20KAN%20Modeling%0A%20%20for%20Efficient%20Medical%20Image%20Segmentation%0AAuthor%3A%20Guojie%20Li%20and%20Anwar%20P.%20P.%20Abdul%20Majeed%20and%20Muhammad%20Ateeq%20and%20Anh%20Nguyen%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20requires%20models%20that%20are%20accurate%2C%20lightweight%2C%0Aand%20interpretable.%20Convolutional%20architectures%20lack%20adaptive%20nonlinearity%20and%0Atransparent%20decision-making%2C%20whereas%20Transformer%20architectures%20are%20hindered%20by%0Aquadratic%20complexity%20and%20opaque%20attention%20mechanisms.%20U-KAN%20addresses%20these%0Achallenges%20using%20Kolmogorov-Arnold%20Networks%2C%20achieving%20higher%20accuracy%20than%0Aboth%20convolutional%20and%20attention-based%20methods%2C%20fewer%20parameters%20than%0ATransformer%20variants%2C%20and%20improved%20interpretability%20compared%20to%20conventional%0Aapproaches.%20However%2C%20its%20O%28C%5E2%29%20complexity%20due%20to%20full-channel%20transformations%0Alimits%20its%20scalability%20as%20the%20number%20of%20channels%20increases.%20To%20overcome%20this%2C%0Awe%20introduce%20GroupKAN%2C%20a%20lightweight%20segmentation%20network%20that%20incorporates%20two%0Anovel%2C%20structured%20functional%20modules%3A%20%281%29%20Grouped%20KAN%20Transform%2C%20which%0Apartitions%20channels%20into%20G%20groups%20for%20multivariate%20spline%20mappings%2C%20reducing%0Acomplexity%20to%20O%28C%5E2/G%29%2C%20and%20%282%29%20Grouped%20KAN%20Activation%2C%20which%20applies%20shared%0Aspline-based%20mappings%20within%20each%20channel%20group%20for%20efficient%2C%20token-wise%0Anonlinearity.%20Evaluated%20on%20three%20medical%20benchmarks%20%28BUSI%2C%20GlaS%2C%20and%20CVC%29%2C%0AGroupKAN%20achieves%20an%20average%20IoU%20of%2079.80%20percent%2C%20surpassing%20U-KAN%20by%20%2B1.11%0Apercent%20while%20requiring%20only%2047.6%20percent%20of%20the%20parameters%20%283.02M%20vs%206.35M%29%2C%0Aand%20shows%20improved%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroupKAN%253A%2520Rethinking%2520Nonlinearity%2520with%2520Grouped%2520Spline-based%2520KAN%2520Modeling%250A%2520%2520for%2520Efficient%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DGuojie%2520Li%2520and%2520Anwar%2520P.%2520P.%2520Abdul%2520Majeed%2520and%2520Muhammad%2520Ateeq%2520and%2520Anh%2520Nguyen%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520requires%2520models%2520that%2520are%2520accurate%252C%2520lightweight%252C%250Aand%2520interpretable.%2520Convolutional%2520architectures%2520lack%2520adaptive%2520nonlinearity%2520and%250Atransparent%2520decision-making%252C%2520whereas%2520Transformer%2520architectures%2520are%2520hindered%2520by%250Aquadratic%2520complexity%2520and%2520opaque%2520attention%2520mechanisms.%2520U-KAN%2520addresses%2520these%250Achallenges%2520using%2520Kolmogorov-Arnold%2520Networks%252C%2520achieving%2520higher%2520accuracy%2520than%250Aboth%2520convolutional%2520and%2520attention-based%2520methods%252C%2520fewer%2520parameters%2520than%250ATransformer%2520variants%252C%2520and%2520improved%2520interpretability%2520compared%2520to%2520conventional%250Aapproaches.%2520However%252C%2520its%2520O%2528C%255E2%2529%2520complexity%2520due%2520to%2520full-channel%2520transformations%250Alimits%2520its%2520scalability%2520as%2520the%2520number%2520of%2520channels%2520increases.%2520To%2520overcome%2520this%252C%250Awe%2520introduce%2520GroupKAN%252C%2520a%2520lightweight%2520segmentation%2520network%2520that%2520incorporates%2520two%250Anovel%252C%2520structured%2520functional%2520modules%253A%2520%25281%2529%2520Grouped%2520KAN%2520Transform%252C%2520which%250Apartitions%2520channels%2520into%2520G%2520groups%2520for%2520multivariate%2520spline%2520mappings%252C%2520reducing%250Acomplexity%2520to%2520O%2528C%255E2/G%2529%252C%2520and%2520%25282%2529%2520Grouped%2520KAN%2520Activation%252C%2520which%2520applies%2520shared%250Aspline-based%2520mappings%2520within%2520each%2520channel%2520group%2520for%2520efficient%252C%2520token-wise%250Anonlinearity.%2520Evaluated%2520on%2520three%2520medical%2520benchmarks%2520%2528BUSI%252C%2520GlaS%252C%2520and%2520CVC%2529%252C%250AGroupKAN%2520achieves%2520an%2520average%2520IoU%2520of%252079.80%2520percent%252C%2520surpassing%2520U-KAN%2520by%2520%252B1.11%250Apercent%2520while%2520requiring%2520only%252047.6%2520percent%2520of%2520the%2520parameters%2520%25283.02M%2520vs%25206.35M%2529%252C%250Aand%2520shows%2520improved%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroupKAN%3A%20Rethinking%20Nonlinearity%20with%20Grouped%20Spline-based%20KAN%20Modeling%0A%20%20for%20Efficient%20Medical%20Image%20Segmentation&entry.906535625=Guojie%20Li%20and%20Anwar%20P.%20P.%20Abdul%20Majeed%20and%20Muhammad%20Ateeq%20and%20Anh%20Nguyen%20and%20Fan%20Zhang&entry.1292438233=%20%20Medical%20image%20segmentation%20requires%20models%20that%20are%20accurate%2C%20lightweight%2C%0Aand%20interpretable.%20Convolutional%20architectures%20lack%20adaptive%20nonlinearity%20and%0Atransparent%20decision-making%2C%20whereas%20Transformer%20architectures%20are%20hindered%20by%0Aquadratic%20complexity%20and%20opaque%20attention%20mechanisms.%20U-KAN%20addresses%20these%0Achallenges%20using%20Kolmogorov-Arnold%20Networks%2C%20achieving%20higher%20accuracy%20than%0Aboth%20convolutional%20and%20attention-based%20methods%2C%20fewer%20parameters%20than%0ATransformer%20variants%2C%20and%20improved%20interpretability%20compared%20to%20conventional%0Aapproaches.%20However%2C%20its%20O%28C%5E2%29%20complexity%20due%20to%20full-channel%20transformations%0Alimits%20its%20scalability%20as%20the%20number%20of%20channels%20increases.%20To%20overcome%20this%2C%0Awe%20introduce%20GroupKAN%2C%20a%20lightweight%20segmentation%20network%20that%20incorporates%20two%0Anovel%2C%20structured%20functional%20modules%3A%20%281%29%20Grouped%20KAN%20Transform%2C%20which%0Apartitions%20channels%20into%20G%20groups%20for%20multivariate%20spline%20mappings%2C%20reducing%0Acomplexity%20to%20O%28C%5E2/G%29%2C%20and%20%282%29%20Grouped%20KAN%20Activation%2C%20which%20applies%20shared%0Aspline-based%20mappings%20within%20each%20channel%20group%20for%20efficient%2C%20token-wise%0Anonlinearity.%20Evaluated%20on%20three%20medical%20benchmarks%20%28BUSI%2C%20GlaS%2C%20and%20CVC%29%2C%0AGroupKAN%20achieves%20an%20average%20IoU%20of%2079.80%20percent%2C%20surpassing%20U-KAN%20by%20%2B1.11%0Apercent%20while%20requiring%20only%2047.6%20percent%20of%20the%20parameters%20%283.02M%20vs%206.35M%29%2C%0Aand%20shows%20improved%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05477v1&entry.124074799=Read"},
{"title": "Large language models as uncertainty-calibrated optimizers for\n  experimental discovery", "author": "Bojana Rankovi\u0107 and Ryan-Rhys Griffiths and Philippe Schwaller", "abstract": "  Scientific discovery increasingly depends on efficient experimental\noptimization to navigate vast design spaces under time and resource\nconstraints. Traditional approaches often require extensive domain expertise\nand feature engineering. While large language models, with their vast\nscientific knowledge, circumvent the feature engineering limitations, they lack\nthe calibrated uncertainty estimates required for high-stakes decision making.\nHence, current optimization methods force a choice between domain knowledge and\nreliability, with no principled approach that affords both. In this work, we\nshow that training language models through the uncertainty-aware objectives of\ntraditional optimization methods enables their use as reliable optimizers\nguided by natural language. By teaching LLMs from experimental outcomes under\nuncertainty, we transform their overconfidence from a fundamental limitation\ninto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a\ncornerstone of pharmaceutical synthesis, our method nearly doubles the\ndiscovery rate of high-yielding reaction conditions, from 24% to 43% in 50\nexperimental iterations starting from 10 unsuccessful conditions. Across 19\ndiverse optimization problems spanning organic synthesis, materials science and\ncatalysis, process chemistry, and molecular design, our approach ranks first on\naverage, establishing a new paradigm for reliable, uncertainty-guided\noptimization with LLMs. Our approach can accelerate discovery by lowering the\nbarrier to using powerful optimization methods, replacing the need for\ndomain-specific feature engineering with more accessible natural language\ninterfaces. These findings highlight that ensuring reliability through\nprincipled uncertainty quantification is critical for realizing the full\npotential of AI-guided experimentation.\n", "link": "http://arxiv.org/abs/2504.06265v3", "date": "2025-11-07", "relevancy": 2.0028, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4959}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20as%20uncertainty-calibrated%20optimizers%20for%0A%20%20experimental%20discovery&body=Title%3A%20Large%20language%20models%20as%20uncertainty-calibrated%20optimizers%20for%0A%20%20experimental%20discovery%0AAuthor%3A%20Bojana%20Rankovi%C4%87%20and%20Ryan-Rhys%20Griffiths%20and%20Philippe%20Schwaller%0AAbstract%3A%20%20%20Scientific%20discovery%20increasingly%20depends%20on%20efficient%20experimental%0Aoptimization%20to%20navigate%20vast%20design%20spaces%20under%20time%20and%20resource%0Aconstraints.%20Traditional%20approaches%20often%20require%20extensive%20domain%20expertise%0Aand%20feature%20engineering.%20While%20large%20language%20models%2C%20with%20their%20vast%0Ascientific%20knowledge%2C%20circumvent%20the%20feature%20engineering%20limitations%2C%20they%20lack%0Athe%20calibrated%20uncertainty%20estimates%20required%20for%20high-stakes%20decision%20making.%0AHence%2C%20current%20optimization%20methods%20force%20a%20choice%20between%20domain%20knowledge%20and%0Areliability%2C%20with%20no%20principled%20approach%20that%20affords%20both.%20In%20this%20work%2C%20we%0Ashow%20that%20training%20language%20models%20through%20the%20uncertainty-aware%20objectives%20of%0Atraditional%20optimization%20methods%20enables%20their%20use%20as%20reliable%20optimizers%0Aguided%20by%20natural%20language.%20By%20teaching%20LLMs%20from%20experimental%20outcomes%20under%0Auncertainty%2C%20we%20transform%20their%20overconfidence%20from%20a%20fundamental%20limitation%0Ainto%20a%20precise%20calibration%20mechanism.%20Applied%20to%20Buchwald-Hartwig%20reactions%2C%20a%0Acornerstone%20of%20pharmaceutical%20synthesis%2C%20our%20method%20nearly%20doubles%20the%0Adiscovery%20rate%20of%20high-yielding%20reaction%20conditions%2C%20from%2024%25%20to%2043%25%20in%2050%0Aexperimental%20iterations%20starting%20from%2010%20unsuccessful%20conditions.%20Across%2019%0Adiverse%20optimization%20problems%20spanning%20organic%20synthesis%2C%20materials%20science%20and%0Acatalysis%2C%20process%20chemistry%2C%20and%20molecular%20design%2C%20our%20approach%20ranks%20first%20on%0Aaverage%2C%20establishing%20a%20new%20paradigm%20for%20reliable%2C%20uncertainty-guided%0Aoptimization%20with%20LLMs.%20Our%20approach%20can%20accelerate%20discovery%20by%20lowering%20the%0Abarrier%20to%20using%20powerful%20optimization%20methods%2C%20replacing%20the%20need%20for%0Adomain-specific%20feature%20engineering%20with%20more%20accessible%20natural%20language%0Ainterfaces.%20These%20findings%20highlight%20that%20ensuring%20reliability%20through%0Aprincipled%20uncertainty%20quantification%20is%20critical%20for%20realizing%20the%20full%0Apotential%20of%20AI-guided%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06265v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520as%2520uncertainty-calibrated%2520optimizers%2520for%250A%2520%2520experimental%2520discovery%26entry.906535625%3DBojana%2520Rankovi%25C4%2587%2520and%2520Ryan-Rhys%2520Griffiths%2520and%2520Philippe%2520Schwaller%26entry.1292438233%3D%2520%2520Scientific%2520discovery%2520increasingly%2520depends%2520on%2520efficient%2520experimental%250Aoptimization%2520to%2520navigate%2520vast%2520design%2520spaces%2520under%2520time%2520and%2520resource%250Aconstraints.%2520Traditional%2520approaches%2520often%2520require%2520extensive%2520domain%2520expertise%250Aand%2520feature%2520engineering.%2520While%2520large%2520language%2520models%252C%2520with%2520their%2520vast%250Ascientific%2520knowledge%252C%2520circumvent%2520the%2520feature%2520engineering%2520limitations%252C%2520they%2520lack%250Athe%2520calibrated%2520uncertainty%2520estimates%2520required%2520for%2520high-stakes%2520decision%2520making.%250AHence%252C%2520current%2520optimization%2520methods%2520force%2520a%2520choice%2520between%2520domain%2520knowledge%2520and%250Areliability%252C%2520with%2520no%2520principled%2520approach%2520that%2520affords%2520both.%2520In%2520this%2520work%252C%2520we%250Ashow%2520that%2520training%2520language%2520models%2520through%2520the%2520uncertainty-aware%2520objectives%2520of%250Atraditional%2520optimization%2520methods%2520enables%2520their%2520use%2520as%2520reliable%2520optimizers%250Aguided%2520by%2520natural%2520language.%2520By%2520teaching%2520LLMs%2520from%2520experimental%2520outcomes%2520under%250Auncertainty%252C%2520we%2520transform%2520their%2520overconfidence%2520from%2520a%2520fundamental%2520limitation%250Ainto%2520a%2520precise%2520calibration%2520mechanism.%2520Applied%2520to%2520Buchwald-Hartwig%2520reactions%252C%2520a%250Acornerstone%2520of%2520pharmaceutical%2520synthesis%252C%2520our%2520method%2520nearly%2520doubles%2520the%250Adiscovery%2520rate%2520of%2520high-yielding%2520reaction%2520conditions%252C%2520from%252024%2525%2520to%252043%2525%2520in%252050%250Aexperimental%2520iterations%2520starting%2520from%252010%2520unsuccessful%2520conditions.%2520Across%252019%250Adiverse%2520optimization%2520problems%2520spanning%2520organic%2520synthesis%252C%2520materials%2520science%2520and%250Acatalysis%252C%2520process%2520chemistry%252C%2520and%2520molecular%2520design%252C%2520our%2520approach%2520ranks%2520first%2520on%250Aaverage%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520reliable%252C%2520uncertainty-guided%250Aoptimization%2520with%2520LLMs.%2520Our%2520approach%2520can%2520accelerate%2520discovery%2520by%2520lowering%2520the%250Abarrier%2520to%2520using%2520powerful%2520optimization%2520methods%252C%2520replacing%2520the%2520need%2520for%250Adomain-specific%2520feature%2520engineering%2520with%2520more%2520accessible%2520natural%2520language%250Ainterfaces.%2520These%2520findings%2520highlight%2520that%2520ensuring%2520reliability%2520through%250Aprincipled%2520uncertainty%2520quantification%2520is%2520critical%2520for%2520realizing%2520the%2520full%250Apotential%2520of%2520AI-guided%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06265v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20as%20uncertainty-calibrated%20optimizers%20for%0A%20%20experimental%20discovery&entry.906535625=Bojana%20Rankovi%C4%87%20and%20Ryan-Rhys%20Griffiths%20and%20Philippe%20Schwaller&entry.1292438233=%20%20Scientific%20discovery%20increasingly%20depends%20on%20efficient%20experimental%0Aoptimization%20to%20navigate%20vast%20design%20spaces%20under%20time%20and%20resource%0Aconstraints.%20Traditional%20approaches%20often%20require%20extensive%20domain%20expertise%0Aand%20feature%20engineering.%20While%20large%20language%20models%2C%20with%20their%20vast%0Ascientific%20knowledge%2C%20circumvent%20the%20feature%20engineering%20limitations%2C%20they%20lack%0Athe%20calibrated%20uncertainty%20estimates%20required%20for%20high-stakes%20decision%20making.%0AHence%2C%20current%20optimization%20methods%20force%20a%20choice%20between%20domain%20knowledge%20and%0Areliability%2C%20with%20no%20principled%20approach%20that%20affords%20both.%20In%20this%20work%2C%20we%0Ashow%20that%20training%20language%20models%20through%20the%20uncertainty-aware%20objectives%20of%0Atraditional%20optimization%20methods%20enables%20their%20use%20as%20reliable%20optimizers%0Aguided%20by%20natural%20language.%20By%20teaching%20LLMs%20from%20experimental%20outcomes%20under%0Auncertainty%2C%20we%20transform%20their%20overconfidence%20from%20a%20fundamental%20limitation%0Ainto%20a%20precise%20calibration%20mechanism.%20Applied%20to%20Buchwald-Hartwig%20reactions%2C%20a%0Acornerstone%20of%20pharmaceutical%20synthesis%2C%20our%20method%20nearly%20doubles%20the%0Adiscovery%20rate%20of%20high-yielding%20reaction%20conditions%2C%20from%2024%25%20to%2043%25%20in%2050%0Aexperimental%20iterations%20starting%20from%2010%20unsuccessful%20conditions.%20Across%2019%0Adiverse%20optimization%20problems%20spanning%20organic%20synthesis%2C%20materials%20science%20and%0Acatalysis%2C%20process%20chemistry%2C%20and%20molecular%20design%2C%20our%20approach%20ranks%20first%20on%0Aaverage%2C%20establishing%20a%20new%20paradigm%20for%20reliable%2C%20uncertainty-guided%0Aoptimization%20with%20LLMs.%20Our%20approach%20can%20accelerate%20discovery%20by%20lowering%20the%0Abarrier%20to%20using%20powerful%20optimization%20methods%2C%20replacing%20the%20need%20for%0Adomain-specific%20feature%20engineering%20with%20more%20accessible%20natural%20language%0Ainterfaces.%20These%20findings%20highlight%20that%20ensuring%20reliability%20through%0Aprincipled%20uncertainty%20quantification%20is%20critical%20for%20realizing%20the%20full%0Apotential%20of%20AI-guided%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06265v3&entry.124074799=Read"},
{"title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning", "author": "Mohamed Bouadi and Pratinav Seth and Aditya Tanna and Vinay Kumar Sankarapu", "abstract": "  Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .\n", "link": "http://arxiv.org/abs/2511.02818v3", "date": "2025-11-07", "relevancy": 1.9782, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orion-MSP%3A%20Multi-Scale%20Sparse%20Attention%20for%20Tabular%20In-Context%20Learning&body=Title%3A%20Orion-MSP%3A%20Multi-Scale%20Sparse%20Attention%20for%20Tabular%20In-Context%20Learning%0AAuthor%3A%20Mohamed%20Bouadi%20and%20Pratinav%20Seth%20and%20Aditya%20Tanna%20and%20Vinay%20Kumar%20Sankarapu%0AAbstract%3A%20%20%20Tabular%20data%20remain%20the%20predominant%20format%20for%20real-world%20applications.%20Yet%2C%0Adeveloping%20effective%20neural%20models%20for%20tabular%20data%20remains%20challenging%20due%20to%0Aheterogeneous%20feature%20types%20and%20complex%20interactions%20occurring%20at%20multiple%0Ascales.%20Recent%20advances%20in%20tabular%20in-context%20learning%20%28ICL%29%2C%20such%20as%20TabPFN%0Aand%20TabICL%2C%20have%20achieved%20state-of-the-art%20performance%20comparable%20to%0Agradient-boosted%20trees%20%28GBTs%29%20without%20task-specific%20fine-tuning.%20However%2C%0Acurrent%20architectures%20exhibit%20key%20limitations%3A%20%281%29%20single-scale%20feature%0Aprocessing%20that%20overlooks%20hierarchical%20dependencies%2C%20%282%29%20dense%20attention%20with%0Aquadratic%20scaling%20in%20table%20width%2C%20and%20%283%29%20strictly%20sequential%20component%0Aprocessing%20that%20prevents%20iterative%20representation%20refinement%20and%0Across-component%20communication.%20To%20address%20these%20challenges%2C%20we%20introduce%0AOrion-MSP%2C%20a%20tabular%20ICL%20architecture%20featuring%20three%20key%20innovations%3A%20%281%29%0Amulti-scale%20processing%20to%20capture%20hierarchical%20feature%20interactions%3B%20%282%29%0Ablock-sparse%20attention%20combining%20windowed%2C%20global%2C%20and%20random%20patterns%20for%0Ascalable%20efficiency%20and%20long-range%20connectivity%3B%20and%20%283%29%20a%20Perceiver-style%0Amemory%20enabling%20safe%20bidirectional%20information%20flow%20across%20components.%20Across%0Adiverse%20benchmarks%2C%20Orion-MSP%20matches%20or%20surpasses%20state-of-the-art%20performance%0Awhile%20scaling%20effectively%20to%20high-dimensional%20tables%2C%20establishing%20a%20new%0Astandard%20for%20efficient%20tabular%20in-context%20learning.%20The%20model%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Lexsi-Labs/Orion-MSP%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.02818v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrion-MSP%253A%2520Multi-Scale%2520Sparse%2520Attention%2520for%2520Tabular%2520In-Context%2520Learning%26entry.906535625%3DMohamed%2520Bouadi%2520and%2520Pratinav%2520Seth%2520and%2520Aditya%2520Tanna%2520and%2520Vinay%2520Kumar%2520Sankarapu%26entry.1292438233%3D%2520%2520Tabular%2520data%2520remain%2520the%2520predominant%2520format%2520for%2520real-world%2520applications.%2520Yet%252C%250Adeveloping%2520effective%2520neural%2520models%2520for%2520tabular%2520data%2520remains%2520challenging%2520due%2520to%250Aheterogeneous%2520feature%2520types%2520and%2520complex%2520interactions%2520occurring%2520at%2520multiple%250Ascales.%2520Recent%2520advances%2520in%2520tabular%2520in-context%2520learning%2520%2528ICL%2529%252C%2520such%2520as%2520TabPFN%250Aand%2520TabICL%252C%2520have%2520achieved%2520state-of-the-art%2520performance%2520comparable%2520to%250Agradient-boosted%2520trees%2520%2528GBTs%2529%2520without%2520task-specific%2520fine-tuning.%2520However%252C%250Acurrent%2520architectures%2520exhibit%2520key%2520limitations%253A%2520%25281%2529%2520single-scale%2520feature%250Aprocessing%2520that%2520overlooks%2520hierarchical%2520dependencies%252C%2520%25282%2529%2520dense%2520attention%2520with%250Aquadratic%2520scaling%2520in%2520table%2520width%252C%2520and%2520%25283%2529%2520strictly%2520sequential%2520component%250Aprocessing%2520that%2520prevents%2520iterative%2520representation%2520refinement%2520and%250Across-component%2520communication.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AOrion-MSP%252C%2520a%2520tabular%2520ICL%2520architecture%2520featuring%2520three%2520key%2520innovations%253A%2520%25281%2529%250Amulti-scale%2520processing%2520to%2520capture%2520hierarchical%2520feature%2520interactions%253B%2520%25282%2529%250Ablock-sparse%2520attention%2520combining%2520windowed%252C%2520global%252C%2520and%2520random%2520patterns%2520for%250Ascalable%2520efficiency%2520and%2520long-range%2520connectivity%253B%2520and%2520%25283%2529%2520a%2520Perceiver-style%250Amemory%2520enabling%2520safe%2520bidirectional%2520information%2520flow%2520across%2520components.%2520Across%250Adiverse%2520benchmarks%252C%2520Orion-MSP%2520matches%2520or%2520surpasses%2520state-of-the-art%2520performance%250Awhile%2520scaling%2520effectively%2520to%2520high-dimensional%2520tables%252C%2520establishing%2520a%2520new%250Astandard%2520for%2520efficient%2520tabular%2520in-context%2520learning.%2520The%2520model%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Lexsi-Labs/Orion-MSP%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02818v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orion-MSP%3A%20Multi-Scale%20Sparse%20Attention%20for%20Tabular%20In-Context%20Learning&entry.906535625=Mohamed%20Bouadi%20and%20Pratinav%20Seth%20and%20Aditya%20Tanna%20and%20Vinay%20Kumar%20Sankarapu&entry.1292438233=%20%20Tabular%20data%20remain%20the%20predominant%20format%20for%20real-world%20applications.%20Yet%2C%0Adeveloping%20effective%20neural%20models%20for%20tabular%20data%20remains%20challenging%20due%20to%0Aheterogeneous%20feature%20types%20and%20complex%20interactions%20occurring%20at%20multiple%0Ascales.%20Recent%20advances%20in%20tabular%20in-context%20learning%20%28ICL%29%2C%20such%20as%20TabPFN%0Aand%20TabICL%2C%20have%20achieved%20state-of-the-art%20performance%20comparable%20to%0Agradient-boosted%20trees%20%28GBTs%29%20without%20task-specific%20fine-tuning.%20However%2C%0Acurrent%20architectures%20exhibit%20key%20limitations%3A%20%281%29%20single-scale%20feature%0Aprocessing%20that%20overlooks%20hierarchical%20dependencies%2C%20%282%29%20dense%20attention%20with%0Aquadratic%20scaling%20in%20table%20width%2C%20and%20%283%29%20strictly%20sequential%20component%0Aprocessing%20that%20prevents%20iterative%20representation%20refinement%20and%0Across-component%20communication.%20To%20address%20these%20challenges%2C%20we%20introduce%0AOrion-MSP%2C%20a%20tabular%20ICL%20architecture%20featuring%20three%20key%20innovations%3A%20%281%29%0Amulti-scale%20processing%20to%20capture%20hierarchical%20feature%20interactions%3B%20%282%29%0Ablock-sparse%20attention%20combining%20windowed%2C%20global%2C%20and%20random%20patterns%20for%0Ascalable%20efficiency%20and%20long-range%20connectivity%3B%20and%20%283%29%20a%20Perceiver-style%0Amemory%20enabling%20safe%20bidirectional%20information%20flow%20across%20components.%20Across%0Adiverse%20benchmarks%2C%20Orion-MSP%20matches%20or%20surpasses%20state-of-the-art%20performance%0Awhile%20scaling%20effectively%20to%20high-dimensional%20tables%2C%20establishing%20a%20new%0Astandard%20for%20efficient%20tabular%20in-context%20learning.%20The%20model%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Lexsi-Labs/Orion-MSP%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.02818v3&entry.124074799=Read"},
{"title": "FedFACT: A Provable Framework for Controllable Group-Fairness\n  Calibration in Federated Learning", "author": "Li Zhang and Zhongxuan Han and Xiaohua Feng and Jiaming Zhang and Yuyuan Li and Chaochao Chen", "abstract": "  With the emerging application of Federated Learning (FL) in decision-making\nscenarios, it is imperative to regulate model fairness to prevent disparities\nacross sensitive groups (e.g., female, male). Current research predominantly\nfocuses on two concepts of group fairness within FL: Global Fairness (overall\nmodel disparity across all clients) and Local Fairness (the disparity within\neach client). However, the non-decomposable, non-differentiable nature of\nfairness criteria poses two fundamental, unresolved challenges for fair FL: (i)\nHarmonizing global and local fairness, especially in multi-class setting; (ii)\nEnabling a controllable, optimal accuracy-fairness trade-off. To tackle these\nchallenges, we propose a novel controllable federated group-fairness\ncalibration framework, named FedFACT. FedFACT identifies the Bayes-optimal\nclassifiers under both global and local fairness constraints, yielding models\nwith minimal performance decline while guaranteeing fairness. Building on the\ncharacterization of the optimal fair classifiers, we reformulate fair federated\nlearning as a personalized cost-sensitive learning problem for in-processing\nand a bi-level optimization for post-processing. Theoretically, we provide\nconvergence and generalization guarantees for FedFACT to approach the\nnear-optimal accuracy under given fairness levels. Extensive experiments on\nmultiple datasets across various data heterogeneity demonstrate that FedFACT\nconsistently outperforms baselines in balancing accuracy and global-local\nfairness.\n", "link": "http://arxiv.org/abs/2506.03777v2", "date": "2025-11-07", "relevancy": 1.971, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4967}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4965}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedFACT%3A%20A%20Provable%20Framework%20for%20Controllable%20Group-Fairness%0A%20%20Calibration%20in%20Federated%20Learning&body=Title%3A%20FedFACT%3A%20A%20Provable%20Framework%20for%20Controllable%20Group-Fairness%0A%20%20Calibration%20in%20Federated%20Learning%0AAuthor%3A%20Li%20Zhang%20and%20Zhongxuan%20Han%20and%20Xiaohua%20Feng%20and%20Jiaming%20Zhang%20and%20Yuyuan%20Li%20and%20Chaochao%20Chen%0AAbstract%3A%20%20%20With%20the%20emerging%20application%20of%20Federated%20Learning%20%28FL%29%20in%20decision-making%0Ascenarios%2C%20it%20is%20imperative%20to%20regulate%20model%20fairness%20to%20prevent%20disparities%0Aacross%20sensitive%20groups%20%28e.g.%2C%20female%2C%20male%29.%20Current%20research%20predominantly%0Afocuses%20on%20two%20concepts%20of%20group%20fairness%20within%20FL%3A%20Global%20Fairness%20%28overall%0Amodel%20disparity%20across%20all%20clients%29%20and%20Local%20Fairness%20%28the%20disparity%20within%0Aeach%20client%29.%20However%2C%20the%20non-decomposable%2C%20non-differentiable%20nature%20of%0Afairness%20criteria%20poses%20two%20fundamental%2C%20unresolved%20challenges%20for%20fair%20FL%3A%20%28i%29%0AHarmonizing%20global%20and%20local%20fairness%2C%20especially%20in%20multi-class%20setting%3B%20%28ii%29%0AEnabling%20a%20controllable%2C%20optimal%20accuracy-fairness%20trade-off.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20novel%20controllable%20federated%20group-fairness%0Acalibration%20framework%2C%20named%20FedFACT.%20FedFACT%20identifies%20the%20Bayes-optimal%0Aclassifiers%20under%20both%20global%20and%20local%20fairness%20constraints%2C%20yielding%20models%0Awith%20minimal%20performance%20decline%20while%20guaranteeing%20fairness.%20Building%20on%20the%0Acharacterization%20of%20the%20optimal%20fair%20classifiers%2C%20we%20reformulate%20fair%20federated%0Alearning%20as%20a%20personalized%20cost-sensitive%20learning%20problem%20for%20in-processing%0Aand%20a%20bi-level%20optimization%20for%20post-processing.%20Theoretically%2C%20we%20provide%0Aconvergence%20and%20generalization%20guarantees%20for%20FedFACT%20to%20approach%20the%0Anear-optimal%20accuracy%20under%20given%20fairness%20levels.%20Extensive%20experiments%20on%0Amultiple%20datasets%20across%20various%20data%20heterogeneity%20demonstrate%20that%20FedFACT%0Aconsistently%20outperforms%20baselines%20in%20balancing%20accuracy%20and%20global-local%0Afairness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedFACT%253A%2520A%2520Provable%2520Framework%2520for%2520Controllable%2520Group-Fairness%250A%2520%2520Calibration%2520in%2520Federated%2520Learning%26entry.906535625%3DLi%2520Zhang%2520and%2520Zhongxuan%2520Han%2520and%2520Xiaohua%2520Feng%2520and%2520Jiaming%2520Zhang%2520and%2520Yuyuan%2520Li%2520and%2520Chaochao%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520emerging%2520application%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520in%2520decision-making%250Ascenarios%252C%2520it%2520is%2520imperative%2520to%2520regulate%2520model%2520fairness%2520to%2520prevent%2520disparities%250Aacross%2520sensitive%2520groups%2520%2528e.g.%252C%2520female%252C%2520male%2529.%2520Current%2520research%2520predominantly%250Afocuses%2520on%2520two%2520concepts%2520of%2520group%2520fairness%2520within%2520FL%253A%2520Global%2520Fairness%2520%2528overall%250Amodel%2520disparity%2520across%2520all%2520clients%2529%2520and%2520Local%2520Fairness%2520%2528the%2520disparity%2520within%250Aeach%2520client%2529.%2520However%252C%2520the%2520non-decomposable%252C%2520non-differentiable%2520nature%2520of%250Afairness%2520criteria%2520poses%2520two%2520fundamental%252C%2520unresolved%2520challenges%2520for%2520fair%2520FL%253A%2520%2528i%2529%250AHarmonizing%2520global%2520and%2520local%2520fairness%252C%2520especially%2520in%2520multi-class%2520setting%253B%2520%2528ii%2529%250AEnabling%2520a%2520controllable%252C%2520optimal%2520accuracy-fairness%2520trade-off.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520controllable%2520federated%2520group-fairness%250Acalibration%2520framework%252C%2520named%2520FedFACT.%2520FedFACT%2520identifies%2520the%2520Bayes-optimal%250Aclassifiers%2520under%2520both%2520global%2520and%2520local%2520fairness%2520constraints%252C%2520yielding%2520models%250Awith%2520minimal%2520performance%2520decline%2520while%2520guaranteeing%2520fairness.%2520Building%2520on%2520the%250Acharacterization%2520of%2520the%2520optimal%2520fair%2520classifiers%252C%2520we%2520reformulate%2520fair%2520federated%250Alearning%2520as%2520a%2520personalized%2520cost-sensitive%2520learning%2520problem%2520for%2520in-processing%250Aand%2520a%2520bi-level%2520optimization%2520for%2520post-processing.%2520Theoretically%252C%2520we%2520provide%250Aconvergence%2520and%2520generalization%2520guarantees%2520for%2520FedFACT%2520to%2520approach%2520the%250Anear-optimal%2520accuracy%2520under%2520given%2520fairness%2520levels.%2520Extensive%2520experiments%2520on%250Amultiple%2520datasets%2520across%2520various%2520data%2520heterogeneity%2520demonstrate%2520that%2520FedFACT%250Aconsistently%2520outperforms%2520baselines%2520in%2520balancing%2520accuracy%2520and%2520global-local%250Afairness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedFACT%3A%20A%20Provable%20Framework%20for%20Controllable%20Group-Fairness%0A%20%20Calibration%20in%20Federated%20Learning&entry.906535625=Li%20Zhang%20and%20Zhongxuan%20Han%20and%20Xiaohua%20Feng%20and%20Jiaming%20Zhang%20and%20Yuyuan%20Li%20and%20Chaochao%20Chen&entry.1292438233=%20%20With%20the%20emerging%20application%20of%20Federated%20Learning%20%28FL%29%20in%20decision-making%0Ascenarios%2C%20it%20is%20imperative%20to%20regulate%20model%20fairness%20to%20prevent%20disparities%0Aacross%20sensitive%20groups%20%28e.g.%2C%20female%2C%20male%29.%20Current%20research%20predominantly%0Afocuses%20on%20two%20concepts%20of%20group%20fairness%20within%20FL%3A%20Global%20Fairness%20%28overall%0Amodel%20disparity%20across%20all%20clients%29%20and%20Local%20Fairness%20%28the%20disparity%20within%0Aeach%20client%29.%20However%2C%20the%20non-decomposable%2C%20non-differentiable%20nature%20of%0Afairness%20criteria%20poses%20two%20fundamental%2C%20unresolved%20challenges%20for%20fair%20FL%3A%20%28i%29%0AHarmonizing%20global%20and%20local%20fairness%2C%20especially%20in%20multi-class%20setting%3B%20%28ii%29%0AEnabling%20a%20controllable%2C%20optimal%20accuracy-fairness%20trade-off.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20novel%20controllable%20federated%20group-fairness%0Acalibration%20framework%2C%20named%20FedFACT.%20FedFACT%20identifies%20the%20Bayes-optimal%0Aclassifiers%20under%20both%20global%20and%20local%20fairness%20constraints%2C%20yielding%20models%0Awith%20minimal%20performance%20decline%20while%20guaranteeing%20fairness.%20Building%20on%20the%0Acharacterization%20of%20the%20optimal%20fair%20classifiers%2C%20we%20reformulate%20fair%20federated%0Alearning%20as%20a%20personalized%20cost-sensitive%20learning%20problem%20for%20in-processing%0Aand%20a%20bi-level%20optimization%20for%20post-processing.%20Theoretically%2C%20we%20provide%0Aconvergence%20and%20generalization%20guarantees%20for%20FedFACT%20to%20approach%20the%0Anear-optimal%20accuracy%20under%20given%20fairness%20levels.%20Extensive%20experiments%20on%0Amultiple%20datasets%20across%20various%20data%20heterogeneity%20demonstrate%20that%20FedFACT%0Aconsistently%20outperforms%20baselines%20in%20balancing%20accuracy%20and%20global-local%0Afairness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03777v2&entry.124074799=Read"},
{"title": "USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired\n  Image IHC Virtual Staining", "author": "Yue Peng and Bing Xiong and Fuqiang Chen and De Eybo and RanRan Zhang and Wanming Hu and Jing Cai and Wenjian Qin", "abstract": "  Immunohistochemical (IHC) virtual staining is a task that generates virtual\nIHC images from H\\&E images while maintaining pathological semantic consistency\nwith adjacent slices. This task aims to achieve cross-domain mapping between\nmorphological structures and staining patterns through generative models,\nproviding an efficient and cost-effective solution for pathological analysis.\nHowever, under weakly paired conditions, spatial heterogeneity between adjacent\nslices presents significant challenges. This can lead to inaccurate one-to-many\nmappings and generate results that are inconsistent with the pathological\nsemantics of adjacent slices. To address this issue, we propose a novel\nunbalanced self-information feature transport for IHC virtual staining, named\nUSIGAN, which extracts global morphological semantics without relying on\npositional correspondence.By removing weakly paired terms in the joint marginal\ndistribution, we effectively mitigate the impact of weak pairing on joint\ndistributions, thereby significantly improving the content consistency and\npathological semantic consistency of the generated results. Moreover, we design\nthe Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the\nPathology Self-Correspondence (PC-SCM) mechanism to construct correlation\nmatrices between H\\&E and generated IHC in image-level and real IHC and\ngenerated IHC image sets in intra-group level.. Experiments conducted on two\npublicly available datasets demonstrate that our method achieves superior\nperformance across multiple clinically significant metrics, such as IoD and\nPearson-R correlation, demonstrating better clinical relevance.\n", "link": "http://arxiv.org/abs/2507.05843v2", "date": "2025-11-07", "relevancy": 1.9591, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5023}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4809}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20USIGAN%3A%20Unbalanced%20Self-Information%20Feature%20Transport%20for%20Weakly%20Paired%0A%20%20Image%20IHC%20Virtual%20Staining&body=Title%3A%20USIGAN%3A%20Unbalanced%20Self-Information%20Feature%20Transport%20for%20Weakly%20Paired%0A%20%20Image%20IHC%20Virtual%20Staining%0AAuthor%3A%20Yue%20Peng%20and%20Bing%20Xiong%20and%20Fuqiang%20Chen%20and%20De%20Eybo%20and%20RanRan%20Zhang%20and%20Wanming%20Hu%20and%20Jing%20Cai%20and%20Wenjian%20Qin%0AAbstract%3A%20%20%20Immunohistochemical%20%28IHC%29%20virtual%20staining%20is%20a%20task%20that%20generates%20virtual%0AIHC%20images%20from%20H%5C%26E%20images%20while%20maintaining%20pathological%20semantic%20consistency%0Awith%20adjacent%20slices.%20This%20task%20aims%20to%20achieve%20cross-domain%20mapping%20between%0Amorphological%20structures%20and%20staining%20patterns%20through%20generative%20models%2C%0Aproviding%20an%20efficient%20and%20cost-effective%20solution%20for%20pathological%20analysis.%0AHowever%2C%20under%20weakly%20paired%20conditions%2C%20spatial%20heterogeneity%20between%20adjacent%0Aslices%20presents%20significant%20challenges.%20This%20can%20lead%20to%20inaccurate%20one-to-many%0Amappings%20and%20generate%20results%20that%20are%20inconsistent%20with%20the%20pathological%0Asemantics%20of%20adjacent%20slices.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aunbalanced%20self-information%20feature%20transport%20for%20IHC%20virtual%20staining%2C%20named%0AUSIGAN%2C%20which%20extracts%20global%20morphological%20semantics%20without%20relying%20on%0Apositional%20correspondence.By%20removing%20weakly%20paired%20terms%20in%20the%20joint%20marginal%0Adistribution%2C%20we%20effectively%20mitigate%20the%20impact%20of%20weak%20pairing%20on%20joint%0Adistributions%2C%20thereby%20significantly%20improving%20the%20content%20consistency%20and%0Apathological%20semantic%20consistency%20of%20the%20generated%20results.%20Moreover%2C%20we%20design%0Athe%20Unbalanced%20Optimal%20Transport%20Consistency%20%28UOT-CTM%29%20mechanism%20and%20the%0APathology%20Self-Correspondence%20%28PC-SCM%29%20mechanism%20to%20construct%20correlation%0Amatrices%20between%20H%5C%26E%20and%20generated%20IHC%20in%20image-level%20and%20real%20IHC%20and%0Agenerated%20IHC%20image%20sets%20in%20intra-group%20level..%20Experiments%20conducted%20on%20two%0Apublicly%20available%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%0Aperformance%20across%20multiple%20clinically%20significant%20metrics%2C%20such%20as%20IoD%20and%0APearson-R%20correlation%2C%20demonstrating%20better%20clinical%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05843v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUSIGAN%253A%2520Unbalanced%2520Self-Information%2520Feature%2520Transport%2520for%2520Weakly%2520Paired%250A%2520%2520Image%2520IHC%2520Virtual%2520Staining%26entry.906535625%3DYue%2520Peng%2520and%2520Bing%2520Xiong%2520and%2520Fuqiang%2520Chen%2520and%2520De%2520Eybo%2520and%2520RanRan%2520Zhang%2520and%2520Wanming%2520Hu%2520and%2520Jing%2520Cai%2520and%2520Wenjian%2520Qin%26entry.1292438233%3D%2520%2520Immunohistochemical%2520%2528IHC%2529%2520virtual%2520staining%2520is%2520a%2520task%2520that%2520generates%2520virtual%250AIHC%2520images%2520from%2520H%255C%2526E%2520images%2520while%2520maintaining%2520pathological%2520semantic%2520consistency%250Awith%2520adjacent%2520slices.%2520This%2520task%2520aims%2520to%2520achieve%2520cross-domain%2520mapping%2520between%250Amorphological%2520structures%2520and%2520staining%2520patterns%2520through%2520generative%2520models%252C%250Aproviding%2520an%2520efficient%2520and%2520cost-effective%2520solution%2520for%2520pathological%2520analysis.%250AHowever%252C%2520under%2520weakly%2520paired%2520conditions%252C%2520spatial%2520heterogeneity%2520between%2520adjacent%250Aslices%2520presents%2520significant%2520challenges.%2520This%2520can%2520lead%2520to%2520inaccurate%2520one-to-many%250Amappings%2520and%2520generate%2520results%2520that%2520are%2520inconsistent%2520with%2520the%2520pathological%250Asemantics%2520of%2520adjacent%2520slices.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250Aunbalanced%2520self-information%2520feature%2520transport%2520for%2520IHC%2520virtual%2520staining%252C%2520named%250AUSIGAN%252C%2520which%2520extracts%2520global%2520morphological%2520semantics%2520without%2520relying%2520on%250Apositional%2520correspondence.By%2520removing%2520weakly%2520paired%2520terms%2520in%2520the%2520joint%2520marginal%250Adistribution%252C%2520we%2520effectively%2520mitigate%2520the%2520impact%2520of%2520weak%2520pairing%2520on%2520joint%250Adistributions%252C%2520thereby%2520significantly%2520improving%2520the%2520content%2520consistency%2520and%250Apathological%2520semantic%2520consistency%2520of%2520the%2520generated%2520results.%2520Moreover%252C%2520we%2520design%250Athe%2520Unbalanced%2520Optimal%2520Transport%2520Consistency%2520%2528UOT-CTM%2529%2520mechanism%2520and%2520the%250APathology%2520Self-Correspondence%2520%2528PC-SCM%2529%2520mechanism%2520to%2520construct%2520correlation%250Amatrices%2520between%2520H%255C%2526E%2520and%2520generated%2520IHC%2520in%2520image-level%2520and%2520real%2520IHC%2520and%250Agenerated%2520IHC%2520image%2520sets%2520in%2520intra-group%2520level..%2520Experiments%2520conducted%2520on%2520two%250Apublicly%2520available%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%250Aperformance%2520across%2520multiple%2520clinically%2520significant%2520metrics%252C%2520such%2520as%2520IoD%2520and%250APearson-R%2520correlation%252C%2520demonstrating%2520better%2520clinical%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05843v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USIGAN%3A%20Unbalanced%20Self-Information%20Feature%20Transport%20for%20Weakly%20Paired%0A%20%20Image%20IHC%20Virtual%20Staining&entry.906535625=Yue%20Peng%20and%20Bing%20Xiong%20and%20Fuqiang%20Chen%20and%20De%20Eybo%20and%20RanRan%20Zhang%20and%20Wanming%20Hu%20and%20Jing%20Cai%20and%20Wenjian%20Qin&entry.1292438233=%20%20Immunohistochemical%20%28IHC%29%20virtual%20staining%20is%20a%20task%20that%20generates%20virtual%0AIHC%20images%20from%20H%5C%26E%20images%20while%20maintaining%20pathological%20semantic%20consistency%0Awith%20adjacent%20slices.%20This%20task%20aims%20to%20achieve%20cross-domain%20mapping%20between%0Amorphological%20structures%20and%20staining%20patterns%20through%20generative%20models%2C%0Aproviding%20an%20efficient%20and%20cost-effective%20solution%20for%20pathological%20analysis.%0AHowever%2C%20under%20weakly%20paired%20conditions%2C%20spatial%20heterogeneity%20between%20adjacent%0Aslices%20presents%20significant%20challenges.%20This%20can%20lead%20to%20inaccurate%20one-to-many%0Amappings%20and%20generate%20results%20that%20are%20inconsistent%20with%20the%20pathological%0Asemantics%20of%20adjacent%20slices.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aunbalanced%20self-information%20feature%20transport%20for%20IHC%20virtual%20staining%2C%20named%0AUSIGAN%2C%20which%20extracts%20global%20morphological%20semantics%20without%20relying%20on%0Apositional%20correspondence.By%20removing%20weakly%20paired%20terms%20in%20the%20joint%20marginal%0Adistribution%2C%20we%20effectively%20mitigate%20the%20impact%20of%20weak%20pairing%20on%20joint%0Adistributions%2C%20thereby%20significantly%20improving%20the%20content%20consistency%20and%0Apathological%20semantic%20consistency%20of%20the%20generated%20results.%20Moreover%2C%20we%20design%0Athe%20Unbalanced%20Optimal%20Transport%20Consistency%20%28UOT-CTM%29%20mechanism%20and%20the%0APathology%20Self-Correspondence%20%28PC-SCM%29%20mechanism%20to%20construct%20correlation%0Amatrices%20between%20H%5C%26E%20and%20generated%20IHC%20in%20image-level%20and%20real%20IHC%20and%0Agenerated%20IHC%20image%20sets%20in%20intra-group%20level..%20Experiments%20conducted%20on%20two%0Apublicly%20available%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%0Aperformance%20across%20multiple%20clinically%20significant%20metrics%2C%20such%20as%20IoD%20and%0APearson-R%20correlation%2C%20demonstrating%20better%20clinical%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05843v2&entry.124074799=Read"},
{"title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models", "author": "Haibo Jin and Ruoxi Chen and Peiyan Zhang and Andy Zhou and Haohan Wang", "abstract": "  The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.\n", "link": "http://arxiv.org/abs/2402.03299v6", "date": "2025-11-07", "relevancy": 1.9586, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5073}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4771}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUARD%3A%20Role-playing%20to%20Generate%20Natural-language%20Jailbreakings%20to%20Test%0A%20%20Guideline%20Adherence%20of%20Large%20Language%20Models&body=Title%3A%20GUARD%3A%20Role-playing%20to%20Generate%20Natural-language%20Jailbreakings%20to%20Test%0A%20%20Guideline%20Adherence%20of%20Large%20Language%20Models%0AAuthor%3A%20Haibo%20Jin%20and%20Ruoxi%20Chen%20and%20Peiyan%20Zhang%20and%20Andy%20Zhou%20and%20Haohan%20Wang%0AAbstract%3A%20%20%20The%20discovery%20of%20%22jailbreaks%22%20to%20bypass%20safety%20filters%20of%20Large%20Language%0AModels%20%28LLMs%29%20and%20harmful%20responses%20have%20encouraged%20the%20community%20to%20implement%0Asafety%20measures.%20One%20major%20safety%20measure%20is%20to%20proactively%20test%20the%20LLMs%20with%0Ajailbreaks%20prior%20to%20the%20release.%20Therefore%2C%20such%20testing%20will%20require%20a%20method%0Athat%20can%20generate%20jailbreaks%20massively%20and%20efficiently.%20In%20this%20paper%2C%20we%0Afollow%20a%20novel%20yet%20intuitive%20strategy%20to%20generate%20jailbreaks%20in%20the%20style%20of%0Athe%20human%20generation.%20We%20propose%20a%20role-playing%20system%20that%20assigns%20four%0Adifferent%20roles%20to%20the%20user%20LLMs%20to%20collaborate%20on%20new%20jailbreaks.%20Furthermore%2C%0Awe%20collect%20existing%20jailbreaks%20and%20split%20them%20into%20different%20independent%0Acharacteristics%20using%20clustering%20frequency%20and%20semantic%20patterns%20sentence%20by%0Asentence.%20We%20organize%20these%20characteristics%20into%20a%20knowledge%20graph%2C%20making%20them%0Amore%20accessible%20and%20easier%20to%20retrieve.%20Our%20system%20of%20different%20roles%20will%0Aleverage%20this%20knowledge%20graph%20to%20generate%20new%20jailbreaks%2C%20which%20have%20proved%0Aeffective%20in%20inducing%20LLMs%20to%20generate%20unethical%20or%20guideline-violating%0Aresponses.%20In%20addition%2C%20we%20also%20pioneer%20a%20setting%20in%20our%20system%20that%20will%0Aautomatically%20follow%20the%20government-issued%20guidelines%20to%20generate%20jailbreaks%20to%0Atest%20whether%20LLMs%20follow%20the%20guidelines%20accordingly.%20We%20refer%20to%20our%20system%20as%0AGUARD%20%28Guideline%20Upholding%20through%20Adaptive%20Role-play%20Diagnostics%29.%20We%20have%0Aempirically%20validated%20the%20effectiveness%20of%20GUARD%20on%20three%20cutting-edge%0Aopen-sourced%20LLMs%20%28Vicuna-13B%2C%20LongChat-7B%2C%20and%20Llama-2-7B%29%2C%20as%20well%20as%20a%0Awidely-utilized%20commercial%20LLM%20%28ChatGPT%29.%20Moreover%2C%20our%20work%20extends%20to%20the%0Arealm%20of%20vision%20language%20models%20%28MiniGPT-v2%20and%20Gemini%20Vision%20Pro%29%2C%20showcasing%0AGUARD%27s%20versatility%20and%20contributing%20valuable%20insights%20for%20the%20development%20of%0Asafer%2C%20more%20reliable%20LLM-based%20applications%20across%20diverse%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03299v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUARD%253A%2520Role-playing%2520to%2520Generate%2520Natural-language%2520Jailbreakings%2520to%2520Test%250A%2520%2520Guideline%2520Adherence%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DHaibo%2520Jin%2520and%2520Ruoxi%2520Chen%2520and%2520Peiyan%2520Zhang%2520and%2520Andy%2520Zhou%2520and%2520Haohan%2520Wang%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520%2522jailbreaks%2522%2520to%2520bypass%2520safety%2520filters%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520and%2520harmful%2520responses%2520have%2520encouraged%2520the%2520community%2520to%2520implement%250Asafety%2520measures.%2520One%2520major%2520safety%2520measure%2520is%2520to%2520proactively%2520test%2520the%2520LLMs%2520with%250Ajailbreaks%2520prior%2520to%2520the%2520release.%2520Therefore%252C%2520such%2520testing%2520will%2520require%2520a%2520method%250Athat%2520can%2520generate%2520jailbreaks%2520massively%2520and%2520efficiently.%2520In%2520this%2520paper%252C%2520we%250Afollow%2520a%2520novel%2520yet%2520intuitive%2520strategy%2520to%2520generate%2520jailbreaks%2520in%2520the%2520style%2520of%250Athe%2520human%2520generation.%2520We%2520propose%2520a%2520role-playing%2520system%2520that%2520assigns%2520four%250Adifferent%2520roles%2520to%2520the%2520user%2520LLMs%2520to%2520collaborate%2520on%2520new%2520jailbreaks.%2520Furthermore%252C%250Awe%2520collect%2520existing%2520jailbreaks%2520and%2520split%2520them%2520into%2520different%2520independent%250Acharacteristics%2520using%2520clustering%2520frequency%2520and%2520semantic%2520patterns%2520sentence%2520by%250Asentence.%2520We%2520organize%2520these%2520characteristics%2520into%2520a%2520knowledge%2520graph%252C%2520making%2520them%250Amore%2520accessible%2520and%2520easier%2520to%2520retrieve.%2520Our%2520system%2520of%2520different%2520roles%2520will%250Aleverage%2520this%2520knowledge%2520graph%2520to%2520generate%2520new%2520jailbreaks%252C%2520which%2520have%2520proved%250Aeffective%2520in%2520inducing%2520LLMs%2520to%2520generate%2520unethical%2520or%2520guideline-violating%250Aresponses.%2520In%2520addition%252C%2520we%2520also%2520pioneer%2520a%2520setting%2520in%2520our%2520system%2520that%2520will%250Aautomatically%2520follow%2520the%2520government-issued%2520guidelines%2520to%2520generate%2520jailbreaks%2520to%250Atest%2520whether%2520LLMs%2520follow%2520the%2520guidelines%2520accordingly.%2520We%2520refer%2520to%2520our%2520system%2520as%250AGUARD%2520%2528Guideline%2520Upholding%2520through%2520Adaptive%2520Role-play%2520Diagnostics%2529.%2520We%2520have%250Aempirically%2520validated%2520the%2520effectiveness%2520of%2520GUARD%2520on%2520three%2520cutting-edge%250Aopen-sourced%2520LLMs%2520%2528Vicuna-13B%252C%2520LongChat-7B%252C%2520and%2520Llama-2-7B%2529%252C%2520as%2520well%2520as%2520a%250Awidely-utilized%2520commercial%2520LLM%2520%2528ChatGPT%2529.%2520Moreover%252C%2520our%2520work%2520extends%2520to%2520the%250Arealm%2520of%2520vision%2520language%2520models%2520%2528MiniGPT-v2%2520and%2520Gemini%2520Vision%2520Pro%2529%252C%2520showcasing%250AGUARD%2527s%2520versatility%2520and%2520contributing%2520valuable%2520insights%2520for%2520the%2520development%2520of%250Asafer%252C%2520more%2520reliable%2520LLM-based%2520applications%2520across%2520diverse%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03299v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUARD%3A%20Role-playing%20to%20Generate%20Natural-language%20Jailbreakings%20to%20Test%0A%20%20Guideline%20Adherence%20of%20Large%20Language%20Models&entry.906535625=Haibo%20Jin%20and%20Ruoxi%20Chen%20and%20Peiyan%20Zhang%20and%20Andy%20Zhou%20and%20Haohan%20Wang&entry.1292438233=%20%20The%20discovery%20of%20%22jailbreaks%22%20to%20bypass%20safety%20filters%20of%20Large%20Language%0AModels%20%28LLMs%29%20and%20harmful%20responses%20have%20encouraged%20the%20community%20to%20implement%0Asafety%20measures.%20One%20major%20safety%20measure%20is%20to%20proactively%20test%20the%20LLMs%20with%0Ajailbreaks%20prior%20to%20the%20release.%20Therefore%2C%20such%20testing%20will%20require%20a%20method%0Athat%20can%20generate%20jailbreaks%20massively%20and%20efficiently.%20In%20this%20paper%2C%20we%0Afollow%20a%20novel%20yet%20intuitive%20strategy%20to%20generate%20jailbreaks%20in%20the%20style%20of%0Athe%20human%20generation.%20We%20propose%20a%20role-playing%20system%20that%20assigns%20four%0Adifferent%20roles%20to%20the%20user%20LLMs%20to%20collaborate%20on%20new%20jailbreaks.%20Furthermore%2C%0Awe%20collect%20existing%20jailbreaks%20and%20split%20them%20into%20different%20independent%0Acharacteristics%20using%20clustering%20frequency%20and%20semantic%20patterns%20sentence%20by%0Asentence.%20We%20organize%20these%20characteristics%20into%20a%20knowledge%20graph%2C%20making%20them%0Amore%20accessible%20and%20easier%20to%20retrieve.%20Our%20system%20of%20different%20roles%20will%0Aleverage%20this%20knowledge%20graph%20to%20generate%20new%20jailbreaks%2C%20which%20have%20proved%0Aeffective%20in%20inducing%20LLMs%20to%20generate%20unethical%20or%20guideline-violating%0Aresponses.%20In%20addition%2C%20we%20also%20pioneer%20a%20setting%20in%20our%20system%20that%20will%0Aautomatically%20follow%20the%20government-issued%20guidelines%20to%20generate%20jailbreaks%20to%0Atest%20whether%20LLMs%20follow%20the%20guidelines%20accordingly.%20We%20refer%20to%20our%20system%20as%0AGUARD%20%28Guideline%20Upholding%20through%20Adaptive%20Role-play%20Diagnostics%29.%20We%20have%0Aempirically%20validated%20the%20effectiveness%20of%20GUARD%20on%20three%20cutting-edge%0Aopen-sourced%20LLMs%20%28Vicuna-13B%2C%20LongChat-7B%2C%20and%20Llama-2-7B%29%2C%20as%20well%20as%20a%0Awidely-utilized%20commercial%20LLM%20%28ChatGPT%29.%20Moreover%2C%20our%20work%20extends%20to%20the%0Arealm%20of%20vision%20language%20models%20%28MiniGPT-v2%20and%20Gemini%20Vision%20Pro%29%2C%20showcasing%0AGUARD%27s%20versatility%20and%20contributing%20valuable%20insights%20for%20the%20development%20of%0Asafer%2C%20more%20reliable%20LLM-based%20applications%20across%20diverse%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03299v6&entry.124074799=Read"},
{"title": "In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies", "author": "Yunbum Kook and Santosh S. Vempala and Matthew S. Zhang", "abstract": "  We present a new random walk for uniformly sampling high-dimensional convex\nbodies. It achieves state-of-the-art runtime complexity with stronger\nguarantees on the output than previously known, namely in R\\'enyi divergence\n(which implies TV, $\\mathcal{W}_2$, KL, $\\chi^2$). The proof departs from known\napproaches for polytime algorithms for the problem -- we utilize a stochastic\ndiffusion perspective to show contraction to the target distribution with the\nrate of convergence determined by functional isoperimetric constants of the\ntarget distribution.\n", "link": "http://arxiv.org/abs/2405.01425v3", "date": "2025-11-07", "relevancy": 1.9209, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4809}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies&body=Title%3A%20In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies%0AAuthor%3A%20Yunbum%20Kook%20and%20Santosh%20S.%20Vempala%20and%20Matthew%20S.%20Zhang%0AAbstract%3A%20%20%20We%20present%20a%20new%20random%20walk%20for%20uniformly%20sampling%20high-dimensional%20convex%0Abodies.%20It%20achieves%20state-of-the-art%20runtime%20complexity%20with%20stronger%0Aguarantees%20on%20the%20output%20than%20previously%20known%2C%20namely%20in%20R%5C%27enyi%20divergence%0A%28which%20implies%20TV%2C%20%24%5Cmathcal%7BW%7D_2%24%2C%20KL%2C%20%24%5Cchi%5E2%24%29.%20The%20proof%20departs%20from%20known%0Aapproaches%20for%20polytime%20algorithms%20for%20the%20problem%20--%20we%20utilize%20a%20stochastic%0Adiffusion%20perspective%20to%20show%20contraction%20to%20the%20target%20distribution%20with%20the%0Arate%20of%20convergence%20determined%20by%20functional%20isoperimetric%20constants%20of%20the%0Atarget%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01425v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-and-Out%253A%2520Algorithmic%2520Diffusion%2520for%2520Sampling%2520Convex%2520Bodies%26entry.906535625%3DYunbum%2520Kook%2520and%2520Santosh%2520S.%2520Vempala%2520and%2520Matthew%2520S.%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520random%2520walk%2520for%2520uniformly%2520sampling%2520high-dimensional%2520convex%250Abodies.%2520It%2520achieves%2520state-of-the-art%2520runtime%2520complexity%2520with%2520stronger%250Aguarantees%2520on%2520the%2520output%2520than%2520previously%2520known%252C%2520namely%2520in%2520R%255C%2527enyi%2520divergence%250A%2528which%2520implies%2520TV%252C%2520%2524%255Cmathcal%257BW%257D_2%2524%252C%2520KL%252C%2520%2524%255Cchi%255E2%2524%2529.%2520The%2520proof%2520departs%2520from%2520known%250Aapproaches%2520for%2520polytime%2520algorithms%2520for%2520the%2520problem%2520--%2520we%2520utilize%2520a%2520stochastic%250Adiffusion%2520perspective%2520to%2520show%2520contraction%2520to%2520the%2520target%2520distribution%2520with%2520the%250Arate%2520of%2520convergence%2520determined%2520by%2520functional%2520isoperimetric%2520constants%2520of%2520the%250Atarget%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01425v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-and-Out%3A%20Algorithmic%20Diffusion%20for%20Sampling%20Convex%20Bodies&entry.906535625=Yunbum%20Kook%20and%20Santosh%20S.%20Vempala%20and%20Matthew%20S.%20Zhang&entry.1292438233=%20%20We%20present%20a%20new%20random%20walk%20for%20uniformly%20sampling%20high-dimensional%20convex%0Abodies.%20It%20achieves%20state-of-the-art%20runtime%20complexity%20with%20stronger%0Aguarantees%20on%20the%20output%20than%20previously%20known%2C%20namely%20in%20R%5C%27enyi%20divergence%0A%28which%20implies%20TV%2C%20%24%5Cmathcal%7BW%7D_2%24%2C%20KL%2C%20%24%5Cchi%5E2%24%29.%20The%20proof%20departs%20from%20known%0Aapproaches%20for%20polytime%20algorithms%20for%20the%20problem%20--%20we%20utilize%20a%20stochastic%0Adiffusion%20perspective%20to%20show%20contraction%20to%20the%20target%20distribution%20with%20the%0Arate%20of%20convergence%20determined%20by%20functional%20isoperimetric%20constants%20of%20the%0Atarget%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01425v3&entry.124074799=Read"},
{"title": "Adversarially Robust Multitask Adaptive Control", "author": "Kasra Fallah and Leonardo F. Toso and James Anderson", "abstract": "  We study adversarially robust multitask adaptive linear quadratic control; a\nsetting where multiple systems collaboratively learn control policies under\nmodel uncertainty and adversarial corruption. We propose a clustered multitask\napproach that integrates clustering and system identification with resilient\naggregation to mitigate corrupted model updates. Our analysis characterizes how\nclustering accuracy, intra-cluster heterogeneity, and adversarial behavior\naffect the expected regret of certainty-equivalent (CE) control across LQR\ntasks. We establish non-asymptotic bounds demonstrating that the regret\ndecreases inversely with the number of honest systems per cluster and that this\nreduction is preserved under a bounded fraction of adversarial systems within\neach cluster.\n", "link": "http://arxiv.org/abs/2511.05444v1", "date": "2025-11-07", "relevancy": 1.8736, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4812}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarially%20Robust%20Multitask%20Adaptive%20Control&body=Title%3A%20Adversarially%20Robust%20Multitask%20Adaptive%20Control%0AAuthor%3A%20Kasra%20Fallah%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson%0AAbstract%3A%20%20%20We%20study%20adversarially%20robust%20multitask%20adaptive%20linear%20quadratic%20control%3B%20a%0Asetting%20where%20multiple%20systems%20collaboratively%20learn%20control%20policies%20under%0Amodel%20uncertainty%20and%20adversarial%20corruption.%20We%20propose%20a%20clustered%20multitask%0Aapproach%20that%20integrates%20clustering%20and%20system%20identification%20with%20resilient%0Aaggregation%20to%20mitigate%20corrupted%20model%20updates.%20Our%20analysis%20characterizes%20how%0Aclustering%20accuracy%2C%20intra-cluster%20heterogeneity%2C%20and%20adversarial%20behavior%0Aaffect%20the%20expected%20regret%20of%20certainty-equivalent%20%28CE%29%20control%20across%20LQR%0Atasks.%20We%20establish%20non-asymptotic%20bounds%20demonstrating%20that%20the%20regret%0Adecreases%20inversely%20with%20the%20number%20of%20honest%20systems%20per%20cluster%20and%20that%20this%0Areduction%20is%20preserved%20under%20a%20bounded%20fraction%20of%20adversarial%20systems%20within%0Aeach%20cluster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarially%2520Robust%2520Multitask%2520Adaptive%2520Control%26entry.906535625%3DKasra%2520Fallah%2520and%2520Leonardo%2520F.%2520Toso%2520and%2520James%2520Anderson%26entry.1292438233%3D%2520%2520We%2520study%2520adversarially%2520robust%2520multitask%2520adaptive%2520linear%2520quadratic%2520control%253B%2520a%250Asetting%2520where%2520multiple%2520systems%2520collaboratively%2520learn%2520control%2520policies%2520under%250Amodel%2520uncertainty%2520and%2520adversarial%2520corruption.%2520We%2520propose%2520a%2520clustered%2520multitask%250Aapproach%2520that%2520integrates%2520clustering%2520and%2520system%2520identification%2520with%2520resilient%250Aaggregation%2520to%2520mitigate%2520corrupted%2520model%2520updates.%2520Our%2520analysis%2520characterizes%2520how%250Aclustering%2520accuracy%252C%2520intra-cluster%2520heterogeneity%252C%2520and%2520adversarial%2520behavior%250Aaffect%2520the%2520expected%2520regret%2520of%2520certainty-equivalent%2520%2528CE%2529%2520control%2520across%2520LQR%250Atasks.%2520We%2520establish%2520non-asymptotic%2520bounds%2520demonstrating%2520that%2520the%2520regret%250Adecreases%2520inversely%2520with%2520the%2520number%2520of%2520honest%2520systems%2520per%2520cluster%2520and%2520that%2520this%250Areduction%2520is%2520preserved%2520under%2520a%2520bounded%2520fraction%2520of%2520adversarial%2520systems%2520within%250Aeach%2520cluster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarially%20Robust%20Multitask%20Adaptive%20Control&entry.906535625=Kasra%20Fallah%20and%20Leonardo%20F.%20Toso%20and%20James%20Anderson&entry.1292438233=%20%20We%20study%20adversarially%20robust%20multitask%20adaptive%20linear%20quadratic%20control%3B%20a%0Asetting%20where%20multiple%20systems%20collaboratively%20learn%20control%20policies%20under%0Amodel%20uncertainty%20and%20adversarial%20corruption.%20We%20propose%20a%20clustered%20multitask%0Aapproach%20that%20integrates%20clustering%20and%20system%20identification%20with%20resilient%0Aaggregation%20to%20mitigate%20corrupted%20model%20updates.%20Our%20analysis%20characterizes%20how%0Aclustering%20accuracy%2C%20intra-cluster%20heterogeneity%2C%20and%20adversarial%20behavior%0Aaffect%20the%20expected%20regret%20of%20certainty-equivalent%20%28CE%29%20control%20across%20LQR%0Atasks.%20We%20establish%20non-asymptotic%20bounds%20demonstrating%20that%20the%20regret%0Adecreases%20inversely%20with%20the%20number%20of%20honest%20systems%20per%20cluster%20and%20that%20this%0Areduction%20is%20preserved%20under%20a%20bounded%20fraction%20of%20adversarial%20systems%20within%0Aeach%20cluster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05444v1&entry.124074799=Read"},
{"title": "Comparative Study on Noise-Augmented Training and its Effect on\n  Adversarial Robustness in ASR Systems", "author": "Karla Pizzi and Mat\u00edas Pizarro and Asja Fischer", "abstract": "  In this study, we investigate whether noise-augmented training can\nconcurrently improve adversarial robustness in automatic speech recognition\n(ASR) systems. We conduct a comparative analysis of the adversarial robustness\nof four different ASR architectures, each trained under three different\naugmentation conditions: (1) background noise, speed variations, and\nreverberations; (2) speed variations only; (3) no data augmentation. We then\nevaluate the robustness of all resulting models against attacks with white-box\nor black-box adversarial examples. Our results demonstrate that noise\naugmentation not only enhances model performance on noisy speech but also\nimproves the model's robustness to adversarial attacks.\n", "link": "http://arxiv.org/abs/2409.01813v4", "date": "2025-11-07", "relevancy": 1.857, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.457}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Study%20on%20Noise-Augmented%20Training%20and%20its%20Effect%20on%0A%20%20Adversarial%20Robustness%20in%20ASR%20Systems&body=Title%3A%20Comparative%20Study%20on%20Noise-Augmented%20Training%20and%20its%20Effect%20on%0A%20%20Adversarial%20Robustness%20in%20ASR%20Systems%0AAuthor%3A%20Karla%20Pizzi%20and%20Mat%C3%ADas%20Pizarro%20and%20Asja%20Fischer%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20investigate%20whether%20noise-augmented%20training%20can%0Aconcurrently%20improve%20adversarial%20robustness%20in%20automatic%20speech%20recognition%0A%28ASR%29%20systems.%20We%20conduct%20a%20comparative%20analysis%20of%20the%20adversarial%20robustness%0Aof%20four%20different%20ASR%20architectures%2C%20each%20trained%20under%20three%20different%0Aaugmentation%20conditions%3A%20%281%29%20background%20noise%2C%20speed%20variations%2C%20and%0Areverberations%3B%20%282%29%20speed%20variations%20only%3B%20%283%29%20no%20data%20augmentation.%20We%20then%0Aevaluate%20the%20robustness%20of%20all%20resulting%20models%20against%20attacks%20with%20white-box%0Aor%20black-box%20adversarial%20examples.%20Our%20results%20demonstrate%20that%20noise%0Aaugmentation%20not%20only%20enhances%20model%20performance%20on%20noisy%20speech%20but%20also%0Aimproves%20the%20model%27s%20robustness%20to%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01813v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Study%2520on%2520Noise-Augmented%2520Training%2520and%2520its%2520Effect%2520on%250A%2520%2520Adversarial%2520Robustness%2520in%2520ASR%2520Systems%26entry.906535625%3DKarla%2520Pizzi%2520and%2520Mat%25C3%25ADas%2520Pizarro%2520and%2520Asja%2520Fischer%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520investigate%2520whether%2520noise-augmented%2520training%2520can%250Aconcurrently%2520improve%2520adversarial%2520robustness%2520in%2520automatic%2520speech%2520recognition%250A%2528ASR%2529%2520systems.%2520We%2520conduct%2520a%2520comparative%2520analysis%2520of%2520the%2520adversarial%2520robustness%250Aof%2520four%2520different%2520ASR%2520architectures%252C%2520each%2520trained%2520under%2520three%2520different%250Aaugmentation%2520conditions%253A%2520%25281%2529%2520background%2520noise%252C%2520speed%2520variations%252C%2520and%250Areverberations%253B%2520%25282%2529%2520speed%2520variations%2520only%253B%2520%25283%2529%2520no%2520data%2520augmentation.%2520We%2520then%250Aevaluate%2520the%2520robustness%2520of%2520all%2520resulting%2520models%2520against%2520attacks%2520with%2520white-box%250Aor%2520black-box%2520adversarial%2520examples.%2520Our%2520results%2520demonstrate%2520that%2520noise%250Aaugmentation%2520not%2520only%2520enhances%2520model%2520performance%2520on%2520noisy%2520speech%2520but%2520also%250Aimproves%2520the%2520model%2527s%2520robustness%2520to%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01813v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Study%20on%20Noise-Augmented%20Training%20and%20its%20Effect%20on%0A%20%20Adversarial%20Robustness%20in%20ASR%20Systems&entry.906535625=Karla%20Pizzi%20and%20Mat%C3%ADas%20Pizarro%20and%20Asja%20Fischer&entry.1292438233=%20%20In%20this%20study%2C%20we%20investigate%20whether%20noise-augmented%20training%20can%0Aconcurrently%20improve%20adversarial%20robustness%20in%20automatic%20speech%20recognition%0A%28ASR%29%20systems.%20We%20conduct%20a%20comparative%20analysis%20of%20the%20adversarial%20robustness%0Aof%20four%20different%20ASR%20architectures%2C%20each%20trained%20under%20three%20different%0Aaugmentation%20conditions%3A%20%281%29%20background%20noise%2C%20speed%20variations%2C%20and%0Areverberations%3B%20%282%29%20speed%20variations%20only%3B%20%283%29%20no%20data%20augmentation.%20We%20then%0Aevaluate%20the%20robustness%20of%20all%20resulting%20models%20against%20attacks%20with%20white-box%0Aor%20black-box%20adversarial%20examples.%20Our%20results%20demonstrate%20that%20noise%0Aaugmentation%20not%20only%20enhances%20model%20performance%20on%20noisy%20speech%20but%20also%0Aimproves%20the%20model%27s%20robustness%20to%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01813v4&entry.124074799=Read"},
{"title": "Synapse: Adaptive Arbitration of Complementary Expertise in Time Series\n  Foundational Models", "author": "Sarkar Snigdha Sarathi Das and Palash Goyal and Mihir Parmar and Yiwen Song and Long T. Le and Lesly Miculicich and Jinsung Yoon and Rui Zhang and Hamid Palangi and Tomas Pfister", "abstract": "  Pre-trained Time Series Foundational Models (TSFMs) represent a significant\nadvance, capable of forecasting diverse time series with complex\ncharacteristics, including varied seasonalities, trends, and long-range\ndependencies. Despite their primary goal of universal time series forecasting,\ntheir efficacy is far from uniform; divergent training protocols and data\nsources cause individual TSFMs to exhibit highly variable performance across\ndifferent forecasting tasks, domains, and horizons. Leveraging this\ncomplementary expertise by arbitrating existing TSFM outputs presents a\ncompelling strategy, yet this remains a largely unexplored area of research. In\nthis paper, we conduct a thorough examination of how different TSFMs exhibit\nspecialized performance profiles across various forecasting settings, and how\nwe can effectively leverage this behavior in arbitration between different time\nseries models. We specifically analyze how factors such as model selection and\nforecast horizon distribution can influence the efficacy of arbitration\nstrategies. Based on this analysis, we propose Synapse, a novel arbitration\nframework for TSFMs. Synapse is designed to dynamically leverage a pool of\nTSFMs, assign and adjust predictive weights based on their relative,\ncontext-dependent performance, and construct a robust forecast distribution by\nadaptively sampling from the output quantiles of constituent models.\nExperimental results demonstrate that Synapse consistently outperforms other\npopular ensembling techniques as well as individual TSFMs, demonstrating\nSynapse's efficacy in time series forecasting.\n", "link": "http://arxiv.org/abs/2511.05460v1", "date": "2025-11-07", "relevancy": 1.8423, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4603}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synapse%3A%20Adaptive%20Arbitration%20of%20Complementary%20Expertise%20in%20Time%20Series%0A%20%20Foundational%20Models&body=Title%3A%20Synapse%3A%20Adaptive%20Arbitration%20of%20Complementary%20Expertise%20in%20Time%20Series%0A%20%20Foundational%20Models%0AAuthor%3A%20Sarkar%20Snigdha%20Sarathi%20Das%20and%20Palash%20Goyal%20and%20Mihir%20Parmar%20and%20Yiwen%20Song%20and%20Long%20T.%20Le%20and%20Lesly%20Miculicich%20and%20Jinsung%20Yoon%20and%20Rui%20Zhang%20and%20Hamid%20Palangi%20and%20Tomas%20Pfister%0AAbstract%3A%20%20%20Pre-trained%20Time%20Series%20Foundational%20Models%20%28TSFMs%29%20represent%20a%20significant%0Aadvance%2C%20capable%20of%20forecasting%20diverse%20time%20series%20with%20complex%0Acharacteristics%2C%20including%20varied%20seasonalities%2C%20trends%2C%20and%20long-range%0Adependencies.%20Despite%20their%20primary%20goal%20of%20universal%20time%20series%20forecasting%2C%0Atheir%20efficacy%20is%20far%20from%20uniform%3B%20divergent%20training%20protocols%20and%20data%0Asources%20cause%20individual%20TSFMs%20to%20exhibit%20highly%20variable%20performance%20across%0Adifferent%20forecasting%20tasks%2C%20domains%2C%20and%20horizons.%20Leveraging%20this%0Acomplementary%20expertise%20by%20arbitrating%20existing%20TSFM%20outputs%20presents%20a%0Acompelling%20strategy%2C%20yet%20this%20remains%20a%20largely%20unexplored%20area%20of%20research.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20examination%20of%20how%20different%20TSFMs%20exhibit%0Aspecialized%20performance%20profiles%20across%20various%20forecasting%20settings%2C%20and%20how%0Awe%20can%20effectively%20leverage%20this%20behavior%20in%20arbitration%20between%20different%20time%0Aseries%20models.%20We%20specifically%20analyze%20how%20factors%20such%20as%20model%20selection%20and%0Aforecast%20horizon%20distribution%20can%20influence%20the%20efficacy%20of%20arbitration%0Astrategies.%20Based%20on%20this%20analysis%2C%20we%20propose%20Synapse%2C%20a%20novel%20arbitration%0Aframework%20for%20TSFMs.%20Synapse%20is%20designed%20to%20dynamically%20leverage%20a%20pool%20of%0ATSFMs%2C%20assign%20and%20adjust%20predictive%20weights%20based%20on%20their%20relative%2C%0Acontext-dependent%20performance%2C%20and%20construct%20a%20robust%20forecast%20distribution%20by%0Aadaptively%20sampling%20from%20the%20output%20quantiles%20of%20constituent%20models.%0AExperimental%20results%20demonstrate%20that%20Synapse%20consistently%20outperforms%20other%0Apopular%20ensembling%20techniques%20as%20well%20as%20individual%20TSFMs%2C%20demonstrating%0ASynapse%27s%20efficacy%20in%20time%20series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynapse%253A%2520Adaptive%2520Arbitration%2520of%2520Complementary%2520Expertise%2520in%2520Time%2520Series%250A%2520%2520Foundational%2520Models%26entry.906535625%3DSarkar%2520Snigdha%2520Sarathi%2520Das%2520and%2520Palash%2520Goyal%2520and%2520Mihir%2520Parmar%2520and%2520Yiwen%2520Song%2520and%2520Long%2520T.%2520Le%2520and%2520Lesly%2520Miculicich%2520and%2520Jinsung%2520Yoon%2520and%2520Rui%2520Zhang%2520and%2520Hamid%2520Palangi%2520and%2520Tomas%2520Pfister%26entry.1292438233%3D%2520%2520Pre-trained%2520Time%2520Series%2520Foundational%2520Models%2520%2528TSFMs%2529%2520represent%2520a%2520significant%250Aadvance%252C%2520capable%2520of%2520forecasting%2520diverse%2520time%2520series%2520with%2520complex%250Acharacteristics%252C%2520including%2520varied%2520seasonalities%252C%2520trends%252C%2520and%2520long-range%250Adependencies.%2520Despite%2520their%2520primary%2520goal%2520of%2520universal%2520time%2520series%2520forecasting%252C%250Atheir%2520efficacy%2520is%2520far%2520from%2520uniform%253B%2520divergent%2520training%2520protocols%2520and%2520data%250Asources%2520cause%2520individual%2520TSFMs%2520to%2520exhibit%2520highly%2520variable%2520performance%2520across%250Adifferent%2520forecasting%2520tasks%252C%2520domains%252C%2520and%2520horizons.%2520Leveraging%2520this%250Acomplementary%2520expertise%2520by%2520arbitrating%2520existing%2520TSFM%2520outputs%2520presents%2520a%250Acompelling%2520strategy%252C%2520yet%2520this%2520remains%2520a%2520largely%2520unexplored%2520area%2520of%2520research.%2520In%250Athis%2520paper%252C%2520we%2520conduct%2520a%2520thorough%2520examination%2520of%2520how%2520different%2520TSFMs%2520exhibit%250Aspecialized%2520performance%2520profiles%2520across%2520various%2520forecasting%2520settings%252C%2520and%2520how%250Awe%2520can%2520effectively%2520leverage%2520this%2520behavior%2520in%2520arbitration%2520between%2520different%2520time%250Aseries%2520models.%2520We%2520specifically%2520analyze%2520how%2520factors%2520such%2520as%2520model%2520selection%2520and%250Aforecast%2520horizon%2520distribution%2520can%2520influence%2520the%2520efficacy%2520of%2520arbitration%250Astrategies.%2520Based%2520on%2520this%2520analysis%252C%2520we%2520propose%2520Synapse%252C%2520a%2520novel%2520arbitration%250Aframework%2520for%2520TSFMs.%2520Synapse%2520is%2520designed%2520to%2520dynamically%2520leverage%2520a%2520pool%2520of%250ATSFMs%252C%2520assign%2520and%2520adjust%2520predictive%2520weights%2520based%2520on%2520their%2520relative%252C%250Acontext-dependent%2520performance%252C%2520and%2520construct%2520a%2520robust%2520forecast%2520distribution%2520by%250Aadaptively%2520sampling%2520from%2520the%2520output%2520quantiles%2520of%2520constituent%2520models.%250AExperimental%2520results%2520demonstrate%2520that%2520Synapse%2520consistently%2520outperforms%2520other%250Apopular%2520ensembling%2520techniques%2520as%2520well%2520as%2520individual%2520TSFMs%252C%2520demonstrating%250ASynapse%2527s%2520efficacy%2520in%2520time%2520series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synapse%3A%20Adaptive%20Arbitration%20of%20Complementary%20Expertise%20in%20Time%20Series%0A%20%20Foundational%20Models&entry.906535625=Sarkar%20Snigdha%20Sarathi%20Das%20and%20Palash%20Goyal%20and%20Mihir%20Parmar%20and%20Yiwen%20Song%20and%20Long%20T.%20Le%20and%20Lesly%20Miculicich%20and%20Jinsung%20Yoon%20and%20Rui%20Zhang%20and%20Hamid%20Palangi%20and%20Tomas%20Pfister&entry.1292438233=%20%20Pre-trained%20Time%20Series%20Foundational%20Models%20%28TSFMs%29%20represent%20a%20significant%0Aadvance%2C%20capable%20of%20forecasting%20diverse%20time%20series%20with%20complex%0Acharacteristics%2C%20including%20varied%20seasonalities%2C%20trends%2C%20and%20long-range%0Adependencies.%20Despite%20their%20primary%20goal%20of%20universal%20time%20series%20forecasting%2C%0Atheir%20efficacy%20is%20far%20from%20uniform%3B%20divergent%20training%20protocols%20and%20data%0Asources%20cause%20individual%20TSFMs%20to%20exhibit%20highly%20variable%20performance%20across%0Adifferent%20forecasting%20tasks%2C%20domains%2C%20and%20horizons.%20Leveraging%20this%0Acomplementary%20expertise%20by%20arbitrating%20existing%20TSFM%20outputs%20presents%20a%0Acompelling%20strategy%2C%20yet%20this%20remains%20a%20largely%20unexplored%20area%20of%20research.%20In%0Athis%20paper%2C%20we%20conduct%20a%20thorough%20examination%20of%20how%20different%20TSFMs%20exhibit%0Aspecialized%20performance%20profiles%20across%20various%20forecasting%20settings%2C%20and%20how%0Awe%20can%20effectively%20leverage%20this%20behavior%20in%20arbitration%20between%20different%20time%0Aseries%20models.%20We%20specifically%20analyze%20how%20factors%20such%20as%20model%20selection%20and%0Aforecast%20horizon%20distribution%20can%20influence%20the%20efficacy%20of%20arbitration%0Astrategies.%20Based%20on%20this%20analysis%2C%20we%20propose%20Synapse%2C%20a%20novel%20arbitration%0Aframework%20for%20TSFMs.%20Synapse%20is%20designed%20to%20dynamically%20leverage%20a%20pool%20of%0ATSFMs%2C%20assign%20and%20adjust%20predictive%20weights%20based%20on%20their%20relative%2C%0Acontext-dependent%20performance%2C%20and%20construct%20a%20robust%20forecast%20distribution%20by%0Aadaptively%20sampling%20from%20the%20output%20quantiles%20of%20constituent%20models.%0AExperimental%20results%20demonstrate%20that%20Synapse%20consistently%20outperforms%20other%0Apopular%20ensembling%20techniques%20as%20well%20as%20individual%20TSFMs%2C%20demonstrating%0ASynapse%27s%20efficacy%20in%20time%20series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05460v1&entry.124074799=Read"},
{"title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving\n  Building Damage from Sentinel-1 and Sentinel-2", "author": "Olivier Dietrich and Merlin Alfredsson and Emilia Arens and Nando Metzger and Torben Peters and Linus Scheibenreif and Jan Dirk Wegner and Konrad Schindler", "abstract": "  Natural disasters demand rapid damage assessment to guide humanitarian\nresponse. Here, we investigate whether medium-resolution Earth observation\nimages from the Copernicus program can support building damage assessment,\ncomplementing very-high resolution imagery with often limited availability. We\nintroduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from\nboth Sentinel-1 and Sentinel-2, spatially and temporally aligned with the\nestablished xBD benchmark. In a series of experiments, we demonstrate that\nbuilding damage can be detected and mapped rather well in many disaster\nscenarios, despite the moderate 10$\\,$m ground sampling distance. We also find\nthat, for damage mapping at that resolution, architectural sophistication does\nnot seem to bring much advantage: more complex model architectures tend to\nstruggle with generalization to unseen disasters, and geospatial foundation\nmodels bring little practical benefit. Our results suggest that Copernicus\nimages are a viable data source for rapid, wide-area damage assessment and\ncould play an important role alongside VHR imagery. We release the xBD-S12\ndataset, code, and trained models to support further research.\n", "link": "http://arxiv.org/abs/2511.05461v1", "date": "2025-11-07", "relevancy": 1.8391, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4656}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Potential%20of%20Copernicus%20Satellites%20for%20Disaster%20Response%3A%20Retrieving%0A%20%20Building%20Damage%20from%20Sentinel-1%20and%20Sentinel-2&body=Title%3A%20The%20Potential%20of%20Copernicus%20Satellites%20for%20Disaster%20Response%3A%20Retrieving%0A%20%20Building%20Damage%20from%20Sentinel-1%20and%20Sentinel-2%0AAuthor%3A%20Olivier%20Dietrich%20and%20Merlin%20Alfredsson%20and%20Emilia%20Arens%20and%20Nando%20Metzger%20and%20Torben%20Peters%20and%20Linus%20Scheibenreif%20and%20Jan%20Dirk%20Wegner%20and%20Konrad%20Schindler%0AAbstract%3A%20%20%20Natural%20disasters%20demand%20rapid%20damage%20assessment%20to%20guide%20humanitarian%0Aresponse.%20Here%2C%20we%20investigate%20whether%20medium-resolution%20Earth%20observation%0Aimages%20from%20the%20Copernicus%20program%20can%20support%20building%20damage%20assessment%2C%0Acomplementing%20very-high%20resolution%20imagery%20with%20often%20limited%20availability.%20We%0Aintroduce%20xBD-S12%2C%20a%20dataset%20of%2010%2C315%20pre-%20and%20post-disaster%20image%20pairs%20from%0Aboth%20Sentinel-1%20and%20Sentinel-2%2C%20spatially%20and%20temporally%20aligned%20with%20the%0Aestablished%20xBD%20benchmark.%20In%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%0Abuilding%20damage%20can%20be%20detected%20and%20mapped%20rather%20well%20in%20many%20disaster%0Ascenarios%2C%20despite%20the%20moderate%2010%24%5C%2C%24m%20ground%20sampling%20distance.%20We%20also%20find%0Athat%2C%20for%20damage%20mapping%20at%20that%20resolution%2C%20architectural%20sophistication%20does%0Anot%20seem%20to%20bring%20much%20advantage%3A%20more%20complex%20model%20architectures%20tend%20to%0Astruggle%20with%20generalization%20to%20unseen%20disasters%2C%20and%20geospatial%20foundation%0Amodels%20bring%20little%20practical%20benefit.%20Our%20results%20suggest%20that%20Copernicus%0Aimages%20are%20a%20viable%20data%20source%20for%20rapid%2C%20wide-area%20damage%20assessment%20and%0Acould%20play%20an%20important%20role%20alongside%20VHR%20imagery.%20We%20release%20the%20xBD-S12%0Adataset%2C%20code%2C%20and%20trained%20models%20to%20support%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Potential%2520of%2520Copernicus%2520Satellites%2520for%2520Disaster%2520Response%253A%2520Retrieving%250A%2520%2520Building%2520Damage%2520from%2520Sentinel-1%2520and%2520Sentinel-2%26entry.906535625%3DOlivier%2520Dietrich%2520and%2520Merlin%2520Alfredsson%2520and%2520Emilia%2520Arens%2520and%2520Nando%2520Metzger%2520and%2520Torben%2520Peters%2520and%2520Linus%2520Scheibenreif%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Konrad%2520Schindler%26entry.1292438233%3D%2520%2520Natural%2520disasters%2520demand%2520rapid%2520damage%2520assessment%2520to%2520guide%2520humanitarian%250Aresponse.%2520Here%252C%2520we%2520investigate%2520whether%2520medium-resolution%2520Earth%2520observation%250Aimages%2520from%2520the%2520Copernicus%2520program%2520can%2520support%2520building%2520damage%2520assessment%252C%250Acomplementing%2520very-high%2520resolution%2520imagery%2520with%2520often%2520limited%2520availability.%2520We%250Aintroduce%2520xBD-S12%252C%2520a%2520dataset%2520of%252010%252C315%2520pre-%2520and%2520post-disaster%2520image%2520pairs%2520from%250Aboth%2520Sentinel-1%2520and%2520Sentinel-2%252C%2520spatially%2520and%2520temporally%2520aligned%2520with%2520the%250Aestablished%2520xBD%2520benchmark.%2520In%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%250Abuilding%2520damage%2520can%2520be%2520detected%2520and%2520mapped%2520rather%2520well%2520in%2520many%2520disaster%250Ascenarios%252C%2520despite%2520the%2520moderate%252010%2524%255C%252C%2524m%2520ground%2520sampling%2520distance.%2520We%2520also%2520find%250Athat%252C%2520for%2520damage%2520mapping%2520at%2520that%2520resolution%252C%2520architectural%2520sophistication%2520does%250Anot%2520seem%2520to%2520bring%2520much%2520advantage%253A%2520more%2520complex%2520model%2520architectures%2520tend%2520to%250Astruggle%2520with%2520generalization%2520to%2520unseen%2520disasters%252C%2520and%2520geospatial%2520foundation%250Amodels%2520bring%2520little%2520practical%2520benefit.%2520Our%2520results%2520suggest%2520that%2520Copernicus%250Aimages%2520are%2520a%2520viable%2520data%2520source%2520for%2520rapid%252C%2520wide-area%2520damage%2520assessment%2520and%250Acould%2520play%2520an%2520important%2520role%2520alongside%2520VHR%2520imagery.%2520We%2520release%2520the%2520xBD-S12%250Adataset%252C%2520code%252C%2520and%2520trained%2520models%2520to%2520support%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Potential%20of%20Copernicus%20Satellites%20for%20Disaster%20Response%3A%20Retrieving%0A%20%20Building%20Damage%20from%20Sentinel-1%20and%20Sentinel-2&entry.906535625=Olivier%20Dietrich%20and%20Merlin%20Alfredsson%20and%20Emilia%20Arens%20and%20Nando%20Metzger%20and%20Torben%20Peters%20and%20Linus%20Scheibenreif%20and%20Jan%20Dirk%20Wegner%20and%20Konrad%20Schindler&entry.1292438233=%20%20Natural%20disasters%20demand%20rapid%20damage%20assessment%20to%20guide%20humanitarian%0Aresponse.%20Here%2C%20we%20investigate%20whether%20medium-resolution%20Earth%20observation%0Aimages%20from%20the%20Copernicus%20program%20can%20support%20building%20damage%20assessment%2C%0Acomplementing%20very-high%20resolution%20imagery%20with%20often%20limited%20availability.%20We%0Aintroduce%20xBD-S12%2C%20a%20dataset%20of%2010%2C315%20pre-%20and%20post-disaster%20image%20pairs%20from%0Aboth%20Sentinel-1%20and%20Sentinel-2%2C%20spatially%20and%20temporally%20aligned%20with%20the%0Aestablished%20xBD%20benchmark.%20In%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%0Abuilding%20damage%20can%20be%20detected%20and%20mapped%20rather%20well%20in%20many%20disaster%0Ascenarios%2C%20despite%20the%20moderate%2010%24%5C%2C%24m%20ground%20sampling%20distance.%20We%20also%20find%0Athat%2C%20for%20damage%20mapping%20at%20that%20resolution%2C%20architectural%20sophistication%20does%0Anot%20seem%20to%20bring%20much%20advantage%3A%20more%20complex%20model%20architectures%20tend%20to%0Astruggle%20with%20generalization%20to%20unseen%20disasters%2C%20and%20geospatial%20foundation%0Amodels%20bring%20little%20practical%20benefit.%20Our%20results%20suggest%20that%20Copernicus%0Aimages%20are%20a%20viable%20data%20source%20for%20rapid%2C%20wide-area%20damage%20assessment%20and%0Acould%20play%20an%20important%20role%20alongside%20VHR%20imagery.%20We%20release%20the%20xBD-S12%0Adataset%2C%20code%2C%20and%20trained%20models%20to%20support%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05461v1&entry.124074799=Read"},
{"title": "FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion\n  Models", "author": "Barbara Toniella Corradini and Mustafa Shukor and Paul Couairon and Guillaume Couairon and Franco Scarselli and Matthieu Cord", "abstract": "  Foundation models have exhibited unprecedented capabilities in tackling many\ndomains and tasks. Models such as CLIP are currently widely used to bridge\ncross-modal representations, and text-to-image diffusion models are arguably\nthe leading models in terms of realistic image generation. Image generative\nmodels are trained on massive datasets that provide them with powerful internal\nspatial representations. In this work, we explore the potential benefits of\nsuch representations, beyond image generation, in particular, for dense visual\nprediction tasks. We focus on the task of image segmentation, which is\ntraditionally solved by training models on closed-vocabulary datasets, with\npixel-level annotations. To avoid the annotation cost or training large\ndiffusion models, we constraint our setup to be zero-shot and training-free. In\na nutshell, our pipeline leverages different and relatively small-sized,\nopen-source foundation models for zero-shot open-vocabulary segmentation. The\npipeline is as follows: the image is passed to both a captioner model (i.e.\nBLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text\ndescription and visual representation, respectively. The features are clustered\nand binarized to obtain class agnostic masks for each object. These masks are\nthen mapped to a textual class, using the CLIP model to support\nopen-vocabulary. Finally, we add a refinement step that allows to obtain a more\nprecise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not\nrely on any training, outperforms many training-based approaches on both Pascal\nVOC and COCO datasets. In addition, we show very competitive results compared\nto the recent weakly-supervised segmentation approaches. We provide\ncomprehensive experiments showing the superiority of diffusion model features\ncompared to other pretrained models. Project page:\nhttps://bcorrad.github.io/freesegdiff/\n", "link": "http://arxiv.org/abs/2403.20105v2", "date": "2025-11-07", "relevancy": 1.8204, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6556}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6019}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models&body=Title%3A%20FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models%0AAuthor%3A%20Barbara%20Toniella%20Corradini%20and%20Mustafa%20Shukor%20and%20Paul%20Couairon%20and%20Guillaume%20Couairon%20and%20Franco%20Scarselli%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Foundation%20models%20have%20exhibited%20unprecedented%20capabilities%20in%20tackling%20many%0Adomains%20and%20tasks.%20Models%20such%20as%20CLIP%20are%20currently%20widely%20used%20to%20bridge%0Across-modal%20representations%2C%20and%20text-to-image%20diffusion%20models%20are%20arguably%0Athe%20leading%20models%20in%20terms%20of%20realistic%20image%20generation.%20Image%20generative%0Amodels%20are%20trained%20on%20massive%20datasets%20that%20provide%20them%20with%20powerful%20internal%0Aspatial%20representations.%20In%20this%20work%2C%20we%20explore%20the%20potential%20benefits%20of%0Asuch%20representations%2C%20beyond%20image%20generation%2C%20in%20particular%2C%20for%20dense%20visual%0Aprediction%20tasks.%20We%20focus%20on%20the%20task%20of%20image%20segmentation%2C%20which%20is%0Atraditionally%20solved%20by%20training%20models%20on%20closed-vocabulary%20datasets%2C%20with%0Apixel-level%20annotations.%20To%20avoid%20the%20annotation%20cost%20or%20training%20large%0Adiffusion%20models%2C%20we%20constraint%20our%20setup%20to%20be%20zero-shot%20and%20training-free.%20In%0Aa%20nutshell%2C%20our%20pipeline%20leverages%20different%20and%20relatively%20small-sized%2C%0Aopen-source%20foundation%20models%20for%20zero-shot%20open-vocabulary%20segmentation.%20The%0Apipeline%20is%20as%20follows%3A%20the%20image%20is%20passed%20to%20both%20a%20captioner%20model%20%28i.e.%0ABLIP%29%20and%20a%20diffusion%20model%20%28i.e.%2C%20Stable%20Diffusion%20Model%29%20to%20generate%20a%20text%0Adescription%20and%20visual%20representation%2C%20respectively.%20The%20features%20are%20clustered%0Aand%20binarized%20to%20obtain%20class%20agnostic%20masks%20for%20each%20object.%20These%20masks%20are%0Athen%20mapped%20to%20a%20textual%20class%2C%20using%20the%20CLIP%20model%20to%20support%0Aopen-vocabulary.%20Finally%2C%20we%20add%20a%20refinement%20step%20that%20allows%20to%20obtain%20a%20more%0Aprecise%20segmentation%20mask.%20Our%20approach%20%28dubbed%20FreeSeg-Diff%29%2C%20which%20does%20not%0Arely%20on%20any%20training%2C%20outperforms%20many%20training-based%20approaches%20on%20both%20Pascal%0AVOC%20and%20COCO%20datasets.%20In%20addition%2C%20we%20show%20very%20competitive%20results%20compared%0Ato%20the%20recent%20weakly-supervised%20segmentation%20approaches.%20We%20provide%0Acomprehensive%20experiments%20showing%20the%20superiority%20of%20diffusion%20model%20features%0Acompared%20to%20other%20pretrained%20models.%20Project%20page%3A%0Ahttps%3A//bcorrad.github.io/freesegdiff/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeSeg-Diff%253A%2520Training-Free%2520Open-Vocabulary%2520Segmentation%2520with%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DBarbara%2520Toniella%2520Corradini%2520and%2520Mustafa%2520Shukor%2520and%2520Paul%2520Couairon%2520and%2520Guillaume%2520Couairon%2520and%2520Franco%2520Scarselli%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520exhibited%2520unprecedented%2520capabilities%2520in%2520tackling%2520many%250Adomains%2520and%2520tasks.%2520Models%2520such%2520as%2520CLIP%2520are%2520currently%2520widely%2520used%2520to%2520bridge%250Across-modal%2520representations%252C%2520and%2520text-to-image%2520diffusion%2520models%2520are%2520arguably%250Athe%2520leading%2520models%2520in%2520terms%2520of%2520realistic%2520image%2520generation.%2520Image%2520generative%250Amodels%2520are%2520trained%2520on%2520massive%2520datasets%2520that%2520provide%2520them%2520with%2520powerful%2520internal%250Aspatial%2520representations.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520potential%2520benefits%2520of%250Asuch%2520representations%252C%2520beyond%2520image%2520generation%252C%2520in%2520particular%252C%2520for%2520dense%2520visual%250Aprediction%2520tasks.%2520We%2520focus%2520on%2520the%2520task%2520of%2520image%2520segmentation%252C%2520which%2520is%250Atraditionally%2520solved%2520by%2520training%2520models%2520on%2520closed-vocabulary%2520datasets%252C%2520with%250Apixel-level%2520annotations.%2520To%2520avoid%2520the%2520annotation%2520cost%2520or%2520training%2520large%250Adiffusion%2520models%252C%2520we%2520constraint%2520our%2520setup%2520to%2520be%2520zero-shot%2520and%2520training-free.%2520In%250Aa%2520nutshell%252C%2520our%2520pipeline%2520leverages%2520different%2520and%2520relatively%2520small-sized%252C%250Aopen-source%2520foundation%2520models%2520for%2520zero-shot%2520open-vocabulary%2520segmentation.%2520The%250Apipeline%2520is%2520as%2520follows%253A%2520the%2520image%2520is%2520passed%2520to%2520both%2520a%2520captioner%2520model%2520%2528i.e.%250ABLIP%2529%2520and%2520a%2520diffusion%2520model%2520%2528i.e.%252C%2520Stable%2520Diffusion%2520Model%2529%2520to%2520generate%2520a%2520text%250Adescription%2520and%2520visual%2520representation%252C%2520respectively.%2520The%2520features%2520are%2520clustered%250Aand%2520binarized%2520to%2520obtain%2520class%2520agnostic%2520masks%2520for%2520each%2520object.%2520These%2520masks%2520are%250Athen%2520mapped%2520to%2520a%2520textual%2520class%252C%2520using%2520the%2520CLIP%2520model%2520to%2520support%250Aopen-vocabulary.%2520Finally%252C%2520we%2520add%2520a%2520refinement%2520step%2520that%2520allows%2520to%2520obtain%2520a%2520more%250Aprecise%2520segmentation%2520mask.%2520Our%2520approach%2520%2528dubbed%2520FreeSeg-Diff%2529%252C%2520which%2520does%2520not%250Arely%2520on%2520any%2520training%252C%2520outperforms%2520many%2520training-based%2520approaches%2520on%2520both%2520Pascal%250AVOC%2520and%2520COCO%2520datasets.%2520In%2520addition%252C%2520we%2520show%2520very%2520competitive%2520results%2520compared%250Ato%2520the%2520recent%2520weakly-supervised%2520segmentation%2520approaches.%2520We%2520provide%250Acomprehensive%2520experiments%2520showing%2520the%2520superiority%2520of%2520diffusion%2520model%2520features%250Acompared%2520to%2520other%2520pretrained%2520models.%2520Project%2520page%253A%250Ahttps%253A//bcorrad.github.io/freesegdiff/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeSeg-Diff%3A%20Training-Free%20Open-Vocabulary%20Segmentation%20with%20Diffusion%0A%20%20Models&entry.906535625=Barbara%20Toniella%20Corradini%20and%20Mustafa%20Shukor%20and%20Paul%20Couairon%20and%20Guillaume%20Couairon%20and%20Franco%20Scarselli%20and%20Matthieu%20Cord&entry.1292438233=%20%20Foundation%20models%20have%20exhibited%20unprecedented%20capabilities%20in%20tackling%20many%0Adomains%20and%20tasks.%20Models%20such%20as%20CLIP%20are%20currently%20widely%20used%20to%20bridge%0Across-modal%20representations%2C%20and%20text-to-image%20diffusion%20models%20are%20arguably%0Athe%20leading%20models%20in%20terms%20of%20realistic%20image%20generation.%20Image%20generative%0Amodels%20are%20trained%20on%20massive%20datasets%20that%20provide%20them%20with%20powerful%20internal%0Aspatial%20representations.%20In%20this%20work%2C%20we%20explore%20the%20potential%20benefits%20of%0Asuch%20representations%2C%20beyond%20image%20generation%2C%20in%20particular%2C%20for%20dense%20visual%0Aprediction%20tasks.%20We%20focus%20on%20the%20task%20of%20image%20segmentation%2C%20which%20is%0Atraditionally%20solved%20by%20training%20models%20on%20closed-vocabulary%20datasets%2C%20with%0Apixel-level%20annotations.%20To%20avoid%20the%20annotation%20cost%20or%20training%20large%0Adiffusion%20models%2C%20we%20constraint%20our%20setup%20to%20be%20zero-shot%20and%20training-free.%20In%0Aa%20nutshell%2C%20our%20pipeline%20leverages%20different%20and%20relatively%20small-sized%2C%0Aopen-source%20foundation%20models%20for%20zero-shot%20open-vocabulary%20segmentation.%20The%0Apipeline%20is%20as%20follows%3A%20the%20image%20is%20passed%20to%20both%20a%20captioner%20model%20%28i.e.%0ABLIP%29%20and%20a%20diffusion%20model%20%28i.e.%2C%20Stable%20Diffusion%20Model%29%20to%20generate%20a%20text%0Adescription%20and%20visual%20representation%2C%20respectively.%20The%20features%20are%20clustered%0Aand%20binarized%20to%20obtain%20class%20agnostic%20masks%20for%20each%20object.%20These%20masks%20are%0Athen%20mapped%20to%20a%20textual%20class%2C%20using%20the%20CLIP%20model%20to%20support%0Aopen-vocabulary.%20Finally%2C%20we%20add%20a%20refinement%20step%20that%20allows%20to%20obtain%20a%20more%0Aprecise%20segmentation%20mask.%20Our%20approach%20%28dubbed%20FreeSeg-Diff%29%2C%20which%20does%20not%0Arely%20on%20any%20training%2C%20outperforms%20many%20training-based%20approaches%20on%20both%20Pascal%0AVOC%20and%20COCO%20datasets.%20In%20addition%2C%20we%20show%20very%20competitive%20results%20compared%0Ato%20the%20recent%20weakly-supervised%20segmentation%20approaches.%20We%20provide%0Acomprehensive%20experiments%20showing%20the%20superiority%20of%20diffusion%20model%20features%0Acompared%20to%20other%20pretrained%20models.%20Project%20page%3A%0Ahttps%3A//bcorrad.github.io/freesegdiff/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20105v2&entry.124074799=Read"},
{"title": "Stochastic Approximation with Unbounded Markovian Noise: A\n  General-Purpose Theorem", "author": "Shaan Ul Haque and Siva Theja Maguluri", "abstract": "  Motivated by engineering applications such as resource allocation in networks\nand inventory systems, we consider average-reward Reinforcement Learning with\nunbounded state space and reward function. Recent works studied this problem in\nthe actor-critic framework and established finite sample bounds assuming access\nto a critic with certain error guarantees. We complement their work by studying\nTemporal Difference (TD) learning with linear function approximation and\nestablishing finite-time bounds with the optimal\n$\\mathcal{O}\\left(1/\\epsilon^2\\right)$ sample complexity. These results are\nobtained using the following general-purpose theorem for non-linear Stochastic\nApproximation (SA).\n  Suppose that one constructs a Lyapunov function for a non-linear SA with\ncertain drift condition. Then, our theorem establishes finite-time bounds when\nthis SA is driven by unbounded Markovian noise under suitable conditions. It\nserves as a black box tool to generalize sample guarantees on SA from i.i.d. or\nmartingale difference case to potentially unbounded Markovian noise. The\ngenerality and the mild assumption of the setup enables broad applicability of\nour theorem. We illustrate its power by studying two more systems: (i) We\nimprove upon the finite-time bounds of $Q$-learning by tightening the error\nbounds and also allowing for a larger class of behavior policies. (ii) We\nestablish the first ever finite-time bounds for distributed stochastic\noptimization of high-dimensional smooth strongly convex function using cyclic\nblock coordinate descent.\n", "link": "http://arxiv.org/abs/2410.21704v2", "date": "2025-11-07", "relevancy": 1.8157, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.45}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Approximation%20with%20Unbounded%20Markovian%20Noise%3A%20A%0A%20%20General-Purpose%20Theorem&body=Title%3A%20Stochastic%20Approximation%20with%20Unbounded%20Markovian%20Noise%3A%20A%0A%20%20General-Purpose%20Theorem%0AAuthor%3A%20Shaan%20Ul%20Haque%20and%20Siva%20Theja%20Maguluri%0AAbstract%3A%20%20%20Motivated%20by%20engineering%20applications%20such%20as%20resource%20allocation%20in%20networks%0Aand%20inventory%20systems%2C%20we%20consider%20average-reward%20Reinforcement%20Learning%20with%0Aunbounded%20state%20space%20and%20reward%20function.%20Recent%20works%20studied%20this%20problem%20in%0Athe%20actor-critic%20framework%20and%20established%20finite%20sample%20bounds%20assuming%20access%0Ato%20a%20critic%20with%20certain%20error%20guarantees.%20We%20complement%20their%20work%20by%20studying%0ATemporal%20Difference%20%28TD%29%20learning%20with%20linear%20function%20approximation%20and%0Aestablishing%20finite-time%20bounds%20with%20the%20optimal%0A%24%5Cmathcal%7BO%7D%5Cleft%281/%5Cepsilon%5E2%5Cright%29%24%20sample%20complexity.%20These%20results%20are%0Aobtained%20using%20the%20following%20general-purpose%20theorem%20for%20non-linear%20Stochastic%0AApproximation%20%28SA%29.%0A%20%20Suppose%20that%20one%20constructs%20a%20Lyapunov%20function%20for%20a%20non-linear%20SA%20with%0Acertain%20drift%20condition.%20Then%2C%20our%20theorem%20establishes%20finite-time%20bounds%20when%0Athis%20SA%20is%20driven%20by%20unbounded%20Markovian%20noise%20under%20suitable%20conditions.%20It%0Aserves%20as%20a%20black%20box%20tool%20to%20generalize%20sample%20guarantees%20on%20SA%20from%20i.i.d.%20or%0Amartingale%20difference%20case%20to%20potentially%20unbounded%20Markovian%20noise.%20The%0Agenerality%20and%20the%20mild%20assumption%20of%20the%20setup%20enables%20broad%20applicability%20of%0Aour%20theorem.%20We%20illustrate%20its%20power%20by%20studying%20two%20more%20systems%3A%20%28i%29%20We%0Aimprove%20upon%20the%20finite-time%20bounds%20of%20%24Q%24-learning%20by%20tightening%20the%20error%0Abounds%20and%20also%20allowing%20for%20a%20larger%20class%20of%20behavior%20policies.%20%28ii%29%20We%0Aestablish%20the%20first%20ever%20finite-time%20bounds%20for%20distributed%20stochastic%0Aoptimization%20of%20high-dimensional%20smooth%20strongly%20convex%20function%20using%20cyclic%0Ablock%20coordinate%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Approximation%2520with%2520Unbounded%2520Markovian%2520Noise%253A%2520A%250A%2520%2520General-Purpose%2520Theorem%26entry.906535625%3DShaan%2520Ul%2520Haque%2520and%2520Siva%2520Theja%2520Maguluri%26entry.1292438233%3D%2520%2520Motivated%2520by%2520engineering%2520applications%2520such%2520as%2520resource%2520allocation%2520in%2520networks%250Aand%2520inventory%2520systems%252C%2520we%2520consider%2520average-reward%2520Reinforcement%2520Learning%2520with%250Aunbounded%2520state%2520space%2520and%2520reward%2520function.%2520Recent%2520works%2520studied%2520this%2520problem%2520in%250Athe%2520actor-critic%2520framework%2520and%2520established%2520finite%2520sample%2520bounds%2520assuming%2520access%250Ato%2520a%2520critic%2520with%2520certain%2520error%2520guarantees.%2520We%2520complement%2520their%2520work%2520by%2520studying%250ATemporal%2520Difference%2520%2528TD%2529%2520learning%2520with%2520linear%2520function%2520approximation%2520and%250Aestablishing%2520finite-time%2520bounds%2520with%2520the%2520optimal%250A%2524%255Cmathcal%257BO%257D%255Cleft%25281/%255Cepsilon%255E2%255Cright%2529%2524%2520sample%2520complexity.%2520These%2520results%2520are%250Aobtained%2520using%2520the%2520following%2520general-purpose%2520theorem%2520for%2520non-linear%2520Stochastic%250AApproximation%2520%2528SA%2529.%250A%2520%2520Suppose%2520that%2520one%2520constructs%2520a%2520Lyapunov%2520function%2520for%2520a%2520non-linear%2520SA%2520with%250Acertain%2520drift%2520condition.%2520Then%252C%2520our%2520theorem%2520establishes%2520finite-time%2520bounds%2520when%250Athis%2520SA%2520is%2520driven%2520by%2520unbounded%2520Markovian%2520noise%2520under%2520suitable%2520conditions.%2520It%250Aserves%2520as%2520a%2520black%2520box%2520tool%2520to%2520generalize%2520sample%2520guarantees%2520on%2520SA%2520from%2520i.i.d.%2520or%250Amartingale%2520difference%2520case%2520to%2520potentially%2520unbounded%2520Markovian%2520noise.%2520The%250Agenerality%2520and%2520the%2520mild%2520assumption%2520of%2520the%2520setup%2520enables%2520broad%2520applicability%2520of%250Aour%2520theorem.%2520We%2520illustrate%2520its%2520power%2520by%2520studying%2520two%2520more%2520systems%253A%2520%2528i%2529%2520We%250Aimprove%2520upon%2520the%2520finite-time%2520bounds%2520of%2520%2524Q%2524-learning%2520by%2520tightening%2520the%2520error%250Abounds%2520and%2520also%2520allowing%2520for%2520a%2520larger%2520class%2520of%2520behavior%2520policies.%2520%2528ii%2529%2520We%250Aestablish%2520the%2520first%2520ever%2520finite-time%2520bounds%2520for%2520distributed%2520stochastic%250Aoptimization%2520of%2520high-dimensional%2520smooth%2520strongly%2520convex%2520function%2520using%2520cyclic%250Ablock%2520coordinate%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Approximation%20with%20Unbounded%20Markovian%20Noise%3A%20A%0A%20%20General-Purpose%20Theorem&entry.906535625=Shaan%20Ul%20Haque%20and%20Siva%20Theja%20Maguluri&entry.1292438233=%20%20Motivated%20by%20engineering%20applications%20such%20as%20resource%20allocation%20in%20networks%0Aand%20inventory%20systems%2C%20we%20consider%20average-reward%20Reinforcement%20Learning%20with%0Aunbounded%20state%20space%20and%20reward%20function.%20Recent%20works%20studied%20this%20problem%20in%0Athe%20actor-critic%20framework%20and%20established%20finite%20sample%20bounds%20assuming%20access%0Ato%20a%20critic%20with%20certain%20error%20guarantees.%20We%20complement%20their%20work%20by%20studying%0ATemporal%20Difference%20%28TD%29%20learning%20with%20linear%20function%20approximation%20and%0Aestablishing%20finite-time%20bounds%20with%20the%20optimal%0A%24%5Cmathcal%7BO%7D%5Cleft%281/%5Cepsilon%5E2%5Cright%29%24%20sample%20complexity.%20These%20results%20are%0Aobtained%20using%20the%20following%20general-purpose%20theorem%20for%20non-linear%20Stochastic%0AApproximation%20%28SA%29.%0A%20%20Suppose%20that%20one%20constructs%20a%20Lyapunov%20function%20for%20a%20non-linear%20SA%20with%0Acertain%20drift%20condition.%20Then%2C%20our%20theorem%20establishes%20finite-time%20bounds%20when%0Athis%20SA%20is%20driven%20by%20unbounded%20Markovian%20noise%20under%20suitable%20conditions.%20It%0Aserves%20as%20a%20black%20box%20tool%20to%20generalize%20sample%20guarantees%20on%20SA%20from%20i.i.d.%20or%0Amartingale%20difference%20case%20to%20potentially%20unbounded%20Markovian%20noise.%20The%0Agenerality%20and%20the%20mild%20assumption%20of%20the%20setup%20enables%20broad%20applicability%20of%0Aour%20theorem.%20We%20illustrate%20its%20power%20by%20studying%20two%20more%20systems%3A%20%28i%29%20We%0Aimprove%20upon%20the%20finite-time%20bounds%20of%20%24Q%24-learning%20by%20tightening%20the%20error%0Abounds%20and%20also%20allowing%20for%20a%20larger%20class%20of%20behavior%20policies.%20%28ii%29%20We%0Aestablish%20the%20first%20ever%20finite-time%20bounds%20for%20distributed%20stochastic%0Aoptimization%20of%20high-dimensional%20smooth%20strongly%20convex%20function%20using%20cyclic%0Ablock%20coordinate%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21704v2&entry.124074799=Read"},
{"title": "Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and\n  Collision Resilience", "author": "Luca Girardi and Gabriel Maquignaz and Stefano Mintchev", "abstract": "  Natural flyers use soft wings to seamlessly enable a wide range of flight\nbehaviours, including agile manoeuvres, squeezing through narrow passageways,\nand withstanding collisions. In contrast, conventional quadrotor designs rely\non rigid frames that support agile flight but inherently limit collision\nresilience and squeezability, thereby constraining flight capabilities in\ncluttered environments. Inspired by the anisotropic stiffness and distributed\nmass-energy structures observed in biological organisms, we introduce\nFlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.\nWe demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more\ncompliant than conventional quadrotors, yet capable of acrobatic manoeuvres\nwith peak speeds above 80 km/h and linear and angular accelerations exceeding 3\ng and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate\naccelerations of rigid counterparts up to a thrust-to-weight ratio of 8.\nSimultaneously, FlexiQuad exhibits fourfold higher collision resilience,\nsurviving frontal impacts at 5 m/s without damage and reducing destabilising\nforces in glancing collisions by a factor of 39. Its frame can fully compress,\nenabling flight through gaps as narrow as 70% of its nominal width. Our\nanalysis identifies an optimal structural softness range, from 0.006 to 0.77\nN/mm, comparable to that of natural flyers' wings, whereby agility,\nsqueezability, and collision resilience are jointly achieved for FlexiQuad\nmodels from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in\ncomplex environments, enabling robust physical interactions without\ncompromising flight performance.\n", "link": "http://arxiv.org/abs/2511.05426v1", "date": "2025-11-07", "relevancy": 1.7988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4483}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bioinspired%20Soft%20Quadrotors%20Jointly%20Unlock%20Agility%2C%20Squeezability%2C%20and%0A%20%20Collision%20Resilience&body=Title%3A%20Bioinspired%20Soft%20Quadrotors%20Jointly%20Unlock%20Agility%2C%20Squeezability%2C%20and%0A%20%20Collision%20Resilience%0AAuthor%3A%20Luca%20Girardi%20and%20Gabriel%20Maquignaz%20and%20Stefano%20Mintchev%0AAbstract%3A%20%20%20Natural%20flyers%20use%20soft%20wings%20to%20seamlessly%20enable%20a%20wide%20range%20of%20flight%0Abehaviours%2C%20including%20agile%20manoeuvres%2C%20squeezing%20through%20narrow%20passageways%2C%0Aand%20withstanding%20collisions.%20In%20contrast%2C%20conventional%20quadrotor%20designs%20rely%0Aon%20rigid%20frames%20that%20support%20agile%20flight%20but%20inherently%20limit%20collision%0Aresilience%20and%20squeezability%2C%20thereby%20constraining%20flight%20capabilities%20in%0Acluttered%20environments.%20Inspired%20by%20the%20anisotropic%20stiffness%20and%20distributed%0Amass-energy%20structures%20observed%20in%20biological%20organisms%2C%20we%20introduce%0AFlexiQuad%2C%20a%20soft-frame%20quadrotor%20design%20approach%20that%20limits%20this%20trade-off.%0AWe%20demonstrate%20a%20405-gram%20FlexiQuad%20prototype%2C%20three%20orders%20of%20magnitude%20more%0Acompliant%20than%20conventional%20quadrotors%2C%20yet%20capable%20of%20acrobatic%20manoeuvres%0Awith%20peak%20speeds%20above%2080%20km/h%20and%20linear%20and%20angular%20accelerations%20exceeding%203%0Ag%20and%20300%20rad/s%24%5E2%24%2C%20respectively.%20Analysis%20demonstrates%20it%20can%20replicate%0Aaccelerations%20of%20rigid%20counterparts%20up%20to%20a%20thrust-to-weight%20ratio%20of%208.%0ASimultaneously%2C%20FlexiQuad%20exhibits%20fourfold%20higher%20collision%20resilience%2C%0Asurviving%20frontal%20impacts%20at%205%20m/s%20without%20damage%20and%20reducing%20destabilising%0Aforces%20in%20glancing%20collisions%20by%20a%20factor%20of%2039.%20Its%20frame%20can%20fully%20compress%2C%0Aenabling%20flight%20through%20gaps%20as%20narrow%20as%2070%25%20of%20its%20nominal%20width.%20Our%0Aanalysis%20identifies%20an%20optimal%20structural%20softness%20range%2C%20from%200.006%20to%200.77%0AN/mm%2C%20comparable%20to%20that%20of%20natural%20flyers%27%20wings%2C%20whereby%20agility%2C%0Asqueezability%2C%20and%20collision%20resilience%20are%20jointly%20achieved%20for%20FlexiQuad%0Amodels%20from%2020%20to%203000%20grams.%20FlexiQuad%20expands%20hovering%20drone%20capabilities%20in%0Acomplex%20environments%2C%20enabling%20robust%20physical%20interactions%20without%0Acompromising%20flight%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioinspired%2520Soft%2520Quadrotors%2520Jointly%2520Unlock%2520Agility%252C%2520Squeezability%252C%2520and%250A%2520%2520Collision%2520Resilience%26entry.906535625%3DLuca%2520Girardi%2520and%2520Gabriel%2520Maquignaz%2520and%2520Stefano%2520Mintchev%26entry.1292438233%3D%2520%2520Natural%2520flyers%2520use%2520soft%2520wings%2520to%2520seamlessly%2520enable%2520a%2520wide%2520range%2520of%2520flight%250Abehaviours%252C%2520including%2520agile%2520manoeuvres%252C%2520squeezing%2520through%2520narrow%2520passageways%252C%250Aand%2520withstanding%2520collisions.%2520In%2520contrast%252C%2520conventional%2520quadrotor%2520designs%2520rely%250Aon%2520rigid%2520frames%2520that%2520support%2520agile%2520flight%2520but%2520inherently%2520limit%2520collision%250Aresilience%2520and%2520squeezability%252C%2520thereby%2520constraining%2520flight%2520capabilities%2520in%250Acluttered%2520environments.%2520Inspired%2520by%2520the%2520anisotropic%2520stiffness%2520and%2520distributed%250Amass-energy%2520structures%2520observed%2520in%2520biological%2520organisms%252C%2520we%2520introduce%250AFlexiQuad%252C%2520a%2520soft-frame%2520quadrotor%2520design%2520approach%2520that%2520limits%2520this%2520trade-off.%250AWe%2520demonstrate%2520a%2520405-gram%2520FlexiQuad%2520prototype%252C%2520three%2520orders%2520of%2520magnitude%2520more%250Acompliant%2520than%2520conventional%2520quadrotors%252C%2520yet%2520capable%2520of%2520acrobatic%2520manoeuvres%250Awith%2520peak%2520speeds%2520above%252080%2520km/h%2520and%2520linear%2520and%2520angular%2520accelerations%2520exceeding%25203%250Ag%2520and%2520300%2520rad/s%2524%255E2%2524%252C%2520respectively.%2520Analysis%2520demonstrates%2520it%2520can%2520replicate%250Aaccelerations%2520of%2520rigid%2520counterparts%2520up%2520to%2520a%2520thrust-to-weight%2520ratio%2520of%25208.%250ASimultaneously%252C%2520FlexiQuad%2520exhibits%2520fourfold%2520higher%2520collision%2520resilience%252C%250Asurviving%2520frontal%2520impacts%2520at%25205%2520m/s%2520without%2520damage%2520and%2520reducing%2520destabilising%250Aforces%2520in%2520glancing%2520collisions%2520by%2520a%2520factor%2520of%252039.%2520Its%2520frame%2520can%2520fully%2520compress%252C%250Aenabling%2520flight%2520through%2520gaps%2520as%2520narrow%2520as%252070%2525%2520of%2520its%2520nominal%2520width.%2520Our%250Aanalysis%2520identifies%2520an%2520optimal%2520structural%2520softness%2520range%252C%2520from%25200.006%2520to%25200.77%250AN/mm%252C%2520comparable%2520to%2520that%2520of%2520natural%2520flyers%2527%2520wings%252C%2520whereby%2520agility%252C%250Asqueezability%252C%2520and%2520collision%2520resilience%2520are%2520jointly%2520achieved%2520for%2520FlexiQuad%250Amodels%2520from%252020%2520to%25203000%2520grams.%2520FlexiQuad%2520expands%2520hovering%2520drone%2520capabilities%2520in%250Acomplex%2520environments%252C%2520enabling%2520robust%2520physical%2520interactions%2520without%250Acompromising%2520flight%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bioinspired%20Soft%20Quadrotors%20Jointly%20Unlock%20Agility%2C%20Squeezability%2C%20and%0A%20%20Collision%20Resilience&entry.906535625=Luca%20Girardi%20and%20Gabriel%20Maquignaz%20and%20Stefano%20Mintchev&entry.1292438233=%20%20Natural%20flyers%20use%20soft%20wings%20to%20seamlessly%20enable%20a%20wide%20range%20of%20flight%0Abehaviours%2C%20including%20agile%20manoeuvres%2C%20squeezing%20through%20narrow%20passageways%2C%0Aand%20withstanding%20collisions.%20In%20contrast%2C%20conventional%20quadrotor%20designs%20rely%0Aon%20rigid%20frames%20that%20support%20agile%20flight%20but%20inherently%20limit%20collision%0Aresilience%20and%20squeezability%2C%20thereby%20constraining%20flight%20capabilities%20in%0Acluttered%20environments.%20Inspired%20by%20the%20anisotropic%20stiffness%20and%20distributed%0Amass-energy%20structures%20observed%20in%20biological%20organisms%2C%20we%20introduce%0AFlexiQuad%2C%20a%20soft-frame%20quadrotor%20design%20approach%20that%20limits%20this%20trade-off.%0AWe%20demonstrate%20a%20405-gram%20FlexiQuad%20prototype%2C%20three%20orders%20of%20magnitude%20more%0Acompliant%20than%20conventional%20quadrotors%2C%20yet%20capable%20of%20acrobatic%20manoeuvres%0Awith%20peak%20speeds%20above%2080%20km/h%20and%20linear%20and%20angular%20accelerations%20exceeding%203%0Ag%20and%20300%20rad/s%24%5E2%24%2C%20respectively.%20Analysis%20demonstrates%20it%20can%20replicate%0Aaccelerations%20of%20rigid%20counterparts%20up%20to%20a%20thrust-to-weight%20ratio%20of%208.%0ASimultaneously%2C%20FlexiQuad%20exhibits%20fourfold%20higher%20collision%20resilience%2C%0Asurviving%20frontal%20impacts%20at%205%20m/s%20without%20damage%20and%20reducing%20destabilising%0Aforces%20in%20glancing%20collisions%20by%20a%20factor%20of%2039.%20Its%20frame%20can%20fully%20compress%2C%0Aenabling%20flight%20through%20gaps%20as%20narrow%20as%2070%25%20of%20its%20nominal%20width.%20Our%0Aanalysis%20identifies%20an%20optimal%20structural%20softness%20range%2C%20from%200.006%20to%200.77%0AN/mm%2C%20comparable%20to%20that%20of%20natural%20flyers%27%20wings%2C%20whereby%20agility%2C%0Asqueezability%2C%20and%20collision%20resilience%20are%20jointly%20achieved%20for%20FlexiQuad%0Amodels%20from%2020%20to%203000%20grams.%20FlexiQuad%20expands%20hovering%20drone%20capabilities%20in%0Acomplex%20environments%2C%20enabling%20robust%20physical%20interactions%20without%0Acompromising%20flight%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05426v1&entry.124074799=Read"},
{"title": "What Can String Probability Tell Us About Grammaticality?", "author": "Jennifer Hu and Ethan Gotlieb Wilcox and Siyuan Song and Kyle Mahowald and Roger P. Levy", "abstract": "  What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.\n", "link": "http://arxiv.org/abs/2510.16227v2", "date": "2025-11-07", "relevancy": 1.7433, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4363}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20String%20Probability%20Tell%20Us%20About%20Grammaticality%3F&body=Title%3A%20What%20Can%20String%20Probability%20Tell%20Us%20About%20Grammaticality%3F%0AAuthor%3A%20Jennifer%20Hu%20and%20Ethan%20Gotlieb%20Wilcox%20and%20Siyuan%20Song%20and%20Kyle%20Mahowald%20and%20Roger%20P.%20Levy%0AAbstract%3A%20%20%20What%20have%20language%20models%20%28LMs%29%20learned%20about%20grammar%3F%20This%20question%20remains%0Ahotly%20debated%2C%20with%20major%20ramifications%20for%20linguistic%20theory.%20However%2C%20since%0Aprobability%20and%20grammaticality%20are%20distinct%20notions%20in%20linguistics%2C%20it%20is%20not%0Aobvious%20what%20string%20probabilities%20can%20reveal%20about%20an%20LM%27s%20underlying%0Agrammatical%20knowledge.%20We%20present%20a%20theoretical%20analysis%20of%20the%20relationship%0Abetween%20grammar%2C%20meaning%2C%20and%20string%20probability%2C%20based%20on%20simple%20assumptions%0Aabout%20the%20generative%20process%20of%20corpus%20data.%20Our%20framework%20makes%20three%0Apredictions%2C%20which%20we%20validate%20empirically%20using%20280K%20sentence%20pairs%20in%20English%0Aand%20Chinese%3A%20%281%29%20correlation%20between%20the%20probability%20of%20strings%20within%20minimal%0Apairs%2C%20i.e.%2C%20string%20pairs%20with%20minimal%20semantic%20differences%3B%20%282%29%20correlation%0Abetween%20models%27%20and%20humans%27%20deltas%20within%20minimal%20pairs%3B%20and%20%283%29%20poor%0Aseparation%20in%20probability%20space%20between%20unpaired%20grammatical%20and%20ungrammatical%0Astrings.%20Our%20analyses%20give%20theoretical%20grounding%20for%20using%20probability%20to%20learn%0Aabout%20LMs%27%20structural%20knowledge%2C%20and%20suggest%20directions%20for%20future%20work%20in%20LM%0Agrammatical%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.16227v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520String%2520Probability%2520Tell%2520Us%2520About%2520Grammaticality%253F%26entry.906535625%3DJennifer%2520Hu%2520and%2520Ethan%2520Gotlieb%2520Wilcox%2520and%2520Siyuan%2520Song%2520and%2520Kyle%2520Mahowald%2520and%2520Roger%2520P.%2520Levy%26entry.1292438233%3D%2520%2520What%2520have%2520language%2520models%2520%2528LMs%2529%2520learned%2520about%2520grammar%253F%2520This%2520question%2520remains%250Ahotly%2520debated%252C%2520with%2520major%2520ramifications%2520for%2520linguistic%2520theory.%2520However%252C%2520since%250Aprobability%2520and%2520grammaticality%2520are%2520distinct%2520notions%2520in%2520linguistics%252C%2520it%2520is%2520not%250Aobvious%2520what%2520string%2520probabilities%2520can%2520reveal%2520about%2520an%2520LM%2527s%2520underlying%250Agrammatical%2520knowledge.%2520We%2520present%2520a%2520theoretical%2520analysis%2520of%2520the%2520relationship%250Abetween%2520grammar%252C%2520meaning%252C%2520and%2520string%2520probability%252C%2520based%2520on%2520simple%2520assumptions%250Aabout%2520the%2520generative%2520process%2520of%2520corpus%2520data.%2520Our%2520framework%2520makes%2520three%250Apredictions%252C%2520which%2520we%2520validate%2520empirically%2520using%2520280K%2520sentence%2520pairs%2520in%2520English%250Aand%2520Chinese%253A%2520%25281%2529%2520correlation%2520between%2520the%2520probability%2520of%2520strings%2520within%2520minimal%250Apairs%252C%2520i.e.%252C%2520string%2520pairs%2520with%2520minimal%2520semantic%2520differences%253B%2520%25282%2529%2520correlation%250Abetween%2520models%2527%2520and%2520humans%2527%2520deltas%2520within%2520minimal%2520pairs%253B%2520and%2520%25283%2529%2520poor%250Aseparation%2520in%2520probability%2520space%2520between%2520unpaired%2520grammatical%2520and%2520ungrammatical%250Astrings.%2520Our%2520analyses%2520give%2520theoretical%2520grounding%2520for%2520using%2520probability%2520to%2520learn%250Aabout%2520LMs%2527%2520structural%2520knowledge%252C%2520and%2520suggest%2520directions%2520for%2520future%2520work%2520in%2520LM%250Agrammatical%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16227v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20String%20Probability%20Tell%20Us%20About%20Grammaticality%3F&entry.906535625=Jennifer%20Hu%20and%20Ethan%20Gotlieb%20Wilcox%20and%20Siyuan%20Song%20and%20Kyle%20Mahowald%20and%20Roger%20P.%20Levy&entry.1292438233=%20%20What%20have%20language%20models%20%28LMs%29%20learned%20about%20grammar%3F%20This%20question%20remains%0Ahotly%20debated%2C%20with%20major%20ramifications%20for%20linguistic%20theory.%20However%2C%20since%0Aprobability%20and%20grammaticality%20are%20distinct%20notions%20in%20linguistics%2C%20it%20is%20not%0Aobvious%20what%20string%20probabilities%20can%20reveal%20about%20an%20LM%27s%20underlying%0Agrammatical%20knowledge.%20We%20present%20a%20theoretical%20analysis%20of%20the%20relationship%0Abetween%20grammar%2C%20meaning%2C%20and%20string%20probability%2C%20based%20on%20simple%20assumptions%0Aabout%20the%20generative%20process%20of%20corpus%20data.%20Our%20framework%20makes%20three%0Apredictions%2C%20which%20we%20validate%20empirically%20using%20280K%20sentence%20pairs%20in%20English%0Aand%20Chinese%3A%20%281%29%20correlation%20between%20the%20probability%20of%20strings%20within%20minimal%0Apairs%2C%20i.e.%2C%20string%20pairs%20with%20minimal%20semantic%20differences%3B%20%282%29%20correlation%0Abetween%20models%27%20and%20humans%27%20deltas%20within%20minimal%20pairs%3B%20and%20%283%29%20poor%0Aseparation%20in%20probability%20space%20between%20unpaired%20grammatical%20and%20ungrammatical%0Astrings.%20Our%20analyses%20give%20theoretical%20grounding%20for%20using%20probability%20to%20learn%0Aabout%20LMs%27%20structural%20knowledge%2C%20and%20suggest%20directions%20for%20future%20work%20in%20LM%0Agrammatical%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.16227v2&entry.124074799=Read"},
{"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language\n  Models", "author": "Yikun Ji and Yan Hong and Jiahui Zhan and Haoxing Chen and jun lan and Huijia Zhu and Weiqiang Wang and Liqing Zhang and Jianfu Zhang", "abstract": "  Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.\n", "link": "http://arxiv.org/abs/2504.14245v2", "date": "2025-11-07", "relevancy": 1.6885, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5702}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5629}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Explainable%20Fake%20Image%20Detection%20with%20Multi-Modal%20Large%20Language%0A%20%20Models&body=Title%3A%20Towards%20Explainable%20Fake%20Image%20Detection%20with%20Multi-Modal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yikun%20Ji%20and%20Yan%20Hong%20and%20Jiahui%20Zhan%20and%20Haoxing%20Chen%20and%20jun%20lan%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Liqing%20Zhang%20and%20Jianfu%20Zhang%0AAbstract%3A%20%20%20Progress%20in%20image%20generation%20raises%20significant%20public%20security%20concerns.%20We%0Aargue%20that%20fake%20image%20detection%20should%20not%20operate%20as%20a%20%22black%20box%22.%20Instead%2C%0Aan%20ideal%20approach%20must%20ensure%20both%20strong%20generalization%20and%20transparency.%0ARecent%20progress%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20offers%20new%0Aopportunities%20for%20reasoning-based%20AI-generated%20image%20detection.%20In%20this%20work%2C%0Awe%20evaluate%20the%20capabilities%20of%20MLLMs%20in%20comparison%20to%20traditional%20detection%0Amethods%20and%20human%20evaluators%2C%20highlighting%20their%20strengths%20and%20limitations.%0AFurthermore%2C%20we%20design%20six%20distinct%20prompts%20and%20propose%20a%20framework%20that%0Aintegrates%20these%20prompts%20to%20develop%20a%20more%20robust%2C%20explainable%2C%20and%0Areasoning-driven%20detection%20system.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Gennadiyev/mllm-defake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Explainable%2520Fake%2520Image%2520Detection%2520with%2520Multi-Modal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYikun%2520Ji%2520and%2520Yan%2520Hong%2520and%2520Jiahui%2520Zhan%2520and%2520Haoxing%2520Chen%2520and%2520jun%2520lan%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Liqing%2520Zhang%2520and%2520Jianfu%2520Zhang%26entry.1292438233%3D%2520%2520Progress%2520in%2520image%2520generation%2520raises%2520significant%2520public%2520security%2520concerns.%2520We%250Aargue%2520that%2520fake%2520image%2520detection%2520should%2520not%2520operate%2520as%2520a%2520%2522black%2520box%2522.%2520Instead%252C%250Aan%2520ideal%2520approach%2520must%2520ensure%2520both%2520strong%2520generalization%2520and%2520transparency.%250ARecent%2520progress%2520in%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520offers%2520new%250Aopportunities%2520for%2520reasoning-based%2520AI-generated%2520image%2520detection.%2520In%2520this%2520work%252C%250Awe%2520evaluate%2520the%2520capabilities%2520of%2520MLLMs%2520in%2520comparison%2520to%2520traditional%2520detection%250Amethods%2520and%2520human%2520evaluators%252C%2520highlighting%2520their%2520strengths%2520and%2520limitations.%250AFurthermore%252C%2520we%2520design%2520six%2520distinct%2520prompts%2520and%2520propose%2520a%2520framework%2520that%250Aintegrates%2520these%2520prompts%2520to%2520develop%2520a%2520more%2520robust%252C%2520explainable%252C%2520and%250Areasoning-driven%2520detection%2520system.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Gennadiyev/mllm-defake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Explainable%20Fake%20Image%20Detection%20with%20Multi-Modal%20Large%20Language%0A%20%20Models&entry.906535625=Yikun%20Ji%20and%20Yan%20Hong%20and%20Jiahui%20Zhan%20and%20Haoxing%20Chen%20and%20jun%20lan%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Liqing%20Zhang%20and%20Jianfu%20Zhang&entry.1292438233=%20%20Progress%20in%20image%20generation%20raises%20significant%20public%20security%20concerns.%20We%0Aargue%20that%20fake%20image%20detection%20should%20not%20operate%20as%20a%20%22black%20box%22.%20Instead%2C%0Aan%20ideal%20approach%20must%20ensure%20both%20strong%20generalization%20and%20transparency.%0ARecent%20progress%20in%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20offers%20new%0Aopportunities%20for%20reasoning-based%20AI-generated%20image%20detection.%20In%20this%20work%2C%0Awe%20evaluate%20the%20capabilities%20of%20MLLMs%20in%20comparison%20to%20traditional%20detection%0Amethods%20and%20human%20evaluators%2C%20highlighting%20their%20strengths%20and%20limitations.%0AFurthermore%2C%20we%20design%20six%20distinct%20prompts%20and%20propose%20a%20framework%20that%0Aintegrates%20these%20prompts%20to%20develop%20a%20more%20robust%2C%20explainable%2C%20and%0Areasoning-driven%20detection%20system.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Gennadiyev/mllm-defake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14245v2&entry.124074799=Read"},
{"title": "DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating\n  Mechanism for Enzyme DDG Prediction", "author": "Abigail Lin", "abstract": "  Predicting the effect of amino acid mutations on enzyme thermodynamic\nstability (DDG) is fundamental to protein engineering and drug design. While\nrecent deep learning approaches have shown promise, they often process sequence\nand structure information independently, failing to capture the intricate\ncoupling between local structural geometry and global sequential patterns. We\npresent DGTN (Diffused Graph-Transformer Network), a novel architecture that\nco-learns graph neural network (GNN) weights for structural priors and\ntransformer attention through a diffusion mechanism. Our key innovation is a\nbidirectional diffusion process where: (1) GNN-derived structural embeddings\nguide transformer attention via learnable diffusion kernels, and (2)\ntransformer representations refine GNN message passing through\nattention-modulated graph updates. We provide rigorous mathematical analysis\nshowing this co-learning scheme achieves provably better approximation bounds\nthan independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves\nstate-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with\n6.2% improvement over best baselines. Ablation studies confirm the diffusion\nmechanism contributes 4.8 points to correlation. Our theoretical analysis\nproves the diffused attention converges to optimal structure-sequence coupling,\nwith convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work\nestablishes a principled framework for integrating heterogeneous protein\nrepresentations through learnable diffusion.\n", "link": "http://arxiv.org/abs/2511.05483v1", "date": "2025-11-07", "relevancy": 1.6826, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.578}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5588}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGTN%3A%20Graph-Enhanced%20Transformer%20with%20Diffusive%20Attention%20Gating%0A%20%20Mechanism%20for%20Enzyme%20DDG%20Prediction&body=Title%3A%20DGTN%3A%20Graph-Enhanced%20Transformer%20with%20Diffusive%20Attention%20Gating%0A%20%20Mechanism%20for%20Enzyme%20DDG%20Prediction%0AAuthor%3A%20Abigail%20Lin%0AAbstract%3A%20%20%20Predicting%20the%20effect%20of%20amino%20acid%20mutations%20on%20enzyme%20thermodynamic%0Astability%20%28DDG%29%20is%20fundamental%20to%20protein%20engineering%20and%20drug%20design.%20While%0Arecent%20deep%20learning%20approaches%20have%20shown%20promise%2C%20they%20often%20process%20sequence%0Aand%20structure%20information%20independently%2C%20failing%20to%20capture%20the%20intricate%0Acoupling%20between%20local%20structural%20geometry%20and%20global%20sequential%20patterns.%20We%0Apresent%20DGTN%20%28Diffused%20Graph-Transformer%20Network%29%2C%20a%20novel%20architecture%20that%0Aco-learns%20graph%20neural%20network%20%28GNN%29%20weights%20for%20structural%20priors%20and%0Atransformer%20attention%20through%20a%20diffusion%20mechanism.%20Our%20key%20innovation%20is%20a%0Abidirectional%20diffusion%20process%20where%3A%20%281%29%20GNN-derived%20structural%20embeddings%0Aguide%20transformer%20attention%20via%20learnable%20diffusion%20kernels%2C%20and%20%282%29%0Atransformer%20representations%20refine%20GNN%20message%20passing%20through%0Aattention-modulated%20graph%20updates.%20We%20provide%20rigorous%20mathematical%20analysis%0Ashowing%20this%20co-learning%20scheme%20achieves%20provably%20better%20approximation%20bounds%0Athan%20independent%20processing.%20On%20ProTherm%20and%20SKEMPI%20benchmarks%2C%20DGTN%20achieves%0Astate-of-the-art%20performance%20%28Pearson%20Rho%20%3D%200.87%2C%20RMSE%20%3D%201.21%20kcal/mol%29%2C%20with%0A6.2%25%20improvement%20over%20best%20baselines.%20Ablation%20studies%20confirm%20the%20diffusion%0Amechanism%20contributes%204.8%20points%20to%20correlation.%20Our%20theoretical%20analysis%0Aproves%20the%20diffused%20attention%20converges%20to%20optimal%20structure-sequence%20coupling%2C%0Awith%20convergence%20rate%20O%281/sqrt%28T%29%20%29%20where%20T%20is%20diffusion%20steps.%20This%20work%0Aestablishes%20a%20principled%20framework%20for%20integrating%20heterogeneous%20protein%0Arepresentations%20through%20learnable%20diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGTN%253A%2520Graph-Enhanced%2520Transformer%2520with%2520Diffusive%2520Attention%2520Gating%250A%2520%2520Mechanism%2520for%2520Enzyme%2520DDG%2520Prediction%26entry.906535625%3DAbigail%2520Lin%26entry.1292438233%3D%2520%2520Predicting%2520the%2520effect%2520of%2520amino%2520acid%2520mutations%2520on%2520enzyme%2520thermodynamic%250Astability%2520%2528DDG%2529%2520is%2520fundamental%2520to%2520protein%2520engineering%2520and%2520drug%2520design.%2520While%250Arecent%2520deep%2520learning%2520approaches%2520have%2520shown%2520promise%252C%2520they%2520often%2520process%2520sequence%250Aand%2520structure%2520information%2520independently%252C%2520failing%2520to%2520capture%2520the%2520intricate%250Acoupling%2520between%2520local%2520structural%2520geometry%2520and%2520global%2520sequential%2520patterns.%2520We%250Apresent%2520DGTN%2520%2528Diffused%2520Graph-Transformer%2520Network%2529%252C%2520a%2520novel%2520architecture%2520that%250Aco-learns%2520graph%2520neural%2520network%2520%2528GNN%2529%2520weights%2520for%2520structural%2520priors%2520and%250Atransformer%2520attention%2520through%2520a%2520diffusion%2520mechanism.%2520Our%2520key%2520innovation%2520is%2520a%250Abidirectional%2520diffusion%2520process%2520where%253A%2520%25281%2529%2520GNN-derived%2520structural%2520embeddings%250Aguide%2520transformer%2520attention%2520via%2520learnable%2520diffusion%2520kernels%252C%2520and%2520%25282%2529%250Atransformer%2520representations%2520refine%2520GNN%2520message%2520passing%2520through%250Aattention-modulated%2520graph%2520updates.%2520We%2520provide%2520rigorous%2520mathematical%2520analysis%250Ashowing%2520this%2520co-learning%2520scheme%2520achieves%2520provably%2520better%2520approximation%2520bounds%250Athan%2520independent%2520processing.%2520On%2520ProTherm%2520and%2520SKEMPI%2520benchmarks%252C%2520DGTN%2520achieves%250Astate-of-the-art%2520performance%2520%2528Pearson%2520Rho%2520%253D%25200.87%252C%2520RMSE%2520%253D%25201.21%2520kcal/mol%2529%252C%2520with%250A6.2%2525%2520improvement%2520over%2520best%2520baselines.%2520Ablation%2520studies%2520confirm%2520the%2520diffusion%250Amechanism%2520contributes%25204.8%2520points%2520to%2520correlation.%2520Our%2520theoretical%2520analysis%250Aproves%2520the%2520diffused%2520attention%2520converges%2520to%2520optimal%2520structure-sequence%2520coupling%252C%250Awith%2520convergence%2520rate%2520O%25281/sqrt%2528T%2529%2520%2529%2520where%2520T%2520is%2520diffusion%2520steps.%2520This%2520work%250Aestablishes%2520a%2520principled%2520framework%2520for%2520integrating%2520heterogeneous%2520protein%250Arepresentations%2520through%2520learnable%2520diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGTN%3A%20Graph-Enhanced%20Transformer%20with%20Diffusive%20Attention%20Gating%0A%20%20Mechanism%20for%20Enzyme%20DDG%20Prediction&entry.906535625=Abigail%20Lin&entry.1292438233=%20%20Predicting%20the%20effect%20of%20amino%20acid%20mutations%20on%20enzyme%20thermodynamic%0Astability%20%28DDG%29%20is%20fundamental%20to%20protein%20engineering%20and%20drug%20design.%20While%0Arecent%20deep%20learning%20approaches%20have%20shown%20promise%2C%20they%20often%20process%20sequence%0Aand%20structure%20information%20independently%2C%20failing%20to%20capture%20the%20intricate%0Acoupling%20between%20local%20structural%20geometry%20and%20global%20sequential%20patterns.%20We%0Apresent%20DGTN%20%28Diffused%20Graph-Transformer%20Network%29%2C%20a%20novel%20architecture%20that%0Aco-learns%20graph%20neural%20network%20%28GNN%29%20weights%20for%20structural%20priors%20and%0Atransformer%20attention%20through%20a%20diffusion%20mechanism.%20Our%20key%20innovation%20is%20a%0Abidirectional%20diffusion%20process%20where%3A%20%281%29%20GNN-derived%20structural%20embeddings%0Aguide%20transformer%20attention%20via%20learnable%20diffusion%20kernels%2C%20and%20%282%29%0Atransformer%20representations%20refine%20GNN%20message%20passing%20through%0Aattention-modulated%20graph%20updates.%20We%20provide%20rigorous%20mathematical%20analysis%0Ashowing%20this%20co-learning%20scheme%20achieves%20provably%20better%20approximation%20bounds%0Athan%20independent%20processing.%20On%20ProTherm%20and%20SKEMPI%20benchmarks%2C%20DGTN%20achieves%0Astate-of-the-art%20performance%20%28Pearson%20Rho%20%3D%200.87%2C%20RMSE%20%3D%201.21%20kcal/mol%29%2C%20with%0A6.2%25%20improvement%20over%20best%20baselines.%20Ablation%20studies%20confirm%20the%20diffusion%0Amechanism%20contributes%204.8%20points%20to%20correlation.%20Our%20theoretical%20analysis%0Aproves%20the%20diffused%20attention%20converges%20to%20optimal%20structure-sequence%20coupling%2C%0Awith%20convergence%20rate%20O%281/sqrt%28T%29%20%29%20where%20T%20is%20diffusion%20steps.%20This%20work%0Aestablishes%20a%20principled%20framework%20for%20integrating%20heterogeneous%20protein%0Arepresentations%20through%20learnable%20diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05483v1&entry.124074799=Read"},
{"title": "Linear combinations of latents in generative models: subspaces and\n  beyond", "author": "Erik Bodin and Alexandru Stere and Dragos D. Margineantu and Carl Henrik Ek and Henry Moss", "abstract": "  Sampling from generative models has become a crucial tool for applications\nlike data synthesis and augmentation. Diffusion, Flow Matching and Continuous\nNormalising Flows have shown effectiveness across various modalities, and rely\non latent variables for generation. For experimental design or creative\napplications that require more control over the generation process, it has\nbecome common to manipulate the latent variable directly. However, existing\napproaches for performing such manipulations (e.g. interpolation or forming\nlow-dimensional representations) only work well in special cases or are network\nor data-modality specific. We propose Latent Optimal Linear combinations (LOL)\nas a general-purpose method to form linear combinations of latent variables\nthat adhere to the assumptions of the generative model. As LOL is easy to\nimplement and naturally addresses the broader task of forming any linear\ncombinations, e.g. the construction of subspaces of the latent space, LOL\ndramatically simplifies the creation of expressive low-dimensional\nrepresentations of high-dimensional objects.\n", "link": "http://arxiv.org/abs/2408.08558v8", "date": "2025-11-07", "relevancy": 1.6315, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6259}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5317}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20combinations%20of%20latents%20in%20generative%20models%3A%20subspaces%20and%0A%20%20beyond&body=Title%3A%20Linear%20combinations%20of%20latents%20in%20generative%20models%3A%20subspaces%20and%0A%20%20beyond%0AAuthor%3A%20Erik%20Bodin%20and%20Alexandru%20Stere%20and%20Dragos%20D.%20Margineantu%20and%20Carl%20Henrik%20Ek%20and%20Henry%20Moss%0AAbstract%3A%20%20%20Sampling%20from%20generative%20models%20has%20become%20a%20crucial%20tool%20for%20applications%0Alike%20data%20synthesis%20and%20augmentation.%20Diffusion%2C%20Flow%20Matching%20and%20Continuous%0ANormalising%20Flows%20have%20shown%20effectiveness%20across%20various%20modalities%2C%20and%20rely%0Aon%20latent%20variables%20for%20generation.%20For%20experimental%20design%20or%20creative%0Aapplications%20that%20require%20more%20control%20over%20the%20generation%20process%2C%20it%20has%0Abecome%20common%20to%20manipulate%20the%20latent%20variable%20directly.%20However%2C%20existing%0Aapproaches%20for%20performing%20such%20manipulations%20%28e.g.%20interpolation%20or%20forming%0Alow-dimensional%20representations%29%20only%20work%20well%20in%20special%20cases%20or%20are%20network%0Aor%20data-modality%20specific.%20We%20propose%20Latent%20Optimal%20Linear%20combinations%20%28LOL%29%0Aas%20a%20general-purpose%20method%20to%20form%20linear%20combinations%20of%20latent%20variables%0Athat%20adhere%20to%20the%20assumptions%20of%20the%20generative%20model.%20As%20LOL%20is%20easy%20to%0Aimplement%20and%20naturally%20addresses%20the%20broader%20task%20of%20forming%20any%20linear%0Acombinations%2C%20e.g.%20the%20construction%20of%20subspaces%20of%20the%20latent%20space%2C%20LOL%0Adramatically%20simplifies%20the%20creation%20of%20expressive%20low-dimensional%0Arepresentations%20of%20high-dimensional%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08558v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520combinations%2520of%2520latents%2520in%2520generative%2520models%253A%2520subspaces%2520and%250A%2520%2520beyond%26entry.906535625%3DErik%2520Bodin%2520and%2520Alexandru%2520Stere%2520and%2520Dragos%2520D.%2520Margineantu%2520and%2520Carl%2520Henrik%2520Ek%2520and%2520Henry%2520Moss%26entry.1292438233%3D%2520%2520Sampling%2520from%2520generative%2520models%2520has%2520become%2520a%2520crucial%2520tool%2520for%2520applications%250Alike%2520data%2520synthesis%2520and%2520augmentation.%2520Diffusion%252C%2520Flow%2520Matching%2520and%2520Continuous%250ANormalising%2520Flows%2520have%2520shown%2520effectiveness%2520across%2520various%2520modalities%252C%2520and%2520rely%250Aon%2520latent%2520variables%2520for%2520generation.%2520For%2520experimental%2520design%2520or%2520creative%250Aapplications%2520that%2520require%2520more%2520control%2520over%2520the%2520generation%2520process%252C%2520it%2520has%250Abecome%2520common%2520to%2520manipulate%2520the%2520latent%2520variable%2520directly.%2520However%252C%2520existing%250Aapproaches%2520for%2520performing%2520such%2520manipulations%2520%2528e.g.%2520interpolation%2520or%2520forming%250Alow-dimensional%2520representations%2529%2520only%2520work%2520well%2520in%2520special%2520cases%2520or%2520are%2520network%250Aor%2520data-modality%2520specific.%2520We%2520propose%2520Latent%2520Optimal%2520Linear%2520combinations%2520%2528LOL%2529%250Aas%2520a%2520general-purpose%2520method%2520to%2520form%2520linear%2520combinations%2520of%2520latent%2520variables%250Athat%2520adhere%2520to%2520the%2520assumptions%2520of%2520the%2520generative%2520model.%2520As%2520LOL%2520is%2520easy%2520to%250Aimplement%2520and%2520naturally%2520addresses%2520the%2520broader%2520task%2520of%2520forming%2520any%2520linear%250Acombinations%252C%2520e.g.%2520the%2520construction%2520of%2520subspaces%2520of%2520the%2520latent%2520space%252C%2520LOL%250Adramatically%2520simplifies%2520the%2520creation%2520of%2520expressive%2520low-dimensional%250Arepresentations%2520of%2520high-dimensional%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08558v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20combinations%20of%20latents%20in%20generative%20models%3A%20subspaces%20and%0A%20%20beyond&entry.906535625=Erik%20Bodin%20and%20Alexandru%20Stere%20and%20Dragos%20D.%20Margineantu%20and%20Carl%20Henrik%20Ek%20and%20Henry%20Moss&entry.1292438233=%20%20Sampling%20from%20generative%20models%20has%20become%20a%20crucial%20tool%20for%20applications%0Alike%20data%20synthesis%20and%20augmentation.%20Diffusion%2C%20Flow%20Matching%20and%20Continuous%0ANormalising%20Flows%20have%20shown%20effectiveness%20across%20various%20modalities%2C%20and%20rely%0Aon%20latent%20variables%20for%20generation.%20For%20experimental%20design%20or%20creative%0Aapplications%20that%20require%20more%20control%20over%20the%20generation%20process%2C%20it%20has%0Abecome%20common%20to%20manipulate%20the%20latent%20variable%20directly.%20However%2C%20existing%0Aapproaches%20for%20performing%20such%20manipulations%20%28e.g.%20interpolation%20or%20forming%0Alow-dimensional%20representations%29%20only%20work%20well%20in%20special%20cases%20or%20are%20network%0Aor%20data-modality%20specific.%20We%20propose%20Latent%20Optimal%20Linear%20combinations%20%28LOL%29%0Aas%20a%20general-purpose%20method%20to%20form%20linear%20combinations%20of%20latent%20variables%0Athat%20adhere%20to%20the%20assumptions%20of%20the%20generative%20model.%20As%20LOL%20is%20easy%20to%0Aimplement%20and%20naturally%20addresses%20the%20broader%20task%20of%20forming%20any%20linear%0Acombinations%2C%20e.g.%20the%20construction%20of%20subspaces%20of%20the%20latent%20space%2C%20LOL%0Adramatically%20simplifies%20the%20creation%20of%20expressive%20low-dimensional%0Arepresentations%20of%20high-dimensional%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08558v8&entry.124074799=Read"},
{"title": "Inference-Time Hyper-Scaling with KV Cache Compression", "author": "Adrian \u0141a\u0144cucki and Konrad Staniszewski and Piotr Nawrot and Edoardo M. Ponti", "abstract": "  Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads.\n", "link": "http://arxiv.org/abs/2506.05345v2", "date": "2025-11-07", "relevancy": 1.6294, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5557}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5301}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-Time%20Hyper-Scaling%20with%20KV%20Cache%20Compression&body=Title%3A%20Inference-Time%20Hyper-Scaling%20with%20KV%20Cache%20Compression%0AAuthor%3A%20Adrian%20%C5%81a%C5%84cucki%20and%20Konrad%20Staniszewski%20and%20Piotr%20Nawrot%20and%20Edoardo%20M.%20Ponti%0AAbstract%3A%20%20%20Inference-time%20scaling%20trades%20efficiency%20for%20increased%20reasoning%20accuracy%20by%0Agenerating%20longer%20or%20more%20parallel%20sequences.%20However%2C%20in%20Transformer%20LLMs%2C%0Ageneration%20cost%20is%20bottlenecked%20by%20the%20size%20of%20the%20key-value%20%28KV%29%20cache%2C%20rather%0Athan%20the%20number%20of%20generated%20tokens.%20Hence%2C%20we%20explore%20inference-time%0Ahyper-scaling%3A%20by%20compressing%20the%20KV%20cache%2C%20we%20can%20generate%20more%20tokens%20within%0Athe%20same%20compute%20budget%20and%20further%20improve%20the%20accuracy%20of%20scaled%20inference.%0AThe%20success%20of%20this%20approach%2C%20however%2C%20hinges%20on%20the%20ability%20of%20compression%0Amethods%20to%20preserve%20accuracy%20even%20at%20high%20compression%20ratios.%20To%20make%0Ahyper-scaling%20practical%2C%20we%20introduce%20Dynamic%20Memory%20Sparsification%20%28DMS%29%2C%20a%0Anovel%20method%20for%20sparsifying%20KV%20caches%20that%20only%20requires%201K%20training%20steps%20to%0Aachieve%208%24%5Ctimes%24%20compression%2C%20while%20maintaining%20better%20accuracy%20than%0Atraining-free%20sparse%20attention.%20Instead%20of%20prematurely%20discarding%20cached%0Atokens%2C%20DMS%20delays%20token%20eviction%2C%20implicitly%20merging%20representations%20and%0Apreserving%20critical%20information.%20We%20demonstrate%20the%20effectiveness%20of%0Ainference-time%20hyper-scaling%20with%20DMS%20on%20multiple%20families%20of%20LLMs%2C%20showing%0Athat%20it%20boosts%20accuracy%20for%20comparable%20inference%20latency%20and%20memory%20load.%20For%0Ainstance%2C%20we%20enhance%20Qwen-R1%2032B%20by%2012.0%20points%20on%20AIME%2024%2C%208.6%20on%20GPQA%2C%20and%0A9.7%20on%20LiveCodeBench%20on%20average%20for%20an%20equivalent%20number%20of%20memory%20reads.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-Time%2520Hyper-Scaling%2520with%2520KV%2520Cache%2520Compression%26entry.906535625%3DAdrian%2520%25C5%2581a%25C5%2584cucki%2520and%2520Konrad%2520Staniszewski%2520and%2520Piotr%2520Nawrot%2520and%2520Edoardo%2520M.%2520Ponti%26entry.1292438233%3D%2520%2520Inference-time%2520scaling%2520trades%2520efficiency%2520for%2520increased%2520reasoning%2520accuracy%2520by%250Agenerating%2520longer%2520or%2520more%2520parallel%2520sequences.%2520However%252C%2520in%2520Transformer%2520LLMs%252C%250Ageneration%2520cost%2520is%2520bottlenecked%2520by%2520the%2520size%2520of%2520the%2520key-value%2520%2528KV%2529%2520cache%252C%2520rather%250Athan%2520the%2520number%2520of%2520generated%2520tokens.%2520Hence%252C%2520we%2520explore%2520inference-time%250Ahyper-scaling%253A%2520by%2520compressing%2520the%2520KV%2520cache%252C%2520we%2520can%2520generate%2520more%2520tokens%2520within%250Athe%2520same%2520compute%2520budget%2520and%2520further%2520improve%2520the%2520accuracy%2520of%2520scaled%2520inference.%250AThe%2520success%2520of%2520this%2520approach%252C%2520however%252C%2520hinges%2520on%2520the%2520ability%2520of%2520compression%250Amethods%2520to%2520preserve%2520accuracy%2520even%2520at%2520high%2520compression%2520ratios.%2520To%2520make%250Ahyper-scaling%2520practical%252C%2520we%2520introduce%2520Dynamic%2520Memory%2520Sparsification%2520%2528DMS%2529%252C%2520a%250Anovel%2520method%2520for%2520sparsifying%2520KV%2520caches%2520that%2520only%2520requires%25201K%2520training%2520steps%2520to%250Aachieve%25208%2524%255Ctimes%2524%2520compression%252C%2520while%2520maintaining%2520better%2520accuracy%2520than%250Atraining-free%2520sparse%2520attention.%2520Instead%2520of%2520prematurely%2520discarding%2520cached%250Atokens%252C%2520DMS%2520delays%2520token%2520eviction%252C%2520implicitly%2520merging%2520representations%2520and%250Apreserving%2520critical%2520information.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%250Ainference-time%2520hyper-scaling%2520with%2520DMS%2520on%2520multiple%2520families%2520of%2520LLMs%252C%2520showing%250Athat%2520it%2520boosts%2520accuracy%2520for%2520comparable%2520inference%2520latency%2520and%2520memory%2520load.%2520For%250Ainstance%252C%2520we%2520enhance%2520Qwen-R1%252032B%2520by%252012.0%2520points%2520on%2520AIME%252024%252C%25208.6%2520on%2520GPQA%252C%2520and%250A9.7%2520on%2520LiveCodeBench%2520on%2520average%2520for%2520an%2520equivalent%2520number%2520of%2520memory%2520reads.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-Time%20Hyper-Scaling%20with%20KV%20Cache%20Compression&entry.906535625=Adrian%20%C5%81a%C5%84cucki%20and%20Konrad%20Staniszewski%20and%20Piotr%20Nawrot%20and%20Edoardo%20M.%20Ponti&entry.1292438233=%20%20Inference-time%20scaling%20trades%20efficiency%20for%20increased%20reasoning%20accuracy%20by%0Agenerating%20longer%20or%20more%20parallel%20sequences.%20However%2C%20in%20Transformer%20LLMs%2C%0Ageneration%20cost%20is%20bottlenecked%20by%20the%20size%20of%20the%20key-value%20%28KV%29%20cache%2C%20rather%0Athan%20the%20number%20of%20generated%20tokens.%20Hence%2C%20we%20explore%20inference-time%0Ahyper-scaling%3A%20by%20compressing%20the%20KV%20cache%2C%20we%20can%20generate%20more%20tokens%20within%0Athe%20same%20compute%20budget%20and%20further%20improve%20the%20accuracy%20of%20scaled%20inference.%0AThe%20success%20of%20this%20approach%2C%20however%2C%20hinges%20on%20the%20ability%20of%20compression%0Amethods%20to%20preserve%20accuracy%20even%20at%20high%20compression%20ratios.%20To%20make%0Ahyper-scaling%20practical%2C%20we%20introduce%20Dynamic%20Memory%20Sparsification%20%28DMS%29%2C%20a%0Anovel%20method%20for%20sparsifying%20KV%20caches%20that%20only%20requires%201K%20training%20steps%20to%0Aachieve%208%24%5Ctimes%24%20compression%2C%20while%20maintaining%20better%20accuracy%20than%0Atraining-free%20sparse%20attention.%20Instead%20of%20prematurely%20discarding%20cached%0Atokens%2C%20DMS%20delays%20token%20eviction%2C%20implicitly%20merging%20representations%20and%0Apreserving%20critical%20information.%20We%20demonstrate%20the%20effectiveness%20of%0Ainference-time%20hyper-scaling%20with%20DMS%20on%20multiple%20families%20of%20LLMs%2C%20showing%0Athat%20it%20boosts%20accuracy%20for%20comparable%20inference%20latency%20and%20memory%20load.%20For%0Ainstance%2C%20we%20enhance%20Qwen-R1%2032B%20by%2012.0%20points%20on%20AIME%2024%2C%208.6%20on%20GPQA%2C%20and%0A9.7%20on%20LiveCodeBench%20on%20average%20for%20an%20equivalent%20number%20of%20memory%20reads.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05345v2&entry.124074799=Read"},
{"title": "Precipitation nowcasting of satellite data using physically conditioned\n  neural networks", "author": "Ant\u00f4nio Cat\u00e3o and Melvin Poveda and Leonardo Voltarelli and Paulo Orenstein", "abstract": "  Accurate short-term precipitation forecasts predominantly rely on dense\nweather-radar networks, limiting operational value in places most exposed to\nclimate extremes. We present TUPANN (Transferable and Universal Physics-Aligned\nNowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike\nmost deep learning models for nowcasting, TUPANN decomposes the forecast into\nphysically meaningful components: a variational encoder-decoder infers motion\nand intensity fields from recent imagery under optical-flow supervision, a\nlead-time-conditioned MaxViT evolves the latent state, and a differentiable\nadvection operator reconstructs future frames. We evaluate TUPANN on both\nGOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,\nManaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics\nover 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and\nhybrid baselines show that TUPANN achieves the best or second-best skill in\nmost settings, with pronounced gains at higher thresholds. Training on multiple\ncities further improves performance, while cross-city experiments show modest\ndegradation and occasional gains for rare heavy-rain regimes. The model\nproduces smooth, interpretable motion fields aligned with numerical optical\nflow and runs in near real time due to the low latency of GOES-16. These\nresults indicate that physically aligned learning can provide nowcasts that are\nskillful, transferable and global.\n", "link": "http://arxiv.org/abs/2511.05471v1", "date": "2025-11-07", "relevancy": 1.5563, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5349}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5174}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precipitation%20nowcasting%20of%20satellite%20data%20using%20physically%20conditioned%0A%20%20neural%20networks&body=Title%3A%20Precipitation%20nowcasting%20of%20satellite%20data%20using%20physically%20conditioned%0A%20%20neural%20networks%0AAuthor%3A%20Ant%C3%B4nio%20Cat%C3%A3o%20and%20Melvin%20Poveda%20and%20Leonardo%20Voltarelli%20and%20Paulo%20Orenstein%0AAbstract%3A%20%20%20Accurate%20short-term%20precipitation%20forecasts%20predominantly%20rely%20on%20dense%0Aweather-radar%20networks%2C%20limiting%20operational%20value%20in%20places%20most%20exposed%20to%0Aclimate%20extremes.%20We%20present%20TUPANN%20%28Transferable%20and%20Universal%20Physics-Aligned%0ANowcasting%20Network%29%2C%20a%20satellite-only%20model%20trained%20on%20GOES-16%20RRQPE.%20Unlike%0Amost%20deep%20learning%20models%20for%20nowcasting%2C%20TUPANN%20decomposes%20the%20forecast%20into%0Aphysically%20meaningful%20components%3A%20a%20variational%20encoder-decoder%20infers%20motion%0Aand%20intensity%20fields%20from%20recent%20imagery%20under%20optical-flow%20supervision%2C%20a%0Alead-time-conditioned%20MaxViT%20evolves%20the%20latent%20state%2C%20and%20a%20differentiable%0Aadvection%20operator%20reconstructs%20future%20frames.%20We%20evaluate%20TUPANN%20on%20both%0AGOES-16%20and%20IMERG%20data%2C%20in%20up%20to%20four%20distinct%20climates%20%28Rio%20de%20Janeiro%2C%0AManaus%2C%20Miami%2C%20La%20Paz%29%20at%2010-180min%20lead%20times%20using%20the%20CSI%20and%20HSS%20metrics%0Aover%204-64%20mm/h%20thresholds.%20Comparisons%20against%20optical-flow%2C%20deep%20learning%20and%0Ahybrid%20baselines%20show%20that%20TUPANN%20achieves%20the%20best%20or%20second-best%20skill%20in%0Amost%20settings%2C%20with%20pronounced%20gains%20at%20higher%20thresholds.%20Training%20on%20multiple%0Acities%20further%20improves%20performance%2C%20while%20cross-city%20experiments%20show%20modest%0Adegradation%20and%20occasional%20gains%20for%20rare%20heavy-rain%20regimes.%20The%20model%0Aproduces%20smooth%2C%20interpretable%20motion%20fields%20aligned%20with%20numerical%20optical%0Aflow%20and%20runs%20in%20near%20real%20time%20due%20to%20the%20low%20latency%20of%20GOES-16.%20These%0Aresults%20indicate%20that%20physically%20aligned%20learning%20can%20provide%20nowcasts%20that%20are%0Askillful%2C%20transferable%20and%20global.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecipitation%2520nowcasting%2520of%2520satellite%2520data%2520using%2520physically%2520conditioned%250A%2520%2520neural%2520networks%26entry.906535625%3DAnt%25C3%25B4nio%2520Cat%25C3%25A3o%2520and%2520Melvin%2520Poveda%2520and%2520Leonardo%2520Voltarelli%2520and%2520Paulo%2520Orenstein%26entry.1292438233%3D%2520%2520Accurate%2520short-term%2520precipitation%2520forecasts%2520predominantly%2520rely%2520on%2520dense%250Aweather-radar%2520networks%252C%2520limiting%2520operational%2520value%2520in%2520places%2520most%2520exposed%2520to%250Aclimate%2520extremes.%2520We%2520present%2520TUPANN%2520%2528Transferable%2520and%2520Universal%2520Physics-Aligned%250ANowcasting%2520Network%2529%252C%2520a%2520satellite-only%2520model%2520trained%2520on%2520GOES-16%2520RRQPE.%2520Unlike%250Amost%2520deep%2520learning%2520models%2520for%2520nowcasting%252C%2520TUPANN%2520decomposes%2520the%2520forecast%2520into%250Aphysically%2520meaningful%2520components%253A%2520a%2520variational%2520encoder-decoder%2520infers%2520motion%250Aand%2520intensity%2520fields%2520from%2520recent%2520imagery%2520under%2520optical-flow%2520supervision%252C%2520a%250Alead-time-conditioned%2520MaxViT%2520evolves%2520the%2520latent%2520state%252C%2520and%2520a%2520differentiable%250Aadvection%2520operator%2520reconstructs%2520future%2520frames.%2520We%2520evaluate%2520TUPANN%2520on%2520both%250AGOES-16%2520and%2520IMERG%2520data%252C%2520in%2520up%2520to%2520four%2520distinct%2520climates%2520%2528Rio%2520de%2520Janeiro%252C%250AManaus%252C%2520Miami%252C%2520La%2520Paz%2529%2520at%252010-180min%2520lead%2520times%2520using%2520the%2520CSI%2520and%2520HSS%2520metrics%250Aover%25204-64%2520mm/h%2520thresholds.%2520Comparisons%2520against%2520optical-flow%252C%2520deep%2520learning%2520and%250Ahybrid%2520baselines%2520show%2520that%2520TUPANN%2520achieves%2520the%2520best%2520or%2520second-best%2520skill%2520in%250Amost%2520settings%252C%2520with%2520pronounced%2520gains%2520at%2520higher%2520thresholds.%2520Training%2520on%2520multiple%250Acities%2520further%2520improves%2520performance%252C%2520while%2520cross-city%2520experiments%2520show%2520modest%250Adegradation%2520and%2520occasional%2520gains%2520for%2520rare%2520heavy-rain%2520regimes.%2520The%2520model%250Aproduces%2520smooth%252C%2520interpretable%2520motion%2520fields%2520aligned%2520with%2520numerical%2520optical%250Aflow%2520and%2520runs%2520in%2520near%2520real%2520time%2520due%2520to%2520the%2520low%2520latency%2520of%2520GOES-16.%2520These%250Aresults%2520indicate%2520that%2520physically%2520aligned%2520learning%2520can%2520provide%2520nowcasts%2520that%2520are%250Askillful%252C%2520transferable%2520and%2520global.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precipitation%20nowcasting%20of%20satellite%20data%20using%20physically%20conditioned%0A%20%20neural%20networks&entry.906535625=Ant%C3%B4nio%20Cat%C3%A3o%20and%20Melvin%20Poveda%20and%20Leonardo%20Voltarelli%20and%20Paulo%20Orenstein&entry.1292438233=%20%20Accurate%20short-term%20precipitation%20forecasts%20predominantly%20rely%20on%20dense%0Aweather-radar%20networks%2C%20limiting%20operational%20value%20in%20places%20most%20exposed%20to%0Aclimate%20extremes.%20We%20present%20TUPANN%20%28Transferable%20and%20Universal%20Physics-Aligned%0ANowcasting%20Network%29%2C%20a%20satellite-only%20model%20trained%20on%20GOES-16%20RRQPE.%20Unlike%0Amost%20deep%20learning%20models%20for%20nowcasting%2C%20TUPANN%20decomposes%20the%20forecast%20into%0Aphysically%20meaningful%20components%3A%20a%20variational%20encoder-decoder%20infers%20motion%0Aand%20intensity%20fields%20from%20recent%20imagery%20under%20optical-flow%20supervision%2C%20a%0Alead-time-conditioned%20MaxViT%20evolves%20the%20latent%20state%2C%20and%20a%20differentiable%0Aadvection%20operator%20reconstructs%20future%20frames.%20We%20evaluate%20TUPANN%20on%20both%0AGOES-16%20and%20IMERG%20data%2C%20in%20up%20to%20four%20distinct%20climates%20%28Rio%20de%20Janeiro%2C%0AManaus%2C%20Miami%2C%20La%20Paz%29%20at%2010-180min%20lead%20times%20using%20the%20CSI%20and%20HSS%20metrics%0Aover%204-64%20mm/h%20thresholds.%20Comparisons%20against%20optical-flow%2C%20deep%20learning%20and%0Ahybrid%20baselines%20show%20that%20TUPANN%20achieves%20the%20best%20or%20second-best%20skill%20in%0Amost%20settings%2C%20with%20pronounced%20gains%20at%20higher%20thresholds.%20Training%20on%20multiple%0Acities%20further%20improves%20performance%2C%20while%20cross-city%20experiments%20show%20modest%0Adegradation%20and%20occasional%20gains%20for%20rare%20heavy-rain%20regimes.%20The%20model%0Aproduces%20smooth%2C%20interpretable%20motion%20fields%20aligned%20with%20numerical%20optical%0Aflow%20and%20runs%20in%20near%20real%20time%20due%20to%20the%20low%20latency%20of%20GOES-16.%20These%0Aresults%20indicate%20that%20physically%20aligned%20learning%20can%20provide%20nowcasts%20that%20are%0Askillful%2C%20transferable%20and%20global.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05471v1&entry.124074799=Read"},
{"title": "Photo Dating by Facial Age Aggregation", "author": "Jakub Paplham and Vojtech Franc", "abstract": "  We introduce a novel method for Photo Dating which estimates the year a\nphotograph was taken by leveraging information from the faces of people present\nin the image. To facilitate this research, we publicly release CSFD-1.6M, a new\ndataset containing over 1.6 million annotated faces, primarily from movie\nstills, with identity and birth year annotations. Uniquely, our dataset\nprovides annotations for multiple individuals within a single image, enabling\nthe study of multi-face information aggregation. We propose a probabilistic\nframework that formally combines visual evidence from modern face recognition\nand age estimation models, and career-based temporal priors to infer the photo\ncapture year. Our experiments demonstrate that aggregating evidence from\nmultiple faces consistently improves the performance and the approach\nsignificantly outperforms strong, scene-based baselines, particularly for\nimages containing several identifiable individuals.\n", "link": "http://arxiv.org/abs/2511.05464v1", "date": "2025-11-07", "relevancy": 1.5349, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5666}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5023}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photo%20Dating%20by%20Facial%20Age%20Aggregation&body=Title%3A%20Photo%20Dating%20by%20Facial%20Age%20Aggregation%0AAuthor%3A%20Jakub%20Paplham%20and%20Vojtech%20Franc%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20Photo%20Dating%20which%20estimates%20the%20year%20a%0Aphotograph%20was%20taken%20by%20leveraging%20information%20from%20the%20faces%20of%20people%20present%0Ain%20the%20image.%20To%20facilitate%20this%20research%2C%20we%20publicly%20release%20CSFD-1.6M%2C%20a%20new%0Adataset%20containing%20over%201.6%20million%20annotated%20faces%2C%20primarily%20from%20movie%0Astills%2C%20with%20identity%20and%20birth%20year%20annotations.%20Uniquely%2C%20our%20dataset%0Aprovides%20annotations%20for%20multiple%20individuals%20within%20a%20single%20image%2C%20enabling%0Athe%20study%20of%20multi-face%20information%20aggregation.%20We%20propose%20a%20probabilistic%0Aframework%20that%20formally%20combines%20visual%20evidence%20from%20modern%20face%20recognition%0Aand%20age%20estimation%20models%2C%20and%20career-based%20temporal%20priors%20to%20infer%20the%20photo%0Acapture%20year.%20Our%20experiments%20demonstrate%20that%20aggregating%20evidence%20from%0Amultiple%20faces%20consistently%20improves%20the%20performance%20and%20the%20approach%0Asignificantly%20outperforms%20strong%2C%20scene-based%20baselines%2C%20particularly%20for%0Aimages%20containing%20several%20identifiable%20individuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhoto%2520Dating%2520by%2520Facial%2520Age%2520Aggregation%26entry.906535625%3DJakub%2520Paplham%2520and%2520Vojtech%2520Franc%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520Photo%2520Dating%2520which%2520estimates%2520the%2520year%2520a%250Aphotograph%2520was%2520taken%2520by%2520leveraging%2520information%2520from%2520the%2520faces%2520of%2520people%2520present%250Ain%2520the%2520image.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520publicly%2520release%2520CSFD-1.6M%252C%2520a%2520new%250Adataset%2520containing%2520over%25201.6%2520million%2520annotated%2520faces%252C%2520primarily%2520from%2520movie%250Astills%252C%2520with%2520identity%2520and%2520birth%2520year%2520annotations.%2520Uniquely%252C%2520our%2520dataset%250Aprovides%2520annotations%2520for%2520multiple%2520individuals%2520within%2520a%2520single%2520image%252C%2520enabling%250Athe%2520study%2520of%2520multi-face%2520information%2520aggregation.%2520We%2520propose%2520a%2520probabilistic%250Aframework%2520that%2520formally%2520combines%2520visual%2520evidence%2520from%2520modern%2520face%2520recognition%250Aand%2520age%2520estimation%2520models%252C%2520and%2520career-based%2520temporal%2520priors%2520to%2520infer%2520the%2520photo%250Acapture%2520year.%2520Our%2520experiments%2520demonstrate%2520that%2520aggregating%2520evidence%2520from%250Amultiple%2520faces%2520consistently%2520improves%2520the%2520performance%2520and%2520the%2520approach%250Asignificantly%2520outperforms%2520strong%252C%2520scene-based%2520baselines%252C%2520particularly%2520for%250Aimages%2520containing%2520several%2520identifiable%2520individuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photo%20Dating%20by%20Facial%20Age%20Aggregation&entry.906535625=Jakub%20Paplham%20and%20Vojtech%20Franc&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20Photo%20Dating%20which%20estimates%20the%20year%20a%0Aphotograph%20was%20taken%20by%20leveraging%20information%20from%20the%20faces%20of%20people%20present%0Ain%20the%20image.%20To%20facilitate%20this%20research%2C%20we%20publicly%20release%20CSFD-1.6M%2C%20a%20new%0Adataset%20containing%20over%201.6%20million%20annotated%20faces%2C%20primarily%20from%20movie%0Astills%2C%20with%20identity%20and%20birth%20year%20annotations.%20Uniquely%2C%20our%20dataset%0Aprovides%20annotations%20for%20multiple%20individuals%20within%20a%20single%20image%2C%20enabling%0Athe%20study%20of%20multi-face%20information%20aggregation.%20We%20propose%20a%20probabilistic%0Aframework%20that%20formally%20combines%20visual%20evidence%20from%20modern%20face%20recognition%0Aand%20age%20estimation%20models%2C%20and%20career-based%20temporal%20priors%20to%20infer%20the%20photo%0Acapture%20year.%20Our%20experiments%20demonstrate%20that%20aggregating%20evidence%20from%0Amultiple%20faces%20consistently%20improves%20the%20performance%20and%20the%20approach%0Asignificantly%20outperforms%20strong%2C%20scene-based%20baselines%2C%20particularly%20for%0Aimages%20containing%20several%20identifiable%20individuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05464v1&entry.124074799=Read"},
{"title": "Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer\n  for Drilling Rate of Penetration Prediction", "author": "Saddam Hussain Khan", "abstract": "  Rate of Penetration (ROP) prediction is critical for drilling optimization\nyet remains challenging due to the nonlinear, dynamic, and heterogeneous\ncharacteristics of drilling data. Conventional empirical, physics-based, and\nstandard machine learning models rely on oversimplified assumptions or\nintensive feature engineering, constraining their capacity to model long-term\ndependencies and intricate feature interactions. To address these issues, this\nstudy presents a new deep learning Hybrid LSTM-Trans-Mixer-Att framework that\nfirst processes input data through a customized Long Short-Term Memory (LSTM)\nnetwork to capture multi-scale temporal dependencies aligned with drilling\ncycles. Subsequently, an Enhanced Transformer encoder with drilling-specific\npositional encodings and real-time optimization refines the features.\nConcurrently, a parallel Time-Series Mixer (TS-Mixer) block introduced\nfacilitates efficient cross-feature interaction modeling of static and\ncategorical parameters, including lithological indices and mud properties. The\nfeature representations extracted from the Enhanced Transformer and TS-Mixer\nmodules are integrated through a dedicated fusion layer. Finally, an adaptive\nattention mechanism then dynamically assigns contextual weights to salient\nfeatures, enhancing discriminative representation learning and enabling\nhigh-fidelity ROP prediction. The proposed framework combines sequential\nmemory, static feature interactions, global context learning, and dynamic\nfeature weighting, providing a comprehensive solution for the heterogeneous and\nevent-driven nature of drilling dynamics. Experimental validation on real-world\ndrilling datasets demonstrates superior performance, achieving an Rsquare of\n0.9991 and a MAPE of 1.447%, significantly outperforming existing baseline and\nhybrid models.\n", "link": "http://arxiv.org/abs/2508.05210v3", "date": "2025-11-07", "relevancy": 1.5318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5127}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Hybrid%20Transformer%20LSTM%20Technique%20with%20Attention%20and%20TS%20Mixer%0A%20%20for%20Drilling%20Rate%20of%20Penetration%20Prediction&body=Title%3A%20Advanced%20Hybrid%20Transformer%20LSTM%20Technique%20with%20Attention%20and%20TS%20Mixer%0A%20%20for%20Drilling%20Rate%20of%20Penetration%20Prediction%0AAuthor%3A%20Saddam%20Hussain%20Khan%0AAbstract%3A%20%20%20Rate%20of%20Penetration%20%28ROP%29%20prediction%20is%20critical%20for%20drilling%20optimization%0Ayet%20remains%20challenging%20due%20to%20the%20nonlinear%2C%20dynamic%2C%20and%20heterogeneous%0Acharacteristics%20of%20drilling%20data.%20Conventional%20empirical%2C%20physics-based%2C%20and%0Astandard%20machine%20learning%20models%20rely%20on%20oversimplified%20assumptions%20or%0Aintensive%20feature%20engineering%2C%20constraining%20their%20capacity%20to%20model%20long-term%0Adependencies%20and%20intricate%20feature%20interactions.%20To%20address%20these%20issues%2C%20this%0Astudy%20presents%20a%20new%20deep%20learning%20Hybrid%20LSTM-Trans-Mixer-Att%20framework%20that%0Afirst%20processes%20input%20data%20through%20a%20customized%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetwork%20to%20capture%20multi-scale%20temporal%20dependencies%20aligned%20with%20drilling%0Acycles.%20Subsequently%2C%20an%20Enhanced%20Transformer%20encoder%20with%20drilling-specific%0Apositional%20encodings%20and%20real-time%20optimization%20refines%20the%20features.%0AConcurrently%2C%20a%20parallel%20Time-Series%20Mixer%20%28TS-Mixer%29%20block%20introduced%0Afacilitates%20efficient%20cross-feature%20interaction%20modeling%20of%20static%20and%0Acategorical%20parameters%2C%20including%20lithological%20indices%20and%20mud%20properties.%20The%0Afeature%20representations%20extracted%20from%20the%20Enhanced%20Transformer%20and%20TS-Mixer%0Amodules%20are%20integrated%20through%20a%20dedicated%20fusion%20layer.%20Finally%2C%20an%20adaptive%0Aattention%20mechanism%20then%20dynamically%20assigns%20contextual%20weights%20to%20salient%0Afeatures%2C%20enhancing%20discriminative%20representation%20learning%20and%20enabling%0Ahigh-fidelity%20ROP%20prediction.%20The%20proposed%20framework%20combines%20sequential%0Amemory%2C%20static%20feature%20interactions%2C%20global%20context%20learning%2C%20and%20dynamic%0Afeature%20weighting%2C%20providing%20a%20comprehensive%20solution%20for%20the%20heterogeneous%20and%0Aevent-driven%20nature%20of%20drilling%20dynamics.%20Experimental%20validation%20on%20real-world%0Adrilling%20datasets%20demonstrates%20superior%20performance%2C%20achieving%20an%20Rsquare%20of%0A0.9991%20and%20a%20MAPE%20of%201.447%25%2C%20significantly%20outperforming%20existing%20baseline%20and%0Ahybrid%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.05210v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Hybrid%2520Transformer%2520LSTM%2520Technique%2520with%2520Attention%2520and%2520TS%2520Mixer%250A%2520%2520for%2520Drilling%2520Rate%2520of%2520Penetration%2520Prediction%26entry.906535625%3DSaddam%2520Hussain%2520Khan%26entry.1292438233%3D%2520%2520Rate%2520of%2520Penetration%2520%2528ROP%2529%2520prediction%2520is%2520critical%2520for%2520drilling%2520optimization%250Ayet%2520remains%2520challenging%2520due%2520to%2520the%2520nonlinear%252C%2520dynamic%252C%2520and%2520heterogeneous%250Acharacteristics%2520of%2520drilling%2520data.%2520Conventional%2520empirical%252C%2520physics-based%252C%2520and%250Astandard%2520machine%2520learning%2520models%2520rely%2520on%2520oversimplified%2520assumptions%2520or%250Aintensive%2520feature%2520engineering%252C%2520constraining%2520their%2520capacity%2520to%2520model%2520long-term%250Adependencies%2520and%2520intricate%2520feature%2520interactions.%2520To%2520address%2520these%2520issues%252C%2520this%250Astudy%2520presents%2520a%2520new%2520deep%2520learning%2520Hybrid%2520LSTM-Trans-Mixer-Att%2520framework%2520that%250Afirst%2520processes%2520input%2520data%2520through%2520a%2520customized%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%250Anetwork%2520to%2520capture%2520multi-scale%2520temporal%2520dependencies%2520aligned%2520with%2520drilling%250Acycles.%2520Subsequently%252C%2520an%2520Enhanced%2520Transformer%2520encoder%2520with%2520drilling-specific%250Apositional%2520encodings%2520and%2520real-time%2520optimization%2520refines%2520the%2520features.%250AConcurrently%252C%2520a%2520parallel%2520Time-Series%2520Mixer%2520%2528TS-Mixer%2529%2520block%2520introduced%250Afacilitates%2520efficient%2520cross-feature%2520interaction%2520modeling%2520of%2520static%2520and%250Acategorical%2520parameters%252C%2520including%2520lithological%2520indices%2520and%2520mud%2520properties.%2520The%250Afeature%2520representations%2520extracted%2520from%2520the%2520Enhanced%2520Transformer%2520and%2520TS-Mixer%250Amodules%2520are%2520integrated%2520through%2520a%2520dedicated%2520fusion%2520layer.%2520Finally%252C%2520an%2520adaptive%250Aattention%2520mechanism%2520then%2520dynamically%2520assigns%2520contextual%2520weights%2520to%2520salient%250Afeatures%252C%2520enhancing%2520discriminative%2520representation%2520learning%2520and%2520enabling%250Ahigh-fidelity%2520ROP%2520prediction.%2520The%2520proposed%2520framework%2520combines%2520sequential%250Amemory%252C%2520static%2520feature%2520interactions%252C%2520global%2520context%2520learning%252C%2520and%2520dynamic%250Afeature%2520weighting%252C%2520providing%2520a%2520comprehensive%2520solution%2520for%2520the%2520heterogeneous%2520and%250Aevent-driven%2520nature%2520of%2520drilling%2520dynamics.%2520Experimental%2520validation%2520on%2520real-world%250Adrilling%2520datasets%2520demonstrates%2520superior%2520performance%252C%2520achieving%2520an%2520Rsquare%2520of%250A0.9991%2520and%2520a%2520MAPE%2520of%25201.447%2525%252C%2520significantly%2520outperforming%2520existing%2520baseline%2520and%250Ahybrid%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05210v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Hybrid%20Transformer%20LSTM%20Technique%20with%20Attention%20and%20TS%20Mixer%0A%20%20for%20Drilling%20Rate%20of%20Penetration%20Prediction&entry.906535625=Saddam%20Hussain%20Khan&entry.1292438233=%20%20Rate%20of%20Penetration%20%28ROP%29%20prediction%20is%20critical%20for%20drilling%20optimization%0Ayet%20remains%20challenging%20due%20to%20the%20nonlinear%2C%20dynamic%2C%20and%20heterogeneous%0Acharacteristics%20of%20drilling%20data.%20Conventional%20empirical%2C%20physics-based%2C%20and%0Astandard%20machine%20learning%20models%20rely%20on%20oversimplified%20assumptions%20or%0Aintensive%20feature%20engineering%2C%20constraining%20their%20capacity%20to%20model%20long-term%0Adependencies%20and%20intricate%20feature%20interactions.%20To%20address%20these%20issues%2C%20this%0Astudy%20presents%20a%20new%20deep%20learning%20Hybrid%20LSTM-Trans-Mixer-Att%20framework%20that%0Afirst%20processes%20input%20data%20through%20a%20customized%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetwork%20to%20capture%20multi-scale%20temporal%20dependencies%20aligned%20with%20drilling%0Acycles.%20Subsequently%2C%20an%20Enhanced%20Transformer%20encoder%20with%20drilling-specific%0Apositional%20encodings%20and%20real-time%20optimization%20refines%20the%20features.%0AConcurrently%2C%20a%20parallel%20Time-Series%20Mixer%20%28TS-Mixer%29%20block%20introduced%0Afacilitates%20efficient%20cross-feature%20interaction%20modeling%20of%20static%20and%0Acategorical%20parameters%2C%20including%20lithological%20indices%20and%20mud%20properties.%20The%0Afeature%20representations%20extracted%20from%20the%20Enhanced%20Transformer%20and%20TS-Mixer%0Amodules%20are%20integrated%20through%20a%20dedicated%20fusion%20layer.%20Finally%2C%20an%20adaptive%0Aattention%20mechanism%20then%20dynamically%20assigns%20contextual%20weights%20to%20salient%0Afeatures%2C%20enhancing%20discriminative%20representation%20learning%20and%20enabling%0Ahigh-fidelity%20ROP%20prediction.%20The%20proposed%20framework%20combines%20sequential%0Amemory%2C%20static%20feature%20interactions%2C%20global%20context%20learning%2C%20and%20dynamic%0Afeature%20weighting%2C%20providing%20a%20comprehensive%20solution%20for%20the%20heterogeneous%20and%0Aevent-driven%20nature%20of%20drilling%20dynamics.%20Experimental%20validation%20on%20real-world%0Adrilling%20datasets%20demonstrates%20superior%20performance%2C%20achieving%20an%20Rsquare%20of%0A0.9991%20and%20a%20MAPE%20of%201.447%25%2C%20significantly%20outperforming%20existing%20baseline%20and%0Ahybrid%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.05210v3&entry.124074799=Read"},
{"title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing", "author": "Zhihui Chen and Mengling Feng", "abstract": "  Medical image editing has emerged as a pivotal technology with broad\napplications in data augmentation, model interpretability, medical education,\nand treatment simulation. However, the lack of large-scale, high-quality, and\nopenly accessible datasets tailored for medical contexts with strict anatomical\nand clinical constraints has significantly hindered progress in this domain. To\nbridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over\n50k medically curated image edits spanning chest X-ray, brain MRI, and fundus\nphotography across 23 diseases. Each sample supports bidirectional lesion\nediting (addition and removal) and is constructed using Gemini-2.5-Flash-Image\nbased on real clinical images. A key differentiator of our dataset is the\nmedically grounded quality control protocol: we employ an LLM-as-Judge\nevaluation framework with criteria such as instruction compliance, structural\nplausibility, image realism, and fidelity preservation, alongside iterative\nrefinement over up to five rounds. Additionally, Med-Banana-50K includes around\n37,000 failed editing attempts with full evaluation logs to support preference\nlearning and alignment research. By offering a large-scale, medically rigorous,\nand fully documented resource, Med-Banana-50K establishes a critical foundation\nfor developing and evaluating reliable medical image editing systems. Our\ndataset and code are publicly available.\n[https://github.com/richardChenzhihui/med-banana-50k].\n", "link": "http://arxiv.org/abs/2511.00801v3", "date": "2025-11-07", "relevancy": 1.5207, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5345}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5094}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Med-Banana-50K%3A%20A%20Cross-modality%20Large-Scale%20Dataset%20for%20Text-guided%0A%20%20Medical%20Image%20Editing&body=Title%3A%20Med-Banana-50K%3A%20A%20Cross-modality%20Large-Scale%20Dataset%20for%20Text-guided%0A%20%20Medical%20Image%20Editing%0AAuthor%3A%20Zhihui%20Chen%20and%20Mengling%20Feng%0AAbstract%3A%20%20%20Medical%20image%20editing%20has%20emerged%20as%20a%20pivotal%20technology%20with%20broad%0Aapplications%20in%20data%20augmentation%2C%20model%20interpretability%2C%20medical%20education%2C%0Aand%20treatment%20simulation.%20However%2C%20the%20lack%20of%20large-scale%2C%20high-quality%2C%20and%0Aopenly%20accessible%20datasets%20tailored%20for%20medical%20contexts%20with%20strict%20anatomical%0Aand%20clinical%20constraints%20has%20significantly%20hindered%20progress%20in%20this%20domain.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20Med-Banana-50K%2C%20a%20comprehensive%20dataset%20of%20over%0A50k%20medically%20curated%20image%20edits%20spanning%20chest%20X-ray%2C%20brain%20MRI%2C%20and%20fundus%0Aphotography%20across%2023%20diseases.%20Each%20sample%20supports%20bidirectional%20lesion%0Aediting%20%28addition%20and%20removal%29%20and%20is%20constructed%20using%20Gemini-2.5-Flash-Image%0Abased%20on%20real%20clinical%20images.%20A%20key%20differentiator%20of%20our%20dataset%20is%20the%0Amedically%20grounded%20quality%20control%20protocol%3A%20we%20employ%20an%20LLM-as-Judge%0Aevaluation%20framework%20with%20criteria%20such%20as%20instruction%20compliance%2C%20structural%0Aplausibility%2C%20image%20realism%2C%20and%20fidelity%20preservation%2C%20alongside%20iterative%0Arefinement%20over%20up%20to%20five%20rounds.%20Additionally%2C%20Med-Banana-50K%20includes%20around%0A37%2C000%20failed%20editing%20attempts%20with%20full%20evaluation%20logs%20to%20support%20preference%0Alearning%20and%20alignment%20research.%20By%20offering%20a%20large-scale%2C%20medically%20rigorous%2C%0Aand%20fully%20documented%20resource%2C%20Med-Banana-50K%20establishes%20a%20critical%20foundation%0Afor%20developing%20and%20evaluating%20reliable%20medical%20image%20editing%20systems.%20Our%0Adataset%20and%20code%20are%20publicly%20available.%0A%5Bhttps%3A//github.com/richardChenzhihui/med-banana-50k%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.00801v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMed-Banana-50K%253A%2520A%2520Cross-modality%2520Large-Scale%2520Dataset%2520for%2520Text-guided%250A%2520%2520Medical%2520Image%2520Editing%26entry.906535625%3DZhihui%2520Chen%2520and%2520Mengling%2520Feng%26entry.1292438233%3D%2520%2520Medical%2520image%2520editing%2520has%2520emerged%2520as%2520a%2520pivotal%2520technology%2520with%2520broad%250Aapplications%2520in%2520data%2520augmentation%252C%2520model%2520interpretability%252C%2520medical%2520education%252C%250Aand%2520treatment%2520simulation.%2520However%252C%2520the%2520lack%2520of%2520large-scale%252C%2520high-quality%252C%2520and%250Aopenly%2520accessible%2520datasets%2520tailored%2520for%2520medical%2520contexts%2520with%2520strict%2520anatomical%250Aand%2520clinical%2520constraints%2520has%2520significantly%2520hindered%2520progress%2520in%2520this%2520domain.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520Med-Banana-50K%252C%2520a%2520comprehensive%2520dataset%2520of%2520over%250A50k%2520medically%2520curated%2520image%2520edits%2520spanning%2520chest%2520X-ray%252C%2520brain%2520MRI%252C%2520and%2520fundus%250Aphotography%2520across%252023%2520diseases.%2520Each%2520sample%2520supports%2520bidirectional%2520lesion%250Aediting%2520%2528addition%2520and%2520removal%2529%2520and%2520is%2520constructed%2520using%2520Gemini-2.5-Flash-Image%250Abased%2520on%2520real%2520clinical%2520images.%2520A%2520key%2520differentiator%2520of%2520our%2520dataset%2520is%2520the%250Amedically%2520grounded%2520quality%2520control%2520protocol%253A%2520we%2520employ%2520an%2520LLM-as-Judge%250Aevaluation%2520framework%2520with%2520criteria%2520such%2520as%2520instruction%2520compliance%252C%2520structural%250Aplausibility%252C%2520image%2520realism%252C%2520and%2520fidelity%2520preservation%252C%2520alongside%2520iterative%250Arefinement%2520over%2520up%2520to%2520five%2520rounds.%2520Additionally%252C%2520Med-Banana-50K%2520includes%2520around%250A37%252C000%2520failed%2520editing%2520attempts%2520with%2520full%2520evaluation%2520logs%2520to%2520support%2520preference%250Alearning%2520and%2520alignment%2520research.%2520By%2520offering%2520a%2520large-scale%252C%2520medically%2520rigorous%252C%250Aand%2520fully%2520documented%2520resource%252C%2520Med-Banana-50K%2520establishes%2520a%2520critical%2520foundation%250Afor%2520developing%2520and%2520evaluating%2520reliable%2520medical%2520image%2520editing%2520systems.%2520Our%250Adataset%2520and%2520code%2520are%2520publicly%2520available.%250A%255Bhttps%253A//github.com/richardChenzhihui/med-banana-50k%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00801v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Med-Banana-50K%3A%20A%20Cross-modality%20Large-Scale%20Dataset%20for%20Text-guided%0A%20%20Medical%20Image%20Editing&entry.906535625=Zhihui%20Chen%20and%20Mengling%20Feng&entry.1292438233=%20%20Medical%20image%20editing%20has%20emerged%20as%20a%20pivotal%20technology%20with%20broad%0Aapplications%20in%20data%20augmentation%2C%20model%20interpretability%2C%20medical%20education%2C%0Aand%20treatment%20simulation.%20However%2C%20the%20lack%20of%20large-scale%2C%20high-quality%2C%20and%0Aopenly%20accessible%20datasets%20tailored%20for%20medical%20contexts%20with%20strict%20anatomical%0Aand%20clinical%20constraints%20has%20significantly%20hindered%20progress%20in%20this%20domain.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20Med-Banana-50K%2C%20a%20comprehensive%20dataset%20of%20over%0A50k%20medically%20curated%20image%20edits%20spanning%20chest%20X-ray%2C%20brain%20MRI%2C%20and%20fundus%0Aphotography%20across%2023%20diseases.%20Each%20sample%20supports%20bidirectional%20lesion%0Aediting%20%28addition%20and%20removal%29%20and%20is%20constructed%20using%20Gemini-2.5-Flash-Image%0Abased%20on%20real%20clinical%20images.%20A%20key%20differentiator%20of%20our%20dataset%20is%20the%0Amedically%20grounded%20quality%20control%20protocol%3A%20we%20employ%20an%20LLM-as-Judge%0Aevaluation%20framework%20with%20criteria%20such%20as%20instruction%20compliance%2C%20structural%0Aplausibility%2C%20image%20realism%2C%20and%20fidelity%20preservation%2C%20alongside%20iterative%0Arefinement%20over%20up%20to%20five%20rounds.%20Additionally%2C%20Med-Banana-50K%20includes%20around%0A37%2C000%20failed%20editing%20attempts%20with%20full%20evaluation%20logs%20to%20support%20preference%0Alearning%20and%20alignment%20research.%20By%20offering%20a%20large-scale%2C%20medically%20rigorous%2C%0Aand%20fully%20documented%20resource%2C%20Med-Banana-50K%20establishes%20a%20critical%20foundation%0Afor%20developing%20and%20evaluating%20reliable%20medical%20image%20editing%20systems.%20Our%0Adataset%20and%20code%20are%20publicly%20available.%0A%5Bhttps%3A//github.com/richardChenzhihui/med-banana-50k%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.00801v3&entry.124074799=Read"},
{"title": "Parameter-Efficient Conditioning for Material Generalization in\n  Graph-Based Simulators", "author": "Naveen Raj Manoharan and Hassan Iqbal and Krishna Kumar", "abstract": "  Graph network-based simulators (GNS) have demonstrated strong potential for\nlearning particle-based physics (such as fluids, deformable solids, and\ngranular flows) while generalizing to unseen geometries due to their inherent\ninductive biases. However, existing models are typically trained for a single\nmaterial type and fail to generalize across distinct constitutive behaviors,\nlimiting their applicability in real-world engineering settings. Using granular\nflows as a running example, we propose a parameter-efficient conditioning\nmechanism that makes the GNS model adaptive to material parameters. We identify\nthat sensitivity to material properties is concentrated in the early\nmessage-passing (MP) layers, a finding we link to the local nature of\nconstitutive models (e.g., Mohr-Coulomb) and their effects on information\npropagation. We empirically validate this by showing that fine-tuning only the\nfirst few (1-5) of 10 MP layers of a pretrained model achieves comparable test\nperformance as compared to fine-tuning the entire network. Building on this\ninsight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM)\nconditioning mechanism designed to specifically target these early layers. This\napproach produces accurate long-term rollouts on unseen, interpolated, or\nmoderately extrapolated values (e.g., up to 2.5 degrees for friction angle and\n0.25 kPa for cohesion) when trained exclusively on as few as 12 short\nsimulation trajectories from new materials, representing a 5-fold data\nreduction compared to a baseline multi-task learning method. Finally, we\nvalidate the model's utility by applying it to an inverse problem, successfully\nidentifying unknown cohesion parameters from trajectory data. This approach\nenables the use of GNS in inverse design and closed-loop control tasks where\nmaterial properties are treated as design variables.\n", "link": "http://arxiv.org/abs/2511.05456v1", "date": "2025-11-07", "relevancy": 1.5116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4965}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Conditioning%20for%20Material%20Generalization%20in%0A%20%20Graph-Based%20Simulators&body=Title%3A%20Parameter-Efficient%20Conditioning%20for%20Material%20Generalization%20in%0A%20%20Graph-Based%20Simulators%0AAuthor%3A%20Naveen%20Raj%20Manoharan%20and%20Hassan%20Iqbal%20and%20Krishna%20Kumar%0AAbstract%3A%20%20%20Graph%20network-based%20simulators%20%28GNS%29%20have%20demonstrated%20strong%20potential%20for%0Alearning%20particle-based%20physics%20%28such%20as%20fluids%2C%20deformable%20solids%2C%20and%0Agranular%20flows%29%20while%20generalizing%20to%20unseen%20geometries%20due%20to%20their%20inherent%0Ainductive%20biases.%20However%2C%20existing%20models%20are%20typically%20trained%20for%20a%20single%0Amaterial%20type%20and%20fail%20to%20generalize%20across%20distinct%20constitutive%20behaviors%2C%0Alimiting%20their%20applicability%20in%20real-world%20engineering%20settings.%20Using%20granular%0Aflows%20as%20a%20running%20example%2C%20we%20propose%20a%20parameter-efficient%20conditioning%0Amechanism%20that%20makes%20the%20GNS%20model%20adaptive%20to%20material%20parameters.%20We%20identify%0Athat%20sensitivity%20to%20material%20properties%20is%20concentrated%20in%20the%20early%0Amessage-passing%20%28MP%29%20layers%2C%20a%20finding%20we%20link%20to%20the%20local%20nature%20of%0Aconstitutive%20models%20%28e.g.%2C%20Mohr-Coulomb%29%20and%20their%20effects%20on%20information%0Apropagation.%20We%20empirically%20validate%20this%20by%20showing%20that%20fine-tuning%20only%20the%0Afirst%20few%20%281-5%29%20of%2010%20MP%20layers%20of%20a%20pretrained%20model%20achieves%20comparable%20test%0Aperformance%20as%20compared%20to%20fine-tuning%20the%20entire%20network.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20parameter-efficient%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%0Aconditioning%20mechanism%20designed%20to%20specifically%20target%20these%20early%20layers.%20This%0Aapproach%20produces%20accurate%20long-term%20rollouts%20on%20unseen%2C%20interpolated%2C%20or%0Amoderately%20extrapolated%20values%20%28e.g.%2C%20up%20to%202.5%20degrees%20for%20friction%20angle%20and%0A0.25%20kPa%20for%20cohesion%29%20when%20trained%20exclusively%20on%20as%20few%20as%2012%20short%0Asimulation%20trajectories%20from%20new%20materials%2C%20representing%20a%205-fold%20data%0Areduction%20compared%20to%20a%20baseline%20multi-task%20learning%20method.%20Finally%2C%20we%0Avalidate%20the%20model%27s%20utility%20by%20applying%20it%20to%20an%20inverse%20problem%2C%20successfully%0Aidentifying%20unknown%20cohesion%20parameters%20from%20trajectory%20data.%20This%20approach%0Aenables%20the%20use%20of%20GNS%20in%20inverse%20design%20and%20closed-loop%20control%20tasks%20where%0Amaterial%20properties%20are%20treated%20as%20design%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Conditioning%2520for%2520Material%2520Generalization%2520in%250A%2520%2520Graph-Based%2520Simulators%26entry.906535625%3DNaveen%2520Raj%2520Manoharan%2520and%2520Hassan%2520Iqbal%2520and%2520Krishna%2520Kumar%26entry.1292438233%3D%2520%2520Graph%2520network-based%2520simulators%2520%2528GNS%2529%2520have%2520demonstrated%2520strong%2520potential%2520for%250Alearning%2520particle-based%2520physics%2520%2528such%2520as%2520fluids%252C%2520deformable%2520solids%252C%2520and%250Agranular%2520flows%2529%2520while%2520generalizing%2520to%2520unseen%2520geometries%2520due%2520to%2520their%2520inherent%250Ainductive%2520biases.%2520However%252C%2520existing%2520models%2520are%2520typically%2520trained%2520for%2520a%2520single%250Amaterial%2520type%2520and%2520fail%2520to%2520generalize%2520across%2520distinct%2520constitutive%2520behaviors%252C%250Alimiting%2520their%2520applicability%2520in%2520real-world%2520engineering%2520settings.%2520Using%2520granular%250Aflows%2520as%2520a%2520running%2520example%252C%2520we%2520propose%2520a%2520parameter-efficient%2520conditioning%250Amechanism%2520that%2520makes%2520the%2520GNS%2520model%2520adaptive%2520to%2520material%2520parameters.%2520We%2520identify%250Athat%2520sensitivity%2520to%2520material%2520properties%2520is%2520concentrated%2520in%2520the%2520early%250Amessage-passing%2520%2528MP%2529%2520layers%252C%2520a%2520finding%2520we%2520link%2520to%2520the%2520local%2520nature%2520of%250Aconstitutive%2520models%2520%2528e.g.%252C%2520Mohr-Coulomb%2529%2520and%2520their%2520effects%2520on%2520information%250Apropagation.%2520We%2520empirically%2520validate%2520this%2520by%2520showing%2520that%2520fine-tuning%2520only%2520the%250Afirst%2520few%2520%25281-5%2529%2520of%252010%2520MP%2520layers%2520of%2520a%2520pretrained%2520model%2520achieves%2520comparable%2520test%250Aperformance%2520as%2520compared%2520to%2520fine-tuning%2520the%2520entire%2520network.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520a%2520parameter-efficient%2520Feature-wise%2520Linear%2520Modulation%2520%2528FiLM%2529%250Aconditioning%2520mechanism%2520designed%2520to%2520specifically%2520target%2520these%2520early%2520layers.%2520This%250Aapproach%2520produces%2520accurate%2520long-term%2520rollouts%2520on%2520unseen%252C%2520interpolated%252C%2520or%250Amoderately%2520extrapolated%2520values%2520%2528e.g.%252C%2520up%2520to%25202.5%2520degrees%2520for%2520friction%2520angle%2520and%250A0.25%2520kPa%2520for%2520cohesion%2529%2520when%2520trained%2520exclusively%2520on%2520as%2520few%2520as%252012%2520short%250Asimulation%2520trajectories%2520from%2520new%2520materials%252C%2520representing%2520a%25205-fold%2520data%250Areduction%2520compared%2520to%2520a%2520baseline%2520multi-task%2520learning%2520method.%2520Finally%252C%2520we%250Avalidate%2520the%2520model%2527s%2520utility%2520by%2520applying%2520it%2520to%2520an%2520inverse%2520problem%252C%2520successfully%250Aidentifying%2520unknown%2520cohesion%2520parameters%2520from%2520trajectory%2520data.%2520This%2520approach%250Aenables%2520the%2520use%2520of%2520GNS%2520in%2520inverse%2520design%2520and%2520closed-loop%2520control%2520tasks%2520where%250Amaterial%2520properties%2520are%2520treated%2520as%2520design%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Conditioning%20for%20Material%20Generalization%20in%0A%20%20Graph-Based%20Simulators&entry.906535625=Naveen%20Raj%20Manoharan%20and%20Hassan%20Iqbal%20and%20Krishna%20Kumar&entry.1292438233=%20%20Graph%20network-based%20simulators%20%28GNS%29%20have%20demonstrated%20strong%20potential%20for%0Alearning%20particle-based%20physics%20%28such%20as%20fluids%2C%20deformable%20solids%2C%20and%0Agranular%20flows%29%20while%20generalizing%20to%20unseen%20geometries%20due%20to%20their%20inherent%0Ainductive%20biases.%20However%2C%20existing%20models%20are%20typically%20trained%20for%20a%20single%0Amaterial%20type%20and%20fail%20to%20generalize%20across%20distinct%20constitutive%20behaviors%2C%0Alimiting%20their%20applicability%20in%20real-world%20engineering%20settings.%20Using%20granular%0Aflows%20as%20a%20running%20example%2C%20we%20propose%20a%20parameter-efficient%20conditioning%0Amechanism%20that%20makes%20the%20GNS%20model%20adaptive%20to%20material%20parameters.%20We%20identify%0Athat%20sensitivity%20to%20material%20properties%20is%20concentrated%20in%20the%20early%0Amessage-passing%20%28MP%29%20layers%2C%20a%20finding%20we%20link%20to%20the%20local%20nature%20of%0Aconstitutive%20models%20%28e.g.%2C%20Mohr-Coulomb%29%20and%20their%20effects%20on%20information%0Apropagation.%20We%20empirically%20validate%20this%20by%20showing%20that%20fine-tuning%20only%20the%0Afirst%20few%20%281-5%29%20of%2010%20MP%20layers%20of%20a%20pretrained%20model%20achieves%20comparable%20test%0Aperformance%20as%20compared%20to%20fine-tuning%20the%20entire%20network.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20a%20parameter-efficient%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%0Aconditioning%20mechanism%20designed%20to%20specifically%20target%20these%20early%20layers.%20This%0Aapproach%20produces%20accurate%20long-term%20rollouts%20on%20unseen%2C%20interpolated%2C%20or%0Amoderately%20extrapolated%20values%20%28e.g.%2C%20up%20to%202.5%20degrees%20for%20friction%20angle%20and%0A0.25%20kPa%20for%20cohesion%29%20when%20trained%20exclusively%20on%20as%20few%20as%2012%20short%0Asimulation%20trajectories%20from%20new%20materials%2C%20representing%20a%205-fold%20data%0Areduction%20compared%20to%20a%20baseline%20multi-task%20learning%20method.%20Finally%2C%20we%0Avalidate%20the%20model%27s%20utility%20by%20applying%20it%20to%20an%20inverse%20problem%2C%20successfully%0Aidentifying%20unknown%20cohesion%20parameters%20from%20trajectory%20data.%20This%20approach%0Aenables%20the%20use%20of%20GNS%20in%20inverse%20design%20and%20closed-loop%20control%20tasks%20where%0Amaterial%20properties%20are%20treated%20as%20design%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05456v1&entry.124074799=Read"},
{"title": "Characterizing the Training Dynamics of Private Fine-tuning with\n  Langevin diffusion", "author": "Shuqi Ke and Charlie Hou and Sewoong Oh and Giulia Fanti", "abstract": "  We show that differentially private full fine-tuning (DP-FFT) can distort\npre-trained backbone features based on both theoretical and empirical results.\nWe identify the cause of the distortion as the misalignment between the\npre-trained backbone and the randomly initialized linear head. We prove that a\nsequential fine-tuning strategy can mitigate the feature distortion:\nfirst-linear-probing-then-fine-tuning (DP-LP-FFT). A new approximation scheme\nallows us to derive approximate upper and lower bounds on the training loss of\nDP-LP and DP-FFT, in a simple but canonical setting of 2-layer neural networks\nwith ReLU activation. Experiments on real-world datasets and architectures are\nconsistent with our theoretical insights. We also derive new upper bounds for\n2-layer linear networks without the approximation. Moreover, our theory\nsuggests a trade-off of privacy budget allocation in multi-phase fine-tuning\nmethods like DP-LP-FFT.\n", "link": "http://arxiv.org/abs/2402.18905v2", "date": "2025-11-07", "relevancy": 1.5055, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5105}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5019}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20the%20Training%20Dynamics%20of%20Private%20Fine-tuning%20with%0A%20%20Langevin%20diffusion&body=Title%3A%20Characterizing%20the%20Training%20Dynamics%20of%20Private%20Fine-tuning%20with%0A%20%20Langevin%20diffusion%0AAuthor%3A%20Shuqi%20Ke%20and%20Charlie%20Hou%20and%20Sewoong%20Oh%20and%20Giulia%20Fanti%0AAbstract%3A%20%20%20We%20show%20that%20differentially%20private%20full%20fine-tuning%20%28DP-FFT%29%20can%20distort%0Apre-trained%20backbone%20features%20based%20on%20both%20theoretical%20and%20empirical%20results.%0AWe%20identify%20the%20cause%20of%20the%20distortion%20as%20the%20misalignment%20between%20the%0Apre-trained%20backbone%20and%20the%20randomly%20initialized%20linear%20head.%20We%20prove%20that%20a%0Asequential%20fine-tuning%20strategy%20can%20mitigate%20the%20feature%20distortion%3A%0Afirst-linear-probing-then-fine-tuning%20%28DP-LP-FFT%29.%20A%20new%20approximation%20scheme%0Aallows%20us%20to%20derive%20approximate%20upper%20and%20lower%20bounds%20on%20the%20training%20loss%20of%0ADP-LP%20and%20DP-FFT%2C%20in%20a%20simple%20but%20canonical%20setting%20of%202-layer%20neural%20networks%0Awith%20ReLU%20activation.%20Experiments%20on%20real-world%20datasets%20and%20architectures%20are%0Aconsistent%20with%20our%20theoretical%20insights.%20We%20also%20derive%20new%20upper%20bounds%20for%0A2-layer%20linear%20networks%20without%20the%20approximation.%20Moreover%2C%20our%20theory%0Asuggests%20a%20trade-off%20of%20privacy%20budget%20allocation%20in%20multi-phase%20fine-tuning%0Amethods%20like%20DP-LP-FFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520the%2520Training%2520Dynamics%2520of%2520Private%2520Fine-tuning%2520with%250A%2520%2520Langevin%2520diffusion%26entry.906535625%3DShuqi%2520Ke%2520and%2520Charlie%2520Hou%2520and%2520Sewoong%2520Oh%2520and%2520Giulia%2520Fanti%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520differentially%2520private%2520full%2520fine-tuning%2520%2528DP-FFT%2529%2520can%2520distort%250Apre-trained%2520backbone%2520features%2520based%2520on%2520both%2520theoretical%2520and%2520empirical%2520results.%250AWe%2520identify%2520the%2520cause%2520of%2520the%2520distortion%2520as%2520the%2520misalignment%2520between%2520the%250Apre-trained%2520backbone%2520and%2520the%2520randomly%2520initialized%2520linear%2520head.%2520We%2520prove%2520that%2520a%250Asequential%2520fine-tuning%2520strategy%2520can%2520mitigate%2520the%2520feature%2520distortion%253A%250Afirst-linear-probing-then-fine-tuning%2520%2528DP-LP-FFT%2529.%2520A%2520new%2520approximation%2520scheme%250Aallows%2520us%2520to%2520derive%2520approximate%2520upper%2520and%2520lower%2520bounds%2520on%2520the%2520training%2520loss%2520of%250ADP-LP%2520and%2520DP-FFT%252C%2520in%2520a%2520simple%2520but%2520canonical%2520setting%2520of%25202-layer%2520neural%2520networks%250Awith%2520ReLU%2520activation.%2520Experiments%2520on%2520real-world%2520datasets%2520and%2520architectures%2520are%250Aconsistent%2520with%2520our%2520theoretical%2520insights.%2520We%2520also%2520derive%2520new%2520upper%2520bounds%2520for%250A2-layer%2520linear%2520networks%2520without%2520the%2520approximation.%2520Moreover%252C%2520our%2520theory%250Asuggests%2520a%2520trade-off%2520of%2520privacy%2520budget%2520allocation%2520in%2520multi-phase%2520fine-tuning%250Amethods%2520like%2520DP-LP-FFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20the%20Training%20Dynamics%20of%20Private%20Fine-tuning%20with%0A%20%20Langevin%20diffusion&entry.906535625=Shuqi%20Ke%20and%20Charlie%20Hou%20and%20Sewoong%20Oh%20and%20Giulia%20Fanti&entry.1292438233=%20%20We%20show%20that%20differentially%20private%20full%20fine-tuning%20%28DP-FFT%29%20can%20distort%0Apre-trained%20backbone%20features%20based%20on%20both%20theoretical%20and%20empirical%20results.%0AWe%20identify%20the%20cause%20of%20the%20distortion%20as%20the%20misalignment%20between%20the%0Apre-trained%20backbone%20and%20the%20randomly%20initialized%20linear%20head.%20We%20prove%20that%20a%0Asequential%20fine-tuning%20strategy%20can%20mitigate%20the%20feature%20distortion%3A%0Afirst-linear-probing-then-fine-tuning%20%28DP-LP-FFT%29.%20A%20new%20approximation%20scheme%0Aallows%20us%20to%20derive%20approximate%20upper%20and%20lower%20bounds%20on%20the%20training%20loss%20of%0ADP-LP%20and%20DP-FFT%2C%20in%20a%20simple%20but%20canonical%20setting%20of%202-layer%20neural%20networks%0Awith%20ReLU%20activation.%20Experiments%20on%20real-world%20datasets%20and%20architectures%20are%0Aconsistent%20with%20our%20theoretical%20insights.%20We%20also%20derive%20new%20upper%20bounds%20for%0A2-layer%20linear%20networks%20without%20the%20approximation.%20Moreover%2C%20our%20theory%0Asuggests%20a%20trade-off%20of%20privacy%20budget%20allocation%20in%20multi-phase%20fine-tuning%0Amethods%20like%20DP-LP-FFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18905v2&entry.124074799=Read"},
{"title": "Tactical Decision Making for Autonomous Trucks by Deep Reinforcement\n  Learning with Total Cost of Operation Based Reward", "author": "Deepthi Pathare and Leo Laine and Morteza Haghir Chehreghani", "abstract": "  We develop a deep reinforcement learning framework for tactical decision\nmaking in an autonomous truck, specifically for Adaptive Cruise Control (ACC)\nand lane change maneuvers in a highway scenario. Our results demonstrate that\nit is beneficial to separate high-level decision-making processes and low-level\ncontrol actions between the reinforcement learning agent and the low-level\ncontrollers based on physical models. In the following, we study optimizing the\nperformance with a realistic and multi-objective reward function based on Total\nCost of Operation (TCOP) of the truck using different approaches; by adding\nweights to reward components, by normalizing the reward components and by using\ncurriculum learning techniques.\n", "link": "http://arxiv.org/abs/2403.06524v2", "date": "2025-11-07", "relevancy": 1.4982, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4738}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactical%20Decision%20Making%20for%20Autonomous%20Trucks%20by%20Deep%20Reinforcement%0A%20%20Learning%20with%20Total%20Cost%20of%20Operation%20Based%20Reward&body=Title%3A%20Tactical%20Decision%20Making%20for%20Autonomous%20Trucks%20by%20Deep%20Reinforcement%0A%20%20Learning%20with%20Total%20Cost%20of%20Operation%20Based%20Reward%0AAuthor%3A%20Deepthi%20Pathare%20and%20Leo%20Laine%20and%20Morteza%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20We%20develop%20a%20deep%20reinforcement%20learning%20framework%20for%20tactical%20decision%0Amaking%20in%20an%20autonomous%20truck%2C%20specifically%20for%20Adaptive%20Cruise%20Control%20%28ACC%29%0Aand%20lane%20change%20maneuvers%20in%20a%20highway%20scenario.%20Our%20results%20demonstrate%20that%0Ait%20is%20beneficial%20to%20separate%20high-level%20decision-making%20processes%20and%20low-level%0Acontrol%20actions%20between%20the%20reinforcement%20learning%20agent%20and%20the%20low-level%0Acontrollers%20based%20on%20physical%20models.%20In%20the%20following%2C%20we%20study%20optimizing%20the%0Aperformance%20with%20a%20realistic%20and%20multi-objective%20reward%20function%20based%20on%20Total%0ACost%20of%20Operation%20%28TCOP%29%20of%20the%20truck%20using%20different%20approaches%3B%20by%20adding%0Aweights%20to%20reward%20components%2C%20by%20normalizing%20the%20reward%20components%20and%20by%20using%0Acurriculum%20learning%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactical%2520Decision%2520Making%2520for%2520Autonomous%2520Trucks%2520by%2520Deep%2520Reinforcement%250A%2520%2520Learning%2520with%2520Total%2520Cost%2520of%2520Operation%2520Based%2520Reward%26entry.906535625%3DDeepthi%2520Pathare%2520and%2520Leo%2520Laine%2520and%2520Morteza%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520deep%2520reinforcement%2520learning%2520framework%2520for%2520tactical%2520decision%250Amaking%2520in%2520an%2520autonomous%2520truck%252C%2520specifically%2520for%2520Adaptive%2520Cruise%2520Control%2520%2528ACC%2529%250Aand%2520lane%2520change%2520maneuvers%2520in%2520a%2520highway%2520scenario.%2520Our%2520results%2520demonstrate%2520that%250Ait%2520is%2520beneficial%2520to%2520separate%2520high-level%2520decision-making%2520processes%2520and%2520low-level%250Acontrol%2520actions%2520between%2520the%2520reinforcement%2520learning%2520agent%2520and%2520the%2520low-level%250Acontrollers%2520based%2520on%2520physical%2520models.%2520In%2520the%2520following%252C%2520we%2520study%2520optimizing%2520the%250Aperformance%2520with%2520a%2520realistic%2520and%2520multi-objective%2520reward%2520function%2520based%2520on%2520Total%250ACost%2520of%2520Operation%2520%2528TCOP%2529%2520of%2520the%2520truck%2520using%2520different%2520approaches%253B%2520by%2520adding%250Aweights%2520to%2520reward%2520components%252C%2520by%2520normalizing%2520the%2520reward%2520components%2520and%2520by%2520using%250Acurriculum%2520learning%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactical%20Decision%20Making%20for%20Autonomous%20Trucks%20by%20Deep%20Reinforcement%0A%20%20Learning%20with%20Total%20Cost%20of%20Operation%20Based%20Reward&entry.906535625=Deepthi%20Pathare%20and%20Leo%20Laine%20and%20Morteza%20Haghir%20Chehreghani&entry.1292438233=%20%20We%20develop%20a%20deep%20reinforcement%20learning%20framework%20for%20tactical%20decision%0Amaking%20in%20an%20autonomous%20truck%2C%20specifically%20for%20Adaptive%20Cruise%20Control%20%28ACC%29%0Aand%20lane%20change%20maneuvers%20in%20a%20highway%20scenario.%20Our%20results%20demonstrate%20that%0Ait%20is%20beneficial%20to%20separate%20high-level%20decision-making%20processes%20and%20low-level%0Acontrol%20actions%20between%20the%20reinforcement%20learning%20agent%20and%20the%20low-level%0Acontrollers%20based%20on%20physical%20models.%20In%20the%20following%2C%20we%20study%20optimizing%20the%0Aperformance%20with%20a%20realistic%20and%20multi-objective%20reward%20function%20based%20on%20Total%0ACost%20of%20Operation%20%28TCOP%29%20of%20the%20truck%20using%20different%20approaches%3B%20by%20adding%0Aweights%20to%20reward%20components%2C%20by%20normalizing%20the%20reward%20components%20and%20by%20using%0Acurriculum%20learning%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06524v2&entry.124074799=Read"},
{"title": "ProDER: A Continual Learning Approach for Fault Prediction in Evolving\n  Smart Grids", "author": "Emad Efatinasab and Nahal Azadi and Davide Dalle Pezze and Gian Antonio Susto and Chuadhry Mujeeb Ahmed and Mirco Rampazzo", "abstract": "  As smart grids evolve to meet growing energy demands and modern operational\nchallenges, the ability to accurately predict faults becomes increasingly\ncritical. However, existing AI-based fault prediction models struggle to ensure\nreliability in evolving environments where they are required to adapt to new\nfault types and operational zones. In this paper, we propose a continual\nlearning (CL) framework in the smart grid context to evolve the model together\nwith the environment. We design four realistic evaluation scenarios grounded in\nclass-incremental and domain-incremental learning to emulate evolving grid\nconditions. We further introduce Prototype-based Dark Experience Replay\n(ProDER), a unified replay-based approach that integrates prototype-based\nfeature regularization, logit distillation, and a prototype-guided replay\nmemory. ProDER achieves the best performance among tested CL techniques, with\nonly a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone\nprediction. These results demonstrate the practicality of CL for scalable,\nreal-world fault prediction in smart grids.\n", "link": "http://arxiv.org/abs/2511.05420v1", "date": "2025-11-07", "relevancy": 1.4237, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4724}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProDER%3A%20A%20Continual%20Learning%20Approach%20for%20Fault%20Prediction%20in%20Evolving%0A%20%20Smart%20Grids&body=Title%3A%20ProDER%3A%20A%20Continual%20Learning%20Approach%20for%20Fault%20Prediction%20in%20Evolving%0A%20%20Smart%20Grids%0AAuthor%3A%20Emad%20Efatinasab%20and%20Nahal%20Azadi%20and%20Davide%20Dalle%20Pezze%20and%20Gian%20Antonio%20Susto%20and%20Chuadhry%20Mujeeb%20Ahmed%20and%20Mirco%20Rampazzo%0AAbstract%3A%20%20%20As%20smart%20grids%20evolve%20to%20meet%20growing%20energy%20demands%20and%20modern%20operational%0Achallenges%2C%20the%20ability%20to%20accurately%20predict%20faults%20becomes%20increasingly%0Acritical.%20However%2C%20existing%20AI-based%20fault%20prediction%20models%20struggle%20to%20ensure%0Areliability%20in%20evolving%20environments%20where%20they%20are%20required%20to%20adapt%20to%20new%0Afault%20types%20and%20operational%20zones.%20In%20this%20paper%2C%20we%20propose%20a%20continual%0Alearning%20%28CL%29%20framework%20in%20the%20smart%20grid%20context%20to%20evolve%20the%20model%20together%0Awith%20the%20environment.%20We%20design%20four%20realistic%20evaluation%20scenarios%20grounded%20in%0Aclass-incremental%20and%20domain-incremental%20learning%20to%20emulate%20evolving%20grid%0Aconditions.%20We%20further%20introduce%20Prototype-based%20Dark%20Experience%20Replay%0A%28ProDER%29%2C%20a%20unified%20replay-based%20approach%20that%20integrates%20prototype-based%0Afeature%20regularization%2C%20logit%20distillation%2C%20and%20a%20prototype-guided%20replay%0Amemory.%20ProDER%20achieves%20the%20best%20performance%20among%20tested%20CL%20techniques%2C%20with%0Aonly%20a%200.045%20accuracy%20drop%20for%20fault%20type%20prediction%20and%200.015%20for%20fault%20zone%0Aprediction.%20These%20results%20demonstrate%20the%20practicality%20of%20CL%20for%20scalable%2C%0Areal-world%20fault%20prediction%20in%20smart%20grids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProDER%253A%2520A%2520Continual%2520Learning%2520Approach%2520for%2520Fault%2520Prediction%2520in%2520Evolving%250A%2520%2520Smart%2520Grids%26entry.906535625%3DEmad%2520Efatinasab%2520and%2520Nahal%2520Azadi%2520and%2520Davide%2520Dalle%2520Pezze%2520and%2520Gian%2520Antonio%2520Susto%2520and%2520Chuadhry%2520Mujeeb%2520Ahmed%2520and%2520Mirco%2520Rampazzo%26entry.1292438233%3D%2520%2520As%2520smart%2520grids%2520evolve%2520to%2520meet%2520growing%2520energy%2520demands%2520and%2520modern%2520operational%250Achallenges%252C%2520the%2520ability%2520to%2520accurately%2520predict%2520faults%2520becomes%2520increasingly%250Acritical.%2520However%252C%2520existing%2520AI-based%2520fault%2520prediction%2520models%2520struggle%2520to%2520ensure%250Areliability%2520in%2520evolving%2520environments%2520where%2520they%2520are%2520required%2520to%2520adapt%2520to%2520new%250Afault%2520types%2520and%2520operational%2520zones.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520continual%250Alearning%2520%2528CL%2529%2520framework%2520in%2520the%2520smart%2520grid%2520context%2520to%2520evolve%2520the%2520model%2520together%250Awith%2520the%2520environment.%2520We%2520design%2520four%2520realistic%2520evaluation%2520scenarios%2520grounded%2520in%250Aclass-incremental%2520and%2520domain-incremental%2520learning%2520to%2520emulate%2520evolving%2520grid%250Aconditions.%2520We%2520further%2520introduce%2520Prototype-based%2520Dark%2520Experience%2520Replay%250A%2528ProDER%2529%252C%2520a%2520unified%2520replay-based%2520approach%2520that%2520integrates%2520prototype-based%250Afeature%2520regularization%252C%2520logit%2520distillation%252C%2520and%2520a%2520prototype-guided%2520replay%250Amemory.%2520ProDER%2520achieves%2520the%2520best%2520performance%2520among%2520tested%2520CL%2520techniques%252C%2520with%250Aonly%2520a%25200.045%2520accuracy%2520drop%2520for%2520fault%2520type%2520prediction%2520and%25200.015%2520for%2520fault%2520zone%250Aprediction.%2520These%2520results%2520demonstrate%2520the%2520practicality%2520of%2520CL%2520for%2520scalable%252C%250Areal-world%2520fault%2520prediction%2520in%2520smart%2520grids.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProDER%3A%20A%20Continual%20Learning%20Approach%20for%20Fault%20Prediction%20in%20Evolving%0A%20%20Smart%20Grids&entry.906535625=Emad%20Efatinasab%20and%20Nahal%20Azadi%20and%20Davide%20Dalle%20Pezze%20and%20Gian%20Antonio%20Susto%20and%20Chuadhry%20Mujeeb%20Ahmed%20and%20Mirco%20Rampazzo&entry.1292438233=%20%20As%20smart%20grids%20evolve%20to%20meet%20growing%20energy%20demands%20and%20modern%20operational%0Achallenges%2C%20the%20ability%20to%20accurately%20predict%20faults%20becomes%20increasingly%0Acritical.%20However%2C%20existing%20AI-based%20fault%20prediction%20models%20struggle%20to%20ensure%0Areliability%20in%20evolving%20environments%20where%20they%20are%20required%20to%20adapt%20to%20new%0Afault%20types%20and%20operational%20zones.%20In%20this%20paper%2C%20we%20propose%20a%20continual%0Alearning%20%28CL%29%20framework%20in%20the%20smart%20grid%20context%20to%20evolve%20the%20model%20together%0Awith%20the%20environment.%20We%20design%20four%20realistic%20evaluation%20scenarios%20grounded%20in%0Aclass-incremental%20and%20domain-incremental%20learning%20to%20emulate%20evolving%20grid%0Aconditions.%20We%20further%20introduce%20Prototype-based%20Dark%20Experience%20Replay%0A%28ProDER%29%2C%20a%20unified%20replay-based%20approach%20that%20integrates%20prototype-based%0Afeature%20regularization%2C%20logit%20distillation%2C%20and%20a%20prototype-guided%20replay%0Amemory.%20ProDER%20achieves%20the%20best%20performance%20among%20tested%20CL%20techniques%2C%20with%0Aonly%20a%200.045%20accuracy%20drop%20for%20fault%20type%20prediction%20and%200.015%20for%20fault%20zone%0Aprediction.%20These%20results%20demonstrate%20the%20practicality%20of%20CL%20for%20scalable%2C%0Areal-world%20fault%20prediction%20in%20smart%20grids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05420v1&entry.124074799=Read"},
{"title": "\"I Like That You Have to Poke Around\": Instructors on How Experiential\n  Approaches to AI Literacy Spark Inquiry and Critical Thinking", "author": "Aparna Maya Warrier and Arav Agarwal and Jaromir Savelka and Christopher Bogart and Heather Burte", "abstract": "  As artificial intelligence (AI) increasingly shapes decision-making across\ndomains, there is a growing need to support AI literacy among learners beyond\ncomputer science. However, many current approaches rely on programming-heavy\ntools or abstract lecture-based content, limiting accessibility for non-STEM\naudiences. This paper presents findings from a study of AI User, a modular,\nweb-based curriculum that teaches core AI concepts through interactive, no-code\nprojects grounded in real-world scenarios. The curriculum includes eight\nprojects; this study focuses on instructor feedback on Projects 5-8, which\naddress applied topics such as natural language processing, computer vision,\ndecision support, and responsible AI. Fifteen community college instructors\nparticipated in structured focus groups, completing the projects as learners\nand providing feedback through individual reflection and group discussion.\nUsing thematic analysis, we examined how instructors evaluated the design,\ninstructional value, and classroom applicability of these experiential\nactivities. Findings highlight instructors' appreciation for exploratory tasks,\nrole-based simulations, and real-world relevance, while also surfacing design\ntrade-offs around cognitive load, guidance, and adaptability for diverse\nlearners. This work extends prior research on AI literacy by centering\ninstructor perspectives on teaching complex AI topics without code. It offers\nactionable insights for designing inclusive, experiential AI learning resources\nthat scale across disciplines and learner backgrounds.\n", "link": "http://arxiv.org/abs/2511.05430v1", "date": "2025-11-07", "relevancy": 1.3927, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5005}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22I%20Like%20That%20You%20Have%20to%20Poke%20Around%22%3A%20Instructors%20on%20How%20Experiential%0A%20%20Approaches%20to%20AI%20Literacy%20Spark%20Inquiry%20and%20Critical%20Thinking&body=Title%3A%20%22I%20Like%20That%20You%20Have%20to%20Poke%20Around%22%3A%20Instructors%20on%20How%20Experiential%0A%20%20Approaches%20to%20AI%20Literacy%20Spark%20Inquiry%20and%20Critical%20Thinking%0AAuthor%3A%20Aparna%20Maya%20Warrier%20and%20Arav%20Agarwal%20and%20Jaromir%20Savelka%20and%20Christopher%20Bogart%20and%20Heather%20Burte%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20%28AI%29%20increasingly%20shapes%20decision-making%20across%0Adomains%2C%20there%20is%20a%20growing%20need%20to%20support%20AI%20literacy%20among%20learners%20beyond%0Acomputer%20science.%20However%2C%20many%20current%20approaches%20rely%20on%20programming-heavy%0Atools%20or%20abstract%20lecture-based%20content%2C%20limiting%20accessibility%20for%20non-STEM%0Aaudiences.%20This%20paper%20presents%20findings%20from%20a%20study%20of%20AI%20User%2C%20a%20modular%2C%0Aweb-based%20curriculum%20that%20teaches%20core%20AI%20concepts%20through%20interactive%2C%20no-code%0Aprojects%20grounded%20in%20real-world%20scenarios.%20The%20curriculum%20includes%20eight%0Aprojects%3B%20this%20study%20focuses%20on%20instructor%20feedback%20on%20Projects%205-8%2C%20which%0Aaddress%20applied%20topics%20such%20as%20natural%20language%20processing%2C%20computer%20vision%2C%0Adecision%20support%2C%20and%20responsible%20AI.%20Fifteen%20community%20college%20instructors%0Aparticipated%20in%20structured%20focus%20groups%2C%20completing%20the%20projects%20as%20learners%0Aand%20providing%20feedback%20through%20individual%20reflection%20and%20group%20discussion.%0AUsing%20thematic%20analysis%2C%20we%20examined%20how%20instructors%20evaluated%20the%20design%2C%0Ainstructional%20value%2C%20and%20classroom%20applicability%20of%20these%20experiential%0Aactivities.%20Findings%20highlight%20instructors%27%20appreciation%20for%20exploratory%20tasks%2C%0Arole-based%20simulations%2C%20and%20real-world%20relevance%2C%20while%20also%20surfacing%20design%0Atrade-offs%20around%20cognitive%20load%2C%20guidance%2C%20and%20adaptability%20for%20diverse%0Alearners.%20This%20work%20extends%20prior%20research%20on%20AI%20literacy%20by%20centering%0Ainstructor%20perspectives%20on%20teaching%20complex%20AI%20topics%20without%20code.%20It%20offers%0Aactionable%20insights%20for%20designing%20inclusive%2C%20experiential%20AI%20learning%20resources%0Athat%20scale%20across%20disciplines%20and%20learner%20backgrounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522I%2520Like%2520That%2520You%2520Have%2520to%2520Poke%2520Around%2522%253A%2520Instructors%2520on%2520How%2520Experiential%250A%2520%2520Approaches%2520to%2520AI%2520Literacy%2520Spark%2520Inquiry%2520and%2520Critical%2520Thinking%26entry.906535625%3DAparna%2520Maya%2520Warrier%2520and%2520Arav%2520Agarwal%2520and%2520Jaromir%2520Savelka%2520and%2520Christopher%2520Bogart%2520and%2520Heather%2520Burte%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520%2528AI%2529%2520increasingly%2520shapes%2520decision-making%2520across%250Adomains%252C%2520there%2520is%2520a%2520growing%2520need%2520to%2520support%2520AI%2520literacy%2520among%2520learners%2520beyond%250Acomputer%2520science.%2520However%252C%2520many%2520current%2520approaches%2520rely%2520on%2520programming-heavy%250Atools%2520or%2520abstract%2520lecture-based%2520content%252C%2520limiting%2520accessibility%2520for%2520non-STEM%250Aaudiences.%2520This%2520paper%2520presents%2520findings%2520from%2520a%2520study%2520of%2520AI%2520User%252C%2520a%2520modular%252C%250Aweb-based%2520curriculum%2520that%2520teaches%2520core%2520AI%2520concepts%2520through%2520interactive%252C%2520no-code%250Aprojects%2520grounded%2520in%2520real-world%2520scenarios.%2520The%2520curriculum%2520includes%2520eight%250Aprojects%253B%2520this%2520study%2520focuses%2520on%2520instructor%2520feedback%2520on%2520Projects%25205-8%252C%2520which%250Aaddress%2520applied%2520topics%2520such%2520as%2520natural%2520language%2520processing%252C%2520computer%2520vision%252C%250Adecision%2520support%252C%2520and%2520responsible%2520AI.%2520Fifteen%2520community%2520college%2520instructors%250Aparticipated%2520in%2520structured%2520focus%2520groups%252C%2520completing%2520the%2520projects%2520as%2520learners%250Aand%2520providing%2520feedback%2520through%2520individual%2520reflection%2520and%2520group%2520discussion.%250AUsing%2520thematic%2520analysis%252C%2520we%2520examined%2520how%2520instructors%2520evaluated%2520the%2520design%252C%250Ainstructional%2520value%252C%2520and%2520classroom%2520applicability%2520of%2520these%2520experiential%250Aactivities.%2520Findings%2520highlight%2520instructors%2527%2520appreciation%2520for%2520exploratory%2520tasks%252C%250Arole-based%2520simulations%252C%2520and%2520real-world%2520relevance%252C%2520while%2520also%2520surfacing%2520design%250Atrade-offs%2520around%2520cognitive%2520load%252C%2520guidance%252C%2520and%2520adaptability%2520for%2520diverse%250Alearners.%2520This%2520work%2520extends%2520prior%2520research%2520on%2520AI%2520literacy%2520by%2520centering%250Ainstructor%2520perspectives%2520on%2520teaching%2520complex%2520AI%2520topics%2520without%2520code.%2520It%2520offers%250Aactionable%2520insights%2520for%2520designing%2520inclusive%252C%2520experiential%2520AI%2520learning%2520resources%250Athat%2520scale%2520across%2520disciplines%2520and%2520learner%2520backgrounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22I%20Like%20That%20You%20Have%20to%20Poke%20Around%22%3A%20Instructors%20on%20How%20Experiential%0A%20%20Approaches%20to%20AI%20Literacy%20Spark%20Inquiry%20and%20Critical%20Thinking&entry.906535625=Aparna%20Maya%20Warrier%20and%20Arav%20Agarwal%20and%20Jaromir%20Savelka%20and%20Christopher%20Bogart%20and%20Heather%20Burte&entry.1292438233=%20%20As%20artificial%20intelligence%20%28AI%29%20increasingly%20shapes%20decision-making%20across%0Adomains%2C%20there%20is%20a%20growing%20need%20to%20support%20AI%20literacy%20among%20learners%20beyond%0Acomputer%20science.%20However%2C%20many%20current%20approaches%20rely%20on%20programming-heavy%0Atools%20or%20abstract%20lecture-based%20content%2C%20limiting%20accessibility%20for%20non-STEM%0Aaudiences.%20This%20paper%20presents%20findings%20from%20a%20study%20of%20AI%20User%2C%20a%20modular%2C%0Aweb-based%20curriculum%20that%20teaches%20core%20AI%20concepts%20through%20interactive%2C%20no-code%0Aprojects%20grounded%20in%20real-world%20scenarios.%20The%20curriculum%20includes%20eight%0Aprojects%3B%20this%20study%20focuses%20on%20instructor%20feedback%20on%20Projects%205-8%2C%20which%0Aaddress%20applied%20topics%20such%20as%20natural%20language%20processing%2C%20computer%20vision%2C%0Adecision%20support%2C%20and%20responsible%20AI.%20Fifteen%20community%20college%20instructors%0Aparticipated%20in%20structured%20focus%20groups%2C%20completing%20the%20projects%20as%20learners%0Aand%20providing%20feedback%20through%20individual%20reflection%20and%20group%20discussion.%0AUsing%20thematic%20analysis%2C%20we%20examined%20how%20instructors%20evaluated%20the%20design%2C%0Ainstructional%20value%2C%20and%20classroom%20applicability%20of%20these%20experiential%0Aactivities.%20Findings%20highlight%20instructors%27%20appreciation%20for%20exploratory%20tasks%2C%0Arole-based%20simulations%2C%20and%20real-world%20relevance%2C%20while%20also%20surfacing%20design%0Atrade-offs%20around%20cognitive%20load%2C%20guidance%2C%20and%20adaptability%20for%20diverse%0Alearners.%20This%20work%20extends%20prior%20research%20on%20AI%20literacy%20by%20centering%0Ainstructor%20perspectives%20on%20teaching%20complex%20AI%20topics%20without%20code.%20It%20offers%0Aactionable%20insights%20for%20designing%20inclusive%2C%20experiential%20AI%20learning%20resources%0Athat%20scale%20across%20disciplines%20and%20learner%20backgrounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05430v1&entry.124074799=Read"},
{"title": "Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants", "author": "Bozhi You and Irene Wang and Zelal Su Mustafaoglu and Abhinav Jangda and Ang\u00e9lica Moreira and Roshan Dathathri and Divya Mahajan and Keshav Pingali", "abstract": "  Attention is a fundamental building block of large language models (LLMs), so\nthere have been many efforts to implement it efficiently. For example,\nFlashAttention leverages tiling and kernel fusion to optimize attention.\nRecently, a number of variants of attention have been introduced to enhance\nmodel quality or efficiency. Supporting them efficiently remains difficult\nsince they usually require specialized kernels or hand-tuned implementations.\nFlexAttention recently addressed part of this gap by using static programming\ntemplates to support FlashAttention-like kernels for a subset of attention\nvariants.\n  In this paper, we introduce Flashlight, a compiler-native framework within\nthe PyTorch ecosystem that automatically generates fused, FlashAttention-style\nkernels for arbitrary attention-based programs, without relying on static\ntemplates or predefined kernel specializations. Flashlight leverages PyTorch's\ncompilation workflow to fuse and tile attention computations transparently,\nenabling efficient execution for diverse attention patterns. Not only does it\nsupport all variants expressible in the FlexAttention model but it also handles\nmore general, data-dependent attention formulations that are beyond the\ncapabilities of FlexAttention.\n  Our results show that Flashlight produces kernels with competitive or\nsuperior performance to FlexAttention, while offering the flexibility of native\nPyTorch code, enabling developers to rapidly explore new attention models\nwithout sacrificing performance.\n", "link": "http://arxiv.org/abs/2511.02043v3", "date": "2025-11-07", "relevancy": 1.391, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4365}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flashlight%3A%20PyTorch%20Compiler%20Extensions%20to%20Accelerate%20Attention%20Variants&body=Title%3A%20Flashlight%3A%20PyTorch%20Compiler%20Extensions%20to%20Accelerate%20Attention%20Variants%0AAuthor%3A%20Bozhi%20You%20and%20Irene%20Wang%20and%20Zelal%20Su%20Mustafaoglu%20and%20Abhinav%20Jangda%20and%20Ang%C3%A9lica%20Moreira%20and%20Roshan%20Dathathri%20and%20Divya%20Mahajan%20and%20Keshav%20Pingali%0AAbstract%3A%20%20%20Attention%20is%20a%20fundamental%20building%20block%20of%20large%20language%20models%20%28LLMs%29%2C%20so%0Athere%20have%20been%20many%20efforts%20to%20implement%20it%20efficiently.%20For%20example%2C%0AFlashAttention%20leverages%20tiling%20and%20kernel%20fusion%20to%20optimize%20attention.%0ARecently%2C%20a%20number%20of%20variants%20of%20attention%20have%20been%20introduced%20to%20enhance%0Amodel%20quality%20or%20efficiency.%20Supporting%20them%20efficiently%20remains%20difficult%0Asince%20they%20usually%20require%20specialized%20kernels%20or%20hand-tuned%20implementations.%0AFlexAttention%20recently%20addressed%20part%20of%20this%20gap%20by%20using%20static%20programming%0Atemplates%20to%20support%20FlashAttention-like%20kernels%20for%20a%20subset%20of%20attention%0Avariants.%0A%20%20In%20this%20paper%2C%20we%20introduce%20Flashlight%2C%20a%20compiler-native%20framework%20within%0Athe%20PyTorch%20ecosystem%20that%20automatically%20generates%20fused%2C%20FlashAttention-style%0Akernels%20for%20arbitrary%20attention-based%20programs%2C%20without%20relying%20on%20static%0Atemplates%20or%20predefined%20kernel%20specializations.%20Flashlight%20leverages%20PyTorch%27s%0Acompilation%20workflow%20to%20fuse%20and%20tile%20attention%20computations%20transparently%2C%0Aenabling%20efficient%20execution%20for%20diverse%20attention%20patterns.%20Not%20only%20does%20it%0Asupport%20all%20variants%20expressible%20in%20the%20FlexAttention%20model%20but%20it%20also%20handles%0Amore%20general%2C%20data-dependent%20attention%20formulations%20that%20are%20beyond%20the%0Acapabilities%20of%20FlexAttention.%0A%20%20Our%20results%20show%20that%20Flashlight%20produces%20kernels%20with%20competitive%20or%0Asuperior%20performance%20to%20FlexAttention%2C%20while%20offering%20the%20flexibility%20of%20native%0APyTorch%20code%2C%20enabling%20developers%20to%20rapidly%20explore%20new%20attention%20models%0Awithout%20sacrificing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.02043v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashlight%253A%2520PyTorch%2520Compiler%2520Extensions%2520to%2520Accelerate%2520Attention%2520Variants%26entry.906535625%3DBozhi%2520You%2520and%2520Irene%2520Wang%2520and%2520Zelal%2520Su%2520Mustafaoglu%2520and%2520Abhinav%2520Jangda%2520and%2520Ang%25C3%25A9lica%2520Moreira%2520and%2520Roshan%2520Dathathri%2520and%2520Divya%2520Mahajan%2520and%2520Keshav%2520Pingali%26entry.1292438233%3D%2520%2520Attention%2520is%2520a%2520fundamental%2520building%2520block%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520so%250Athere%2520have%2520been%2520many%2520efforts%2520to%2520implement%2520it%2520efficiently.%2520For%2520example%252C%250AFlashAttention%2520leverages%2520tiling%2520and%2520kernel%2520fusion%2520to%2520optimize%2520attention.%250ARecently%252C%2520a%2520number%2520of%2520variants%2520of%2520attention%2520have%2520been%2520introduced%2520to%2520enhance%250Amodel%2520quality%2520or%2520efficiency.%2520Supporting%2520them%2520efficiently%2520remains%2520difficult%250Asince%2520they%2520usually%2520require%2520specialized%2520kernels%2520or%2520hand-tuned%2520implementations.%250AFlexAttention%2520recently%2520addressed%2520part%2520of%2520this%2520gap%2520by%2520using%2520static%2520programming%250Atemplates%2520to%2520support%2520FlashAttention-like%2520kernels%2520for%2520a%2520subset%2520of%2520attention%250Avariants.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Flashlight%252C%2520a%2520compiler-native%2520framework%2520within%250Athe%2520PyTorch%2520ecosystem%2520that%2520automatically%2520generates%2520fused%252C%2520FlashAttention-style%250Akernels%2520for%2520arbitrary%2520attention-based%2520programs%252C%2520without%2520relying%2520on%2520static%250Atemplates%2520or%2520predefined%2520kernel%2520specializations.%2520Flashlight%2520leverages%2520PyTorch%2527s%250Acompilation%2520workflow%2520to%2520fuse%2520and%2520tile%2520attention%2520computations%2520transparently%252C%250Aenabling%2520efficient%2520execution%2520for%2520diverse%2520attention%2520patterns.%2520Not%2520only%2520does%2520it%250Asupport%2520all%2520variants%2520expressible%2520in%2520the%2520FlexAttention%2520model%2520but%2520it%2520also%2520handles%250Amore%2520general%252C%2520data-dependent%2520attention%2520formulations%2520that%2520are%2520beyond%2520the%250Acapabilities%2520of%2520FlexAttention.%250A%2520%2520Our%2520results%2520show%2520that%2520Flashlight%2520produces%2520kernels%2520with%2520competitive%2520or%250Asuperior%2520performance%2520to%2520FlexAttention%252C%2520while%2520offering%2520the%2520flexibility%2520of%2520native%250APyTorch%2520code%252C%2520enabling%2520developers%2520to%2520rapidly%2520explore%2520new%2520attention%2520models%250Awithout%2520sacrificing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02043v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flashlight%3A%20PyTorch%20Compiler%20Extensions%20to%20Accelerate%20Attention%20Variants&entry.906535625=Bozhi%20You%20and%20Irene%20Wang%20and%20Zelal%20Su%20Mustafaoglu%20and%20Abhinav%20Jangda%20and%20Ang%C3%A9lica%20Moreira%20and%20Roshan%20Dathathri%20and%20Divya%20Mahajan%20and%20Keshav%20Pingali&entry.1292438233=%20%20Attention%20is%20a%20fundamental%20building%20block%20of%20large%20language%20models%20%28LLMs%29%2C%20so%0Athere%20have%20been%20many%20efforts%20to%20implement%20it%20efficiently.%20For%20example%2C%0AFlashAttention%20leverages%20tiling%20and%20kernel%20fusion%20to%20optimize%20attention.%0ARecently%2C%20a%20number%20of%20variants%20of%20attention%20have%20been%20introduced%20to%20enhance%0Amodel%20quality%20or%20efficiency.%20Supporting%20them%20efficiently%20remains%20difficult%0Asince%20they%20usually%20require%20specialized%20kernels%20or%20hand-tuned%20implementations.%0AFlexAttention%20recently%20addressed%20part%20of%20this%20gap%20by%20using%20static%20programming%0Atemplates%20to%20support%20FlashAttention-like%20kernels%20for%20a%20subset%20of%20attention%0Avariants.%0A%20%20In%20this%20paper%2C%20we%20introduce%20Flashlight%2C%20a%20compiler-native%20framework%20within%0Athe%20PyTorch%20ecosystem%20that%20automatically%20generates%20fused%2C%20FlashAttention-style%0Akernels%20for%20arbitrary%20attention-based%20programs%2C%20without%20relying%20on%20static%0Atemplates%20or%20predefined%20kernel%20specializations.%20Flashlight%20leverages%20PyTorch%27s%0Acompilation%20workflow%20to%20fuse%20and%20tile%20attention%20computations%20transparently%2C%0Aenabling%20efficient%20execution%20for%20diverse%20attention%20patterns.%20Not%20only%20does%20it%0Asupport%20all%20variants%20expressible%20in%20the%20FlexAttention%20model%20but%20it%20also%20handles%0Amore%20general%2C%20data-dependent%20attention%20formulations%20that%20are%20beyond%20the%0Acapabilities%20of%20FlexAttention.%0A%20%20Our%20results%20show%20that%20Flashlight%20produces%20kernels%20with%20competitive%20or%0Asuperior%20performance%20to%20FlexAttention%2C%20while%20offering%20the%20flexibility%20of%20native%0APyTorch%20code%2C%20enabling%20developers%20to%20rapidly%20explore%20new%20attention%20models%0Awithout%20sacrificing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.02043v3&entry.124074799=Read"},
{"title": "Steering Language Models with Weight Arithmetic", "author": "Constanza Fierro and Fabien Roger", "abstract": "  Providing high-quality feedback to Large Language Models (LLMs) on a diverse\ntraining distribution can be difficult and expensive, and providing feedback\nonly on a narrow distribution can result in unintended generalizations. To\nbetter leverage narrow training data, we propose contrastive weight steering, a\nsimple post-training method that edits the model parameters using weight\narithmetic. We isolate a behavior direction in weight-space by subtracting the\nweight deltas from two small fine-tunes -- one that induces the desired\nbehavior and another that induces its opposite -- and then add or remove this\ndirection to modify the model's weights. We apply this technique to mitigate\nsycophancy and induce misalignment, and find that weight steering often\ngeneralizes further than activation steering, achieving stronger\nout-of-distribution behavioral control before degrading general capabilities.\nWe also show that, in the context of task-specific fine-tuning, weight steering\ncan partially mitigate undesired behavioral drift: it can reduce sycophancy and\nunder-refusals introduced during fine-tuning while preserving task performance\ngains. Finally, we provide preliminary evidence that emergent misalignment can\nbe detected by measuring the similarity between fine-tuning updates and an\n\"evil\" weight direction, suggesting that it may be possible to monitor the\nevolution of weights during training and detect rare misaligned behaviors that\nnever manifest during training or evaluations.\n", "link": "http://arxiv.org/abs/2511.05408v1", "date": "2025-11-07", "relevancy": 1.3653, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4567}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Language%20Models%20with%20Weight%20Arithmetic&body=Title%3A%20Steering%20Language%20Models%20with%20Weight%20Arithmetic%0AAuthor%3A%20Constanza%20Fierro%20and%20Fabien%20Roger%0AAbstract%3A%20%20%20Providing%20high-quality%20feedback%20to%20Large%20Language%20Models%20%28LLMs%29%20on%20a%20diverse%0Atraining%20distribution%20can%20be%20difficult%20and%20expensive%2C%20and%20providing%20feedback%0Aonly%20on%20a%20narrow%20distribution%20can%20result%20in%20unintended%20generalizations.%20To%0Abetter%20leverage%20narrow%20training%20data%2C%20we%20propose%20contrastive%20weight%20steering%2C%20a%0Asimple%20post-training%20method%20that%20edits%20the%20model%20parameters%20using%20weight%0Aarithmetic.%20We%20isolate%20a%20behavior%20direction%20in%20weight-space%20by%20subtracting%20the%0Aweight%20deltas%20from%20two%20small%20fine-tunes%20--%20one%20that%20induces%20the%20desired%0Abehavior%20and%20another%20that%20induces%20its%20opposite%20--%20and%20then%20add%20or%20remove%20this%0Adirection%20to%20modify%20the%20model%27s%20weights.%20We%20apply%20this%20technique%20to%20mitigate%0Asycophancy%20and%20induce%20misalignment%2C%20and%20find%20that%20weight%20steering%20often%0Ageneralizes%20further%20than%20activation%20steering%2C%20achieving%20stronger%0Aout-of-distribution%20behavioral%20control%20before%20degrading%20general%20capabilities.%0AWe%20also%20show%20that%2C%20in%20the%20context%20of%20task-specific%20fine-tuning%2C%20weight%20steering%0Acan%20partially%20mitigate%20undesired%20behavioral%20drift%3A%20it%20can%20reduce%20sycophancy%20and%0Aunder-refusals%20introduced%20during%20fine-tuning%20while%20preserving%20task%20performance%0Agains.%20Finally%2C%20we%20provide%20preliminary%20evidence%20that%20emergent%20misalignment%20can%0Abe%20detected%20by%20measuring%20the%20similarity%20between%20fine-tuning%20updates%20and%20an%0A%22evil%22%20weight%20direction%2C%20suggesting%20that%20it%20may%20be%20possible%20to%20monitor%20the%0Aevolution%20of%20weights%20during%20training%20and%20detect%20rare%20misaligned%20behaviors%20that%0Anever%20manifest%20during%20training%20or%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Language%2520Models%2520with%2520Weight%2520Arithmetic%26entry.906535625%3DConstanza%2520Fierro%2520and%2520Fabien%2520Roger%26entry.1292438233%3D%2520%2520Providing%2520high-quality%2520feedback%2520to%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520a%2520diverse%250Atraining%2520distribution%2520can%2520be%2520difficult%2520and%2520expensive%252C%2520and%2520providing%2520feedback%250Aonly%2520on%2520a%2520narrow%2520distribution%2520can%2520result%2520in%2520unintended%2520generalizations.%2520To%250Abetter%2520leverage%2520narrow%2520training%2520data%252C%2520we%2520propose%2520contrastive%2520weight%2520steering%252C%2520a%250Asimple%2520post-training%2520method%2520that%2520edits%2520the%2520model%2520parameters%2520using%2520weight%250Aarithmetic.%2520We%2520isolate%2520a%2520behavior%2520direction%2520in%2520weight-space%2520by%2520subtracting%2520the%250Aweight%2520deltas%2520from%2520two%2520small%2520fine-tunes%2520--%2520one%2520that%2520induces%2520the%2520desired%250Abehavior%2520and%2520another%2520that%2520induces%2520its%2520opposite%2520--%2520and%2520then%2520add%2520or%2520remove%2520this%250Adirection%2520to%2520modify%2520the%2520model%2527s%2520weights.%2520We%2520apply%2520this%2520technique%2520to%2520mitigate%250Asycophancy%2520and%2520induce%2520misalignment%252C%2520and%2520find%2520that%2520weight%2520steering%2520often%250Ageneralizes%2520further%2520than%2520activation%2520steering%252C%2520achieving%2520stronger%250Aout-of-distribution%2520behavioral%2520control%2520before%2520degrading%2520general%2520capabilities.%250AWe%2520also%2520show%2520that%252C%2520in%2520the%2520context%2520of%2520task-specific%2520fine-tuning%252C%2520weight%2520steering%250Acan%2520partially%2520mitigate%2520undesired%2520behavioral%2520drift%253A%2520it%2520can%2520reduce%2520sycophancy%2520and%250Aunder-refusals%2520introduced%2520during%2520fine-tuning%2520while%2520preserving%2520task%2520performance%250Agains.%2520Finally%252C%2520we%2520provide%2520preliminary%2520evidence%2520that%2520emergent%2520misalignment%2520can%250Abe%2520detected%2520by%2520measuring%2520the%2520similarity%2520between%2520fine-tuning%2520updates%2520and%2520an%250A%2522evil%2522%2520weight%2520direction%252C%2520suggesting%2520that%2520it%2520may%2520be%2520possible%2520to%2520monitor%2520the%250Aevolution%2520of%2520weights%2520during%2520training%2520and%2520detect%2520rare%2520misaligned%2520behaviors%2520that%250Anever%2520manifest%2520during%2520training%2520or%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Language%20Models%20with%20Weight%20Arithmetic&entry.906535625=Constanza%20Fierro%20and%20Fabien%20Roger&entry.1292438233=%20%20Providing%20high-quality%20feedback%20to%20Large%20Language%20Models%20%28LLMs%29%20on%20a%20diverse%0Atraining%20distribution%20can%20be%20difficult%20and%20expensive%2C%20and%20providing%20feedback%0Aonly%20on%20a%20narrow%20distribution%20can%20result%20in%20unintended%20generalizations.%20To%0Abetter%20leverage%20narrow%20training%20data%2C%20we%20propose%20contrastive%20weight%20steering%2C%20a%0Asimple%20post-training%20method%20that%20edits%20the%20model%20parameters%20using%20weight%0Aarithmetic.%20We%20isolate%20a%20behavior%20direction%20in%20weight-space%20by%20subtracting%20the%0Aweight%20deltas%20from%20two%20small%20fine-tunes%20--%20one%20that%20induces%20the%20desired%0Abehavior%20and%20another%20that%20induces%20its%20opposite%20--%20and%20then%20add%20or%20remove%20this%0Adirection%20to%20modify%20the%20model%27s%20weights.%20We%20apply%20this%20technique%20to%20mitigate%0Asycophancy%20and%20induce%20misalignment%2C%20and%20find%20that%20weight%20steering%20often%0Ageneralizes%20further%20than%20activation%20steering%2C%20achieving%20stronger%0Aout-of-distribution%20behavioral%20control%20before%20degrading%20general%20capabilities.%0AWe%20also%20show%20that%2C%20in%20the%20context%20of%20task-specific%20fine-tuning%2C%20weight%20steering%0Acan%20partially%20mitigate%20undesired%20behavioral%20drift%3A%20it%20can%20reduce%20sycophancy%20and%0Aunder-refusals%20introduced%20during%20fine-tuning%20while%20preserving%20task%20performance%0Agains.%20Finally%2C%20we%20provide%20preliminary%20evidence%20that%20emergent%20misalignment%20can%0Abe%20detected%20by%20measuring%20the%20similarity%20between%20fine-tuning%20updates%20and%20an%0A%22evil%22%20weight%20direction%2C%20suggesting%20that%20it%20may%20be%20possible%20to%20monitor%20the%0Aevolution%20of%20weights%20during%20training%20and%20detect%20rare%20misaligned%20behaviors%20that%0Anever%20manifest%20during%20training%20or%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05408v1&entry.124074799=Read"},
{"title": "EventFlow: Real-Time Neuromorphic Event-Driven Classification of\n  Two-Phase Boiling Flow Regimes", "author": "Sanghyeon Chang and Srikar Arani and Nishant Sai Nuthalapati and Youngjoon Suh and Nicholas Choi and Siavash Khodakarami and Md Rakibul Hasan Roni and Nenad Miljkovic and Aparna Chandramowlishwaran and Yoonjin Won", "abstract": "  Flow boiling is an efficient heat transfer mechanism capable of dissipating\nhigh heat loads with minimal temperature variation, making it an ideal thermal\nmanagement method. However, sudden shifts between flow regimes can disrupt\nthermal performance and system reliability, highlighting the need for accurate\nand low-latency real-time monitoring. Conventional optical imaging methods are\nlimited by high computational demands and insufficient temporal resolution,\nmaking them inadequate for capturing transient flow behavior. To address this,\nwe propose a real-time framework based on signals from neuromorphic sensors for\nflow regime classification. Neuromorphic sensors detect changes in brightness\nat individual pixels, which typically correspond to motion at edges, enabling\nfast and efficient detection without full-frame reconstruction, providing\nevent-based information. We develop five classification models using both\ntraditional image data and event-based data, demonstrating that models\nleveraging event data outperform frame-based approaches due to their\nsensitivity to dynamic flow features. Among these models, the event-based long\nshort-term memory model provides the best balance between accuracy and speed,\nachieving 97.6% classification accuracy with a processing time of 0.28 ms. Our\nasynchronous processing pipeline supports continuous, low-latency predictions\nand delivers stable output through a majority voting mechanisms, enabling\nreliable real-time feedback for experimental control and intelligent thermal\nmanagement.\n", "link": "http://arxiv.org/abs/2511.05467v1", "date": "2025-11-07", "relevancy": 1.0248, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5469}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventFlow%3A%20Real-Time%20Neuromorphic%20Event-Driven%20Classification%20of%0A%20%20Two-Phase%20Boiling%20Flow%20Regimes&body=Title%3A%20EventFlow%3A%20Real-Time%20Neuromorphic%20Event-Driven%20Classification%20of%0A%20%20Two-Phase%20Boiling%20Flow%20Regimes%0AAuthor%3A%20Sanghyeon%20Chang%20and%20Srikar%20Arani%20and%20Nishant%20Sai%20Nuthalapati%20and%20Youngjoon%20Suh%20and%20Nicholas%20Choi%20and%20Siavash%20Khodakarami%20and%20Md%20Rakibul%20Hasan%20Roni%20and%20Nenad%20Miljkovic%20and%20Aparna%20Chandramowlishwaran%20and%20Yoonjin%20Won%0AAbstract%3A%20%20%20Flow%20boiling%20is%20an%20efficient%20heat%20transfer%20mechanism%20capable%20of%20dissipating%0Ahigh%20heat%20loads%20with%20minimal%20temperature%20variation%2C%20making%20it%20an%20ideal%20thermal%0Amanagement%20method.%20However%2C%20sudden%20shifts%20between%20flow%20regimes%20can%20disrupt%0Athermal%20performance%20and%20system%20reliability%2C%20highlighting%20the%20need%20for%20accurate%0Aand%20low-latency%20real-time%20monitoring.%20Conventional%20optical%20imaging%20methods%20are%0Alimited%20by%20high%20computational%20demands%20and%20insufficient%20temporal%20resolution%2C%0Amaking%20them%20inadequate%20for%20capturing%20transient%20flow%20behavior.%20To%20address%20this%2C%0Awe%20propose%20a%20real-time%20framework%20based%20on%20signals%20from%20neuromorphic%20sensors%20for%0Aflow%20regime%20classification.%20Neuromorphic%20sensors%20detect%20changes%20in%20brightness%0Aat%20individual%20pixels%2C%20which%20typically%20correspond%20to%20motion%20at%20edges%2C%20enabling%0Afast%20and%20efficient%20detection%20without%20full-frame%20reconstruction%2C%20providing%0Aevent-based%20information.%20We%20develop%20five%20classification%20models%20using%20both%0Atraditional%20image%20data%20and%20event-based%20data%2C%20demonstrating%20that%20models%0Aleveraging%20event%20data%20outperform%20frame-based%20approaches%20due%20to%20their%0Asensitivity%20to%20dynamic%20flow%20features.%20Among%20these%20models%2C%20the%20event-based%20long%0Ashort-term%20memory%20model%20provides%20the%20best%20balance%20between%20accuracy%20and%20speed%2C%0Aachieving%2097.6%25%20classification%20accuracy%20with%20a%20processing%20time%20of%200.28%20ms.%20Our%0Aasynchronous%20processing%20pipeline%20supports%20continuous%2C%20low-latency%20predictions%0Aand%20delivers%20stable%20output%20through%20a%20majority%20voting%20mechanisms%2C%20enabling%0Areliable%20real-time%20feedback%20for%20experimental%20control%20and%20intelligent%20thermal%0Amanagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventFlow%253A%2520Real-Time%2520Neuromorphic%2520Event-Driven%2520Classification%2520of%250A%2520%2520Two-Phase%2520Boiling%2520Flow%2520Regimes%26entry.906535625%3DSanghyeon%2520Chang%2520and%2520Srikar%2520Arani%2520and%2520Nishant%2520Sai%2520Nuthalapati%2520and%2520Youngjoon%2520Suh%2520and%2520Nicholas%2520Choi%2520and%2520Siavash%2520Khodakarami%2520and%2520Md%2520Rakibul%2520Hasan%2520Roni%2520and%2520Nenad%2520Miljkovic%2520and%2520Aparna%2520Chandramowlishwaran%2520and%2520Yoonjin%2520Won%26entry.1292438233%3D%2520%2520Flow%2520boiling%2520is%2520an%2520efficient%2520heat%2520transfer%2520mechanism%2520capable%2520of%2520dissipating%250Ahigh%2520heat%2520loads%2520with%2520minimal%2520temperature%2520variation%252C%2520making%2520it%2520an%2520ideal%2520thermal%250Amanagement%2520method.%2520However%252C%2520sudden%2520shifts%2520between%2520flow%2520regimes%2520can%2520disrupt%250Athermal%2520performance%2520and%2520system%2520reliability%252C%2520highlighting%2520the%2520need%2520for%2520accurate%250Aand%2520low-latency%2520real-time%2520monitoring.%2520Conventional%2520optical%2520imaging%2520methods%2520are%250Alimited%2520by%2520high%2520computational%2520demands%2520and%2520insufficient%2520temporal%2520resolution%252C%250Amaking%2520them%2520inadequate%2520for%2520capturing%2520transient%2520flow%2520behavior.%2520To%2520address%2520this%252C%250Awe%2520propose%2520a%2520real-time%2520framework%2520based%2520on%2520signals%2520from%2520neuromorphic%2520sensors%2520for%250Aflow%2520regime%2520classification.%2520Neuromorphic%2520sensors%2520detect%2520changes%2520in%2520brightness%250Aat%2520individual%2520pixels%252C%2520which%2520typically%2520correspond%2520to%2520motion%2520at%2520edges%252C%2520enabling%250Afast%2520and%2520efficient%2520detection%2520without%2520full-frame%2520reconstruction%252C%2520providing%250Aevent-based%2520information.%2520We%2520develop%2520five%2520classification%2520models%2520using%2520both%250Atraditional%2520image%2520data%2520and%2520event-based%2520data%252C%2520demonstrating%2520that%2520models%250Aleveraging%2520event%2520data%2520outperform%2520frame-based%2520approaches%2520due%2520to%2520their%250Asensitivity%2520to%2520dynamic%2520flow%2520features.%2520Among%2520these%2520models%252C%2520the%2520event-based%2520long%250Ashort-term%2520memory%2520model%2520provides%2520the%2520best%2520balance%2520between%2520accuracy%2520and%2520speed%252C%250Aachieving%252097.6%2525%2520classification%2520accuracy%2520with%2520a%2520processing%2520time%2520of%25200.28%2520ms.%2520Our%250Aasynchronous%2520processing%2520pipeline%2520supports%2520continuous%252C%2520low-latency%2520predictions%250Aand%2520delivers%2520stable%2520output%2520through%2520a%2520majority%2520voting%2520mechanisms%252C%2520enabling%250Areliable%2520real-time%2520feedback%2520for%2520experimental%2520control%2520and%2520intelligent%2520thermal%250Amanagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventFlow%3A%20Real-Time%20Neuromorphic%20Event-Driven%20Classification%20of%0A%20%20Two-Phase%20Boiling%20Flow%20Regimes&entry.906535625=Sanghyeon%20Chang%20and%20Srikar%20Arani%20and%20Nishant%20Sai%20Nuthalapati%20and%20Youngjoon%20Suh%20and%20Nicholas%20Choi%20and%20Siavash%20Khodakarami%20and%20Md%20Rakibul%20Hasan%20Roni%20and%20Nenad%20Miljkovic%20and%20Aparna%20Chandramowlishwaran%20and%20Yoonjin%20Won&entry.1292438233=%20%20Flow%20boiling%20is%20an%20efficient%20heat%20transfer%20mechanism%20capable%20of%20dissipating%0Ahigh%20heat%20loads%20with%20minimal%20temperature%20variation%2C%20making%20it%20an%20ideal%20thermal%0Amanagement%20method.%20However%2C%20sudden%20shifts%20between%20flow%20regimes%20can%20disrupt%0Athermal%20performance%20and%20system%20reliability%2C%20highlighting%20the%20need%20for%20accurate%0Aand%20low-latency%20real-time%20monitoring.%20Conventional%20optical%20imaging%20methods%20are%0Alimited%20by%20high%20computational%20demands%20and%20insufficient%20temporal%20resolution%2C%0Amaking%20them%20inadequate%20for%20capturing%20transient%20flow%20behavior.%20To%20address%20this%2C%0Awe%20propose%20a%20real-time%20framework%20based%20on%20signals%20from%20neuromorphic%20sensors%20for%0Aflow%20regime%20classification.%20Neuromorphic%20sensors%20detect%20changes%20in%20brightness%0Aat%20individual%20pixels%2C%20which%20typically%20correspond%20to%20motion%20at%20edges%2C%20enabling%0Afast%20and%20efficient%20detection%20without%20full-frame%20reconstruction%2C%20providing%0Aevent-based%20information.%20We%20develop%20five%20classification%20models%20using%20both%0Atraditional%20image%20data%20and%20event-based%20data%2C%20demonstrating%20that%20models%0Aleveraging%20event%20data%20outperform%20frame-based%20approaches%20due%20to%20their%0Asensitivity%20to%20dynamic%20flow%20features.%20Among%20these%20models%2C%20the%20event-based%20long%0Ashort-term%20memory%20model%20provides%20the%20best%20balance%20between%20accuracy%20and%20speed%2C%0Aachieving%2097.6%25%20classification%20accuracy%20with%20a%20processing%20time%20of%200.28%20ms.%20Our%0Aasynchronous%20processing%20pipeline%20supports%20continuous%2C%20low-latency%20predictions%0Aand%20delivers%20stable%20output%20through%20a%20majority%20voting%20mechanisms%2C%20enabling%0Areliable%20real-time%20feedback%20for%20experimental%20control%20and%20intelligent%20thermal%0Amanagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05467v1&entry.124074799=Read"},
{"title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics", "author": "Akshara Prabhakar and Roshan Ram and Zixiang Chen and Silvio Savarese and Frank Wang and Caiming Xiong and Huan Wang and Weiran Yao", "abstract": "  As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200\n", "link": "http://arxiv.org/abs/2510.17797v2", "date": "2025-11-07", "relevancy": 1.0103, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enterprise%20Deep%20Research%3A%20Steerable%20Multi-Agent%20Deep%20Research%20for%0A%20%20Enterprise%20Analytics&body=Title%3A%20Enterprise%20Deep%20Research%3A%20Steerable%20Multi-Agent%20Deep%20Research%20for%0A%20%20Enterprise%20Analytics%0AAuthor%3A%20Akshara%20Prabhakar%20and%20Roshan%20Ram%20and%20Zixiang%20Chen%20and%20Silvio%20Savarese%20and%20Frank%20Wang%20and%20Caiming%20Xiong%20and%20Huan%20Wang%20and%20Weiran%20Yao%0AAbstract%3A%20%20%20As%20information%20grows%20exponentially%2C%20enterprises%20face%20increasing%20pressure%20to%0Atransform%20unstructured%20data%20into%20coherent%2C%20actionable%20insights.%20While%0Aautonomous%20agents%20show%20promise%2C%20they%20often%20struggle%20with%20domain-specific%0Anuances%2C%20intent%20alignment%2C%20and%20enterprise%20integration.%20We%20present%20Enterprise%0ADeep%20Research%20%28EDR%29%2C%20a%20multi-agent%20system%20that%20integrates%20%281%29%20a%20Master%20Planning%0AAgent%20for%20adaptive%20query%20decomposition%2C%20%282%29%20four%20specialized%20search%20agents%0A%28General%2C%20Academic%2C%20GitHub%2C%20LinkedIn%29%2C%20%283%29%20an%20extensible%20MCP-based%20tool%0Aecosystem%20supporting%20NL2SQL%2C%20file%20analysis%2C%20and%20enterprise%20workflows%2C%20%284%29%20a%0AVisualization%20Agent%20for%20data-driven%20insights%2C%20and%20%285%29%20a%20reflection%20mechanism%0Athat%20detects%20knowledge%20gaps%20and%20updates%20research%20direction%20with%20optional%0Ahuman-in-the-loop%20steering%20guidance.%20These%20components%20enable%20automated%20report%0Ageneration%2C%20real-time%20streaming%2C%20and%20seamless%20enterprise%20deployment%2C%20as%0Avalidated%20on%20internal%20datasets.%20On%20open-ended%20benchmarks%20including%20DeepResearch%0ABench%20and%20DeepConsult%2C%20EDR%20outperforms%20state-of-the-art%20agentic%20systems%20without%0Aany%20human%20steering.%20We%20release%20the%20EDR%20framework%20and%20benchmark%20trajectories%20to%0Aadvance%20research%20on%20multi-agent%20reasoning%20applications.%0A%20%20Code%20at%20https%3A//github.com/SalesforceAIResearch/enterprise-deep-research%20and%0ADataset%20at%20https%3A//huggingface.co/datasets/Salesforce/EDR-200%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnterprise%2520Deep%2520Research%253A%2520Steerable%2520Multi-Agent%2520Deep%2520Research%2520for%250A%2520%2520Enterprise%2520Analytics%26entry.906535625%3DAkshara%2520Prabhakar%2520and%2520Roshan%2520Ram%2520and%2520Zixiang%2520Chen%2520and%2520Silvio%2520Savarese%2520and%2520Frank%2520Wang%2520and%2520Caiming%2520Xiong%2520and%2520Huan%2520Wang%2520and%2520Weiran%2520Yao%26entry.1292438233%3D%2520%2520As%2520information%2520grows%2520exponentially%252C%2520enterprises%2520face%2520increasing%2520pressure%2520to%250Atransform%2520unstructured%2520data%2520into%2520coherent%252C%2520actionable%2520insights.%2520While%250Aautonomous%2520agents%2520show%2520promise%252C%2520they%2520often%2520struggle%2520with%2520domain-specific%250Anuances%252C%2520intent%2520alignment%252C%2520and%2520enterprise%2520integration.%2520We%2520present%2520Enterprise%250ADeep%2520Research%2520%2528EDR%2529%252C%2520a%2520multi-agent%2520system%2520that%2520integrates%2520%25281%2529%2520a%2520Master%2520Planning%250AAgent%2520for%2520adaptive%2520query%2520decomposition%252C%2520%25282%2529%2520four%2520specialized%2520search%2520agents%250A%2528General%252C%2520Academic%252C%2520GitHub%252C%2520LinkedIn%2529%252C%2520%25283%2529%2520an%2520extensible%2520MCP-based%2520tool%250Aecosystem%2520supporting%2520NL2SQL%252C%2520file%2520analysis%252C%2520and%2520enterprise%2520workflows%252C%2520%25284%2529%2520a%250AVisualization%2520Agent%2520for%2520data-driven%2520insights%252C%2520and%2520%25285%2529%2520a%2520reflection%2520mechanism%250Athat%2520detects%2520knowledge%2520gaps%2520and%2520updates%2520research%2520direction%2520with%2520optional%250Ahuman-in-the-loop%2520steering%2520guidance.%2520These%2520components%2520enable%2520automated%2520report%250Ageneration%252C%2520real-time%2520streaming%252C%2520and%2520seamless%2520enterprise%2520deployment%252C%2520as%250Avalidated%2520on%2520internal%2520datasets.%2520On%2520open-ended%2520benchmarks%2520including%2520DeepResearch%250ABench%2520and%2520DeepConsult%252C%2520EDR%2520outperforms%2520state-of-the-art%2520agentic%2520systems%2520without%250Aany%2520human%2520steering.%2520We%2520release%2520the%2520EDR%2520framework%2520and%2520benchmark%2520trajectories%2520to%250Aadvance%2520research%2520on%2520multi-agent%2520reasoning%2520applications.%250A%2520%2520Code%2520at%2520https%253A//github.com/SalesforceAIResearch/enterprise-deep-research%2520and%250ADataset%2520at%2520https%253A//huggingface.co/datasets/Salesforce/EDR-200%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enterprise%20Deep%20Research%3A%20Steerable%20Multi-Agent%20Deep%20Research%20for%0A%20%20Enterprise%20Analytics&entry.906535625=Akshara%20Prabhakar%20and%20Roshan%20Ram%20and%20Zixiang%20Chen%20and%20Silvio%20Savarese%20and%20Frank%20Wang%20and%20Caiming%20Xiong%20and%20Huan%20Wang%20and%20Weiran%20Yao&entry.1292438233=%20%20As%20information%20grows%20exponentially%2C%20enterprises%20face%20increasing%20pressure%20to%0Atransform%20unstructured%20data%20into%20coherent%2C%20actionable%20insights.%20While%0Aautonomous%20agents%20show%20promise%2C%20they%20often%20struggle%20with%20domain-specific%0Anuances%2C%20intent%20alignment%2C%20and%20enterprise%20integration.%20We%20present%20Enterprise%0ADeep%20Research%20%28EDR%29%2C%20a%20multi-agent%20system%20that%20integrates%20%281%29%20a%20Master%20Planning%0AAgent%20for%20adaptive%20query%20decomposition%2C%20%282%29%20four%20specialized%20search%20agents%0A%28General%2C%20Academic%2C%20GitHub%2C%20LinkedIn%29%2C%20%283%29%20an%20extensible%20MCP-based%20tool%0Aecosystem%20supporting%20NL2SQL%2C%20file%20analysis%2C%20and%20enterprise%20workflows%2C%20%284%29%20a%0AVisualization%20Agent%20for%20data-driven%20insights%2C%20and%20%285%29%20a%20reflection%20mechanism%0Athat%20detects%20knowledge%20gaps%20and%20updates%20research%20direction%20with%20optional%0Ahuman-in-the-loop%20steering%20guidance.%20These%20components%20enable%20automated%20report%0Ageneration%2C%20real-time%20streaming%2C%20and%20seamless%20enterprise%20deployment%2C%20as%0Avalidated%20on%20internal%20datasets.%20On%20open-ended%20benchmarks%20including%20DeepResearch%0ABench%20and%20DeepConsult%2C%20EDR%20outperforms%20state-of-the-art%20agentic%20systems%20without%0Aany%20human%20steering.%20We%20release%20the%20EDR%20framework%20and%20benchmark%20trajectories%20to%0Aadvance%20research%20on%20multi-agent%20reasoning%20applications.%0A%20%20Code%20at%20https%3A//github.com/SalesforceAIResearch/enterprise-deep-research%20and%0ADataset%20at%20https%3A//huggingface.co/datasets/Salesforce/EDR-200%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17797v2&entry.124074799=Read"},
{"title": "AI Literacy Assessment Revisited: A Task-Oriented Approach Aligned with\n  Real-world Occupations", "author": "Christopher Bogart and Aparna Warrier and Arav Agarwal and Ross Higashi and Yufan Zhang and Jesse Flot and Jaromir Savelka and Heather Burte and Majd Sakr", "abstract": "  As artificial intelligence (AI) systems become ubiquitous in professional\ncontexts, there is an urgent need to equip workers, often with backgrounds\noutside of STEM, with the skills to use these tools effectively as well as\nresponsibly, that is, to be AI literate. However, prevailing definitions and\ntherefore assessments of AI literacy often emphasize foundational technical\nknowledge, such as programming, mathematics, and statistics, over practical\nknowledge such as interpreting model outputs, selecting tools, or identifying\nethical concerns. This leaves a noticeable gap in assessing someone's AI\nliteracy for real-world job use. We propose a work-task-oriented assessment\nmodel for AI literacy which is grounded in the competencies required for\neffective use of AI tools in professional settings. We describe the development\nof a novel AI literacy assessment instrument, and accompanying formative\nassessments, in the context of a US Navy robotics training program. The program\nincluded training in robotics and AI literacy, as well as a competition with\npractical tasks and a multiple choice scenario task meant to simulate use of AI\nin a job setting. We found that, as a measure of applied AI literacy, the\ncompetition's scenario task outperformed the tests we adopted from past\nresearch or developed ourselves. We argue that when training people for\nAI-related work, educators should consider evaluating them with instruments\nthat emphasize highly contextualized practical skills rather than abstract\ntechnical knowledge, especially when preparing workers without technical\nbackgrounds for AI-integrated roles.\n", "link": "http://arxiv.org/abs/2511.05475v1", "date": "2025-11-07", "relevancy": 0.9198, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Literacy%20Assessment%20Revisited%3A%20A%20Task-Oriented%20Approach%20Aligned%20with%0A%20%20Real-world%20Occupations&body=Title%3A%20AI%20Literacy%20Assessment%20Revisited%3A%20A%20Task-Oriented%20Approach%20Aligned%20with%0A%20%20Real-world%20Occupations%0AAuthor%3A%20Christopher%20Bogart%20and%20Aparna%20Warrier%20and%20Arav%20Agarwal%20and%20Ross%20Higashi%20and%20Yufan%20Zhang%20and%20Jesse%20Flot%20and%20Jaromir%20Savelka%20and%20Heather%20Burte%20and%20Majd%20Sakr%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20%28AI%29%20systems%20become%20ubiquitous%20in%20professional%0Acontexts%2C%20there%20is%20an%20urgent%20need%20to%20equip%20workers%2C%20often%20with%20backgrounds%0Aoutside%20of%20STEM%2C%20with%20the%20skills%20to%20use%20these%20tools%20effectively%20as%20well%20as%0Aresponsibly%2C%20that%20is%2C%20to%20be%20AI%20literate.%20However%2C%20prevailing%20definitions%20and%0Atherefore%20assessments%20of%20AI%20literacy%20often%20emphasize%20foundational%20technical%0Aknowledge%2C%20such%20as%20programming%2C%20mathematics%2C%20and%20statistics%2C%20over%20practical%0Aknowledge%20such%20as%20interpreting%20model%20outputs%2C%20selecting%20tools%2C%20or%20identifying%0Aethical%20concerns.%20This%20leaves%20a%20noticeable%20gap%20in%20assessing%20someone%27s%20AI%0Aliteracy%20for%20real-world%20job%20use.%20We%20propose%20a%20work-task-oriented%20assessment%0Amodel%20for%20AI%20literacy%20which%20is%20grounded%20in%20the%20competencies%20required%20for%0Aeffective%20use%20of%20AI%20tools%20in%20professional%20settings.%20We%20describe%20the%20development%0Aof%20a%20novel%20AI%20literacy%20assessment%20instrument%2C%20and%20accompanying%20formative%0Aassessments%2C%20in%20the%20context%20of%20a%20US%20Navy%20robotics%20training%20program.%20The%20program%0Aincluded%20training%20in%20robotics%20and%20AI%20literacy%2C%20as%20well%20as%20a%20competition%20with%0Apractical%20tasks%20and%20a%20multiple%20choice%20scenario%20task%20meant%20to%20simulate%20use%20of%20AI%0Ain%20a%20job%20setting.%20We%20found%20that%2C%20as%20a%20measure%20of%20applied%20AI%20literacy%2C%20the%0Acompetition%27s%20scenario%20task%20outperformed%20the%20tests%20we%20adopted%20from%20past%0Aresearch%20or%20developed%20ourselves.%20We%20argue%20that%20when%20training%20people%20for%0AAI-related%20work%2C%20educators%20should%20consider%20evaluating%20them%20with%20instruments%0Athat%20emphasize%20highly%20contextualized%20practical%20skills%20rather%20than%20abstract%0Atechnical%20knowledge%2C%20especially%20when%20preparing%20workers%20without%20technical%0Abackgrounds%20for%20AI-integrated%20roles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Literacy%2520Assessment%2520Revisited%253A%2520A%2520Task-Oriented%2520Approach%2520Aligned%2520with%250A%2520%2520Real-world%2520Occupations%26entry.906535625%3DChristopher%2520Bogart%2520and%2520Aparna%2520Warrier%2520and%2520Arav%2520Agarwal%2520and%2520Ross%2520Higashi%2520and%2520Yufan%2520Zhang%2520and%2520Jesse%2520Flot%2520and%2520Jaromir%2520Savelka%2520and%2520Heather%2520Burte%2520and%2520Majd%2520Sakr%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520become%2520ubiquitous%2520in%2520professional%250Acontexts%252C%2520there%2520is%2520an%2520urgent%2520need%2520to%2520equip%2520workers%252C%2520often%2520with%2520backgrounds%250Aoutside%2520of%2520STEM%252C%2520with%2520the%2520skills%2520to%2520use%2520these%2520tools%2520effectively%2520as%2520well%2520as%250Aresponsibly%252C%2520that%2520is%252C%2520to%2520be%2520AI%2520literate.%2520However%252C%2520prevailing%2520definitions%2520and%250Atherefore%2520assessments%2520of%2520AI%2520literacy%2520often%2520emphasize%2520foundational%2520technical%250Aknowledge%252C%2520such%2520as%2520programming%252C%2520mathematics%252C%2520and%2520statistics%252C%2520over%2520practical%250Aknowledge%2520such%2520as%2520interpreting%2520model%2520outputs%252C%2520selecting%2520tools%252C%2520or%2520identifying%250Aethical%2520concerns.%2520This%2520leaves%2520a%2520noticeable%2520gap%2520in%2520assessing%2520someone%2527s%2520AI%250Aliteracy%2520for%2520real-world%2520job%2520use.%2520We%2520propose%2520a%2520work-task-oriented%2520assessment%250Amodel%2520for%2520AI%2520literacy%2520which%2520is%2520grounded%2520in%2520the%2520competencies%2520required%2520for%250Aeffective%2520use%2520of%2520AI%2520tools%2520in%2520professional%2520settings.%2520We%2520describe%2520the%2520development%250Aof%2520a%2520novel%2520AI%2520literacy%2520assessment%2520instrument%252C%2520and%2520accompanying%2520formative%250Aassessments%252C%2520in%2520the%2520context%2520of%2520a%2520US%2520Navy%2520robotics%2520training%2520program.%2520The%2520program%250Aincluded%2520training%2520in%2520robotics%2520and%2520AI%2520literacy%252C%2520as%2520well%2520as%2520a%2520competition%2520with%250Apractical%2520tasks%2520and%2520a%2520multiple%2520choice%2520scenario%2520task%2520meant%2520to%2520simulate%2520use%2520of%2520AI%250Ain%2520a%2520job%2520setting.%2520We%2520found%2520that%252C%2520as%2520a%2520measure%2520of%2520applied%2520AI%2520literacy%252C%2520the%250Acompetition%2527s%2520scenario%2520task%2520outperformed%2520the%2520tests%2520we%2520adopted%2520from%2520past%250Aresearch%2520or%2520developed%2520ourselves.%2520We%2520argue%2520that%2520when%2520training%2520people%2520for%250AAI-related%2520work%252C%2520educators%2520should%2520consider%2520evaluating%2520them%2520with%2520instruments%250Athat%2520emphasize%2520highly%2520contextualized%2520practical%2520skills%2520rather%2520than%2520abstract%250Atechnical%2520knowledge%252C%2520especially%2520when%2520preparing%2520workers%2520without%2520technical%250Abackgrounds%2520for%2520AI-integrated%2520roles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Literacy%20Assessment%20Revisited%3A%20A%20Task-Oriented%20Approach%20Aligned%20with%0A%20%20Real-world%20Occupations&entry.906535625=Christopher%20Bogart%20and%20Aparna%20Warrier%20and%20Arav%20Agarwal%20and%20Ross%20Higashi%20and%20Yufan%20Zhang%20and%20Jesse%20Flot%20and%20Jaromir%20Savelka%20and%20Heather%20Burte%20and%20Majd%20Sakr&entry.1292438233=%20%20As%20artificial%20intelligence%20%28AI%29%20systems%20become%20ubiquitous%20in%20professional%0Acontexts%2C%20there%20is%20an%20urgent%20need%20to%20equip%20workers%2C%20often%20with%20backgrounds%0Aoutside%20of%20STEM%2C%20with%20the%20skills%20to%20use%20these%20tools%20effectively%20as%20well%20as%0Aresponsibly%2C%20that%20is%2C%20to%20be%20AI%20literate.%20However%2C%20prevailing%20definitions%20and%0Atherefore%20assessments%20of%20AI%20literacy%20often%20emphasize%20foundational%20technical%0Aknowledge%2C%20such%20as%20programming%2C%20mathematics%2C%20and%20statistics%2C%20over%20practical%0Aknowledge%20such%20as%20interpreting%20model%20outputs%2C%20selecting%20tools%2C%20or%20identifying%0Aethical%20concerns.%20This%20leaves%20a%20noticeable%20gap%20in%20assessing%20someone%27s%20AI%0Aliteracy%20for%20real-world%20job%20use.%20We%20propose%20a%20work-task-oriented%20assessment%0Amodel%20for%20AI%20literacy%20which%20is%20grounded%20in%20the%20competencies%20required%20for%0Aeffective%20use%20of%20AI%20tools%20in%20professional%20settings.%20We%20describe%20the%20development%0Aof%20a%20novel%20AI%20literacy%20assessment%20instrument%2C%20and%20accompanying%20formative%0Aassessments%2C%20in%20the%20context%20of%20a%20US%20Navy%20robotics%20training%20program.%20The%20program%0Aincluded%20training%20in%20robotics%20and%20AI%20literacy%2C%20as%20well%20as%20a%20competition%20with%0Apractical%20tasks%20and%20a%20multiple%20choice%20scenario%20task%20meant%20to%20simulate%20use%20of%20AI%0Ain%20a%20job%20setting.%20We%20found%20that%2C%20as%20a%20measure%20of%20applied%20AI%20literacy%2C%20the%0Acompetition%27s%20scenario%20task%20outperformed%20the%20tests%20we%20adopted%20from%20past%0Aresearch%20or%20developed%20ourselves.%20We%20argue%20that%20when%20training%20people%20for%0AAI-related%20work%2C%20educators%20should%20consider%20evaluating%20them%20with%20instruments%0Athat%20emphasize%20highly%20contextualized%20practical%20skills%20rather%20than%20abstract%0Atechnical%20knowledge%2C%20especially%20when%20preparing%20workers%20without%20technical%0Abackgrounds%20for%20AI-integrated%20roles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05475v1&entry.124074799=Read"},
{"title": "On Flow Matching KL Divergence", "author": "Maojiang Su and Jerry Yao-Chieh Hu and Sophia Pi and Han Liu", "abstract": "  We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler\n(KL) divergence of the flow-matching distribution approximation. In particular,\nif the $L_2$ flow-matching loss is bounded by $\\epsilon^2 > 0$, then the KL\ndivergence between the true data distribution and the estimated distribution is\nbounded by $A_1 \\epsilon + A_2 \\epsilon^2$. Here, the constants $A_1$ and $A_2$\ndepend only on the regularities of the data and velocity fields. Consequently,\nthis bound implies statistical convergence rates of Flow Matching Transformers\nunder the Total Variation (TV) distance. We show that, flow matching achieves\nnearly minimax-optimal efficiency in estimating smooth distributions. Our\nresults make the statistical efficiency of flow matching comparable to that of\ndiffusion models under the TV distance. Numerical studies on synthetic and\nlearned velocities corroborate our theory.\n", "link": "http://arxiv.org/abs/2511.05480v1", "date": "2025-11-07", "relevancy": 0.8591, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.402}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Flow%20Matching%20KL%20Divergence&body=Title%3A%20On%20Flow%20Matching%20KL%20Divergence%0AAuthor%3A%20Maojiang%20Su%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Sophia%20Pi%20and%20Han%20Liu%0AAbstract%3A%20%20%20We%20derive%20a%20deterministic%2C%20non-asymptotic%20upper%20bound%20on%20the%20Kullback-Leibler%0A%28KL%29%20divergence%20of%20the%20flow-matching%20distribution%20approximation.%20In%20particular%2C%0Aif%20the%20%24L_2%24%20flow-matching%20loss%20is%20bounded%20by%20%24%5Cepsilon%5E2%20%3E%200%24%2C%20then%20the%20KL%0Adivergence%20between%20the%20true%20data%20distribution%20and%20the%20estimated%20distribution%20is%0Abounded%20by%20%24A_1%20%5Cepsilon%20%2B%20A_2%20%5Cepsilon%5E2%24.%20Here%2C%20the%20constants%20%24A_1%24%20and%20%24A_2%24%0Adepend%20only%20on%20the%20regularities%20of%20the%20data%20and%20velocity%20fields.%20Consequently%2C%0Athis%20bound%20implies%20statistical%20convergence%20rates%20of%20Flow%20Matching%20Transformers%0Aunder%20the%20Total%20Variation%20%28TV%29%20distance.%20We%20show%20that%2C%20flow%20matching%20achieves%0Anearly%20minimax-optimal%20efficiency%20in%20estimating%20smooth%20distributions.%20Our%0Aresults%20make%20the%20statistical%20efficiency%20of%20flow%20matching%20comparable%20to%20that%20of%0Adiffusion%20models%20under%20the%20TV%20distance.%20Numerical%20studies%20on%20synthetic%20and%0Alearned%20velocities%20corroborate%20our%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2511.05480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Flow%2520Matching%2520KL%2520Divergence%26entry.906535625%3DMaojiang%2520Su%2520and%2520Jerry%2520Yao-Chieh%2520Hu%2520and%2520Sophia%2520Pi%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520We%2520derive%2520a%2520deterministic%252C%2520non-asymptotic%2520upper%2520bound%2520on%2520the%2520Kullback-Leibler%250A%2528KL%2529%2520divergence%2520of%2520the%2520flow-matching%2520distribution%2520approximation.%2520In%2520particular%252C%250Aif%2520the%2520%2524L_2%2524%2520flow-matching%2520loss%2520is%2520bounded%2520by%2520%2524%255Cepsilon%255E2%2520%253E%25200%2524%252C%2520then%2520the%2520KL%250Adivergence%2520between%2520the%2520true%2520data%2520distribution%2520and%2520the%2520estimated%2520distribution%2520is%250Abounded%2520by%2520%2524A_1%2520%255Cepsilon%2520%252B%2520A_2%2520%255Cepsilon%255E2%2524.%2520Here%252C%2520the%2520constants%2520%2524A_1%2524%2520and%2520%2524A_2%2524%250Adepend%2520only%2520on%2520the%2520regularities%2520of%2520the%2520data%2520and%2520velocity%2520fields.%2520Consequently%252C%250Athis%2520bound%2520implies%2520statistical%2520convergence%2520rates%2520of%2520Flow%2520Matching%2520Transformers%250Aunder%2520the%2520Total%2520Variation%2520%2528TV%2529%2520distance.%2520We%2520show%2520that%252C%2520flow%2520matching%2520achieves%250Anearly%2520minimax-optimal%2520efficiency%2520in%2520estimating%2520smooth%2520distributions.%2520Our%250Aresults%2520make%2520the%2520statistical%2520efficiency%2520of%2520flow%2520matching%2520comparable%2520to%2520that%2520of%250Adiffusion%2520models%2520under%2520the%2520TV%2520distance.%2520Numerical%2520studies%2520on%2520synthetic%2520and%250Alearned%2520velocities%2520corroborate%2520our%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Flow%20Matching%20KL%20Divergence&entry.906535625=Maojiang%20Su%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Sophia%20Pi%20and%20Han%20Liu&entry.1292438233=%20%20We%20derive%20a%20deterministic%2C%20non-asymptotic%20upper%20bound%20on%20the%20Kullback-Leibler%0A%28KL%29%20divergence%20of%20the%20flow-matching%20distribution%20approximation.%20In%20particular%2C%0Aif%20the%20%24L_2%24%20flow-matching%20loss%20is%20bounded%20by%20%24%5Cepsilon%5E2%20%3E%200%24%2C%20then%20the%20KL%0Adivergence%20between%20the%20true%20data%20distribution%20and%20the%20estimated%20distribution%20is%0Abounded%20by%20%24A_1%20%5Cepsilon%20%2B%20A_2%20%5Cepsilon%5E2%24.%20Here%2C%20the%20constants%20%24A_1%24%20and%20%24A_2%24%0Adepend%20only%20on%20the%20regularities%20of%20the%20data%20and%20velocity%20fields.%20Consequently%2C%0Athis%20bound%20implies%20statistical%20convergence%20rates%20of%20Flow%20Matching%20Transformers%0Aunder%20the%20Total%20Variation%20%28TV%29%20distance.%20We%20show%20that%2C%20flow%20matching%20achieves%0Anearly%20minimax-optimal%20efficiency%20in%20estimating%20smooth%20distributions.%20Our%0Aresults%20make%20the%20statistical%20efficiency%20of%20flow%20matching%20comparable%20to%20that%20of%0Adiffusion%20models%20under%20the%20TV%20distance.%20Numerical%20studies%20on%20synthetic%20and%0Alearned%20velocities%20corroborate%20our%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2511.05480v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


