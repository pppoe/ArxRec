<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250721.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting", "author": "Ruijie Zhu and Mulin Yu and Linning Xu and Lihan Jiang and Yixuan Li and Tianzhu Zhang and Jiangmiao Pang and Bo Dai", "abstract": "  3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page\n", "link": "http://arxiv.org/abs/2507.15454v1", "date": "2025-07-21", "relevancy": 3.5005, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7183}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7082}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjectGS%3A%20Object-aware%20Scene%20Reconstruction%20and%20Scene%20Understanding%20via%0A%20%20Gaussian%20Splatting&body=Title%3A%20ObjectGS%3A%20Object-aware%20Scene%20Reconstruction%20and%20Scene%20Understanding%20via%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Ruijie%20Zhu%20and%20Mulin%20Yu%20and%20Linning%20Xu%20and%20Lihan%20Jiang%20and%20Yixuan%20Li%20and%20Tianzhu%20Zhang%20and%20Jiangmiao%20Pang%20and%20Bo%20Dai%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20is%20renowned%20for%20its%20high-fidelity%20reconstructions%20and%0Areal-time%20novel%20view%20synthesis%2C%20yet%20its%20lack%20of%20semantic%20understanding%20limits%0Aobject-level%20perception.%20In%20this%20work%2C%20we%20propose%20ObjectGS%2C%20an%20object-aware%0Aframework%20that%20unifies%203D%20scene%20reconstruction%20with%20semantic%20understanding.%0AInstead%20of%20treating%20the%20scene%20as%20a%20unified%20whole%2C%20ObjectGS%20models%20individual%0Aobjects%20as%20local%20anchors%20that%20generate%20neural%20Gaussians%20and%20share%20object%20IDs%2C%0Aenabling%20precise%20object-level%20reconstruction.%20During%20training%2C%20we%20dynamically%0Agrow%20or%20prune%20these%20anchors%20and%20optimize%20their%20features%2C%20while%20a%20one-hot%20ID%0Aencoding%20with%20a%20classification%20loss%20enforces%20clear%20semantic%20constraints.%20We%0Ashow%20through%20extensive%20experiments%20that%20ObjectGS%20not%20only%20outperforms%0Astate-of-the-art%20methods%20on%20open-vocabulary%20and%20panoptic%20segmentation%20tasks%2C%0Abut%20also%20integrates%20seamlessly%20with%20applications%20like%20mesh%20extraction%20and%20scene%0Aediting.%20Project%20page%3A%20https%3A//ruijiezhu94.github.io/ObjectGS_page%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjectGS%253A%2520Object-aware%2520Scene%2520Reconstruction%2520and%2520Scene%2520Understanding%2520via%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DRuijie%2520Zhu%2520and%2520Mulin%2520Yu%2520and%2520Linning%2520Xu%2520and%2520Lihan%2520Jiang%2520and%2520Yixuan%2520Li%2520and%2520Tianzhu%2520Zhang%2520and%2520Jiangmiao%2520Pang%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520is%2520renowned%2520for%2520its%2520high-fidelity%2520reconstructions%2520and%250Areal-time%2520novel%2520view%2520synthesis%252C%2520yet%2520its%2520lack%2520of%2520semantic%2520understanding%2520limits%250Aobject-level%2520perception.%2520In%2520this%2520work%252C%2520we%2520propose%2520ObjectGS%252C%2520an%2520object-aware%250Aframework%2520that%2520unifies%25203D%2520scene%2520reconstruction%2520with%2520semantic%2520understanding.%250AInstead%2520of%2520treating%2520the%2520scene%2520as%2520a%2520unified%2520whole%252C%2520ObjectGS%2520models%2520individual%250Aobjects%2520as%2520local%2520anchors%2520that%2520generate%2520neural%2520Gaussians%2520and%2520share%2520object%2520IDs%252C%250Aenabling%2520precise%2520object-level%2520reconstruction.%2520During%2520training%252C%2520we%2520dynamically%250Agrow%2520or%2520prune%2520these%2520anchors%2520and%2520optimize%2520their%2520features%252C%2520while%2520a%2520one-hot%2520ID%250Aencoding%2520with%2520a%2520classification%2520loss%2520enforces%2520clear%2520semantic%2520constraints.%2520We%250Ashow%2520through%2520extensive%2520experiments%2520that%2520ObjectGS%2520not%2520only%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520open-vocabulary%2520and%2520panoptic%2520segmentation%2520tasks%252C%250Abut%2520also%2520integrates%2520seamlessly%2520with%2520applications%2520like%2520mesh%2520extraction%2520and%2520scene%250Aediting.%2520Project%2520page%253A%2520https%253A//ruijiezhu94.github.io/ObjectGS_page%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjectGS%3A%20Object-aware%20Scene%20Reconstruction%20and%20Scene%20Understanding%20via%0A%20%20Gaussian%20Splatting&entry.906535625=Ruijie%20Zhu%20and%20Mulin%20Yu%20and%20Linning%20Xu%20and%20Lihan%20Jiang%20and%20Yixuan%20Li%20and%20Tianzhu%20Zhang%20and%20Jiangmiao%20Pang%20and%20Bo%20Dai&entry.1292438233=%20%203D%20Gaussian%20Splatting%20is%20renowned%20for%20its%20high-fidelity%20reconstructions%20and%0Areal-time%20novel%20view%20synthesis%2C%20yet%20its%20lack%20of%20semantic%20understanding%20limits%0Aobject-level%20perception.%20In%20this%20work%2C%20we%20propose%20ObjectGS%2C%20an%20object-aware%0Aframework%20that%20unifies%203D%20scene%20reconstruction%20with%20semantic%20understanding.%0AInstead%20of%20treating%20the%20scene%20as%20a%20unified%20whole%2C%20ObjectGS%20models%20individual%0Aobjects%20as%20local%20anchors%20that%20generate%20neural%20Gaussians%20and%20share%20object%20IDs%2C%0Aenabling%20precise%20object-level%20reconstruction.%20During%20training%2C%20we%20dynamically%0Agrow%20or%20prune%20these%20anchors%20and%20optimize%20their%20features%2C%20while%20a%20one-hot%20ID%0Aencoding%20with%20a%20classification%20loss%20enforces%20clear%20semantic%20constraints.%20We%0Ashow%20through%20extensive%20experiments%20that%20ObjectGS%20not%20only%20outperforms%0Astate-of-the-art%20methods%20on%20open-vocabulary%20and%20panoptic%20segmentation%20tasks%2C%0Abut%20also%20integrates%20seamlessly%20with%20applications%20like%20mesh%20extraction%20and%20scene%0Aediting.%20Project%20page%3A%20https%3A//ruijiezhu94.github.io/ObjectGS_page%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15454v1&entry.124074799=Read"},
{"title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "author": "Zihui Gao and Jia-Wang Bian and Guosheng Lin and Hao Chen and Chunhua Shen", "abstract": "  Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.\n", "link": "http://arxiv.org/abs/2507.15602v1", "date": "2025-07-21", "relevancy": 3.374, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7105}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6869}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurfaceSplat%3A%20Connecting%20Surface%20Reconstruction%20and%20Gaussian%20Splatting&body=Title%3A%20SurfaceSplat%3A%20Connecting%20Surface%20Reconstruction%20and%20Gaussian%20Splatting%0AAuthor%3A%20Zihui%20Gao%20and%20Jia-Wang%20Bian%20and%20Guosheng%20Lin%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Surface%20reconstruction%20and%20novel%20view%20rendering%20from%20sparse-view%20images%20are%0Achallenging.%20Signed%20Distance%20Function%20%28SDF%29-based%20methods%20struggle%20with%20fine%0Adetails%2C%20while%203D%20Gaussian%20Splatting%20%283DGS%29-based%20approaches%20lack%20global%0Ageometry%20coherence.%20We%20propose%20a%20novel%20hybrid%20method%20that%20combines%20the%0Astrengths%20of%20both%20approaches%3A%20SDF%20captures%20coarse%20geometry%20to%20enhance%0A3DGS-based%20rendering%2C%20while%20newly%20rendered%20images%20from%203DGS%20refine%20the%20details%0Aof%20SDF%20for%20accurate%20surface%20reconstruction.%20As%20a%20result%2C%20our%20method%20surpasses%0Astate-of-the-art%20approaches%20in%20surface%20reconstruction%20and%20novel%20view%20synthesis%0Aon%20the%20DTU%20and%20MobileBrick%20datasets.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/Gaozihui/SurfaceSplat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfaceSplat%253A%2520Connecting%2520Surface%2520Reconstruction%2520and%2520Gaussian%2520Splatting%26entry.906535625%3DZihui%2520Gao%2520and%2520Jia-Wang%2520Bian%2520and%2520Guosheng%2520Lin%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Surface%2520reconstruction%2520and%2520novel%2520view%2520rendering%2520from%2520sparse-view%2520images%2520are%250Achallenging.%2520Signed%2520Distance%2520Function%2520%2528SDF%2529-based%2520methods%2520struggle%2520with%2520fine%250Adetails%252C%2520while%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529-based%2520approaches%2520lack%2520global%250Ageometry%2520coherence.%2520We%2520propose%2520a%2520novel%2520hybrid%2520method%2520that%2520combines%2520the%250Astrengths%2520of%2520both%2520approaches%253A%2520SDF%2520captures%2520coarse%2520geometry%2520to%2520enhance%250A3DGS-based%2520rendering%252C%2520while%2520newly%2520rendered%2520images%2520from%25203DGS%2520refine%2520the%2520details%250Aof%2520SDF%2520for%2520accurate%2520surface%2520reconstruction.%2520As%2520a%2520result%252C%2520our%2520method%2520surpasses%250Astate-of-the-art%2520approaches%2520in%2520surface%2520reconstruction%2520and%2520novel%2520view%2520synthesis%250Aon%2520the%2520DTU%2520and%2520MobileBrick%2520datasets.%2520Code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/Gaozihui/SurfaceSplat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurfaceSplat%3A%20Connecting%20Surface%20Reconstruction%20and%20Gaussian%20Splatting&entry.906535625=Zihui%20Gao%20and%20Jia-Wang%20Bian%20and%20Guosheng%20Lin%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20Surface%20reconstruction%20and%20novel%20view%20rendering%20from%20sparse-view%20images%20are%0Achallenging.%20Signed%20Distance%20Function%20%28SDF%29-based%20methods%20struggle%20with%20fine%0Adetails%2C%20while%203D%20Gaussian%20Splatting%20%283DGS%29-based%20approaches%20lack%20global%0Ageometry%20coherence.%20We%20propose%20a%20novel%20hybrid%20method%20that%20combines%20the%0Astrengths%20of%20both%20approaches%3A%20SDF%20captures%20coarse%20geometry%20to%20enhance%0A3DGS-based%20rendering%2C%20while%20newly%20rendered%20images%20from%203DGS%20refine%20the%20details%0Aof%20SDF%20for%20accurate%20surface%20reconstruction.%20As%20a%20result%2C%20our%20method%20surpasses%0Astate-of-the-art%20approaches%20in%20surface%20reconstruction%20and%20novel%20view%20synthesis%0Aon%20the%20DTU%20and%20MobileBrick%20datasets.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/Gaozihui/SurfaceSplat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15602v1&entry.124074799=Read"},
{"title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization\n  for Remote Sensing", "author": "Boni Hu and Zhenyu Xia and Lin Chen and Pengcheng Han and Shuhui Bu", "abstract": "  Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.\n", "link": "http://arxiv.org/abs/2507.15683v1", "date": "2025-07-21", "relevancy": 3.3111, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7146}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6528}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi%5E2-GSLoc%3A%20Dual-Hierarchical%20Gaussian-Specific%20Visual%20Relocalization%0A%20%20for%20Remote%20Sensing&body=Title%3A%20Hi%5E2-GSLoc%3A%20Dual-Hierarchical%20Gaussian-Specific%20Visual%20Relocalization%0A%20%20for%20Remote%20Sensing%0AAuthor%3A%20Boni%20Hu%20and%20Zhenyu%20Xia%20and%20Lin%20Chen%20and%20Pengcheng%20Han%20and%20Shuhui%20Bu%0AAbstract%3A%20%20%20Visual%20relocalization%2C%20which%20estimates%20the%206-degree-of-freedom%20%286-DoF%29%20camera%0Apose%20from%20query%20images%2C%20is%20fundamental%20to%20remote%20sensing%20and%20UAV%20applications.%0AExisting%20methods%20face%20inherent%20trade-offs%3A%20image-based%20retrieval%20and%20pose%0Aregression%20approaches%20lack%20precision%2C%20while%20structure-based%20methods%20that%0Aregister%20queries%20to%20Structure-from-Motion%20%28SfM%29%20models%20suffer%20from%0Acomputational%20complexity%20and%20limited%20scalability.%20These%20challenges%20are%0Aparticularly%20pronounced%20in%20remote%20sensing%20scenarios%20due%20to%20large-scale%20scenes%2C%0Ahigh%20altitude%20variations%2C%20and%20domain%20gaps%20of%20existing%20visual%20priors.%20To%0Aovercome%20these%20limitations%2C%20we%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20novel%0Ascene%20representation%20that%20compactly%20encodes%20both%203D%20geometry%20and%20appearance.%20We%0Aintroduce%20%24%5Cmathrm%7BHi%7D%5E2%24-GSLoc%2C%20a%20dual-hierarchical%20relocalization%20framework%0Athat%20follows%20a%20sparse-to-dense%20and%20coarse-to-fine%20paradigm%2C%20fully%20exploiting%0Athe%20rich%20semantic%20information%20and%20geometric%20constraints%20inherent%20in%20Gaussian%0Aprimitives.%20To%20handle%20large-scale%20remote%20sensing%20scenarios%2C%20we%20incorporate%0Apartitioned%20Gaussian%20training%2C%20GPU-accelerated%20parallel%20matching%2C%20and%20dynamic%0Amemory%20management%20strategies.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20sparse%0Astage%20featuring%20a%20Gaussian-specific%20consistent%20render-aware%20sampling%20strategy%0Aand%20landmark-guided%20detector%20for%20robust%20and%20accurate%20initial%20pose%20estimation%2C%0Aand%20%282%29%20a%20dense%20stage%20that%20iteratively%20refines%20poses%20through%20coarse-to-fine%0Adense%20rasterization%20matching%20while%20incorporating%20reliability%20verification.%0AThrough%20comprehensive%20evaluation%20on%20simulation%20data%2C%20public%20datasets%2C%20and%20real%0Aflight%20experiments%2C%20we%20demonstrate%20that%20our%20method%20delivers%20competitive%0Alocalization%20accuracy%2C%20recall%20rate%2C%20and%20computational%20efficiency%20while%0Aeffectively%20filtering%20unreliable%20pose%20estimates.%20The%20results%20confirm%20the%0Aeffectiveness%20of%20our%20approach%20for%20practical%20remote%20sensing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi%255E2-GSLoc%253A%2520Dual-Hierarchical%2520Gaussian-Specific%2520Visual%2520Relocalization%250A%2520%2520for%2520Remote%2520Sensing%26entry.906535625%3DBoni%2520Hu%2520and%2520Zhenyu%2520Xia%2520and%2520Lin%2520Chen%2520and%2520Pengcheng%2520Han%2520and%2520Shuhui%2520Bu%26entry.1292438233%3D%2520%2520Visual%2520relocalization%252C%2520which%2520estimates%2520the%25206-degree-of-freedom%2520%25286-DoF%2529%2520camera%250Apose%2520from%2520query%2520images%252C%2520is%2520fundamental%2520to%2520remote%2520sensing%2520and%2520UAV%2520applications.%250AExisting%2520methods%2520face%2520inherent%2520trade-offs%253A%2520image-based%2520retrieval%2520and%2520pose%250Aregression%2520approaches%2520lack%2520precision%252C%2520while%2520structure-based%2520methods%2520that%250Aregister%2520queries%2520to%2520Structure-from-Motion%2520%2528SfM%2529%2520models%2520suffer%2520from%250Acomputational%2520complexity%2520and%2520limited%2520scalability.%2520These%2520challenges%2520are%250Aparticularly%2520pronounced%2520in%2520remote%2520sensing%2520scenarios%2520due%2520to%2520large-scale%2520scenes%252C%250Ahigh%2520altitude%2520variations%252C%2520and%2520domain%2520gaps%2520of%2520existing%2520visual%2520priors.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520leverage%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520as%2520a%2520novel%250Ascene%2520representation%2520that%2520compactly%2520encodes%2520both%25203D%2520geometry%2520and%2520appearance.%2520We%250Aintroduce%2520%2524%255Cmathrm%257BHi%257D%255E2%2524-GSLoc%252C%2520a%2520dual-hierarchical%2520relocalization%2520framework%250Athat%2520follows%2520a%2520sparse-to-dense%2520and%2520coarse-to-fine%2520paradigm%252C%2520fully%2520exploiting%250Athe%2520rich%2520semantic%2520information%2520and%2520geometric%2520constraints%2520inherent%2520in%2520Gaussian%250Aprimitives.%2520To%2520handle%2520large-scale%2520remote%2520sensing%2520scenarios%252C%2520we%2520incorporate%250Apartitioned%2520Gaussian%2520training%252C%2520GPU-accelerated%2520parallel%2520matching%252C%2520and%2520dynamic%250Amemory%2520management%2520strategies.%2520Our%2520approach%2520consists%2520of%2520two%2520stages%253A%2520%25281%2529%2520a%2520sparse%250Astage%2520featuring%2520a%2520Gaussian-specific%2520consistent%2520render-aware%2520sampling%2520strategy%250Aand%2520landmark-guided%2520detector%2520for%2520robust%2520and%2520accurate%2520initial%2520pose%2520estimation%252C%250Aand%2520%25282%2529%2520a%2520dense%2520stage%2520that%2520iteratively%2520refines%2520poses%2520through%2520coarse-to-fine%250Adense%2520rasterization%2520matching%2520while%2520incorporating%2520reliability%2520verification.%250AThrough%2520comprehensive%2520evaluation%2520on%2520simulation%2520data%252C%2520public%2520datasets%252C%2520and%2520real%250Aflight%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520delivers%2520competitive%250Alocalization%2520accuracy%252C%2520recall%2520rate%252C%2520and%2520computational%2520efficiency%2520while%250Aeffectively%2520filtering%2520unreliable%2520pose%2520estimates.%2520The%2520results%2520confirm%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520for%2520practical%2520remote%2520sensing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi%5E2-GSLoc%3A%20Dual-Hierarchical%20Gaussian-Specific%20Visual%20Relocalization%0A%20%20for%20Remote%20Sensing&entry.906535625=Boni%20Hu%20and%20Zhenyu%20Xia%20and%20Lin%20Chen%20and%20Pengcheng%20Han%20and%20Shuhui%20Bu&entry.1292438233=%20%20Visual%20relocalization%2C%20which%20estimates%20the%206-degree-of-freedom%20%286-DoF%29%20camera%0Apose%20from%20query%20images%2C%20is%20fundamental%20to%20remote%20sensing%20and%20UAV%20applications.%0AExisting%20methods%20face%20inherent%20trade-offs%3A%20image-based%20retrieval%20and%20pose%0Aregression%20approaches%20lack%20precision%2C%20while%20structure-based%20methods%20that%0Aregister%20queries%20to%20Structure-from-Motion%20%28SfM%29%20models%20suffer%20from%0Acomputational%20complexity%20and%20limited%20scalability.%20These%20challenges%20are%0Aparticularly%20pronounced%20in%20remote%20sensing%20scenarios%20due%20to%20large-scale%20scenes%2C%0Ahigh%20altitude%20variations%2C%20and%20domain%20gaps%20of%20existing%20visual%20priors.%20To%0Aovercome%20these%20limitations%2C%20we%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20novel%0Ascene%20representation%20that%20compactly%20encodes%20both%203D%20geometry%20and%20appearance.%20We%0Aintroduce%20%24%5Cmathrm%7BHi%7D%5E2%24-GSLoc%2C%20a%20dual-hierarchical%20relocalization%20framework%0Athat%20follows%20a%20sparse-to-dense%20and%20coarse-to-fine%20paradigm%2C%20fully%20exploiting%0Athe%20rich%20semantic%20information%20and%20geometric%20constraints%20inherent%20in%20Gaussian%0Aprimitives.%20To%20handle%20large-scale%20remote%20sensing%20scenarios%2C%20we%20incorporate%0Apartitioned%20Gaussian%20training%2C%20GPU-accelerated%20parallel%20matching%2C%20and%20dynamic%0Amemory%20management%20strategies.%20Our%20approach%20consists%20of%20two%20stages%3A%20%281%29%20a%20sparse%0Astage%20featuring%20a%20Gaussian-specific%20consistent%20render-aware%20sampling%20strategy%0Aand%20landmark-guided%20detector%20for%20robust%20and%20accurate%20initial%20pose%20estimation%2C%0Aand%20%282%29%20a%20dense%20stage%20that%20iteratively%20refines%20poses%20through%20coarse-to-fine%0Adense%20rasterization%20matching%20while%20incorporating%20reliability%20verification.%0AThrough%20comprehensive%20evaluation%20on%20simulation%20data%2C%20public%20datasets%2C%20and%20real%0Aflight%20experiments%2C%20we%20demonstrate%20that%20our%20method%20delivers%20competitive%0Alocalization%20accuracy%2C%20recall%20rate%2C%20and%20computational%20efficiency%20while%0Aeffectively%20filtering%20unreliable%20pose%20estimates.%20The%20results%20confirm%20the%0Aeffectiveness%20of%20our%20approach%20for%20practical%20remote%20sensing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15683v1&entry.124074799=Read"},
{"title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "author": "Zuo-Liang Zhu and Jian Yang and Beibei Wang", "abstract": "  3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.\n", "link": "http://arxiv.org/abs/2507.15629v1", "date": "2025-07-21", "relevancy": 3.3086, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6971}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6514}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20with%20Discretized%20SDF%20for%20Relightable%20Assets&body=Title%3A%20Gaussian%20Splatting%20with%20Discretized%20SDF%20for%20Relightable%20Assets%0AAuthor%3A%20Zuo-Liang%20Zhu%20and%20Jian%20Yang%20and%20Beibei%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20shown%20its%20detailed%20expressive%20ability%20and%0Ahighly%20efficient%20rendering%20speed%20in%20the%20novel%20view%20synthesis%20%28NVS%29%20task.%20The%0Aapplication%20to%20inverse%20rendering%20still%20faces%20several%20challenges%2C%20as%20the%0Adiscrete%20nature%20of%20Gaussian%20primitives%20makes%20it%20difficult%20to%20apply%20geometry%0Aconstraints.%20Recent%20works%20introduce%20the%20signed%20distance%20field%20%28SDF%29%20as%20an%20extra%0Acontinuous%20representation%20to%20regularize%20the%20geometry%20defined%20by%20Gaussian%0Aprimitives.%20It%20improves%20the%20decomposition%20quality%2C%20at%20the%20cost%20of%20increasing%0Amemory%20usage%20and%20complicating%20training.%20Unlike%20these%20works%2C%20we%20introduce%20a%0Adiscretized%20SDF%20to%20represent%20the%20continuous%20SDF%20in%20a%20discrete%20manner%20by%0Aencoding%20it%20within%20each%20Gaussian%20using%20a%20sampled%20value.%20This%20approach%20allows%20us%0Ato%20link%20the%20SDF%20with%20the%20Gaussian%20opacity%20through%20an%20SDF-to-opacity%0Atransformation%2C%20enabling%20rendering%20the%20SDF%20via%20splatting%20and%20avoiding%20the%0Acomputational%20cost%20of%20ray%20marching.The%20key%20challenge%20is%20to%20regularize%20the%0Adiscrete%20samples%20to%20be%20consistent%20with%20the%20underlying%20SDF%2C%20as%20the%20discrete%0Arepresentation%20can%20hardly%20apply%20the%20gradient-based%20constraints%20%28%5Ceg%20Eikonal%0Aloss%29.%20For%20this%2C%20we%20project%20Gaussians%20onto%20the%20zero-level%20set%20of%20SDF%20and%0Aenforce%20alignment%20with%20the%20surface%20from%20splatting%2C%20namely%20a%20projection-based%0Aconsistency%20loss.%20Thanks%20to%20the%20discretized%20SDF%2C%20our%20method%20achieves%20higher%0Arelighting%20quality%2C%20while%20requiring%20no%20extra%20memory%20beyond%20GS%20and%20avoiding%0Acomplex%20manually%20designed%20optimization.%20The%20experiments%20reveal%20that%20our%20method%0Aoutperforms%20existing%20Gaussian-based%20inverse%20rendering%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/NK-CS-ZZL/DiscretizedSDF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520with%2520Discretized%2520SDF%2520for%2520Relightable%2520Assets%26entry.906535625%3DZuo-Liang%2520Zhu%2520and%2520Jian%2520Yang%2520and%2520Beibei%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splatting%2520%25283DGS%2529%2520has%2520shown%2520its%2520detailed%2520expressive%2520ability%2520and%250Ahighly%2520efficient%2520rendering%2520speed%2520in%2520the%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520task.%2520The%250Aapplication%2520to%2520inverse%2520rendering%2520still%2520faces%2520several%2520challenges%252C%2520as%2520the%250Adiscrete%2520nature%2520of%2520Gaussian%2520primitives%2520makes%2520it%2520difficult%2520to%2520apply%2520geometry%250Aconstraints.%2520Recent%2520works%2520introduce%2520the%2520signed%2520distance%2520field%2520%2528SDF%2529%2520as%2520an%2520extra%250Acontinuous%2520representation%2520to%2520regularize%2520the%2520geometry%2520defined%2520by%2520Gaussian%250Aprimitives.%2520It%2520improves%2520the%2520decomposition%2520quality%252C%2520at%2520the%2520cost%2520of%2520increasing%250Amemory%2520usage%2520and%2520complicating%2520training.%2520Unlike%2520these%2520works%252C%2520we%2520introduce%2520a%250Adiscretized%2520SDF%2520to%2520represent%2520the%2520continuous%2520SDF%2520in%2520a%2520discrete%2520manner%2520by%250Aencoding%2520it%2520within%2520each%2520Gaussian%2520using%2520a%2520sampled%2520value.%2520This%2520approach%2520allows%2520us%250Ato%2520link%2520the%2520SDF%2520with%2520the%2520Gaussian%2520opacity%2520through%2520an%2520SDF-to-opacity%250Atransformation%252C%2520enabling%2520rendering%2520the%2520SDF%2520via%2520splatting%2520and%2520avoiding%2520the%250Acomputational%2520cost%2520of%2520ray%2520marching.The%2520key%2520challenge%2520is%2520to%2520regularize%2520the%250Adiscrete%2520samples%2520to%2520be%2520consistent%2520with%2520the%2520underlying%2520SDF%252C%2520as%2520the%2520discrete%250Arepresentation%2520can%2520hardly%2520apply%2520the%2520gradient-based%2520constraints%2520%2528%255Ceg%2520Eikonal%250Aloss%2529.%2520For%2520this%252C%2520we%2520project%2520Gaussians%2520onto%2520the%2520zero-level%2520set%2520of%2520SDF%2520and%250Aenforce%2520alignment%2520with%2520the%2520surface%2520from%2520splatting%252C%2520namely%2520a%2520projection-based%250Aconsistency%2520loss.%2520Thanks%2520to%2520the%2520discretized%2520SDF%252C%2520our%2520method%2520achieves%2520higher%250Arelighting%2520quality%252C%2520while%2520requiring%2520no%2520extra%2520memory%2520beyond%2520GS%2520and%2520avoiding%250Acomplex%2520manually%2520designed%2520optimization.%2520The%2520experiments%2520reveal%2520that%2520our%2520method%250Aoutperforms%2520existing%2520Gaussian-based%2520inverse%2520rendering%2520methods.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/NK-CS-ZZL/DiscretizedSDF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20with%20Discretized%20SDF%20for%20Relightable%20Assets&entry.906535625=Zuo-Liang%20Zhu%20and%20Jian%20Yang%20and%20Beibei%20Wang&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20shown%20its%20detailed%20expressive%20ability%20and%0Ahighly%20efficient%20rendering%20speed%20in%20the%20novel%20view%20synthesis%20%28NVS%29%20task.%20The%0Aapplication%20to%20inverse%20rendering%20still%20faces%20several%20challenges%2C%20as%20the%0Adiscrete%20nature%20of%20Gaussian%20primitives%20makes%20it%20difficult%20to%20apply%20geometry%0Aconstraints.%20Recent%20works%20introduce%20the%20signed%20distance%20field%20%28SDF%29%20as%20an%20extra%0Acontinuous%20representation%20to%20regularize%20the%20geometry%20defined%20by%20Gaussian%0Aprimitives.%20It%20improves%20the%20decomposition%20quality%2C%20at%20the%20cost%20of%20increasing%0Amemory%20usage%20and%20complicating%20training.%20Unlike%20these%20works%2C%20we%20introduce%20a%0Adiscretized%20SDF%20to%20represent%20the%20continuous%20SDF%20in%20a%20discrete%20manner%20by%0Aencoding%20it%20within%20each%20Gaussian%20using%20a%20sampled%20value.%20This%20approach%20allows%20us%0Ato%20link%20the%20SDF%20with%20the%20Gaussian%20opacity%20through%20an%20SDF-to-opacity%0Atransformation%2C%20enabling%20rendering%20the%20SDF%20via%20splatting%20and%20avoiding%20the%0Acomputational%20cost%20of%20ray%20marching.The%20key%20challenge%20is%20to%20regularize%20the%0Adiscrete%20samples%20to%20be%20consistent%20with%20the%20underlying%20SDF%2C%20as%20the%20discrete%0Arepresentation%20can%20hardly%20apply%20the%20gradient-based%20constraints%20%28%5Ceg%20Eikonal%0Aloss%29.%20For%20this%2C%20we%20project%20Gaussians%20onto%20the%20zero-level%20set%20of%20SDF%20and%0Aenforce%20alignment%20with%20the%20surface%20from%20splatting%2C%20namely%20a%20projection-based%0Aconsistency%20loss.%20Thanks%20to%20the%20discretized%20SDF%2C%20our%20method%20achieves%20higher%0Arelighting%20quality%2C%20while%20requiring%20no%20extra%20memory%20beyond%20GS%20and%20avoiding%0Acomplex%20manually%20designed%20optimization.%20The%20experiments%20reveal%20that%20our%20method%0Aoutperforms%20existing%20Gaussian-based%20inverse%20rendering%20methods.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/NK-CS-ZZL/DiscretizedSDF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15629v1&entry.124074799=Read"},
{"title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling", "author": "Hayeon Kim and Ji Ha Jang and Se Young Chun", "abstract": "  Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing. Code is available at\nhttps://janeyeon.github.io/romap.\n", "link": "http://arxiv.org/abs/2507.11061v2", "date": "2025-07-21", "relevancy": 3.2074, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6571}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6412}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%203D-Masked%20Part-level%20Editing%20in%203D%20Gaussian%20Splatting%20with%0A%20%20Regularized%20Score%20Distillation%20Sampling&body=Title%3A%20Robust%203D-Masked%20Part-level%20Editing%20in%203D%20Gaussian%20Splatting%20with%0A%20%20Regularized%20Score%20Distillation%20Sampling%0AAuthor%3A%20Hayeon%20Kim%20and%20Ji%20Ha%20Jang%20and%20Se%20Young%20Chun%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20neural%20representations%20and%20instance-level%20editing%0Amodels%20have%20enabled%20the%20efficient%20creation%20of%20high-quality%203D%20content.%20However%2C%0Aachieving%20precise%20local%203D%20edits%20remains%20challenging%2C%20especially%20for%20Gaussian%0ASplatting%2C%20due%20to%20inconsistent%20multi-view%202D%20part%20segmentations%20and%20inherently%0Aambiguous%20nature%20of%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%20To%20address%20these%0Alimitations%2C%20we%20propose%20RoMaP%2C%20a%20novel%20local%203D%20Gaussian%20editing%20framework%20that%0Aenables%20precise%20and%20drastic%20part-level%20modifications.%20First%2C%20we%20introduce%20a%0Arobust%203D%20mask%20generation%20module%20with%20our%203D-Geometry%20Aware%20Label%20Prediction%0A%283D-GALP%29%2C%20which%20uses%20spherical%20harmonics%20%28SH%29%20coefficients%20to%20model%0Aview-dependent%20label%20variations%20and%20soft-label%20property%2C%20yielding%20accurate%20and%0Aconsistent%20part%20segmentations%20across%20viewpoints.%20Second%2C%20we%20propose%20a%0Aregularized%20SDS%20loss%20that%20combines%20the%20standard%20SDS%20loss%20with%20additional%0Aregularizers.%20In%20particular%2C%20an%20L1%20anchor%20loss%20is%20introduced%20via%20our%20Scheduled%0ALatent%20Mixing%20and%20Part%20%28SLaMP%29%20editing%20method%2C%20which%20generates%20high-quality%0Apart-edited%202D%20images%20and%20confines%20modifications%20only%20to%20the%20target%20region%0Awhile%20preserving%20contextual%20coherence.%20Additional%20regularizers%2C%20such%20as%0AGaussian%20prior%20removal%2C%20further%20improve%20flexibility%20by%20allowing%20changes%20beyond%0Athe%20existing%20context%2C%20and%20robust%203D%20masking%20prevents%20unintended%20edits.%0AExperimental%20results%20demonstrate%20that%20our%20RoMaP%20achieves%20state-of-the-art%20local%0A3D%20editing%20on%20both%20reconstructed%20and%20generated%20Gaussian%20scenes%20and%20objects%0Aqualitatively%20and%20quantitatively%2C%20making%20it%20possible%20for%20more%20robust%20and%0Aflexible%20part-level%203D%20Gaussian%20editing.%20Code%20is%20available%20at%0Ahttps%3A//janeyeon.github.io/romap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%25203D-Masked%2520Part-level%2520Editing%2520in%25203D%2520Gaussian%2520Splatting%2520with%250A%2520%2520Regularized%2520Score%2520Distillation%2520Sampling%26entry.906535625%3DHayeon%2520Kim%2520and%2520Ji%2520Ha%2520Jang%2520and%2520Se%2520Young%2520Chun%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520neural%2520representations%2520and%2520instance-level%2520editing%250Amodels%2520have%2520enabled%2520the%2520efficient%2520creation%2520of%2520high-quality%25203D%2520content.%2520However%252C%250Aachieving%2520precise%2520local%25203D%2520edits%2520remains%2520challenging%252C%2520especially%2520for%2520Gaussian%250ASplatting%252C%2520due%2520to%2520inconsistent%2520multi-view%25202D%2520part%2520segmentations%2520and%2520inherently%250Aambiguous%2520nature%2520of%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520loss.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520RoMaP%252C%2520a%2520novel%2520local%25203D%2520Gaussian%2520editing%2520framework%2520that%250Aenables%2520precise%2520and%2520drastic%2520part-level%2520modifications.%2520First%252C%2520we%2520introduce%2520a%250Arobust%25203D%2520mask%2520generation%2520module%2520with%2520our%25203D-Geometry%2520Aware%2520Label%2520Prediction%250A%25283D-GALP%2529%252C%2520which%2520uses%2520spherical%2520harmonics%2520%2528SH%2529%2520coefficients%2520to%2520model%250Aview-dependent%2520label%2520variations%2520and%2520soft-label%2520property%252C%2520yielding%2520accurate%2520and%250Aconsistent%2520part%2520segmentations%2520across%2520viewpoints.%2520Second%252C%2520we%2520propose%2520a%250Aregularized%2520SDS%2520loss%2520that%2520combines%2520the%2520standard%2520SDS%2520loss%2520with%2520additional%250Aregularizers.%2520In%2520particular%252C%2520an%2520L1%2520anchor%2520loss%2520is%2520introduced%2520via%2520our%2520Scheduled%250ALatent%2520Mixing%2520and%2520Part%2520%2528SLaMP%2529%2520editing%2520method%252C%2520which%2520generates%2520high-quality%250Apart-edited%25202D%2520images%2520and%2520confines%2520modifications%2520only%2520to%2520the%2520target%2520region%250Awhile%2520preserving%2520contextual%2520coherence.%2520Additional%2520regularizers%252C%2520such%2520as%250AGaussian%2520prior%2520removal%252C%2520further%2520improve%2520flexibility%2520by%2520allowing%2520changes%2520beyond%250Athe%2520existing%2520context%252C%2520and%2520robust%25203D%2520masking%2520prevents%2520unintended%2520edits.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520RoMaP%2520achieves%2520state-of-the-art%2520local%250A3D%2520editing%2520on%2520both%2520reconstructed%2520and%2520generated%2520Gaussian%2520scenes%2520and%2520objects%250Aqualitatively%2520and%2520quantitatively%252C%2520making%2520it%2520possible%2520for%2520more%2520robust%2520and%250Aflexible%2520part-level%25203D%2520Gaussian%2520editing.%2520Code%2520is%2520available%2520at%250Ahttps%253A//janeyeon.github.io/romap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%203D-Masked%20Part-level%20Editing%20in%203D%20Gaussian%20Splatting%20with%0A%20%20Regularized%20Score%20Distillation%20Sampling&entry.906535625=Hayeon%20Kim%20and%20Ji%20Ha%20Jang%20and%20Se%20Young%20Chun&entry.1292438233=%20%20Recent%20advances%20in%203D%20neural%20representations%20and%20instance-level%20editing%0Amodels%20have%20enabled%20the%20efficient%20creation%20of%20high-quality%203D%20content.%20However%2C%0Aachieving%20precise%20local%203D%20edits%20remains%20challenging%2C%20especially%20for%20Gaussian%0ASplatting%2C%20due%20to%20inconsistent%20multi-view%202D%20part%20segmentations%20and%20inherently%0Aambiguous%20nature%20of%20Score%20Distillation%20Sampling%20%28SDS%29%20loss.%20To%20address%20these%0Alimitations%2C%20we%20propose%20RoMaP%2C%20a%20novel%20local%203D%20Gaussian%20editing%20framework%20that%0Aenables%20precise%20and%20drastic%20part-level%20modifications.%20First%2C%20we%20introduce%20a%0Arobust%203D%20mask%20generation%20module%20with%20our%203D-Geometry%20Aware%20Label%20Prediction%0A%283D-GALP%29%2C%20which%20uses%20spherical%20harmonics%20%28SH%29%20coefficients%20to%20model%0Aview-dependent%20label%20variations%20and%20soft-label%20property%2C%20yielding%20accurate%20and%0Aconsistent%20part%20segmentations%20across%20viewpoints.%20Second%2C%20we%20propose%20a%0Aregularized%20SDS%20loss%20that%20combines%20the%20standard%20SDS%20loss%20with%20additional%0Aregularizers.%20In%20particular%2C%20an%20L1%20anchor%20loss%20is%20introduced%20via%20our%20Scheduled%0ALatent%20Mixing%20and%20Part%20%28SLaMP%29%20editing%20method%2C%20which%20generates%20high-quality%0Apart-edited%202D%20images%20and%20confines%20modifications%20only%20to%20the%20target%20region%0Awhile%20preserving%20contextual%20coherence.%20Additional%20regularizers%2C%20such%20as%0AGaussian%20prior%20removal%2C%20further%20improve%20flexibility%20by%20allowing%20changes%20beyond%0Athe%20existing%20context%2C%20and%20robust%203D%20masking%20prevents%20unintended%20edits.%0AExperimental%20results%20demonstrate%20that%20our%20RoMaP%20achieves%20state-of-the-art%20local%0A3D%20editing%20on%20both%20reconstructed%20and%20generated%20Gaussian%20scenes%20and%20objects%0Aqualitatively%20and%20quantitatively%2C%20making%20it%20possible%20for%20more%20robust%20and%0Aflexible%20part-level%203D%20Gaussian%20editing.%20Code%20is%20available%20at%0Ahttps%3A//janeyeon.github.io/romap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11061v2&entry.124074799=Read"},
{"title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised\n  Autonomous Driving", "author": "Jiawei Xu and Kai Deng and Zexin Fan and Shenlong Wang and Jin Xie and Jian Yang", "abstract": "  Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.\n", "link": "http://arxiv.org/abs/2507.12137v2", "date": "2025-07-21", "relevancy": 3.1622, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6374}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6366}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AD-GS%3A%20Object-Aware%20B-Spline%20Gaussian%20Splatting%20for%20Self-Supervised%0A%20%20Autonomous%20Driving&body=Title%3A%20AD-GS%3A%20Object-Aware%20B-Spline%20Gaussian%20Splatting%20for%20Self-Supervised%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Jiawei%20Xu%20and%20Kai%20Deng%20and%20Zexin%20Fan%20and%20Shenlong%20Wang%20and%20Jin%20Xie%20and%20Jian%20Yang%0AAbstract%3A%20%20%20Modeling%20and%20rendering%20dynamic%20urban%20driving%20scenes%20is%20crucial%20for%0Aself-driving%20simulation.%20Current%20high-quality%20methods%20typically%20rely%20on%20costly%0Amanual%20object%20tracklet%20annotations%2C%20while%20self-supervised%20approaches%20fail%20to%0Acapture%20dynamic%20object%20motions%20accurately%20and%20decompose%20scenes%20properly%2C%0Aresulting%20in%20rendering%20artifacts.%20We%20introduce%20AD-GS%2C%20a%20novel%20self-supervised%0Aframework%20for%20high-quality%20free-viewpoint%20rendering%20of%20driving%20scenes%20from%20a%0Asingle%20log.%20At%20its%20core%20is%20a%20novel%20learnable%20motion%20model%20that%20integrates%0Alocality-aware%20B-spline%20curves%20with%20global-aware%20trigonometric%20functions%2C%0Aenabling%20flexible%20yet%20precise%20dynamic%20object%20modeling.%20Rather%20than%20requiring%0Acomprehensive%20semantic%20labeling%2C%20AD-GS%20automatically%20segments%20scenes%20into%0Aobjects%20and%20background%20with%20the%20simplified%20pseudo%202D%20segmentation%2C%20representing%0Aobjects%20using%20dynamic%20Gaussians%20and%20bidirectional%20temporal%20visibility%20masks.%0AFurther%2C%20our%20model%20incorporates%20visibility%20reasoning%20and%20physically%20rigid%0Aregularization%20to%20enhance%20robustness.%20Extensive%20evaluations%20demonstrate%20that%0Aour%20annotation-free%20model%20significantly%20outperforms%20current%20state-of-the-art%0Aannotation-free%20methods%20and%20is%20competitive%20with%20annotation-dependent%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAD-GS%253A%2520Object-Aware%2520B-Spline%2520Gaussian%2520Splatting%2520for%2520Self-Supervised%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DJiawei%2520Xu%2520and%2520Kai%2520Deng%2520and%2520Zexin%2520Fan%2520and%2520Shenlong%2520Wang%2520and%2520Jin%2520Xie%2520and%2520Jian%2520Yang%26entry.1292438233%3D%2520%2520Modeling%2520and%2520rendering%2520dynamic%2520urban%2520driving%2520scenes%2520is%2520crucial%2520for%250Aself-driving%2520simulation.%2520Current%2520high-quality%2520methods%2520typically%2520rely%2520on%2520costly%250Amanual%2520object%2520tracklet%2520annotations%252C%2520while%2520self-supervised%2520approaches%2520fail%2520to%250Acapture%2520dynamic%2520object%2520motions%2520accurately%2520and%2520decompose%2520scenes%2520properly%252C%250Aresulting%2520in%2520rendering%2520artifacts.%2520We%2520introduce%2520AD-GS%252C%2520a%2520novel%2520self-supervised%250Aframework%2520for%2520high-quality%2520free-viewpoint%2520rendering%2520of%2520driving%2520scenes%2520from%2520a%250Asingle%2520log.%2520At%2520its%2520core%2520is%2520a%2520novel%2520learnable%2520motion%2520model%2520that%2520integrates%250Alocality-aware%2520B-spline%2520curves%2520with%2520global-aware%2520trigonometric%2520functions%252C%250Aenabling%2520flexible%2520yet%2520precise%2520dynamic%2520object%2520modeling.%2520Rather%2520than%2520requiring%250Acomprehensive%2520semantic%2520labeling%252C%2520AD-GS%2520automatically%2520segments%2520scenes%2520into%250Aobjects%2520and%2520background%2520with%2520the%2520simplified%2520pseudo%25202D%2520segmentation%252C%2520representing%250Aobjects%2520using%2520dynamic%2520Gaussians%2520and%2520bidirectional%2520temporal%2520visibility%2520masks.%250AFurther%252C%2520our%2520model%2520incorporates%2520visibility%2520reasoning%2520and%2520physically%2520rigid%250Aregularization%2520to%2520enhance%2520robustness.%2520Extensive%2520evaluations%2520demonstrate%2520that%250Aour%2520annotation-free%2520model%2520significantly%2520outperforms%2520current%2520state-of-the-art%250Aannotation-free%2520methods%2520and%2520is%2520competitive%2520with%2520annotation-dependent%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AD-GS%3A%20Object-Aware%20B-Spline%20Gaussian%20Splatting%20for%20Self-Supervised%0A%20%20Autonomous%20Driving&entry.906535625=Jiawei%20Xu%20and%20Kai%20Deng%20and%20Zexin%20Fan%20and%20Shenlong%20Wang%20and%20Jin%20Xie%20and%20Jian%20Yang&entry.1292438233=%20%20Modeling%20and%20rendering%20dynamic%20urban%20driving%20scenes%20is%20crucial%20for%0Aself-driving%20simulation.%20Current%20high-quality%20methods%20typically%20rely%20on%20costly%0Amanual%20object%20tracklet%20annotations%2C%20while%20self-supervised%20approaches%20fail%20to%0Acapture%20dynamic%20object%20motions%20accurately%20and%20decompose%20scenes%20properly%2C%0Aresulting%20in%20rendering%20artifacts.%20We%20introduce%20AD-GS%2C%20a%20novel%20self-supervised%0Aframework%20for%20high-quality%20free-viewpoint%20rendering%20of%20driving%20scenes%20from%20a%0Asingle%20log.%20At%20its%20core%20is%20a%20novel%20learnable%20motion%20model%20that%20integrates%0Alocality-aware%20B-spline%20curves%20with%20global-aware%20trigonometric%20functions%2C%0Aenabling%20flexible%20yet%20precise%20dynamic%20object%20modeling.%20Rather%20than%20requiring%0Acomprehensive%20semantic%20labeling%2C%20AD-GS%20automatically%20segments%20scenes%20into%0Aobjects%20and%20background%20with%20the%20simplified%20pseudo%202D%20segmentation%2C%20representing%0Aobjects%20using%20dynamic%20Gaussians%20and%20bidirectional%20temporal%20visibility%20masks.%0AFurther%2C%20our%20model%20incorporates%20visibility%20reasoning%20and%20physically%20rigid%0Aregularization%20to%20enhance%20robustness.%20Extensive%20evaluations%20demonstrate%20that%0Aour%20annotation-free%20model%20significantly%20outperforms%20current%20state-of-the-art%0Aannotation-free%20methods%20and%20is%20competitive%20with%20annotation-dependent%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12137v2&entry.124074799=Read"},
{"title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis", "author": "Alexandre Symeonidis-Herzig and \u00d6zge Mercano\u011flu Sincan and Richard Bowden", "abstract": "  Realistic, high-fidelity 3D facial animations are crucial for expressive\navatar systems in human-computer interaction and accessibility. Although prior\nmethods show promising quality, their reliance on the mesh domain limits their\nability to fully leverage the rapid visual innovations seen in 2D computer\nvision and graphics. We propose VisualSpeaker, a novel method that bridges this\ngap using photorealistic differentiable rendering, supervised by visual speech\nrecognition, for improved 3D facial animation. Our contribution is a perceptual\nlip-reading loss, derived by passing photorealistic 3D Gaussian Splatting\navatar renders through a pre-trained Visual Automatic Speech Recognition model\nduring training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker\nimproves both the standard Lip Vertex Error metric by 56.1% and the perceptual\nquality of the generated animations, while retaining the controllability of\nmesh-driven animation. This perceptual focus naturally supports accurate\nmouthings, essential cues that disambiguate similar manual signs in sign\nlanguage avatars.\n", "link": "http://arxiv.org/abs/2507.06060v2", "date": "2025-07-21", "relevancy": 3.1291, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6419}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6419}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis&body=Title%3A%20VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis%0AAuthor%3A%20Alexandre%20Symeonidis-Herzig%20and%20%C3%96zge%20Mercano%C4%9Flu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Realistic%2C%20high-fidelity%203D%20facial%20animations%20are%20crucial%20for%20expressive%0Aavatar%20systems%20in%20human-computer%20interaction%20and%20accessibility.%20Although%20prior%0Amethods%20show%20promising%20quality%2C%20their%20reliance%20on%20the%20mesh%20domain%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rapid%20visual%20innovations%20seen%20in%202D%20computer%0Avision%20and%20graphics.%20We%20propose%20VisualSpeaker%2C%20a%20novel%20method%20that%20bridges%20this%0Agap%20using%20photorealistic%20differentiable%20rendering%2C%20supervised%20by%20visual%20speech%0Arecognition%2C%20for%20improved%203D%20facial%20animation.%20Our%20contribution%20is%20a%20perceptual%0Alip-reading%20loss%2C%20derived%20by%20passing%20photorealistic%203D%20Gaussian%20Splatting%0Aavatar%20renders%20through%20a%20pre-trained%20Visual%20Automatic%20Speech%20Recognition%20model%0Aduring%20training.%20Evaluation%20on%20the%20MEAD%20dataset%20demonstrates%20that%20VisualSpeaker%0Aimproves%20both%20the%20standard%20Lip%20Vertex%20Error%20metric%20by%2056.1%25%20and%20the%20perceptual%0Aquality%20of%20the%20generated%20animations%2C%20while%20retaining%20the%20controllability%20of%0Amesh-driven%20animation.%20This%20perceptual%20focus%20naturally%20supports%20accurate%0Amouthings%2C%20essential%20cues%20that%20disambiguate%20similar%20manual%20signs%20in%20sign%0Alanguage%20avatars.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualSpeaker%253A%2520Visually-Guided%25203D%2520Avatar%2520Lip%2520Synthesis%26entry.906535625%3DAlexandre%2520Symeonidis-Herzig%2520and%2520%25C3%2596zge%2520Mercano%25C4%259Flu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Realistic%252C%2520high-fidelity%25203D%2520facial%2520animations%2520are%2520crucial%2520for%2520expressive%250Aavatar%2520systems%2520in%2520human-computer%2520interaction%2520and%2520accessibility.%2520Although%2520prior%250Amethods%2520show%2520promising%2520quality%252C%2520their%2520reliance%2520on%2520the%2520mesh%2520domain%2520limits%2520their%250Aability%2520to%2520fully%2520leverage%2520the%2520rapid%2520visual%2520innovations%2520seen%2520in%25202D%2520computer%250Avision%2520and%2520graphics.%2520We%2520propose%2520VisualSpeaker%252C%2520a%2520novel%2520method%2520that%2520bridges%2520this%250Agap%2520using%2520photorealistic%2520differentiable%2520rendering%252C%2520supervised%2520by%2520visual%2520speech%250Arecognition%252C%2520for%2520improved%25203D%2520facial%2520animation.%2520Our%2520contribution%2520is%2520a%2520perceptual%250Alip-reading%2520loss%252C%2520derived%2520by%2520passing%2520photorealistic%25203D%2520Gaussian%2520Splatting%250Aavatar%2520renders%2520through%2520a%2520pre-trained%2520Visual%2520Automatic%2520Speech%2520Recognition%2520model%250Aduring%2520training.%2520Evaluation%2520on%2520the%2520MEAD%2520dataset%2520demonstrates%2520that%2520VisualSpeaker%250Aimproves%2520both%2520the%2520standard%2520Lip%2520Vertex%2520Error%2520metric%2520by%252056.1%2525%2520and%2520the%2520perceptual%250Aquality%2520of%2520the%2520generated%2520animations%252C%2520while%2520retaining%2520the%2520controllability%2520of%250Amesh-driven%2520animation.%2520This%2520perceptual%2520focus%2520naturally%2520supports%2520accurate%250Amouthings%252C%2520essential%2520cues%2520that%2520disambiguate%2520similar%2520manual%2520signs%2520in%2520sign%250Alanguage%2520avatars.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualSpeaker%3A%20Visually-Guided%203D%20Avatar%20Lip%20Synthesis&entry.906535625=Alexandre%20Symeonidis-Herzig%20and%20%C3%96zge%20Mercano%C4%9Flu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Realistic%2C%20high-fidelity%203D%20facial%20animations%20are%20crucial%20for%20expressive%0Aavatar%20systems%20in%20human-computer%20interaction%20and%20accessibility.%20Although%20prior%0Amethods%20show%20promising%20quality%2C%20their%20reliance%20on%20the%20mesh%20domain%20limits%20their%0Aability%20to%20fully%20leverage%20the%20rapid%20visual%20innovations%20seen%20in%202D%20computer%0Avision%20and%20graphics.%20We%20propose%20VisualSpeaker%2C%20a%20novel%20method%20that%20bridges%20this%0Agap%20using%20photorealistic%20differentiable%20rendering%2C%20supervised%20by%20visual%20speech%0Arecognition%2C%20for%20improved%203D%20facial%20animation.%20Our%20contribution%20is%20a%20perceptual%0Alip-reading%20loss%2C%20derived%20by%20passing%20photorealistic%203D%20Gaussian%20Splatting%0Aavatar%20renders%20through%20a%20pre-trained%20Visual%20Automatic%20Speech%20Recognition%20model%0Aduring%20training.%20Evaluation%20on%20the%20MEAD%20dataset%20demonstrates%20that%20VisualSpeaker%0Aimproves%20both%20the%20standard%20Lip%20Vertex%20Error%20metric%20by%2056.1%25%20and%20the%20perceptual%0Aquality%20of%20the%20generated%20animations%2C%20while%20retaining%20the%20controllability%20of%0Amesh-driven%20animation.%20This%20perceptual%20focus%20naturally%20supports%20accurate%0Amouthings%2C%20essential%20cues%20that%20disambiguate%20similar%20manual%20signs%20in%20sign%0Alanguage%20avatars.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06060v2&entry.124074799=Read"},
{"title": "InstaScene: Towards Complete 3D Instance Decomposition and\n  Reconstruction from Cluttered Scenes", "author": "Zesong Yang and Bangbang Yang and Wenqi Dong and Chenxuan Cao and Liyuan Cui and Yuewen Ma and Zhaopeng Cui and Hujun Bao", "abstract": "  Humans can naturally identify and mentally complete occluded objects in\ncluttered environments. However, imparting similar cognitive ability to\nrobotics remains challenging even with advanced reconstruction techniques,\nwhich models scenes as undifferentiated wholes and fails to recognize complete\nobject from partial observations. In this paper, we propose InstaScene, a new\nparadigm towards holistic 3D perception of complex scenes with a primary goal:\ndecomposing arbitrary instances while ensuring complete reconstruction. To\nachieve precise decomposition, we develop a novel spatial contrastive learning\nby tracing rasterization of each instance across views, significantly enhancing\nsemantic supervision in cluttered scenes. To overcome incompleteness from\nlimited observations, we introduce in-situ generation that harnesses valuable\nobservations and geometric cues, effectively guiding 3D generative models to\nreconstruct complete instances that seamlessly align with the real world.\nExperiments on scene decomposition and object completion across complex\nreal-world and synthetic scenes demonstrate that our method achieves superior\ndecomposition accuracy while producing geometrically faithful and visually\nintact objects.\n", "link": "http://arxiv.org/abs/2507.08416v2", "date": "2025-07-21", "relevancy": 3.0885, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6203}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstaScene%3A%20Towards%20Complete%203D%20Instance%20Decomposition%20and%0A%20%20Reconstruction%20from%20Cluttered%20Scenes&body=Title%3A%20InstaScene%3A%20Towards%20Complete%203D%20Instance%20Decomposition%20and%0A%20%20Reconstruction%20from%20Cluttered%20Scenes%0AAuthor%3A%20Zesong%20Yang%20and%20Bangbang%20Yang%20and%20Wenqi%20Dong%20and%20Chenxuan%20Cao%20and%20Liyuan%20Cui%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui%20and%20Hujun%20Bao%0AAbstract%3A%20%20%20Humans%20can%20naturally%20identify%20and%20mentally%20complete%20occluded%20objects%20in%0Acluttered%20environments.%20However%2C%20imparting%20similar%20cognitive%20ability%20to%0Arobotics%20remains%20challenging%20even%20with%20advanced%20reconstruction%20techniques%2C%0Awhich%20models%20scenes%20as%20undifferentiated%20wholes%20and%20fails%20to%20recognize%20complete%0Aobject%20from%20partial%20observations.%20In%20this%20paper%2C%20we%20propose%20InstaScene%2C%20a%20new%0Aparadigm%20towards%20holistic%203D%20perception%20of%20complex%20scenes%20with%20a%20primary%20goal%3A%0Adecomposing%20arbitrary%20instances%20while%20ensuring%20complete%20reconstruction.%20To%0Aachieve%20precise%20decomposition%2C%20we%20develop%20a%20novel%20spatial%20contrastive%20learning%0Aby%20tracing%20rasterization%20of%20each%20instance%20across%20views%2C%20significantly%20enhancing%0Asemantic%20supervision%20in%20cluttered%20scenes.%20To%20overcome%20incompleteness%20from%0Alimited%20observations%2C%20we%20introduce%20in-situ%20generation%20that%20harnesses%20valuable%0Aobservations%20and%20geometric%20cues%2C%20effectively%20guiding%203D%20generative%20models%20to%0Areconstruct%20complete%20instances%20that%20seamlessly%20align%20with%20the%20real%20world.%0AExperiments%20on%20scene%20decomposition%20and%20object%20completion%20across%20complex%0Areal-world%20and%20synthetic%20scenes%20demonstrate%20that%20our%20method%20achieves%20superior%0Adecomposition%20accuracy%20while%20producing%20geometrically%20faithful%20and%20visually%0Aintact%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstaScene%253A%2520Towards%2520Complete%25203D%2520Instance%2520Decomposition%2520and%250A%2520%2520Reconstruction%2520from%2520Cluttered%2520Scenes%26entry.906535625%3DZesong%2520Yang%2520and%2520Bangbang%2520Yang%2520and%2520Wenqi%2520Dong%2520and%2520Chenxuan%2520Cao%2520and%2520Liyuan%2520Cui%2520and%2520Yuewen%2520Ma%2520and%2520Zhaopeng%2520Cui%2520and%2520Hujun%2520Bao%26entry.1292438233%3D%2520%2520Humans%2520can%2520naturally%2520identify%2520and%2520mentally%2520complete%2520occluded%2520objects%2520in%250Acluttered%2520environments.%2520However%252C%2520imparting%2520similar%2520cognitive%2520ability%2520to%250Arobotics%2520remains%2520challenging%2520even%2520with%2520advanced%2520reconstruction%2520techniques%252C%250Awhich%2520models%2520scenes%2520as%2520undifferentiated%2520wholes%2520and%2520fails%2520to%2520recognize%2520complete%250Aobject%2520from%2520partial%2520observations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520InstaScene%252C%2520a%2520new%250Aparadigm%2520towards%2520holistic%25203D%2520perception%2520of%2520complex%2520scenes%2520with%2520a%2520primary%2520goal%253A%250Adecomposing%2520arbitrary%2520instances%2520while%2520ensuring%2520complete%2520reconstruction.%2520To%250Aachieve%2520precise%2520decomposition%252C%2520we%2520develop%2520a%2520novel%2520spatial%2520contrastive%2520learning%250Aby%2520tracing%2520rasterization%2520of%2520each%2520instance%2520across%2520views%252C%2520significantly%2520enhancing%250Asemantic%2520supervision%2520in%2520cluttered%2520scenes.%2520To%2520overcome%2520incompleteness%2520from%250Alimited%2520observations%252C%2520we%2520introduce%2520in-situ%2520generation%2520that%2520harnesses%2520valuable%250Aobservations%2520and%2520geometric%2520cues%252C%2520effectively%2520guiding%25203D%2520generative%2520models%2520to%250Areconstruct%2520complete%2520instances%2520that%2520seamlessly%2520align%2520with%2520the%2520real%2520world.%250AExperiments%2520on%2520scene%2520decomposition%2520and%2520object%2520completion%2520across%2520complex%250Areal-world%2520and%2520synthetic%2520scenes%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%250Adecomposition%2520accuracy%2520while%2520producing%2520geometrically%2520faithful%2520and%2520visually%250Aintact%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstaScene%3A%20Towards%20Complete%203D%20Instance%20Decomposition%20and%0A%20%20Reconstruction%20from%20Cluttered%20Scenes&entry.906535625=Zesong%20Yang%20and%20Bangbang%20Yang%20and%20Wenqi%20Dong%20and%20Chenxuan%20Cao%20and%20Liyuan%20Cui%20and%20Yuewen%20Ma%20and%20Zhaopeng%20Cui%20and%20Hujun%20Bao&entry.1292438233=%20%20Humans%20can%20naturally%20identify%20and%20mentally%20complete%20occluded%20objects%20in%0Acluttered%20environments.%20However%2C%20imparting%20similar%20cognitive%20ability%20to%0Arobotics%20remains%20challenging%20even%20with%20advanced%20reconstruction%20techniques%2C%0Awhich%20models%20scenes%20as%20undifferentiated%20wholes%20and%20fails%20to%20recognize%20complete%0Aobject%20from%20partial%20observations.%20In%20this%20paper%2C%20we%20propose%20InstaScene%2C%20a%20new%0Aparadigm%20towards%20holistic%203D%20perception%20of%20complex%20scenes%20with%20a%20primary%20goal%3A%0Adecomposing%20arbitrary%20instances%20while%20ensuring%20complete%20reconstruction.%20To%0Aachieve%20precise%20decomposition%2C%20we%20develop%20a%20novel%20spatial%20contrastive%20learning%0Aby%20tracing%20rasterization%20of%20each%20instance%20across%20views%2C%20significantly%20enhancing%0Asemantic%20supervision%20in%20cluttered%20scenes.%20To%20overcome%20incompleteness%20from%0Alimited%20observations%2C%20we%20introduce%20in-situ%20generation%20that%20harnesses%20valuable%0Aobservations%20and%20geometric%20cues%2C%20effectively%20guiding%203D%20generative%20models%20to%0Areconstruct%20complete%20instances%20that%20seamlessly%20align%20with%20the%20real%20world.%0AExperiments%20on%20scene%20decomposition%20and%20object%20completion%20across%20complex%0Areal-world%20and%20synthetic%20scenes%20demonstrate%20that%20our%20method%20achieves%20superior%0Adecomposition%20accuracy%20while%20producing%20geometrically%20faithful%20and%20visually%0Aintact%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08416v2&entry.124074799=Read"},
{"title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical\n  Commonsense Understanding in Video Generation Models", "author": "Enes Sanli and Baris Sarper Tezcan and Aykut Erdem and Erkut Erdem", "abstract": "  Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.\n", "link": "http://arxiv.org/abs/2507.15824v1", "date": "2025-07-21", "relevancy": 3.0585, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6676}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Your%20Model%20Separate%20Yolks%20with%20a%20Water%20Bottle%3F%20Benchmarking%20Physical%0A%20%20Commonsense%20Understanding%20in%20Video%20Generation%20Models&body=Title%3A%20Can%20Your%20Model%20Separate%20Yolks%20with%20a%20Water%20Bottle%3F%20Benchmarking%20Physical%0A%20%20Commonsense%20Understanding%20in%20Video%20Generation%20Models%0AAuthor%3A%20Enes%20Sanli%20and%20Baris%20Sarper%20Tezcan%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem%0AAbstract%3A%20%20%20Recent%20progress%20in%20text-to-video%20%28T2V%29%20generation%20has%20enabled%20the%20synthesis%0Aof%20visually%20compelling%20and%20temporally%20coherent%20videos%20from%20natural%20language.%0AHowever%2C%20these%20models%20often%20fall%20short%20in%20basic%20physical%20commonsense%2C%20producing%0Aoutputs%20that%20violate%20intuitive%20expectations%20around%20causality%2C%20object%20behavior%2C%0Aand%20tool%20use.%20Addressing%20this%20gap%2C%20we%20present%20PhysVidBench%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20physical%20reasoning%20capabilities%20of%20T2V%20systems.%20The%0Abenchmark%20includes%20383%20carefully%20curated%20prompts%2C%20emphasizing%20tool%20use%2C%0Amaterial%20properties%2C%20and%20procedural%20interactions%2C%20and%20domains%20where%20physical%0Aplausibility%20is%20crucial.%20For%20each%20prompt%2C%20we%20generate%20videos%20using%20diverse%0Astate-of-the-art%20models%20and%20adopt%20a%20three-stage%20evaluation%20pipeline%3A%20%281%29%0Aformulate%20grounded%20physics%20questions%20from%20the%20prompt%2C%20%282%29%20caption%20the%20generated%0Avideo%20with%20a%20vision-language%20model%2C%20and%20%283%29%20task%20a%20language%20model%20to%20answer%0Aseveral%20physics-involved%20questions%20using%20only%20the%20caption.%20This%20indirect%0Astrategy%20circumvents%20common%20hallucination%20issues%20in%20direct%20video-based%0Aevaluation.%20By%20highlighting%20affordances%20and%20tool-mediated%20actions%2C%20areas%0Aoverlooked%20in%20current%20T2V%20evaluations%2C%20PhysVidBench%20provides%20a%20structured%2C%0Ainterpretable%20framework%20for%20assessing%20physical%20commonsense%20in%20generative%20video%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Your%2520Model%2520Separate%2520Yolks%2520with%2520a%2520Water%2520Bottle%253F%2520Benchmarking%2520Physical%250A%2520%2520Commonsense%2520Understanding%2520in%2520Video%2520Generation%2520Models%26entry.906535625%3DEnes%2520Sanli%2520and%2520Baris%2520Sarper%2520Tezcan%2520and%2520Aykut%2520Erdem%2520and%2520Erkut%2520Erdem%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520text-to-video%2520%2528T2V%2529%2520generation%2520has%2520enabled%2520the%2520synthesis%250Aof%2520visually%2520compelling%2520and%2520temporally%2520coherent%2520videos%2520from%2520natural%2520language.%250AHowever%252C%2520these%2520models%2520often%2520fall%2520short%2520in%2520basic%2520physical%2520commonsense%252C%2520producing%250Aoutputs%2520that%2520violate%2520intuitive%2520expectations%2520around%2520causality%252C%2520object%2520behavior%252C%250Aand%2520tool%2520use.%2520Addressing%2520this%2520gap%252C%2520we%2520present%2520PhysVidBench%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520the%2520physical%2520reasoning%2520capabilities%2520of%2520T2V%2520systems.%2520The%250Abenchmark%2520includes%2520383%2520carefully%2520curated%2520prompts%252C%2520emphasizing%2520tool%2520use%252C%250Amaterial%2520properties%252C%2520and%2520procedural%2520interactions%252C%2520and%2520domains%2520where%2520physical%250Aplausibility%2520is%2520crucial.%2520For%2520each%2520prompt%252C%2520we%2520generate%2520videos%2520using%2520diverse%250Astate-of-the-art%2520models%2520and%2520adopt%2520a%2520three-stage%2520evaluation%2520pipeline%253A%2520%25281%2529%250Aformulate%2520grounded%2520physics%2520questions%2520from%2520the%2520prompt%252C%2520%25282%2529%2520caption%2520the%2520generated%250Avideo%2520with%2520a%2520vision-language%2520model%252C%2520and%2520%25283%2529%2520task%2520a%2520language%2520model%2520to%2520answer%250Aseveral%2520physics-involved%2520questions%2520using%2520only%2520the%2520caption.%2520This%2520indirect%250Astrategy%2520circumvents%2520common%2520hallucination%2520issues%2520in%2520direct%2520video-based%250Aevaluation.%2520By%2520highlighting%2520affordances%2520and%2520tool-mediated%2520actions%252C%2520areas%250Aoverlooked%2520in%2520current%2520T2V%2520evaluations%252C%2520PhysVidBench%2520provides%2520a%2520structured%252C%250Ainterpretable%2520framework%2520for%2520assessing%2520physical%2520commonsense%2520in%2520generative%2520video%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Your%20Model%20Separate%20Yolks%20with%20a%20Water%20Bottle%3F%20Benchmarking%20Physical%0A%20%20Commonsense%20Understanding%20in%20Video%20Generation%20Models&entry.906535625=Enes%20Sanli%20and%20Baris%20Sarper%20Tezcan%20and%20Aykut%20Erdem%20and%20Erkut%20Erdem&entry.1292438233=%20%20Recent%20progress%20in%20text-to-video%20%28T2V%29%20generation%20has%20enabled%20the%20synthesis%0Aof%20visually%20compelling%20and%20temporally%20coherent%20videos%20from%20natural%20language.%0AHowever%2C%20these%20models%20often%20fall%20short%20in%20basic%20physical%20commonsense%2C%20producing%0Aoutputs%20that%20violate%20intuitive%20expectations%20around%20causality%2C%20object%20behavior%2C%0Aand%20tool%20use.%20Addressing%20this%20gap%2C%20we%20present%20PhysVidBench%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20physical%20reasoning%20capabilities%20of%20T2V%20systems.%20The%0Abenchmark%20includes%20383%20carefully%20curated%20prompts%2C%20emphasizing%20tool%20use%2C%0Amaterial%20properties%2C%20and%20procedural%20interactions%2C%20and%20domains%20where%20physical%0Aplausibility%20is%20crucial.%20For%20each%20prompt%2C%20we%20generate%20videos%20using%20diverse%0Astate-of-the-art%20models%20and%20adopt%20a%20three-stage%20evaluation%20pipeline%3A%20%281%29%0Aformulate%20grounded%20physics%20questions%20from%20the%20prompt%2C%20%282%29%20caption%20the%20generated%0Avideo%20with%20a%20vision-language%20model%2C%20and%20%283%29%20task%20a%20language%20model%20to%20answer%0Aseveral%20physics-involved%20questions%20using%20only%20the%20caption.%20This%20indirect%0Astrategy%20circumvents%20common%20hallucination%20issues%20in%20direct%20video-based%0Aevaluation.%20By%20highlighting%20affordances%20and%20tool-mediated%20actions%2C%20areas%0Aoverlooked%20in%20current%20T2V%20evaluations%2C%20PhysVidBench%20provides%20a%20structured%2C%0Ainterpretable%20framework%20for%20assessing%20physical%20commonsense%20in%20generative%20video%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15824v1&entry.124074799=Read"},
{"title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian\n  Splatting", "author": "Hung Nguyen and Runfa Li and An Le and Truong Nguyen", "abstract": "  Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.\n", "link": "http://arxiv.org/abs/2507.15690v1", "date": "2025-07-21", "relevancy": 3.0562, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.627}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6255}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DWTGS%3A%20Rethinking%20Frequency%20Regularization%20for%20Sparse-view%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20DWTGS%3A%20Rethinking%20Frequency%20Regularization%20for%20Sparse-view%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen%0AAbstract%3A%20%20%20Sparse-view%203D%20Gaussian%20Splatting%20%283DGS%29%20presents%20significant%20challenges%20in%0Areconstructing%20high-quality%20novel%20views%2C%20as%20it%20often%20overfits%20to%20the%0Awidely-varying%20high-frequency%20%28HF%29%20details%20of%20the%20sparse%20training%20views.%20While%0Afrequency%20regularization%20can%20be%20a%20promising%20approach%2C%20its%20typical%20reliance%20on%0AFourier%20transforms%20causes%20difficult%20parameter%20tuning%20and%20biases%20towards%0Adetrimental%20HF%20learning.%20We%20propose%20DWTGS%2C%20a%20framework%20that%20rethinks%20frequency%0Aregularization%20by%20leveraging%20wavelet-space%20losses%20that%20provide%20additional%0Aspatial%20supervision.%20Specifically%2C%20we%20supervise%20only%20the%20low-frequency%20%28LF%29%20LL%0Asubbands%20at%20multiple%20DWT%20levels%2C%20while%20enforcing%20sparsity%20on%20the%20HF%20HH%20subband%0Ain%20a%20self-supervised%20manner.%20Experiments%20across%20benchmarks%20show%20that%20DWTGS%0Aconsistently%20outperforms%20Fourier-based%20counterparts%2C%20as%20this%20LF-centric%0Astrategy%20improves%20generalization%20and%20reduces%20HF%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDWTGS%253A%2520Rethinking%2520Frequency%2520Regularization%2520for%2520Sparse-view%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DHung%2520Nguyen%2520and%2520Runfa%2520Li%2520and%2520An%2520Le%2520and%2520Truong%2520Nguyen%26entry.1292438233%3D%2520%2520Sparse-view%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520presents%2520significant%2520challenges%2520in%250Areconstructing%2520high-quality%2520novel%2520views%252C%2520as%2520it%2520often%2520overfits%2520to%2520the%250Awidely-varying%2520high-frequency%2520%2528HF%2529%2520details%2520of%2520the%2520sparse%2520training%2520views.%2520While%250Afrequency%2520regularization%2520can%2520be%2520a%2520promising%2520approach%252C%2520its%2520typical%2520reliance%2520on%250AFourier%2520transforms%2520causes%2520difficult%2520parameter%2520tuning%2520and%2520biases%2520towards%250Adetrimental%2520HF%2520learning.%2520We%2520propose%2520DWTGS%252C%2520a%2520framework%2520that%2520rethinks%2520frequency%250Aregularization%2520by%2520leveraging%2520wavelet-space%2520losses%2520that%2520provide%2520additional%250Aspatial%2520supervision.%2520Specifically%252C%2520we%2520supervise%2520only%2520the%2520low-frequency%2520%2528LF%2529%2520LL%250Asubbands%2520at%2520multiple%2520DWT%2520levels%252C%2520while%2520enforcing%2520sparsity%2520on%2520the%2520HF%2520HH%2520subband%250Ain%2520a%2520self-supervised%2520manner.%2520Experiments%2520across%2520benchmarks%2520show%2520that%2520DWTGS%250Aconsistently%2520outperforms%2520Fourier-based%2520counterparts%252C%2520as%2520this%2520LF-centric%250Astrategy%2520improves%2520generalization%2520and%2520reduces%2520HF%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DWTGS%3A%20Rethinking%20Frequency%20Regularization%20for%20Sparse-view%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen&entry.1292438233=%20%20Sparse-view%203D%20Gaussian%20Splatting%20%283DGS%29%20presents%20significant%20challenges%20in%0Areconstructing%20high-quality%20novel%20views%2C%20as%20it%20often%20overfits%20to%20the%0Awidely-varying%20high-frequency%20%28HF%29%20details%20of%20the%20sparse%20training%20views.%20While%0Afrequency%20regularization%20can%20be%20a%20promising%20approach%2C%20its%20typical%20reliance%20on%0AFourier%20transforms%20causes%20difficult%20parameter%20tuning%20and%20biases%20towards%0Adetrimental%20HF%20learning.%20We%20propose%20DWTGS%2C%20a%20framework%20that%20rethinks%20frequency%0Aregularization%20by%20leveraging%20wavelet-space%20losses%20that%20provide%20additional%0Aspatial%20supervision.%20Specifically%2C%20we%20supervise%20only%20the%20low-frequency%20%28LF%29%20LL%0Asubbands%20at%20multiple%20DWT%20levels%2C%20while%20enforcing%20sparsity%20on%20the%20HF%20HH%20subband%0Ain%20a%20self-supervised%20manner.%20Experiments%20across%20benchmarks%20show%20that%20DWTGS%0Aconsistently%20outperforms%20Fourier-based%20counterparts%2C%20as%20this%20LF-centric%0Astrategy%20improves%20generalization%20and%20reduces%20HF%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15690v1&entry.124074799=Read"},
{"title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers", "author": "Zerui Chen and Rolandos Alexandros Potamias and Shizhe Chen and Cordelia Schmid", "abstract": "  Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.\n", "link": "http://arxiv.org/abs/2503.21313v2", "date": "2025-07-21", "relevancy": 2.9774, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6312}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HORT%3A%20Monocular%20Hand-held%20Objects%20Reconstruction%20with%20Transformers&body=Title%3A%20HORT%3A%20Monocular%20Hand-held%20Objects%20Reconstruction%20with%20Transformers%0AAuthor%3A%20Zerui%20Chen%20and%20Rolandos%20Alexandros%20Potamias%20and%20Shizhe%20Chen%20and%20Cordelia%20Schmid%0AAbstract%3A%20%20%20Reconstructing%20hand-held%20objects%20in%203D%20from%20monocular%20images%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20Most%20existing%20approaches%20rely%20on%0Aimplicit%203D%20representations%2C%20which%20produce%20overly%20smooth%20reconstructions%20and%0Aare%20time-consuming%20to%20generate%20explicit%203D%20shapes.%20While%20more%20recent%20methods%0Adirectly%20reconstruct%20point%20clouds%20with%20diffusion%20models%2C%20the%20multi-step%0Adenoising%20makes%20high-resolution%20reconstruction%20inefficient.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20transformer-based%20model%20to%20efficiently%20reconstruct%0Adense%203D%20point%20clouds%20of%20hand-held%20objects.%20Our%20method%20follows%20a%20coarse-to-fine%0Astrategy%2C%20first%20generating%20a%20sparse%20point%20cloud%20from%20the%20image%20and%0Aprogressively%20refining%20it%20into%20a%20dense%20representation%20using%20pixel-aligned%20image%0Afeatures.%20To%20enhance%20reconstruction%20accuracy%2C%20we%20integrate%20image%20features%20with%0A3D%20hand%20geometry%20to%20jointly%20predict%20the%20object%20point%20cloud%20and%20its%20pose%0Arelative%20to%20the%20hand.%20Our%20model%20is%20trained%20end-to-end%20for%20optimal%20performance.%0AExperimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20accuracy%20with%20much%20faster%20inference%20speed%2C%0Awhile%20generalizing%20well%20to%20in-the-wild%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHORT%253A%2520Monocular%2520Hand-held%2520Objects%2520Reconstruction%2520with%2520Transformers%26entry.906535625%3DZerui%2520Chen%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Shizhe%2520Chen%2520and%2520Cordelia%2520Schmid%26entry.1292438233%3D%2520%2520Reconstructing%2520hand-held%2520objects%2520in%25203D%2520from%2520monocular%2520images%2520remains%2520a%250Asignificant%2520challenge%2520in%2520computer%2520vision.%2520Most%2520existing%2520approaches%2520rely%2520on%250Aimplicit%25203D%2520representations%252C%2520which%2520produce%2520overly%2520smooth%2520reconstructions%2520and%250Aare%2520time-consuming%2520to%2520generate%2520explicit%25203D%2520shapes.%2520While%2520more%2520recent%2520methods%250Adirectly%2520reconstruct%2520point%2520clouds%2520with%2520diffusion%2520models%252C%2520the%2520multi-step%250Adenoising%2520makes%2520high-resolution%2520reconstruction%2520inefficient.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520transformer-based%2520model%2520to%2520efficiently%2520reconstruct%250Adense%25203D%2520point%2520clouds%2520of%2520hand-held%2520objects.%2520Our%2520method%2520follows%2520a%2520coarse-to-fine%250Astrategy%252C%2520first%2520generating%2520a%2520sparse%2520point%2520cloud%2520from%2520the%2520image%2520and%250Aprogressively%2520refining%2520it%2520into%2520a%2520dense%2520representation%2520using%2520pixel-aligned%2520image%250Afeatures.%2520To%2520enhance%2520reconstruction%2520accuracy%252C%2520we%2520integrate%2520image%2520features%2520with%250A3D%2520hand%2520geometry%2520to%2520jointly%2520predict%2520the%2520object%2520point%2520cloud%2520and%2520its%2520pose%250Arelative%2520to%2520the%2520hand.%2520Our%2520model%2520is%2520trained%2520end-to-end%2520for%2520optimal%2520performance.%250AExperimental%2520results%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520state-of-the-art%2520accuracy%2520with%2520much%2520faster%2520inference%2520speed%252C%250Awhile%2520generalizing%2520well%2520to%2520in-the-wild%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HORT%3A%20Monocular%20Hand-held%20Objects%20Reconstruction%20with%20Transformers&entry.906535625=Zerui%20Chen%20and%20Rolandos%20Alexandros%20Potamias%20and%20Shizhe%20Chen%20and%20Cordelia%20Schmid&entry.1292438233=%20%20Reconstructing%20hand-held%20objects%20in%203D%20from%20monocular%20images%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20Most%20existing%20approaches%20rely%20on%0Aimplicit%203D%20representations%2C%20which%20produce%20overly%20smooth%20reconstructions%20and%0Aare%20time-consuming%20to%20generate%20explicit%203D%20shapes.%20While%20more%20recent%20methods%0Adirectly%20reconstruct%20point%20clouds%20with%20diffusion%20models%2C%20the%20multi-step%0Adenoising%20makes%20high-resolution%20reconstruction%20inefficient.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20transformer-based%20model%20to%20efficiently%20reconstruct%0Adense%203D%20point%20clouds%20of%20hand-held%20objects.%20Our%20method%20follows%20a%20coarse-to-fine%0Astrategy%2C%20first%20generating%20a%20sparse%20point%20cloud%20from%20the%20image%20and%0Aprogressively%20refining%20it%20into%20a%20dense%20representation%20using%20pixel-aligned%20image%0Afeatures.%20To%20enhance%20reconstruction%20accuracy%2C%20we%20integrate%20image%20features%20with%0A3D%20hand%20geometry%20to%20jointly%20predict%20the%20object%20point%20cloud%20and%20its%20pose%0Arelative%20to%20the%20hand.%20Our%20model%20is%20trained%20end-to-end%20for%20optimal%20performance.%0AExperimental%20results%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20accuracy%20with%20much%20faster%20inference%20speed%2C%0Awhile%20generalizing%20well%20to%20in-the-wild%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21313v2&entry.124074799=Read"},
{"title": "DynImg: Key Frames with Visual Prompts are Good Representation for\n  Multi-Modal Video Understanding", "author": "Xiaoyi Bao and Chenwei Xie and Hao Tang and Tingyu Weng and Xiaofeng Wang and Yun Zheng and Xingang Wang", "abstract": "  In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.\n", "link": "http://arxiv.org/abs/2507.15569v1", "date": "2025-07-21", "relevancy": 2.9731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynImg%3A%20Key%20Frames%20with%20Visual%20Prompts%20are%20Good%20Representation%20for%0A%20%20Multi-Modal%20Video%20Understanding&body=Title%3A%20DynImg%3A%20Key%20Frames%20with%20Visual%20Prompts%20are%20Good%20Representation%20for%0A%20%20Multi-Modal%20Video%20Understanding%0AAuthor%3A%20Xiaoyi%20Bao%20and%20Chenwei%20Xie%20and%20Hao%20Tang%20and%20Tingyu%20Weng%20and%20Xiaofeng%20Wang%20and%20Yun%20Zheng%20and%20Xingang%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20introduction%20of%20Multi-modal%20Large%20Language%20Models%0A%28MLLMs%29%20into%20video%20understanding%20tasks%20has%20become%20increasingly%20prevalent.%0AHowever%2C%20how%20to%20effectively%20integrate%20temporal%20information%20remains%20a%20critical%0Aresearch%20focus.%20Traditional%20approaches%20treat%20spatial%20and%20temporal%20information%0Aseparately.%20Due%20to%20issues%20like%20motion%20blur%2C%20it%20is%20challenging%20to%20accurately%0Arepresent%20the%20spatial%20information%20of%20rapidly%20moving%20objects.%20This%20can%20lead%20to%0Atemporally%20important%20regions%20being%20underemphasized%20during%20spatial%20feature%0Aextraction%2C%20which%20in%20turn%20hinders%20accurate%20spatio-temporal%20interaction%20and%0Avideo%20understanding.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20innovative%20video%0Arepresentation%20method%20called%20Dynamic-Image%20%28DynImg%29.%20Specifically%2C%20we%20introduce%0Aa%20set%20of%20non-key%20frames%20as%20temporal%20prompts%20to%20highlight%20the%20spatial%20areas%0Acontaining%20fast-moving%20objects.%20During%20the%20process%20of%20visual%20feature%0Aextraction%2C%20these%20prompts%20guide%20the%20model%20to%20pay%20additional%20attention%20to%20the%0Afine-grained%20spatial%20features%20corresponding%20to%20these%20regions.%20Moreover%2C%20to%0Amaintain%20the%20correct%20sequence%20for%20DynImg%2C%20we%20employ%20a%20corresponding%204D%20video%0ARotary%20Position%20Embedding.%20This%20retains%20both%20the%20temporal%20and%20spatial%20adjacency%0Aof%20DynImg%2C%20helping%20MLLM%20understand%20the%20spatio-temporal%20order%20within%20this%0Acombined%20format.%20Experimental%20evaluations%20reveal%20that%20DynImg%20surpasses%20the%0Astate-of-the-art%20methods%20by%20approximately%202%25%20across%20multiple%20video%0Aunderstanding%20benchmarks%2C%20proving%20the%20effectiveness%20of%20our%20temporal%20prompts%20in%0Aenhancing%20video%20comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynImg%253A%2520Key%2520Frames%2520with%2520Visual%2520Prompts%2520are%2520Good%2520Representation%2520for%250A%2520%2520Multi-Modal%2520Video%2520Understanding%26entry.906535625%3DXiaoyi%2520Bao%2520and%2520Chenwei%2520Xie%2520and%2520Hao%2520Tang%2520and%2520Tingyu%2520Weng%2520and%2520Xiaofeng%2520Wang%2520and%2520Yun%2520Zheng%2520and%2520Xingang%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520introduction%2520of%2520Multi-modal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520into%2520video%2520understanding%2520tasks%2520has%2520become%2520increasingly%2520prevalent.%250AHowever%252C%2520how%2520to%2520effectively%2520integrate%2520temporal%2520information%2520remains%2520a%2520critical%250Aresearch%2520focus.%2520Traditional%2520approaches%2520treat%2520spatial%2520and%2520temporal%2520information%250Aseparately.%2520Due%2520to%2520issues%2520like%2520motion%2520blur%252C%2520it%2520is%2520challenging%2520to%2520accurately%250Arepresent%2520the%2520spatial%2520information%2520of%2520rapidly%2520moving%2520objects.%2520This%2520can%2520lead%2520to%250Atemporally%2520important%2520regions%2520being%2520underemphasized%2520during%2520spatial%2520feature%250Aextraction%252C%2520which%2520in%2520turn%2520hinders%2520accurate%2520spatio-temporal%2520interaction%2520and%250Avideo%2520understanding.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520an%2520innovative%2520video%250Arepresentation%2520method%2520called%2520Dynamic-Image%2520%2528DynImg%2529.%2520Specifically%252C%2520we%2520introduce%250Aa%2520set%2520of%2520non-key%2520frames%2520as%2520temporal%2520prompts%2520to%2520highlight%2520the%2520spatial%2520areas%250Acontaining%2520fast-moving%2520objects.%2520During%2520the%2520process%2520of%2520visual%2520feature%250Aextraction%252C%2520these%2520prompts%2520guide%2520the%2520model%2520to%2520pay%2520additional%2520attention%2520to%2520the%250Afine-grained%2520spatial%2520features%2520corresponding%2520to%2520these%2520regions.%2520Moreover%252C%2520to%250Amaintain%2520the%2520correct%2520sequence%2520for%2520DynImg%252C%2520we%2520employ%2520a%2520corresponding%25204D%2520video%250ARotary%2520Position%2520Embedding.%2520This%2520retains%2520both%2520the%2520temporal%2520and%2520spatial%2520adjacency%250Aof%2520DynImg%252C%2520helping%2520MLLM%2520understand%2520the%2520spatio-temporal%2520order%2520within%2520this%250Acombined%2520format.%2520Experimental%2520evaluations%2520reveal%2520that%2520DynImg%2520surpasses%2520the%250Astate-of-the-art%2520methods%2520by%2520approximately%25202%2525%2520across%2520multiple%2520video%250Aunderstanding%2520benchmarks%252C%2520proving%2520the%2520effectiveness%2520of%2520our%2520temporal%2520prompts%2520in%250Aenhancing%2520video%2520comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynImg%3A%20Key%20Frames%20with%20Visual%20Prompts%20are%20Good%20Representation%20for%0A%20%20Multi-Modal%20Video%20Understanding&entry.906535625=Xiaoyi%20Bao%20and%20Chenwei%20Xie%20and%20Hao%20Tang%20and%20Tingyu%20Weng%20and%20Xiaofeng%20Wang%20and%20Yun%20Zheng%20and%20Xingang%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20introduction%20of%20Multi-modal%20Large%20Language%20Models%0A%28MLLMs%29%20into%20video%20understanding%20tasks%20has%20become%20increasingly%20prevalent.%0AHowever%2C%20how%20to%20effectively%20integrate%20temporal%20information%20remains%20a%20critical%0Aresearch%20focus.%20Traditional%20approaches%20treat%20spatial%20and%20temporal%20information%0Aseparately.%20Due%20to%20issues%20like%20motion%20blur%2C%20it%20is%20challenging%20to%20accurately%0Arepresent%20the%20spatial%20information%20of%20rapidly%20moving%20objects.%20This%20can%20lead%20to%0Atemporally%20important%20regions%20being%20underemphasized%20during%20spatial%20feature%0Aextraction%2C%20which%20in%20turn%20hinders%20accurate%20spatio-temporal%20interaction%20and%0Avideo%20understanding.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20innovative%20video%0Arepresentation%20method%20called%20Dynamic-Image%20%28DynImg%29.%20Specifically%2C%20we%20introduce%0Aa%20set%20of%20non-key%20frames%20as%20temporal%20prompts%20to%20highlight%20the%20spatial%20areas%0Acontaining%20fast-moving%20objects.%20During%20the%20process%20of%20visual%20feature%0Aextraction%2C%20these%20prompts%20guide%20the%20model%20to%20pay%20additional%20attention%20to%20the%0Afine-grained%20spatial%20features%20corresponding%20to%20these%20regions.%20Moreover%2C%20to%0Amaintain%20the%20correct%20sequence%20for%20DynImg%2C%20we%20employ%20a%20corresponding%204D%20video%0ARotary%20Position%20Embedding.%20This%20retains%20both%20the%20temporal%20and%20spatial%20adjacency%0Aof%20DynImg%2C%20helping%20MLLM%20understand%20the%20spatio-temporal%20order%20within%20this%0Acombined%20format.%20Experimental%20evaluations%20reveal%20that%20DynImg%20surpasses%20the%0Astate-of-the-art%20methods%20by%20approximately%202%25%20across%20multiple%20video%0Aunderstanding%20benchmarks%2C%20proving%20the%20effectiveness%20of%20our%20temporal%20prompts%20in%0Aenhancing%20video%20comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15569v1&entry.124074799=Read"},
{"title": "One Last Attention for Your Vision-Language Model", "author": "Liang Chen and Ghazi Shazan Ahmad and Tianjun Yao and Lingqiao Liu and Zhiqiang Shen", "abstract": "  Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.\n", "link": "http://arxiv.org/abs/2507.15480v1", "date": "2025-07-21", "relevancy": 2.909, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Last%20Attention%20for%20Your%20Vision-Language%20Model&body=Title%3A%20One%20Last%20Attention%20for%20Your%20Vision-Language%20Model%0AAuthor%3A%20Liang%20Chen%20and%20Ghazi%20Shazan%20Ahmad%20and%20Tianjun%20Yao%20and%20Lingqiao%20Liu%20and%20Zhiqiang%20Shen%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20achieve%20remarkable%0Azero-shot%20performance%2C%20yet%20their%20downstream%20potential%20hinges%20on%20effective%0Afine-tuning.%20Most%20adaptation%20methods%20typically%20focus%20on%20refining%20representation%0Afrom%20separate%20modalities%20%28text%20or%20vision%29%20but%20neglect%20the%20critical%20role%20of%0Atheir%20fused%20representations%20in%20the%20decision-making%20process%2C%20%5Cemph%7B%5Cie%7D%20rational%0Amatrix%20that%20drives%20the%20final%20prediction.%20To%20bridge%20the%20gap%2C%20we%20propose%20a%20simple%0Ayet%20effective%20%5Ctextbf%7BR%7Dational%20%5Ctextbf%7BAda%7Dptaion%20%28%7BRAda%7D%29%20to%20explicitly%0Aexploit%20the%20final%20fused%20representation%20during%20fine-tuning.%20RAda%20employs%20a%0Alearned%20mask%2C%20obtained%20from%20a%20lightweight%20attention%20layer%20attached%20at%20the%20end%0Aof%20a%20VLM%2C%20to%20dynamically%20calibrate%20the%20contribution%20of%20each%20element%20in%20the%0Arational%20matrix%2C%20enabling%20targeted%20adjustments%20to%20the%20final%20cross-modal%0Ainteractions%20without%20incurring%20costly%20modifications%20to%20intermediate%20features.%0AExperiments%20in%20different%20settings%20%28i.e.%2C%20updating%2C%20or%20freezing%20pretrained%0Aencoders%20in%20adaptation%2C%20and%20test-time%20training%20that%20can%20only%20access%20the%0Aunlabeled%20test%20data%29%20show%20that%20RAda%20serves%20as%20a%20versatile%20fine-tuning%0Atechnique%2C%20improving%20the%20baseline%20with%20minimal%20code%20and%20performing%20comparably%0Aagainst%20current%20arts%20in%20most%20settings.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/khufia/RAda/tree/main%7D%7Bgithub.com/khufia/RAda%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Last%2520Attention%2520for%2520Your%2520Vision-Language%2520Model%26entry.906535625%3DLiang%2520Chen%2520and%2520Ghazi%2520Shazan%2520Ahmad%2520and%2520Tianjun%2520Yao%2520and%2520Lingqiao%2520Liu%2520and%2520Zhiqiang%2520Shen%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520achieve%2520remarkable%250Azero-shot%2520performance%252C%2520yet%2520their%2520downstream%2520potential%2520hinges%2520on%2520effective%250Afine-tuning.%2520Most%2520adaptation%2520methods%2520typically%2520focus%2520on%2520refining%2520representation%250Afrom%2520separate%2520modalities%2520%2528text%2520or%2520vision%2529%2520but%2520neglect%2520the%2520critical%2520role%2520of%250Atheir%2520fused%2520representations%2520in%2520the%2520decision-making%2520process%252C%2520%255Cemph%257B%255Cie%257D%2520rational%250Amatrix%2520that%2520drives%2520the%2520final%2520prediction.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520propose%2520a%2520simple%250Ayet%2520effective%2520%255Ctextbf%257BR%257Dational%2520%255Ctextbf%257BAda%257Dptaion%2520%2528%257BRAda%257D%2529%2520to%2520explicitly%250Aexploit%2520the%2520final%2520fused%2520representation%2520during%2520fine-tuning.%2520RAda%2520employs%2520a%250Alearned%2520mask%252C%2520obtained%2520from%2520a%2520lightweight%2520attention%2520layer%2520attached%2520at%2520the%2520end%250Aof%2520a%2520VLM%252C%2520to%2520dynamically%2520calibrate%2520the%2520contribution%2520of%2520each%2520element%2520in%2520the%250Arational%2520matrix%252C%2520enabling%2520targeted%2520adjustments%2520to%2520the%2520final%2520cross-modal%250Ainteractions%2520without%2520incurring%2520costly%2520modifications%2520to%2520intermediate%2520features.%250AExperiments%2520in%2520different%2520settings%2520%2528i.e.%252C%2520updating%252C%2520or%2520freezing%2520pretrained%250Aencoders%2520in%2520adaptation%252C%2520and%2520test-time%2520training%2520that%2520can%2520only%2520access%2520the%250Aunlabeled%2520test%2520data%2529%2520show%2520that%2520RAda%2520serves%2520as%2520a%2520versatile%2520fine-tuning%250Atechnique%252C%2520improving%2520the%2520baseline%2520with%2520minimal%2520code%2520and%2520performing%2520comparably%250Aagainst%2520current%2520arts%2520in%2520most%2520settings.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/khufia/RAda/tree/main%257D%257Bgithub.com/khufia/RAda%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Last%20Attention%20for%20Your%20Vision-Language%20Model&entry.906535625=Liang%20Chen%20and%20Ghazi%20Shazan%20Ahmad%20and%20Tianjun%20Yao%20and%20Lingqiao%20Liu%20and%20Zhiqiang%20Shen&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20achieve%20remarkable%0Azero-shot%20performance%2C%20yet%20their%20downstream%20potential%20hinges%20on%20effective%0Afine-tuning.%20Most%20adaptation%20methods%20typically%20focus%20on%20refining%20representation%0Afrom%20separate%20modalities%20%28text%20or%20vision%29%20but%20neglect%20the%20critical%20role%20of%0Atheir%20fused%20representations%20in%20the%20decision-making%20process%2C%20%5Cemph%7B%5Cie%7D%20rational%0Amatrix%20that%20drives%20the%20final%20prediction.%20To%20bridge%20the%20gap%2C%20we%20propose%20a%20simple%0Ayet%20effective%20%5Ctextbf%7BR%7Dational%20%5Ctextbf%7BAda%7Dptaion%20%28%7BRAda%7D%29%20to%20explicitly%0Aexploit%20the%20final%20fused%20representation%20during%20fine-tuning.%20RAda%20employs%20a%0Alearned%20mask%2C%20obtained%20from%20a%20lightweight%20attention%20layer%20attached%20at%20the%20end%0Aof%20a%20VLM%2C%20to%20dynamically%20calibrate%20the%20contribution%20of%20each%20element%20in%20the%0Arational%20matrix%2C%20enabling%20targeted%20adjustments%20to%20the%20final%20cross-modal%0Ainteractions%20without%20incurring%20costly%20modifications%20to%20intermediate%20features.%0AExperiments%20in%20different%20settings%20%28i.e.%2C%20updating%2C%20or%20freezing%20pretrained%0Aencoders%20in%20adaptation%2C%20and%20test-time%20training%20that%20can%20only%20access%20the%0Aunlabeled%20test%20data%29%20show%20that%20RAda%20serves%20as%20a%20versatile%20fine-tuning%0Atechnique%2C%20improving%20the%20baseline%20with%20minimal%20code%20and%20performing%20comparably%0Aagainst%20current%20arts%20in%20most%20settings.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/khufia/RAda/tree/main%7D%7Bgithub.com/khufia/RAda%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15480v1&entry.124074799=Read"},
{"title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs", "author": "Bo-Cheng Chiu and Jen-Jee Chen and Yu-Chee Tseng and Feng-Chi Chen", "abstract": "  Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with LLM-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.\n", "link": "http://arxiv.org/abs/2506.11558v3", "date": "2025-07-21", "relevancy": 2.8847, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%0A%20%20with%20Video%20LLMs&body=Title%3A%20DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%0A%20%20with%20Video%20LLMs%0AAuthor%3A%20Bo-Cheng%20Chiu%20and%20Jen-Jee%20Chen%20and%20Yu-Chee%20Tseng%20and%20Feng-Chi%20Chen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20extended%20to%20the%20video%20domain%2C%0Aenabling%20sophisticated%20video-language%20understanding.%20However%2C%20existing%20Video%0ALLMs%20often%20exhibit%20limitations%20in%20fine-grained%20temporal%20reasoning%2C%20restricting%0Atheir%20ability%20to%20precisely%20attribute%20responses%20to%20specific%20video%20moments%2C%0Aespecially%20under%20constrained%20supervision.%20We%20introduce%20DaMO%2C%20a%20data-efficient%0AVideo%20LLM%20explicitly%20designed%20for%20accurate%20temporal%20reasoning%20and%20multimodal%0Aunderstanding.%20At%20its%20core%2C%20the%20proposed%20Temporal-aware%20Fuseformer%20employs%20a%0Ahierarchical%20dual-stream%20architecture%20that%20progressively%20captures%20temporal%0Adynamics%20within%20each%20modality%20and%20effectively%20fuses%20complementary%20visual%20and%0Aaudio%20information.%20To%20further%20enhance%20computational%20efficiency%2C%20DaMO%20integrates%0Aa%20global%20residual%20that%20reduces%20spatial%20redundancy%20while%20preserving%20essential%0Asemantic%20details.%20We%20train%20DaMO%20via%20a%20structured%20four-stage%20progressive%0Atraining%20paradigm%2C%20incrementally%20equipping%20the%20model%20with%20multimodal%20alignment%2C%0Asemantic%20grounding%2C%20and%20temporal%20reasoning%20capabilities.%20This%20work%20also%0Acontributes%20multiple%20datasets%20augmented%20from%20existing%20ones%20with%20LLM-generated%0Atemporally%20grounded%20QA%20pairs%20for%20tasks%20requiring%20temporal%20supervision.%0AComprehensive%20experiments%20on%20temporal%20grounding%20and%20video%20QA%20benchmarks%0Ademonstrate%20that%20DaMO%20consistently%20surpasses%20prior%20methods%2C%20particularly%20in%0Atasks%20demanding%20precise%20temporal%20alignment%20and%20reasoning.%20Our%20work%20establishes%0Aa%20promising%20direction%20for%20data-efficient%20video-language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11558v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDaMO%253A%2520A%2520Data-Efficient%2520Multimodal%2520Orchestrator%2520for%2520Temporal%2520Reasoning%250A%2520%2520with%2520Video%2520LLMs%26entry.906535625%3DBo-Cheng%2520Chiu%2520and%2520Jen-Jee%2520Chen%2520and%2520Yu-Chee%2520Tseng%2520and%2520Feng-Chi%2520Chen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520been%2520extended%2520to%2520the%2520video%2520domain%252C%250Aenabling%2520sophisticated%2520video-language%2520understanding.%2520However%252C%2520existing%2520Video%250ALLMs%2520often%2520exhibit%2520limitations%2520in%2520fine-grained%2520temporal%2520reasoning%252C%2520restricting%250Atheir%2520ability%2520to%2520precisely%2520attribute%2520responses%2520to%2520specific%2520video%2520moments%252C%250Aespecially%2520under%2520constrained%2520supervision.%2520We%2520introduce%2520DaMO%252C%2520a%2520data-efficient%250AVideo%2520LLM%2520explicitly%2520designed%2520for%2520accurate%2520temporal%2520reasoning%2520and%2520multimodal%250Aunderstanding.%2520At%2520its%2520core%252C%2520the%2520proposed%2520Temporal-aware%2520Fuseformer%2520employs%2520a%250Ahierarchical%2520dual-stream%2520architecture%2520that%2520progressively%2520captures%2520temporal%250Adynamics%2520within%2520each%2520modality%2520and%2520effectively%2520fuses%2520complementary%2520visual%2520and%250Aaudio%2520information.%2520To%2520further%2520enhance%2520computational%2520efficiency%252C%2520DaMO%2520integrates%250Aa%2520global%2520residual%2520that%2520reduces%2520spatial%2520redundancy%2520while%2520preserving%2520essential%250Asemantic%2520details.%2520We%2520train%2520DaMO%2520via%2520a%2520structured%2520four-stage%2520progressive%250Atraining%2520paradigm%252C%2520incrementally%2520equipping%2520the%2520model%2520with%2520multimodal%2520alignment%252C%250Asemantic%2520grounding%252C%2520and%2520temporal%2520reasoning%2520capabilities.%2520This%2520work%2520also%250Acontributes%2520multiple%2520datasets%2520augmented%2520from%2520existing%2520ones%2520with%2520LLM-generated%250Atemporally%2520grounded%2520QA%2520pairs%2520for%2520tasks%2520requiring%2520temporal%2520supervision.%250AComprehensive%2520experiments%2520on%2520temporal%2520grounding%2520and%2520video%2520QA%2520benchmarks%250Ademonstrate%2520that%2520DaMO%2520consistently%2520surpasses%2520prior%2520methods%252C%2520particularly%2520in%250Atasks%2520demanding%2520precise%2520temporal%2520alignment%2520and%2520reasoning.%2520Our%2520work%2520establishes%250Aa%2520promising%2520direction%2520for%2520data-efficient%2520video-language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11558v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%0A%20%20with%20Video%20LLMs&entry.906535625=Bo-Cheng%20Chiu%20and%20Jen-Jee%20Chen%20and%20Yu-Chee%20Tseng%20and%20Feng-Chi%20Chen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20extended%20to%20the%20video%20domain%2C%0Aenabling%20sophisticated%20video-language%20understanding.%20However%2C%20existing%20Video%0ALLMs%20often%20exhibit%20limitations%20in%20fine-grained%20temporal%20reasoning%2C%20restricting%0Atheir%20ability%20to%20precisely%20attribute%20responses%20to%20specific%20video%20moments%2C%0Aespecially%20under%20constrained%20supervision.%20We%20introduce%20DaMO%2C%20a%20data-efficient%0AVideo%20LLM%20explicitly%20designed%20for%20accurate%20temporal%20reasoning%20and%20multimodal%0Aunderstanding.%20At%20its%20core%2C%20the%20proposed%20Temporal-aware%20Fuseformer%20employs%20a%0Ahierarchical%20dual-stream%20architecture%20that%20progressively%20captures%20temporal%0Adynamics%20within%20each%20modality%20and%20effectively%20fuses%20complementary%20visual%20and%0Aaudio%20information.%20To%20further%20enhance%20computational%20efficiency%2C%20DaMO%20integrates%0Aa%20global%20residual%20that%20reduces%20spatial%20redundancy%20while%20preserving%20essential%0Asemantic%20details.%20We%20train%20DaMO%20via%20a%20structured%20four-stage%20progressive%0Atraining%20paradigm%2C%20incrementally%20equipping%20the%20model%20with%20multimodal%20alignment%2C%0Asemantic%20grounding%2C%20and%20temporal%20reasoning%20capabilities.%20This%20work%20also%0Acontributes%20multiple%20datasets%20augmented%20from%20existing%20ones%20with%20LLM-generated%0Atemporally%20grounded%20QA%20pairs%20for%20tasks%20requiring%20temporal%20supervision.%0AComprehensive%20experiments%20on%20temporal%20grounding%20and%20video%20QA%20benchmarks%0Ademonstrate%20that%20DaMO%20consistently%20surpasses%20prior%20methods%2C%20particularly%20in%0Atasks%20demanding%20precise%20temporal%20alignment%20and%20reasoning.%20Our%20work%20establishes%0Aa%20promising%20direction%20for%20data-efficient%20video-language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11558v3&entry.124074799=Read"},
{"title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud\n  Geometry Compression", "author": "Wenjie Huang and Qi Yang and Shuting Xia and He Huang and Zhu Li and Yiling Xu", "abstract": "  Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.\n", "link": "http://arxiv.org/abs/2507.15686v1", "date": "2025-07-21", "relevancy": 2.8444, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6394}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.546}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LINR-PCGC%3A%20Lossless%20Implicit%20Neural%20Representations%20for%20Point%20Cloud%0A%20%20Geometry%20Compression&body=Title%3A%20LINR-PCGC%3A%20Lossless%20Implicit%20Neural%20Representations%20for%20Point%20Cloud%0A%20%20Geometry%20Compression%0AAuthor%3A%20Wenjie%20Huang%20and%20Qi%20Yang%20and%20Shuting%20Xia%20and%20He%20Huang%20and%20Zhu%20Li%20and%20Yiling%20Xu%0AAbstract%3A%20%20%20Existing%20AI-based%20point%20cloud%20compression%20methods%20struggle%20with%20dependence%20on%0Aspecific%20training%20data%20distributions%2C%20which%20limits%20their%20real-world%20deployment.%0AImplicit%20Neural%20Representation%20%28INR%29%20methods%20solve%20the%20above%20problem%20by%0Aencoding%20overfitted%20network%20parameters%20to%20the%20bitstream%2C%20resulting%20in%20more%0Adistribution-agnostic%20results.%20However%2C%20due%20to%20the%20limitation%20of%20encoding%20time%0Aand%20decoder%20size%2C%20current%20INR%20based%20methods%20only%20consider%20lossy%20geometry%0Acompression.%20In%20this%20paper%2C%20we%20propose%20the%20first%20INR%20based%20lossless%20point%20cloud%0Ageometry%20compression%20method%20called%20Lossless%20Implicit%20Neural%20Representations%20for%0APoint%20Cloud%20Geometry%20Compression%20%28LINR-PCGC%29.%20To%20accelerate%20encoding%20speed%2C%20we%0Adesign%20a%20group%20of%20point%20clouds%20level%20coding%20framework%20with%20an%20effective%20network%0Ainitialization%20strategy%2C%20which%20can%20reduce%20around%2060%25%20encoding%20time.%20A%0Alightweight%20coding%20network%20based%20on%20multiscale%20SparseConv%2C%20consisting%20of%20scale%0Acontext%20extraction%2C%20child%20node%20prediction%2C%20and%20model%20compression%20modules%2C%20is%0Aproposed%20to%20realize%20fast%20inference%20and%20compact%20decoder%20size.%20Experimental%0Aresults%20show%20that%20our%20method%20consistently%20outperforms%20traditional%20and%20AI-based%0Amethods%3A%20for%20example%2C%20with%20the%20convergence%20time%20in%20the%20MVUB%20dataset%2C%20our%20method%0Areduces%20the%20bitstream%20by%20approximately%2021.21%25%20compared%20to%20G-PCC%20TMC13v23%20and%0A21.95%25%20compared%20to%20SparsePCGC.%20Our%20project%20can%20be%20seen%20on%0Ahttps%3A//huangwenjie2023.github.io/LINR-PCGC/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLINR-PCGC%253A%2520Lossless%2520Implicit%2520Neural%2520Representations%2520for%2520Point%2520Cloud%250A%2520%2520Geometry%2520Compression%26entry.906535625%3DWenjie%2520Huang%2520and%2520Qi%2520Yang%2520and%2520Shuting%2520Xia%2520and%2520He%2520Huang%2520and%2520Zhu%2520Li%2520and%2520Yiling%2520Xu%26entry.1292438233%3D%2520%2520Existing%2520AI-based%2520point%2520cloud%2520compression%2520methods%2520struggle%2520with%2520dependence%2520on%250Aspecific%2520training%2520data%2520distributions%252C%2520which%2520limits%2520their%2520real-world%2520deployment.%250AImplicit%2520Neural%2520Representation%2520%2528INR%2529%2520methods%2520solve%2520the%2520above%2520problem%2520by%250Aencoding%2520overfitted%2520network%2520parameters%2520to%2520the%2520bitstream%252C%2520resulting%2520in%2520more%250Adistribution-agnostic%2520results.%2520However%252C%2520due%2520to%2520the%2520limitation%2520of%2520encoding%2520time%250Aand%2520decoder%2520size%252C%2520current%2520INR%2520based%2520methods%2520only%2520consider%2520lossy%2520geometry%250Acompression.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520INR%2520based%2520lossless%2520point%2520cloud%250Ageometry%2520compression%2520method%2520called%2520Lossless%2520Implicit%2520Neural%2520Representations%2520for%250APoint%2520Cloud%2520Geometry%2520Compression%2520%2528LINR-PCGC%2529.%2520To%2520accelerate%2520encoding%2520speed%252C%2520we%250Adesign%2520a%2520group%2520of%2520point%2520clouds%2520level%2520coding%2520framework%2520with%2520an%2520effective%2520network%250Ainitialization%2520strategy%252C%2520which%2520can%2520reduce%2520around%252060%2525%2520encoding%2520time.%2520A%250Alightweight%2520coding%2520network%2520based%2520on%2520multiscale%2520SparseConv%252C%2520consisting%2520of%2520scale%250Acontext%2520extraction%252C%2520child%2520node%2520prediction%252C%2520and%2520model%2520compression%2520modules%252C%2520is%250Aproposed%2520to%2520realize%2520fast%2520inference%2520and%2520compact%2520decoder%2520size.%2520Experimental%250Aresults%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520traditional%2520and%2520AI-based%250Amethods%253A%2520for%2520example%252C%2520with%2520the%2520convergence%2520time%2520in%2520the%2520MVUB%2520dataset%252C%2520our%2520method%250Areduces%2520the%2520bitstream%2520by%2520approximately%252021.21%2525%2520compared%2520to%2520G-PCC%2520TMC13v23%2520and%250A21.95%2525%2520compared%2520to%2520SparsePCGC.%2520Our%2520project%2520can%2520be%2520seen%2520on%250Ahttps%253A//huangwenjie2023.github.io/LINR-PCGC/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LINR-PCGC%3A%20Lossless%20Implicit%20Neural%20Representations%20for%20Point%20Cloud%0A%20%20Geometry%20Compression&entry.906535625=Wenjie%20Huang%20and%20Qi%20Yang%20and%20Shuting%20Xia%20and%20He%20Huang%20and%20Zhu%20Li%20and%20Yiling%20Xu&entry.1292438233=%20%20Existing%20AI-based%20point%20cloud%20compression%20methods%20struggle%20with%20dependence%20on%0Aspecific%20training%20data%20distributions%2C%20which%20limits%20their%20real-world%20deployment.%0AImplicit%20Neural%20Representation%20%28INR%29%20methods%20solve%20the%20above%20problem%20by%0Aencoding%20overfitted%20network%20parameters%20to%20the%20bitstream%2C%20resulting%20in%20more%0Adistribution-agnostic%20results.%20However%2C%20due%20to%20the%20limitation%20of%20encoding%20time%0Aand%20decoder%20size%2C%20current%20INR%20based%20methods%20only%20consider%20lossy%20geometry%0Acompression.%20In%20this%20paper%2C%20we%20propose%20the%20first%20INR%20based%20lossless%20point%20cloud%0Ageometry%20compression%20method%20called%20Lossless%20Implicit%20Neural%20Representations%20for%0APoint%20Cloud%20Geometry%20Compression%20%28LINR-PCGC%29.%20To%20accelerate%20encoding%20speed%2C%20we%0Adesign%20a%20group%20of%20point%20clouds%20level%20coding%20framework%20with%20an%20effective%20network%0Ainitialization%20strategy%2C%20which%20can%20reduce%20around%2060%25%20encoding%20time.%20A%0Alightweight%20coding%20network%20based%20on%20multiscale%20SparseConv%2C%20consisting%20of%20scale%0Acontext%20extraction%2C%20child%20node%20prediction%2C%20and%20model%20compression%20modules%2C%20is%0Aproposed%20to%20realize%20fast%20inference%20and%20compact%20decoder%20size.%20Experimental%0Aresults%20show%20that%20our%20method%20consistently%20outperforms%20traditional%20and%20AI-based%0Amethods%3A%20for%20example%2C%20with%20the%20convergence%20time%20in%20the%20MVUB%20dataset%2C%20our%20method%0Areduces%20the%20bitstream%20by%20approximately%2021.21%25%20compared%20to%20G-PCC%20TMC13v23%20and%0A21.95%25%20compared%20to%20SparsePCGC.%20Our%20project%20can%20be%20seen%20on%0Ahttps%3A//huangwenjie2023.github.io/LINR-PCGC/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15686v1&entry.124074799=Read"},
{"title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation\n  Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "author": "Danhui Chen and Ziquan Liu and Chuxi Yang and Dan Wang and Yan Yan and Yi Xu and Xiangyang Ji", "abstract": "  Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.\n", "link": "http://arxiv.org/abs/2507.15803v1", "date": "2025-07-21", "relevancy": 2.8119, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConformalSAM%3A%20Unlocking%20the%20Potential%20of%20Foundational%20Segmentation%0A%20%20Models%20in%20Semi-Supervised%20Semantic%20Segmentation%20with%20Conformal%20Prediction&body=Title%3A%20ConformalSAM%3A%20Unlocking%20the%20Potential%20of%20Foundational%20Segmentation%0A%20%20Models%20in%20Semi-Supervised%20Semantic%20Segmentation%20with%20Conformal%20Prediction%0AAuthor%3A%20Danhui%20Chen%20and%20Ziquan%20Liu%20and%20Chuxi%20Yang%20and%20Dan%20Wang%20and%20Yan%20Yan%20and%20Yi%20Xu%20and%20Xiangyang%20Ji%0AAbstract%3A%20%20%20Pixel-level%20vision%20tasks%2C%20such%20as%20semantic%20segmentation%2C%20require%20extensive%0Aand%20high-quality%20annotated%20data%2C%20which%20is%20costly%20to%20obtain.%20Semi-supervised%0Asemantic%20segmentation%20%28SSSS%29%20has%20emerged%20as%20a%20solution%20to%20alleviate%20the%0Alabeling%20burden%20by%20leveraging%20both%20labeled%20and%20unlabeled%20data%20through%0Aself-training%20techniques.%20Meanwhile%2C%20the%20advent%20of%20foundational%20segmentation%0Amodels%20pre-trained%20on%20massive%20data%2C%20has%20shown%20the%20potential%20to%20generalize%0Aacross%20domains%20effectively.%20This%20work%20explores%20whether%20a%20foundational%0Asegmentation%20model%20can%20address%20label%20scarcity%20in%20the%20pixel-level%20vision%20task%20as%0Aan%20annotator%20for%20unlabeled%20images.%20Specifically%2C%20we%20investigate%20the%20efficacy%20of%0Ausing%20SEEM%2C%20a%20Segment%20Anything%20Model%20%28SAM%29%20variant%20fine-tuned%20for%20textual%0Ainput%2C%20to%20generate%20predictive%20masks%20for%20unlabeled%20data.%20To%20address%20the%0Ashortcomings%20of%20using%20SEEM-generated%20masks%20as%20supervision%2C%20we%20propose%0AConformalSAM%2C%20a%20novel%20SSSS%20framework%20which%20first%20calibrates%20the%20foundation%0Amodel%20using%20the%20target%20domain%27s%20labeled%20data%20and%20then%20filters%20out%20unreliable%0Apixel%20labels%20of%20unlabeled%20data%20so%20that%20only%20high-confidence%20labels%20are%20used%20as%0Asupervision.%20By%20leveraging%20conformal%20prediction%20%28CP%29%20to%20adapt%20foundation%20models%0Ato%20target%20data%20through%20uncertainty%20calibration%2C%20ConformalSAM%20exploits%20the%0Astrong%20capability%20of%20the%20foundational%20segmentation%20model%20reliably%20which%0Abenefits%20the%20early-stage%20learning%2C%20while%20a%20subsequent%20self-reliance%20training%0Astrategy%20mitigates%20overfitting%20to%20SEEM-generated%20masks%20in%20the%20later%20training%0Astage.%20Our%20experiment%20demonstrates%20that%2C%20on%20three%20standard%20benchmarks%20of%20SSSS%2C%0AConformalSAM%20achieves%20superior%20performance%20compared%20to%20recent%20SSSS%20methods%20and%0Ahelps%20boost%20the%20performance%20of%20those%20methods%20as%20a%20plug-in.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformalSAM%253A%2520Unlocking%2520the%2520Potential%2520of%2520Foundational%2520Segmentation%250A%2520%2520Models%2520in%2520Semi-Supervised%2520Semantic%2520Segmentation%2520with%2520Conformal%2520Prediction%26entry.906535625%3DDanhui%2520Chen%2520and%2520Ziquan%2520Liu%2520and%2520Chuxi%2520Yang%2520and%2520Dan%2520Wang%2520and%2520Yan%2520Yan%2520and%2520Yi%2520Xu%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3D%2520%2520Pixel-level%2520vision%2520tasks%252C%2520such%2520as%2520semantic%2520segmentation%252C%2520require%2520extensive%250Aand%2520high-quality%2520annotated%2520data%252C%2520which%2520is%2520costly%2520to%2520obtain.%2520Semi-supervised%250Asemantic%2520segmentation%2520%2528SSSS%2529%2520has%2520emerged%2520as%2520a%2520solution%2520to%2520alleviate%2520the%250Alabeling%2520burden%2520by%2520leveraging%2520both%2520labeled%2520and%2520unlabeled%2520data%2520through%250Aself-training%2520techniques.%2520Meanwhile%252C%2520the%2520advent%2520of%2520foundational%2520segmentation%250Amodels%2520pre-trained%2520on%2520massive%2520data%252C%2520has%2520shown%2520the%2520potential%2520to%2520generalize%250Aacross%2520domains%2520effectively.%2520This%2520work%2520explores%2520whether%2520a%2520foundational%250Asegmentation%2520model%2520can%2520address%2520label%2520scarcity%2520in%2520the%2520pixel-level%2520vision%2520task%2520as%250Aan%2520annotator%2520for%2520unlabeled%2520images.%2520Specifically%252C%2520we%2520investigate%2520the%2520efficacy%2520of%250Ausing%2520SEEM%252C%2520a%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520variant%2520fine-tuned%2520for%2520textual%250Ainput%252C%2520to%2520generate%2520predictive%2520masks%2520for%2520unlabeled%2520data.%2520To%2520address%2520the%250Ashortcomings%2520of%2520using%2520SEEM-generated%2520masks%2520as%2520supervision%252C%2520we%2520propose%250AConformalSAM%252C%2520a%2520novel%2520SSSS%2520framework%2520which%2520first%2520calibrates%2520the%2520foundation%250Amodel%2520using%2520the%2520target%2520domain%2527s%2520labeled%2520data%2520and%2520then%2520filters%2520out%2520unreliable%250Apixel%2520labels%2520of%2520unlabeled%2520data%2520so%2520that%2520only%2520high-confidence%2520labels%2520are%2520used%2520as%250Asupervision.%2520By%2520leveraging%2520conformal%2520prediction%2520%2528CP%2529%2520to%2520adapt%2520foundation%2520models%250Ato%2520target%2520data%2520through%2520uncertainty%2520calibration%252C%2520ConformalSAM%2520exploits%2520the%250Astrong%2520capability%2520of%2520the%2520foundational%2520segmentation%2520model%2520reliably%2520which%250Abenefits%2520the%2520early-stage%2520learning%252C%2520while%2520a%2520subsequent%2520self-reliance%2520training%250Astrategy%2520mitigates%2520overfitting%2520to%2520SEEM-generated%2520masks%2520in%2520the%2520later%2520training%250Astage.%2520Our%2520experiment%2520demonstrates%2520that%252C%2520on%2520three%2520standard%2520benchmarks%2520of%2520SSSS%252C%250AConformalSAM%2520achieves%2520superior%2520performance%2520compared%2520to%2520recent%2520SSSS%2520methods%2520and%250Ahelps%2520boost%2520the%2520performance%2520of%2520those%2520methods%2520as%2520a%2520plug-in.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConformalSAM%3A%20Unlocking%20the%20Potential%20of%20Foundational%20Segmentation%0A%20%20Models%20in%20Semi-Supervised%20Semantic%20Segmentation%20with%20Conformal%20Prediction&entry.906535625=Danhui%20Chen%20and%20Ziquan%20Liu%20and%20Chuxi%20Yang%20and%20Dan%20Wang%20and%20Yan%20Yan%20and%20Yi%20Xu%20and%20Xiangyang%20Ji&entry.1292438233=%20%20Pixel-level%20vision%20tasks%2C%20such%20as%20semantic%20segmentation%2C%20require%20extensive%0Aand%20high-quality%20annotated%20data%2C%20which%20is%20costly%20to%20obtain.%20Semi-supervised%0Asemantic%20segmentation%20%28SSSS%29%20has%20emerged%20as%20a%20solution%20to%20alleviate%20the%0Alabeling%20burden%20by%20leveraging%20both%20labeled%20and%20unlabeled%20data%20through%0Aself-training%20techniques.%20Meanwhile%2C%20the%20advent%20of%20foundational%20segmentation%0Amodels%20pre-trained%20on%20massive%20data%2C%20has%20shown%20the%20potential%20to%20generalize%0Aacross%20domains%20effectively.%20This%20work%20explores%20whether%20a%20foundational%0Asegmentation%20model%20can%20address%20label%20scarcity%20in%20the%20pixel-level%20vision%20task%20as%0Aan%20annotator%20for%20unlabeled%20images.%20Specifically%2C%20we%20investigate%20the%20efficacy%20of%0Ausing%20SEEM%2C%20a%20Segment%20Anything%20Model%20%28SAM%29%20variant%20fine-tuned%20for%20textual%0Ainput%2C%20to%20generate%20predictive%20masks%20for%20unlabeled%20data.%20To%20address%20the%0Ashortcomings%20of%20using%20SEEM-generated%20masks%20as%20supervision%2C%20we%20propose%0AConformalSAM%2C%20a%20novel%20SSSS%20framework%20which%20first%20calibrates%20the%20foundation%0Amodel%20using%20the%20target%20domain%27s%20labeled%20data%20and%20then%20filters%20out%20unreliable%0Apixel%20labels%20of%20unlabeled%20data%20so%20that%20only%20high-confidence%20labels%20are%20used%20as%0Asupervision.%20By%20leveraging%20conformal%20prediction%20%28CP%29%20to%20adapt%20foundation%20models%0Ato%20target%20data%20through%20uncertainty%20calibration%2C%20ConformalSAM%20exploits%20the%0Astrong%20capability%20of%20the%20foundational%20segmentation%20model%20reliably%20which%0Abenefits%20the%20early-stage%20learning%2C%20while%20a%20subsequent%20self-reliance%20training%0Astrategy%20mitigates%20overfitting%20to%20SEEM-generated%20masks%20in%20the%20later%20training%0Astage.%20Our%20experiment%20demonstrates%20that%2C%20on%20three%20standard%20benchmarks%20of%20SSSS%2C%0AConformalSAM%20achieves%20superior%20performance%20compared%20to%20recent%20SSSS%20methods%20and%0Ahelps%20boost%20the%20performance%20of%20those%20methods%20as%20a%20plug-in.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15803v1&entry.124074799=Read"},
{"title": "Visual-Language Model Knowledge Distillation Method for Image Quality\n  Assessment", "author": "Yongkang Hou and Jiarun Song", "abstract": "  Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.\n", "link": "http://arxiv.org/abs/2507.15680v1", "date": "2025-07-21", "relevancy": 2.7581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment&body=Title%3A%20Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment%0AAuthor%3A%20Yongkang%20Hou%20and%20Jiarun%20Song%0AAbstract%3A%20%20%20Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20core%20task%20in%20computer%20vision.%20Multimodal%0Amethods%20based%20on%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aexceptional%20generalization%20capabilities%20in%20IQA%20tasks.%20To%20address%20the%20issues%20of%0Aexcessive%20parameter%20burden%20and%20insufficient%20ability%20to%20identify%20local%20distorted%0Afeatures%20in%20CLIP%20for%20IQA%2C%20this%20study%20proposes%20a%20visual-language%20model%20knowledge%0Adistillation%20method%20aimed%20at%20guiding%20the%20training%20of%20models%20with%20architectural%0Aadvantages%20using%20CLIP%27s%20IQA%20knowledge.%20First%2C%20quality-graded%20prompt%20templates%0Awere%20designed%20to%20guide%20CLIP%20to%20output%20quality%20scores.%20Then%2C%20CLIP%20is%20fine-tuned%0Ato%20enhance%20its%20capabilities%20in%20IQA%20tasks.%20Finally%2C%20a%20modality-adaptive%0Aknowledge%20distillation%20strategy%20is%20proposed%20to%20achieve%20guidance%20from%20the%20CLIP%0Ateacher%20model%20to%20the%20student%20model.%20Our%20experiments%20were%20conducted%20on%20multiple%0AIQA%20datasets%2C%20and%20the%20results%20show%20that%20the%20proposed%20method%20significantly%0Areduces%20model%20complexity%20while%20outperforming%20existing%20IQA%20methods%2C%0Ademonstrating%20strong%20potential%20for%20practical%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Language%2520Model%2520Knowledge%2520Distillation%2520Method%2520for%2520Image%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DYongkang%2520Hou%2520and%2520Jiarun%2520Song%26entry.1292438233%3D%2520%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520is%2520a%2520core%2520task%2520in%2520computer%2520vision.%2520Multimodal%250Amethods%2520based%2520on%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%250Aexceptional%2520generalization%2520capabilities%2520in%2520IQA%2520tasks.%2520To%2520address%2520the%2520issues%2520of%250Aexcessive%2520parameter%2520burden%2520and%2520insufficient%2520ability%2520to%2520identify%2520local%2520distorted%250Afeatures%2520in%2520CLIP%2520for%2520IQA%252C%2520this%2520study%2520proposes%2520a%2520visual-language%2520model%2520knowledge%250Adistillation%2520method%2520aimed%2520at%2520guiding%2520the%2520training%2520of%2520models%2520with%2520architectural%250Aadvantages%2520using%2520CLIP%2527s%2520IQA%2520knowledge.%2520First%252C%2520quality-graded%2520prompt%2520templates%250Awere%2520designed%2520to%2520guide%2520CLIP%2520to%2520output%2520quality%2520scores.%2520Then%252C%2520CLIP%2520is%2520fine-tuned%250Ato%2520enhance%2520its%2520capabilities%2520in%2520IQA%2520tasks.%2520Finally%252C%2520a%2520modality-adaptive%250Aknowledge%2520distillation%2520strategy%2520is%2520proposed%2520to%2520achieve%2520guidance%2520from%2520the%2520CLIP%250Ateacher%2520model%2520to%2520the%2520student%2520model.%2520Our%2520experiments%2520were%2520conducted%2520on%2520multiple%250AIQA%2520datasets%252C%2520and%2520the%2520results%2520show%2520that%2520the%2520proposed%2520method%2520significantly%250Areduces%2520model%2520complexity%2520while%2520outperforming%2520existing%2520IQA%2520methods%252C%250Ademonstrating%2520strong%2520potential%2520for%2520practical%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment&entry.906535625=Yongkang%20Hou%20and%20Jiarun%20Song&entry.1292438233=%20%20Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20core%20task%20in%20computer%20vision.%20Multimodal%0Amethods%20based%20on%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aexceptional%20generalization%20capabilities%20in%20IQA%20tasks.%20To%20address%20the%20issues%20of%0Aexcessive%20parameter%20burden%20and%20insufficient%20ability%20to%20identify%20local%20distorted%0Afeatures%20in%20CLIP%20for%20IQA%2C%20this%20study%20proposes%20a%20visual-language%20model%20knowledge%0Adistillation%20method%20aimed%20at%20guiding%20the%20training%20of%20models%20with%20architectural%0Aadvantages%20using%20CLIP%27s%20IQA%20knowledge.%20First%2C%20quality-graded%20prompt%20templates%0Awere%20designed%20to%20guide%20CLIP%20to%20output%20quality%20scores.%20Then%2C%20CLIP%20is%20fine-tuned%0Ato%20enhance%20its%20capabilities%20in%20IQA%20tasks.%20Finally%2C%20a%20modality-adaptive%0Aknowledge%20distillation%20strategy%20is%20proposed%20to%20achieve%20guidance%20from%20the%20CLIP%0Ateacher%20model%20to%20the%20student%20model.%20Our%20experiments%20were%20conducted%20on%20multiple%0AIQA%20datasets%2C%20and%20the%20results%20show%20that%20the%20proposed%20method%20significantly%0Areduces%20model%20complexity%20while%20outperforming%20existing%20IQA%20methods%2C%0Ademonstrating%20strong%20potential%20for%20practical%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15680v1&entry.124074799=Read"},
{"title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "author": "Fei Tang and Zhangxuan Gu and Zhengxi Lu and Xuyang Liu and Shuheng Shen and Changhua Meng and Wen Wang and Wenqi Zhang and Yongliang Shen and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.\n", "link": "http://arxiv.org/abs/2507.15846v1", "date": "2025-07-21", "relevancy": 2.7009, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&body=Title%3A%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding%0AAuthor%3A%20Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-G%2524%255E2%2524%253A%2520Gaussian%2520Reward%2520Modeling%2520for%2520GUI%2520Grounding%26entry.906535625%3DFei%2520Tang%2520and%2520Zhangxuan%2520Gu%2520and%2520Zhengxi%2520Lu%2520and%2520Xuyang%2520Liu%2520and%2520Shuheng%2520Shen%2520and%2520Changhua%2520Meng%2520and%2520Wen%2520Wang%2520and%2520Wenqi%2520Zhang%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520grounding%2520maps%2520natural%2520language%2520instructions%250Ato%2520precise%2520interface%2520locations%2520for%2520autonomous%2520interaction.%2520Current%250Areinforcement%2520learning%2520approaches%2520use%2520binary%2520rewards%2520that%2520treat%2520elements%2520as%250Ahit-or-miss%2520targets%252C%2520creating%2520sparse%2520signals%2520that%2520ignore%2520the%2520continuous%2520nature%250Aof%2520spatial%2520interactions.%2520Motivated%2520by%2520human%2520clicking%2520behavior%2520that%2520naturally%250Aforms%2520Gaussian%2520distributions%2520centered%2520on%2520target%2520elements%252C%2520we%2520introduce%2520GUI%250AGaussian%2520Grounding%2520Rewards%2520%2528GUI-G%2524%255E2%2524%2529%252C%2520a%2520principled%2520reward%2520framework%2520that%250Amodels%2520GUI%2520elements%2520as%2520continuous%2520Gaussian%2520distributions%2520across%2520the%2520interface%250Aplane.%2520GUI-G%2524%255E2%2524%2520incorporates%2520two%2520synergistic%2520mechanisms%253A%2520Gaussian%2520point%250Arewards%2520model%2520precise%2520localization%2520through%2520exponentially%2520decaying%2520distributions%250Acentered%2520on%2520element%2520centroids%252C%2520while%2520coverage%2520rewards%2520assess%2520spatial%2520alignment%250Aby%2520measuring%2520the%2520overlap%2520between%2520predicted%2520Gaussian%2520distributions%2520and%2520target%250Aregions.%2520To%2520handle%2520diverse%2520element%2520scales%252C%2520we%2520develop%2520an%2520adaptive%2520variance%250Amechanism%2520that%2520calibrates%2520reward%2520distributions%2520based%2520on%2520element%2520dimensions.%250AThis%2520framework%2520transforms%2520GUI%2520grounding%2520from%2520sparse%2520binary%2520classification%2520to%250Adense%2520continuous%2520optimization%252C%2520where%2520Gaussian%2520distributions%2520generate%2520rich%250Agradient%2520signals%2520that%2520guide%2520models%2520toward%2520optimal%2520interaction%2520positions.%250AExtensive%2520experiments%2520across%2520ScreenSpot%252C%2520ScreenSpot-v2%252C%2520and%2520ScreenSpot-Pro%250Abenchmarks%2520demonstrate%2520that%2520GUI-G%2524%255E2%2524%252C%2520substantially%2520outperforms%250Astate-of-the-art%2520method%2520UI-TARS-72B%252C%2520with%2520the%2520most%2520significant%2520improvement%2520of%250A24.7%2525%2520on%2520ScreenSpot-Pro.%2520Our%2520analysis%2520reveals%2520that%2520continuous%2520modeling%2520provides%250Asuperior%2520robustness%2520to%2520interface%2520variations%2520and%2520enhanced%2520generalization%2520to%250Aunseen%2520layouts%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520spatial%2520reasoning%2520in%2520GUI%250Ainteraction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&entry.906535625=Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15846v1&entry.124074799=Read"},
{"title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "author": "Lingyu Li and Yang Yao and Yixu Wang and Chubo Li and Yan Teng and Yingchun Wang", "abstract": "  As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.\n", "link": "http://arxiv.org/abs/2507.15851v1", "date": "2025-07-21", "relevancy": 2.6711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Other%20Mind%3A%20How%20Language%20Models%20Exhibit%20Human%20Temporal%20Cognition&body=Title%3A%20The%20Other%20Mind%3A%20How%20Language%20Models%20Exhibit%20Human%20Temporal%20Cognition%0AAuthor%3A%20Lingyu%20Li%20and%20Yang%20Yao%20and%20Yixu%20Wang%20and%20Chubo%20Li%20and%20Yan%20Teng%20and%20Yingchun%20Wang%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20advance%2C%20they%20exhibit%20certain%0Acognitive%20patterns%20similar%20to%20those%20of%20humans%20that%20are%20not%20directly%20specified%0Ain%20training%20data.%20This%20study%20investigates%20this%20phenomenon%20by%20focusing%20on%0Atemporal%20cognition%20in%20LLMs.%20Leveraging%20the%20similarity%20judgment%20task%2C%20we%20find%0Athat%20larger%20models%20spontaneously%20establish%20a%20subjective%20temporal%20reference%0Apoint%20and%20adhere%20to%20the%20Weber-Fechner%20law%2C%20whereby%20the%20perceived%20distance%0Alogarithmically%20compresses%20as%20years%20recede%20from%20this%20reference%20point.%20To%0Auncover%20the%20mechanisms%20behind%20this%20behavior%2C%20we%20conducted%20multiple%20analyses%0Aacross%20neuronal%2C%20representational%2C%20and%20informational%20levels.%20We%20first%20identify%0Aa%20set%20of%20temporal-preferential%20neurons%20and%20find%20that%20this%20group%20exhibits%0Aminimal%20activation%20at%20the%20subjective%20reference%20point%20and%20implements%20a%0Alogarithmic%20coding%20scheme%20convergently%20found%20in%20biological%20systems.%20Probing%0Arepresentations%20of%20years%20reveals%20a%20hierarchical%20construction%20process%2C%20where%0Ayears%20evolve%20from%20basic%20numerical%20values%20in%20shallow%20layers%20to%20abstract%20temporal%0Aorientation%20in%20deep%20layers.%20Finally%2C%20using%20pre-trained%20embedding%20models%2C%20we%0Afound%20that%20the%20training%20corpus%20itself%20possesses%20an%20inherent%2C%20non-linear%0Atemporal%20structure%2C%20which%20provides%20the%20raw%20material%20for%20the%20model%27s%20internal%0Aconstruction.%20In%20discussion%2C%20we%20propose%20an%20experientialist%20perspective%20for%0Aunderstanding%20these%20findings%2C%20where%20the%20LLMs%27%20cognition%20is%20viewed%20as%20a%0Asubjective%20construction%20of%20the%20external%20world%20by%20its%20internal%20representational%0Asystem.%20This%20nuanced%20perspective%20implies%20the%20potential%20emergence%20of%20alien%0Acognitive%20frameworks%20that%20humans%20cannot%20intuitively%20predict%2C%20pointing%20toward%20a%0Adirection%20for%20AI%20alignment%20that%20focuses%20on%20guiding%20internal%20constructions.%20Our%0Acode%20is%20available%20at%20https%3A//TheOtherMind.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Other%2520Mind%253A%2520How%2520Language%2520Models%2520Exhibit%2520Human%2520Temporal%2520Cognition%26entry.906535625%3DLingyu%2520Li%2520and%2520Yang%2520Yao%2520and%2520Yixu%2520Wang%2520and%2520Chubo%2520Li%2520and%2520Yan%2520Teng%2520and%2520Yingchun%2520Wang%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continue%2520to%2520advance%252C%2520they%2520exhibit%2520certain%250Acognitive%2520patterns%2520similar%2520to%2520those%2520of%2520humans%2520that%2520are%2520not%2520directly%2520specified%250Ain%2520training%2520data.%2520This%2520study%2520investigates%2520this%2520phenomenon%2520by%2520focusing%2520on%250Atemporal%2520cognition%2520in%2520LLMs.%2520Leveraging%2520the%2520similarity%2520judgment%2520task%252C%2520we%2520find%250Athat%2520larger%2520models%2520spontaneously%2520establish%2520a%2520subjective%2520temporal%2520reference%250Apoint%2520and%2520adhere%2520to%2520the%2520Weber-Fechner%2520law%252C%2520whereby%2520the%2520perceived%2520distance%250Alogarithmically%2520compresses%2520as%2520years%2520recede%2520from%2520this%2520reference%2520point.%2520To%250Auncover%2520the%2520mechanisms%2520behind%2520this%2520behavior%252C%2520we%2520conducted%2520multiple%2520analyses%250Aacross%2520neuronal%252C%2520representational%252C%2520and%2520informational%2520levels.%2520We%2520first%2520identify%250Aa%2520set%2520of%2520temporal-preferential%2520neurons%2520and%2520find%2520that%2520this%2520group%2520exhibits%250Aminimal%2520activation%2520at%2520the%2520subjective%2520reference%2520point%2520and%2520implements%2520a%250Alogarithmic%2520coding%2520scheme%2520convergently%2520found%2520in%2520biological%2520systems.%2520Probing%250Arepresentations%2520of%2520years%2520reveals%2520a%2520hierarchical%2520construction%2520process%252C%2520where%250Ayears%2520evolve%2520from%2520basic%2520numerical%2520values%2520in%2520shallow%2520layers%2520to%2520abstract%2520temporal%250Aorientation%2520in%2520deep%2520layers.%2520Finally%252C%2520using%2520pre-trained%2520embedding%2520models%252C%2520we%250Afound%2520that%2520the%2520training%2520corpus%2520itself%2520possesses%2520an%2520inherent%252C%2520non-linear%250Atemporal%2520structure%252C%2520which%2520provides%2520the%2520raw%2520material%2520for%2520the%2520model%2527s%2520internal%250Aconstruction.%2520In%2520discussion%252C%2520we%2520propose%2520an%2520experientialist%2520perspective%2520for%250Aunderstanding%2520these%2520findings%252C%2520where%2520the%2520LLMs%2527%2520cognition%2520is%2520viewed%2520as%2520a%250Asubjective%2520construction%2520of%2520the%2520external%2520world%2520by%2520its%2520internal%2520representational%250Asystem.%2520This%2520nuanced%2520perspective%2520implies%2520the%2520potential%2520emergence%2520of%2520alien%250Acognitive%2520frameworks%2520that%2520humans%2520cannot%2520intuitively%2520predict%252C%2520pointing%2520toward%2520a%250Adirection%2520for%2520AI%2520alignment%2520that%2520focuses%2520on%2520guiding%2520internal%2520constructions.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//TheOtherMind.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Other%20Mind%3A%20How%20Language%20Models%20Exhibit%20Human%20Temporal%20Cognition&entry.906535625=Lingyu%20Li%20and%20Yang%20Yao%20and%20Yixu%20Wang%20and%20Chubo%20Li%20and%20Yan%20Teng%20and%20Yingchun%20Wang&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20advance%2C%20they%20exhibit%20certain%0Acognitive%20patterns%20similar%20to%20those%20of%20humans%20that%20are%20not%20directly%20specified%0Ain%20training%20data.%20This%20study%20investigates%20this%20phenomenon%20by%20focusing%20on%0Atemporal%20cognition%20in%20LLMs.%20Leveraging%20the%20similarity%20judgment%20task%2C%20we%20find%0Athat%20larger%20models%20spontaneously%20establish%20a%20subjective%20temporal%20reference%0Apoint%20and%20adhere%20to%20the%20Weber-Fechner%20law%2C%20whereby%20the%20perceived%20distance%0Alogarithmically%20compresses%20as%20years%20recede%20from%20this%20reference%20point.%20To%0Auncover%20the%20mechanisms%20behind%20this%20behavior%2C%20we%20conducted%20multiple%20analyses%0Aacross%20neuronal%2C%20representational%2C%20and%20informational%20levels.%20We%20first%20identify%0Aa%20set%20of%20temporal-preferential%20neurons%20and%20find%20that%20this%20group%20exhibits%0Aminimal%20activation%20at%20the%20subjective%20reference%20point%20and%20implements%20a%0Alogarithmic%20coding%20scheme%20convergently%20found%20in%20biological%20systems.%20Probing%0Arepresentations%20of%20years%20reveals%20a%20hierarchical%20construction%20process%2C%20where%0Ayears%20evolve%20from%20basic%20numerical%20values%20in%20shallow%20layers%20to%20abstract%20temporal%0Aorientation%20in%20deep%20layers.%20Finally%2C%20using%20pre-trained%20embedding%20models%2C%20we%0Afound%20that%20the%20training%20corpus%20itself%20possesses%20an%20inherent%2C%20non-linear%0Atemporal%20structure%2C%20which%20provides%20the%20raw%20material%20for%20the%20model%27s%20internal%0Aconstruction.%20In%20discussion%2C%20we%20propose%20an%20experientialist%20perspective%20for%0Aunderstanding%20these%20findings%2C%20where%20the%20LLMs%27%20cognition%20is%20viewed%20as%20a%0Asubjective%20construction%20of%20the%20external%20world%20by%20its%20internal%20representational%0Asystem.%20This%20nuanced%20perspective%20implies%20the%20potential%20emergence%20of%20alien%0Acognitive%20frameworks%20that%20humans%20cannot%20intuitively%20predict%2C%20pointing%20toward%20a%0Adirection%20for%20AI%20alignment%20that%20focuses%20on%20guiding%20internal%20constructions.%20Our%0Acode%20is%20available%20at%20https%3A//TheOtherMind.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15851v1&entry.124074799=Read"},
{"title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature\n  Adaptation", "author": "Qinqian Lei and Bo Wang and Robby T. Tan", "abstract": "  Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.\n", "link": "http://arxiv.org/abs/2507.15542v1", "date": "2025-07-21", "relevancy": 2.6631, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5265}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOLa%3A%20Zero-Shot%20HOI%20Detection%20with%20Low-Rank%20Decomposed%20VLM%20Feature%0A%20%20Adaptation&body=Title%3A%20HOLa%3A%20Zero-Shot%20HOI%20Detection%20with%20Low-Rank%20Decomposed%20VLM%20Feature%0A%20%20Adaptation%0AAuthor%3A%20Qinqian%20Lei%20and%20Bo%20Wang%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%20Zero-shot%20human-object%20interaction%20%28HOI%29%20detection%20remains%20a%20challenging%0Atask%2C%20particularly%20in%20generalizing%20to%20unseen%20actions.%20Existing%20methods%20address%0Athis%20challenge%20by%20tapping%20Vision-Language%20Models%20%28VLMs%29%20to%20access%20knowledge%0Abeyond%20the%20training%20data.%20However%2C%20they%20either%20struggle%20to%20distinguish%20actions%0Ainvolving%20the%20same%20object%20or%20demonstrate%20limited%20generalization%20to%20unseen%0Aclasses.%20In%20this%20paper%2C%20we%20introduce%20HOLa%20%28Zero-Shot%20HOI%20Detection%20with%0ALow-Rank%20Decomposed%20VLM%20Feature%20Adaptation%29%2C%20a%20novel%20approach%20that%20both%0Aenhances%20generalization%20to%20unseen%20classes%20and%20improves%20action%20distinction.%20In%0Atraining%2C%20HOLa%20decomposes%20VLM%20text%20features%20for%20given%20HOI%20classes%20via%20low-rank%0Afactorization%2C%20producing%20class-shared%20basis%20features%20and%20adaptable%20weights.%0AThese%20features%20and%20weights%20form%20a%20compact%20HOI%20representation%20that%20preserves%0Ashared%20information%20across%20classes%2C%20enhancing%20generalization%20to%20unseen%20classes.%0ASubsequently%2C%20we%20refine%20action%20distinction%20by%20adapting%20weights%20for%20each%20HOI%0Aclass%20and%20introducing%20human-object%20tokens%20to%20enrich%20visual%20interaction%0Arepresentations.%20To%20further%20distinguish%20unseen%20actions%2C%20we%20guide%20the%20weight%0Aadaptation%20with%20LLM-derived%20action%20regularization.%20Experimental%20results%20show%0Athat%20our%20method%20sets%20a%20new%20state-of-the-art%20across%20zero-shot%20HOI%20settings%20on%0AHICO-DET%2C%20achieving%20an%20unseen-class%20mAP%20of%2027.91%20in%20the%20unseen-verb%20setting.%0AOur%20code%20is%20available%20at%20https%3A//github.com/ChelsieLei/HOLa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOLa%253A%2520Zero-Shot%2520HOI%2520Detection%2520with%2520Low-Rank%2520Decomposed%2520VLM%2520Feature%250A%2520%2520Adaptation%26entry.906535625%3DQinqian%2520Lei%2520and%2520Bo%2520Wang%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3D%2520%2520Zero-shot%2520human-object%2520interaction%2520%2528HOI%2529%2520detection%2520remains%2520a%2520challenging%250Atask%252C%2520particularly%2520in%2520generalizing%2520to%2520unseen%2520actions.%2520Existing%2520methods%2520address%250Athis%2520challenge%2520by%2520tapping%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520access%2520knowledge%250Abeyond%2520the%2520training%2520data.%2520However%252C%2520they%2520either%2520struggle%2520to%2520distinguish%2520actions%250Ainvolving%2520the%2520same%2520object%2520or%2520demonstrate%2520limited%2520generalization%2520to%2520unseen%250Aclasses.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HOLa%2520%2528Zero-Shot%2520HOI%2520Detection%2520with%250ALow-Rank%2520Decomposed%2520VLM%2520Feature%2520Adaptation%2529%252C%2520a%2520novel%2520approach%2520that%2520both%250Aenhances%2520generalization%2520to%2520unseen%2520classes%2520and%2520improves%2520action%2520distinction.%2520In%250Atraining%252C%2520HOLa%2520decomposes%2520VLM%2520text%2520features%2520for%2520given%2520HOI%2520classes%2520via%2520low-rank%250Afactorization%252C%2520producing%2520class-shared%2520basis%2520features%2520and%2520adaptable%2520weights.%250AThese%2520features%2520and%2520weights%2520form%2520a%2520compact%2520HOI%2520representation%2520that%2520preserves%250Ashared%2520information%2520across%2520classes%252C%2520enhancing%2520generalization%2520to%2520unseen%2520classes.%250ASubsequently%252C%2520we%2520refine%2520action%2520distinction%2520by%2520adapting%2520weights%2520for%2520each%2520HOI%250Aclass%2520and%2520introducing%2520human-object%2520tokens%2520to%2520enrich%2520visual%2520interaction%250Arepresentations.%2520To%2520further%2520distinguish%2520unseen%2520actions%252C%2520we%2520guide%2520the%2520weight%250Aadaptation%2520with%2520LLM-derived%2520action%2520regularization.%2520Experimental%2520results%2520show%250Athat%2520our%2520method%2520sets%2520a%2520new%2520state-of-the-art%2520across%2520zero-shot%2520HOI%2520settings%2520on%250AHICO-DET%252C%2520achieving%2520an%2520unseen-class%2520mAP%2520of%252027.91%2520in%2520the%2520unseen-verb%2520setting.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/ChelsieLei/HOLa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOLa%3A%20Zero-Shot%20HOI%20Detection%20with%20Low-Rank%20Decomposed%20VLM%20Feature%0A%20%20Adaptation&entry.906535625=Qinqian%20Lei%20and%20Bo%20Wang%20and%20Robby%20T.%20Tan&entry.1292438233=%20%20Zero-shot%20human-object%20interaction%20%28HOI%29%20detection%20remains%20a%20challenging%0Atask%2C%20particularly%20in%20generalizing%20to%20unseen%20actions.%20Existing%20methods%20address%0Athis%20challenge%20by%20tapping%20Vision-Language%20Models%20%28VLMs%29%20to%20access%20knowledge%0Abeyond%20the%20training%20data.%20However%2C%20they%20either%20struggle%20to%20distinguish%20actions%0Ainvolving%20the%20same%20object%20or%20demonstrate%20limited%20generalization%20to%20unseen%0Aclasses.%20In%20this%20paper%2C%20we%20introduce%20HOLa%20%28Zero-Shot%20HOI%20Detection%20with%0ALow-Rank%20Decomposed%20VLM%20Feature%20Adaptation%29%2C%20a%20novel%20approach%20that%20both%0Aenhances%20generalization%20to%20unseen%20classes%20and%20improves%20action%20distinction.%20In%0Atraining%2C%20HOLa%20decomposes%20VLM%20text%20features%20for%20given%20HOI%20classes%20via%20low-rank%0Afactorization%2C%20producing%20class-shared%20basis%20features%20and%20adaptable%20weights.%0AThese%20features%20and%20weights%20form%20a%20compact%20HOI%20representation%20that%20preserves%0Ashared%20information%20across%20classes%2C%20enhancing%20generalization%20to%20unseen%20classes.%0ASubsequently%2C%20we%20refine%20action%20distinction%20by%20adapting%20weights%20for%20each%20HOI%0Aclass%20and%20introducing%20human-object%20tokens%20to%20enrich%20visual%20interaction%0Arepresentations.%20To%20further%20distinguish%20unseen%20actions%2C%20we%20guide%20the%20weight%0Aadaptation%20with%20LLM-derived%20action%20regularization.%20Experimental%20results%20show%0Athat%20our%20method%20sets%20a%20new%20state-of-the-art%20across%20zero-shot%20HOI%20settings%20on%0AHICO-DET%2C%20achieving%20an%20unseen-class%20mAP%20of%2027.91%20in%20the%20unseen-verb%20setting.%0AOur%20code%20is%20available%20at%20https%3A//github.com/ChelsieLei/HOLa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15542v1&entry.124074799=Read"},
{"title": "GeoHNNs: Geometric Hamiltonian Neural Networks", "author": "Amine Mohamed Aboussalah and Abdessalam Ed-dib", "abstract": "  The fundamental laws of physics are intrinsically geometric, dictating the\nevolution of systems through principles of symmetry and conservation. While\nmodern machine learning offers powerful tools for modeling complex dynamics\nfrom data, common methods often ignore this underlying geometric fabric.\nPhysics-informed neural networks, for instance, can violate fundamental\nphysical principles, leading to predictions that are unstable over long\nperiods, particularly for high-dimensional and chaotic systems. Here, we\nintroduce \\textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework\nthat learns dynamics by explicitly encoding the geometric priors inherent to\nphysical laws. Our approach enforces two fundamental structures: the Riemannian\ngeometry of inertia, by parameterizing inertia matrices in their natural\nmathematical space of symmetric positive-definite matrices, and the symplectic\ngeometry of phase space, using a constrained autoencoder to ensure the\npreservation of phase space volume in a reduced latent space. We demonstrate\nthrough experiments on systems ranging from coupled oscillators to\nhigh-dimensional deformable objects that GeoHNN significantly outperforms\nexisting models. It achieves superior long-term stability, accuracy, and energy\nconservation, confirming that embedding the geometry of physics is not just a\ntheoretical appeal but a practical necessity for creating robust and\ngeneralizable models of the physical world.\n", "link": "http://arxiv.org/abs/2507.15678v1", "date": "2025-07-21", "relevancy": 2.656, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.535}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5329}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoHNNs%3A%20Geometric%20Hamiltonian%20Neural%20Networks&body=Title%3A%20GeoHNNs%3A%20Geometric%20Hamiltonian%20Neural%20Networks%0AAuthor%3A%20Amine%20Mohamed%20Aboussalah%20and%20Abdessalam%20Ed-dib%0AAbstract%3A%20%20%20The%20fundamental%20laws%20of%20physics%20are%20intrinsically%20geometric%2C%20dictating%20the%0Aevolution%20of%20systems%20through%20principles%20of%20symmetry%20and%20conservation.%20While%0Amodern%20machine%20learning%20offers%20powerful%20tools%20for%20modeling%20complex%20dynamics%0Afrom%20data%2C%20common%20methods%20often%20ignore%20this%20underlying%20geometric%20fabric.%0APhysics-informed%20neural%20networks%2C%20for%20instance%2C%20can%20violate%20fundamental%0Aphysical%20principles%2C%20leading%20to%20predictions%20that%20are%20unstable%20over%20long%0Aperiods%2C%20particularly%20for%20high-dimensional%20and%20chaotic%20systems.%20Here%2C%20we%0Aintroduce%20%5Ctextit%7BGeometric%20Hamiltonian%20Neural%20Networks%20%28GeoHNN%29%7D%2C%20a%20framework%0Athat%20learns%20dynamics%20by%20explicitly%20encoding%20the%20geometric%20priors%20inherent%20to%0Aphysical%20laws.%20Our%20approach%20enforces%20two%20fundamental%20structures%3A%20the%20Riemannian%0Ageometry%20of%20inertia%2C%20by%20parameterizing%20inertia%20matrices%20in%20their%20natural%0Amathematical%20space%20of%20symmetric%20positive-definite%20matrices%2C%20and%20the%20symplectic%0Ageometry%20of%20phase%20space%2C%20using%20a%20constrained%20autoencoder%20to%20ensure%20the%0Apreservation%20of%20phase%20space%20volume%20in%20a%20reduced%20latent%20space.%20We%20demonstrate%0Athrough%20experiments%20on%20systems%20ranging%20from%20coupled%20oscillators%20to%0Ahigh-dimensional%20deformable%20objects%20that%20GeoHNN%20significantly%20outperforms%0Aexisting%20models.%20It%20achieves%20superior%20long-term%20stability%2C%20accuracy%2C%20and%20energy%0Aconservation%2C%20confirming%20that%20embedding%20the%20geometry%20of%20physics%20is%20not%20just%20a%0Atheoretical%20appeal%20but%20a%20practical%20necessity%20for%20creating%20robust%20and%0Ageneralizable%20models%20of%20the%20physical%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoHNNs%253A%2520Geometric%2520Hamiltonian%2520Neural%2520Networks%26entry.906535625%3DAmine%2520Mohamed%2520Aboussalah%2520and%2520Abdessalam%2520Ed-dib%26entry.1292438233%3D%2520%2520The%2520fundamental%2520laws%2520of%2520physics%2520are%2520intrinsically%2520geometric%252C%2520dictating%2520the%250Aevolution%2520of%2520systems%2520through%2520principles%2520of%2520symmetry%2520and%2520conservation.%2520While%250Amodern%2520machine%2520learning%2520offers%2520powerful%2520tools%2520for%2520modeling%2520complex%2520dynamics%250Afrom%2520data%252C%2520common%2520methods%2520often%2520ignore%2520this%2520underlying%2520geometric%2520fabric.%250APhysics-informed%2520neural%2520networks%252C%2520for%2520instance%252C%2520can%2520violate%2520fundamental%250Aphysical%2520principles%252C%2520leading%2520to%2520predictions%2520that%2520are%2520unstable%2520over%2520long%250Aperiods%252C%2520particularly%2520for%2520high-dimensional%2520and%2520chaotic%2520systems.%2520Here%252C%2520we%250Aintroduce%2520%255Ctextit%257BGeometric%2520Hamiltonian%2520Neural%2520Networks%2520%2528GeoHNN%2529%257D%252C%2520a%2520framework%250Athat%2520learns%2520dynamics%2520by%2520explicitly%2520encoding%2520the%2520geometric%2520priors%2520inherent%2520to%250Aphysical%2520laws.%2520Our%2520approach%2520enforces%2520two%2520fundamental%2520structures%253A%2520the%2520Riemannian%250Ageometry%2520of%2520inertia%252C%2520by%2520parameterizing%2520inertia%2520matrices%2520in%2520their%2520natural%250Amathematical%2520space%2520of%2520symmetric%2520positive-definite%2520matrices%252C%2520and%2520the%2520symplectic%250Ageometry%2520of%2520phase%2520space%252C%2520using%2520a%2520constrained%2520autoencoder%2520to%2520ensure%2520the%250Apreservation%2520of%2520phase%2520space%2520volume%2520in%2520a%2520reduced%2520latent%2520space.%2520We%2520demonstrate%250Athrough%2520experiments%2520on%2520systems%2520ranging%2520from%2520coupled%2520oscillators%2520to%250Ahigh-dimensional%2520deformable%2520objects%2520that%2520GeoHNN%2520significantly%2520outperforms%250Aexisting%2520models.%2520It%2520achieves%2520superior%2520long-term%2520stability%252C%2520accuracy%252C%2520and%2520energy%250Aconservation%252C%2520confirming%2520that%2520embedding%2520the%2520geometry%2520of%2520physics%2520is%2520not%2520just%2520a%250Atheoretical%2520appeal%2520but%2520a%2520practical%2520necessity%2520for%2520creating%2520robust%2520and%250Ageneralizable%2520models%2520of%2520the%2520physical%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoHNNs%3A%20Geometric%20Hamiltonian%20Neural%20Networks&entry.906535625=Amine%20Mohamed%20Aboussalah%20and%20Abdessalam%20Ed-dib&entry.1292438233=%20%20The%20fundamental%20laws%20of%20physics%20are%20intrinsically%20geometric%2C%20dictating%20the%0Aevolution%20of%20systems%20through%20principles%20of%20symmetry%20and%20conservation.%20While%0Amodern%20machine%20learning%20offers%20powerful%20tools%20for%20modeling%20complex%20dynamics%0Afrom%20data%2C%20common%20methods%20often%20ignore%20this%20underlying%20geometric%20fabric.%0APhysics-informed%20neural%20networks%2C%20for%20instance%2C%20can%20violate%20fundamental%0Aphysical%20principles%2C%20leading%20to%20predictions%20that%20are%20unstable%20over%20long%0Aperiods%2C%20particularly%20for%20high-dimensional%20and%20chaotic%20systems.%20Here%2C%20we%0Aintroduce%20%5Ctextit%7BGeometric%20Hamiltonian%20Neural%20Networks%20%28GeoHNN%29%7D%2C%20a%20framework%0Athat%20learns%20dynamics%20by%20explicitly%20encoding%20the%20geometric%20priors%20inherent%20to%0Aphysical%20laws.%20Our%20approach%20enforces%20two%20fundamental%20structures%3A%20the%20Riemannian%0Ageometry%20of%20inertia%2C%20by%20parameterizing%20inertia%20matrices%20in%20their%20natural%0Amathematical%20space%20of%20symmetric%20positive-definite%20matrices%2C%20and%20the%20symplectic%0Ageometry%20of%20phase%20space%2C%20using%20a%20constrained%20autoencoder%20to%20ensure%20the%0Apreservation%20of%20phase%20space%20volume%20in%20a%20reduced%20latent%20space.%20We%20demonstrate%0Athrough%20experiments%20on%20systems%20ranging%20from%20coupled%20oscillators%20to%0Ahigh-dimensional%20deformable%20objects%20that%20GeoHNN%20significantly%20outperforms%0Aexisting%20models.%20It%20achieves%20superior%20long-term%20stability%2C%20accuracy%2C%20and%20energy%0Aconservation%2C%20confirming%20that%20embedding%20the%20geometry%20of%20physics%20is%20not%20just%20a%0Atheoretical%20appeal%20but%20a%20practical%20necessity%20for%20creating%20robust%20and%0Ageneralizable%20models%20of%20the%20physical%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15678v1&entry.124074799=Read"},
{"title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "author": "Ghassen Baklouti and Julio Silva-Rodr\u00edguez and Jose Dolz and Houda Bahig and Ismail Ben Ayed", "abstract": "  Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA\n", "link": "http://arxiv.org/abs/2507.15793v1", "date": "2025-07-21", "relevancy": 2.6504, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Low-Rank%20Adaptation%20for%20Few-Shot%20Organ%20Segmentation&body=Title%3A%20Regularized%20Low-Rank%20Adaptation%20for%20Few-Shot%20Organ%20Segmentation%0AAuthor%3A%20Ghassen%20Baklouti%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Jose%20Dolz%20and%20Houda%20Bahig%20and%20Ismail%20Ben%20Ayed%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20of%20pre-trained%20foundation%20models%20is%0Aincreasingly%20attracting%20interest%20in%20medical%20imaging%20due%20to%20its%20effectiveness%0Aand%20computational%20efficiency.%20Among%20these%20methods%2C%20Low-Rank%20Adaptation%20%28LoRA%29%0Ais%20a%20notable%20approach%20based%20on%20the%20assumption%20that%20the%20adaptation%20inherently%0Aoccurs%20in%20a%20low-dimensional%20subspace.%20While%20it%20has%20shown%20good%20performance%2C%20its%0Aimplementation%20requires%20a%20fixed%20and%20unalterable%20rank%2C%20which%20might%20be%0Achallenging%20to%20select%20given%20the%20unique%20complexities%20and%20requirements%20of%20each%0Amedical%20imaging%20downstream%20task.%20Inspired%20by%20advancements%20in%20natural%20image%0Aprocessing%2C%20we%20introduce%20a%20novel%20approach%20for%20medical%20image%20segmentation%20that%0Adynamically%20adjusts%20the%20intrinsic%20rank%20during%20adaptation.%20Viewing%20the%20low-rank%0Arepresentation%20of%20the%20trainable%20weight%20matrices%20as%20a%20singular%20value%0Adecomposition%2C%20we%20introduce%20an%20l_1%20sparsity%20regularizer%20to%20the%20loss%20function%2C%0Aand%20tackle%20it%20with%20a%20proximal%20optimizer.%20The%20regularizer%20could%20be%20viewed%20as%20a%0Apenalty%20on%20the%20decomposition%20rank.%20Hence%2C%20its%20minimization%20enables%20to%20find%0Atask-adapted%20ranks%20automatically.%20Our%20method%20is%20evaluated%20in%20a%20realistic%0Afew-shot%20fine-tuning%20setting%2C%20where%20we%20compare%20it%20first%20to%20the%20standard%20LoRA%0Aand%20then%20to%20several%20other%20PEFT%20methods%20across%20two%20distinguishable%20tasks%3A%20base%0Aorgans%20and%20novel%20organs.%20Our%20extensive%20experiments%20demonstrate%20the%20significant%0Aperformance%20improvements%20driven%20by%20our%20method%2C%20highlighting%20its%20efficiency%20and%0Arobustness%20against%20suboptimal%20rank%20initialization.%20Our%20code%20is%20publicly%0Aavailable%3A%20https%3A//github.com/ghassenbaklouti/ARENA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Low-Rank%2520Adaptation%2520for%2520Few-Shot%2520Organ%2520Segmentation%26entry.906535625%3DGhassen%2520Baklouti%2520and%2520Julio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Jose%2520Dolz%2520and%2520Houda%2520Bahig%2520and%2520Ismail%2520Ben%2520Ayed%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520of%2520pre-trained%2520foundation%2520models%2520is%250Aincreasingly%2520attracting%2520interest%2520in%2520medical%2520imaging%2520due%2520to%2520its%2520effectiveness%250Aand%2520computational%2520efficiency.%2520Among%2520these%2520methods%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Ais%2520a%2520notable%2520approach%2520based%2520on%2520the%2520assumption%2520that%2520the%2520adaptation%2520inherently%250Aoccurs%2520in%2520a%2520low-dimensional%2520subspace.%2520While%2520it%2520has%2520shown%2520good%2520performance%252C%2520its%250Aimplementation%2520requires%2520a%2520fixed%2520and%2520unalterable%2520rank%252C%2520which%2520might%2520be%250Achallenging%2520to%2520select%2520given%2520the%2520unique%2520complexities%2520and%2520requirements%2520of%2520each%250Amedical%2520imaging%2520downstream%2520task.%2520Inspired%2520by%2520advancements%2520in%2520natural%2520image%250Aprocessing%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520medical%2520image%2520segmentation%2520that%250Adynamically%2520adjusts%2520the%2520intrinsic%2520rank%2520during%2520adaptation.%2520Viewing%2520the%2520low-rank%250Arepresentation%2520of%2520the%2520trainable%2520weight%2520matrices%2520as%2520a%2520singular%2520value%250Adecomposition%252C%2520we%2520introduce%2520an%2520l_1%2520sparsity%2520regularizer%2520to%2520the%2520loss%2520function%252C%250Aand%2520tackle%2520it%2520with%2520a%2520proximal%2520optimizer.%2520The%2520regularizer%2520could%2520be%2520viewed%2520as%2520a%250Apenalty%2520on%2520the%2520decomposition%2520rank.%2520Hence%252C%2520its%2520minimization%2520enables%2520to%2520find%250Atask-adapted%2520ranks%2520automatically.%2520Our%2520method%2520is%2520evaluated%2520in%2520a%2520realistic%250Afew-shot%2520fine-tuning%2520setting%252C%2520where%2520we%2520compare%2520it%2520first%2520to%2520the%2520standard%2520LoRA%250Aand%2520then%2520to%2520several%2520other%2520PEFT%2520methods%2520across%2520two%2520distinguishable%2520tasks%253A%2520base%250Aorgans%2520and%2520novel%2520organs.%2520Our%2520extensive%2520experiments%2520demonstrate%2520the%2520significant%250Aperformance%2520improvements%2520driven%2520by%2520our%2520method%252C%2520highlighting%2520its%2520efficiency%2520and%250Arobustness%2520against%2520suboptimal%2520rank%2520initialization.%2520Our%2520code%2520is%2520publicly%250Aavailable%253A%2520https%253A//github.com/ghassenbaklouti/ARENA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Low-Rank%20Adaptation%20for%20Few-Shot%20Organ%20Segmentation&entry.906535625=Ghassen%20Baklouti%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Jose%20Dolz%20and%20Houda%20Bahig%20and%20Ismail%20Ben%20Ayed&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20of%20pre-trained%20foundation%20models%20is%0Aincreasingly%20attracting%20interest%20in%20medical%20imaging%20due%20to%20its%20effectiveness%0Aand%20computational%20efficiency.%20Among%20these%20methods%2C%20Low-Rank%20Adaptation%20%28LoRA%29%0Ais%20a%20notable%20approach%20based%20on%20the%20assumption%20that%20the%20adaptation%20inherently%0Aoccurs%20in%20a%20low-dimensional%20subspace.%20While%20it%20has%20shown%20good%20performance%2C%20its%0Aimplementation%20requires%20a%20fixed%20and%20unalterable%20rank%2C%20which%20might%20be%0Achallenging%20to%20select%20given%20the%20unique%20complexities%20and%20requirements%20of%20each%0Amedical%20imaging%20downstream%20task.%20Inspired%20by%20advancements%20in%20natural%20image%0Aprocessing%2C%20we%20introduce%20a%20novel%20approach%20for%20medical%20image%20segmentation%20that%0Adynamically%20adjusts%20the%20intrinsic%20rank%20during%20adaptation.%20Viewing%20the%20low-rank%0Arepresentation%20of%20the%20trainable%20weight%20matrices%20as%20a%20singular%20value%0Adecomposition%2C%20we%20introduce%20an%20l_1%20sparsity%20regularizer%20to%20the%20loss%20function%2C%0Aand%20tackle%20it%20with%20a%20proximal%20optimizer.%20The%20regularizer%20could%20be%20viewed%20as%20a%0Apenalty%20on%20the%20decomposition%20rank.%20Hence%2C%20its%20minimization%20enables%20to%20find%0Atask-adapted%20ranks%20automatically.%20Our%20method%20is%20evaluated%20in%20a%20realistic%0Afew-shot%20fine-tuning%20setting%2C%20where%20we%20compare%20it%20first%20to%20the%20standard%20LoRA%0Aand%20then%20to%20several%20other%20PEFT%20methods%20across%20two%20distinguishable%20tasks%3A%20base%0Aorgans%20and%20novel%20organs.%20Our%20extensive%20experiments%20demonstrate%20the%20significant%0Aperformance%20improvements%20driven%20by%20our%20method%2C%20highlighting%20its%20efficiency%20and%0Arobustness%20against%20suboptimal%20rank%20initialization.%20Our%20code%20is%20publicly%0Aavailable%3A%20https%3A//github.com/ghassenbaklouti/ARENA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15793v1&entry.124074799=Read"},
{"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment\n  Induced by Model Interventions in Multilingual Language Models", "author": "Anirudh Sundar and Sinead Williamson and Katherine Metcalf and Barry-John Theobald and Skyler Seto and Masha Fedzechkina", "abstract": "  Aligned representations across languages is a desired property in\nmultilingual large language models (mLLMs), as alignment can improve\nperformance in cross-lingual tasks. Typically alignment requires fine-tuning a\nmodel, which is computationally expensive, and sizable language data, which\noften may not be available. A data-efficient alternative to fine-tuning is\nmodel interventions -- a method for manipulating model activations to steer\ngeneration into the desired direction. We analyze the effect of a popular\nintervention (finding experts) on the alignment of cross-lingual\nrepresentations in mLLMs. We identify the neurons to manipulate for a given\nlanguage and introspect the embedding space of mLLMs pre- and\npost-manipulation. We show that modifying the mLLM's activations changes its\nembedding space such that cross-lingual alignment is enhanced. Further, we show\nthat the changes to the embedding space translate into improved downstream\nperformance on retrieval tasks, with up to 2x improvements in top-1 accuracy on\ncross-lingual retrieval.\n", "link": "http://arxiv.org/abs/2502.15639v2", "date": "2025-07-21", "relevancy": 2.6415, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models&body=Title%3A%20Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models%0AAuthor%3A%20Anirudh%20Sundar%20and%20Sinead%20Williamson%20and%20Katherine%20Metcalf%20and%20Barry-John%20Theobald%20and%20Skyler%20Seto%20and%20Masha%20Fedzechkina%0AAbstract%3A%20%20%20Aligned%20representations%20across%20languages%20is%20a%20desired%20property%20in%0Amultilingual%20large%20language%20models%20%28mLLMs%29%2C%20as%20alignment%20can%20improve%0Aperformance%20in%20cross-lingual%20tasks.%20Typically%20alignment%20requires%20fine-tuning%20a%0Amodel%2C%20which%20is%20computationally%20expensive%2C%20and%20sizable%20language%20data%2C%20which%0Aoften%20may%20not%20be%20available.%20A%20data-efficient%20alternative%20to%20fine-tuning%20is%0Amodel%20interventions%20--%20a%20method%20for%20manipulating%20model%20activations%20to%20steer%0Ageneration%20into%20the%20desired%20direction.%20We%20analyze%20the%20effect%20of%20a%20popular%0Aintervention%20%28finding%20experts%29%20on%20the%20alignment%20of%20cross-lingual%0Arepresentations%20in%20mLLMs.%20We%20identify%20the%20neurons%20to%20manipulate%20for%20a%20given%0Alanguage%20and%20introspect%20the%20embedding%20space%20of%20mLLMs%20pre-%20and%0Apost-manipulation.%20We%20show%20that%20modifying%20the%20mLLM%27s%20activations%20changes%20its%0Aembedding%20space%20such%20that%20cross-lingual%20alignment%20is%20enhanced.%20Further%2C%20we%20show%0Athat%20the%20changes%20to%20the%20embedding%20space%20translate%20into%20improved%20downstream%0Aperformance%20on%20retrieval%20tasks%2C%20with%20up%20to%202x%20improvements%20in%20top-1%20accuracy%20on%0Across-lingual%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520into%2520New%2520Embedding%2520Spaces%253A%2520Analyzing%2520Cross-Lingual%2520Alignment%250A%2520%2520Induced%2520by%2520Model%2520Interventions%2520in%2520Multilingual%2520Language%2520Models%26entry.906535625%3DAnirudh%2520Sundar%2520and%2520Sinead%2520Williamson%2520and%2520Katherine%2520Metcalf%2520and%2520Barry-John%2520Theobald%2520and%2520Skyler%2520Seto%2520and%2520Masha%2520Fedzechkina%26entry.1292438233%3D%2520%2520Aligned%2520representations%2520across%2520languages%2520is%2520a%2520desired%2520property%2520in%250Amultilingual%2520large%2520language%2520models%2520%2528mLLMs%2529%252C%2520as%2520alignment%2520can%2520improve%250Aperformance%2520in%2520cross-lingual%2520tasks.%2520Typically%2520alignment%2520requires%2520fine-tuning%2520a%250Amodel%252C%2520which%2520is%2520computationally%2520expensive%252C%2520and%2520sizable%2520language%2520data%252C%2520which%250Aoften%2520may%2520not%2520be%2520available.%2520A%2520data-efficient%2520alternative%2520to%2520fine-tuning%2520is%250Amodel%2520interventions%2520--%2520a%2520method%2520for%2520manipulating%2520model%2520activations%2520to%2520steer%250Ageneration%2520into%2520the%2520desired%2520direction.%2520We%2520analyze%2520the%2520effect%2520of%2520a%2520popular%250Aintervention%2520%2528finding%2520experts%2529%2520on%2520the%2520alignment%2520of%2520cross-lingual%250Arepresentations%2520in%2520mLLMs.%2520We%2520identify%2520the%2520neurons%2520to%2520manipulate%2520for%2520a%2520given%250Alanguage%2520and%2520introspect%2520the%2520embedding%2520space%2520of%2520mLLMs%2520pre-%2520and%250Apost-manipulation.%2520We%2520show%2520that%2520modifying%2520the%2520mLLM%2527s%2520activations%2520changes%2520its%250Aembedding%2520space%2520such%2520that%2520cross-lingual%2520alignment%2520is%2520enhanced.%2520Further%252C%2520we%2520show%250Athat%2520the%2520changes%2520to%2520the%2520embedding%2520space%2520translate%2520into%2520improved%2520downstream%250Aperformance%2520on%2520retrieval%2520tasks%252C%2520with%2520up%2520to%25202x%2520improvements%2520in%2520top-1%2520accuracy%2520on%250Across-lingual%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20into%20New%20Embedding%20Spaces%3A%20Analyzing%20Cross-Lingual%20Alignment%0A%20%20Induced%20by%20Model%20Interventions%20in%20Multilingual%20Language%20Models&entry.906535625=Anirudh%20Sundar%20and%20Sinead%20Williamson%20and%20Katherine%20Metcalf%20and%20Barry-John%20Theobald%20and%20Skyler%20Seto%20and%20Masha%20Fedzechkina&entry.1292438233=%20%20Aligned%20representations%20across%20languages%20is%20a%20desired%20property%20in%0Amultilingual%20large%20language%20models%20%28mLLMs%29%2C%20as%20alignment%20can%20improve%0Aperformance%20in%20cross-lingual%20tasks.%20Typically%20alignment%20requires%20fine-tuning%20a%0Amodel%2C%20which%20is%20computationally%20expensive%2C%20and%20sizable%20language%20data%2C%20which%0Aoften%20may%20not%20be%20available.%20A%20data-efficient%20alternative%20to%20fine-tuning%20is%0Amodel%20interventions%20--%20a%20method%20for%20manipulating%20model%20activations%20to%20steer%0Ageneration%20into%20the%20desired%20direction.%20We%20analyze%20the%20effect%20of%20a%20popular%0Aintervention%20%28finding%20experts%29%20on%20the%20alignment%20of%20cross-lingual%0Arepresentations%20in%20mLLMs.%20We%20identify%20the%20neurons%20to%20manipulate%20for%20a%20given%0Alanguage%20and%20introspect%20the%20embedding%20space%20of%20mLLMs%20pre-%20and%0Apost-manipulation.%20We%20show%20that%20modifying%20the%20mLLM%27s%20activations%20changes%20its%0Aembedding%20space%20such%20that%20cross-lingual%20alignment%20is%20enhanced.%20Further%2C%20we%20show%0Athat%20the%20changes%20to%20the%20embedding%20space%20translate%20into%20improved%20downstream%0Aperformance%20on%20retrieval%20tasks%2C%20with%20up%20to%202x%20improvements%20in%20top-1%20accuracy%20on%0Across-lingual%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15639v2&entry.124074799=Read"},
{"title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz\n  Imaging", "author": "Nicolas Poggi and Shashank Agnihotri and Margret Keuper", "abstract": "  Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.\n", "link": "http://arxiv.org/abs/2507.15576v1", "date": "2025-07-21", "relevancy": 2.6272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smart%20Eyes%20for%20Silent%20Threats%3A%20VLMs%20and%20In-Context%20Learning%20for%20THz%0A%20%20Imaging&body=Title%3A%20Smart%20Eyes%20for%20Silent%20Threats%3A%20VLMs%20and%20In-Context%20Learning%20for%20THz%0A%20%20Imaging%0AAuthor%3A%20Nicolas%20Poggi%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Terahertz%20%28THz%29%20imaging%20enables%20non-invasive%20analysis%20for%20applications%20such%0Aas%20security%20screening%20and%20material%20classification%2C%20but%20effective%20image%0Aclassification%20remains%20challenging%20due%20to%20limited%20annotations%2C%20low%20resolution%2C%0Aand%20visual%20ambiguity.%20We%20introduce%20In-Context%20Learning%20%28ICL%29%20with%0AVision-Language%20Models%20%28VLMs%29%20as%20a%20flexible%2C%20interpretable%20alternative%20that%0Arequires%20no%20fine-tuning.%20Using%20a%20modality-aligned%20prompting%20framework%2C%20we%20adapt%0Atwo%20open-weight%20VLMs%20to%20the%20THz%20domain%20and%20evaluate%20them%20under%20zero-shot%20and%0Aone-shot%20settings.%20Our%20results%20show%20that%20ICL%20improves%20classification%20and%0Ainterpretability%20in%20low-data%20regimes.%20This%20is%20the%20first%20application%20of%0AICL-enhanced%20VLMs%20to%20THz%20imaging%2C%20offering%20a%20promising%20direction%20for%0Aresource-constrained%20scientific%20domains.%20Code%3A%0A%5Chref%7Bhttps%3A//github.com/Nicolas-Poggi/Project_THz_Classification/tree/main%7D%7BGitHub%0Arepository%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmart%2520Eyes%2520for%2520Silent%2520Threats%253A%2520VLMs%2520and%2520In-Context%2520Learning%2520for%2520THz%250A%2520%2520Imaging%26entry.906535625%3DNicolas%2520Poggi%2520and%2520Shashank%2520Agnihotri%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Terahertz%2520%2528THz%2529%2520imaging%2520enables%2520non-invasive%2520analysis%2520for%2520applications%2520such%250Aas%2520security%2520screening%2520and%2520material%2520classification%252C%2520but%2520effective%2520image%250Aclassification%2520remains%2520challenging%2520due%2520to%2520limited%2520annotations%252C%2520low%2520resolution%252C%250Aand%2520visual%2520ambiguity.%2520We%2520introduce%2520In-Context%2520Learning%2520%2528ICL%2529%2520with%250AVision-Language%2520Models%2520%2528VLMs%2529%2520as%2520a%2520flexible%252C%2520interpretable%2520alternative%2520that%250Arequires%2520no%2520fine-tuning.%2520Using%2520a%2520modality-aligned%2520prompting%2520framework%252C%2520we%2520adapt%250Atwo%2520open-weight%2520VLMs%2520to%2520the%2520THz%2520domain%2520and%2520evaluate%2520them%2520under%2520zero-shot%2520and%250Aone-shot%2520settings.%2520Our%2520results%2520show%2520that%2520ICL%2520improves%2520classification%2520and%250Ainterpretability%2520in%2520low-data%2520regimes.%2520This%2520is%2520the%2520first%2520application%2520of%250AICL-enhanced%2520VLMs%2520to%2520THz%2520imaging%252C%2520offering%2520a%2520promising%2520direction%2520for%250Aresource-constrained%2520scientific%2520domains.%2520Code%253A%250A%255Chref%257Bhttps%253A//github.com/Nicolas-Poggi/Project_THz_Classification/tree/main%257D%257BGitHub%250Arepository%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smart%20Eyes%20for%20Silent%20Threats%3A%20VLMs%20and%20In-Context%20Learning%20for%20THz%0A%20%20Imaging&entry.906535625=Nicolas%20Poggi%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper&entry.1292438233=%20%20Terahertz%20%28THz%29%20imaging%20enables%20non-invasive%20analysis%20for%20applications%20such%0Aas%20security%20screening%20and%20material%20classification%2C%20but%20effective%20image%0Aclassification%20remains%20challenging%20due%20to%20limited%20annotations%2C%20low%20resolution%2C%0Aand%20visual%20ambiguity.%20We%20introduce%20In-Context%20Learning%20%28ICL%29%20with%0AVision-Language%20Models%20%28VLMs%29%20as%20a%20flexible%2C%20interpretable%20alternative%20that%0Arequires%20no%20fine-tuning.%20Using%20a%20modality-aligned%20prompting%20framework%2C%20we%20adapt%0Atwo%20open-weight%20VLMs%20to%20the%20THz%20domain%20and%20evaluate%20them%20under%20zero-shot%20and%0Aone-shot%20settings.%20Our%20results%20show%20that%20ICL%20improves%20classification%20and%0Ainterpretability%20in%20low-data%20regimes.%20This%20is%20the%20first%20application%20of%0AICL-enhanced%20VLMs%20to%20THz%20imaging%2C%20offering%20a%20promising%20direction%20for%0Aresource-constrained%20scientific%20domains.%20Code%3A%0A%5Chref%7Bhttps%3A//github.com/Nicolas-Poggi/Project_THz_Classification/tree/main%7D%7BGitHub%0Arepository%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15576v1&entry.124074799=Read"},
{"title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI\n  Lesion Classification", "author": "Dezhen Wang and Sheng Miao and Rongxin Chai and Jiufa Cui", "abstract": "  Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.\n", "link": "http://arxiv.org/abs/2507.15487v1", "date": "2025-07-21", "relevancy": 2.6202, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5212}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeSamba%3A%20Decoupled%20Spectral%20Adaptive%20Framework%20for%203D%20Multi-Sequence%20MRI%0A%20%20Lesion%20Classification&body=Title%3A%20DeSamba%3A%20Decoupled%20Spectral%20Adaptive%20Framework%20for%203D%20Multi-Sequence%20MRI%0A%20%20Lesion%20Classification%0AAuthor%3A%20Dezhen%20Wang%20and%20Sheng%20Miao%20and%20Rongxin%20Chai%20and%20Jiufa%20Cui%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20sequences%20provide%20rich%20spatial%20and%20frequency%0Adomain%20information%2C%20which%20is%20crucial%20for%20accurate%20lesion%20classification%20in%0Amedical%20imaging.%20However%2C%20effectively%20integrating%20multi-sequence%20MRI%20data%20for%0Arobust%203D%20lesion%20classification%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%0ADeSamba%20%28Decoupled%20Spectral%20Adaptive%20Network%20and%20Mamba-Based%20Model%29%2C%20a%20novel%0Aframework%20designed%20to%20extract%20decoupled%20representations%20and%20adaptively%20fuse%0Aspatial%20and%20spectral%20features%20for%20lesion%20classification.%20DeSamba%20introduces%20a%0ADecoupled%20Representation%20Learning%20Module%20%28DRLM%29%20that%20decouples%20features%20from%0Adifferent%20MRI%20sequences%20through%20self-reconstruction%20and%20cross-reconstruction%2C%0Aand%20a%20Spectral%20Adaptive%20Modulation%20Block%20%28SAMB%29%20within%20the%20proposed%20SAMNet%2C%0Aenabling%20dynamic%20fusion%20of%20spectral%20and%20spatial%20information%20based%20on%20lesion%0Acharacteristics.%20We%20evaluate%20DeSamba%20on%20two%20clinically%20relevant%203D%20datasets.%20On%0Aa%20six-class%20spinal%20metastasis%20dataset%20%28n%3D1%2C448%29%2C%20DeSamba%20achieves%2062.10%25%20Top-1%0Aaccuracy%2C%2063.62%25%20F1-score%2C%2087.71%25%20AUC%2C%20and%2093.55%25%20Top-3%20accuracy%20on%20an%20external%0Avalidation%20set%20%28n%3D372%29%2C%20outperforming%20all%20state-of-the-art%20%28SOTA%29%20baselines.%20On%0Aa%20spondylitis%20dataset%20%28n%3D251%29%20involving%20a%20challenging%20binary%20classification%0Atask%2C%20DeSamba%20achieves%2070.00%25/64.52%25%20accuracy%20and%2074.75/73.88%20AUC%20on%20internal%0Aand%20external%20validation%20sets%2C%20respectively.%20Ablation%20studies%20demonstrate%20that%0Aboth%20DRLM%20and%20SAMB%20significantly%20contribute%20to%20overall%20performance%2C%20with%20over%0A10%25%20relative%20improvement%20compared%20to%20the%20baseline.%20Our%20results%20highlight%20the%0Apotential%20of%20DeSamba%20as%20a%20generalizable%20and%20effective%20solution%20for%203D%20lesion%0Aclassification%20in%20multi-sequence%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeSamba%253A%2520Decoupled%2520Spectral%2520Adaptive%2520Framework%2520for%25203D%2520Multi-Sequence%2520MRI%250A%2520%2520Lesion%2520Classification%26entry.906535625%3DDezhen%2520Wang%2520and%2520Sheng%2520Miao%2520and%2520Rongxin%2520Chai%2520and%2520Jiufa%2520Cui%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520sequences%2520provide%2520rich%2520spatial%2520and%2520frequency%250Adomain%2520information%252C%2520which%2520is%2520crucial%2520for%2520accurate%2520lesion%2520classification%2520in%250Amedical%2520imaging.%2520However%252C%2520effectively%2520integrating%2520multi-sequence%2520MRI%2520data%2520for%250Arobust%25203D%2520lesion%2520classification%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%250ADeSamba%2520%2528Decoupled%2520Spectral%2520Adaptive%2520Network%2520and%2520Mamba-Based%2520Model%2529%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520extract%2520decoupled%2520representations%2520and%2520adaptively%2520fuse%250Aspatial%2520and%2520spectral%2520features%2520for%2520lesion%2520classification.%2520DeSamba%2520introduces%2520a%250ADecoupled%2520Representation%2520Learning%2520Module%2520%2528DRLM%2529%2520that%2520decouples%2520features%2520from%250Adifferent%2520MRI%2520sequences%2520through%2520self-reconstruction%2520and%2520cross-reconstruction%252C%250Aand%2520a%2520Spectral%2520Adaptive%2520Modulation%2520Block%2520%2528SAMB%2529%2520within%2520the%2520proposed%2520SAMNet%252C%250Aenabling%2520dynamic%2520fusion%2520of%2520spectral%2520and%2520spatial%2520information%2520based%2520on%2520lesion%250Acharacteristics.%2520We%2520evaluate%2520DeSamba%2520on%2520two%2520clinically%2520relevant%25203D%2520datasets.%2520On%250Aa%2520six-class%2520spinal%2520metastasis%2520dataset%2520%2528n%253D1%252C448%2529%252C%2520DeSamba%2520achieves%252062.10%2525%2520Top-1%250Aaccuracy%252C%252063.62%2525%2520F1-score%252C%252087.71%2525%2520AUC%252C%2520and%252093.55%2525%2520Top-3%2520accuracy%2520on%2520an%2520external%250Avalidation%2520set%2520%2528n%253D372%2529%252C%2520outperforming%2520all%2520state-of-the-art%2520%2528SOTA%2529%2520baselines.%2520On%250Aa%2520spondylitis%2520dataset%2520%2528n%253D251%2529%2520involving%2520a%2520challenging%2520binary%2520classification%250Atask%252C%2520DeSamba%2520achieves%252070.00%2525/64.52%2525%2520accuracy%2520and%252074.75/73.88%2520AUC%2520on%2520internal%250Aand%2520external%2520validation%2520sets%252C%2520respectively.%2520Ablation%2520studies%2520demonstrate%2520that%250Aboth%2520DRLM%2520and%2520SAMB%2520significantly%2520contribute%2520to%2520overall%2520performance%252C%2520with%2520over%250A10%2525%2520relative%2520improvement%2520compared%2520to%2520the%2520baseline.%2520Our%2520results%2520highlight%2520the%250Apotential%2520of%2520DeSamba%2520as%2520a%2520generalizable%2520and%2520effective%2520solution%2520for%25203D%2520lesion%250Aclassification%2520in%2520multi-sequence%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeSamba%3A%20Decoupled%20Spectral%20Adaptive%20Framework%20for%203D%20Multi-Sequence%20MRI%0A%20%20Lesion%20Classification&entry.906535625=Dezhen%20Wang%20and%20Sheng%20Miao%20and%20Rongxin%20Chai%20and%20Jiufa%20Cui&entry.1292438233=%20%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20sequences%20provide%20rich%20spatial%20and%20frequency%0Adomain%20information%2C%20which%20is%20crucial%20for%20accurate%20lesion%20classification%20in%0Amedical%20imaging.%20However%2C%20effectively%20integrating%20multi-sequence%20MRI%20data%20for%0Arobust%203D%20lesion%20classification%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%0ADeSamba%20%28Decoupled%20Spectral%20Adaptive%20Network%20and%20Mamba-Based%20Model%29%2C%20a%20novel%0Aframework%20designed%20to%20extract%20decoupled%20representations%20and%20adaptively%20fuse%0Aspatial%20and%20spectral%20features%20for%20lesion%20classification.%20DeSamba%20introduces%20a%0ADecoupled%20Representation%20Learning%20Module%20%28DRLM%29%20that%20decouples%20features%20from%0Adifferent%20MRI%20sequences%20through%20self-reconstruction%20and%20cross-reconstruction%2C%0Aand%20a%20Spectral%20Adaptive%20Modulation%20Block%20%28SAMB%29%20within%20the%20proposed%20SAMNet%2C%0Aenabling%20dynamic%20fusion%20of%20spectral%20and%20spatial%20information%20based%20on%20lesion%0Acharacteristics.%20We%20evaluate%20DeSamba%20on%20two%20clinically%20relevant%203D%20datasets.%20On%0Aa%20six-class%20spinal%20metastasis%20dataset%20%28n%3D1%2C448%29%2C%20DeSamba%20achieves%2062.10%25%20Top-1%0Aaccuracy%2C%2063.62%25%20F1-score%2C%2087.71%25%20AUC%2C%20and%2093.55%25%20Top-3%20accuracy%20on%20an%20external%0Avalidation%20set%20%28n%3D372%29%2C%20outperforming%20all%20state-of-the-art%20%28SOTA%29%20baselines.%20On%0Aa%20spondylitis%20dataset%20%28n%3D251%29%20involving%20a%20challenging%20binary%20classification%0Atask%2C%20DeSamba%20achieves%2070.00%25/64.52%25%20accuracy%20and%2074.75/73.88%20AUC%20on%20internal%0Aand%20external%20validation%20sets%2C%20respectively.%20Ablation%20studies%20demonstrate%20that%0Aboth%20DRLM%20and%20SAMB%20significantly%20contribute%20to%20overall%20performance%2C%20with%20over%0A10%25%20relative%20improvement%20compared%20to%20the%20baseline.%20Our%20results%20highlight%20the%0Apotential%20of%20DeSamba%20as%20a%20generalizable%20and%20effective%20solution%20for%203D%20lesion%0Aclassification%20in%20multi-sequence%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15487v1&entry.124074799=Read"},
{"title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers\n  for 3DGS", "author": "Jisu Shin and Richard Shaw and Seunghyun Shin and Anton Pelykh and Zhensong Zhang and Hae-Gon Jeon and Eduardo Perez-Pellitero", "abstract": "  Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.\n", "link": "http://arxiv.org/abs/2507.15748v1", "date": "2025-07-21", "relevancy": 2.5912, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6834}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.623}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Appearance%20Harmonization%20via%20Bilateral%20Grid%20Prediction%20with%20Transformers%0A%20%20for%203DGS&body=Title%3A%20Appearance%20Harmonization%20via%20Bilateral%20Grid%20Prediction%20with%20Transformers%0A%20%20for%203DGS%0AAuthor%3A%20Jisu%20Shin%20and%20Richard%20Shaw%20and%20Seunghyun%20Shin%20and%20Anton%20Pelykh%20and%20Zhensong%20Zhang%20and%20Hae-Gon%20Jeon%20and%20Eduardo%20Perez-Pellitero%0AAbstract%3A%20%20%20Modern%20camera%20pipelines%20apply%20extensive%20on-device%20processing%2C%20such%20as%0Aexposure%20adjustment%2C%20white%20balance%2C%20and%20color%20correction%2C%20which%2C%20while%0Abeneficial%20individually%2C%20often%20introduce%20photometric%20inconsistencies%20across%0Aviews.%20These%20appearance%20variations%20violate%20multi-view%20consistency%20and%20degrade%0Athe%20quality%20of%20novel%20view%20synthesis.%20Joint%20optimization%20of%20scene%0Arepresentations%20and%20per-image%20appearance%20embeddings%20has%20been%20proposed%20to%0Aaddress%20this%20issue%2C%20but%20at%20the%20cost%20of%20increased%20computational%20complexity%20and%0Aslower%20training.%20In%20this%20work%2C%20we%20propose%20a%20transformer-based%20method%20that%0Apredicts%20spatially%20adaptive%20bilateral%20grids%20to%20correct%20photometric%20variations%0Ain%20a%20multi-view%20consistent%20manner%2C%20enabling%20robust%20cross-scene%20generalization%0Awithout%20the%20need%20for%20scene-specific%20retraining.%20By%20incorporating%20the%20learned%0Agrids%20into%20the%203D%20Gaussian%20Splatting%20pipeline%2C%20we%20improve%20reconstruction%0Aquality%20while%20maintaining%20high%20training%20efficiency.%20Extensive%20experiments%20show%0Athat%20our%20approach%20outperforms%20or%20matches%20existing%20scene-specific%20optimization%0Amethods%20in%20reconstruction%20fidelity%20and%20convergence%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAppearance%2520Harmonization%2520via%2520Bilateral%2520Grid%2520Prediction%2520with%2520Transformers%250A%2520%2520for%25203DGS%26entry.906535625%3DJisu%2520Shin%2520and%2520Richard%2520Shaw%2520and%2520Seunghyun%2520Shin%2520and%2520Anton%2520Pelykh%2520and%2520Zhensong%2520Zhang%2520and%2520Hae-Gon%2520Jeon%2520and%2520Eduardo%2520Perez-Pellitero%26entry.1292438233%3D%2520%2520Modern%2520camera%2520pipelines%2520apply%2520extensive%2520on-device%2520processing%252C%2520such%2520as%250Aexposure%2520adjustment%252C%2520white%2520balance%252C%2520and%2520color%2520correction%252C%2520which%252C%2520while%250Abeneficial%2520individually%252C%2520often%2520introduce%2520photometric%2520inconsistencies%2520across%250Aviews.%2520These%2520appearance%2520variations%2520violate%2520multi-view%2520consistency%2520and%2520degrade%250Athe%2520quality%2520of%2520novel%2520view%2520synthesis.%2520Joint%2520optimization%2520of%2520scene%250Arepresentations%2520and%2520per-image%2520appearance%2520embeddings%2520has%2520been%2520proposed%2520to%250Aaddress%2520this%2520issue%252C%2520but%2520at%2520the%2520cost%2520of%2520increased%2520computational%2520complexity%2520and%250Aslower%2520training.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520transformer-based%2520method%2520that%250Apredicts%2520spatially%2520adaptive%2520bilateral%2520grids%2520to%2520correct%2520photometric%2520variations%250Ain%2520a%2520multi-view%2520consistent%2520manner%252C%2520enabling%2520robust%2520cross-scene%2520generalization%250Awithout%2520the%2520need%2520for%2520scene-specific%2520retraining.%2520By%2520incorporating%2520the%2520learned%250Agrids%2520into%2520the%25203D%2520Gaussian%2520Splatting%2520pipeline%252C%2520we%2520improve%2520reconstruction%250Aquality%2520while%2520maintaining%2520high%2520training%2520efficiency.%2520Extensive%2520experiments%2520show%250Athat%2520our%2520approach%2520outperforms%2520or%2520matches%2520existing%2520scene-specific%2520optimization%250Amethods%2520in%2520reconstruction%2520fidelity%2520and%2520convergence%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Appearance%20Harmonization%20via%20Bilateral%20Grid%20Prediction%20with%20Transformers%0A%20%20for%203DGS&entry.906535625=Jisu%20Shin%20and%20Richard%20Shaw%20and%20Seunghyun%20Shin%20and%20Anton%20Pelykh%20and%20Zhensong%20Zhang%20and%20Hae-Gon%20Jeon%20and%20Eduardo%20Perez-Pellitero&entry.1292438233=%20%20Modern%20camera%20pipelines%20apply%20extensive%20on-device%20processing%2C%20such%20as%0Aexposure%20adjustment%2C%20white%20balance%2C%20and%20color%20correction%2C%20which%2C%20while%0Abeneficial%20individually%2C%20often%20introduce%20photometric%20inconsistencies%20across%0Aviews.%20These%20appearance%20variations%20violate%20multi-view%20consistency%20and%20degrade%0Athe%20quality%20of%20novel%20view%20synthesis.%20Joint%20optimization%20of%20scene%0Arepresentations%20and%20per-image%20appearance%20embeddings%20has%20been%20proposed%20to%0Aaddress%20this%20issue%2C%20but%20at%20the%20cost%20of%20increased%20computational%20complexity%20and%0Aslower%20training.%20In%20this%20work%2C%20we%20propose%20a%20transformer-based%20method%20that%0Apredicts%20spatially%20adaptive%20bilateral%20grids%20to%20correct%20photometric%20variations%0Ain%20a%20multi-view%20consistent%20manner%2C%20enabling%20robust%20cross-scene%20generalization%0Awithout%20the%20need%20for%20scene-specific%20retraining.%20By%20incorporating%20the%20learned%0Agrids%20into%20the%203D%20Gaussian%20Splatting%20pipeline%2C%20we%20improve%20reconstruction%0Aquality%20while%20maintaining%20high%20training%20efficiency.%20Extensive%20experiments%20show%0Athat%20our%20approach%20outperforms%20or%20matches%20existing%20scene-specific%20optimization%0Amethods%20in%20reconstruction%20fidelity%20and%20convergence%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15748v1&entry.124074799=Read"},
{"title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from\n  Global Prompts Using a Monocular Foundation Model", "author": "Osher Rafaeli and Tal Svoray and Ariel Nahlieli", "abstract": "  High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.\n", "link": "http://arxiv.org/abs/2507.09681v2", "date": "2025-07-21", "relevancy": 2.582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5147}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt2DEM%3A%20High-Resolution%20DEMs%20for%20Urban%20and%20Open%20Environments%20from%0A%20%20Global%20Prompts%20Using%20a%20Monocular%20Foundation%20Model&body=Title%3A%20Prompt2DEM%3A%20High-Resolution%20DEMs%20for%20Urban%20and%20Open%20Environments%20from%0A%20%20Global%20Prompts%20Using%20a%20Monocular%20Foundation%20Model%0AAuthor%3A%20Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli%0AAbstract%3A%20%20%20High-resolution%20elevation%20estimations%20are%20essential%20to%20understand%20catchment%0Aand%20hillslope%20hydrology%2C%20study%20urban%20morphology%20and%20dynamics%2C%20and%20monitor%20the%0Agrowth%2C%20decline%2C%20and%20mortality%20of%20terrestrial%20ecosystems.%20Various%20deep%20learning%0Aapproaches%20%28e.g.%2C%20super-resolution%20techniques%2C%20monocular%20depth%20estimation%29%20have%0Abeen%20developed%20to%20create%20high-resolution%20Digital%20Elevation%20Models%20%28DEMs%29.%0AHowever%2C%20super-resolution%20techniques%20are%20limited%20by%20the%20upscaling%20factor%2C%20and%0Amonocular%20depth%20estimation%20lacks%20global%20elevation%20context%2C%20making%20its%0Aconversion%20to%20a%20seamless%20DEM%20restricted.%20The%20recently%20introduced%20technique%20of%0Aprompt-based%20monocular%20depth%20estimation%20has%20opened%20new%20opportunities%20to%20extract%0Aestimates%20of%20absolute%20elevation%20in%20a%20global%20context.%20We%20present%20here%20a%0Aframework%20for%20the%20estimation%20of%20high-resolution%20DEMs%20as%20a%20new%20paradigm%20for%0Aabsolute%20global%20elevation%20mapping.%20It%20is%20exemplified%20using%20low-resolution%0AShuttle%20Radar%20Topography%20Mission%20%28SRTM%29%20elevation%20data%20as%20prompts%20and%0Ahigh-resolution%20RGB%20imagery%20from%20the%20National%20Agriculture%20Imagery%20Program%0A%28NAIP%29.%20The%20approach%20fine-tunes%20a%20vision%20transformer%20encoder%20with%20LiDAR-derived%0ADEMs%20and%20employs%20a%20versatile%20prompting%20strategy%2C%20enabling%20tasks%20such%20as%20DEM%0Aestimation%2C%20void%20filling%2C%20and%20updating.%20Our%20framework%20achieves%20a%20100x%0Aresolution%20gain%20%28from%2030-m%20to%2030-cm%29%2C%20surpassing%20prior%20methods%20by%20an%20order%20of%0Amagnitude.%20Evaluations%20across%20three%20diverse%20U.S.%20landscapes%20show%20robust%0Ageneralization%2C%20capturing%20urban%20structures%20and%20fine-scale%20terrain%20features%20with%0A%3C%205%20m%20MAE%20relative%20to%20LiDAR%2C%20improving%20over%20SRTM%20by%20up%20to%2018%25.%20Hydrological%0Aanalysis%20confirms%20suitability%20for%20hazard%20and%20environmental%20studies.%20We%0Ademonstrate%20scalability%20by%20applying%20the%20framework%20to%20large%20regions%20in%20the%20U.S.%0Aand%20Israel.%20All%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//osherr1996.github.io/prompt2dem_propage/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt2DEM%253A%2520High-Resolution%2520DEMs%2520for%2520Urban%2520and%2520Open%2520Environments%2520from%250A%2520%2520Global%2520Prompts%2520Using%2520a%2520Monocular%2520Foundation%2520Model%26entry.906535625%3DOsher%2520Rafaeli%2520and%2520Tal%2520Svoray%2520and%2520Ariel%2520Nahlieli%26entry.1292438233%3D%2520%2520High-resolution%2520elevation%2520estimations%2520are%2520essential%2520to%2520understand%2520catchment%250Aand%2520hillslope%2520hydrology%252C%2520study%2520urban%2520morphology%2520and%2520dynamics%252C%2520and%2520monitor%2520the%250Agrowth%252C%2520decline%252C%2520and%2520mortality%2520of%2520terrestrial%2520ecosystems.%2520Various%2520deep%2520learning%250Aapproaches%2520%2528e.g.%252C%2520super-resolution%2520techniques%252C%2520monocular%2520depth%2520estimation%2529%2520have%250Abeen%2520developed%2520to%2520create%2520high-resolution%2520Digital%2520Elevation%2520Models%2520%2528DEMs%2529.%250AHowever%252C%2520super-resolution%2520techniques%2520are%2520limited%2520by%2520the%2520upscaling%2520factor%252C%2520and%250Amonocular%2520depth%2520estimation%2520lacks%2520global%2520elevation%2520context%252C%2520making%2520its%250Aconversion%2520to%2520a%2520seamless%2520DEM%2520restricted.%2520The%2520recently%2520introduced%2520technique%2520of%250Aprompt-based%2520monocular%2520depth%2520estimation%2520has%2520opened%2520new%2520opportunities%2520to%2520extract%250Aestimates%2520of%2520absolute%2520elevation%2520in%2520a%2520global%2520context.%2520We%2520present%2520here%2520a%250Aframework%2520for%2520the%2520estimation%2520of%2520high-resolution%2520DEMs%2520as%2520a%2520new%2520paradigm%2520for%250Aabsolute%2520global%2520elevation%2520mapping.%2520It%2520is%2520exemplified%2520using%2520low-resolution%250AShuttle%2520Radar%2520Topography%2520Mission%2520%2528SRTM%2529%2520elevation%2520data%2520as%2520prompts%2520and%250Ahigh-resolution%2520RGB%2520imagery%2520from%2520the%2520National%2520Agriculture%2520Imagery%2520Program%250A%2528NAIP%2529.%2520The%2520approach%2520fine-tunes%2520a%2520vision%2520transformer%2520encoder%2520with%2520LiDAR-derived%250ADEMs%2520and%2520employs%2520a%2520versatile%2520prompting%2520strategy%252C%2520enabling%2520tasks%2520such%2520as%2520DEM%250Aestimation%252C%2520void%2520filling%252C%2520and%2520updating.%2520Our%2520framework%2520achieves%2520a%2520100x%250Aresolution%2520gain%2520%2528from%252030-m%2520to%252030-cm%2529%252C%2520surpassing%2520prior%2520methods%2520by%2520an%2520order%2520of%250Amagnitude.%2520Evaluations%2520across%2520three%2520diverse%2520U.S.%2520landscapes%2520show%2520robust%250Ageneralization%252C%2520capturing%2520urban%2520structures%2520and%2520fine-scale%2520terrain%2520features%2520with%250A%253C%25205%2520m%2520MAE%2520relative%2520to%2520LiDAR%252C%2520improving%2520over%2520SRTM%2520by%2520up%2520to%252018%2525.%2520Hydrological%250Aanalysis%2520confirms%2520suitability%2520for%2520hazard%2520and%2520environmental%2520studies.%2520We%250Ademonstrate%2520scalability%2520by%2520applying%2520the%2520framework%2520to%2520large%2520regions%2520in%2520the%2520U.S.%250Aand%2520Israel.%2520All%2520code%2520and%2520pretrained%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//osherr1996.github.io/prompt2dem_propage/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt2DEM%3A%20High-Resolution%20DEMs%20for%20Urban%20and%20Open%20Environments%20from%0A%20%20Global%20Prompts%20Using%20a%20Monocular%20Foundation%20Model&entry.906535625=Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Ariel%20Nahlieli&entry.1292438233=%20%20High-resolution%20elevation%20estimations%20are%20essential%20to%20understand%20catchment%0Aand%20hillslope%20hydrology%2C%20study%20urban%20morphology%20and%20dynamics%2C%20and%20monitor%20the%0Agrowth%2C%20decline%2C%20and%20mortality%20of%20terrestrial%20ecosystems.%20Various%20deep%20learning%0Aapproaches%20%28e.g.%2C%20super-resolution%20techniques%2C%20monocular%20depth%20estimation%29%20have%0Abeen%20developed%20to%20create%20high-resolution%20Digital%20Elevation%20Models%20%28DEMs%29.%0AHowever%2C%20super-resolution%20techniques%20are%20limited%20by%20the%20upscaling%20factor%2C%20and%0Amonocular%20depth%20estimation%20lacks%20global%20elevation%20context%2C%20making%20its%0Aconversion%20to%20a%20seamless%20DEM%20restricted.%20The%20recently%20introduced%20technique%20of%0Aprompt-based%20monocular%20depth%20estimation%20has%20opened%20new%20opportunities%20to%20extract%0Aestimates%20of%20absolute%20elevation%20in%20a%20global%20context.%20We%20present%20here%20a%0Aframework%20for%20the%20estimation%20of%20high-resolution%20DEMs%20as%20a%20new%20paradigm%20for%0Aabsolute%20global%20elevation%20mapping.%20It%20is%20exemplified%20using%20low-resolution%0AShuttle%20Radar%20Topography%20Mission%20%28SRTM%29%20elevation%20data%20as%20prompts%20and%0Ahigh-resolution%20RGB%20imagery%20from%20the%20National%20Agriculture%20Imagery%20Program%0A%28NAIP%29.%20The%20approach%20fine-tunes%20a%20vision%20transformer%20encoder%20with%20LiDAR-derived%0ADEMs%20and%20employs%20a%20versatile%20prompting%20strategy%2C%20enabling%20tasks%20such%20as%20DEM%0Aestimation%2C%20void%20filling%2C%20and%20updating.%20Our%20framework%20achieves%20a%20100x%0Aresolution%20gain%20%28from%2030-m%20to%2030-cm%29%2C%20surpassing%20prior%20methods%20by%20an%20order%20of%0Amagnitude.%20Evaluations%20across%20three%20diverse%20U.S.%20landscapes%20show%20robust%0Ageneralization%2C%20capturing%20urban%20structures%20and%20fine-scale%20terrain%20features%20with%0A%3C%205%20m%20MAE%20relative%20to%20LiDAR%2C%20improving%20over%20SRTM%20by%20up%20to%2018%25.%20Hydrological%0Aanalysis%20confirms%20suitability%20for%20hazard%20and%20environmental%20studies.%20We%0Ademonstrate%20scalability%20by%20applying%20the%20framework%20to%20large%20regions%20in%20the%20U.S.%0Aand%20Israel.%20All%20code%20and%20pretrained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//osherr1996.github.io/prompt2dem_propage/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09681v2&entry.124074799=Read"},
{"title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "author": "Wenqi Ouyang and Zeqi Xiao and Danni Yang and Yifan Zhou and Shuai Yang and Lei Yang and Jianlou Si and Xingang Pan", "abstract": "  Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .\n", "link": "http://arxiv.org/abs/2507.15728v1", "date": "2025-07-21", "relevancy": 2.5659, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6667}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6303}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokensGen%3A%20Harnessing%20Condensed%20Tokens%20for%20Long%20Video%20Generation&body=Title%3A%20TokensGen%3A%20Harnessing%20Condensed%20Tokens%20for%20Long%20Video%20Generation%0AAuthor%3A%20Wenqi%20Ouyang%20and%20Zeqi%20Xiao%20and%20Danni%20Yang%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Lei%20Yang%20and%20Jianlou%20Si%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20Generating%20consistent%20long%20videos%20is%20a%20complex%20challenge%3A%20while%0Adiffusion-based%20generative%20models%20generate%20visually%20impressive%20short%20clips%2C%0Aextending%20them%20to%20longer%20durations%20often%20leads%20to%20memory%20bottlenecks%20and%0Along-term%20inconsistency.%20In%20this%20paper%2C%20we%20propose%20TokensGen%2C%20a%20novel%20two-stage%0Aframework%20that%20leverages%20condensed%20tokens%20to%20address%20these%20issues.%20Our%20method%0Adecomposes%20long%20video%20generation%20into%20three%20core%20tasks%3A%20%281%29%20inner-clip%20semantic%0Acontrol%2C%20%282%29%20long-term%20consistency%20control%2C%20and%20%283%29%20inter-clip%20smooth%0Atransition.%20First%2C%20we%20train%20To2V%20%28Token-to-Video%29%2C%20a%20short%20video%20diffusion%0Amodel%20guided%20by%20text%20and%20video%20tokens%2C%20with%20a%20Video%20Tokenizer%20that%20condenses%0Ashort%20clips%20into%20semantically%20rich%20tokens.%20Second%2C%20we%20introduce%20T2To%0A%28Text-to-Token%29%2C%20a%20video%20token%20diffusion%20transformer%20that%20generates%20all%20tokens%0Aat%20once%2C%20ensuring%20global%20consistency%20across%20clips.%20Finally%2C%20during%20inference%2C%0Aan%20adaptive%20FIFO-Diffusion%20strategy%20seamlessly%20connects%20adjacent%20clips%2C%0Areducing%20boundary%20artifacts%20and%20enhancing%20smooth%20transitions.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20significantly%20enhances%20long-term%20temporal%0Aand%20content%20coherence%20without%20incurring%20prohibitive%20computational%20overhead.%20By%0Aleveraging%20condensed%20tokens%20and%20pre-trained%20short%20video%20models%2C%20our%20method%0Aprovides%20a%20scalable%2C%20modular%20solution%20for%20long%20video%20generation%2C%20opening%20new%0Apossibilities%20for%20storytelling%2C%20cinematic%20production%2C%20and%20immersive%0Asimulations.%20Please%20see%20our%20project%20page%20at%0Ahttps%3A//vicky0522.github.io/tokensgen-webpage/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokensGen%253A%2520Harnessing%2520Condensed%2520Tokens%2520for%2520Long%2520Video%2520Generation%26entry.906535625%3DWenqi%2520Ouyang%2520and%2520Zeqi%2520Xiao%2520and%2520Danni%2520Yang%2520and%2520Yifan%2520Zhou%2520and%2520Shuai%2520Yang%2520and%2520Lei%2520Yang%2520and%2520Jianlou%2520Si%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520Generating%2520consistent%2520long%2520videos%2520is%2520a%2520complex%2520challenge%253A%2520while%250Adiffusion-based%2520generative%2520models%2520generate%2520visually%2520impressive%2520short%2520clips%252C%250Aextending%2520them%2520to%2520longer%2520durations%2520often%2520leads%2520to%2520memory%2520bottlenecks%2520and%250Along-term%2520inconsistency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TokensGen%252C%2520a%2520novel%2520two-stage%250Aframework%2520that%2520leverages%2520condensed%2520tokens%2520to%2520address%2520these%2520issues.%2520Our%2520method%250Adecomposes%2520long%2520video%2520generation%2520into%2520three%2520core%2520tasks%253A%2520%25281%2529%2520inner-clip%2520semantic%250Acontrol%252C%2520%25282%2529%2520long-term%2520consistency%2520control%252C%2520and%2520%25283%2529%2520inter-clip%2520smooth%250Atransition.%2520First%252C%2520we%2520train%2520To2V%2520%2528Token-to-Video%2529%252C%2520a%2520short%2520video%2520diffusion%250Amodel%2520guided%2520by%2520text%2520and%2520video%2520tokens%252C%2520with%2520a%2520Video%2520Tokenizer%2520that%2520condenses%250Ashort%2520clips%2520into%2520semantically%2520rich%2520tokens.%2520Second%252C%2520we%2520introduce%2520T2To%250A%2528Text-to-Token%2529%252C%2520a%2520video%2520token%2520diffusion%2520transformer%2520that%2520generates%2520all%2520tokens%250Aat%2520once%252C%2520ensuring%2520global%2520consistency%2520across%2520clips.%2520Finally%252C%2520during%2520inference%252C%250Aan%2520adaptive%2520FIFO-Diffusion%2520strategy%2520seamlessly%2520connects%2520adjacent%2520clips%252C%250Areducing%2520boundary%2520artifacts%2520and%2520enhancing%2520smooth%2520transitions.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520long-term%2520temporal%250Aand%2520content%2520coherence%2520without%2520incurring%2520prohibitive%2520computational%2520overhead.%2520By%250Aleveraging%2520condensed%2520tokens%2520and%2520pre-trained%2520short%2520video%2520models%252C%2520our%2520method%250Aprovides%2520a%2520scalable%252C%2520modular%2520solution%2520for%2520long%2520video%2520generation%252C%2520opening%2520new%250Apossibilities%2520for%2520storytelling%252C%2520cinematic%2520production%252C%2520and%2520immersive%250Asimulations.%2520Please%2520see%2520our%2520project%2520page%2520at%250Ahttps%253A//vicky0522.github.io/tokensgen-webpage/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokensGen%3A%20Harnessing%20Condensed%20Tokens%20for%20Long%20Video%20Generation&entry.906535625=Wenqi%20Ouyang%20and%20Zeqi%20Xiao%20and%20Danni%20Yang%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Lei%20Yang%20and%20Jianlou%20Si%20and%20Xingang%20Pan&entry.1292438233=%20%20Generating%20consistent%20long%20videos%20is%20a%20complex%20challenge%3A%20while%0Adiffusion-based%20generative%20models%20generate%20visually%20impressive%20short%20clips%2C%0Aextending%20them%20to%20longer%20durations%20often%20leads%20to%20memory%20bottlenecks%20and%0Along-term%20inconsistency.%20In%20this%20paper%2C%20we%20propose%20TokensGen%2C%20a%20novel%20two-stage%0Aframework%20that%20leverages%20condensed%20tokens%20to%20address%20these%20issues.%20Our%20method%0Adecomposes%20long%20video%20generation%20into%20three%20core%20tasks%3A%20%281%29%20inner-clip%20semantic%0Acontrol%2C%20%282%29%20long-term%20consistency%20control%2C%20and%20%283%29%20inter-clip%20smooth%0Atransition.%20First%2C%20we%20train%20To2V%20%28Token-to-Video%29%2C%20a%20short%20video%20diffusion%0Amodel%20guided%20by%20text%20and%20video%20tokens%2C%20with%20a%20Video%20Tokenizer%20that%20condenses%0Ashort%20clips%20into%20semantically%20rich%20tokens.%20Second%2C%20we%20introduce%20T2To%0A%28Text-to-Token%29%2C%20a%20video%20token%20diffusion%20transformer%20that%20generates%20all%20tokens%0Aat%20once%2C%20ensuring%20global%20consistency%20across%20clips.%20Finally%2C%20during%20inference%2C%0Aan%20adaptive%20FIFO-Diffusion%20strategy%20seamlessly%20connects%20adjacent%20clips%2C%0Areducing%20boundary%20artifacts%20and%20enhancing%20smooth%20transitions.%20Experimental%0Aresults%20demonstrate%20that%20our%20approach%20significantly%20enhances%20long-term%20temporal%0Aand%20content%20coherence%20without%20incurring%20prohibitive%20computational%20overhead.%20By%0Aleveraging%20condensed%20tokens%20and%20pre-trained%20short%20video%20models%2C%20our%20method%0Aprovides%20a%20scalable%2C%20modular%20solution%20for%20long%20video%20generation%2C%20opening%20new%0Apossibilities%20for%20storytelling%2C%20cinematic%20production%2C%20and%20immersive%0Asimulations.%20Please%20see%20our%20project%20page%20at%0Ahttps%3A//vicky0522.github.io/tokensgen-webpage/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15728v1&entry.124074799=Read"},
{"title": "Gradient-Guided Annealing for Domain Generalization", "author": "Aristotelis Ballas and Christos Diou", "abstract": "  Domain Generalization (DG) research has gained considerable traction as of\nlate, since the ability to generalize to unseen data distributions is a\nrequirement that eludes even state-of-the-art training algorithms. In this\npaper we observe that the initial iterations of model training play a key role\nin domain generalization effectiveness, since the loss landscape may be\nsignificantly different across the training and test distributions, contrary to\nthe case of i.i.d. data. Conflicts between gradients of the loss components of\neach domain lead the optimization procedure to undesirable local minima that do\nnot capture the domain-invariant features of the target classes. We propose\nalleviating domain conflicts in model optimization, by iteratively annealing\nthe parameters of a model in the early stages of training and searching for\npoints where gradients align between domains. By discovering a set of parameter\nvalues where gradients are updated towards the same direction for each data\ndistribution present in the training set, the proposed Gradient-Guided\nAnnealing (GGA) algorithm encourages models to seek out minima that exhibit\nimproved robustness against domain shifts. The efficacy of GGA is evaluated on\nfive widely accepted and challenging image classification domain generalization\nbenchmarks, where its use alone is able to establish highly competitive or even\nstate-of-the-art performance. Moreover, when combined with previously proposed\ndomain-generalization algorithms it is able to consistently improve their\neffectiveness by significant margins.\n", "link": "http://arxiv.org/abs/2502.20162v7", "date": "2025-07-21", "relevancy": 2.5626, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5069}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient-Guided%20Annealing%20for%20Domain%20Generalization&body=Title%3A%20Gradient-Guided%20Annealing%20for%20Domain%20Generalization%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%20research%20has%20gained%20considerable%20traction%20as%20of%0Alate%2C%20since%20the%20ability%20to%20generalize%20to%20unseen%20data%20distributions%20is%20a%0Arequirement%20that%20eludes%20even%20state-of-the-art%20training%20algorithms.%20In%20this%0Apaper%20we%20observe%20that%20the%20initial%20iterations%20of%20model%20training%20play%20a%20key%20role%0Ain%20domain%20generalization%20effectiveness%2C%20since%20the%20loss%20landscape%20may%20be%0Asignificantly%20different%20across%20the%20training%20and%20test%20distributions%2C%20contrary%20to%0Athe%20case%20of%20i.i.d.%20data.%20Conflicts%20between%20gradients%20of%20the%20loss%20components%20of%0Aeach%20domain%20lead%20the%20optimization%20procedure%20to%20undesirable%20local%20minima%20that%20do%0Anot%20capture%20the%20domain-invariant%20features%20of%20the%20target%20classes.%20We%20propose%0Aalleviating%20domain%20conflicts%20in%20model%20optimization%2C%20by%20iteratively%20annealing%0Athe%20parameters%20of%20a%20model%20in%20the%20early%20stages%20of%20training%20and%20searching%20for%0Apoints%20where%20gradients%20align%20between%20domains.%20By%20discovering%20a%20set%20of%20parameter%0Avalues%20where%20gradients%20are%20updated%20towards%20the%20same%20direction%20for%20each%20data%0Adistribution%20present%20in%20the%20training%20set%2C%20the%20proposed%20Gradient-Guided%0AAnnealing%20%28GGA%29%20algorithm%20encourages%20models%20to%20seek%20out%20minima%20that%20exhibit%0Aimproved%20robustness%20against%20domain%20shifts.%20The%20efficacy%20of%20GGA%20is%20evaluated%20on%0Afive%20widely%20accepted%20and%20challenging%20image%20classification%20domain%20generalization%0Abenchmarks%2C%20where%20its%20use%20alone%20is%20able%20to%20establish%20highly%20competitive%20or%20even%0Astate-of-the-art%20performance.%20Moreover%2C%20when%20combined%20with%20previously%20proposed%0Adomain-generalization%20algorithms%20it%20is%20able%20to%20consistently%20improve%20their%0Aeffectiveness%20by%20significant%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20162v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient-Guided%2520Annealing%2520for%2520Domain%2520Generalization%26entry.906535625%3DAristotelis%2520Ballas%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%2520research%2520has%2520gained%2520considerable%2520traction%2520as%2520of%250Alate%252C%2520since%2520the%2520ability%2520to%2520generalize%2520to%2520unseen%2520data%2520distributions%2520is%2520a%250Arequirement%2520that%2520eludes%2520even%2520state-of-the-art%2520training%2520algorithms.%2520In%2520this%250Apaper%2520we%2520observe%2520that%2520the%2520initial%2520iterations%2520of%2520model%2520training%2520play%2520a%2520key%2520role%250Ain%2520domain%2520generalization%2520effectiveness%252C%2520since%2520the%2520loss%2520landscape%2520may%2520be%250Asignificantly%2520different%2520across%2520the%2520training%2520and%2520test%2520distributions%252C%2520contrary%2520to%250Athe%2520case%2520of%2520i.i.d.%2520data.%2520Conflicts%2520between%2520gradients%2520of%2520the%2520loss%2520components%2520of%250Aeach%2520domain%2520lead%2520the%2520optimization%2520procedure%2520to%2520undesirable%2520local%2520minima%2520that%2520do%250Anot%2520capture%2520the%2520domain-invariant%2520features%2520of%2520the%2520target%2520classes.%2520We%2520propose%250Aalleviating%2520domain%2520conflicts%2520in%2520model%2520optimization%252C%2520by%2520iteratively%2520annealing%250Athe%2520parameters%2520of%2520a%2520model%2520in%2520the%2520early%2520stages%2520of%2520training%2520and%2520searching%2520for%250Apoints%2520where%2520gradients%2520align%2520between%2520domains.%2520By%2520discovering%2520a%2520set%2520of%2520parameter%250Avalues%2520where%2520gradients%2520are%2520updated%2520towards%2520the%2520same%2520direction%2520for%2520each%2520data%250Adistribution%2520present%2520in%2520the%2520training%2520set%252C%2520the%2520proposed%2520Gradient-Guided%250AAnnealing%2520%2528GGA%2529%2520algorithm%2520encourages%2520models%2520to%2520seek%2520out%2520minima%2520that%2520exhibit%250Aimproved%2520robustness%2520against%2520domain%2520shifts.%2520The%2520efficacy%2520of%2520GGA%2520is%2520evaluated%2520on%250Afive%2520widely%2520accepted%2520and%2520challenging%2520image%2520classification%2520domain%2520generalization%250Abenchmarks%252C%2520where%2520its%2520use%2520alone%2520is%2520able%2520to%2520establish%2520highly%2520competitive%2520or%2520even%250Astate-of-the-art%2520performance.%2520Moreover%252C%2520when%2520combined%2520with%2520previously%2520proposed%250Adomain-generalization%2520algorithms%2520it%2520is%2520able%2520to%2520consistently%2520improve%2520their%250Aeffectiveness%2520by%2520significant%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20162v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Guided%20Annealing%20for%20Domain%20Generalization&entry.906535625=Aristotelis%20Ballas%20and%20Christos%20Diou&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%20research%20has%20gained%20considerable%20traction%20as%20of%0Alate%2C%20since%20the%20ability%20to%20generalize%20to%20unseen%20data%20distributions%20is%20a%0Arequirement%20that%20eludes%20even%20state-of-the-art%20training%20algorithms.%20In%20this%0Apaper%20we%20observe%20that%20the%20initial%20iterations%20of%20model%20training%20play%20a%20key%20role%0Ain%20domain%20generalization%20effectiveness%2C%20since%20the%20loss%20landscape%20may%20be%0Asignificantly%20different%20across%20the%20training%20and%20test%20distributions%2C%20contrary%20to%0Athe%20case%20of%20i.i.d.%20data.%20Conflicts%20between%20gradients%20of%20the%20loss%20components%20of%0Aeach%20domain%20lead%20the%20optimization%20procedure%20to%20undesirable%20local%20minima%20that%20do%0Anot%20capture%20the%20domain-invariant%20features%20of%20the%20target%20classes.%20We%20propose%0Aalleviating%20domain%20conflicts%20in%20model%20optimization%2C%20by%20iteratively%20annealing%0Athe%20parameters%20of%20a%20model%20in%20the%20early%20stages%20of%20training%20and%20searching%20for%0Apoints%20where%20gradients%20align%20between%20domains.%20By%20discovering%20a%20set%20of%20parameter%0Avalues%20where%20gradients%20are%20updated%20towards%20the%20same%20direction%20for%20each%20data%0Adistribution%20present%20in%20the%20training%20set%2C%20the%20proposed%20Gradient-Guided%0AAnnealing%20%28GGA%29%20algorithm%20encourages%20models%20to%20seek%20out%20minima%20that%20exhibit%0Aimproved%20robustness%20against%20domain%20shifts.%20The%20efficacy%20of%20GGA%20is%20evaluated%20on%0Afive%20widely%20accepted%20and%20challenging%20image%20classification%20domain%20generalization%0Abenchmarks%2C%20where%20its%20use%20alone%20is%20able%20to%20establish%20highly%20competitive%20or%20even%0Astate-of-the-art%20performance.%20Moreover%2C%20when%20combined%20with%20previously%20proposed%0Adomain-generalization%20algorithms%20it%20is%20able%20to%20consistently%20improve%20their%0Aeffectiveness%20by%20significant%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20162v7&entry.124074799=Read"},
{"title": "Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for\n  motion modelling in radiotherapy: beyond classic voxel-based methods", "author": "Xia Li and Runzhao Yang and Muheng Li and Xiangtai Li and Antony J. Lomax and Joachim M. Buhmann and Ye Zhang", "abstract": "  Deformable image registration (DIR) is a crucial tool in radiotherapy for\nanalyzing anatomical changes and motion patterns. Current DIR implementations\nrely on discrete volumetric motion representation, which often leads to\ncompromised accuracy and uncertainty when handling significant anatomical\nchanges and sliding boundaries. This limitation affects the reliability of\nsubsequent contour propagation and dose accumulation procedures, particularly\nin regions with complex anatomical interfaces such as the lung-chest wall\nboundary. Given that organ motion is inherently a continuous process in both\nspace and time, we aimed to develop a model that preserves these fundamental\nproperties. Drawing inspiration from fluid mechanics, we propose a novel\napproach using implicit neural representation (INR) for continuous modeling of\npatient anatomical motion. This approach ensures spatial and temporal\ncontinuity while effectively unifying Eulerian and Lagrangian specifications to\nenable natural continuous motion modeling and frame interpolation. The\nintegration of these specifications provides a more comprehensive understanding\nof anatomical deformation patterns. By leveraging the continuous\nrepresentations, the CPT-DIR method significantly enhances registration and\ninterpolation accuracy, automation, and speed. The method demonstrates superior\nperformance in landmark and contour precision, particularly in challenging\nanatomical regions, representing a substantial advancement over conventional\napproaches in deformable image registration. The improved efficiency and\naccuracy of CPT-DIR make it particularly suitable for real-time adaptive\nradiotherapy applications.\n", "link": "http://arxiv.org/abs/2405.00430v2", "date": "2025-07-21", "relevancy": 2.5605, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5016}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods&body=Title%3A%20Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods%0AAuthor%3A%20Xia%20Li%20and%20Runzhao%20Yang%20and%20Muheng%20Li%20and%20Xiangtai%20Li%20and%20Antony%20J.%20Lomax%20and%20Joachim%20M.%20Buhmann%20and%20Ye%20Zhang%0AAbstract%3A%20%20%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20crucial%20tool%20in%20radiotherapy%20for%0Aanalyzing%20anatomical%20changes%20and%20motion%20patterns.%20Current%20DIR%20implementations%0Arely%20on%20discrete%20volumetric%20motion%20representation%2C%20which%20often%20leads%20to%0Acompromised%20accuracy%20and%20uncertainty%20when%20handling%20significant%20anatomical%0Achanges%20and%20sliding%20boundaries.%20This%20limitation%20affects%20the%20reliability%20of%0Asubsequent%20contour%20propagation%20and%20dose%20accumulation%20procedures%2C%20particularly%0Ain%20regions%20with%20complex%20anatomical%20interfaces%20such%20as%20the%20lung-chest%20wall%0Aboundary.%20Given%20that%20organ%20motion%20is%20inherently%20a%20continuous%20process%20in%20both%0Aspace%20and%20time%2C%20we%20aimed%20to%20develop%20a%20model%20that%20preserves%20these%20fundamental%0Aproperties.%20Drawing%20inspiration%20from%20fluid%20mechanics%2C%20we%20propose%20a%20novel%0Aapproach%20using%20implicit%20neural%20representation%20%28INR%29%20for%20continuous%20modeling%20of%0Apatient%20anatomical%20motion.%20This%20approach%20ensures%20spatial%20and%20temporal%0Acontinuity%20while%20effectively%20unifying%20Eulerian%20and%20Lagrangian%20specifications%20to%0Aenable%20natural%20continuous%20motion%20modeling%20and%20frame%20interpolation.%20The%0Aintegration%20of%20these%20specifications%20provides%20a%20more%20comprehensive%20understanding%0Aof%20anatomical%20deformation%20patterns.%20By%20leveraging%20the%20continuous%0Arepresentations%2C%20the%20CPT-DIR%20method%20significantly%20enhances%20registration%20and%0Ainterpolation%20accuracy%2C%20automation%2C%20and%20speed.%20The%20method%20demonstrates%20superior%0Aperformance%20in%20landmark%20and%20contour%20precision%2C%20particularly%20in%20challenging%0Aanatomical%20regions%2C%20representing%20a%20substantial%20advancement%20over%20conventional%0Aapproaches%20in%20deformable%20image%20registration.%20The%20improved%20efficiency%20and%0Aaccuracy%20of%20CPT-DIR%20make%20it%20particularly%20suitable%20for%20real-time%20adaptive%0Aradiotherapy%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520sPatial-Temporal%2520Deformable%2520Image%2520Registration%2520%2528CPT-DIR%2529%2520for%250A%2520%2520motion%2520modelling%2520in%2520radiotherapy%253A%2520beyond%2520classic%2520voxel-based%2520methods%26entry.906535625%3DXia%2520Li%2520and%2520Runzhao%2520Yang%2520and%2520Muheng%2520Li%2520and%2520Xiangtai%2520Li%2520and%2520Antony%2520J.%2520Lomax%2520and%2520Joachim%2520M.%2520Buhmann%2520and%2520Ye%2520Zhang%26entry.1292438233%3D%2520%2520Deformable%2520image%2520registration%2520%2528DIR%2529%2520is%2520a%2520crucial%2520tool%2520in%2520radiotherapy%2520for%250Aanalyzing%2520anatomical%2520changes%2520and%2520motion%2520patterns.%2520Current%2520DIR%2520implementations%250Arely%2520on%2520discrete%2520volumetric%2520motion%2520representation%252C%2520which%2520often%2520leads%2520to%250Acompromised%2520accuracy%2520and%2520uncertainty%2520when%2520handling%2520significant%2520anatomical%250Achanges%2520and%2520sliding%2520boundaries.%2520This%2520limitation%2520affects%2520the%2520reliability%2520of%250Asubsequent%2520contour%2520propagation%2520and%2520dose%2520accumulation%2520procedures%252C%2520particularly%250Ain%2520regions%2520with%2520complex%2520anatomical%2520interfaces%2520such%2520as%2520the%2520lung-chest%2520wall%250Aboundary.%2520Given%2520that%2520organ%2520motion%2520is%2520inherently%2520a%2520continuous%2520process%2520in%2520both%250Aspace%2520and%2520time%252C%2520we%2520aimed%2520to%2520develop%2520a%2520model%2520that%2520preserves%2520these%2520fundamental%250Aproperties.%2520Drawing%2520inspiration%2520from%2520fluid%2520mechanics%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520using%2520implicit%2520neural%2520representation%2520%2528INR%2529%2520for%2520continuous%2520modeling%2520of%250Apatient%2520anatomical%2520motion.%2520This%2520approach%2520ensures%2520spatial%2520and%2520temporal%250Acontinuity%2520while%2520effectively%2520unifying%2520Eulerian%2520and%2520Lagrangian%2520specifications%2520to%250Aenable%2520natural%2520continuous%2520motion%2520modeling%2520and%2520frame%2520interpolation.%2520The%250Aintegration%2520of%2520these%2520specifications%2520provides%2520a%2520more%2520comprehensive%2520understanding%250Aof%2520anatomical%2520deformation%2520patterns.%2520By%2520leveraging%2520the%2520continuous%250Arepresentations%252C%2520the%2520CPT-DIR%2520method%2520significantly%2520enhances%2520registration%2520and%250Ainterpolation%2520accuracy%252C%2520automation%252C%2520and%2520speed.%2520The%2520method%2520demonstrates%2520superior%250Aperformance%2520in%2520landmark%2520and%2520contour%2520precision%252C%2520particularly%2520in%2520challenging%250Aanatomical%2520regions%252C%2520representing%2520a%2520substantial%2520advancement%2520over%2520conventional%250Aapproaches%2520in%2520deformable%2520image%2520registration.%2520The%2520improved%2520efficiency%2520and%250Aaccuracy%2520of%2520CPT-DIR%2520make%2520it%2520particularly%2520suitable%2520for%2520real-time%2520adaptive%250Aradiotherapy%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20sPatial-Temporal%20Deformable%20Image%20Registration%20%28CPT-DIR%29%20for%0A%20%20motion%20modelling%20in%20radiotherapy%3A%20beyond%20classic%20voxel-based%20methods&entry.906535625=Xia%20Li%20and%20Runzhao%20Yang%20and%20Muheng%20Li%20and%20Xiangtai%20Li%20and%20Antony%20J.%20Lomax%20and%20Joachim%20M.%20Buhmann%20and%20Ye%20Zhang&entry.1292438233=%20%20Deformable%20image%20registration%20%28DIR%29%20is%20a%20crucial%20tool%20in%20radiotherapy%20for%0Aanalyzing%20anatomical%20changes%20and%20motion%20patterns.%20Current%20DIR%20implementations%0Arely%20on%20discrete%20volumetric%20motion%20representation%2C%20which%20often%20leads%20to%0Acompromised%20accuracy%20and%20uncertainty%20when%20handling%20significant%20anatomical%0Achanges%20and%20sliding%20boundaries.%20This%20limitation%20affects%20the%20reliability%20of%0Asubsequent%20contour%20propagation%20and%20dose%20accumulation%20procedures%2C%20particularly%0Ain%20regions%20with%20complex%20anatomical%20interfaces%20such%20as%20the%20lung-chest%20wall%0Aboundary.%20Given%20that%20organ%20motion%20is%20inherently%20a%20continuous%20process%20in%20both%0Aspace%20and%20time%2C%20we%20aimed%20to%20develop%20a%20model%20that%20preserves%20these%20fundamental%0Aproperties.%20Drawing%20inspiration%20from%20fluid%20mechanics%2C%20we%20propose%20a%20novel%0Aapproach%20using%20implicit%20neural%20representation%20%28INR%29%20for%20continuous%20modeling%20of%0Apatient%20anatomical%20motion.%20This%20approach%20ensures%20spatial%20and%20temporal%0Acontinuity%20while%20effectively%20unifying%20Eulerian%20and%20Lagrangian%20specifications%20to%0Aenable%20natural%20continuous%20motion%20modeling%20and%20frame%20interpolation.%20The%0Aintegration%20of%20these%20specifications%20provides%20a%20more%20comprehensive%20understanding%0Aof%20anatomical%20deformation%20patterns.%20By%20leveraging%20the%20continuous%0Arepresentations%2C%20the%20CPT-DIR%20method%20significantly%20enhances%20registration%20and%0Ainterpolation%20accuracy%2C%20automation%2C%20and%20speed.%20The%20method%20demonstrates%20superior%0Aperformance%20in%20landmark%20and%20contour%20precision%2C%20particularly%20in%20challenging%0Aanatomical%20regions%2C%20representing%20a%20substantial%20advancement%20over%20conventional%0Aapproaches%20in%20deformable%20image%20registration.%20The%20improved%20efficiency%20and%0Aaccuracy%20of%20CPT-DIR%20make%20it%20particularly%20suitable%20for%20real-time%20adaptive%0Aradiotherapy%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00430v2&entry.124074799=Read"},
{"title": "3D Active Metric-Semantic SLAM", "author": "Yuezhan Tao and Xu Liu and Igor Spasojevic and Saurav Agarwal and Vijay Kumar", "abstract": "  In this letter, we address the problem of exploration and metric-semantic\nmapping of multi-floor GPS-denied indoor environments using Size Weight and\nPower (SWaP) constrained aerial robots. Most previous work in exploration\nassumes that robot localization is solved. However, neglecting the state\nuncertainty of the agent can ultimately lead to cascading errors both in the\nresulting map and in the state of the agent itself. Furthermore, actions that\nreduce localization errors may be at direct odds with the exploration task. We\npropose a framework that balances the efficiency of exploration with actions\nthat reduce the state uncertainty of the agent. In particular, our algorithmic\napproach for active metric-semantic SLAM is built upon sparse information\nabstracted from raw problem data, to make it suitable for SWaP-constrained\nrobots. Furthermore, we integrate this framework within a fully autonomous\naerial robotic system that achieves autonomous exploration in cluttered, 3D\nenvironments. From extensive real-world experiments, we showed that by\nincluding Semantic Loop Closure (SLC), we can reduce the robot pose estimation\nerrors by over 90% in translation and approximately 75% in yaw, and the\nuncertainties in pose estimates and semantic maps by over 70% and 65%,\nrespectively. Although discussed in the context of indoor multi-floor\nexploration, our system can be used for various other applications, such as\ninfrastructure inspection and precision agriculture where reliable GPS data may\nnot be available.\n", "link": "http://arxiv.org/abs/2309.06950v4", "date": "2025-07-21", "relevancy": 2.5235, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6459}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6354}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Active%20Metric-Semantic%20SLAM&body=Title%3A%203D%20Active%20Metric-Semantic%20SLAM%0AAuthor%3A%20Yuezhan%20Tao%20and%20Xu%20Liu%20and%20Igor%20Spasojevic%20and%20Saurav%20Agarwal%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20In%20this%20letter%2C%20we%20address%20the%20problem%20of%20exploration%20and%20metric-semantic%0Amapping%20of%20multi-floor%20GPS-denied%20indoor%20environments%20using%20Size%20Weight%20and%0APower%20%28SWaP%29%20constrained%20aerial%20robots.%20Most%20previous%20work%20in%20exploration%0Aassumes%20that%20robot%20localization%20is%20solved.%20However%2C%20neglecting%20the%20state%0Auncertainty%20of%20the%20agent%20can%20ultimately%20lead%20to%20cascading%20errors%20both%20in%20the%0Aresulting%20map%20and%20in%20the%20state%20of%20the%20agent%20itself.%20Furthermore%2C%20actions%20that%0Areduce%20localization%20errors%20may%20be%20at%20direct%20odds%20with%20the%20exploration%20task.%20We%0Apropose%20a%20framework%20that%20balances%20the%20efficiency%20of%20exploration%20with%20actions%0Athat%20reduce%20the%20state%20uncertainty%20of%20the%20agent.%20In%20particular%2C%20our%20algorithmic%0Aapproach%20for%20active%20metric-semantic%20SLAM%20is%20built%20upon%20sparse%20information%0Aabstracted%20from%20raw%20problem%20data%2C%20to%20make%20it%20suitable%20for%20SWaP-constrained%0Arobots.%20Furthermore%2C%20we%20integrate%20this%20framework%20within%20a%20fully%20autonomous%0Aaerial%20robotic%20system%20that%20achieves%20autonomous%20exploration%20in%20cluttered%2C%203D%0Aenvironments.%20From%20extensive%20real-world%20experiments%2C%20we%20showed%20that%20by%0Aincluding%20Semantic%20Loop%20Closure%20%28SLC%29%2C%20we%20can%20reduce%20the%20robot%20pose%20estimation%0Aerrors%20by%20over%2090%25%20in%20translation%20and%20approximately%2075%25%20in%20yaw%2C%20and%20the%0Auncertainties%20in%20pose%20estimates%20and%20semantic%20maps%20by%20over%2070%25%20and%2065%25%2C%0Arespectively.%20Although%20discussed%20in%20the%20context%20of%20indoor%20multi-floor%0Aexploration%2C%20our%20system%20can%20be%20used%20for%20various%20other%20applications%2C%20such%20as%0Ainfrastructure%20inspection%20and%20precision%20agriculture%20where%20reliable%20GPS%20data%20may%0Anot%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.06950v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Active%2520Metric-Semantic%2520SLAM%26entry.906535625%3DYuezhan%2520Tao%2520and%2520Xu%2520Liu%2520and%2520Igor%2520Spasojevic%2520and%2520Saurav%2520Agarwal%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520In%2520this%2520letter%252C%2520we%2520address%2520the%2520problem%2520of%2520exploration%2520and%2520metric-semantic%250Amapping%2520of%2520multi-floor%2520GPS-denied%2520indoor%2520environments%2520using%2520Size%2520Weight%2520and%250APower%2520%2528SWaP%2529%2520constrained%2520aerial%2520robots.%2520Most%2520previous%2520work%2520in%2520exploration%250Aassumes%2520that%2520robot%2520localization%2520is%2520solved.%2520However%252C%2520neglecting%2520the%2520state%250Auncertainty%2520of%2520the%2520agent%2520can%2520ultimately%2520lead%2520to%2520cascading%2520errors%2520both%2520in%2520the%250Aresulting%2520map%2520and%2520in%2520the%2520state%2520of%2520the%2520agent%2520itself.%2520Furthermore%252C%2520actions%2520that%250Areduce%2520localization%2520errors%2520may%2520be%2520at%2520direct%2520odds%2520with%2520the%2520exploration%2520task.%2520We%250Apropose%2520a%2520framework%2520that%2520balances%2520the%2520efficiency%2520of%2520exploration%2520with%2520actions%250Athat%2520reduce%2520the%2520state%2520uncertainty%2520of%2520the%2520agent.%2520In%2520particular%252C%2520our%2520algorithmic%250Aapproach%2520for%2520active%2520metric-semantic%2520SLAM%2520is%2520built%2520upon%2520sparse%2520information%250Aabstracted%2520from%2520raw%2520problem%2520data%252C%2520to%2520make%2520it%2520suitable%2520for%2520SWaP-constrained%250Arobots.%2520Furthermore%252C%2520we%2520integrate%2520this%2520framework%2520within%2520a%2520fully%2520autonomous%250Aaerial%2520robotic%2520system%2520that%2520achieves%2520autonomous%2520exploration%2520in%2520cluttered%252C%25203D%250Aenvironments.%2520From%2520extensive%2520real-world%2520experiments%252C%2520we%2520showed%2520that%2520by%250Aincluding%2520Semantic%2520Loop%2520Closure%2520%2528SLC%2529%252C%2520we%2520can%2520reduce%2520the%2520robot%2520pose%2520estimation%250Aerrors%2520by%2520over%252090%2525%2520in%2520translation%2520and%2520approximately%252075%2525%2520in%2520yaw%252C%2520and%2520the%250Auncertainties%2520in%2520pose%2520estimates%2520and%2520semantic%2520maps%2520by%2520over%252070%2525%2520and%252065%2525%252C%250Arespectively.%2520Although%2520discussed%2520in%2520the%2520context%2520of%2520indoor%2520multi-floor%250Aexploration%252C%2520our%2520system%2520can%2520be%2520used%2520for%2520various%2520other%2520applications%252C%2520such%2520as%250Ainfrastructure%2520inspection%2520and%2520precision%2520agriculture%2520where%2520reliable%2520GPS%2520data%2520may%250Anot%2520be%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.06950v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Active%20Metric-Semantic%20SLAM&entry.906535625=Yuezhan%20Tao%20and%20Xu%20Liu%20and%20Igor%20Spasojevic%20and%20Saurav%20Agarwal%20and%20Vijay%20Kumar&entry.1292438233=%20%20In%20this%20letter%2C%20we%20address%20the%20problem%20of%20exploration%20and%20metric-semantic%0Amapping%20of%20multi-floor%20GPS-denied%20indoor%20environments%20using%20Size%20Weight%20and%0APower%20%28SWaP%29%20constrained%20aerial%20robots.%20Most%20previous%20work%20in%20exploration%0Aassumes%20that%20robot%20localization%20is%20solved.%20However%2C%20neglecting%20the%20state%0Auncertainty%20of%20the%20agent%20can%20ultimately%20lead%20to%20cascading%20errors%20both%20in%20the%0Aresulting%20map%20and%20in%20the%20state%20of%20the%20agent%20itself.%20Furthermore%2C%20actions%20that%0Areduce%20localization%20errors%20may%20be%20at%20direct%20odds%20with%20the%20exploration%20task.%20We%0Apropose%20a%20framework%20that%20balances%20the%20efficiency%20of%20exploration%20with%20actions%0Athat%20reduce%20the%20state%20uncertainty%20of%20the%20agent.%20In%20particular%2C%20our%20algorithmic%0Aapproach%20for%20active%20metric-semantic%20SLAM%20is%20built%20upon%20sparse%20information%0Aabstracted%20from%20raw%20problem%20data%2C%20to%20make%20it%20suitable%20for%20SWaP-constrained%0Arobots.%20Furthermore%2C%20we%20integrate%20this%20framework%20within%20a%20fully%20autonomous%0Aaerial%20robotic%20system%20that%20achieves%20autonomous%20exploration%20in%20cluttered%2C%203D%0Aenvironments.%20From%20extensive%20real-world%20experiments%2C%20we%20showed%20that%20by%0Aincluding%20Semantic%20Loop%20Closure%20%28SLC%29%2C%20we%20can%20reduce%20the%20robot%20pose%20estimation%0Aerrors%20by%20over%2090%25%20in%20translation%20and%20approximately%2075%25%20in%20yaw%2C%20and%20the%0Auncertainties%20in%20pose%20estimates%20and%20semantic%20maps%20by%20over%2070%25%20and%2065%25%2C%0Arespectively.%20Although%20discussed%20in%20the%20context%20of%20indoor%20multi-floor%0Aexploration%2C%20our%20system%20can%20be%20used%20for%20various%20other%20applications%2C%20such%20as%0Ainfrastructure%20inspection%20and%20precision%20agriculture%20where%20reliable%20GPS%20data%20may%0Anot%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.06950v4&entry.124074799=Read"},
{"title": "GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction", "author": "Zesong Yang and Ru Zhang and Jiale Shi and Zixiang Ai and Boming Zhao and Hujun Bao and Luwei Yang and Zhaopeng Cui", "abstract": "  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n", "link": "http://arxiv.org/abs/2412.14939v3", "date": "2025-07-21", "relevancy": 2.5002, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6233}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&body=Title%3A%20GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui%0AAbstract%3A%20%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14939v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGURecon%253A%2520Learning%2520Detailed%25203D%2520Geometric%2520Uncertainties%2520for%2520Neural%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DZesong%2520Yang%2520and%2520Ru%2520Zhang%2520and%2520Jiale%2520Shi%2520and%2520Zixiang%2520Ai%2520and%2520Boming%2520Zhao%2520and%2520Hujun%2520Bao%2520and%2520Luwei%2520Yang%2520and%2520Zhaopeng%2520Cui%26entry.1292438233%3D%2520%2520Neural%2520surface%2520representation%2520has%2520demonstrated%2520remarkable%2520success%2520in%2520the%250Aareas%2520of%2520novel%2520view%2520synthesis%2520and%25203D%2520reconstruction.%2520However%252C%2520assessing%2520the%250Ageometric%2520quality%2520of%25203D%2520reconstructions%2520in%2520the%2520absence%2520of%2520ground%2520truth%2520mesh%250Aremains%2520a%2520significant%2520challenge%252C%2520due%2520to%2520its%2520rendering-based%2520optimization%250Aprocess%2520and%2520entangled%2520learning%2520of%2520appearance%2520and%2520geometry%2520with%2520photometric%250Alosses.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520i.e%252C%2520GURecon%252C%2520which%250Aestablishes%2520a%2520geometric%2520uncertainty%2520field%2520for%2520the%2520neural%2520surface%2520based%2520on%250Ageometric%2520consistency.%2520Different%2520from%2520existing%2520methods%2520that%2520rely%2520on%250Arendering-based%2520measurement%252C%2520GURecon%2520models%2520a%2520continuous%25203D%2520uncertainty%2520field%250Afor%2520the%2520reconstructed%2520surface%252C%2520and%2520is%2520learned%2520by%2520an%2520online%2520distillation%250Aapproach%2520without%2520introducing%2520real%2520geometric%2520information%2520for%2520supervision.%250AMoreover%252C%2520in%2520order%2520to%2520mitigate%2520the%2520interference%2520of%2520illumination%2520on%2520geometric%250Aconsistency%252C%2520a%2520decoupled%2520field%2520is%2520learned%2520and%2520exploited%2520to%2520finetune%2520the%250Auncertainty%2520field.%2520Experiments%2520on%2520various%2520datasets%2520demonstrate%2520the%2520superiority%250Aof%2520GURecon%2520in%2520modeling%25203D%2520geometric%2520uncertainty%252C%2520as%2520well%2520as%2520its%2520plug-and-play%250Aextension%2520to%2520various%2520neural%2520surface%2520representations%2520and%2520improvement%2520on%250Adownstream%2520tasks%2520such%2520as%2520incremental%2520reconstruction.%2520The%2520code%2520and%2520supplementary%250Amaterial%2520are%2520available%2520on%2520the%2520project%2520website%253A%250Ahttps%253A//zju3dv.github.io/GURecon/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14939v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GURecon%3A%20Learning%20Detailed%203D%20Geometric%20Uncertainties%20for%20Neural%20Surface%0A%20%20Reconstruction&entry.906535625=Zesong%20Yang%20and%20Ru%20Zhang%20and%20Jiale%20Shi%20and%20Zixiang%20Ai%20and%20Boming%20Zhao%20and%20Hujun%20Bao%20and%20Luwei%20Yang%20and%20Zhaopeng%20Cui&entry.1292438233=%20%20Neural%20surface%20representation%20has%20demonstrated%20remarkable%20success%20in%20the%0Aareas%20of%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%20assessing%20the%0Ageometric%20quality%20of%203D%20reconstructions%20in%20the%20absence%20of%20ground%20truth%20mesh%0Aremains%20a%20significant%20challenge%2C%20due%20to%20its%20rendering-based%20optimization%0Aprocess%20and%20entangled%20learning%20of%20appearance%20and%20geometry%20with%20photometric%0Alosses.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%2C%20i.e%2C%20GURecon%2C%20which%0Aestablishes%20a%20geometric%20uncertainty%20field%20for%20the%20neural%20surface%20based%20on%0Ageometric%20consistency.%20Different%20from%20existing%20methods%20that%20rely%20on%0Arendering-based%20measurement%2C%20GURecon%20models%20a%20continuous%203D%20uncertainty%20field%0Afor%20the%20reconstructed%20surface%2C%20and%20is%20learned%20by%20an%20online%20distillation%0Aapproach%20without%20introducing%20real%20geometric%20information%20for%20supervision.%0AMoreover%2C%20in%20order%20to%20mitigate%20the%20interference%20of%20illumination%20on%20geometric%0Aconsistency%2C%20a%20decoupled%20field%20is%20learned%20and%20exploited%20to%20finetune%20the%0Auncertainty%20field.%20Experiments%20on%20various%20datasets%20demonstrate%20the%20superiority%0Aof%20GURecon%20in%20modeling%203D%20geometric%20uncertainty%2C%20as%20well%20as%20its%20plug-and-play%0Aextension%20to%20various%20neural%20surface%20representations%20and%20improvement%20on%0Adownstream%20tasks%20such%20as%20incremental%20reconstruction.%20The%20code%20and%20supplementary%0Amaterial%20are%20available%20on%20the%20project%20website%3A%0Ahttps%3A//zju3dv.github.io/GURecon/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14939v3&entry.124074799=Read"},
{"title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point\n  Clouds and Images", "author": "JunYing Huang and Ao Xu and DongSun Yong and KeRen Li and YuanFeng Wang and Qi Qin", "abstract": "  Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.\n", "link": "http://arxiv.org/abs/2507.15496v1", "date": "2025-07-21", "relevancy": 2.4732, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6424}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6073}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense-depth%20map%20guided%20deep%20Lidar-Visual%20Odometry%20with%20Sparse%20Point%0A%20%20Clouds%20and%20Images&body=Title%3A%20Dense-depth%20map%20guided%20deep%20Lidar-Visual%20Odometry%20with%20Sparse%20Point%0A%20%20Clouds%20and%20Images%0AAuthor%3A%20JunYing%20Huang%20and%20Ao%20Xu%20and%20DongSun%20Yong%20and%20KeRen%20Li%20and%20YuanFeng%20Wang%20and%20Qi%20Qin%0AAbstract%3A%20%20%20Odometry%20is%20a%20critical%20task%20for%20autonomous%20systems%20for%20self-localization%20and%0Anavigation.%20We%20propose%20a%20novel%20LiDAR-Visual%20odometry%20framework%20that%20integrates%0ALiDAR%20point%20clouds%20and%20images%20for%20accurate%20and%20robust%20pose%20estimation.%20Our%0Amethod%20utilizes%20a%20dense-depth%20map%20estimated%20from%20point%20clouds%20and%20images%0Athrough%20depth%20completion%2C%20and%20incorporates%20a%20multi-scale%20feature%20extraction%0Anetwork%20with%20attention%20mechanisms%2C%20enabling%20adaptive%20depth-aware%0Arepresentations.%20Furthermore%2C%20we%20leverage%20dense%20depth%20information%20to%20refine%0Aflow%20estimation%20and%20mitigate%20errors%20in%20occlusion-prone%20regions.%20Our%0Ahierarchical%20pose%20refinement%20module%20optimizes%20motion%20estimation%20progressively%2C%0Aensuring%20robust%20predictions%20against%20dynamic%20environments%20and%20scale%20ambiguities.%0AComprehensive%20experiments%20on%20the%20KITTI%20odometry%20benchmark%20demonstrate%20that%20our%0Aapproach%20achieves%20similar%20or%20superior%20accuracy%20and%20robustness%20compared%20to%0Astate-of-the-art%20visual%20and%20LiDAR%20odometry%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense-depth%2520map%2520guided%2520deep%2520Lidar-Visual%2520Odometry%2520with%2520Sparse%2520Point%250A%2520%2520Clouds%2520and%2520Images%26entry.906535625%3DJunYing%2520Huang%2520and%2520Ao%2520Xu%2520and%2520DongSun%2520Yong%2520and%2520KeRen%2520Li%2520and%2520YuanFeng%2520Wang%2520and%2520Qi%2520Qin%26entry.1292438233%3D%2520%2520Odometry%2520is%2520a%2520critical%2520task%2520for%2520autonomous%2520systems%2520for%2520self-localization%2520and%250Anavigation.%2520We%2520propose%2520a%2520novel%2520LiDAR-Visual%2520odometry%2520framework%2520that%2520integrates%250ALiDAR%2520point%2520clouds%2520and%2520images%2520for%2520accurate%2520and%2520robust%2520pose%2520estimation.%2520Our%250Amethod%2520utilizes%2520a%2520dense-depth%2520map%2520estimated%2520from%2520point%2520clouds%2520and%2520images%250Athrough%2520depth%2520completion%252C%2520and%2520incorporates%2520a%2520multi-scale%2520feature%2520extraction%250Anetwork%2520with%2520attention%2520mechanisms%252C%2520enabling%2520adaptive%2520depth-aware%250Arepresentations.%2520Furthermore%252C%2520we%2520leverage%2520dense%2520depth%2520information%2520to%2520refine%250Aflow%2520estimation%2520and%2520mitigate%2520errors%2520in%2520occlusion-prone%2520regions.%2520Our%250Ahierarchical%2520pose%2520refinement%2520module%2520optimizes%2520motion%2520estimation%2520progressively%252C%250Aensuring%2520robust%2520predictions%2520against%2520dynamic%2520environments%2520and%2520scale%2520ambiguities.%250AComprehensive%2520experiments%2520on%2520the%2520KITTI%2520odometry%2520benchmark%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520similar%2520or%2520superior%2520accuracy%2520and%2520robustness%2520compared%2520to%250Astate-of-the-art%2520visual%2520and%2520LiDAR%2520odometry%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense-depth%20map%20guided%20deep%20Lidar-Visual%20Odometry%20with%20Sparse%20Point%0A%20%20Clouds%20and%20Images&entry.906535625=JunYing%20Huang%20and%20Ao%20Xu%20and%20DongSun%20Yong%20and%20KeRen%20Li%20and%20YuanFeng%20Wang%20and%20Qi%20Qin&entry.1292438233=%20%20Odometry%20is%20a%20critical%20task%20for%20autonomous%20systems%20for%20self-localization%20and%0Anavigation.%20We%20propose%20a%20novel%20LiDAR-Visual%20odometry%20framework%20that%20integrates%0ALiDAR%20point%20clouds%20and%20images%20for%20accurate%20and%20robust%20pose%20estimation.%20Our%0Amethod%20utilizes%20a%20dense-depth%20map%20estimated%20from%20point%20clouds%20and%20images%0Athrough%20depth%20completion%2C%20and%20incorporates%20a%20multi-scale%20feature%20extraction%0Anetwork%20with%20attention%20mechanisms%2C%20enabling%20adaptive%20depth-aware%0Arepresentations.%20Furthermore%2C%20we%20leverage%20dense%20depth%20information%20to%20refine%0Aflow%20estimation%20and%20mitigate%20errors%20in%20occlusion-prone%20regions.%20Our%0Ahierarchical%20pose%20refinement%20module%20optimizes%20motion%20estimation%20progressively%2C%0Aensuring%20robust%20predictions%20against%20dynamic%20environments%20and%20scale%20ambiguities.%0AComprehensive%20experiments%20on%20the%20KITTI%20odometry%20benchmark%20demonstrate%20that%20our%0Aapproach%20achieves%20similar%20or%20superior%20accuracy%20and%20robustness%20compared%20to%0Astate-of-the-art%20visual%20and%20LiDAR%20odometry%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15496v1&entry.124074799=Read"},
{"title": "An aerial color image anomaly dataset for search missions in complex\n  forested terrain", "author": "Rakesh John Amala Arokia Nathan and Matthias Gessner and Nurullah \u00d6zkan and Marius Bock and Mohamed Youssef and Maximilian Mews and Bj\u00f6rn Piltz and Ralf Berger and Oliver Bimber", "abstract": "  After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.\n", "link": "http://arxiv.org/abs/2507.15492v1", "date": "2025-07-21", "relevancy": 2.4698, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20aerial%20color%20image%20anomaly%20dataset%20for%20search%20missions%20in%20complex%0A%20%20forested%20terrain&body=Title%3A%20An%20aerial%20color%20image%20anomaly%20dataset%20for%20search%20missions%20in%20complex%0A%20%20forested%20terrain%0AAuthor%3A%20Rakesh%20John%20Amala%20Arokia%20Nathan%20and%20Matthias%20Gessner%20and%20Nurullah%20%C3%96zkan%20and%20Marius%20Bock%20and%20Mohamed%20Youssef%20and%20Maximilian%20Mews%20and%20Bj%C3%B6rn%20Piltz%20and%20Ralf%20Berger%20and%20Oliver%20Bimber%0AAbstract%3A%20%20%20After%20a%20family%20murder%20in%20rural%20Germany%2C%20authorities%20failed%20to%20locate%20the%0Asuspect%20in%20a%20vast%20forest%20despite%20a%20massive%20search.%20To%20aid%20the%20search%2C%20a%0Aresearch%20aircraft%20captured%20high-resolution%20aerial%20imagery.%20Due%20to%20dense%0Avegetation%20obscuring%20small%20clues%2C%20automated%20analysis%20was%20ineffective%2C%20prompting%0Aa%20crowd-search%20initiative.%20This%20effort%20produced%20a%20unique%20dataset%20of%20labeled%2C%0Ahard-to-detect%20anomalies%20under%20occluded%2C%20real-world%20conditions.%20It%20can%20serve%20as%0Aa%20benchmark%20for%20improving%20anomaly%20detection%20approaches%20in%20complex%20forest%0Aenvironments%2C%20supporting%20manhunts%20and%20rescue%20operations.%20Initial%20benchmark%0Atests%20showed%20existing%20methods%20performed%20poorly%2C%20highlighting%20the%20need%20for%0Acontext-aware%20approaches.%20The%20dataset%20is%20openly%20accessible%20for%20offline%0Aprocessing.%20An%20additional%20interactive%20web%20interface%20supports%20online%20viewing%20and%0Adynamic%20growth%20by%20allowing%20users%20to%20annotate%20and%20submit%20new%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520aerial%2520color%2520image%2520anomaly%2520dataset%2520for%2520search%2520missions%2520in%2520complex%250A%2520%2520forested%2520terrain%26entry.906535625%3DRakesh%2520John%2520Amala%2520Arokia%2520Nathan%2520and%2520Matthias%2520Gessner%2520and%2520Nurullah%2520%25C3%2596zkan%2520and%2520Marius%2520Bock%2520and%2520Mohamed%2520Youssef%2520and%2520Maximilian%2520Mews%2520and%2520Bj%25C3%25B6rn%2520Piltz%2520and%2520Ralf%2520Berger%2520and%2520Oliver%2520Bimber%26entry.1292438233%3D%2520%2520After%2520a%2520family%2520murder%2520in%2520rural%2520Germany%252C%2520authorities%2520failed%2520to%2520locate%2520the%250Asuspect%2520in%2520a%2520vast%2520forest%2520despite%2520a%2520massive%2520search.%2520To%2520aid%2520the%2520search%252C%2520a%250Aresearch%2520aircraft%2520captured%2520high-resolution%2520aerial%2520imagery.%2520Due%2520to%2520dense%250Avegetation%2520obscuring%2520small%2520clues%252C%2520automated%2520analysis%2520was%2520ineffective%252C%2520prompting%250Aa%2520crowd-search%2520initiative.%2520This%2520effort%2520produced%2520a%2520unique%2520dataset%2520of%2520labeled%252C%250Ahard-to-detect%2520anomalies%2520under%2520occluded%252C%2520real-world%2520conditions.%2520It%2520can%2520serve%2520as%250Aa%2520benchmark%2520for%2520improving%2520anomaly%2520detection%2520approaches%2520in%2520complex%2520forest%250Aenvironments%252C%2520supporting%2520manhunts%2520and%2520rescue%2520operations.%2520Initial%2520benchmark%250Atests%2520showed%2520existing%2520methods%2520performed%2520poorly%252C%2520highlighting%2520the%2520need%2520for%250Acontext-aware%2520approaches.%2520The%2520dataset%2520is%2520openly%2520accessible%2520for%2520offline%250Aprocessing.%2520An%2520additional%2520interactive%2520web%2520interface%2520supports%2520online%2520viewing%2520and%250Adynamic%2520growth%2520by%2520allowing%2520users%2520to%2520annotate%2520and%2520submit%2520new%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20aerial%20color%20image%20anomaly%20dataset%20for%20search%20missions%20in%20complex%0A%20%20forested%20terrain&entry.906535625=Rakesh%20John%20Amala%20Arokia%20Nathan%20and%20Matthias%20Gessner%20and%20Nurullah%20%C3%96zkan%20and%20Marius%20Bock%20and%20Mohamed%20Youssef%20and%20Maximilian%20Mews%20and%20Bj%C3%B6rn%20Piltz%20and%20Ralf%20Berger%20and%20Oliver%20Bimber&entry.1292438233=%20%20After%20a%20family%20murder%20in%20rural%20Germany%2C%20authorities%20failed%20to%20locate%20the%0Asuspect%20in%20a%20vast%20forest%20despite%20a%20massive%20search.%20To%20aid%20the%20search%2C%20a%0Aresearch%20aircraft%20captured%20high-resolution%20aerial%20imagery.%20Due%20to%20dense%0Avegetation%20obscuring%20small%20clues%2C%20automated%20analysis%20was%20ineffective%2C%20prompting%0Aa%20crowd-search%20initiative.%20This%20effort%20produced%20a%20unique%20dataset%20of%20labeled%2C%0Ahard-to-detect%20anomalies%20under%20occluded%2C%20real-world%20conditions.%20It%20can%20serve%20as%0Aa%20benchmark%20for%20improving%20anomaly%20detection%20approaches%20in%20complex%20forest%0Aenvironments%2C%20supporting%20manhunts%20and%20rescue%20operations.%20Initial%20benchmark%0Atests%20showed%20existing%20methods%20performed%20poorly%2C%20highlighting%20the%20need%20for%0Acontext-aware%20approaches.%20The%20dataset%20is%20openly%20accessible%20for%20offline%0Aprocessing.%20An%20additional%20interactive%20web%20interface%20supports%20online%20viewing%20and%0Adynamic%20growth%20by%20allowing%20users%20to%20annotate%20and%20submit%20new%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15492v1&entry.124074799=Read"},
{"title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement\n  Learning", "author": "Sneheel Sarangi and Hanan Salam", "abstract": "  Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.\n", "link": "http://arxiv.org/abs/2507.15788v1", "date": "2025-07-21", "relevancy": 2.4653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20LLMs%20Do%20Not%20Learn%20a%20Generalizable%20Theory%20of%20Mind%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20Small%20LLMs%20Do%20Not%20Learn%20a%20Generalizable%20Theory%20of%20Mind%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Sneheel%20Sarangi%20and%20Hanan%20Salam%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aemergent%20capabilities%20in%20complex%20reasoning%2C%20largely%20spurred%20by%20rule-based%0AReinforcement%20Learning%20%28RL%29%20techniques%20applied%20during%20the%20post-training.%20This%0Ahas%20raised%20the%20question%20of%20whether%20similar%20methods%20can%20instill%20more%20nuanced%2C%0Ahuman-like%20social%20intelligence%2C%20such%20as%20a%20Theory%20of%20Mind%20%28ToM%29%2C%20in%20LLMs.%20This%0Apaper%20investigates%20whether%20small-scale%20LLMs%20can%20acquire%20a%20robust%20and%0Ageneralizable%20ToM%20capability%20through%20RL%20with%20verifiable%20rewards%20%28RLVR%29.%20We%0Aconduct%20a%20systematic%20evaluation%20by%20training%20models%20on%20various%20combinations%20of%0Aprominent%20ToM%20datasets%20%28HiToM%2C%20ExploreToM%2C%20FANToM%29%20and%20testing%20for%0Ageneralization%20on%20held-out%20datasets%20%28e.g.%2C%20OpenToM%29.%20Our%20findings%20indicate%20that%0Asmall%20LLMs%20struggle%20to%20develop%20a%20generic%20ToM%20capability.%20While%20performance%20on%0Ain-distribution%20tasks%20improves%2C%20this%20capability%20fails%20to%20transfer%20to%20unseen%20ToM%0Atasks%20with%20different%20characteristics.%20Furthermore%2C%20we%20demonstrate%20that%0Aprolonged%20RL%20training%20leads%20to%20models%20%60%60hacking%27%27%20the%20statistical%20patterns%20of%0Athe%20training%20datasets%2C%20resulting%20in%20significant%20performance%20gains%20on%20in-domain%0Adata%20but%20no%20change%2C%20or%20degradation%20of%20performance%20on%20out-of-distribution%20tasks.%0AThis%20suggests%20the%20learned%20behavior%20is%20a%20form%20of%20narrow%20overfitting%20rather%20than%0Athe%20acquisition%20of%20a%20true%2C%20abstract%20ToM%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520LLMs%2520Do%2520Not%2520Learn%2520a%2520Generalizable%2520Theory%2520of%2520Mind%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSneheel%2520Sarangi%2520and%2520Hanan%2520Salam%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aemergent%2520capabilities%2520in%2520complex%2520reasoning%252C%2520largely%2520spurred%2520by%2520rule-based%250AReinforcement%2520Learning%2520%2528RL%2529%2520techniques%2520applied%2520during%2520the%2520post-training.%2520This%250Ahas%2520raised%2520the%2520question%2520of%2520whether%2520similar%2520methods%2520can%2520instill%2520more%2520nuanced%252C%250Ahuman-like%2520social%2520intelligence%252C%2520such%2520as%2520a%2520Theory%2520of%2520Mind%2520%2528ToM%2529%252C%2520in%2520LLMs.%2520This%250Apaper%2520investigates%2520whether%2520small-scale%2520LLMs%2520can%2520acquire%2520a%2520robust%2520and%250Ageneralizable%2520ToM%2520capability%2520through%2520RL%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529.%2520We%250Aconduct%2520a%2520systematic%2520evaluation%2520by%2520training%2520models%2520on%2520various%2520combinations%2520of%250Aprominent%2520ToM%2520datasets%2520%2528HiToM%252C%2520ExploreToM%252C%2520FANToM%2529%2520and%2520testing%2520for%250Ageneralization%2520on%2520held-out%2520datasets%2520%2528e.g.%252C%2520OpenToM%2529.%2520Our%2520findings%2520indicate%2520that%250Asmall%2520LLMs%2520struggle%2520to%2520develop%2520a%2520generic%2520ToM%2520capability.%2520While%2520performance%2520on%250Ain-distribution%2520tasks%2520improves%252C%2520this%2520capability%2520fails%2520to%2520transfer%2520to%2520unseen%2520ToM%250Atasks%2520with%2520different%2520characteristics.%2520Furthermore%252C%2520we%2520demonstrate%2520that%250Aprolonged%2520RL%2520training%2520leads%2520to%2520models%2520%2560%2560hacking%2527%2527%2520the%2520statistical%2520patterns%2520of%250Athe%2520training%2520datasets%252C%2520resulting%2520in%2520significant%2520performance%2520gains%2520on%2520in-domain%250Adata%2520but%2520no%2520change%252C%2520or%2520degradation%2520of%2520performance%2520on%2520out-of-distribution%2520tasks.%250AThis%2520suggests%2520the%2520learned%2520behavior%2520is%2520a%2520form%2520of%2520narrow%2520overfitting%2520rather%2520than%250Athe%2520acquisition%2520of%2520a%2520true%252C%2520abstract%2520ToM%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20LLMs%20Do%20Not%20Learn%20a%20Generalizable%20Theory%20of%20Mind%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Sneheel%20Sarangi%20and%20Hanan%20Salam&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aemergent%20capabilities%20in%20complex%20reasoning%2C%20largely%20spurred%20by%20rule-based%0AReinforcement%20Learning%20%28RL%29%20techniques%20applied%20during%20the%20post-training.%20This%0Ahas%20raised%20the%20question%20of%20whether%20similar%20methods%20can%20instill%20more%20nuanced%2C%0Ahuman-like%20social%20intelligence%2C%20such%20as%20a%20Theory%20of%20Mind%20%28ToM%29%2C%20in%20LLMs.%20This%0Apaper%20investigates%20whether%20small-scale%20LLMs%20can%20acquire%20a%20robust%20and%0Ageneralizable%20ToM%20capability%20through%20RL%20with%20verifiable%20rewards%20%28RLVR%29.%20We%0Aconduct%20a%20systematic%20evaluation%20by%20training%20models%20on%20various%20combinations%20of%0Aprominent%20ToM%20datasets%20%28HiToM%2C%20ExploreToM%2C%20FANToM%29%20and%20testing%20for%0Ageneralization%20on%20held-out%20datasets%20%28e.g.%2C%20OpenToM%29.%20Our%20findings%20indicate%20that%0Asmall%20LLMs%20struggle%20to%20develop%20a%20generic%20ToM%20capability.%20While%20performance%20on%0Ain-distribution%20tasks%20improves%2C%20this%20capability%20fails%20to%20transfer%20to%20unseen%20ToM%0Atasks%20with%20different%20characteristics.%20Furthermore%2C%20we%20demonstrate%20that%0Aprolonged%20RL%20training%20leads%20to%20models%20%60%60hacking%27%27%20the%20statistical%20patterns%20of%0Athe%20training%20datasets%2C%20resulting%20in%20significant%20performance%20gains%20on%20in-domain%0Adata%20but%20no%20change%2C%20or%20degradation%20of%20performance%20on%20out-of-distribution%20tasks.%0AThis%20suggests%20the%20learned%20behavior%20is%20a%20form%20of%20narrow%20overfitting%20rather%20than%0Athe%20acquisition%20of%20a%20true%2C%20abstract%20ToM%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15788v1&entry.124074799=Read"},
{"title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction", "author": "Zhixiong Zhang and Shuangrui Ding and Xiaoyi Dong and Songxin He and Jianfan Lin and Junsong Tang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.\n", "link": "http://arxiv.org/abs/2507.15852v1", "date": "2025-07-21", "relevancy": 2.4377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6253}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction&body=Title%3A%20SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction%0AAuthor%3A%20Zhixiong%20Zhang%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Songxin%20He%20and%20Jianfan%20Lin%20and%20Junsong%20Tang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Video%20Object%20Segmentation%20%28VOS%29%20is%20a%20core%20task%20in%20computer%20vision%2C%20requiring%0Amodels%20to%20track%20and%20segment%20target%20objects%20across%20video%20frames.%20Despite%20notable%0Aadvances%20with%20recent%20efforts%2C%20current%20techniques%20still%20lag%20behind%20human%0Acapabilities%20in%20handling%20drastic%20visual%20variations%2C%20occlusions%2C%20and%20complex%0Ascene%20changes.%20This%20limitation%20arises%20from%20their%20reliance%20on%20appearance%0Amatching%2C%20neglecting%20the%20human-like%20conceptual%20understanding%20of%20objects%20that%0Aenables%20robust%20identification%20across%20temporal%20dynamics.%20Motivated%20by%20this%20gap%2C%0Awe%20propose%20Segment%20Concept%20%28SeC%29%2C%20a%20concept-driven%20segmentation%20framework%20that%0Ashifts%20from%20conventional%20feature%20matching%20to%20the%20progressive%20construction%20and%0Autilization%20of%20high-level%2C%20object-centric%20representations.%20SeC%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20integrate%20visual%20cues%20across%20diverse%20frames%2C%0Aconstructing%20robust%20conceptual%20priors.%20During%20inference%2C%20SeC%20forms%20a%0Acomprehensive%20semantic%20representation%20of%20the%20target%20based%20on%20processed%20frames%2C%0Arealizing%20robust%20segmentation%20of%20follow-up%20frames.%20Furthermore%2C%20SeC%20adaptively%0Abalances%20LVLM-based%20semantic%20reasoning%20with%20enhanced%20feature%20matching%2C%0Adynamically%20adjusting%20computational%20efforts%20based%20on%20scene%20complexity.%20To%0Arigorously%20assess%20VOS%20methods%20in%20scenarios%20demanding%20high-level%20conceptual%0Areasoning%20and%20robust%20semantic%20understanding%2C%20we%20introduce%20the%20Semantic%20Complex%0AScenarios%20Video%20Object%20Segmentation%20benchmark%20%28SeCVOS%29.%20SeCVOS%20comprises%20160%0Amanually%20annotated%20multi-scenario%20videos%20designed%20to%20challenge%20models%20with%0Asubstantial%20appearance%20variations%20and%20dynamic%20scene%20transformations.%20In%0Aparticular%2C%20SeC%20achieves%20an%2011.8-point%20improvement%20over%20SAM%202.1%20on%20SeCVOS%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20concept-aware%20video%20object%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeC%253A%2520Advancing%2520Complex%2520Video%2520Object%2520Segmentation%2520via%2520Progressive%2520Concept%250A%2520%2520Construction%26entry.906535625%3DZhixiong%2520Zhang%2520and%2520Shuangrui%2520Ding%2520and%2520Xiaoyi%2520Dong%2520and%2520Songxin%2520He%2520and%2520Jianfan%2520Lin%2520and%2520Junsong%2520Tang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520is%2520a%2520core%2520task%2520in%2520computer%2520vision%252C%2520requiring%250Amodels%2520to%2520track%2520and%2520segment%2520target%2520objects%2520across%2520video%2520frames.%2520Despite%2520notable%250Aadvances%2520with%2520recent%2520efforts%252C%2520current%2520techniques%2520still%2520lag%2520behind%2520human%250Acapabilities%2520in%2520handling%2520drastic%2520visual%2520variations%252C%2520occlusions%252C%2520and%2520complex%250Ascene%2520changes.%2520This%2520limitation%2520arises%2520from%2520their%2520reliance%2520on%2520appearance%250Amatching%252C%2520neglecting%2520the%2520human-like%2520conceptual%2520understanding%2520of%2520objects%2520that%250Aenables%2520robust%2520identification%2520across%2520temporal%2520dynamics.%2520Motivated%2520by%2520this%2520gap%252C%250Awe%2520propose%2520Segment%2520Concept%2520%2528SeC%2529%252C%2520a%2520concept-driven%2520segmentation%2520framework%2520that%250Ashifts%2520from%2520conventional%2520feature%2520matching%2520to%2520the%2520progressive%2520construction%2520and%250Autilization%2520of%2520high-level%252C%2520object-centric%2520representations.%2520SeC%2520employs%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520integrate%2520visual%2520cues%2520across%2520diverse%2520frames%252C%250Aconstructing%2520robust%2520conceptual%2520priors.%2520During%2520inference%252C%2520SeC%2520forms%2520a%250Acomprehensive%2520semantic%2520representation%2520of%2520the%2520target%2520based%2520on%2520processed%2520frames%252C%250Arealizing%2520robust%2520segmentation%2520of%2520follow-up%2520frames.%2520Furthermore%252C%2520SeC%2520adaptively%250Abalances%2520LVLM-based%2520semantic%2520reasoning%2520with%2520enhanced%2520feature%2520matching%252C%250Adynamically%2520adjusting%2520computational%2520efforts%2520based%2520on%2520scene%2520complexity.%2520To%250Arigorously%2520assess%2520VOS%2520methods%2520in%2520scenarios%2520demanding%2520high-level%2520conceptual%250Areasoning%2520and%2520robust%2520semantic%2520understanding%252C%2520we%2520introduce%2520the%2520Semantic%2520Complex%250AScenarios%2520Video%2520Object%2520Segmentation%2520benchmark%2520%2528SeCVOS%2529.%2520SeCVOS%2520comprises%2520160%250Amanually%2520annotated%2520multi-scenario%2520videos%2520designed%2520to%2520challenge%2520models%2520with%250Asubstantial%2520appearance%2520variations%2520and%2520dynamic%2520scene%2520transformations.%2520In%250Aparticular%252C%2520SeC%2520achieves%2520an%252011.8-point%2520improvement%2520over%2520SAM%25202.1%2520on%2520SeCVOS%252C%250Aestablishing%2520a%2520new%2520state-of-the-art%2520in%2520concept-aware%2520video%2520object%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction&entry.906535625=Zhixiong%20Zhang%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Songxin%20He%20and%20Jianfan%20Lin%20and%20Junsong%20Tang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Video%20Object%20Segmentation%20%28VOS%29%20is%20a%20core%20task%20in%20computer%20vision%2C%20requiring%0Amodels%20to%20track%20and%20segment%20target%20objects%20across%20video%20frames.%20Despite%20notable%0Aadvances%20with%20recent%20efforts%2C%20current%20techniques%20still%20lag%20behind%20human%0Acapabilities%20in%20handling%20drastic%20visual%20variations%2C%20occlusions%2C%20and%20complex%0Ascene%20changes.%20This%20limitation%20arises%20from%20their%20reliance%20on%20appearance%0Amatching%2C%20neglecting%20the%20human-like%20conceptual%20understanding%20of%20objects%20that%0Aenables%20robust%20identification%20across%20temporal%20dynamics.%20Motivated%20by%20this%20gap%2C%0Awe%20propose%20Segment%20Concept%20%28SeC%29%2C%20a%20concept-driven%20segmentation%20framework%20that%0Ashifts%20from%20conventional%20feature%20matching%20to%20the%20progressive%20construction%20and%0Autilization%20of%20high-level%2C%20object-centric%20representations.%20SeC%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20integrate%20visual%20cues%20across%20diverse%20frames%2C%0Aconstructing%20robust%20conceptual%20priors.%20During%20inference%2C%20SeC%20forms%20a%0Acomprehensive%20semantic%20representation%20of%20the%20target%20based%20on%20processed%20frames%2C%0Arealizing%20robust%20segmentation%20of%20follow-up%20frames.%20Furthermore%2C%20SeC%20adaptively%0Abalances%20LVLM-based%20semantic%20reasoning%20with%20enhanced%20feature%20matching%2C%0Adynamically%20adjusting%20computational%20efforts%20based%20on%20scene%20complexity.%20To%0Arigorously%20assess%20VOS%20methods%20in%20scenarios%20demanding%20high-level%20conceptual%0Areasoning%20and%20robust%20semantic%20understanding%2C%20we%20introduce%20the%20Semantic%20Complex%0AScenarios%20Video%20Object%20Segmentation%20benchmark%20%28SeCVOS%29.%20SeCVOS%20comprises%20160%0Amanually%20annotated%20multi-scenario%20videos%20designed%20to%20challenge%20models%20with%0Asubstantial%20appearance%20variations%20and%20dynamic%20scene%20transformations.%20In%0Aparticular%2C%20SeC%20achieves%20an%2011.8-point%20improvement%20over%20SAM%202.1%20on%20SeCVOS%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20concept-aware%20video%20object%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15852v1&entry.124074799=Read"},
{"title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection", "author": "Ruijun Feng and Hammond Pearce and Pietro Liguori and Yulei Sui", "abstract": "  Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs.\nHowever, existing fine-tuning techniques often treat source code as plain text,\nlosing the graph-based structural information inherent in code.\n  Graph-enhanced soft prompt tuning addresses this by translating the\nstructural information into contextual cues that the LLM can understand.\nHowever, current methods are primarily designed for general graph-related tasks\nand focus more on adjacency information, they fall short in preserving the rich\nsemantic information (e.g., control/data flow) within code graphs. They also\nfail to ensure computational efficiency while capturing graph-text interactions\nin their cross-modal alignment module.\n  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection. CGP-Tuning introduces\ntype-aware embeddings to capture the rich semantic information within code\ngraphs, along with an efficient cross-modal alignment module that achieves\nlinear computational costs while incorporating graph-text interactions. It is\nevaluated on the latest DiverseVul dataset and three advanced open-source code\nLLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that\nCGP-Tuning delivers model-agnostic improvements and maintains practical\ninference speed, surpassing the best graph-enhanced soft prompt tuning baseline\nby an average of four percentage points and outperforming non-tuned zero-shot\nprompting by 15 percentage points.\n", "link": "http://arxiv.org/abs/2501.04510v2", "date": "2025-07-21", "relevancy": 2.4018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection&body=Title%3A%20CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection%0AAuthor%3A%20Ruijun%20Feng%20and%20Hammond%20Pearce%20and%20Pietro%20Liguori%20and%20Yulei%20Sui%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20proposed%20as%20powerful%20tools%20for%0Adetecting%20software%20vulnerabilities%2C%20where%20task-specific%20fine-tuning%20is%0Atypically%20employed%20to%20provide%20vulnerability-specific%20knowledge%20to%20the%20LLMs.%0AHowever%2C%20existing%20fine-tuning%20techniques%20often%20treat%20source%20code%20as%20plain%20text%2C%0Alosing%20the%20graph-based%20structural%20information%20inherent%20in%20code.%0A%20%20Graph-enhanced%20soft%20prompt%20tuning%20addresses%20this%20by%20translating%20the%0Astructural%20information%20into%20contextual%20cues%20that%20the%20LLM%20can%20understand.%0AHowever%2C%20current%20methods%20are%20primarily%20designed%20for%20general%20graph-related%20tasks%0Aand%20focus%20more%20on%20adjacency%20information%2C%20they%20fall%20short%20in%20preserving%20the%20rich%0Asemantic%20information%20%28e.g.%2C%20control/data%20flow%29%20within%20code%20graphs.%20They%20also%0Afail%20to%20ensure%20computational%20efficiency%20while%20capturing%20graph-text%20interactions%0Ain%20their%20cross-modal%20alignment%20module.%0A%20%20This%20paper%20presents%20CGP-Tuning%2C%20a%20new%20code%20graph-enhanced%2C%20structure-aware%0Asoft%20prompt%20tuning%20method%20for%20vulnerability%20detection.%20CGP-Tuning%20introduces%0Atype-aware%20embeddings%20to%20capture%20the%20rich%20semantic%20information%20within%20code%0Agraphs%2C%20along%20with%20an%20efficient%20cross-modal%20alignment%20module%20that%20achieves%0Alinear%20computational%20costs%20while%20incorporating%20graph-text%20interactions.%20It%20is%0Aevaluated%20on%20the%20latest%20DiverseVul%20dataset%20and%20three%20advanced%20open-source%20code%0ALLMs%2C%20CodeLlama%2C%20CodeGemma%2C%20and%20Qwen2.5-Coder.%20Experimental%20results%20show%20that%0ACGP-Tuning%20delivers%20model-agnostic%20improvements%20and%20maintains%20practical%0Ainference%20speed%2C%20surpassing%20the%20best%20graph-enhanced%20soft%20prompt%20tuning%20baseline%0Aby%20an%20average%20of%20four%20percentage%20points%20and%20outperforming%20non-tuned%20zero-shot%0Aprompting%20by%2015%20percentage%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGP-Tuning%253A%2520Structure-Aware%2520Soft%2520Prompt%2520Tuning%2520for%2520Code%2520Vulnerability%250A%2520%2520Detection%26entry.906535625%3DRuijun%2520Feng%2520and%2520Hammond%2520Pearce%2520and%2520Pietro%2520Liguori%2520and%2520Yulei%2520Sui%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520proposed%2520as%2520powerful%2520tools%2520for%250Adetecting%2520software%2520vulnerabilities%252C%2520where%2520task-specific%2520fine-tuning%2520is%250Atypically%2520employed%2520to%2520provide%2520vulnerability-specific%2520knowledge%2520to%2520the%2520LLMs.%250AHowever%252C%2520existing%2520fine-tuning%2520techniques%2520often%2520treat%2520source%2520code%2520as%2520plain%2520text%252C%250Alosing%2520the%2520graph-based%2520structural%2520information%2520inherent%2520in%2520code.%250A%2520%2520Graph-enhanced%2520soft%2520prompt%2520tuning%2520addresses%2520this%2520by%2520translating%2520the%250Astructural%2520information%2520into%2520contextual%2520cues%2520that%2520the%2520LLM%2520can%2520understand.%250AHowever%252C%2520current%2520methods%2520are%2520primarily%2520designed%2520for%2520general%2520graph-related%2520tasks%250Aand%2520focus%2520more%2520on%2520adjacency%2520information%252C%2520they%2520fall%2520short%2520in%2520preserving%2520the%2520rich%250Asemantic%2520information%2520%2528e.g.%252C%2520control/data%2520flow%2529%2520within%2520code%2520graphs.%2520They%2520also%250Afail%2520to%2520ensure%2520computational%2520efficiency%2520while%2520capturing%2520graph-text%2520interactions%250Ain%2520their%2520cross-modal%2520alignment%2520module.%250A%2520%2520This%2520paper%2520presents%2520CGP-Tuning%252C%2520a%2520new%2520code%2520graph-enhanced%252C%2520structure-aware%250Asoft%2520prompt%2520tuning%2520method%2520for%2520vulnerability%2520detection.%2520CGP-Tuning%2520introduces%250Atype-aware%2520embeddings%2520to%2520capture%2520the%2520rich%2520semantic%2520information%2520within%2520code%250Agraphs%252C%2520along%2520with%2520an%2520efficient%2520cross-modal%2520alignment%2520module%2520that%2520achieves%250Alinear%2520computational%2520costs%2520while%2520incorporating%2520graph-text%2520interactions.%2520It%2520is%250Aevaluated%2520on%2520the%2520latest%2520DiverseVul%2520dataset%2520and%2520three%2520advanced%2520open-source%2520code%250ALLMs%252C%2520CodeLlama%252C%2520CodeGemma%252C%2520and%2520Qwen2.5-Coder.%2520Experimental%2520results%2520show%2520that%250ACGP-Tuning%2520delivers%2520model-agnostic%2520improvements%2520and%2520maintains%2520practical%250Ainference%2520speed%252C%2520surpassing%2520the%2520best%2520graph-enhanced%2520soft%2520prompt%2520tuning%2520baseline%250Aby%2520an%2520average%2520of%2520four%2520percentage%2520points%2520and%2520outperforming%2520non-tuned%2520zero-shot%250Aprompting%2520by%252015%2520percentage%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGP-Tuning%3A%20Structure-Aware%20Soft%20Prompt%20Tuning%20for%20Code%20Vulnerability%0A%20%20Detection&entry.906535625=Ruijun%20Feng%20and%20Hammond%20Pearce%20and%20Pietro%20Liguori%20and%20Yulei%20Sui&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20proposed%20as%20powerful%20tools%20for%0Adetecting%20software%20vulnerabilities%2C%20where%20task-specific%20fine-tuning%20is%0Atypically%20employed%20to%20provide%20vulnerability-specific%20knowledge%20to%20the%20LLMs.%0AHowever%2C%20existing%20fine-tuning%20techniques%20often%20treat%20source%20code%20as%20plain%20text%2C%0Alosing%20the%20graph-based%20structural%20information%20inherent%20in%20code.%0A%20%20Graph-enhanced%20soft%20prompt%20tuning%20addresses%20this%20by%20translating%20the%0Astructural%20information%20into%20contextual%20cues%20that%20the%20LLM%20can%20understand.%0AHowever%2C%20current%20methods%20are%20primarily%20designed%20for%20general%20graph-related%20tasks%0Aand%20focus%20more%20on%20adjacency%20information%2C%20they%20fall%20short%20in%20preserving%20the%20rich%0Asemantic%20information%20%28e.g.%2C%20control/data%20flow%29%20within%20code%20graphs.%20They%20also%0Afail%20to%20ensure%20computational%20efficiency%20while%20capturing%20graph-text%20interactions%0Ain%20their%20cross-modal%20alignment%20module.%0A%20%20This%20paper%20presents%20CGP-Tuning%2C%20a%20new%20code%20graph-enhanced%2C%20structure-aware%0Asoft%20prompt%20tuning%20method%20for%20vulnerability%20detection.%20CGP-Tuning%20introduces%0Atype-aware%20embeddings%20to%20capture%20the%20rich%20semantic%20information%20within%20code%0Agraphs%2C%20along%20with%20an%20efficient%20cross-modal%20alignment%20module%20that%20achieves%0Alinear%20computational%20costs%20while%20incorporating%20graph-text%20interactions.%20It%20is%0Aevaluated%20on%20the%20latest%20DiverseVul%20dataset%20and%20three%20advanced%20open-source%20code%0ALLMs%2C%20CodeLlama%2C%20CodeGemma%2C%20and%20Qwen2.5-Coder.%20Experimental%20results%20show%20that%0ACGP-Tuning%20delivers%20model-agnostic%20improvements%20and%20maintains%20practical%0Ainference%20speed%2C%20surpassing%20the%20best%20graph-enhanced%20soft%20prompt%20tuning%20baseline%0Aby%20an%20average%20of%20four%20percentage%20points%20and%20outperforming%20non-tuned%20zero-shot%0Aprompting%20by%2015%20percentage%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04510v2&entry.124074799=Read"},
{"title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization", "author": "Xingyu Wu and Yuchen Yan and Shangke Lyu and Linjuan Wu and Yiwen Qiu and Yongliang Shen and Weiming Lu and Jian Shao and Jun Xiao and Yueting Zhuang", "abstract": "  Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.\n", "link": "http://arxiv.org/abs/2507.15758v1", "date": "2025-07-21", "relevancy": 2.3981, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4964}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAPO%3A%20Internalizing%20Reasoning%20Efficiency%20via%20Length-Adaptive%20Policy%0A%20%20Optimization&body=Title%3A%20LAPO%3A%20Internalizing%20Reasoning%20Efficiency%20via%20Length-Adaptive%20Policy%0A%20%20Optimization%0AAuthor%3A%20Xingyu%20Wu%20and%20Yuchen%20Yan%20and%20Shangke%20Lyu%20and%20Linjuan%20Wu%20and%20Yiwen%20Qiu%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jian%20Shao%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20reasoning%20models%20have%20achieved%20remarkable%20performance%20through%20extended%0Achain-of-thought%20sequences%2C%20yet%20this%20computational%20freedom%20leads%20to%20excessive%0Atoken%20generation%20even%20for%20simple%20problems.%20We%20present%20Length-Adaptive%20Policy%0AOptimization%20%28LAPO%29%2C%20a%20novel%20framework%20that%20transforms%20reasoning%20length%20control%0Afrom%20an%20external%20constraint%20into%20an%20intrinsic%20model%20capability.%20Unlike%20existing%0Aapproaches%20that%20impose%20rigid%20limits%20or%20rely%20on%20post-hoc%20interventions%2C%20LAPO%0Aenables%20models%20to%20internalize%20an%20understanding%20of%20appropriate%20reasoning%20depth%0Athrough%20a%20two-stage%20reinforcement%20learning%20process.%20In%20the%20first%20stage%2C%20models%0Alearn%20natural%20reasoning%20patterns%20by%20discovering%20the%20statistical%20distribution%20of%0Asuccessful%20solution%20lengths.%20The%20second%20stage%20leverages%20these%20patterns%20as%0Ameta-cognitive%20guidance%2C%20embedding%20them%20directly%20within%20the%20model%27s%20reasoning%0Acontext%20to%20ensure%20inference-time%20flexibility.%20Experiments%20on%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20LAPO%20reduces%20token%20usage%20by%20up%20to%2040.9%5C%25%0Awhile%20improving%20accuracy%20by%202.3%5C%25.%20Our%20analysis%20reveals%20that%20models%20trained%0Awith%20LAPO%20develop%20emergent%20abilities%20to%20allocate%20computational%20resources%20based%0Aon%20problem%20complexity%2C%20achieving%20efficient%20reasoning%20without%20sacrificing%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAPO%253A%2520Internalizing%2520Reasoning%2520Efficiency%2520via%2520Length-Adaptive%2520Policy%250A%2520%2520Optimization%26entry.906535625%3DXingyu%2520Wu%2520and%2520Yuchen%2520Yan%2520and%2520Shangke%2520Lyu%2520and%2520Linjuan%2520Wu%2520and%2520Yiwen%2520Qiu%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Jian%2520Shao%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520have%2520achieved%2520remarkable%2520performance%2520through%2520extended%250Achain-of-thought%2520sequences%252C%2520yet%2520this%2520computational%2520freedom%2520leads%2520to%2520excessive%250Atoken%2520generation%2520even%2520for%2520simple%2520problems.%2520We%2520present%2520Length-Adaptive%2520Policy%250AOptimization%2520%2528LAPO%2529%252C%2520a%2520novel%2520framework%2520that%2520transforms%2520reasoning%2520length%2520control%250Afrom%2520an%2520external%2520constraint%2520into%2520an%2520intrinsic%2520model%2520capability.%2520Unlike%2520existing%250Aapproaches%2520that%2520impose%2520rigid%2520limits%2520or%2520rely%2520on%2520post-hoc%2520interventions%252C%2520LAPO%250Aenables%2520models%2520to%2520internalize%2520an%2520understanding%2520of%2520appropriate%2520reasoning%2520depth%250Athrough%2520a%2520two-stage%2520reinforcement%2520learning%2520process.%2520In%2520the%2520first%2520stage%252C%2520models%250Alearn%2520natural%2520reasoning%2520patterns%2520by%2520discovering%2520the%2520statistical%2520distribution%2520of%250Asuccessful%2520solution%2520lengths.%2520The%2520second%2520stage%2520leverages%2520these%2520patterns%2520as%250Ameta-cognitive%2520guidance%252C%2520embedding%2520them%2520directly%2520within%2520the%2520model%2527s%2520reasoning%250Acontext%2520to%2520ensure%2520inference-time%2520flexibility.%2520Experiments%2520on%2520mathematical%250Areasoning%2520benchmarks%2520demonstrate%2520that%2520LAPO%2520reduces%2520token%2520usage%2520by%2520up%2520to%252040.9%255C%2525%250Awhile%2520improving%2520accuracy%2520by%25202.3%255C%2525.%2520Our%2520analysis%2520reveals%2520that%2520models%2520trained%250Awith%2520LAPO%2520develop%2520emergent%2520abilities%2520to%2520allocate%2520computational%2520resources%2520based%250Aon%2520problem%2520complexity%252C%2520achieving%2520efficient%2520reasoning%2520without%2520sacrificing%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAPO%3A%20Internalizing%20Reasoning%20Efficiency%20via%20Length-Adaptive%20Policy%0A%20%20Optimization&entry.906535625=Xingyu%20Wu%20and%20Yuchen%20Yan%20and%20Shangke%20Lyu%20and%20Linjuan%20Wu%20and%20Yiwen%20Qiu%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jian%20Shao%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20reasoning%20models%20have%20achieved%20remarkable%20performance%20through%20extended%0Achain-of-thought%20sequences%2C%20yet%20this%20computational%20freedom%20leads%20to%20excessive%0Atoken%20generation%20even%20for%20simple%20problems.%20We%20present%20Length-Adaptive%20Policy%0AOptimization%20%28LAPO%29%2C%20a%20novel%20framework%20that%20transforms%20reasoning%20length%20control%0Afrom%20an%20external%20constraint%20into%20an%20intrinsic%20model%20capability.%20Unlike%20existing%0Aapproaches%20that%20impose%20rigid%20limits%20or%20rely%20on%20post-hoc%20interventions%2C%20LAPO%0Aenables%20models%20to%20internalize%20an%20understanding%20of%20appropriate%20reasoning%20depth%0Athrough%20a%20two-stage%20reinforcement%20learning%20process.%20In%20the%20first%20stage%2C%20models%0Alearn%20natural%20reasoning%20patterns%20by%20discovering%20the%20statistical%20distribution%20of%0Asuccessful%20solution%20lengths.%20The%20second%20stage%20leverages%20these%20patterns%20as%0Ameta-cognitive%20guidance%2C%20embedding%20them%20directly%20within%20the%20model%27s%20reasoning%0Acontext%20to%20ensure%20inference-time%20flexibility.%20Experiments%20on%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20LAPO%20reduces%20token%20usage%20by%20up%20to%2040.9%5C%25%0Awhile%20improving%20accuracy%20by%202.3%5C%25.%20Our%20analysis%20reveals%20that%20models%20trained%0Awith%20LAPO%20develop%20emergent%20abilities%20to%20allocate%20computational%20resources%20based%0Aon%20problem%20complexity%2C%20achieving%20efficient%20reasoning%20without%20sacrificing%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15758v1&entry.124074799=Read"},
{"title": "Optimizing Canaries for Privacy Auditing with Metagradient Descent", "author": "Matteo Boglioni and Terrance Liu and Andrew Ilyas and Zhiwei Steven Wu", "abstract": "  In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.\n", "link": "http://arxiv.org/abs/2507.15836v1", "date": "2025-07-21", "relevancy": 2.3753, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4775}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Canaries%20for%20Privacy%20Auditing%20with%20Metagradient%20Descent&body=Title%3A%20Optimizing%20Canaries%20for%20Privacy%20Auditing%20with%20Metagradient%20Descent%0AAuthor%3A%20Matteo%20Boglioni%20and%20Terrance%20Liu%20and%20Andrew%20Ilyas%20and%20Zhiwei%20Steven%20Wu%0AAbstract%3A%20%20%20In%20this%20work%20we%20study%20black-box%20privacy%20auditing%2C%20where%20the%20goal%20is%20to%20lower%0Abound%20the%20privacy%20parameter%20of%20a%20differentially%20private%20learning%20algorithm%0Ausing%20only%20the%20algorithm%27s%20outputs%20%28i.e.%2C%20final%20trained%20model%29.%20For%20DP-SGD%20%28the%0Amost%20successful%20method%20for%20training%20differentially%20private%20deep%20learning%0Amodels%29%2C%20the%20canonical%20approach%20auditing%20uses%20membership%20inference-an%20auditor%0Acomes%20with%20a%20small%20set%20of%20special%20%22canary%22%20examples%2C%20inserts%20a%20random%20subset%20of%0Athem%20into%20the%20training%20set%2C%20and%20then%20tries%20to%20discern%20which%20of%20their%20canaries%0Awere%20included%20in%20the%20training%20set%20%28typically%20via%20a%20membership%20inference%0Aattack%29.%20The%20auditor%27s%20success%20rate%20then%20provides%20a%20lower%20bound%20on%20the%20privacy%0Aparameters%20of%20the%20learning%20algorithm.%20Our%20main%20contribution%20is%20a%20method%20for%0Aoptimizing%20the%20auditor%27s%20canary%20set%20to%20improve%20privacy%20auditing%2C%20leveraging%0Arecent%20work%20on%20metagradient%20optimization.%20Our%20empirical%20evaluation%20demonstrates%0Athat%20by%20using%20such%20optimized%20canaries%2C%20we%20can%20improve%20empirical%20lower%20bounds%0Afor%20differentially%20private%20image%20classification%20models%20by%20over%202x%20in%20certain%0Ainstances.%20Furthermore%2C%20we%20demonstrate%20that%20our%20method%20is%20transferable%20and%0Aefficient%3A%20canaries%20optimized%20for%20non-private%20SGD%20with%20a%20small%20model%0Aarchitecture%20remain%20effective%20when%20auditing%20larger%20models%20trained%20with%20DP-SGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Canaries%2520for%2520Privacy%2520Auditing%2520with%2520Metagradient%2520Descent%26entry.906535625%3DMatteo%2520Boglioni%2520and%2520Terrance%2520Liu%2520and%2520Andrew%2520Ilyas%2520and%2520Zhiwei%2520Steven%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520study%2520black-box%2520privacy%2520auditing%252C%2520where%2520the%2520goal%2520is%2520to%2520lower%250Abound%2520the%2520privacy%2520parameter%2520of%2520a%2520differentially%2520private%2520learning%2520algorithm%250Ausing%2520only%2520the%2520algorithm%2527s%2520outputs%2520%2528i.e.%252C%2520final%2520trained%2520model%2529.%2520For%2520DP-SGD%2520%2528the%250Amost%2520successful%2520method%2520for%2520training%2520differentially%2520private%2520deep%2520learning%250Amodels%2529%252C%2520the%2520canonical%2520approach%2520auditing%2520uses%2520membership%2520inference-an%2520auditor%250Acomes%2520with%2520a%2520small%2520set%2520of%2520special%2520%2522canary%2522%2520examples%252C%2520inserts%2520a%2520random%2520subset%2520of%250Athem%2520into%2520the%2520training%2520set%252C%2520and%2520then%2520tries%2520to%2520discern%2520which%2520of%2520their%2520canaries%250Awere%2520included%2520in%2520the%2520training%2520set%2520%2528typically%2520via%2520a%2520membership%2520inference%250Aattack%2529.%2520The%2520auditor%2527s%2520success%2520rate%2520then%2520provides%2520a%2520lower%2520bound%2520on%2520the%2520privacy%250Aparameters%2520of%2520the%2520learning%2520algorithm.%2520Our%2520main%2520contribution%2520is%2520a%2520method%2520for%250Aoptimizing%2520the%2520auditor%2527s%2520canary%2520set%2520to%2520improve%2520privacy%2520auditing%252C%2520leveraging%250Arecent%2520work%2520on%2520metagradient%2520optimization.%2520Our%2520empirical%2520evaluation%2520demonstrates%250Athat%2520by%2520using%2520such%2520optimized%2520canaries%252C%2520we%2520can%2520improve%2520empirical%2520lower%2520bounds%250Afor%2520differentially%2520private%2520image%2520classification%2520models%2520by%2520over%25202x%2520in%2520certain%250Ainstances.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520is%2520transferable%2520and%250Aefficient%253A%2520canaries%2520optimized%2520for%2520non-private%2520SGD%2520with%2520a%2520small%2520model%250Aarchitecture%2520remain%2520effective%2520when%2520auditing%2520larger%2520models%2520trained%2520with%2520DP-SGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Canaries%20for%20Privacy%20Auditing%20with%20Metagradient%20Descent&entry.906535625=Matteo%20Boglioni%20and%20Terrance%20Liu%20and%20Andrew%20Ilyas%20and%20Zhiwei%20Steven%20Wu&entry.1292438233=%20%20In%20this%20work%20we%20study%20black-box%20privacy%20auditing%2C%20where%20the%20goal%20is%20to%20lower%0Abound%20the%20privacy%20parameter%20of%20a%20differentially%20private%20learning%20algorithm%0Ausing%20only%20the%20algorithm%27s%20outputs%20%28i.e.%2C%20final%20trained%20model%29.%20For%20DP-SGD%20%28the%0Amost%20successful%20method%20for%20training%20differentially%20private%20deep%20learning%0Amodels%29%2C%20the%20canonical%20approach%20auditing%20uses%20membership%20inference-an%20auditor%0Acomes%20with%20a%20small%20set%20of%20special%20%22canary%22%20examples%2C%20inserts%20a%20random%20subset%20of%0Athem%20into%20the%20training%20set%2C%20and%20then%20tries%20to%20discern%20which%20of%20their%20canaries%0Awere%20included%20in%20the%20training%20set%20%28typically%20via%20a%20membership%20inference%0Aattack%29.%20The%20auditor%27s%20success%20rate%20then%20provides%20a%20lower%20bound%20on%20the%20privacy%0Aparameters%20of%20the%20learning%20algorithm.%20Our%20main%20contribution%20is%20a%20method%20for%0Aoptimizing%20the%20auditor%27s%20canary%20set%20to%20improve%20privacy%20auditing%2C%20leveraging%0Arecent%20work%20on%20metagradient%20optimization.%20Our%20empirical%20evaluation%20demonstrates%0Athat%20by%20using%20such%20optimized%20canaries%2C%20we%20can%20improve%20empirical%20lower%20bounds%0Afor%20differentially%20private%20image%20classification%20models%20by%20over%202x%20in%20certain%0Ainstances.%20Furthermore%2C%20we%20demonstrate%20that%20our%20method%20is%20transferable%20and%0Aefficient%3A%20canaries%20optimized%20for%20non-private%20SGD%20with%20a%20small%20model%0Aarchitecture%20remain%20effective%20when%20auditing%20larger%20models%20trained%20with%20DP-SGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15836v1&entry.124074799=Read"},
{"title": "Efficient Multi-Camera Tokenization with Triplanes for End-to-End\n  Driving", "author": "Boris Ivanovic and Cristiano Saltori and Yurong You and Yan Wang and Wenjie Luo and Marco Pavone", "abstract": "  Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations.\n", "link": "http://arxiv.org/abs/2506.12251v2", "date": "2025-07-21", "relevancy": 2.3709, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6121}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Multi-Camera%20Tokenization%20with%20Triplanes%20for%20End-to-End%0A%20%20Driving&body=Title%3A%20Efficient%20Multi-Camera%20Tokenization%20with%20Triplanes%20for%20End-to-End%0A%20%20Driving%0AAuthor%3A%20Boris%20Ivanovic%20and%20Cristiano%20Saltori%20and%20Yurong%20You%20and%20Yan%20Wang%20and%20Wenjie%20Luo%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20Autoregressive%20Transformers%20are%20increasingly%20being%20deployed%20as%20end-to-end%0Arobot%20and%20autonomous%20vehicle%20%28AV%29%20policy%20architectures%2C%20owing%20to%20their%0Ascalability%20and%20potential%20to%20leverage%20internet-scale%20pretraining%20for%0Ageneralization.%20Accordingly%2C%20tokenizing%20sensor%20data%20efficiently%20is%20paramount%20to%0Aensuring%20the%20real-time%20feasibility%20of%20such%20architectures%20on%20embedded%20hardware.%0ATo%20this%20end%2C%20we%20present%20an%20efficient%20triplane-based%20multi-camera%20tokenization%0Astrategy%20that%20leverages%20recent%20advances%20in%203D%20neural%20reconstruction%20and%0Arendering%20to%20produce%20sensor%20tokens%20that%20are%20agnostic%20to%20the%20number%20of%20input%0Acameras%20and%20their%20resolution%2C%20while%20explicitly%20accounting%20for%20their%20geometry%0Aaround%20an%20AV.%20Experiments%20on%20a%20large-scale%20AV%20dataset%20and%20state-of-the-art%0Aneural%20simulator%20demonstrate%20that%20our%20approach%20yields%20significant%20savings%20over%0Acurrent%20image%20patch-based%20tokenization%20strategies%2C%20producing%20up%20to%2072%25%20fewer%0Atokens%2C%20resulting%20in%20up%20to%2050%25%20faster%20policy%20inference%20while%20achieving%20the%20same%0Aopen-loop%20motion%20planning%20accuracy%20and%20improved%20offroad%20rates%20in%20closed-loop%0Adriving%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12251v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Multi-Camera%2520Tokenization%2520with%2520Triplanes%2520for%2520End-to-End%250A%2520%2520Driving%26entry.906535625%3DBoris%2520Ivanovic%2520and%2520Cristiano%2520Saltori%2520and%2520Yurong%2520You%2520and%2520Yan%2520Wang%2520and%2520Wenjie%2520Luo%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520Autoregressive%2520Transformers%2520are%2520increasingly%2520being%2520deployed%2520as%2520end-to-end%250Arobot%2520and%2520autonomous%2520vehicle%2520%2528AV%2529%2520policy%2520architectures%252C%2520owing%2520to%2520their%250Ascalability%2520and%2520potential%2520to%2520leverage%2520internet-scale%2520pretraining%2520for%250Ageneralization.%2520Accordingly%252C%2520tokenizing%2520sensor%2520data%2520efficiently%2520is%2520paramount%2520to%250Aensuring%2520the%2520real-time%2520feasibility%2520of%2520such%2520architectures%2520on%2520embedded%2520hardware.%250ATo%2520this%2520end%252C%2520we%2520present%2520an%2520efficient%2520triplane-based%2520multi-camera%2520tokenization%250Astrategy%2520that%2520leverages%2520recent%2520advances%2520in%25203D%2520neural%2520reconstruction%2520and%250Arendering%2520to%2520produce%2520sensor%2520tokens%2520that%2520are%2520agnostic%2520to%2520the%2520number%2520of%2520input%250Acameras%2520and%2520their%2520resolution%252C%2520while%2520explicitly%2520accounting%2520for%2520their%2520geometry%250Aaround%2520an%2520AV.%2520Experiments%2520on%2520a%2520large-scale%2520AV%2520dataset%2520and%2520state-of-the-art%250Aneural%2520simulator%2520demonstrate%2520that%2520our%2520approach%2520yields%2520significant%2520savings%2520over%250Acurrent%2520image%2520patch-based%2520tokenization%2520strategies%252C%2520producing%2520up%2520to%252072%2525%2520fewer%250Atokens%252C%2520resulting%2520in%2520up%2520to%252050%2525%2520faster%2520policy%2520inference%2520while%2520achieving%2520the%2520same%250Aopen-loop%2520motion%2520planning%2520accuracy%2520and%2520improved%2520offroad%2520rates%2520in%2520closed-loop%250Adriving%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12251v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Multi-Camera%20Tokenization%20with%20Triplanes%20for%20End-to-End%0A%20%20Driving&entry.906535625=Boris%20Ivanovic%20and%20Cristiano%20Saltori%20and%20Yurong%20You%20and%20Yan%20Wang%20and%20Wenjie%20Luo%20and%20Marco%20Pavone&entry.1292438233=%20%20Autoregressive%20Transformers%20are%20increasingly%20being%20deployed%20as%20end-to-end%0Arobot%20and%20autonomous%20vehicle%20%28AV%29%20policy%20architectures%2C%20owing%20to%20their%0Ascalability%20and%20potential%20to%20leverage%20internet-scale%20pretraining%20for%0Ageneralization.%20Accordingly%2C%20tokenizing%20sensor%20data%20efficiently%20is%20paramount%20to%0Aensuring%20the%20real-time%20feasibility%20of%20such%20architectures%20on%20embedded%20hardware.%0ATo%20this%20end%2C%20we%20present%20an%20efficient%20triplane-based%20multi-camera%20tokenization%0Astrategy%20that%20leverages%20recent%20advances%20in%203D%20neural%20reconstruction%20and%0Arendering%20to%20produce%20sensor%20tokens%20that%20are%20agnostic%20to%20the%20number%20of%20input%0Acameras%20and%20their%20resolution%2C%20while%20explicitly%20accounting%20for%20their%20geometry%0Aaround%20an%20AV.%20Experiments%20on%20a%20large-scale%20AV%20dataset%20and%20state-of-the-art%0Aneural%20simulator%20demonstrate%20that%20our%20approach%20yields%20significant%20savings%20over%0Acurrent%20image%20patch-based%20tokenization%20strategies%2C%20producing%20up%20to%2072%25%20fewer%0Atokens%2C%20resulting%20in%20up%20to%2050%25%20faster%20policy%20inference%20while%20achieving%20the%20same%0Aopen-loop%20motion%20planning%20accuracy%20and%20improved%20offroad%20rates%20in%20closed-loop%0Adriving%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12251v2&entry.124074799=Read"},
{"title": "Adaptive Visuo-Tactile Fusion with Predictive Force Attention for\n  Dexterous Manipulation", "author": "Jinzhou Li and Tianhao Wu and Jiyao Zhang and Zeyuan Chen and Haotian Jin and Mingdong Wu and Yujun Shen and Yaodong Yang and Hao Dong", "abstract": "  Effectively utilizing multi-sensory data is important for robots to\ngeneralize across diverse tasks. However, the heterogeneous nature of these\nmodalities makes fusion challenging. Existing methods propose strategies to\nobtain comprehensively fused features but often ignore the fact that each\nmodality requires different levels of attention at different manipulation\nstages. To address this, we propose a force-guided attention fusion module that\nadaptively adjusts the weights of visual and tactile features without human\nlabeling. We also introduce a self-supervised future force prediction auxiliary\ntask to reinforce the tactile modality, improve data imbalance, and encourage\nproper adjustment. Our method achieves an average success rate of 93% across\nthree fine-grained, contactrich tasks in real-world experiments. Further\nanalysis shows that our policy appropriately adjusts attention to each modality\nat different manipulation stages. The videos can be viewed at\nhttps://adaptac-dex.github.io/.\n", "link": "http://arxiv.org/abs/2505.13982v2", "date": "2025-07-21", "relevancy": 2.3618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6128}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Visuo-Tactile%20Fusion%20with%20Predictive%20Force%20Attention%20for%0A%20%20Dexterous%20Manipulation&body=Title%3A%20Adaptive%20Visuo-Tactile%20Fusion%20with%20Predictive%20Force%20Attention%20for%0A%20%20Dexterous%20Manipulation%0AAuthor%3A%20Jinzhou%20Li%20and%20Tianhao%20Wu%20and%20Jiyao%20Zhang%20and%20Zeyuan%20Chen%20and%20Haotian%20Jin%20and%20Mingdong%20Wu%20and%20Yujun%20Shen%20and%20Yaodong%20Yang%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Effectively%20utilizing%20multi-sensory%20data%20is%20important%20for%20robots%20to%0Ageneralize%20across%20diverse%20tasks.%20However%2C%20the%20heterogeneous%20nature%20of%20these%0Amodalities%20makes%20fusion%20challenging.%20Existing%20methods%20propose%20strategies%20to%0Aobtain%20comprehensively%20fused%20features%20but%20often%20ignore%20the%20fact%20that%20each%0Amodality%20requires%20different%20levels%20of%20attention%20at%20different%20manipulation%0Astages.%20To%20address%20this%2C%20we%20propose%20a%20force-guided%20attention%20fusion%20module%20that%0Aadaptively%20adjusts%20the%20weights%20of%20visual%20and%20tactile%20features%20without%20human%0Alabeling.%20We%20also%20introduce%20a%20self-supervised%20future%20force%20prediction%20auxiliary%0Atask%20to%20reinforce%20the%20tactile%20modality%2C%20improve%20data%20imbalance%2C%20and%20encourage%0Aproper%20adjustment.%20Our%20method%20achieves%20an%20average%20success%20rate%20of%2093%25%20across%0Athree%20fine-grained%2C%20contactrich%20tasks%20in%20real-world%20experiments.%20Further%0Aanalysis%20shows%20that%20our%20policy%20appropriately%20adjusts%20attention%20to%20each%20modality%0Aat%20different%20manipulation%20stages.%20The%20videos%20can%20be%20viewed%20at%0Ahttps%3A//adaptac-dex.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Visuo-Tactile%2520Fusion%2520with%2520Predictive%2520Force%2520Attention%2520for%250A%2520%2520Dexterous%2520Manipulation%26entry.906535625%3DJinzhou%2520Li%2520and%2520Tianhao%2520Wu%2520and%2520Jiyao%2520Zhang%2520and%2520Zeyuan%2520Chen%2520and%2520Haotian%2520Jin%2520and%2520Mingdong%2520Wu%2520and%2520Yujun%2520Shen%2520and%2520Yaodong%2520Yang%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520Effectively%2520utilizing%2520multi-sensory%2520data%2520is%2520important%2520for%2520robots%2520to%250Ageneralize%2520across%2520diverse%2520tasks.%2520However%252C%2520the%2520heterogeneous%2520nature%2520of%2520these%250Amodalities%2520makes%2520fusion%2520challenging.%2520Existing%2520methods%2520propose%2520strategies%2520to%250Aobtain%2520comprehensively%2520fused%2520features%2520but%2520often%2520ignore%2520the%2520fact%2520that%2520each%250Amodality%2520requires%2520different%2520levels%2520of%2520attention%2520at%2520different%2520manipulation%250Astages.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520force-guided%2520attention%2520fusion%2520module%2520that%250Aadaptively%2520adjusts%2520the%2520weights%2520of%2520visual%2520and%2520tactile%2520features%2520without%2520human%250Alabeling.%2520We%2520also%2520introduce%2520a%2520self-supervised%2520future%2520force%2520prediction%2520auxiliary%250Atask%2520to%2520reinforce%2520the%2520tactile%2520modality%252C%2520improve%2520data%2520imbalance%252C%2520and%2520encourage%250Aproper%2520adjustment.%2520Our%2520method%2520achieves%2520an%2520average%2520success%2520rate%2520of%252093%2525%2520across%250Athree%2520fine-grained%252C%2520contactrich%2520tasks%2520in%2520real-world%2520experiments.%2520Further%250Aanalysis%2520shows%2520that%2520our%2520policy%2520appropriately%2520adjusts%2520attention%2520to%2520each%2520modality%250Aat%2520different%2520manipulation%2520stages.%2520The%2520videos%2520can%2520be%2520viewed%2520at%250Ahttps%253A//adaptac-dex.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Visuo-Tactile%20Fusion%20with%20Predictive%20Force%20Attention%20for%0A%20%20Dexterous%20Manipulation&entry.906535625=Jinzhou%20Li%20and%20Tianhao%20Wu%20and%20Jiyao%20Zhang%20and%20Zeyuan%20Chen%20and%20Haotian%20Jin%20and%20Mingdong%20Wu%20and%20Yujun%20Shen%20and%20Yaodong%20Yang%20and%20Hao%20Dong&entry.1292438233=%20%20Effectively%20utilizing%20multi-sensory%20data%20is%20important%20for%20robots%20to%0Ageneralize%20across%20diverse%20tasks.%20However%2C%20the%20heterogeneous%20nature%20of%20these%0Amodalities%20makes%20fusion%20challenging.%20Existing%20methods%20propose%20strategies%20to%0Aobtain%20comprehensively%20fused%20features%20but%20often%20ignore%20the%20fact%20that%20each%0Amodality%20requires%20different%20levels%20of%20attention%20at%20different%20manipulation%0Astages.%20To%20address%20this%2C%20we%20propose%20a%20force-guided%20attention%20fusion%20module%20that%0Aadaptively%20adjusts%20the%20weights%20of%20visual%20and%20tactile%20features%20without%20human%0Alabeling.%20We%20also%20introduce%20a%20self-supervised%20future%20force%20prediction%20auxiliary%0Atask%20to%20reinforce%20the%20tactile%20modality%2C%20improve%20data%20imbalance%2C%20and%20encourage%0Aproper%20adjustment.%20Our%20method%20achieves%20an%20average%20success%20rate%20of%2093%25%20across%0Athree%20fine-grained%2C%20contactrich%20tasks%20in%20real-world%20experiments.%20Further%0Aanalysis%20shows%20that%20our%20policy%20appropriately%20adjusts%20attention%20to%20each%20modality%0Aat%20different%20manipulation%20stages.%20The%20videos%20can%20be%20viewed%20at%0Ahttps%3A//adaptac-dex.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13982v2&entry.124074799=Read"},
{"title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and\n  Online Residual Learning", "author": "Yanbo Chen and Yunzhe Tan and Yaojia Wang and Zhengzhe Xu and Junbo Tan and Xueqian Wang", "abstract": "  Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.\n", "link": "http://arxiv.org/abs/2507.15607v1", "date": "2025-07-21", "relevancy": 2.344, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5964}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Vehicle-Trailer%20Navigation%20System%20with%20Neural%20Kinematics%20and%0A%20%20Online%20Residual%20Learning&body=Title%3A%20A%20Universal%20Vehicle-Trailer%20Navigation%20System%20with%20Neural%20Kinematics%20and%0A%20%20Online%20Residual%20Learning%0AAuthor%3A%20Yanbo%20Chen%20and%20Yunzhe%20Tan%20and%20Yaojia%20Wang%20and%20Zhengzhe%20Xu%20and%20Junbo%20Tan%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Autonomous%20navigation%20of%20vehicle-trailer%20systems%20is%20crucial%20in%20environments%0Alike%20airports%2C%20supermarkets%2C%20and%20concert%20venues%2C%20where%20various%20types%20of%0Atrailers%20are%20needed%20to%20navigate%20with%20different%20payloads%20and%20conditions.%0AHowever%2C%20accurately%20modeling%20such%20systems%20remains%20challenging%2C%20especially%20for%0Atrailers%20with%20castor%20wheels.%20In%20this%20work%2C%20we%20propose%20a%20novel%20universal%0Avehicle-trailer%20navigation%20system%20that%20integrates%20a%20hybrid%20nominal%20kinematic%0Amodel--combining%20classical%20nonholonomic%20constraints%20for%20vehicles%20and%20neural%0Anetwork-based%20trailer%20kinematics--with%20a%20lightweight%20online%20residual%20learning%0Amodule%20to%20correct%20real-time%20modeling%20discrepancies%20and%20disturbances.%0AAdditionally%2C%20we%20develop%20a%20model%20predictive%20control%20framework%20with%20a%20weighted%0Amodel%20combination%20strategy%20that%20improves%20long-horizon%20prediction%20accuracy%20and%0Aensures%20safer%20motion%20planning.%20Our%20approach%20is%20validated%20through%20extensive%0Areal-world%20experiments%20involving%20multiple%20trailer%20types%20and%20varying%20payload%0Aconditions%2C%20demonstrating%20robust%20performance%20without%20manual%20tuning%20or%0Atrailer-specific%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Vehicle-Trailer%2520Navigation%2520System%2520with%2520Neural%2520Kinematics%2520and%250A%2520%2520Online%2520Residual%2520Learning%26entry.906535625%3DYanbo%2520Chen%2520and%2520Yunzhe%2520Tan%2520and%2520Yaojia%2520Wang%2520and%2520Zhengzhe%2520Xu%2520and%2520Junbo%2520Tan%2520and%2520Xueqian%2520Wang%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520of%2520vehicle-trailer%2520systems%2520is%2520crucial%2520in%2520environments%250Alike%2520airports%252C%2520supermarkets%252C%2520and%2520concert%2520venues%252C%2520where%2520various%2520types%2520of%250Atrailers%2520are%2520needed%2520to%2520navigate%2520with%2520different%2520payloads%2520and%2520conditions.%250AHowever%252C%2520accurately%2520modeling%2520such%2520systems%2520remains%2520challenging%252C%2520especially%2520for%250Atrailers%2520with%2520castor%2520wheels.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520universal%250Avehicle-trailer%2520navigation%2520system%2520that%2520integrates%2520a%2520hybrid%2520nominal%2520kinematic%250Amodel--combining%2520classical%2520nonholonomic%2520constraints%2520for%2520vehicles%2520and%2520neural%250Anetwork-based%2520trailer%2520kinematics--with%2520a%2520lightweight%2520online%2520residual%2520learning%250Amodule%2520to%2520correct%2520real-time%2520modeling%2520discrepancies%2520and%2520disturbances.%250AAdditionally%252C%2520we%2520develop%2520a%2520model%2520predictive%2520control%2520framework%2520with%2520a%2520weighted%250Amodel%2520combination%2520strategy%2520that%2520improves%2520long-horizon%2520prediction%2520accuracy%2520and%250Aensures%2520safer%2520motion%2520planning.%2520Our%2520approach%2520is%2520validated%2520through%2520extensive%250Areal-world%2520experiments%2520involving%2520multiple%2520trailer%2520types%2520and%2520varying%2520payload%250Aconditions%252C%2520demonstrating%2520robust%2520performance%2520without%2520manual%2520tuning%2520or%250Atrailer-specific%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Vehicle-Trailer%20Navigation%20System%20with%20Neural%20Kinematics%20and%0A%20%20Online%20Residual%20Learning&entry.906535625=Yanbo%20Chen%20and%20Yunzhe%20Tan%20and%20Yaojia%20Wang%20and%20Zhengzhe%20Xu%20and%20Junbo%20Tan%20and%20Xueqian%20Wang&entry.1292438233=%20%20Autonomous%20navigation%20of%20vehicle-trailer%20systems%20is%20crucial%20in%20environments%0Alike%20airports%2C%20supermarkets%2C%20and%20concert%20venues%2C%20where%20various%20types%20of%0Atrailers%20are%20needed%20to%20navigate%20with%20different%20payloads%20and%20conditions.%0AHowever%2C%20accurately%20modeling%20such%20systems%20remains%20challenging%2C%20especially%20for%0Atrailers%20with%20castor%20wheels.%20In%20this%20work%2C%20we%20propose%20a%20novel%20universal%0Avehicle-trailer%20navigation%20system%20that%20integrates%20a%20hybrid%20nominal%20kinematic%0Amodel--combining%20classical%20nonholonomic%20constraints%20for%20vehicles%20and%20neural%0Anetwork-based%20trailer%20kinematics--with%20a%20lightweight%20online%20residual%20learning%0Amodule%20to%20correct%20real-time%20modeling%20discrepancies%20and%20disturbances.%0AAdditionally%2C%20we%20develop%20a%20model%20predictive%20control%20framework%20with%20a%20weighted%0Amodel%20combination%20strategy%20that%20improves%20long-horizon%20prediction%20accuracy%20and%0Aensures%20safer%20motion%20planning.%20Our%20approach%20is%20validated%20through%20extensive%0Areal-world%20experiments%20involving%20multiple%20trailer%20types%20and%20varying%20payload%0Aconditions%2C%20demonstrating%20robust%20performance%20without%20manual%20tuning%20or%0Atrailer-specific%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15607v1&entry.124074799=Read"},
{"title": "True Multimodal In-Context Learning Needs Attention to the Visual\n  Context", "author": "Shuo Chen and Jianzhe Liu and Zhen Han and Yan Xia and Daniel Cremers and Philip Torr and Volker Tresp and Jindong Gu", "abstract": "  Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .\n", "link": "http://arxiv.org/abs/2507.15807v1", "date": "2025-07-21", "relevancy": 2.3438, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6122}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20True%20Multimodal%20In-Context%20Learning%20Needs%20Attention%20to%20the%20Visual%0A%20%20Context&body=Title%3A%20True%20Multimodal%20In-Context%20Learning%20Needs%20Attention%20to%20the%20Visual%0A%20%20Context%0AAuthor%3A%20Shuo%20Chen%20and%20Jianzhe%20Liu%20and%20Zhen%20Han%20and%20Yan%20Xia%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20built%20on%20powerful%20language%0Abackbones%2C%20have%20enabled%20Multimodal%20In-Context%20Learning%20%28MICL%29-adapting%20to%20new%0Atasks%20from%20a%20few%20multimodal%20demonstrations%20consisting%20of%20images%2C%20questions%2C%20and%0Aanswers.%20Despite%20showing%20noticeable%20improvement%20on%20standard%20vision-language%0Adatasets%2C%20current%20MLLMs%20struggle%20to%20leverage%20visual%20information%20in%20the%0Ademonstrations.%20Specifically%2C%20they%20tend%20to%20neglect%20visual%20cues%20and%20over-rely%20on%0Atextual%20patterns%2C%20leading%20to%20mere%20text%20imitation%20rather%20than%20genuine%20multimodal%0Aadaptation.%20This%20behavior%20makes%20MICL%20still%20unimodal%20and%20largely%20restricts%20its%0Apractical%20utility.%20More%20importantly%2C%20this%20limitation%20is%20often%20concealed%20by%20the%0Aimproved%20performance%20on%20tasks%20that%20do%20not%20require%20understanding%20the%20visual%0Acontext.%20As%20a%20result%2C%20how%20to%20effectively%20enhance%20MICL%20ability%20and%20reliably%0Aevaluate%20the%20MICL%20performance%20remains%20underexplored.%20To%20address%20these%20issues%2C%0Awe%20first%20introduce%20Dynamic%20Attention%20Reallocation%20%28DARA%29%2C%20an%20efficient%0Afine-tuning%20strategy%20that%20encourages%20models%20to%20attend%20to%20the%20visual%20context%20by%0Arebalancing%20attention%20across%20visual%20and%20textual%20tokens.%20In%20addition%2C%20we%20present%0ATrueMICL%2C%20an%20MICL-dedicated%20dataset%20with%20both%20support%20and%20test%20sets%20that%0Aexplicitly%20requires%20the%20integration%20of%20multimodal%20information-particularly%0Avisual%20content-for%20correct%20task%20completion.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20holistic%20solution%2C%20showcasing%20substantial%20improvements%0Ain%20the%20true%20multimodal%20in-context%20learning%20capabilities.%20Code%20and%20datasets%20are%0Aavailable%20at%20https%3A//chenxshuo.github.io/true-micl-colm%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrue%2520Multimodal%2520In-Context%2520Learning%2520Needs%2520Attention%2520to%2520the%2520Visual%250A%2520%2520Context%26entry.906535625%3DShuo%2520Chen%2520and%2520Jianzhe%2520Liu%2520and%2520Zhen%2520Han%2520and%2520Yan%2520Xia%2520and%2520Daniel%2520Cremers%2520and%2520Philip%2520Torr%2520and%2520Volker%2520Tresp%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520built%2520on%2520powerful%2520language%250Abackbones%252C%2520have%2520enabled%2520Multimodal%2520In-Context%2520Learning%2520%2528MICL%2529-adapting%2520to%2520new%250Atasks%2520from%2520a%2520few%2520multimodal%2520demonstrations%2520consisting%2520of%2520images%252C%2520questions%252C%2520and%250Aanswers.%2520Despite%2520showing%2520noticeable%2520improvement%2520on%2520standard%2520vision-language%250Adatasets%252C%2520current%2520MLLMs%2520struggle%2520to%2520leverage%2520visual%2520information%2520in%2520the%250Ademonstrations.%2520Specifically%252C%2520they%2520tend%2520to%2520neglect%2520visual%2520cues%2520and%2520over-rely%2520on%250Atextual%2520patterns%252C%2520leading%2520to%2520mere%2520text%2520imitation%2520rather%2520than%2520genuine%2520multimodal%250Aadaptation.%2520This%2520behavior%2520makes%2520MICL%2520still%2520unimodal%2520and%2520largely%2520restricts%2520its%250Apractical%2520utility.%2520More%2520importantly%252C%2520this%2520limitation%2520is%2520often%2520concealed%2520by%2520the%250Aimproved%2520performance%2520on%2520tasks%2520that%2520do%2520not%2520require%2520understanding%2520the%2520visual%250Acontext.%2520As%2520a%2520result%252C%2520how%2520to%2520effectively%2520enhance%2520MICL%2520ability%2520and%2520reliably%250Aevaluate%2520the%2520MICL%2520performance%2520remains%2520underexplored.%2520To%2520address%2520these%2520issues%252C%250Awe%2520first%2520introduce%2520Dynamic%2520Attention%2520Reallocation%2520%2528DARA%2529%252C%2520an%2520efficient%250Afine-tuning%2520strategy%2520that%2520encourages%2520models%2520to%2520attend%2520to%2520the%2520visual%2520context%2520by%250Arebalancing%2520attention%2520across%2520visual%2520and%2520textual%2520tokens.%2520In%2520addition%252C%2520we%2520present%250ATrueMICL%252C%2520an%2520MICL-dedicated%2520dataset%2520with%2520both%2520support%2520and%2520test%2520sets%2520that%250Aexplicitly%2520requires%2520the%2520integration%2520of%2520multimodal%2520information-particularly%250Avisual%2520content-for%2520correct%2520task%2520completion.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520holistic%2520solution%252C%2520showcasing%2520substantial%2520improvements%250Ain%2520the%2520true%2520multimodal%2520in-context%2520learning%2520capabilities.%2520Code%2520and%2520datasets%2520are%250Aavailable%2520at%2520https%253A//chenxshuo.github.io/true-micl-colm%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=True%20Multimodal%20In-Context%20Learning%20Needs%20Attention%20to%20the%20Visual%0A%20%20Context&entry.906535625=Shuo%20Chen%20and%20Jianzhe%20Liu%20and%20Zhen%20Han%20and%20Yan%20Xia%20and%20Daniel%20Cremers%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20built%20on%20powerful%20language%0Abackbones%2C%20have%20enabled%20Multimodal%20In-Context%20Learning%20%28MICL%29-adapting%20to%20new%0Atasks%20from%20a%20few%20multimodal%20demonstrations%20consisting%20of%20images%2C%20questions%2C%20and%0Aanswers.%20Despite%20showing%20noticeable%20improvement%20on%20standard%20vision-language%0Adatasets%2C%20current%20MLLMs%20struggle%20to%20leverage%20visual%20information%20in%20the%0Ademonstrations.%20Specifically%2C%20they%20tend%20to%20neglect%20visual%20cues%20and%20over-rely%20on%0Atextual%20patterns%2C%20leading%20to%20mere%20text%20imitation%20rather%20than%20genuine%20multimodal%0Aadaptation.%20This%20behavior%20makes%20MICL%20still%20unimodal%20and%20largely%20restricts%20its%0Apractical%20utility.%20More%20importantly%2C%20this%20limitation%20is%20often%20concealed%20by%20the%0Aimproved%20performance%20on%20tasks%20that%20do%20not%20require%20understanding%20the%20visual%0Acontext.%20As%20a%20result%2C%20how%20to%20effectively%20enhance%20MICL%20ability%20and%20reliably%0Aevaluate%20the%20MICL%20performance%20remains%20underexplored.%20To%20address%20these%20issues%2C%0Awe%20first%20introduce%20Dynamic%20Attention%20Reallocation%20%28DARA%29%2C%20an%20efficient%0Afine-tuning%20strategy%20that%20encourages%20models%20to%20attend%20to%20the%20visual%20context%20by%0Arebalancing%20attention%20across%20visual%20and%20textual%20tokens.%20In%20addition%2C%20we%20present%0ATrueMICL%2C%20an%20MICL-dedicated%20dataset%20with%20both%20support%20and%20test%20sets%20that%0Aexplicitly%20requires%20the%20integration%20of%20multimodal%20information-particularly%0Avisual%20content-for%20correct%20task%20completion.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20our%20holistic%20solution%2C%20showcasing%20substantial%20improvements%0Ain%20the%20true%20multimodal%20in-context%20learning%20capabilities.%20Code%20and%20datasets%20are%0Aavailable%20at%20https%3A//chenxshuo.github.io/true-micl-colm%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15807v1&entry.124074799=Read"},
{"title": "Latent Denoising Makes Good Visual Tokenizers", "author": "Jiawei Yang and Tianhong Li and Lijie Fan and Yonglong Tian and Yue Wang", "abstract": "  Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.\n", "link": "http://arxiv.org/abs/2507.15856v1", "date": "2025-07-21", "relevancy": 2.3149, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5451}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Denoising%20Makes%20Good%20Visual%20Tokenizers&body=Title%3A%20Latent%20Denoising%20Makes%20Good%20Visual%20Tokenizers%0AAuthor%3A%20Jiawei%20Yang%20and%20Tianhong%20Li%20and%20Lijie%20Fan%20and%20Yonglong%20Tian%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Despite%20their%20fundamental%20role%2C%20it%20remains%20unclear%20what%20properties%20could%20make%0Avisual%20tokenizers%20more%20effective%20for%20generative%20modeling.%20We%20observe%20that%0Amodern%20generative%20models%20share%20a%20conceptually%20similar%20training%20objective%20--%0Areconstructing%20clean%20signals%20from%20corrupted%20inputs%20such%20as%20Gaussian%20noise%20or%0Amasking%20--%20a%20process%20we%20term%20denoising.%20Motivated%20by%20this%20insight%2C%20we%20propose%0Aaligning%20tokenizer%20embeddings%20directly%20with%20the%20downstream%20denoising%20objective%2C%0Aencouraging%20latent%20embeddings%20to%20be%20more%20easily%20reconstructed%20even%20when%20heavily%0Acorrupted.%20To%20achieve%20this%2C%20we%20introduce%20the%20Latent%20Denoising%20Tokenizer%0A%28l-DeTok%29%2C%20a%20simple%20yet%20effective%20tokenizer%20trained%20to%20reconstruct%20clean%20images%0Afrom%20latent%20embeddings%20corrupted%20by%20interpolative%20noise%20and%20random%20masking.%0AExtensive%20experiments%20on%20ImageNet%20256x256%20demonstrate%20that%20our%20tokenizer%0Aconsistently%20outperforms%20standard%20tokenizers%20across%20six%20representative%0Agenerative%20models.%20Our%20findings%20highlight%20denoising%20as%20a%20fundamental%20design%0Aprinciple%20for%20tokenizer%20development%2C%20and%20we%20hope%20it%20could%20motivate%20new%0Aperspectives%20for%20future%20tokenizer%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Denoising%2520Makes%2520Good%2520Visual%2520Tokenizers%26entry.906535625%3DJiawei%2520Yang%2520and%2520Tianhong%2520Li%2520and%2520Lijie%2520Fan%2520and%2520Yonglong%2520Tian%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520their%2520fundamental%2520role%252C%2520it%2520remains%2520unclear%2520what%2520properties%2520could%2520make%250Avisual%2520tokenizers%2520more%2520effective%2520for%2520generative%2520modeling.%2520We%2520observe%2520that%250Amodern%2520generative%2520models%2520share%2520a%2520conceptually%2520similar%2520training%2520objective%2520--%250Areconstructing%2520clean%2520signals%2520from%2520corrupted%2520inputs%2520such%2520as%2520Gaussian%2520noise%2520or%250Amasking%2520--%2520a%2520process%2520we%2520term%2520denoising.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%250Aaligning%2520tokenizer%2520embeddings%2520directly%2520with%2520the%2520downstream%2520denoising%2520objective%252C%250Aencouraging%2520latent%2520embeddings%2520to%2520be%2520more%2520easily%2520reconstructed%2520even%2520when%2520heavily%250Acorrupted.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520the%2520Latent%2520Denoising%2520Tokenizer%250A%2528l-DeTok%2529%252C%2520a%2520simple%2520yet%2520effective%2520tokenizer%2520trained%2520to%2520reconstruct%2520clean%2520images%250Afrom%2520latent%2520embeddings%2520corrupted%2520by%2520interpolative%2520noise%2520and%2520random%2520masking.%250AExtensive%2520experiments%2520on%2520ImageNet%2520256x256%2520demonstrate%2520that%2520our%2520tokenizer%250Aconsistently%2520outperforms%2520standard%2520tokenizers%2520across%2520six%2520representative%250Agenerative%2520models.%2520Our%2520findings%2520highlight%2520denoising%2520as%2520a%2520fundamental%2520design%250Aprinciple%2520for%2520tokenizer%2520development%252C%2520and%2520we%2520hope%2520it%2520could%2520motivate%2520new%250Aperspectives%2520for%2520future%2520tokenizer%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Denoising%20Makes%20Good%20Visual%20Tokenizers&entry.906535625=Jiawei%20Yang%20and%20Tianhong%20Li%20and%20Lijie%20Fan%20and%20Yonglong%20Tian%20and%20Yue%20Wang&entry.1292438233=%20%20Despite%20their%20fundamental%20role%2C%20it%20remains%20unclear%20what%20properties%20could%20make%0Avisual%20tokenizers%20more%20effective%20for%20generative%20modeling.%20We%20observe%20that%0Amodern%20generative%20models%20share%20a%20conceptually%20similar%20training%20objective%20--%0Areconstructing%20clean%20signals%20from%20corrupted%20inputs%20such%20as%20Gaussian%20noise%20or%0Amasking%20--%20a%20process%20we%20term%20denoising.%20Motivated%20by%20this%20insight%2C%20we%20propose%0Aaligning%20tokenizer%20embeddings%20directly%20with%20the%20downstream%20denoising%20objective%2C%0Aencouraging%20latent%20embeddings%20to%20be%20more%20easily%20reconstructed%20even%20when%20heavily%0Acorrupted.%20To%20achieve%20this%2C%20we%20introduce%20the%20Latent%20Denoising%20Tokenizer%0A%28l-DeTok%29%2C%20a%20simple%20yet%20effective%20tokenizer%20trained%20to%20reconstruct%20clean%20images%0Afrom%20latent%20embeddings%20corrupted%20by%20interpolative%20noise%20and%20random%20masking.%0AExtensive%20experiments%20on%20ImageNet%20256x256%20demonstrate%20that%20our%20tokenizer%0Aconsistently%20outperforms%20standard%20tokenizers%20across%20six%20representative%0Agenerative%20models.%20Our%20findings%20highlight%20denoising%20as%20a%20fundamental%20design%0Aprinciple%20for%20tokenizer%20development%2C%20and%20we%20hope%20it%20could%20motivate%20new%0Aperspectives%20for%20future%20tokenizer%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15856v1&entry.124074799=Read"},
{"title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning", "author": "Kun Xiang and Heng Li and Terry Jingchen Zhang and Yinya Huang and Zirong Liu and Peixin Qu and Jixi He and Jiaqi Chen and Yu-Jie Yuan and Jianhua Han and Hang Xu and Hanhui Li and Mrinmaya Sachan and Xiaodan Liang", "abstract": "  We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.\n", "link": "http://arxiv.org/abs/2505.19099v5", "date": "2025-07-21", "relevancy": 2.2771, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning&body=Title%3A%20SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning%0AAuthor%3A%20Kun%20Xiang%20and%20Heng%20Li%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Zirong%20Liu%20and%20Peixin%20Qu%20and%20Jixi%20He%20and%20Jiaqi%20Chen%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Hanhui%20Li%20and%20Mrinmaya%20Sachan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20We%20present%20SeePhys%2C%20a%20large-scale%20multimodal%20benchmark%20for%20LLM%20reasoning%0Agrounded%20in%20physics%20questions%20ranging%20from%20middle%20school%20to%20PhD%20qualifying%0Aexams.%20The%20benchmark%20covers%207%20fundamental%20domains%20spanning%20the%20physics%0Adiscipline%2C%20incorporating%2021%20categories%20of%20highly%20heterogeneous%20diagrams.%20In%0Acontrast%20to%20prior%20works%20where%20visual%20elements%20mainly%20serve%20auxiliary%20purposes%2C%0Aour%20benchmark%20features%20a%20substantial%20proportion%20of%20vision-essential%20problems%0A%2875%25%29%20that%20mandate%20visual%20information%20extraction%20for%20correct%20solutions.%20Through%0Aextensive%20evaluation%2C%20we%20observe%20that%20even%20the%20most%20advanced%20visual%20reasoning%0Amodels%20%28e.g.%2C%20Gemini-2.5-pro%20and%20o4-mini%29%20achieve%20sub-60%25%20accuracy%20on%20our%0Abenchmark.%20These%20results%20reveal%20fundamental%20challenges%20in%20current%20large%0Alanguage%20models%27%20visual%20understanding%20capabilities%2C%20particularly%20in%3A%20%28i%29%0Aestablishing%20rigorous%20coupling%20between%20diagram%20interpretation%20and%20physics%0Areasoning%2C%20and%20%28ii%29%20overcoming%20their%20persistent%20reliance%20on%20textual%20cues%20as%0Acognitive%20shortcuts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19099v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeePhys%253A%2520Does%2520Seeing%2520Help%2520Thinking%253F%2520--%2520Benchmarking%2520Vision-Based%2520Physics%250A%2520%2520Reasoning%26entry.906535625%3DKun%2520Xiang%2520and%2520Heng%2520Li%2520and%2520Terry%2520Jingchen%2520Zhang%2520and%2520Yinya%2520Huang%2520and%2520Zirong%2520Liu%2520and%2520Peixin%2520Qu%2520and%2520Jixi%2520He%2520and%2520Jiaqi%2520Chen%2520and%2520Yu-Jie%2520Yuan%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Hanhui%2520Li%2520and%2520Mrinmaya%2520Sachan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520We%2520present%2520SeePhys%252C%2520a%2520large-scale%2520multimodal%2520benchmark%2520for%2520LLM%2520reasoning%250Agrounded%2520in%2520physics%2520questions%2520ranging%2520from%2520middle%2520school%2520to%2520PhD%2520qualifying%250Aexams.%2520The%2520benchmark%2520covers%25207%2520fundamental%2520domains%2520spanning%2520the%2520physics%250Adiscipline%252C%2520incorporating%252021%2520categories%2520of%2520highly%2520heterogeneous%2520diagrams.%2520In%250Acontrast%2520to%2520prior%2520works%2520where%2520visual%2520elements%2520mainly%2520serve%2520auxiliary%2520purposes%252C%250Aour%2520benchmark%2520features%2520a%2520substantial%2520proportion%2520of%2520vision-essential%2520problems%250A%252875%2525%2529%2520that%2520mandate%2520visual%2520information%2520extraction%2520for%2520correct%2520solutions.%2520Through%250Aextensive%2520evaluation%252C%2520we%2520observe%2520that%2520even%2520the%2520most%2520advanced%2520visual%2520reasoning%250Amodels%2520%2528e.g.%252C%2520Gemini-2.5-pro%2520and%2520o4-mini%2529%2520achieve%2520sub-60%2525%2520accuracy%2520on%2520our%250Abenchmark.%2520These%2520results%2520reveal%2520fundamental%2520challenges%2520in%2520current%2520large%250Alanguage%2520models%2527%2520visual%2520understanding%2520capabilities%252C%2520particularly%2520in%253A%2520%2528i%2529%250Aestablishing%2520rigorous%2520coupling%2520between%2520diagram%2520interpretation%2520and%2520physics%250Areasoning%252C%2520and%2520%2528ii%2529%2520overcoming%2520their%2520persistent%2520reliance%2520on%2520textual%2520cues%2520as%250Acognitive%2520shortcuts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19099v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeePhys%3A%20Does%20Seeing%20Help%20Thinking%3F%20--%20Benchmarking%20Vision-Based%20Physics%0A%20%20Reasoning&entry.906535625=Kun%20Xiang%20and%20Heng%20Li%20and%20Terry%20Jingchen%20Zhang%20and%20Yinya%20Huang%20and%20Zirong%20Liu%20and%20Peixin%20Qu%20and%20Jixi%20He%20and%20Jiaqi%20Chen%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Hanhui%20Li%20and%20Mrinmaya%20Sachan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20We%20present%20SeePhys%2C%20a%20large-scale%20multimodal%20benchmark%20for%20LLM%20reasoning%0Agrounded%20in%20physics%20questions%20ranging%20from%20middle%20school%20to%20PhD%20qualifying%0Aexams.%20The%20benchmark%20covers%207%20fundamental%20domains%20spanning%20the%20physics%0Adiscipline%2C%20incorporating%2021%20categories%20of%20highly%20heterogeneous%20diagrams.%20In%0Acontrast%20to%20prior%20works%20where%20visual%20elements%20mainly%20serve%20auxiliary%20purposes%2C%0Aour%20benchmark%20features%20a%20substantial%20proportion%20of%20vision-essential%20problems%0A%2875%25%29%20that%20mandate%20visual%20information%20extraction%20for%20correct%20solutions.%20Through%0Aextensive%20evaluation%2C%20we%20observe%20that%20even%20the%20most%20advanced%20visual%20reasoning%0Amodels%20%28e.g.%2C%20Gemini-2.5-pro%20and%20o4-mini%29%20achieve%20sub-60%25%20accuracy%20on%20our%0Abenchmark.%20These%20results%20reveal%20fundamental%20challenges%20in%20current%20large%0Alanguage%20models%27%20visual%20understanding%20capabilities%2C%20particularly%20in%3A%20%28i%29%0Aestablishing%20rigorous%20coupling%20between%20diagram%20interpretation%20and%20physics%0Areasoning%2C%20and%20%28ii%29%20overcoming%20their%20persistent%20reliance%20on%20textual%20cues%20as%0Acognitive%20shortcuts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19099v5&entry.124074799=Read"},
{"title": "All-UWB SLAM Using UWB Radar and UWB AOA", "author": "Charith Premachandra and Achala Athukorala and U-Xuan Tan", "abstract": "  There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.\n", "link": "http://arxiv.org/abs/2507.15474v1", "date": "2025-07-21", "relevancy": 2.2771, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All-UWB%20SLAM%20Using%20UWB%20Radar%20and%20UWB%20AOA&body=Title%3A%20All-UWB%20SLAM%20Using%20UWB%20Radar%20and%20UWB%20AOA%0AAuthor%3A%20Charith%20Premachandra%20and%20Achala%20Athukorala%20and%20U-Xuan%20Tan%0AAbstract%3A%20%20%20There%20has%20been%20a%20growing%20interest%20in%20autonomous%20systems%20designed%20to%20operate%0Ain%20adverse%20conditions%20%28e.g.%20smoke%2C%20dust%29%2C%20where%20the%20visible%20light%20spectrum%0Afails.%20In%20this%20context%2C%20Ultra-wideband%20%28UWB%29%20radar%20is%20capable%20of%20penetrating%0Athrough%20such%20challenging%20environmental%20conditions%20due%20to%20the%20lower%20frequency%0Acomponents%20within%20its%20broad%20bandwidth.%20Therefore%2C%20UWB%20radar%20has%20emerged%20as%20a%0Apotential%20sensing%20technology%20for%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Ain%20vision-denied%20environments%20where%20optical%20sensors%20%28e.g.%20LiDAR%2C%20Camera%29%20are%0Aprone%20to%20failure.%20Existing%20approaches%20involving%20UWB%20radar%20as%20the%20primary%0Aexteroceptive%20sensor%20generally%20extract%20features%20in%20the%20environment%2C%20which%20are%0Alater%20initialized%20as%20landmarks%20in%20a%20map.%20However%2C%20these%20methods%20are%20constrained%0Aby%20the%20number%20of%20distinguishable%20features%20in%20the%20environment.%20Hence%2C%20this%20paper%0Aproposes%20a%20novel%20method%20incorporating%20UWB%20Angle%20of%20Arrival%20%28AOA%29%20measurements%0Ainto%20UWB%20radar-based%20SLAM%20systems%20to%20improve%20the%20accuracy%20and%20scalability%20of%0ASLAM%20in%20feature-deficient%20environments.%20The%20AOA%20measurements%20are%20obtained%20using%0AUWB%20anchor-tag%20units%20which%20are%20dynamically%20deployed%20by%20the%20robot%20in%20featureless%0Aareas%20during%20mapping%20of%20the%20environment.%20This%20paper%20thoroughly%20discusses%0Aprevailing%20constraints%20associated%20with%20UWB%20AOA%20measurement%20units%20and%20presents%0Asolutions%20to%20overcome%20them.%20Our%20experimental%20results%20show%20that%20integrating%20UWB%0AAOA%20units%20with%20UWB%20radar%20enables%20SLAM%20in%20vision-denied%20feature-deficient%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll-UWB%2520SLAM%2520Using%2520UWB%2520Radar%2520and%2520UWB%2520AOA%26entry.906535625%3DCharith%2520Premachandra%2520and%2520Achala%2520Athukorala%2520and%2520U-Xuan%2520Tan%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520growing%2520interest%2520in%2520autonomous%2520systems%2520designed%2520to%2520operate%250Ain%2520adverse%2520conditions%2520%2528e.g.%2520smoke%252C%2520dust%2529%252C%2520where%2520the%2520visible%2520light%2520spectrum%250Afails.%2520In%2520this%2520context%252C%2520Ultra-wideband%2520%2528UWB%2529%2520radar%2520is%2520capable%2520of%2520penetrating%250Athrough%2520such%2520challenging%2520environmental%2520conditions%2520due%2520to%2520the%2520lower%2520frequency%250Acomponents%2520within%2520its%2520broad%2520bandwidth.%2520Therefore%252C%2520UWB%2520radar%2520has%2520emerged%2520as%2520a%250Apotential%2520sensing%2520technology%2520for%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%250Ain%2520vision-denied%2520environments%2520where%2520optical%2520sensors%2520%2528e.g.%2520LiDAR%252C%2520Camera%2529%2520are%250Aprone%2520to%2520failure.%2520Existing%2520approaches%2520involving%2520UWB%2520radar%2520as%2520the%2520primary%250Aexteroceptive%2520sensor%2520generally%2520extract%2520features%2520in%2520the%2520environment%252C%2520which%2520are%250Alater%2520initialized%2520as%2520landmarks%2520in%2520a%2520map.%2520However%252C%2520these%2520methods%2520are%2520constrained%250Aby%2520the%2520number%2520of%2520distinguishable%2520features%2520in%2520the%2520environment.%2520Hence%252C%2520this%2520paper%250Aproposes%2520a%2520novel%2520method%2520incorporating%2520UWB%2520Angle%2520of%2520Arrival%2520%2528AOA%2529%2520measurements%250Ainto%2520UWB%2520radar-based%2520SLAM%2520systems%2520to%2520improve%2520the%2520accuracy%2520and%2520scalability%2520of%250ASLAM%2520in%2520feature-deficient%2520environments.%2520The%2520AOA%2520measurements%2520are%2520obtained%2520using%250AUWB%2520anchor-tag%2520units%2520which%2520are%2520dynamically%2520deployed%2520by%2520the%2520robot%2520in%2520featureless%250Aareas%2520during%2520mapping%2520of%2520the%2520environment.%2520This%2520paper%2520thoroughly%2520discusses%250Aprevailing%2520constraints%2520associated%2520with%2520UWB%2520AOA%2520measurement%2520units%2520and%2520presents%250Asolutions%2520to%2520overcome%2520them.%2520Our%2520experimental%2520results%2520show%2520that%2520integrating%2520UWB%250AAOA%2520units%2520with%2520UWB%2520radar%2520enables%2520SLAM%2520in%2520vision-denied%2520feature-deficient%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All-UWB%20SLAM%20Using%20UWB%20Radar%20and%20UWB%20AOA&entry.906535625=Charith%20Premachandra%20and%20Achala%20Athukorala%20and%20U-Xuan%20Tan&entry.1292438233=%20%20There%20has%20been%20a%20growing%20interest%20in%20autonomous%20systems%20designed%20to%20operate%0Ain%20adverse%20conditions%20%28e.g.%20smoke%2C%20dust%29%2C%20where%20the%20visible%20light%20spectrum%0Afails.%20In%20this%20context%2C%20Ultra-wideband%20%28UWB%29%20radar%20is%20capable%20of%20penetrating%0Athrough%20such%20challenging%20environmental%20conditions%20due%20to%20the%20lower%20frequency%0Acomponents%20within%20its%20broad%20bandwidth.%20Therefore%2C%20UWB%20radar%20has%20emerged%20as%20a%0Apotential%20sensing%20technology%20for%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Ain%20vision-denied%20environments%20where%20optical%20sensors%20%28e.g.%20LiDAR%2C%20Camera%29%20are%0Aprone%20to%20failure.%20Existing%20approaches%20involving%20UWB%20radar%20as%20the%20primary%0Aexteroceptive%20sensor%20generally%20extract%20features%20in%20the%20environment%2C%20which%20are%0Alater%20initialized%20as%20landmarks%20in%20a%20map.%20However%2C%20these%20methods%20are%20constrained%0Aby%20the%20number%20of%20distinguishable%20features%20in%20the%20environment.%20Hence%2C%20this%20paper%0Aproposes%20a%20novel%20method%20incorporating%20UWB%20Angle%20of%20Arrival%20%28AOA%29%20measurements%0Ainto%20UWB%20radar-based%20SLAM%20systems%20to%20improve%20the%20accuracy%20and%20scalability%20of%0ASLAM%20in%20feature-deficient%20environments.%20The%20AOA%20measurements%20are%20obtained%20using%0AUWB%20anchor-tag%20units%20which%20are%20dynamically%20deployed%20by%20the%20robot%20in%20featureless%0Aareas%20during%20mapping%20of%20the%20environment.%20This%20paper%20thoroughly%20discusses%0Aprevailing%20constraints%20associated%20with%20UWB%20AOA%20measurement%20units%20and%20presents%0Asolutions%20to%20overcome%20them.%20Our%20experimental%20results%20show%20that%20integrating%20UWB%0AAOA%20units%20with%20UWB%20radar%20enables%20SLAM%20in%20vision-denied%20feature-deficient%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15474v1&entry.124074799=Read"},
{"title": "Estimation of Payload Inertial Parameters from Human Demonstrations by\n  Hand Guiding", "author": "Johannes Hartwig and Philipp Lienhardt and Dominik Henrich", "abstract": "  As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.\n", "link": "http://arxiv.org/abs/2507.15604v1", "date": "2025-07-21", "relevancy": 2.2718, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6141}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5887}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimation%20of%20Payload%20Inertial%20Parameters%20from%20Human%20Demonstrations%20by%0A%20%20Hand%20Guiding&body=Title%3A%20Estimation%20of%20Payload%20Inertial%20Parameters%20from%20Human%20Demonstrations%20by%0A%20%20Hand%20Guiding%0AAuthor%3A%20Johannes%20Hartwig%20and%20Philipp%20Lienhardt%20and%20Dominik%20Henrich%0AAbstract%3A%20%20%20As%20the%20availability%20of%20cobots%20increases%2C%20it%20is%20essential%20to%20address%20the%20needs%0Aof%20users%20with%20little%20to%20no%20programming%20knowledge%20to%20operate%20such%20systems%0Aefficiently.%20Programming%20concepts%20often%20use%20intuitive%20interaction%20modalities%2C%0Asuch%20as%20hand%20guiding%2C%20to%20address%20this.%20When%20programming%20in-contact%20motions%2C%0Asuch%20frameworks%20require%20knowledge%20of%20the%20robot%20tool%27s%20payload%20inertial%0Aparameters%20%28PIP%29%20in%20addition%20to%20the%20demonstrated%20velocities%20and%20forces%20to%0Aensure%20effective%20hybrid%20motion-force%20control.%20This%20paper%20aims%20to%20enable%0Anon-expert%20users%20to%20program%20in-contact%20motions%20more%20efficiently%20by%20eliminating%0Athe%20need%20for%20a%20dedicated%20PIP%20calibration%2C%20thereby%20enabling%20flexible%20robot%20tool%0Achanges.%20Since%20demonstrated%20tasks%20generally%20also%20contain%20motions%20with%0Anon-contact%2C%20our%20approach%20uses%20these%20parts%20to%20estimate%20the%20robot%27s%20PIP%20using%0Aestablished%20estimation%20techniques.%20The%20results%20show%20that%20the%20estimation%20of%20the%0Apayload%27s%20mass%20is%20accurate%2C%20whereas%20the%20center%20of%20mass%20and%20the%20inertia%20tensor%0Aare%20affected%20by%20noise%20and%20a%20lack%20of%20excitation.%20Overall%2C%20these%20findings%20show%0Athe%20feasibility%20of%20PIP%20estimation%20during%20hand%20guiding%20but%20also%20highlight%20the%0Aneed%20for%20sufficient%20payload%20accelerations%20for%20an%20accurate%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimation%2520of%2520Payload%2520Inertial%2520Parameters%2520from%2520Human%2520Demonstrations%2520by%250A%2520%2520Hand%2520Guiding%26entry.906535625%3DJohannes%2520Hartwig%2520and%2520Philipp%2520Lienhardt%2520and%2520Dominik%2520Henrich%26entry.1292438233%3D%2520%2520As%2520the%2520availability%2520of%2520cobots%2520increases%252C%2520it%2520is%2520essential%2520to%2520address%2520the%2520needs%250Aof%2520users%2520with%2520little%2520to%2520no%2520programming%2520knowledge%2520to%2520operate%2520such%2520systems%250Aefficiently.%2520Programming%2520concepts%2520often%2520use%2520intuitive%2520interaction%2520modalities%252C%250Asuch%2520as%2520hand%2520guiding%252C%2520to%2520address%2520this.%2520When%2520programming%2520in-contact%2520motions%252C%250Asuch%2520frameworks%2520require%2520knowledge%2520of%2520the%2520robot%2520tool%2527s%2520payload%2520inertial%250Aparameters%2520%2528PIP%2529%2520in%2520addition%2520to%2520the%2520demonstrated%2520velocities%2520and%2520forces%2520to%250Aensure%2520effective%2520hybrid%2520motion-force%2520control.%2520This%2520paper%2520aims%2520to%2520enable%250Anon-expert%2520users%2520to%2520program%2520in-contact%2520motions%2520more%2520efficiently%2520by%2520eliminating%250Athe%2520need%2520for%2520a%2520dedicated%2520PIP%2520calibration%252C%2520thereby%2520enabling%2520flexible%2520robot%2520tool%250Achanges.%2520Since%2520demonstrated%2520tasks%2520generally%2520also%2520contain%2520motions%2520with%250Anon-contact%252C%2520our%2520approach%2520uses%2520these%2520parts%2520to%2520estimate%2520the%2520robot%2527s%2520PIP%2520using%250Aestablished%2520estimation%2520techniques.%2520The%2520results%2520show%2520that%2520the%2520estimation%2520of%2520the%250Apayload%2527s%2520mass%2520is%2520accurate%252C%2520whereas%2520the%2520center%2520of%2520mass%2520and%2520the%2520inertia%2520tensor%250Aare%2520affected%2520by%2520noise%2520and%2520a%2520lack%2520of%2520excitation.%2520Overall%252C%2520these%2520findings%2520show%250Athe%2520feasibility%2520of%2520PIP%2520estimation%2520during%2520hand%2520guiding%2520but%2520also%2520highlight%2520the%250Aneed%2520for%2520sufficient%2520payload%2520accelerations%2520for%2520an%2520accurate%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimation%20of%20Payload%20Inertial%20Parameters%20from%20Human%20Demonstrations%20by%0A%20%20Hand%20Guiding&entry.906535625=Johannes%20Hartwig%20and%20Philipp%20Lienhardt%20and%20Dominik%20Henrich&entry.1292438233=%20%20As%20the%20availability%20of%20cobots%20increases%2C%20it%20is%20essential%20to%20address%20the%20needs%0Aof%20users%20with%20little%20to%20no%20programming%20knowledge%20to%20operate%20such%20systems%0Aefficiently.%20Programming%20concepts%20often%20use%20intuitive%20interaction%20modalities%2C%0Asuch%20as%20hand%20guiding%2C%20to%20address%20this.%20When%20programming%20in-contact%20motions%2C%0Asuch%20frameworks%20require%20knowledge%20of%20the%20robot%20tool%27s%20payload%20inertial%0Aparameters%20%28PIP%29%20in%20addition%20to%20the%20demonstrated%20velocities%20and%20forces%20to%0Aensure%20effective%20hybrid%20motion-force%20control.%20This%20paper%20aims%20to%20enable%0Anon-expert%20users%20to%20program%20in-contact%20motions%20more%20efficiently%20by%20eliminating%0Athe%20need%20for%20a%20dedicated%20PIP%20calibration%2C%20thereby%20enabling%20flexible%20robot%20tool%0Achanges.%20Since%20demonstrated%20tasks%20generally%20also%20contain%20motions%20with%0Anon-contact%2C%20our%20approach%20uses%20these%20parts%20to%20estimate%20the%20robot%27s%20PIP%20using%0Aestablished%20estimation%20techniques.%20The%20results%20show%20that%20the%20estimation%20of%20the%0Apayload%27s%20mass%20is%20accurate%2C%20whereas%20the%20center%20of%20mass%20and%20the%20inertia%20tensor%0Aare%20affected%20by%20noise%20and%20a%20lack%20of%20excitation.%20Overall%2C%20these%20findings%20show%0Athe%20feasibility%20of%20PIP%20estimation%20during%20hand%20guiding%20but%20also%20highlight%20the%0Aneed%20for%20sufficient%20payload%20accelerations%20for%20an%20accurate%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15604v1&entry.124074799=Read"},
{"title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image\n  Generation", "author": "Ru Jia and Xiaozhuang Ma and Jianji Wang and Nanning Zheng", "abstract": "  While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.\n", "link": "http://arxiv.org/abs/2507.15606v1", "date": "2025-07-21", "relevancy": 2.2702, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5719}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CylinderPlane%3A%20Nested%20Cylinder%20Representation%20for%203D-aware%20Image%0A%20%20Generation&body=Title%3A%20CylinderPlane%3A%20Nested%20Cylinder%20Representation%20for%203D-aware%20Image%0A%20%20Generation%0AAuthor%3A%20Ru%20Jia%20and%20Xiaozhuang%20Ma%20and%20Jianji%20Wang%20and%20Nanning%20Zheng%0AAbstract%3A%20%20%20While%20the%20proposal%20of%20the%20Tri-plane%20representation%20has%20advanced%20the%0Adevelopment%20of%20the%203D-aware%20image%20generative%20models%2C%20problems%20rooted%20in%20its%0Ainherent%20structure%2C%20such%20as%20multi-face%20artifacts%20caused%20by%20sharing%20the%20same%0Afeatures%20in%20symmetric%20regions%2C%20limit%20its%20ability%20to%20generate%20360%24%5E%5Ccirc%24%20view%0Aimages.%20In%20this%20paper%2C%20we%20propose%20CylinderPlane%2C%20a%20novel%20implicit%0Arepresentation%20based%20on%20Cylindrical%20Coordinate%20System%2C%20to%20eliminate%20the%20feature%0Aambiguity%20issue%20and%20ensure%20multi-view%20consistency%20in%20360%24%5E%5Ccirc%24.%20Different%0Afrom%20the%20inevitable%20feature%20entanglement%20in%20Cartesian%20coordinate-based%0ATri-plane%20representation%2C%20the%20cylindrical%20coordinate%20system%20explicitly%0Aseparates%20features%20at%20different%20angles%2C%20allowing%20our%20cylindrical%20representation%0Apossible%20to%20achieve%20high-quality%2C%20artifacts-free%20360%24%5E%5Ccirc%24%20image%20synthesis.%0AWe%20further%20introduce%20the%20nested%20cylinder%20representation%20that%20composites%0Amultiple%20cylinders%20at%20different%20scales%2C%20thereby%20enabling%20the%20model%20more%0Aadaptable%20to%20complex%20geometry%20and%20varying%20resolutions.%20The%20combination%20of%0Acylinders%20with%20different%20resolutions%20can%20effectively%20capture%20more%20critical%0Alocations%20and%20multi-scale%20features%2C%20greatly%20facilitates%20fine%20detail%20learning%0Aand%20robustness%20to%20different%20resolutions.%20Moreover%2C%20our%20representation%20is%0Aagnostic%20to%20implicit%20rendering%20methods%20and%20can%20be%20easily%20integrated%20into%20any%0Aneural%20rendering%20pipeline.%20Extensive%20experiments%20on%20both%20synthetic%20dataset%20and%0Aunstructured%20in-the-wild%20images%20demonstrate%20that%20our%20proposed%20representation%0Aachieves%20superior%20performance%20over%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCylinderPlane%253A%2520Nested%2520Cylinder%2520Representation%2520for%25203D-aware%2520Image%250A%2520%2520Generation%26entry.906535625%3DRu%2520Jia%2520and%2520Xiaozhuang%2520Ma%2520and%2520Jianji%2520Wang%2520and%2520Nanning%2520Zheng%26entry.1292438233%3D%2520%2520While%2520the%2520proposal%2520of%2520the%2520Tri-plane%2520representation%2520has%2520advanced%2520the%250Adevelopment%2520of%2520the%25203D-aware%2520image%2520generative%2520models%252C%2520problems%2520rooted%2520in%2520its%250Ainherent%2520structure%252C%2520such%2520as%2520multi-face%2520artifacts%2520caused%2520by%2520sharing%2520the%2520same%250Afeatures%2520in%2520symmetric%2520regions%252C%2520limit%2520its%2520ability%2520to%2520generate%2520360%2524%255E%255Ccirc%2524%2520view%250Aimages.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CylinderPlane%252C%2520a%2520novel%2520implicit%250Arepresentation%2520based%2520on%2520Cylindrical%2520Coordinate%2520System%252C%2520to%2520eliminate%2520the%2520feature%250Aambiguity%2520issue%2520and%2520ensure%2520multi-view%2520consistency%2520in%2520360%2524%255E%255Ccirc%2524.%2520Different%250Afrom%2520the%2520inevitable%2520feature%2520entanglement%2520in%2520Cartesian%2520coordinate-based%250ATri-plane%2520representation%252C%2520the%2520cylindrical%2520coordinate%2520system%2520explicitly%250Aseparates%2520features%2520at%2520different%2520angles%252C%2520allowing%2520our%2520cylindrical%2520representation%250Apossible%2520to%2520achieve%2520high-quality%252C%2520artifacts-free%2520360%2524%255E%255Ccirc%2524%2520image%2520synthesis.%250AWe%2520further%2520introduce%2520the%2520nested%2520cylinder%2520representation%2520that%2520composites%250Amultiple%2520cylinders%2520at%2520different%2520scales%252C%2520thereby%2520enabling%2520the%2520model%2520more%250Aadaptable%2520to%2520complex%2520geometry%2520and%2520varying%2520resolutions.%2520The%2520combination%2520of%250Acylinders%2520with%2520different%2520resolutions%2520can%2520effectively%2520capture%2520more%2520critical%250Alocations%2520and%2520multi-scale%2520features%252C%2520greatly%2520facilitates%2520fine%2520detail%2520learning%250Aand%2520robustness%2520to%2520different%2520resolutions.%2520Moreover%252C%2520our%2520representation%2520is%250Aagnostic%2520to%2520implicit%2520rendering%2520methods%2520and%2520can%2520be%2520easily%2520integrated%2520into%2520any%250Aneural%2520rendering%2520pipeline.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520dataset%2520and%250Aunstructured%2520in-the-wild%2520images%2520demonstrate%2520that%2520our%2520proposed%2520representation%250Aachieves%2520superior%2520performance%2520over%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CylinderPlane%3A%20Nested%20Cylinder%20Representation%20for%203D-aware%20Image%0A%20%20Generation&entry.906535625=Ru%20Jia%20and%20Xiaozhuang%20Ma%20and%20Jianji%20Wang%20and%20Nanning%20Zheng&entry.1292438233=%20%20While%20the%20proposal%20of%20the%20Tri-plane%20representation%20has%20advanced%20the%0Adevelopment%20of%20the%203D-aware%20image%20generative%20models%2C%20problems%20rooted%20in%20its%0Ainherent%20structure%2C%20such%20as%20multi-face%20artifacts%20caused%20by%20sharing%20the%20same%0Afeatures%20in%20symmetric%20regions%2C%20limit%20its%20ability%20to%20generate%20360%24%5E%5Ccirc%24%20view%0Aimages.%20In%20this%20paper%2C%20we%20propose%20CylinderPlane%2C%20a%20novel%20implicit%0Arepresentation%20based%20on%20Cylindrical%20Coordinate%20System%2C%20to%20eliminate%20the%20feature%0Aambiguity%20issue%20and%20ensure%20multi-view%20consistency%20in%20360%24%5E%5Ccirc%24.%20Different%0Afrom%20the%20inevitable%20feature%20entanglement%20in%20Cartesian%20coordinate-based%0ATri-plane%20representation%2C%20the%20cylindrical%20coordinate%20system%20explicitly%0Aseparates%20features%20at%20different%20angles%2C%20allowing%20our%20cylindrical%20representation%0Apossible%20to%20achieve%20high-quality%2C%20artifacts-free%20360%24%5E%5Ccirc%24%20image%20synthesis.%0AWe%20further%20introduce%20the%20nested%20cylinder%20representation%20that%20composites%0Amultiple%20cylinders%20at%20different%20scales%2C%20thereby%20enabling%20the%20model%20more%0Aadaptable%20to%20complex%20geometry%20and%20varying%20resolutions.%20The%20combination%20of%0Acylinders%20with%20different%20resolutions%20can%20effectively%20capture%20more%20critical%0Alocations%20and%20multi-scale%20features%2C%20greatly%20facilitates%20fine%20detail%20learning%0Aand%20robustness%20to%20different%20resolutions.%20Moreover%2C%20our%20representation%20is%0Aagnostic%20to%20implicit%20rendering%20methods%20and%20can%20be%20easily%20integrated%20into%20any%0Aneural%20rendering%20pipeline.%20Extensive%20experiments%20on%20both%20synthetic%20dataset%20and%0Aunstructured%20in-the-wild%20images%20demonstrate%20that%20our%20proposed%20representation%0Aachieves%20superior%20performance%20over%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15606v1&entry.124074799=Read"},
{"title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified\n  Prompt and Representation Enhancement", "author": "Xiao Zhang and Fei Wei and Yong Wang and Wenda Zhao and Feiyi Li and Xiangxiang Chu", "abstract": "  Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the\nlack of images in the target domain. Previous approaches leverage\nVision-Language Models (VLMs) to tackle this challenge, exploiting their\nzero-shot learning capabilities. However, these methods primarily address\ndomain distribution shifts and overlook the misalignment between the detection\ntask and VLMs, which rely on manually crafted prompts. To overcome these\nlimitations, we propose the unified prompt and representation enhancement\n(UPRE) framework, which jointly optimizes both textual prompts and visual\nrepresentations. Specifically, our approach introduces a multi-view domain\nprompt that combines linguistic domain priors with detection-specific\nknowledge, and a visual representation enhancement module that produces domain\nstyle variations. Furthermore, we introduce multi-level enhancement strategies,\nincluding relative domain distance and positive-negative separation, which\nalign multi-modal representations at the image level and capture diverse visual\nrepresentations at the instance level, respectively. Extensive experiments\nconducted on nine benchmark datasets demonstrate the superior performance of\nour framework in ZSDA detection scenarios. Code is available at\nhttps://github.com/AMAP-ML/UPRE.\n", "link": "http://arxiv.org/abs/2507.00721v2", "date": "2025-07-21", "relevancy": 2.2592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.576}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5677}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UPRE%3A%20Zero-Shot%20Domain%20Adaptation%20for%20Object%20Detection%20via%20Unified%0A%20%20Prompt%20and%20Representation%20Enhancement&body=Title%3A%20UPRE%3A%20Zero-Shot%20Domain%20Adaptation%20for%20Object%20Detection%20via%20Unified%0A%20%20Prompt%20and%20Representation%20Enhancement%0AAuthor%3A%20Xiao%20Zhang%20and%20Fei%20Wei%20and%20Yong%20Wang%20and%20Wenda%20Zhao%20and%20Feiyi%20Li%20and%20Xiangxiang%20Chu%0AAbstract%3A%20%20%20Zero-shot%20domain%20adaptation%20%28ZSDA%29%20presents%20substantial%20challenges%20due%20to%20the%0Alack%20of%20images%20in%20the%20target%20domain.%20Previous%20approaches%20leverage%0AVision-Language%20Models%20%28VLMs%29%20to%20tackle%20this%20challenge%2C%20exploiting%20their%0Azero-shot%20learning%20capabilities.%20However%2C%20these%20methods%20primarily%20address%0Adomain%20distribution%20shifts%20and%20overlook%20the%20misalignment%20between%20the%20detection%0Atask%20and%20VLMs%2C%20which%20rely%20on%20manually%20crafted%20prompts.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20the%20unified%20prompt%20and%20representation%20enhancement%0A%28UPRE%29%20framework%2C%20which%20jointly%20optimizes%20both%20textual%20prompts%20and%20visual%0Arepresentations.%20Specifically%2C%20our%20approach%20introduces%20a%20multi-view%20domain%0Aprompt%20that%20combines%20linguistic%20domain%20priors%20with%20detection-specific%0Aknowledge%2C%20and%20a%20visual%20representation%20enhancement%20module%20that%20produces%20domain%0Astyle%20variations.%20Furthermore%2C%20we%20introduce%20multi-level%20enhancement%20strategies%2C%0Aincluding%20relative%20domain%20distance%20and%20positive-negative%20separation%2C%20which%0Aalign%20multi-modal%20representations%20at%20the%20image%20level%20and%20capture%20diverse%20visual%0Arepresentations%20at%20the%20instance%20level%2C%20respectively.%20Extensive%20experiments%0Aconducted%20on%20nine%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%0Aour%20framework%20in%20ZSDA%20detection%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/AMAP-ML/UPRE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00721v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUPRE%253A%2520Zero-Shot%2520Domain%2520Adaptation%2520for%2520Object%2520Detection%2520via%2520Unified%250A%2520%2520Prompt%2520and%2520Representation%2520Enhancement%26entry.906535625%3DXiao%2520Zhang%2520and%2520Fei%2520Wei%2520and%2520Yong%2520Wang%2520and%2520Wenda%2520Zhao%2520and%2520Feiyi%2520Li%2520and%2520Xiangxiang%2520Chu%26entry.1292438233%3D%2520%2520Zero-shot%2520domain%2520adaptation%2520%2528ZSDA%2529%2520presents%2520substantial%2520challenges%2520due%2520to%2520the%250Alack%2520of%2520images%2520in%2520the%2520target%2520domain.%2520Previous%2520approaches%2520leverage%250AVision-Language%2520Models%2520%2528VLMs%2529%2520to%2520tackle%2520this%2520challenge%252C%2520exploiting%2520their%250Azero-shot%2520learning%2520capabilities.%2520However%252C%2520these%2520methods%2520primarily%2520address%250Adomain%2520distribution%2520shifts%2520and%2520overlook%2520the%2520misalignment%2520between%2520the%2520detection%250Atask%2520and%2520VLMs%252C%2520which%2520rely%2520on%2520manually%2520crafted%2520prompts.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520the%2520unified%2520prompt%2520and%2520representation%2520enhancement%250A%2528UPRE%2529%2520framework%252C%2520which%2520jointly%2520optimizes%2520both%2520textual%2520prompts%2520and%2520visual%250Arepresentations.%2520Specifically%252C%2520our%2520approach%2520introduces%2520a%2520multi-view%2520domain%250Aprompt%2520that%2520combines%2520linguistic%2520domain%2520priors%2520with%2520detection-specific%250Aknowledge%252C%2520and%2520a%2520visual%2520representation%2520enhancement%2520module%2520that%2520produces%2520domain%250Astyle%2520variations.%2520Furthermore%252C%2520we%2520introduce%2520multi-level%2520enhancement%2520strategies%252C%250Aincluding%2520relative%2520domain%2520distance%2520and%2520positive-negative%2520separation%252C%2520which%250Aalign%2520multi-modal%2520representations%2520at%2520the%2520image%2520level%2520and%2520capture%2520diverse%2520visual%250Arepresentations%2520at%2520the%2520instance%2520level%252C%2520respectively.%2520Extensive%2520experiments%250Aconducted%2520on%2520nine%2520benchmark%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%250Aour%2520framework%2520in%2520ZSDA%2520detection%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/AMAP-ML/UPRE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00721v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UPRE%3A%20Zero-Shot%20Domain%20Adaptation%20for%20Object%20Detection%20via%20Unified%0A%20%20Prompt%20and%20Representation%20Enhancement&entry.906535625=Xiao%20Zhang%20and%20Fei%20Wei%20and%20Yong%20Wang%20and%20Wenda%20Zhao%20and%20Feiyi%20Li%20and%20Xiangxiang%20Chu&entry.1292438233=%20%20Zero-shot%20domain%20adaptation%20%28ZSDA%29%20presents%20substantial%20challenges%20due%20to%20the%0Alack%20of%20images%20in%20the%20target%20domain.%20Previous%20approaches%20leverage%0AVision-Language%20Models%20%28VLMs%29%20to%20tackle%20this%20challenge%2C%20exploiting%20their%0Azero-shot%20learning%20capabilities.%20However%2C%20these%20methods%20primarily%20address%0Adomain%20distribution%20shifts%20and%20overlook%20the%20misalignment%20between%20the%20detection%0Atask%20and%20VLMs%2C%20which%20rely%20on%20manually%20crafted%20prompts.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20the%20unified%20prompt%20and%20representation%20enhancement%0A%28UPRE%29%20framework%2C%20which%20jointly%20optimizes%20both%20textual%20prompts%20and%20visual%0Arepresentations.%20Specifically%2C%20our%20approach%20introduces%20a%20multi-view%20domain%0Aprompt%20that%20combines%20linguistic%20domain%20priors%20with%20detection-specific%0Aknowledge%2C%20and%20a%20visual%20representation%20enhancement%20module%20that%20produces%20domain%0Astyle%20variations.%20Furthermore%2C%20we%20introduce%20multi-level%20enhancement%20strategies%2C%0Aincluding%20relative%20domain%20distance%20and%20positive-negative%20separation%2C%20which%0Aalign%20multi-modal%20representations%20at%20the%20image%20level%20and%20capture%20diverse%20visual%0Arepresentations%20at%20the%20instance%20level%2C%20respectively.%20Extensive%20experiments%0Aconducted%20on%20nine%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%0Aour%20framework%20in%20ZSDA%20detection%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/AMAP-ML/UPRE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00721v2&entry.124074799=Read"},
{"title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression\n  Recognition via Distributionally Robust Optimization", "author": "Feng-Qi Cui and Anyang Tong and Jinyang Huang and Jie Zhang and Dan Guo and Zhi Liu and Meng Wang", "abstract": "  Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.\n", "link": "http://arxiv.org/abs/2507.15765v1", "date": "2025-07-21", "relevancy": 2.2516, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.577}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5555}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Heterogeneity%3A%20Generalizing%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20via%20Distributionally%20Robust%20Optimization&body=Title%3A%20Learning%20from%20Heterogeneity%3A%20Generalizing%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20via%20Distributionally%20Robust%20Optimization%0AAuthor%3A%20Feng-Qi%20Cui%20and%20Anyang%20Tong%20and%20Jinyang%20Huang%20and%20Jie%20Zhang%20and%20Dan%20Guo%20and%20Zhi%20Liu%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20plays%20a%20critical%20role%20in%0Aaffective%20computing%20and%20human-computer%20interaction.%20Although%20existing%20methods%0Aachieve%20comparable%20performance%2C%20they%20inevitably%20suffer%20from%20performance%0Adegradation%20under%20sample%20heterogeneity%20caused%20by%20multi-source%20data%20and%0Aindividual%20expression%20variability.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20framework%2C%20called%20Heterogeneity-aware%20Distributional%20Framework%20%28HDF%29%2C%20and%0Adesign%20two%20plug-and-play%20modules%20to%20enhance%20time-frequency%20modeling%20and%0Amitigate%20optimization%20imbalance%20caused%20by%20hard%20samples.%20Specifically%2C%20the%0ATime-Frequency%20Distributional%20Attention%20Module%20%28DAM%29%20captures%20both%20temporal%0Aconsistency%20and%20frequency%20robustness%20through%20a%20dual-branch%20attention%20design%2C%0Aimproving%20tolerance%20to%20sequence%20inconsistency%20and%20visual%20style%20shifts.%20Then%2C%0Abased%20on%20gradient%20sensitivity%20and%20information%20bottleneck%20principles%2C%20an%0Aadaptive%20optimization%20module%20Distribution-aware%20Scaling%20Module%20%28DSM%29%20is%0Aintroduced%20to%20dynamically%20balance%20classification%20and%20contrastive%20losses%2C%0Aenabling%20more%20stable%20and%20discriminative%20representation%20learning.%20Extensive%0Aexperiments%20on%20two%20widely%20used%20datasets%2C%20DFEW%20and%20FERV39k%2C%20demonstrate%20that%20HDF%0Asignificantly%20improves%20both%20recognition%20accuracy%20and%20robustness.%20Our%20method%0Aachieves%20superior%20weighted%20average%20recall%20%28WAR%29%20and%20unweighted%20average%20recall%0A%28UAR%29%20while%20maintaining%20strong%20generalization%20across%20diverse%20and%20imbalanced%0Ascenarios.%20Codes%20are%20released%20at%20https%3A//github.com/QIcita/HDF_DFER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Heterogeneity%253A%2520Generalizing%2520Dynamic%2520Facial%2520Expression%250A%2520%2520Recognition%2520via%2520Distributionally%2520Robust%2520Optimization%26entry.906535625%3DFeng-Qi%2520Cui%2520and%2520Anyang%2520Tong%2520and%2520Jinyang%2520Huang%2520and%2520Jie%2520Zhang%2520and%2520Dan%2520Guo%2520and%2520Zhi%2520Liu%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Dynamic%2520Facial%2520Expression%2520Recognition%2520%2528DFER%2529%2520plays%2520a%2520critical%2520role%2520in%250Aaffective%2520computing%2520and%2520human-computer%2520interaction.%2520Although%2520existing%2520methods%250Aachieve%2520comparable%2520performance%252C%2520they%2520inevitably%2520suffer%2520from%2520performance%250Adegradation%2520under%2520sample%2520heterogeneity%2520caused%2520by%2520multi-source%2520data%2520and%250Aindividual%2520expression%2520variability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520called%2520Heterogeneity-aware%2520Distributional%2520Framework%2520%2528HDF%2529%252C%2520and%250Adesign%2520two%2520plug-and-play%2520modules%2520to%2520enhance%2520time-frequency%2520modeling%2520and%250Amitigate%2520optimization%2520imbalance%2520caused%2520by%2520hard%2520samples.%2520Specifically%252C%2520the%250ATime-Frequency%2520Distributional%2520Attention%2520Module%2520%2528DAM%2529%2520captures%2520both%2520temporal%250Aconsistency%2520and%2520frequency%2520robustness%2520through%2520a%2520dual-branch%2520attention%2520design%252C%250Aimproving%2520tolerance%2520to%2520sequence%2520inconsistency%2520and%2520visual%2520style%2520shifts.%2520Then%252C%250Abased%2520on%2520gradient%2520sensitivity%2520and%2520information%2520bottleneck%2520principles%252C%2520an%250Aadaptive%2520optimization%2520module%2520Distribution-aware%2520Scaling%2520Module%2520%2528DSM%2529%2520is%250Aintroduced%2520to%2520dynamically%2520balance%2520classification%2520and%2520contrastive%2520losses%252C%250Aenabling%2520more%2520stable%2520and%2520discriminative%2520representation%2520learning.%2520Extensive%250Aexperiments%2520on%2520two%2520widely%2520used%2520datasets%252C%2520DFEW%2520and%2520FERV39k%252C%2520demonstrate%2520that%2520HDF%250Asignificantly%2520improves%2520both%2520recognition%2520accuracy%2520and%2520robustness.%2520Our%2520method%250Aachieves%2520superior%2520weighted%2520average%2520recall%2520%2528WAR%2529%2520and%2520unweighted%2520average%2520recall%250A%2528UAR%2529%2520while%2520maintaining%2520strong%2520generalization%2520across%2520diverse%2520and%2520imbalanced%250Ascenarios.%2520Codes%2520are%2520released%2520at%2520https%253A//github.com/QIcita/HDF_DFER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Heterogeneity%3A%20Generalizing%20Dynamic%20Facial%20Expression%0A%20%20Recognition%20via%20Distributionally%20Robust%20Optimization&entry.906535625=Feng-Qi%20Cui%20and%20Anyang%20Tong%20and%20Jinyang%20Huang%20and%20Jie%20Zhang%20and%20Dan%20Guo%20and%20Zhi%20Liu%20and%20Meng%20Wang&entry.1292438233=%20%20Dynamic%20Facial%20Expression%20Recognition%20%28DFER%29%20plays%20a%20critical%20role%20in%0Aaffective%20computing%20and%20human-computer%20interaction.%20Although%20existing%20methods%0Aachieve%20comparable%20performance%2C%20they%20inevitably%20suffer%20from%20performance%0Adegradation%20under%20sample%20heterogeneity%20caused%20by%20multi-source%20data%20and%0Aindividual%20expression%20variability.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20framework%2C%20called%20Heterogeneity-aware%20Distributional%20Framework%20%28HDF%29%2C%20and%0Adesign%20two%20plug-and-play%20modules%20to%20enhance%20time-frequency%20modeling%20and%0Amitigate%20optimization%20imbalance%20caused%20by%20hard%20samples.%20Specifically%2C%20the%0ATime-Frequency%20Distributional%20Attention%20Module%20%28DAM%29%20captures%20both%20temporal%0Aconsistency%20and%20frequency%20robustness%20through%20a%20dual-branch%20attention%20design%2C%0Aimproving%20tolerance%20to%20sequence%20inconsistency%20and%20visual%20style%20shifts.%20Then%2C%0Abased%20on%20gradient%20sensitivity%20and%20information%20bottleneck%20principles%2C%20an%0Aadaptive%20optimization%20module%20Distribution-aware%20Scaling%20Module%20%28DSM%29%20is%0Aintroduced%20to%20dynamically%20balance%20classification%20and%20contrastive%20losses%2C%0Aenabling%20more%20stable%20and%20discriminative%20representation%20learning.%20Extensive%0Aexperiments%20on%20two%20widely%20used%20datasets%2C%20DFEW%20and%20FERV39k%2C%20demonstrate%20that%20HDF%0Asignificantly%20improves%20both%20recognition%20accuracy%20and%20robustness.%20Our%20method%0Aachieves%20superior%20weighted%20average%20recall%20%28WAR%29%20and%20unweighted%20average%20recall%0A%28UAR%29%20while%20maintaining%20strong%20generalization%20across%20diverse%20and%20imbalanced%0Ascenarios.%20Codes%20are%20released%20at%20https%3A//github.com/QIcita/HDF_DFER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15765v1&entry.124074799=Read"},
{"title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for\n  Low-Light Image Enhancement", "author": "Hanting Li and Fei Zhou and Xin Sun and Yang Hua and Jungong Han and Liang-Jie Zhang", "abstract": "  Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.\n", "link": "http://arxiv.org/abs/2507.15520v1", "date": "2025-07-21", "relevancy": 2.2452, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5647}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5601}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAIGFormer%3A%20A%20Spatially-Adaptive%20Illumination-Guided%20Network%20for%0A%20%20Low-Light%20Image%20Enhancement&body=Title%3A%20SAIGFormer%3A%20A%20Spatially-Adaptive%20Illumination-Guided%20Network%20for%0A%20%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Hanting%20Li%20and%20Fei%20Zhou%20and%20Xin%20Sun%20and%20Yang%20Hua%20and%20Jungong%20Han%20and%20Liang-Jie%20Zhang%0AAbstract%3A%20%20%20Recent%20Transformer-based%20low-light%20enhancement%20methods%20have%20made%20promising%0Aprogress%20in%20recovering%20global%20illumination.%20However%2C%20they%20still%20struggle%20with%0Anon-uniform%20lighting%20scenarios%2C%20such%20as%20backlit%20and%20shadow%2C%20appearing%20as%0Aover-exposure%20or%20inadequate%20brightness%20restoration.%20To%20address%20this%20challenge%2C%0Awe%20present%20a%20Spatially-Adaptive%20Illumination-Guided%20Transformer%20%28SAIGFormer%29%0Aframework%20that%20enables%20accurate%20illumination%20restoration.%20Specifically%2C%20we%0Apropose%20a%20dynamic%20integral%20image%20representation%20to%20model%20the%20spatially-varying%0Aillumination%2C%20and%20further%20construct%20a%20novel%20Spatially-Adaptive%20Integral%0AIllumination%20Estimator%20%28%24%5Ctext%7BSAI%7D%5E2%5Ctext%7BE%7D%24%29.%20Moreover%2C%20we%20introduce%20an%0AIllumination-Guided%20Multi-head%20Self-Attention%20%28IG-MSA%29%20mechanism%2C%20which%0Aleverages%20the%20illumination%20to%20calibrate%20the%20lightness-relevant%20features%20toward%0Avisual-pleased%20illumination%20enhancement.%20Extensive%20experiments%20on%20five%20standard%0Alow-light%20datasets%20and%20a%20cross-domain%20benchmark%20%28LOL-Blur%29%20demonstrate%20that%20our%0ASAIGFormer%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%0Aquantitative%20and%20qualitative%20metrics.%20In%20particular%2C%20our%20method%20achieves%0Asuperior%20performance%20in%20non-uniform%20illumination%20enhancement%20while%20exhibiting%0Astrong%20generalization%20capabilities%20across%20multiple%20datasets.%20Code%20is%20available%0Aat%20https%3A//github.com/LHTcode/SAIGFormer.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAIGFormer%253A%2520A%2520Spatially-Adaptive%2520Illumination-Guided%2520Network%2520for%250A%2520%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DHanting%2520Li%2520and%2520Fei%2520Zhou%2520and%2520Xin%2520Sun%2520and%2520Yang%2520Hua%2520and%2520Jungong%2520Han%2520and%2520Liang-Jie%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520Transformer-based%2520low-light%2520enhancement%2520methods%2520have%2520made%2520promising%250Aprogress%2520in%2520recovering%2520global%2520illumination.%2520However%252C%2520they%2520still%2520struggle%2520with%250Anon-uniform%2520lighting%2520scenarios%252C%2520such%2520as%2520backlit%2520and%2520shadow%252C%2520appearing%2520as%250Aover-exposure%2520or%2520inadequate%2520brightness%2520restoration.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520present%2520a%2520Spatially-Adaptive%2520Illumination-Guided%2520Transformer%2520%2528SAIGFormer%2529%250Aframework%2520that%2520enables%2520accurate%2520illumination%2520restoration.%2520Specifically%252C%2520we%250Apropose%2520a%2520dynamic%2520integral%2520image%2520representation%2520to%2520model%2520the%2520spatially-varying%250Aillumination%252C%2520and%2520further%2520construct%2520a%2520novel%2520Spatially-Adaptive%2520Integral%250AIllumination%2520Estimator%2520%2528%2524%255Ctext%257BSAI%257D%255E2%255Ctext%257BE%257D%2524%2529.%2520Moreover%252C%2520we%2520introduce%2520an%250AIllumination-Guided%2520Multi-head%2520Self-Attention%2520%2528IG-MSA%2529%2520mechanism%252C%2520which%250Aleverages%2520the%2520illumination%2520to%2520calibrate%2520the%2520lightness-relevant%2520features%2520toward%250Avisual-pleased%2520illumination%2520enhancement.%2520Extensive%2520experiments%2520on%2520five%2520standard%250Alow-light%2520datasets%2520and%2520a%2520cross-domain%2520benchmark%2520%2528LOL-Blur%2529%2520demonstrate%2520that%2520our%250ASAIGFormer%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%250Aquantitative%2520and%2520qualitative%2520metrics.%2520In%2520particular%252C%2520our%2520method%2520achieves%250Asuperior%2520performance%2520in%2520non-uniform%2520illumination%2520enhancement%2520while%2520exhibiting%250Astrong%2520generalization%2520capabilities%2520across%2520multiple%2520datasets.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/LHTcode/SAIGFormer.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAIGFormer%3A%20A%20Spatially-Adaptive%20Illumination-Guided%20Network%20for%0A%20%20Low-Light%20Image%20Enhancement&entry.906535625=Hanting%20Li%20and%20Fei%20Zhou%20and%20Xin%20Sun%20and%20Yang%20Hua%20and%20Jungong%20Han%20and%20Liang-Jie%20Zhang&entry.1292438233=%20%20Recent%20Transformer-based%20low-light%20enhancement%20methods%20have%20made%20promising%0Aprogress%20in%20recovering%20global%20illumination.%20However%2C%20they%20still%20struggle%20with%0Anon-uniform%20lighting%20scenarios%2C%20such%20as%20backlit%20and%20shadow%2C%20appearing%20as%0Aover-exposure%20or%20inadequate%20brightness%20restoration.%20To%20address%20this%20challenge%2C%0Awe%20present%20a%20Spatially-Adaptive%20Illumination-Guided%20Transformer%20%28SAIGFormer%29%0Aframework%20that%20enables%20accurate%20illumination%20restoration.%20Specifically%2C%20we%0Apropose%20a%20dynamic%20integral%20image%20representation%20to%20model%20the%20spatially-varying%0Aillumination%2C%20and%20further%20construct%20a%20novel%20Spatially-Adaptive%20Integral%0AIllumination%20Estimator%20%28%24%5Ctext%7BSAI%7D%5E2%5Ctext%7BE%7D%24%29.%20Moreover%2C%20we%20introduce%20an%0AIllumination-Guided%20Multi-head%20Self-Attention%20%28IG-MSA%29%20mechanism%2C%20which%0Aleverages%20the%20illumination%20to%20calibrate%20the%20lightness-relevant%20features%20toward%0Avisual-pleased%20illumination%20enhancement.%20Extensive%20experiments%20on%20five%20standard%0Alow-light%20datasets%20and%20a%20cross-domain%20benchmark%20%28LOL-Blur%29%20demonstrate%20that%20our%0ASAIGFormer%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%0Aquantitative%20and%20qualitative%20metrics.%20In%20particular%2C%20our%20method%20achieves%0Asuperior%20performance%20in%20non-uniform%20illumination%20enhancement%20while%20exhibiting%0Astrong%20generalization%20capabilities%20across%20multiple%20datasets.%20Code%20is%20available%0Aat%20https%3A//github.com/LHTcode/SAIGFormer.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15520v1&entry.124074799=Read"},
{"title": "Sparsification Under Siege: Defending Against Poisoning Attacks in\n  Communication-Efficient Federated Learning", "author": "Zhiyong Jin and Runhua Xu and Chao Li and Yizhong Liu and Jianxin Li and James Joshi", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency.\n", "link": "http://arxiv.org/abs/2505.01454v4", "date": "2025-07-21", "relevancy": 2.2352, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&body=Title%3A%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning%0AAuthor%3A%20Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li%20and%20James%20Joshi%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01454v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsification%2520Under%2520Siege%253A%2520Defending%2520Against%2520Poisoning%2520Attacks%2520in%250A%2520%2520Communication-Efficient%2520Federated%2520Learning%26entry.906535625%3DZhiyong%2520Jin%2520and%2520Runhua%2520Xu%2520and%2520Chao%2520Li%2520and%2520Yizhong%2520Liu%2520and%2520Jianxin%2520Li%2520and%2520James%2520Joshi%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy%252C%2520yet%2520it%2520faces%2520significant%250Achallenges%2520in%2520communication%2520efficiency%2520and%2520vulnerability%2520to%2520poisoning%2520attacks.%250AWhile%2520sparsification%2520techniques%2520mitigate%2520communication%2520overhead%2520by%2520transmitting%250Aonly%2520critical%2520model%2520parameters%252C%2520they%2520inadvertently%2520amplify%2520security%2520risks%253A%250Aadversarial%2520clients%2520can%2520exploit%2520sparse%2520updates%2520to%2520evade%2520detection%2520and%2520degrade%250Amodel%2520performance.%2520Existing%2520defense%2520mechanisms%252C%2520designed%2520for%2520standard%2520FL%250Acommunication%2520scenarios%252C%2520are%2520ineffective%2520in%2520addressing%2520these%2520vulnerabilities%250Awithin%2520sparsified%2520FL.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520FLARE%252C%2520a%2520novel%2520federated%250Alearning%2520framework%2520that%2520integrates%2520sparse%2520index%2520mask%2520inspection%2520and%2520model%250Aupdate%2520sign%2520similarity%2520analysis%2520to%2520detect%2520and%2520mitigate%2520poisoning%2520attacks%2520in%250Asparsified%2520FL.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520adversarial%250Ascenarios%2520demonstrate%2520that%2520FLARE%2520significantly%2520outperforms%2520existing%2520defense%250Astrategies%252C%2520effectively%2520securing%2520sparsified%2520FL%2520against%2520poisoning%2520attacks%2520while%250Amaintaining%2520communication%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01454v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&entry.906535625=Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li%20and%20James%20Joshi&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01454v4&entry.124074799=Read"},
{"title": "Optimizer's Information Criterion: Dissecting and Correcting Bias in\n  Data-Driven Optimization", "author": "Garud Iyengar and Henry Lam and Tianyu Wang", "abstract": "  In data-driven optimization, the sample performance of the obtained decision\ntypically incurs an optimistic bias against the true performance, a phenomenon\ncommonly known as the Optimizer's Curse and intimately related to overfitting\nin machine learning. Common techniques to correct this bias, such as\ncross-validation, require repeatedly solving additional optimization problems\nand are therefore computationally expensive. We develop a general bias\ncorrection approach, building on what we call Optimizer's Information Criterion\n(OIC), that directly approximates the first-order bias and does not require\nsolving any additional optimization problems. Our OIC generalizes the\ncelebrated Akaike Information Criterion to evaluate the objective performance\nin data-driven optimization, which crucially involves not only model fitting\nbut also its interplay with the downstream optimization. As such it can be used\nfor decision selection instead of only model selection. We apply our approach\nto a range of data-driven optimization formulations comprising empirical and\nparametric models, their regularized counterparts, and furthermore contextual\noptimization. Finally, we provide numerical validation on the superior\nperformance of our approach under synthetic and real-world datasets.\n", "link": "http://arxiv.org/abs/2306.10081v4", "date": "2025-07-21", "relevancy": 2.2206, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4602}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4376}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizer%27s%20Information%20Criterion%3A%20Dissecting%20and%20Correcting%20Bias%20in%0A%20%20Data-Driven%20Optimization&body=Title%3A%20Optimizer%27s%20Information%20Criterion%3A%20Dissecting%20and%20Correcting%20Bias%20in%0A%20%20Data-Driven%20Optimization%0AAuthor%3A%20Garud%20Iyengar%20and%20Henry%20Lam%20and%20Tianyu%20Wang%0AAbstract%3A%20%20%20In%20data-driven%20optimization%2C%20the%20sample%20performance%20of%20the%20obtained%20decision%0Atypically%20incurs%20an%20optimistic%20bias%20against%20the%20true%20performance%2C%20a%20phenomenon%0Acommonly%20known%20as%20the%20Optimizer%27s%20Curse%20and%20intimately%20related%20to%20overfitting%0Ain%20machine%20learning.%20Common%20techniques%20to%20correct%20this%20bias%2C%20such%20as%0Across-validation%2C%20require%20repeatedly%20solving%20additional%20optimization%20problems%0Aand%20are%20therefore%20computationally%20expensive.%20We%20develop%20a%20general%20bias%0Acorrection%20approach%2C%20building%20on%20what%20we%20call%20Optimizer%27s%20Information%20Criterion%0A%28OIC%29%2C%20that%20directly%20approximates%20the%20first-order%20bias%20and%20does%20not%20require%0Asolving%20any%20additional%20optimization%20problems.%20Our%20OIC%20generalizes%20the%0Acelebrated%20Akaike%20Information%20Criterion%20to%20evaluate%20the%20objective%20performance%0Ain%20data-driven%20optimization%2C%20which%20crucially%20involves%20not%20only%20model%20fitting%0Abut%20also%20its%20interplay%20with%20the%20downstream%20optimization.%20As%20such%20it%20can%20be%20used%0Afor%20decision%20selection%20instead%20of%20only%20model%20selection.%20We%20apply%20our%20approach%0Ato%20a%20range%20of%20data-driven%20optimization%20formulations%20comprising%20empirical%20and%0Aparametric%20models%2C%20their%20regularized%20counterparts%2C%20and%20furthermore%20contextual%0Aoptimization.%20Finally%2C%20we%20provide%20numerical%20validation%20on%20the%20superior%0Aperformance%20of%20our%20approach%20under%20synthetic%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10081v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizer%2527s%2520Information%2520Criterion%253A%2520Dissecting%2520and%2520Correcting%2520Bias%2520in%250A%2520%2520Data-Driven%2520Optimization%26entry.906535625%3DGarud%2520Iyengar%2520and%2520Henry%2520Lam%2520and%2520Tianyu%2520Wang%26entry.1292438233%3D%2520%2520In%2520data-driven%2520optimization%252C%2520the%2520sample%2520performance%2520of%2520the%2520obtained%2520decision%250Atypically%2520incurs%2520an%2520optimistic%2520bias%2520against%2520the%2520true%2520performance%252C%2520a%2520phenomenon%250Acommonly%2520known%2520as%2520the%2520Optimizer%2527s%2520Curse%2520and%2520intimately%2520related%2520to%2520overfitting%250Ain%2520machine%2520learning.%2520Common%2520techniques%2520to%2520correct%2520this%2520bias%252C%2520such%2520as%250Across-validation%252C%2520require%2520repeatedly%2520solving%2520additional%2520optimization%2520problems%250Aand%2520are%2520therefore%2520computationally%2520expensive.%2520We%2520develop%2520a%2520general%2520bias%250Acorrection%2520approach%252C%2520building%2520on%2520what%2520we%2520call%2520Optimizer%2527s%2520Information%2520Criterion%250A%2528OIC%2529%252C%2520that%2520directly%2520approximates%2520the%2520first-order%2520bias%2520and%2520does%2520not%2520require%250Asolving%2520any%2520additional%2520optimization%2520problems.%2520Our%2520OIC%2520generalizes%2520the%250Acelebrated%2520Akaike%2520Information%2520Criterion%2520to%2520evaluate%2520the%2520objective%2520performance%250Ain%2520data-driven%2520optimization%252C%2520which%2520crucially%2520involves%2520not%2520only%2520model%2520fitting%250Abut%2520also%2520its%2520interplay%2520with%2520the%2520downstream%2520optimization.%2520As%2520such%2520it%2520can%2520be%2520used%250Afor%2520decision%2520selection%2520instead%2520of%2520only%2520model%2520selection.%2520We%2520apply%2520our%2520approach%250Ato%2520a%2520range%2520of%2520data-driven%2520optimization%2520formulations%2520comprising%2520empirical%2520and%250Aparametric%2520models%252C%2520their%2520regularized%2520counterparts%252C%2520and%2520furthermore%2520contextual%250Aoptimization.%2520Finally%252C%2520we%2520provide%2520numerical%2520validation%2520on%2520the%2520superior%250Aperformance%2520of%2520our%2520approach%2520under%2520synthetic%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10081v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizer%27s%20Information%20Criterion%3A%20Dissecting%20and%20Correcting%20Bias%20in%0A%20%20Data-Driven%20Optimization&entry.906535625=Garud%20Iyengar%20and%20Henry%20Lam%20and%20Tianyu%20Wang&entry.1292438233=%20%20In%20data-driven%20optimization%2C%20the%20sample%20performance%20of%20the%20obtained%20decision%0Atypically%20incurs%20an%20optimistic%20bias%20against%20the%20true%20performance%2C%20a%20phenomenon%0Acommonly%20known%20as%20the%20Optimizer%27s%20Curse%20and%20intimately%20related%20to%20overfitting%0Ain%20machine%20learning.%20Common%20techniques%20to%20correct%20this%20bias%2C%20such%20as%0Across-validation%2C%20require%20repeatedly%20solving%20additional%20optimization%20problems%0Aand%20are%20therefore%20computationally%20expensive.%20We%20develop%20a%20general%20bias%0Acorrection%20approach%2C%20building%20on%20what%20we%20call%20Optimizer%27s%20Information%20Criterion%0A%28OIC%29%2C%20that%20directly%20approximates%20the%20first-order%20bias%20and%20does%20not%20require%0Asolving%20any%20additional%20optimization%20problems.%20Our%20OIC%20generalizes%20the%0Acelebrated%20Akaike%20Information%20Criterion%20to%20evaluate%20the%20objective%20performance%0Ain%20data-driven%20optimization%2C%20which%20crucially%20involves%20not%20only%20model%20fitting%0Abut%20also%20its%20interplay%20with%20the%20downstream%20optimization.%20As%20such%20it%20can%20be%20used%0Afor%20decision%20selection%20instead%20of%20only%20model%20selection.%20We%20apply%20our%20approach%0Ato%20a%20range%20of%20data-driven%20optimization%20formulations%20comprising%20empirical%20and%0Aparametric%20models%2C%20their%20regularized%20counterparts%2C%20and%20furthermore%20contextual%0Aoptimization.%20Finally%2C%20we%20provide%20numerical%20validation%20on%20the%20superior%0Aperformance%20of%20our%20approach%20under%20synthetic%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10081v4&entry.124074799=Read"},
{"title": "Extracting Visual Facts from Intermediate Layers for Mitigating\n  Hallucinations in Multimodal Large Language Models", "author": "Haoran Zhou and Zihan Zhang and Hao Chen", "abstract": "  Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.\n", "link": "http://arxiv.org/abs/2507.15652v1", "date": "2025-07-21", "relevancy": 2.217, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Visual%20Facts%20from%20Intermediate%20Layers%20for%20Mitigating%0A%20%20Hallucinations%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Extracting%20Visual%20Facts%20from%20Intermediate%20Layers%20for%20Mitigating%0A%20%20Hallucinations%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20Zhou%20and%20Zihan%20Zhang%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%20strides%20by%0Acombining%20visual%20recognition%20and%20language%20understanding%20to%20generate%20content%0Athat%20is%20both%20coherent%20and%20contextually%20accurate.%20However%2C%20MLLMs%20continue%20to%0Astruggle%20with%20object%20hallucinations%2C%20where%20models%20produce%20seemingly%20plausible%0Abut%20factually%20incorrect%20outputs%2C%20including%20objects%20that%20do%20not%20exist%20in%20the%0Aimage.%20Recent%20work%20has%20revealed%20that%20the%20prior%20knowledge%20in%20MLLMs%20significantly%0Asuppresses%20visual%20information%20in%20deep%20layers%2C%20causing%20hallucinatory%20outputs.%0AHowever%2C%20how%20these%20priors%20suppress%20visual%20information%20at%20the%20intermediate%20layer%0Astage%20in%20MLLMs%20remains%20unclear.%20We%20observe%20that%20visual%20factual%20knowledge%20and%0Athe%20differences%20between%20intermediate-layer%20prior/original%20probability%0Adistributions%20show%20similar%20evolutionary%20trends%20in%20intermediate%20layers.%0AMotivated%20by%20this%2C%20we%20introduce%20Decoding%20by%20Extracting%20Visual%20Facts%20%28EVA%29%2C%20a%0Asimple%2C%20training-free%20method%20that%20dynamically%20selects%20intermediate%20layers%20with%0Athe%20most%20significant%20visual%20factual%20information.%20By%20contrasting%20the%20output%0Adistributions%20of%20the%20selected%20layer%20derived%20from%20the%20original%20input%20and%0Apure-text%20input%2C%20EVA%20extracts%20visual%20factual%20knowledge%20and%20proportionally%0Aincorporates%20it%20into%20the%20final%20layer%20to%20correct%20the%20output%20logits.%20Importantly%2C%0AEVA%20is%20model-agnostic%2C%20seamlessly%20integrates%20with%20various%20classic%20decoding%0Astrategies%2C%20and%20is%20applicable%20across%20different%20MLLMs.%20We%20validate%20EVA%20on%0Awidely-used%20benchmarks%2C%20and%20the%20results%20show%20that%20it%20significantly%20reduces%0Ahallucination%20rates%20compared%20to%20baseline%20methods%2C%20underscoring%20its%0Aeffectiveness%20in%20mitigating%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Visual%2520Facts%2520from%2520Intermediate%2520Layers%2520for%2520Mitigating%250A%2520%2520Hallucinations%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520Zhou%2520and%2520Zihan%2520Zhang%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520strides%2520by%250Acombining%2520visual%2520recognition%2520and%2520language%2520understanding%2520to%2520generate%2520content%250Athat%2520is%2520both%2520coherent%2520and%2520contextually%2520accurate.%2520However%252C%2520MLLMs%2520continue%2520to%250Astruggle%2520with%2520object%2520hallucinations%252C%2520where%2520models%2520produce%2520seemingly%2520plausible%250Abut%2520factually%2520incorrect%2520outputs%252C%2520including%2520objects%2520that%2520do%2520not%2520exist%2520in%2520the%250Aimage.%2520Recent%2520work%2520has%2520revealed%2520that%2520the%2520prior%2520knowledge%2520in%2520MLLMs%2520significantly%250Asuppresses%2520visual%2520information%2520in%2520deep%2520layers%252C%2520causing%2520hallucinatory%2520outputs.%250AHowever%252C%2520how%2520these%2520priors%2520suppress%2520visual%2520information%2520at%2520the%2520intermediate%2520layer%250Astage%2520in%2520MLLMs%2520remains%2520unclear.%2520We%2520observe%2520that%2520visual%2520factual%2520knowledge%2520and%250Athe%2520differences%2520between%2520intermediate-layer%2520prior/original%2520probability%250Adistributions%2520show%2520similar%2520evolutionary%2520trends%2520in%2520intermediate%2520layers.%250AMotivated%2520by%2520this%252C%2520we%2520introduce%2520Decoding%2520by%2520Extracting%2520Visual%2520Facts%2520%2528EVA%2529%252C%2520a%250Asimple%252C%2520training-free%2520method%2520that%2520dynamically%2520selects%2520intermediate%2520layers%2520with%250Athe%2520most%2520significant%2520visual%2520factual%2520information.%2520By%2520contrasting%2520the%2520output%250Adistributions%2520of%2520the%2520selected%2520layer%2520derived%2520from%2520the%2520original%2520input%2520and%250Apure-text%2520input%252C%2520EVA%2520extracts%2520visual%2520factual%2520knowledge%2520and%2520proportionally%250Aincorporates%2520it%2520into%2520the%2520final%2520layer%2520to%2520correct%2520the%2520output%2520logits.%2520Importantly%252C%250AEVA%2520is%2520model-agnostic%252C%2520seamlessly%2520integrates%2520with%2520various%2520classic%2520decoding%250Astrategies%252C%2520and%2520is%2520applicable%2520across%2520different%2520MLLMs.%2520We%2520validate%2520EVA%2520on%250Awidely-used%2520benchmarks%252C%2520and%2520the%2520results%2520show%2520that%2520it%2520significantly%2520reduces%250Ahallucination%2520rates%2520compared%2520to%2520baseline%2520methods%252C%2520underscoring%2520its%250Aeffectiveness%2520in%2520mitigating%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Visual%20Facts%20from%20Intermediate%20Layers%20for%20Mitigating%0A%20%20Hallucinations%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Haoran%20Zhou%20and%20Zihan%20Zhang%20and%20Hao%20Chen&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%20strides%20by%0Acombining%20visual%20recognition%20and%20language%20understanding%20to%20generate%20content%0Athat%20is%20both%20coherent%20and%20contextually%20accurate.%20However%2C%20MLLMs%20continue%20to%0Astruggle%20with%20object%20hallucinations%2C%20where%20models%20produce%20seemingly%20plausible%0Abut%20factually%20incorrect%20outputs%2C%20including%20objects%20that%20do%20not%20exist%20in%20the%0Aimage.%20Recent%20work%20has%20revealed%20that%20the%20prior%20knowledge%20in%20MLLMs%20significantly%0Asuppresses%20visual%20information%20in%20deep%20layers%2C%20causing%20hallucinatory%20outputs.%0AHowever%2C%20how%20these%20priors%20suppress%20visual%20information%20at%20the%20intermediate%20layer%0Astage%20in%20MLLMs%20remains%20unclear.%20We%20observe%20that%20visual%20factual%20knowledge%20and%0Athe%20differences%20between%20intermediate-layer%20prior/original%20probability%0Adistributions%20show%20similar%20evolutionary%20trends%20in%20intermediate%20layers.%0AMotivated%20by%20this%2C%20we%20introduce%20Decoding%20by%20Extracting%20Visual%20Facts%20%28EVA%29%2C%20a%0Asimple%2C%20training-free%20method%20that%20dynamically%20selects%20intermediate%20layers%20with%0Athe%20most%20significant%20visual%20factual%20information.%20By%20contrasting%20the%20output%0Adistributions%20of%20the%20selected%20layer%20derived%20from%20the%20original%20input%20and%0Apure-text%20input%2C%20EVA%20extracts%20visual%20factual%20knowledge%20and%20proportionally%0Aincorporates%20it%20into%20the%20final%20layer%20to%20correct%20the%20output%20logits.%20Importantly%2C%0AEVA%20is%20model-agnostic%2C%20seamlessly%20integrates%20with%20various%20classic%20decoding%0Astrategies%2C%20and%20is%20applicable%20across%20different%20MLLMs.%20We%20validate%20EVA%20on%0Awidely-used%20benchmarks%2C%20and%20the%20results%20show%20that%20it%20significantly%20reduces%0Ahallucination%20rates%20compared%20to%20baseline%20methods%2C%20underscoring%20its%0Aeffectiveness%20in%20mitigating%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15652v1&entry.124074799=Read"},
{"title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant\n  Agents", "author": "Simon Kohaut and Felix Divo and Navid Hamid and Benedict Flade and Julian Eggert and Devendra Singh Dhami and Kristian Kersting", "abstract": "  Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.\n", "link": "http://arxiv.org/abs/2507.15478v1", "date": "2025-07-21", "relevancy": 2.1955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5439}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Constitutional%20Controller%3A%20Doubt-Calibrated%20Steering%20of%20Compliant%0A%20%20Agents&body=Title%3A%20The%20Constitutional%20Controller%3A%20Doubt-Calibrated%20Steering%20of%20Compliant%0A%20%20Agents%0AAuthor%3A%20Simon%20Kohaut%20and%20Felix%20Divo%20and%20Navid%20Hamid%20and%20Benedict%20Flade%20and%20Julian%20Eggert%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Ensuring%20reliable%20and%20rule-compliant%20behavior%20of%20autonomous%20agents%20in%0Auncertain%20environments%20remains%20a%20fundamental%20challenge%20in%20modern%20robotics.%20Our%0Awork%20shows%20how%20neuro-symbolic%20systems%2C%20which%20integrate%20probabilistic%2C%20symbolic%0Awhite-box%20reasoning%20models%20with%20deep%20learning%20methods%2C%20offer%20a%20powerful%0Asolution%20to%20this%20challenge.%20This%20enables%20the%20simultaneous%20consideration%20of%0Aexplicit%20rules%20and%20neural%20models%20trained%20on%20noisy%20data%2C%20combining%20the%20strength%0Aof%20structured%20reasoning%20with%20flexible%20representations.%20To%20this%20end%2C%20we%0Aintroduce%20the%20Constitutional%20Controller%20%28CoCo%29%2C%20a%20novel%20framework%20designed%20to%0Aenhance%20the%20safety%20and%20reliability%20of%20agents%20by%20reasoning%20over%20deep%0Aprobabilistic%20logic%20programs%20representing%20constraints%20such%20as%20those%20found%20in%0Ashared%20traffic%20spaces.%20Furthermore%2C%20we%20propose%20the%20concept%20of%20self-doubt%2C%0Aimplemented%20as%20a%20probability%20density%20conditioned%20on%20doubt%20features%20such%20as%0Atravel%20velocity%2C%20employed%20sensors%2C%20or%20health%20factors.%20In%20a%20real-world%20aerial%0Amobility%20study%2C%20we%20demonstrate%20CoCo%27s%20advantages%20for%20intelligent%20autonomous%0Asystems%20to%20learn%20appropriate%20doubts%20and%20navigate%20complex%20and%20uncertain%0Aenvironments%20safely%20and%20compliantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Constitutional%2520Controller%253A%2520Doubt-Calibrated%2520Steering%2520of%2520Compliant%250A%2520%2520Agents%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Felix%2520Divo%2520and%2520Navid%2520Hamid%2520and%2520Benedict%2520Flade%2520and%2520Julian%2520Eggert%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Ensuring%2520reliable%2520and%2520rule-compliant%2520behavior%2520of%2520autonomous%2520agents%2520in%250Auncertain%2520environments%2520remains%2520a%2520fundamental%2520challenge%2520in%2520modern%2520robotics.%2520Our%250Awork%2520shows%2520how%2520neuro-symbolic%2520systems%252C%2520which%2520integrate%2520probabilistic%252C%2520symbolic%250Awhite-box%2520reasoning%2520models%2520with%2520deep%2520learning%2520methods%252C%2520offer%2520a%2520powerful%250Asolution%2520to%2520this%2520challenge.%2520This%2520enables%2520the%2520simultaneous%2520consideration%2520of%250Aexplicit%2520rules%2520and%2520neural%2520models%2520trained%2520on%2520noisy%2520data%252C%2520combining%2520the%2520strength%250Aof%2520structured%2520reasoning%2520with%2520flexible%2520representations.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520the%2520Constitutional%2520Controller%2520%2528CoCo%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%250Aenhance%2520the%2520safety%2520and%2520reliability%2520of%2520agents%2520by%2520reasoning%2520over%2520deep%250Aprobabilistic%2520logic%2520programs%2520representing%2520constraints%2520such%2520as%2520those%2520found%2520in%250Ashared%2520traffic%2520spaces.%2520Furthermore%252C%2520we%2520propose%2520the%2520concept%2520of%2520self-doubt%252C%250Aimplemented%2520as%2520a%2520probability%2520density%2520conditioned%2520on%2520doubt%2520features%2520such%2520as%250Atravel%2520velocity%252C%2520employed%2520sensors%252C%2520or%2520health%2520factors.%2520In%2520a%2520real-world%2520aerial%250Amobility%2520study%252C%2520we%2520demonstrate%2520CoCo%2527s%2520advantages%2520for%2520intelligent%2520autonomous%250Asystems%2520to%2520learn%2520appropriate%2520doubts%2520and%2520navigate%2520complex%2520and%2520uncertain%250Aenvironments%2520safely%2520and%2520compliantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Constitutional%20Controller%3A%20Doubt-Calibrated%20Steering%20of%20Compliant%0A%20%20Agents&entry.906535625=Simon%20Kohaut%20and%20Felix%20Divo%20and%20Navid%20Hamid%20and%20Benedict%20Flade%20and%20Julian%20Eggert%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting&entry.1292438233=%20%20Ensuring%20reliable%20and%20rule-compliant%20behavior%20of%20autonomous%20agents%20in%0Auncertain%20environments%20remains%20a%20fundamental%20challenge%20in%20modern%20robotics.%20Our%0Awork%20shows%20how%20neuro-symbolic%20systems%2C%20which%20integrate%20probabilistic%2C%20symbolic%0Awhite-box%20reasoning%20models%20with%20deep%20learning%20methods%2C%20offer%20a%20powerful%0Asolution%20to%20this%20challenge.%20This%20enables%20the%20simultaneous%20consideration%20of%0Aexplicit%20rules%20and%20neural%20models%20trained%20on%20noisy%20data%2C%20combining%20the%20strength%0Aof%20structured%20reasoning%20with%20flexible%20representations.%20To%20this%20end%2C%20we%0Aintroduce%20the%20Constitutional%20Controller%20%28CoCo%29%2C%20a%20novel%20framework%20designed%20to%0Aenhance%20the%20safety%20and%20reliability%20of%20agents%20by%20reasoning%20over%20deep%0Aprobabilistic%20logic%20programs%20representing%20constraints%20such%20as%20those%20found%20in%0Ashared%20traffic%20spaces.%20Furthermore%2C%20we%20propose%20the%20concept%20of%20self-doubt%2C%0Aimplemented%20as%20a%20probability%20density%20conditioned%20on%20doubt%20features%20such%20as%0Atravel%20velocity%2C%20employed%20sensors%2C%20or%20health%20factors.%20In%20a%20real-world%20aerial%0Amobility%20study%2C%20we%20demonstrate%20CoCo%27s%20advantages%20for%20intelligent%20autonomous%0Asystems%20to%20learn%20appropriate%20doubts%20and%20navigate%20complex%20and%20uncertain%0Aenvironments%20safely%20and%20compliantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15478v1&entry.124074799=Read"},
{"title": "HER-Seg: Holistically Efficient Segmentation for High-Resolution Medical\n  Images", "author": "Qing Xu and Zhenye Lou and Chenxin Li and Yue Li and Xiangjian He and Tesema Fiseha Berhanu and Rong Qu and Wenting Duan and Zhen Chen", "abstract": "  High-resolution segmentation is critical for precise disease diagnosis by\nextracting fine-grained morphological details. Existing hierarchical\nencoder-decoder frameworks have demonstrated remarkable adaptability across\ndiverse medical segmentation tasks. While beneficial, they usually require the\nhuge computation and memory cost when handling large-size segmentation, which\nlimits their applications in foundation model building and real-world clinical\nscenarios. To address this limitation, we propose a holistically efficient\nframework for high-resolution medical image segmentation, called HER-Seg.\nSpecifically, we first devise a computation-efficient image encoder\n(CE-Encoder) to model long-range dependencies with linear complexity while\nmaintaining sufficient representations. In particular, we introduce the\ndual-gated linear attention (DLA) mechanism to perform cascaded token\nfiltering, selectively retaining important tokens while ignoring irrelevant\nones to enhance attention computation efficiency. Then, we introduce a\nmemory-efficient mask decoder (ME-Decoder) to eliminate the demand for the\nhierarchical structure by leveraging cross-scale segmentation decoding.\nExtensive experiments reveal that HER-Seg outperforms state-of-the-arts in\nhigh-resolution medical 2D, 3D and video segmentation tasks. In particular, our\nHER-Seg requires only 0.59GB training GPU memory and 9.39G inference FLOPs per\n1024$\\times$1024 image, demonstrating superior memory and computation\nefficiency. The code is available at https://github.com/xq141839/HER-Seg.\n", "link": "http://arxiv.org/abs/2504.06205v2", "date": "2025-07-21", "relevancy": 2.182, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HER-Seg%3A%20Holistically%20Efficient%20Segmentation%20for%20High-Resolution%20Medical%0A%20%20Images&body=Title%3A%20HER-Seg%3A%20Holistically%20Efficient%20Segmentation%20for%20High-Resolution%20Medical%0A%20%20Images%0AAuthor%3A%20Qing%20Xu%20and%20Zhenye%20Lou%20and%20Chenxin%20Li%20and%20Yue%20Li%20and%20Xiangjian%20He%20and%20Tesema%20Fiseha%20Berhanu%20and%20Rong%20Qu%20and%20Wenting%20Duan%20and%20Zhen%20Chen%0AAbstract%3A%20%20%20High-resolution%20segmentation%20is%20critical%20for%20precise%20disease%20diagnosis%20by%0Aextracting%20fine-grained%20morphological%20details.%20Existing%20hierarchical%0Aencoder-decoder%20frameworks%20have%20demonstrated%20remarkable%20adaptability%20across%0Adiverse%20medical%20segmentation%20tasks.%20While%20beneficial%2C%20they%20usually%20require%20the%0Ahuge%20computation%20and%20memory%20cost%20when%20handling%20large-size%20segmentation%2C%20which%0Alimits%20their%20applications%20in%20foundation%20model%20building%20and%20real-world%20clinical%0Ascenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20holistically%20efficient%0Aframework%20for%20high-resolution%20medical%20image%20segmentation%2C%20called%20HER-Seg.%0ASpecifically%2C%20we%20first%20devise%20a%20computation-efficient%20image%20encoder%0A%28CE-Encoder%29%20to%20model%20long-range%20dependencies%20with%20linear%20complexity%20while%0Amaintaining%20sufficient%20representations.%20In%20particular%2C%20we%20introduce%20the%0Adual-gated%20linear%20attention%20%28DLA%29%20mechanism%20to%20perform%20cascaded%20token%0Afiltering%2C%20selectively%20retaining%20important%20tokens%20while%20ignoring%20irrelevant%0Aones%20to%20enhance%20attention%20computation%20efficiency.%20Then%2C%20we%20introduce%20a%0Amemory-efficient%20mask%20decoder%20%28ME-Decoder%29%20to%20eliminate%20the%20demand%20for%20the%0Ahierarchical%20structure%20by%20leveraging%20cross-scale%20segmentation%20decoding.%0AExtensive%20experiments%20reveal%20that%20HER-Seg%20outperforms%20state-of-the-arts%20in%0Ahigh-resolution%20medical%202D%2C%203D%20and%20video%20segmentation%20tasks.%20In%20particular%2C%20our%0AHER-Seg%20requires%20only%200.59GB%20training%20GPU%20memory%20and%209.39G%20inference%20FLOPs%20per%0A1024%24%5Ctimes%241024%20image%2C%20demonstrating%20superior%20memory%20and%20computation%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/xq141839/HER-Seg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHER-Seg%253A%2520Holistically%2520Efficient%2520Segmentation%2520for%2520High-Resolution%2520Medical%250A%2520%2520Images%26entry.906535625%3DQing%2520Xu%2520and%2520Zhenye%2520Lou%2520and%2520Chenxin%2520Li%2520and%2520Yue%2520Li%2520and%2520Xiangjian%2520He%2520and%2520Tesema%2520Fiseha%2520Berhanu%2520and%2520Rong%2520Qu%2520and%2520Wenting%2520Duan%2520and%2520Zhen%2520Chen%26entry.1292438233%3D%2520%2520High-resolution%2520segmentation%2520is%2520critical%2520for%2520precise%2520disease%2520diagnosis%2520by%250Aextracting%2520fine-grained%2520morphological%2520details.%2520Existing%2520hierarchical%250Aencoder-decoder%2520frameworks%2520have%2520demonstrated%2520remarkable%2520adaptability%2520across%250Adiverse%2520medical%2520segmentation%2520tasks.%2520While%2520beneficial%252C%2520they%2520usually%2520require%2520the%250Ahuge%2520computation%2520and%2520memory%2520cost%2520when%2520handling%2520large-size%2520segmentation%252C%2520which%250Alimits%2520their%2520applications%2520in%2520foundation%2520model%2520building%2520and%2520real-world%2520clinical%250Ascenarios.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520holistically%2520efficient%250Aframework%2520for%2520high-resolution%2520medical%2520image%2520segmentation%252C%2520called%2520HER-Seg.%250ASpecifically%252C%2520we%2520first%2520devise%2520a%2520computation-efficient%2520image%2520encoder%250A%2528CE-Encoder%2529%2520to%2520model%2520long-range%2520dependencies%2520with%2520linear%2520complexity%2520while%250Amaintaining%2520sufficient%2520representations.%2520In%2520particular%252C%2520we%2520introduce%2520the%250Adual-gated%2520linear%2520attention%2520%2528DLA%2529%2520mechanism%2520to%2520perform%2520cascaded%2520token%250Afiltering%252C%2520selectively%2520retaining%2520important%2520tokens%2520while%2520ignoring%2520irrelevant%250Aones%2520to%2520enhance%2520attention%2520computation%2520efficiency.%2520Then%252C%2520we%2520introduce%2520a%250Amemory-efficient%2520mask%2520decoder%2520%2528ME-Decoder%2529%2520to%2520eliminate%2520the%2520demand%2520for%2520the%250Ahierarchical%2520structure%2520by%2520leveraging%2520cross-scale%2520segmentation%2520decoding.%250AExtensive%2520experiments%2520reveal%2520that%2520HER-Seg%2520outperforms%2520state-of-the-arts%2520in%250Ahigh-resolution%2520medical%25202D%252C%25203D%2520and%2520video%2520segmentation%2520tasks.%2520In%2520particular%252C%2520our%250AHER-Seg%2520requires%2520only%25200.59GB%2520training%2520GPU%2520memory%2520and%25209.39G%2520inference%2520FLOPs%2520per%250A1024%2524%255Ctimes%25241024%2520image%252C%2520demonstrating%2520superior%2520memory%2520and%2520computation%250Aefficiency.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/xq141839/HER-Seg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HER-Seg%3A%20Holistically%20Efficient%20Segmentation%20for%20High-Resolution%20Medical%0A%20%20Images&entry.906535625=Qing%20Xu%20and%20Zhenye%20Lou%20and%20Chenxin%20Li%20and%20Yue%20Li%20and%20Xiangjian%20He%20and%20Tesema%20Fiseha%20Berhanu%20and%20Rong%20Qu%20and%20Wenting%20Duan%20and%20Zhen%20Chen&entry.1292438233=%20%20High-resolution%20segmentation%20is%20critical%20for%20precise%20disease%20diagnosis%20by%0Aextracting%20fine-grained%20morphological%20details.%20Existing%20hierarchical%0Aencoder-decoder%20frameworks%20have%20demonstrated%20remarkable%20adaptability%20across%0Adiverse%20medical%20segmentation%20tasks.%20While%20beneficial%2C%20they%20usually%20require%20the%0Ahuge%20computation%20and%20memory%20cost%20when%20handling%20large-size%20segmentation%2C%20which%0Alimits%20their%20applications%20in%20foundation%20model%20building%20and%20real-world%20clinical%0Ascenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20holistically%20efficient%0Aframework%20for%20high-resolution%20medical%20image%20segmentation%2C%20called%20HER-Seg.%0ASpecifically%2C%20we%20first%20devise%20a%20computation-efficient%20image%20encoder%0A%28CE-Encoder%29%20to%20model%20long-range%20dependencies%20with%20linear%20complexity%20while%0Amaintaining%20sufficient%20representations.%20In%20particular%2C%20we%20introduce%20the%0Adual-gated%20linear%20attention%20%28DLA%29%20mechanism%20to%20perform%20cascaded%20token%0Afiltering%2C%20selectively%20retaining%20important%20tokens%20while%20ignoring%20irrelevant%0Aones%20to%20enhance%20attention%20computation%20efficiency.%20Then%2C%20we%20introduce%20a%0Amemory-efficient%20mask%20decoder%20%28ME-Decoder%29%20to%20eliminate%20the%20demand%20for%20the%0Ahierarchical%20structure%20by%20leveraging%20cross-scale%20segmentation%20decoding.%0AExtensive%20experiments%20reveal%20that%20HER-Seg%20outperforms%20state-of-the-arts%20in%0Ahigh-resolution%20medical%202D%2C%203D%20and%20video%20segmentation%20tasks.%20In%20particular%2C%20our%0AHER-Seg%20requires%20only%200.59GB%20training%20GPU%20memory%20and%209.39G%20inference%20FLOPs%20per%0A1024%24%5Ctimes%241024%20image%2C%20demonstrating%20superior%20memory%20and%20computation%0Aefficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/xq141839/HER-Seg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06205v2&entry.124074799=Read"},
{"title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for\n  Multi-Camera Vehicle Tracking", "author": "Yuqiang Lin and Sam Lockyer and Mingxuan Sui and Li Gan and Florian Stanek and Markus Zarbock and Wenbin Li and Adrian Evans and Nic Zhang", "abstract": "  The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git\n", "link": "http://arxiv.org/abs/2507.08729v2", "date": "2025-07-21", "relevancy": 2.1793, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5792}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking&body=Title%3A%20RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking%0AAuthor%3A%20Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Mingxuan%20Sui%20and%20Li%20Gan%20and%20Florian%20Stanek%20and%20Markus%20Zarbock%20and%20Wenbin%20Li%20and%20Adrian%20Evans%20and%20Nic%20Zhang%0AAbstract%3A%20%20%20The%20multi-camera%20vehicle%20tracking%20%28MCVT%29%20framework%20holds%20significant%0Apotential%20for%20smart%20city%20applications%2C%20including%20anomaly%20detection%2C%20traffic%0Adensity%20estimation%2C%20and%20suspect%20vehicle%20tracking.%20However%2C%20current%20publicly%0Aavailable%20datasets%20exhibit%20limitations%2C%20such%20as%20overly%20simplistic%20scenarios%2C%0Alow-resolution%20footage%2C%20and%20insufficiently%20diverse%20conditions%2C%20creating%20a%0Aconsiderable%20gap%20between%20academic%20research%20and%20real-world%20scenario.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20RoundaboutHD%2C%20a%20comprehensive%2C%20high-resolution%0Amulti-camera%20vehicle%20tracking%20benchmark%20dataset%20specifically%20designed%20to%0Arepresent%20real-world%20roundabout%20scenarios.%20RoundaboutHD%20provides%20a%20total%20of%2040%0Aminutes%20of%20labelled%20video%20footage%20captured%20by%20four%20non-overlapping%2C%0Ahigh-resolution%20%284K%20resolution%2C%2015%20fps%29%20cameras.%20In%20total%2C%20512%20unique%20vehicle%0Aidentities%20are%20annotated%20across%20different%20camera%20views%2C%20offering%20rich%0Across-camera%20association%20data.%20RoundaboutHD%20offers%20temporal%20consistency%20video%0Afootage%20and%20enhanced%20challenges%2C%20including%20increased%20occlusions%20and%20nonlinear%0Amovement%20inside%20the%20roundabout.%20In%20addition%20to%20the%20full%20MCVT%20dataset%2C%20several%0Asubsets%20are%20also%20available%20for%20object%20detection%2C%20single%20camera%20tracking%2C%20and%0Aimage-based%20vehicle%20re-identification%20%28ReID%29%20tasks.%20Vehicle%20model%20information%0Aand%20camera%20modelling/%20geometry%20information%20are%20also%20included%20to%20support%20further%0Aanalysis.%20We%20provide%20baseline%20results%20for%20vehicle%20detection%2C%20single-camera%0Atracking%2C%20image-based%20vehicle%20re-identification%2C%20and%20multi-camera%20tracking.%20The%0Adataset%20and%20the%20evaluation%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/siri-rouser/RoundaboutHD.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoundaboutHD%253A%2520High-Resolution%2520Real-World%2520Urban%2520Environment%2520Benchmark%2520for%250A%2520%2520Multi-Camera%2520Vehicle%2520Tracking%26entry.906535625%3DYuqiang%2520Lin%2520and%2520Sam%2520Lockyer%2520and%2520Mingxuan%2520Sui%2520and%2520Li%2520Gan%2520and%2520Florian%2520Stanek%2520and%2520Markus%2520Zarbock%2520and%2520Wenbin%2520Li%2520and%2520Adrian%2520Evans%2520and%2520Nic%2520Zhang%26entry.1292438233%3D%2520%2520The%2520multi-camera%2520vehicle%2520tracking%2520%2528MCVT%2529%2520framework%2520holds%2520significant%250Apotential%2520for%2520smart%2520city%2520applications%252C%2520including%2520anomaly%2520detection%252C%2520traffic%250Adensity%2520estimation%252C%2520and%2520suspect%2520vehicle%2520tracking.%2520However%252C%2520current%2520publicly%250Aavailable%2520datasets%2520exhibit%2520limitations%252C%2520such%2520as%2520overly%2520simplistic%2520scenarios%252C%250Alow-resolution%2520footage%252C%2520and%2520insufficiently%2520diverse%2520conditions%252C%2520creating%2520a%250Aconsiderable%2520gap%2520between%2520academic%2520research%2520and%2520real-world%2520scenario.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520introduce%2520RoundaboutHD%252C%2520a%2520comprehensive%252C%2520high-resolution%250Amulti-camera%2520vehicle%2520tracking%2520benchmark%2520dataset%2520specifically%2520designed%2520to%250Arepresent%2520real-world%2520roundabout%2520scenarios.%2520RoundaboutHD%2520provides%2520a%2520total%2520of%252040%250Aminutes%2520of%2520labelled%2520video%2520footage%2520captured%2520by%2520four%2520non-overlapping%252C%250Ahigh-resolution%2520%25284K%2520resolution%252C%252015%2520fps%2529%2520cameras.%2520In%2520total%252C%2520512%2520unique%2520vehicle%250Aidentities%2520are%2520annotated%2520across%2520different%2520camera%2520views%252C%2520offering%2520rich%250Across-camera%2520association%2520data.%2520RoundaboutHD%2520offers%2520temporal%2520consistency%2520video%250Afootage%2520and%2520enhanced%2520challenges%252C%2520including%2520increased%2520occlusions%2520and%2520nonlinear%250Amovement%2520inside%2520the%2520roundabout.%2520In%2520addition%2520to%2520the%2520full%2520MCVT%2520dataset%252C%2520several%250Asubsets%2520are%2520also%2520available%2520for%2520object%2520detection%252C%2520single%2520camera%2520tracking%252C%2520and%250Aimage-based%2520vehicle%2520re-identification%2520%2528ReID%2529%2520tasks.%2520Vehicle%2520model%2520information%250Aand%2520camera%2520modelling/%2520geometry%2520information%2520are%2520also%2520included%2520to%2520support%2520further%250Aanalysis.%2520We%2520provide%2520baseline%2520results%2520for%2520vehicle%2520detection%252C%2520single-camera%250Atracking%252C%2520image-based%2520vehicle%2520re-identification%252C%2520and%2520multi-camera%2520tracking.%2520The%250Adataset%2520and%2520the%2520evaluation%2520code%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/siri-rouser/RoundaboutHD.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoundaboutHD%3A%20High-Resolution%20Real-World%20Urban%20Environment%20Benchmark%20for%0A%20%20Multi-Camera%20Vehicle%20Tracking&entry.906535625=Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Mingxuan%20Sui%20and%20Li%20Gan%20and%20Florian%20Stanek%20and%20Markus%20Zarbock%20and%20Wenbin%20Li%20and%20Adrian%20Evans%20and%20Nic%20Zhang&entry.1292438233=%20%20The%20multi-camera%20vehicle%20tracking%20%28MCVT%29%20framework%20holds%20significant%0Apotential%20for%20smart%20city%20applications%2C%20including%20anomaly%20detection%2C%20traffic%0Adensity%20estimation%2C%20and%20suspect%20vehicle%20tracking.%20However%2C%20current%20publicly%0Aavailable%20datasets%20exhibit%20limitations%2C%20such%20as%20overly%20simplistic%20scenarios%2C%0Alow-resolution%20footage%2C%20and%20insufficiently%20diverse%20conditions%2C%20creating%20a%0Aconsiderable%20gap%20between%20academic%20research%20and%20real-world%20scenario.%20To%20fill%0Athis%20gap%2C%20we%20introduce%20RoundaboutHD%2C%20a%20comprehensive%2C%20high-resolution%0Amulti-camera%20vehicle%20tracking%20benchmark%20dataset%20specifically%20designed%20to%0Arepresent%20real-world%20roundabout%20scenarios.%20RoundaboutHD%20provides%20a%20total%20of%2040%0Aminutes%20of%20labelled%20video%20footage%20captured%20by%20four%20non-overlapping%2C%0Ahigh-resolution%20%284K%20resolution%2C%2015%20fps%29%20cameras.%20In%20total%2C%20512%20unique%20vehicle%0Aidentities%20are%20annotated%20across%20different%20camera%20views%2C%20offering%20rich%0Across-camera%20association%20data.%20RoundaboutHD%20offers%20temporal%20consistency%20video%0Afootage%20and%20enhanced%20challenges%2C%20including%20increased%20occlusions%20and%20nonlinear%0Amovement%20inside%20the%20roundabout.%20In%20addition%20to%20the%20full%20MCVT%20dataset%2C%20several%0Asubsets%20are%20also%20available%20for%20object%20detection%2C%20single%20camera%20tracking%2C%20and%0Aimage-based%20vehicle%20re-identification%20%28ReID%29%20tasks.%20Vehicle%20model%20information%0Aand%20camera%20modelling/%20geometry%20information%20are%20also%20included%20to%20support%20further%0Aanalysis.%20We%20provide%20baseline%20results%20for%20vehicle%20detection%2C%20single-camera%0Atracking%2C%20image-based%20vehicle%20re-identification%2C%20and%20multi-camera%20tracking.%20The%0Adataset%20and%20the%20evaluation%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/siri-rouser/RoundaboutHD.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08729v2&entry.124074799=Read"},
{"title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding\n  with a Comprehensive VQA Benchmark", "author": "Aniket Pal and Ajoy Mondal and Minesh Mathew and C. V. Jawahar", "abstract": "  The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.\n", "link": "http://arxiv.org/abs/2507.15655v1", "date": "2025-07-21", "relevancy": 2.1773, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HW-MLVQA%3A%20Elucidating%20Multilingual%20Handwritten%20Document%20Understanding%0A%20%20with%20a%20Comprehensive%20VQA%20Benchmark&body=Title%3A%20HW-MLVQA%3A%20Elucidating%20Multilingual%20Handwritten%20Document%20Understanding%0A%20%20with%20a%20Comprehensive%20VQA%20Benchmark%0AAuthor%3A%20Aniket%20Pal%20and%20Ajoy%20Mondal%20and%20Minesh%20Mathew%20and%20C.%20V.%20Jawahar%0AAbstract%3A%20%20%20The%20proliferation%20of%20MultiLingual%20Visual%20Question%20Answering%20%28MLVQA%29%0Abenchmarks%20augments%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20and%0Amulti-modal%20LLMs%2C%20thereby%20enabling%20them%20to%20adeptly%20capture%20the%20intricate%0Alinguistic%20subtleties%20and%20visual%20complexities%20inherent%20across%20diverse%0Alanguages.%20Despite%20its%20potential%2C%20the%20current%20MLVQA%20model%20struggles%20to%20fully%0Autilize%20its%20capabilities%20when%20dealing%20with%20the%20extensive%20variety%20of%20handwritten%0Adocuments.%20This%20article%20delineates%20HW-MLVQA%2C%20an%20avant-garde%20VQA%20benchmark%0Ameticulously%20crafted%20to%20mitigate%20the%20dearth%20of%20authentic%20Multilingual%0AHandwritten%20document%20comprehension.%20HW-MLVQA%20encompasses%20an%20extensive%0Acollection%20of%201%2C600%20handwritten%20Pages%20complemented%20by%202%2C400%20question-answers.%0AFurthermore%2C%20it%20provides%20a%20robust%20benchmark%20evaluation%20framework%20spanning%20three%0Adistinct%20modalities%3A%20text%2C%20image%2C%20and%20an%20integrated%20image%20%26%20text%20modality.%20To%0Asimulate%20authentic%20real-world%20contexts%20devoid%20of%20ground%20truth%20textual%0Atranscriptions%2C%20we%20facilitates%20a%20rigorous%20assessment%20of%20proprietary%20and%0Aopen-source%20OCR%20models.%20The%20benchmark%20aspires%20to%20facilitate%20pivotal%0Aadvancements%20in%20multilingual%20handwritten%20document%20interpretation%2C%20fostering%0Ainnovation%20and%20scholarly%20inquiry%20within%20this%20specialized%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHW-MLVQA%253A%2520Elucidating%2520Multilingual%2520Handwritten%2520Document%2520Understanding%250A%2520%2520with%2520a%2520Comprehensive%2520VQA%2520Benchmark%26entry.906535625%3DAniket%2520Pal%2520and%2520Ajoy%2520Mondal%2520and%2520Minesh%2520Mathew%2520and%2520C.%2520V.%2520Jawahar%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520MultiLingual%2520Visual%2520Question%2520Answering%2520%2528MLVQA%2529%250Abenchmarks%2520augments%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Amulti-modal%2520LLMs%252C%2520thereby%2520enabling%2520them%2520to%2520adeptly%2520capture%2520the%2520intricate%250Alinguistic%2520subtleties%2520and%2520visual%2520complexities%2520inherent%2520across%2520diverse%250Alanguages.%2520Despite%2520its%2520potential%252C%2520the%2520current%2520MLVQA%2520model%2520struggles%2520to%2520fully%250Autilize%2520its%2520capabilities%2520when%2520dealing%2520with%2520the%2520extensive%2520variety%2520of%2520handwritten%250Adocuments.%2520This%2520article%2520delineates%2520HW-MLVQA%252C%2520an%2520avant-garde%2520VQA%2520benchmark%250Ameticulously%2520crafted%2520to%2520mitigate%2520the%2520dearth%2520of%2520authentic%2520Multilingual%250AHandwritten%2520document%2520comprehension.%2520HW-MLVQA%2520encompasses%2520an%2520extensive%250Acollection%2520of%25201%252C600%2520handwritten%2520Pages%2520complemented%2520by%25202%252C400%2520question-answers.%250AFurthermore%252C%2520it%2520provides%2520a%2520robust%2520benchmark%2520evaluation%2520framework%2520spanning%2520three%250Adistinct%2520modalities%253A%2520text%252C%2520image%252C%2520and%2520an%2520integrated%2520image%2520%2526%2520text%2520modality.%2520To%250Asimulate%2520authentic%2520real-world%2520contexts%2520devoid%2520of%2520ground%2520truth%2520textual%250Atranscriptions%252C%2520we%2520facilitates%2520a%2520rigorous%2520assessment%2520of%2520proprietary%2520and%250Aopen-source%2520OCR%2520models.%2520The%2520benchmark%2520aspires%2520to%2520facilitate%2520pivotal%250Aadvancements%2520in%2520multilingual%2520handwritten%2520document%2520interpretation%252C%2520fostering%250Ainnovation%2520and%2520scholarly%2520inquiry%2520within%2520this%2520specialized%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HW-MLVQA%3A%20Elucidating%20Multilingual%20Handwritten%20Document%20Understanding%0A%20%20with%20a%20Comprehensive%20VQA%20Benchmark&entry.906535625=Aniket%20Pal%20and%20Ajoy%20Mondal%20and%20Minesh%20Mathew%20and%20C.%20V.%20Jawahar&entry.1292438233=%20%20The%20proliferation%20of%20MultiLingual%20Visual%20Question%20Answering%20%28MLVQA%29%0Abenchmarks%20augments%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20and%0Amulti-modal%20LLMs%2C%20thereby%20enabling%20them%20to%20adeptly%20capture%20the%20intricate%0Alinguistic%20subtleties%20and%20visual%20complexities%20inherent%20across%20diverse%0Alanguages.%20Despite%20its%20potential%2C%20the%20current%20MLVQA%20model%20struggles%20to%20fully%0Autilize%20its%20capabilities%20when%20dealing%20with%20the%20extensive%20variety%20of%20handwritten%0Adocuments.%20This%20article%20delineates%20HW-MLVQA%2C%20an%20avant-garde%20VQA%20benchmark%0Ameticulously%20crafted%20to%20mitigate%20the%20dearth%20of%20authentic%20Multilingual%0AHandwritten%20document%20comprehension.%20HW-MLVQA%20encompasses%20an%20extensive%0Acollection%20of%201%2C600%20handwritten%20Pages%20complemented%20by%202%2C400%20question-answers.%0AFurthermore%2C%20it%20provides%20a%20robust%20benchmark%20evaluation%20framework%20spanning%20three%0Adistinct%20modalities%3A%20text%2C%20image%2C%20and%20an%20integrated%20image%20%26%20text%20modality.%20To%0Asimulate%20authentic%20real-world%20contexts%20devoid%20of%20ground%20truth%20textual%0Atranscriptions%2C%20we%20facilitates%20a%20rigorous%20assessment%20of%20proprietary%20and%0Aopen-source%20OCR%20models.%20The%20benchmark%20aspires%20to%20facilitate%20pivotal%0Aadvancements%20in%20multilingual%20handwritten%20document%20interpretation%2C%20fostering%0Ainnovation%20and%20scholarly%20inquiry%20within%20this%20specialized%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15655v1&entry.124074799=Read"},
{"title": "Exploring Superposition and Interference in State-of-the-Art\n  Low-Parameter Vision Models", "author": "Lilian Hollard and Lucas Mohimont and Nathalie Gaveau and Luiz-Angelo Steffenel", "abstract": "  The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel\n", "link": "http://arxiv.org/abs/2507.15798v1", "date": "2025-07-21", "relevancy": 2.1681, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5432}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Superposition%20and%20Interference%20in%20State-of-the-Art%0A%20%20Low-Parameter%20Vision%20Models&body=Title%3A%20Exploring%20Superposition%20and%20Interference%20in%20State-of-the-Art%0A%20%20Low-Parameter%20Vision%20Models%0AAuthor%3A%20Lilian%20Hollard%20and%20Lucas%20Mohimont%20and%20Nathalie%20Gaveau%20and%20Luiz-Angelo%20Steffenel%0AAbstract%3A%20%20%20The%20paper%20investigates%20the%20performance%20of%20state-of-the-art%20low-parameter%20deep%0Aneural%20networks%20for%20computer%20vision%2C%20focusing%20on%20bottleneck%20architectures%20and%0Atheir%20behavior%20using%20superlinear%20activation%20functions.%20We%20address%20interference%0Ain%20feature%20maps%2C%20a%20phenomenon%20associated%20with%20superposition%2C%20where%20neurons%0Asimultaneously%20encode%20multiple%20characteristics.%20Our%20research%20suggests%20that%0Alimiting%20interference%20can%20enhance%20scaling%20and%20accuracy%20in%20very%20low-scaled%0Anetworks%20%28under%201.5M%20parameters%29.%20We%20identify%20key%20design%20elements%20that%20reduce%0Ainterference%20by%20examining%20various%20bottleneck%20architectures%2C%20leading%20to%20a%20more%0Aefficient%20neural%20network.%20Consequently%2C%20we%20propose%20a%20proof-of-concept%0Aarchitecture%20named%20NoDepth%20Bottleneck%20built%20on%20mechanistic%20insights%20from%20our%0Aexperiments%2C%20demonstrating%20robust%20scaling%20accuracy%20on%20the%20ImageNet%20dataset.%0AThese%20findings%20contribute%20to%20more%20efficient%20and%20scalable%20neural%20networks%20for%0Athe%20low-parameter%20range%20and%20advance%20the%20understanding%20of%20bottlenecks%20in%0Acomputer%20vision.%20https%3A//caiac.pubpub.org/pub/3dh6rsel%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Superposition%2520and%2520Interference%2520in%2520State-of-the-Art%250A%2520%2520Low-Parameter%2520Vision%2520Models%26entry.906535625%3DLilian%2520Hollard%2520and%2520Lucas%2520Mohimont%2520and%2520Nathalie%2520Gaveau%2520and%2520Luiz-Angelo%2520Steffenel%26entry.1292438233%3D%2520%2520The%2520paper%2520investigates%2520the%2520performance%2520of%2520state-of-the-art%2520low-parameter%2520deep%250Aneural%2520networks%2520for%2520computer%2520vision%252C%2520focusing%2520on%2520bottleneck%2520architectures%2520and%250Atheir%2520behavior%2520using%2520superlinear%2520activation%2520functions.%2520We%2520address%2520interference%250Ain%2520feature%2520maps%252C%2520a%2520phenomenon%2520associated%2520with%2520superposition%252C%2520where%2520neurons%250Asimultaneously%2520encode%2520multiple%2520characteristics.%2520Our%2520research%2520suggests%2520that%250Alimiting%2520interference%2520can%2520enhance%2520scaling%2520and%2520accuracy%2520in%2520very%2520low-scaled%250Anetworks%2520%2528under%25201.5M%2520parameters%2529.%2520We%2520identify%2520key%2520design%2520elements%2520that%2520reduce%250Ainterference%2520by%2520examining%2520various%2520bottleneck%2520architectures%252C%2520leading%2520to%2520a%2520more%250Aefficient%2520neural%2520network.%2520Consequently%252C%2520we%2520propose%2520a%2520proof-of-concept%250Aarchitecture%2520named%2520NoDepth%2520Bottleneck%2520built%2520on%2520mechanistic%2520insights%2520from%2520our%250Aexperiments%252C%2520demonstrating%2520robust%2520scaling%2520accuracy%2520on%2520the%2520ImageNet%2520dataset.%250AThese%2520findings%2520contribute%2520to%2520more%2520efficient%2520and%2520scalable%2520neural%2520networks%2520for%250Athe%2520low-parameter%2520range%2520and%2520advance%2520the%2520understanding%2520of%2520bottlenecks%2520in%250Acomputer%2520vision.%2520https%253A//caiac.pubpub.org/pub/3dh6rsel%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Superposition%20and%20Interference%20in%20State-of-the-Art%0A%20%20Low-Parameter%20Vision%20Models&entry.906535625=Lilian%20Hollard%20and%20Lucas%20Mohimont%20and%20Nathalie%20Gaveau%20and%20Luiz-Angelo%20Steffenel&entry.1292438233=%20%20The%20paper%20investigates%20the%20performance%20of%20state-of-the-art%20low-parameter%20deep%0Aneural%20networks%20for%20computer%20vision%2C%20focusing%20on%20bottleneck%20architectures%20and%0Atheir%20behavior%20using%20superlinear%20activation%20functions.%20We%20address%20interference%0Ain%20feature%20maps%2C%20a%20phenomenon%20associated%20with%20superposition%2C%20where%20neurons%0Asimultaneously%20encode%20multiple%20characteristics.%20Our%20research%20suggests%20that%0Alimiting%20interference%20can%20enhance%20scaling%20and%20accuracy%20in%20very%20low-scaled%0Anetworks%20%28under%201.5M%20parameters%29.%20We%20identify%20key%20design%20elements%20that%20reduce%0Ainterference%20by%20examining%20various%20bottleneck%20architectures%2C%20leading%20to%20a%20more%0Aefficient%20neural%20network.%20Consequently%2C%20we%20propose%20a%20proof-of-concept%0Aarchitecture%20named%20NoDepth%20Bottleneck%20built%20on%20mechanistic%20insights%20from%20our%0Aexperiments%2C%20demonstrating%20robust%20scaling%20accuracy%20on%20the%20ImageNet%20dataset.%0AThese%20findings%20contribute%20to%20more%20efficient%20and%20scalable%20neural%20networks%20for%0Athe%20low-parameter%20range%20and%20advance%20the%20understanding%20of%20bottlenecks%20in%0Acomputer%20vision.%20https%3A//caiac.pubpub.org/pub/3dh6rsel%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15798v1&entry.124074799=Read"},
{"title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot\n  Segmentation", "author": "Nico Catalano and Stefano Samele and Paolo Pertino and Matteo Matteucci", "abstract": "  Few Shot Segmentation aims to segment novel object classes given only a\nhandful of labeled examples, enabling rapid adaptation with minimal\nsupervision. Current literature crucially lacks a selection method that goes\nbeyond visual similarity between the query and example images, leading to\nsuboptimal predictions. We present MARS, a plug-and-play ranking system that\nleverages multimodal cues to filter and merge mask proposals robustly. Starting\nfrom a set of mask predictions for a single query image, we score, filter, and\nmerge them to improve results. Proposals are evaluated using multimodal scores\ncomputed at local and global levels. Extensive experiments on COCO-20i,\nPascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring\ncomponents is crucial for robust ranking, validating our contribution. As MARS\ncan be effortlessly integrated with various mask proposal systems, we deploy it\nacross a wide range of top-performer methods and achieve new state-of-the-art\nresults on multiple existing benchmarks. Code will be available upon\nacceptance.\n", "link": "http://arxiv.org/abs/2504.07942v2", "date": "2025-07-21", "relevancy": 2.1602, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5272}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARS%3A%20a%20Multimodal%20Alignment%20and%20Ranking%20System%20for%20Few-Shot%0A%20%20Segmentation&body=Title%3A%20MARS%3A%20a%20Multimodal%20Alignment%20and%20Ranking%20System%20for%20Few-Shot%0A%20%20Segmentation%0AAuthor%3A%20Nico%20Catalano%20and%20Stefano%20Samele%20and%20Paolo%20Pertino%20and%20Matteo%20Matteucci%0AAbstract%3A%20%20%20Few%20Shot%20Segmentation%20aims%20to%20segment%20novel%20object%20classes%20given%20only%20a%0Ahandful%20of%20labeled%20examples%2C%20enabling%20rapid%20adaptation%20with%20minimal%0Asupervision.%20Current%20literature%20crucially%20lacks%20a%20selection%20method%20that%20goes%0Abeyond%20visual%20similarity%20between%20the%20query%20and%20example%20images%2C%20leading%20to%0Asuboptimal%20predictions.%20We%20present%20MARS%2C%20a%20plug-and-play%20ranking%20system%20that%0Aleverages%20multimodal%20cues%20to%20filter%20and%20merge%20mask%20proposals%20robustly.%20Starting%0Afrom%20a%20set%20of%20mask%20predictions%20for%20a%20single%20query%20image%2C%20we%20score%2C%20filter%2C%20and%0Amerge%20them%20to%20improve%20results.%20Proposals%20are%20evaluated%20using%20multimodal%20scores%0Acomputed%20at%20local%20and%20global%20levels.%20Extensive%20experiments%20on%20COCO-20i%2C%0APascal-5i%2C%20LVIS-92i%2C%20and%20FSS-1000%20demonstrate%20that%20integrating%20all%20four%20scoring%0Acomponents%20is%20crucial%20for%20robust%20ranking%2C%20validating%20our%20contribution.%20As%20MARS%0Acan%20be%20effortlessly%20integrated%20with%20various%20mask%20proposal%20systems%2C%20we%20deploy%20it%0Aacross%20a%20wide%20range%20of%20top-performer%20methods%20and%20achieve%20new%20state-of-the-art%0Aresults%20on%20multiple%20existing%20benchmarks.%20Code%20will%20be%20available%20upon%0Aacceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARS%253A%2520a%2520Multimodal%2520Alignment%2520and%2520Ranking%2520System%2520for%2520Few-Shot%250A%2520%2520Segmentation%26entry.906535625%3DNico%2520Catalano%2520and%2520Stefano%2520Samele%2520and%2520Paolo%2520Pertino%2520and%2520Matteo%2520Matteucci%26entry.1292438233%3D%2520%2520Few%2520Shot%2520Segmentation%2520aims%2520to%2520segment%2520novel%2520object%2520classes%2520given%2520only%2520a%250Ahandful%2520of%2520labeled%2520examples%252C%2520enabling%2520rapid%2520adaptation%2520with%2520minimal%250Asupervision.%2520Current%2520literature%2520crucially%2520lacks%2520a%2520selection%2520method%2520that%2520goes%250Abeyond%2520visual%2520similarity%2520between%2520the%2520query%2520and%2520example%2520images%252C%2520leading%2520to%250Asuboptimal%2520predictions.%2520We%2520present%2520MARS%252C%2520a%2520plug-and-play%2520ranking%2520system%2520that%250Aleverages%2520multimodal%2520cues%2520to%2520filter%2520and%2520merge%2520mask%2520proposals%2520robustly.%2520Starting%250Afrom%2520a%2520set%2520of%2520mask%2520predictions%2520for%2520a%2520single%2520query%2520image%252C%2520we%2520score%252C%2520filter%252C%2520and%250Amerge%2520them%2520to%2520improve%2520results.%2520Proposals%2520are%2520evaluated%2520using%2520multimodal%2520scores%250Acomputed%2520at%2520local%2520and%2520global%2520levels.%2520Extensive%2520experiments%2520on%2520COCO-20i%252C%250APascal-5i%252C%2520LVIS-92i%252C%2520and%2520FSS-1000%2520demonstrate%2520that%2520integrating%2520all%2520four%2520scoring%250Acomponents%2520is%2520crucial%2520for%2520robust%2520ranking%252C%2520validating%2520our%2520contribution.%2520As%2520MARS%250Acan%2520be%2520effortlessly%2520integrated%2520with%2520various%2520mask%2520proposal%2520systems%252C%2520we%2520deploy%2520it%250Aacross%2520a%2520wide%2520range%2520of%2520top-performer%2520methods%2520and%2520achieve%2520new%2520state-of-the-art%250Aresults%2520on%2520multiple%2520existing%2520benchmarks.%2520Code%2520will%2520be%2520available%2520upon%250Aacceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARS%3A%20a%20Multimodal%20Alignment%20and%20Ranking%20System%20for%20Few-Shot%0A%20%20Segmentation&entry.906535625=Nico%20Catalano%20and%20Stefano%20Samele%20and%20Paolo%20Pertino%20and%20Matteo%20Matteucci&entry.1292438233=%20%20Few%20Shot%20Segmentation%20aims%20to%20segment%20novel%20object%20classes%20given%20only%20a%0Ahandful%20of%20labeled%20examples%2C%20enabling%20rapid%20adaptation%20with%20minimal%0Asupervision.%20Current%20literature%20crucially%20lacks%20a%20selection%20method%20that%20goes%0Abeyond%20visual%20similarity%20between%20the%20query%20and%20example%20images%2C%20leading%20to%0Asuboptimal%20predictions.%20We%20present%20MARS%2C%20a%20plug-and-play%20ranking%20system%20that%0Aleverages%20multimodal%20cues%20to%20filter%20and%20merge%20mask%20proposals%20robustly.%20Starting%0Afrom%20a%20set%20of%20mask%20predictions%20for%20a%20single%20query%20image%2C%20we%20score%2C%20filter%2C%20and%0Amerge%20them%20to%20improve%20results.%20Proposals%20are%20evaluated%20using%20multimodal%20scores%0Acomputed%20at%20local%20and%20global%20levels.%20Extensive%20experiments%20on%20COCO-20i%2C%0APascal-5i%2C%20LVIS-92i%2C%20and%20FSS-1000%20demonstrate%20that%20integrating%20all%20four%20scoring%0Acomponents%20is%20crucial%20for%20robust%20ranking%2C%20validating%20our%20contribution.%20As%20MARS%0Acan%20be%20effortlessly%20integrated%20with%20various%20mask%20proposal%20systems%2C%20we%20deploy%20it%0Aacross%20a%20wide%20range%20of%20top-performer%20methods%20and%20achieve%20new%20state-of-the-art%0Aresults%20on%20multiple%20existing%20benchmarks.%20Code%20will%20be%20available%20upon%0Aacceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07942v2&entry.124074799=Read"},
{"title": "Dictionary-Learning-Based Data Pruning for System Identification", "author": "Tingna Wang and Sikai Zhang and Mingming Song and Limin Sun", "abstract": "  System identification is normally involved in augmenting time series data by\ntime shifting and nonlinearisation (e.g., polynomial basis), both of which\nintroduce redundancy in features and samples. Many research works focus on\nreducing redundancy feature-wise, while less attention is paid to sample-wise\nredundancy. This paper proposes a novel data pruning method, called mini-batch\nFastCan, to reduce sample-wise redundancy based on dictionary learning. Time\nseries data is represented by some representative samples, called atoms, via\ndictionary learning. The useful samples are selected based on their correlation\nwith the atoms. The method is tested on one simulated dataset and two benchmark\ndatasets. The R-squared between the coefficients of models trained on the full\ndatasets and the coefficients of models trained on pruned datasets is adopted\nto evaluate the performance of data pruning methods. It is found that the\nproposed method significantly outperforms the random pruning method.\n", "link": "http://arxiv.org/abs/2502.11484v2", "date": "2025-07-21", "relevancy": 2.1509, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4475}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4258}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dictionary-Learning-Based%20Data%20Pruning%20for%20System%20Identification&body=Title%3A%20Dictionary-Learning-Based%20Data%20Pruning%20for%20System%20Identification%0AAuthor%3A%20Tingna%20Wang%20and%20Sikai%20Zhang%20and%20Mingming%20Song%20and%20Limin%20Sun%0AAbstract%3A%20%20%20System%20identification%20is%20normally%20involved%20in%20augmenting%20time%20series%20data%20by%0Atime%20shifting%20and%20nonlinearisation%20%28e.g.%2C%20polynomial%20basis%29%2C%20both%20of%20which%0Aintroduce%20redundancy%20in%20features%20and%20samples.%20Many%20research%20works%20focus%20on%0Areducing%20redundancy%20feature-wise%2C%20while%20less%20attention%20is%20paid%20to%20sample-wise%0Aredundancy.%20This%20paper%20proposes%20a%20novel%20data%20pruning%20method%2C%20called%20mini-batch%0AFastCan%2C%20to%20reduce%20sample-wise%20redundancy%20based%20on%20dictionary%20learning.%20Time%0Aseries%20data%20is%20represented%20by%20some%20representative%20samples%2C%20called%20atoms%2C%20via%0Adictionary%20learning.%20The%20useful%20samples%20are%20selected%20based%20on%20their%20correlation%0Awith%20the%20atoms.%20The%20method%20is%20tested%20on%20one%20simulated%20dataset%20and%20two%20benchmark%0Adatasets.%20The%20R-squared%20between%20the%20coefficients%20of%20models%20trained%20on%20the%20full%0Adatasets%20and%20the%20coefficients%20of%20models%20trained%20on%20pruned%20datasets%20is%20adopted%0Ato%20evaluate%20the%20performance%20of%20data%20pruning%20methods.%20It%20is%20found%20that%20the%0Aproposed%20method%20significantly%20outperforms%20the%20random%20pruning%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDictionary-Learning-Based%2520Data%2520Pruning%2520for%2520System%2520Identification%26entry.906535625%3DTingna%2520Wang%2520and%2520Sikai%2520Zhang%2520and%2520Mingming%2520Song%2520and%2520Limin%2520Sun%26entry.1292438233%3D%2520%2520System%2520identification%2520is%2520normally%2520involved%2520in%2520augmenting%2520time%2520series%2520data%2520by%250Atime%2520shifting%2520and%2520nonlinearisation%2520%2528e.g.%252C%2520polynomial%2520basis%2529%252C%2520both%2520of%2520which%250Aintroduce%2520redundancy%2520in%2520features%2520and%2520samples.%2520Many%2520research%2520works%2520focus%2520on%250Areducing%2520redundancy%2520feature-wise%252C%2520while%2520less%2520attention%2520is%2520paid%2520to%2520sample-wise%250Aredundancy.%2520This%2520paper%2520proposes%2520a%2520novel%2520data%2520pruning%2520method%252C%2520called%2520mini-batch%250AFastCan%252C%2520to%2520reduce%2520sample-wise%2520redundancy%2520based%2520on%2520dictionary%2520learning.%2520Time%250Aseries%2520data%2520is%2520represented%2520by%2520some%2520representative%2520samples%252C%2520called%2520atoms%252C%2520via%250Adictionary%2520learning.%2520The%2520useful%2520samples%2520are%2520selected%2520based%2520on%2520their%2520correlation%250Awith%2520the%2520atoms.%2520The%2520method%2520is%2520tested%2520on%2520one%2520simulated%2520dataset%2520and%2520two%2520benchmark%250Adatasets.%2520The%2520R-squared%2520between%2520the%2520coefficients%2520of%2520models%2520trained%2520on%2520the%2520full%250Adatasets%2520and%2520the%2520coefficients%2520of%2520models%2520trained%2520on%2520pruned%2520datasets%2520is%2520adopted%250Ato%2520evaluate%2520the%2520performance%2520of%2520data%2520pruning%2520methods.%2520It%2520is%2520found%2520that%2520the%250Aproposed%2520method%2520significantly%2520outperforms%2520the%2520random%2520pruning%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dictionary-Learning-Based%20Data%20Pruning%20for%20System%20Identification&entry.906535625=Tingna%20Wang%20and%20Sikai%20Zhang%20and%20Mingming%20Song%20and%20Limin%20Sun&entry.1292438233=%20%20System%20identification%20is%20normally%20involved%20in%20augmenting%20time%20series%20data%20by%0Atime%20shifting%20and%20nonlinearisation%20%28e.g.%2C%20polynomial%20basis%29%2C%20both%20of%20which%0Aintroduce%20redundancy%20in%20features%20and%20samples.%20Many%20research%20works%20focus%20on%0Areducing%20redundancy%20feature-wise%2C%20while%20less%20attention%20is%20paid%20to%20sample-wise%0Aredundancy.%20This%20paper%20proposes%20a%20novel%20data%20pruning%20method%2C%20called%20mini-batch%0AFastCan%2C%20to%20reduce%20sample-wise%20redundancy%20based%20on%20dictionary%20learning.%20Time%0Aseries%20data%20is%20represented%20by%20some%20representative%20samples%2C%20called%20atoms%2C%20via%0Adictionary%20learning.%20The%20useful%20samples%20are%20selected%20based%20on%20their%20correlation%0Awith%20the%20atoms.%20The%20method%20is%20tested%20on%20one%20simulated%20dataset%20and%20two%20benchmark%0Adatasets.%20The%20R-squared%20between%20the%20coefficients%20of%20models%20trained%20on%20the%20full%0Adatasets%20and%20the%20coefficients%20of%20models%20trained%20on%20pruned%20datasets%20is%20adopted%0Ato%20evaluate%20the%20performance%20of%20data%20pruning%20methods.%20It%20is%20found%20that%20the%0Aproposed%20method%20significantly%20outperforms%20the%20random%20pruning%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11484v2&entry.124074799=Read"},
{"title": "Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval", "author": "Deyu Zhang and Tingting Long and Jinrui Zhang and Ligeng Chen and Ju Ren and Yaoxue Zhang", "abstract": "  Enabling efficient text-video retrieval on edge-end devices is critical for\nreal-world applications. Yet, existing methods face a critical challenge in\nbalancing accuracy and computational efficiency: uniform frame sampling methods\nensure content coverage but incur prohibitive computational costs, while\nsalient-frame sampling methods reduce overhead but suffer from query-agnostic\nframe selection that biases retrieval results. To address this, we propose\nProCLIP, a user-centric framework that achieves state-of-the-art accuracy with\nsignificantly improved efficiency. We design a prompt-aware frame sampling\nstrategy that dynamically guides lightweight feature extractors using textual\nprompts to select semantically relevant frames, overcoming the limitations of\nexisting salient-frame sampling methods which rely on static, query-agnostic\nselection criteria. Moreover, we adopt a two-stage candidate pruning strategy\nthat combines rapid coarse filtering via a lightweight module with CLIP-powered\nfine-grained re-ranking, enhancing retrieval efficiency while preserving\naccuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency\nreduction versus baselines while maintaining competitive accuracy, i.e.,\nR@1=49.0 in MSR-VTT dataset. Code is available at\nhttps://github.com/tiffylong/ProCLIP.\n", "link": "http://arxiv.org/abs/2507.15491v1", "date": "2025-07-21", "relevancy": 2.1319, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5315}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-aware%20of%20Frame%20Sampling%20for%20Efficient%20Text-Video%20Retrieval&body=Title%3A%20Prompt-aware%20of%20Frame%20Sampling%20for%20Efficient%20Text-Video%20Retrieval%0AAuthor%3A%20Deyu%20Zhang%20and%20Tingting%20Long%20and%20Jinrui%20Zhang%20and%20Ligeng%20Chen%20and%20Ju%20Ren%20and%20Yaoxue%20Zhang%0AAbstract%3A%20%20%20Enabling%20efficient%20text-video%20retrieval%20on%20edge-end%20devices%20is%20critical%20for%0Areal-world%20applications.%20Yet%2C%20existing%20methods%20face%20a%20critical%20challenge%20in%0Abalancing%20accuracy%20and%20computational%20efficiency%3A%20uniform%20frame%20sampling%20methods%0Aensure%20content%20coverage%20but%20incur%20prohibitive%20computational%20costs%2C%20while%0Asalient-frame%20sampling%20methods%20reduce%20overhead%20but%20suffer%20from%20query-agnostic%0Aframe%20selection%20that%20biases%20retrieval%20results.%20To%20address%20this%2C%20we%20propose%0AProCLIP%2C%20a%20user-centric%20framework%20that%20achieves%20state-of-the-art%20accuracy%20with%0Asignificantly%20improved%20efficiency.%20We%20design%20a%20prompt-aware%20frame%20sampling%0Astrategy%20that%20dynamically%20guides%20lightweight%20feature%20extractors%20using%20textual%0Aprompts%20to%20select%20semantically%20relevant%20frames%2C%20overcoming%20the%20limitations%20of%0Aexisting%20salient-frame%20sampling%20methods%20which%20rely%20on%20static%2C%20query-agnostic%0Aselection%20criteria.%20Moreover%2C%20we%20adopt%20a%20two-stage%20candidate%20pruning%20strategy%0Athat%20combines%20rapid%20coarse%20filtering%20via%20a%20lightweight%20module%20with%20CLIP-powered%0Afine-grained%20re-ranking%2C%20enhancing%20retrieval%20efficiency%20while%20preserving%0Aaccuracy.%20Experiments%20across%20benchmarks%20show%20ProCLIP%20achieves%2075.3%25%20latency%0Areduction%20versus%20baselines%20while%20maintaining%20competitive%20accuracy%2C%20i.e.%2C%0AR%401%3D49.0%20in%20MSR-VTT%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/tiffylong/ProCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-aware%2520of%2520Frame%2520Sampling%2520for%2520Efficient%2520Text-Video%2520Retrieval%26entry.906535625%3DDeyu%2520Zhang%2520and%2520Tingting%2520Long%2520and%2520Jinrui%2520Zhang%2520and%2520Ligeng%2520Chen%2520and%2520Ju%2520Ren%2520and%2520Yaoxue%2520Zhang%26entry.1292438233%3D%2520%2520Enabling%2520efficient%2520text-video%2520retrieval%2520on%2520edge-end%2520devices%2520is%2520critical%2520for%250Areal-world%2520applications.%2520Yet%252C%2520existing%2520methods%2520face%2520a%2520critical%2520challenge%2520in%250Abalancing%2520accuracy%2520and%2520computational%2520efficiency%253A%2520uniform%2520frame%2520sampling%2520methods%250Aensure%2520content%2520coverage%2520but%2520incur%2520prohibitive%2520computational%2520costs%252C%2520while%250Asalient-frame%2520sampling%2520methods%2520reduce%2520overhead%2520but%2520suffer%2520from%2520query-agnostic%250Aframe%2520selection%2520that%2520biases%2520retrieval%2520results.%2520To%2520address%2520this%252C%2520we%2520propose%250AProCLIP%252C%2520a%2520user-centric%2520framework%2520that%2520achieves%2520state-of-the-art%2520accuracy%2520with%250Asignificantly%2520improved%2520efficiency.%2520We%2520design%2520a%2520prompt-aware%2520frame%2520sampling%250Astrategy%2520that%2520dynamically%2520guides%2520lightweight%2520feature%2520extractors%2520using%2520textual%250Aprompts%2520to%2520select%2520semantically%2520relevant%2520frames%252C%2520overcoming%2520the%2520limitations%2520of%250Aexisting%2520salient-frame%2520sampling%2520methods%2520which%2520rely%2520on%2520static%252C%2520query-agnostic%250Aselection%2520criteria.%2520Moreover%252C%2520we%2520adopt%2520a%2520two-stage%2520candidate%2520pruning%2520strategy%250Athat%2520combines%2520rapid%2520coarse%2520filtering%2520via%2520a%2520lightweight%2520module%2520with%2520CLIP-powered%250Afine-grained%2520re-ranking%252C%2520enhancing%2520retrieval%2520efficiency%2520while%2520preserving%250Aaccuracy.%2520Experiments%2520across%2520benchmarks%2520show%2520ProCLIP%2520achieves%252075.3%2525%2520latency%250Areduction%2520versus%2520baselines%2520while%2520maintaining%2520competitive%2520accuracy%252C%2520i.e.%252C%250AR%25401%253D49.0%2520in%2520MSR-VTT%2520dataset.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/tiffylong/ProCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-aware%20of%20Frame%20Sampling%20for%20Efficient%20Text-Video%20Retrieval&entry.906535625=Deyu%20Zhang%20and%20Tingting%20Long%20and%20Jinrui%20Zhang%20and%20Ligeng%20Chen%20and%20Ju%20Ren%20and%20Yaoxue%20Zhang&entry.1292438233=%20%20Enabling%20efficient%20text-video%20retrieval%20on%20edge-end%20devices%20is%20critical%20for%0Areal-world%20applications.%20Yet%2C%20existing%20methods%20face%20a%20critical%20challenge%20in%0Abalancing%20accuracy%20and%20computational%20efficiency%3A%20uniform%20frame%20sampling%20methods%0Aensure%20content%20coverage%20but%20incur%20prohibitive%20computational%20costs%2C%20while%0Asalient-frame%20sampling%20methods%20reduce%20overhead%20but%20suffer%20from%20query-agnostic%0Aframe%20selection%20that%20biases%20retrieval%20results.%20To%20address%20this%2C%20we%20propose%0AProCLIP%2C%20a%20user-centric%20framework%20that%20achieves%20state-of-the-art%20accuracy%20with%0Asignificantly%20improved%20efficiency.%20We%20design%20a%20prompt-aware%20frame%20sampling%0Astrategy%20that%20dynamically%20guides%20lightweight%20feature%20extractors%20using%20textual%0Aprompts%20to%20select%20semantically%20relevant%20frames%2C%20overcoming%20the%20limitations%20of%0Aexisting%20salient-frame%20sampling%20methods%20which%20rely%20on%20static%2C%20query-agnostic%0Aselection%20criteria.%20Moreover%2C%20we%20adopt%20a%20two-stage%20candidate%20pruning%20strategy%0Athat%20combines%20rapid%20coarse%20filtering%20via%20a%20lightweight%20module%20with%20CLIP-powered%0Afine-grained%20re-ranking%2C%20enhancing%20retrieval%20efficiency%20while%20preserving%0Aaccuracy.%20Experiments%20across%20benchmarks%20show%20ProCLIP%20achieves%2075.3%25%20latency%0Areduction%20versus%20baselines%20while%20maintaining%20competitive%20accuracy%2C%20i.e.%2C%0AR%401%3D49.0%20in%20MSR-VTT%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/tiffylong/ProCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15491v1&entry.124074799=Read"},
{"title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts", "author": "Sungmin Yun and Seonyong Park and Hwayong Nam and Younjoo Lee and Gunjun Lee and Kwanhee Kyung and Sangpyo Kim and Nam Sung Kim and Jongmin Kim and Hyungyo Kim and Juhwan Cho and Seungmin Baek and Jung Ho Ahn", "abstract": "  Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.\n", "link": "http://arxiv.org/abs/2507.15465v1", "date": "2025-07-21", "relevancy": 2.1235, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5514}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20New%20LLM%20Bottleneck%3A%20A%20Systems%20Perspective%20on%20Latent%20Attention%20and%0A%20%20Mixture-of-Experts&body=Title%3A%20The%20New%20LLM%20Bottleneck%3A%20A%20Systems%20Perspective%20on%20Latent%20Attention%20and%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Sungmin%20Yun%20and%20Seonyong%20Park%20and%20Hwayong%20Nam%20and%20Younjoo%20Lee%20and%20Gunjun%20Lee%20and%20Kwanhee%20Kyung%20and%20Sangpyo%20Kim%20and%20Nam%20Sung%20Kim%20and%20Jongmin%20Kim%20and%20Hyungyo%20Kim%20and%20Juhwan%20Cho%20and%20Seungmin%20Baek%20and%20Jung%20Ho%20Ahn%0AAbstract%3A%20%20%20Computational%20workloads%20composing%20traditional%20Transformer%20models%20are%20starkly%0Abifurcated.%20Multi-Head%20Attention%20%28MHA%29%20is%20memory-bound%2C%20with%20low%20arithmetic%0Aintensity%2C%20while%20feedforward%20layers%20are%20compute-bound.%20This%20dichotomy%20has%20long%0Amotivated%20research%20into%20specialized%20hardware%20to%20mitigate%20the%20MHA%20bottleneck.%0A%20%20This%20paper%20argues%20that%20recent%20architectural%20shifts%2C%20namely%20Multi-head%20Latent%0AAttention%20%28MLA%29%20and%20Mixture-of-Experts%20%28MoE%29%2C%20challenge%20the%20premise%20of%0Aspecialized%20attention%20hardware.%20We%20make%20two%20key%20observations.%20First%2C%20the%0Aarithmetic%20intensity%20of%20MLA%20is%20over%20two%20orders%20of%20magnitude%20greater%20than%20that%0Aof%20MHA%2C%20shifting%20it%20close%20to%20a%20compute-bound%20regime%20well-suited%20for%20modern%0Aaccelerators%20like%20GPUs.%20Second%2C%20by%20distributing%20MoE%20experts%20across%20a%20pool%20of%0Aaccelerators%2C%20their%20arithmetic%20intensity%20can%20be%20tuned%20through%20batching%20to%20match%0Athat%20of%20the%20dense%20layers%2C%20creating%20a%20more%20balanced%20computational%20profile.%0A%20%20These%20findings%20reveal%20a%20diminishing%20need%20for%20specialized%20attention%20hardware.%0AThe%20central%20challenge%20for%20next-generation%20Transformers%20is%20no%20longer%0Aaccelerating%20a%20single%20memory-bound%20layer.%20Instead%2C%20the%20focus%20must%20shift%20to%0Adesigning%20balanced%20systems%20with%20sufficient%20compute%2C%20memory%20capacity%2C%20memory%0Abandwidth%2C%20and%20high-bandwidth%20interconnects%20to%20manage%20the%20diverse%20demands%20of%0Alarge-scale%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520New%2520LLM%2520Bottleneck%253A%2520A%2520Systems%2520Perspective%2520on%2520Latent%2520Attention%2520and%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DSungmin%2520Yun%2520and%2520Seonyong%2520Park%2520and%2520Hwayong%2520Nam%2520and%2520Younjoo%2520Lee%2520and%2520Gunjun%2520Lee%2520and%2520Kwanhee%2520Kyung%2520and%2520Sangpyo%2520Kim%2520and%2520Nam%2520Sung%2520Kim%2520and%2520Jongmin%2520Kim%2520and%2520Hyungyo%2520Kim%2520and%2520Juhwan%2520Cho%2520and%2520Seungmin%2520Baek%2520and%2520Jung%2520Ho%2520Ahn%26entry.1292438233%3D%2520%2520Computational%2520workloads%2520composing%2520traditional%2520Transformer%2520models%2520are%2520starkly%250Abifurcated.%2520Multi-Head%2520Attention%2520%2528MHA%2529%2520is%2520memory-bound%252C%2520with%2520low%2520arithmetic%250Aintensity%252C%2520while%2520feedforward%2520layers%2520are%2520compute-bound.%2520This%2520dichotomy%2520has%2520long%250Amotivated%2520research%2520into%2520specialized%2520hardware%2520to%2520mitigate%2520the%2520MHA%2520bottleneck.%250A%2520%2520This%2520paper%2520argues%2520that%2520recent%2520architectural%2520shifts%252C%2520namely%2520Multi-head%2520Latent%250AAttention%2520%2528MLA%2529%2520and%2520Mixture-of-Experts%2520%2528MoE%2529%252C%2520challenge%2520the%2520premise%2520of%250Aspecialized%2520attention%2520hardware.%2520We%2520make%2520two%2520key%2520observations.%2520First%252C%2520the%250Aarithmetic%2520intensity%2520of%2520MLA%2520is%2520over%2520two%2520orders%2520of%2520magnitude%2520greater%2520than%2520that%250Aof%2520MHA%252C%2520shifting%2520it%2520close%2520to%2520a%2520compute-bound%2520regime%2520well-suited%2520for%2520modern%250Aaccelerators%2520like%2520GPUs.%2520Second%252C%2520by%2520distributing%2520MoE%2520experts%2520across%2520a%2520pool%2520of%250Aaccelerators%252C%2520their%2520arithmetic%2520intensity%2520can%2520be%2520tuned%2520through%2520batching%2520to%2520match%250Athat%2520of%2520the%2520dense%2520layers%252C%2520creating%2520a%2520more%2520balanced%2520computational%2520profile.%250A%2520%2520These%2520findings%2520reveal%2520a%2520diminishing%2520need%2520for%2520specialized%2520attention%2520hardware.%250AThe%2520central%2520challenge%2520for%2520next-generation%2520Transformers%2520is%2520no%2520longer%250Aaccelerating%2520a%2520single%2520memory-bound%2520layer.%2520Instead%252C%2520the%2520focus%2520must%2520shift%2520to%250Adesigning%2520balanced%2520systems%2520with%2520sufficient%2520compute%252C%2520memory%2520capacity%252C%2520memory%250Abandwidth%252C%2520and%2520high-bandwidth%2520interconnects%2520to%2520manage%2520the%2520diverse%2520demands%2520of%250Alarge-scale%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20New%20LLM%20Bottleneck%3A%20A%20Systems%20Perspective%20on%20Latent%20Attention%20and%0A%20%20Mixture-of-Experts&entry.906535625=Sungmin%20Yun%20and%20Seonyong%20Park%20and%20Hwayong%20Nam%20and%20Younjoo%20Lee%20and%20Gunjun%20Lee%20and%20Kwanhee%20Kyung%20and%20Sangpyo%20Kim%20and%20Nam%20Sung%20Kim%20and%20Jongmin%20Kim%20and%20Hyungyo%20Kim%20and%20Juhwan%20Cho%20and%20Seungmin%20Baek%20and%20Jung%20Ho%20Ahn&entry.1292438233=%20%20Computational%20workloads%20composing%20traditional%20Transformer%20models%20are%20starkly%0Abifurcated.%20Multi-Head%20Attention%20%28MHA%29%20is%20memory-bound%2C%20with%20low%20arithmetic%0Aintensity%2C%20while%20feedforward%20layers%20are%20compute-bound.%20This%20dichotomy%20has%20long%0Amotivated%20research%20into%20specialized%20hardware%20to%20mitigate%20the%20MHA%20bottleneck.%0A%20%20This%20paper%20argues%20that%20recent%20architectural%20shifts%2C%20namely%20Multi-head%20Latent%0AAttention%20%28MLA%29%20and%20Mixture-of-Experts%20%28MoE%29%2C%20challenge%20the%20premise%20of%0Aspecialized%20attention%20hardware.%20We%20make%20two%20key%20observations.%20First%2C%20the%0Aarithmetic%20intensity%20of%20MLA%20is%20over%20two%20orders%20of%20magnitude%20greater%20than%20that%0Aof%20MHA%2C%20shifting%20it%20close%20to%20a%20compute-bound%20regime%20well-suited%20for%20modern%0Aaccelerators%20like%20GPUs.%20Second%2C%20by%20distributing%20MoE%20experts%20across%20a%20pool%20of%0Aaccelerators%2C%20their%20arithmetic%20intensity%20can%20be%20tuned%20through%20batching%20to%20match%0Athat%20of%20the%20dense%20layers%2C%20creating%20a%20more%20balanced%20computational%20profile.%0A%20%20These%20findings%20reveal%20a%20diminishing%20need%20for%20specialized%20attention%20hardware.%0AThe%20central%20challenge%20for%20next-generation%20Transformers%20is%20no%20longer%0Aaccelerating%20a%20single%20memory-bound%20layer.%20Instead%2C%20the%20focus%20must%20shift%20to%0Adesigning%20balanced%20systems%20with%20sufficient%20compute%2C%20memory%20capacity%2C%20memory%0Abandwidth%2C%20and%20high-bandwidth%20interconnects%20to%20manage%20the%20diverse%20demands%20of%0Alarge-scale%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15465v1&entry.124074799=Read"},
{"title": "How to Leverage Predictive Uncertainty Estimates for Reducing\n  Catastrophic Forgetting in Online Continual Learning", "author": "Giuseppe Serra and Ben Werner and Florian Buettner", "abstract": "  Many real-world applications require machine-learning models to be able to\ndeal with non-stationary data distributions and thus learn autonomously over an\nextended period of time, often in an online setting. One of the main challenges\nin this scenario is the so-called catastrophic forgetting (CF) for which the\nlearning model tends to focus on the most recent tasks while experiencing\npredictive degradation on older ones. In the online setting, the most effective\nsolutions employ a fixed-size memory buffer to store old samples used for\nreplay when training on new tasks. Many approaches have been presented to\ntackle this problem. However, it is not clear how predictive uncertainty\ninformation for memory management can be leveraged in the most effective manner\nand conflicting strategies are proposed to populate the memory. Are the\neasiest-to-forget or the easiest-to-remember samples more effective in\ncombating CF? Starting from the intuition that predictive uncertainty provides\nan idea of the samples' location in the decision space, this work presents an\nin-depth analysis of different uncertainty estimates and strategies for\npopulating the memory. The investigation provides a better understanding of the\ncharacteristics data points should have for alleviating CF. Then, we propose an\nalternative method for estimating predictive uncertainty via the generalised\nvariance induced by the negative log-likelihood. Finally, we demonstrate that\nthe use of predictive uncertainty measures helps in reducing CF in different\nsettings.\n", "link": "http://arxiv.org/abs/2407.07668v3", "date": "2025-07-21", "relevancy": 2.1171, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5314}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.529}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning&body=Title%3A%20How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning%0AAuthor%3A%20Giuseppe%20Serra%20and%20Ben%20Werner%20and%20Florian%20Buettner%0AAbstract%3A%20%20%20Many%20real-world%20applications%20require%20machine-learning%20models%20to%20be%20able%20to%0Adeal%20with%20non-stationary%20data%20distributions%20and%20thus%20learn%20autonomously%20over%20an%0Aextended%20period%20of%20time%2C%20often%20in%20an%20online%20setting.%20One%20of%20the%20main%20challenges%0Ain%20this%20scenario%20is%20the%20so-called%20catastrophic%20forgetting%20%28CF%29%20for%20which%20the%0Alearning%20model%20tends%20to%20focus%20on%20the%20most%20recent%20tasks%20while%20experiencing%0Apredictive%20degradation%20on%20older%20ones.%20In%20the%20online%20setting%2C%20the%20most%20effective%0Asolutions%20employ%20a%20fixed-size%20memory%20buffer%20to%20store%20old%20samples%20used%20for%0Areplay%20when%20training%20on%20new%20tasks.%20Many%20approaches%20have%20been%20presented%20to%0Atackle%20this%20problem.%20However%2C%20it%20is%20not%20clear%20how%20predictive%20uncertainty%0Ainformation%20for%20memory%20management%20can%20be%20leveraged%20in%20the%20most%20effective%20manner%0Aand%20conflicting%20strategies%20are%20proposed%20to%20populate%20the%20memory.%20Are%20the%0Aeasiest-to-forget%20or%20the%20easiest-to-remember%20samples%20more%20effective%20in%0Acombating%20CF%3F%20Starting%20from%20the%20intuition%20that%20predictive%20uncertainty%20provides%0Aan%20idea%20of%20the%20samples%27%20location%20in%20the%20decision%20space%2C%20this%20work%20presents%20an%0Ain-depth%20analysis%20of%20different%20uncertainty%20estimates%20and%20strategies%20for%0Apopulating%20the%20memory.%20The%20investigation%20provides%20a%20better%20understanding%20of%20the%0Acharacteristics%20data%20points%20should%20have%20for%20alleviating%20CF.%20Then%2C%20we%20propose%20an%0Aalternative%20method%20for%20estimating%20predictive%20uncertainty%20via%20the%20generalised%0Avariance%20induced%20by%20the%20negative%20log-likelihood.%20Finally%2C%20we%20demonstrate%20that%0Athe%20use%20of%20predictive%20uncertainty%20measures%20helps%20in%20reducing%20CF%20in%20different%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Leverage%2520Predictive%2520Uncertainty%2520Estimates%2520for%2520Reducing%250A%2520%2520Catastrophic%2520Forgetting%2520in%2520Online%2520Continual%2520Learning%26entry.906535625%3DGiuseppe%2520Serra%2520and%2520Ben%2520Werner%2520and%2520Florian%2520Buettner%26entry.1292438233%3D%2520%2520Many%2520real-world%2520applications%2520require%2520machine-learning%2520models%2520to%2520be%2520able%2520to%250Adeal%2520with%2520non-stationary%2520data%2520distributions%2520and%2520thus%2520learn%2520autonomously%2520over%2520an%250Aextended%2520period%2520of%2520time%252C%2520often%2520in%2520an%2520online%2520setting.%2520One%2520of%2520the%2520main%2520challenges%250Ain%2520this%2520scenario%2520is%2520the%2520so-called%2520catastrophic%2520forgetting%2520%2528CF%2529%2520for%2520which%2520the%250Alearning%2520model%2520tends%2520to%2520focus%2520on%2520the%2520most%2520recent%2520tasks%2520while%2520experiencing%250Apredictive%2520degradation%2520on%2520older%2520ones.%2520In%2520the%2520online%2520setting%252C%2520the%2520most%2520effective%250Asolutions%2520employ%2520a%2520fixed-size%2520memory%2520buffer%2520to%2520store%2520old%2520samples%2520used%2520for%250Areplay%2520when%2520training%2520on%2520new%2520tasks.%2520Many%2520approaches%2520have%2520been%2520presented%2520to%250Atackle%2520this%2520problem.%2520However%252C%2520it%2520is%2520not%2520clear%2520how%2520predictive%2520uncertainty%250Ainformation%2520for%2520memory%2520management%2520can%2520be%2520leveraged%2520in%2520the%2520most%2520effective%2520manner%250Aand%2520conflicting%2520strategies%2520are%2520proposed%2520to%2520populate%2520the%2520memory.%2520Are%2520the%250Aeasiest-to-forget%2520or%2520the%2520easiest-to-remember%2520samples%2520more%2520effective%2520in%250Acombating%2520CF%253F%2520Starting%2520from%2520the%2520intuition%2520that%2520predictive%2520uncertainty%2520provides%250Aan%2520idea%2520of%2520the%2520samples%2527%2520location%2520in%2520the%2520decision%2520space%252C%2520this%2520work%2520presents%2520an%250Ain-depth%2520analysis%2520of%2520different%2520uncertainty%2520estimates%2520and%2520strategies%2520for%250Apopulating%2520the%2520memory.%2520The%2520investigation%2520provides%2520a%2520better%2520understanding%2520of%2520the%250Acharacteristics%2520data%2520points%2520should%2520have%2520for%2520alleviating%2520CF.%2520Then%252C%2520we%2520propose%2520an%250Aalternative%2520method%2520for%2520estimating%2520predictive%2520uncertainty%2520via%2520the%2520generalised%250Avariance%2520induced%2520by%2520the%2520negative%2520log-likelihood.%2520Finally%252C%2520we%2520demonstrate%2520that%250Athe%2520use%2520of%2520predictive%2520uncertainty%2520measures%2520helps%2520in%2520reducing%2520CF%2520in%2520different%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Leverage%20Predictive%20Uncertainty%20Estimates%20for%20Reducing%0A%20%20Catastrophic%20Forgetting%20in%20Online%20Continual%20Learning&entry.906535625=Giuseppe%20Serra%20and%20Ben%20Werner%20and%20Florian%20Buettner&entry.1292438233=%20%20Many%20real-world%20applications%20require%20machine-learning%20models%20to%20be%20able%20to%0Adeal%20with%20non-stationary%20data%20distributions%20and%20thus%20learn%20autonomously%20over%20an%0Aextended%20period%20of%20time%2C%20often%20in%20an%20online%20setting.%20One%20of%20the%20main%20challenges%0Ain%20this%20scenario%20is%20the%20so-called%20catastrophic%20forgetting%20%28CF%29%20for%20which%20the%0Alearning%20model%20tends%20to%20focus%20on%20the%20most%20recent%20tasks%20while%20experiencing%0Apredictive%20degradation%20on%20older%20ones.%20In%20the%20online%20setting%2C%20the%20most%20effective%0Asolutions%20employ%20a%20fixed-size%20memory%20buffer%20to%20store%20old%20samples%20used%20for%0Areplay%20when%20training%20on%20new%20tasks.%20Many%20approaches%20have%20been%20presented%20to%0Atackle%20this%20problem.%20However%2C%20it%20is%20not%20clear%20how%20predictive%20uncertainty%0Ainformation%20for%20memory%20management%20can%20be%20leveraged%20in%20the%20most%20effective%20manner%0Aand%20conflicting%20strategies%20are%20proposed%20to%20populate%20the%20memory.%20Are%20the%0Aeasiest-to-forget%20or%20the%20easiest-to-remember%20samples%20more%20effective%20in%0Acombating%20CF%3F%20Starting%20from%20the%20intuition%20that%20predictive%20uncertainty%20provides%0Aan%20idea%20of%20the%20samples%27%20location%20in%20the%20decision%20space%2C%20this%20work%20presents%20an%0Ain-depth%20analysis%20of%20different%20uncertainty%20estimates%20and%20strategies%20for%0Apopulating%20the%20memory.%20The%20investigation%20provides%20a%20better%20understanding%20of%20the%0Acharacteristics%20data%20points%20should%20have%20for%20alleviating%20CF.%20Then%2C%20we%20propose%20an%0Aalternative%20method%20for%20estimating%20predictive%20uncertainty%20via%20the%20generalised%0Avariance%20induced%20by%20the%20negative%20log-likelihood.%20Finally%2C%20we%20demonstrate%20that%0Athe%20use%20of%20predictive%20uncertainty%20measures%20helps%20in%20reducing%20CF%20in%20different%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07668v3&entry.124074799=Read"},
{"title": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave\n  Vehicular Networks", "author": "Ahmad M. Nazar and Abdulkadir Celik and Mohamed Y. Selim and Asmaa Abdallah and Daji Qiao and Ahmed M. Eltawil", "abstract": "  Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments.\n", "link": "http://arxiv.org/abs/2507.15769v1", "date": "2025-07-21", "relevancy": 2.1131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6041}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Sensor%20Fusion%20for%20Proactive%20Blockage%20Prediction%20in%20mmWave%0A%20%20Vehicular%20Networks&body=Title%3A%20Multi-Modal%20Sensor%20Fusion%20for%20Proactive%20Blockage%20Prediction%20in%20mmWave%0A%20%20Vehicular%20Networks%0AAuthor%3A%20Ahmad%20M.%20Nazar%20and%20Abdulkadir%20Celik%20and%20Mohamed%20Y.%20Selim%20and%20Asmaa%20Abdallah%20and%20Daji%20Qiao%20and%20Ahmed%20M.%20Eltawil%0AAbstract%3A%20%20%20Vehicular%20communication%20systems%20operating%20in%20the%20millimeter%20wave%20%28mmWave%29%0Aband%20are%20highly%20susceptible%20to%20signal%20blockage%20from%20dynamic%20obstacles%20such%20as%0Avehicles%2C%20pedestrians%2C%20and%20infrastructure.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20proactive%20blockage%20prediction%20framework%20that%20utilizes%20multi-modal%0Asensing%2C%20including%20camera%2C%20GPS%2C%20LiDAR%2C%20and%20radar%20inputs%20in%20an%0Ainfrastructure-to-vehicle%20%28I2V%29%20setting.%20This%20approach%20uses%20modality-specific%0Adeep%20learning%20models%20to%20process%20each%20sensor%20stream%20independently%20and%20fuses%0Atheir%20outputs%20using%20a%20softmax-weighted%20ensemble%20strategy%20based%20on%20validation%0Aperformance.%20Our%20evaluations%2C%20for%20up%20to%201.5s%20in%20advance%2C%20show%20that%20the%0Acamera-only%20model%20achieves%20the%20best%20standalone%20trade-off%20with%20an%20F1-score%20of%0A97.1%25%20and%20an%20inference%20time%20of%2089.8ms.%20A%20camera%2Bradar%20configuration%20further%0Aimproves%20accuracy%20to%2097.2%25%20F1%20at%2095.7ms.%20Our%20results%20display%20the%20effectiveness%0Aand%20efficiency%20of%20multi-modal%20sensing%20for%20mmWave%20blockage%20prediction%20and%0Aprovide%20a%20pathway%20for%20proactive%20wireless%20communication%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Sensor%2520Fusion%2520for%2520Proactive%2520Blockage%2520Prediction%2520in%2520mmWave%250A%2520%2520Vehicular%2520Networks%26entry.906535625%3DAhmad%2520M.%2520Nazar%2520and%2520Abdulkadir%2520Celik%2520and%2520Mohamed%2520Y.%2520Selim%2520and%2520Asmaa%2520Abdallah%2520and%2520Daji%2520Qiao%2520and%2520Ahmed%2520M.%2520Eltawil%26entry.1292438233%3D%2520%2520Vehicular%2520communication%2520systems%2520operating%2520in%2520the%2520millimeter%2520wave%2520%2528mmWave%2529%250Aband%2520are%2520highly%2520susceptible%2520to%2520signal%2520blockage%2520from%2520dynamic%2520obstacles%2520such%2520as%250Avehicles%252C%2520pedestrians%252C%2520and%2520infrastructure.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520proactive%2520blockage%2520prediction%2520framework%2520that%2520utilizes%2520multi-modal%250Asensing%252C%2520including%2520camera%252C%2520GPS%252C%2520LiDAR%252C%2520and%2520radar%2520inputs%2520in%2520an%250Ainfrastructure-to-vehicle%2520%2528I2V%2529%2520setting.%2520This%2520approach%2520uses%2520modality-specific%250Adeep%2520learning%2520models%2520to%2520process%2520each%2520sensor%2520stream%2520independently%2520and%2520fuses%250Atheir%2520outputs%2520using%2520a%2520softmax-weighted%2520ensemble%2520strategy%2520based%2520on%2520validation%250Aperformance.%2520Our%2520evaluations%252C%2520for%2520up%2520to%25201.5s%2520in%2520advance%252C%2520show%2520that%2520the%250Acamera-only%2520model%2520achieves%2520the%2520best%2520standalone%2520trade-off%2520with%2520an%2520F1-score%2520of%250A97.1%2525%2520and%2520an%2520inference%2520time%2520of%252089.8ms.%2520A%2520camera%252Bradar%2520configuration%2520further%250Aimproves%2520accuracy%2520to%252097.2%2525%2520F1%2520at%252095.7ms.%2520Our%2520results%2520display%2520the%2520effectiveness%250Aand%2520efficiency%2520of%2520multi-modal%2520sensing%2520for%2520mmWave%2520blockage%2520prediction%2520and%250Aprovide%2520a%2520pathway%2520for%2520proactive%2520wireless%2520communication%2520in%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Sensor%20Fusion%20for%20Proactive%20Blockage%20Prediction%20in%20mmWave%0A%20%20Vehicular%20Networks&entry.906535625=Ahmad%20M.%20Nazar%20and%20Abdulkadir%20Celik%20and%20Mohamed%20Y.%20Selim%20and%20Asmaa%20Abdallah%20and%20Daji%20Qiao%20and%20Ahmed%20M.%20Eltawil&entry.1292438233=%20%20Vehicular%20communication%20systems%20operating%20in%20the%20millimeter%20wave%20%28mmWave%29%0Aband%20are%20highly%20susceptible%20to%20signal%20blockage%20from%20dynamic%20obstacles%20such%20as%0Avehicles%2C%20pedestrians%2C%20and%20infrastructure.%20To%20address%20this%20challenge%2C%20we%0Apropose%20a%20proactive%20blockage%20prediction%20framework%20that%20utilizes%20multi-modal%0Asensing%2C%20including%20camera%2C%20GPS%2C%20LiDAR%2C%20and%20radar%20inputs%20in%20an%0Ainfrastructure-to-vehicle%20%28I2V%29%20setting.%20This%20approach%20uses%20modality-specific%0Adeep%20learning%20models%20to%20process%20each%20sensor%20stream%20independently%20and%20fuses%0Atheir%20outputs%20using%20a%20softmax-weighted%20ensemble%20strategy%20based%20on%20validation%0Aperformance.%20Our%20evaluations%2C%20for%20up%20to%201.5s%20in%20advance%2C%20show%20that%20the%0Acamera-only%20model%20achieves%20the%20best%20standalone%20trade-off%20with%20an%20F1-score%20of%0A97.1%25%20and%20an%20inference%20time%20of%2089.8ms.%20A%20camera%2Bradar%20configuration%20further%0Aimproves%20accuracy%20to%2097.2%25%20F1%20at%2095.7ms.%20Our%20results%20display%20the%20effectiveness%0Aand%20efficiency%20of%20multi-modal%20sensing%20for%20mmWave%20blockage%20prediction%20and%0Aprovide%20a%20pathway%20for%20proactive%20wireless%20communication%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15769v1&entry.124074799=Read"},
{"title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "author": "Syed Ahmed Mahmood and Ali Shah Ali and Umer Ahmed and Fawad Javed Fateh and M. Zeeshan Zia and Quoc-Huy Tran", "abstract": "  We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.\n", "link": "http://arxiv.org/abs/2507.15540v1", "date": "2025-07-21", "relevancy": 2.1019, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5402}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5211}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Procedure%20Learning%20via%20Regularized%20Gromov-Wasserstein%20Optimal%20Transport&body=Title%3A%20Procedure%20Learning%20via%20Regularized%20Gromov-Wasserstein%20Optimal%20Transport%0AAuthor%3A%20Syed%20Ahmed%20Mahmood%20and%20Ali%20Shah%20Ali%20and%20Umer%20Ahmed%20and%20Fawad%20Javed%20Fateh%20and%20M.%20Zeeshan%20Zia%20and%20Quoc-Huy%20Tran%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20self-supervised%20procedure%20learning%2C%20which%20discovers%0Akey%20steps%20and%20establishes%20their%20order%20from%20a%20set%20of%20unlabeled%20procedural%0Avideos.%20Previous%20procedure%20learning%20methods%20typically%20learn%20frame-to-frame%0Acorrespondences%20between%20videos%20before%20determining%20key%20steps%20and%20their%20order.%0AHowever%2C%20their%20performance%20often%20suffers%20from%20order%20variations%2C%0Abackground/redundant%20frames%2C%20and%20repeated%20actions.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20self-supervised%20procedure%20learning%20framework%2C%20which%0Autilizes%20a%20fused%20Gromov-Wasserstein%20optimal%20transport%20formulation%20with%20a%0Astructural%20prior%20for%20computing%20frame-to-frame%20mapping%20between%20videos.%20However%2C%0Aoptimizing%20exclusively%20for%20the%20above%20temporal%20alignment%20term%20may%20lead%20to%0Adegenerate%20solutions%2C%20where%20all%20frames%20are%20mapped%20to%20a%20small%20cluster%20in%20the%0Aembedding%20space%20and%20hence%20every%20video%20is%20associated%20with%20only%20one%20key%20step.%20To%0Aaddress%20that%20limitation%2C%20we%20further%20integrate%20a%20contrastive%20regularization%0Aterm%2C%20which%20maps%20different%20frames%20to%20different%20points%20in%20the%20embedding%20space%2C%0Aavoiding%20the%20collapse%20to%20trivial%20solutions.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20large-scale%20egocentric%20%28i.e.%2C%20EgoProceL%29%20and%20third-person%20%28i.e.%2C%0AProceL%20and%20CrossTask%29%20benchmarks%20to%20demonstrate%20superior%20performance%20by%20our%0Aapproach%20against%20previous%20methods%2C%20including%20OPEL%20which%20relies%20on%20a%20traditional%0AKantorovich%20optimal%20transport%20formulation%20with%20an%20optimality%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcedure%2520Learning%2520via%2520Regularized%2520Gromov-Wasserstein%2520Optimal%2520Transport%26entry.906535625%3DSyed%2520Ahmed%2520Mahmood%2520and%2520Ali%2520Shah%2520Ali%2520and%2520Umer%2520Ahmed%2520and%2520Fawad%2520Javed%2520Fateh%2520and%2520M.%2520Zeeshan%2520Zia%2520and%2520Quoc-Huy%2520Tran%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520self-supervised%2520procedure%2520learning%252C%2520which%2520discovers%250Akey%2520steps%2520and%2520establishes%2520their%2520order%2520from%2520a%2520set%2520of%2520unlabeled%2520procedural%250Avideos.%2520Previous%2520procedure%2520learning%2520methods%2520typically%2520learn%2520frame-to-frame%250Acorrespondences%2520between%2520videos%2520before%2520determining%2520key%2520steps%2520and%2520their%2520order.%250AHowever%252C%2520their%2520performance%2520often%2520suffers%2520from%2520order%2520variations%252C%250Abackground/redundant%2520frames%252C%2520and%2520repeated%2520actions.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520self-supervised%2520procedure%2520learning%2520framework%252C%2520which%250Autilizes%2520a%2520fused%2520Gromov-Wasserstein%2520optimal%2520transport%2520formulation%2520with%2520a%250Astructural%2520prior%2520for%2520computing%2520frame-to-frame%2520mapping%2520between%2520videos.%2520However%252C%250Aoptimizing%2520exclusively%2520for%2520the%2520above%2520temporal%2520alignment%2520term%2520may%2520lead%2520to%250Adegenerate%2520solutions%252C%2520where%2520all%2520frames%2520are%2520mapped%2520to%2520a%2520small%2520cluster%2520in%2520the%250Aembedding%2520space%2520and%2520hence%2520every%2520video%2520is%2520associated%2520with%2520only%2520one%2520key%2520step.%2520To%250Aaddress%2520that%2520limitation%252C%2520we%2520further%2520integrate%2520a%2520contrastive%2520regularization%250Aterm%252C%2520which%2520maps%2520different%2520frames%2520to%2520different%2520points%2520in%2520the%2520embedding%2520space%252C%250Aavoiding%2520the%2520collapse%2520to%2520trivial%2520solutions.%2520Finally%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520on%2520large-scale%2520egocentric%2520%2528i.e.%252C%2520EgoProceL%2529%2520and%2520third-person%2520%2528i.e.%252C%250AProceL%2520and%2520CrossTask%2529%2520benchmarks%2520to%2520demonstrate%2520superior%2520performance%2520by%2520our%250Aapproach%2520against%2520previous%2520methods%252C%2520including%2520OPEL%2520which%2520relies%2520on%2520a%2520traditional%250AKantorovich%2520optimal%2520transport%2520formulation%2520with%2520an%2520optimality%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Procedure%20Learning%20via%20Regularized%20Gromov-Wasserstein%20Optimal%20Transport&entry.906535625=Syed%20Ahmed%20Mahmood%20and%20Ali%20Shah%20Ali%20and%20Umer%20Ahmed%20and%20Fawad%20Javed%20Fateh%20and%20M.%20Zeeshan%20Zia%20and%20Quoc-Huy%20Tran&entry.1292438233=%20%20We%20study%20the%20problem%20of%20self-supervised%20procedure%20learning%2C%20which%20discovers%0Akey%20steps%20and%20establishes%20their%20order%20from%20a%20set%20of%20unlabeled%20procedural%0Avideos.%20Previous%20procedure%20learning%20methods%20typically%20learn%20frame-to-frame%0Acorrespondences%20between%20videos%20before%20determining%20key%20steps%20and%20their%20order.%0AHowever%2C%20their%20performance%20often%20suffers%20from%20order%20variations%2C%0Abackground/redundant%20frames%2C%20and%20repeated%20actions.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20self-supervised%20procedure%20learning%20framework%2C%20which%0Autilizes%20a%20fused%20Gromov-Wasserstein%20optimal%20transport%20formulation%20with%20a%0Astructural%20prior%20for%20computing%20frame-to-frame%20mapping%20between%20videos.%20However%2C%0Aoptimizing%20exclusively%20for%20the%20above%20temporal%20alignment%20term%20may%20lead%20to%0Adegenerate%20solutions%2C%20where%20all%20frames%20are%20mapped%20to%20a%20small%20cluster%20in%20the%0Aembedding%20space%20and%20hence%20every%20video%20is%20associated%20with%20only%20one%20key%20step.%20To%0Aaddress%20that%20limitation%2C%20we%20further%20integrate%20a%20contrastive%20regularization%0Aterm%2C%20which%20maps%20different%20frames%20to%20different%20points%20in%20the%20embedding%20space%2C%0Aavoiding%20the%20collapse%20to%20trivial%20solutions.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20large-scale%20egocentric%20%28i.e.%2C%20EgoProceL%29%20and%20third-person%20%28i.e.%2C%0AProceL%20and%20CrossTask%29%20benchmarks%20to%20demonstrate%20superior%20performance%20by%20our%0Aapproach%20against%20previous%20methods%2C%20including%20OPEL%20which%20relies%20on%20a%20traditional%0AKantorovich%20optimal%20transport%20formulation%20with%20an%20optimality%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15540v1&entry.124074799=Read"},
{"title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image\n  Segmentation", "author": "Simon Winther Albertsen and Hjalte Svaneborg Bj\u00f8rnstrup and Mostafa Mehdipour Ghazi", "abstract": "  Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.\n", "link": "http://arxiv.org/abs/2507.15524v1", "date": "2025-07-21", "relevancy": 2.1002, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5346}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5198}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RARE-UNet%3A%20Resolution-Aligned%20Routing%20Entry%20for%20Adaptive%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20RARE-UNet%3A%20Resolution-Aligned%20Routing%20Entry%20for%20Adaptive%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Simon%20Winther%20Albertsen%20and%20Hjalte%20Svaneborg%20Bj%C3%B8rnstrup%20and%20Mostafa%20Mehdipour%20Ghazi%0AAbstract%3A%20%20%20Accurate%20segmentation%20is%20crucial%20for%20clinical%20applications%2C%20but%20existing%0Amodels%20often%20assume%20fixed%2C%20high-resolution%20inputs%20and%20degrade%20significantly%0Awhen%20faced%20with%20lower-resolution%20data%20in%20real-world%20scenarios.%20To%20address%20this%0Alimitation%2C%20we%20propose%20RARE-UNet%2C%20a%20resolution-aware%20multi-scale%20segmentation%0Aarchitecture%20that%20dynamically%20adapts%20its%20inference%20path%20to%20the%20spatial%0Aresolution%20of%20the%20input.%20Central%20to%20our%20design%20are%20multi-scale%20blocks%0Aintegrated%20at%20multiple%20encoder%20depths%2C%20a%20resolution-aware%20routing%20mechanism%2C%0Aand%20consistency-driven%20training%20that%20aligns%20multi-resolution%20features%20with%0Afull-resolution%20representations.%20We%20evaluate%20RARE-UNet%20on%20two%20benchmark%20brain%0Aimaging%20tasks%20for%20hippocampus%20and%20tumor%20segmentation.%20Compared%20to%20standard%0AUNet%2C%20its%20multi-resolution%20augmented%20variant%2C%20and%20nnUNet%2C%20our%20model%20achieves%0Athe%20highest%20average%20Dice%20scores%20of%200.84%20and%200.65%20across%20resolution%2C%20while%0Amaintaining%20consistent%20performance%20and%20significantly%20reduced%20inference%20time%20at%0Alower%20resolutions.%20These%20results%20highlight%20the%20effectiveness%20and%20scalability%20of%0Aour%20architecture%20in%20achieving%20resolution-robust%20segmentation.%20The%20codes%20are%0Aavailable%20at%3A%20https%3A//github.com/simonsejse/RARE-UNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRARE-UNet%253A%2520Resolution-Aligned%2520Routing%2520Entry%2520for%2520Adaptive%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DSimon%2520Winther%2520Albertsen%2520and%2520Hjalte%2520Svaneborg%2520Bj%25C3%25B8rnstrup%2520and%2520Mostafa%2520Mehdipour%2520Ghazi%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520is%2520crucial%2520for%2520clinical%2520applications%252C%2520but%2520existing%250Amodels%2520often%2520assume%2520fixed%252C%2520high-resolution%2520inputs%2520and%2520degrade%2520significantly%250Awhen%2520faced%2520with%2520lower-resolution%2520data%2520in%2520real-world%2520scenarios.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520RARE-UNet%252C%2520a%2520resolution-aware%2520multi-scale%2520segmentation%250Aarchitecture%2520that%2520dynamically%2520adapts%2520its%2520inference%2520path%2520to%2520the%2520spatial%250Aresolution%2520of%2520the%2520input.%2520Central%2520to%2520our%2520design%2520are%2520multi-scale%2520blocks%250Aintegrated%2520at%2520multiple%2520encoder%2520depths%252C%2520a%2520resolution-aware%2520routing%2520mechanism%252C%250Aand%2520consistency-driven%2520training%2520that%2520aligns%2520multi-resolution%2520features%2520with%250Afull-resolution%2520representations.%2520We%2520evaluate%2520RARE-UNet%2520on%2520two%2520benchmark%2520brain%250Aimaging%2520tasks%2520for%2520hippocampus%2520and%2520tumor%2520segmentation.%2520Compared%2520to%2520standard%250AUNet%252C%2520its%2520multi-resolution%2520augmented%2520variant%252C%2520and%2520nnUNet%252C%2520our%2520model%2520achieves%250Athe%2520highest%2520average%2520Dice%2520scores%2520of%25200.84%2520and%25200.65%2520across%2520resolution%252C%2520while%250Amaintaining%2520consistent%2520performance%2520and%2520significantly%2520reduced%2520inference%2520time%2520at%250Alower%2520resolutions.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520and%2520scalability%2520of%250Aour%2520architecture%2520in%2520achieving%2520resolution-robust%2520segmentation.%2520The%2520codes%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/simonsejse/RARE-UNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RARE-UNet%3A%20Resolution-Aligned%20Routing%20Entry%20for%20Adaptive%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Simon%20Winther%20Albertsen%20and%20Hjalte%20Svaneborg%20Bj%C3%B8rnstrup%20and%20Mostafa%20Mehdipour%20Ghazi&entry.1292438233=%20%20Accurate%20segmentation%20is%20crucial%20for%20clinical%20applications%2C%20but%20existing%0Amodels%20often%20assume%20fixed%2C%20high-resolution%20inputs%20and%20degrade%20significantly%0Awhen%20faced%20with%20lower-resolution%20data%20in%20real-world%20scenarios.%20To%20address%20this%0Alimitation%2C%20we%20propose%20RARE-UNet%2C%20a%20resolution-aware%20multi-scale%20segmentation%0Aarchitecture%20that%20dynamically%20adapts%20its%20inference%20path%20to%20the%20spatial%0Aresolution%20of%20the%20input.%20Central%20to%20our%20design%20are%20multi-scale%20blocks%0Aintegrated%20at%20multiple%20encoder%20depths%2C%20a%20resolution-aware%20routing%20mechanism%2C%0Aand%20consistency-driven%20training%20that%20aligns%20multi-resolution%20features%20with%0Afull-resolution%20representations.%20We%20evaluate%20RARE-UNet%20on%20two%20benchmark%20brain%0Aimaging%20tasks%20for%20hippocampus%20and%20tumor%20segmentation.%20Compared%20to%20standard%0AUNet%2C%20its%20multi-resolution%20augmented%20variant%2C%20and%20nnUNet%2C%20our%20model%20achieves%0Athe%20highest%20average%20Dice%20scores%20of%200.84%20and%200.65%20across%20resolution%2C%20while%0Amaintaining%20consistent%20performance%20and%20significantly%20reduced%20inference%20time%20at%0Alower%20resolutions.%20These%20results%20highlight%20the%20effectiveness%20and%20scalability%20of%0Aour%20architecture%20in%20achieving%20resolution-robust%20segmentation.%20The%20codes%20are%0Aavailable%20at%3A%20https%3A//github.com/simonsejse/RARE-UNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15524v1&entry.124074799=Read"},
{"title": "Efficient Face Image Quality Assessment via Self-training and Knowledge\n  Distillation", "author": "Wei Sun and Weixia Zhang and Linhan Cao and Jun Jia and Xiangyang Zhu and Dandan Zhu and Xiongkuo Min and Guangtao Zhai", "abstract": "  Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.\n", "link": "http://arxiv.org/abs/2507.15709v1", "date": "2025-07-21", "relevancy": 2.0961, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5316}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5218}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Face%20Image%20Quality%20Assessment%20via%20Self-training%20and%20Knowledge%0A%20%20Distillation&body=Title%3A%20Efficient%20Face%20Image%20Quality%20Assessment%20via%20Self-training%20and%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Wei%20Sun%20and%20Weixia%20Zhang%20and%20Linhan%20Cao%20and%20Jun%20Jia%20and%20Xiangyang%20Zhu%20and%20Dandan%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Face%20image%20quality%20assessment%20%28FIQA%29%20is%20essential%20for%20various%20face-related%0Aapplications.%20Although%20FIQA%20has%20been%20extensively%20studied%20and%20achieved%0Asignificant%20progress%2C%20the%20computational%20complexity%20of%20FIQA%20algorithms%20remains%20a%0Akey%20concern%20for%20ensuring%20scalability%20and%20practical%20deployment%20in%20real-world%0Asystems.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20computationally%20efficient%20FIQA%0Amethod%20that%20can%20be%20easily%20deployed%20in%20real-world%20applications.%20Specifically%2C%0Aour%20method%20consists%20of%20two%20stages%3A%20training%20a%20powerful%20teacher%20model%20and%0Adistilling%20a%20lightweight%20student%20model%20from%20it.%20To%20build%20a%20strong%20teacher%0Amodel%2C%20we%20adopt%20a%20self-training%20strategy%20to%20improve%20its%20capacity.%20We%20first%0Atrain%20the%20teacher%20model%20using%20labeled%20face%20images%2C%20then%20use%20it%20to%20generate%0Apseudo-labels%20for%20a%20set%20of%20unlabeled%20images.%20These%20pseudo-labeled%20samples%20are%0Aused%20in%20two%20ways%3A%20%281%29%20to%20distill%20knowledge%20into%20the%20student%20model%2C%20and%20%282%29%20to%0Acombine%20with%20the%20original%20labeled%20images%20to%20further%20enhance%20the%20teacher%20model%0Athrough%20self-training.%20The%20enhanced%20teacher%20model%20is%20used%20to%20further%0Apseudo-label%20another%20set%20of%20unlabeled%20images%20for%20distilling%20the%20student%20models.%0AThe%20student%20model%20is%20trained%20using%20a%20combination%20of%20labeled%20images%2C%0Apseudo-labeled%20images%20from%20the%20original%20teacher%20model%2C%20and%20pseudo-labeled%0Aimages%20from%20the%20enhanced%20teacher%20model.%20Experimental%20results%20demonstrate%20that%0Aour%20student%20model%20achieves%20comparable%20performance%20to%20the%20teacher%20model%20with%20an%0Aextremely%20low%20computational%20overhead.%20Moreover%2C%20our%20method%20achieved%20first%20place%0Ain%20the%20ICCV%202025%20VQualA%20FIQA%20Challenge.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunwei925/Efficient-FIQA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Face%2520Image%2520Quality%2520Assessment%2520via%2520Self-training%2520and%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DWei%2520Sun%2520and%2520Weixia%2520Zhang%2520and%2520Linhan%2520Cao%2520and%2520Jun%2520Jia%2520and%2520Xiangyang%2520Zhu%2520and%2520Dandan%2520Zhu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Face%2520image%2520quality%2520assessment%2520%2528FIQA%2529%2520is%2520essential%2520for%2520various%2520face-related%250Aapplications.%2520Although%2520FIQA%2520has%2520been%2520extensively%2520studied%2520and%2520achieved%250Asignificant%2520progress%252C%2520the%2520computational%2520complexity%2520of%2520FIQA%2520algorithms%2520remains%2520a%250Akey%2520concern%2520for%2520ensuring%2520scalability%2520and%2520practical%2520deployment%2520in%2520real-world%250Asystems.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520develop%2520a%2520computationally%2520efficient%2520FIQA%250Amethod%2520that%2520can%2520be%2520easily%2520deployed%2520in%2520real-world%2520applications.%2520Specifically%252C%250Aour%2520method%2520consists%2520of%2520two%2520stages%253A%2520training%2520a%2520powerful%2520teacher%2520model%2520and%250Adistilling%2520a%2520lightweight%2520student%2520model%2520from%2520it.%2520To%2520build%2520a%2520strong%2520teacher%250Amodel%252C%2520we%2520adopt%2520a%2520self-training%2520strategy%2520to%2520improve%2520its%2520capacity.%2520We%2520first%250Atrain%2520the%2520teacher%2520model%2520using%2520labeled%2520face%2520images%252C%2520then%2520use%2520it%2520to%2520generate%250Apseudo-labels%2520for%2520a%2520set%2520of%2520unlabeled%2520images.%2520These%2520pseudo-labeled%2520samples%2520are%250Aused%2520in%2520two%2520ways%253A%2520%25281%2529%2520to%2520distill%2520knowledge%2520into%2520the%2520student%2520model%252C%2520and%2520%25282%2529%2520to%250Acombine%2520with%2520the%2520original%2520labeled%2520images%2520to%2520further%2520enhance%2520the%2520teacher%2520model%250Athrough%2520self-training.%2520The%2520enhanced%2520teacher%2520model%2520is%2520used%2520to%2520further%250Apseudo-label%2520another%2520set%2520of%2520unlabeled%2520images%2520for%2520distilling%2520the%2520student%2520models.%250AThe%2520student%2520model%2520is%2520trained%2520using%2520a%2520combination%2520of%2520labeled%2520images%252C%250Apseudo-labeled%2520images%2520from%2520the%2520original%2520teacher%2520model%252C%2520and%2520pseudo-labeled%250Aimages%2520from%2520the%2520enhanced%2520teacher%2520model.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520student%2520model%2520achieves%2520comparable%2520performance%2520to%2520the%2520teacher%2520model%2520with%2520an%250Aextremely%2520low%2520computational%2520overhead.%2520Moreover%252C%2520our%2520method%2520achieved%2520first%2520place%250Ain%2520the%2520ICCV%25202025%2520VQualA%2520FIQA%2520Challenge.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sunwei925/Efficient-FIQA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Face%20Image%20Quality%20Assessment%20via%20Self-training%20and%20Knowledge%0A%20%20Distillation&entry.906535625=Wei%20Sun%20and%20Weixia%20Zhang%20and%20Linhan%20Cao%20and%20Jun%20Jia%20and%20Xiangyang%20Zhu%20and%20Dandan%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Face%20image%20quality%20assessment%20%28FIQA%29%20is%20essential%20for%20various%20face-related%0Aapplications.%20Although%20FIQA%20has%20been%20extensively%20studied%20and%20achieved%0Asignificant%20progress%2C%20the%20computational%20complexity%20of%20FIQA%20algorithms%20remains%20a%0Akey%20concern%20for%20ensuring%20scalability%20and%20practical%20deployment%20in%20real-world%0Asystems.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20computationally%20efficient%20FIQA%0Amethod%20that%20can%20be%20easily%20deployed%20in%20real-world%20applications.%20Specifically%2C%0Aour%20method%20consists%20of%20two%20stages%3A%20training%20a%20powerful%20teacher%20model%20and%0Adistilling%20a%20lightweight%20student%20model%20from%20it.%20To%20build%20a%20strong%20teacher%0Amodel%2C%20we%20adopt%20a%20self-training%20strategy%20to%20improve%20its%20capacity.%20We%20first%0Atrain%20the%20teacher%20model%20using%20labeled%20face%20images%2C%20then%20use%20it%20to%20generate%0Apseudo-labels%20for%20a%20set%20of%20unlabeled%20images.%20These%20pseudo-labeled%20samples%20are%0Aused%20in%20two%20ways%3A%20%281%29%20to%20distill%20knowledge%20into%20the%20student%20model%2C%20and%20%282%29%20to%0Acombine%20with%20the%20original%20labeled%20images%20to%20further%20enhance%20the%20teacher%20model%0Athrough%20self-training.%20The%20enhanced%20teacher%20model%20is%20used%20to%20further%0Apseudo-label%20another%20set%20of%20unlabeled%20images%20for%20distilling%20the%20student%20models.%0AThe%20student%20model%20is%20trained%20using%20a%20combination%20of%20labeled%20images%2C%0Apseudo-labeled%20images%20from%20the%20original%20teacher%20model%2C%20and%20pseudo-labeled%0Aimages%20from%20the%20enhanced%20teacher%20model.%20Experimental%20results%20demonstrate%20that%0Aour%20student%20model%20achieves%20comparable%20performance%20to%20the%20teacher%20model%20with%20an%0Aextremely%20low%20computational%20overhead.%20Moreover%2C%20our%20method%20achieved%20first%20place%0Ain%20the%20ICCV%202025%20VQualA%20FIQA%20Challenge.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunwei925/Efficient-FIQA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15709v1&entry.124074799=Read"},
{"title": "BGM: Background Mixup for X-ray Prohibited Items Detection", "author": "Weizhe Liu and Renshuai Tao and Hongguang Zhu and Yunda Sun and Yao Zhao and Yunchao Wei", "abstract": "  Current data-driven approaches for X-ray prohibited items detection remain\nunder-explored, particularly in the design of effective data augmentations.\nExisting natural image augmentations for reflected light imaging neglect the\ndata characteristics of X-ray security images. Moreover, prior X-ray\naugmentation methods have predominantly focused on foreground prohibited items,\noverlooking informative background cues. In this paper, we propose Background\nMixup (BGM), a background-based augmentation technique tailored for X-ray\nsecurity imaging domain. Unlike conventional methods, BGM is founded on an\nin-depth analysis of physical properties including: 1) X-ray Transmission\nImagery: Transmitted X-ray pixels represent composite information from multiple\nmaterials along the imaging path. 2) Material-based Pseudo-coloring:\nPseudo-coloring in X-ray images correlates directly with material properties,\naiding in material distinction. Building upon the above insights, BGM mixes\nbackground patches across regions on both 1) texture structure and 2) material\nvariation, to benefit models from complicated background cues. This enhances\nthe model's capability to handle domain-specific challenges such as\nocclusion-induced discriminative imbalance. Importantly, BGM is orthogonal and\nfully compatible with existing foreground-focused augmentation techniques,\nenabling joint use to further enhance detection performance. Extensive\nexperiments on multiple X-ray security benchmarks show that BGM consistently\nsurpasses strong baselines, without additional annotations or significant\ntraining overhead. This work pioneers the exploration of background-aware\naugmentation in X-ray prohibited items detection and provides a lightweight,\nplug-and-play solution with broad applicability.\n", "link": "http://arxiv.org/abs/2412.00460v2", "date": "2025-07-21", "relevancy": 2.0954, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5541}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5212}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BGM%3A%20Background%20Mixup%20for%20X-ray%20Prohibited%20Items%20Detection&body=Title%3A%20BGM%3A%20Background%20Mixup%20for%20X-ray%20Prohibited%20Items%20Detection%0AAuthor%3A%20Weizhe%20Liu%20and%20Renshuai%20Tao%20and%20Hongguang%20Zhu%20and%20Yunda%20Sun%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Current%20data-driven%20approaches%20for%20X-ray%20prohibited%20items%20detection%20remain%0Aunder-explored%2C%20particularly%20in%20the%20design%20of%20effective%20data%20augmentations.%0AExisting%20natural%20image%20augmentations%20for%20reflected%20light%20imaging%20neglect%20the%0Adata%20characteristics%20of%20X-ray%20security%20images.%20Moreover%2C%20prior%20X-ray%0Aaugmentation%20methods%20have%20predominantly%20focused%20on%20foreground%20prohibited%20items%2C%0Aoverlooking%20informative%20background%20cues.%20In%20this%20paper%2C%20we%20propose%20Background%0AMixup%20%28BGM%29%2C%20a%20background-based%20augmentation%20technique%20tailored%20for%20X-ray%0Asecurity%20imaging%20domain.%20Unlike%20conventional%20methods%2C%20BGM%20is%20founded%20on%20an%0Ain-depth%20analysis%20of%20physical%20properties%20including%3A%201%29%20X-ray%20Transmission%0AImagery%3A%20Transmitted%20X-ray%20pixels%20represent%20composite%20information%20from%20multiple%0Amaterials%20along%20the%20imaging%20path.%202%29%20Material-based%20Pseudo-coloring%3A%0APseudo-coloring%20in%20X-ray%20images%20correlates%20directly%20with%20material%20properties%2C%0Aaiding%20in%20material%20distinction.%20Building%20upon%20the%20above%20insights%2C%20BGM%20mixes%0Abackground%20patches%20across%20regions%20on%20both%201%29%20texture%20structure%20and%202%29%20material%0Avariation%2C%20to%20benefit%20models%20from%20complicated%20background%20cues.%20This%20enhances%0Athe%20model%27s%20capability%20to%20handle%20domain-specific%20challenges%20such%20as%0Aocclusion-induced%20discriminative%20imbalance.%20Importantly%2C%20BGM%20is%20orthogonal%20and%0Afully%20compatible%20with%20existing%20foreground-focused%20augmentation%20techniques%2C%0Aenabling%20joint%20use%20to%20further%20enhance%20detection%20performance.%20Extensive%0Aexperiments%20on%20multiple%20X-ray%20security%20benchmarks%20show%20that%20BGM%20consistently%0Asurpasses%20strong%20baselines%2C%20without%20additional%20annotations%20or%20significant%0Atraining%20overhead.%20This%20work%20pioneers%20the%20exploration%20of%20background-aware%0Aaugmentation%20in%20X-ray%20prohibited%20items%20detection%20and%20provides%20a%20lightweight%2C%0Aplug-and-play%20solution%20with%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00460v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBGM%253A%2520Background%2520Mixup%2520for%2520X-ray%2520Prohibited%2520Items%2520Detection%26entry.906535625%3DWeizhe%2520Liu%2520and%2520Renshuai%2520Tao%2520and%2520Hongguang%2520Zhu%2520and%2520Yunda%2520Sun%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Current%2520data-driven%2520approaches%2520for%2520X-ray%2520prohibited%2520items%2520detection%2520remain%250Aunder-explored%252C%2520particularly%2520in%2520the%2520design%2520of%2520effective%2520data%2520augmentations.%250AExisting%2520natural%2520image%2520augmentations%2520for%2520reflected%2520light%2520imaging%2520neglect%2520the%250Adata%2520characteristics%2520of%2520X-ray%2520security%2520images.%2520Moreover%252C%2520prior%2520X-ray%250Aaugmentation%2520methods%2520have%2520predominantly%2520focused%2520on%2520foreground%2520prohibited%2520items%252C%250Aoverlooking%2520informative%2520background%2520cues.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Background%250AMixup%2520%2528BGM%2529%252C%2520a%2520background-based%2520augmentation%2520technique%2520tailored%2520for%2520X-ray%250Asecurity%2520imaging%2520domain.%2520Unlike%2520conventional%2520methods%252C%2520BGM%2520is%2520founded%2520on%2520an%250Ain-depth%2520analysis%2520of%2520physical%2520properties%2520including%253A%25201%2529%2520X-ray%2520Transmission%250AImagery%253A%2520Transmitted%2520X-ray%2520pixels%2520represent%2520composite%2520information%2520from%2520multiple%250Amaterials%2520along%2520the%2520imaging%2520path.%25202%2529%2520Material-based%2520Pseudo-coloring%253A%250APseudo-coloring%2520in%2520X-ray%2520images%2520correlates%2520directly%2520with%2520material%2520properties%252C%250Aaiding%2520in%2520material%2520distinction.%2520Building%2520upon%2520the%2520above%2520insights%252C%2520BGM%2520mixes%250Abackground%2520patches%2520across%2520regions%2520on%2520both%25201%2529%2520texture%2520structure%2520and%25202%2529%2520material%250Avariation%252C%2520to%2520benefit%2520models%2520from%2520complicated%2520background%2520cues.%2520This%2520enhances%250Athe%2520model%2527s%2520capability%2520to%2520handle%2520domain-specific%2520challenges%2520such%2520as%250Aocclusion-induced%2520discriminative%2520imbalance.%2520Importantly%252C%2520BGM%2520is%2520orthogonal%2520and%250Afully%2520compatible%2520with%2520existing%2520foreground-focused%2520augmentation%2520techniques%252C%250Aenabling%2520joint%2520use%2520to%2520further%2520enhance%2520detection%2520performance.%2520Extensive%250Aexperiments%2520on%2520multiple%2520X-ray%2520security%2520benchmarks%2520show%2520that%2520BGM%2520consistently%250Asurpasses%2520strong%2520baselines%252C%2520without%2520additional%2520annotations%2520or%2520significant%250Atraining%2520overhead.%2520This%2520work%2520pioneers%2520the%2520exploration%2520of%2520background-aware%250Aaugmentation%2520in%2520X-ray%2520prohibited%2520items%2520detection%2520and%2520provides%2520a%2520lightweight%252C%250Aplug-and-play%2520solution%2520with%2520broad%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00460v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BGM%3A%20Background%20Mixup%20for%20X-ray%20Prohibited%20Items%20Detection&entry.906535625=Weizhe%20Liu%20and%20Renshuai%20Tao%20and%20Hongguang%20Zhu%20and%20Yunda%20Sun%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20Current%20data-driven%20approaches%20for%20X-ray%20prohibited%20items%20detection%20remain%0Aunder-explored%2C%20particularly%20in%20the%20design%20of%20effective%20data%20augmentations.%0AExisting%20natural%20image%20augmentations%20for%20reflected%20light%20imaging%20neglect%20the%0Adata%20characteristics%20of%20X-ray%20security%20images.%20Moreover%2C%20prior%20X-ray%0Aaugmentation%20methods%20have%20predominantly%20focused%20on%20foreground%20prohibited%20items%2C%0Aoverlooking%20informative%20background%20cues.%20In%20this%20paper%2C%20we%20propose%20Background%0AMixup%20%28BGM%29%2C%20a%20background-based%20augmentation%20technique%20tailored%20for%20X-ray%0Asecurity%20imaging%20domain.%20Unlike%20conventional%20methods%2C%20BGM%20is%20founded%20on%20an%0Ain-depth%20analysis%20of%20physical%20properties%20including%3A%201%29%20X-ray%20Transmission%0AImagery%3A%20Transmitted%20X-ray%20pixels%20represent%20composite%20information%20from%20multiple%0Amaterials%20along%20the%20imaging%20path.%202%29%20Material-based%20Pseudo-coloring%3A%0APseudo-coloring%20in%20X-ray%20images%20correlates%20directly%20with%20material%20properties%2C%0Aaiding%20in%20material%20distinction.%20Building%20upon%20the%20above%20insights%2C%20BGM%20mixes%0Abackground%20patches%20across%20regions%20on%20both%201%29%20texture%20structure%20and%202%29%20material%0Avariation%2C%20to%20benefit%20models%20from%20complicated%20background%20cues.%20This%20enhances%0Athe%20model%27s%20capability%20to%20handle%20domain-specific%20challenges%20such%20as%0Aocclusion-induced%20discriminative%20imbalance.%20Importantly%2C%20BGM%20is%20orthogonal%20and%0Afully%20compatible%20with%20existing%20foreground-focused%20augmentation%20techniques%2C%0Aenabling%20joint%20use%20to%20further%20enhance%20detection%20performance.%20Extensive%0Aexperiments%20on%20multiple%20X-ray%20security%20benchmarks%20show%20that%20BGM%20consistently%0Asurpasses%20strong%20baselines%2C%20without%20additional%20annotations%20or%20significant%0Atraining%20overhead.%20This%20work%20pioneers%20the%20exploration%20of%20background-aware%0Aaugmentation%20in%20X-ray%20prohibited%20items%20detection%20and%20provides%20a%20lightweight%2C%0Aplug-and-play%20solution%20with%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00460v2&entry.124074799=Read"},
{"title": "Predictive Planner for Autonomous Driving with Consistency Models", "author": "Anjian Li and Sangjae Bae and David Isele and Ryne Beeson and Faizan M. Tariq", "abstract": "  Trajectory prediction and planning are essential for autonomous vehicles to\nnavigate safely and efficiently in dynamic environments. Traditional approaches\noften treat them separately, limiting the ability for interactive planning.\nWhile recent diffusion-based generative models have shown promise in\nmulti-agent trajectory generation, their slow sampling is less suitable for\nhigh-frequency planning tasks. In this paper, we leverage the consistency model\nto build a predictive planner that samples from a joint distribution of ego and\nsurrounding agents, conditioned on the ego vehicle's navigational goal. Trained\non real-world human driving datasets, our consistency model generates\nhigher-quality trajectories with fewer sampling steps than standard diffusion\nmodels, making it more suitable for real-time deployment. To enforce multiple\nplanning constraints simultaneously on the ego trajectory, a novel online\nguided sampling approach inspired by the Alternating Direction Method of\nMultipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset\n(WOMD), our method enables proactive behavior such as nudging and yielding, and\nalso demonstrates smoother, safer, and more efficient trajectories and\nsatisfaction of multiple constraints under a limited computational budget.\n", "link": "http://arxiv.org/abs/2502.08033v3", "date": "2025-07-21", "relevancy": 2.0929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5197}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Planner%20for%20Autonomous%20Driving%20with%20Consistency%20Models&body=Title%3A%20Predictive%20Planner%20for%20Autonomous%20Driving%20with%20Consistency%20Models%0AAuthor%3A%20Anjian%20Li%20and%20Sangjae%20Bae%20and%20David%20Isele%20and%20Ryne%20Beeson%20and%20Faizan%20M.%20Tariq%0AAbstract%3A%20%20%20Trajectory%20prediction%20and%20planning%20are%20essential%20for%20autonomous%20vehicles%20to%0Anavigate%20safely%20and%20efficiently%20in%20dynamic%20environments.%20Traditional%20approaches%0Aoften%20treat%20them%20separately%2C%20limiting%20the%20ability%20for%20interactive%20planning.%0AWhile%20recent%20diffusion-based%20generative%20models%20have%20shown%20promise%20in%0Amulti-agent%20trajectory%20generation%2C%20their%20slow%20sampling%20is%20less%20suitable%20for%0Ahigh-frequency%20planning%20tasks.%20In%20this%20paper%2C%20we%20leverage%20the%20consistency%20model%0Ato%20build%20a%20predictive%20planner%20that%20samples%20from%20a%20joint%20distribution%20of%20ego%20and%0Asurrounding%20agents%2C%20conditioned%20on%20the%20ego%20vehicle%27s%20navigational%20goal.%20Trained%0Aon%20real-world%20human%20driving%20datasets%2C%20our%20consistency%20model%20generates%0Ahigher-quality%20trajectories%20with%20fewer%20sampling%20steps%20than%20standard%20diffusion%0Amodels%2C%20making%20it%20more%20suitable%20for%20real-time%20deployment.%20To%20enforce%20multiple%0Aplanning%20constraints%20simultaneously%20on%20the%20ego%20trajectory%2C%20a%20novel%20online%0Aguided%20sampling%20approach%20inspired%20by%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20is%20introduced.%20Evaluated%20on%20the%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20our%20method%20enables%20proactive%20behavior%20such%20as%20nudging%20and%20yielding%2C%20and%0Aalso%20demonstrates%20smoother%2C%20safer%2C%20and%20more%20efficient%20trajectories%20and%0Asatisfaction%20of%20multiple%20constraints%20under%20a%20limited%20computational%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08033v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Planner%2520for%2520Autonomous%2520Driving%2520with%2520Consistency%2520Models%26entry.906535625%3DAnjian%2520Li%2520and%2520Sangjae%2520Bae%2520and%2520David%2520Isele%2520and%2520Ryne%2520Beeson%2520and%2520Faizan%2520M.%2520Tariq%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520and%2520planning%2520are%2520essential%2520for%2520autonomous%2520vehicles%2520to%250Anavigate%2520safely%2520and%2520efficiently%2520in%2520dynamic%2520environments.%2520Traditional%2520approaches%250Aoften%2520treat%2520them%2520separately%252C%2520limiting%2520the%2520ability%2520for%2520interactive%2520planning.%250AWhile%2520recent%2520diffusion-based%2520generative%2520models%2520have%2520shown%2520promise%2520in%250Amulti-agent%2520trajectory%2520generation%252C%2520their%2520slow%2520sampling%2520is%2520less%2520suitable%2520for%250Ahigh-frequency%2520planning%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520the%2520consistency%2520model%250Ato%2520build%2520a%2520predictive%2520planner%2520that%2520samples%2520from%2520a%2520joint%2520distribution%2520of%2520ego%2520and%250Asurrounding%2520agents%252C%2520conditioned%2520on%2520the%2520ego%2520vehicle%2527s%2520navigational%2520goal.%2520Trained%250Aon%2520real-world%2520human%2520driving%2520datasets%252C%2520our%2520consistency%2520model%2520generates%250Ahigher-quality%2520trajectories%2520with%2520fewer%2520sampling%2520steps%2520than%2520standard%2520diffusion%250Amodels%252C%2520making%2520it%2520more%2520suitable%2520for%2520real-time%2520deployment.%2520To%2520enforce%2520multiple%250Aplanning%2520constraints%2520simultaneously%2520on%2520the%2520ego%2520trajectory%252C%2520a%2520novel%2520online%250Aguided%2520sampling%2520approach%2520inspired%2520by%2520the%2520Alternating%2520Direction%2520Method%2520of%250AMultipliers%2520%2528ADMM%2529%2520is%2520introduced.%2520Evaluated%2520on%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%250A%2528WOMD%2529%252C%2520our%2520method%2520enables%2520proactive%2520behavior%2520such%2520as%2520nudging%2520and%2520yielding%252C%2520and%250Aalso%2520demonstrates%2520smoother%252C%2520safer%252C%2520and%2520more%2520efficient%2520trajectories%2520and%250Asatisfaction%2520of%2520multiple%2520constraints%2520under%2520a%2520limited%2520computational%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08033v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Planner%20for%20Autonomous%20Driving%20with%20Consistency%20Models&entry.906535625=Anjian%20Li%20and%20Sangjae%20Bae%20and%20David%20Isele%20and%20Ryne%20Beeson%20and%20Faizan%20M.%20Tariq&entry.1292438233=%20%20Trajectory%20prediction%20and%20planning%20are%20essential%20for%20autonomous%20vehicles%20to%0Anavigate%20safely%20and%20efficiently%20in%20dynamic%20environments.%20Traditional%20approaches%0Aoften%20treat%20them%20separately%2C%20limiting%20the%20ability%20for%20interactive%20planning.%0AWhile%20recent%20diffusion-based%20generative%20models%20have%20shown%20promise%20in%0Amulti-agent%20trajectory%20generation%2C%20their%20slow%20sampling%20is%20less%20suitable%20for%0Ahigh-frequency%20planning%20tasks.%20In%20this%20paper%2C%20we%20leverage%20the%20consistency%20model%0Ato%20build%20a%20predictive%20planner%20that%20samples%20from%20a%20joint%20distribution%20of%20ego%20and%0Asurrounding%20agents%2C%20conditioned%20on%20the%20ego%20vehicle%27s%20navigational%20goal.%20Trained%0Aon%20real-world%20human%20driving%20datasets%2C%20our%20consistency%20model%20generates%0Ahigher-quality%20trajectories%20with%20fewer%20sampling%20steps%20than%20standard%20diffusion%0Amodels%2C%20making%20it%20more%20suitable%20for%20real-time%20deployment.%20To%20enforce%20multiple%0Aplanning%20constraints%20simultaneously%20on%20the%20ego%20trajectory%2C%20a%20novel%20online%0Aguided%20sampling%20approach%20inspired%20by%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20is%20introduced.%20Evaluated%20on%20the%20Waymo%20Open%20Motion%20Dataset%0A%28WOMD%29%2C%20our%20method%20enables%20proactive%20behavior%20such%20as%20nudging%20and%20yielding%2C%20and%0Aalso%20demonstrates%20smoother%2C%20safer%2C%20and%20more%20efficient%20trajectories%20and%0Asatisfaction%20of%20multiple%20constraints%20under%20a%20limited%20computational%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08033v3&entry.124074799=Read"},
{"title": "The added value for MRI radiomics and deep-learning for glioblastoma\n  prognostication compared to clinical and molecular information", "author": "D. Abler and O. Pusterla and A. Joye-K\u00fchnis and N. Andratschke and M. Bach and A. Bink and S. M. Christ and P. Hagmann and B. Pouymayou and E. Pravat\u00e0 and P. Radojewski and M. Reyes and L. Ruinelli and R. Schaer and B. Stieltjes and G. Treglia and W. Valenzuela and R. Wiest and S. Zoergiebel and M. Guckenberger and S. Tanadini-Lang and A. Depeursinge", "abstract": "  Background: Radiomics shows promise in characterizing glioblastoma, but its\nadded value over clinical and molecular predictors has yet to be proven. This\nstudy assessed the added value of conventional radiomics (CR) and deep learning\n(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on\na large multi-center dataset.\n  Methods: After patient selection, our curated dataset gathers 1152\nglioblastoma (WHO 2016) patients from five Swiss centers and one public source.\nIt included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI\ndata (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were\ndeveloped using standard methods and evaluated on internal and external\ncohorts. Sub-analyses assessed models with different feature sets\n(imaging-only, clinical/molecular-only, combined-features) and patient subsets\n(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).\n  Results: The best performance was observed in the full cohort (S-1). In\nexternal validation, the combined-feature CR model achieved an AUC of 0.75,\nslightly, but significantly outperforming clinical-only (0.74) and imaging-only\n(0.68) models. DL models showed similar trends, though without statistical\nsignificance. In S-2 and S-3, combined models did not outperform clinical-only\nmodels. Exploratory analysis of CR models for overall survival prediction\nsuggested greater relevance of imaging data: across all subsets,\ncombined-feature models significantly outperformed clinical-only models, though\nwith a modest advantage of 2-4 C-index points.\n  Conclusions: While confirming the predictive value of anatomical MRI\nsequences for glioblastoma prognosis, this multi-center study found standard CR\nand DL radiomics approaches offer minimal added value over demographic\npredictors such as age and gender.\n", "link": "http://arxiv.org/abs/2507.15548v1", "date": "2025-07-21", "relevancy": 2.0929, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4335}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4335}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20added%20value%20for%20MRI%20radiomics%20and%20deep-learning%20for%20glioblastoma%0A%20%20prognostication%20compared%20to%20clinical%20and%20molecular%20information&body=Title%3A%20The%20added%20value%20for%20MRI%20radiomics%20and%20deep-learning%20for%20glioblastoma%0A%20%20prognostication%20compared%20to%20clinical%20and%20molecular%20information%0AAuthor%3A%20D.%20Abler%20and%20O.%20Pusterla%20and%20A.%20Joye-K%C3%BChnis%20and%20N.%20Andratschke%20and%20M.%20Bach%20and%20A.%20Bink%20and%20S.%20M.%20Christ%20and%20P.%20Hagmann%20and%20B.%20Pouymayou%20and%20E.%20Pravat%C3%A0%20and%20P.%20Radojewski%20and%20M.%20Reyes%20and%20L.%20Ruinelli%20and%20R.%20Schaer%20and%20B.%20Stieltjes%20and%20G.%20Treglia%20and%20W.%20Valenzuela%20and%20R.%20Wiest%20and%20S.%20Zoergiebel%20and%20M.%20Guckenberger%20and%20S.%20Tanadini-Lang%20and%20A.%20Depeursinge%0AAbstract%3A%20%20%20Background%3A%20Radiomics%20shows%20promise%20in%20characterizing%20glioblastoma%2C%20but%20its%0Aadded%20value%20over%20clinical%20and%20molecular%20predictors%20has%20yet%20to%20be%20proven.%20This%0Astudy%20assessed%20the%20added%20value%20of%20conventional%20radiomics%20%28CR%29%20and%20deep%20learning%0A%28DL%29%20MRI%20radiomics%20for%20glioblastoma%20prognosis%20%28%3C%3D%206%20vs%20%3E%206%20months%20survival%29%20on%0Aa%20large%20multi-center%20dataset.%0A%20%20Methods%3A%20After%20patient%20selection%2C%20our%20curated%20dataset%20gathers%201152%0Aglioblastoma%20%28WHO%202016%29%20patients%20from%20five%20Swiss%20centers%20and%20one%20public%20source.%0AIt%20included%20clinical%20%28age%2C%20gender%29%2C%20molecular%20%28MGMT%2C%20IDH%29%2C%20and%20baseline%20MRI%0Adata%20%28T1%2C%20T1%20contrast%2C%20FLAIR%2C%20T2%29%20with%20tumor%20regions.%20CR%20and%20DL%20models%20were%0Adeveloped%20using%20standard%20methods%20and%20evaluated%20on%20internal%20and%20external%0Acohorts.%20Sub-analyses%20assessed%20models%20with%20different%20feature%20sets%0A%28imaging-only%2C%20clinical/molecular-only%2C%20combined-features%29%20and%20patient%20subsets%0A%28S-1%3A%20all%20patients%2C%20S-2%3A%20with%20molecular%20data%2C%20S-3%3A%20IDH%20wildtype%29.%0A%20%20Results%3A%20The%20best%20performance%20was%20observed%20in%20the%20full%20cohort%20%28S-1%29.%20In%0Aexternal%20validation%2C%20the%20combined-feature%20CR%20model%20achieved%20an%20AUC%20of%200.75%2C%0Aslightly%2C%20but%20significantly%20outperforming%20clinical-only%20%280.74%29%20and%20imaging-only%0A%280.68%29%20models.%20DL%20models%20showed%20similar%20trends%2C%20though%20without%20statistical%0Asignificance.%20In%20S-2%20and%20S-3%2C%20combined%20models%20did%20not%20outperform%20clinical-only%0Amodels.%20Exploratory%20analysis%20of%20CR%20models%20for%20overall%20survival%20prediction%0Asuggested%20greater%20relevance%20of%20imaging%20data%3A%20across%20all%20subsets%2C%0Acombined-feature%20models%20significantly%20outperformed%20clinical-only%20models%2C%20though%0Awith%20a%20modest%20advantage%20of%202-4%20C-index%20points.%0A%20%20Conclusions%3A%20While%20confirming%20the%20predictive%20value%20of%20anatomical%20MRI%0Asequences%20for%20glioblastoma%20prognosis%2C%20this%20multi-center%20study%20found%20standard%20CR%0Aand%20DL%20radiomics%20approaches%20offer%20minimal%20added%20value%20over%20demographic%0Apredictors%20such%20as%20age%20and%20gender.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520added%2520value%2520for%2520MRI%2520radiomics%2520and%2520deep-learning%2520for%2520glioblastoma%250A%2520%2520prognostication%2520compared%2520to%2520clinical%2520and%2520molecular%2520information%26entry.906535625%3DD.%2520Abler%2520and%2520O.%2520Pusterla%2520and%2520A.%2520Joye-K%25C3%25BChnis%2520and%2520N.%2520Andratschke%2520and%2520M.%2520Bach%2520and%2520A.%2520Bink%2520and%2520S.%2520M.%2520Christ%2520and%2520P.%2520Hagmann%2520and%2520B.%2520Pouymayou%2520and%2520E.%2520Pravat%25C3%25A0%2520and%2520P.%2520Radojewski%2520and%2520M.%2520Reyes%2520and%2520L.%2520Ruinelli%2520and%2520R.%2520Schaer%2520and%2520B.%2520Stieltjes%2520and%2520G.%2520Treglia%2520and%2520W.%2520Valenzuela%2520and%2520R.%2520Wiest%2520and%2520S.%2520Zoergiebel%2520and%2520M.%2520Guckenberger%2520and%2520S.%2520Tanadini-Lang%2520and%2520A.%2520Depeursinge%26entry.1292438233%3D%2520%2520Background%253A%2520Radiomics%2520shows%2520promise%2520in%2520characterizing%2520glioblastoma%252C%2520but%2520its%250Aadded%2520value%2520over%2520clinical%2520and%2520molecular%2520predictors%2520has%2520yet%2520to%2520be%2520proven.%2520This%250Astudy%2520assessed%2520the%2520added%2520value%2520of%2520conventional%2520radiomics%2520%2528CR%2529%2520and%2520deep%2520learning%250A%2528DL%2529%2520MRI%2520radiomics%2520for%2520glioblastoma%2520prognosis%2520%2528%253C%253D%25206%2520vs%2520%253E%25206%2520months%2520survival%2529%2520on%250Aa%2520large%2520multi-center%2520dataset.%250A%2520%2520Methods%253A%2520After%2520patient%2520selection%252C%2520our%2520curated%2520dataset%2520gathers%25201152%250Aglioblastoma%2520%2528WHO%25202016%2529%2520patients%2520from%2520five%2520Swiss%2520centers%2520and%2520one%2520public%2520source.%250AIt%2520included%2520clinical%2520%2528age%252C%2520gender%2529%252C%2520molecular%2520%2528MGMT%252C%2520IDH%2529%252C%2520and%2520baseline%2520MRI%250Adata%2520%2528T1%252C%2520T1%2520contrast%252C%2520FLAIR%252C%2520T2%2529%2520with%2520tumor%2520regions.%2520CR%2520and%2520DL%2520models%2520were%250Adeveloped%2520using%2520standard%2520methods%2520and%2520evaluated%2520on%2520internal%2520and%2520external%250Acohorts.%2520Sub-analyses%2520assessed%2520models%2520with%2520different%2520feature%2520sets%250A%2528imaging-only%252C%2520clinical/molecular-only%252C%2520combined-features%2529%2520and%2520patient%2520subsets%250A%2528S-1%253A%2520all%2520patients%252C%2520S-2%253A%2520with%2520molecular%2520data%252C%2520S-3%253A%2520IDH%2520wildtype%2529.%250A%2520%2520Results%253A%2520The%2520best%2520performance%2520was%2520observed%2520in%2520the%2520full%2520cohort%2520%2528S-1%2529.%2520In%250Aexternal%2520validation%252C%2520the%2520combined-feature%2520CR%2520model%2520achieved%2520an%2520AUC%2520of%25200.75%252C%250Aslightly%252C%2520but%2520significantly%2520outperforming%2520clinical-only%2520%25280.74%2529%2520and%2520imaging-only%250A%25280.68%2529%2520models.%2520DL%2520models%2520showed%2520similar%2520trends%252C%2520though%2520without%2520statistical%250Asignificance.%2520In%2520S-2%2520and%2520S-3%252C%2520combined%2520models%2520did%2520not%2520outperform%2520clinical-only%250Amodels.%2520Exploratory%2520analysis%2520of%2520CR%2520models%2520for%2520overall%2520survival%2520prediction%250Asuggested%2520greater%2520relevance%2520of%2520imaging%2520data%253A%2520across%2520all%2520subsets%252C%250Acombined-feature%2520models%2520significantly%2520outperformed%2520clinical-only%2520models%252C%2520though%250Awith%2520a%2520modest%2520advantage%2520of%25202-4%2520C-index%2520points.%250A%2520%2520Conclusions%253A%2520While%2520confirming%2520the%2520predictive%2520value%2520of%2520anatomical%2520MRI%250Asequences%2520for%2520glioblastoma%2520prognosis%252C%2520this%2520multi-center%2520study%2520found%2520standard%2520CR%250Aand%2520DL%2520radiomics%2520approaches%2520offer%2520minimal%2520added%2520value%2520over%2520demographic%250Apredictors%2520such%2520as%2520age%2520and%2520gender.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20added%20value%20for%20MRI%20radiomics%20and%20deep-learning%20for%20glioblastoma%0A%20%20prognostication%20compared%20to%20clinical%20and%20molecular%20information&entry.906535625=D.%20Abler%20and%20O.%20Pusterla%20and%20A.%20Joye-K%C3%BChnis%20and%20N.%20Andratschke%20and%20M.%20Bach%20and%20A.%20Bink%20and%20S.%20M.%20Christ%20and%20P.%20Hagmann%20and%20B.%20Pouymayou%20and%20E.%20Pravat%C3%A0%20and%20P.%20Radojewski%20and%20M.%20Reyes%20and%20L.%20Ruinelli%20and%20R.%20Schaer%20and%20B.%20Stieltjes%20and%20G.%20Treglia%20and%20W.%20Valenzuela%20and%20R.%20Wiest%20and%20S.%20Zoergiebel%20and%20M.%20Guckenberger%20and%20S.%20Tanadini-Lang%20and%20A.%20Depeursinge&entry.1292438233=%20%20Background%3A%20Radiomics%20shows%20promise%20in%20characterizing%20glioblastoma%2C%20but%20its%0Aadded%20value%20over%20clinical%20and%20molecular%20predictors%20has%20yet%20to%20be%20proven.%20This%0Astudy%20assessed%20the%20added%20value%20of%20conventional%20radiomics%20%28CR%29%20and%20deep%20learning%0A%28DL%29%20MRI%20radiomics%20for%20glioblastoma%20prognosis%20%28%3C%3D%206%20vs%20%3E%206%20months%20survival%29%20on%0Aa%20large%20multi-center%20dataset.%0A%20%20Methods%3A%20After%20patient%20selection%2C%20our%20curated%20dataset%20gathers%201152%0Aglioblastoma%20%28WHO%202016%29%20patients%20from%20five%20Swiss%20centers%20and%20one%20public%20source.%0AIt%20included%20clinical%20%28age%2C%20gender%29%2C%20molecular%20%28MGMT%2C%20IDH%29%2C%20and%20baseline%20MRI%0Adata%20%28T1%2C%20T1%20contrast%2C%20FLAIR%2C%20T2%29%20with%20tumor%20regions.%20CR%20and%20DL%20models%20were%0Adeveloped%20using%20standard%20methods%20and%20evaluated%20on%20internal%20and%20external%0Acohorts.%20Sub-analyses%20assessed%20models%20with%20different%20feature%20sets%0A%28imaging-only%2C%20clinical/molecular-only%2C%20combined-features%29%20and%20patient%20subsets%0A%28S-1%3A%20all%20patients%2C%20S-2%3A%20with%20molecular%20data%2C%20S-3%3A%20IDH%20wildtype%29.%0A%20%20Results%3A%20The%20best%20performance%20was%20observed%20in%20the%20full%20cohort%20%28S-1%29.%20In%0Aexternal%20validation%2C%20the%20combined-feature%20CR%20model%20achieved%20an%20AUC%20of%200.75%2C%0Aslightly%2C%20but%20significantly%20outperforming%20clinical-only%20%280.74%29%20and%20imaging-only%0A%280.68%29%20models.%20DL%20models%20showed%20similar%20trends%2C%20though%20without%20statistical%0Asignificance.%20In%20S-2%20and%20S-3%2C%20combined%20models%20did%20not%20outperform%20clinical-only%0Amodels.%20Exploratory%20analysis%20of%20CR%20models%20for%20overall%20survival%20prediction%0Asuggested%20greater%20relevance%20of%20imaging%20data%3A%20across%20all%20subsets%2C%0Acombined-feature%20models%20significantly%20outperformed%20clinical-only%20models%2C%20though%0Awith%20a%20modest%20advantage%20of%202-4%20C-index%20points.%0A%20%20Conclusions%3A%20While%20confirming%20the%20predictive%20value%20of%20anatomical%20MRI%0Asequences%20for%20glioblastoma%20prognosis%2C%20this%20multi-center%20study%20found%20standard%20CR%0Aand%20DL%20radiomics%20approaches%20offer%20minimal%20added%20value%20over%20demographic%0Apredictors%20such%20as%20age%20and%20gender.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15548v1&entry.124074799=Read"},
{"title": "Towards Holistic Surgical Scene Graph", "author": "Jongmin Shin and Enki Cho and Ka Yong Kim and Jung Yong Kim and Seong Tae Kim and Namkee Oh", "abstract": "  Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com\n", "link": "http://arxiv.org/abs/2507.15541v1", "date": "2025-07-21", "relevancy": 2.0917, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Holistic%20Surgical%20Scene%20Graph&body=Title%3A%20Towards%20Holistic%20Surgical%20Scene%20Graph%0AAuthor%3A%20Jongmin%20Shin%20and%20Enki%20Cho%20and%20Ka%20Yong%20Kim%20and%20Jung%20Yong%20Kim%20and%20Seong%20Tae%20Kim%20and%20Namkee%20Oh%0AAbstract%3A%20%20%20Surgical%20scene%20understanding%20is%20crucial%20for%20computer-assisted%20intervention%0Asystems%2C%20requiring%20visual%20comprehension%20of%20surgical%20scenes%20that%20involves%0Adiverse%20elements%20such%20as%20surgical%20tools%2C%20anatomical%20structures%2C%20and%20their%0Ainteractions.%20To%20effectively%20represent%20the%20complex%20information%20in%20surgical%0Ascenes%2C%20graph-based%20approaches%20have%20been%20explored%20to%20structurally%20model%0Asurgical%20entities%20and%20their%20relationships.%20Previous%20surgical%20scene%20graph%0Astudies%20have%20demonstrated%20the%20feasibility%20of%20representing%20surgical%20scenes%20using%0Agraphs.%20However%2C%20certain%20aspects%20of%20surgical%20scenes-such%20as%20diverse%0Acombinations%20of%20tool-action-target%20and%20the%20identity%20of%20the%20hand%20operating%20the%0Atool-remain%20underexplored%20in%20graph-based%20representations%2C%20despite%20their%0Aimportance.%20To%20incorporate%20these%20aspects%20into%20graph%20representations%2C%20we%20propose%0AEndoscapes-SG201%20dataset%2C%20which%20includes%20annotations%20for%20tool-action-target%0Acombinations%20and%20hand%20identity.%20We%20also%20introduce%20SSG-Com%2C%20a%20graph-based%20method%0Adesigned%20to%20learn%20and%20represent%20these%20critical%20elements.%20Through%20experiments%20on%0Adownstream%20tasks%20such%20as%20critical%20view%20of%20safety%20assessment%20and%20action%20triplet%0Arecognition%2C%20we%20demonstrated%20the%20importance%20of%20integrating%20these%20essential%0Ascene%20graph%20components%2C%20highlighting%20their%20significant%20contribution%20to%20surgical%0Ascene%20understanding.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ailab-kyunghee/SSG-Com%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Holistic%2520Surgical%2520Scene%2520Graph%26entry.906535625%3DJongmin%2520Shin%2520and%2520Enki%2520Cho%2520and%2520Ka%2520Yong%2520Kim%2520and%2520Jung%2520Yong%2520Kim%2520and%2520Seong%2520Tae%2520Kim%2520and%2520Namkee%2520Oh%26entry.1292438233%3D%2520%2520Surgical%2520scene%2520understanding%2520is%2520crucial%2520for%2520computer-assisted%2520intervention%250Asystems%252C%2520requiring%2520visual%2520comprehension%2520of%2520surgical%2520scenes%2520that%2520involves%250Adiverse%2520elements%2520such%2520as%2520surgical%2520tools%252C%2520anatomical%2520structures%252C%2520and%2520their%250Ainteractions.%2520To%2520effectively%2520represent%2520the%2520complex%2520information%2520in%2520surgical%250Ascenes%252C%2520graph-based%2520approaches%2520have%2520been%2520explored%2520to%2520structurally%2520model%250Asurgical%2520entities%2520and%2520their%2520relationships.%2520Previous%2520surgical%2520scene%2520graph%250Astudies%2520have%2520demonstrated%2520the%2520feasibility%2520of%2520representing%2520surgical%2520scenes%2520using%250Agraphs.%2520However%252C%2520certain%2520aspects%2520of%2520surgical%2520scenes-such%2520as%2520diverse%250Acombinations%2520of%2520tool-action-target%2520and%2520the%2520identity%2520of%2520the%2520hand%2520operating%2520the%250Atool-remain%2520underexplored%2520in%2520graph-based%2520representations%252C%2520despite%2520their%250Aimportance.%2520To%2520incorporate%2520these%2520aspects%2520into%2520graph%2520representations%252C%2520we%2520propose%250AEndoscapes-SG201%2520dataset%252C%2520which%2520includes%2520annotations%2520for%2520tool-action-target%250Acombinations%2520and%2520hand%2520identity.%2520We%2520also%2520introduce%2520SSG-Com%252C%2520a%2520graph-based%2520method%250Adesigned%2520to%2520learn%2520and%2520represent%2520these%2520critical%2520elements.%2520Through%2520experiments%2520on%250Adownstream%2520tasks%2520such%2520as%2520critical%2520view%2520of%2520safety%2520assessment%2520and%2520action%2520triplet%250Arecognition%252C%2520we%2520demonstrated%2520the%2520importance%2520of%2520integrating%2520these%2520essential%250Ascene%2520graph%2520components%252C%2520highlighting%2520their%2520significant%2520contribution%2520to%2520surgical%250Ascene%2520understanding.%2520The%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/ailab-kyunghee/SSG-Com%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Holistic%20Surgical%20Scene%20Graph&entry.906535625=Jongmin%20Shin%20and%20Enki%20Cho%20and%20Ka%20Yong%20Kim%20and%20Jung%20Yong%20Kim%20and%20Seong%20Tae%20Kim%20and%20Namkee%20Oh&entry.1292438233=%20%20Surgical%20scene%20understanding%20is%20crucial%20for%20computer-assisted%20intervention%0Asystems%2C%20requiring%20visual%20comprehension%20of%20surgical%20scenes%20that%20involves%0Adiverse%20elements%20such%20as%20surgical%20tools%2C%20anatomical%20structures%2C%20and%20their%0Ainteractions.%20To%20effectively%20represent%20the%20complex%20information%20in%20surgical%0Ascenes%2C%20graph-based%20approaches%20have%20been%20explored%20to%20structurally%20model%0Asurgical%20entities%20and%20their%20relationships.%20Previous%20surgical%20scene%20graph%0Astudies%20have%20demonstrated%20the%20feasibility%20of%20representing%20surgical%20scenes%20using%0Agraphs.%20However%2C%20certain%20aspects%20of%20surgical%20scenes-such%20as%20diverse%0Acombinations%20of%20tool-action-target%20and%20the%20identity%20of%20the%20hand%20operating%20the%0Atool-remain%20underexplored%20in%20graph-based%20representations%2C%20despite%20their%0Aimportance.%20To%20incorporate%20these%20aspects%20into%20graph%20representations%2C%20we%20propose%0AEndoscapes-SG201%20dataset%2C%20which%20includes%20annotations%20for%20tool-action-target%0Acombinations%20and%20hand%20identity.%20We%20also%20introduce%20SSG-Com%2C%20a%20graph-based%20method%0Adesigned%20to%20learn%20and%20represent%20these%20critical%20elements.%20Through%20experiments%20on%0Adownstream%20tasks%20such%20as%20critical%20view%20of%20safety%20assessment%20and%20action%20triplet%0Arecognition%2C%20we%20demonstrated%20the%20importance%20of%20integrating%20these%20essential%0Ascene%20graph%20components%2C%20highlighting%20their%20significant%20contribution%20to%20surgical%0Ascene%20understanding.%20The%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ailab-kyunghee/SSG-Com%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15541v1&entry.124074799=Read"},
{"title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "author": "Sizhou Chen and Shufan Jiang and Chi Zhang and Xiao-Lei Zhang and Xuelong Li", "abstract": "  Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.\n", "link": "http://arxiv.org/abs/2507.15518v1", "date": "2025-07-21", "relevancy": 2.0897, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5305}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5265}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMLET%3A%20Hyperadaptive%20Agent-based%20Modeling%20for%20Live%20Embodied%20Theatrics&body=Title%3A%20HAMLET%3A%20Hyperadaptive%20Agent-based%20Modeling%20for%20Live%20Embodied%20Theatrics%0AAuthor%3A%20Sizhou%20Chen%20and%20Shufan%20Jiang%20and%20Chi%20Zhang%20and%20Xiao-Lei%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Creating%20an%20immersive%20and%20interactive%20theatrical%20experience%20is%20a%20long-term%0Agoal%20in%20the%20field%20of%20interactive%20narrative.%20The%20emergence%20of%20large%20language%0Amodel%20%28LLM%29%20is%20providing%20a%20new%20path%20to%20achieve%20this%20goal.%20However%2C%20existing%0ALLM-based%20drama%20generation%20methods%20often%20result%20in%20AI%20agents%20that%20lack%0Ainitiative%20and%20cannot%20interact%20with%20the%20physical%20environment.%20Furthermore%2C%0Athese%20methods%20typically%20require%20detailed%20user%20input%20to%20drive%20the%20drama.%20These%0Alimitations%20reduce%20the%20interactivity%20and%20immersion%20of%20online%20real-time%0Aperformance.%20To%20address%20the%20above%20challenges%2C%20we%20propose%20HAMLET%2C%20a%20multi-agent%0Aframework%20focused%20on%20drama%20creation%20and%20online%20performance.%20Given%20a%20simple%0Atopic%2C%20the%20framework%20generates%20a%20narrative%20blueprint%2C%20guiding%20the%20subsequent%0Aimprovisational%20performance.%20During%20the%20online%20performance%2C%20each%20actor%20is%20given%0Aan%20autonomous%20mind.%20This%20means%20that%20actors%20can%20make%20independent%20decisions%20based%0Aon%20their%20own%20background%2C%20goals%2C%20and%20emotional%20state.%20In%20addition%20to%0Aconversations%20with%20other%20actors%2C%20their%20decisions%20can%20also%20change%20the%20state%20of%0Ascene%20props%20through%20actions%20such%20as%20opening%20a%20letter%20or%20picking%20up%20a%20weapon.%0AThe%20change%20is%20then%20broadcast%20to%20other%20related%20actors%2C%20updating%20what%20they%20know%0Aand%20care%20about%2C%20which%20in%20turn%20influences%20their%20next%20action.%20To%20evaluate%20the%0Aquality%20of%20drama%20performance%2C%20we%20designed%20an%20evaluation%20method%20to%20assess%20three%0Aprimary%20aspects%2C%20including%20character%20performance%2C%20narrative%20quality%2C%20and%0Ainteraction%20experience.%20The%20experimental%20evaluation%20shows%20that%20HAMLET%20can%0Acreate%20expressive%20and%20coherent%20theatrical%20experiences.%20Our%20code%2C%20dataset%20and%0Amodels%20are%20available%20at%20https%3A//github.com/HAMLET-2025/HAMLET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMLET%253A%2520Hyperadaptive%2520Agent-based%2520Modeling%2520for%2520Live%2520Embodied%2520Theatrics%26entry.906535625%3DSizhou%2520Chen%2520and%2520Shufan%2520Jiang%2520and%2520Chi%2520Zhang%2520and%2520Xiao-Lei%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Creating%2520an%2520immersive%2520and%2520interactive%2520theatrical%2520experience%2520is%2520a%2520long-term%250Agoal%2520in%2520the%2520field%2520of%2520interactive%2520narrative.%2520The%2520emergence%2520of%2520large%2520language%250Amodel%2520%2528LLM%2529%2520is%2520providing%2520a%2520new%2520path%2520to%2520achieve%2520this%2520goal.%2520However%252C%2520existing%250ALLM-based%2520drama%2520generation%2520methods%2520often%2520result%2520in%2520AI%2520agents%2520that%2520lack%250Ainitiative%2520and%2520cannot%2520interact%2520with%2520the%2520physical%2520environment.%2520Furthermore%252C%250Athese%2520methods%2520typically%2520require%2520detailed%2520user%2520input%2520to%2520drive%2520the%2520drama.%2520These%250Alimitations%2520reduce%2520the%2520interactivity%2520and%2520immersion%2520of%2520online%2520real-time%250Aperformance.%2520To%2520address%2520the%2520above%2520challenges%252C%2520we%2520propose%2520HAMLET%252C%2520a%2520multi-agent%250Aframework%2520focused%2520on%2520drama%2520creation%2520and%2520online%2520performance.%2520Given%2520a%2520simple%250Atopic%252C%2520the%2520framework%2520generates%2520a%2520narrative%2520blueprint%252C%2520guiding%2520the%2520subsequent%250Aimprovisational%2520performance.%2520During%2520the%2520online%2520performance%252C%2520each%2520actor%2520is%2520given%250Aan%2520autonomous%2520mind.%2520This%2520means%2520that%2520actors%2520can%2520make%2520independent%2520decisions%2520based%250Aon%2520their%2520own%2520background%252C%2520goals%252C%2520and%2520emotional%2520state.%2520In%2520addition%2520to%250Aconversations%2520with%2520other%2520actors%252C%2520their%2520decisions%2520can%2520also%2520change%2520the%2520state%2520of%250Ascene%2520props%2520through%2520actions%2520such%2520as%2520opening%2520a%2520letter%2520or%2520picking%2520up%2520a%2520weapon.%250AThe%2520change%2520is%2520then%2520broadcast%2520to%2520other%2520related%2520actors%252C%2520updating%2520what%2520they%2520know%250Aand%2520care%2520about%252C%2520which%2520in%2520turn%2520influences%2520their%2520next%2520action.%2520To%2520evaluate%2520the%250Aquality%2520of%2520drama%2520performance%252C%2520we%2520designed%2520an%2520evaluation%2520method%2520to%2520assess%2520three%250Aprimary%2520aspects%252C%2520including%2520character%2520performance%252C%2520narrative%2520quality%252C%2520and%250Ainteraction%2520experience.%2520The%2520experimental%2520evaluation%2520shows%2520that%2520HAMLET%2520can%250Acreate%2520expressive%2520and%2520coherent%2520theatrical%2520experiences.%2520Our%2520code%252C%2520dataset%2520and%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/HAMLET-2025/HAMLET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMLET%3A%20Hyperadaptive%20Agent-based%20Modeling%20for%20Live%20Embodied%20Theatrics&entry.906535625=Sizhou%20Chen%20and%20Shufan%20Jiang%20and%20Chi%20Zhang%20and%20Xiao-Lei%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20Creating%20an%20immersive%20and%20interactive%20theatrical%20experience%20is%20a%20long-term%0Agoal%20in%20the%20field%20of%20interactive%20narrative.%20The%20emergence%20of%20large%20language%0Amodel%20%28LLM%29%20is%20providing%20a%20new%20path%20to%20achieve%20this%20goal.%20However%2C%20existing%0ALLM-based%20drama%20generation%20methods%20often%20result%20in%20AI%20agents%20that%20lack%0Ainitiative%20and%20cannot%20interact%20with%20the%20physical%20environment.%20Furthermore%2C%0Athese%20methods%20typically%20require%20detailed%20user%20input%20to%20drive%20the%20drama.%20These%0Alimitations%20reduce%20the%20interactivity%20and%20immersion%20of%20online%20real-time%0Aperformance.%20To%20address%20the%20above%20challenges%2C%20we%20propose%20HAMLET%2C%20a%20multi-agent%0Aframework%20focused%20on%20drama%20creation%20and%20online%20performance.%20Given%20a%20simple%0Atopic%2C%20the%20framework%20generates%20a%20narrative%20blueprint%2C%20guiding%20the%20subsequent%0Aimprovisational%20performance.%20During%20the%20online%20performance%2C%20each%20actor%20is%20given%0Aan%20autonomous%20mind.%20This%20means%20that%20actors%20can%20make%20independent%20decisions%20based%0Aon%20their%20own%20background%2C%20goals%2C%20and%20emotional%20state.%20In%20addition%20to%0Aconversations%20with%20other%20actors%2C%20their%20decisions%20can%20also%20change%20the%20state%20of%0Ascene%20props%20through%20actions%20such%20as%20opening%20a%20letter%20or%20picking%20up%20a%20weapon.%0AThe%20change%20is%20then%20broadcast%20to%20other%20related%20actors%2C%20updating%20what%20they%20know%0Aand%20care%20about%2C%20which%20in%20turn%20influences%20their%20next%20action.%20To%20evaluate%20the%0Aquality%20of%20drama%20performance%2C%20we%20designed%20an%20evaluation%20method%20to%20assess%20three%0Aprimary%20aspects%2C%20including%20character%20performance%2C%20narrative%20quality%2C%20and%0Ainteraction%20experience.%20The%20experimental%20evaluation%20shows%20that%20HAMLET%20can%0Acreate%20expressive%20and%20coherent%20theatrical%20experiences.%20Our%20code%2C%20dataset%20and%0Amodels%20are%20available%20at%20https%3A//github.com/HAMLET-2025/HAMLET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15518v1&entry.124074799=Read"},
{"title": "Applying the Chinese Wall Reverse Engineering Technique to Large\n  Language Model Code Editing", "author": "Manatsawin Hanmongkolchai", "abstract": "  Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.\n", "link": "http://arxiv.org/abs/2507.15599v1", "date": "2025-07-21", "relevancy": 2.0816, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20the%20Chinese%20Wall%20Reverse%20Engineering%20Technique%20to%20Large%0A%20%20Language%20Model%20Code%20Editing&body=Title%3A%20Applying%20the%20Chinese%20Wall%20Reverse%20Engineering%20Technique%20to%20Large%0A%20%20Language%20Model%20Code%20Editing%0AAuthor%3A%20Manatsawin%20Hanmongkolchai%0AAbstract%3A%20%20%20Large%20language%20models%20for%20code%20%28Code%20LLM%29%20are%20increasingly%20utilized%20in%0Aprogramming%20environments.%20Despite%20their%20utility%2C%20the%20training%20datasets%20for%20top%0ALLM%20remain%20undisclosed%2C%20raising%20concerns%20about%20potential%20copyright%20violations.%0ASome%20models%2C%20such%20as%20Pleias%20and%20Comma%20put%20emphasis%20on%20data%20curation%20and%0Alicenses%2C%20however%2C%20with%20limited%20training%20data%20these%20models%20are%20not%20competitive%0Aand%20only%20serve%20as%20proof%20of%20concepts.%20To%20improve%20the%20utility%20of%20these%20models%2C%20we%0Apropose%20an%20application%20of%20the%20%22Chinese%20Wall%22%20technique%2C%20inspired%20by%20the%20reverse%0Aengineering%20technique%20of%20the%20same%20name%20--%20a%20high%20quality%20model%20is%20used%20to%0Agenerate%20detailed%20instructions%20for%20a%20weaker%20model.%20By%20doing%20so%2C%20a%20weaker%20but%0Aethically%20aligned%20model%20may%20be%20used%20to%20perform%20complicated%20tasks%20that%2C%0Aotherwise%2C%20can%20only%20be%20completed%20by%20more%20powerful%20models.%20In%20our%20evaluation%2C%0Awe%27ve%20found%20that%20this%20technique%20improves%20Comma%20v0.1%201T%27s%20performance%20in%0ACanItEdit%20benchmark%20by%20over%2066%25%2C%20and%20Starcoder2%20Instruct%20by%20roughly%2020%25%0Acompared%20to%20when%20running%20the%20same%20model%20on%20the%20benchmark%20alone.%20The%20practical%0Aapplication%20of%20this%20technique%20today%2C%20however%2C%20may%20be%20limited%20due%20to%20the%20lack%20of%0Amodels%20trained%20on%20public%20domain%20content%20without%20copyright%20restrictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520the%2520Chinese%2520Wall%2520Reverse%2520Engineering%2520Technique%2520to%2520Large%250A%2520%2520Language%2520Model%2520Code%2520Editing%26entry.906535625%3DManatsawin%2520Hanmongkolchai%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520for%2520code%2520%2528Code%2520LLM%2529%2520are%2520increasingly%2520utilized%2520in%250Aprogramming%2520environments.%2520Despite%2520their%2520utility%252C%2520the%2520training%2520datasets%2520for%2520top%250ALLM%2520remain%2520undisclosed%252C%2520raising%2520concerns%2520about%2520potential%2520copyright%2520violations.%250ASome%2520models%252C%2520such%2520as%2520Pleias%2520and%2520Comma%2520put%2520emphasis%2520on%2520data%2520curation%2520and%250Alicenses%252C%2520however%252C%2520with%2520limited%2520training%2520data%2520these%2520models%2520are%2520not%2520competitive%250Aand%2520only%2520serve%2520as%2520proof%2520of%2520concepts.%2520To%2520improve%2520the%2520utility%2520of%2520these%2520models%252C%2520we%250Apropose%2520an%2520application%2520of%2520the%2520%2522Chinese%2520Wall%2522%2520technique%252C%2520inspired%2520by%2520the%2520reverse%250Aengineering%2520technique%2520of%2520the%2520same%2520name%2520--%2520a%2520high%2520quality%2520model%2520is%2520used%2520to%250Agenerate%2520detailed%2520instructions%2520for%2520a%2520weaker%2520model.%2520By%2520doing%2520so%252C%2520a%2520weaker%2520but%250Aethically%2520aligned%2520model%2520may%2520be%2520used%2520to%2520perform%2520complicated%2520tasks%2520that%252C%250Aotherwise%252C%2520can%2520only%2520be%2520completed%2520by%2520more%2520powerful%2520models.%2520In%2520our%2520evaluation%252C%250Awe%2527ve%2520found%2520that%2520this%2520technique%2520improves%2520Comma%2520v0.1%25201T%2527s%2520performance%2520in%250ACanItEdit%2520benchmark%2520by%2520over%252066%2525%252C%2520and%2520Starcoder2%2520Instruct%2520by%2520roughly%252020%2525%250Acompared%2520to%2520when%2520running%2520the%2520same%2520model%2520on%2520the%2520benchmark%2520alone.%2520The%2520practical%250Aapplication%2520of%2520this%2520technique%2520today%252C%2520however%252C%2520may%2520be%2520limited%2520due%2520to%2520the%2520lack%2520of%250Amodels%2520trained%2520on%2520public%2520domain%2520content%2520without%2520copyright%2520restrictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20the%20Chinese%20Wall%20Reverse%20Engineering%20Technique%20to%20Large%0A%20%20Language%20Model%20Code%20Editing&entry.906535625=Manatsawin%20Hanmongkolchai&entry.1292438233=%20%20Large%20language%20models%20for%20code%20%28Code%20LLM%29%20are%20increasingly%20utilized%20in%0Aprogramming%20environments.%20Despite%20their%20utility%2C%20the%20training%20datasets%20for%20top%0ALLM%20remain%20undisclosed%2C%20raising%20concerns%20about%20potential%20copyright%20violations.%0ASome%20models%2C%20such%20as%20Pleias%20and%20Comma%20put%20emphasis%20on%20data%20curation%20and%0Alicenses%2C%20however%2C%20with%20limited%20training%20data%20these%20models%20are%20not%20competitive%0Aand%20only%20serve%20as%20proof%20of%20concepts.%20To%20improve%20the%20utility%20of%20these%20models%2C%20we%0Apropose%20an%20application%20of%20the%20%22Chinese%20Wall%22%20technique%2C%20inspired%20by%20the%20reverse%0Aengineering%20technique%20of%20the%20same%20name%20--%20a%20high%20quality%20model%20is%20used%20to%0Agenerate%20detailed%20instructions%20for%20a%20weaker%20model.%20By%20doing%20so%2C%20a%20weaker%20but%0Aethically%20aligned%20model%20may%20be%20used%20to%20perform%20complicated%20tasks%20that%2C%0Aotherwise%2C%20can%20only%20be%20completed%20by%20more%20powerful%20models.%20In%20our%20evaluation%2C%0Awe%27ve%20found%20that%20this%20technique%20improves%20Comma%20v0.1%201T%27s%20performance%20in%0ACanItEdit%20benchmark%20by%20over%2066%25%2C%20and%20Starcoder2%20Instruct%20by%20roughly%2020%25%0Acompared%20to%20when%20running%20the%20same%20model%20on%20the%20benchmark%20alone.%20The%20practical%0Aapplication%20of%20this%20technique%20today%2C%20however%2C%20may%20be%20limited%20due%20to%20the%20lack%20of%0Amodels%20trained%20on%20public%20domain%20content%20without%20copyright%20restrictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15599v1&entry.124074799=Read"},
{"title": "Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications", "author": "V\u00edt Kr\u00e1tk\u00fd and Giuseppe Silano and Matou\u0161 Vrba and Christos Papaioannidis and Ioannis Mademlis and Robert P\u011bni\u010dka and Ioannis Pitas and Martin Saska", "abstract": "  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to\nmonitor the safety of human workers, particularly those operating at heights.\nIn the proposed dynamic formation scheme, one UAV acts as the formation leader,\nequipped with sensors for detecting human workers and recognizing gestures. The\nfollower UAVs maintain a predetermined formation relative to the worker's\nposition, providing additional perspectives of the monitored scene. Hand\ngestures enable the human worker to specify movement and action commands for\nthe UAV team and to initiate other mission-related tasks without requiring\nadditional communication channels or specific markers. Combined with a novel\nunified human detection and tracking algorithm, a human position estimation\nmethod, and a gesture detection pipeline, the proposed approach represents the\nfirst instance of an HSI system incorporating all these modules onboard\nreal-world UAVs. Simulations and field experiments involving three UAVs and a\nhuman worker in a mock-up scenario demonstrate the effectiveness and\nresponsiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2403.15333v4", "date": "2025-07-21", "relevancy": 2.0717, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5354}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5155}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&body=Title%3A%20Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications%0AAuthor%3A%20V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20designed%20to%0Amonitor%20the%20safety%20of%20human%20workers%2C%20particularly%20those%20operating%20at%20heights.%0AIn%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20formation%20leader%2C%0Aequipped%20with%20sensors%20for%20detecting%20human%20workers%20and%20recognizing%20gestures.%20The%0Afollower%20UAVs%20maintain%20a%20predetermined%20formation%20relative%20to%20the%20worker%27s%0Aposition%2C%20providing%20additional%20perspectives%20of%20the%20monitored%20scene.%20Hand%0Agestures%20enable%20the%20human%20worker%20to%20specify%20movement%20and%20action%20commands%20for%0Athe%20UAV%20team%20and%20to%20initiate%20other%20mission-related%20tasks%20without%20requiring%0Aadditional%20communication%20channels%20or%20specific%20markers.%20Combined%20with%20a%20novel%0Aunified%20human%20detection%20and%20tracking%20algorithm%2C%20a%20human%20position%20estimation%0Amethod%2C%20and%20a%20gesture%20detection%20pipeline%2C%20the%20proposed%20approach%20represents%20the%0Afirst%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%20modules%20onboard%0Areal-world%20UAVs.%20Simulations%20and%20field%20experiments%20involving%20three%20UAVs%20and%20a%0Ahuman%20worker%20in%20a%20mock-up%20scenario%20demonstrate%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15333v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGesture-Controlled%2520Aerial%2520Robot%2520Formation%2520for%2520Human-Swarm%2520Interaction%2520in%250A%2520%2520Safety%2520Monitoring%2520Applications%26entry.906535625%3DV%25C3%25ADt%2520Kr%25C3%25A1tk%25C3%25BD%2520and%2520Giuseppe%2520Silano%2520and%2520Matou%25C5%25A1%2520Vrba%2520and%2520Christos%2520Papaioannidis%2520and%2520Ioannis%2520Mademlis%2520and%2520Robert%2520P%25C4%259Bni%25C4%258Dka%2520and%2520Ioannis%2520Pitas%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520formation%2520control%2520approach%2520for%2520contactless%250Agesture-based%2520Human-Swarm%2520Interaction%2520%2528HSI%2529%2520between%2520a%2520team%2520of%2520multi-rotor%250AUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%2520a%2520human%2520worker.%2520The%2520approach%2520is%2520designed%2520to%250Amonitor%2520the%2520safety%2520of%2520human%2520workers%252C%2520particularly%2520those%2520operating%2520at%2520heights.%250AIn%2520the%2520proposed%2520dynamic%2520formation%2520scheme%252C%2520one%2520UAV%2520acts%2520as%2520the%2520formation%2520leader%252C%250Aequipped%2520with%2520sensors%2520for%2520detecting%2520human%2520workers%2520and%2520recognizing%2520gestures.%2520The%250Afollower%2520UAVs%2520maintain%2520a%2520predetermined%2520formation%2520relative%2520to%2520the%2520worker%2527s%250Aposition%252C%2520providing%2520additional%2520perspectives%2520of%2520the%2520monitored%2520scene.%2520Hand%250Agestures%2520enable%2520the%2520human%2520worker%2520to%2520specify%2520movement%2520and%2520action%2520commands%2520for%250Athe%2520UAV%2520team%2520and%2520to%2520initiate%2520other%2520mission-related%2520tasks%2520without%2520requiring%250Aadditional%2520communication%2520channels%2520or%2520specific%2520markers.%2520Combined%2520with%2520a%2520novel%250Aunified%2520human%2520detection%2520and%2520tracking%2520algorithm%252C%2520a%2520human%2520position%2520estimation%250Amethod%252C%2520and%2520a%2520gesture%2520detection%2520pipeline%252C%2520the%2520proposed%2520approach%2520represents%2520the%250Afirst%2520instance%2520of%2520an%2520HSI%2520system%2520incorporating%2520all%2520these%2520modules%2520onboard%250Areal-world%2520UAVs.%2520Simulations%2520and%2520field%2520experiments%2520involving%2520three%2520UAVs%2520and%2520a%250Ahuman%2520worker%2520in%2520a%2520mock-up%2520scenario%2520demonstrate%2520the%2520effectiveness%2520and%250Aresponsiveness%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15333v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gesture-Controlled%20Aerial%20Robot%20Formation%20for%20Human-Swarm%20Interaction%20in%0A%20%20Safety%20Monitoring%20Applications&entry.906535625=V%C3%ADt%20Kr%C3%A1tk%C3%BD%20and%20Giuseppe%20Silano%20and%20Matou%C5%A1%20Vrba%20and%20Christos%20Papaioannidis%20and%20Ioannis%20Mademlis%20and%20Robert%20P%C4%9Bni%C4%8Dka%20and%20Ioannis%20Pitas%20and%20Martin%20Saska&entry.1292438233=%20%20This%20paper%20presents%20a%20formation%20control%20approach%20for%20contactless%0Agesture-based%20Human-Swarm%20Interaction%20%28HSI%29%20between%20a%20team%20of%20multi-rotor%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%20a%20human%20worker.%20The%20approach%20is%20designed%20to%0Amonitor%20the%20safety%20of%20human%20workers%2C%20particularly%20those%20operating%20at%20heights.%0AIn%20the%20proposed%20dynamic%20formation%20scheme%2C%20one%20UAV%20acts%20as%20the%20formation%20leader%2C%0Aequipped%20with%20sensors%20for%20detecting%20human%20workers%20and%20recognizing%20gestures.%20The%0Afollower%20UAVs%20maintain%20a%20predetermined%20formation%20relative%20to%20the%20worker%27s%0Aposition%2C%20providing%20additional%20perspectives%20of%20the%20monitored%20scene.%20Hand%0Agestures%20enable%20the%20human%20worker%20to%20specify%20movement%20and%20action%20commands%20for%0Athe%20UAV%20team%20and%20to%20initiate%20other%20mission-related%20tasks%20without%20requiring%0Aadditional%20communication%20channels%20or%20specific%20markers.%20Combined%20with%20a%20novel%0Aunified%20human%20detection%20and%20tracking%20algorithm%2C%20a%20human%20position%20estimation%0Amethod%2C%20and%20a%20gesture%20detection%20pipeline%2C%20the%20proposed%20approach%20represents%20the%0Afirst%20instance%20of%20an%20HSI%20system%20incorporating%20all%20these%20modules%20onboard%0Areal-world%20UAVs.%20Simulations%20and%20field%20experiments%20involving%20three%20UAVs%20and%20a%0Ahuman%20worker%20in%20a%20mock-up%20scenario%20demonstrate%20the%20effectiveness%20and%0Aresponsiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15333v4&entry.124074799=Read"},
{"title": "Hypergraphs on high dimensional time series sets using signature\n  transform", "author": "R\u00e9mi Vaucher and Paul Minchella", "abstract": "  In recent decades, hypergraphs and their analysis through Topological Data\nAnalysis (TDA) have emerged as powerful tools for understanding complex data\nstructures. Various methods have been developed to construct hypergraphs --\nreferred to as simplicial complexes in the TDA framework -- over datasets,\nenabling the formation of edges between more than two vertices. This paper\naddresses the challenge of constructing hypergraphs from collections of\nmultivariate time series. While prior work has focused on the case of a single\nmultivariate time series, we extend this framework to handle collections of\nsuch time series. Our approach generalizes the method proposed in Chretien and\nal. by leveraging the properties of signature transforms to introduce\ncontrolled randomness, thereby enhancing the robustness of the construction\nprocess. We validate our method on synthetic datasets and present promising\nresults.\n", "link": "http://arxiv.org/abs/2507.15802v1", "date": "2025-07-21", "relevancy": 2.0624, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4238}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.414}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hypergraphs%20on%20high%20dimensional%20time%20series%20sets%20using%20signature%0A%20%20transform&body=Title%3A%20Hypergraphs%20on%20high%20dimensional%20time%20series%20sets%20using%20signature%0A%20%20transform%0AAuthor%3A%20R%C3%A9mi%20Vaucher%20and%20Paul%20Minchella%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20hypergraphs%20and%20their%20analysis%20through%20Topological%20Data%0AAnalysis%20%28TDA%29%20have%20emerged%20as%20powerful%20tools%20for%20understanding%20complex%20data%0Astructures.%20Various%20methods%20have%20been%20developed%20to%20construct%20hypergraphs%20--%0Areferred%20to%20as%20simplicial%20complexes%20in%20the%20TDA%20framework%20--%20over%20datasets%2C%0Aenabling%20the%20formation%20of%20edges%20between%20more%20than%20two%20vertices.%20This%20paper%0Aaddresses%20the%20challenge%20of%20constructing%20hypergraphs%20from%20collections%20of%0Amultivariate%20time%20series.%20While%20prior%20work%20has%20focused%20on%20the%20case%20of%20a%20single%0Amultivariate%20time%20series%2C%20we%20extend%20this%20framework%20to%20handle%20collections%20of%0Asuch%20time%20series.%20Our%20approach%20generalizes%20the%20method%20proposed%20in%20Chretien%20and%0Aal.%20by%20leveraging%20the%20properties%20of%20signature%20transforms%20to%20introduce%0Acontrolled%20randomness%2C%20thereby%20enhancing%20the%20robustness%20of%20the%20construction%0Aprocess.%20We%20validate%20our%20method%20on%20synthetic%20datasets%20and%20present%20promising%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHypergraphs%2520on%2520high%2520dimensional%2520time%2520series%2520sets%2520using%2520signature%250A%2520%2520transform%26entry.906535625%3DR%25C3%25A9mi%2520Vaucher%2520and%2520Paul%2520Minchella%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520hypergraphs%2520and%2520their%2520analysis%2520through%2520Topological%2520Data%250AAnalysis%2520%2528TDA%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520understanding%2520complex%2520data%250Astructures.%2520Various%2520methods%2520have%2520been%2520developed%2520to%2520construct%2520hypergraphs%2520--%250Areferred%2520to%2520as%2520simplicial%2520complexes%2520in%2520the%2520TDA%2520framework%2520--%2520over%2520datasets%252C%250Aenabling%2520the%2520formation%2520of%2520edges%2520between%2520more%2520than%2520two%2520vertices.%2520This%2520paper%250Aaddresses%2520the%2520challenge%2520of%2520constructing%2520hypergraphs%2520from%2520collections%2520of%250Amultivariate%2520time%2520series.%2520While%2520prior%2520work%2520has%2520focused%2520on%2520the%2520case%2520of%2520a%2520single%250Amultivariate%2520time%2520series%252C%2520we%2520extend%2520this%2520framework%2520to%2520handle%2520collections%2520of%250Asuch%2520time%2520series.%2520Our%2520approach%2520generalizes%2520the%2520method%2520proposed%2520in%2520Chretien%2520and%250Aal.%2520by%2520leveraging%2520the%2520properties%2520of%2520signature%2520transforms%2520to%2520introduce%250Acontrolled%2520randomness%252C%2520thereby%2520enhancing%2520the%2520robustness%2520of%2520the%2520construction%250Aprocess.%2520We%2520validate%2520our%2520method%2520on%2520synthetic%2520datasets%2520and%2520present%2520promising%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraphs%20on%20high%20dimensional%20time%20series%20sets%20using%20signature%0A%20%20transform&entry.906535625=R%C3%A9mi%20Vaucher%20and%20Paul%20Minchella&entry.1292438233=%20%20In%20recent%20decades%2C%20hypergraphs%20and%20their%20analysis%20through%20Topological%20Data%0AAnalysis%20%28TDA%29%20have%20emerged%20as%20powerful%20tools%20for%20understanding%20complex%20data%0Astructures.%20Various%20methods%20have%20been%20developed%20to%20construct%20hypergraphs%20--%0Areferred%20to%20as%20simplicial%20complexes%20in%20the%20TDA%20framework%20--%20over%20datasets%2C%0Aenabling%20the%20formation%20of%20edges%20between%20more%20than%20two%20vertices.%20This%20paper%0Aaddresses%20the%20challenge%20of%20constructing%20hypergraphs%20from%20collections%20of%0Amultivariate%20time%20series.%20While%20prior%20work%20has%20focused%20on%20the%20case%20of%20a%20single%0Amultivariate%20time%20series%2C%20we%20extend%20this%20framework%20to%20handle%20collections%20of%0Asuch%20time%20series.%20Our%20approach%20generalizes%20the%20method%20proposed%20in%20Chretien%20and%0Aal.%20by%20leveraging%20the%20properties%20of%20signature%20transforms%20to%20introduce%0Acontrolled%20randomness%2C%20thereby%20enhancing%20the%20robustness%20of%20the%20construction%0Aprocess.%20We%20validate%20our%20method%20on%20synthetic%20datasets%20and%20present%20promising%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15802v1&entry.124074799=Read"},
{"title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image\n  Augmentation", "author": "Hugo Carlesso and Maria Eliza Patulea and Moncef Garouani and Radu Tudor Ionescu and Josiane Mothe", "abstract": "  Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.\n", "link": "http://arxiv.org/abs/2507.15577v1", "date": "2025-07-21", "relevancy": 2.0622, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5463}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5113}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation&body=Title%3A%20GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation%0AAuthor%3A%20Hugo%20Carlesso%20and%20Maria%20Eliza%20Patulea%20and%20Moncef%20Garouani%20and%20Radu%20Tudor%20Ionescu%20and%20Josiane%20Mothe%0AAbstract%3A%20%20%20Mixup%20has%20become%20a%20popular%20augmentation%20strategy%20for%20image%20classification%2C%0Ayet%20its%20naive%20pixel-wise%20interpolation%20often%20produces%20unrealistic%20images%20that%0Acan%20hinder%20learning%2C%20particularly%20in%20high-stakes%20medical%20applications.%20We%0Apropose%20GeMix%2C%20a%20two-stage%20framework%20that%20replaces%20heuristic%20blending%20with%20a%0Alearned%2C%20label-aware%20interpolation%20powered%20by%20class-conditional%20GANs.%20First%2C%20a%0AStyleGAN2-ADA%20generator%20is%20trained%20on%20the%20target%20dataset.%20During%20augmentation%2C%0Awe%20sample%20two%20label%20vectors%20from%20Dirichlet%20priors%20biased%20toward%20different%0Aclasses%20and%20blend%20them%20via%20a%20Beta-distributed%20coefficient.%20Then%2C%20we%20condition%0Athe%20generator%20on%20this%20soft%20label%20to%20synthesize%20visually%20coherent%20images%20that%0Alie%20along%20a%20continuous%20class%20manifold.%20We%20benchmark%20GeMix%20on%20the%20large-scale%0ACOVIDx-CT-3%20dataset%20using%20three%20backbones%20%28ResNet-50%2C%20ResNet-101%2C%0AEfficientNet-B0%29.%20When%20combined%20with%20real%20data%2C%20our%20method%20increases%20macro-F1%0Aover%20traditional%20mixup%20for%20all%20backbones%2C%20reducing%20the%20false%20negative%20rate%20for%0ACOVID-19%20detection.%20GeMix%20is%20thus%20a%20drop-in%20replacement%20for%20pixel-space%20mixup%2C%0Adelivering%20stronger%20regularization%20and%20greater%20semantic%20fidelity%2C%20without%0Adisrupting%20existing%20training%20pipelines.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/hugocarlesso/GeMix%20to%20foster%20reproducibility%20and%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeMix%253A%2520Conditional%2520GAN-Based%2520Mixup%2520for%2520Improved%2520Medical%2520Image%250A%2520%2520Augmentation%26entry.906535625%3DHugo%2520Carlesso%2520and%2520Maria%2520Eliza%2520Patulea%2520and%2520Moncef%2520Garouani%2520and%2520Radu%2520Tudor%2520Ionescu%2520and%2520Josiane%2520Mothe%26entry.1292438233%3D%2520%2520Mixup%2520has%2520become%2520a%2520popular%2520augmentation%2520strategy%2520for%2520image%2520classification%252C%250Ayet%2520its%2520naive%2520pixel-wise%2520interpolation%2520often%2520produces%2520unrealistic%2520images%2520that%250Acan%2520hinder%2520learning%252C%2520particularly%2520in%2520high-stakes%2520medical%2520applications.%2520We%250Apropose%2520GeMix%252C%2520a%2520two-stage%2520framework%2520that%2520replaces%2520heuristic%2520blending%2520with%2520a%250Alearned%252C%2520label-aware%2520interpolation%2520powered%2520by%2520class-conditional%2520GANs.%2520First%252C%2520a%250AStyleGAN2-ADA%2520generator%2520is%2520trained%2520on%2520the%2520target%2520dataset.%2520During%2520augmentation%252C%250Awe%2520sample%2520two%2520label%2520vectors%2520from%2520Dirichlet%2520priors%2520biased%2520toward%2520different%250Aclasses%2520and%2520blend%2520them%2520via%2520a%2520Beta-distributed%2520coefficient.%2520Then%252C%2520we%2520condition%250Athe%2520generator%2520on%2520this%2520soft%2520label%2520to%2520synthesize%2520visually%2520coherent%2520images%2520that%250Alie%2520along%2520a%2520continuous%2520class%2520manifold.%2520We%2520benchmark%2520GeMix%2520on%2520the%2520large-scale%250ACOVIDx-CT-3%2520dataset%2520using%2520three%2520backbones%2520%2528ResNet-50%252C%2520ResNet-101%252C%250AEfficientNet-B0%2529.%2520When%2520combined%2520with%2520real%2520data%252C%2520our%2520method%2520increases%2520macro-F1%250Aover%2520traditional%2520mixup%2520for%2520all%2520backbones%252C%2520reducing%2520the%2520false%2520negative%2520rate%2520for%250ACOVID-19%2520detection.%2520GeMix%2520is%2520thus%2520a%2520drop-in%2520replacement%2520for%2520pixel-space%2520mixup%252C%250Adelivering%2520stronger%2520regularization%2520and%2520greater%2520semantic%2520fidelity%252C%2520without%250Adisrupting%2520existing%2520training%2520pipelines.%2520We%2520publicly%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/hugocarlesso/GeMix%2520to%2520foster%2520reproducibility%2520and%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeMix%3A%20Conditional%20GAN-Based%20Mixup%20for%20Improved%20Medical%20Image%0A%20%20Augmentation&entry.906535625=Hugo%20Carlesso%20and%20Maria%20Eliza%20Patulea%20and%20Moncef%20Garouani%20and%20Radu%20Tudor%20Ionescu%20and%20Josiane%20Mothe&entry.1292438233=%20%20Mixup%20has%20become%20a%20popular%20augmentation%20strategy%20for%20image%20classification%2C%0Ayet%20its%20naive%20pixel-wise%20interpolation%20often%20produces%20unrealistic%20images%20that%0Acan%20hinder%20learning%2C%20particularly%20in%20high-stakes%20medical%20applications.%20We%0Apropose%20GeMix%2C%20a%20two-stage%20framework%20that%20replaces%20heuristic%20blending%20with%20a%0Alearned%2C%20label-aware%20interpolation%20powered%20by%20class-conditional%20GANs.%20First%2C%20a%0AStyleGAN2-ADA%20generator%20is%20trained%20on%20the%20target%20dataset.%20During%20augmentation%2C%0Awe%20sample%20two%20label%20vectors%20from%20Dirichlet%20priors%20biased%20toward%20different%0Aclasses%20and%20blend%20them%20via%20a%20Beta-distributed%20coefficient.%20Then%2C%20we%20condition%0Athe%20generator%20on%20this%20soft%20label%20to%20synthesize%20visually%20coherent%20images%20that%0Alie%20along%20a%20continuous%20class%20manifold.%20We%20benchmark%20GeMix%20on%20the%20large-scale%0ACOVIDx-CT-3%20dataset%20using%20three%20backbones%20%28ResNet-50%2C%20ResNet-101%2C%0AEfficientNet-B0%29.%20When%20combined%20with%20real%20data%2C%20our%20method%20increases%20macro-F1%0Aover%20traditional%20mixup%20for%20all%20backbones%2C%20reducing%20the%20false%20negative%20rate%20for%0ACOVID-19%20detection.%20GeMix%20is%20thus%20a%20drop-in%20replacement%20for%20pixel-space%20mixup%2C%0Adelivering%20stronger%20regularization%20and%20greater%20semantic%20fidelity%2C%20without%0Adisrupting%20existing%20training%20pipelines.%20We%20publicly%20release%20our%20code%20at%0Ahttps%3A//github.com/hugocarlesso/GeMix%20to%20foster%20reproducibility%20and%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15577v1&entry.124074799=Read"},
{"title": "Multi-Strategy Improved Snake Optimizer Accelerated\n  CNN-LSTM-Attention-Adaboost for Trajectory Prediction", "author": "Shiyang Li", "abstract": "  To address the limitations of medium- and long-term four-dimensional (4D)\ntrajectory prediction models, this paper proposes a hybrid\nCNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy\nimproved snake-herd optimization (SO) algorithm. The model applies the Adaboost\nalgorithm to divide multiple weak learners, and each submodel utilizes CNN to\nextract spatial features, LSTM to capture temporal features, and attention\nmechanism to capture global features comprehensively. The strong learner model,\ncombined with multiple sub-models, then optimizes the hyperparameters of the\nprediction model through the natural selection behavior pattern simulated by\nSO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the\ncomparison experiments and ablation studies of multiple optimizers are carried\nout, and a comprehensive test and evaluation analysis is carried out. The\nresults show that SO-CLA-adaboost outperforms traditional optimizers such as\nparticle swarm, whale, and gray wolf in handling large-scale high-dimensional\ntrajectory data. In addition, introducing the full-strategy collaborative\nimprovement SO algorithm improves the model's prediction accuracy by 39.89%.\n", "link": "http://arxiv.org/abs/2507.15832v1", "date": "2025-07-21", "relevancy": 2.0611, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5336}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Strategy%20Improved%20Snake%20Optimizer%20Accelerated%0A%20%20CNN-LSTM-Attention-Adaboost%20for%20Trajectory%20Prediction&body=Title%3A%20Multi-Strategy%20Improved%20Snake%20Optimizer%20Accelerated%0A%20%20CNN-LSTM-Attention-Adaboost%20for%20Trajectory%20Prediction%0AAuthor%3A%20Shiyang%20Li%0AAbstract%3A%20%20%20To%20address%20the%20limitations%20of%20medium-%20and%20long-term%20four-dimensional%20%284D%29%0Atrajectory%20prediction%20models%2C%20this%20paper%20proposes%20a%20hybrid%0ACNN-LSTM-attention-adaboost%20neural%20network%20model%20incorporating%20a%20multi-strategy%0Aimproved%20snake-herd%20optimization%20%28SO%29%20algorithm.%20The%20model%20applies%20the%20Adaboost%0Aalgorithm%20to%20divide%20multiple%20weak%20learners%2C%20and%20each%20submodel%20utilizes%20CNN%20to%0Aextract%20spatial%20features%2C%20LSTM%20to%20capture%20temporal%20features%2C%20and%20attention%0Amechanism%20to%20capture%20global%20features%20comprehensively.%20The%20strong%20learner%20model%2C%0Acombined%20with%20multiple%20sub-models%2C%20then%20optimizes%20the%20hyperparameters%20of%20the%0Aprediction%20model%20through%20the%20natural%20selection%20behavior%20pattern%20simulated%20by%0ASO.%20In%20this%20study%2C%20based%20on%20the%20real%20ADS-B%20data%20from%20Xi%27an%20to%20Tianjin%2C%20the%0Acomparison%20experiments%20and%20ablation%20studies%20of%20multiple%20optimizers%20are%20carried%0Aout%2C%20and%20a%20comprehensive%20test%20and%20evaluation%20analysis%20is%20carried%20out.%20The%0Aresults%20show%20that%20SO-CLA-adaboost%20outperforms%20traditional%20optimizers%20such%20as%0Aparticle%20swarm%2C%20whale%2C%20and%20gray%20wolf%20in%20handling%20large-scale%20high-dimensional%0Atrajectory%20data.%20In%20addition%2C%20introducing%20the%20full-strategy%20collaborative%0Aimprovement%20SO%20algorithm%20improves%20the%20model%27s%20prediction%20accuracy%20by%2039.89%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Strategy%2520Improved%2520Snake%2520Optimizer%2520Accelerated%250A%2520%2520CNN-LSTM-Attention-Adaboost%2520for%2520Trajectory%2520Prediction%26entry.906535625%3DShiyang%2520Li%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520limitations%2520of%2520medium-%2520and%2520long-term%2520four-dimensional%2520%25284D%2529%250Atrajectory%2520prediction%2520models%252C%2520this%2520paper%2520proposes%2520a%2520hybrid%250ACNN-LSTM-attention-adaboost%2520neural%2520network%2520model%2520incorporating%2520a%2520multi-strategy%250Aimproved%2520snake-herd%2520optimization%2520%2528SO%2529%2520algorithm.%2520The%2520model%2520applies%2520the%2520Adaboost%250Aalgorithm%2520to%2520divide%2520multiple%2520weak%2520learners%252C%2520and%2520each%2520submodel%2520utilizes%2520CNN%2520to%250Aextract%2520spatial%2520features%252C%2520LSTM%2520to%2520capture%2520temporal%2520features%252C%2520and%2520attention%250Amechanism%2520to%2520capture%2520global%2520features%2520comprehensively.%2520The%2520strong%2520learner%2520model%252C%250Acombined%2520with%2520multiple%2520sub-models%252C%2520then%2520optimizes%2520the%2520hyperparameters%2520of%2520the%250Aprediction%2520model%2520through%2520the%2520natural%2520selection%2520behavior%2520pattern%2520simulated%2520by%250ASO.%2520In%2520this%2520study%252C%2520based%2520on%2520the%2520real%2520ADS-B%2520data%2520from%2520Xi%2527an%2520to%2520Tianjin%252C%2520the%250Acomparison%2520experiments%2520and%2520ablation%2520studies%2520of%2520multiple%2520optimizers%2520are%2520carried%250Aout%252C%2520and%2520a%2520comprehensive%2520test%2520and%2520evaluation%2520analysis%2520is%2520carried%2520out.%2520The%250Aresults%2520show%2520that%2520SO-CLA-adaboost%2520outperforms%2520traditional%2520optimizers%2520such%2520as%250Aparticle%2520swarm%252C%2520whale%252C%2520and%2520gray%2520wolf%2520in%2520handling%2520large-scale%2520high-dimensional%250Atrajectory%2520data.%2520In%2520addition%252C%2520introducing%2520the%2520full-strategy%2520collaborative%250Aimprovement%2520SO%2520algorithm%2520improves%2520the%2520model%2527s%2520prediction%2520accuracy%2520by%252039.89%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Strategy%20Improved%20Snake%20Optimizer%20Accelerated%0A%20%20CNN-LSTM-Attention-Adaboost%20for%20Trajectory%20Prediction&entry.906535625=Shiyang%20Li&entry.1292438233=%20%20To%20address%20the%20limitations%20of%20medium-%20and%20long-term%20four-dimensional%20%284D%29%0Atrajectory%20prediction%20models%2C%20this%20paper%20proposes%20a%20hybrid%0ACNN-LSTM-attention-adaboost%20neural%20network%20model%20incorporating%20a%20multi-strategy%0Aimproved%20snake-herd%20optimization%20%28SO%29%20algorithm.%20The%20model%20applies%20the%20Adaboost%0Aalgorithm%20to%20divide%20multiple%20weak%20learners%2C%20and%20each%20submodel%20utilizes%20CNN%20to%0Aextract%20spatial%20features%2C%20LSTM%20to%20capture%20temporal%20features%2C%20and%20attention%0Amechanism%20to%20capture%20global%20features%20comprehensively.%20The%20strong%20learner%20model%2C%0Acombined%20with%20multiple%20sub-models%2C%20then%20optimizes%20the%20hyperparameters%20of%20the%0Aprediction%20model%20through%20the%20natural%20selection%20behavior%20pattern%20simulated%20by%0ASO.%20In%20this%20study%2C%20based%20on%20the%20real%20ADS-B%20data%20from%20Xi%27an%20to%20Tianjin%2C%20the%0Acomparison%20experiments%20and%20ablation%20studies%20of%20multiple%20optimizers%20are%20carried%0Aout%2C%20and%20a%20comprehensive%20test%20and%20evaluation%20analysis%20is%20carried%20out.%20The%0Aresults%20show%20that%20SO-CLA-adaboost%20outperforms%20traditional%20optimizers%20such%20as%0Aparticle%20swarm%2C%20whale%2C%20and%20gray%20wolf%20in%20handling%20large-scale%20high-dimensional%0Atrajectory%20data.%20In%20addition%2C%20introducing%20the%20full-strategy%20collaborative%0Aimprovement%20SO%20algorithm%20improves%20the%20model%27s%20prediction%20accuracy%20by%2039.89%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15832v1&entry.124074799=Read"},
{"title": "PhenoBench: A Comprehensive Benchmark for Cell Phenotyping", "author": "Fabian H. Reith and Claudia Winklmayr and Jerome Luescher and Nora Koreuber and Jannik Franzen and Elias Baumann and Christian M. Schuerch and Dagmar Kainmueller and Josef Lorenz Rumberger", "abstract": "  Digital pathology has seen the advent of a wealth of foundational models\n(FM), yet to date their performance on cell phenotyping has not been\nbenchmarked in a unified manner. We therefore propose PhenoBench: A\ncomprehensive benchmark for cell phenotyping on Hematoxylin and Eosin (H&E)\nstained histopathology images. We provide both PhenoCell, a new H&E dataset\nfeaturing 14 granular cell types identified by using multiplexed imaging, and\nready-to-use fine-tuning and benchmarking code that allows the systematic\nevaluation of multiple prominent pathology FMs in terms of dense cell phenotype\npredictions in different generalization scenarios. We perform extensive\nbenchmarking of existing FMs, providing insights into their generalization\nbehavior under technical vs. medical domain shifts. Furthermore, while FMs\nachieve macro F1 scores > 0.70 on previously established benchmarks such as\nLizard and PanNuke, on PhenoCell, we observe scores as low as 0.20. This\nindicates a much more challenging task not captured by previous benchmarks,\nestablishing PhenoCell as a prime asset for future benchmarking of FMs and\nsupervised models alike. Code and data are available on GitHub.\n", "link": "http://arxiv.org/abs/2507.03532v5", "date": "2025-07-21", "relevancy": 2.0602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4244}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhenoBench%3A%20A%20Comprehensive%20Benchmark%20for%20Cell%20Phenotyping&body=Title%3A%20PhenoBench%3A%20A%20Comprehensive%20Benchmark%20for%20Cell%20Phenotyping%0AAuthor%3A%20Fabian%20H.%20Reith%20and%20Claudia%20Winklmayr%20and%20Jerome%20Luescher%20and%20Nora%20Koreuber%20and%20Jannik%20Franzen%20and%20Elias%20Baumann%20and%20Christian%20M.%20Schuerch%20and%20Dagmar%20Kainmueller%20and%20Josef%20Lorenz%20Rumberger%0AAbstract%3A%20%20%20Digital%20pathology%20has%20seen%20the%20advent%20of%20a%20wealth%20of%20foundational%20models%0A%28FM%29%2C%20yet%20to%20date%20their%20performance%20on%20cell%20phenotyping%20has%20not%20been%0Abenchmarked%20in%20a%20unified%20manner.%20We%20therefore%20propose%20PhenoBench%3A%20A%0Acomprehensive%20benchmark%20for%20cell%20phenotyping%20on%20Hematoxylin%20and%20Eosin%20%28H%26E%29%0Astained%20histopathology%20images.%20We%20provide%20both%20PhenoCell%2C%20a%20new%20H%26E%20dataset%0Afeaturing%2014%20granular%20cell%20types%20identified%20by%20using%20multiplexed%20imaging%2C%20and%0Aready-to-use%20fine-tuning%20and%20benchmarking%20code%20that%20allows%20the%20systematic%0Aevaluation%20of%20multiple%20prominent%20pathology%20FMs%20in%20terms%20of%20dense%20cell%20phenotype%0Apredictions%20in%20different%20generalization%20scenarios.%20We%20perform%20extensive%0Abenchmarking%20of%20existing%20FMs%2C%20providing%20insights%20into%20their%20generalization%0Abehavior%20under%20technical%20vs.%20medical%20domain%20shifts.%20Furthermore%2C%20while%20FMs%0Aachieve%20macro%20F1%20scores%20%3E%200.70%20on%20previously%20established%20benchmarks%20such%20as%0ALizard%20and%20PanNuke%2C%20on%20PhenoCell%2C%20we%20observe%20scores%20as%20low%20as%200.20.%20This%0Aindicates%20a%20much%20more%20challenging%20task%20not%20captured%20by%20previous%20benchmarks%2C%0Aestablishing%20PhenoCell%20as%20a%20prime%20asset%20for%20future%20benchmarking%20of%20FMs%20and%0Asupervised%20models%20alike.%20Code%20and%20data%20are%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03532v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhenoBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Cell%2520Phenotyping%26entry.906535625%3DFabian%2520H.%2520Reith%2520and%2520Claudia%2520Winklmayr%2520and%2520Jerome%2520Luescher%2520and%2520Nora%2520Koreuber%2520and%2520Jannik%2520Franzen%2520and%2520Elias%2520Baumann%2520and%2520Christian%2520M.%2520Schuerch%2520and%2520Dagmar%2520Kainmueller%2520and%2520Josef%2520Lorenz%2520Rumberger%26entry.1292438233%3D%2520%2520Digital%2520pathology%2520has%2520seen%2520the%2520advent%2520of%2520a%2520wealth%2520of%2520foundational%2520models%250A%2528FM%2529%252C%2520yet%2520to%2520date%2520their%2520performance%2520on%2520cell%2520phenotyping%2520has%2520not%2520been%250Abenchmarked%2520in%2520a%2520unified%2520manner.%2520We%2520therefore%2520propose%2520PhenoBench%253A%2520A%250Acomprehensive%2520benchmark%2520for%2520cell%2520phenotyping%2520on%2520Hematoxylin%2520and%2520Eosin%2520%2528H%2526E%2529%250Astained%2520histopathology%2520images.%2520We%2520provide%2520both%2520PhenoCell%252C%2520a%2520new%2520H%2526E%2520dataset%250Afeaturing%252014%2520granular%2520cell%2520types%2520identified%2520by%2520using%2520multiplexed%2520imaging%252C%2520and%250Aready-to-use%2520fine-tuning%2520and%2520benchmarking%2520code%2520that%2520allows%2520the%2520systematic%250Aevaluation%2520of%2520multiple%2520prominent%2520pathology%2520FMs%2520in%2520terms%2520of%2520dense%2520cell%2520phenotype%250Apredictions%2520in%2520different%2520generalization%2520scenarios.%2520We%2520perform%2520extensive%250Abenchmarking%2520of%2520existing%2520FMs%252C%2520providing%2520insights%2520into%2520their%2520generalization%250Abehavior%2520under%2520technical%2520vs.%2520medical%2520domain%2520shifts.%2520Furthermore%252C%2520while%2520FMs%250Aachieve%2520macro%2520F1%2520scores%2520%253E%25200.70%2520on%2520previously%2520established%2520benchmarks%2520such%2520as%250ALizard%2520and%2520PanNuke%252C%2520on%2520PhenoCell%252C%2520we%2520observe%2520scores%2520as%2520low%2520as%25200.20.%2520This%250Aindicates%2520a%2520much%2520more%2520challenging%2520task%2520not%2520captured%2520by%2520previous%2520benchmarks%252C%250Aestablishing%2520PhenoCell%2520as%2520a%2520prime%2520asset%2520for%2520future%2520benchmarking%2520of%2520FMs%2520and%250Asupervised%2520models%2520alike.%2520Code%2520and%2520data%2520are%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03532v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhenoBench%3A%20A%20Comprehensive%20Benchmark%20for%20Cell%20Phenotyping&entry.906535625=Fabian%20H.%20Reith%20and%20Claudia%20Winklmayr%20and%20Jerome%20Luescher%20and%20Nora%20Koreuber%20and%20Jannik%20Franzen%20and%20Elias%20Baumann%20and%20Christian%20M.%20Schuerch%20and%20Dagmar%20Kainmueller%20and%20Josef%20Lorenz%20Rumberger&entry.1292438233=%20%20Digital%20pathology%20has%20seen%20the%20advent%20of%20a%20wealth%20of%20foundational%20models%0A%28FM%29%2C%20yet%20to%20date%20their%20performance%20on%20cell%20phenotyping%20has%20not%20been%0Abenchmarked%20in%20a%20unified%20manner.%20We%20therefore%20propose%20PhenoBench%3A%20A%0Acomprehensive%20benchmark%20for%20cell%20phenotyping%20on%20Hematoxylin%20and%20Eosin%20%28H%26E%29%0Astained%20histopathology%20images.%20We%20provide%20both%20PhenoCell%2C%20a%20new%20H%26E%20dataset%0Afeaturing%2014%20granular%20cell%20types%20identified%20by%20using%20multiplexed%20imaging%2C%20and%0Aready-to-use%20fine-tuning%20and%20benchmarking%20code%20that%20allows%20the%20systematic%0Aevaluation%20of%20multiple%20prominent%20pathology%20FMs%20in%20terms%20of%20dense%20cell%20phenotype%0Apredictions%20in%20different%20generalization%20scenarios.%20We%20perform%20extensive%0Abenchmarking%20of%20existing%20FMs%2C%20providing%20insights%20into%20their%20generalization%0Abehavior%20under%20technical%20vs.%20medical%20domain%20shifts.%20Furthermore%2C%20while%20FMs%0Aachieve%20macro%20F1%20scores%20%3E%200.70%20on%20previously%20established%20benchmarks%20such%20as%0ALizard%20and%20PanNuke%2C%20on%20PhenoCell%2C%20we%20observe%20scores%20as%20low%20as%200.20.%20This%0Aindicates%20a%20much%20more%20challenging%20task%20not%20captured%20by%20previous%20benchmarks%2C%0Aestablishing%20PhenoCell%20as%20a%20prime%20asset%20for%20future%20benchmarking%20of%20FMs%20and%0Asupervised%20models%20alike.%20Code%20and%20data%20are%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03532v5&entry.124074799=Read"},
{"title": "Reciprocity-Aware Convolutional Neural Networks for Map-Based Path Loss\n  Prediction", "author": "Ryan G. Dempsey and Jonathan Ethier and Halim Yanikomeroglu", "abstract": "  Path loss modeling is a widely used technique for estimating point-to-point\nlosses along a communications link from transmitter (Tx) to receiver (Rx).\nAccurate path loss predictions can optimize use of the radio frequency spectrum\nand minimize unwanted interference. Modern path loss modeling often leverages\ndata-driven approaches, using machine learning to train models on drive test\nmeasurement datasets. Drive tests primarily represent downlink scenarios, where\nthe Tx is located on a building and the Rx is located on a moving vehicle.\nConsequently, trained models are frequently reserved for downlink coverage\nestimation, lacking representation of uplink scenarios. In this paper, we\ndemonstrate that data augmentation can be used to train a path loss model that\nis generalized to uplink, downlink, and backhaul scenarios, training using only\ndownlink drive test measurements. By adding a small number of synthetic samples\nrepresenting uplink scenarios to the training set, root mean squared error is\nreduced by > 8 dB on uplink examples in the test set.\n", "link": "http://arxiv.org/abs/2504.03625v2", "date": "2025-07-21", "relevancy": 2.0413, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reciprocity-Aware%20Convolutional%20Neural%20Networks%20for%20Map-Based%20Path%20Loss%0A%20%20Prediction&body=Title%3A%20Reciprocity-Aware%20Convolutional%20Neural%20Networks%20for%20Map-Based%20Path%20Loss%0A%20%20Prediction%0AAuthor%3A%20Ryan%20G.%20Dempsey%20and%20Jonathan%20Ethier%20and%20Halim%20Yanikomeroglu%0AAbstract%3A%20%20%20Path%20loss%20modeling%20is%20a%20widely%20used%20technique%20for%20estimating%20point-to-point%0Alosses%20along%20a%20communications%20link%20from%20transmitter%20%28Tx%29%20to%20receiver%20%28Rx%29.%0AAccurate%20path%20loss%20predictions%20can%20optimize%20use%20of%20the%20radio%20frequency%20spectrum%0Aand%20minimize%20unwanted%20interference.%20Modern%20path%20loss%20modeling%20often%20leverages%0Adata-driven%20approaches%2C%20using%20machine%20learning%20to%20train%20models%20on%20drive%20test%0Ameasurement%20datasets.%20Drive%20tests%20primarily%20represent%20downlink%20scenarios%2C%20where%0Athe%20Tx%20is%20located%20on%20a%20building%20and%20the%20Rx%20is%20located%20on%20a%20moving%20vehicle.%0AConsequently%2C%20trained%20models%20are%20frequently%20reserved%20for%20downlink%20coverage%0Aestimation%2C%20lacking%20representation%20of%20uplink%20scenarios.%20In%20this%20paper%2C%20we%0Ademonstrate%20that%20data%20augmentation%20can%20be%20used%20to%20train%20a%20path%20loss%20model%20that%0Ais%20generalized%20to%20uplink%2C%20downlink%2C%20and%20backhaul%20scenarios%2C%20training%20using%20only%0Adownlink%20drive%20test%20measurements.%20By%20adding%20a%20small%20number%20of%20synthetic%20samples%0Arepresenting%20uplink%20scenarios%20to%20the%20training%20set%2C%20root%20mean%20squared%20error%20is%0Areduced%20by%20%3E%208%20dB%20on%20uplink%20examples%20in%20the%20test%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReciprocity-Aware%2520Convolutional%2520Neural%2520Networks%2520for%2520Map-Based%2520Path%2520Loss%250A%2520%2520Prediction%26entry.906535625%3DRyan%2520G.%2520Dempsey%2520and%2520Jonathan%2520Ethier%2520and%2520Halim%2520Yanikomeroglu%26entry.1292438233%3D%2520%2520Path%2520loss%2520modeling%2520is%2520a%2520widely%2520used%2520technique%2520for%2520estimating%2520point-to-point%250Alosses%2520along%2520a%2520communications%2520link%2520from%2520transmitter%2520%2528Tx%2529%2520to%2520receiver%2520%2528Rx%2529.%250AAccurate%2520path%2520loss%2520predictions%2520can%2520optimize%2520use%2520of%2520the%2520radio%2520frequency%2520spectrum%250Aand%2520minimize%2520unwanted%2520interference.%2520Modern%2520path%2520loss%2520modeling%2520often%2520leverages%250Adata-driven%2520approaches%252C%2520using%2520machine%2520learning%2520to%2520train%2520models%2520on%2520drive%2520test%250Ameasurement%2520datasets.%2520Drive%2520tests%2520primarily%2520represent%2520downlink%2520scenarios%252C%2520where%250Athe%2520Tx%2520is%2520located%2520on%2520a%2520building%2520and%2520the%2520Rx%2520is%2520located%2520on%2520a%2520moving%2520vehicle.%250AConsequently%252C%2520trained%2520models%2520are%2520frequently%2520reserved%2520for%2520downlink%2520coverage%250Aestimation%252C%2520lacking%2520representation%2520of%2520uplink%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Ademonstrate%2520that%2520data%2520augmentation%2520can%2520be%2520used%2520to%2520train%2520a%2520path%2520loss%2520model%2520that%250Ais%2520generalized%2520to%2520uplink%252C%2520downlink%252C%2520and%2520backhaul%2520scenarios%252C%2520training%2520using%2520only%250Adownlink%2520drive%2520test%2520measurements.%2520By%2520adding%2520a%2520small%2520number%2520of%2520synthetic%2520samples%250Arepresenting%2520uplink%2520scenarios%2520to%2520the%2520training%2520set%252C%2520root%2520mean%2520squared%2520error%2520is%250Areduced%2520by%2520%253E%25208%2520dB%2520on%2520uplink%2520examples%2520in%2520the%2520test%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reciprocity-Aware%20Convolutional%20Neural%20Networks%20for%20Map-Based%20Path%20Loss%0A%20%20Prediction&entry.906535625=Ryan%20G.%20Dempsey%20and%20Jonathan%20Ethier%20and%20Halim%20Yanikomeroglu&entry.1292438233=%20%20Path%20loss%20modeling%20is%20a%20widely%20used%20technique%20for%20estimating%20point-to-point%0Alosses%20along%20a%20communications%20link%20from%20transmitter%20%28Tx%29%20to%20receiver%20%28Rx%29.%0AAccurate%20path%20loss%20predictions%20can%20optimize%20use%20of%20the%20radio%20frequency%20spectrum%0Aand%20minimize%20unwanted%20interference.%20Modern%20path%20loss%20modeling%20often%20leverages%0Adata-driven%20approaches%2C%20using%20machine%20learning%20to%20train%20models%20on%20drive%20test%0Ameasurement%20datasets.%20Drive%20tests%20primarily%20represent%20downlink%20scenarios%2C%20where%0Athe%20Tx%20is%20located%20on%20a%20building%20and%20the%20Rx%20is%20located%20on%20a%20moving%20vehicle.%0AConsequently%2C%20trained%20models%20are%20frequently%20reserved%20for%20downlink%20coverage%0Aestimation%2C%20lacking%20representation%20of%20uplink%20scenarios.%20In%20this%20paper%2C%20we%0Ademonstrate%20that%20data%20augmentation%20can%20be%20used%20to%20train%20a%20path%20loss%20model%20that%0Ais%20generalized%20to%20uplink%2C%20downlink%2C%20and%20backhaul%20scenarios%2C%20training%20using%20only%0Adownlink%20drive%20test%20measurements.%20By%20adding%20a%20small%20number%20of%20synthetic%20samples%0Arepresenting%20uplink%20scenarios%20to%20the%20training%20set%2C%20root%20mean%20squared%20error%20is%0Areduced%20by%20%3E%208%20dB%20on%20uplink%20examples%20in%20the%20test%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03625v2&entry.124074799=Read"},
{"title": "A Study of LLMs' Preferences for Libraries and Programming Languages", "author": "Lukas Twist and Jie M. Zhang and Mark Harman and Don Syme and Joost Noppen and Helen Yannakoudakis and Detlef Nauck", "abstract": "  Large Language Models (LLMs) are increasingly used to generate code,\ninfluencing users' choices of libraries and programming languages in critical\nreal-world projects. However, little is known about their systematic biases or\npreferences toward certain libraries and programming languages, which can\nsignificantly impact software development practices. To fill this gap, we\nperform the first empirical study of LLMs' preferences for libraries and\nprogramming languages when generating code, covering eight diverse LLMs. Our\nresults reveal that LLMs exhibit a strong tendency to overuse widely adopted\nlibraries such as NumPy; in up to 48% of cases, this usage is unnecessary and\ndeviates from the ground-truth solutions. LLMs also exhibit a significant\npreference toward Python as their default language. For high-performance\nproject initialisation tasks where Python is not the optimal language, it\nremains the dominant choice in 58% of cases, and Rust is not used a single\ntime. These results indicate that LLMs may prioritise familiarity and\npopularity over suitability and task-specific optimality. This will introduce\nsecurity vulnerabilities and technical debt, and limit exposure to newly\ndeveloped, better-suited tools and languages. Understanding and addressing\nthese biases is essential for the responsible integration of LLMs into software\ndevelopment workflows.\n", "link": "http://arxiv.org/abs/2503.17181v2", "date": "2025-07-21", "relevancy": 2.0398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4231}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20LLMs%27%20Preferences%20for%20Libraries%20and%20Programming%20Languages&body=Title%3A%20A%20Study%20of%20LLMs%27%20Preferences%20for%20Libraries%20and%20Programming%20Languages%0AAuthor%3A%20Lukas%20Twist%20and%20Jie%20M.%20Zhang%20and%20Mark%20Harman%20and%20Don%20Syme%20and%20Joost%20Noppen%20and%20Helen%20Yannakoudakis%20and%20Detlef%20Nauck%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20to%20generate%20code%2C%0Ainfluencing%20users%27%20choices%20of%20libraries%20and%20programming%20languages%20in%20critical%0Areal-world%20projects.%20However%2C%20little%20is%20known%20about%20their%20systematic%20biases%20or%0Apreferences%20toward%20certain%20libraries%20and%20programming%20languages%2C%20which%20can%0Asignificantly%20impact%20software%20development%20practices.%20To%20fill%20this%20gap%2C%20we%0Aperform%20the%20first%20empirical%20study%20of%20LLMs%27%20preferences%20for%20libraries%20and%0Aprogramming%20languages%20when%20generating%20code%2C%20covering%20eight%20diverse%20LLMs.%20Our%0Aresults%20reveal%20that%20LLMs%20exhibit%20a%20strong%20tendency%20to%20overuse%20widely%20adopted%0Alibraries%20such%20as%20NumPy%3B%20in%20up%20to%2048%25%20of%20cases%2C%20this%20usage%20is%20unnecessary%20and%0Adeviates%20from%20the%20ground-truth%20solutions.%20LLMs%20also%20exhibit%20a%20significant%0Apreference%20toward%20Python%20as%20their%20default%20language.%20For%20high-performance%0Aproject%20initialisation%20tasks%20where%20Python%20is%20not%20the%20optimal%20language%2C%20it%0Aremains%20the%20dominant%20choice%20in%2058%25%20of%20cases%2C%20and%20Rust%20is%20not%20used%20a%20single%0Atime.%20These%20results%20indicate%20that%20LLMs%20may%20prioritise%20familiarity%20and%0Apopularity%20over%20suitability%20and%20task-specific%20optimality.%20This%20will%20introduce%0Asecurity%20vulnerabilities%20and%20technical%20debt%2C%20and%20limit%20exposure%20to%20newly%0Adeveloped%2C%20better-suited%20tools%20and%20languages.%20Understanding%20and%20addressing%0Athese%20biases%20is%20essential%20for%20the%20responsible%20integration%20of%20LLMs%20into%20software%0Adevelopment%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17181v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520LLMs%2527%2520Preferences%2520for%2520Libraries%2520and%2520Programming%2520Languages%26entry.906535625%3DLukas%2520Twist%2520and%2520Jie%2520M.%2520Zhang%2520and%2520Mark%2520Harman%2520and%2520Don%2520Syme%2520and%2520Joost%2520Noppen%2520and%2520Helen%2520Yannakoudakis%2520and%2520Detlef%2520Nauck%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520to%2520generate%2520code%252C%250Ainfluencing%2520users%2527%2520choices%2520of%2520libraries%2520and%2520programming%2520languages%2520in%2520critical%250Areal-world%2520projects.%2520However%252C%2520little%2520is%2520known%2520about%2520their%2520systematic%2520biases%2520or%250Apreferences%2520toward%2520certain%2520libraries%2520and%2520programming%2520languages%252C%2520which%2520can%250Asignificantly%2520impact%2520software%2520development%2520practices.%2520To%2520fill%2520this%2520gap%252C%2520we%250Aperform%2520the%2520first%2520empirical%2520study%2520of%2520LLMs%2527%2520preferences%2520for%2520libraries%2520and%250Aprogramming%2520languages%2520when%2520generating%2520code%252C%2520covering%2520eight%2520diverse%2520LLMs.%2520Our%250Aresults%2520reveal%2520that%2520LLMs%2520exhibit%2520a%2520strong%2520tendency%2520to%2520overuse%2520widely%2520adopted%250Alibraries%2520such%2520as%2520NumPy%253B%2520in%2520up%2520to%252048%2525%2520of%2520cases%252C%2520this%2520usage%2520is%2520unnecessary%2520and%250Adeviates%2520from%2520the%2520ground-truth%2520solutions.%2520LLMs%2520also%2520exhibit%2520a%2520significant%250Apreference%2520toward%2520Python%2520as%2520their%2520default%2520language.%2520For%2520high-performance%250Aproject%2520initialisation%2520tasks%2520where%2520Python%2520is%2520not%2520the%2520optimal%2520language%252C%2520it%250Aremains%2520the%2520dominant%2520choice%2520in%252058%2525%2520of%2520cases%252C%2520and%2520Rust%2520is%2520not%2520used%2520a%2520single%250Atime.%2520These%2520results%2520indicate%2520that%2520LLMs%2520may%2520prioritise%2520familiarity%2520and%250Apopularity%2520over%2520suitability%2520and%2520task-specific%2520optimality.%2520This%2520will%2520introduce%250Asecurity%2520vulnerabilities%2520and%2520technical%2520debt%252C%2520and%2520limit%2520exposure%2520to%2520newly%250Adeveloped%252C%2520better-suited%2520tools%2520and%2520languages.%2520Understanding%2520and%2520addressing%250Athese%2520biases%2520is%2520essential%2520for%2520the%2520responsible%2520integration%2520of%2520LLMs%2520into%2520software%250Adevelopment%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17181v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20LLMs%27%20Preferences%20for%20Libraries%20and%20Programming%20Languages&entry.906535625=Lukas%20Twist%20and%20Jie%20M.%20Zhang%20and%20Mark%20Harman%20and%20Don%20Syme%20and%20Joost%20Noppen%20and%20Helen%20Yannakoudakis%20and%20Detlef%20Nauck&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20to%20generate%20code%2C%0Ainfluencing%20users%27%20choices%20of%20libraries%20and%20programming%20languages%20in%20critical%0Areal-world%20projects.%20However%2C%20little%20is%20known%20about%20their%20systematic%20biases%20or%0Apreferences%20toward%20certain%20libraries%20and%20programming%20languages%2C%20which%20can%0Asignificantly%20impact%20software%20development%20practices.%20To%20fill%20this%20gap%2C%20we%0Aperform%20the%20first%20empirical%20study%20of%20LLMs%27%20preferences%20for%20libraries%20and%0Aprogramming%20languages%20when%20generating%20code%2C%20covering%20eight%20diverse%20LLMs.%20Our%0Aresults%20reveal%20that%20LLMs%20exhibit%20a%20strong%20tendency%20to%20overuse%20widely%20adopted%0Alibraries%20such%20as%20NumPy%3B%20in%20up%20to%2048%25%20of%20cases%2C%20this%20usage%20is%20unnecessary%20and%0Adeviates%20from%20the%20ground-truth%20solutions.%20LLMs%20also%20exhibit%20a%20significant%0Apreference%20toward%20Python%20as%20their%20default%20language.%20For%20high-performance%0Aproject%20initialisation%20tasks%20where%20Python%20is%20not%20the%20optimal%20language%2C%20it%0Aremains%20the%20dominant%20choice%20in%2058%25%20of%20cases%2C%20and%20Rust%20is%20not%20used%20a%20single%0Atime.%20These%20results%20indicate%20that%20LLMs%20may%20prioritise%20familiarity%20and%0Apopularity%20over%20suitability%20and%20task-specific%20optimality.%20This%20will%20introduce%0Asecurity%20vulnerabilities%20and%20technical%20debt%2C%20and%20limit%20exposure%20to%20newly%0Adeveloped%2C%20better-suited%20tools%20and%20languages.%20Understanding%20and%20addressing%0Athese%20biases%20is%20essential%20for%20the%20responsible%20integration%20of%20LLMs%20into%20software%0Adevelopment%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17181v2&entry.124074799=Read"},
{"title": "Uncovering Critical Features for Deepfake Detection through the Lottery\n  Ticket Hypothesis", "author": "Lisan Al Amin and Md. Ismail Hossain and Thanh Thi Nguyen and Tasnim Jahan and Mahbubul Islam and Faisal Quader", "abstract": "  Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.\n", "link": "http://arxiv.org/abs/2507.15636v1", "date": "2025-07-21", "relevancy": 2.0397, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5184}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5165}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Critical%20Features%20for%20Deepfake%20Detection%20through%20the%20Lottery%0A%20%20Ticket%20Hypothesis&body=Title%3A%20Uncovering%20Critical%20Features%20for%20Deepfake%20Detection%20through%20the%20Lottery%0A%20%20Ticket%20Hypothesis%0AAuthor%3A%20Lisan%20Al%20Amin%20and%20Md.%20Ismail%20Hossain%20and%20Thanh%20Thi%20Nguyen%20and%20Tasnim%20Jahan%20and%20Mahbubul%20Islam%20and%20Faisal%20Quader%0AAbstract%3A%20%20%20Recent%20advances%20in%20deepfake%20technology%20have%20created%20increasingly%20convincing%0Asynthetic%20media%20that%20poses%20significant%20challenges%20to%20information%20integrity%20and%0Asocial%20trust.%20While%20current%20detection%20methods%20show%20promise%2C%20their%20underlying%0Amechanisms%20remain%20poorly%20understood%2C%20and%20the%20large%20sizes%20of%20their%20models%20make%0Athem%20challenging%20to%20deploy%20in%20resource-limited%20environments.%20This%20study%0Ainvestigates%20the%20application%20of%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%20to%20deepfake%0Adetection%2C%20aiming%20to%20identify%20the%20key%20features%20crucial%20for%20recognizing%0Adeepfakes.%20We%20examine%20how%20neural%20networks%20can%20be%20efficiently%20pruned%20while%0Amaintaining%20high%20detection%20accuracy.%20Through%20extensive%20experiments%20with%0AMesoNet%2C%20CNN-5%2C%20and%20ResNet-18%20architectures%20on%20the%20OpenForensic%20and%0AFaceForensics%2B%2B%20datasets%2C%20we%20find%20that%20deepfake%20detection%20networks%20contain%0Awinning%20tickets%2C%20i.e.%2C%20subnetworks%2C%20that%20preserve%20performance%20even%20at%0Asubstantial%20sparsity%20levels.%20Our%20results%20indicate%20that%20MesoNet%20retains%2056.2%25%0Aaccuracy%20at%2080%25%20sparsity%20on%20the%20OpenForensic%20dataset%2C%20with%20only%203%2C000%0Aparameters%2C%20which%20is%20about%2090%25%20of%20its%20baseline%20accuracy%20%2862.6%25%29.%20The%20results%0Aalso%20show%20that%20our%20proposed%20LTH-based%20iterative%20magnitude%20pruning%20approach%0Aconsistently%20outperforms%20one-shot%20pruning%20methods.%20Using%20Grad-CAM%0Avisualization%2C%20we%20analyze%20how%20pruned%20networks%20maintain%20their%20focus%20on%20critical%0Afacial%20regions%20for%20deepfake%20detection.%20Additionally%2C%20we%20demonstrate%20the%0Atransferability%20of%20winning%20tickets%20across%20datasets%2C%20suggesting%20potential%20for%0Aefficient%2C%20deployable%20deepfake%20detection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Critical%2520Features%2520for%2520Deepfake%2520Detection%2520through%2520the%2520Lottery%250A%2520%2520Ticket%2520Hypothesis%26entry.906535625%3DLisan%2520Al%2520Amin%2520and%2520Md.%2520Ismail%2520Hossain%2520and%2520Thanh%2520Thi%2520Nguyen%2520and%2520Tasnim%2520Jahan%2520and%2520Mahbubul%2520Islam%2520and%2520Faisal%2520Quader%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deepfake%2520technology%2520have%2520created%2520increasingly%2520convincing%250Asynthetic%2520media%2520that%2520poses%2520significant%2520challenges%2520to%2520information%2520integrity%2520and%250Asocial%2520trust.%2520While%2520current%2520detection%2520methods%2520show%2520promise%252C%2520their%2520underlying%250Amechanisms%2520remain%2520poorly%2520understood%252C%2520and%2520the%2520large%2520sizes%2520of%2520their%2520models%2520make%250Athem%2520challenging%2520to%2520deploy%2520in%2520resource-limited%2520environments.%2520This%2520study%250Ainvestigates%2520the%2520application%2520of%2520the%2520Lottery%2520Ticket%2520Hypothesis%2520%2528LTH%2529%2520to%2520deepfake%250Adetection%252C%2520aiming%2520to%2520identify%2520the%2520key%2520features%2520crucial%2520for%2520recognizing%250Adeepfakes.%2520We%2520examine%2520how%2520neural%2520networks%2520can%2520be%2520efficiently%2520pruned%2520while%250Amaintaining%2520high%2520detection%2520accuracy.%2520Through%2520extensive%2520experiments%2520with%250AMesoNet%252C%2520CNN-5%252C%2520and%2520ResNet-18%2520architectures%2520on%2520the%2520OpenForensic%2520and%250AFaceForensics%252B%252B%2520datasets%252C%2520we%2520find%2520that%2520deepfake%2520detection%2520networks%2520contain%250Awinning%2520tickets%252C%2520i.e.%252C%2520subnetworks%252C%2520that%2520preserve%2520performance%2520even%2520at%250Asubstantial%2520sparsity%2520levels.%2520Our%2520results%2520indicate%2520that%2520MesoNet%2520retains%252056.2%2525%250Aaccuracy%2520at%252080%2525%2520sparsity%2520on%2520the%2520OpenForensic%2520dataset%252C%2520with%2520only%25203%252C000%250Aparameters%252C%2520which%2520is%2520about%252090%2525%2520of%2520its%2520baseline%2520accuracy%2520%252862.6%2525%2529.%2520The%2520results%250Aalso%2520show%2520that%2520our%2520proposed%2520LTH-based%2520iterative%2520magnitude%2520pruning%2520approach%250Aconsistently%2520outperforms%2520one-shot%2520pruning%2520methods.%2520Using%2520Grad-CAM%250Avisualization%252C%2520we%2520analyze%2520how%2520pruned%2520networks%2520maintain%2520their%2520focus%2520on%2520critical%250Afacial%2520regions%2520for%2520deepfake%2520detection.%2520Additionally%252C%2520we%2520demonstrate%2520the%250Atransferability%2520of%2520winning%2520tickets%2520across%2520datasets%252C%2520suggesting%2520potential%2520for%250Aefficient%252C%2520deployable%2520deepfake%2520detection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Critical%20Features%20for%20Deepfake%20Detection%20through%20the%20Lottery%0A%20%20Ticket%20Hypothesis&entry.906535625=Lisan%20Al%20Amin%20and%20Md.%20Ismail%20Hossain%20and%20Thanh%20Thi%20Nguyen%20and%20Tasnim%20Jahan%20and%20Mahbubul%20Islam%20and%20Faisal%20Quader&entry.1292438233=%20%20Recent%20advances%20in%20deepfake%20technology%20have%20created%20increasingly%20convincing%0Asynthetic%20media%20that%20poses%20significant%20challenges%20to%20information%20integrity%20and%0Asocial%20trust.%20While%20current%20detection%20methods%20show%20promise%2C%20their%20underlying%0Amechanisms%20remain%20poorly%20understood%2C%20and%20the%20large%20sizes%20of%20their%20models%20make%0Athem%20challenging%20to%20deploy%20in%20resource-limited%20environments.%20This%20study%0Ainvestigates%20the%20application%20of%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%20to%20deepfake%0Adetection%2C%20aiming%20to%20identify%20the%20key%20features%20crucial%20for%20recognizing%0Adeepfakes.%20We%20examine%20how%20neural%20networks%20can%20be%20efficiently%20pruned%20while%0Amaintaining%20high%20detection%20accuracy.%20Through%20extensive%20experiments%20with%0AMesoNet%2C%20CNN-5%2C%20and%20ResNet-18%20architectures%20on%20the%20OpenForensic%20and%0AFaceForensics%2B%2B%20datasets%2C%20we%20find%20that%20deepfake%20detection%20networks%20contain%0Awinning%20tickets%2C%20i.e.%2C%20subnetworks%2C%20that%20preserve%20performance%20even%20at%0Asubstantial%20sparsity%20levels.%20Our%20results%20indicate%20that%20MesoNet%20retains%2056.2%25%0Aaccuracy%20at%2080%25%20sparsity%20on%20the%20OpenForensic%20dataset%2C%20with%20only%203%2C000%0Aparameters%2C%20which%20is%20about%2090%25%20of%20its%20baseline%20accuracy%20%2862.6%25%29.%20The%20results%0Aalso%20show%20that%20our%20proposed%20LTH-based%20iterative%20magnitude%20pruning%20approach%0Aconsistently%20outperforms%20one-shot%20pruning%20methods.%20Using%20Grad-CAM%0Avisualization%2C%20we%20analyze%20how%20pruned%20networks%20maintain%20their%20focus%20on%20critical%0Afacial%20regions%20for%20deepfake%20detection.%20Additionally%2C%20we%20demonstrate%20the%0Atransferability%20of%20winning%20tickets%20across%20datasets%2C%20suggesting%20potential%20for%0Aefficient%2C%20deployable%20deepfake%20detection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15636v1&entry.124074799=Read"},
{"title": "An Investigation of Test-time Adaptation for Audio Classification under\n  Background Noise", "author": "Weichuang Shao and Iman Yi Liao and Tomas Henrique Bode Maul and Tissa Chandesa", "abstract": "  Domain shift is a prominent problem in Deep Learning, causing a model\npre-trained on a source dataset to suffer significant performance degradation\non test datasets. This research aims to address the issue of audio\nclassification under domain shift caused by background noise using Test-Time\nAdaptation (TTA), a technique that adapts a pre-trained model during testing\nusing only unlabelled test data before making predictions. We adopt two common\nTTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and\ninvestigate their respective performance on two popular audio classification\ndatasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types\nof background noise and noise severity levels. The experimental results reveal\nthat our proposed modified version of CoNMix produced the highest\nclassification accuracy under domain shift (5.31% error rate under 10 dB\nexercise bike background noise and 12.75% error rate under 3 dB running tap\nbackground noise for AM) compared to TTT and TENT. The literature search\nprovided no evidence of similar works, thereby motivating the work reported\nhere as the first study to leverage TTA techniques for audio classification\nunder domain shift.\n", "link": "http://arxiv.org/abs/2507.15523v1", "date": "2025-07-21", "relevancy": 2.0387, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Investigation%20of%20Test-time%20Adaptation%20for%20Audio%20Classification%20under%0A%20%20Background%20Noise&body=Title%3A%20An%20Investigation%20of%20Test-time%20Adaptation%20for%20Audio%20Classification%20under%0A%20%20Background%20Noise%0AAuthor%3A%20Weichuang%20Shao%20and%20Iman%20Yi%20Liao%20and%20Tomas%20Henrique%20Bode%20Maul%20and%20Tissa%20Chandesa%0AAbstract%3A%20%20%20Domain%20shift%20is%20a%20prominent%20problem%20in%20Deep%20Learning%2C%20causing%20a%20model%0Apre-trained%20on%20a%20source%20dataset%20to%20suffer%20significant%20performance%20degradation%0Aon%20test%20datasets.%20This%20research%20aims%20to%20address%20the%20issue%20of%20audio%0Aclassification%20under%20domain%20shift%20caused%20by%20background%20noise%20using%20Test-Time%0AAdaptation%20%28TTA%29%2C%20a%20technique%20that%20adapts%20a%20pre-trained%20model%20during%20testing%0Ausing%20only%20unlabelled%20test%20data%20before%20making%20predictions.%20We%20adopt%20two%20common%0ATTA%20methods%2C%20TTT%20and%20TENT%2C%20and%20a%20state-of-the-art%20method%20CoNMix%2C%20and%0Ainvestigate%20their%20respective%20performance%20on%20two%20popular%20audio%20classification%0Adatasets%2C%20AudioMNIST%20%28AM%29%20and%20SpeechCommands%20V1%20%28SC%29%2C%20against%20different%20types%0Aof%20background%20noise%20and%20noise%20severity%20levels.%20The%20experimental%20results%20reveal%0Athat%20our%20proposed%20modified%20version%20of%20CoNMix%20produced%20the%20highest%0Aclassification%20accuracy%20under%20domain%20shift%20%285.31%25%20error%20rate%20under%2010%20dB%0Aexercise%20bike%20background%20noise%20and%2012.75%25%20error%20rate%20under%203%20dB%20running%20tap%0Abackground%20noise%20for%20AM%29%20compared%20to%20TTT%20and%20TENT.%20The%20literature%20search%0Aprovided%20no%20evidence%20of%20similar%20works%2C%20thereby%20motivating%20the%20work%20reported%0Ahere%20as%20the%20first%20study%20to%20leverage%20TTA%20techniques%20for%20audio%20classification%0Aunder%20domain%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Investigation%2520of%2520Test-time%2520Adaptation%2520for%2520Audio%2520Classification%2520under%250A%2520%2520Background%2520Noise%26entry.906535625%3DWeichuang%2520Shao%2520and%2520Iman%2520Yi%2520Liao%2520and%2520Tomas%2520Henrique%2520Bode%2520Maul%2520and%2520Tissa%2520Chandesa%26entry.1292438233%3D%2520%2520Domain%2520shift%2520is%2520a%2520prominent%2520problem%2520in%2520Deep%2520Learning%252C%2520causing%2520a%2520model%250Apre-trained%2520on%2520a%2520source%2520dataset%2520to%2520suffer%2520significant%2520performance%2520degradation%250Aon%2520test%2520datasets.%2520This%2520research%2520aims%2520to%2520address%2520the%2520issue%2520of%2520audio%250Aclassification%2520under%2520domain%2520shift%2520caused%2520by%2520background%2520noise%2520using%2520Test-Time%250AAdaptation%2520%2528TTA%2529%252C%2520a%2520technique%2520that%2520adapts%2520a%2520pre-trained%2520model%2520during%2520testing%250Ausing%2520only%2520unlabelled%2520test%2520data%2520before%2520making%2520predictions.%2520We%2520adopt%2520two%2520common%250ATTA%2520methods%252C%2520TTT%2520and%2520TENT%252C%2520and%2520a%2520state-of-the-art%2520method%2520CoNMix%252C%2520and%250Ainvestigate%2520their%2520respective%2520performance%2520on%2520two%2520popular%2520audio%2520classification%250Adatasets%252C%2520AudioMNIST%2520%2528AM%2529%2520and%2520SpeechCommands%2520V1%2520%2528SC%2529%252C%2520against%2520different%2520types%250Aof%2520background%2520noise%2520and%2520noise%2520severity%2520levels.%2520The%2520experimental%2520results%2520reveal%250Athat%2520our%2520proposed%2520modified%2520version%2520of%2520CoNMix%2520produced%2520the%2520highest%250Aclassification%2520accuracy%2520under%2520domain%2520shift%2520%25285.31%2525%2520error%2520rate%2520under%252010%2520dB%250Aexercise%2520bike%2520background%2520noise%2520and%252012.75%2525%2520error%2520rate%2520under%25203%2520dB%2520running%2520tap%250Abackground%2520noise%2520for%2520AM%2529%2520compared%2520to%2520TTT%2520and%2520TENT.%2520The%2520literature%2520search%250Aprovided%2520no%2520evidence%2520of%2520similar%2520works%252C%2520thereby%2520motivating%2520the%2520work%2520reported%250Ahere%2520as%2520the%2520first%2520study%2520to%2520leverage%2520TTA%2520techniques%2520for%2520audio%2520classification%250Aunder%2520domain%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Investigation%20of%20Test-time%20Adaptation%20for%20Audio%20Classification%20under%0A%20%20Background%20Noise&entry.906535625=Weichuang%20Shao%20and%20Iman%20Yi%20Liao%20and%20Tomas%20Henrique%20Bode%20Maul%20and%20Tissa%20Chandesa&entry.1292438233=%20%20Domain%20shift%20is%20a%20prominent%20problem%20in%20Deep%20Learning%2C%20causing%20a%20model%0Apre-trained%20on%20a%20source%20dataset%20to%20suffer%20significant%20performance%20degradation%0Aon%20test%20datasets.%20This%20research%20aims%20to%20address%20the%20issue%20of%20audio%0Aclassification%20under%20domain%20shift%20caused%20by%20background%20noise%20using%20Test-Time%0AAdaptation%20%28TTA%29%2C%20a%20technique%20that%20adapts%20a%20pre-trained%20model%20during%20testing%0Ausing%20only%20unlabelled%20test%20data%20before%20making%20predictions.%20We%20adopt%20two%20common%0ATTA%20methods%2C%20TTT%20and%20TENT%2C%20and%20a%20state-of-the-art%20method%20CoNMix%2C%20and%0Ainvestigate%20their%20respective%20performance%20on%20two%20popular%20audio%20classification%0Adatasets%2C%20AudioMNIST%20%28AM%29%20and%20SpeechCommands%20V1%20%28SC%29%2C%20against%20different%20types%0Aof%20background%20noise%20and%20noise%20severity%20levels.%20The%20experimental%20results%20reveal%0Athat%20our%20proposed%20modified%20version%20of%20CoNMix%20produced%20the%20highest%0Aclassification%20accuracy%20under%20domain%20shift%20%285.31%25%20error%20rate%20under%2010%20dB%0Aexercise%20bike%20background%20noise%20and%2012.75%25%20error%20rate%20under%203%20dB%20running%20tap%0Abackground%20noise%20for%20AM%29%20compared%20to%20TTT%20and%20TENT.%20The%20literature%20search%0Aprovided%20no%20evidence%20of%20similar%20works%2C%20thereby%20motivating%20the%20work%20reported%0Ahere%20as%20the%20first%20study%20to%20leverage%20TTA%20techniques%20for%20audio%20classification%0Aunder%20domain%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15523v1&entry.124074799=Read"},
{"title": "Improving Functional Reliability of Near-Field Monitoring for Emergency\n  Braking in Autonomous Vehicles", "author": "Junnan Pan and Prodromos Sotiriadis and Vladislav Nenchev and Ferdinand Englberger", "abstract": "  Autonomous vehicles require reliable hazard detection. However, primary\nsensor systems may miss near-field obstacles, resulting in safety risks.\nAlthough a dedicated fast-reacting near-field monitoring system can mitigate\nthis, it typically suffers from false positives. To mitigate these, in this\npaper, we introduce three monitoring strategies based on dynamic spatial\nproperties, relevant object sizes, and motion-aware prediction. In experiments\nin a validated simulation, we compare the initial monitoring strategy against\nthe proposed improvements. The results demonstrate that the proposed strategies\ncan significantly improve the reliability of near-field monitoring systems.\n", "link": "http://arxiv.org/abs/2507.15594v1", "date": "2025-07-21", "relevancy": 2.038, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5478}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Functional%20Reliability%20of%20Near-Field%20Monitoring%20for%20Emergency%0A%20%20Braking%20in%20Autonomous%20Vehicles&body=Title%3A%20Improving%20Functional%20Reliability%20of%20Near-Field%20Monitoring%20for%20Emergency%0A%20%20Braking%20in%20Autonomous%20Vehicles%0AAuthor%3A%20Junnan%20Pan%20and%20Prodromos%20Sotiriadis%20and%20Vladislav%20Nenchev%20and%20Ferdinand%20Englberger%0AAbstract%3A%20%20%20Autonomous%20vehicles%20require%20reliable%20hazard%20detection.%20However%2C%20primary%0Asensor%20systems%20may%20miss%20near-field%20obstacles%2C%20resulting%20in%20safety%20risks.%0AAlthough%20a%20dedicated%20fast-reacting%20near-field%20monitoring%20system%20can%20mitigate%0Athis%2C%20it%20typically%20suffers%20from%20false%20positives.%20To%20mitigate%20these%2C%20in%20this%0Apaper%2C%20we%20introduce%20three%20monitoring%20strategies%20based%20on%20dynamic%20spatial%0Aproperties%2C%20relevant%20object%20sizes%2C%20and%20motion-aware%20prediction.%20In%20experiments%0Ain%20a%20validated%20simulation%2C%20we%20compare%20the%20initial%20monitoring%20strategy%20against%0Athe%20proposed%20improvements.%20The%20results%20demonstrate%20that%20the%20proposed%20strategies%0Acan%20significantly%20improve%20the%20reliability%20of%20near-field%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Functional%2520Reliability%2520of%2520Near-Field%2520Monitoring%2520for%2520Emergency%250A%2520%2520Braking%2520in%2520Autonomous%2520Vehicles%26entry.906535625%3DJunnan%2520Pan%2520and%2520Prodromos%2520Sotiriadis%2520and%2520Vladislav%2520Nenchev%2520and%2520Ferdinand%2520Englberger%26entry.1292438233%3D%2520%2520Autonomous%2520vehicles%2520require%2520reliable%2520hazard%2520detection.%2520However%252C%2520primary%250Asensor%2520systems%2520may%2520miss%2520near-field%2520obstacles%252C%2520resulting%2520in%2520safety%2520risks.%250AAlthough%2520a%2520dedicated%2520fast-reacting%2520near-field%2520monitoring%2520system%2520can%2520mitigate%250Athis%252C%2520it%2520typically%2520suffers%2520from%2520false%2520positives.%2520To%2520mitigate%2520these%252C%2520in%2520this%250Apaper%252C%2520we%2520introduce%2520three%2520monitoring%2520strategies%2520based%2520on%2520dynamic%2520spatial%250Aproperties%252C%2520relevant%2520object%2520sizes%252C%2520and%2520motion-aware%2520prediction.%2520In%2520experiments%250Ain%2520a%2520validated%2520simulation%252C%2520we%2520compare%2520the%2520initial%2520monitoring%2520strategy%2520against%250Athe%2520proposed%2520improvements.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520strategies%250Acan%2520significantly%2520improve%2520the%2520reliability%2520of%2520near-field%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Functional%20Reliability%20of%20Near-Field%20Monitoring%20for%20Emergency%0A%20%20Braking%20in%20Autonomous%20Vehicles&entry.906535625=Junnan%20Pan%20and%20Prodromos%20Sotiriadis%20and%20Vladislav%20Nenchev%20and%20Ferdinand%20Englberger&entry.1292438233=%20%20Autonomous%20vehicles%20require%20reliable%20hazard%20detection.%20However%2C%20primary%0Asensor%20systems%20may%20miss%20near-field%20obstacles%2C%20resulting%20in%20safety%20risks.%0AAlthough%20a%20dedicated%20fast-reacting%20near-field%20monitoring%20system%20can%20mitigate%0Athis%2C%20it%20typically%20suffers%20from%20false%20positives.%20To%20mitigate%20these%2C%20in%20this%0Apaper%2C%20we%20introduce%20three%20monitoring%20strategies%20based%20on%20dynamic%20spatial%0Aproperties%2C%20relevant%20object%20sizes%2C%20and%20motion-aware%20prediction.%20In%20experiments%0Ain%20a%20validated%20simulation%2C%20we%20compare%20the%20initial%20monitoring%20strategy%20against%0Athe%20proposed%20improvements.%20The%20results%20demonstrate%20that%20the%20proposed%20strategies%0Acan%20significantly%20improve%20the%20reliability%20of%20near-field%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15594v1&entry.124074799=Read"},
{"title": "A Survey on Efficiency Optimization Techniques for DNN-based Video\n  Analytics: Process Systems, Algorithms, and Applications", "author": "Shanjiang Tang and Rui Huang and Hsinyu Luo and Chunjiang Wang and Ce Yu and Yusen Li and Hao Fu and Chao Sun and and Jian Xiao", "abstract": "  The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.\n", "link": "http://arxiv.org/abs/2507.15628v1", "date": "2025-07-21", "relevancy": 2.0345, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Efficiency%20Optimization%20Techniques%20for%20DNN-based%20Video%0A%20%20Analytics%3A%20Process%20Systems%2C%20Algorithms%2C%20and%20Applications&body=Title%3A%20A%20Survey%20on%20Efficiency%20Optimization%20Techniques%20for%20DNN-based%20Video%0A%20%20Analytics%3A%20Process%20Systems%2C%20Algorithms%2C%20and%20Applications%0AAuthor%3A%20Shanjiang%20Tang%20and%20Rui%20Huang%20and%20Hsinyu%20Luo%20and%20Chunjiang%20Wang%20and%20Ce%20Yu%20and%20Yusen%20Li%20and%20Hao%20Fu%20and%20Chao%20Sun%20and%20and%20Jian%20Xiao%0AAbstract%3A%20%20%20The%20explosive%20growth%20of%20video%20data%20in%20recent%20years%20has%20brought%20higher%20demands%0Afor%20video%20analytics%2C%20where%20accuracy%20and%20efficiency%20remain%20the%20two%20primary%0Aconcerns.%20Deep%20neural%20networks%20%28DNNs%29%20have%20been%20widely%20adopted%20to%20ensure%0Aaccuracy%3B%20however%2C%20improving%20their%20efficiency%20in%20video%20analytics%20remains%20an%0Aopen%20challenge.%20Different%20from%20existing%20surveys%20that%20make%20summaries%20of%0ADNN-based%20video%20mainly%20from%20the%20accuracy%20optimization%20aspect%2C%20in%20this%20survey%2C%0Awe%20aim%20to%20provide%20a%20thorough%20review%20of%20optimization%20techniques%20focusing%20on%20the%0Aimprovement%20of%20the%20efficiency%20of%20DNNs%20in%20video%20analytics.%20We%20organize%20existing%0Amethods%20in%20a%20bottom-up%20manner%2C%20covering%20multiple%20perspectives%20such%20as%20hardware%0Asupport%2C%20data%20processing%2C%20operational%20deployment%2C%20etc.%20Finally%2C%20based%20on%20the%0Aoptimization%20framework%20and%20existing%20works%2C%20we%20analyze%20and%20discuss%20the%20problems%0Aand%20challenges%20in%20the%20performance%20optimization%20of%20DNN-based%20video%20analytics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Efficiency%2520Optimization%2520Techniques%2520for%2520DNN-based%2520Video%250A%2520%2520Analytics%253A%2520Process%2520Systems%252C%2520Algorithms%252C%2520and%2520Applications%26entry.906535625%3DShanjiang%2520Tang%2520and%2520Rui%2520Huang%2520and%2520Hsinyu%2520Luo%2520and%2520Chunjiang%2520Wang%2520and%2520Ce%2520Yu%2520and%2520Yusen%2520Li%2520and%2520Hao%2520Fu%2520and%2520Chao%2520Sun%2520and%2520and%2520Jian%2520Xiao%26entry.1292438233%3D%2520%2520The%2520explosive%2520growth%2520of%2520video%2520data%2520in%2520recent%2520years%2520has%2520brought%2520higher%2520demands%250Afor%2520video%2520analytics%252C%2520where%2520accuracy%2520and%2520efficiency%2520remain%2520the%2520two%2520primary%250Aconcerns.%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520been%2520widely%2520adopted%2520to%2520ensure%250Aaccuracy%253B%2520however%252C%2520improving%2520their%2520efficiency%2520in%2520video%2520analytics%2520remains%2520an%250Aopen%2520challenge.%2520Different%2520from%2520existing%2520surveys%2520that%2520make%2520summaries%2520of%250ADNN-based%2520video%2520mainly%2520from%2520the%2520accuracy%2520optimization%2520aspect%252C%2520in%2520this%2520survey%252C%250Awe%2520aim%2520to%2520provide%2520a%2520thorough%2520review%2520of%2520optimization%2520techniques%2520focusing%2520on%2520the%250Aimprovement%2520of%2520the%2520efficiency%2520of%2520DNNs%2520in%2520video%2520analytics.%2520We%2520organize%2520existing%250Amethods%2520in%2520a%2520bottom-up%2520manner%252C%2520covering%2520multiple%2520perspectives%2520such%2520as%2520hardware%250Asupport%252C%2520data%2520processing%252C%2520operational%2520deployment%252C%2520etc.%2520Finally%252C%2520based%2520on%2520the%250Aoptimization%2520framework%2520and%2520existing%2520works%252C%2520we%2520analyze%2520and%2520discuss%2520the%2520problems%250Aand%2520challenges%2520in%2520the%2520performance%2520optimization%2520of%2520DNN-based%2520video%2520analytics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Efficiency%20Optimization%20Techniques%20for%20DNN-based%20Video%0A%20%20Analytics%3A%20Process%20Systems%2C%20Algorithms%2C%20and%20Applications&entry.906535625=Shanjiang%20Tang%20and%20Rui%20Huang%20and%20Hsinyu%20Luo%20and%20Chunjiang%20Wang%20and%20Ce%20Yu%20and%20Yusen%20Li%20and%20Hao%20Fu%20and%20Chao%20Sun%20and%20and%20Jian%20Xiao&entry.1292438233=%20%20The%20explosive%20growth%20of%20video%20data%20in%20recent%20years%20has%20brought%20higher%20demands%0Afor%20video%20analytics%2C%20where%20accuracy%20and%20efficiency%20remain%20the%20two%20primary%0Aconcerns.%20Deep%20neural%20networks%20%28DNNs%29%20have%20been%20widely%20adopted%20to%20ensure%0Aaccuracy%3B%20however%2C%20improving%20their%20efficiency%20in%20video%20analytics%20remains%20an%0Aopen%20challenge.%20Different%20from%20existing%20surveys%20that%20make%20summaries%20of%0ADNN-based%20video%20mainly%20from%20the%20accuracy%20optimization%20aspect%2C%20in%20this%20survey%2C%0Awe%20aim%20to%20provide%20a%20thorough%20review%20of%20optimization%20techniques%20focusing%20on%20the%0Aimprovement%20of%20the%20efficiency%20of%20DNNs%20in%20video%20analytics.%20We%20organize%20existing%0Amethods%20in%20a%20bottom-up%20manner%2C%20covering%20multiple%20perspectives%20such%20as%20hardware%0Asupport%2C%20data%20processing%2C%20operational%20deployment%2C%20etc.%20Finally%2C%20based%20on%20the%0Aoptimization%20framework%20and%20existing%20works%2C%20we%20analyze%20and%20discuss%20the%20problems%0Aand%20challenges%20in%20the%20performance%20optimization%20of%20DNN-based%20video%20analytics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15628v1&entry.124074799=Read"},
{"title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft\n  II", "author": "Weiyu Ma and Jiwen Jiang and Haobo Fu and Haifeng Zhang", "abstract": "  We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.\n", "link": "http://arxiv.org/abs/2507.15618v1", "date": "2025-07-21", "relevancy": 2.031, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5241}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TacticCraft%3A%20Natural%20Language-Driven%20Tactical%20Adaptation%20for%20StarCraft%0A%20%20II&body=Title%3A%20TacticCraft%3A%20Natural%20Language-Driven%20Tactical%20Adaptation%20for%20StarCraft%0A%20%20II%0AAuthor%3A%20Weiyu%20Ma%20and%20Jiwen%20Jiang%20and%20Haobo%20Fu%20and%20Haifeng%20Zhang%0AAbstract%3A%20%20%20We%20present%20an%20adapter-based%20approach%20for%20tactical%20conditioning%20of%20StarCraft%0AII%20AI%20agents.%20Current%20agents%2C%20while%20powerful%2C%20lack%20the%20ability%20to%20adapt%20their%0Astrategies%20based%20on%20high-level%20tactical%20directives.%20Our%20method%20freezes%20a%0Apre-trained%20policy%20network%20%28DI-Star%29%20and%20attaches%20lightweight%20adapter%20modules%0Ato%20each%20action%20head%2C%20conditioned%20on%20a%20tactical%20tensor%20that%20encodes%20strategic%0Apreferences.%20By%20training%20these%20adapters%20with%20KL%20divergence%20constraints%2C%20we%0Aensure%20the%20policy%20maintains%20core%20competencies%20while%20exhibiting%20tactical%0Avariations.%20Experimental%20results%20show%20our%20approach%20successfully%20modulates%20agent%0Abehavior%20across%20tactical%20dimensions%20including%20aggression%2C%20expansion%20patterns%2C%0Aand%20technology%20preferences%2C%20while%20maintaining%20competitive%20performance.%20Our%0Amethod%20enables%20flexible%20tactical%20control%20with%20minimal%20computational%20overhead%2C%0Aoffering%20practical%20strategy%20customization%20for%20complex%20real-time%20strategy%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTacticCraft%253A%2520Natural%2520Language-Driven%2520Tactical%2520Adaptation%2520for%2520StarCraft%250A%2520%2520II%26entry.906535625%3DWeiyu%2520Ma%2520and%2520Jiwen%2520Jiang%2520and%2520Haobo%2520Fu%2520and%2520Haifeng%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520adapter-based%2520approach%2520for%2520tactical%2520conditioning%2520of%2520StarCraft%250AII%2520AI%2520agents.%2520Current%2520agents%252C%2520while%2520powerful%252C%2520lack%2520the%2520ability%2520to%2520adapt%2520their%250Astrategies%2520based%2520on%2520high-level%2520tactical%2520directives.%2520Our%2520method%2520freezes%2520a%250Apre-trained%2520policy%2520network%2520%2528DI-Star%2529%2520and%2520attaches%2520lightweight%2520adapter%2520modules%250Ato%2520each%2520action%2520head%252C%2520conditioned%2520on%2520a%2520tactical%2520tensor%2520that%2520encodes%2520strategic%250Apreferences.%2520By%2520training%2520these%2520adapters%2520with%2520KL%2520divergence%2520constraints%252C%2520we%250Aensure%2520the%2520policy%2520maintains%2520core%2520competencies%2520while%2520exhibiting%2520tactical%250Avariations.%2520Experimental%2520results%2520show%2520our%2520approach%2520successfully%2520modulates%2520agent%250Abehavior%2520across%2520tactical%2520dimensions%2520including%2520aggression%252C%2520expansion%2520patterns%252C%250Aand%2520technology%2520preferences%252C%2520while%2520maintaining%2520competitive%2520performance.%2520Our%250Amethod%2520enables%2520flexible%2520tactical%2520control%2520with%2520minimal%2520computational%2520overhead%252C%250Aoffering%2520practical%2520strategy%2520customization%2520for%2520complex%2520real-time%2520strategy%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TacticCraft%3A%20Natural%20Language-Driven%20Tactical%20Adaptation%20for%20StarCraft%0A%20%20II&entry.906535625=Weiyu%20Ma%20and%20Jiwen%20Jiang%20and%20Haobo%20Fu%20and%20Haifeng%20Zhang&entry.1292438233=%20%20We%20present%20an%20adapter-based%20approach%20for%20tactical%20conditioning%20of%20StarCraft%0AII%20AI%20agents.%20Current%20agents%2C%20while%20powerful%2C%20lack%20the%20ability%20to%20adapt%20their%0Astrategies%20based%20on%20high-level%20tactical%20directives.%20Our%20method%20freezes%20a%0Apre-trained%20policy%20network%20%28DI-Star%29%20and%20attaches%20lightweight%20adapter%20modules%0Ato%20each%20action%20head%2C%20conditioned%20on%20a%20tactical%20tensor%20that%20encodes%20strategic%0Apreferences.%20By%20training%20these%20adapters%20with%20KL%20divergence%20constraints%2C%20we%0Aensure%20the%20policy%20maintains%20core%20competencies%20while%20exhibiting%20tactical%0Avariations.%20Experimental%20results%20show%20our%20approach%20successfully%20modulates%20agent%0Abehavior%20across%20tactical%20dimensions%20including%20aggression%2C%20expansion%20patterns%2C%0Aand%20technology%20preferences%2C%20while%20maintaining%20competitive%20performance.%20Our%0Amethod%20enables%20flexible%20tactical%20control%20with%20minimal%20computational%20overhead%2C%0Aoffering%20practical%20strategy%20customization%20for%20complex%20real-time%20strategy%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15618v1&entry.124074799=Read"},
{"title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological\n  Knowledge and Reasoning", "author": "Sahana Srinivasan and Xuguang Ai and Thaddaeus Wai Soon Lo and Aidan Gilson and Minjie Zou and Ke Zou and Hyunjae Kim and Mingjia Yang and Krithi Pushpanathan and Samantha Yew and Wan Ting Loke and Jocelyn Goh and Yibing Chen and Yiming Kong and Emily Yuelei Fu and Michelle Ongyong Hui and Kristen Nwanyanwu and Amisha Dave and Kelvin Zhenghao Li and Chen-Hsin Sun and Mark Chia and Gabriel Dawei Yang and Wendy Meihua Wong and David Ziyou Chen and Dianbo Liu and Maxwell Singer and Fares Antaki and Lucian V Del Priore and Jost Jonas and Ron Adelman and Qingyu Chen and Yih-Chung Tham", "abstract": "  Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.\n", "link": "http://arxiv.org/abs/2507.15717v1", "date": "2025-07-21", "relevancy": 2.03, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEnchmarking%20LLMs%20for%20Ophthalmology%20%28BELO%29%20for%20Ophthalmological%0A%20%20Knowledge%20and%20Reasoning&body=Title%3A%20BEnchmarking%20LLMs%20for%20Ophthalmology%20%28BELO%29%20for%20Ophthalmological%0A%20%20Knowledge%20and%20Reasoning%0AAuthor%3A%20Sahana%20Srinivasan%20and%20Xuguang%20Ai%20and%20Thaddaeus%20Wai%20Soon%20Lo%20and%20Aidan%20Gilson%20and%20Minjie%20Zou%20and%20Ke%20Zou%20and%20Hyunjae%20Kim%20and%20Mingjia%20Yang%20and%20Krithi%20Pushpanathan%20and%20Samantha%20Yew%20and%20Wan%20Ting%20Loke%20and%20Jocelyn%20Goh%20and%20Yibing%20Chen%20and%20Yiming%20Kong%20and%20Emily%20Yuelei%20Fu%20and%20Michelle%20Ongyong%20Hui%20and%20Kristen%20Nwanyanwu%20and%20Amisha%20Dave%20and%20Kelvin%20Zhenghao%20Li%20and%20Chen-Hsin%20Sun%20and%20Mark%20Chia%20and%20Gabriel%20Dawei%20Yang%20and%20Wendy%20Meihua%20Wong%20and%20David%20Ziyou%20Chen%20and%20Dianbo%20Liu%20and%20Maxwell%20Singer%20and%20Fares%20Antaki%20and%20Lucian%20V%20Del%20Priore%20and%20Jost%20Jonas%20and%20Ron%20Adelman%20and%20Qingyu%20Chen%20and%20Yih-Chung%20Tham%0AAbstract%3A%20%20%20Current%20benchmarks%20evaluating%20large%20language%20models%20%28LLMs%29%20in%20ophthalmology%0Aare%20limited%20in%20scope%20and%20disproportionately%20prioritise%20accuracy.%20We%20introduce%0ABELO%20%28BEnchmarking%20LLMs%20for%20Ophthalmology%29%2C%20a%20standardized%20and%20comprehensive%0Aevaluation%20benchmark%20developed%20through%20multiple%20rounds%20of%20expert%20checking%20by%2013%0Aophthalmologists.%20BELO%20assesses%20ophthalmology-related%20clinical%20accuracy%20and%0Areasoning%20quality.%20Using%20keyword%20matching%20and%20a%20fine-tuned%20PubMedBERT%20model%2C%20we%0Acurated%20ophthalmology-specific%20multiple-choice-questions%20%28MCQs%29%20from%20diverse%0Amedical%20datasets%20%28BCSC%2C%20MedMCQA%2C%20MedQA%2C%20BioASQ%2C%20and%20PubMedQA%29.%20The%20dataset%0Aunderwent%20multiple%20rounds%20of%20expert%20checking.%20Duplicate%20and%20substandard%0Aquestions%20were%20systematically%20removed.%20Ten%20ophthalmologists%20refined%20the%0Aexplanations%20of%20each%20MCQ%27s%20correct%20answer.%20This%20was%20further%20adjudicated%20by%0Athree%20senior%20ophthalmologists.%20To%20illustrate%20BELO%27s%20utility%2C%20we%20evaluated%20six%0ALLMs%20%28OpenAI%20o1%2C%20o3-mini%2C%20GPT-4o%2C%20DeepSeek-R1%2C%20Llama-3-8B%2C%20and%20Gemini%201.5%20Pro%29%0Ausing%20accuracy%2C%20macro-F1%2C%20and%20five%20text-generation%20metrics%20%28ROUGE-L%2C%20BERTScore%2C%0ABARTScore%2C%20METEOR%2C%20and%20AlignScore%29.%20In%20a%20further%20evaluation%20involving%20human%0Aexperts%2C%20two%20ophthalmologists%20qualitatively%20reviewed%2050%20randomly%20selected%0Aoutputs%20for%20accuracy%2C%20comprehensiveness%2C%20and%20completeness.%20BELO%20consists%20of%20900%0Ahigh-quality%2C%20expert-reviewed%20questions%20aggregated%20from%20five%20sources%3A%20BCSC%0A%28260%29%2C%20BioASQ%20%2810%29%2C%20MedMCQA%20%28572%29%2C%20MedQA%20%2840%29%2C%20and%20PubMedQA%20%2818%29.%20A%20public%0Aleaderboard%20has%20been%20established%20to%20promote%20transparent%20evaluation%20and%0Areporting.%20Importantly%2C%20the%20BELO%20dataset%20will%20remain%20a%20hold-out%2C%0Aevaluation-only%20benchmark%20to%20ensure%20fair%20and%20reproducible%20comparisons%20of%20future%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEnchmarking%2520LLMs%2520for%2520Ophthalmology%2520%2528BELO%2529%2520for%2520Ophthalmological%250A%2520%2520Knowledge%2520and%2520Reasoning%26entry.906535625%3DSahana%2520Srinivasan%2520and%2520Xuguang%2520Ai%2520and%2520Thaddaeus%2520Wai%2520Soon%2520Lo%2520and%2520Aidan%2520Gilson%2520and%2520Minjie%2520Zou%2520and%2520Ke%2520Zou%2520and%2520Hyunjae%2520Kim%2520and%2520Mingjia%2520Yang%2520and%2520Krithi%2520Pushpanathan%2520and%2520Samantha%2520Yew%2520and%2520Wan%2520Ting%2520Loke%2520and%2520Jocelyn%2520Goh%2520and%2520Yibing%2520Chen%2520and%2520Yiming%2520Kong%2520and%2520Emily%2520Yuelei%2520Fu%2520and%2520Michelle%2520Ongyong%2520Hui%2520and%2520Kristen%2520Nwanyanwu%2520and%2520Amisha%2520Dave%2520and%2520Kelvin%2520Zhenghao%2520Li%2520and%2520Chen-Hsin%2520Sun%2520and%2520Mark%2520Chia%2520and%2520Gabriel%2520Dawei%2520Yang%2520and%2520Wendy%2520Meihua%2520Wong%2520and%2520David%2520Ziyou%2520Chen%2520and%2520Dianbo%2520Liu%2520and%2520Maxwell%2520Singer%2520and%2520Fares%2520Antaki%2520and%2520Lucian%2520V%2520Del%2520Priore%2520and%2520Jost%2520Jonas%2520and%2520Ron%2520Adelman%2520and%2520Qingyu%2520Chen%2520and%2520Yih-Chung%2520Tham%26entry.1292438233%3D%2520%2520Current%2520benchmarks%2520evaluating%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520ophthalmology%250Aare%2520limited%2520in%2520scope%2520and%2520disproportionately%2520prioritise%2520accuracy.%2520We%2520introduce%250ABELO%2520%2528BEnchmarking%2520LLMs%2520for%2520Ophthalmology%2529%252C%2520a%2520standardized%2520and%2520comprehensive%250Aevaluation%2520benchmark%2520developed%2520through%2520multiple%2520rounds%2520of%2520expert%2520checking%2520by%252013%250Aophthalmologists.%2520BELO%2520assesses%2520ophthalmology-related%2520clinical%2520accuracy%2520and%250Areasoning%2520quality.%2520Using%2520keyword%2520matching%2520and%2520a%2520fine-tuned%2520PubMedBERT%2520model%252C%2520we%250Acurated%2520ophthalmology-specific%2520multiple-choice-questions%2520%2528MCQs%2529%2520from%2520diverse%250Amedical%2520datasets%2520%2528BCSC%252C%2520MedMCQA%252C%2520MedQA%252C%2520BioASQ%252C%2520and%2520PubMedQA%2529.%2520The%2520dataset%250Aunderwent%2520multiple%2520rounds%2520of%2520expert%2520checking.%2520Duplicate%2520and%2520substandard%250Aquestions%2520were%2520systematically%2520removed.%2520Ten%2520ophthalmologists%2520refined%2520the%250Aexplanations%2520of%2520each%2520MCQ%2527s%2520correct%2520answer.%2520This%2520was%2520further%2520adjudicated%2520by%250Athree%2520senior%2520ophthalmologists.%2520To%2520illustrate%2520BELO%2527s%2520utility%252C%2520we%2520evaluated%2520six%250ALLMs%2520%2528OpenAI%2520o1%252C%2520o3-mini%252C%2520GPT-4o%252C%2520DeepSeek-R1%252C%2520Llama-3-8B%252C%2520and%2520Gemini%25201.5%2520Pro%2529%250Ausing%2520accuracy%252C%2520macro-F1%252C%2520and%2520five%2520text-generation%2520metrics%2520%2528ROUGE-L%252C%2520BERTScore%252C%250ABARTScore%252C%2520METEOR%252C%2520and%2520AlignScore%2529.%2520In%2520a%2520further%2520evaluation%2520involving%2520human%250Aexperts%252C%2520two%2520ophthalmologists%2520qualitatively%2520reviewed%252050%2520randomly%2520selected%250Aoutputs%2520for%2520accuracy%252C%2520comprehensiveness%252C%2520and%2520completeness.%2520BELO%2520consists%2520of%2520900%250Ahigh-quality%252C%2520expert-reviewed%2520questions%2520aggregated%2520from%2520five%2520sources%253A%2520BCSC%250A%2528260%2529%252C%2520BioASQ%2520%252810%2529%252C%2520MedMCQA%2520%2528572%2529%252C%2520MedQA%2520%252840%2529%252C%2520and%2520PubMedQA%2520%252818%2529.%2520A%2520public%250Aleaderboard%2520has%2520been%2520established%2520to%2520promote%2520transparent%2520evaluation%2520and%250Areporting.%2520Importantly%252C%2520the%2520BELO%2520dataset%2520will%2520remain%2520a%2520hold-out%252C%250Aevaluation-only%2520benchmark%2520to%2520ensure%2520fair%2520and%2520reproducible%2520comparisons%2520of%2520future%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEnchmarking%20LLMs%20for%20Ophthalmology%20%28BELO%29%20for%20Ophthalmological%0A%20%20Knowledge%20and%20Reasoning&entry.906535625=Sahana%20Srinivasan%20and%20Xuguang%20Ai%20and%20Thaddaeus%20Wai%20Soon%20Lo%20and%20Aidan%20Gilson%20and%20Minjie%20Zou%20and%20Ke%20Zou%20and%20Hyunjae%20Kim%20and%20Mingjia%20Yang%20and%20Krithi%20Pushpanathan%20and%20Samantha%20Yew%20and%20Wan%20Ting%20Loke%20and%20Jocelyn%20Goh%20and%20Yibing%20Chen%20and%20Yiming%20Kong%20and%20Emily%20Yuelei%20Fu%20and%20Michelle%20Ongyong%20Hui%20and%20Kristen%20Nwanyanwu%20and%20Amisha%20Dave%20and%20Kelvin%20Zhenghao%20Li%20and%20Chen-Hsin%20Sun%20and%20Mark%20Chia%20and%20Gabriel%20Dawei%20Yang%20and%20Wendy%20Meihua%20Wong%20and%20David%20Ziyou%20Chen%20and%20Dianbo%20Liu%20and%20Maxwell%20Singer%20and%20Fares%20Antaki%20and%20Lucian%20V%20Del%20Priore%20and%20Jost%20Jonas%20and%20Ron%20Adelman%20and%20Qingyu%20Chen%20and%20Yih-Chung%20Tham&entry.1292438233=%20%20Current%20benchmarks%20evaluating%20large%20language%20models%20%28LLMs%29%20in%20ophthalmology%0Aare%20limited%20in%20scope%20and%20disproportionately%20prioritise%20accuracy.%20We%20introduce%0ABELO%20%28BEnchmarking%20LLMs%20for%20Ophthalmology%29%2C%20a%20standardized%20and%20comprehensive%0Aevaluation%20benchmark%20developed%20through%20multiple%20rounds%20of%20expert%20checking%20by%2013%0Aophthalmologists.%20BELO%20assesses%20ophthalmology-related%20clinical%20accuracy%20and%0Areasoning%20quality.%20Using%20keyword%20matching%20and%20a%20fine-tuned%20PubMedBERT%20model%2C%20we%0Acurated%20ophthalmology-specific%20multiple-choice-questions%20%28MCQs%29%20from%20diverse%0Amedical%20datasets%20%28BCSC%2C%20MedMCQA%2C%20MedQA%2C%20BioASQ%2C%20and%20PubMedQA%29.%20The%20dataset%0Aunderwent%20multiple%20rounds%20of%20expert%20checking.%20Duplicate%20and%20substandard%0Aquestions%20were%20systematically%20removed.%20Ten%20ophthalmologists%20refined%20the%0Aexplanations%20of%20each%20MCQ%27s%20correct%20answer.%20This%20was%20further%20adjudicated%20by%0Athree%20senior%20ophthalmologists.%20To%20illustrate%20BELO%27s%20utility%2C%20we%20evaluated%20six%0ALLMs%20%28OpenAI%20o1%2C%20o3-mini%2C%20GPT-4o%2C%20DeepSeek-R1%2C%20Llama-3-8B%2C%20and%20Gemini%201.5%20Pro%29%0Ausing%20accuracy%2C%20macro-F1%2C%20and%20five%20text-generation%20metrics%20%28ROUGE-L%2C%20BERTScore%2C%0ABARTScore%2C%20METEOR%2C%20and%20AlignScore%29.%20In%20a%20further%20evaluation%20involving%20human%0Aexperts%2C%20two%20ophthalmologists%20qualitatively%20reviewed%2050%20randomly%20selected%0Aoutputs%20for%20accuracy%2C%20comprehensiveness%2C%20and%20completeness.%20BELO%20consists%20of%20900%0Ahigh-quality%2C%20expert-reviewed%20questions%20aggregated%20from%20five%20sources%3A%20BCSC%0A%28260%29%2C%20BioASQ%20%2810%29%2C%20MedMCQA%20%28572%29%2C%20MedQA%20%2840%29%2C%20and%20PubMedQA%20%2818%29.%20A%20public%0Aleaderboard%20has%20been%20established%20to%20promote%20transparent%20evaluation%20and%0Areporting.%20Importantly%2C%20the%20BELO%20dataset%20will%20remain%20a%20hold-out%2C%0Aevaluation-only%20benchmark%20to%20ensure%20fair%20and%20reproducible%20comparisons%20of%20future%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15717v1&entry.124074799=Read"},
{"title": "The Constitutional Filter: Bayesian Estimation of Compliant Agents", "author": "Simon Kohaut and Felix Divo and Benedict Flade and Devendra Singh Dhami and Julian Eggert and Kristian Kersting", "abstract": "  Predicting agents impacted by legal policies, physical limitations, and\noperational preferences is inherently difficult. In recent years,\nneuro-symbolic methods have emerged, integrating machine learning and symbolic\nreasoning models into end-to-end learnable systems. Hereby, a promising avenue\nfor expressing high-level constraints over multi-modal input data in robotics\nhas opened up. This work introduces an approach for Bayesian estimation of\nagents expected to comply with a human-interpretable neuro-symbolic model we\ncall its Constitution. Hence, we present the Constitutional Filter (CoFi),\nleading to improved tracking of agents by leveraging expert knowledge,\nincorporating deep learning architectures, and accounting for environmental\nuncertainties. CoFi extends the general, recursive Bayesian estimation setting,\nensuring compatibility with a vast landscape of established techniques such as\nParticle Filters. To underpin the advantages of CoFi, we evaluate its\nperformance on real-world marine traffic data. Beyond improved performance, we\nshow how CoFi can learn to trust and adapt to the level of compliance of an\nagent, recovering baseline performance even if the assumed Constitution clashes\nwith reality.\n", "link": "http://arxiv.org/abs/2412.18347v3", "date": "2025-07-21", "relevancy": 2.0277, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Constitutional%20Filter%3A%20Bayesian%20Estimation%20of%20Compliant%20Agents&body=Title%3A%20The%20Constitutional%20Filter%3A%20Bayesian%20Estimation%20of%20Compliant%20Agents%0AAuthor%3A%20Simon%20Kohaut%20and%20Felix%20Divo%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Predicting%20agents%20impacted%20by%20legal%20policies%2C%20physical%20limitations%2C%20and%0Aoperational%20preferences%20is%20inherently%20difficult.%20In%20recent%20years%2C%0Aneuro-symbolic%20methods%20have%20emerged%2C%20integrating%20machine%20learning%20and%20symbolic%0Areasoning%20models%20into%20end-to-end%20learnable%20systems.%20Hereby%2C%20a%20promising%20avenue%0Afor%20expressing%20high-level%20constraints%20over%20multi-modal%20input%20data%20in%20robotics%0Ahas%20opened%20up.%20This%20work%20introduces%20an%20approach%20for%20Bayesian%20estimation%20of%0Aagents%20expected%20to%20comply%20with%20a%20human-interpretable%20neuro-symbolic%20model%20we%0Acall%20its%20Constitution.%20Hence%2C%20we%20present%20the%20Constitutional%20Filter%20%28CoFi%29%2C%0Aleading%20to%20improved%20tracking%20of%20agents%20by%20leveraging%20expert%20knowledge%2C%0Aincorporating%20deep%20learning%20architectures%2C%20and%20accounting%20for%20environmental%0Auncertainties.%20CoFi%20extends%20the%20general%2C%20recursive%20Bayesian%20estimation%20setting%2C%0Aensuring%20compatibility%20with%20a%20vast%20landscape%20of%20established%20techniques%20such%20as%0AParticle%20Filters.%20To%20underpin%20the%20advantages%20of%20CoFi%2C%20we%20evaluate%20its%0Aperformance%20on%20real-world%20marine%20traffic%20data.%20Beyond%20improved%20performance%2C%20we%0Ashow%20how%20CoFi%20can%20learn%20to%20trust%20and%20adapt%20to%20the%20level%20of%20compliance%20of%20an%0Aagent%2C%20recovering%20baseline%20performance%20even%20if%20the%20assumed%20Constitution%20clashes%0Awith%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18347v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Constitutional%2520Filter%253A%2520Bayesian%2520Estimation%2520of%2520Compliant%2520Agents%26entry.906535625%3DSimon%2520Kohaut%2520and%2520Felix%2520Divo%2520and%2520Benedict%2520Flade%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Julian%2520Eggert%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Predicting%2520agents%2520impacted%2520by%2520legal%2520policies%252C%2520physical%2520limitations%252C%2520and%250Aoperational%2520preferences%2520is%2520inherently%2520difficult.%2520In%2520recent%2520years%252C%250Aneuro-symbolic%2520methods%2520have%2520emerged%252C%2520integrating%2520machine%2520learning%2520and%2520symbolic%250Areasoning%2520models%2520into%2520end-to-end%2520learnable%2520systems.%2520Hereby%252C%2520a%2520promising%2520avenue%250Afor%2520expressing%2520high-level%2520constraints%2520over%2520multi-modal%2520input%2520data%2520in%2520robotics%250Ahas%2520opened%2520up.%2520This%2520work%2520introduces%2520an%2520approach%2520for%2520Bayesian%2520estimation%2520of%250Aagents%2520expected%2520to%2520comply%2520with%2520a%2520human-interpretable%2520neuro-symbolic%2520model%2520we%250Acall%2520its%2520Constitution.%2520Hence%252C%2520we%2520present%2520the%2520Constitutional%2520Filter%2520%2528CoFi%2529%252C%250Aleading%2520to%2520improved%2520tracking%2520of%2520agents%2520by%2520leveraging%2520expert%2520knowledge%252C%250Aincorporating%2520deep%2520learning%2520architectures%252C%2520and%2520accounting%2520for%2520environmental%250Auncertainties.%2520CoFi%2520extends%2520the%2520general%252C%2520recursive%2520Bayesian%2520estimation%2520setting%252C%250Aensuring%2520compatibility%2520with%2520a%2520vast%2520landscape%2520of%2520established%2520techniques%2520such%2520as%250AParticle%2520Filters.%2520To%2520underpin%2520the%2520advantages%2520of%2520CoFi%252C%2520we%2520evaluate%2520its%250Aperformance%2520on%2520real-world%2520marine%2520traffic%2520data.%2520Beyond%2520improved%2520performance%252C%2520we%250Ashow%2520how%2520CoFi%2520can%2520learn%2520to%2520trust%2520and%2520adapt%2520to%2520the%2520level%2520of%2520compliance%2520of%2520an%250Aagent%252C%2520recovering%2520baseline%2520performance%2520even%2520if%2520the%2520assumed%2520Constitution%2520clashes%250Awith%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18347v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Constitutional%20Filter%3A%20Bayesian%20Estimation%20of%20Compliant%20Agents&entry.906535625=Simon%20Kohaut%20and%20Felix%20Divo%20and%20Benedict%20Flade%20and%20Devendra%20Singh%20Dhami%20and%20Julian%20Eggert%20and%20Kristian%20Kersting&entry.1292438233=%20%20Predicting%20agents%20impacted%20by%20legal%20policies%2C%20physical%20limitations%2C%20and%0Aoperational%20preferences%20is%20inherently%20difficult.%20In%20recent%20years%2C%0Aneuro-symbolic%20methods%20have%20emerged%2C%20integrating%20machine%20learning%20and%20symbolic%0Areasoning%20models%20into%20end-to-end%20learnable%20systems.%20Hereby%2C%20a%20promising%20avenue%0Afor%20expressing%20high-level%20constraints%20over%20multi-modal%20input%20data%20in%20robotics%0Ahas%20opened%20up.%20This%20work%20introduces%20an%20approach%20for%20Bayesian%20estimation%20of%0Aagents%20expected%20to%20comply%20with%20a%20human-interpretable%20neuro-symbolic%20model%20we%0Acall%20its%20Constitution.%20Hence%2C%20we%20present%20the%20Constitutional%20Filter%20%28CoFi%29%2C%0Aleading%20to%20improved%20tracking%20of%20agents%20by%20leveraging%20expert%20knowledge%2C%0Aincorporating%20deep%20learning%20architectures%2C%20and%20accounting%20for%20environmental%0Auncertainties.%20CoFi%20extends%20the%20general%2C%20recursive%20Bayesian%20estimation%20setting%2C%0Aensuring%20compatibility%20with%20a%20vast%20landscape%20of%20established%20techniques%20such%20as%0AParticle%20Filters.%20To%20underpin%20the%20advantages%20of%20CoFi%2C%20we%20evaluate%20its%0Aperformance%20on%20real-world%20marine%20traffic%20data.%20Beyond%20improved%20performance%2C%20we%0Ashow%20how%20CoFi%20can%20learn%20to%20trust%20and%20adapt%20to%20the%20level%20of%20compliance%20of%20an%0Aagent%2C%20recovering%20baseline%20performance%20even%20if%20the%20assumed%20Constitution%20clashes%0Awith%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18347v3&entry.124074799=Read"},
{"title": "Ultra-fast feature learning for the training of two-layer neural\n  networks in the two-timescale regime", "author": "Rapha\u00ebl Barboni and Gabriel Peyr\u00e9 and Fran\u00e7ois-Xavier Vialard", "abstract": "  We study the convergence of gradient methods for the training of mean-field\nsingle-hidden-layer neural networks with square loss. For this high-dimensional\nand non-convex optimization problem, most known convergence results are either\nqualitative or rely on a neural tangent kernel analysis where nonlinear\nrepresentations of the data are fixed. Using that this problem belongs to the\nclass of separable nonlinear least squares problems, we consider here a\nVariable Projection (VarPro) or two-timescale learning algorithm, thereby\neliminating the linear variables and reducing the learning problem to the\ntraining of nonlinear features. In a teacher-student scenario, we show such a\nstrategy enables provable convergence rates for the sampling of a teacher\nfeature distribution. Precisely, in the limit where the regularization strength\nvanishes, we show that the dynamic of the feature distribution corresponds to a\nweighted ultra-fast diffusion equation. Recent results on the asymptotic\nbehavior of such PDEs then give quantitative guarantees for the convergence of\nthe learned feature distribution.\n", "link": "http://arxiv.org/abs/2504.18208v2", "date": "2025-07-21", "relevancy": 2.0273, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5343}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5019}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-fast%20feature%20learning%20for%20the%20training%20of%20two-layer%20neural%0A%20%20networks%20in%20the%20two-timescale%20regime&body=Title%3A%20Ultra-fast%20feature%20learning%20for%20the%20training%20of%20two-layer%20neural%0A%20%20networks%20in%20the%20two-timescale%20regime%0AAuthor%3A%20Rapha%C3%ABl%20Barboni%20and%20Gabriel%20Peyr%C3%A9%20and%20Fran%C3%A7ois-Xavier%20Vialard%0AAbstract%3A%20%20%20We%20study%20the%20convergence%20of%20gradient%20methods%20for%20the%20training%20of%20mean-field%0Asingle-hidden-layer%20neural%20networks%20with%20square%20loss.%20For%20this%20high-dimensional%0Aand%20non-convex%20optimization%20problem%2C%20most%20known%20convergence%20results%20are%20either%0Aqualitative%20or%20rely%20on%20a%20neural%20tangent%20kernel%20analysis%20where%20nonlinear%0Arepresentations%20of%20the%20data%20are%20fixed.%20Using%20that%20this%20problem%20belongs%20to%20the%0Aclass%20of%20separable%20nonlinear%20least%20squares%20problems%2C%20we%20consider%20here%20a%0AVariable%20Projection%20%28VarPro%29%20or%20two-timescale%20learning%20algorithm%2C%20thereby%0Aeliminating%20the%20linear%20variables%20and%20reducing%20the%20learning%20problem%20to%20the%0Atraining%20of%20nonlinear%20features.%20In%20a%20teacher-student%20scenario%2C%20we%20show%20such%20a%0Astrategy%20enables%20provable%20convergence%20rates%20for%20the%20sampling%20of%20a%20teacher%0Afeature%20distribution.%20Precisely%2C%20in%20the%20limit%20where%20the%20regularization%20strength%0Avanishes%2C%20we%20show%20that%20the%20dynamic%20of%20the%20feature%20distribution%20corresponds%20to%20a%0Aweighted%20ultra-fast%20diffusion%20equation.%20Recent%20results%20on%20the%20asymptotic%0Abehavior%20of%20such%20PDEs%20then%20give%20quantitative%20guarantees%20for%20the%20convergence%20of%0Athe%20learned%20feature%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-fast%2520feature%2520learning%2520for%2520the%2520training%2520of%2520two-layer%2520neural%250A%2520%2520networks%2520in%2520the%2520two-timescale%2520regime%26entry.906535625%3DRapha%25C3%25ABl%2520Barboni%2520and%2520Gabriel%2520Peyr%25C3%25A9%2520and%2520Fran%25C3%25A7ois-Xavier%2520Vialard%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520convergence%2520of%2520gradient%2520methods%2520for%2520the%2520training%2520of%2520mean-field%250Asingle-hidden-layer%2520neural%2520networks%2520with%2520square%2520loss.%2520For%2520this%2520high-dimensional%250Aand%2520non-convex%2520optimization%2520problem%252C%2520most%2520known%2520convergence%2520results%2520are%2520either%250Aqualitative%2520or%2520rely%2520on%2520a%2520neural%2520tangent%2520kernel%2520analysis%2520where%2520nonlinear%250Arepresentations%2520of%2520the%2520data%2520are%2520fixed.%2520Using%2520that%2520this%2520problem%2520belongs%2520to%2520the%250Aclass%2520of%2520separable%2520nonlinear%2520least%2520squares%2520problems%252C%2520we%2520consider%2520here%2520a%250AVariable%2520Projection%2520%2528VarPro%2529%2520or%2520two-timescale%2520learning%2520algorithm%252C%2520thereby%250Aeliminating%2520the%2520linear%2520variables%2520and%2520reducing%2520the%2520learning%2520problem%2520to%2520the%250Atraining%2520of%2520nonlinear%2520features.%2520In%2520a%2520teacher-student%2520scenario%252C%2520we%2520show%2520such%2520a%250Astrategy%2520enables%2520provable%2520convergence%2520rates%2520for%2520the%2520sampling%2520of%2520a%2520teacher%250Afeature%2520distribution.%2520Precisely%252C%2520in%2520the%2520limit%2520where%2520the%2520regularization%2520strength%250Avanishes%252C%2520we%2520show%2520that%2520the%2520dynamic%2520of%2520the%2520feature%2520distribution%2520corresponds%2520to%2520a%250Aweighted%2520ultra-fast%2520diffusion%2520equation.%2520Recent%2520results%2520on%2520the%2520asymptotic%250Abehavior%2520of%2520such%2520PDEs%2520then%2520give%2520quantitative%2520guarantees%2520for%2520the%2520convergence%2520of%250Athe%2520learned%2520feature%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-fast%20feature%20learning%20for%20the%20training%20of%20two-layer%20neural%0A%20%20networks%20in%20the%20two-timescale%20regime&entry.906535625=Rapha%C3%ABl%20Barboni%20and%20Gabriel%20Peyr%C3%A9%20and%20Fran%C3%A7ois-Xavier%20Vialard&entry.1292438233=%20%20We%20study%20the%20convergence%20of%20gradient%20methods%20for%20the%20training%20of%20mean-field%0Asingle-hidden-layer%20neural%20networks%20with%20square%20loss.%20For%20this%20high-dimensional%0Aand%20non-convex%20optimization%20problem%2C%20most%20known%20convergence%20results%20are%20either%0Aqualitative%20or%20rely%20on%20a%20neural%20tangent%20kernel%20analysis%20where%20nonlinear%0Arepresentations%20of%20the%20data%20are%20fixed.%20Using%20that%20this%20problem%20belongs%20to%20the%0Aclass%20of%20separable%20nonlinear%20least%20squares%20problems%2C%20we%20consider%20here%20a%0AVariable%20Projection%20%28VarPro%29%20or%20two-timescale%20learning%20algorithm%2C%20thereby%0Aeliminating%20the%20linear%20variables%20and%20reducing%20the%20learning%20problem%20to%20the%0Atraining%20of%20nonlinear%20features.%20In%20a%20teacher-student%20scenario%2C%20we%20show%20such%20a%0Astrategy%20enables%20provable%20convergence%20rates%20for%20the%20sampling%20of%20a%20teacher%0Afeature%20distribution.%20Precisely%2C%20in%20the%20limit%20where%20the%20regularization%20strength%0Avanishes%2C%20we%20show%20that%20the%20dynamic%20of%20the%20feature%20distribution%20corresponds%20to%20a%0Aweighted%20ultra-fast%20diffusion%20equation.%20Recent%20results%20on%20the%20asymptotic%0Abehavior%20of%20such%20PDEs%20then%20give%20quantitative%20guarantees%20for%20the%20convergence%20of%0Athe%20learned%20feature%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18208v2&entry.124074799=Read"},
{"title": "Learning Null Geodesics for Gravitational Lensing Rendering in General\n  Relativity", "author": "Mingyuan Sun and Zheng Fang and Jiaxu Wang and Kunyi Zhang and Qiang Zhang and Renjing Xu", "abstract": "  We present GravLensX, an innovative method for rendering black holes with\ngravitational lensing effects using neural networks. The methodology involves\ntraining neural networks to fit the spacetime around black holes and then\nemploying these trained models to generate the path of light rays affected by\ngravitational lensing. This enables efficient and scalable simulations of black\nholes with optically thin accretion disks, significantly decreasing the time\nrequired for rendering compared to traditional methods. We validate our\napproach through extensive rendering of multiple black hole systems with\nsuperposed Kerr metric, demonstrating its capability to produce accurate\nvisualizations with significantly $15\\times$ reduced computational time. Our\nfindings suggest that neural networks offer a promising alternative for\nrendering complex astrophysical phenomena, potentially paving a new path to\nastronomical visualization.\n", "link": "http://arxiv.org/abs/2507.15775v1", "date": "2025-07-21", "relevancy": 2.0255, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5184}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.501}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Null%20Geodesics%20for%20Gravitational%20Lensing%20Rendering%20in%20General%0A%20%20Relativity&body=Title%3A%20Learning%20Null%20Geodesics%20for%20Gravitational%20Lensing%20Rendering%20in%20General%0A%20%20Relativity%0AAuthor%3A%20Mingyuan%20Sun%20and%20Zheng%20Fang%20and%20Jiaxu%20Wang%20and%20Kunyi%20Zhang%20and%20Qiang%20Zhang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20We%20present%20GravLensX%2C%20an%20innovative%20method%20for%20rendering%20black%20holes%20with%0Agravitational%20lensing%20effects%20using%20neural%20networks.%20The%20methodology%20involves%0Atraining%20neural%20networks%20to%20fit%20the%20spacetime%20around%20black%20holes%20and%20then%0Aemploying%20these%20trained%20models%20to%20generate%20the%20path%20of%20light%20rays%20affected%20by%0Agravitational%20lensing.%20This%20enables%20efficient%20and%20scalable%20simulations%20of%20black%0Aholes%20with%20optically%20thin%20accretion%20disks%2C%20significantly%20decreasing%20the%20time%0Arequired%20for%20rendering%20compared%20to%20traditional%20methods.%20We%20validate%20our%0Aapproach%20through%20extensive%20rendering%20of%20multiple%20black%20hole%20systems%20with%0Asuperposed%20Kerr%20metric%2C%20demonstrating%20its%20capability%20to%20produce%20accurate%0Avisualizations%20with%20significantly%20%2415%5Ctimes%24%20reduced%20computational%20time.%20Our%0Afindings%20suggest%20that%20neural%20networks%20offer%20a%20promising%20alternative%20for%0Arendering%20complex%20astrophysical%20phenomena%2C%20potentially%20paving%20a%20new%20path%20to%0Aastronomical%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Null%2520Geodesics%2520for%2520Gravitational%2520Lensing%2520Rendering%2520in%2520General%250A%2520%2520Relativity%26entry.906535625%3DMingyuan%2520Sun%2520and%2520Zheng%2520Fang%2520and%2520Jiaxu%2520Wang%2520and%2520Kunyi%2520Zhang%2520and%2520Qiang%2520Zhang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520We%2520present%2520GravLensX%252C%2520an%2520innovative%2520method%2520for%2520rendering%2520black%2520holes%2520with%250Agravitational%2520lensing%2520effects%2520using%2520neural%2520networks.%2520The%2520methodology%2520involves%250Atraining%2520neural%2520networks%2520to%2520fit%2520the%2520spacetime%2520around%2520black%2520holes%2520and%2520then%250Aemploying%2520these%2520trained%2520models%2520to%2520generate%2520the%2520path%2520of%2520light%2520rays%2520affected%2520by%250Agravitational%2520lensing.%2520This%2520enables%2520efficient%2520and%2520scalable%2520simulations%2520of%2520black%250Aholes%2520with%2520optically%2520thin%2520accretion%2520disks%252C%2520significantly%2520decreasing%2520the%2520time%250Arequired%2520for%2520rendering%2520compared%2520to%2520traditional%2520methods.%2520We%2520validate%2520our%250Aapproach%2520through%2520extensive%2520rendering%2520of%2520multiple%2520black%2520hole%2520systems%2520with%250Asuperposed%2520Kerr%2520metric%252C%2520demonstrating%2520its%2520capability%2520to%2520produce%2520accurate%250Avisualizations%2520with%2520significantly%2520%252415%255Ctimes%2524%2520reduced%2520computational%2520time.%2520Our%250Afindings%2520suggest%2520that%2520neural%2520networks%2520offer%2520a%2520promising%2520alternative%2520for%250Arendering%2520complex%2520astrophysical%2520phenomena%252C%2520potentially%2520paving%2520a%2520new%2520path%2520to%250Aastronomical%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Null%20Geodesics%20for%20Gravitational%20Lensing%20Rendering%20in%20General%0A%20%20Relativity&entry.906535625=Mingyuan%20Sun%20and%20Zheng%20Fang%20and%20Jiaxu%20Wang%20and%20Kunyi%20Zhang%20and%20Qiang%20Zhang%20and%20Renjing%20Xu&entry.1292438233=%20%20We%20present%20GravLensX%2C%20an%20innovative%20method%20for%20rendering%20black%20holes%20with%0Agravitational%20lensing%20effects%20using%20neural%20networks.%20The%20methodology%20involves%0Atraining%20neural%20networks%20to%20fit%20the%20spacetime%20around%20black%20holes%20and%20then%0Aemploying%20these%20trained%20models%20to%20generate%20the%20path%20of%20light%20rays%20affected%20by%0Agravitational%20lensing.%20This%20enables%20efficient%20and%20scalable%20simulations%20of%20black%0Aholes%20with%20optically%20thin%20accretion%20disks%2C%20significantly%20decreasing%20the%20time%0Arequired%20for%20rendering%20compared%20to%20traditional%20methods.%20We%20validate%20our%0Aapproach%20through%20extensive%20rendering%20of%20multiple%20black%20hole%20systems%20with%0Asuperposed%20Kerr%20metric%2C%20demonstrating%20its%20capability%20to%20produce%20accurate%0Avisualizations%20with%20significantly%20%2415%5Ctimes%24%20reduced%20computational%20time.%20Our%0Afindings%20suggest%20that%20neural%20networks%20offer%20a%20promising%20alternative%20for%0Arendering%20complex%20astrophysical%20phenomena%2C%20potentially%20paving%20a%20new%20path%20to%0Aastronomical%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15775v1&entry.124074799=Read"},
{"title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and\n  Foveated Vision Transformers", "author": "Ian Chuang and Andrew Lee and Dechen Gao and Jinyu Zou and Iman Soltani", "abstract": "  Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/\n", "link": "http://arxiv.org/abs/2507.15833v1", "date": "2025-07-21", "relevancy": 1.6797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5663}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5584}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers&body=Title%3A%20Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers%0AAuthor%3A%20Ian%20Chuang%20and%20Andrew%20Lee%20and%20Dechen%20Gao%20and%20Jinyu%20Zou%20and%20Iman%20Soltani%0AAbstract%3A%20%20%20Human%20vision%20is%20a%20highly%20active%20process%20driven%20by%20gaze%2C%20which%20directs%0Aattention%20and%20fixation%20to%20task-relevant%20regions%20and%20dramatically%20reduces%20visual%0Aprocessing.%20In%20contrast%2C%20robot%20learning%20systems%20typically%20rely%20on%20passive%2C%0Auniform%20processing%20of%20raw%20camera%20images.%20In%20this%20work%2C%20we%20explore%20how%0Aincorporating%20human-like%20active%20gaze%20into%20robotic%20policies%20can%20enhance%20both%0Aefficiency%20and%20performance.%20We%20build%20on%20recent%20advances%20in%20foveated%20image%0Aprocessing%20and%20apply%20them%20to%20an%20Active%20Vision%20robot%20system%20that%20emulates%20both%0Ahuman%20head%20movement%20and%20eye%20tracking.%20Extending%20prior%20work%20on%20the%20AV-ALOHA%0Arobot%20simulation%20platform%2C%20we%20introduce%20a%20framework%20for%20simultaneously%0Acollecting%20eye-tracking%20data%20and%20robot%20demonstrations%20from%20a%20human%20operator%20as%0Awell%20as%20a%20simulation%20benchmark%20and%20dataset%20for%20training%20robot%20policies%20that%0Aincorporate%20human%20gaze.%20Given%20the%20widespread%20use%20of%20Vision%20Transformers%20%28ViTs%29%0Ain%20robot%20learning%2C%20we%20integrate%20gaze%20information%20into%20ViTs%20using%20a%20foveated%0Apatch%20tokenization%20scheme%20inspired%20by%20recent%20work%20in%20image%20segmentation.%0ACompared%20to%20uniform%20patch%20tokenization%2C%20this%20significantly%20reduces%20the%20number%0Aof%20tokens-and%20thus%20computation-without%20sacrificing%20visual%20fidelity%20near%20regions%0Aof%20interest.%20We%20also%20explore%20two%20approaches%20to%20gaze%20imitation%20and%20prediction%0Afrom%20human%20data.%20The%20first%20is%20a%20two-stage%20model%20that%20predicts%20gaze%20to%20guide%0Afoveation%20and%20action%3B%20the%20second%20integrates%20gaze%20into%20the%20action%20space%2C%0Aallowing%20the%20policy%20to%20jointly%20predict%20gaze%20and%20actions%20end-to-end.%20Our%20results%0Ashow%20that%20our%20method%20for%20foveated%20robot%20vision%20not%20only%20drastically%20reduces%0Acomputational%20overhead%2C%20but%20also%20improves%20performance%20for%20high%20precision%20tasks%0Aand%20robustness%20to%20unseen%20distractors.%20Together%2C%20these%20findings%20suggest%20that%0Ahuman-inspired%20visual%20processing%20offers%20a%20useful%20inductive%20bias%20for%20robotic%0Avision%20systems.%20https%3A//ian-chuang.github.io/gaze-av-aloha/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%252C%2520Focus%252C%2520Act%253A%2520Efficient%2520and%2520Robust%2520Robot%2520Learning%2520via%2520Human%2520Gaze%2520and%250A%2520%2520Foveated%2520Vision%2520Transformers%26entry.906535625%3DIan%2520Chuang%2520and%2520Andrew%2520Lee%2520and%2520Dechen%2520Gao%2520and%2520Jinyu%2520Zou%2520and%2520Iman%2520Soltani%26entry.1292438233%3D%2520%2520Human%2520vision%2520is%2520a%2520highly%2520active%2520process%2520driven%2520by%2520gaze%252C%2520which%2520directs%250Aattention%2520and%2520fixation%2520to%2520task-relevant%2520regions%2520and%2520dramatically%2520reduces%2520visual%250Aprocessing.%2520In%2520contrast%252C%2520robot%2520learning%2520systems%2520typically%2520rely%2520on%2520passive%252C%250Auniform%2520processing%2520of%2520raw%2520camera%2520images.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%250Aincorporating%2520human-like%2520active%2520gaze%2520into%2520robotic%2520policies%2520can%2520enhance%2520both%250Aefficiency%2520and%2520performance.%2520We%2520build%2520on%2520recent%2520advances%2520in%2520foveated%2520image%250Aprocessing%2520and%2520apply%2520them%2520to%2520an%2520Active%2520Vision%2520robot%2520system%2520that%2520emulates%2520both%250Ahuman%2520head%2520movement%2520and%2520eye%2520tracking.%2520Extending%2520prior%2520work%2520on%2520the%2520AV-ALOHA%250Arobot%2520simulation%2520platform%252C%2520we%2520introduce%2520a%2520framework%2520for%2520simultaneously%250Acollecting%2520eye-tracking%2520data%2520and%2520robot%2520demonstrations%2520from%2520a%2520human%2520operator%2520as%250Awell%2520as%2520a%2520simulation%2520benchmark%2520and%2520dataset%2520for%2520training%2520robot%2520policies%2520that%250Aincorporate%2520human%2520gaze.%2520Given%2520the%2520widespread%2520use%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529%250Ain%2520robot%2520learning%252C%2520we%2520integrate%2520gaze%2520information%2520into%2520ViTs%2520using%2520a%2520foveated%250Apatch%2520tokenization%2520scheme%2520inspired%2520by%2520recent%2520work%2520in%2520image%2520segmentation.%250ACompared%2520to%2520uniform%2520patch%2520tokenization%252C%2520this%2520significantly%2520reduces%2520the%2520number%250Aof%2520tokens-and%2520thus%2520computation-without%2520sacrificing%2520visual%2520fidelity%2520near%2520regions%250Aof%2520interest.%2520We%2520also%2520explore%2520two%2520approaches%2520to%2520gaze%2520imitation%2520and%2520prediction%250Afrom%2520human%2520data.%2520The%2520first%2520is%2520a%2520two-stage%2520model%2520that%2520predicts%2520gaze%2520to%2520guide%250Afoveation%2520and%2520action%253B%2520the%2520second%2520integrates%2520gaze%2520into%2520the%2520action%2520space%252C%250Aallowing%2520the%2520policy%2520to%2520jointly%2520predict%2520gaze%2520and%2520actions%2520end-to-end.%2520Our%2520results%250Ashow%2520that%2520our%2520method%2520for%2520foveated%2520robot%2520vision%2520not%2520only%2520drastically%2520reduces%250Acomputational%2520overhead%252C%2520but%2520also%2520improves%2520performance%2520for%2520high%2520precision%2520tasks%250Aand%2520robustness%2520to%2520unseen%2520distractors.%2520Together%252C%2520these%2520findings%2520suggest%2520that%250Ahuman-inspired%2520visual%2520processing%2520offers%2520a%2520useful%2520inductive%2520bias%2520for%2520robotic%250Avision%2520systems.%2520https%253A//ian-chuang.github.io/gaze-av-aloha/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%2C%20Focus%2C%20Act%3A%20Efficient%20and%20Robust%20Robot%20Learning%20via%20Human%20Gaze%20and%0A%20%20Foveated%20Vision%20Transformers&entry.906535625=Ian%20Chuang%20and%20Andrew%20Lee%20and%20Dechen%20Gao%20and%20Jinyu%20Zou%20and%20Iman%20Soltani&entry.1292438233=%20%20Human%20vision%20is%20a%20highly%20active%20process%20driven%20by%20gaze%2C%20which%20directs%0Aattention%20and%20fixation%20to%20task-relevant%20regions%20and%20dramatically%20reduces%20visual%0Aprocessing.%20In%20contrast%2C%20robot%20learning%20systems%20typically%20rely%20on%20passive%2C%0Auniform%20processing%20of%20raw%20camera%20images.%20In%20this%20work%2C%20we%20explore%20how%0Aincorporating%20human-like%20active%20gaze%20into%20robotic%20policies%20can%20enhance%20both%0Aefficiency%20and%20performance.%20We%20build%20on%20recent%20advances%20in%20foveated%20image%0Aprocessing%20and%20apply%20them%20to%20an%20Active%20Vision%20robot%20system%20that%20emulates%20both%0Ahuman%20head%20movement%20and%20eye%20tracking.%20Extending%20prior%20work%20on%20the%20AV-ALOHA%0Arobot%20simulation%20platform%2C%20we%20introduce%20a%20framework%20for%20simultaneously%0Acollecting%20eye-tracking%20data%20and%20robot%20demonstrations%20from%20a%20human%20operator%20as%0Awell%20as%20a%20simulation%20benchmark%20and%20dataset%20for%20training%20robot%20policies%20that%0Aincorporate%20human%20gaze.%20Given%20the%20widespread%20use%20of%20Vision%20Transformers%20%28ViTs%29%0Ain%20robot%20learning%2C%20we%20integrate%20gaze%20information%20into%20ViTs%20using%20a%20foveated%0Apatch%20tokenization%20scheme%20inspired%20by%20recent%20work%20in%20image%20segmentation.%0ACompared%20to%20uniform%20patch%20tokenization%2C%20this%20significantly%20reduces%20the%20number%0Aof%20tokens-and%20thus%20computation-without%20sacrificing%20visual%20fidelity%20near%20regions%0Aof%20interest.%20We%20also%20explore%20two%20approaches%20to%20gaze%20imitation%20and%20prediction%0Afrom%20human%20data.%20The%20first%20is%20a%20two-stage%20model%20that%20predicts%20gaze%20to%20guide%0Afoveation%20and%20action%3B%20the%20second%20integrates%20gaze%20into%20the%20action%20space%2C%0Aallowing%20the%20policy%20to%20jointly%20predict%20gaze%20and%20actions%20end-to-end.%20Our%20results%0Ashow%20that%20our%20method%20for%20foveated%20robot%20vision%20not%20only%20drastically%20reduces%0Acomputational%20overhead%2C%20but%20also%20improves%20performance%20for%20high%20precision%20tasks%0Aand%20robustness%20to%20unseen%20distractors.%20Together%2C%20these%20findings%20suggest%20that%0Ahuman-inspired%20visual%20processing%20offers%20a%20useful%20inductive%20bias%20for%20robotic%0Avision%20systems.%20https%3A//ian-chuang.github.io/gaze-av-aloha/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15833v1&entry.124074799=Read"},
{"title": "Omegance: A Single Parameter for Various Granularities in\n  Diffusion-Based Synthesis", "author": "Xinyu Hou and Zongsheng Yue and Xiaoming Li and Chen Change Loy", "abstract": "  In this work, we show that we only need a single parameter $\\omega$ to\neffectively control granularity in diffusion-based synthesis. This parameter is\nincorporated during the denoising steps of the diffusion model's reverse\nprocess. This simple approach does not require model retraining or\narchitectural modifications and incurs negligible computational overhead, yet\nenables precise control over the level of details in the generated outputs.\nMoreover, spatial masks or denoising schedules with varying $\\omega$ values can\nbe applied to achieve region-specific or timestep-specific granularity control.\nExternal control signals or reference images can guide the creation of precise\n$\\omega$ masks, allowing targeted granularity adjustments. Despite its\nsimplicity, the method demonstrates impressive performance across various image\nand video synthesis tasks and is adaptable to advanced diffusion models. The\ncode is available at https://github.com/itsmag11/Omegance.\n", "link": "http://arxiv.org/abs/2411.17769v2", "date": "2025-07-21", "relevancy": 1.6723, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5607}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5536}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omegance%3A%20A%20Single%20Parameter%20for%20Various%20Granularities%20in%0A%20%20Diffusion-Based%20Synthesis&body=Title%3A%20Omegance%3A%20A%20Single%20Parameter%20for%20Various%20Granularities%20in%0A%20%20Diffusion-Based%20Synthesis%0AAuthor%3A%20Xinyu%20Hou%20and%20Zongsheng%20Yue%20and%20Xiaoming%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20show%20that%20we%20only%20need%20a%20single%20parameter%20%24%5Comega%24%20to%0Aeffectively%20control%20granularity%20in%20diffusion-based%20synthesis.%20This%20parameter%20is%0Aincorporated%20during%20the%20denoising%20steps%20of%20the%20diffusion%20model%27s%20reverse%0Aprocess.%20This%20simple%20approach%20does%20not%20require%20model%20retraining%20or%0Aarchitectural%20modifications%20and%20incurs%20negligible%20computational%20overhead%2C%20yet%0Aenables%20precise%20control%20over%20the%20level%20of%20details%20in%20the%20generated%20outputs.%0AMoreover%2C%20spatial%20masks%20or%20denoising%20schedules%20with%20varying%20%24%5Comega%24%20values%20can%0Abe%20applied%20to%20achieve%20region-specific%20or%20timestep-specific%20granularity%20control.%0AExternal%20control%20signals%20or%20reference%20images%20can%20guide%20the%20creation%20of%20precise%0A%24%5Comega%24%20masks%2C%20allowing%20targeted%20granularity%20adjustments.%20Despite%20its%0Asimplicity%2C%20the%20method%20demonstrates%20impressive%20performance%20across%20various%20image%0Aand%20video%20synthesis%20tasks%20and%20is%20adaptable%20to%20advanced%20diffusion%20models.%20The%0Acode%20is%20available%20at%20https%3A//github.com/itsmag11/Omegance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmegance%253A%2520A%2520Single%2520Parameter%2520for%2520Various%2520Granularities%2520in%250A%2520%2520Diffusion-Based%2520Synthesis%26entry.906535625%3DXinyu%2520Hou%2520and%2520Zongsheng%2520Yue%2520and%2520Xiaoming%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520we%2520only%2520need%2520a%2520single%2520parameter%2520%2524%255Comega%2524%2520to%250Aeffectively%2520control%2520granularity%2520in%2520diffusion-based%2520synthesis.%2520This%2520parameter%2520is%250Aincorporated%2520during%2520the%2520denoising%2520steps%2520of%2520the%2520diffusion%2520model%2527s%2520reverse%250Aprocess.%2520This%2520simple%2520approach%2520does%2520not%2520require%2520model%2520retraining%2520or%250Aarchitectural%2520modifications%2520and%2520incurs%2520negligible%2520computational%2520overhead%252C%2520yet%250Aenables%2520precise%2520control%2520over%2520the%2520level%2520of%2520details%2520in%2520the%2520generated%2520outputs.%250AMoreover%252C%2520spatial%2520masks%2520or%2520denoising%2520schedules%2520with%2520varying%2520%2524%255Comega%2524%2520values%2520can%250Abe%2520applied%2520to%2520achieve%2520region-specific%2520or%2520timestep-specific%2520granularity%2520control.%250AExternal%2520control%2520signals%2520or%2520reference%2520images%2520can%2520guide%2520the%2520creation%2520of%2520precise%250A%2524%255Comega%2524%2520masks%252C%2520allowing%2520targeted%2520granularity%2520adjustments.%2520Despite%2520its%250Asimplicity%252C%2520the%2520method%2520demonstrates%2520impressive%2520performance%2520across%2520various%2520image%250Aand%2520video%2520synthesis%2520tasks%2520and%2520is%2520adaptable%2520to%2520advanced%2520diffusion%2520models.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/itsmag11/Omegance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omegance%3A%20A%20Single%20Parameter%20for%20Various%20Granularities%20in%0A%20%20Diffusion-Based%20Synthesis&entry.906535625=Xinyu%20Hou%20and%20Zongsheng%20Yue%20and%20Xiaoming%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20In%20this%20work%2C%20we%20show%20that%20we%20only%20need%20a%20single%20parameter%20%24%5Comega%24%20to%0Aeffectively%20control%20granularity%20in%20diffusion-based%20synthesis.%20This%20parameter%20is%0Aincorporated%20during%20the%20denoising%20steps%20of%20the%20diffusion%20model%27s%20reverse%0Aprocess.%20This%20simple%20approach%20does%20not%20require%20model%20retraining%20or%0Aarchitectural%20modifications%20and%20incurs%20negligible%20computational%20overhead%2C%20yet%0Aenables%20precise%20control%20over%20the%20level%20of%20details%20in%20the%20generated%20outputs.%0AMoreover%2C%20spatial%20masks%20or%20denoising%20schedules%20with%20varying%20%24%5Comega%24%20values%20can%0Abe%20applied%20to%20achieve%20region-specific%20or%20timestep-specific%20granularity%20control.%0AExternal%20control%20signals%20or%20reference%20images%20can%20guide%20the%20creation%20of%20precise%0A%24%5Comega%24%20masks%2C%20allowing%20targeted%20granularity%20adjustments.%20Despite%20its%0Asimplicity%2C%20the%20method%20demonstrates%20impressive%20performance%20across%20various%20image%0Aand%20video%20synthesis%20tasks%20and%20is%20adaptable%20to%20advanced%20diffusion%20models.%20The%0Acode%20is%20available%20at%20https%3A//github.com/itsmag11/Omegance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17769v2&entry.124074799=Read"},
{"title": "Automated Classification of Volcanic Earthquakes Using Transformer\n  Encoders: Insights into Data Quality and Model Interpretability", "author": "Y. Suzuki and Y. Yukutake and T. Ohminato and M. Yamasaki and Ahyi Kim", "abstract": "  Precisely classifying earthquake types is crucial for elucidating the\nrelationship between volcanic earthquakes and volcanic activity. However,\ntraditional methods rely on subjective human judgment, which requires\nconsiderable time and effort. To address this issue, we developed a deep\nlearning model using a transformer encoder for a more objective and efficient\nclassification. Tested on Mount Asama's diverse seismic activity, our model\nachieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency\nearthquakes, and 0.980 for noise), superior to a conventional CNN-based method.\nTo enhance interpretability, attention weight visualizations were analyzed,\nrevealing that the model focuses on key waveform features similarly to human\nexperts. However, inconsistencies in training data, such as ambiguously labeled\nB-type events with S-waves, were found to influence classification accuracy and\nattention weight distributions. Experiments addressing data selection and\naugmentation demonstrated the importance of balancing data quality and\ndiversity. In addition, stations within 3 km of the crater played an important\nrole in improving model performance and interpretability. These findings\nhighlight the potential of Transformer-based models for automated volcanic\nearthquake classification, particularly in improving efficiency and\ninterpretability. By addressing challenges such as data imbalance and\nsubjective labeling, our approach provides a robust framework for understanding\nseismic activity at Mount Asama. Moreover, this framework offers opportunities\nfor transfer learning to other volcanic regions, paving the way for enhanced\nvolcanic hazard assessments and disaster mitigation strategies.\n", "link": "http://arxiv.org/abs/2507.01260v2", "date": "2025-07-21", "relevancy": 1.9131, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Classification%20of%20Volcanic%20Earthquakes%20Using%20Transformer%0A%20%20Encoders%3A%20Insights%20into%20Data%20Quality%20and%20Model%20Interpretability&body=Title%3A%20Automated%20Classification%20of%20Volcanic%20Earthquakes%20Using%20Transformer%0A%20%20Encoders%3A%20Insights%20into%20Data%20Quality%20and%20Model%20Interpretability%0AAuthor%3A%20Y.%20Suzuki%20and%20Y.%20Yukutake%20and%20T.%20Ohminato%20and%20M.%20Yamasaki%20and%20Ahyi%20Kim%0AAbstract%3A%20%20%20Precisely%20classifying%20earthquake%20types%20is%20crucial%20for%20elucidating%20the%0Arelationship%20between%20volcanic%20earthquakes%20and%20volcanic%20activity.%20However%2C%0Atraditional%20methods%20rely%20on%20subjective%20human%20judgment%2C%20which%20requires%0Aconsiderable%20time%20and%20effort.%20To%20address%20this%20issue%2C%20we%20developed%20a%20deep%0Alearning%20model%20using%20a%20transformer%20encoder%20for%20a%20more%20objective%20and%20efficient%0Aclassification.%20Tested%20on%20Mount%20Asama%27s%20diverse%20seismic%20activity%2C%20our%20model%0Aachieved%20high%20F1%20scores%20%280.930%20for%20volcano%20tectonic%2C%200.931%20for%20low-frequency%0Aearthquakes%2C%20and%200.980%20for%20noise%29%2C%20superior%20to%20a%20conventional%20CNN-based%20method.%0ATo%20enhance%20interpretability%2C%20attention%20weight%20visualizations%20were%20analyzed%2C%0Arevealing%20that%20the%20model%20focuses%20on%20key%20waveform%20features%20similarly%20to%20human%0Aexperts.%20However%2C%20inconsistencies%20in%20training%20data%2C%20such%20as%20ambiguously%20labeled%0AB-type%20events%20with%20S-waves%2C%20were%20found%20to%20influence%20classification%20accuracy%20and%0Aattention%20weight%20distributions.%20Experiments%20addressing%20data%20selection%20and%0Aaugmentation%20demonstrated%20the%20importance%20of%20balancing%20data%20quality%20and%0Adiversity.%20In%20addition%2C%20stations%20within%203%20km%20of%20the%20crater%20played%20an%20important%0Arole%20in%20improving%20model%20performance%20and%20interpretability.%20These%20findings%0Ahighlight%20the%20potential%20of%20Transformer-based%20models%20for%20automated%20volcanic%0Aearthquake%20classification%2C%20particularly%20in%20improving%20efficiency%20and%0Ainterpretability.%20By%20addressing%20challenges%20such%20as%20data%20imbalance%20and%0Asubjective%20labeling%2C%20our%20approach%20provides%20a%20robust%20framework%20for%20understanding%0Aseismic%20activity%20at%20Mount%20Asama.%20Moreover%2C%20this%20framework%20offers%20opportunities%0Afor%20transfer%20learning%20to%20other%20volcanic%20regions%2C%20paving%20the%20way%20for%20enhanced%0Avolcanic%20hazard%20assessments%20and%20disaster%20mitigation%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01260v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Classification%2520of%2520Volcanic%2520Earthquakes%2520Using%2520Transformer%250A%2520%2520Encoders%253A%2520Insights%2520into%2520Data%2520Quality%2520and%2520Model%2520Interpretability%26entry.906535625%3DY.%2520Suzuki%2520and%2520Y.%2520Yukutake%2520and%2520T.%2520Ohminato%2520and%2520M.%2520Yamasaki%2520and%2520Ahyi%2520Kim%26entry.1292438233%3D%2520%2520Precisely%2520classifying%2520earthquake%2520types%2520is%2520crucial%2520for%2520elucidating%2520the%250Arelationship%2520between%2520volcanic%2520earthquakes%2520and%2520volcanic%2520activity.%2520However%252C%250Atraditional%2520methods%2520rely%2520on%2520subjective%2520human%2520judgment%252C%2520which%2520requires%250Aconsiderable%2520time%2520and%2520effort.%2520To%2520address%2520this%2520issue%252C%2520we%2520developed%2520a%2520deep%250Alearning%2520model%2520using%2520a%2520transformer%2520encoder%2520for%2520a%2520more%2520objective%2520and%2520efficient%250Aclassification.%2520Tested%2520on%2520Mount%2520Asama%2527s%2520diverse%2520seismic%2520activity%252C%2520our%2520model%250Aachieved%2520high%2520F1%2520scores%2520%25280.930%2520for%2520volcano%2520tectonic%252C%25200.931%2520for%2520low-frequency%250Aearthquakes%252C%2520and%25200.980%2520for%2520noise%2529%252C%2520superior%2520to%2520a%2520conventional%2520CNN-based%2520method.%250ATo%2520enhance%2520interpretability%252C%2520attention%2520weight%2520visualizations%2520were%2520analyzed%252C%250Arevealing%2520that%2520the%2520model%2520focuses%2520on%2520key%2520waveform%2520features%2520similarly%2520to%2520human%250Aexperts.%2520However%252C%2520inconsistencies%2520in%2520training%2520data%252C%2520such%2520as%2520ambiguously%2520labeled%250AB-type%2520events%2520with%2520S-waves%252C%2520were%2520found%2520to%2520influence%2520classification%2520accuracy%2520and%250Aattention%2520weight%2520distributions.%2520Experiments%2520addressing%2520data%2520selection%2520and%250Aaugmentation%2520demonstrated%2520the%2520importance%2520of%2520balancing%2520data%2520quality%2520and%250Adiversity.%2520In%2520addition%252C%2520stations%2520within%25203%2520km%2520of%2520the%2520crater%2520played%2520an%2520important%250Arole%2520in%2520improving%2520model%2520performance%2520and%2520interpretability.%2520These%2520findings%250Ahighlight%2520the%2520potential%2520of%2520Transformer-based%2520models%2520for%2520automated%2520volcanic%250Aearthquake%2520classification%252C%2520particularly%2520in%2520improving%2520efficiency%2520and%250Ainterpretability.%2520By%2520addressing%2520challenges%2520such%2520as%2520data%2520imbalance%2520and%250Asubjective%2520labeling%252C%2520our%2520approach%2520provides%2520a%2520robust%2520framework%2520for%2520understanding%250Aseismic%2520activity%2520at%2520Mount%2520Asama.%2520Moreover%252C%2520this%2520framework%2520offers%2520opportunities%250Afor%2520transfer%2520learning%2520to%2520other%2520volcanic%2520regions%252C%2520paving%2520the%2520way%2520for%2520enhanced%250Avolcanic%2520hazard%2520assessments%2520and%2520disaster%2520mitigation%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01260v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Classification%20of%20Volcanic%20Earthquakes%20Using%20Transformer%0A%20%20Encoders%3A%20Insights%20into%20Data%20Quality%20and%20Model%20Interpretability&entry.906535625=Y.%20Suzuki%20and%20Y.%20Yukutake%20and%20T.%20Ohminato%20and%20M.%20Yamasaki%20and%20Ahyi%20Kim&entry.1292438233=%20%20Precisely%20classifying%20earthquake%20types%20is%20crucial%20for%20elucidating%20the%0Arelationship%20between%20volcanic%20earthquakes%20and%20volcanic%20activity.%20However%2C%0Atraditional%20methods%20rely%20on%20subjective%20human%20judgment%2C%20which%20requires%0Aconsiderable%20time%20and%20effort.%20To%20address%20this%20issue%2C%20we%20developed%20a%20deep%0Alearning%20model%20using%20a%20transformer%20encoder%20for%20a%20more%20objective%20and%20efficient%0Aclassification.%20Tested%20on%20Mount%20Asama%27s%20diverse%20seismic%20activity%2C%20our%20model%0Aachieved%20high%20F1%20scores%20%280.930%20for%20volcano%20tectonic%2C%200.931%20for%20low-frequency%0Aearthquakes%2C%20and%200.980%20for%20noise%29%2C%20superior%20to%20a%20conventional%20CNN-based%20method.%0ATo%20enhance%20interpretability%2C%20attention%20weight%20visualizations%20were%20analyzed%2C%0Arevealing%20that%20the%20model%20focuses%20on%20key%20waveform%20features%20similarly%20to%20human%0Aexperts.%20However%2C%20inconsistencies%20in%20training%20data%2C%20such%20as%20ambiguously%20labeled%0AB-type%20events%20with%20S-waves%2C%20were%20found%20to%20influence%20classification%20accuracy%20and%0Aattention%20weight%20distributions.%20Experiments%20addressing%20data%20selection%20and%0Aaugmentation%20demonstrated%20the%20importance%20of%20balancing%20data%20quality%20and%0Adiversity.%20In%20addition%2C%20stations%20within%203%20km%20of%20the%20crater%20played%20an%20important%0Arole%20in%20improving%20model%20performance%20and%20interpretability.%20These%20findings%0Ahighlight%20the%20potential%20of%20Transformer-based%20models%20for%20automated%20volcanic%0Aearthquake%20classification%2C%20particularly%20in%20improving%20efficiency%20and%0Ainterpretability.%20By%20addressing%20challenges%20such%20as%20data%20imbalance%20and%0Asubjective%20labeling%2C%20our%20approach%20provides%20a%20robust%20framework%20for%20understanding%0Aseismic%20activity%20at%20Mount%20Asama.%20Moreover%2C%20this%20framework%20offers%20opportunities%0Afor%20transfer%20learning%20to%20other%20volcanic%20regions%2C%20paving%20the%20way%20for%20enhanced%0Avolcanic%20hazard%20assessments%20and%20disaster%20mitigation%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01260v2&entry.124074799=Read"},
{"title": "Doing More with Less: A Survey on Routing Strategies for Resource\n  Optimisation in Large Language Model-Based Systems", "author": "Clovis Varangot-Reille and Christophe Bouvard and Antoine Gourru and Mathieu Ciancone and Marion Schaeffer and Fran\u00e7ois Jacquenet", "abstract": "  Large Language Model (LLM)-based systems, i.e. interconnected elements that\ninclude an LLM as a central component, such as conversational agents, are\nusually designed with monolithic, static architectures that rely on a single,\ngeneral-purpose LLM to handle all user queries. However, these systems may be\ninefficient as different queries may require different levels of reasoning,\ndomain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o,\nClaude-Sonnet) perform well across a wide range of tasks, they may incur\nsignificant financial, energy and computational costs. These costs may be\ndisproportionate for simpler queries, resulting in unnecessary resource\nutilisation. A routing mechanism can therefore be employed to route queries to\nmore appropriate components, such as smaller or specialised models, thereby\nimproving efficiency and optimising resource consumption. This survey aims to\nprovide a comprehensive overview of routing strategies in LLM-based systems.\nSpecifically, it reviews when, why, and how routing should be integrated into\nLLM pipelines to improve efficiency, scalability, and performance. We define\nthe objectives to optimise, such as cost minimisation and performance\nmaximisation, and discuss the timing of routing within the LLM workflow,\nwhether it occurs before or after generation. We also detail the various\nimplementation strategies, including similarity-based, supervised,\nreinforcement learning-based, and generative methods. Practical considerations\nsuch as industrial applications and current limitations are also examined, like\nstandardising routing experiments, accounting for non-financial costs, and\ndesigning adaptive strategies. By formalising routing as a performance-cost\noptimisation problem, this survey provides tools and directions to guide future\nresearch and development of adaptive low-cost LLM-based systems.\n", "link": "http://arxiv.org/abs/2502.00409v3", "date": "2025-07-21", "relevancy": 1.8917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doing%20More%20with%20Less%3A%20A%20Survey%20on%20Routing%20Strategies%20for%20Resource%0A%20%20Optimisation%20in%20Large%20Language%20Model-Based%20Systems&body=Title%3A%20Doing%20More%20with%20Less%3A%20A%20Survey%20on%20Routing%20Strategies%20for%20Resource%0A%20%20Optimisation%20in%20Large%20Language%20Model-Based%20Systems%0AAuthor%3A%20Clovis%20Varangot-Reille%20and%20Christophe%20Bouvard%20and%20Antoine%20Gourru%20and%20Mathieu%20Ciancone%20and%20Marion%20Schaeffer%20and%20Fran%C3%A7ois%20Jacquenet%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20systems%2C%20i.e.%20interconnected%20elements%20that%0Ainclude%20an%20LLM%20as%20a%20central%20component%2C%20such%20as%20conversational%20agents%2C%20are%0Ausually%20designed%20with%20monolithic%2C%20static%20architectures%20that%20rely%20on%20a%20single%2C%0Ageneral-purpose%20LLM%20to%20handle%20all%20user%20queries.%20However%2C%20these%20systems%20may%20be%0Ainefficient%20as%20different%20queries%20may%20require%20different%20levels%20of%20reasoning%2C%0Adomain%20knowledge%20or%20pre-processing.%20While%20generalist%20LLMs%20%28e.g.%20GPT-4o%2C%0AClaude-Sonnet%29%20perform%20well%20across%20a%20wide%20range%20of%20tasks%2C%20they%20may%20incur%0Asignificant%20financial%2C%20energy%20and%20computational%20costs.%20These%20costs%20may%20be%0Adisproportionate%20for%20simpler%20queries%2C%20resulting%20in%20unnecessary%20resource%0Autilisation.%20A%20routing%20mechanism%20can%20therefore%20be%20employed%20to%20route%20queries%20to%0Amore%20appropriate%20components%2C%20such%20as%20smaller%20or%20specialised%20models%2C%20thereby%0Aimproving%20efficiency%20and%20optimising%20resource%20consumption.%20This%20survey%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20routing%20strategies%20in%20LLM-based%20systems.%0ASpecifically%2C%20it%20reviews%20when%2C%20why%2C%20and%20how%20routing%20should%20be%20integrated%20into%0ALLM%20pipelines%20to%20improve%20efficiency%2C%20scalability%2C%20and%20performance.%20We%20define%0Athe%20objectives%20to%20optimise%2C%20such%20as%20cost%20minimisation%20and%20performance%0Amaximisation%2C%20and%20discuss%20the%20timing%20of%20routing%20within%20the%20LLM%20workflow%2C%0Awhether%20it%20occurs%20before%20or%20after%20generation.%20We%20also%20detail%20the%20various%0Aimplementation%20strategies%2C%20including%20similarity-based%2C%20supervised%2C%0Areinforcement%20learning-based%2C%20and%20generative%20methods.%20Practical%20considerations%0Asuch%20as%20industrial%20applications%20and%20current%20limitations%20are%20also%20examined%2C%20like%0Astandardising%20routing%20experiments%2C%20accounting%20for%20non-financial%20costs%2C%20and%0Adesigning%20adaptive%20strategies.%20By%20formalising%20routing%20as%20a%20performance-cost%0Aoptimisation%20problem%2C%20this%20survey%20provides%20tools%20and%20directions%20to%20guide%20future%0Aresearch%20and%20development%20of%20adaptive%20low-cost%20LLM-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoing%2520More%2520with%2520Less%253A%2520A%2520Survey%2520on%2520Routing%2520Strategies%2520for%2520Resource%250A%2520%2520Optimisation%2520in%2520Large%2520Language%2520Model-Based%2520Systems%26entry.906535625%3DClovis%2520Varangot-Reille%2520and%2520Christophe%2520Bouvard%2520and%2520Antoine%2520Gourru%2520and%2520Mathieu%2520Ciancone%2520and%2520Marion%2520Schaeffer%2520and%2520Fran%25C3%25A7ois%2520Jacquenet%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520systems%252C%2520i.e.%2520interconnected%2520elements%2520that%250Ainclude%2520an%2520LLM%2520as%2520a%2520central%2520component%252C%2520such%2520as%2520conversational%2520agents%252C%2520are%250Ausually%2520designed%2520with%2520monolithic%252C%2520static%2520architectures%2520that%2520rely%2520on%2520a%2520single%252C%250Ageneral-purpose%2520LLM%2520to%2520handle%2520all%2520user%2520queries.%2520However%252C%2520these%2520systems%2520may%2520be%250Ainefficient%2520as%2520different%2520queries%2520may%2520require%2520different%2520levels%2520of%2520reasoning%252C%250Adomain%2520knowledge%2520or%2520pre-processing.%2520While%2520generalist%2520LLMs%2520%2528e.g.%2520GPT-4o%252C%250AClaude-Sonnet%2529%2520perform%2520well%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520they%2520may%2520incur%250Asignificant%2520financial%252C%2520energy%2520and%2520computational%2520costs.%2520These%2520costs%2520may%2520be%250Adisproportionate%2520for%2520simpler%2520queries%252C%2520resulting%2520in%2520unnecessary%2520resource%250Autilisation.%2520A%2520routing%2520mechanism%2520can%2520therefore%2520be%2520employed%2520to%2520route%2520queries%2520to%250Amore%2520appropriate%2520components%252C%2520such%2520as%2520smaller%2520or%2520specialised%2520models%252C%2520thereby%250Aimproving%2520efficiency%2520and%2520optimising%2520resource%2520consumption.%2520This%2520survey%2520aims%2520to%250Aprovide%2520a%2520comprehensive%2520overview%2520of%2520routing%2520strategies%2520in%2520LLM-based%2520systems.%250ASpecifically%252C%2520it%2520reviews%2520when%252C%2520why%252C%2520and%2520how%2520routing%2520should%2520be%2520integrated%2520into%250ALLM%2520pipelines%2520to%2520improve%2520efficiency%252C%2520scalability%252C%2520and%2520performance.%2520We%2520define%250Athe%2520objectives%2520to%2520optimise%252C%2520such%2520as%2520cost%2520minimisation%2520and%2520performance%250Amaximisation%252C%2520and%2520discuss%2520the%2520timing%2520of%2520routing%2520within%2520the%2520LLM%2520workflow%252C%250Awhether%2520it%2520occurs%2520before%2520or%2520after%2520generation.%2520We%2520also%2520detail%2520the%2520various%250Aimplementation%2520strategies%252C%2520including%2520similarity-based%252C%2520supervised%252C%250Areinforcement%2520learning-based%252C%2520and%2520generative%2520methods.%2520Practical%2520considerations%250Asuch%2520as%2520industrial%2520applications%2520and%2520current%2520limitations%2520are%2520also%2520examined%252C%2520like%250Astandardising%2520routing%2520experiments%252C%2520accounting%2520for%2520non-financial%2520costs%252C%2520and%250Adesigning%2520adaptive%2520strategies.%2520By%2520formalising%2520routing%2520as%2520a%2520performance-cost%250Aoptimisation%2520problem%252C%2520this%2520survey%2520provides%2520tools%2520and%2520directions%2520to%2520guide%2520future%250Aresearch%2520and%2520development%2520of%2520adaptive%2520low-cost%2520LLM-based%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doing%20More%20with%20Less%3A%20A%20Survey%20on%20Routing%20Strategies%20for%20Resource%0A%20%20Optimisation%20in%20Large%20Language%20Model-Based%20Systems&entry.906535625=Clovis%20Varangot-Reille%20and%20Christophe%20Bouvard%20and%20Antoine%20Gourru%20and%20Mathieu%20Ciancone%20and%20Marion%20Schaeffer%20and%20Fran%C3%A7ois%20Jacquenet&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20systems%2C%20i.e.%20interconnected%20elements%20that%0Ainclude%20an%20LLM%20as%20a%20central%20component%2C%20such%20as%20conversational%20agents%2C%20are%0Ausually%20designed%20with%20monolithic%2C%20static%20architectures%20that%20rely%20on%20a%20single%2C%0Ageneral-purpose%20LLM%20to%20handle%20all%20user%20queries.%20However%2C%20these%20systems%20may%20be%0Ainefficient%20as%20different%20queries%20may%20require%20different%20levels%20of%20reasoning%2C%0Adomain%20knowledge%20or%20pre-processing.%20While%20generalist%20LLMs%20%28e.g.%20GPT-4o%2C%0AClaude-Sonnet%29%20perform%20well%20across%20a%20wide%20range%20of%20tasks%2C%20they%20may%20incur%0Asignificant%20financial%2C%20energy%20and%20computational%20costs.%20These%20costs%20may%20be%0Adisproportionate%20for%20simpler%20queries%2C%20resulting%20in%20unnecessary%20resource%0Autilisation.%20A%20routing%20mechanism%20can%20therefore%20be%20employed%20to%20route%20queries%20to%0Amore%20appropriate%20components%2C%20such%20as%20smaller%20or%20specialised%20models%2C%20thereby%0Aimproving%20efficiency%20and%20optimising%20resource%20consumption.%20This%20survey%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20routing%20strategies%20in%20LLM-based%20systems.%0ASpecifically%2C%20it%20reviews%20when%2C%20why%2C%20and%20how%20routing%20should%20be%20integrated%20into%0ALLM%20pipelines%20to%20improve%20efficiency%2C%20scalability%2C%20and%20performance.%20We%20define%0Athe%20objectives%20to%20optimise%2C%20such%20as%20cost%20minimisation%20and%20performance%0Amaximisation%2C%20and%20discuss%20the%20timing%20of%20routing%20within%20the%20LLM%20workflow%2C%0Awhether%20it%20occurs%20before%20or%20after%20generation.%20We%20also%20detail%20the%20various%0Aimplementation%20strategies%2C%20including%20similarity-based%2C%20supervised%2C%0Areinforcement%20learning-based%2C%20and%20generative%20methods.%20Practical%20considerations%0Asuch%20as%20industrial%20applications%20and%20current%20limitations%20are%20also%20examined%2C%20like%0Astandardising%20routing%20experiments%2C%20accounting%20for%20non-financial%20costs%2C%20and%0Adesigning%20adaptive%20strategies.%20By%20formalising%20routing%20as%20a%20performance-cost%0Aoptimisation%20problem%2C%20this%20survey%20provides%20tools%20and%20directions%20to%20guide%20future%0Aresearch%20and%20development%20of%20adaptive%20low-cost%20LLM-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00409v3&entry.124074799=Read"},
{"title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "author": "Kasper Engelen and Guillermo A. P\u00e9rez and Marnix Suilen", "abstract": "  Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.\n", "link": "http://arxiv.org/abs/2507.15532v1", "date": "2025-07-21", "relevancy": 1.3508, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4543}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Safe%20Policy%20Improvement%20Using%20Parametric%20Structure&body=Title%3A%20Data-Efficient%20Safe%20Policy%20Improvement%20Using%20Parametric%20Structure%0AAuthor%3A%20Kasper%20Engelen%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Marnix%20Suilen%0AAbstract%3A%20%20%20Safe%20policy%20improvement%20%28SPI%29%20is%20an%20offline%20reinforcement%20learning%20problem%20in%0Awhich%20a%20new%20policy%20that%20reliably%20outperforms%20the%20behavior%20policy%20with%20high%0Aconfidence%20needs%20to%20be%20computed%20using%20only%20a%20dataset%20and%20the%20behavior%20policy.%0AMarkov%20decision%20processes%20%28MDPs%29%20are%20the%20standard%20formalism%20for%20modeling%0Aenvironments%20in%20SPI.%20In%20many%20applications%2C%20additional%20information%20in%20the%20form%0Aof%20parametric%20dependencies%20between%20distributions%20in%20the%20transition%20dynamics%20is%0Aavailable.%20We%20make%20SPI%20more%20data-efficient%20by%20leveraging%20these%20dependencies%0Athrough%20three%20contributions%3A%20%281%29%20a%20parametric%20SPI%20algorithm%20that%20exploits%20known%0Acorrelations%20between%20distributions%20to%20more%20accurately%20estimate%20the%20transition%0Adynamics%20using%20the%20same%20amount%20of%20data%3B%20%282%29%20a%20preprocessing%20technique%20that%0Aprunes%20redundant%20actions%20from%20the%20environment%20through%20a%20game-based%20abstraction%3B%0Aand%20%283%29%20a%20more%20advanced%20preprocessing%20technique%2C%20based%20on%20satisfiability%20modulo%0Atheory%20%28SMT%29%20solving%2C%20that%20can%20identify%20more%20actions%20to%20prune.%20Empirical%0Aresults%20and%20an%20ablation%20study%20show%20that%20our%20techniques%20increase%20the%20data%0Aefficiency%20of%20SPI%20by%20multiple%20orders%20of%20magnitude%20while%20maintaining%20the%20same%0Areliability%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Safe%2520Policy%2520Improvement%2520Using%2520Parametric%2520Structure%26entry.906535625%3DKasper%2520Engelen%2520and%2520Guillermo%2520A.%2520P%25C3%25A9rez%2520and%2520Marnix%2520Suilen%26entry.1292438233%3D%2520%2520Safe%2520policy%2520improvement%2520%2528SPI%2529%2520is%2520an%2520offline%2520reinforcement%2520learning%2520problem%2520in%250Awhich%2520a%2520new%2520policy%2520that%2520reliably%2520outperforms%2520the%2520behavior%2520policy%2520with%2520high%250Aconfidence%2520needs%2520to%2520be%2520computed%2520using%2520only%2520a%2520dataset%2520and%2520the%2520behavior%2520policy.%250AMarkov%2520decision%2520processes%2520%2528MDPs%2529%2520are%2520the%2520standard%2520formalism%2520for%2520modeling%250Aenvironments%2520in%2520SPI.%2520In%2520many%2520applications%252C%2520additional%2520information%2520in%2520the%2520form%250Aof%2520parametric%2520dependencies%2520between%2520distributions%2520in%2520the%2520transition%2520dynamics%2520is%250Aavailable.%2520We%2520make%2520SPI%2520more%2520data-efficient%2520by%2520leveraging%2520these%2520dependencies%250Athrough%2520three%2520contributions%253A%2520%25281%2529%2520a%2520parametric%2520SPI%2520algorithm%2520that%2520exploits%2520known%250Acorrelations%2520between%2520distributions%2520to%2520more%2520accurately%2520estimate%2520the%2520transition%250Adynamics%2520using%2520the%2520same%2520amount%2520of%2520data%253B%2520%25282%2529%2520a%2520preprocessing%2520technique%2520that%250Aprunes%2520redundant%2520actions%2520from%2520the%2520environment%2520through%2520a%2520game-based%2520abstraction%253B%250Aand%2520%25283%2529%2520a%2520more%2520advanced%2520preprocessing%2520technique%252C%2520based%2520on%2520satisfiability%2520modulo%250Atheory%2520%2528SMT%2529%2520solving%252C%2520that%2520can%2520identify%2520more%2520actions%2520to%2520prune.%2520Empirical%250Aresults%2520and%2520an%2520ablation%2520study%2520show%2520that%2520our%2520techniques%2520increase%2520the%2520data%250Aefficiency%2520of%2520SPI%2520by%2520multiple%2520orders%2520of%2520magnitude%2520while%2520maintaining%2520the%2520same%250Areliability%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Safe%20Policy%20Improvement%20Using%20Parametric%20Structure&entry.906535625=Kasper%20Engelen%20and%20Guillermo%20A.%20P%C3%A9rez%20and%20Marnix%20Suilen&entry.1292438233=%20%20Safe%20policy%20improvement%20%28SPI%29%20is%20an%20offline%20reinforcement%20learning%20problem%20in%0Awhich%20a%20new%20policy%20that%20reliably%20outperforms%20the%20behavior%20policy%20with%20high%0Aconfidence%20needs%20to%20be%20computed%20using%20only%20a%20dataset%20and%20the%20behavior%20policy.%0AMarkov%20decision%20processes%20%28MDPs%29%20are%20the%20standard%20formalism%20for%20modeling%0Aenvironments%20in%20SPI.%20In%20many%20applications%2C%20additional%20information%20in%20the%20form%0Aof%20parametric%20dependencies%20between%20distributions%20in%20the%20transition%20dynamics%20is%0Aavailable.%20We%20make%20SPI%20more%20data-efficient%20by%20leveraging%20these%20dependencies%0Athrough%20three%20contributions%3A%20%281%29%20a%20parametric%20SPI%20algorithm%20that%20exploits%20known%0Acorrelations%20between%20distributions%20to%20more%20accurately%20estimate%20the%20transition%0Adynamics%20using%20the%20same%20amount%20of%20data%3B%20%282%29%20a%20preprocessing%20technique%20that%0Aprunes%20redundant%20actions%20from%20the%20environment%20through%20a%20game-based%20abstraction%3B%0Aand%20%283%29%20a%20more%20advanced%20preprocessing%20technique%2C%20based%20on%20satisfiability%20modulo%0Atheory%20%28SMT%29%20solving%2C%20that%20can%20identify%20more%20actions%20to%20prune.%20Empirical%0Aresults%20and%20an%20ablation%20study%20show%20that%20our%20techniques%20increase%20the%20data%0Aefficiency%20of%20SPI%20by%20multiple%20orders%20of%20magnitude%20while%20maintaining%20the%20same%0Areliability%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15532v1&entry.124074799=Read"},
{"title": "Stimulating Imagination: Towards General-purpose \"Something Something\n  Placement\"", "author": "Jianyang Wu and Jie Gu and Xiaokang Ma and Fangzhou Qiu and Chu Tang and Jingmin Chen", "abstract": "  General-purpose object placement is a fundamental capability of an\nintelligent generalist robot: being capable of rearranging objects following\nprecise human instructions even in novel environments. This work is dedicated\nto achieving general-purpose object placement with ``something something''\ninstructions. Specifically, we break the entire process down into three parts,\nincluding object localization, goal imagination and robot control, and propose\na method named SPORT. SPORT leverages a pre-trained large vision model for\nbroad semantic reasoning about objects, and learns a diffusion-based pose\nestimator to ensure physically-realistic results in 3D space. Only object types\n(movable or reference) are communicated between these two parts, which brings\ntwo benefits. One is that we can fully leverage the powerful ability of\nopen-set object recognition and localization since no specific fine-tuning is\nneeded for the robotic scenario. Moreover, the diffusion-based estimator only\nneed to ``imagine\" the object poses after the placement, while no necessity for\ntheir semantic information. Thus the training burden is greatly reduced and no\nmassive training is required. The training data for the goal pose estimation is\ncollected in simulation and annotated by using GPT-4. Experimental results\ndemonstrate the effectiveness of our approach. SPORT can not only generate\npromising 3D goal poses for unseen simulated objects, but also be seamlessly\napplied to real-world settings.\n", "link": "http://arxiv.org/abs/2408.01655v2", "date": "2025-07-21", "relevancy": 1.2168, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6278}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6093}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stimulating%20Imagination%3A%20Towards%20General-purpose%20%22Something%20Something%0A%20%20Placement%22&body=Title%3A%20Stimulating%20Imagination%3A%20Towards%20General-purpose%20%22Something%20Something%0A%20%20Placement%22%0AAuthor%3A%20Jianyang%20Wu%20and%20Jie%20Gu%20and%20Xiaokang%20Ma%20and%20Fangzhou%20Qiu%20and%20Chu%20Tang%20and%20Jingmin%20Chen%0AAbstract%3A%20%20%20General-purpose%20object%20placement%20is%20a%20fundamental%20capability%20of%20an%0Aintelligent%20generalist%20robot%3A%20being%20capable%20of%20rearranging%20objects%20following%0Aprecise%20human%20instructions%20even%20in%20novel%20environments.%20This%20work%20is%20dedicated%0Ato%20achieving%20general-purpose%20object%20placement%20with%20%60%60something%20something%27%27%0Ainstructions.%20Specifically%2C%20we%20break%20the%20entire%20process%20down%20into%20three%20parts%2C%0Aincluding%20object%20localization%2C%20goal%20imagination%20and%20robot%20control%2C%20and%20propose%0Aa%20method%20named%20SPORT.%20SPORT%20leverages%20a%20pre-trained%20large%20vision%20model%20for%0Abroad%20semantic%20reasoning%20about%20objects%2C%20and%20learns%20a%20diffusion-based%20pose%0Aestimator%20to%20ensure%20physically-realistic%20results%20in%203D%20space.%20Only%20object%20types%0A%28movable%20or%20reference%29%20are%20communicated%20between%20these%20two%20parts%2C%20which%20brings%0Atwo%20benefits.%20One%20is%20that%20we%20can%20fully%20leverage%20the%20powerful%20ability%20of%0Aopen-set%20object%20recognition%20and%20localization%20since%20no%20specific%20fine-tuning%20is%0Aneeded%20for%20the%20robotic%20scenario.%20Moreover%2C%20the%20diffusion-based%20estimator%20only%0Aneed%20to%20%60%60imagine%22%20the%20object%20poses%20after%20the%20placement%2C%20while%20no%20necessity%20for%0Atheir%20semantic%20information.%20Thus%20the%20training%20burden%20is%20greatly%20reduced%20and%20no%0Amassive%20training%20is%20required.%20The%20training%20data%20for%20the%20goal%20pose%20estimation%20is%0Acollected%20in%20simulation%20and%20annotated%20by%20using%20GPT-4.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%20SPORT%20can%20not%20only%20generate%0Apromising%203D%20goal%20poses%20for%20unseen%20simulated%20objects%2C%20but%20also%20be%20seamlessly%0Aapplied%20to%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStimulating%2520Imagination%253A%2520Towards%2520General-purpose%2520%2522Something%2520Something%250A%2520%2520Placement%2522%26entry.906535625%3DJianyang%2520Wu%2520and%2520Jie%2520Gu%2520and%2520Xiaokang%2520Ma%2520and%2520Fangzhou%2520Qiu%2520and%2520Chu%2520Tang%2520and%2520Jingmin%2520Chen%26entry.1292438233%3D%2520%2520General-purpose%2520object%2520placement%2520is%2520a%2520fundamental%2520capability%2520of%2520an%250Aintelligent%2520generalist%2520robot%253A%2520being%2520capable%2520of%2520rearranging%2520objects%2520following%250Aprecise%2520human%2520instructions%2520even%2520in%2520novel%2520environments.%2520This%2520work%2520is%2520dedicated%250Ato%2520achieving%2520general-purpose%2520object%2520placement%2520with%2520%2560%2560something%2520something%2527%2527%250Ainstructions.%2520Specifically%252C%2520we%2520break%2520the%2520entire%2520process%2520down%2520into%2520three%2520parts%252C%250Aincluding%2520object%2520localization%252C%2520goal%2520imagination%2520and%2520robot%2520control%252C%2520and%2520propose%250Aa%2520method%2520named%2520SPORT.%2520SPORT%2520leverages%2520a%2520pre-trained%2520large%2520vision%2520model%2520for%250Abroad%2520semantic%2520reasoning%2520about%2520objects%252C%2520and%2520learns%2520a%2520diffusion-based%2520pose%250Aestimator%2520to%2520ensure%2520physically-realistic%2520results%2520in%25203D%2520space.%2520Only%2520object%2520types%250A%2528movable%2520or%2520reference%2529%2520are%2520communicated%2520between%2520these%2520two%2520parts%252C%2520which%2520brings%250Atwo%2520benefits.%2520One%2520is%2520that%2520we%2520can%2520fully%2520leverage%2520the%2520powerful%2520ability%2520of%250Aopen-set%2520object%2520recognition%2520and%2520localization%2520since%2520no%2520specific%2520fine-tuning%2520is%250Aneeded%2520for%2520the%2520robotic%2520scenario.%2520Moreover%252C%2520the%2520diffusion-based%2520estimator%2520only%250Aneed%2520to%2520%2560%2560imagine%2522%2520the%2520object%2520poses%2520after%2520the%2520placement%252C%2520while%2520no%2520necessity%2520for%250Atheir%2520semantic%2520information.%2520Thus%2520the%2520training%2520burden%2520is%2520greatly%2520reduced%2520and%2520no%250Amassive%2520training%2520is%2520required.%2520The%2520training%2520data%2520for%2520the%2520goal%2520pose%2520estimation%2520is%250Acollected%2520in%2520simulation%2520and%2520annotated%2520by%2520using%2520GPT-4.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520SPORT%2520can%2520not%2520only%2520generate%250Apromising%25203D%2520goal%2520poses%2520for%2520unseen%2520simulated%2520objects%252C%2520but%2520also%2520be%2520seamlessly%250Aapplied%2520to%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stimulating%20Imagination%3A%20Towards%20General-purpose%20%22Something%20Something%0A%20%20Placement%22&entry.906535625=Jianyang%20Wu%20and%20Jie%20Gu%20and%20Xiaokang%20Ma%20and%20Fangzhou%20Qiu%20and%20Chu%20Tang%20and%20Jingmin%20Chen&entry.1292438233=%20%20General-purpose%20object%20placement%20is%20a%20fundamental%20capability%20of%20an%0Aintelligent%20generalist%20robot%3A%20being%20capable%20of%20rearranging%20objects%20following%0Aprecise%20human%20instructions%20even%20in%20novel%20environments.%20This%20work%20is%20dedicated%0Ato%20achieving%20general-purpose%20object%20placement%20with%20%60%60something%20something%27%27%0Ainstructions.%20Specifically%2C%20we%20break%20the%20entire%20process%20down%20into%20three%20parts%2C%0Aincluding%20object%20localization%2C%20goal%20imagination%20and%20robot%20control%2C%20and%20propose%0Aa%20method%20named%20SPORT.%20SPORT%20leverages%20a%20pre-trained%20large%20vision%20model%20for%0Abroad%20semantic%20reasoning%20about%20objects%2C%20and%20learns%20a%20diffusion-based%20pose%0Aestimator%20to%20ensure%20physically-realistic%20results%20in%203D%20space.%20Only%20object%20types%0A%28movable%20or%20reference%29%20are%20communicated%20between%20these%20two%20parts%2C%20which%20brings%0Atwo%20benefits.%20One%20is%20that%20we%20can%20fully%20leverage%20the%20powerful%20ability%20of%0Aopen-set%20object%20recognition%20and%20localization%20since%20no%20specific%20fine-tuning%20is%0Aneeded%20for%20the%20robotic%20scenario.%20Moreover%2C%20the%20diffusion-based%20estimator%20only%0Aneed%20to%20%60%60imagine%22%20the%20object%20poses%20after%20the%20placement%2C%20while%20no%20necessity%20for%0Atheir%20semantic%20information.%20Thus%20the%20training%20burden%20is%20greatly%20reduced%20and%20no%0Amassive%20training%20is%20required.%20The%20training%20data%20for%20the%20goal%20pose%20estimation%20is%0Acollected%20in%20simulation%20and%20annotated%20by%20using%20GPT-4.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%20SPORT%20can%20not%20only%20generate%0Apromising%203D%20goal%20poses%20for%20unseen%20simulated%20objects%2C%20but%20also%20be%20seamlessly%0Aapplied%20to%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01655v2&entry.124074799=Read"},
{"title": "Enhancing Natural Language Inference Performance with Knowledge Graph\n  for COVID-19 Automated Fact-Checking in Indonesian Language", "author": "Arief Purnama Muharram and Ayu Purwarianti", "abstract": "  Automated fact-checking is a key strategy to overcome the spread of COVID-19\nmisinformation on the internet. These systems typically leverage deep learning\napproaches through Natural Language Inference (NLI) to verify the truthfulness\nof information based on supporting evidence. However, one challenge that arises\nin deep learning is performance stagnation due to a lack of knowledge during\ntraining. This study proposes using a Knowledge Graph (KG) as external\nknowledge to enhance NLI performance for automated COVID-19 fact-checking in\nthe Indonesian language. The proposed model architecture comprises three\nmodules: a fact module, an NLI module, and a classifier module. The fact module\nprocesses information from the KG, while the NLI module handles semantic\nrelationships between the given premise and hypothesis. The representation\nvectors from both modules are concatenated and fed into the classifier module\nto produce the final result. The model was trained using the generated\nIndonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.\nOur study demonstrates that incorporating KGs can significantly improve NLI\nperformance in fact-checking, achieving the best accuracy of 0.8616. This\nsuggests that KGs are a valuable component for enhancing NLI performance in\nautomated fact-checking.\n", "link": "http://arxiv.org/abs/2409.00061v2", "date": "2025-07-21", "relevancy": 1.243, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4205}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Natural%20Language%20Inference%20Performance%20with%20Knowledge%20Graph%0A%20%20for%20COVID-19%20Automated%20Fact-Checking%20in%20Indonesian%20Language&body=Title%3A%20Enhancing%20Natural%20Language%20Inference%20Performance%20with%20Knowledge%20Graph%0A%20%20for%20COVID-19%20Automated%20Fact-Checking%20in%20Indonesian%20Language%0AAuthor%3A%20Arief%20Purnama%20Muharram%20and%20Ayu%20Purwarianti%0AAbstract%3A%20%20%20Automated%20fact-checking%20is%20a%20key%20strategy%20to%20overcome%20the%20spread%20of%20COVID-19%0Amisinformation%20on%20the%20internet.%20These%20systems%20typically%20leverage%20deep%20learning%0Aapproaches%20through%20Natural%20Language%20Inference%20%28NLI%29%20to%20verify%20the%20truthfulness%0Aof%20information%20based%20on%20supporting%20evidence.%20However%2C%20one%20challenge%20that%20arises%0Ain%20deep%20learning%20is%20performance%20stagnation%20due%20to%20a%20lack%20of%20knowledge%20during%0Atraining.%20This%20study%20proposes%20using%20a%20Knowledge%20Graph%20%28KG%29%20as%20external%0Aknowledge%20to%20enhance%20NLI%20performance%20for%20automated%20COVID-19%20fact-checking%20in%0Athe%20Indonesian%20language.%20The%20proposed%20model%20architecture%20comprises%20three%0Amodules%3A%20a%20fact%20module%2C%20an%20NLI%20module%2C%20and%20a%20classifier%20module.%20The%20fact%20module%0Aprocesses%20information%20from%20the%20KG%2C%20while%20the%20NLI%20module%20handles%20semantic%0Arelationships%20between%20the%20given%20premise%20and%20hypothesis.%20The%20representation%0Avectors%20from%20both%20modules%20are%20concatenated%20and%20fed%20into%20the%20classifier%20module%0Ato%20produce%20the%20final%20result.%20The%20model%20was%20trained%20using%20the%20generated%0AIndonesian%20COVID-19%20fact-checking%20dataset%20and%20the%20COVID-19%20KG%20Bahasa%20Indonesia.%0AOur%20study%20demonstrates%20that%20incorporating%20KGs%20can%20significantly%20improve%20NLI%0Aperformance%20in%20fact-checking%2C%20achieving%20the%20best%20accuracy%20of%200.8616.%20This%0Asuggests%20that%20KGs%20are%20a%20valuable%20component%20for%20enhancing%20NLI%20performance%20in%0Aautomated%20fact-checking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Natural%2520Language%2520Inference%2520Performance%2520with%2520Knowledge%2520Graph%250A%2520%2520for%2520COVID-19%2520Automated%2520Fact-Checking%2520in%2520Indonesian%2520Language%26entry.906535625%3DArief%2520Purnama%2520Muharram%2520and%2520Ayu%2520Purwarianti%26entry.1292438233%3D%2520%2520Automated%2520fact-checking%2520is%2520a%2520key%2520strategy%2520to%2520overcome%2520the%2520spread%2520of%2520COVID-19%250Amisinformation%2520on%2520the%2520internet.%2520These%2520systems%2520typically%2520leverage%2520deep%2520learning%250Aapproaches%2520through%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520to%2520verify%2520the%2520truthfulness%250Aof%2520information%2520based%2520on%2520supporting%2520evidence.%2520However%252C%2520one%2520challenge%2520that%2520arises%250Ain%2520deep%2520learning%2520is%2520performance%2520stagnation%2520due%2520to%2520a%2520lack%2520of%2520knowledge%2520during%250Atraining.%2520This%2520study%2520proposes%2520using%2520a%2520Knowledge%2520Graph%2520%2528KG%2529%2520as%2520external%250Aknowledge%2520to%2520enhance%2520NLI%2520performance%2520for%2520automated%2520COVID-19%2520fact-checking%2520in%250Athe%2520Indonesian%2520language.%2520The%2520proposed%2520model%2520architecture%2520comprises%2520three%250Amodules%253A%2520a%2520fact%2520module%252C%2520an%2520NLI%2520module%252C%2520and%2520a%2520classifier%2520module.%2520The%2520fact%2520module%250Aprocesses%2520information%2520from%2520the%2520KG%252C%2520while%2520the%2520NLI%2520module%2520handles%2520semantic%250Arelationships%2520between%2520the%2520given%2520premise%2520and%2520hypothesis.%2520The%2520representation%250Avectors%2520from%2520both%2520modules%2520are%2520concatenated%2520and%2520fed%2520into%2520the%2520classifier%2520module%250Ato%2520produce%2520the%2520final%2520result.%2520The%2520model%2520was%2520trained%2520using%2520the%2520generated%250AIndonesian%2520COVID-19%2520fact-checking%2520dataset%2520and%2520the%2520COVID-19%2520KG%2520Bahasa%2520Indonesia.%250AOur%2520study%2520demonstrates%2520that%2520incorporating%2520KGs%2520can%2520significantly%2520improve%2520NLI%250Aperformance%2520in%2520fact-checking%252C%2520achieving%2520the%2520best%2520accuracy%2520of%25200.8616.%2520This%250Asuggests%2520that%2520KGs%2520are%2520a%2520valuable%2520component%2520for%2520enhancing%2520NLI%2520performance%2520in%250Aautomated%2520fact-checking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Natural%20Language%20Inference%20Performance%20with%20Knowledge%20Graph%0A%20%20for%20COVID-19%20Automated%20Fact-Checking%20in%20Indonesian%20Language&entry.906535625=Arief%20Purnama%20Muharram%20and%20Ayu%20Purwarianti&entry.1292438233=%20%20Automated%20fact-checking%20is%20a%20key%20strategy%20to%20overcome%20the%20spread%20of%20COVID-19%0Amisinformation%20on%20the%20internet.%20These%20systems%20typically%20leverage%20deep%20learning%0Aapproaches%20through%20Natural%20Language%20Inference%20%28NLI%29%20to%20verify%20the%20truthfulness%0Aof%20information%20based%20on%20supporting%20evidence.%20However%2C%20one%20challenge%20that%20arises%0Ain%20deep%20learning%20is%20performance%20stagnation%20due%20to%20a%20lack%20of%20knowledge%20during%0Atraining.%20This%20study%20proposes%20using%20a%20Knowledge%20Graph%20%28KG%29%20as%20external%0Aknowledge%20to%20enhance%20NLI%20performance%20for%20automated%20COVID-19%20fact-checking%20in%0Athe%20Indonesian%20language.%20The%20proposed%20model%20architecture%20comprises%20three%0Amodules%3A%20a%20fact%20module%2C%20an%20NLI%20module%2C%20and%20a%20classifier%20module.%20The%20fact%20module%0Aprocesses%20information%20from%20the%20KG%2C%20while%20the%20NLI%20module%20handles%20semantic%0Arelationships%20between%20the%20given%20premise%20and%20hypothesis.%20The%20representation%0Avectors%20from%20both%20modules%20are%20concatenated%20and%20fed%20into%20the%20classifier%20module%0Ato%20produce%20the%20final%20result.%20The%20model%20was%20trained%20using%20the%20generated%0AIndonesian%20COVID-19%20fact-checking%20dataset%20and%20the%20COVID-19%20KG%20Bahasa%20Indonesia.%0AOur%20study%20demonstrates%20that%20incorporating%20KGs%20can%20significantly%20improve%20NLI%0Aperformance%20in%20fact-checking%2C%20achieving%20the%20best%20accuracy%20of%200.8616.%20This%0Asuggests%20that%20KGs%20are%20a%20valuable%20component%20for%20enhancing%20NLI%20performance%20in%0Aautomated%20fact-checking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00061v2&entry.124074799=Read"},
{"title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos", "author": "Hao Luo and Yicheng Feng and Wanpeng Zhang and Sipeng Zheng and Ye Wang and Haoqi Yuan and Jiazheng Liu and Chaoyi Xu and Qin Jin and Zongqing Lu", "abstract": "  We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.\n", "link": "http://arxiv.org/abs/2507.15597v1", "date": "2025-07-21", "relevancy": 1.7774, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6027}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5849}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Being-H0%3A%20Vision-Language-Action%20Pretraining%20from%20Large-Scale%20Human%0A%20%20Videos&body=Title%3A%20Being-H0%3A%20Vision-Language-Action%20Pretraining%20from%20Large-Scale%20Human%0A%20%20Videos%0AAuthor%3A%20Hao%20Luo%20and%20Yicheng%20Feng%20and%20Wanpeng%20Zhang%20and%20Sipeng%20Zheng%20and%20Ye%20Wang%20and%20Haoqi%20Yuan%20and%20Jiazheng%20Liu%20and%20Chaoyi%20Xu%20and%20Qin%20Jin%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20We%20introduce%20Being-H0%2C%20a%20dexterous%20Vision-Language-Action%20model%20%28VLA%29%20trained%0Aon%20large-scale%20human%20videos.%20Existing%20VLAs%20struggle%20with%20complex%20manipulation%0Atasks%20requiring%20high%20dexterity%20and%20generalize%20poorly%20to%20novel%20scenarios%20and%0Atasks%2C%20primarily%20due%20to%20their%20reliance%20on%20synthetic%20data%20with%20significant%0Asim-to-real%20gaps%20or%20teleoperated%20demonstrations%20lacking%20scale%20and%20diversity.%20To%0Aaddress%20this%20data%20bottleneck%2C%20we%20propose%20leveraging%20human%20hands%20as%20a%20foundation%0Amanipulator%2C%20capitalizing%20on%20the%20rich%20dexterity%20and%20scalability%20present%20in%20web%0Adata.%20Our%20approach%20centers%20on%20physical%20instruction%20tuning%2C%20a%20novel%20training%0Aparadigm%20that%20combines%20large-scale%20VLA%20pretraining%20from%20human%20videos%2C%20physical%0Aspace%20alignment%20for%203D%20reasoning%2C%20and%20post-training%20adaptation%20for%20robotic%0Atasks.%20Additionally%2C%20we%20introduce%20a%20part-level%20motion%20tokenization%20method%20which%0Aachieves%20millimeter-level%20reconstruction%20accuracy%20to%20model%20precise%20hand%0Atrajectories%20for%20action%20learning.%20To%20support%20our%20proposed%20paradigm%2C%20we%20further%0Adevelop%20a%20comprehensive%20data%20curation%20pipeline%20that%20integrates%20heterogeneous%0Asources%20--%20including%20motion%20capture%2C%20VR%2C%20and%20RGB-only%20videos%20--%20into%20a%0Alarge-scale%20dataset%20with%20millions%20of%20motion-based%20instructional%20instances.%20We%0Aempirically%20show%20the%20excellence%20of%20Being-H0%20in%20hand%20motion%20generation%20and%0Ainstruction%20following%2C%20and%20it%20also%20scales%20well%20with%20model%20and%20data%20sizes.%0AImportantly%2C%20we%20observe%20the%20expected%20gains%20of%20Being-H0%20in%20real-world%20robotic%0Amanipulation%20as%20physical%20instruction%20tuning%20is%20applied.%20More%20details%20are%0Aavailable%20at%20https%3A//beingbeyond.github.io/Being-H0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeing-H0%253A%2520Vision-Language-Action%2520Pretraining%2520from%2520Large-Scale%2520Human%250A%2520%2520Videos%26entry.906535625%3DHao%2520Luo%2520and%2520Yicheng%2520Feng%2520and%2520Wanpeng%2520Zhang%2520and%2520Sipeng%2520Zheng%2520and%2520Ye%2520Wang%2520and%2520Haoqi%2520Yuan%2520and%2520Jiazheng%2520Liu%2520and%2520Chaoyi%2520Xu%2520and%2520Qin%2520Jin%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Being-H0%252C%2520a%2520dexterous%2520Vision-Language-Action%2520model%2520%2528VLA%2529%2520trained%250Aon%2520large-scale%2520human%2520videos.%2520Existing%2520VLAs%2520struggle%2520with%2520complex%2520manipulation%250Atasks%2520requiring%2520high%2520dexterity%2520and%2520generalize%2520poorly%2520to%2520novel%2520scenarios%2520and%250Atasks%252C%2520primarily%2520due%2520to%2520their%2520reliance%2520on%2520synthetic%2520data%2520with%2520significant%250Asim-to-real%2520gaps%2520or%2520teleoperated%2520demonstrations%2520lacking%2520scale%2520and%2520diversity.%2520To%250Aaddress%2520this%2520data%2520bottleneck%252C%2520we%2520propose%2520leveraging%2520human%2520hands%2520as%2520a%2520foundation%250Amanipulator%252C%2520capitalizing%2520on%2520the%2520rich%2520dexterity%2520and%2520scalability%2520present%2520in%2520web%250Adata.%2520Our%2520approach%2520centers%2520on%2520physical%2520instruction%2520tuning%252C%2520a%2520novel%2520training%250Aparadigm%2520that%2520combines%2520large-scale%2520VLA%2520pretraining%2520from%2520human%2520videos%252C%2520physical%250Aspace%2520alignment%2520for%25203D%2520reasoning%252C%2520and%2520post-training%2520adaptation%2520for%2520robotic%250Atasks.%2520Additionally%252C%2520we%2520introduce%2520a%2520part-level%2520motion%2520tokenization%2520method%2520which%250Aachieves%2520millimeter-level%2520reconstruction%2520accuracy%2520to%2520model%2520precise%2520hand%250Atrajectories%2520for%2520action%2520learning.%2520To%2520support%2520our%2520proposed%2520paradigm%252C%2520we%2520further%250Adevelop%2520a%2520comprehensive%2520data%2520curation%2520pipeline%2520that%2520integrates%2520heterogeneous%250Asources%2520--%2520including%2520motion%2520capture%252C%2520VR%252C%2520and%2520RGB-only%2520videos%2520--%2520into%2520a%250Alarge-scale%2520dataset%2520with%2520millions%2520of%2520motion-based%2520instructional%2520instances.%2520We%250Aempirically%2520show%2520the%2520excellence%2520of%2520Being-H0%2520in%2520hand%2520motion%2520generation%2520and%250Ainstruction%2520following%252C%2520and%2520it%2520also%2520scales%2520well%2520with%2520model%2520and%2520data%2520sizes.%250AImportantly%252C%2520we%2520observe%2520the%2520expected%2520gains%2520of%2520Being-H0%2520in%2520real-world%2520robotic%250Amanipulation%2520as%2520physical%2520instruction%2520tuning%2520is%2520applied.%2520More%2520details%2520are%250Aavailable%2520at%2520https%253A//beingbeyond.github.io/Being-H0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Being-H0%3A%20Vision-Language-Action%20Pretraining%20from%20Large-Scale%20Human%0A%20%20Videos&entry.906535625=Hao%20Luo%20and%20Yicheng%20Feng%20and%20Wanpeng%20Zhang%20and%20Sipeng%20Zheng%20and%20Ye%20Wang%20and%20Haoqi%20Yuan%20and%20Jiazheng%20Liu%20and%20Chaoyi%20Xu%20and%20Qin%20Jin%20and%20Zongqing%20Lu&entry.1292438233=%20%20We%20introduce%20Being-H0%2C%20a%20dexterous%20Vision-Language-Action%20model%20%28VLA%29%20trained%0Aon%20large-scale%20human%20videos.%20Existing%20VLAs%20struggle%20with%20complex%20manipulation%0Atasks%20requiring%20high%20dexterity%20and%20generalize%20poorly%20to%20novel%20scenarios%20and%0Atasks%2C%20primarily%20due%20to%20their%20reliance%20on%20synthetic%20data%20with%20significant%0Asim-to-real%20gaps%20or%20teleoperated%20demonstrations%20lacking%20scale%20and%20diversity.%20To%0Aaddress%20this%20data%20bottleneck%2C%20we%20propose%20leveraging%20human%20hands%20as%20a%20foundation%0Amanipulator%2C%20capitalizing%20on%20the%20rich%20dexterity%20and%20scalability%20present%20in%20web%0Adata.%20Our%20approach%20centers%20on%20physical%20instruction%20tuning%2C%20a%20novel%20training%0Aparadigm%20that%20combines%20large-scale%20VLA%20pretraining%20from%20human%20videos%2C%20physical%0Aspace%20alignment%20for%203D%20reasoning%2C%20and%20post-training%20adaptation%20for%20robotic%0Atasks.%20Additionally%2C%20we%20introduce%20a%20part-level%20motion%20tokenization%20method%20which%0Aachieves%20millimeter-level%20reconstruction%20accuracy%20to%20model%20precise%20hand%0Atrajectories%20for%20action%20learning.%20To%20support%20our%20proposed%20paradigm%2C%20we%20further%0Adevelop%20a%20comprehensive%20data%20curation%20pipeline%20that%20integrates%20heterogeneous%0Asources%20--%20including%20motion%20capture%2C%20VR%2C%20and%20RGB-only%20videos%20--%20into%20a%0Alarge-scale%20dataset%20with%20millions%20of%20motion-based%20instructional%20instances.%20We%0Aempirically%20show%20the%20excellence%20of%20Being-H0%20in%20hand%20motion%20generation%20and%0Ainstruction%20following%2C%20and%20it%20also%20scales%20well%20with%20model%20and%20data%20sizes.%0AImportantly%2C%20we%20observe%20the%20expected%20gains%20of%20Being-H0%20in%20real-world%20robotic%0Amanipulation%20as%20physical%20instruction%20tuning%20is%20applied.%20More%20details%20are%0Aavailable%20at%20https%3A//beingbeyond.github.io/Being-H0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15597v1&entry.124074799=Read"},
{"title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "author": "Congmin Zheng and Jiachen Zhu and Jianghao Lin and Xinyi Dai and Yong Yu and Weinan Zhang and Mengyue Yang", "abstract": "  Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.\n", "link": "http://arxiv.org/abs/2507.15698v1", "date": "2025-07-21", "relevancy": 1.8486, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoLD%3A%20Counterfactually-Guided%20Length%20Debiasing%20for%20Process%20Reward%20Models&body=Title%3A%20CoLD%3A%20Counterfactually-Guided%20Length%20Debiasing%20for%20Process%20Reward%20Models%0AAuthor%3A%20Congmin%20Zheng%20and%20Jiachen%20Zhu%20and%20Jianghao%20Lin%20and%20Xinyi%20Dai%20and%20Yong%20Yu%20and%20Weinan%20Zhang%20and%20Mengyue%20Yang%0AAbstract%3A%20%20%20Process%20Reward%20Models%20%28PRMs%29%20play%20a%20central%20role%20in%20evaluating%20and%20guiding%0Amulti-step%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%20especially%20for%0Amathematical%20problem%20solving.%20However%2C%20we%20identify%20a%20pervasive%20length%20bias%20in%0Aexisting%20PRMs%3A%20they%20tend%20to%20assign%20higher%20scores%20to%20longer%20reasoning%20steps%2C%0Aeven%20when%20the%20semantic%20content%20and%20logical%20validity%20are%20unchanged.%20This%20bias%0Aundermines%20the%20reliability%20of%20reward%20predictions%20and%20leads%20to%20overly%20verbose%0Aoutputs%20during%20inference.%20To%20address%20this%20issue%2C%20we%20propose%0ACoLD%28Counterfactually-Guided%20Length%20Debiasing%29%2C%20a%20unified%20framework%20that%0Amitigates%20length%20bias%20through%20three%20components%3A%20an%20explicit%20length-penalty%0Aadjustment%2C%20a%20learned%20bias%20estimator%20trained%20to%20capture%20spurious%20length-related%0Asignals%2C%20and%20a%20joint%20training%20strategy%20that%20enforces%20length-invariance%20in%0Areward%20predictions.%20Our%20approach%20is%20grounded%20in%20counterfactual%20reasoning%20and%0Ainformed%20by%20causal%20graph%20analysis.%20Extensive%20experiments%20on%20MATH500%20and%0AGSM-Plus%20show%20that%20CoLD%20consistently%20reduces%20reward-length%20correlation%2C%0Aimproves%20accuracy%20in%20step%20selection%2C%20and%20encourages%20more%20concise%2C%20logically%0Avalid%20reasoning.%20These%20results%20demonstrate%20the%20effectiveness%20and%20practicality%0Aof%20CoLD%20in%20improving%20the%20fidelity%20and%20robustness%20of%20PRMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoLD%253A%2520Counterfactually-Guided%2520Length%2520Debiasing%2520for%2520Process%2520Reward%2520Models%26entry.906535625%3DCongmin%2520Zheng%2520and%2520Jiachen%2520Zhu%2520and%2520Jianghao%2520Lin%2520and%2520Xinyi%2520Dai%2520and%2520Yong%2520Yu%2520and%2520Weinan%2520Zhang%2520and%2520Mengyue%2520Yang%26entry.1292438233%3D%2520%2520Process%2520Reward%2520Models%2520%2528PRMs%2529%2520play%2520a%2520central%2520role%2520in%2520evaluating%2520and%2520guiding%250Amulti-step%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520especially%2520for%250Amathematical%2520problem%2520solving.%2520However%252C%2520we%2520identify%2520a%2520pervasive%2520length%2520bias%2520in%250Aexisting%2520PRMs%253A%2520they%2520tend%2520to%2520assign%2520higher%2520scores%2520to%2520longer%2520reasoning%2520steps%252C%250Aeven%2520when%2520the%2520semantic%2520content%2520and%2520logical%2520validity%2520are%2520unchanged.%2520This%2520bias%250Aundermines%2520the%2520reliability%2520of%2520reward%2520predictions%2520and%2520leads%2520to%2520overly%2520verbose%250Aoutputs%2520during%2520inference.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250ACoLD%2528Counterfactually-Guided%2520Length%2520Debiasing%2529%252C%2520a%2520unified%2520framework%2520that%250Amitigates%2520length%2520bias%2520through%2520three%2520components%253A%2520an%2520explicit%2520length-penalty%250Aadjustment%252C%2520a%2520learned%2520bias%2520estimator%2520trained%2520to%2520capture%2520spurious%2520length-related%250Asignals%252C%2520and%2520a%2520joint%2520training%2520strategy%2520that%2520enforces%2520length-invariance%2520in%250Areward%2520predictions.%2520Our%2520approach%2520is%2520grounded%2520in%2520counterfactual%2520reasoning%2520and%250Ainformed%2520by%2520causal%2520graph%2520analysis.%2520Extensive%2520experiments%2520on%2520MATH500%2520and%250AGSM-Plus%2520show%2520that%2520CoLD%2520consistently%2520reduces%2520reward-length%2520correlation%252C%250Aimproves%2520accuracy%2520in%2520step%2520selection%252C%2520and%2520encourages%2520more%2520concise%252C%2520logically%250Avalid%2520reasoning.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520and%2520practicality%250Aof%2520CoLD%2520in%2520improving%2520the%2520fidelity%2520and%2520robustness%2520of%2520PRMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLD%3A%20Counterfactually-Guided%20Length%20Debiasing%20for%20Process%20Reward%20Models&entry.906535625=Congmin%20Zheng%20and%20Jiachen%20Zhu%20and%20Jianghao%20Lin%20and%20Xinyi%20Dai%20and%20Yong%20Yu%20and%20Weinan%20Zhang%20and%20Mengyue%20Yang&entry.1292438233=%20%20Process%20Reward%20Models%20%28PRMs%29%20play%20a%20central%20role%20in%20evaluating%20and%20guiding%0Amulti-step%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%20especially%20for%0Amathematical%20problem%20solving.%20However%2C%20we%20identify%20a%20pervasive%20length%20bias%20in%0Aexisting%20PRMs%3A%20they%20tend%20to%20assign%20higher%20scores%20to%20longer%20reasoning%20steps%2C%0Aeven%20when%20the%20semantic%20content%20and%20logical%20validity%20are%20unchanged.%20This%20bias%0Aundermines%20the%20reliability%20of%20reward%20predictions%20and%20leads%20to%20overly%20verbose%0Aoutputs%20during%20inference.%20To%20address%20this%20issue%2C%20we%20propose%0ACoLD%28Counterfactually-Guided%20Length%20Debiasing%29%2C%20a%20unified%20framework%20that%0Amitigates%20length%20bias%20through%20three%20components%3A%20an%20explicit%20length-penalty%0Aadjustment%2C%20a%20learned%20bias%20estimator%20trained%20to%20capture%20spurious%20length-related%0Asignals%2C%20and%20a%20joint%20training%20strategy%20that%20enforces%20length-invariance%20in%0Areward%20predictions.%20Our%20approach%20is%20grounded%20in%20counterfactual%20reasoning%20and%0Ainformed%20by%20causal%20graph%20analysis.%20Extensive%20experiments%20on%20MATH500%20and%0AGSM-Plus%20show%20that%20CoLD%20consistently%20reduces%20reward-length%20correlation%2C%0Aimproves%20accuracy%20in%20step%20selection%2C%20and%20encourages%20more%20concise%2C%20logically%0Avalid%20reasoning.%20These%20results%20demonstrate%20the%20effectiveness%20and%20practicality%0Aof%20CoLD%20in%20improving%20the%20fidelity%20and%20robustness%20of%20PRMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15698v1&entry.124074799=Read"},
{"title": "Quantum Learning Theory Beyond Batch Binary Classification", "author": "Preetham Mohan and Ambuj Tewari", "abstract": "  Arunachalam and de Wolf (2018) showed that the sample complexity of quantum\nbatch learning of boolean functions, in the realizable and agnostic settings,\nhas the same form and order as the corresponding classical sample complexities.\nIn this paper, we extend this, ostensibly surprising, message to batch\nmulticlass learning, online boolean learning, and online multiclass learning.\nFor our online learning results, we first consider an adaptive adversary\nvariant of the classical model of Dawid and Tewari (2022). Then, we introduce\nthe first (to the best of our knowledge) model of online learning with quantum\nexamples.\n", "link": "http://arxiv.org/abs/2302.07409v5", "date": "2025-07-21", "relevancy": 1.7631, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4544}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4383}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Learning%20Theory%20Beyond%20Batch%20Binary%20Classification&body=Title%3A%20Quantum%20Learning%20Theory%20Beyond%20Batch%20Binary%20Classification%0AAuthor%3A%20Preetham%20Mohan%20and%20Ambuj%20Tewari%0AAbstract%3A%20%20%20Arunachalam%20and%20de%20Wolf%20%282018%29%20showed%20that%20the%20sample%20complexity%20of%20quantum%0Abatch%20learning%20of%20boolean%20functions%2C%20in%20the%20realizable%20and%20agnostic%20settings%2C%0Ahas%20the%20same%20form%20and%20order%20as%20the%20corresponding%20classical%20sample%20complexities.%0AIn%20this%20paper%2C%20we%20extend%20this%2C%20ostensibly%20surprising%2C%20message%20to%20batch%0Amulticlass%20learning%2C%20online%20boolean%20learning%2C%20and%20online%20multiclass%20learning.%0AFor%20our%20online%20learning%20results%2C%20we%20first%20consider%20an%20adaptive%20adversary%0Avariant%20of%20the%20classical%20model%20of%20Dawid%20and%20Tewari%20%282022%29.%20Then%2C%20we%20introduce%0Athe%20first%20%28to%20the%20best%20of%20our%20knowledge%29%20model%20of%20online%20learning%20with%20quantum%0Aexamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.07409v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Learning%2520Theory%2520Beyond%2520Batch%2520Binary%2520Classification%26entry.906535625%3DPreetham%2520Mohan%2520and%2520Ambuj%2520Tewari%26entry.1292438233%3D%2520%2520Arunachalam%2520and%2520de%2520Wolf%2520%25282018%2529%2520showed%2520that%2520the%2520sample%2520complexity%2520of%2520quantum%250Abatch%2520learning%2520of%2520boolean%2520functions%252C%2520in%2520the%2520realizable%2520and%2520agnostic%2520settings%252C%250Ahas%2520the%2520same%2520form%2520and%2520order%2520as%2520the%2520corresponding%2520classical%2520sample%2520complexities.%250AIn%2520this%2520paper%252C%2520we%2520extend%2520this%252C%2520ostensibly%2520surprising%252C%2520message%2520to%2520batch%250Amulticlass%2520learning%252C%2520online%2520boolean%2520learning%252C%2520and%2520online%2520multiclass%2520learning.%250AFor%2520our%2520online%2520learning%2520results%252C%2520we%2520first%2520consider%2520an%2520adaptive%2520adversary%250Avariant%2520of%2520the%2520classical%2520model%2520of%2520Dawid%2520and%2520Tewari%2520%25282022%2529.%2520Then%252C%2520we%2520introduce%250Athe%2520first%2520%2528to%2520the%2520best%2520of%2520our%2520knowledge%2529%2520model%2520of%2520online%2520learning%2520with%2520quantum%250Aexamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.07409v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Learning%20Theory%20Beyond%20Batch%20Binary%20Classification&entry.906535625=Preetham%20Mohan%20and%20Ambuj%20Tewari&entry.1292438233=%20%20Arunachalam%20and%20de%20Wolf%20%282018%29%20showed%20that%20the%20sample%20complexity%20of%20quantum%0Abatch%20learning%20of%20boolean%20functions%2C%20in%20the%20realizable%20and%20agnostic%20settings%2C%0Ahas%20the%20same%20form%20and%20order%20as%20the%20corresponding%20classical%20sample%20complexities.%0AIn%20this%20paper%2C%20we%20extend%20this%2C%20ostensibly%20surprising%2C%20message%20to%20batch%0Amulticlass%20learning%2C%20online%20boolean%20learning%2C%20and%20online%20multiclass%20learning.%0AFor%20our%20online%20learning%20results%2C%20we%20first%20consider%20an%20adaptive%20adversary%0Avariant%20of%20the%20classical%20model%20of%20Dawid%20and%20Tewari%20%282022%29.%20Then%2C%20we%20introduce%0Athe%20first%20%28to%20the%20best%20of%20our%20knowledge%29%20model%20of%20online%20learning%20with%20quantum%0Aexamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.07409v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


