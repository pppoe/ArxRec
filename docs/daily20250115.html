<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250114.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Eigen Models for Human Heads", "author": "Wojciech Zielonka and Timo Bolkart and Thabo Beeler and Justus Thies", "abstract": "  Current personalized neural head avatars face a trade-off: lightweight models\nlack detail and realism, while high-quality, animatable avatars require\nsignificant computational resources, making them unsuitable for commodity\ndevices. To address this gap, we introduce Gaussian Eigen Models (GEM), which\nprovide high-quality, lightweight, and easily controllable head avatars. GEM\nutilizes 3D Gaussian primitives for representing the appearance combined with\nGaussian splatting for rendering. Building on the success of mesh-based 3D\nmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbases\nfor representing the head appearance of a specific subject. In particular, we\nconstruct linear bases to represent the position, scale, rotation, and opacity\nof the 3D Gaussians. This allows us to efficiently generate Gaussian primitives\nof a specific head shape by a linear combination of the basis vectors, only\nrequiring a low-dimensional parameter vector that contains the respective\ncoefficients. We propose to construct these linear bases (GEM) by distilling\nhigh-quality compute-intense CNN-based Gaussian avatar models that can generate\nexpression-dependent appearance changes like wrinkles. These high-quality\nmodels are trained on multi-view videos of a subject and are distilled using a\nseries of principal component analyses. Once we have obtained the bases that\nrepresent the animatable appearance space of a specific human, we learn a\nregressor that takes a single RGB image as input and predicts the\nlow-dimensional parameter vector that corresponds to the shown facial\nexpression. In a series of experiments, we compare GEM's self-reenactment and\ncross-person reenactment results to state-of-the-art 3D avatar methods,\ndemonstrating GEM's higher visual quality and better generalization to new\nexpressions.\n", "link": "http://arxiv.org/abs/2407.04545v2", "date": "2025-01-14", "relevancy": 3.7023, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7885}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7885}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Eigen%20Models%20for%20Human%20Heads&body=Title%3A%20Gaussian%20Eigen%20Models%20for%20Human%20Heads%0AAuthor%3A%20Wojciech%20Zielonka%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler%20and%20Justus%20Thies%0AAbstract%3A%20%20%20Current%20personalized%20neural%20head%20avatars%20face%20a%20trade-off%3A%20lightweight%20models%0Alack%20detail%20and%20realism%2C%20while%20high-quality%2C%20animatable%20avatars%20require%0Asignificant%20computational%20resources%2C%20making%20them%20unsuitable%20for%20commodity%0Adevices.%20To%20address%20this%20gap%2C%20we%20introduce%20Gaussian%20Eigen%20Models%20%28GEM%29%2C%20which%0Aprovide%20high-quality%2C%20lightweight%2C%20and%20easily%20controllable%20head%20avatars.%20GEM%0Autilizes%203D%20Gaussian%20primitives%20for%20representing%20the%20appearance%20combined%20with%0AGaussian%20splatting%20for%20rendering.%20Building%20on%20the%20success%20of%20mesh-based%203D%0Amorphable%20face%20models%20%283DMM%29%2C%20we%20define%20GEM%20as%20an%20ensemble%20of%20linear%20eigenbases%0Afor%20representing%20the%20head%20appearance%20of%20a%20specific%20subject.%20In%20particular%2C%20we%0Aconstruct%20linear%20bases%20to%20represent%20the%20position%2C%20scale%2C%20rotation%2C%20and%20opacity%0Aof%20the%203D%20Gaussians.%20This%20allows%20us%20to%20efficiently%20generate%20Gaussian%20primitives%0Aof%20a%20specific%20head%20shape%20by%20a%20linear%20combination%20of%20the%20basis%20vectors%2C%20only%0Arequiring%20a%20low-dimensional%20parameter%20vector%20that%20contains%20the%20respective%0Acoefficients.%20We%20propose%20to%20construct%20these%20linear%20bases%20%28GEM%29%20by%20distilling%0Ahigh-quality%20compute-intense%20CNN-based%20Gaussian%20avatar%20models%20that%20can%20generate%0Aexpression-dependent%20appearance%20changes%20like%20wrinkles.%20These%20high-quality%0Amodels%20are%20trained%20on%20multi-view%20videos%20of%20a%20subject%20and%20are%20distilled%20using%20a%0Aseries%20of%20principal%20component%20analyses.%20Once%20we%20have%20obtained%20the%20bases%20that%0Arepresent%20the%20animatable%20appearance%20space%20of%20a%20specific%20human%2C%20we%20learn%20a%0Aregressor%20that%20takes%20a%20single%20RGB%20image%20as%20input%20and%20predicts%20the%0Alow-dimensional%20parameter%20vector%20that%20corresponds%20to%20the%20shown%20facial%0Aexpression.%20In%20a%20series%20of%20experiments%2C%20we%20compare%20GEM%27s%20self-reenactment%20and%0Across-person%20reenactment%20results%20to%20state-of-the-art%203D%20avatar%20methods%2C%0Ademonstrating%20GEM%27s%20higher%20visual%20quality%20and%20better%20generalization%20to%20new%0Aexpressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Eigen%2520Models%2520for%2520Human%2520Heads%26entry.906535625%3DWojciech%2520Zielonka%2520and%2520Timo%2520Bolkart%2520and%2520Thabo%2520Beeler%2520and%2520Justus%2520Thies%26entry.1292438233%3D%2520%2520Current%2520personalized%2520neural%2520head%2520avatars%2520face%2520a%2520trade-off%253A%2520lightweight%2520models%250Alack%2520detail%2520and%2520realism%252C%2520while%2520high-quality%252C%2520animatable%2520avatars%2520require%250Asignificant%2520computational%2520resources%252C%2520making%2520them%2520unsuitable%2520for%2520commodity%250Adevices.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Gaussian%2520Eigen%2520Models%2520%2528GEM%2529%252C%2520which%250Aprovide%2520high-quality%252C%2520lightweight%252C%2520and%2520easily%2520controllable%2520head%2520avatars.%2520GEM%250Autilizes%25203D%2520Gaussian%2520primitives%2520for%2520representing%2520the%2520appearance%2520combined%2520with%250AGaussian%2520splatting%2520for%2520rendering.%2520Building%2520on%2520the%2520success%2520of%2520mesh-based%25203D%250Amorphable%2520face%2520models%2520%25283DMM%2529%252C%2520we%2520define%2520GEM%2520as%2520an%2520ensemble%2520of%2520linear%2520eigenbases%250Afor%2520representing%2520the%2520head%2520appearance%2520of%2520a%2520specific%2520subject.%2520In%2520particular%252C%2520we%250Aconstruct%2520linear%2520bases%2520to%2520represent%2520the%2520position%252C%2520scale%252C%2520rotation%252C%2520and%2520opacity%250Aof%2520the%25203D%2520Gaussians.%2520This%2520allows%2520us%2520to%2520efficiently%2520generate%2520Gaussian%2520primitives%250Aof%2520a%2520specific%2520head%2520shape%2520by%2520a%2520linear%2520combination%2520of%2520the%2520basis%2520vectors%252C%2520only%250Arequiring%2520a%2520low-dimensional%2520parameter%2520vector%2520that%2520contains%2520the%2520respective%250Acoefficients.%2520We%2520propose%2520to%2520construct%2520these%2520linear%2520bases%2520%2528GEM%2529%2520by%2520distilling%250Ahigh-quality%2520compute-intense%2520CNN-based%2520Gaussian%2520avatar%2520models%2520that%2520can%2520generate%250Aexpression-dependent%2520appearance%2520changes%2520like%2520wrinkles.%2520These%2520high-quality%250Amodels%2520are%2520trained%2520on%2520multi-view%2520videos%2520of%2520a%2520subject%2520and%2520are%2520distilled%2520using%2520a%250Aseries%2520of%2520principal%2520component%2520analyses.%2520Once%2520we%2520have%2520obtained%2520the%2520bases%2520that%250Arepresent%2520the%2520animatable%2520appearance%2520space%2520of%2520a%2520specific%2520human%252C%2520we%2520learn%2520a%250Aregressor%2520that%2520takes%2520a%2520single%2520RGB%2520image%2520as%2520input%2520and%2520predicts%2520the%250Alow-dimensional%2520parameter%2520vector%2520that%2520corresponds%2520to%2520the%2520shown%2520facial%250Aexpression.%2520In%2520a%2520series%2520of%2520experiments%252C%2520we%2520compare%2520GEM%2527s%2520self-reenactment%2520and%250Across-person%2520reenactment%2520results%2520to%2520state-of-the-art%25203D%2520avatar%2520methods%252C%250Ademonstrating%2520GEM%2527s%2520higher%2520visual%2520quality%2520and%2520better%2520generalization%2520to%2520new%250Aexpressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Eigen%20Models%20for%20Human%20Heads&entry.906535625=Wojciech%20Zielonka%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler%20and%20Justus%20Thies&entry.1292438233=%20%20Current%20personalized%20neural%20head%20avatars%20face%20a%20trade-off%3A%20lightweight%20models%0Alack%20detail%20and%20realism%2C%20while%20high-quality%2C%20animatable%20avatars%20require%0Asignificant%20computational%20resources%2C%20making%20them%20unsuitable%20for%20commodity%0Adevices.%20To%20address%20this%20gap%2C%20we%20introduce%20Gaussian%20Eigen%20Models%20%28GEM%29%2C%20which%0Aprovide%20high-quality%2C%20lightweight%2C%20and%20easily%20controllable%20head%20avatars.%20GEM%0Autilizes%203D%20Gaussian%20primitives%20for%20representing%20the%20appearance%20combined%20with%0AGaussian%20splatting%20for%20rendering.%20Building%20on%20the%20success%20of%20mesh-based%203D%0Amorphable%20face%20models%20%283DMM%29%2C%20we%20define%20GEM%20as%20an%20ensemble%20of%20linear%20eigenbases%0Afor%20representing%20the%20head%20appearance%20of%20a%20specific%20subject.%20In%20particular%2C%20we%0Aconstruct%20linear%20bases%20to%20represent%20the%20position%2C%20scale%2C%20rotation%2C%20and%20opacity%0Aof%20the%203D%20Gaussians.%20This%20allows%20us%20to%20efficiently%20generate%20Gaussian%20primitives%0Aof%20a%20specific%20head%20shape%20by%20a%20linear%20combination%20of%20the%20basis%20vectors%2C%20only%0Arequiring%20a%20low-dimensional%20parameter%20vector%20that%20contains%20the%20respective%0Acoefficients.%20We%20propose%20to%20construct%20these%20linear%20bases%20%28GEM%29%20by%20distilling%0Ahigh-quality%20compute-intense%20CNN-based%20Gaussian%20avatar%20models%20that%20can%20generate%0Aexpression-dependent%20appearance%20changes%20like%20wrinkles.%20These%20high-quality%0Amodels%20are%20trained%20on%20multi-view%20videos%20of%20a%20subject%20and%20are%20distilled%20using%20a%0Aseries%20of%20principal%20component%20analyses.%20Once%20we%20have%20obtained%20the%20bases%20that%0Arepresent%20the%20animatable%20appearance%20space%20of%20a%20specific%20human%2C%20we%20learn%20a%0Aregressor%20that%20takes%20a%20single%20RGB%20image%20as%20input%20and%20predicts%20the%0Alow-dimensional%20parameter%20vector%20that%20corresponds%20to%20the%20shown%20facial%0Aexpression.%20In%20a%20series%20of%20experiments%2C%20we%20compare%20GEM%27s%20self-reenactment%20and%0Across-person%20reenactment%20results%20to%20state-of-the-art%203D%20avatar%20methods%2C%0Ademonstrating%20GEM%27s%20higher%20visual%20quality%20and%20better%20generalization%20to%20new%0Aexpressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04545v2&entry.124074799=Read"},
{"title": "Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale\n  Super-Resolution", "author": "Du Chen and Liyi Chen and Zhengqiang Zhang and Lei Zhang", "abstract": "  Equipped with the continuous representation capability of Multi-Layer\nPerceptron (MLP), Implicit Neural Representation (INR) has been successfully\nemployed for Arbitrary-scale Super-Resolution (ASR). However, the limited\nreceptive field of the linear layers in MLP restricts the representation\ncapability of INR, while it is computationally expensive to query the MLP\nnumerous times to render each pixel. Recently, Gaussian Splatting (GS) has\nshown its advantages over INR in both visual quality and rendering speed in 3D\ntasks, which motivates us to explore whether GS can be employed for the ASR\ntask. However, directly applying GS to ASR is exceptionally challenging because\nthe original GS is an optimization-based method through overfitting each single\nscene, while in ASR we aim to learn a single model that can generalize to\ndifferent images and scaling factors. We overcome these challenges by\ndeveloping two novel techniques. Firstly, to generalize GS for ASR, we\nelaborately design an architecture to predict the corresponding\nimage-conditioned Gaussians of the input low-resolution image in a feed-forward\nmanner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based\nscale-aware rasterization to render super-resolved images by sampling discrete\nRGB values from the predicted contiguous Gaussians. Via end-to-end training,\nour optimized network, namely GSASR, can perform ASR for any image and unseen\nscaling factors. Extensive experiments validate the effectiveness of our\nproposed method. The project page can be found at\n\\url{https://mt-cly.github.io/GSASR.github.io/}.\n", "link": "http://arxiv.org/abs/2501.06838v2", "date": "2025-01-14", "relevancy": 3.3857, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7219}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6563}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution&body=Title%3A%20Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution%0AAuthor%3A%20Du%20Chen%20and%20Liyi%20Chen%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Equipped%20with%20the%20continuous%20representation%20capability%20of%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20Implicit%20Neural%20Representation%20%28INR%29%20has%20been%20successfully%0Aemployed%20for%20Arbitrary-scale%20Super-Resolution%20%28ASR%29.%20However%2C%20the%20limited%0Areceptive%20field%20of%20the%20linear%20layers%20in%20MLP%20restricts%20the%20representation%0Acapability%20of%20INR%2C%20while%20it%20is%20computationally%20expensive%20to%20query%20the%20MLP%0Anumerous%20times%20to%20render%20each%20pixel.%20Recently%2C%20Gaussian%20Splatting%20%28GS%29%20has%0Ashown%20its%20advantages%20over%20INR%20in%20both%20visual%20quality%20and%20rendering%20speed%20in%203D%0Atasks%2C%20which%20motivates%20us%20to%20explore%20whether%20GS%20can%20be%20employed%20for%20the%20ASR%0Atask.%20However%2C%20directly%20applying%20GS%20to%20ASR%20is%20exceptionally%20challenging%20because%0Athe%20original%20GS%20is%20an%20optimization-based%20method%20through%20overfitting%20each%20single%0Ascene%2C%20while%20in%20ASR%20we%20aim%20to%20learn%20a%20single%20model%20that%20can%20generalize%20to%0Adifferent%20images%20and%20scaling%20factors.%20We%20overcome%20these%20challenges%20by%0Adeveloping%20two%20novel%20techniques.%20Firstly%2C%20to%20generalize%20GS%20for%20ASR%2C%20we%0Aelaborately%20design%20an%20architecture%20to%20predict%20the%20corresponding%0Aimage-conditioned%20Gaussians%20of%20the%20input%20low-resolution%20image%20in%20a%20feed-forward%0Amanner.%20Secondly%2C%20we%20implement%20an%20efficient%20differentiable%202D%20GPU/CUDA-based%0Ascale-aware%20rasterization%20to%20render%20super-resolved%20images%20by%20sampling%20discrete%0ARGB%20values%20from%20the%20predicted%20contiguous%20Gaussians.%20Via%20end-to-end%20training%2C%0Aour%20optimized%20network%2C%20namely%20GSASR%2C%20can%20perform%20ASR%20for%20any%20image%20and%20unseen%0Ascaling%20factors.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%0Aproposed%20method.%20The%20project%20page%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//mt-cly.github.io/GSASR.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520and%2520Efficient%25202D%2520Gaussian%2520Splatting%2520for%2520Arbitrary-scale%250A%2520%2520Super-Resolution%26entry.906535625%3DDu%2520Chen%2520and%2520Liyi%2520Chen%2520and%2520Zhengqiang%2520Zhang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Equipped%2520with%2520the%2520continuous%2520representation%2520capability%2520of%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%252C%2520Implicit%2520Neural%2520Representation%2520%2528INR%2529%2520has%2520been%2520successfully%250Aemployed%2520for%2520Arbitrary-scale%2520Super-Resolution%2520%2528ASR%2529.%2520However%252C%2520the%2520limited%250Areceptive%2520field%2520of%2520the%2520linear%2520layers%2520in%2520MLP%2520restricts%2520the%2520representation%250Acapability%2520of%2520INR%252C%2520while%2520it%2520is%2520computationally%2520expensive%2520to%2520query%2520the%2520MLP%250Anumerous%2520times%2520to%2520render%2520each%2520pixel.%2520Recently%252C%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%250Ashown%2520its%2520advantages%2520over%2520INR%2520in%2520both%2520visual%2520quality%2520and%2520rendering%2520speed%2520in%25203D%250Atasks%252C%2520which%2520motivates%2520us%2520to%2520explore%2520whether%2520GS%2520can%2520be%2520employed%2520for%2520the%2520ASR%250Atask.%2520However%252C%2520directly%2520applying%2520GS%2520to%2520ASR%2520is%2520exceptionally%2520challenging%2520because%250Athe%2520original%2520GS%2520is%2520an%2520optimization-based%2520method%2520through%2520overfitting%2520each%2520single%250Ascene%252C%2520while%2520in%2520ASR%2520we%2520aim%2520to%2520learn%2520a%2520single%2520model%2520that%2520can%2520generalize%2520to%250Adifferent%2520images%2520and%2520scaling%2520factors.%2520We%2520overcome%2520these%2520challenges%2520by%250Adeveloping%2520two%2520novel%2520techniques.%2520Firstly%252C%2520to%2520generalize%2520GS%2520for%2520ASR%252C%2520we%250Aelaborately%2520design%2520an%2520architecture%2520to%2520predict%2520the%2520corresponding%250Aimage-conditioned%2520Gaussians%2520of%2520the%2520input%2520low-resolution%2520image%2520in%2520a%2520feed-forward%250Amanner.%2520Secondly%252C%2520we%2520implement%2520an%2520efficient%2520differentiable%25202D%2520GPU/CUDA-based%250Ascale-aware%2520rasterization%2520to%2520render%2520super-resolved%2520images%2520by%2520sampling%2520discrete%250ARGB%2520values%2520from%2520the%2520predicted%2520contiguous%2520Gaussians.%2520Via%2520end-to-end%2520training%252C%250Aour%2520optimized%2520network%252C%2520namely%2520GSASR%252C%2520can%2520perform%2520ASR%2520for%2520any%2520image%2520and%2520unseen%250Ascaling%2520factors.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%2520The%2520project%2520page%2520can%2520be%2520found%2520at%250A%255Curl%257Bhttps%253A//mt-cly.github.io/GSASR.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20and%20Efficient%202D%20Gaussian%20Splatting%20for%20Arbitrary-scale%0A%20%20Super-Resolution&entry.906535625=Du%20Chen%20and%20Liyi%20Chen%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang&entry.1292438233=%20%20Equipped%20with%20the%20continuous%20representation%20capability%20of%20Multi-Layer%0APerceptron%20%28MLP%29%2C%20Implicit%20Neural%20Representation%20%28INR%29%20has%20been%20successfully%0Aemployed%20for%20Arbitrary-scale%20Super-Resolution%20%28ASR%29.%20However%2C%20the%20limited%0Areceptive%20field%20of%20the%20linear%20layers%20in%20MLP%20restricts%20the%20representation%0Acapability%20of%20INR%2C%20while%20it%20is%20computationally%20expensive%20to%20query%20the%20MLP%0Anumerous%20times%20to%20render%20each%20pixel.%20Recently%2C%20Gaussian%20Splatting%20%28GS%29%20has%0Ashown%20its%20advantages%20over%20INR%20in%20both%20visual%20quality%20and%20rendering%20speed%20in%203D%0Atasks%2C%20which%20motivates%20us%20to%20explore%20whether%20GS%20can%20be%20employed%20for%20the%20ASR%0Atask.%20However%2C%20directly%20applying%20GS%20to%20ASR%20is%20exceptionally%20challenging%20because%0Athe%20original%20GS%20is%20an%20optimization-based%20method%20through%20overfitting%20each%20single%0Ascene%2C%20while%20in%20ASR%20we%20aim%20to%20learn%20a%20single%20model%20that%20can%20generalize%20to%0Adifferent%20images%20and%20scaling%20factors.%20We%20overcome%20these%20challenges%20by%0Adeveloping%20two%20novel%20techniques.%20Firstly%2C%20to%20generalize%20GS%20for%20ASR%2C%20we%0Aelaborately%20design%20an%20architecture%20to%20predict%20the%20corresponding%0Aimage-conditioned%20Gaussians%20of%20the%20input%20low-resolution%20image%20in%20a%20feed-forward%0Amanner.%20Secondly%2C%20we%20implement%20an%20efficient%20differentiable%202D%20GPU/CUDA-based%0Ascale-aware%20rasterization%20to%20render%20super-resolved%20images%20by%20sampling%20discrete%0ARGB%20values%20from%20the%20predicted%20contiguous%20Gaussians.%20Via%20end-to-end%20training%2C%0Aour%20optimized%20network%2C%20namely%20GSASR%2C%20can%20perform%20ASR%20for%20any%20image%20and%20unseen%0Ascaling%20factors.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%20our%0Aproposed%20method.%20The%20project%20page%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//mt-cly.github.io/GSASR.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06838v2&entry.124074799=Read"},
{"title": "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes", "author": "Ke Wu and Zicheng Zhang and Muer Tie and Ziqing Ai and Zhongxue Gan and Wenchao Ding", "abstract": "  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.\n", "link": "http://arxiv.org/abs/2501.08286v1", "date": "2025-01-14", "relevancy": 3.2085, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6209}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VINGS-Mono%3A%20Visual-Inertial%20Gaussian%20Splatting%20Monocular%20SLAM%20in%20Large%0A%20%20Scenes&body=Title%3A%20VINGS-Mono%3A%20Visual-Inertial%20Gaussian%20Splatting%20Monocular%20SLAM%20in%20Large%0A%20%20Scenes%0AAuthor%3A%20Ke%20Wu%20and%20Zicheng%20Zhang%20and%20Muer%20Tie%20and%20Ziqing%20Ai%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding%0AAbstract%3A%20%20%20VINGS-Mono%20is%20a%20monocular%20%28inertial%29%20Gaussian%20Splatting%20%28GS%29%20SLAM%20framework%0Adesigned%20for%20large%20scenes.%20The%20framework%20comprises%20four%20main%20components%3A%20VIO%0AFront%20End%2C%202D%20Gaussian%20Map%2C%20NVS%20Loop%20Closure%2C%20and%20Dynamic%20Eraser.%20In%20the%20VIO%0AFront%20End%2C%20RGB%20frames%20are%20processed%20through%20dense%20bundle%20adjustment%20and%0Auncertainty%20estimation%20to%20extract%20scene%20geometry%20and%20poses.%20Based%20on%20this%0Aoutput%2C%20the%20mapping%20module%20incrementally%20constructs%20and%20maintains%20a%202D%20Gaussian%0Amap.%20Key%20components%20of%20the%202D%20Gaussian%20Map%20include%20a%20Sample-based%20Rasterizer%2C%0AScore%20Manager%2C%20and%20Pose%20Refinement%2C%20which%20collectively%20improve%20mapping%20speed%0Aand%20localization%20accuracy.%20This%20enables%20the%20SLAM%20system%20to%20handle%20large-scale%0Aurban%20environments%20with%20up%20to%2050%20million%20Gaussian%20ellipsoids.%20To%20ensure%20global%0Aconsistency%20in%20large-scale%20scenes%2C%20we%20design%20a%20Loop%20Closure%20module%2C%20which%0Ainnovatively%20leverages%20the%20Novel%20View%20Synthesis%20%28NVS%29%20capabilities%20of%20Gaussian%0ASplatting%20for%20loop%20closure%20detection%20and%20correction%20of%20the%20Gaussian%20map.%0AAdditionally%2C%20we%20propose%20a%20Dynamic%20Eraser%20to%20address%20the%20inevitable%20presence%20of%0Adynamic%20objects%20in%20real-world%20outdoor%20scenes.%20Extensive%20evaluations%20in%20indoor%0Aand%20outdoor%20environments%20demonstrate%20that%20our%20approach%20achieves%20localization%0Aperformance%20on%20par%20with%20Visual-Inertial%20Odometry%20while%20surpassing%20recent%0AGS/NeRF%20SLAM%20methods.%20It%20also%20significantly%20outperforms%20all%20existing%20methods%20in%0Aterms%20of%20mapping%20and%20rendering%20quality.%20Furthermore%2C%20we%20developed%20a%20mobile%20app%0Aand%20verified%20that%20our%20framework%20can%20generate%20high-quality%20Gaussian%20maps%20in%20real%0Atime%20using%20only%20a%20smartphone%20camera%20and%20a%20low-frequency%20IMU%20sensor.%20To%20the%20best%0Aof%20our%20knowledge%2C%20VINGS-Mono%20is%20the%20first%20monocular%20Gaussian%20SLAM%20method%0Acapable%20of%20operating%20in%20outdoor%20environments%20and%20supporting%20kilometer-scale%0Alarge%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVINGS-Mono%253A%2520Visual-Inertial%2520Gaussian%2520Splatting%2520Monocular%2520SLAM%2520in%2520Large%250A%2520%2520Scenes%26entry.906535625%3DKe%2520Wu%2520and%2520Zicheng%2520Zhang%2520and%2520Muer%2520Tie%2520and%2520Ziqing%2520Ai%2520and%2520Zhongxue%2520Gan%2520and%2520Wenchao%2520Ding%26entry.1292438233%3D%2520%2520VINGS-Mono%2520is%2520a%2520monocular%2520%2528inertial%2529%2520Gaussian%2520Splatting%2520%2528GS%2529%2520SLAM%2520framework%250Adesigned%2520for%2520large%2520scenes.%2520The%2520framework%2520comprises%2520four%2520main%2520components%253A%2520VIO%250AFront%2520End%252C%25202D%2520Gaussian%2520Map%252C%2520NVS%2520Loop%2520Closure%252C%2520and%2520Dynamic%2520Eraser.%2520In%2520the%2520VIO%250AFront%2520End%252C%2520RGB%2520frames%2520are%2520processed%2520through%2520dense%2520bundle%2520adjustment%2520and%250Auncertainty%2520estimation%2520to%2520extract%2520scene%2520geometry%2520and%2520poses.%2520Based%2520on%2520this%250Aoutput%252C%2520the%2520mapping%2520module%2520incrementally%2520constructs%2520and%2520maintains%2520a%25202D%2520Gaussian%250Amap.%2520Key%2520components%2520of%2520the%25202D%2520Gaussian%2520Map%2520include%2520a%2520Sample-based%2520Rasterizer%252C%250AScore%2520Manager%252C%2520and%2520Pose%2520Refinement%252C%2520which%2520collectively%2520improve%2520mapping%2520speed%250Aand%2520localization%2520accuracy.%2520This%2520enables%2520the%2520SLAM%2520system%2520to%2520handle%2520large-scale%250Aurban%2520environments%2520with%2520up%2520to%252050%2520million%2520Gaussian%2520ellipsoids.%2520To%2520ensure%2520global%250Aconsistency%2520in%2520large-scale%2520scenes%252C%2520we%2520design%2520a%2520Loop%2520Closure%2520module%252C%2520which%250Ainnovatively%2520leverages%2520the%2520Novel%2520View%2520Synthesis%2520%2528NVS%2529%2520capabilities%2520of%2520Gaussian%250ASplatting%2520for%2520loop%2520closure%2520detection%2520and%2520correction%2520of%2520the%2520Gaussian%2520map.%250AAdditionally%252C%2520we%2520propose%2520a%2520Dynamic%2520Eraser%2520to%2520address%2520the%2520inevitable%2520presence%2520of%250Adynamic%2520objects%2520in%2520real-world%2520outdoor%2520scenes.%2520Extensive%2520evaluations%2520in%2520indoor%250Aand%2520outdoor%2520environments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520localization%250Aperformance%2520on%2520par%2520with%2520Visual-Inertial%2520Odometry%2520while%2520surpassing%2520recent%250AGS/NeRF%2520SLAM%2520methods.%2520It%2520also%2520significantly%2520outperforms%2520all%2520existing%2520methods%2520in%250Aterms%2520of%2520mapping%2520and%2520rendering%2520quality.%2520Furthermore%252C%2520we%2520developed%2520a%2520mobile%2520app%250Aand%2520verified%2520that%2520our%2520framework%2520can%2520generate%2520high-quality%2520Gaussian%2520maps%2520in%2520real%250Atime%2520using%2520only%2520a%2520smartphone%2520camera%2520and%2520a%2520low-frequency%2520IMU%2520sensor.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520VINGS-Mono%2520is%2520the%2520first%2520monocular%2520Gaussian%2520SLAM%2520method%250Acapable%2520of%2520operating%2520in%2520outdoor%2520environments%2520and%2520supporting%2520kilometer-scale%250Alarge%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VINGS-Mono%3A%20Visual-Inertial%20Gaussian%20Splatting%20Monocular%20SLAM%20in%20Large%0A%20%20Scenes&entry.906535625=Ke%20Wu%20and%20Zicheng%20Zhang%20and%20Muer%20Tie%20and%20Ziqing%20Ai%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding&entry.1292438233=%20%20VINGS-Mono%20is%20a%20monocular%20%28inertial%29%20Gaussian%20Splatting%20%28GS%29%20SLAM%20framework%0Adesigned%20for%20large%20scenes.%20The%20framework%20comprises%20four%20main%20components%3A%20VIO%0AFront%20End%2C%202D%20Gaussian%20Map%2C%20NVS%20Loop%20Closure%2C%20and%20Dynamic%20Eraser.%20In%20the%20VIO%0AFront%20End%2C%20RGB%20frames%20are%20processed%20through%20dense%20bundle%20adjustment%20and%0Auncertainty%20estimation%20to%20extract%20scene%20geometry%20and%20poses.%20Based%20on%20this%0Aoutput%2C%20the%20mapping%20module%20incrementally%20constructs%20and%20maintains%20a%202D%20Gaussian%0Amap.%20Key%20components%20of%20the%202D%20Gaussian%20Map%20include%20a%20Sample-based%20Rasterizer%2C%0AScore%20Manager%2C%20and%20Pose%20Refinement%2C%20which%20collectively%20improve%20mapping%20speed%0Aand%20localization%20accuracy.%20This%20enables%20the%20SLAM%20system%20to%20handle%20large-scale%0Aurban%20environments%20with%20up%20to%2050%20million%20Gaussian%20ellipsoids.%20To%20ensure%20global%0Aconsistency%20in%20large-scale%20scenes%2C%20we%20design%20a%20Loop%20Closure%20module%2C%20which%0Ainnovatively%20leverages%20the%20Novel%20View%20Synthesis%20%28NVS%29%20capabilities%20of%20Gaussian%0ASplatting%20for%20loop%20closure%20detection%20and%20correction%20of%20the%20Gaussian%20map.%0AAdditionally%2C%20we%20propose%20a%20Dynamic%20Eraser%20to%20address%20the%20inevitable%20presence%20of%0Adynamic%20objects%20in%20real-world%20outdoor%20scenes.%20Extensive%20evaluations%20in%20indoor%0Aand%20outdoor%20environments%20demonstrate%20that%20our%20approach%20achieves%20localization%0Aperformance%20on%20par%20with%20Visual-Inertial%20Odometry%20while%20surpassing%20recent%0AGS/NeRF%20SLAM%20methods.%20It%20also%20significantly%20outperforms%20all%20existing%20methods%20in%0Aterms%20of%20mapping%20and%20rendering%20quality.%20Furthermore%2C%20we%20developed%20a%20mobile%20app%0Aand%20verified%20that%20our%20framework%20can%20generate%20high-quality%20Gaussian%20maps%20in%20real%0Atime%20using%20only%20a%20smartphone%20camera%20and%20a%20low-frequency%20IMU%20sensor.%20To%20the%20best%0Aof%20our%20knowledge%2C%20VINGS-Mono%20is%20the%20first%20monocular%20Gaussian%20SLAM%20method%0Acapable%20of%20operating%20in%20outdoor%20environments%20and%20supporting%20kilometer-scale%0Alarge%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08286v1&entry.124074799=Read"},
{"title": "Revisiting Birds Eye View Perception Models with Frozen Foundation\n  Models: DINOv2 and Metric3Dv2", "author": "Seamie Hayes and Ganesh Sistu and Ciar\u00e1n Eising", "abstract": "  Birds Eye View perception models require extensive data to perform and\ngeneralize effectively. While traditional datasets often provide abundant\ndriving scenes from diverse locations, this is not always the case. It is\ncrucial to maximize the utility of the available training data. With the advent\nof large foundation models such as DINOv2 and Metric3Dv2, a pertinent question\narises: can these models be integrated into existing model architectures to not\nonly reduce the required training data but surpass the performance of current\nmodels? We choose two model architectures in the vehicle segmentation domain to\nalter: Lift-Splat-Shoot, and Simple-BEV. For Lift-Splat-Shoot, we explore the\nimplementation of frozen DINOv2 for feature extraction and Metric3Dv2 for depth\nestimation, where we greatly exceed the baseline results by 7.4 IoU while\nutilizing only half the training data and iterations. Furthermore, we introduce\nan innovative application of Metric3Dv2's depth information as a PseudoLiDAR\npoint cloud incorporated into the Simple-BEV architecture, replacing\ntraditional LiDAR. This integration results in a +3 IoU improvement compared to\nthe Camera-only model.\n", "link": "http://arxiv.org/abs/2501.08118v1", "date": "2025-01-14", "relevancy": 3.1467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6527}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Birds%20Eye%20View%20Perception%20Models%20with%20Frozen%20Foundation%0A%20%20Models%3A%20DINOv2%20and%20Metric3Dv2&body=Title%3A%20Revisiting%20Birds%20Eye%20View%20Perception%20Models%20with%20Frozen%20Foundation%0A%20%20Models%3A%20DINOv2%20and%20Metric3Dv2%0AAuthor%3A%20Seamie%20Hayes%20and%20Ganesh%20Sistu%20and%20Ciar%C3%A1n%20Eising%0AAbstract%3A%20%20%20Birds%20Eye%20View%20perception%20models%20require%20extensive%20data%20to%20perform%20and%0Ageneralize%20effectively.%20While%20traditional%20datasets%20often%20provide%20abundant%0Adriving%20scenes%20from%20diverse%20locations%2C%20this%20is%20not%20always%20the%20case.%20It%20is%0Acrucial%20to%20maximize%20the%20utility%20of%20the%20available%20training%20data.%20With%20the%20advent%0Aof%20large%20foundation%20models%20such%20as%20DINOv2%20and%20Metric3Dv2%2C%20a%20pertinent%20question%0Aarises%3A%20can%20these%20models%20be%20integrated%20into%20existing%20model%20architectures%20to%20not%0Aonly%20reduce%20the%20required%20training%20data%20but%20surpass%20the%20performance%20of%20current%0Amodels%3F%20We%20choose%20two%20model%20architectures%20in%20the%20vehicle%20segmentation%20domain%20to%0Aalter%3A%20Lift-Splat-Shoot%2C%20and%20Simple-BEV.%20For%20Lift-Splat-Shoot%2C%20we%20explore%20the%0Aimplementation%20of%20frozen%20DINOv2%20for%20feature%20extraction%20and%20Metric3Dv2%20for%20depth%0Aestimation%2C%20where%20we%20greatly%20exceed%20the%20baseline%20results%20by%207.4%20IoU%20while%0Autilizing%20only%20half%20the%20training%20data%20and%20iterations.%20Furthermore%2C%20we%20introduce%0Aan%20innovative%20application%20of%20Metric3Dv2%27s%20depth%20information%20as%20a%20PseudoLiDAR%0Apoint%20cloud%20incorporated%20into%20the%20Simple-BEV%20architecture%2C%20replacing%0Atraditional%20LiDAR.%20This%20integration%20results%20in%20a%20%2B3%20IoU%20improvement%20compared%20to%0Athe%20Camera-only%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Birds%2520Eye%2520View%2520Perception%2520Models%2520with%2520Frozen%2520Foundation%250A%2520%2520Models%253A%2520DINOv2%2520and%2520Metric3Dv2%26entry.906535625%3DSeamie%2520Hayes%2520and%2520Ganesh%2520Sistu%2520and%2520Ciar%25C3%25A1n%2520Eising%26entry.1292438233%3D%2520%2520Birds%2520Eye%2520View%2520perception%2520models%2520require%2520extensive%2520data%2520to%2520perform%2520and%250Ageneralize%2520effectively.%2520While%2520traditional%2520datasets%2520often%2520provide%2520abundant%250Adriving%2520scenes%2520from%2520diverse%2520locations%252C%2520this%2520is%2520not%2520always%2520the%2520case.%2520It%2520is%250Acrucial%2520to%2520maximize%2520the%2520utility%2520of%2520the%2520available%2520training%2520data.%2520With%2520the%2520advent%250Aof%2520large%2520foundation%2520models%2520such%2520as%2520DINOv2%2520and%2520Metric3Dv2%252C%2520a%2520pertinent%2520question%250Aarises%253A%2520can%2520these%2520models%2520be%2520integrated%2520into%2520existing%2520model%2520architectures%2520to%2520not%250Aonly%2520reduce%2520the%2520required%2520training%2520data%2520but%2520surpass%2520the%2520performance%2520of%2520current%250Amodels%253F%2520We%2520choose%2520two%2520model%2520architectures%2520in%2520the%2520vehicle%2520segmentation%2520domain%2520to%250Aalter%253A%2520Lift-Splat-Shoot%252C%2520and%2520Simple-BEV.%2520For%2520Lift-Splat-Shoot%252C%2520we%2520explore%2520the%250Aimplementation%2520of%2520frozen%2520DINOv2%2520for%2520feature%2520extraction%2520and%2520Metric3Dv2%2520for%2520depth%250Aestimation%252C%2520where%2520we%2520greatly%2520exceed%2520the%2520baseline%2520results%2520by%25207.4%2520IoU%2520while%250Autilizing%2520only%2520half%2520the%2520training%2520data%2520and%2520iterations.%2520Furthermore%252C%2520we%2520introduce%250Aan%2520innovative%2520application%2520of%2520Metric3Dv2%2527s%2520depth%2520information%2520as%2520a%2520PseudoLiDAR%250Apoint%2520cloud%2520incorporated%2520into%2520the%2520Simple-BEV%2520architecture%252C%2520replacing%250Atraditional%2520LiDAR.%2520This%2520integration%2520results%2520in%2520a%2520%252B3%2520IoU%2520improvement%2520compared%2520to%250Athe%2520Camera-only%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Birds%20Eye%20View%20Perception%20Models%20with%20Frozen%20Foundation%0A%20%20Models%3A%20DINOv2%20and%20Metric3Dv2&entry.906535625=Seamie%20Hayes%20and%20Ganesh%20Sistu%20and%20Ciar%C3%A1n%20Eising&entry.1292438233=%20%20Birds%20Eye%20View%20perception%20models%20require%20extensive%20data%20to%20perform%20and%0Ageneralize%20effectively.%20While%20traditional%20datasets%20often%20provide%20abundant%0Adriving%20scenes%20from%20diverse%20locations%2C%20this%20is%20not%20always%20the%20case.%20It%20is%0Acrucial%20to%20maximize%20the%20utility%20of%20the%20available%20training%20data.%20With%20the%20advent%0Aof%20large%20foundation%20models%20such%20as%20DINOv2%20and%20Metric3Dv2%2C%20a%20pertinent%20question%0Aarises%3A%20can%20these%20models%20be%20integrated%20into%20existing%20model%20architectures%20to%20not%0Aonly%20reduce%20the%20required%20training%20data%20but%20surpass%20the%20performance%20of%20current%0Amodels%3F%20We%20choose%20two%20model%20architectures%20in%20the%20vehicle%20segmentation%20domain%20to%0Aalter%3A%20Lift-Splat-Shoot%2C%20and%20Simple-BEV.%20For%20Lift-Splat-Shoot%2C%20we%20explore%20the%0Aimplementation%20of%20frozen%20DINOv2%20for%20feature%20extraction%20and%20Metric3Dv2%20for%20depth%0Aestimation%2C%20where%20we%20greatly%20exceed%20the%20baseline%20results%20by%207.4%20IoU%20while%0Autilizing%20only%20half%20the%20training%20data%20and%20iterations.%20Furthermore%2C%20we%20introduce%0Aan%20innovative%20application%20of%20Metric3Dv2%27s%20depth%20information%20as%20a%20PseudoLiDAR%0Apoint%20cloud%20incorporated%20into%20the%20Simple-BEV%20architecture%2C%20replacing%0Atraditional%20LiDAR.%20This%20integration%20results%20in%20a%20%2B3%20IoU%20improvement%20compared%20to%0Athe%20Camera-only%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08118v1&entry.124074799=Read"},
{"title": "GameFactory: Creating New Games with Generative Interactive Videos", "author": "Jiwen Yu and Yiran Qin and Xintao Wang and Pengfei Wan and Di Zhang and Xihui Liu", "abstract": "  Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\n\\url{https://vvictoryuki.github.io/gamefactory/}.\n", "link": "http://arxiv.org/abs/2501.08325v1", "date": "2025-01-14", "relevancy": 3.1213, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6706}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6239}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GameFactory%3A%20Creating%20New%20Games%20with%20Generative%20Interactive%20Videos&body=Title%3A%20GameFactory%3A%20Creating%20New%20Games%20with%20Generative%20Interactive%20Videos%0AAuthor%3A%20Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Generative%20game%20engines%20have%20the%20potential%20to%20revolutionize%20game%20development%0Aby%20autonomously%20creating%20new%20content%20and%20reducing%20manual%20workload.%20However%2C%0Aexisting%20video-based%20game%20generation%20methods%20fail%20to%20address%20the%20critical%0Achallenge%20of%20scene%20generalization%2C%20limiting%20their%20applicability%20to%20existing%0Agames%20with%20fixed%20styles%20and%20scenes.%20In%20this%20paper%2C%20we%20present%20GameFactory%2C%20a%0Aframework%20focused%20on%20exploring%20scene%20generalization%20in%20game%20video%20generation.%0ATo%20enable%20the%20creation%20of%20entirely%20new%20and%20diverse%20games%2C%20we%20leverage%0Apre-trained%20video%20diffusion%20models%20trained%20on%20open-domain%20video%20data.%20To%20bridge%0Athe%20domain%20gap%20between%20open-domain%20priors%20and%20small-scale%20game%20dataset%2C%20we%0Apropose%20a%20multi-phase%20training%20strategy%20that%20decouples%20game%20style%20learning%20from%0Aaction%20control%2C%20preserving%20open-domain%20generalization%20while%20achieving%20action%0Acontrollability.%20Using%20Minecraft%20as%20our%20data%20source%2C%20we%20release%20GF-Minecraft%2C%20a%0Ahigh-quality%20and%20diversity%20action-annotated%20video%20dataset%20for%20research.%0AFurthermore%2C%20we%20extend%20our%20framework%20to%20enable%20autoregressive%0Aaction-controllable%20game%20video%20generation%2C%20allowing%20the%20production%20of%0Aunlimited-length%20interactive%20game%20videos.%20Experimental%20results%20demonstrate%20that%0AGameFactory%20effectively%20generates%20open-domain%2C%20diverse%2C%20and%20action-controllable%0Agame%20videos%2C%20representing%20a%20significant%20step%20forward%20in%20AI-driven%20game%0Ageneration.%20Our%20dataset%20and%20project%20page%20are%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//vvictoryuki.github.io/gamefactory/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGameFactory%253A%2520Creating%2520New%2520Games%2520with%2520Generative%2520Interactive%2520Videos%26entry.906535625%3DJiwen%2520Yu%2520and%2520Yiran%2520Qin%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Generative%2520game%2520engines%2520have%2520the%2520potential%2520to%2520revolutionize%2520game%2520development%250Aby%2520autonomously%2520creating%2520new%2520content%2520and%2520reducing%2520manual%2520workload.%2520However%252C%250Aexisting%2520video-based%2520game%2520generation%2520methods%2520fail%2520to%2520address%2520the%2520critical%250Achallenge%2520of%2520scene%2520generalization%252C%2520limiting%2520their%2520applicability%2520to%2520existing%250Agames%2520with%2520fixed%2520styles%2520and%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520present%2520GameFactory%252C%2520a%250Aframework%2520focused%2520on%2520exploring%2520scene%2520generalization%2520in%2520game%2520video%2520generation.%250ATo%2520enable%2520the%2520creation%2520of%2520entirely%2520new%2520and%2520diverse%2520games%252C%2520we%2520leverage%250Apre-trained%2520video%2520diffusion%2520models%2520trained%2520on%2520open-domain%2520video%2520data.%2520To%2520bridge%250Athe%2520domain%2520gap%2520between%2520open-domain%2520priors%2520and%2520small-scale%2520game%2520dataset%252C%2520we%250Apropose%2520a%2520multi-phase%2520training%2520strategy%2520that%2520decouples%2520game%2520style%2520learning%2520from%250Aaction%2520control%252C%2520preserving%2520open-domain%2520generalization%2520while%2520achieving%2520action%250Acontrollability.%2520Using%2520Minecraft%2520as%2520our%2520data%2520source%252C%2520we%2520release%2520GF-Minecraft%252C%2520a%250Ahigh-quality%2520and%2520diversity%2520action-annotated%2520video%2520dataset%2520for%2520research.%250AFurthermore%252C%2520we%2520extend%2520our%2520framework%2520to%2520enable%2520autoregressive%250Aaction-controllable%2520game%2520video%2520generation%252C%2520allowing%2520the%2520production%2520of%250Aunlimited-length%2520interactive%2520game%2520videos.%2520Experimental%2520results%2520demonstrate%2520that%250AGameFactory%2520effectively%2520generates%2520open-domain%252C%2520diverse%252C%2520and%2520action-controllable%250Agame%2520videos%252C%2520representing%2520a%2520significant%2520step%2520forward%2520in%2520AI-driven%2520game%250Ageneration.%2520Our%2520dataset%2520and%2520project%2520page%2520are%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//vvictoryuki.github.io/gamefactory/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GameFactory%3A%20Creating%20New%20Games%20with%20Generative%20Interactive%20Videos&entry.906535625=Jiwen%20Yu%20and%20Yiran%20Qin%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Xihui%20Liu&entry.1292438233=%20%20Generative%20game%20engines%20have%20the%20potential%20to%20revolutionize%20game%20development%0Aby%20autonomously%20creating%20new%20content%20and%20reducing%20manual%20workload.%20However%2C%0Aexisting%20video-based%20game%20generation%20methods%20fail%20to%20address%20the%20critical%0Achallenge%20of%20scene%20generalization%2C%20limiting%20their%20applicability%20to%20existing%0Agames%20with%20fixed%20styles%20and%20scenes.%20In%20this%20paper%2C%20we%20present%20GameFactory%2C%20a%0Aframework%20focused%20on%20exploring%20scene%20generalization%20in%20game%20video%20generation.%0ATo%20enable%20the%20creation%20of%20entirely%20new%20and%20diverse%20games%2C%20we%20leverage%0Apre-trained%20video%20diffusion%20models%20trained%20on%20open-domain%20video%20data.%20To%20bridge%0Athe%20domain%20gap%20between%20open-domain%20priors%20and%20small-scale%20game%20dataset%2C%20we%0Apropose%20a%20multi-phase%20training%20strategy%20that%20decouples%20game%20style%20learning%20from%0Aaction%20control%2C%20preserving%20open-domain%20generalization%20while%20achieving%20action%0Acontrollability.%20Using%20Minecraft%20as%20our%20data%20source%2C%20we%20release%20GF-Minecraft%2C%20a%0Ahigh-quality%20and%20diversity%20action-annotated%20video%20dataset%20for%20research.%0AFurthermore%2C%20we%20extend%20our%20framework%20to%20enable%20autoregressive%0Aaction-controllable%20game%20video%20generation%2C%20allowing%20the%20production%20of%0Aunlimited-length%20interactive%20game%20videos.%20Experimental%20results%20demonstrate%20that%0AGameFactory%20effectively%20generates%20open-domain%2C%20diverse%2C%20and%20action-controllable%0Agame%20videos%2C%20representing%20a%20significant%20step%20forward%20in%20AI-driven%20game%0Ageneration.%20Our%20dataset%20and%20project%20page%20are%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//vvictoryuki.github.io/gamefactory/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08325v1&entry.124074799=Read"},
{"title": "FaVoR: Features via Voxel Rendering for Camera Relocalization", "author": "Vincenzo Polizzi and Marco Cannici and Davide Scaramuzza and Jonathan Kelly", "abstract": "  Camera relocalization methods range from dense image alignment to direct\ncamera pose regression from a query image. Among these, sparse feature matching\nstands out as an efficient, versatile, and generally lightweight approach with\nnumerous applications. However, feature-based methods often struggle with\nsignificant viewpoint and appearance changes, leading to matching failures and\ninaccurate pose estimates. To overcome this limitation, we propose a novel\napproach that leverages a globally sparse yet locally dense 3D representation\nof 2D features. By tracking and triangulating landmarks over a sequence of\nframes, we construct a sparse voxel map optimized to render image patch\ndescriptors observed during tracking. Given an initial pose estimate, we first\nsynthesize descriptors from the voxels using volumetric rendering and then\nperform feature matching to estimate the camera pose. This methodology enables\nthe generation of descriptors for unseen views, enhancing robustness to view\nchanges. We extensively evaluate our method on the 7-Scenes and Cambridge\nLandmarks datasets. Our results show that our method significantly outperforms\nexisting state-of-the-art feature representation techniques in indoor\nenvironments, achieving up to a 39% improvement in median translation error.\nAdditionally, our approach yields comparable results to other methods for\noutdoor scenarios while maintaining lower memory and computational costs.\n", "link": "http://arxiv.org/abs/2409.07571v3", "date": "2025-01-14", "relevancy": 2.989, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.619}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaVoR%3A%20Features%20via%20Voxel%20Rendering%20for%20Camera%20Relocalization&body=Title%3A%20FaVoR%3A%20Features%20via%20Voxel%20Rendering%20for%20Camera%20Relocalization%0AAuthor%3A%20Vincenzo%20Polizzi%20and%20Marco%20Cannici%20and%20Davide%20Scaramuzza%20and%20Jonathan%20Kelly%0AAbstract%3A%20%20%20Camera%20relocalization%20methods%20range%20from%20dense%20image%20alignment%20to%20direct%0Acamera%20pose%20regression%20from%20a%20query%20image.%20Among%20these%2C%20sparse%20feature%20matching%0Astands%20out%20as%20an%20efficient%2C%20versatile%2C%20and%20generally%20lightweight%20approach%20with%0Anumerous%20applications.%20However%2C%20feature-based%20methods%20often%20struggle%20with%0Asignificant%20viewpoint%20and%20appearance%20changes%2C%20leading%20to%20matching%20failures%20and%0Ainaccurate%20pose%20estimates.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%0Aapproach%20that%20leverages%20a%20globally%20sparse%20yet%20locally%20dense%203D%20representation%0Aof%202D%20features.%20By%20tracking%20and%20triangulating%20landmarks%20over%20a%20sequence%20of%0Aframes%2C%20we%20construct%20a%20sparse%20voxel%20map%20optimized%20to%20render%20image%20patch%0Adescriptors%20observed%20during%20tracking.%20Given%20an%20initial%20pose%20estimate%2C%20we%20first%0Asynthesize%20descriptors%20from%20the%20voxels%20using%20volumetric%20rendering%20and%20then%0Aperform%20feature%20matching%20to%20estimate%20the%20camera%20pose.%20This%20methodology%20enables%0Athe%20generation%20of%20descriptors%20for%20unseen%20views%2C%20enhancing%20robustness%20to%20view%0Achanges.%20We%20extensively%20evaluate%20our%20method%20on%20the%207-Scenes%20and%20Cambridge%0ALandmarks%20datasets.%20Our%20results%20show%20that%20our%20method%20significantly%20outperforms%0Aexisting%20state-of-the-art%20feature%20representation%20techniques%20in%20indoor%0Aenvironments%2C%20achieving%20up%20to%20a%2039%25%20improvement%20in%20median%20translation%20error.%0AAdditionally%2C%20our%20approach%20yields%20comparable%20results%20to%20other%20methods%20for%0Aoutdoor%20scenarios%20while%20maintaining%20lower%20memory%20and%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaVoR%253A%2520Features%2520via%2520Voxel%2520Rendering%2520for%2520Camera%2520Relocalization%26entry.906535625%3DVincenzo%2520Polizzi%2520and%2520Marco%2520Cannici%2520and%2520Davide%2520Scaramuzza%2520and%2520Jonathan%2520Kelly%26entry.1292438233%3D%2520%2520Camera%2520relocalization%2520methods%2520range%2520from%2520dense%2520image%2520alignment%2520to%2520direct%250Acamera%2520pose%2520regression%2520from%2520a%2520query%2520image.%2520Among%2520these%252C%2520sparse%2520feature%2520matching%250Astands%2520out%2520as%2520an%2520efficient%252C%2520versatile%252C%2520and%2520generally%2520lightweight%2520approach%2520with%250Anumerous%2520applications.%2520However%252C%2520feature-based%2520methods%2520often%2520struggle%2520with%250Asignificant%2520viewpoint%2520and%2520appearance%2520changes%252C%2520leading%2520to%2520matching%2520failures%2520and%250Ainaccurate%2520pose%2520estimates.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520that%2520leverages%2520a%2520globally%2520sparse%2520yet%2520locally%2520dense%25203D%2520representation%250Aof%25202D%2520features.%2520By%2520tracking%2520and%2520triangulating%2520landmarks%2520over%2520a%2520sequence%2520of%250Aframes%252C%2520we%2520construct%2520a%2520sparse%2520voxel%2520map%2520optimized%2520to%2520render%2520image%2520patch%250Adescriptors%2520observed%2520during%2520tracking.%2520Given%2520an%2520initial%2520pose%2520estimate%252C%2520we%2520first%250Asynthesize%2520descriptors%2520from%2520the%2520voxels%2520using%2520volumetric%2520rendering%2520and%2520then%250Aperform%2520feature%2520matching%2520to%2520estimate%2520the%2520camera%2520pose.%2520This%2520methodology%2520enables%250Athe%2520generation%2520of%2520descriptors%2520for%2520unseen%2520views%252C%2520enhancing%2520robustness%2520to%2520view%250Achanges.%2520We%2520extensively%2520evaluate%2520our%2520method%2520on%2520the%25207-Scenes%2520and%2520Cambridge%250ALandmarks%2520datasets.%2520Our%2520results%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%250Aexisting%2520state-of-the-art%2520feature%2520representation%2520techniques%2520in%2520indoor%250Aenvironments%252C%2520achieving%2520up%2520to%2520a%252039%2525%2520improvement%2520in%2520median%2520translation%2520error.%250AAdditionally%252C%2520our%2520approach%2520yields%2520comparable%2520results%2520to%2520other%2520methods%2520for%250Aoutdoor%2520scenarios%2520while%2520maintaining%2520lower%2520memory%2520and%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaVoR%3A%20Features%20via%20Voxel%20Rendering%20for%20Camera%20Relocalization&entry.906535625=Vincenzo%20Polizzi%20and%20Marco%20Cannici%20and%20Davide%20Scaramuzza%20and%20Jonathan%20Kelly&entry.1292438233=%20%20Camera%20relocalization%20methods%20range%20from%20dense%20image%20alignment%20to%20direct%0Acamera%20pose%20regression%20from%20a%20query%20image.%20Among%20these%2C%20sparse%20feature%20matching%0Astands%20out%20as%20an%20efficient%2C%20versatile%2C%20and%20generally%20lightweight%20approach%20with%0Anumerous%20applications.%20However%2C%20feature-based%20methods%20often%20struggle%20with%0Asignificant%20viewpoint%20and%20appearance%20changes%2C%20leading%20to%20matching%20failures%20and%0Ainaccurate%20pose%20estimates.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%0Aapproach%20that%20leverages%20a%20globally%20sparse%20yet%20locally%20dense%203D%20representation%0Aof%202D%20features.%20By%20tracking%20and%20triangulating%20landmarks%20over%20a%20sequence%20of%0Aframes%2C%20we%20construct%20a%20sparse%20voxel%20map%20optimized%20to%20render%20image%20patch%0Adescriptors%20observed%20during%20tracking.%20Given%20an%20initial%20pose%20estimate%2C%20we%20first%0Asynthesize%20descriptors%20from%20the%20voxels%20using%20volumetric%20rendering%20and%20then%0Aperform%20feature%20matching%20to%20estimate%20the%20camera%20pose.%20This%20methodology%20enables%0Athe%20generation%20of%20descriptors%20for%20unseen%20views%2C%20enhancing%20robustness%20to%20view%0Achanges.%20We%20extensively%20evaluate%20our%20method%20on%20the%207-Scenes%20and%20Cambridge%0ALandmarks%20datasets.%20Our%20results%20show%20that%20our%20method%20significantly%20outperforms%0Aexisting%20state-of-the-art%20feature%20representation%20techniques%20in%20indoor%0Aenvironments%2C%20achieving%20up%20to%20a%2039%25%20improvement%20in%20median%20translation%20error.%0AAdditionally%2C%20our%20approach%20yields%20comparable%20results%20to%20other%20methods%20for%0Aoutdoor%20scenarios%20while%20maintaining%20lower%20memory%20and%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07571v3&entry.124074799=Read"},
{"title": "GAC-Net_Geometric and attention-based Network for Depth Completion", "author": "Kuang Zhu and Xingli Gan and Min Sun", "abstract": "  Depth completion is a key task in autonomous driving, aiming to complete\nsparse LiDAR depth measurements into high-quality dense depth maps through\nimage guidance. However, existing methods usually treat depth maps as an\nadditional channel of color images, or directly perform convolution on sparse\ndata, failing to fully exploit the 3D geometric information in depth maps,\nespecially with limited performance in complex boundaries and sparse areas. To\naddress these issues, this paper proposes a depth completion network combining\nchannel attention mechanism and 3D global feature perception (CGA-Net). The\nmain innovations include: 1) Utilizing PointNet++ to extract global 3D\ngeometric features from sparse depth maps, enhancing the scene perception\nability of low-line LiDAR data; 2) Designing a channel-attention-based\nmultimodal feature fusion module to efficiently integrate sparse depth, RGB\nimages, and 3D geometric features; 3) Combining residual learning with CSPN++\nto optimize the depth refinement stage, further improving the completion\nquality in edge areas and complex scenes. Experiments on the KITTI depth\ncompletion dataset show that CGA-Net can significantly improve the prediction\naccuracy of dense depth maps, achieving a new state-of-the-art (SOTA), and\ndemonstrating strong robustness to sparse and complex scenes.\n", "link": "http://arxiv.org/abs/2501.07988v1", "date": "2025-01-14", "relevancy": 2.959, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.605}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5881}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAC-Net_Geometric%20and%20attention-based%20Network%20for%20Depth%20Completion&body=Title%3A%20GAC-Net_Geometric%20and%20attention-based%20Network%20for%20Depth%20Completion%0AAuthor%3A%20Kuang%20Zhu%20and%20Xingli%20Gan%20and%20Min%20Sun%0AAbstract%3A%20%20%20Depth%20completion%20is%20a%20key%20task%20in%20autonomous%20driving%2C%20aiming%20to%20complete%0Asparse%20LiDAR%20depth%20measurements%20into%20high-quality%20dense%20depth%20maps%20through%0Aimage%20guidance.%20However%2C%20existing%20methods%20usually%20treat%20depth%20maps%20as%20an%0Aadditional%20channel%20of%20color%20images%2C%20or%20directly%20perform%20convolution%20on%20sparse%0Adata%2C%20failing%20to%20fully%20exploit%20the%203D%20geometric%20information%20in%20depth%20maps%2C%0Aespecially%20with%20limited%20performance%20in%20complex%20boundaries%20and%20sparse%20areas.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20proposes%20a%20depth%20completion%20network%20combining%0Achannel%20attention%20mechanism%20and%203D%20global%20feature%20perception%20%28CGA-Net%29.%20The%0Amain%20innovations%20include%3A%201%29%20Utilizing%20PointNet%2B%2B%20to%20extract%20global%203D%0Ageometric%20features%20from%20sparse%20depth%20maps%2C%20enhancing%20the%20scene%20perception%0Aability%20of%20low-line%20LiDAR%20data%3B%202%29%20Designing%20a%20channel-attention-based%0Amultimodal%20feature%20fusion%20module%20to%20efficiently%20integrate%20sparse%20depth%2C%20RGB%0Aimages%2C%20and%203D%20geometric%20features%3B%203%29%20Combining%20residual%20learning%20with%20CSPN%2B%2B%0Ato%20optimize%20the%20depth%20refinement%20stage%2C%20further%20improving%20the%20completion%0Aquality%20in%20edge%20areas%20and%20complex%20scenes.%20Experiments%20on%20the%20KITTI%20depth%0Acompletion%20dataset%20show%20that%20CGA-Net%20can%20significantly%20improve%20the%20prediction%0Aaccuracy%20of%20dense%20depth%20maps%2C%20achieving%20a%20new%20state-of-the-art%20%28SOTA%29%2C%20and%0Ademonstrating%20strong%20robustness%20to%20sparse%20and%20complex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAC-Net_Geometric%2520and%2520attention-based%2520Network%2520for%2520Depth%2520Completion%26entry.906535625%3DKuang%2520Zhu%2520and%2520Xingli%2520Gan%2520and%2520Min%2520Sun%26entry.1292438233%3D%2520%2520Depth%2520completion%2520is%2520a%2520key%2520task%2520in%2520autonomous%2520driving%252C%2520aiming%2520to%2520complete%250Asparse%2520LiDAR%2520depth%2520measurements%2520into%2520high-quality%2520dense%2520depth%2520maps%2520through%250Aimage%2520guidance.%2520However%252C%2520existing%2520methods%2520usually%2520treat%2520depth%2520maps%2520as%2520an%250Aadditional%2520channel%2520of%2520color%2520images%252C%2520or%2520directly%2520perform%2520convolution%2520on%2520sparse%250Adata%252C%2520failing%2520to%2520fully%2520exploit%2520the%25203D%2520geometric%2520information%2520in%2520depth%2520maps%252C%250Aespecially%2520with%2520limited%2520performance%2520in%2520complex%2520boundaries%2520and%2520sparse%2520areas.%2520To%250Aaddress%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520depth%2520completion%2520network%2520combining%250Achannel%2520attention%2520mechanism%2520and%25203D%2520global%2520feature%2520perception%2520%2528CGA-Net%2529.%2520The%250Amain%2520innovations%2520include%253A%25201%2529%2520Utilizing%2520PointNet%252B%252B%2520to%2520extract%2520global%25203D%250Ageometric%2520features%2520from%2520sparse%2520depth%2520maps%252C%2520enhancing%2520the%2520scene%2520perception%250Aability%2520of%2520low-line%2520LiDAR%2520data%253B%25202%2529%2520Designing%2520a%2520channel-attention-based%250Amultimodal%2520feature%2520fusion%2520module%2520to%2520efficiently%2520integrate%2520sparse%2520depth%252C%2520RGB%250Aimages%252C%2520and%25203D%2520geometric%2520features%253B%25203%2529%2520Combining%2520residual%2520learning%2520with%2520CSPN%252B%252B%250Ato%2520optimize%2520the%2520depth%2520refinement%2520stage%252C%2520further%2520improving%2520the%2520completion%250Aquality%2520in%2520edge%2520areas%2520and%2520complex%2520scenes.%2520Experiments%2520on%2520the%2520KITTI%2520depth%250Acompletion%2520dataset%2520show%2520that%2520CGA-Net%2520can%2520significantly%2520improve%2520the%2520prediction%250Aaccuracy%2520of%2520dense%2520depth%2520maps%252C%2520achieving%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%252C%2520and%250Ademonstrating%2520strong%2520robustness%2520to%2520sparse%2520and%2520complex%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAC-Net_Geometric%20and%20attention-based%20Network%20for%20Depth%20Completion&entry.906535625=Kuang%20Zhu%20and%20Xingli%20Gan%20and%20Min%20Sun&entry.1292438233=%20%20Depth%20completion%20is%20a%20key%20task%20in%20autonomous%20driving%2C%20aiming%20to%20complete%0Asparse%20LiDAR%20depth%20measurements%20into%20high-quality%20dense%20depth%20maps%20through%0Aimage%20guidance.%20However%2C%20existing%20methods%20usually%20treat%20depth%20maps%20as%20an%0Aadditional%20channel%20of%20color%20images%2C%20or%20directly%20perform%20convolution%20on%20sparse%0Adata%2C%20failing%20to%20fully%20exploit%20the%203D%20geometric%20information%20in%20depth%20maps%2C%0Aespecially%20with%20limited%20performance%20in%20complex%20boundaries%20and%20sparse%20areas.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20proposes%20a%20depth%20completion%20network%20combining%0Achannel%20attention%20mechanism%20and%203D%20global%20feature%20perception%20%28CGA-Net%29.%20The%0Amain%20innovations%20include%3A%201%29%20Utilizing%20PointNet%2B%2B%20to%20extract%20global%203D%0Ageometric%20features%20from%20sparse%20depth%20maps%2C%20enhancing%20the%20scene%20perception%0Aability%20of%20low-line%20LiDAR%20data%3B%202%29%20Designing%20a%20channel-attention-based%0Amultimodal%20feature%20fusion%20module%20to%20efficiently%20integrate%20sparse%20depth%2C%20RGB%0Aimages%2C%20and%203D%20geometric%20features%3B%203%29%20Combining%20residual%20learning%20with%20CSPN%2B%2B%0Ato%20optimize%20the%20depth%20refinement%20stage%2C%20further%20improving%20the%20completion%0Aquality%20in%20edge%20areas%20and%20complex%20scenes.%20Experiments%20on%20the%20KITTI%20depth%0Acompletion%20dataset%20show%20that%20CGA-Net%20can%20significantly%20improve%20the%20prediction%0Aaccuracy%20of%20dense%20depth%20maps%2C%20achieving%20a%20new%20state-of-the-art%20%28SOTA%29%2C%20and%0Ademonstrating%20strong%20robustness%20to%20sparse%20and%20complex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07988v1&entry.124074799=Read"},
{"title": "Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference\n  Identification", "author": "Matti Kr\u00fcger and Yutaka Oshima and Yu Fang", "abstract": "  The visible orientation of human eyes creates some transparency about\npeople's spatial attention and other mental states. This leads to a dual role\nfor the eyes as a means of sensing and communication. Accordingly, artificial\neye models are being explored as communication media in human-machine\ninteraction scenarios. One challenge in the use of eye models for communication\nconsists of resolving spatial reference ambiguities, especially for\nscreen-based models. Here, we introduce an approach for overcoming this\nchallenge through the introduction of reflection-like features that are\ncontingent on artificial eye movements. We conducted a user study with 30\nparticipants in which participants had to use spatial references provided by\ndynamic eye models to advance in a fast-paced group interaction task. Compared\nto a non-reflective eye model and a pure reflection mode, their combination in\nthe new approach resulted in a higher identification accuracy and user\nexperience, suggesting a synergistic benefit.\n", "link": "http://arxiv.org/abs/2412.07344v3", "date": "2025-01-14", "relevancy": 2.9341, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%0A%20%20Identification&body=Title%3A%20Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%0A%20%20Identification%0AAuthor%3A%20Matti%20Kr%C3%BCger%20and%20Yutaka%20Oshima%20and%20Yu%20Fang%0AAbstract%3A%20%20%20The%20visible%20orientation%20of%20human%20eyes%20creates%20some%20transparency%20about%0Apeople%27s%20spatial%20attention%20and%20other%20mental%20states.%20This%20leads%20to%20a%20dual%20role%0Afor%20the%20eyes%20as%20a%20means%20of%20sensing%20and%20communication.%20Accordingly%2C%20artificial%0Aeye%20models%20are%20being%20explored%20as%20communication%20media%20in%20human-machine%0Ainteraction%20scenarios.%20One%20challenge%20in%20the%20use%20of%20eye%20models%20for%20communication%0Aconsists%20of%20resolving%20spatial%20reference%20ambiguities%2C%20especially%20for%0Ascreen-based%20models.%20Here%2C%20we%20introduce%20an%20approach%20for%20overcoming%20this%0Achallenge%20through%20the%20introduction%20of%20reflection-like%20features%20that%20are%0Acontingent%20on%20artificial%20eye%20movements.%20We%20conducted%20a%20user%20study%20with%2030%0Aparticipants%20in%20which%20participants%20had%20to%20use%20spatial%20references%20provided%20by%0Adynamic%20eye%20models%20to%20advance%20in%20a%20fast-paced%20group%20interaction%20task.%20Compared%0Ato%20a%20non-reflective%20eye%20model%20and%20a%20pure%20reflection%20mode%2C%20their%20combination%20in%0Athe%20new%20approach%20resulted%20in%20a%20higher%20identification%20accuracy%20and%20user%0Aexperience%2C%20suggesting%20a%20synergistic%20benefit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520Reflections%2520on%2520a%2520Dynamic%25202D%2520Eye%2520Model%2520Improve%2520Spatial%2520Reference%250A%2520%2520Identification%26entry.906535625%3DMatti%2520Kr%25C3%25BCger%2520and%2520Yutaka%2520Oshima%2520and%2520Yu%2520Fang%26entry.1292438233%3D%2520%2520The%2520visible%2520orientation%2520of%2520human%2520eyes%2520creates%2520some%2520transparency%2520about%250Apeople%2527s%2520spatial%2520attention%2520and%2520other%2520mental%2520states.%2520This%2520leads%2520to%2520a%2520dual%2520role%250Afor%2520the%2520eyes%2520as%2520a%2520means%2520of%2520sensing%2520and%2520communication.%2520Accordingly%252C%2520artificial%250Aeye%2520models%2520are%2520being%2520explored%2520as%2520communication%2520media%2520in%2520human-machine%250Ainteraction%2520scenarios.%2520One%2520challenge%2520in%2520the%2520use%2520of%2520eye%2520models%2520for%2520communication%250Aconsists%2520of%2520resolving%2520spatial%2520reference%2520ambiguities%252C%2520especially%2520for%250Ascreen-based%2520models.%2520Here%252C%2520we%2520introduce%2520an%2520approach%2520for%2520overcoming%2520this%250Achallenge%2520through%2520the%2520introduction%2520of%2520reflection-like%2520features%2520that%2520are%250Acontingent%2520on%2520artificial%2520eye%2520movements.%2520We%2520conducted%2520a%2520user%2520study%2520with%252030%250Aparticipants%2520in%2520which%2520participants%2520had%2520to%2520use%2520spatial%2520references%2520provided%2520by%250Adynamic%2520eye%2520models%2520to%2520advance%2520in%2520a%2520fast-paced%2520group%2520interaction%2520task.%2520Compared%250Ato%2520a%2520non-reflective%2520eye%2520model%2520and%2520a%2520pure%2520reflection%2520mode%252C%2520their%2520combination%2520in%250Athe%2520new%2520approach%2520resulted%2520in%2520a%2520higher%2520identification%2520accuracy%2520and%2520user%250Aexperience%252C%2520suggesting%2520a%2520synergistic%2520benefit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20Reflections%20on%20a%20Dynamic%202D%20Eye%20Model%20Improve%20Spatial%20Reference%0A%20%20Identification&entry.906535625=Matti%20Kr%C3%BCger%20and%20Yutaka%20Oshima%20and%20Yu%20Fang&entry.1292438233=%20%20The%20visible%20orientation%20of%20human%20eyes%20creates%20some%20transparency%20about%0Apeople%27s%20spatial%20attention%20and%20other%20mental%20states.%20This%20leads%20to%20a%20dual%20role%0Afor%20the%20eyes%20as%20a%20means%20of%20sensing%20and%20communication.%20Accordingly%2C%20artificial%0Aeye%20models%20are%20being%20explored%20as%20communication%20media%20in%20human-machine%0Ainteraction%20scenarios.%20One%20challenge%20in%20the%20use%20of%20eye%20models%20for%20communication%0Aconsists%20of%20resolving%20spatial%20reference%20ambiguities%2C%20especially%20for%0Ascreen-based%20models.%20Here%2C%20we%20introduce%20an%20approach%20for%20overcoming%20this%0Achallenge%20through%20the%20introduction%20of%20reflection-like%20features%20that%20are%0Acontingent%20on%20artificial%20eye%20movements.%20We%20conducted%20a%20user%20study%20with%2030%0Aparticipants%20in%20which%20participants%20had%20to%20use%20spatial%20references%20provided%20by%0Adynamic%20eye%20models%20to%20advance%20in%20a%20fast-paced%20group%20interaction%20task.%20Compared%0Ato%20a%20non-reflective%20eye%20model%20and%20a%20pure%20reflection%20mode%2C%20their%20combination%20in%0Athe%20new%20approach%20resulted%20in%20a%20higher%20identification%20accuracy%20and%20user%0Aexperience%2C%20suggesting%20a%20synergistic%20benefit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07344v3&entry.124074799=Read"},
{"title": "Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model", "author": "Huan Ma and Yan Zhu and Changqing Zhang and Peilin Zhao and Baoyuan Wu and Long-Kai Huang and Qinghua Hu and Bingzhe Wu", "abstract": "  Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.\n", "link": "http://arxiv.org/abs/2403.00376v3", "date": "2025-01-14", "relevancy": 2.8391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spurious%20Feature%20Eraser%3A%20Stabilizing%20Test-Time%20Adaptation%20for%0A%20%20Vision-Language%20Foundation%20Model&body=Title%3A%20Spurious%20Feature%20Eraser%3A%20Stabilizing%20Test-Time%20Adaptation%20for%0A%20%20Vision-Language%20Foundation%20Model%0AAuthor%3A%20Huan%20Ma%20and%20Yan%20Zhu%20and%20Changqing%20Zhang%20and%20Peilin%20Zhao%20and%20Baoyuan%20Wu%20and%20Long-Kai%20Huang%20and%20Qinghua%20Hu%20and%20Bingzhe%20Wu%0AAbstract%3A%20%20%20Vision-language%20foundation%20models%20have%20exhibited%20remarkable%20success%20across%20a%0Amultitude%20of%20downstream%20tasks%20due%20to%20their%20scalability%20on%20extensive%20image-text%0Apaired%20data.%20However%2C%20these%20models%20also%20display%20significant%20limitations%20when%0Aapplied%20to%20downstream%20tasks%2C%20such%20as%20fine-grained%20image%20classification%2C%20as%20a%0Aresult%20of%20%60%60decision%20shortcuts%27%27%20that%20hinder%20their%20generalization%20capabilities.%0AIn%20this%20work%2C%20we%20find%20that%20the%20CLIP%20model%20possesses%20a%20rich%20set%20of%20features%2C%0Aencompassing%20both%20%5Ctextit%7Bdesired%20invariant%20causal%20features%7D%20and%0A%5Ctextit%7Bundesired%20decision%20shortcuts%7D.%20Moreover%2C%20the%20underperformance%20of%20CLIP%0Aon%20downstream%20tasks%20originates%20from%20its%20inability%20to%20effectively%20utilize%0Apre-trained%20features%20in%20accordance%20with%20specific%20task%20requirements.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20Spurious%20Feature%0AEraser%20%28SEraser%29%2C%20to%20alleviate%20the%20decision%20shortcuts%20by%20erasing%20the%20spurious%0Afeatures.%20Specifically%2C%20we%20introduce%20a%20test-time%20prompt%20tuning%20paradigm%20that%0Aoptimizes%20a%20learnable%20prompt%2C%20thereby%20compelling%20the%20model%20to%20exploit%20invariant%0Afeatures%20while%20disregarding%20decision%20shortcuts%20during%20the%20inference%20phase.%20The%0Aproposed%20method%20effectively%20alleviates%20excessive%20dependence%20on%20potentially%0Amisleading%20spurious%20information.%20We%20conduct%20comparative%20analysis%20of%20the%0Aproposed%20method%20against%20various%20approaches%20which%20validates%20the%20significant%0Asuperiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00376v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpurious%2520Feature%2520Eraser%253A%2520Stabilizing%2520Test-Time%2520Adaptation%2520for%250A%2520%2520Vision-Language%2520Foundation%2520Model%26entry.906535625%3DHuan%2520Ma%2520and%2520Yan%2520Zhu%2520and%2520Changqing%2520Zhang%2520and%2520Peilin%2520Zhao%2520and%2520Baoyuan%2520Wu%2520and%2520Long-Kai%2520Huang%2520and%2520Qinghua%2520Hu%2520and%2520Bingzhe%2520Wu%26entry.1292438233%3D%2520%2520Vision-language%2520foundation%2520models%2520have%2520exhibited%2520remarkable%2520success%2520across%2520a%250Amultitude%2520of%2520downstream%2520tasks%2520due%2520to%2520their%2520scalability%2520on%2520extensive%2520image-text%250Apaired%2520data.%2520However%252C%2520these%2520models%2520also%2520display%2520significant%2520limitations%2520when%250Aapplied%2520to%2520downstream%2520tasks%252C%2520such%2520as%2520fine-grained%2520image%2520classification%252C%2520as%2520a%250Aresult%2520of%2520%2560%2560decision%2520shortcuts%2527%2527%2520that%2520hinder%2520their%2520generalization%2520capabilities.%250AIn%2520this%2520work%252C%2520we%2520find%2520that%2520the%2520CLIP%2520model%2520possesses%2520a%2520rich%2520set%2520of%2520features%252C%250Aencompassing%2520both%2520%255Ctextit%257Bdesired%2520invariant%2520causal%2520features%257D%2520and%250A%255Ctextit%257Bundesired%2520decision%2520shortcuts%257D.%2520Moreover%252C%2520the%2520underperformance%2520of%2520CLIP%250Aon%2520downstream%2520tasks%2520originates%2520from%2520its%2520inability%2520to%2520effectively%2520utilize%250Apre-trained%2520features%2520in%2520accordance%2520with%2520specific%2520task%2520requirements.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520Spurious%2520Feature%250AEraser%2520%2528SEraser%2529%252C%2520to%2520alleviate%2520the%2520decision%2520shortcuts%2520by%2520erasing%2520the%2520spurious%250Afeatures.%2520Specifically%252C%2520we%2520introduce%2520a%2520test-time%2520prompt%2520tuning%2520paradigm%2520that%250Aoptimizes%2520a%2520learnable%2520prompt%252C%2520thereby%2520compelling%2520the%2520model%2520to%2520exploit%2520invariant%250Afeatures%2520while%2520disregarding%2520decision%2520shortcuts%2520during%2520the%2520inference%2520phase.%2520The%250Aproposed%2520method%2520effectively%2520alleviates%2520excessive%2520dependence%2520on%2520potentially%250Amisleading%2520spurious%2520information.%2520We%2520conduct%2520comparative%2520analysis%2520of%2520the%250Aproposed%2520method%2520against%2520various%2520approaches%2520which%2520validates%2520the%2520significant%250Asuperiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00376v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spurious%20Feature%20Eraser%3A%20Stabilizing%20Test-Time%20Adaptation%20for%0A%20%20Vision-Language%20Foundation%20Model&entry.906535625=Huan%20Ma%20and%20Yan%20Zhu%20and%20Changqing%20Zhang%20and%20Peilin%20Zhao%20and%20Baoyuan%20Wu%20and%20Long-Kai%20Huang%20and%20Qinghua%20Hu%20and%20Bingzhe%20Wu&entry.1292438233=%20%20Vision-language%20foundation%20models%20have%20exhibited%20remarkable%20success%20across%20a%0Amultitude%20of%20downstream%20tasks%20due%20to%20their%20scalability%20on%20extensive%20image-text%0Apaired%20data.%20However%2C%20these%20models%20also%20display%20significant%20limitations%20when%0Aapplied%20to%20downstream%20tasks%2C%20such%20as%20fine-grained%20image%20classification%2C%20as%20a%0Aresult%20of%20%60%60decision%20shortcuts%27%27%20that%20hinder%20their%20generalization%20capabilities.%0AIn%20this%20work%2C%20we%20find%20that%20the%20CLIP%20model%20possesses%20a%20rich%20set%20of%20features%2C%0Aencompassing%20both%20%5Ctextit%7Bdesired%20invariant%20causal%20features%7D%20and%0A%5Ctextit%7Bundesired%20decision%20shortcuts%7D.%20Moreover%2C%20the%20underperformance%20of%20CLIP%0Aon%20downstream%20tasks%20originates%20from%20its%20inability%20to%20effectively%20utilize%0Apre-trained%20features%20in%20accordance%20with%20specific%20task%20requirements.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20Spurious%20Feature%0AEraser%20%28SEraser%29%2C%20to%20alleviate%20the%20decision%20shortcuts%20by%20erasing%20the%20spurious%0Afeatures.%20Specifically%2C%20we%20introduce%20a%20test-time%20prompt%20tuning%20paradigm%20that%0Aoptimizes%20a%20learnable%20prompt%2C%20thereby%20compelling%20the%20model%20to%20exploit%20invariant%0Afeatures%20while%20disregarding%20decision%20shortcuts%20during%20the%20inference%20phase.%20The%0Aproposed%20method%20effectively%20alleviates%20excessive%20dependence%20on%20potentially%0Amisleading%20spurious%20information.%20We%20conduct%20comparative%20analysis%20of%20the%0Aproposed%20method%20against%20various%20approaches%20which%20validates%20the%20significant%0Asuperiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00376v3&entry.124074799=Read"},
{"title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained\n  Spatial-Temporal Understanding", "author": "Hongyu Li and Jinyu Chen and Ziyu Wei and Shaofei Huang and Tianrui Hui and Jialin Gao and Xiaoming Wei and Si Liu", "abstract": "  Recent advancements in multimodal large language models (MLLMs) have shown\npromising results, yet existing approaches struggle to effectively handle both\ntemporal and spatial localization simultaneously. This challenge stems from two\nkey issues: first, incorporating spatial-temporal localization introduces a\nvast number of coordinate combinations, complicating the alignment of\nlinguistic and visual coordinate representations; second, encoding fine-grained\ntemporal and spatial information during video feature compression is inherently\ndifficult. To address these issues, we propose LLaVA-ST, a MLLM for\nfine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we propose\nLanguage-Aligned Positional Embedding, which embeds the textual coordinate\nspecial token into the visual space, simplifying the alignment of fine-grained\nspatial-temporal correspondences. Additionally, we design the Spatial-Temporal\nPacker, which decouples the feature compression of temporal and spatial\nresolutions into two distinct point-to-region attention processing streams.\nFurthermore, we propose ST-Align dataset with 4.3M training samples for\nfine-grained spatial-temporal multimodal understanding. With ST-align, we\npresent a progressive training pipeline that aligns the visual and textual\nfeature through sequential coarse-to-fine stages.Additionally, we introduce an\nST-Align benchmark to evaluate spatial-temporal interleaved fine-grained\nunderstanding tasks, which include Spatial-Temporal Video Grounding (STVG) ,\nEvent Localization and Captioning (ELC) and Spatial Video Grounding (SVG).\nLLaVA-ST achieves outstanding performance on 11 benchmarks requiring\nfine-grained temporal, spatial, or spatial-temporal interleaving multimodal\nunderstanding. Our code, data and benchmark will be released at Our code, data\nand benchmark will be released at https://github.com/appletea233/LLaVA-ST .\n", "link": "http://arxiv.org/abs/2501.08282v1", "date": "2025-01-14", "relevancy": 2.8303, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-ST%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Fine-Grained%0A%20%20Spatial-Temporal%20Understanding&body=Title%3A%20LLaVA-ST%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Fine-Grained%0A%20%20Spatial-Temporal%20Understanding%0AAuthor%3A%20Hongyu%20Li%20and%20Jinyu%20Chen%20and%20Ziyu%20Wei%20and%20Shaofei%20Huang%20and%20Tianrui%20Hui%20and%20Jialin%20Gao%20and%20Xiaoming%20Wei%20and%20Si%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Apromising%20results%2C%20yet%20existing%20approaches%20struggle%20to%20effectively%20handle%20both%0Atemporal%20and%20spatial%20localization%20simultaneously.%20This%20challenge%20stems%20from%20two%0Akey%20issues%3A%20first%2C%20incorporating%20spatial-temporal%20localization%20introduces%20a%0Avast%20number%20of%20coordinate%20combinations%2C%20complicating%20the%20alignment%20of%0Alinguistic%20and%20visual%20coordinate%20representations%3B%20second%2C%20encoding%20fine-grained%0Atemporal%20and%20spatial%20information%20during%20video%20feature%20compression%20is%20inherently%0Adifficult.%20To%20address%20these%20issues%2C%20we%20propose%20LLaVA-ST%2C%20a%20MLLM%20for%0Afine-grained%20spatial-temporal%20multimodal%20understanding.%20In%20LLaVA-ST%2C%20we%20propose%0ALanguage-Aligned%20Positional%20Embedding%2C%20which%20embeds%20the%20textual%20coordinate%0Aspecial%20token%20into%20the%20visual%20space%2C%20simplifying%20the%20alignment%20of%20fine-grained%0Aspatial-temporal%20correspondences.%20Additionally%2C%20we%20design%20the%20Spatial-Temporal%0APacker%2C%20which%20decouples%20the%20feature%20compression%20of%20temporal%20and%20spatial%0Aresolutions%20into%20two%20distinct%20point-to-region%20attention%20processing%20streams.%0AFurthermore%2C%20we%20propose%20ST-Align%20dataset%20with%204.3M%20training%20samples%20for%0Afine-grained%20spatial-temporal%20multimodal%20understanding.%20With%20ST-align%2C%20we%0Apresent%20a%20progressive%20training%20pipeline%20that%20aligns%20the%20visual%20and%20textual%0Afeature%20through%20sequential%20coarse-to-fine%20stages.Additionally%2C%20we%20introduce%20an%0AST-Align%20benchmark%20to%20evaluate%20spatial-temporal%20interleaved%20fine-grained%0Aunderstanding%20tasks%2C%20which%20include%20Spatial-Temporal%20Video%20Grounding%20%28STVG%29%20%2C%0AEvent%20Localization%20and%20Captioning%20%28ELC%29%20and%20Spatial%20Video%20Grounding%20%28SVG%29.%0ALLaVA-ST%20achieves%20outstanding%20performance%20on%2011%20benchmarks%20requiring%0Afine-grained%20temporal%2C%20spatial%2C%20or%20spatial-temporal%20interleaving%20multimodal%0Aunderstanding.%20Our%20code%2C%20data%20and%20benchmark%20will%20be%20released%20at%20Our%20code%2C%20data%0Aand%20benchmark%20will%20be%20released%20at%20https%3A//github.com/appletea233/LLaVA-ST%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-ST%253A%2520A%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Fine-Grained%250A%2520%2520Spatial-Temporal%2520Understanding%26entry.906535625%3DHongyu%2520Li%2520and%2520Jinyu%2520Chen%2520and%2520Ziyu%2520Wei%2520and%2520Shaofei%2520Huang%2520and%2520Tianrui%2520Hui%2520and%2520Jialin%2520Gao%2520and%2520Xiaoming%2520Wei%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%250Apromising%2520results%252C%2520yet%2520existing%2520approaches%2520struggle%2520to%2520effectively%2520handle%2520both%250Atemporal%2520and%2520spatial%2520localization%2520simultaneously.%2520This%2520challenge%2520stems%2520from%2520two%250Akey%2520issues%253A%2520first%252C%2520incorporating%2520spatial-temporal%2520localization%2520introduces%2520a%250Avast%2520number%2520of%2520coordinate%2520combinations%252C%2520complicating%2520the%2520alignment%2520of%250Alinguistic%2520and%2520visual%2520coordinate%2520representations%253B%2520second%252C%2520encoding%2520fine-grained%250Atemporal%2520and%2520spatial%2520information%2520during%2520video%2520feature%2520compression%2520is%2520inherently%250Adifficult.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520LLaVA-ST%252C%2520a%2520MLLM%2520for%250Afine-grained%2520spatial-temporal%2520multimodal%2520understanding.%2520In%2520LLaVA-ST%252C%2520we%2520propose%250ALanguage-Aligned%2520Positional%2520Embedding%252C%2520which%2520embeds%2520the%2520textual%2520coordinate%250Aspecial%2520token%2520into%2520the%2520visual%2520space%252C%2520simplifying%2520the%2520alignment%2520of%2520fine-grained%250Aspatial-temporal%2520correspondences.%2520Additionally%252C%2520we%2520design%2520the%2520Spatial-Temporal%250APacker%252C%2520which%2520decouples%2520the%2520feature%2520compression%2520of%2520temporal%2520and%2520spatial%250Aresolutions%2520into%2520two%2520distinct%2520point-to-region%2520attention%2520processing%2520streams.%250AFurthermore%252C%2520we%2520propose%2520ST-Align%2520dataset%2520with%25204.3M%2520training%2520samples%2520for%250Afine-grained%2520spatial-temporal%2520multimodal%2520understanding.%2520With%2520ST-align%252C%2520we%250Apresent%2520a%2520progressive%2520training%2520pipeline%2520that%2520aligns%2520the%2520visual%2520and%2520textual%250Afeature%2520through%2520sequential%2520coarse-to-fine%2520stages.Additionally%252C%2520we%2520introduce%2520an%250AST-Align%2520benchmark%2520to%2520evaluate%2520spatial-temporal%2520interleaved%2520fine-grained%250Aunderstanding%2520tasks%252C%2520which%2520include%2520Spatial-Temporal%2520Video%2520Grounding%2520%2528STVG%2529%2520%252C%250AEvent%2520Localization%2520and%2520Captioning%2520%2528ELC%2529%2520and%2520Spatial%2520Video%2520Grounding%2520%2528SVG%2529.%250ALLaVA-ST%2520achieves%2520outstanding%2520performance%2520on%252011%2520benchmarks%2520requiring%250Afine-grained%2520temporal%252C%2520spatial%252C%2520or%2520spatial-temporal%2520interleaving%2520multimodal%250Aunderstanding.%2520Our%2520code%252C%2520data%2520and%2520benchmark%2520will%2520be%2520released%2520at%2520Our%2520code%252C%2520data%250Aand%2520benchmark%2520will%2520be%2520released%2520at%2520https%253A//github.com/appletea233/LLaVA-ST%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-ST%3A%20A%20Multimodal%20Large%20Language%20Model%20for%20Fine-Grained%0A%20%20Spatial-Temporal%20Understanding&entry.906535625=Hongyu%20Li%20and%20Jinyu%20Chen%20and%20Ziyu%20Wei%20and%20Shaofei%20Huang%20and%20Tianrui%20Hui%20and%20Jialin%20Gao%20and%20Xiaoming%20Wei%20and%20Si%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Apromising%20results%2C%20yet%20existing%20approaches%20struggle%20to%20effectively%20handle%20both%0Atemporal%20and%20spatial%20localization%20simultaneously.%20This%20challenge%20stems%20from%20two%0Akey%20issues%3A%20first%2C%20incorporating%20spatial-temporal%20localization%20introduces%20a%0Avast%20number%20of%20coordinate%20combinations%2C%20complicating%20the%20alignment%20of%0Alinguistic%20and%20visual%20coordinate%20representations%3B%20second%2C%20encoding%20fine-grained%0Atemporal%20and%20spatial%20information%20during%20video%20feature%20compression%20is%20inherently%0Adifficult.%20To%20address%20these%20issues%2C%20we%20propose%20LLaVA-ST%2C%20a%20MLLM%20for%0Afine-grained%20spatial-temporal%20multimodal%20understanding.%20In%20LLaVA-ST%2C%20we%20propose%0ALanguage-Aligned%20Positional%20Embedding%2C%20which%20embeds%20the%20textual%20coordinate%0Aspecial%20token%20into%20the%20visual%20space%2C%20simplifying%20the%20alignment%20of%20fine-grained%0Aspatial-temporal%20correspondences.%20Additionally%2C%20we%20design%20the%20Spatial-Temporal%0APacker%2C%20which%20decouples%20the%20feature%20compression%20of%20temporal%20and%20spatial%0Aresolutions%20into%20two%20distinct%20point-to-region%20attention%20processing%20streams.%0AFurthermore%2C%20we%20propose%20ST-Align%20dataset%20with%204.3M%20training%20samples%20for%0Afine-grained%20spatial-temporal%20multimodal%20understanding.%20With%20ST-align%2C%20we%0Apresent%20a%20progressive%20training%20pipeline%20that%20aligns%20the%20visual%20and%20textual%0Afeature%20through%20sequential%20coarse-to-fine%20stages.Additionally%2C%20we%20introduce%20an%0AST-Align%20benchmark%20to%20evaluate%20spatial-temporal%20interleaved%20fine-grained%0Aunderstanding%20tasks%2C%20which%20include%20Spatial-Temporal%20Video%20Grounding%20%28STVG%29%20%2C%0AEvent%20Localization%20and%20Captioning%20%28ELC%29%20and%20Spatial%20Video%20Grounding%20%28SVG%29.%0ALLaVA-ST%20achieves%20outstanding%20performance%20on%2011%20benchmarks%20requiring%0Afine-grained%20temporal%2C%20spatial%2C%20or%20spatial-temporal%20interleaving%20multimodal%0Aunderstanding.%20Our%20code%2C%20data%20and%20benchmark%20will%20be%20released%20at%20Our%20code%2C%20data%0Aand%20benchmark%20will%20be%20released%20at%20https%3A//github.com/appletea233/LLaVA-ST%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08282v1&entry.124074799=Read"},
{"title": "Threshold Attention Network for Semantic Segmentation of Remote Sensing\n  Images", "author": "Wei Long and Yongjun Zhang and Zhongwei Cui and Yujie Xu and Xuexue Zhang", "abstract": "  Semantic segmentation of remote sensing images is essential for various\napplications, including vegetation monitoring, disaster management, and urban\nplanning. Previous studies have demonstrated that the self-attention mechanism\n(SA) is an effective approach for designing segmentation networks that can\ncapture long-range pixel dependencies. SA enables the network to model the\nglobal dependencies between the input features, resulting in improved\nsegmentation outcomes. However, the high density of attentional feature maps\nused in this mechanism causes exponential increases in computational\ncomplexity. Additionally, it introduces redundant information that negatively\nimpacts the feature representation. Inspired by traditional threshold\nsegmentation algorithms, we propose a novel threshold attention mechanism\n(TAM). This mechanism significantly reduces computational effort while also\nbetter modeling the correlation between different regions of the feature map.\nBased on TAM, we present a threshold attention network (TANet) for semantic\nsegmentation. TANet consists of an attentional feature enhancement module\n(AFEM) for global feature enhancement of shallow features and a threshold\nattention pyramid pooling module (TAPP) for acquiring feature information at\ndifferent scales for deep features. We have conducted extensive experiments on\nthe ISPRS Vaihingen and Potsdam datasets. The results demonstrate the validity\nand superiority of our proposed TANet compared to the most state-of-the-art\nmodels.\n", "link": "http://arxiv.org/abs/2501.07984v1", "date": "2025-01-14", "relevancy": 2.811, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5578}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Threshold%20Attention%20Network%20for%20Semantic%20Segmentation%20of%20Remote%20Sensing%0A%20%20Images&body=Title%3A%20Threshold%20Attention%20Network%20for%20Semantic%20Segmentation%20of%20Remote%20Sensing%0A%20%20Images%0AAuthor%3A%20Wei%20Long%20and%20Yongjun%20Zhang%20and%20Zhongwei%20Cui%20and%20Yujie%20Xu%20and%20Xuexue%20Zhang%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20essential%20for%20various%0Aapplications%2C%20including%20vegetation%20monitoring%2C%20disaster%20management%2C%20and%20urban%0Aplanning.%20Previous%20studies%20have%20demonstrated%20that%20the%20self-attention%20mechanism%0A%28SA%29%20is%20an%20effective%20approach%20for%20designing%20segmentation%20networks%20that%20can%0Acapture%20long-range%20pixel%20dependencies.%20SA%20enables%20the%20network%20to%20model%20the%0Aglobal%20dependencies%20between%20the%20input%20features%2C%20resulting%20in%20improved%0Asegmentation%20outcomes.%20However%2C%20the%20high%20density%20of%20attentional%20feature%20maps%0Aused%20in%20this%20mechanism%20causes%20exponential%20increases%20in%20computational%0Acomplexity.%20Additionally%2C%20it%20introduces%20redundant%20information%20that%20negatively%0Aimpacts%20the%20feature%20representation.%20Inspired%20by%20traditional%20threshold%0Asegmentation%20algorithms%2C%20we%20propose%20a%20novel%20threshold%20attention%20mechanism%0A%28TAM%29.%20This%20mechanism%20significantly%20reduces%20computational%20effort%20while%20also%0Abetter%20modeling%20the%20correlation%20between%20different%20regions%20of%20the%20feature%20map.%0ABased%20on%20TAM%2C%20we%20present%20a%20threshold%20attention%20network%20%28TANet%29%20for%20semantic%0Asegmentation.%20TANet%20consists%20of%20an%20attentional%20feature%20enhancement%20module%0A%28AFEM%29%20for%20global%20feature%20enhancement%20of%20shallow%20features%20and%20a%20threshold%0Aattention%20pyramid%20pooling%20module%20%28TAPP%29%20for%20acquiring%20feature%20information%20at%0Adifferent%20scales%20for%20deep%20features.%20We%20have%20conducted%20extensive%20experiments%20on%0Athe%20ISPRS%20Vaihingen%20and%20Potsdam%20datasets.%20The%20results%20demonstrate%20the%20validity%0Aand%20superiority%20of%20our%20proposed%20TANet%20compared%20to%20the%20most%20state-of-the-art%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThreshold%2520Attention%2520Network%2520for%2520Semantic%2520Segmentation%2520of%2520Remote%2520Sensing%250A%2520%2520Images%26entry.906535625%3DWei%2520Long%2520and%2520Yongjun%2520Zhang%2520and%2520Zhongwei%2520Cui%2520and%2520Yujie%2520Xu%2520and%2520Xuexue%2520Zhang%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520remote%2520sensing%2520images%2520is%2520essential%2520for%2520various%250Aapplications%252C%2520including%2520vegetation%2520monitoring%252C%2520disaster%2520management%252C%2520and%2520urban%250Aplanning.%2520Previous%2520studies%2520have%2520demonstrated%2520that%2520the%2520self-attention%2520mechanism%250A%2528SA%2529%2520is%2520an%2520effective%2520approach%2520for%2520designing%2520segmentation%2520networks%2520that%2520can%250Acapture%2520long-range%2520pixel%2520dependencies.%2520SA%2520enables%2520the%2520network%2520to%2520model%2520the%250Aglobal%2520dependencies%2520between%2520the%2520input%2520features%252C%2520resulting%2520in%2520improved%250Asegmentation%2520outcomes.%2520However%252C%2520the%2520high%2520density%2520of%2520attentional%2520feature%2520maps%250Aused%2520in%2520this%2520mechanism%2520causes%2520exponential%2520increases%2520in%2520computational%250Acomplexity.%2520Additionally%252C%2520it%2520introduces%2520redundant%2520information%2520that%2520negatively%250Aimpacts%2520the%2520feature%2520representation.%2520Inspired%2520by%2520traditional%2520threshold%250Asegmentation%2520algorithms%252C%2520we%2520propose%2520a%2520novel%2520threshold%2520attention%2520mechanism%250A%2528TAM%2529.%2520This%2520mechanism%2520significantly%2520reduces%2520computational%2520effort%2520while%2520also%250Abetter%2520modeling%2520the%2520correlation%2520between%2520different%2520regions%2520of%2520the%2520feature%2520map.%250ABased%2520on%2520TAM%252C%2520we%2520present%2520a%2520threshold%2520attention%2520network%2520%2528TANet%2529%2520for%2520semantic%250Asegmentation.%2520TANet%2520consists%2520of%2520an%2520attentional%2520feature%2520enhancement%2520module%250A%2528AFEM%2529%2520for%2520global%2520feature%2520enhancement%2520of%2520shallow%2520features%2520and%2520a%2520threshold%250Aattention%2520pyramid%2520pooling%2520module%2520%2528TAPP%2529%2520for%2520acquiring%2520feature%2520information%2520at%250Adifferent%2520scales%2520for%2520deep%2520features.%2520We%2520have%2520conducted%2520extensive%2520experiments%2520on%250Athe%2520ISPRS%2520Vaihingen%2520and%2520Potsdam%2520datasets.%2520The%2520results%2520demonstrate%2520the%2520validity%250Aand%2520superiority%2520of%2520our%2520proposed%2520TANet%2520compared%2520to%2520the%2520most%2520state-of-the-art%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Threshold%20Attention%20Network%20for%20Semantic%20Segmentation%20of%20Remote%20Sensing%0A%20%20Images&entry.906535625=Wei%20Long%20and%20Yongjun%20Zhang%20and%20Zhongwei%20Cui%20and%20Yujie%20Xu%20and%20Xuexue%20Zhang&entry.1292438233=%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20essential%20for%20various%0Aapplications%2C%20including%20vegetation%20monitoring%2C%20disaster%20management%2C%20and%20urban%0Aplanning.%20Previous%20studies%20have%20demonstrated%20that%20the%20self-attention%20mechanism%0A%28SA%29%20is%20an%20effective%20approach%20for%20designing%20segmentation%20networks%20that%20can%0Acapture%20long-range%20pixel%20dependencies.%20SA%20enables%20the%20network%20to%20model%20the%0Aglobal%20dependencies%20between%20the%20input%20features%2C%20resulting%20in%20improved%0Asegmentation%20outcomes.%20However%2C%20the%20high%20density%20of%20attentional%20feature%20maps%0Aused%20in%20this%20mechanism%20causes%20exponential%20increases%20in%20computational%0Acomplexity.%20Additionally%2C%20it%20introduces%20redundant%20information%20that%20negatively%0Aimpacts%20the%20feature%20representation.%20Inspired%20by%20traditional%20threshold%0Asegmentation%20algorithms%2C%20we%20propose%20a%20novel%20threshold%20attention%20mechanism%0A%28TAM%29.%20This%20mechanism%20significantly%20reduces%20computational%20effort%20while%20also%0Abetter%20modeling%20the%20correlation%20between%20different%20regions%20of%20the%20feature%20map.%0ABased%20on%20TAM%2C%20we%20present%20a%20threshold%20attention%20network%20%28TANet%29%20for%20semantic%0Asegmentation.%20TANet%20consists%20of%20an%20attentional%20feature%20enhancement%20module%0A%28AFEM%29%20for%20global%20feature%20enhancement%20of%20shallow%20features%20and%20a%20threshold%0Aattention%20pyramid%20pooling%20module%20%28TAPP%29%20for%20acquiring%20feature%20information%20at%0Adifferent%20scales%20for%20deep%20features.%20We%20have%20conducted%20extensive%20experiments%20on%0Athe%20ISPRS%20Vaihingen%20and%20Potsdam%20datasets.%20The%20results%20demonstrate%20the%20validity%0Aand%20superiority%20of%20our%20proposed%20TANet%20compared%20to%20the%20most%20state-of-the-art%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07984v1&entry.124074799=Read"},
{"title": "TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping", "author": "Despina Konstantinidou and Christos Koutlis and Symeon Papadopoulos", "abstract": "  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\nCode available at https : //github.com/mever-team/texture-crop.\n", "link": "http://arxiv.org/abs/2407.15500v3", "date": "2025-01-14", "relevancy": 2.7521, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5638}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.558}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping&body=Title%3A%20TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping%0AAuthor%3A%20Despina%20Konstantinidou%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20%20%20Generative%20AI%20technologies%20produce%20increasingly%20realistic%20imagery%2C%20which%2C%0Adespite%20its%20potential%20for%20creative%20applications%2C%20can%20also%20be%20misused%20to%20produce%0Amisleading%20and%20harmful%20content.%20This%20renders%20Synthetic%20Image%20Detection%20%28SID%29%0Amethods%20essential%20for%20identifying%20AI-generated%20content%20online.%20State-of-the-art%0ASID%20methods%20typically%20resize%20or%20center-crop%20input%20images%20due%20to%20architectural%0Aor%20computational%20constraints%2C%20which%20hampers%20the%20detection%20of%20artifacts%20that%0Aappear%20in%20high-resolution%20images.%20To%20address%20this%20limitation%2C%20we%20propose%0ATextureCrop%2C%20an%20image%20pre-processing%20component%20that%20can%20be%20plugged%20in%20any%0Apre-trained%20SID%20model%20to%20improve%20its%20performance.%20By%20focusing%20on%20high-frequency%0Aimage%20parts%20where%20generative%20artifacts%20are%20prevalent%2C%20TextureCrop%20enhances%20SID%0Aperformance%20with%20manageable%20memory%20requirements.%20Experimental%20results%0Ademonstrate%20a%20consistent%20improvement%20in%20AUC%20across%20various%20detectors%20by%206.1%25%0Acompared%20to%20center%20cropping%20and%20by%2015%25%20compared%20to%20resizing%2C%20across%0Ahigh-resolution%20images%20from%20the%20Forensynths%2C%20Synthbuster%20and%20TWIGMA%20datasets.%0ACode%20available%20at%20https%20%3A%20//github.com/mever-team/texture-crop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15500v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextureCrop%253A%2520Enhancing%2520Synthetic%2520Image%2520Detection%2520through%2520Texture-based%250A%2520%2520Cropping%26entry.906535625%3DDespina%2520Konstantinidou%2520and%2520Christos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3D%2520%2520Generative%2520AI%2520technologies%2520produce%2520increasingly%2520realistic%2520imagery%252C%2520which%252C%250Adespite%2520its%2520potential%2520for%2520creative%2520applications%252C%2520can%2520also%2520be%2520misused%2520to%2520produce%250Amisleading%2520and%2520harmful%2520content.%2520This%2520renders%2520Synthetic%2520Image%2520Detection%2520%2528SID%2529%250Amethods%2520essential%2520for%2520identifying%2520AI-generated%2520content%2520online.%2520State-of-the-art%250ASID%2520methods%2520typically%2520resize%2520or%2520center-crop%2520input%2520images%2520due%2520to%2520architectural%250Aor%2520computational%2520constraints%252C%2520which%2520hampers%2520the%2520detection%2520of%2520artifacts%2520that%250Aappear%2520in%2520high-resolution%2520images.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250ATextureCrop%252C%2520an%2520image%2520pre-processing%2520component%2520that%2520can%2520be%2520plugged%2520in%2520any%250Apre-trained%2520SID%2520model%2520to%2520improve%2520its%2520performance.%2520By%2520focusing%2520on%2520high-frequency%250Aimage%2520parts%2520where%2520generative%2520artifacts%2520are%2520prevalent%252C%2520TextureCrop%2520enhances%2520SID%250Aperformance%2520with%2520manageable%2520memory%2520requirements.%2520Experimental%2520results%250Ademonstrate%2520a%2520consistent%2520improvement%2520in%2520AUC%2520across%2520various%2520detectors%2520by%25206.1%2525%250Acompared%2520to%2520center%2520cropping%2520and%2520by%252015%2525%2520compared%2520to%2520resizing%252C%2520across%250Ahigh-resolution%2520images%2520from%2520the%2520Forensynths%252C%2520Synthbuster%2520and%2520TWIGMA%2520datasets.%250ACode%2520available%2520at%2520https%2520%253A%2520//github.com/mever-team/texture-crop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15500v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextureCrop%3A%20Enhancing%20Synthetic%20Image%20Detection%20through%20Texture-based%0A%20%20Cropping&entry.906535625=Despina%20Konstantinidou%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos&entry.1292438233=%20%20Generative%20AI%20technologies%20produce%20increasingly%20realistic%20imagery%2C%20which%2C%0Adespite%20its%20potential%20for%20creative%20applications%2C%20can%20also%20be%20misused%20to%20produce%0Amisleading%20and%20harmful%20content.%20This%20renders%20Synthetic%20Image%20Detection%20%28SID%29%0Amethods%20essential%20for%20identifying%20AI-generated%20content%20online.%20State-of-the-art%0ASID%20methods%20typically%20resize%20or%20center-crop%20input%20images%20due%20to%20architectural%0Aor%20computational%20constraints%2C%20which%20hampers%20the%20detection%20of%20artifacts%20that%0Aappear%20in%20high-resolution%20images.%20To%20address%20this%20limitation%2C%20we%20propose%0ATextureCrop%2C%20an%20image%20pre-processing%20component%20that%20can%20be%20plugged%20in%20any%0Apre-trained%20SID%20model%20to%20improve%20its%20performance.%20By%20focusing%20on%20high-frequency%0Aimage%20parts%20where%20generative%20artifacts%20are%20prevalent%2C%20TextureCrop%20enhances%20SID%0Aperformance%20with%20manageable%20memory%20requirements.%20Experimental%20results%0Ademonstrate%20a%20consistent%20improvement%20in%20AUC%20across%20various%20detectors%20by%206.1%25%0Acompared%20to%20center%20cropping%20and%20by%2015%25%20compared%20to%20resizing%2C%20across%0Ahigh-resolution%20images%20from%20the%20Forensynths%2C%20Synthbuster%20and%20TWIGMA%20datasets.%0ACode%20available%20at%20https%20%3A%20//github.com/mever-team/texture-crop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15500v3&entry.124074799=Read"},
{"title": "Omni-RGPT: Unifying Image and Video Region-level Understanding via Token\n  Marks", "author": "Miran Heo and Min-Hung Chen and De-An Huang and Sifei Liu and Subhashree Radhakrishnan and Seon Joo Kim and Yu-Chiang Frank Wang and Ryo Hachiuma", "abstract": "  We present Omni-RGPT, a multimodal large language model designed to\nfacilitate region-level comprehension for both images and videos. To achieve\nconsistent region representation across spatio-temporal dimensions, we\nintroduce Token Mark, a set of tokens highlighting the target regions within\nthe visual feature space. These tokens are directly embedded into spatial\nregions using region prompts (e.g., boxes or masks) and simultaneously\nincorporated into the text prompt to specify the target, establishing a direct\nconnection between visual and text tokens. To further support robust video\nunderstanding without requiring tracklets, we introduce an auxiliary task that\nguides Token Mark by leveraging the consistency of the tokens, enabling stable\nregion interpretation across the video. Additionally, we introduce a\nlarge-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT\nachieves state-of-the-art results on image and video-based commonsense\nreasoning benchmarks while showing strong performance in captioning and\nreferring expression comprehension tasks.\n", "link": "http://arxiv.org/abs/2501.08326v1", "date": "2025-01-14", "relevancy": 2.7444, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-RGPT%3A%20Unifying%20Image%20and%20Video%20Region-level%20Understanding%20via%20Token%0A%20%20Marks&body=Title%3A%20Omni-RGPT%3A%20Unifying%20Image%20and%20Video%20Region-level%20Understanding%20via%20Token%0A%20%20Marks%0AAuthor%3A%20Miran%20Heo%20and%20Min-Hung%20Chen%20and%20De-An%20Huang%20and%20Sifei%20Liu%20and%20Subhashree%20Radhakrishnan%20and%20Seon%20Joo%20Kim%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma%0AAbstract%3A%20%20%20We%20present%20Omni-RGPT%2C%20a%20multimodal%20large%20language%20model%20designed%20to%0Afacilitate%20region-level%20comprehension%20for%20both%20images%20and%20videos.%20To%20achieve%0Aconsistent%20region%20representation%20across%20spatio-temporal%20dimensions%2C%20we%0Aintroduce%20Token%20Mark%2C%20a%20set%20of%20tokens%20highlighting%20the%20target%20regions%20within%0Athe%20visual%20feature%20space.%20These%20tokens%20are%20directly%20embedded%20into%20spatial%0Aregions%20using%20region%20prompts%20%28e.g.%2C%20boxes%20or%20masks%29%20and%20simultaneously%0Aincorporated%20into%20the%20text%20prompt%20to%20specify%20the%20target%2C%20establishing%20a%20direct%0Aconnection%20between%20visual%20and%20text%20tokens.%20To%20further%20support%20robust%20video%0Aunderstanding%20without%20requiring%20tracklets%2C%20we%20introduce%20an%20auxiliary%20task%20that%0Aguides%20Token%20Mark%20by%20leveraging%20the%20consistency%20of%20the%20tokens%2C%20enabling%20stable%0Aregion%20interpretation%20across%20the%20video.%20Additionally%2C%20we%20introduce%20a%0Alarge-scale%20region-level%20video%20instruction%20dataset%20%28RegVID-300k%29.%20Omni-RGPT%0Aachieves%20state-of-the-art%20results%20on%20image%20and%20video-based%20commonsense%0Areasoning%20benchmarks%20while%20showing%20strong%20performance%20in%20captioning%20and%0Areferring%20expression%20comprehension%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-RGPT%253A%2520Unifying%2520Image%2520and%2520Video%2520Region-level%2520Understanding%2520via%2520Token%250A%2520%2520Marks%26entry.906535625%3DMiran%2520Heo%2520and%2520Min-Hung%2520Chen%2520and%2520De-An%2520Huang%2520and%2520Sifei%2520Liu%2520and%2520Subhashree%2520Radhakrishnan%2520and%2520Seon%2520Joo%2520Kim%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Ryo%2520Hachiuma%26entry.1292438233%3D%2520%2520We%2520present%2520Omni-RGPT%252C%2520a%2520multimodal%2520large%2520language%2520model%2520designed%2520to%250Afacilitate%2520region-level%2520comprehension%2520for%2520both%2520images%2520and%2520videos.%2520To%2520achieve%250Aconsistent%2520region%2520representation%2520across%2520spatio-temporal%2520dimensions%252C%2520we%250Aintroduce%2520Token%2520Mark%252C%2520a%2520set%2520of%2520tokens%2520highlighting%2520the%2520target%2520regions%2520within%250Athe%2520visual%2520feature%2520space.%2520These%2520tokens%2520are%2520directly%2520embedded%2520into%2520spatial%250Aregions%2520using%2520region%2520prompts%2520%2528e.g.%252C%2520boxes%2520or%2520masks%2529%2520and%2520simultaneously%250Aincorporated%2520into%2520the%2520text%2520prompt%2520to%2520specify%2520the%2520target%252C%2520establishing%2520a%2520direct%250Aconnection%2520between%2520visual%2520and%2520text%2520tokens.%2520To%2520further%2520support%2520robust%2520video%250Aunderstanding%2520without%2520requiring%2520tracklets%252C%2520we%2520introduce%2520an%2520auxiliary%2520task%2520that%250Aguides%2520Token%2520Mark%2520by%2520leveraging%2520the%2520consistency%2520of%2520the%2520tokens%252C%2520enabling%2520stable%250Aregion%2520interpretation%2520across%2520the%2520video.%2520Additionally%252C%2520we%2520introduce%2520a%250Alarge-scale%2520region-level%2520video%2520instruction%2520dataset%2520%2528RegVID-300k%2529.%2520Omni-RGPT%250Aachieves%2520state-of-the-art%2520results%2520on%2520image%2520and%2520video-based%2520commonsense%250Areasoning%2520benchmarks%2520while%2520showing%2520strong%2520performance%2520in%2520captioning%2520and%250Areferring%2520expression%2520comprehension%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-RGPT%3A%20Unifying%20Image%20and%20Video%20Region-level%20Understanding%20via%20Token%0A%20%20Marks&entry.906535625=Miran%20Heo%20and%20Min-Hung%20Chen%20and%20De-An%20Huang%20and%20Sifei%20Liu%20and%20Subhashree%20Radhakrishnan%20and%20Seon%20Joo%20Kim%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma&entry.1292438233=%20%20We%20present%20Omni-RGPT%2C%20a%20multimodal%20large%20language%20model%20designed%20to%0Afacilitate%20region-level%20comprehension%20for%20both%20images%20and%20videos.%20To%20achieve%0Aconsistent%20region%20representation%20across%20spatio-temporal%20dimensions%2C%20we%0Aintroduce%20Token%20Mark%2C%20a%20set%20of%20tokens%20highlighting%20the%20target%20regions%20within%0Athe%20visual%20feature%20space.%20These%20tokens%20are%20directly%20embedded%20into%20spatial%0Aregions%20using%20region%20prompts%20%28e.g.%2C%20boxes%20or%20masks%29%20and%20simultaneously%0Aincorporated%20into%20the%20text%20prompt%20to%20specify%20the%20target%2C%20establishing%20a%20direct%0Aconnection%20between%20visual%20and%20text%20tokens.%20To%20further%20support%20robust%20video%0Aunderstanding%20without%20requiring%20tracklets%2C%20we%20introduce%20an%20auxiliary%20task%20that%0Aguides%20Token%20Mark%20by%20leveraging%20the%20consistency%20of%20the%20tokens%2C%20enabling%20stable%0Aregion%20interpretation%20across%20the%20video.%20Additionally%2C%20we%20introduce%20a%0Alarge-scale%20region-level%20video%20instruction%20dataset%20%28RegVID-300k%29.%20Omni-RGPT%0Aachieves%20state-of-the-art%20results%20on%20image%20and%20video-based%20commonsense%0Areasoning%20benchmarks%20while%20showing%20strong%20performance%20in%20captioning%20and%0Areferring%20expression%20comprehension%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08326v1&entry.124074799=Read"},
{"title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs", "author": "Jiacheng Zhang and Yang Jiao and Shaoxiang Chen and Na Zhao and Jingjing Chen", "abstract": "  Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.\n", "link": "http://arxiv.org/abs/2409.16597v3", "date": "2025-01-14", "relevancy": 2.7251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventHallusion%3A%20Diagnosing%20Event%20Hallucinations%20in%20Video%20LLMs&body=Title%3A%20EventHallusion%3A%20Diagnosing%20Event%20Hallucinations%20in%20Video%20LLMs%0AAuthor%3A%20Jiacheng%20Zhang%20and%20Yang%20Jiao%20and%20Shaoxiang%20Chen%20and%20Na%20Zhao%20and%20Jingjing%20Chen%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%0Aprogress%20in%20the%20video%20comprehension%20field.%20Despite%20remarkable%20content%20reasoning%0Aand%20instruction%20following%20capabilities%20they%20demonstrated%2C%20the%20hallucination%0Aproblem%20of%20these%20VideoLLMs%20is%20less%20explored%20compared%20with%20its%20counterpart%20in%0Athe%20image%20domain.%20To%20mitigate%20this%20gap%2C%20we%20propose%20EventHallusion%2C%20a%20novel%0Abenchmark%20that%20focuses%20on%20assessing%20the%20VideoLLMs%27%20hallucination%20toward%20event%2C%0Athe%20crux%20of%20video%20analysis.%20From%20a%20hallucination%20attribution%20perspective%2C%20our%0AEventHallusion%20benchmark%20is%20curated%20to%20assess%20a%20VideoLLM%27s%20susceptibility%0Atoward%20language%20priors%20and%20vision-language%20biases.%20On%20the%20other%20hand%2C%20we%20also%0Apropose%20a%20simple%20yet%20effective%20method%2C%20called%20Temporal%20Contrastive%20Decoding%0A%28TCD%29%2C%20to%20tackle%20the%20hallucination%20problems%20of%20VideoLLMs.%20The%20proposed%20TCD%0Amethod%20rectifies%20the%20model%27s%20bias%20toward%20its%20priors%20during%20the%20decoding%20stage%0Aby%20comparing%20the%20original%20video%20with%20a%20modified%20version%2C%20in%20which%20temporal%20cues%0Aare%20disrupted.%20Through%20comprehensive%20evaluation%20of%20eight%20open-source%20and%20two%0Aclosed-source%20VideoLLMs%20on%20the%20proposed%20EventHallusion%20benchmark%2C%20we%20observe%0Athat%20the%20open-source%20models%20suffer%20significantly%20from%20hallucination%20problems%2C%0Awhereas%20the%20closed-source%20ones%20perform%20markedly%20better.%20By%20further%20equipping%0Aopen-source%20VideoLLMs%20with%20the%20proposed%20TCD%20approach%2C%20evident%20performance%0Aimprovements%20are%20achieved%20across%20most%20metrics%20in%20the%20EventHallusion%20benchmark.%0AOur%20codes%20and%20benchmark%20data%20are%20available%20at%0Ahttps%3A//github.com/Stevetich/EventHallusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16597v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventHallusion%253A%2520Diagnosing%2520Event%2520Hallucinations%2520in%2520Video%2520LLMs%26entry.906535625%3DJiacheng%2520Zhang%2520and%2520Yang%2520Jiao%2520and%2520Shaoxiang%2520Chen%2520and%2520Na%2520Zhao%2520and%2520Jingjing%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520made%2520significant%250Aprogress%2520in%2520the%2520video%2520comprehension%2520field.%2520Despite%2520remarkable%2520content%2520reasoning%250Aand%2520instruction%2520following%2520capabilities%2520they%2520demonstrated%252C%2520the%2520hallucination%250Aproblem%2520of%2520these%2520VideoLLMs%2520is%2520less%2520explored%2520compared%2520with%2520its%2520counterpart%2520in%250Athe%2520image%2520domain.%2520To%2520mitigate%2520this%2520gap%252C%2520we%2520propose%2520EventHallusion%252C%2520a%2520novel%250Abenchmark%2520that%2520focuses%2520on%2520assessing%2520the%2520VideoLLMs%2527%2520hallucination%2520toward%2520event%252C%250Athe%2520crux%2520of%2520video%2520analysis.%2520From%2520a%2520hallucination%2520attribution%2520perspective%252C%2520our%250AEventHallusion%2520benchmark%2520is%2520curated%2520to%2520assess%2520a%2520VideoLLM%2527s%2520susceptibility%250Atoward%2520language%2520priors%2520and%2520vision-language%2520biases.%2520On%2520the%2520other%2520hand%252C%2520we%2520also%250Apropose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520called%2520Temporal%2520Contrastive%2520Decoding%250A%2528TCD%2529%252C%2520to%2520tackle%2520the%2520hallucination%2520problems%2520of%2520VideoLLMs.%2520The%2520proposed%2520TCD%250Amethod%2520rectifies%2520the%2520model%2527s%2520bias%2520toward%2520its%2520priors%2520during%2520the%2520decoding%2520stage%250Aby%2520comparing%2520the%2520original%2520video%2520with%2520a%2520modified%2520version%252C%2520in%2520which%2520temporal%2520cues%250Aare%2520disrupted.%2520Through%2520comprehensive%2520evaluation%2520of%2520eight%2520open-source%2520and%2520two%250Aclosed-source%2520VideoLLMs%2520on%2520the%2520proposed%2520EventHallusion%2520benchmark%252C%2520we%2520observe%250Athat%2520the%2520open-source%2520models%2520suffer%2520significantly%2520from%2520hallucination%2520problems%252C%250Awhereas%2520the%2520closed-source%2520ones%2520perform%2520markedly%2520better.%2520By%2520further%2520equipping%250Aopen-source%2520VideoLLMs%2520with%2520the%2520proposed%2520TCD%2520approach%252C%2520evident%2520performance%250Aimprovements%2520are%2520achieved%2520across%2520most%2520metrics%2520in%2520the%2520EventHallusion%2520benchmark.%250AOur%2520codes%2520and%2520benchmark%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Stevetich/EventHallusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16597v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventHallusion%3A%20Diagnosing%20Event%20Hallucinations%20in%20Video%20LLMs&entry.906535625=Jiacheng%20Zhang%20and%20Yang%20Jiao%20and%20Shaoxiang%20Chen%20and%20Na%20Zhao%20and%20Jingjing%20Chen&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20significant%0Aprogress%20in%20the%20video%20comprehension%20field.%20Despite%20remarkable%20content%20reasoning%0Aand%20instruction%20following%20capabilities%20they%20demonstrated%2C%20the%20hallucination%0Aproblem%20of%20these%20VideoLLMs%20is%20less%20explored%20compared%20with%20its%20counterpart%20in%0Athe%20image%20domain.%20To%20mitigate%20this%20gap%2C%20we%20propose%20EventHallusion%2C%20a%20novel%0Abenchmark%20that%20focuses%20on%20assessing%20the%20VideoLLMs%27%20hallucination%20toward%20event%2C%0Athe%20crux%20of%20video%20analysis.%20From%20a%20hallucination%20attribution%20perspective%2C%20our%0AEventHallusion%20benchmark%20is%20curated%20to%20assess%20a%20VideoLLM%27s%20susceptibility%0Atoward%20language%20priors%20and%20vision-language%20biases.%20On%20the%20other%20hand%2C%20we%20also%0Apropose%20a%20simple%20yet%20effective%20method%2C%20called%20Temporal%20Contrastive%20Decoding%0A%28TCD%29%2C%20to%20tackle%20the%20hallucination%20problems%20of%20VideoLLMs.%20The%20proposed%20TCD%0Amethod%20rectifies%20the%20model%27s%20bias%20toward%20its%20priors%20during%20the%20decoding%20stage%0Aby%20comparing%20the%20original%20video%20with%20a%20modified%20version%2C%20in%20which%20temporal%20cues%0Aare%20disrupted.%20Through%20comprehensive%20evaluation%20of%20eight%20open-source%20and%20two%0Aclosed-source%20VideoLLMs%20on%20the%20proposed%20EventHallusion%20benchmark%2C%20we%20observe%0Athat%20the%20open-source%20models%20suffer%20significantly%20from%20hallucination%20problems%2C%0Awhereas%20the%20closed-source%20ones%20perform%20markedly%20better.%20By%20further%20equipping%0Aopen-source%20VideoLLMs%20with%20the%20proposed%20TCD%20approach%2C%20evident%20performance%0Aimprovements%20are%20achieved%20across%20most%20metrics%20in%20the%20EventHallusion%20benchmark.%0AOur%20codes%20and%20benchmark%20data%20are%20available%20at%0Ahttps%3A//github.com/Stevetich/EventHallusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16597v3&entry.124074799=Read"},
{"title": "Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval", "author": "Delong Liu and Haiwen Li and Zhicheng Zhao and Yuan Dong and Nikolaos V. Boulgouris", "abstract": "  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n", "link": "http://arxiv.org/abs/2307.09059v3", "date": "2025-01-14", "relevancy": 2.7208, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5285}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval&body=Title%3A%20Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval%0AAuthor%3A%20Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Yuan%20Dong%20and%20Nikolaos%20V.%20Boulgouris%0AAbstract%3A%20%20%20The%20goal%20of%20Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20is%20to%20retrieve%20specific%0Aperson%20images%20according%20to%20the%20given%20textual%20descriptions.%20A%20primary%20challenge%0Ain%20this%20task%20is%20bridging%20the%20substantial%20representational%20gap%20between%20visual%0Aand%20textual%20modalities.%20The%20prevailing%20methods%20map%20texts%20and%20images%20into%0Aunified%20embedding%20space%20for%20matching%2C%20while%20the%20intricate%20semantic%0Acorrespondences%20between%20texts%20and%20images%20are%20still%20not%20effectively%20constructed.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20TIPR%20framework%20to%20build%20fine-grained%0Ainteractions%20and%20alignment%20between%20person%20images%20and%20the%20corresponding%20texts.%0ASpecifically%2C%20via%20fine-tuning%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%2C%20a%20visual-textual%20dual%20encoder%20is%20firstly%20constructed%2C%20to%0Apreliminarily%20align%20the%20image%20and%20text%20features.%20Secondly%2C%20a%20Text-guided%20Image%0ARestoration%20%28TIR%29%20auxiliary%20task%20is%20proposed%20to%20map%20abstract%20textual%20entities%0Ato%20specific%20image%20regions%2C%20improving%20the%20alignment%20between%20local%20textual%20and%0Avisual%20embeddings.%20Additionally%2C%20a%20cross-modal%20triplet%20loss%20is%20presented%20to%0Ahandle%20hard%20samples%2C%20and%20further%20enhance%20the%20model%27s%20discriminability%20for%20minor%0Adifferences.%20Moreover%2C%20a%20pruning-based%20text%20data%20augmentation%20approach%20is%0Aproposed%20to%20enhance%20focus%20on%20essential%20elements%20in%20descriptions%2C%20thereby%0Aavoiding%20excessive%20model%20attention%20to%20less%20significant%20information.%20The%0Aexperimental%20results%20show%20our%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20three%20popular%20benchmark%20datasets%2C%20and%20the%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/Delong-liu-bupt/SEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09059v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%2520Image%2520Restoration%2520and%2520Semantic%2520Enhancement%2520for%2520Text-to-Image%250A%2520%2520Person%2520Retrieval%26entry.906535625%3DDelong%2520Liu%2520and%2520Haiwen%2520Li%2520and%2520Zhicheng%2520Zhao%2520and%2520Yuan%2520Dong%2520and%2520Nikolaos%2520V.%2520Boulgouris%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520Text-to-Image%2520Person%2520Retrieval%2520%2528TIPR%2529%2520is%2520to%2520retrieve%2520specific%250Aperson%2520images%2520according%2520to%2520the%2520given%2520textual%2520descriptions.%2520A%2520primary%2520challenge%250Ain%2520this%2520task%2520is%2520bridging%2520the%2520substantial%2520representational%2520gap%2520between%2520visual%250Aand%2520textual%2520modalities.%2520The%2520prevailing%2520methods%2520map%2520texts%2520and%2520images%2520into%250Aunified%2520embedding%2520space%2520for%2520matching%252C%2520while%2520the%2520intricate%2520semantic%250Acorrespondences%2520between%2520texts%2520and%2520images%2520are%2520still%2520not%2520effectively%2520constructed.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520TIPR%2520framework%2520to%2520build%2520fine-grained%250Ainteractions%2520and%2520alignment%2520between%2520person%2520images%2520and%2520the%2520corresponding%2520texts.%250ASpecifically%252C%2520via%2520fine-tuning%2520the%2520Contrastive%2520Language-Image%2520Pre-training%250A%2528CLIP%2529%2520model%252C%2520a%2520visual-textual%2520dual%2520encoder%2520is%2520firstly%2520constructed%252C%2520to%250Apreliminarily%2520align%2520the%2520image%2520and%2520text%2520features.%2520Secondly%252C%2520a%2520Text-guided%2520Image%250ARestoration%2520%2528TIR%2529%2520auxiliary%2520task%2520is%2520proposed%2520to%2520map%2520abstract%2520textual%2520entities%250Ato%2520specific%2520image%2520regions%252C%2520improving%2520the%2520alignment%2520between%2520local%2520textual%2520and%250Avisual%2520embeddings.%2520Additionally%252C%2520a%2520cross-modal%2520triplet%2520loss%2520is%2520presented%2520to%250Ahandle%2520hard%2520samples%252C%2520and%2520further%2520enhance%2520the%2520model%2527s%2520discriminability%2520for%2520minor%250Adifferences.%2520Moreover%252C%2520a%2520pruning-based%2520text%2520data%2520augmentation%2520approach%2520is%250Aproposed%2520to%2520enhance%2520focus%2520on%2520essential%2520elements%2520in%2520descriptions%252C%2520thereby%250Aavoiding%2520excessive%2520model%2520attention%2520to%2520less%2520significant%2520information.%2520The%250Aexperimental%2520results%2520show%2520our%2520proposed%2520method%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520three%2520popular%2520benchmark%2520datasets%252C%2520and%2520the%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Delong-liu-bupt/SEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09059v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%20Image%20Restoration%20and%20Semantic%20Enhancement%20for%20Text-to-Image%0A%20%20Person%20Retrieval&entry.906535625=Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhicheng%20Zhao%20and%20Yuan%20Dong%20and%20Nikolaos%20V.%20Boulgouris&entry.1292438233=%20%20The%20goal%20of%20Text-to-Image%20Person%20Retrieval%20%28TIPR%29%20is%20to%20retrieve%20specific%0Aperson%20images%20according%20to%20the%20given%20textual%20descriptions.%20A%20primary%20challenge%0Ain%20this%20task%20is%20bridging%20the%20substantial%20representational%20gap%20between%20visual%0Aand%20textual%20modalities.%20The%20prevailing%20methods%20map%20texts%20and%20images%20into%0Aunified%20embedding%20space%20for%20matching%2C%20while%20the%20intricate%20semantic%0Acorrespondences%20between%20texts%20and%20images%20are%20still%20not%20effectively%20constructed.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20TIPR%20framework%20to%20build%20fine-grained%0Ainteractions%20and%20alignment%20between%20person%20images%20and%20the%20corresponding%20texts.%0ASpecifically%2C%20via%20fine-tuning%20the%20Contrastive%20Language-Image%20Pre-training%0A%28CLIP%29%20model%2C%20a%20visual-textual%20dual%20encoder%20is%20firstly%20constructed%2C%20to%0Apreliminarily%20align%20the%20image%20and%20text%20features.%20Secondly%2C%20a%20Text-guided%20Image%0ARestoration%20%28TIR%29%20auxiliary%20task%20is%20proposed%20to%20map%20abstract%20textual%20entities%0Ato%20specific%20image%20regions%2C%20improving%20the%20alignment%20between%20local%20textual%20and%0Avisual%20embeddings.%20Additionally%2C%20a%20cross-modal%20triplet%20loss%20is%20presented%20to%0Ahandle%20hard%20samples%2C%20and%20further%20enhance%20the%20model%27s%20discriminability%20for%20minor%0Adifferences.%20Moreover%2C%20a%20pruning-based%20text%20data%20augmentation%20approach%20is%0Aproposed%20to%20enhance%20focus%20on%20essential%20elements%20in%20descriptions%2C%20thereby%0Aavoiding%20excessive%20model%20attention%20to%20less%20significant%20information.%20The%0Aexperimental%20results%20show%20our%20proposed%20method%20outperforms%20state-of-the-art%0Amethods%20on%20three%20popular%20benchmark%20datasets%2C%20and%20the%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/Delong-liu-bupt/SEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09059v3&entry.124074799=Read"},
{"title": "DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction", "author": "Yucong Meng and Zhiwei Yang and Zhijian Song and Yonghong Shi", "abstract": "  The accelerated MRI reconstruction poses a challenging ill-posed inverse\nproblem due to the significant undersampling in k-space. Deep neural networks,\nsuch as CNNs and ViT, have shown substantial performance improvements for this\ntask while encountering the dilemma between global receptive fields and\nefficient computation. To this end, this paper pioneers exploring Mamba, a new\nparadigm for long-range dependency modeling with linear complexity, for\nefficient and effective MRI reconstruction. However, directly applying Mamba to\nMRI reconstruction faces three significant issues: (1) Mamba's row-wise and\ncolumn-wise scanning disrupts k-space's unique spectrum, leaving its potential\nin k-space learning unexplored. (2) Existing Mamba methods unfold feature maps\nwith multiple lengthy scanning paths, leading to long-range forgetting and high\ncomputational burden. (3) Mamba struggles with spatially-varying contents,\nresulting in limited diversity of local representations. To address these, we\npropose a dual-domain multi-scale Mamba for MRI reconstruction from the\nfollowing perspectives: (1) We pioneer vision Mamba in k-space learning. A\ncircular scanning is customized for spectrum unfolding, benefiting the global\nmodeling of k-space. (2) We propose a multi-scale Mamba with an efficient\nscanning strategy in both image and k-space domains. It mitigates long-range\nforgetting and achieves a better trade-off between efficiency and performance.\n(3) We develop a local diversity enhancement module to improve the\nspatially-varying representation of Mamba. Extensive experiments are conducted\non three public datasets for MRI reconstruction under various undersampling\npatterns. Comprehensive results demonstrate that our method significantly\noutperforms state-of-the-art methods with lower computational cost.\nImplementation code will be available at\nhttps://github.com/XiaoMengLiLiLi/DM-Mamba.\n", "link": "http://arxiv.org/abs/2501.08163v1", "date": "2025-01-14", "relevancy": 2.6937, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5356}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DM-Mamba%3A%20Dual-domain%20Multi-scale%20Mamba%20for%20MRI%20reconstruction&body=Title%3A%20DM-Mamba%3A%20Dual-domain%20Multi-scale%20Mamba%20for%20MRI%20reconstruction%0AAuthor%3A%20Yucong%20Meng%20and%20Zhiwei%20Yang%20and%20Zhijian%20Song%20and%20Yonghong%20Shi%0AAbstract%3A%20%20%20The%20accelerated%20MRI%20reconstruction%20poses%20a%20challenging%20ill-posed%20inverse%0Aproblem%20due%20to%20the%20significant%20undersampling%20in%20k-space.%20Deep%20neural%20networks%2C%0Asuch%20as%20CNNs%20and%20ViT%2C%20have%20shown%20substantial%20performance%20improvements%20for%20this%0Atask%20while%20encountering%20the%20dilemma%20between%20global%20receptive%20fields%20and%0Aefficient%20computation.%20To%20this%20end%2C%20this%20paper%20pioneers%20exploring%20Mamba%2C%20a%20new%0Aparadigm%20for%20long-range%20dependency%20modeling%20with%20linear%20complexity%2C%20for%0Aefficient%20and%20effective%20MRI%20reconstruction.%20However%2C%20directly%20applying%20Mamba%20to%0AMRI%20reconstruction%20faces%20three%20significant%20issues%3A%20%281%29%20Mamba%27s%20row-wise%20and%0Acolumn-wise%20scanning%20disrupts%20k-space%27s%20unique%20spectrum%2C%20leaving%20its%20potential%0Ain%20k-space%20learning%20unexplored.%20%282%29%20Existing%20Mamba%20methods%20unfold%20feature%20maps%0Awith%20multiple%20lengthy%20scanning%20paths%2C%20leading%20to%20long-range%20forgetting%20and%20high%0Acomputational%20burden.%20%283%29%20Mamba%20struggles%20with%20spatially-varying%20contents%2C%0Aresulting%20in%20limited%20diversity%20of%20local%20representations.%20To%20address%20these%2C%20we%0Apropose%20a%20dual-domain%20multi-scale%20Mamba%20for%20MRI%20reconstruction%20from%20the%0Afollowing%20perspectives%3A%20%281%29%20We%20pioneer%20vision%20Mamba%20in%20k-space%20learning.%20A%0Acircular%20scanning%20is%20customized%20for%20spectrum%20unfolding%2C%20benefiting%20the%20global%0Amodeling%20of%20k-space.%20%282%29%20We%20propose%20a%20multi-scale%20Mamba%20with%20an%20efficient%0Ascanning%20strategy%20in%20both%20image%20and%20k-space%20domains.%20It%20mitigates%20long-range%0Aforgetting%20and%20achieves%20a%20better%20trade-off%20between%20efficiency%20and%20performance.%0A%283%29%20We%20develop%20a%20local%20diversity%20enhancement%20module%20to%20improve%20the%0Aspatially-varying%20representation%20of%20Mamba.%20Extensive%20experiments%20are%20conducted%0Aon%20three%20public%20datasets%20for%20MRI%20reconstruction%20under%20various%20undersampling%0Apatterns.%20Comprehensive%20results%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20state-of-the-art%20methods%20with%20lower%20computational%20cost.%0AImplementation%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/XiaoMengLiLiLi/DM-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDM-Mamba%253A%2520Dual-domain%2520Multi-scale%2520Mamba%2520for%2520MRI%2520reconstruction%26entry.906535625%3DYucong%2520Meng%2520and%2520Zhiwei%2520Yang%2520and%2520Zhijian%2520Song%2520and%2520Yonghong%2520Shi%26entry.1292438233%3D%2520%2520The%2520accelerated%2520MRI%2520reconstruction%2520poses%2520a%2520challenging%2520ill-posed%2520inverse%250Aproblem%2520due%2520to%2520the%2520significant%2520undersampling%2520in%2520k-space.%2520Deep%2520neural%2520networks%252C%250Asuch%2520as%2520CNNs%2520and%2520ViT%252C%2520have%2520shown%2520substantial%2520performance%2520improvements%2520for%2520this%250Atask%2520while%2520encountering%2520the%2520dilemma%2520between%2520global%2520receptive%2520fields%2520and%250Aefficient%2520computation.%2520To%2520this%2520end%252C%2520this%2520paper%2520pioneers%2520exploring%2520Mamba%252C%2520a%2520new%250Aparadigm%2520for%2520long-range%2520dependency%2520modeling%2520with%2520linear%2520complexity%252C%2520for%250Aefficient%2520and%2520effective%2520MRI%2520reconstruction.%2520However%252C%2520directly%2520applying%2520Mamba%2520to%250AMRI%2520reconstruction%2520faces%2520three%2520significant%2520issues%253A%2520%25281%2529%2520Mamba%2527s%2520row-wise%2520and%250Acolumn-wise%2520scanning%2520disrupts%2520k-space%2527s%2520unique%2520spectrum%252C%2520leaving%2520its%2520potential%250Ain%2520k-space%2520learning%2520unexplored.%2520%25282%2529%2520Existing%2520Mamba%2520methods%2520unfold%2520feature%2520maps%250Awith%2520multiple%2520lengthy%2520scanning%2520paths%252C%2520leading%2520to%2520long-range%2520forgetting%2520and%2520high%250Acomputational%2520burden.%2520%25283%2529%2520Mamba%2520struggles%2520with%2520spatially-varying%2520contents%252C%250Aresulting%2520in%2520limited%2520diversity%2520of%2520local%2520representations.%2520To%2520address%2520these%252C%2520we%250Apropose%2520a%2520dual-domain%2520multi-scale%2520Mamba%2520for%2520MRI%2520reconstruction%2520from%2520the%250Afollowing%2520perspectives%253A%2520%25281%2529%2520We%2520pioneer%2520vision%2520Mamba%2520in%2520k-space%2520learning.%2520A%250Acircular%2520scanning%2520is%2520customized%2520for%2520spectrum%2520unfolding%252C%2520benefiting%2520the%2520global%250Amodeling%2520of%2520k-space.%2520%25282%2529%2520We%2520propose%2520a%2520multi-scale%2520Mamba%2520with%2520an%2520efficient%250Ascanning%2520strategy%2520in%2520both%2520image%2520and%2520k-space%2520domains.%2520It%2520mitigates%2520long-range%250Aforgetting%2520and%2520achieves%2520a%2520better%2520trade-off%2520between%2520efficiency%2520and%2520performance.%250A%25283%2529%2520We%2520develop%2520a%2520local%2520diversity%2520enhancement%2520module%2520to%2520improve%2520the%250Aspatially-varying%2520representation%2520of%2520Mamba.%2520Extensive%2520experiments%2520are%2520conducted%250Aon%2520three%2520public%2520datasets%2520for%2520MRI%2520reconstruction%2520under%2520various%2520undersampling%250Apatterns.%2520Comprehensive%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520state-of-the-art%2520methods%2520with%2520lower%2520computational%2520cost.%250AImplementation%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/XiaoMengLiLiLi/DM-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DM-Mamba%3A%20Dual-domain%20Multi-scale%20Mamba%20for%20MRI%20reconstruction&entry.906535625=Yucong%20Meng%20and%20Zhiwei%20Yang%20and%20Zhijian%20Song%20and%20Yonghong%20Shi&entry.1292438233=%20%20The%20accelerated%20MRI%20reconstruction%20poses%20a%20challenging%20ill-posed%20inverse%0Aproblem%20due%20to%20the%20significant%20undersampling%20in%20k-space.%20Deep%20neural%20networks%2C%0Asuch%20as%20CNNs%20and%20ViT%2C%20have%20shown%20substantial%20performance%20improvements%20for%20this%0Atask%20while%20encountering%20the%20dilemma%20between%20global%20receptive%20fields%20and%0Aefficient%20computation.%20To%20this%20end%2C%20this%20paper%20pioneers%20exploring%20Mamba%2C%20a%20new%0Aparadigm%20for%20long-range%20dependency%20modeling%20with%20linear%20complexity%2C%20for%0Aefficient%20and%20effective%20MRI%20reconstruction.%20However%2C%20directly%20applying%20Mamba%20to%0AMRI%20reconstruction%20faces%20three%20significant%20issues%3A%20%281%29%20Mamba%27s%20row-wise%20and%0Acolumn-wise%20scanning%20disrupts%20k-space%27s%20unique%20spectrum%2C%20leaving%20its%20potential%0Ain%20k-space%20learning%20unexplored.%20%282%29%20Existing%20Mamba%20methods%20unfold%20feature%20maps%0Awith%20multiple%20lengthy%20scanning%20paths%2C%20leading%20to%20long-range%20forgetting%20and%20high%0Acomputational%20burden.%20%283%29%20Mamba%20struggles%20with%20spatially-varying%20contents%2C%0Aresulting%20in%20limited%20diversity%20of%20local%20representations.%20To%20address%20these%2C%20we%0Apropose%20a%20dual-domain%20multi-scale%20Mamba%20for%20MRI%20reconstruction%20from%20the%0Afollowing%20perspectives%3A%20%281%29%20We%20pioneer%20vision%20Mamba%20in%20k-space%20learning.%20A%0Acircular%20scanning%20is%20customized%20for%20spectrum%20unfolding%2C%20benefiting%20the%20global%0Amodeling%20of%20k-space.%20%282%29%20We%20propose%20a%20multi-scale%20Mamba%20with%20an%20efficient%0Ascanning%20strategy%20in%20both%20image%20and%20k-space%20domains.%20It%20mitigates%20long-range%0Aforgetting%20and%20achieves%20a%20better%20trade-off%20between%20efficiency%20and%20performance.%0A%283%29%20We%20develop%20a%20local%20diversity%20enhancement%20module%20to%20improve%20the%0Aspatially-varying%20representation%20of%20Mamba.%20Extensive%20experiments%20are%20conducted%0Aon%20three%20public%20datasets%20for%20MRI%20reconstruction%20under%20various%20undersampling%0Apatterns.%20Comprehensive%20results%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20state-of-the-art%20methods%20with%20lower%20computational%20cost.%0AImplementation%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/XiaoMengLiLiLi/DM-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08163v1&entry.124074799=Read"},
{"title": "AgentPose: Progressive Distribution Alignment via Feature Agent for\n  Human Pose Distillation", "author": "Feng Zhang and Jinwei Liu and Xiatian Zhu and Lei Chen", "abstract": "  Pose distillation is widely adopted to reduce model size in human pose\nestimation. However, existing methods primarily emphasize the transfer of\nteacher knowledge while often neglecting the performance degradation resulted\nfrom the curse of capacity gap between teacher and student. To address this\nissue, we propose AgentPose, a novel pose distillation method that integrates a\nfeature agent to model the distribution of teacher features and progressively\naligns the distribution of student features with that of the teacher feature,\neffectively overcoming the capacity gap and enhancing the ability of knowledge\ntransfer. Our comprehensive experiments conducted on the COCO dataset\nsubstantiate the effectiveness of our method in knowledge transfer,\nparticularly in scenarios with a high capacity gap.\n", "link": "http://arxiv.org/abs/2501.08088v1", "date": "2025-01-14", "relevancy": 2.6915, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5625}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5351}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentPose%3A%20Progressive%20Distribution%20Alignment%20via%20Feature%20Agent%20for%0A%20%20Human%20Pose%20Distillation&body=Title%3A%20AgentPose%3A%20Progressive%20Distribution%20Alignment%20via%20Feature%20Agent%20for%0A%20%20Human%20Pose%20Distillation%0AAuthor%3A%20Feng%20Zhang%20and%20Jinwei%20Liu%20and%20Xiatian%20Zhu%20and%20Lei%20Chen%0AAbstract%3A%20%20%20Pose%20distillation%20is%20widely%20adopted%20to%20reduce%20model%20size%20in%20human%20pose%0Aestimation.%20However%2C%20existing%20methods%20primarily%20emphasize%20the%20transfer%20of%0Ateacher%20knowledge%20while%20often%20neglecting%20the%20performance%20degradation%20resulted%0Afrom%20the%20curse%20of%20capacity%20gap%20between%20teacher%20and%20student.%20To%20address%20this%0Aissue%2C%20we%20propose%20AgentPose%2C%20a%20novel%20pose%20distillation%20method%20that%20integrates%20a%0Afeature%20agent%20to%20model%20the%20distribution%20of%20teacher%20features%20and%20progressively%0Aaligns%20the%20distribution%20of%20student%20features%20with%20that%20of%20the%20teacher%20feature%2C%0Aeffectively%20overcoming%20the%20capacity%20gap%20and%20enhancing%20the%20ability%20of%20knowledge%0Atransfer.%20Our%20comprehensive%20experiments%20conducted%20on%20the%20COCO%20dataset%0Asubstantiate%20the%20effectiveness%20of%20our%20method%20in%20knowledge%20transfer%2C%0Aparticularly%20in%20scenarios%20with%20a%20high%20capacity%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentPose%253A%2520Progressive%2520Distribution%2520Alignment%2520via%2520Feature%2520Agent%2520for%250A%2520%2520Human%2520Pose%2520Distillation%26entry.906535625%3DFeng%2520Zhang%2520and%2520Jinwei%2520Liu%2520and%2520Xiatian%2520Zhu%2520and%2520Lei%2520Chen%26entry.1292438233%3D%2520%2520Pose%2520distillation%2520is%2520widely%2520adopted%2520to%2520reduce%2520model%2520size%2520in%2520human%2520pose%250Aestimation.%2520However%252C%2520existing%2520methods%2520primarily%2520emphasize%2520the%2520transfer%2520of%250Ateacher%2520knowledge%2520while%2520often%2520neglecting%2520the%2520performance%2520degradation%2520resulted%250Afrom%2520the%2520curse%2520of%2520capacity%2520gap%2520between%2520teacher%2520and%2520student.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520AgentPose%252C%2520a%2520novel%2520pose%2520distillation%2520method%2520that%2520integrates%2520a%250Afeature%2520agent%2520to%2520model%2520the%2520distribution%2520of%2520teacher%2520features%2520and%2520progressively%250Aaligns%2520the%2520distribution%2520of%2520student%2520features%2520with%2520that%2520of%2520the%2520teacher%2520feature%252C%250Aeffectively%2520overcoming%2520the%2520capacity%2520gap%2520and%2520enhancing%2520the%2520ability%2520of%2520knowledge%250Atransfer.%2520Our%2520comprehensive%2520experiments%2520conducted%2520on%2520the%2520COCO%2520dataset%250Asubstantiate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520knowledge%2520transfer%252C%250Aparticularly%2520in%2520scenarios%2520with%2520a%2520high%2520capacity%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentPose%3A%20Progressive%20Distribution%20Alignment%20via%20Feature%20Agent%20for%0A%20%20Human%20Pose%20Distillation&entry.906535625=Feng%20Zhang%20and%20Jinwei%20Liu%20and%20Xiatian%20Zhu%20and%20Lei%20Chen&entry.1292438233=%20%20Pose%20distillation%20is%20widely%20adopted%20to%20reduce%20model%20size%20in%20human%20pose%0Aestimation.%20However%2C%20existing%20methods%20primarily%20emphasize%20the%20transfer%20of%0Ateacher%20knowledge%20while%20often%20neglecting%20the%20performance%20degradation%20resulted%0Afrom%20the%20curse%20of%20capacity%20gap%20between%20teacher%20and%20student.%20To%20address%20this%0Aissue%2C%20we%20propose%20AgentPose%2C%20a%20novel%20pose%20distillation%20method%20that%20integrates%20a%0Afeature%20agent%20to%20model%20the%20distribution%20of%20teacher%20features%20and%20progressively%0Aaligns%20the%20distribution%20of%20student%20features%20with%20that%20of%20the%20teacher%20feature%2C%0Aeffectively%20overcoming%20the%20capacity%20gap%20and%20enhancing%20the%20ability%20of%20knowledge%0Atransfer.%20Our%20comprehensive%20experiments%20conducted%20on%20the%20COCO%20dataset%0Asubstantiate%20the%20effectiveness%20of%20our%20method%20in%20knowledge%20transfer%2C%0Aparticularly%20in%20scenarios%20with%20a%20high%20capacity%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08088v1&entry.124074799=Read"},
{"title": "RMem: Restricted Memory Banks Improve Video Object Segmentation", "author": "Junbao Zhou and Ziqi Pang and Yu-Xiong Wang", "abstract": "  With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/.\n", "link": "http://arxiv.org/abs/2406.08476v2", "date": "2025-01-14", "relevancy": 2.6883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation&body=Title%3A%20RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation%0AAuthor%3A%20Junbao%20Zhou%20and%20Ziqi%20Pang%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20With%20recent%20video%20object%20segmentation%20%28VOS%29%20benchmarks%20evolving%20to%0Achallenging%20scenarios%2C%20we%20revisit%20a%20simple%20but%20overlooked%20strategy%3A%20restricting%0Athe%20size%20of%20memory%20banks.%20This%20diverges%20from%20the%20prevalent%20practice%20of%0Aexpanding%20memory%20banks%20to%20accommodate%20extensive%20historical%20information.%20Our%0Aspecially%20designed%20%22memory%20deciphering%22%20study%20offers%20a%20pivotal%20insight%0Aunderpinning%20such%20a%20strategy%3A%20expanding%20memory%20banks%2C%20while%20seemingly%0Abeneficial%2C%20actually%20increases%20the%20difficulty%20for%20VOS%20modules%20to%20decode%0Arelevant%20features%20due%20to%20the%20confusion%20from%20redundant%20information.%20By%0Arestricting%20memory%20banks%20to%20a%20limited%20number%20of%20essential%20frames%2C%20we%20achieve%20a%0Anotable%20improvement%20in%20VOS%20accuracy.%20This%20process%20balances%20the%20importance%20and%0Afreshness%20of%20frames%20to%20maintain%20an%20informative%20memory%20bank%20within%20a%20bounded%0Acapacity.%20Additionally%2C%20restricted%20memory%20banks%20reduce%20the%20training-inference%0Adiscrepancy%20in%20memory%20lengths%20compared%20with%20continuous%20expansion.%20This%20fosters%0Anew%20opportunities%20in%20temporal%20reasoning%20and%20enables%20us%20to%20introduce%20the%0Apreviously%20overlooked%20%22temporal%20positional%20embedding.%22%20Finally%2C%20our%20insights%0Aare%20embodied%20in%20%22RMem%22%20%28%22R%22%20for%20restricted%29%2C%20a%20simple%20yet%20effective%20VOS%0Amodification%20that%20excels%20at%20challenging%20VOS%20scenarios%20and%20establishes%20new%20state%0Aof%20the%20art%20for%20object%20state%20changes%20%28on%20the%20VOST%20dataset%29%20and%20long%20videos%20%28on%0Athe%20Long%20Videos%20dataset%29.%20Our%20code%20and%20demo%20are%20available%20at%0Ahttps%3A//restricted-memory.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRMem%253A%2520Restricted%2520Memory%2520Banks%2520Improve%2520Video%2520Object%2520Segmentation%26entry.906535625%3DJunbao%2520Zhou%2520and%2520Ziqi%2520Pang%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520With%2520recent%2520video%2520object%2520segmentation%2520%2528VOS%2529%2520benchmarks%2520evolving%2520to%250Achallenging%2520scenarios%252C%2520we%2520revisit%2520a%2520simple%2520but%2520overlooked%2520strategy%253A%2520restricting%250Athe%2520size%2520of%2520memory%2520banks.%2520This%2520diverges%2520from%2520the%2520prevalent%2520practice%2520of%250Aexpanding%2520memory%2520banks%2520to%2520accommodate%2520extensive%2520historical%2520information.%2520Our%250Aspecially%2520designed%2520%2522memory%2520deciphering%2522%2520study%2520offers%2520a%2520pivotal%2520insight%250Aunderpinning%2520such%2520a%2520strategy%253A%2520expanding%2520memory%2520banks%252C%2520while%2520seemingly%250Abeneficial%252C%2520actually%2520increases%2520the%2520difficulty%2520for%2520VOS%2520modules%2520to%2520decode%250Arelevant%2520features%2520due%2520to%2520the%2520confusion%2520from%2520redundant%2520information.%2520By%250Arestricting%2520memory%2520banks%2520to%2520a%2520limited%2520number%2520of%2520essential%2520frames%252C%2520we%2520achieve%2520a%250Anotable%2520improvement%2520in%2520VOS%2520accuracy.%2520This%2520process%2520balances%2520the%2520importance%2520and%250Afreshness%2520of%2520frames%2520to%2520maintain%2520an%2520informative%2520memory%2520bank%2520within%2520a%2520bounded%250Acapacity.%2520Additionally%252C%2520restricted%2520memory%2520banks%2520reduce%2520the%2520training-inference%250Adiscrepancy%2520in%2520memory%2520lengths%2520compared%2520with%2520continuous%2520expansion.%2520This%2520fosters%250Anew%2520opportunities%2520in%2520temporal%2520reasoning%2520and%2520enables%2520us%2520to%2520introduce%2520the%250Apreviously%2520overlooked%2520%2522temporal%2520positional%2520embedding.%2522%2520Finally%252C%2520our%2520insights%250Aare%2520embodied%2520in%2520%2522RMem%2522%2520%2528%2522R%2522%2520for%2520restricted%2529%252C%2520a%2520simple%2520yet%2520effective%2520VOS%250Amodification%2520that%2520excels%2520at%2520challenging%2520VOS%2520scenarios%2520and%2520establishes%2520new%2520state%250Aof%2520the%2520art%2520for%2520object%2520state%2520changes%2520%2528on%2520the%2520VOST%2520dataset%2529%2520and%2520long%2520videos%2520%2528on%250Athe%2520Long%2520Videos%2520dataset%2529.%2520Our%2520code%2520and%2520demo%2520are%2520available%2520at%250Ahttps%253A//restricted-memory.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation&entry.906535625=Junbao%20Zhou%20and%20Ziqi%20Pang%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20With%20recent%20video%20object%20segmentation%20%28VOS%29%20benchmarks%20evolving%20to%0Achallenging%20scenarios%2C%20we%20revisit%20a%20simple%20but%20overlooked%20strategy%3A%20restricting%0Athe%20size%20of%20memory%20banks.%20This%20diverges%20from%20the%20prevalent%20practice%20of%0Aexpanding%20memory%20banks%20to%20accommodate%20extensive%20historical%20information.%20Our%0Aspecially%20designed%20%22memory%20deciphering%22%20study%20offers%20a%20pivotal%20insight%0Aunderpinning%20such%20a%20strategy%3A%20expanding%20memory%20banks%2C%20while%20seemingly%0Abeneficial%2C%20actually%20increases%20the%20difficulty%20for%20VOS%20modules%20to%20decode%0Arelevant%20features%20due%20to%20the%20confusion%20from%20redundant%20information.%20By%0Arestricting%20memory%20banks%20to%20a%20limited%20number%20of%20essential%20frames%2C%20we%20achieve%20a%0Anotable%20improvement%20in%20VOS%20accuracy.%20This%20process%20balances%20the%20importance%20and%0Afreshness%20of%20frames%20to%20maintain%20an%20informative%20memory%20bank%20within%20a%20bounded%0Acapacity.%20Additionally%2C%20restricted%20memory%20banks%20reduce%20the%20training-inference%0Adiscrepancy%20in%20memory%20lengths%20compared%20with%20continuous%20expansion.%20This%20fosters%0Anew%20opportunities%20in%20temporal%20reasoning%20and%20enables%20us%20to%20introduce%20the%0Apreviously%20overlooked%20%22temporal%20positional%20embedding.%22%20Finally%2C%20our%20insights%0Aare%20embodied%20in%20%22RMem%22%20%28%22R%22%20for%20restricted%29%2C%20a%20simple%20yet%20effective%20VOS%0Amodification%20that%20excels%20at%20challenging%20VOS%20scenarios%20and%20establishes%20new%20state%0Aof%20the%20art%20for%20object%20state%20changes%20%28on%20the%20VOST%20dataset%29%20and%20long%20videos%20%28on%0Athe%20Long%20Videos%20dataset%29.%20Our%20code%20and%20demo%20are%20available%20at%0Ahttps%3A//restricted-memory.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08476v2&entry.124074799=Read"},
{"title": "Relaxed Rotational Equivariance via $G$-Biases in Vision", "author": "Zhiqiang Wu and Yingjie Liu and Licheng Sun and Jian Yang and Hanlin Dong and Shing-Ho J. Lin and Xuan Tang and Jinpeng Mi and Bo Jin and Xian Wei", "abstract": "  Group Equivariant Convolution (GConv) can capture rotational equivariance\nfrom original data. It assumes uniform and strict rotational equivariance\nacross all features as the transformations under the specific group. However,\nthe presentation or distribution of real-world data rarely conforms to strict\nrotational equivariance, commonly referred to as Rotational Symmetry-Breaking\n(RSB) in the system or dataset, making GConv unable to adapt effectively to\nthis phenomenon. Motivated by this, we propose a simple but highly effective\nmethod to address this problem, which utilizes a set of learnable biases called\n$G$-Biases under the group order to break strict group constraints and then\nachieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate the\nefficiency of RREConv, we conduct extensive ablation experiments on the\ndiscrete rotational group $\\mathcal{C}_n$. Experiments demonstrate that the\nproposed RREConv-based methods achieve excellent performance compared to\nexisting GConv-based methods in both classification and 2D object detection\ntasks on the natural image datasets.\n", "link": "http://arxiv.org/abs/2408.12454v3", "date": "2025-01-14", "relevancy": 2.6281, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5324}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5285}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision&body=Title%3A%20Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision%0AAuthor%3A%20Zhiqiang%20Wu%20and%20Yingjie%20Liu%20and%20Licheng%20Sun%20and%20Jian%20Yang%20and%20Hanlin%20Dong%20and%20Shing-Ho%20J.%20Lin%20and%20Xuan%20Tang%20and%20Jinpeng%20Mi%20and%20Bo%20Jin%20and%20Xian%20Wei%0AAbstract%3A%20%20%20Group%20Equivariant%20Convolution%20%28GConv%29%20can%20capture%20rotational%20equivariance%0Afrom%20original%20data.%20It%20assumes%20uniform%20and%20strict%20rotational%20equivariance%0Aacross%20all%20features%20as%20the%20transformations%20under%20the%20specific%20group.%20However%2C%0Athe%20presentation%20or%20distribution%20of%20real-world%20data%20rarely%20conforms%20to%20strict%0Arotational%20equivariance%2C%20commonly%20referred%20to%20as%20Rotational%20Symmetry-Breaking%0A%28RSB%29%20in%20the%20system%20or%20dataset%2C%20making%20GConv%20unable%20to%20adapt%20effectively%20to%0Athis%20phenomenon.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%20but%20highly%20effective%0Amethod%20to%20address%20this%20problem%2C%20which%20utilizes%20a%20set%20of%20learnable%20biases%20called%0A%24G%24-Biases%20under%20the%20group%20order%20to%20break%20strict%20group%20constraints%20and%20then%0Aachieve%20a%20Relaxed%20Rotational%20Equivariant%20Convolution%20%28RREConv%29.%20To%20validate%20the%0Aefficiency%20of%20RREConv%2C%20we%20conduct%20extensive%20ablation%20experiments%20on%20the%0Adiscrete%20rotational%20group%20%24%5Cmathcal%7BC%7D_n%24.%20Experiments%20demonstrate%20that%20the%0Aproposed%20RREConv-based%20methods%20achieve%20excellent%20performance%20compared%20to%0Aexisting%20GConv-based%20methods%20in%20both%20classification%20and%202D%20object%20detection%0Atasks%20on%20the%20natural%20image%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12454v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaxed%2520Rotational%2520Equivariance%2520via%2520%2524G%2524-Biases%2520in%2520Vision%26entry.906535625%3DZhiqiang%2520Wu%2520and%2520Yingjie%2520Liu%2520and%2520Licheng%2520Sun%2520and%2520Jian%2520Yang%2520and%2520Hanlin%2520Dong%2520and%2520Shing-Ho%2520J.%2520Lin%2520and%2520Xuan%2520Tang%2520and%2520Jinpeng%2520Mi%2520and%2520Bo%2520Jin%2520and%2520Xian%2520Wei%26entry.1292438233%3D%2520%2520Group%2520Equivariant%2520Convolution%2520%2528GConv%2529%2520can%2520capture%2520rotational%2520equivariance%250Afrom%2520original%2520data.%2520It%2520assumes%2520uniform%2520and%2520strict%2520rotational%2520equivariance%250Aacross%2520all%2520features%2520as%2520the%2520transformations%2520under%2520the%2520specific%2520group.%2520However%252C%250Athe%2520presentation%2520or%2520distribution%2520of%2520real-world%2520data%2520rarely%2520conforms%2520to%2520strict%250Arotational%2520equivariance%252C%2520commonly%2520referred%2520to%2520as%2520Rotational%2520Symmetry-Breaking%250A%2528RSB%2529%2520in%2520the%2520system%2520or%2520dataset%252C%2520making%2520GConv%2520unable%2520to%2520adapt%2520effectively%2520to%250Athis%2520phenomenon.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520simple%2520but%2520highly%2520effective%250Amethod%2520to%2520address%2520this%2520problem%252C%2520which%2520utilizes%2520a%2520set%2520of%2520learnable%2520biases%2520called%250A%2524G%2524-Biases%2520under%2520the%2520group%2520order%2520to%2520break%2520strict%2520group%2520constraints%2520and%2520then%250Aachieve%2520a%2520Relaxed%2520Rotational%2520Equivariant%2520Convolution%2520%2528RREConv%2529.%2520To%2520validate%2520the%250Aefficiency%2520of%2520RREConv%252C%2520we%2520conduct%2520extensive%2520ablation%2520experiments%2520on%2520the%250Adiscrete%2520rotational%2520group%2520%2524%255Cmathcal%257BC%257D_n%2524.%2520Experiments%2520demonstrate%2520that%2520the%250Aproposed%2520RREConv-based%2520methods%2520achieve%2520excellent%2520performance%2520compared%2520to%250Aexisting%2520GConv-based%2520methods%2520in%2520both%2520classification%2520and%25202D%2520object%2520detection%250Atasks%2520on%2520the%2520natural%2520image%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12454v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision&entry.906535625=Zhiqiang%20Wu%20and%20Yingjie%20Liu%20and%20Licheng%20Sun%20and%20Jian%20Yang%20and%20Hanlin%20Dong%20and%20Shing-Ho%20J.%20Lin%20and%20Xuan%20Tang%20and%20Jinpeng%20Mi%20and%20Bo%20Jin%20and%20Xian%20Wei&entry.1292438233=%20%20Group%20Equivariant%20Convolution%20%28GConv%29%20can%20capture%20rotational%20equivariance%0Afrom%20original%20data.%20It%20assumes%20uniform%20and%20strict%20rotational%20equivariance%0Aacross%20all%20features%20as%20the%20transformations%20under%20the%20specific%20group.%20However%2C%0Athe%20presentation%20or%20distribution%20of%20real-world%20data%20rarely%20conforms%20to%20strict%0Arotational%20equivariance%2C%20commonly%20referred%20to%20as%20Rotational%20Symmetry-Breaking%0A%28RSB%29%20in%20the%20system%20or%20dataset%2C%20making%20GConv%20unable%20to%20adapt%20effectively%20to%0Athis%20phenomenon.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%20but%20highly%20effective%0Amethod%20to%20address%20this%20problem%2C%20which%20utilizes%20a%20set%20of%20learnable%20biases%20called%0A%24G%24-Biases%20under%20the%20group%20order%20to%20break%20strict%20group%20constraints%20and%20then%0Aachieve%20a%20Relaxed%20Rotational%20Equivariant%20Convolution%20%28RREConv%29.%20To%20validate%20the%0Aefficiency%20of%20RREConv%2C%20we%20conduct%20extensive%20ablation%20experiments%20on%20the%0Adiscrete%20rotational%20group%20%24%5Cmathcal%7BC%7D_n%24.%20Experiments%20demonstrate%20that%20the%0Aproposed%20RREConv-based%20methods%20achieve%20excellent%20performance%20compared%20to%0Aexisting%20GConv-based%20methods%20in%20both%20classification%20and%202D%20object%20detection%0Atasks%20on%20the%20natural%20image%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12454v3&entry.124074799=Read"},
{"title": "Object-Centric 2D Gaussian Splatting: Background Removal and\n  Occlusion-Aware Pruning for Compact Object Models", "author": "Marcel Rogge and Didier Stricker", "abstract": "  Current Gaussian Splatting approaches are effective for reconstructing entire\nscenes but lack the option to target specific objects, making them\ncomputationally expensive and unsuitable for object-specific applications. We\npropose a novel approach that leverages object masks to enable targeted\nreconstruction, resulting in object-centric models. Additionally, we introduce\nan occlusion-aware pruning strategy to minimize the number of Gaussians without\ncompromising quality. Our method reconstructs compact object models, yielding\nobject-centric Gaussian and mesh representations that are up to 96\\% smaller\nand up to 71\\% faster to train compared to the baseline while retaining\ncompetitive quality. These representations are immediately usable for\ndownstream applications such as appearance editing and physics simulation\nwithout additional processing.\n", "link": "http://arxiv.org/abs/2501.08174v1", "date": "2025-01-14", "relevancy": 2.6046, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.656}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6525}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-Centric%202D%20Gaussian%20Splatting%3A%20Background%20Removal%20and%0A%20%20Occlusion-Aware%20Pruning%20for%20Compact%20Object%20Models&body=Title%3A%20Object-Centric%202D%20Gaussian%20Splatting%3A%20Background%20Removal%20and%0A%20%20Occlusion-Aware%20Pruning%20for%20Compact%20Object%20Models%0AAuthor%3A%20Marcel%20Rogge%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Current%20Gaussian%20Splatting%20approaches%20are%20effective%20for%20reconstructing%20entire%0Ascenes%20but%20lack%20the%20option%20to%20target%20specific%20objects%2C%20making%20them%0Acomputationally%20expensive%20and%20unsuitable%20for%20object-specific%20applications.%20We%0Apropose%20a%20novel%20approach%20that%20leverages%20object%20masks%20to%20enable%20targeted%0Areconstruction%2C%20resulting%20in%20object-centric%20models.%20Additionally%2C%20we%20introduce%0Aan%20occlusion-aware%20pruning%20strategy%20to%20minimize%20the%20number%20of%20Gaussians%20without%0Acompromising%20quality.%20Our%20method%20reconstructs%20compact%20object%20models%2C%20yielding%0Aobject-centric%20Gaussian%20and%20mesh%20representations%20that%20are%20up%20to%2096%5C%25%20smaller%0Aand%20up%20to%2071%5C%25%20faster%20to%20train%20compared%20to%20the%20baseline%20while%20retaining%0Acompetitive%20quality.%20These%20representations%20are%20immediately%20usable%20for%0Adownstream%20applications%20such%20as%20appearance%20editing%20and%20physics%20simulation%0Awithout%20additional%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08174v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-Centric%25202D%2520Gaussian%2520Splatting%253A%2520Background%2520Removal%2520and%250A%2520%2520Occlusion-Aware%2520Pruning%2520for%2520Compact%2520Object%2520Models%26entry.906535625%3DMarcel%2520Rogge%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520Current%2520Gaussian%2520Splatting%2520approaches%2520are%2520effective%2520for%2520reconstructing%2520entire%250Ascenes%2520but%2520lack%2520the%2520option%2520to%2520target%2520specific%2520objects%252C%2520making%2520them%250Acomputationally%2520expensive%2520and%2520unsuitable%2520for%2520object-specific%2520applications.%2520We%250Apropose%2520a%2520novel%2520approach%2520that%2520leverages%2520object%2520masks%2520to%2520enable%2520targeted%250Areconstruction%252C%2520resulting%2520in%2520object-centric%2520models.%2520Additionally%252C%2520we%2520introduce%250Aan%2520occlusion-aware%2520pruning%2520strategy%2520to%2520minimize%2520the%2520number%2520of%2520Gaussians%2520without%250Acompromising%2520quality.%2520Our%2520method%2520reconstructs%2520compact%2520object%2520models%252C%2520yielding%250Aobject-centric%2520Gaussian%2520and%2520mesh%2520representations%2520that%2520are%2520up%2520to%252096%255C%2525%2520smaller%250Aand%2520up%2520to%252071%255C%2525%2520faster%2520to%2520train%2520compared%2520to%2520the%2520baseline%2520while%2520retaining%250Acompetitive%2520quality.%2520These%2520representations%2520are%2520immediately%2520usable%2520for%250Adownstream%2520applications%2520such%2520as%2520appearance%2520editing%2520and%2520physics%2520simulation%250Awithout%2520additional%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08174v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-Centric%202D%20Gaussian%20Splatting%3A%20Background%20Removal%20and%0A%20%20Occlusion-Aware%20Pruning%20for%20Compact%20Object%20Models&entry.906535625=Marcel%20Rogge%20and%20Didier%20Stricker&entry.1292438233=%20%20Current%20Gaussian%20Splatting%20approaches%20are%20effective%20for%20reconstructing%20entire%0Ascenes%20but%20lack%20the%20option%20to%20target%20specific%20objects%2C%20making%20them%0Acomputationally%20expensive%20and%20unsuitable%20for%20object-specific%20applications.%20We%0Apropose%20a%20novel%20approach%20that%20leverages%20object%20masks%20to%20enable%20targeted%0Areconstruction%2C%20resulting%20in%20object-centric%20models.%20Additionally%2C%20we%20introduce%0Aan%20occlusion-aware%20pruning%20strategy%20to%20minimize%20the%20number%20of%20Gaussians%20without%0Acompromising%20quality.%20Our%20method%20reconstructs%20compact%20object%20models%2C%20yielding%0Aobject-centric%20Gaussian%20and%20mesh%20representations%20that%20are%20up%20to%2096%5C%25%20smaller%0Aand%20up%20to%2071%5C%25%20faster%20to%20train%20compared%20to%20the%20baseline%20while%20retaining%0Acompetitive%20quality.%20These%20representations%20are%20immediately%20usable%20for%0Adownstream%20applications%20such%20as%20appearance%20editing%20and%20physics%20simulation%0Awithout%20additional%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08174v1&entry.124074799=Read"},
{"title": "DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video\n  Diffusion Models", "author": "Hyeonwoo Kim and Sangwon Beak and Hanbyul Joo", "abstract": "  Understanding the ability of humans to use objects is crucial for AI to\nimprove daily life. Existing studies for learning such ability focus on\nhuman-object patterns (e.g., contact, spatial relation, orientation) in static\nsituations, and learning Human-Object Interaction (HOI) patterns over time\n(i.e., movement of human and object) is relatively less explored. In this\npaper, we introduce a novel type of affordance named Dynamic Affordance. For a\ngiven input 3D object mesh, we learn dynamic affordance which models the\ndistribution of both (1) human motion and (2) human-guided object pose during\ninteractions. As a core idea, we present a method to learn the 3D dynamic\naffordance from synthetically generated 2D videos, leveraging a pre-trained\nvideo diffusion model. Specifically, we propose a pipeline that first generates\n2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI\nsamples. Once we generate diverse 4D HOI samples on various target objects, we\ntrain our DAViD, where we present a method based on the Low-Rank Adaptation\n(LoRA) module for pre-trained human motion diffusion model (MDM) and an object\npose diffusion model with human pose guidance. Our motion diffusion model is\nextended for multi-object interactions, demonstrating the advantage of our\npipeline with LoRA for combining the concepts of object usage. Through\nextensive experiments, we demonstrate our DAViD outperforms the baselines in\ngenerating human motion with HOIs.\n", "link": "http://arxiv.org/abs/2501.08333v1", "date": "2025-01-14", "relevancy": 2.6026, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7045}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6243}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20using%20Pre-trained%20Video%0A%20%20Diffusion%20Models&body=Title%3A%20DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20using%20Pre-trained%20Video%0A%20%20Diffusion%20Models%0AAuthor%3A%20Hyeonwoo%20Kim%20and%20Sangwon%20Beak%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20Understanding%20the%20ability%20of%20humans%20to%20use%20objects%20is%20crucial%20for%20AI%20to%0Aimprove%20daily%20life.%20Existing%20studies%20for%20learning%20such%20ability%20focus%20on%0Ahuman-object%20patterns%20%28e.g.%2C%20contact%2C%20spatial%20relation%2C%20orientation%29%20in%20static%0Asituations%2C%20and%20learning%20Human-Object%20Interaction%20%28HOI%29%20patterns%20over%20time%0A%28i.e.%2C%20movement%20of%20human%20and%20object%29%20is%20relatively%20less%20explored.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20type%20of%20affordance%20named%20Dynamic%20Affordance.%20For%20a%0Agiven%20input%203D%20object%20mesh%2C%20we%20learn%20dynamic%20affordance%20which%20models%20the%0Adistribution%20of%20both%20%281%29%20human%20motion%20and%20%282%29%20human-guided%20object%20pose%20during%0Ainteractions.%20As%20a%20core%20idea%2C%20we%20present%20a%20method%20to%20learn%20the%203D%20dynamic%0Aaffordance%20from%20synthetically%20generated%202D%20videos%2C%20leveraging%20a%20pre-trained%0Avideo%20diffusion%20model.%20Specifically%2C%20we%20propose%20a%20pipeline%20that%20first%20generates%0A2D%20HOI%20videos%20from%20the%203D%20object%20and%20then%20lifts%20them%20into%203D%20to%20generate%204D%20HOI%0Asamples.%20Once%20we%20generate%20diverse%204D%20HOI%20samples%20on%20various%20target%20objects%2C%20we%0Atrain%20our%20DAViD%2C%20where%20we%20present%20a%20method%20based%20on%20the%20Low-Rank%20Adaptation%0A%28LoRA%29%20module%20for%20pre-trained%20human%20motion%20diffusion%20model%20%28MDM%29%20and%20an%20object%0Apose%20diffusion%20model%20with%20human%20pose%20guidance.%20Our%20motion%20diffusion%20model%20is%0Aextended%20for%20multi-object%20interactions%2C%20demonstrating%20the%20advantage%20of%20our%0Apipeline%20with%20LoRA%20for%20combining%20the%20concepts%20of%20object%20usage.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20our%20DAViD%20outperforms%20the%20baselines%20in%0Agenerating%20human%20motion%20with%20HOIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAViD%253A%2520Modeling%2520Dynamic%2520Affordance%2520of%25203D%2520Objects%2520using%2520Pre-trained%2520Video%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DHyeonwoo%2520Kim%2520and%2520Sangwon%2520Beak%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3D%2520%2520Understanding%2520the%2520ability%2520of%2520humans%2520to%2520use%2520objects%2520is%2520crucial%2520for%2520AI%2520to%250Aimprove%2520daily%2520life.%2520Existing%2520studies%2520for%2520learning%2520such%2520ability%2520focus%2520on%250Ahuman-object%2520patterns%2520%2528e.g.%252C%2520contact%252C%2520spatial%2520relation%252C%2520orientation%2529%2520in%2520static%250Asituations%252C%2520and%2520learning%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520patterns%2520over%2520time%250A%2528i.e.%252C%2520movement%2520of%2520human%2520and%2520object%2529%2520is%2520relatively%2520less%2520explored.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520type%2520of%2520affordance%2520named%2520Dynamic%2520Affordance.%2520For%2520a%250Agiven%2520input%25203D%2520object%2520mesh%252C%2520we%2520learn%2520dynamic%2520affordance%2520which%2520models%2520the%250Adistribution%2520of%2520both%2520%25281%2529%2520human%2520motion%2520and%2520%25282%2529%2520human-guided%2520object%2520pose%2520during%250Ainteractions.%2520As%2520a%2520core%2520idea%252C%2520we%2520present%2520a%2520method%2520to%2520learn%2520the%25203D%2520dynamic%250Aaffordance%2520from%2520synthetically%2520generated%25202D%2520videos%252C%2520leveraging%2520a%2520pre-trained%250Avideo%2520diffusion%2520model.%2520Specifically%252C%2520we%2520propose%2520a%2520pipeline%2520that%2520first%2520generates%250A2D%2520HOI%2520videos%2520from%2520the%25203D%2520object%2520and%2520then%2520lifts%2520them%2520into%25203D%2520to%2520generate%25204D%2520HOI%250Asamples.%2520Once%2520we%2520generate%2520diverse%25204D%2520HOI%2520samples%2520on%2520various%2520target%2520objects%252C%2520we%250Atrain%2520our%2520DAViD%252C%2520where%2520we%2520present%2520a%2520method%2520based%2520on%2520the%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%2520module%2520for%2520pre-trained%2520human%2520motion%2520diffusion%2520model%2520%2528MDM%2529%2520and%2520an%2520object%250Apose%2520diffusion%2520model%2520with%2520human%2520pose%2520guidance.%2520Our%2520motion%2520diffusion%2520model%2520is%250Aextended%2520for%2520multi-object%2520interactions%252C%2520demonstrating%2520the%2520advantage%2520of%2520our%250Apipeline%2520with%2520LoRA%2520for%2520combining%2520the%2520concepts%2520of%2520object%2520usage.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520our%2520DAViD%2520outperforms%2520the%2520baselines%2520in%250Agenerating%2520human%2520motion%2520with%2520HOIs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20using%20Pre-trained%20Video%0A%20%20Diffusion%20Models&entry.906535625=Hyeonwoo%20Kim%20and%20Sangwon%20Beak%20and%20Hanbyul%20Joo&entry.1292438233=%20%20Understanding%20the%20ability%20of%20humans%20to%20use%20objects%20is%20crucial%20for%20AI%20to%0Aimprove%20daily%20life.%20Existing%20studies%20for%20learning%20such%20ability%20focus%20on%0Ahuman-object%20patterns%20%28e.g.%2C%20contact%2C%20spatial%20relation%2C%20orientation%29%20in%20static%0Asituations%2C%20and%20learning%20Human-Object%20Interaction%20%28HOI%29%20patterns%20over%20time%0A%28i.e.%2C%20movement%20of%20human%20and%20object%29%20is%20relatively%20less%20explored.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20type%20of%20affordance%20named%20Dynamic%20Affordance.%20For%20a%0Agiven%20input%203D%20object%20mesh%2C%20we%20learn%20dynamic%20affordance%20which%20models%20the%0Adistribution%20of%20both%20%281%29%20human%20motion%20and%20%282%29%20human-guided%20object%20pose%20during%0Ainteractions.%20As%20a%20core%20idea%2C%20we%20present%20a%20method%20to%20learn%20the%203D%20dynamic%0Aaffordance%20from%20synthetically%20generated%202D%20videos%2C%20leveraging%20a%20pre-trained%0Avideo%20diffusion%20model.%20Specifically%2C%20we%20propose%20a%20pipeline%20that%20first%20generates%0A2D%20HOI%20videos%20from%20the%203D%20object%20and%20then%20lifts%20them%20into%203D%20to%20generate%204D%20HOI%0Asamples.%20Once%20we%20generate%20diverse%204D%20HOI%20samples%20on%20various%20target%20objects%2C%20we%0Atrain%20our%20DAViD%2C%20where%20we%20present%20a%20method%20based%20on%20the%20Low-Rank%20Adaptation%0A%28LoRA%29%20module%20for%20pre-trained%20human%20motion%20diffusion%20model%20%28MDM%29%20and%20an%20object%0Apose%20diffusion%20model%20with%20human%20pose%20guidance.%20Our%20motion%20diffusion%20model%20is%0Aextended%20for%20multi-object%20interactions%2C%20demonstrating%20the%20advantage%20of%20our%0Apipeline%20with%20LoRA%20for%20combining%20the%20concepts%20of%20object%20usage.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20our%20DAViD%20outperforms%20the%20baselines%20in%0Agenerating%20human%20motion%20with%20HOIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08333v1&entry.124074799=Read"},
{"title": "MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention\n  Mechanism for Tiny Datasets", "author": "Bowei Zhang and Yi Zhang", "abstract": "  Vision Transformer (ViT) has demonstrated significant potential in various\nvision tasks due to its strong ability in modelling long-range dependencies.\nHowever, such success is largely fueled by training on massive samples. In real\napplications, the large-scale datasets are not always available, and ViT\nperforms worse than Convolutional Neural Networks (CNNs) if it is only trained\non small scale dataset (called tiny dataset), since it requires large amount of\ntraining data to ensure its representational capacity. In this paper, a\nsmall-size ViT architecture with multi-scale self-attention mechanism and\nconvolution blocks is presented (dubbed MSCViT) to model different scales of\nattention at each layer. Firstly, we introduced wavelet convolution, which\nselectively combines the high-frequency components obtained by frequency\ndivision with our convolution channel to extract local features. Then, a\nlightweight multi-head attention module is developed to reduce the number of\ntokens and computational costs. Finally, the positional encoding (PE) in the\nbackbone is replaced by a local feature extraction module. Compared with the\noriginal ViT, it is parameter-efficient and is particularly suitable for tiny\ndatasets. Extensive experiments have been conducted on tiny datasets, in which\nour model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and\n2.5 GFLOPs, without pre-training on large datasets.\n", "link": "http://arxiv.org/abs/2501.06040v2", "date": "2025-01-14", "relevancy": 2.581, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5279}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets&body=Title%3A%20MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets%0AAuthor%3A%20Bowei%20Zhang%20and%20Yi%20Zhang%0AAbstract%3A%20%20%20Vision%20Transformer%20%28ViT%29%20has%20demonstrated%20significant%20potential%20in%20various%0Avision%20tasks%20due%20to%20its%20strong%20ability%20in%20modelling%20long-range%20dependencies.%0AHowever%2C%20such%20success%20is%20largely%20fueled%20by%20training%20on%20massive%20samples.%20In%20real%0Aapplications%2C%20the%20large-scale%20datasets%20are%20not%20always%20available%2C%20and%20ViT%0Aperforms%20worse%20than%20Convolutional%20Neural%20Networks%20%28CNNs%29%20if%20it%20is%20only%20trained%0Aon%20small%20scale%20dataset%20%28called%20tiny%20dataset%29%2C%20since%20it%20requires%20large%20amount%20of%0Atraining%20data%20to%20ensure%20its%20representational%20capacity.%20In%20this%20paper%2C%20a%0Asmall-size%20ViT%20architecture%20with%20multi-scale%20self-attention%20mechanism%20and%0Aconvolution%20blocks%20is%20presented%20%28dubbed%20MSCViT%29%20to%20model%20different%20scales%20of%0Aattention%20at%20each%20layer.%20Firstly%2C%20we%20introduced%20wavelet%20convolution%2C%20which%0Aselectively%20combines%20the%20high-frequency%20components%20obtained%20by%20frequency%0Adivision%20with%20our%20convolution%20channel%20to%20extract%20local%20features.%20Then%2C%20a%0Alightweight%20multi-head%20attention%20module%20is%20developed%20to%20reduce%20the%20number%20of%0Atokens%20and%20computational%20costs.%20Finally%2C%20the%20positional%20encoding%20%28PE%29%20in%20the%0Abackbone%20is%20replaced%20by%20a%20local%20feature%20extraction%20module.%20Compared%20with%20the%0Aoriginal%20ViT%2C%20it%20is%20parameter-efficient%20and%20is%20particularly%20suitable%20for%20tiny%0Adatasets.%20Extensive%20experiments%20have%20been%20conducted%20on%20tiny%20datasets%2C%20in%20which%0Aour%20model%20achieves%20an%20accuracy%20of%2084.68%25%20on%20CIFAR-100%20with%2014.0M%20parameters%20and%0A2.5%20GFLOPs%2C%20without%20pre-training%20on%20large%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSCViT%253A%2520A%2520Small-size%2520ViT%2520architecture%2520with%2520Multi-Scale%2520Self-Attention%250A%2520%2520Mechanism%2520for%2520Tiny%2520Datasets%26entry.906535625%3DBowei%2520Zhang%2520and%2520Yi%2520Zhang%26entry.1292438233%3D%2520%2520Vision%2520Transformer%2520%2528ViT%2529%2520has%2520demonstrated%2520significant%2520potential%2520in%2520various%250Avision%2520tasks%2520due%2520to%2520its%2520strong%2520ability%2520in%2520modelling%2520long-range%2520dependencies.%250AHowever%252C%2520such%2520success%2520is%2520largely%2520fueled%2520by%2520training%2520on%2520massive%2520samples.%2520In%2520real%250Aapplications%252C%2520the%2520large-scale%2520datasets%2520are%2520not%2520always%2520available%252C%2520and%2520ViT%250Aperforms%2520worse%2520than%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520if%2520it%2520is%2520only%2520trained%250Aon%2520small%2520scale%2520dataset%2520%2528called%2520tiny%2520dataset%2529%252C%2520since%2520it%2520requires%2520large%2520amount%2520of%250Atraining%2520data%2520to%2520ensure%2520its%2520representational%2520capacity.%2520In%2520this%2520paper%252C%2520a%250Asmall-size%2520ViT%2520architecture%2520with%2520multi-scale%2520self-attention%2520mechanism%2520and%250Aconvolution%2520blocks%2520is%2520presented%2520%2528dubbed%2520MSCViT%2529%2520to%2520model%2520different%2520scales%2520of%250Aattention%2520at%2520each%2520layer.%2520Firstly%252C%2520we%2520introduced%2520wavelet%2520convolution%252C%2520which%250Aselectively%2520combines%2520the%2520high-frequency%2520components%2520obtained%2520by%2520frequency%250Adivision%2520with%2520our%2520convolution%2520channel%2520to%2520extract%2520local%2520features.%2520Then%252C%2520a%250Alightweight%2520multi-head%2520attention%2520module%2520is%2520developed%2520to%2520reduce%2520the%2520number%2520of%250Atokens%2520and%2520computational%2520costs.%2520Finally%252C%2520the%2520positional%2520encoding%2520%2528PE%2529%2520in%2520the%250Abackbone%2520is%2520replaced%2520by%2520a%2520local%2520feature%2520extraction%2520module.%2520Compared%2520with%2520the%250Aoriginal%2520ViT%252C%2520it%2520is%2520parameter-efficient%2520and%2520is%2520particularly%2520suitable%2520for%2520tiny%250Adatasets.%2520Extensive%2520experiments%2520have%2520been%2520conducted%2520on%2520tiny%2520datasets%252C%2520in%2520which%250Aour%2520model%2520achieves%2520an%2520accuracy%2520of%252084.68%2525%2520on%2520CIFAR-100%2520with%252014.0M%2520parameters%2520and%250A2.5%2520GFLOPs%252C%2520without%2520pre-training%2520on%2520large%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSCViT%3A%20A%20Small-size%20ViT%20architecture%20with%20Multi-Scale%20Self-Attention%0A%20%20Mechanism%20for%20Tiny%20Datasets&entry.906535625=Bowei%20Zhang%20and%20Yi%20Zhang&entry.1292438233=%20%20Vision%20Transformer%20%28ViT%29%20has%20demonstrated%20significant%20potential%20in%20various%0Avision%20tasks%20due%20to%20its%20strong%20ability%20in%20modelling%20long-range%20dependencies.%0AHowever%2C%20such%20success%20is%20largely%20fueled%20by%20training%20on%20massive%20samples.%20In%20real%0Aapplications%2C%20the%20large-scale%20datasets%20are%20not%20always%20available%2C%20and%20ViT%0Aperforms%20worse%20than%20Convolutional%20Neural%20Networks%20%28CNNs%29%20if%20it%20is%20only%20trained%0Aon%20small%20scale%20dataset%20%28called%20tiny%20dataset%29%2C%20since%20it%20requires%20large%20amount%20of%0Atraining%20data%20to%20ensure%20its%20representational%20capacity.%20In%20this%20paper%2C%20a%0Asmall-size%20ViT%20architecture%20with%20multi-scale%20self-attention%20mechanism%20and%0Aconvolution%20blocks%20is%20presented%20%28dubbed%20MSCViT%29%20to%20model%20different%20scales%20of%0Aattention%20at%20each%20layer.%20Firstly%2C%20we%20introduced%20wavelet%20convolution%2C%20which%0Aselectively%20combines%20the%20high-frequency%20components%20obtained%20by%20frequency%0Adivision%20with%20our%20convolution%20channel%20to%20extract%20local%20features.%20Then%2C%20a%0Alightweight%20multi-head%20attention%20module%20is%20developed%20to%20reduce%20the%20number%20of%0Atokens%20and%20computational%20costs.%20Finally%2C%20the%20positional%20encoding%20%28PE%29%20in%20the%0Abackbone%20is%20replaced%20by%20a%20local%20feature%20extraction%20module.%20Compared%20with%20the%0Aoriginal%20ViT%2C%20it%20is%20parameter-efficient%20and%20is%20particularly%20suitable%20for%20tiny%0Adatasets.%20Extensive%20experiments%20have%20been%20conducted%20on%20tiny%20datasets%2C%20in%20which%0Aour%20model%20achieves%20an%20accuracy%20of%2084.68%25%20on%20CIFAR-100%20with%2014.0M%20parameters%20and%0A2.5%20GFLOPs%2C%20without%20pre-training%20on%20large%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06040v2&entry.124074799=Read"},
{"title": "EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision", "author": "Diego Velazquez and Pau Rodriguez L\u00f3pez and Sergio Alonso and Josep M. Gonfaus and Jordi Gonzalez and Gerardo Richarte and Javier Marin and Yoshua Bengio and Alexandre Lacoste", "abstract": "  This paper presents EarthView, a comprehensive dataset specifically designed\nfor self-supervision on remote sensing data, intended to enhance deep learning\napplications on Earth monitoring tasks. The dataset spans 15 tera pixels of\nglobal remote-sensing data, combining imagery from a diverse range of sources,\nincluding NEON, Sentinel, and a novel release of 1m spatial resolution data\nfrom Satellogic. Our dataset provides a wide spectrum of image data with\nvarying resolutions, harnessed from different sensors and organized coherently\ninto an accessible HuggingFace dataset in parquet format. This data spans five\nyears, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a\ntailored Masked Autoencoder, developed to tackle the distinct challenges of\nremote sensing data. Trained in a self-supervised fashion, EarthMAE effectively\nprocesses different data modalities such as hyperspectral, multispectral,\ntopographical data, segmentation maps, and temporal structure. This model helps\nus show that pre-training on Satellogic data improves performance on downstream\ntasks. While there is still a gap to fill in MAE for heterogeneous data, we\nregard this innovative combination of an expansive, diverse dataset and a\nversatile model adapted for self-supervised learning as a stride forward in\ndeep learning for Earth monitoring.\n", "link": "http://arxiv.org/abs/2501.08111v1", "date": "2025-01-14", "relevancy": 2.5685, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5262}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5101}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthView%3A%20A%20Large%20Scale%20Remote%20Sensing%20Dataset%20for%20Self-Supervision&body=Title%3A%20EarthView%3A%20A%20Large%20Scale%20Remote%20Sensing%20Dataset%20for%20Self-Supervision%0AAuthor%3A%20Diego%20Velazquez%20and%20Pau%20Rodriguez%20L%C3%B3pez%20and%20Sergio%20Alonso%20and%20Josep%20M.%20Gonfaus%20and%20Jordi%20Gonzalez%20and%20Gerardo%20Richarte%20and%20Javier%20Marin%20and%20Yoshua%20Bengio%20and%20Alexandre%20Lacoste%0AAbstract%3A%20%20%20This%20paper%20presents%20EarthView%2C%20a%20comprehensive%20dataset%20specifically%20designed%0Afor%20self-supervision%20on%20remote%20sensing%20data%2C%20intended%20to%20enhance%20deep%20learning%0Aapplications%20on%20Earth%20monitoring%20tasks.%20The%20dataset%20spans%2015%20tera%20pixels%20of%0Aglobal%20remote-sensing%20data%2C%20combining%20imagery%20from%20a%20diverse%20range%20of%20sources%2C%0Aincluding%20NEON%2C%20Sentinel%2C%20and%20a%20novel%20release%20of%201m%20spatial%20resolution%20data%0Afrom%20Satellogic.%20Our%20dataset%20provides%20a%20wide%20spectrum%20of%20image%20data%20with%0Avarying%20resolutions%2C%20harnessed%20from%20different%20sensors%20and%20organized%20coherently%0Ainto%20an%20accessible%20HuggingFace%20dataset%20in%20parquet%20format.%20This%20data%20spans%20five%0Ayears%2C%20from%202017%20to%202022.%20Accompanying%20the%20dataset%2C%20we%20introduce%20EarthMAE%2C%20a%0Atailored%20Masked%20Autoencoder%2C%20developed%20to%20tackle%20the%20distinct%20challenges%20of%0Aremote%20sensing%20data.%20Trained%20in%20a%20self-supervised%20fashion%2C%20EarthMAE%20effectively%0Aprocesses%20different%20data%20modalities%20such%20as%20hyperspectral%2C%20multispectral%2C%0Atopographical%20data%2C%20segmentation%20maps%2C%20and%20temporal%20structure.%20This%20model%20helps%0Aus%20show%20that%20pre-training%20on%20Satellogic%20data%20improves%20performance%20on%20downstream%0Atasks.%20While%20there%20is%20still%20a%20gap%20to%20fill%20in%20MAE%20for%20heterogeneous%20data%2C%20we%0Aregard%20this%20innovative%20combination%20of%20an%20expansive%2C%20diverse%20dataset%20and%20a%0Aversatile%20model%20adapted%20for%20self-supervised%20learning%20as%20a%20stride%20forward%20in%0Adeep%20learning%20for%20Earth%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthView%253A%2520A%2520Large%2520Scale%2520Remote%2520Sensing%2520Dataset%2520for%2520Self-Supervision%26entry.906535625%3DDiego%2520Velazquez%2520and%2520Pau%2520Rodriguez%2520L%25C3%25B3pez%2520and%2520Sergio%2520Alonso%2520and%2520Josep%2520M.%2520Gonfaus%2520and%2520Jordi%2520Gonzalez%2520and%2520Gerardo%2520Richarte%2520and%2520Javier%2520Marin%2520and%2520Yoshua%2520Bengio%2520and%2520Alexandre%2520Lacoste%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520EarthView%252C%2520a%2520comprehensive%2520dataset%2520specifically%2520designed%250Afor%2520self-supervision%2520on%2520remote%2520sensing%2520data%252C%2520intended%2520to%2520enhance%2520deep%2520learning%250Aapplications%2520on%2520Earth%2520monitoring%2520tasks.%2520The%2520dataset%2520spans%252015%2520tera%2520pixels%2520of%250Aglobal%2520remote-sensing%2520data%252C%2520combining%2520imagery%2520from%2520a%2520diverse%2520range%2520of%2520sources%252C%250Aincluding%2520NEON%252C%2520Sentinel%252C%2520and%2520a%2520novel%2520release%2520of%25201m%2520spatial%2520resolution%2520data%250Afrom%2520Satellogic.%2520Our%2520dataset%2520provides%2520a%2520wide%2520spectrum%2520of%2520image%2520data%2520with%250Avarying%2520resolutions%252C%2520harnessed%2520from%2520different%2520sensors%2520and%2520organized%2520coherently%250Ainto%2520an%2520accessible%2520HuggingFace%2520dataset%2520in%2520parquet%2520format.%2520This%2520data%2520spans%2520five%250Ayears%252C%2520from%25202017%2520to%25202022.%2520Accompanying%2520the%2520dataset%252C%2520we%2520introduce%2520EarthMAE%252C%2520a%250Atailored%2520Masked%2520Autoencoder%252C%2520developed%2520to%2520tackle%2520the%2520distinct%2520challenges%2520of%250Aremote%2520sensing%2520data.%2520Trained%2520in%2520a%2520self-supervised%2520fashion%252C%2520EarthMAE%2520effectively%250Aprocesses%2520different%2520data%2520modalities%2520such%2520as%2520hyperspectral%252C%2520multispectral%252C%250Atopographical%2520data%252C%2520segmentation%2520maps%252C%2520and%2520temporal%2520structure.%2520This%2520model%2520helps%250Aus%2520show%2520that%2520pre-training%2520on%2520Satellogic%2520data%2520improves%2520performance%2520on%2520downstream%250Atasks.%2520While%2520there%2520is%2520still%2520a%2520gap%2520to%2520fill%2520in%2520MAE%2520for%2520heterogeneous%2520data%252C%2520we%250Aregard%2520this%2520innovative%2520combination%2520of%2520an%2520expansive%252C%2520diverse%2520dataset%2520and%2520a%250Aversatile%2520model%2520adapted%2520for%2520self-supervised%2520learning%2520as%2520a%2520stride%2520forward%2520in%250Adeep%2520learning%2520for%2520Earth%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthView%3A%20A%20Large%20Scale%20Remote%20Sensing%20Dataset%20for%20Self-Supervision&entry.906535625=Diego%20Velazquez%20and%20Pau%20Rodriguez%20L%C3%B3pez%20and%20Sergio%20Alonso%20and%20Josep%20M.%20Gonfaus%20and%20Jordi%20Gonzalez%20and%20Gerardo%20Richarte%20and%20Javier%20Marin%20and%20Yoshua%20Bengio%20and%20Alexandre%20Lacoste&entry.1292438233=%20%20This%20paper%20presents%20EarthView%2C%20a%20comprehensive%20dataset%20specifically%20designed%0Afor%20self-supervision%20on%20remote%20sensing%20data%2C%20intended%20to%20enhance%20deep%20learning%0Aapplications%20on%20Earth%20monitoring%20tasks.%20The%20dataset%20spans%2015%20tera%20pixels%20of%0Aglobal%20remote-sensing%20data%2C%20combining%20imagery%20from%20a%20diverse%20range%20of%20sources%2C%0Aincluding%20NEON%2C%20Sentinel%2C%20and%20a%20novel%20release%20of%201m%20spatial%20resolution%20data%0Afrom%20Satellogic.%20Our%20dataset%20provides%20a%20wide%20spectrum%20of%20image%20data%20with%0Avarying%20resolutions%2C%20harnessed%20from%20different%20sensors%20and%20organized%20coherently%0Ainto%20an%20accessible%20HuggingFace%20dataset%20in%20parquet%20format.%20This%20data%20spans%20five%0Ayears%2C%20from%202017%20to%202022.%20Accompanying%20the%20dataset%2C%20we%20introduce%20EarthMAE%2C%20a%0Atailored%20Masked%20Autoencoder%2C%20developed%20to%20tackle%20the%20distinct%20challenges%20of%0Aremote%20sensing%20data.%20Trained%20in%20a%20self-supervised%20fashion%2C%20EarthMAE%20effectively%0Aprocesses%20different%20data%20modalities%20such%20as%20hyperspectral%2C%20multispectral%2C%0Atopographical%20data%2C%20segmentation%20maps%2C%20and%20temporal%20structure.%20This%20model%20helps%0Aus%20show%20that%20pre-training%20on%20Satellogic%20data%20improves%20performance%20on%20downstream%0Atasks.%20While%20there%20is%20still%20a%20gap%20to%20fill%20in%20MAE%20for%20heterogeneous%20data%2C%20we%0Aregard%20this%20innovative%20combination%20of%20an%20expansive%2C%20diverse%20dataset%20and%20a%0Aversatile%20model%20adapted%20for%20self-supervised%20learning%20as%20a%20stride%20forward%20in%0Adeep%20learning%20for%20Earth%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08111v1&entry.124074799=Read"},
{"title": "FramePainter: Endowing Interactive Image Editing with Video Diffusion\n  Priors", "author": "Yabo Zhang and Xinpeng Zhou and Yihan Zeng and Hang Xu and Hui Li and Wangmeng Zuo", "abstract": "  Interactive image editing allows users to modify images through visual\ninteraction operations such as drawing, clicking, and dragging. Existing\nmethods construct such supervision signals from videos, as they capture how\nobjects change with various physical interactions. However, these models are\nusually built upon text-to-image diffusion models, so necessitate (i) massive\ntraining samples and (ii) an additional reference encoder to learn real-world\ndynamics and visual consistency. In this paper, we reformulate this task as an\nimage-to-video generation problem, so that inherit powerful video diffusion\npriors to reduce training costs and ensure temporal consistency. Specifically,\nwe introduce FramePainter as an efficient instantiation of this formulation.\nInitialized with Stable Video Diffusion, it only uses a lightweight sparse\ncontrol encoder to inject editing signals. Considering the limitations of\ntemporal attention in handling large motion between two frames, we further\npropose matching attention to enlarge the receptive field while encouraging\ndense correspondence between edited and source image tokens. We highlight the\neffectiveness and efficiency of FramePainter across various of editing signals:\nit domainantly outperforms previous state-of-the-art methods with far less\ntraining data, achieving highly seamless and coherent editing of images, \\eg,\nautomatically adjust the reflection of the cup. Moreover, FramePainter also\nexhibits exceptional generalization in scenarios not present in real-world\nvideos, \\eg, transform the clownfish into shark-like shape. Our code will be\navailable at https://github.com/YBYBZhang/FramePainter.\n", "link": "http://arxiv.org/abs/2501.08225v1", "date": "2025-01-14", "relevancy": 2.5652, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6665}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6546}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FramePainter%3A%20Endowing%20Interactive%20Image%20Editing%20with%20Video%20Diffusion%0A%20%20Priors&body=Title%3A%20FramePainter%3A%20Endowing%20Interactive%20Image%20Editing%20with%20Video%20Diffusion%0A%20%20Priors%0AAuthor%3A%20Yabo%20Zhang%20and%20Xinpeng%20Zhou%20and%20Yihan%20Zeng%20and%20Hang%20Xu%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Interactive%20image%20editing%20allows%20users%20to%20modify%20images%20through%20visual%0Ainteraction%20operations%20such%20as%20drawing%2C%20clicking%2C%20and%20dragging.%20Existing%0Amethods%20construct%20such%20supervision%20signals%20from%20videos%2C%20as%20they%20capture%20how%0Aobjects%20change%20with%20various%20physical%20interactions.%20However%2C%20these%20models%20are%0Ausually%20built%20upon%20text-to-image%20diffusion%20models%2C%20so%20necessitate%20%28i%29%20massive%0Atraining%20samples%20and%20%28ii%29%20an%20additional%20reference%20encoder%20to%20learn%20real-world%0Adynamics%20and%20visual%20consistency.%20In%20this%20paper%2C%20we%20reformulate%20this%20task%20as%20an%0Aimage-to-video%20generation%20problem%2C%20so%20that%20inherit%20powerful%20video%20diffusion%0Apriors%20to%20reduce%20training%20costs%20and%20ensure%20temporal%20consistency.%20Specifically%2C%0Awe%20introduce%20FramePainter%20as%20an%20efficient%20instantiation%20of%20this%20formulation.%0AInitialized%20with%20Stable%20Video%20Diffusion%2C%20it%20only%20uses%20a%20lightweight%20sparse%0Acontrol%20encoder%20to%20inject%20editing%20signals.%20Considering%20the%20limitations%20of%0Atemporal%20attention%20in%20handling%20large%20motion%20between%20two%20frames%2C%20we%20further%0Apropose%20matching%20attention%20to%20enlarge%20the%20receptive%20field%20while%20encouraging%0Adense%20correspondence%20between%20edited%20and%20source%20image%20tokens.%20We%20highlight%20the%0Aeffectiveness%20and%20efficiency%20of%20FramePainter%20across%20various%20of%20editing%20signals%3A%0Ait%20domainantly%20outperforms%20previous%20state-of-the-art%20methods%20with%20far%20less%0Atraining%20data%2C%20achieving%20highly%20seamless%20and%20coherent%20editing%20of%20images%2C%20%5Ceg%2C%0Aautomatically%20adjust%20the%20reflection%20of%20the%20cup.%20Moreover%2C%20FramePainter%20also%0Aexhibits%20exceptional%20generalization%20in%20scenarios%20not%20present%20in%20real-world%0Avideos%2C%20%5Ceg%2C%20transform%20the%20clownfish%20into%20shark-like%20shape.%20Our%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/YBYBZhang/FramePainter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramePainter%253A%2520Endowing%2520Interactive%2520Image%2520Editing%2520with%2520Video%2520Diffusion%250A%2520%2520Priors%26entry.906535625%3DYabo%2520Zhang%2520and%2520Xinpeng%2520Zhou%2520and%2520Yihan%2520Zeng%2520and%2520Hang%2520Xu%2520and%2520Hui%2520Li%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Interactive%2520image%2520editing%2520allows%2520users%2520to%2520modify%2520images%2520through%2520visual%250Ainteraction%2520operations%2520such%2520as%2520drawing%252C%2520clicking%252C%2520and%2520dragging.%2520Existing%250Amethods%2520construct%2520such%2520supervision%2520signals%2520from%2520videos%252C%2520as%2520they%2520capture%2520how%250Aobjects%2520change%2520with%2520various%2520physical%2520interactions.%2520However%252C%2520these%2520models%2520are%250Ausually%2520built%2520upon%2520text-to-image%2520diffusion%2520models%252C%2520so%2520necessitate%2520%2528i%2529%2520massive%250Atraining%2520samples%2520and%2520%2528ii%2529%2520an%2520additional%2520reference%2520encoder%2520to%2520learn%2520real-world%250Adynamics%2520and%2520visual%2520consistency.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520this%2520task%2520as%2520an%250Aimage-to-video%2520generation%2520problem%252C%2520so%2520that%2520inherit%2520powerful%2520video%2520diffusion%250Apriors%2520to%2520reduce%2520training%2520costs%2520and%2520ensure%2520temporal%2520consistency.%2520Specifically%252C%250Awe%2520introduce%2520FramePainter%2520as%2520an%2520efficient%2520instantiation%2520of%2520this%2520formulation.%250AInitialized%2520with%2520Stable%2520Video%2520Diffusion%252C%2520it%2520only%2520uses%2520a%2520lightweight%2520sparse%250Acontrol%2520encoder%2520to%2520inject%2520editing%2520signals.%2520Considering%2520the%2520limitations%2520of%250Atemporal%2520attention%2520in%2520handling%2520large%2520motion%2520between%2520two%2520frames%252C%2520we%2520further%250Apropose%2520matching%2520attention%2520to%2520enlarge%2520the%2520receptive%2520field%2520while%2520encouraging%250Adense%2520correspondence%2520between%2520edited%2520and%2520source%2520image%2520tokens.%2520We%2520highlight%2520the%250Aeffectiveness%2520and%2520efficiency%2520of%2520FramePainter%2520across%2520various%2520of%2520editing%2520signals%253A%250Ait%2520domainantly%2520outperforms%2520previous%2520state-of-the-art%2520methods%2520with%2520far%2520less%250Atraining%2520data%252C%2520achieving%2520highly%2520seamless%2520and%2520coherent%2520editing%2520of%2520images%252C%2520%255Ceg%252C%250Aautomatically%2520adjust%2520the%2520reflection%2520of%2520the%2520cup.%2520Moreover%252C%2520FramePainter%2520also%250Aexhibits%2520exceptional%2520generalization%2520in%2520scenarios%2520not%2520present%2520in%2520real-world%250Avideos%252C%2520%255Ceg%252C%2520transform%2520the%2520clownfish%2520into%2520shark-like%2520shape.%2520Our%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/YBYBZhang/FramePainter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FramePainter%3A%20Endowing%20Interactive%20Image%20Editing%20with%20Video%20Diffusion%0A%20%20Priors&entry.906535625=Yabo%20Zhang%20and%20Xinpeng%20Zhou%20and%20Yihan%20Zeng%20and%20Hang%20Xu%20and%20Hui%20Li%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Interactive%20image%20editing%20allows%20users%20to%20modify%20images%20through%20visual%0Ainteraction%20operations%20such%20as%20drawing%2C%20clicking%2C%20and%20dragging.%20Existing%0Amethods%20construct%20such%20supervision%20signals%20from%20videos%2C%20as%20they%20capture%20how%0Aobjects%20change%20with%20various%20physical%20interactions.%20However%2C%20these%20models%20are%0Ausually%20built%20upon%20text-to-image%20diffusion%20models%2C%20so%20necessitate%20%28i%29%20massive%0Atraining%20samples%20and%20%28ii%29%20an%20additional%20reference%20encoder%20to%20learn%20real-world%0Adynamics%20and%20visual%20consistency.%20In%20this%20paper%2C%20we%20reformulate%20this%20task%20as%20an%0Aimage-to-video%20generation%20problem%2C%20so%20that%20inherit%20powerful%20video%20diffusion%0Apriors%20to%20reduce%20training%20costs%20and%20ensure%20temporal%20consistency.%20Specifically%2C%0Awe%20introduce%20FramePainter%20as%20an%20efficient%20instantiation%20of%20this%20formulation.%0AInitialized%20with%20Stable%20Video%20Diffusion%2C%20it%20only%20uses%20a%20lightweight%20sparse%0Acontrol%20encoder%20to%20inject%20editing%20signals.%20Considering%20the%20limitations%20of%0Atemporal%20attention%20in%20handling%20large%20motion%20between%20two%20frames%2C%20we%20further%0Apropose%20matching%20attention%20to%20enlarge%20the%20receptive%20field%20while%20encouraging%0Adense%20correspondence%20between%20edited%20and%20source%20image%20tokens.%20We%20highlight%20the%0Aeffectiveness%20and%20efficiency%20of%20FramePainter%20across%20various%20of%20editing%20signals%3A%0Ait%20domainantly%20outperforms%20previous%20state-of-the-art%20methods%20with%20far%20less%0Atraining%20data%2C%20achieving%20highly%20seamless%20and%20coherent%20editing%20of%20images%2C%20%5Ceg%2C%0Aautomatically%20adjust%20the%20reflection%20of%20the%20cup.%20Moreover%2C%20FramePainter%20also%0Aexhibits%20exceptional%20generalization%20in%20scenarios%20not%20present%20in%20real-world%0Avideos%2C%20%5Ceg%2C%20transform%20the%20clownfish%20into%20shark-like%20shape.%20Our%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/YBYBZhang/FramePainter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08225v1&entry.124074799=Read"},
{"title": "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays", "author": "Mikko Heikkinen and Archontis Politis and Konstantinos Drossos and Tuomas Virtanen", "abstract": "  Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.\n", "link": "http://arxiv.org/abs/2501.08047v1", "date": "2025-01-14", "relevancy": 2.5287, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gen-A%3A%20Generalizing%20Ambisonics%20Neural%20Encoding%20to%20Unseen%20Microphone%0A%20%20Arrays&body=Title%3A%20Gen-A%3A%20Generalizing%20Ambisonics%20Neural%20Encoding%20to%20Unseen%20Microphone%0A%20%20Arrays%0AAuthor%3A%20Mikko%20Heikkinen%20and%20Archontis%20Politis%20and%20Konstantinos%20Drossos%20and%20Tuomas%20Virtanen%0AAbstract%3A%20%20%20Using%20deep%20neural%20networks%20%28DNNs%29%20for%20encoding%20of%20microphone%20array%20%28MA%29%0Asignals%20to%20the%20Ambisonics%20spatial%20audio%20format%20can%20surpass%20certain%20limitations%0Aof%20established%20conventional%20methods%2C%20but%20existing%20DNN-based%20methods%20need%20to%20be%0Atrained%20separately%20for%20each%20MA.%20This%20paper%20proposes%20a%20DNN-based%20method%20for%0AAmbisonics%20encoding%20that%20can%20generalize%20to%20arbitrary%20MA%20geometries%20unseen%0Aduring%20training.%20The%20method%20takes%20as%20inputs%20the%20MA%20geometry%20and%20MA%20signals%20and%0Auses%20a%20multi-level%20encoder%20consisting%20of%20separate%20paths%20for%20geometry%20and%20signal%0Adata%2C%20where%20geometry%20features%20inform%20the%20signal%20encoder%20at%20each%20level.%20The%0Amethod%20is%20validated%20in%20simulated%20anechoic%20and%20reverberant%20conditions%20with%20one%0Aand%20two%20sources.%20The%20results%20indicate%20improvement%20over%20conventional%20encoding%0Aacross%20the%20whole%20frequency%20range%20for%20dry%20scenes%2C%20while%20for%20reverberant%20scenes%0Athe%20improvement%20is%20frequency-dependent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGen-A%253A%2520Generalizing%2520Ambisonics%2520Neural%2520Encoding%2520to%2520Unseen%2520Microphone%250A%2520%2520Arrays%26entry.906535625%3DMikko%2520Heikkinen%2520and%2520Archontis%2520Politis%2520and%2520Konstantinos%2520Drossos%2520and%2520Tuomas%2520Virtanen%26entry.1292438233%3D%2520%2520Using%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520for%2520encoding%2520of%2520microphone%2520array%2520%2528MA%2529%250Asignals%2520to%2520the%2520Ambisonics%2520spatial%2520audio%2520format%2520can%2520surpass%2520certain%2520limitations%250Aof%2520established%2520conventional%2520methods%252C%2520but%2520existing%2520DNN-based%2520methods%2520need%2520to%2520be%250Atrained%2520separately%2520for%2520each%2520MA.%2520This%2520paper%2520proposes%2520a%2520DNN-based%2520method%2520for%250AAmbisonics%2520encoding%2520that%2520can%2520generalize%2520to%2520arbitrary%2520MA%2520geometries%2520unseen%250Aduring%2520training.%2520The%2520method%2520takes%2520as%2520inputs%2520the%2520MA%2520geometry%2520and%2520MA%2520signals%2520and%250Auses%2520a%2520multi-level%2520encoder%2520consisting%2520of%2520separate%2520paths%2520for%2520geometry%2520and%2520signal%250Adata%252C%2520where%2520geometry%2520features%2520inform%2520the%2520signal%2520encoder%2520at%2520each%2520level.%2520The%250Amethod%2520is%2520validated%2520in%2520simulated%2520anechoic%2520and%2520reverberant%2520conditions%2520with%2520one%250Aand%2520two%2520sources.%2520The%2520results%2520indicate%2520improvement%2520over%2520conventional%2520encoding%250Aacross%2520the%2520whole%2520frequency%2520range%2520for%2520dry%2520scenes%252C%2520while%2520for%2520reverberant%2520scenes%250Athe%2520improvement%2520is%2520frequency-dependent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen-A%3A%20Generalizing%20Ambisonics%20Neural%20Encoding%20to%20Unseen%20Microphone%0A%20%20Arrays&entry.906535625=Mikko%20Heikkinen%20and%20Archontis%20Politis%20and%20Konstantinos%20Drossos%20and%20Tuomas%20Virtanen&entry.1292438233=%20%20Using%20deep%20neural%20networks%20%28DNNs%29%20for%20encoding%20of%20microphone%20array%20%28MA%29%0Asignals%20to%20the%20Ambisonics%20spatial%20audio%20format%20can%20surpass%20certain%20limitations%0Aof%20established%20conventional%20methods%2C%20but%20existing%20DNN-based%20methods%20need%20to%20be%0Atrained%20separately%20for%20each%20MA.%20This%20paper%20proposes%20a%20DNN-based%20method%20for%0AAmbisonics%20encoding%20that%20can%20generalize%20to%20arbitrary%20MA%20geometries%20unseen%0Aduring%20training.%20The%20method%20takes%20as%20inputs%20the%20MA%20geometry%20and%20MA%20signals%20and%0Auses%20a%20multi-level%20encoder%20consisting%20of%20separate%20paths%20for%20geometry%20and%20signal%0Adata%2C%20where%20geometry%20features%20inform%20the%20signal%20encoder%20at%20each%20level.%20The%0Amethod%20is%20validated%20in%20simulated%20anechoic%20and%20reverberant%20conditions%20with%20one%0Aand%20two%20sources.%20The%20results%20indicate%20improvement%20over%20conventional%20encoding%0Aacross%20the%20whole%20frequency%20range%20for%20dry%20scenes%2C%20while%20for%20reverberant%20scenes%0Athe%20improvement%20is%20frequency-dependent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08047v1&entry.124074799=Read"},
{"title": "Revolutionizing Communication with Deep Learning and XAI for Enhanced\n  Arabic Sign Language Recognition", "author": "Mazen Balat and Rewaa Awaad and Ahmed B. Zaky and Salah A. Aly", "abstract": "  This study introduces an integrated approach to recognizing Arabic Sign\nLanguage (ArSL) using state-of-the-art deep learning models such as\nMobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced\nby explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and\nRGB Arabic Alphabets Sign Language (AASL) datasets are employed, with\nEfficientNet-B2 achieving peak accuracies of 99.48\\% and 98.99\\%, respectively.\nKey innovations include sophisticated data augmentation methods to mitigate\nclass imbalance, implementation of stratified 5-fold cross-validation for\nbetter generalization, and the use of Grad-CAM for clear model decision\ntransparency. The proposed system not only sets new benchmarks in recognition\naccuracy but also emphasizes interpretability, making it suitable for\napplications in healthcare, education, and inclusive communication\ntechnologies.\n", "link": "http://arxiv.org/abs/2501.08169v1", "date": "2025-01-14", "relevancy": 2.5127, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revolutionizing%20Communication%20with%20Deep%20Learning%20and%20XAI%20for%20Enhanced%0A%20%20Arabic%20Sign%20Language%20Recognition&body=Title%3A%20Revolutionizing%20Communication%20with%20Deep%20Learning%20and%20XAI%20for%20Enhanced%0A%20%20Arabic%20Sign%20Language%20Recognition%0AAuthor%3A%20Mazen%20Balat%20and%20Rewaa%20Awaad%20and%20Ahmed%20B.%20Zaky%20and%20Salah%20A.%20Aly%0AAbstract%3A%20%20%20This%20study%20introduces%20an%20integrated%20approach%20to%20recognizing%20Arabic%20Sign%0ALanguage%20%28ArSL%29%20using%20state-of-the-art%20deep%20learning%20models%20such%20as%0AMobileNetV3%2C%20ResNet50%2C%20and%20EfficientNet-B2.%20These%20models%20are%20further%20enhanced%0Aby%20explainable%20AI%20%28XAI%29%20techniques%20to%20boost%20interpretability.%20The%20ArSL2018%20and%0ARGB%20Arabic%20Alphabets%20Sign%20Language%20%28AASL%29%20datasets%20are%20employed%2C%20with%0AEfficientNet-B2%20achieving%20peak%20accuracies%20of%2099.48%5C%25%20and%2098.99%5C%25%2C%20respectively.%0AKey%20innovations%20include%20sophisticated%20data%20augmentation%20methods%20to%20mitigate%0Aclass%20imbalance%2C%20implementation%20of%20stratified%205-fold%20cross-validation%20for%0Abetter%20generalization%2C%20and%20the%20use%20of%20Grad-CAM%20for%20clear%20model%20decision%0Atransparency.%20The%20proposed%20system%20not%20only%20sets%20new%20benchmarks%20in%20recognition%0Aaccuracy%20but%20also%20emphasizes%20interpretability%2C%20making%20it%20suitable%20for%0Aapplications%20in%20healthcare%2C%20education%2C%20and%20inclusive%20communication%0Atechnologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevolutionizing%2520Communication%2520with%2520Deep%2520Learning%2520and%2520XAI%2520for%2520Enhanced%250A%2520%2520Arabic%2520Sign%2520Language%2520Recognition%26entry.906535625%3DMazen%2520Balat%2520and%2520Rewaa%2520Awaad%2520and%2520Ahmed%2520B.%2520Zaky%2520and%2520Salah%2520A.%2520Aly%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520an%2520integrated%2520approach%2520to%2520recognizing%2520Arabic%2520Sign%250ALanguage%2520%2528ArSL%2529%2520using%2520state-of-the-art%2520deep%2520learning%2520models%2520such%2520as%250AMobileNetV3%252C%2520ResNet50%252C%2520and%2520EfficientNet-B2.%2520These%2520models%2520are%2520further%2520enhanced%250Aby%2520explainable%2520AI%2520%2528XAI%2529%2520techniques%2520to%2520boost%2520interpretability.%2520The%2520ArSL2018%2520and%250ARGB%2520Arabic%2520Alphabets%2520Sign%2520Language%2520%2528AASL%2529%2520datasets%2520are%2520employed%252C%2520with%250AEfficientNet-B2%2520achieving%2520peak%2520accuracies%2520of%252099.48%255C%2525%2520and%252098.99%255C%2525%252C%2520respectively.%250AKey%2520innovations%2520include%2520sophisticated%2520data%2520augmentation%2520methods%2520to%2520mitigate%250Aclass%2520imbalance%252C%2520implementation%2520of%2520stratified%25205-fold%2520cross-validation%2520for%250Abetter%2520generalization%252C%2520and%2520the%2520use%2520of%2520Grad-CAM%2520for%2520clear%2520model%2520decision%250Atransparency.%2520The%2520proposed%2520system%2520not%2520only%2520sets%2520new%2520benchmarks%2520in%2520recognition%250Aaccuracy%2520but%2520also%2520emphasizes%2520interpretability%252C%2520making%2520it%2520suitable%2520for%250Aapplications%2520in%2520healthcare%252C%2520education%252C%2520and%2520inclusive%2520communication%250Atechnologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revolutionizing%20Communication%20with%20Deep%20Learning%20and%20XAI%20for%20Enhanced%0A%20%20Arabic%20Sign%20Language%20Recognition&entry.906535625=Mazen%20Balat%20and%20Rewaa%20Awaad%20and%20Ahmed%20B.%20Zaky%20and%20Salah%20A.%20Aly&entry.1292438233=%20%20This%20study%20introduces%20an%20integrated%20approach%20to%20recognizing%20Arabic%20Sign%0ALanguage%20%28ArSL%29%20using%20state-of-the-art%20deep%20learning%20models%20such%20as%0AMobileNetV3%2C%20ResNet50%2C%20and%20EfficientNet-B2.%20These%20models%20are%20further%20enhanced%0Aby%20explainable%20AI%20%28XAI%29%20techniques%20to%20boost%20interpretability.%20The%20ArSL2018%20and%0ARGB%20Arabic%20Alphabets%20Sign%20Language%20%28AASL%29%20datasets%20are%20employed%2C%20with%0AEfficientNet-B2%20achieving%20peak%20accuracies%20of%2099.48%5C%25%20and%2098.99%5C%25%2C%20respectively.%0AKey%20innovations%20include%20sophisticated%20data%20augmentation%20methods%20to%20mitigate%0Aclass%20imbalance%2C%20implementation%20of%20stratified%205-fold%20cross-validation%20for%0Abetter%20generalization%2C%20and%20the%20use%20of%20Grad-CAM%20for%20clear%20model%20decision%0Atransparency.%20The%20proposed%20system%20not%20only%20sets%20new%20benchmarks%20in%20recognition%0Aaccuracy%20but%20also%20emphasizes%20interpretability%2C%20making%20it%20suitable%20for%0Aapplications%20in%20healthcare%2C%20education%2C%20and%20inclusive%20communication%0Atechnologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08169v1&entry.124074799=Read"},
{"title": "Decoding Interpretable Logic Rules from Neural Networks", "author": "Chuqin Geng and Xiaojie Xu and Zhaoyue Wang and Ziyu Zhao and Xujie Si", "abstract": "  As deep neural networks continue to excel across various domains, their\nblack-box nature has raised concerns about transparency and trust. In\nparticular, interpretability has become increasingly essential for applications\nthat demand high safety and knowledge rigor, such as drug discovery, autonomous\ndriving, and genomics. However, progress in understanding even the simplest\ndeep neural networks - such as fully connected networks - has been limited,\ndespite their role as foundational elements in state-of-the-art models like\nResNet and Transformer. In this paper, we address this challenge by introducing\nNeuroLogic, a novel approach for decoding interpretable logic rules from neural\nnetworks. NeuroLogic leverages neural activation patterns to capture the\nmodel's critical decision-making processes, translating them into logical rules\nrepresented by hidden predicates. Thanks to its flexible design in the\ngrounding phase, NeuroLogic can be adapted to a wide range of neural networks.\nFor simple fully connected neural networks, hidden predicates can be grounded\nin certain split patterns of original input features to derive\ndecision-tree-like rules. For large, complex vision neural networks, NeuroLogic\ngrounds hidden predicates into high-level visual concepts that are\nunderstandable to humans. Our empirical study demonstrates that NeuroLogic can\nextract global and interpretable rules from state-of-the-art models such as\nResNet, a task at which existing work struggles. We believe NeuroLogic can help\npave the way for understanding the black-box nature of neural networks.\n", "link": "http://arxiv.org/abs/2501.08281v1", "date": "2025-01-14", "relevancy": 2.5059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5165}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Interpretable%20Logic%20Rules%20from%20Neural%20Networks&body=Title%3A%20Decoding%20Interpretable%20Logic%20Rules%20from%20Neural%20Networks%0AAuthor%3A%20Chuqin%20Geng%20and%20Xiaojie%20Xu%20and%20Zhaoyue%20Wang%20and%20Ziyu%20Zhao%20and%20Xujie%20Si%0AAbstract%3A%20%20%20As%20deep%20neural%20networks%20continue%20to%20excel%20across%20various%20domains%2C%20their%0Ablack-box%20nature%20has%20raised%20concerns%20about%20transparency%20and%20trust.%20In%0Aparticular%2C%20interpretability%20has%20become%20increasingly%20essential%20for%20applications%0Athat%20demand%20high%20safety%20and%20knowledge%20rigor%2C%20such%20as%20drug%20discovery%2C%20autonomous%0Adriving%2C%20and%20genomics.%20However%2C%20progress%20in%20understanding%20even%20the%20simplest%0Adeep%20neural%20networks%20-%20such%20as%20fully%20connected%20networks%20-%20has%20been%20limited%2C%0Adespite%20their%20role%20as%20foundational%20elements%20in%20state-of-the-art%20models%20like%0AResNet%20and%20Transformer.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20introducing%0ANeuroLogic%2C%20a%20novel%20approach%20for%20decoding%20interpretable%20logic%20rules%20from%20neural%0Anetworks.%20NeuroLogic%20leverages%20neural%20activation%20patterns%20to%20capture%20the%0Amodel%27s%20critical%20decision-making%20processes%2C%20translating%20them%20into%20logical%20rules%0Arepresented%20by%20hidden%20predicates.%20Thanks%20to%20its%20flexible%20design%20in%20the%0Agrounding%20phase%2C%20NeuroLogic%20can%20be%20adapted%20to%20a%20wide%20range%20of%20neural%20networks.%0AFor%20simple%20fully%20connected%20neural%20networks%2C%20hidden%20predicates%20can%20be%20grounded%0Ain%20certain%20split%20patterns%20of%20original%20input%20features%20to%20derive%0Adecision-tree-like%20rules.%20For%20large%2C%20complex%20vision%20neural%20networks%2C%20NeuroLogic%0Agrounds%20hidden%20predicates%20into%20high-level%20visual%20concepts%20that%20are%0Aunderstandable%20to%20humans.%20Our%20empirical%20study%20demonstrates%20that%20NeuroLogic%20can%0Aextract%20global%20and%20interpretable%20rules%20from%20state-of-the-art%20models%20such%20as%0AResNet%2C%20a%20task%20at%20which%20existing%20work%20struggles.%20We%20believe%20NeuroLogic%20can%20help%0Apave%20the%20way%20for%20understanding%20the%20black-box%20nature%20of%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Interpretable%2520Logic%2520Rules%2520from%2520Neural%2520Networks%26entry.906535625%3DChuqin%2520Geng%2520and%2520Xiaojie%2520Xu%2520and%2520Zhaoyue%2520Wang%2520and%2520Ziyu%2520Zhao%2520and%2520Xujie%2520Si%26entry.1292438233%3D%2520%2520As%2520deep%2520neural%2520networks%2520continue%2520to%2520excel%2520across%2520various%2520domains%252C%2520their%250Ablack-box%2520nature%2520has%2520raised%2520concerns%2520about%2520transparency%2520and%2520trust.%2520In%250Aparticular%252C%2520interpretability%2520has%2520become%2520increasingly%2520essential%2520for%2520applications%250Athat%2520demand%2520high%2520safety%2520and%2520knowledge%2520rigor%252C%2520such%2520as%2520drug%2520discovery%252C%2520autonomous%250Adriving%252C%2520and%2520genomics.%2520However%252C%2520progress%2520in%2520understanding%2520even%2520the%2520simplest%250Adeep%2520neural%2520networks%2520-%2520such%2520as%2520fully%2520connected%2520networks%2520-%2520has%2520been%2520limited%252C%250Adespite%2520their%2520role%2520as%2520foundational%2520elements%2520in%2520state-of-the-art%2520models%2520like%250AResNet%2520and%2520Transformer.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%250ANeuroLogic%252C%2520a%2520novel%2520approach%2520for%2520decoding%2520interpretable%2520logic%2520rules%2520from%2520neural%250Anetworks.%2520NeuroLogic%2520leverages%2520neural%2520activation%2520patterns%2520to%2520capture%2520the%250Amodel%2527s%2520critical%2520decision-making%2520processes%252C%2520translating%2520them%2520into%2520logical%2520rules%250Arepresented%2520by%2520hidden%2520predicates.%2520Thanks%2520to%2520its%2520flexible%2520design%2520in%2520the%250Agrounding%2520phase%252C%2520NeuroLogic%2520can%2520be%2520adapted%2520to%2520a%2520wide%2520range%2520of%2520neural%2520networks.%250AFor%2520simple%2520fully%2520connected%2520neural%2520networks%252C%2520hidden%2520predicates%2520can%2520be%2520grounded%250Ain%2520certain%2520split%2520patterns%2520of%2520original%2520input%2520features%2520to%2520derive%250Adecision-tree-like%2520rules.%2520For%2520large%252C%2520complex%2520vision%2520neural%2520networks%252C%2520NeuroLogic%250Agrounds%2520hidden%2520predicates%2520into%2520high-level%2520visual%2520concepts%2520that%2520are%250Aunderstandable%2520to%2520humans.%2520Our%2520empirical%2520study%2520demonstrates%2520that%2520NeuroLogic%2520can%250Aextract%2520global%2520and%2520interpretable%2520rules%2520from%2520state-of-the-art%2520models%2520such%2520as%250AResNet%252C%2520a%2520task%2520at%2520which%2520existing%2520work%2520struggles.%2520We%2520believe%2520NeuroLogic%2520can%2520help%250Apave%2520the%2520way%2520for%2520understanding%2520the%2520black-box%2520nature%2520of%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Interpretable%20Logic%20Rules%20from%20Neural%20Networks&entry.906535625=Chuqin%20Geng%20and%20Xiaojie%20Xu%20and%20Zhaoyue%20Wang%20and%20Ziyu%20Zhao%20and%20Xujie%20Si&entry.1292438233=%20%20As%20deep%20neural%20networks%20continue%20to%20excel%20across%20various%20domains%2C%20their%0Ablack-box%20nature%20has%20raised%20concerns%20about%20transparency%20and%20trust.%20In%0Aparticular%2C%20interpretability%20has%20become%20increasingly%20essential%20for%20applications%0Athat%20demand%20high%20safety%20and%20knowledge%20rigor%2C%20such%20as%20drug%20discovery%2C%20autonomous%0Adriving%2C%20and%20genomics.%20However%2C%20progress%20in%20understanding%20even%20the%20simplest%0Adeep%20neural%20networks%20-%20such%20as%20fully%20connected%20networks%20-%20has%20been%20limited%2C%0Adespite%20their%20role%20as%20foundational%20elements%20in%20state-of-the-art%20models%20like%0AResNet%20and%20Transformer.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20introducing%0ANeuroLogic%2C%20a%20novel%20approach%20for%20decoding%20interpretable%20logic%20rules%20from%20neural%0Anetworks.%20NeuroLogic%20leverages%20neural%20activation%20patterns%20to%20capture%20the%0Amodel%27s%20critical%20decision-making%20processes%2C%20translating%20them%20into%20logical%20rules%0Arepresented%20by%20hidden%20predicates.%20Thanks%20to%20its%20flexible%20design%20in%20the%0Agrounding%20phase%2C%20NeuroLogic%20can%20be%20adapted%20to%20a%20wide%20range%20of%20neural%20networks.%0AFor%20simple%20fully%20connected%20neural%20networks%2C%20hidden%20predicates%20can%20be%20grounded%0Ain%20certain%20split%20patterns%20of%20original%20input%20features%20to%20derive%0Adecision-tree-like%20rules.%20For%20large%2C%20complex%20vision%20neural%20networks%2C%20NeuroLogic%0Agrounds%20hidden%20predicates%20into%20high-level%20visual%20concepts%20that%20are%0Aunderstandable%20to%20humans.%20Our%20empirical%20study%20demonstrates%20that%20NeuroLogic%20can%0Aextract%20global%20and%20interpretable%20rules%20from%20state-of-the-art%20models%20such%20as%0AResNet%2C%20a%20task%20at%20which%20existing%20work%20struggles.%20We%20believe%20NeuroLogic%20can%20help%0Apave%20the%20way%20for%20understanding%20the%20black-box%20nature%20of%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08281v1&entry.124074799=Read"},
{"title": "Skeleton and Font Generation Network for Zero-shot Chinese Character\n  Generation", "author": "Mobai Xue and Jun Du and Zhenrong Zhang and Jiefeng Ma and Qikai Chang and Pengfei Hu and Jianshu Zhang and Yu Hu", "abstract": "  Automatic font generation remains a challenging research issue, primarily due\nto the vast number of Chinese characters, each with unique and intricate\nstructures. Our investigation of previous studies reveals inherent bias capable\nof causing structural changes in characters. Specifically, when generating a\nChinese character similar to, but different from, those in the training\nsamples, the bias is prone to either correcting or ignoring these subtle\nvariations. To address this concern, we propose a novel Skeleton and Font\nGeneration Network (SFGN) to achieve a more robust Chinese character font\ngeneration. Our approach includes a skeleton builder and font generator. The\nskeleton builder synthesizes content features using low-resource text input,\nenabling our technique to realize font generation independently of content\nimage inputs. Unlike previous font generation methods that treat font style as\na global embedding, we introduce a font generator to align content and style\nfeatures on the radical level, which is a brand-new perspective for font\ngeneration. Except for common characters, we also conduct experiments on\nmisspelled characters, a substantial portion of which slightly differs from the\ncommon ones. Our approach visually demonstrates the efficacy of generated\nimages and outperforms current state-of-the-art font generation methods.\nMoreover, we believe that misspelled character generation have significant\npedagogical implications and verify such supposition through experiments. We\nused generated misspelled characters as data augmentation in Chinese character\nerror correction tasks, simulating the scenario where students learn\nhandwritten Chinese characters with the help of misspelled characters. The\nsignificantly improved performance of error correction tasks demonstrates the\neffectiveness of our proposed approach and the value of misspelled character\ngeneration.\n", "link": "http://arxiv.org/abs/2501.08062v1", "date": "2025-01-14", "relevancy": 2.4884, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5145}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4902}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton%20and%20Font%20Generation%20Network%20for%20Zero-shot%20Chinese%20Character%0A%20%20Generation&body=Title%3A%20Skeleton%20and%20Font%20Generation%20Network%20for%20Zero-shot%20Chinese%20Character%0A%20%20Generation%0AAuthor%3A%20Mobai%20Xue%20and%20Jun%20Du%20and%20Zhenrong%20Zhang%20and%20Jiefeng%20Ma%20and%20Qikai%20Chang%20and%20Pengfei%20Hu%20and%20Jianshu%20Zhang%20and%20Yu%20Hu%0AAbstract%3A%20%20%20Automatic%20font%20generation%20remains%20a%20challenging%20research%20issue%2C%20primarily%20due%0Ato%20the%20vast%20number%20of%20Chinese%20characters%2C%20each%20with%20unique%20and%20intricate%0Astructures.%20Our%20investigation%20of%20previous%20studies%20reveals%20inherent%20bias%20capable%0Aof%20causing%20structural%20changes%20in%20characters.%20Specifically%2C%20when%20generating%20a%0AChinese%20character%20similar%20to%2C%20but%20different%20from%2C%20those%20in%20the%20training%0Asamples%2C%20the%20bias%20is%20prone%20to%20either%20correcting%20or%20ignoring%20these%20subtle%0Avariations.%20To%20address%20this%20concern%2C%20we%20propose%20a%20novel%20Skeleton%20and%20Font%0AGeneration%20Network%20%28SFGN%29%20to%20achieve%20a%20more%20robust%20Chinese%20character%20font%0Ageneration.%20Our%20approach%20includes%20a%20skeleton%20builder%20and%20font%20generator.%20The%0Askeleton%20builder%20synthesizes%20content%20features%20using%20low-resource%20text%20input%2C%0Aenabling%20our%20technique%20to%20realize%20font%20generation%20independently%20of%20content%0Aimage%20inputs.%20Unlike%20previous%20font%20generation%20methods%20that%20treat%20font%20style%20as%0Aa%20global%20embedding%2C%20we%20introduce%20a%20font%20generator%20to%20align%20content%20and%20style%0Afeatures%20on%20the%20radical%20level%2C%20which%20is%20a%20brand-new%20perspective%20for%20font%0Ageneration.%20Except%20for%20common%20characters%2C%20we%20also%20conduct%20experiments%20on%0Amisspelled%20characters%2C%20a%20substantial%20portion%20of%20which%20slightly%20differs%20from%20the%0Acommon%20ones.%20Our%20approach%20visually%20demonstrates%20the%20efficacy%20of%20generated%0Aimages%20and%20outperforms%20current%20state-of-the-art%20font%20generation%20methods.%0AMoreover%2C%20we%20believe%20that%20misspelled%20character%20generation%20have%20significant%0Apedagogical%20implications%20and%20verify%20such%20supposition%20through%20experiments.%20We%0Aused%20generated%20misspelled%20characters%20as%20data%20augmentation%20in%20Chinese%20character%0Aerror%20correction%20tasks%2C%20simulating%20the%20scenario%20where%20students%20learn%0Ahandwritten%20Chinese%20characters%20with%20the%20help%20of%20misspelled%20characters.%20The%0Asignificantly%20improved%20performance%20of%20error%20correction%20tasks%20demonstrates%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20and%20the%20value%20of%20misspelled%20character%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton%2520and%2520Font%2520Generation%2520Network%2520for%2520Zero-shot%2520Chinese%2520Character%250A%2520%2520Generation%26entry.906535625%3DMobai%2520Xue%2520and%2520Jun%2520Du%2520and%2520Zhenrong%2520Zhang%2520and%2520Jiefeng%2520Ma%2520and%2520Qikai%2520Chang%2520and%2520Pengfei%2520Hu%2520and%2520Jianshu%2520Zhang%2520and%2520Yu%2520Hu%26entry.1292438233%3D%2520%2520Automatic%2520font%2520generation%2520remains%2520a%2520challenging%2520research%2520issue%252C%2520primarily%2520due%250Ato%2520the%2520vast%2520number%2520of%2520Chinese%2520characters%252C%2520each%2520with%2520unique%2520and%2520intricate%250Astructures.%2520Our%2520investigation%2520of%2520previous%2520studies%2520reveals%2520inherent%2520bias%2520capable%250Aof%2520causing%2520structural%2520changes%2520in%2520characters.%2520Specifically%252C%2520when%2520generating%2520a%250AChinese%2520character%2520similar%2520to%252C%2520but%2520different%2520from%252C%2520those%2520in%2520the%2520training%250Asamples%252C%2520the%2520bias%2520is%2520prone%2520to%2520either%2520correcting%2520or%2520ignoring%2520these%2520subtle%250Avariations.%2520To%2520address%2520this%2520concern%252C%2520we%2520propose%2520a%2520novel%2520Skeleton%2520and%2520Font%250AGeneration%2520Network%2520%2528SFGN%2529%2520to%2520achieve%2520a%2520more%2520robust%2520Chinese%2520character%2520font%250Ageneration.%2520Our%2520approach%2520includes%2520a%2520skeleton%2520builder%2520and%2520font%2520generator.%2520The%250Askeleton%2520builder%2520synthesizes%2520content%2520features%2520using%2520low-resource%2520text%2520input%252C%250Aenabling%2520our%2520technique%2520to%2520realize%2520font%2520generation%2520independently%2520of%2520content%250Aimage%2520inputs.%2520Unlike%2520previous%2520font%2520generation%2520methods%2520that%2520treat%2520font%2520style%2520as%250Aa%2520global%2520embedding%252C%2520we%2520introduce%2520a%2520font%2520generator%2520to%2520align%2520content%2520and%2520style%250Afeatures%2520on%2520the%2520radical%2520level%252C%2520which%2520is%2520a%2520brand-new%2520perspective%2520for%2520font%250Ageneration.%2520Except%2520for%2520common%2520characters%252C%2520we%2520also%2520conduct%2520experiments%2520on%250Amisspelled%2520characters%252C%2520a%2520substantial%2520portion%2520of%2520which%2520slightly%2520differs%2520from%2520the%250Acommon%2520ones.%2520Our%2520approach%2520visually%2520demonstrates%2520the%2520efficacy%2520of%2520generated%250Aimages%2520and%2520outperforms%2520current%2520state-of-the-art%2520font%2520generation%2520methods.%250AMoreover%252C%2520we%2520believe%2520that%2520misspelled%2520character%2520generation%2520have%2520significant%250Apedagogical%2520implications%2520and%2520verify%2520such%2520supposition%2520through%2520experiments.%2520We%250Aused%2520generated%2520misspelled%2520characters%2520as%2520data%2520augmentation%2520in%2520Chinese%2520character%250Aerror%2520correction%2520tasks%252C%2520simulating%2520the%2520scenario%2520where%2520students%2520learn%250Ahandwritten%2520Chinese%2520characters%2520with%2520the%2520help%2520of%2520misspelled%2520characters.%2520The%250Asignificantly%2520improved%2520performance%2520of%2520error%2520correction%2520tasks%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520approach%2520and%2520the%2520value%2520of%2520misspelled%2520character%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton%20and%20Font%20Generation%20Network%20for%20Zero-shot%20Chinese%20Character%0A%20%20Generation&entry.906535625=Mobai%20Xue%20and%20Jun%20Du%20and%20Zhenrong%20Zhang%20and%20Jiefeng%20Ma%20and%20Qikai%20Chang%20and%20Pengfei%20Hu%20and%20Jianshu%20Zhang%20and%20Yu%20Hu&entry.1292438233=%20%20Automatic%20font%20generation%20remains%20a%20challenging%20research%20issue%2C%20primarily%20due%0Ato%20the%20vast%20number%20of%20Chinese%20characters%2C%20each%20with%20unique%20and%20intricate%0Astructures.%20Our%20investigation%20of%20previous%20studies%20reveals%20inherent%20bias%20capable%0Aof%20causing%20structural%20changes%20in%20characters.%20Specifically%2C%20when%20generating%20a%0AChinese%20character%20similar%20to%2C%20but%20different%20from%2C%20those%20in%20the%20training%0Asamples%2C%20the%20bias%20is%20prone%20to%20either%20correcting%20or%20ignoring%20these%20subtle%0Avariations.%20To%20address%20this%20concern%2C%20we%20propose%20a%20novel%20Skeleton%20and%20Font%0AGeneration%20Network%20%28SFGN%29%20to%20achieve%20a%20more%20robust%20Chinese%20character%20font%0Ageneration.%20Our%20approach%20includes%20a%20skeleton%20builder%20and%20font%20generator.%20The%0Askeleton%20builder%20synthesizes%20content%20features%20using%20low-resource%20text%20input%2C%0Aenabling%20our%20technique%20to%20realize%20font%20generation%20independently%20of%20content%0Aimage%20inputs.%20Unlike%20previous%20font%20generation%20methods%20that%20treat%20font%20style%20as%0Aa%20global%20embedding%2C%20we%20introduce%20a%20font%20generator%20to%20align%20content%20and%20style%0Afeatures%20on%20the%20radical%20level%2C%20which%20is%20a%20brand-new%20perspective%20for%20font%0Ageneration.%20Except%20for%20common%20characters%2C%20we%20also%20conduct%20experiments%20on%0Amisspelled%20characters%2C%20a%20substantial%20portion%20of%20which%20slightly%20differs%20from%20the%0Acommon%20ones.%20Our%20approach%20visually%20demonstrates%20the%20efficacy%20of%20generated%0Aimages%20and%20outperforms%20current%20state-of-the-art%20font%20generation%20methods.%0AMoreover%2C%20we%20believe%20that%20misspelled%20character%20generation%20have%20significant%0Apedagogical%20implications%20and%20verify%20such%20supposition%20through%20experiments.%20We%0Aused%20generated%20misspelled%20characters%20as%20data%20augmentation%20in%20Chinese%20character%0Aerror%20correction%20tasks%2C%20simulating%20the%20scenario%20where%20students%20learn%0Ahandwritten%20Chinese%20characters%20with%20the%20help%20of%20misspelled%20characters.%20The%0Asignificantly%20improved%20performance%20of%20error%20correction%20tasks%20demonstrates%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20and%20the%20value%20of%20misspelled%20character%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08062v1&entry.124074799=Read"},
{"title": "EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition", "author": "Yassine El Boudouri and Amine Bohi", "abstract": "  Facial expressions play a crucial role in human communication serving as a\npowerful and impactful means to express a wide range of emotions. With\nadvancements in artificial intelligence and computer vision, deep neural\nnetworks have emerged as effective tools for facial emotion recognition. In\nthis paper, we propose EmoNeXt, a novel deep learning framework for facial\nexpression recognition based on an adapted ConvNeXt architecture network. We\nintegrate a Spatial Transformer Network (STN) to focus on feature-rich regions\nof the face and Squeeze-and-Excitation blocks to capture channel-wise\ndependencies. Moreover, we introduce a self-attention regularization term,\nencouraging the model to generate compact feature vectors. We demonstrate the\nsuperiority of our model over existing state-of-the-art deep learning models on\nthe FER2013 dataset regarding emotion classification accuracy.\n", "link": "http://arxiv.org/abs/2501.08199v1", "date": "2025-01-14", "relevancy": 2.4723, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5254}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoNeXt%3A%20an%20Adapted%20ConvNeXt%20for%20Facial%20Emotion%20Recognition&body=Title%3A%20EmoNeXt%3A%20an%20Adapted%20ConvNeXt%20for%20Facial%20Emotion%20Recognition%0AAuthor%3A%20Yassine%20El%20Boudouri%20and%20Amine%20Bohi%0AAbstract%3A%20%20%20Facial%20expressions%20play%20a%20crucial%20role%20in%20human%20communication%20serving%20as%20a%0Apowerful%20and%20impactful%20means%20to%20express%20a%20wide%20range%20of%20emotions.%20With%0Aadvancements%20in%20artificial%20intelligence%20and%20computer%20vision%2C%20deep%20neural%0Anetworks%20have%20emerged%20as%20effective%20tools%20for%20facial%20emotion%20recognition.%20In%0Athis%20paper%2C%20we%20propose%20EmoNeXt%2C%20a%20novel%20deep%20learning%20framework%20for%20facial%0Aexpression%20recognition%20based%20on%20an%20adapted%20ConvNeXt%20architecture%20network.%20We%0Aintegrate%20a%20Spatial%20Transformer%20Network%20%28STN%29%20to%20focus%20on%20feature-rich%20regions%0Aof%20the%20face%20and%20Squeeze-and-Excitation%20blocks%20to%20capture%20channel-wise%0Adependencies.%20Moreover%2C%20we%20introduce%20a%20self-attention%20regularization%20term%2C%0Aencouraging%20the%20model%20to%20generate%20compact%20feature%20vectors.%20We%20demonstrate%20the%0Asuperiority%20of%20our%20model%20over%20existing%20state-of-the-art%20deep%20learning%20models%20on%0Athe%20FER2013%20dataset%20regarding%20emotion%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoNeXt%253A%2520an%2520Adapted%2520ConvNeXt%2520for%2520Facial%2520Emotion%2520Recognition%26entry.906535625%3DYassine%2520El%2520Boudouri%2520and%2520Amine%2520Bohi%26entry.1292438233%3D%2520%2520Facial%2520expressions%2520play%2520a%2520crucial%2520role%2520in%2520human%2520communication%2520serving%2520as%2520a%250Apowerful%2520and%2520impactful%2520means%2520to%2520express%2520a%2520wide%2520range%2520of%2520emotions.%2520With%250Aadvancements%2520in%2520artificial%2520intelligence%2520and%2520computer%2520vision%252C%2520deep%2520neural%250Anetworks%2520have%2520emerged%2520as%2520effective%2520tools%2520for%2520facial%2520emotion%2520recognition.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520EmoNeXt%252C%2520a%2520novel%2520deep%2520learning%2520framework%2520for%2520facial%250Aexpression%2520recognition%2520based%2520on%2520an%2520adapted%2520ConvNeXt%2520architecture%2520network.%2520We%250Aintegrate%2520a%2520Spatial%2520Transformer%2520Network%2520%2528STN%2529%2520to%2520focus%2520on%2520feature-rich%2520regions%250Aof%2520the%2520face%2520and%2520Squeeze-and-Excitation%2520blocks%2520to%2520capture%2520channel-wise%250Adependencies.%2520Moreover%252C%2520we%2520introduce%2520a%2520self-attention%2520regularization%2520term%252C%250Aencouraging%2520the%2520model%2520to%2520generate%2520compact%2520feature%2520vectors.%2520We%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520model%2520over%2520existing%2520state-of-the-art%2520deep%2520learning%2520models%2520on%250Athe%2520FER2013%2520dataset%2520regarding%2520emotion%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoNeXt%3A%20an%20Adapted%20ConvNeXt%20for%20Facial%20Emotion%20Recognition&entry.906535625=Yassine%20El%20Boudouri%20and%20Amine%20Bohi&entry.1292438233=%20%20Facial%20expressions%20play%20a%20crucial%20role%20in%20human%20communication%20serving%20as%20a%0Apowerful%20and%20impactful%20means%20to%20express%20a%20wide%20range%20of%20emotions.%20With%0Aadvancements%20in%20artificial%20intelligence%20and%20computer%20vision%2C%20deep%20neural%0Anetworks%20have%20emerged%20as%20effective%20tools%20for%20facial%20emotion%20recognition.%20In%0Athis%20paper%2C%20we%20propose%20EmoNeXt%2C%20a%20novel%20deep%20learning%20framework%20for%20facial%0Aexpression%20recognition%20based%20on%20an%20adapted%20ConvNeXt%20architecture%20network.%20We%0Aintegrate%20a%20Spatial%20Transformer%20Network%20%28STN%29%20to%20focus%20on%20feature-rich%20regions%0Aof%20the%20face%20and%20Squeeze-and-Excitation%20blocks%20to%20capture%20channel-wise%0Adependencies.%20Moreover%2C%20we%20introduce%20a%20self-attention%20regularization%20term%2C%0Aencouraging%20the%20model%20to%20generate%20compact%20feature%20vectors.%20We%20demonstrate%20the%0Asuperiority%20of%20our%20model%20over%20existing%20state-of-the-art%20deep%20learning%20models%20on%0Athe%20FER2013%20dataset%20regarding%20emotion%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08199v1&entry.124074799=Read"},
{"title": "Less is More: The Influence of Pruning on the Explainability of CNNs", "author": "Florian Merkle and David Weber and Pascal Sch\u00f6ttle and Stephan Schl\u00f6gl and Martin Nocker", "abstract": "  Over the last century, deep learning models have become the state-of-the-art\nfor solving complex computer vision problems. These modern computer vision\nmodels have millions of parameters, which presents two major challenges: (1)\nthe increased computational requirements hamper the deployment in\nresource-constrained environments, such as mobile or IoT devices, and (2)\nexplaining the complex decisions of such networks to humans is challenging.\nNetwork pruning is a technical approach to reduce the complexity of models,\nwhere less important parameters are removed. The work presented in this paper\ninvestigates whether this reduction in technical complexity also helps with\nperceived explainability. To do so, we conducted a pre-study and two\nhuman-grounded experiments, assessing the effects of different pruning ratios\non explainability. Overall, we evaluate four different compression rates (i.e.,\n2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that\nlower compression rates have a positive influence on explainability, while\nhigher compression rates show negative effects. Furthermore, we were able to\nidentify sweet spots that increase both the perceived explainability and the\nmodel's performance.\n", "link": "http://arxiv.org/abs/2302.08878v3", "date": "2025-01-14", "relevancy": 2.4663, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4966}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs&body=Title%3A%20Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs%0AAuthor%3A%20Florian%20Merkle%20and%20David%20Weber%20and%20Pascal%20Sch%C3%B6ttle%20and%20Stephan%20Schl%C3%B6gl%20and%20Martin%20Nocker%0AAbstract%3A%20%20%20Over%20the%20last%20century%2C%20deep%20learning%20models%20have%20become%20the%20state-of-the-art%0Afor%20solving%20complex%20computer%20vision%20problems.%20These%20modern%20computer%20vision%0Amodels%20have%20millions%20of%20parameters%2C%20which%20presents%20two%20major%20challenges%3A%20%281%29%0Athe%20increased%20computational%20requirements%20hamper%20the%20deployment%20in%0Aresource-constrained%20environments%2C%20such%20as%20mobile%20or%20IoT%20devices%2C%20and%20%282%29%0Aexplaining%20the%20complex%20decisions%20of%20such%20networks%20to%20humans%20is%20challenging.%0ANetwork%20pruning%20is%20a%20technical%20approach%20to%20reduce%20the%20complexity%20of%20models%2C%0Awhere%20less%20important%20parameters%20are%20removed.%20The%20work%20presented%20in%20this%20paper%0Ainvestigates%20whether%20this%20reduction%20in%20technical%20complexity%20also%20helps%20with%0Aperceived%20explainability.%20To%20do%20so%2C%20we%20conducted%20a%20pre-study%20and%20two%0Ahuman-grounded%20experiments%2C%20assessing%20the%20effects%20of%20different%20pruning%20ratios%0Aon%20explainability.%20Overall%2C%20we%20evaluate%20four%20different%20compression%20rates%20%28i.e.%2C%0A2%2C%204%2C%208%2C%20and%2032%29%20with%2037%20500%20tasks%20on%20Mechanical%20Turk.%20Results%20indicate%20that%0Alower%20compression%20rates%20have%20a%20positive%20influence%20on%20explainability%2C%20while%0Ahigher%20compression%20rates%20show%20negative%20effects.%20Furthermore%2C%20we%20were%20able%20to%0Aidentify%20sweet%20spots%20that%20increase%20both%20the%20perceived%20explainability%20and%20the%0Amodel%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08878v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520is%2520More%253A%2520The%2520Influence%2520of%2520Pruning%2520on%2520the%2520Explainability%2520of%2520CNNs%26entry.906535625%3DFlorian%2520Merkle%2520and%2520David%2520Weber%2520and%2520Pascal%2520Sch%25C3%25B6ttle%2520and%2520Stephan%2520Schl%25C3%25B6gl%2520and%2520Martin%2520Nocker%26entry.1292438233%3D%2520%2520Over%2520the%2520last%2520century%252C%2520deep%2520learning%2520models%2520have%2520become%2520the%2520state-of-the-art%250Afor%2520solving%2520complex%2520computer%2520vision%2520problems.%2520These%2520modern%2520computer%2520vision%250Amodels%2520have%2520millions%2520of%2520parameters%252C%2520which%2520presents%2520two%2520major%2520challenges%253A%2520%25281%2529%250Athe%2520increased%2520computational%2520requirements%2520hamper%2520the%2520deployment%2520in%250Aresource-constrained%2520environments%252C%2520such%2520as%2520mobile%2520or%2520IoT%2520devices%252C%2520and%2520%25282%2529%250Aexplaining%2520the%2520complex%2520decisions%2520of%2520such%2520networks%2520to%2520humans%2520is%2520challenging.%250ANetwork%2520pruning%2520is%2520a%2520technical%2520approach%2520to%2520reduce%2520the%2520complexity%2520of%2520models%252C%250Awhere%2520less%2520important%2520parameters%2520are%2520removed.%2520The%2520work%2520presented%2520in%2520this%2520paper%250Ainvestigates%2520whether%2520this%2520reduction%2520in%2520technical%2520complexity%2520also%2520helps%2520with%250Aperceived%2520explainability.%2520To%2520do%2520so%252C%2520we%2520conducted%2520a%2520pre-study%2520and%2520two%250Ahuman-grounded%2520experiments%252C%2520assessing%2520the%2520effects%2520of%2520different%2520pruning%2520ratios%250Aon%2520explainability.%2520Overall%252C%2520we%2520evaluate%2520four%2520different%2520compression%2520rates%2520%2528i.e.%252C%250A2%252C%25204%252C%25208%252C%2520and%252032%2529%2520with%252037%2520500%2520tasks%2520on%2520Mechanical%2520Turk.%2520Results%2520indicate%2520that%250Alower%2520compression%2520rates%2520have%2520a%2520positive%2520influence%2520on%2520explainability%252C%2520while%250Ahigher%2520compression%2520rates%2520show%2520negative%2520effects.%2520Furthermore%252C%2520we%2520were%2520able%2520to%250Aidentify%2520sweet%2520spots%2520that%2520increase%2520both%2520the%2520perceived%2520explainability%2520and%2520the%250Amodel%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08878v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20is%20More%3A%20The%20Influence%20of%20Pruning%20on%20the%20Explainability%20of%20CNNs&entry.906535625=Florian%20Merkle%20and%20David%20Weber%20and%20Pascal%20Sch%C3%B6ttle%20and%20Stephan%20Schl%C3%B6gl%20and%20Martin%20Nocker&entry.1292438233=%20%20Over%20the%20last%20century%2C%20deep%20learning%20models%20have%20become%20the%20state-of-the-art%0Afor%20solving%20complex%20computer%20vision%20problems.%20These%20modern%20computer%20vision%0Amodels%20have%20millions%20of%20parameters%2C%20which%20presents%20two%20major%20challenges%3A%20%281%29%0Athe%20increased%20computational%20requirements%20hamper%20the%20deployment%20in%0Aresource-constrained%20environments%2C%20such%20as%20mobile%20or%20IoT%20devices%2C%20and%20%282%29%0Aexplaining%20the%20complex%20decisions%20of%20such%20networks%20to%20humans%20is%20challenging.%0ANetwork%20pruning%20is%20a%20technical%20approach%20to%20reduce%20the%20complexity%20of%20models%2C%0Awhere%20less%20important%20parameters%20are%20removed.%20The%20work%20presented%20in%20this%20paper%0Ainvestigates%20whether%20this%20reduction%20in%20technical%20complexity%20also%20helps%20with%0Aperceived%20explainability.%20To%20do%20so%2C%20we%20conducted%20a%20pre-study%20and%20two%0Ahuman-grounded%20experiments%2C%20assessing%20the%20effects%20of%20different%20pruning%20ratios%0Aon%20explainability.%20Overall%2C%20we%20evaluate%20four%20different%20compression%20rates%20%28i.e.%2C%0A2%2C%204%2C%208%2C%20and%2032%29%20with%2037%20500%20tasks%20on%20Mechanical%20Turk.%20Results%20indicate%20that%0Alower%20compression%20rates%20have%20a%20positive%20influence%20on%20explainability%2C%20while%0Ahigher%20compression%20rates%20show%20negative%20effects.%20Furthermore%2C%20we%20were%20able%20to%0Aidentify%20sweet%20spots%20that%20increase%20both%20the%20perceived%20explainability%20and%20the%0Amodel%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08878v3&entry.124074799=Read"},
{"title": "EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through\n  Self-Supervised State Reconstruction-Primed Riemannian Dynamics", "author": "Zirui Wang and Zhenxi Song and Yi Guo and Yuxin Liu and Guoyang Xu and Min Zhang and Zhiguo Zhang", "abstract": "  The development of EEG decoding algorithms confronts challenges such as data\nsparsity, subject variability, and the need for precise annotations, all of\nwhich are vital for advancing brain-computer interfaces and enhancing the\ndiagnosis of diseases. To address these issues, we propose a novel two-stage\napproach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics\n(EEG-ReMinD) , which mitigates reliance on supervised learning and integrates\ninherent geometric features. This approach efficiently handles EEG data\ncorruptions and reduces the dependency on labels. EEG-ReMinD utilizes\nself-supervised and geometric learning techniques, along with an attention\nmechanism, to analyze the temporal dynamics of EEG features within the\nframework of Riemannian geometry, referred to as Riemannian dynamics.\nComparative analyses on both intact and corrupted datasets from two different\nneurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.\n", "link": "http://arxiv.org/abs/2501.08139v1", "date": "2025-01-14", "relevancy": 2.451, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-ReMinD%3A%20Enhancing%20Neurodegenerative%20EEG%20Decoding%20through%0A%20%20Self-Supervised%20State%20Reconstruction-Primed%20Riemannian%20Dynamics&body=Title%3A%20EEG-ReMinD%3A%20Enhancing%20Neurodegenerative%20EEG%20Decoding%20through%0A%20%20Self-Supervised%20State%20Reconstruction-Primed%20Riemannian%20Dynamics%0AAuthor%3A%20Zirui%20Wang%20and%20Zhenxi%20Song%20and%20Yi%20Guo%20and%20Yuxin%20Liu%20and%20Guoyang%20Xu%20and%20Min%20Zhang%20and%20Zhiguo%20Zhang%0AAbstract%3A%20%20%20The%20development%20of%20EEG%20decoding%20algorithms%20confronts%20challenges%20such%20as%20data%0Asparsity%2C%20subject%20variability%2C%20and%20the%20need%20for%20precise%20annotations%2C%20all%20of%0Awhich%20are%20vital%20for%20advancing%20brain-computer%20interfaces%20and%20enhancing%20the%0Adiagnosis%20of%20diseases.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20two-stage%0Aapproach%20named%20Self-Supervised%20State%20Reconstruction-Primed%20Riemannian%20Dynamics%0A%28EEG-ReMinD%29%20%2C%20which%20mitigates%20reliance%20on%20supervised%20learning%20and%20integrates%0Ainherent%20geometric%20features.%20This%20approach%20efficiently%20handles%20EEG%20data%0Acorruptions%20and%20reduces%20the%20dependency%20on%20labels.%20EEG-ReMinD%20utilizes%0Aself-supervised%20and%20geometric%20learning%20techniques%2C%20along%20with%20an%20attention%0Amechanism%2C%20to%20analyze%20the%20temporal%20dynamics%20of%20EEG%20features%20within%20the%0Aframework%20of%20Riemannian%20geometry%2C%20referred%20to%20as%20Riemannian%20dynamics.%0AComparative%20analyses%20on%20both%20intact%20and%20corrupted%20datasets%20from%20two%20different%0Aneurodegenerative%20disorders%20underscore%20the%20enhanced%20performance%20of%20EEG-ReMinD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-ReMinD%253A%2520Enhancing%2520Neurodegenerative%2520EEG%2520Decoding%2520through%250A%2520%2520Self-Supervised%2520State%2520Reconstruction-Primed%2520Riemannian%2520Dynamics%26entry.906535625%3DZirui%2520Wang%2520and%2520Zhenxi%2520Song%2520and%2520Yi%2520Guo%2520and%2520Yuxin%2520Liu%2520and%2520Guoyang%2520Xu%2520and%2520Min%2520Zhang%2520and%2520Zhiguo%2520Zhang%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520EEG%2520decoding%2520algorithms%2520confronts%2520challenges%2520such%2520as%2520data%250Asparsity%252C%2520subject%2520variability%252C%2520and%2520the%2520need%2520for%2520precise%2520annotations%252C%2520all%2520of%250Awhich%2520are%2520vital%2520for%2520advancing%2520brain-computer%2520interfaces%2520and%2520enhancing%2520the%250Adiagnosis%2520of%2520diseases.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520two-stage%250Aapproach%2520named%2520Self-Supervised%2520State%2520Reconstruction-Primed%2520Riemannian%2520Dynamics%250A%2528EEG-ReMinD%2529%2520%252C%2520which%2520mitigates%2520reliance%2520on%2520supervised%2520learning%2520and%2520integrates%250Ainherent%2520geometric%2520features.%2520This%2520approach%2520efficiently%2520handles%2520EEG%2520data%250Acorruptions%2520and%2520reduces%2520the%2520dependency%2520on%2520labels.%2520EEG-ReMinD%2520utilizes%250Aself-supervised%2520and%2520geometric%2520learning%2520techniques%252C%2520along%2520with%2520an%2520attention%250Amechanism%252C%2520to%2520analyze%2520the%2520temporal%2520dynamics%2520of%2520EEG%2520features%2520within%2520the%250Aframework%2520of%2520Riemannian%2520geometry%252C%2520referred%2520to%2520as%2520Riemannian%2520dynamics.%250AComparative%2520analyses%2520on%2520both%2520intact%2520and%2520corrupted%2520datasets%2520from%2520two%2520different%250Aneurodegenerative%2520disorders%2520underscore%2520the%2520enhanced%2520performance%2520of%2520EEG-ReMinD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-ReMinD%3A%20Enhancing%20Neurodegenerative%20EEG%20Decoding%20through%0A%20%20Self-Supervised%20State%20Reconstruction-Primed%20Riemannian%20Dynamics&entry.906535625=Zirui%20Wang%20and%20Zhenxi%20Song%20and%20Yi%20Guo%20and%20Yuxin%20Liu%20and%20Guoyang%20Xu%20and%20Min%20Zhang%20and%20Zhiguo%20Zhang&entry.1292438233=%20%20The%20development%20of%20EEG%20decoding%20algorithms%20confronts%20challenges%20such%20as%20data%0Asparsity%2C%20subject%20variability%2C%20and%20the%20need%20for%20precise%20annotations%2C%20all%20of%0Awhich%20are%20vital%20for%20advancing%20brain-computer%20interfaces%20and%20enhancing%20the%0Adiagnosis%20of%20diseases.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20two-stage%0Aapproach%20named%20Self-Supervised%20State%20Reconstruction-Primed%20Riemannian%20Dynamics%0A%28EEG-ReMinD%29%20%2C%20which%20mitigates%20reliance%20on%20supervised%20learning%20and%20integrates%0Ainherent%20geometric%20features.%20This%20approach%20efficiently%20handles%20EEG%20data%0Acorruptions%20and%20reduces%20the%20dependency%20on%20labels.%20EEG-ReMinD%20utilizes%0Aself-supervised%20and%20geometric%20learning%20techniques%2C%20along%20with%20an%20attention%0Amechanism%2C%20to%20analyze%20the%20temporal%20dynamics%20of%20EEG%20features%20within%20the%0Aframework%20of%20Riemannian%20geometry%2C%20referred%20to%20as%20Riemannian%20dynamics.%0AComparative%20analyses%20on%20both%20intact%20and%20corrupted%20datasets%20from%20two%20different%0Aneurodegenerative%20disorders%20underscore%20the%20enhanced%20performance%20of%20EEG-ReMinD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08139v1&entry.124074799=Read"},
{"title": "CellOMaps: A Compact Representation for Robust Classification of Lung\n  Adenocarcinoma Growth Patterns", "author": "Arwa Al-Rubaian and Gozde N. Gunesli and Wajd A. Althakfi and Ayesha Azam and David Snead and Nasir M. Rajpoot and Shan E Ahmed Raza", "abstract": "  Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,\ncharacterized by five primary histological growth patterns. The classification\nof such patterns is crucial due to their direct relation to prognosis but the\nhigh subjectivity and observer variability pose a major challenge. Although\nseveral studies have developed machine learning methods for growth pattern\nclassification, they either only report the predominant pattern per slide or\nlack proper evaluation. We propose a generalizable machine learning pipeline\ncapable of classifying lung tissue into one of the five patterns or as\nnon-tumor. The proposed pipeline's strength lies in a novel compact Cell\nOrganization Maps (cellOMaps) representation that captures the cellular spatial\npatterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed\npipeline provides state-of-the-art performance on LUAD growth pattern\nclassification when evaluated on both internal unseen slides and external\ndatasets, significantly outperforming the current approaches. In addition, our\npreliminary results show that the model's outputs can be used to predict\npatients Tumor Mutational Burden (TMB) levels.\n", "link": "http://arxiv.org/abs/2501.08094v1", "date": "2025-01-14", "relevancy": 2.4242, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4892}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CellOMaps%3A%20A%20Compact%20Representation%20for%20Robust%20Classification%20of%20Lung%0A%20%20Adenocarcinoma%20Growth%20Patterns&body=Title%3A%20CellOMaps%3A%20A%20Compact%20Representation%20for%20Robust%20Classification%20of%20Lung%0A%20%20Adenocarcinoma%20Growth%20Patterns%0AAuthor%3A%20Arwa%20Al-Rubaian%20and%20Gozde%20N.%20Gunesli%20and%20Wajd%20A.%20Althakfi%20and%20Ayesha%20Azam%20and%20David%20Snead%20and%20Nasir%20M.%20Rajpoot%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Lung%20adenocarcinoma%20%28LUAD%29%20is%20a%20morphologically%20heterogeneous%20disease%2C%0Acharacterized%20by%20five%20primary%20histological%20growth%20patterns.%20The%20classification%0Aof%20such%20patterns%20is%20crucial%20due%20to%20their%20direct%20relation%20to%20prognosis%20but%20the%0Ahigh%20subjectivity%20and%20observer%20variability%20pose%20a%20major%20challenge.%20Although%0Aseveral%20studies%20have%20developed%20machine%20learning%20methods%20for%20growth%20pattern%0Aclassification%2C%20they%20either%20only%20report%20the%20predominant%20pattern%20per%20slide%20or%0Alack%20proper%20evaluation.%20We%20propose%20a%20generalizable%20machine%20learning%20pipeline%0Acapable%20of%20classifying%20lung%20tissue%20into%20one%20of%20the%20five%20patterns%20or%20as%0Anon-tumor.%20The%20proposed%20pipeline%27s%20strength%20lies%20in%20a%20novel%20compact%20Cell%0AOrganization%20Maps%20%28cellOMaps%29%20representation%20that%20captures%20the%20cellular%20spatial%0Apatterns%20from%20Hematoxylin%20and%20Eosin%20whole%20slide%20images%20%28WSIs%29.%20The%20proposed%0Apipeline%20provides%20state-of-the-art%20performance%20on%20LUAD%20growth%20pattern%0Aclassification%20when%20evaluated%20on%20both%20internal%20unseen%20slides%20and%20external%0Adatasets%2C%20significantly%20outperforming%20the%20current%20approaches.%20In%20addition%2C%20our%0Apreliminary%20results%20show%20that%20the%20model%27s%20outputs%20can%20be%20used%20to%20predict%0Apatients%20Tumor%20Mutational%20Burden%20%28TMB%29%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCellOMaps%253A%2520A%2520Compact%2520Representation%2520for%2520Robust%2520Classification%2520of%2520Lung%250A%2520%2520Adenocarcinoma%2520Growth%2520Patterns%26entry.906535625%3DArwa%2520Al-Rubaian%2520and%2520Gozde%2520N.%2520Gunesli%2520and%2520Wajd%2520A.%2520Althakfi%2520and%2520Ayesha%2520Azam%2520and%2520David%2520Snead%2520and%2520Nasir%2520M.%2520Rajpoot%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Lung%2520adenocarcinoma%2520%2528LUAD%2529%2520is%2520a%2520morphologically%2520heterogeneous%2520disease%252C%250Acharacterized%2520by%2520five%2520primary%2520histological%2520growth%2520patterns.%2520The%2520classification%250Aof%2520such%2520patterns%2520is%2520crucial%2520due%2520to%2520their%2520direct%2520relation%2520to%2520prognosis%2520but%2520the%250Ahigh%2520subjectivity%2520and%2520observer%2520variability%2520pose%2520a%2520major%2520challenge.%2520Although%250Aseveral%2520studies%2520have%2520developed%2520machine%2520learning%2520methods%2520for%2520growth%2520pattern%250Aclassification%252C%2520they%2520either%2520only%2520report%2520the%2520predominant%2520pattern%2520per%2520slide%2520or%250Alack%2520proper%2520evaluation.%2520We%2520propose%2520a%2520generalizable%2520machine%2520learning%2520pipeline%250Acapable%2520of%2520classifying%2520lung%2520tissue%2520into%2520one%2520of%2520the%2520five%2520patterns%2520or%2520as%250Anon-tumor.%2520The%2520proposed%2520pipeline%2527s%2520strength%2520lies%2520in%2520a%2520novel%2520compact%2520Cell%250AOrganization%2520Maps%2520%2528cellOMaps%2529%2520representation%2520that%2520captures%2520the%2520cellular%2520spatial%250Apatterns%2520from%2520Hematoxylin%2520and%2520Eosin%2520whole%2520slide%2520images%2520%2528WSIs%2529.%2520The%2520proposed%250Apipeline%2520provides%2520state-of-the-art%2520performance%2520on%2520LUAD%2520growth%2520pattern%250Aclassification%2520when%2520evaluated%2520on%2520both%2520internal%2520unseen%2520slides%2520and%2520external%250Adatasets%252C%2520significantly%2520outperforming%2520the%2520current%2520approaches.%2520In%2520addition%252C%2520our%250Apreliminary%2520results%2520show%2520that%2520the%2520model%2527s%2520outputs%2520can%2520be%2520used%2520to%2520predict%250Apatients%2520Tumor%2520Mutational%2520Burden%2520%2528TMB%2529%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CellOMaps%3A%20A%20Compact%20Representation%20for%20Robust%20Classification%20of%20Lung%0A%20%20Adenocarcinoma%20Growth%20Patterns&entry.906535625=Arwa%20Al-Rubaian%20and%20Gozde%20N.%20Gunesli%20and%20Wajd%20A.%20Althakfi%20and%20Ayesha%20Azam%20and%20David%20Snead%20and%20Nasir%20M.%20Rajpoot%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Lung%20adenocarcinoma%20%28LUAD%29%20is%20a%20morphologically%20heterogeneous%20disease%2C%0Acharacterized%20by%20five%20primary%20histological%20growth%20patterns.%20The%20classification%0Aof%20such%20patterns%20is%20crucial%20due%20to%20their%20direct%20relation%20to%20prognosis%20but%20the%0Ahigh%20subjectivity%20and%20observer%20variability%20pose%20a%20major%20challenge.%20Although%0Aseveral%20studies%20have%20developed%20machine%20learning%20methods%20for%20growth%20pattern%0Aclassification%2C%20they%20either%20only%20report%20the%20predominant%20pattern%20per%20slide%20or%0Alack%20proper%20evaluation.%20We%20propose%20a%20generalizable%20machine%20learning%20pipeline%0Acapable%20of%20classifying%20lung%20tissue%20into%20one%20of%20the%20five%20patterns%20or%20as%0Anon-tumor.%20The%20proposed%20pipeline%27s%20strength%20lies%20in%20a%20novel%20compact%20Cell%0AOrganization%20Maps%20%28cellOMaps%29%20representation%20that%20captures%20the%20cellular%20spatial%0Apatterns%20from%20Hematoxylin%20and%20Eosin%20whole%20slide%20images%20%28WSIs%29.%20The%20proposed%0Apipeline%20provides%20state-of-the-art%20performance%20on%20LUAD%20growth%20pattern%0Aclassification%20when%20evaluated%20on%20both%20internal%20unseen%20slides%20and%20external%0Adatasets%2C%20significantly%20outperforming%20the%20current%20approaches.%20In%20addition%2C%20our%0Apreliminary%20results%20show%20that%20the%20model%27s%20outputs%20can%20be%20used%20to%20predict%0Apatients%20Tumor%20Mutational%20Burden%20%28TMB%29%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08094v1&entry.124074799=Read"},
{"title": "Diversified Augmentation with Domain Adaptation for Debiased Video\n  Temporal Grounding", "author": "Junlong Ren and Gangjian Zhang and Haifeng Sun and Hao Wang", "abstract": "  Temporal sentence grounding in videos (TSGV) faces challenges due to public\nTSGV datasets containing significant temporal biases, which are attributed to\nthe uneven temporal distributions of target moments. Existing methods generate\naugmented videos, where target moments are forced to have varying temporal\nlocations. However, since the video lengths of the given datasets have small\nvariations, only changing the temporal locations results in poor generalization\nability in videos with varying lengths. In this paper, we propose a novel\ntraining framework complemented by diversified data augmentation and a domain\ndiscriminator. The data augmentation generates videos with various lengths and\ntarget moment locations to diversify temporal distributions. However, augmented\nvideos inevitably exhibit distinct feature distributions which may introduce\nnoise. To address this, we design a domain adaptation auxiliary task to\ndiminish feature discrepancies between original and augmented videos. We also\nencourage the model to produce distinct predictions for videos with the same\ntext queries but different moment locations to promote debiased training.\nExperiments on Charades-CD and ActivityNet-CD datasets demonstrate the\neffectiveness and generalization abilities of our method in multiple grounding\nstructures, achieving state-of-the-art results.\n", "link": "http://arxiv.org/abs/2501.06746v2", "date": "2025-01-14", "relevancy": 2.423, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6309}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6152}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diversified%20Augmentation%20with%20Domain%20Adaptation%20for%20Debiased%20Video%0A%20%20Temporal%20Grounding&body=Title%3A%20Diversified%20Augmentation%20with%20Domain%20Adaptation%20for%20Debiased%20Video%0A%20%20Temporal%20Grounding%0AAuthor%3A%20Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Haifeng%20Sun%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Temporal%20sentence%20grounding%20in%20videos%20%28TSGV%29%20faces%20challenges%20due%20to%20public%0ATSGV%20datasets%20containing%20significant%20temporal%20biases%2C%20which%20are%20attributed%20to%0Athe%20uneven%20temporal%20distributions%20of%20target%20moments.%20Existing%20methods%20generate%0Aaugmented%20videos%2C%20where%20target%20moments%20are%20forced%20to%20have%20varying%20temporal%0Alocations.%20However%2C%20since%20the%20video%20lengths%20of%20the%20given%20datasets%20have%20small%0Avariations%2C%20only%20changing%20the%20temporal%20locations%20results%20in%20poor%20generalization%0Aability%20in%20videos%20with%20varying%20lengths.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Atraining%20framework%20complemented%20by%20diversified%20data%20augmentation%20and%20a%20domain%0Adiscriminator.%20The%20data%20augmentation%20generates%20videos%20with%20various%20lengths%20and%0Atarget%20moment%20locations%20to%20diversify%20temporal%20distributions.%20However%2C%20augmented%0Avideos%20inevitably%20exhibit%20distinct%20feature%20distributions%20which%20may%20introduce%0Anoise.%20To%20address%20this%2C%20we%20design%20a%20domain%20adaptation%20auxiliary%20task%20to%0Adiminish%20feature%20discrepancies%20between%20original%20and%20augmented%20videos.%20We%20also%0Aencourage%20the%20model%20to%20produce%20distinct%20predictions%20for%20videos%20with%20the%20same%0Atext%20queries%20but%20different%20moment%20locations%20to%20promote%20debiased%20training.%0AExperiments%20on%20Charades-CD%20and%20ActivityNet-CD%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generalization%20abilities%20of%20our%20method%20in%20multiple%20grounding%0Astructures%2C%20achieving%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiversified%2520Augmentation%2520with%2520Domain%2520Adaptation%2520for%2520Debiased%2520Video%250A%2520%2520Temporal%2520Grounding%26entry.906535625%3DJunlong%2520Ren%2520and%2520Gangjian%2520Zhang%2520and%2520Haifeng%2520Sun%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Temporal%2520sentence%2520grounding%2520in%2520videos%2520%2528TSGV%2529%2520faces%2520challenges%2520due%2520to%2520public%250ATSGV%2520datasets%2520containing%2520significant%2520temporal%2520biases%252C%2520which%2520are%2520attributed%2520to%250Athe%2520uneven%2520temporal%2520distributions%2520of%2520target%2520moments.%2520Existing%2520methods%2520generate%250Aaugmented%2520videos%252C%2520where%2520target%2520moments%2520are%2520forced%2520to%2520have%2520varying%2520temporal%250Alocations.%2520However%252C%2520since%2520the%2520video%2520lengths%2520of%2520the%2520given%2520datasets%2520have%2520small%250Avariations%252C%2520only%2520changing%2520the%2520temporal%2520locations%2520results%2520in%2520poor%2520generalization%250Aability%2520in%2520videos%2520with%2520varying%2520lengths.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Atraining%2520framework%2520complemented%2520by%2520diversified%2520data%2520augmentation%2520and%2520a%2520domain%250Adiscriminator.%2520The%2520data%2520augmentation%2520generates%2520videos%2520with%2520various%2520lengths%2520and%250Atarget%2520moment%2520locations%2520to%2520diversify%2520temporal%2520distributions.%2520However%252C%2520augmented%250Avideos%2520inevitably%2520exhibit%2520distinct%2520feature%2520distributions%2520which%2520may%2520introduce%250Anoise.%2520To%2520address%2520this%252C%2520we%2520design%2520a%2520domain%2520adaptation%2520auxiliary%2520task%2520to%250Adiminish%2520feature%2520discrepancies%2520between%2520original%2520and%2520augmented%2520videos.%2520We%2520also%250Aencourage%2520the%2520model%2520to%2520produce%2520distinct%2520predictions%2520for%2520videos%2520with%2520the%2520same%250Atext%2520queries%2520but%2520different%2520moment%2520locations%2520to%2520promote%2520debiased%2520training.%250AExperiments%2520on%2520Charades-CD%2520and%2520ActivityNet-CD%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generalization%2520abilities%2520of%2520our%2520method%2520in%2520multiple%2520grounding%250Astructures%252C%2520achieving%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diversified%20Augmentation%20with%20Domain%20Adaptation%20for%20Debiased%20Video%0A%20%20Temporal%20Grounding&entry.906535625=Junlong%20Ren%20and%20Gangjian%20Zhang%20and%20Haifeng%20Sun%20and%20Hao%20Wang&entry.1292438233=%20%20Temporal%20sentence%20grounding%20in%20videos%20%28TSGV%29%20faces%20challenges%20due%20to%20public%0ATSGV%20datasets%20containing%20significant%20temporal%20biases%2C%20which%20are%20attributed%20to%0Athe%20uneven%20temporal%20distributions%20of%20target%20moments.%20Existing%20methods%20generate%0Aaugmented%20videos%2C%20where%20target%20moments%20are%20forced%20to%20have%20varying%20temporal%0Alocations.%20However%2C%20since%20the%20video%20lengths%20of%20the%20given%20datasets%20have%20small%0Avariations%2C%20only%20changing%20the%20temporal%20locations%20results%20in%20poor%20generalization%0Aability%20in%20videos%20with%20varying%20lengths.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Atraining%20framework%20complemented%20by%20diversified%20data%20augmentation%20and%20a%20domain%0Adiscriminator.%20The%20data%20augmentation%20generates%20videos%20with%20various%20lengths%20and%0Atarget%20moment%20locations%20to%20diversify%20temporal%20distributions.%20However%2C%20augmented%0Avideos%20inevitably%20exhibit%20distinct%20feature%20distributions%20which%20may%20introduce%0Anoise.%20To%20address%20this%2C%20we%20design%20a%20domain%20adaptation%20auxiliary%20task%20to%0Adiminish%20feature%20discrepancies%20between%20original%20and%20augmented%20videos.%20We%20also%0Aencourage%20the%20model%20to%20produce%20distinct%20predictions%20for%20videos%20with%20the%20same%0Atext%20queries%20but%20different%20moment%20locations%20to%20promote%20debiased%20training.%0AExperiments%20on%20Charades-CD%20and%20ActivityNet-CD%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generalization%20abilities%20of%20our%20method%20in%20multiple%20grounding%0Astructures%2C%20achieving%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06746v2&entry.124074799=Read"},
{"title": "FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware\n  Classification", "author": "Nurit Cohen-Inger and Lior Rokach and Bracha Shapira and Seffi Cohen", "abstract": "  Algorithmic decision-making has become deeply ingrained in many domains, yet\nbiases in machine learning models can still produce discriminatory outcomes,\noften harming unprivileged groups. Achieving fair classification is inherently\nchallenging, requiring a careful balance between predictive performance and\nethical considerations. We present FairTTTS, a novel post-processing bias\nmitigation method inspired by the Tree Test Time Simulation (TTTS) method.\nOriginally developed to enhance accuracy and robustness against adversarial\ninputs through probabilistic decision-path adjustments, TTTS serves as the\nfoundation for FairTTTS. By building on this accuracy-enhancing technique,\nFairTTTS mitigates bias and improves predictive performance. FairTTTS uses a\ndistance-based heuristic to adjust decisions at protected attribute nodes,\nensuring fairness for unprivileged samples. This fairness-oriented adjustment\noccurs as a post-processing step, allowing FairTTTS to be applied to\npre-trained models, diverse datasets, and various fairness metrics without\nretraining. Extensive evaluation on seven benchmark datasets shows that\nFairTTTS outperforms traditional methods in fairness improvement, achieving a\n20.96% average increase over the baseline compared to 18.78% for related work,\nand further enhances accuracy by 0.55%. In contrast, competing methods\ntypically reduce accuracy by 0.42%. These results confirm that FairTTTS\neffectively promotes more equitable decision-making while simultaneously\nimproving predictive performance.\n", "link": "http://arxiv.org/abs/2501.08155v1", "date": "2025-01-14", "relevancy": 2.4059, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4826}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairTTTS%3A%20A%20Tree%20Test%20Time%20Simulation%20Method%20for%20Fairness-Aware%0A%20%20Classification&body=Title%3A%20FairTTTS%3A%20A%20Tree%20Test%20Time%20Simulation%20Method%20for%20Fairness-Aware%0A%20%20Classification%0AAuthor%3A%20Nurit%20Cohen-Inger%20and%20Lior%20Rokach%20and%20Bracha%20Shapira%20and%20Seffi%20Cohen%0AAbstract%3A%20%20%20Algorithmic%20decision-making%20has%20become%20deeply%20ingrained%20in%20many%20domains%2C%20yet%0Abiases%20in%20machine%20learning%20models%20can%20still%20produce%20discriminatory%20outcomes%2C%0Aoften%20harming%20unprivileged%20groups.%20Achieving%20fair%20classification%20is%20inherently%0Achallenging%2C%20requiring%20a%20careful%20balance%20between%20predictive%20performance%20and%0Aethical%20considerations.%20We%20present%20FairTTTS%2C%20a%20novel%20post-processing%20bias%0Amitigation%20method%20inspired%20by%20the%20Tree%20Test%20Time%20Simulation%20%28TTTS%29%20method.%0AOriginally%20developed%20to%20enhance%20accuracy%20and%20robustness%20against%20adversarial%0Ainputs%20through%20probabilistic%20decision-path%20adjustments%2C%20TTTS%20serves%20as%20the%0Afoundation%20for%20FairTTTS.%20By%20building%20on%20this%20accuracy-enhancing%20technique%2C%0AFairTTTS%20mitigates%20bias%20and%20improves%20predictive%20performance.%20FairTTTS%20uses%20a%0Adistance-based%20heuristic%20to%20adjust%20decisions%20at%20protected%20attribute%20nodes%2C%0Aensuring%20fairness%20for%20unprivileged%20samples.%20This%20fairness-oriented%20adjustment%0Aoccurs%20as%20a%20post-processing%20step%2C%20allowing%20FairTTTS%20to%20be%20applied%20to%0Apre-trained%20models%2C%20diverse%20datasets%2C%20and%20various%20fairness%20metrics%20without%0Aretraining.%20Extensive%20evaluation%20on%20seven%20benchmark%20datasets%20shows%20that%0AFairTTTS%20outperforms%20traditional%20methods%20in%20fairness%20improvement%2C%20achieving%20a%0A20.96%25%20average%20increase%20over%20the%20baseline%20compared%20to%2018.78%25%20for%20related%20work%2C%0Aand%20further%20enhances%20accuracy%20by%200.55%25.%20In%20contrast%2C%20competing%20methods%0Atypically%20reduce%20accuracy%20by%200.42%25.%20These%20results%20confirm%20that%20FairTTTS%0Aeffectively%20promotes%20more%20equitable%20decision-making%20while%20simultaneously%0Aimproving%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairTTTS%253A%2520A%2520Tree%2520Test%2520Time%2520Simulation%2520Method%2520for%2520Fairness-Aware%250A%2520%2520Classification%26entry.906535625%3DNurit%2520Cohen-Inger%2520and%2520Lior%2520Rokach%2520and%2520Bracha%2520Shapira%2520and%2520Seffi%2520Cohen%26entry.1292438233%3D%2520%2520Algorithmic%2520decision-making%2520has%2520become%2520deeply%2520ingrained%2520in%2520many%2520domains%252C%2520yet%250Abiases%2520in%2520machine%2520learning%2520models%2520can%2520still%2520produce%2520discriminatory%2520outcomes%252C%250Aoften%2520harming%2520unprivileged%2520groups.%2520Achieving%2520fair%2520classification%2520is%2520inherently%250Achallenging%252C%2520requiring%2520a%2520careful%2520balance%2520between%2520predictive%2520performance%2520and%250Aethical%2520considerations.%2520We%2520present%2520FairTTTS%252C%2520a%2520novel%2520post-processing%2520bias%250Amitigation%2520method%2520inspired%2520by%2520the%2520Tree%2520Test%2520Time%2520Simulation%2520%2528TTTS%2529%2520method.%250AOriginally%2520developed%2520to%2520enhance%2520accuracy%2520and%2520robustness%2520against%2520adversarial%250Ainputs%2520through%2520probabilistic%2520decision-path%2520adjustments%252C%2520TTTS%2520serves%2520as%2520the%250Afoundation%2520for%2520FairTTTS.%2520By%2520building%2520on%2520this%2520accuracy-enhancing%2520technique%252C%250AFairTTTS%2520mitigates%2520bias%2520and%2520improves%2520predictive%2520performance.%2520FairTTTS%2520uses%2520a%250Adistance-based%2520heuristic%2520to%2520adjust%2520decisions%2520at%2520protected%2520attribute%2520nodes%252C%250Aensuring%2520fairness%2520for%2520unprivileged%2520samples.%2520This%2520fairness-oriented%2520adjustment%250Aoccurs%2520as%2520a%2520post-processing%2520step%252C%2520allowing%2520FairTTTS%2520to%2520be%2520applied%2520to%250Apre-trained%2520models%252C%2520diverse%2520datasets%252C%2520and%2520various%2520fairness%2520metrics%2520without%250Aretraining.%2520Extensive%2520evaluation%2520on%2520seven%2520benchmark%2520datasets%2520shows%2520that%250AFairTTTS%2520outperforms%2520traditional%2520methods%2520in%2520fairness%2520improvement%252C%2520achieving%2520a%250A20.96%2525%2520average%2520increase%2520over%2520the%2520baseline%2520compared%2520to%252018.78%2525%2520for%2520related%2520work%252C%250Aand%2520further%2520enhances%2520accuracy%2520by%25200.55%2525.%2520In%2520contrast%252C%2520competing%2520methods%250Atypically%2520reduce%2520accuracy%2520by%25200.42%2525.%2520These%2520results%2520confirm%2520that%2520FairTTTS%250Aeffectively%2520promotes%2520more%2520equitable%2520decision-making%2520while%2520simultaneously%250Aimproving%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairTTTS%3A%20A%20Tree%20Test%20Time%20Simulation%20Method%20for%20Fairness-Aware%0A%20%20Classification&entry.906535625=Nurit%20Cohen-Inger%20and%20Lior%20Rokach%20and%20Bracha%20Shapira%20and%20Seffi%20Cohen&entry.1292438233=%20%20Algorithmic%20decision-making%20has%20become%20deeply%20ingrained%20in%20many%20domains%2C%20yet%0Abiases%20in%20machine%20learning%20models%20can%20still%20produce%20discriminatory%20outcomes%2C%0Aoften%20harming%20unprivileged%20groups.%20Achieving%20fair%20classification%20is%20inherently%0Achallenging%2C%20requiring%20a%20careful%20balance%20between%20predictive%20performance%20and%0Aethical%20considerations.%20We%20present%20FairTTTS%2C%20a%20novel%20post-processing%20bias%0Amitigation%20method%20inspired%20by%20the%20Tree%20Test%20Time%20Simulation%20%28TTTS%29%20method.%0AOriginally%20developed%20to%20enhance%20accuracy%20and%20robustness%20against%20adversarial%0Ainputs%20through%20probabilistic%20decision-path%20adjustments%2C%20TTTS%20serves%20as%20the%0Afoundation%20for%20FairTTTS.%20By%20building%20on%20this%20accuracy-enhancing%20technique%2C%0AFairTTTS%20mitigates%20bias%20and%20improves%20predictive%20performance.%20FairTTTS%20uses%20a%0Adistance-based%20heuristic%20to%20adjust%20decisions%20at%20protected%20attribute%20nodes%2C%0Aensuring%20fairness%20for%20unprivileged%20samples.%20This%20fairness-oriented%20adjustment%0Aoccurs%20as%20a%20post-processing%20step%2C%20allowing%20FairTTTS%20to%20be%20applied%20to%0Apre-trained%20models%2C%20diverse%20datasets%2C%20and%20various%20fairness%20metrics%20without%0Aretraining.%20Extensive%20evaluation%20on%20seven%20benchmark%20datasets%20shows%20that%0AFairTTTS%20outperforms%20traditional%20methods%20in%20fairness%20improvement%2C%20achieving%20a%0A20.96%25%20average%20increase%20over%20the%20baseline%20compared%20to%2018.78%25%20for%20related%20work%2C%0Aand%20further%20enhances%20accuracy%20by%200.55%25.%20In%20contrast%2C%20competing%20methods%0Atypically%20reduce%20accuracy%20by%200.42%25.%20These%20results%20confirm%20that%20FairTTTS%0Aeffectively%20promotes%20more%20equitable%20decision-making%20while%20simultaneously%0Aimproving%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08155v1&entry.124074799=Read"},
{"title": "A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation", "author": "Steven Landgraf and Rongjun Qin and Markus Ulrich", "abstract": "  While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.\n", "link": "http://arxiv.org/abs/2501.08188v1", "date": "2025-01-14", "relevancy": 2.4021, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6275}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Critical%20Synthesis%20of%20Uncertainty%20Quantification%20and%20Foundation%20Models%0A%20%20in%20Monocular%20Depth%20Estimation&body=Title%3A%20A%20Critical%20Synthesis%20of%20Uncertainty%20Quantification%20and%20Foundation%20Models%0A%20%20in%20Monocular%20Depth%20Estimation%0AAuthor%3A%20Steven%20Landgraf%20and%20Rongjun%20Qin%20and%20Markus%20Ulrich%0AAbstract%3A%20%20%20While%20recent%20foundation%20models%20have%20enabled%20significant%20breakthroughs%20in%0Amonocular%20depth%20estimation%2C%20a%20clear%20path%20towards%20safe%20and%20reliable%20deployment%0Ain%20the%20real-world%20remains%20elusive.%20Metric%20depth%20estimation%2C%20which%20involves%0Apredicting%20absolute%20distances%2C%20poses%20particular%20challenges%2C%20as%20even%20the%20most%0Aadvanced%20foundation%20models%20remain%20prone%20to%20critical%20errors.%20Since%20quantifying%0Athe%20uncertainty%20has%20emerged%20as%20a%20promising%20endeavor%20to%20address%20these%0Alimitations%20and%20enable%20trustworthy%20deployment%2C%20we%20fuse%20five%20different%0Auncertainty%20quantification%20methods%20with%20the%20current%20state-of-the-art%0ADepthAnythingV2%20foundation%20model.%20To%20cover%20a%20wide%20range%20of%20metric%20depth%0Adomains%2C%20we%20evaluate%20their%20performance%20on%20four%20diverse%20datasets.%20Our%20findings%0Aidentify%20fine-tuning%20with%20the%20Gaussian%20Negative%20Log-Likelihood%20Loss%20%28GNLL%29%20as%20a%0Aparticularly%20promising%20approach%2C%20offering%20reliable%20uncertainty%20estimates%20while%0Amaintaining%20predictive%20performance%20and%20computational%20efficiency%20on%20par%20with%20the%0Abaseline%2C%20encompassing%20both%20training%20and%20inference%20time.%20By%20fusing%20uncertainty%0Aquantification%20and%20foundation%20models%20within%20the%20context%20of%20monocular%20depth%0Aestimation%2C%20this%20paper%20lays%20a%20critical%20foundation%20for%20future%20research%20aimed%20at%0Aimproving%20not%20only%20model%20performance%20but%20also%20its%20explainability.%20Extending%0Athis%20critical%20synthesis%20of%20uncertainty%20quantification%20and%20foundation%20models%0Ainto%20other%20crucial%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20pose%20estimation%2C%0Apresents%20exciting%20opportunities%20for%20safer%20and%20more%20reliable%20machine%20vision%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Critical%2520Synthesis%2520of%2520Uncertainty%2520Quantification%2520and%2520Foundation%2520Models%250A%2520%2520in%2520Monocular%2520Depth%2520Estimation%26entry.906535625%3DSteven%2520Landgraf%2520and%2520Rongjun%2520Qin%2520and%2520Markus%2520Ulrich%26entry.1292438233%3D%2520%2520While%2520recent%2520foundation%2520models%2520have%2520enabled%2520significant%2520breakthroughs%2520in%250Amonocular%2520depth%2520estimation%252C%2520a%2520clear%2520path%2520towards%2520safe%2520and%2520reliable%2520deployment%250Ain%2520the%2520real-world%2520remains%2520elusive.%2520Metric%2520depth%2520estimation%252C%2520which%2520involves%250Apredicting%2520absolute%2520distances%252C%2520poses%2520particular%2520challenges%252C%2520as%2520even%2520the%2520most%250Aadvanced%2520foundation%2520models%2520remain%2520prone%2520to%2520critical%2520errors.%2520Since%2520quantifying%250Athe%2520uncertainty%2520has%2520emerged%2520as%2520a%2520promising%2520endeavor%2520to%2520address%2520these%250Alimitations%2520and%2520enable%2520trustworthy%2520deployment%252C%2520we%2520fuse%2520five%2520different%250Auncertainty%2520quantification%2520methods%2520with%2520the%2520current%2520state-of-the-art%250ADepthAnythingV2%2520foundation%2520model.%2520To%2520cover%2520a%2520wide%2520range%2520of%2520metric%2520depth%250Adomains%252C%2520we%2520evaluate%2520their%2520performance%2520on%2520four%2520diverse%2520datasets.%2520Our%2520findings%250Aidentify%2520fine-tuning%2520with%2520the%2520Gaussian%2520Negative%2520Log-Likelihood%2520Loss%2520%2528GNLL%2529%2520as%2520a%250Aparticularly%2520promising%2520approach%252C%2520offering%2520reliable%2520uncertainty%2520estimates%2520while%250Amaintaining%2520predictive%2520performance%2520and%2520computational%2520efficiency%2520on%2520par%2520with%2520the%250Abaseline%252C%2520encompassing%2520both%2520training%2520and%2520inference%2520time.%2520By%2520fusing%2520uncertainty%250Aquantification%2520and%2520foundation%2520models%2520within%2520the%2520context%2520of%2520monocular%2520depth%250Aestimation%252C%2520this%2520paper%2520lays%2520a%2520critical%2520foundation%2520for%2520future%2520research%2520aimed%2520at%250Aimproving%2520not%2520only%2520model%2520performance%2520but%2520also%2520its%2520explainability.%2520Extending%250Athis%2520critical%2520synthesis%2520of%2520uncertainty%2520quantification%2520and%2520foundation%2520models%250Ainto%2520other%2520crucial%2520tasks%252C%2520such%2520as%2520semantic%2520segmentation%2520and%2520pose%2520estimation%252C%250Apresents%2520exciting%2520opportunities%2520for%2520safer%2520and%2520more%2520reliable%2520machine%2520vision%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Critical%20Synthesis%20of%20Uncertainty%20Quantification%20and%20Foundation%20Models%0A%20%20in%20Monocular%20Depth%20Estimation&entry.906535625=Steven%20Landgraf%20and%20Rongjun%20Qin%20and%20Markus%20Ulrich&entry.1292438233=%20%20While%20recent%20foundation%20models%20have%20enabled%20significant%20breakthroughs%20in%0Amonocular%20depth%20estimation%2C%20a%20clear%20path%20towards%20safe%20and%20reliable%20deployment%0Ain%20the%20real-world%20remains%20elusive.%20Metric%20depth%20estimation%2C%20which%20involves%0Apredicting%20absolute%20distances%2C%20poses%20particular%20challenges%2C%20as%20even%20the%20most%0Aadvanced%20foundation%20models%20remain%20prone%20to%20critical%20errors.%20Since%20quantifying%0Athe%20uncertainty%20has%20emerged%20as%20a%20promising%20endeavor%20to%20address%20these%0Alimitations%20and%20enable%20trustworthy%20deployment%2C%20we%20fuse%20five%20different%0Auncertainty%20quantification%20methods%20with%20the%20current%20state-of-the-art%0ADepthAnythingV2%20foundation%20model.%20To%20cover%20a%20wide%20range%20of%20metric%20depth%0Adomains%2C%20we%20evaluate%20their%20performance%20on%20four%20diverse%20datasets.%20Our%20findings%0Aidentify%20fine-tuning%20with%20the%20Gaussian%20Negative%20Log-Likelihood%20Loss%20%28GNLL%29%20as%20a%0Aparticularly%20promising%20approach%2C%20offering%20reliable%20uncertainty%20estimates%20while%0Amaintaining%20predictive%20performance%20and%20computational%20efficiency%20on%20par%20with%20the%0Abaseline%2C%20encompassing%20both%20training%20and%20inference%20time.%20By%20fusing%20uncertainty%0Aquantification%20and%20foundation%20models%20within%20the%20context%20of%20monocular%20depth%0Aestimation%2C%20this%20paper%20lays%20a%20critical%20foundation%20for%20future%20research%20aimed%20at%0Aimproving%20not%20only%20model%20performance%20but%20also%20its%20explainability.%20Extending%0Athis%20critical%20synthesis%20of%20uncertainty%20quantification%20and%20foundation%20models%0Ainto%20other%20crucial%20tasks%2C%20such%20as%20semantic%20segmentation%20and%20pose%20estimation%2C%0Apresents%20exciting%20opportunities%20for%20safer%20and%20more%20reliable%20machine%20vision%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08188v1&entry.124074799=Read"},
{"title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution", "author": "Fengyuan Liu and Nikhil Kandpal and Colin Raffel", "abstract": "  The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.\n", "link": "http://arxiv.org/abs/2411.15102v2", "date": "2025-01-14", "relevancy": 2.3898, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution&body=Title%3A%20AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution%0AAuthor%3A%20Fengyuan%20Liu%20and%20Nikhil%20Kandpal%20and%20Colin%20Raffel%0AAbstract%3A%20%20%20The%20influence%20of%20contextual%20input%20on%20the%20behavior%20of%20large%20language%20models%0A%28LLMs%29%20has%20prompted%20the%20development%20of%20context%20attribution%20methods%20that%20aim%20to%0Aquantify%20each%20context%20span%27s%20effect%20on%20an%20LLM%27s%20generations.%20The%20leave-one-out%0A%28LOO%29%20error%2C%20which%20measures%20the%20change%20in%20the%20likelihood%20of%20the%20LLM%27s%20response%0Awhen%20a%20given%20span%20of%20the%20context%20is%20removed%2C%20provides%20a%20principled%20way%20to%0Aperform%20context%20attribution%2C%20but%20can%20be%20prohibitively%20expensive%20to%20compute%20for%0Alarge%20models.%20In%20this%20work%2C%20we%20introduce%20AttriBoT%2C%20a%20series%20of%20novel%20techniques%0Afor%20efficiently%20computing%20an%20approximation%20of%20the%20LOO%20error%20for%20context%0Aattribution.%20Specifically%2C%20AttriBoT%20uses%20cached%20activations%20to%20avoid%20redundant%0Aoperations%2C%20performs%20hierarchical%20attribution%20to%20reduce%20computation%2C%20and%0Aemulates%20the%20behavior%20of%20large%20target%20models%20with%20smaller%20proxy%20models.%20Taken%0Atogether%2C%20AttriBoT%20can%20provide%20a%20%3E300x%20speedup%20while%20remaining%20more%20faithful%20to%0Aa%20target%20model%27s%20LOO%20error%20than%20prior%20context%20attribution%20methods.%20This%20stark%0Aincrease%20in%20performance%20makes%20computing%20context%20attributions%20for%20a%20given%0Aresponse%2030x%20faster%20than%20generating%20the%20response%20itself%2C%20empowering%20real-world%0Aapplications%20that%20require%20computing%20attributions%20at%20scale.%20We%20release%20a%0Auser-friendly%20and%20efficient%20implementation%20of%20AttriBoT%20to%20enable%20efficient%20LLM%0Ainterpretability%20as%20well%20as%20encourage%20future%20development%20of%20efficient%20context%0Aattribution%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15102v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttriBoT%253A%2520A%2520Bag%2520of%2520Tricks%2520for%2520Efficiently%2520Approximating%2520Leave-One-Out%250A%2520%2520Context%2520Attribution%26entry.906535625%3DFengyuan%2520Liu%2520and%2520Nikhil%2520Kandpal%2520and%2520Colin%2520Raffel%26entry.1292438233%3D%2520%2520The%2520influence%2520of%2520contextual%2520input%2520on%2520the%2520behavior%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520has%2520prompted%2520the%2520development%2520of%2520context%2520attribution%2520methods%2520that%2520aim%2520to%250Aquantify%2520each%2520context%2520span%2527s%2520effect%2520on%2520an%2520LLM%2527s%2520generations.%2520The%2520leave-one-out%250A%2528LOO%2529%2520error%252C%2520which%2520measures%2520the%2520change%2520in%2520the%2520likelihood%2520of%2520the%2520LLM%2527s%2520response%250Awhen%2520a%2520given%2520span%2520of%2520the%2520context%2520is%2520removed%252C%2520provides%2520a%2520principled%2520way%2520to%250Aperform%2520context%2520attribution%252C%2520but%2520can%2520be%2520prohibitively%2520expensive%2520to%2520compute%2520for%250Alarge%2520models.%2520In%2520this%2520work%252C%2520we%2520introduce%2520AttriBoT%252C%2520a%2520series%2520of%2520novel%2520techniques%250Afor%2520efficiently%2520computing%2520an%2520approximation%2520of%2520the%2520LOO%2520error%2520for%2520context%250Aattribution.%2520Specifically%252C%2520AttriBoT%2520uses%2520cached%2520activations%2520to%2520avoid%2520redundant%250Aoperations%252C%2520performs%2520hierarchical%2520attribution%2520to%2520reduce%2520computation%252C%2520and%250Aemulates%2520the%2520behavior%2520of%2520large%2520target%2520models%2520with%2520smaller%2520proxy%2520models.%2520Taken%250Atogether%252C%2520AttriBoT%2520can%2520provide%2520a%2520%253E300x%2520speedup%2520while%2520remaining%2520more%2520faithful%2520to%250Aa%2520target%2520model%2527s%2520LOO%2520error%2520than%2520prior%2520context%2520attribution%2520methods.%2520This%2520stark%250Aincrease%2520in%2520performance%2520makes%2520computing%2520context%2520attributions%2520for%2520a%2520given%250Aresponse%252030x%2520faster%2520than%2520generating%2520the%2520response%2520itself%252C%2520empowering%2520real-world%250Aapplications%2520that%2520require%2520computing%2520attributions%2520at%2520scale.%2520We%2520release%2520a%250Auser-friendly%2520and%2520efficient%2520implementation%2520of%2520AttriBoT%2520to%2520enable%2520efficient%2520LLM%250Ainterpretability%2520as%2520well%2520as%2520encourage%2520future%2520development%2520of%2520efficient%2520context%250Aattribution%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15102v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttriBoT%3A%20A%20Bag%20of%20Tricks%20for%20Efficiently%20Approximating%20Leave-One-Out%0A%20%20Context%20Attribution&entry.906535625=Fengyuan%20Liu%20and%20Nikhil%20Kandpal%20and%20Colin%20Raffel&entry.1292438233=%20%20The%20influence%20of%20contextual%20input%20on%20the%20behavior%20of%20large%20language%20models%0A%28LLMs%29%20has%20prompted%20the%20development%20of%20context%20attribution%20methods%20that%20aim%20to%0Aquantify%20each%20context%20span%27s%20effect%20on%20an%20LLM%27s%20generations.%20The%20leave-one-out%0A%28LOO%29%20error%2C%20which%20measures%20the%20change%20in%20the%20likelihood%20of%20the%20LLM%27s%20response%0Awhen%20a%20given%20span%20of%20the%20context%20is%20removed%2C%20provides%20a%20principled%20way%20to%0Aperform%20context%20attribution%2C%20but%20can%20be%20prohibitively%20expensive%20to%20compute%20for%0Alarge%20models.%20In%20this%20work%2C%20we%20introduce%20AttriBoT%2C%20a%20series%20of%20novel%20techniques%0Afor%20efficiently%20computing%20an%20approximation%20of%20the%20LOO%20error%20for%20context%0Aattribution.%20Specifically%2C%20AttriBoT%20uses%20cached%20activations%20to%20avoid%20redundant%0Aoperations%2C%20performs%20hierarchical%20attribution%20to%20reduce%20computation%2C%20and%0Aemulates%20the%20behavior%20of%20large%20target%20models%20with%20smaller%20proxy%20models.%20Taken%0Atogether%2C%20AttriBoT%20can%20provide%20a%20%3E300x%20speedup%20while%20remaining%20more%20faithful%20to%0Aa%20target%20model%27s%20LOO%20error%20than%20prior%20context%20attribution%20methods.%20This%20stark%0Aincrease%20in%20performance%20makes%20computing%20context%20attributions%20for%20a%20given%0Aresponse%2030x%20faster%20than%20generating%20the%20response%20itself%2C%20empowering%20real-world%0Aapplications%20that%20require%20computing%20attributions%20at%20scale.%20We%20release%20a%0Auser-friendly%20and%20efficient%20implementation%20of%20AttriBoT%20to%20enable%20efficient%20LLM%0Ainterpretability%20as%20well%20as%20encourage%20future%20development%20of%20efficient%20context%0Aattribution%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15102v2&entry.124074799=Read"},
{"title": "Facial Dynamics in Video: Instruction Tuning for Improved Facial\n  Expression Perception and Contextual Awareness", "author": "Jiaxing Zhao and Boyuan Sun and Xiang Chen and Xihan Wei", "abstract": "  Facial expression captioning has found widespread application across various\ndomains. Recently, the emergence of video Multimodal Large Language Models\n(MLLMs) has shown promise in general video understanding tasks. However,\ndescribing facial expressions within videos poses two major challenges for\nthese models: (1) the lack of adequate datasets and benchmarks, and (2) the\nlimited visual token capacity of video MLLMs. To address these issues, this\npaper introduces a new instruction-following dataset tailored for dynamic\nfacial expression caption. The dataset comprises 5,033 high-quality video clips\nannotated manually, containing over 700,000 tokens. Its purpose is to improve\nthe capability of video MLLMs to discern subtle facial nuances. Furthermore, we\npropose FaceTrack-MM, which leverages a limited number of tokens to encode the\nmain character's face. This model demonstrates superior performance in tracking\nfaces and focusing on the facial expressions of the main characters, even in\nintricate multi-person scenarios. Additionally, we introduce a novel evaluation\nmetric combining event extraction, relation classification, and the longest\ncommon subsequence (LCS) algorithm to assess the content consistency and\ntemporal sequence consistency of generated text. Moreover, we present\nFEC-Bench, a benchmark designed to assess the performance of existing video\nMLLMs in this specific task. All data and source code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2501.07978v1", "date": "2025-01-14", "relevancy": 2.3863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5968}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facial%20Dynamics%20in%20Video%3A%20Instruction%20Tuning%20for%20Improved%20Facial%0A%20%20Expression%20Perception%20and%20Contextual%20Awareness&body=Title%3A%20Facial%20Dynamics%20in%20Video%3A%20Instruction%20Tuning%20for%20Improved%20Facial%0A%20%20Expression%20Perception%20and%20Contextual%20Awareness%0AAuthor%3A%20Jiaxing%20Zhao%20and%20Boyuan%20Sun%20and%20Xiang%20Chen%20and%20Xihan%20Wei%0AAbstract%3A%20%20%20Facial%20expression%20captioning%20has%20found%20widespread%20application%20across%20various%0Adomains.%20Recently%2C%20the%20emergence%20of%20video%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20has%20shown%20promise%20in%20general%20video%20understanding%20tasks.%20However%2C%0Adescribing%20facial%20expressions%20within%20videos%20poses%20two%20major%20challenges%20for%0Athese%20models%3A%20%281%29%20the%20lack%20of%20adequate%20datasets%20and%20benchmarks%2C%20and%20%282%29%20the%0Alimited%20visual%20token%20capacity%20of%20video%20MLLMs.%20To%20address%20these%20issues%2C%20this%0Apaper%20introduces%20a%20new%20instruction-following%20dataset%20tailored%20for%20dynamic%0Afacial%20expression%20caption.%20The%20dataset%20comprises%205%2C033%20high-quality%20video%20clips%0Aannotated%20manually%2C%20containing%20over%20700%2C000%20tokens.%20Its%20purpose%20is%20to%20improve%0Athe%20capability%20of%20video%20MLLMs%20to%20discern%20subtle%20facial%20nuances.%20Furthermore%2C%20we%0Apropose%20FaceTrack-MM%2C%20which%20leverages%20a%20limited%20number%20of%20tokens%20to%20encode%20the%0Amain%20character%27s%20face.%20This%20model%20demonstrates%20superior%20performance%20in%20tracking%0Afaces%20and%20focusing%20on%20the%20facial%20expressions%20of%20the%20main%20characters%2C%20even%20in%0Aintricate%20multi-person%20scenarios.%20Additionally%2C%20we%20introduce%20a%20novel%20evaluation%0Ametric%20combining%20event%20extraction%2C%20relation%20classification%2C%20and%20the%20longest%0Acommon%20subsequence%20%28LCS%29%20algorithm%20to%20assess%20the%20content%20consistency%20and%0Atemporal%20sequence%20consistency%20of%20generated%20text.%20Moreover%2C%20we%20present%0AFEC-Bench%2C%20a%20benchmark%20designed%20to%20assess%20the%20performance%20of%20existing%20video%0AMLLMs%20in%20this%20specific%20task.%20All%20data%20and%20source%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacial%2520Dynamics%2520in%2520Video%253A%2520Instruction%2520Tuning%2520for%2520Improved%2520Facial%250A%2520%2520Expression%2520Perception%2520and%2520Contextual%2520Awareness%26entry.906535625%3DJiaxing%2520Zhao%2520and%2520Boyuan%2520Sun%2520and%2520Xiang%2520Chen%2520and%2520Xihan%2520Wei%26entry.1292438233%3D%2520%2520Facial%2520expression%2520captioning%2520has%2520found%2520widespread%2520application%2520across%2520various%250Adomains.%2520Recently%252C%2520the%2520emergence%2520of%2520video%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520has%2520shown%2520promise%2520in%2520general%2520video%2520understanding%2520tasks.%2520However%252C%250Adescribing%2520facial%2520expressions%2520within%2520videos%2520poses%2520two%2520major%2520challenges%2520for%250Athese%2520models%253A%2520%25281%2529%2520the%2520lack%2520of%2520adequate%2520datasets%2520and%2520benchmarks%252C%2520and%2520%25282%2529%2520the%250Alimited%2520visual%2520token%2520capacity%2520of%2520video%2520MLLMs.%2520To%2520address%2520these%2520issues%252C%2520this%250Apaper%2520introduces%2520a%2520new%2520instruction-following%2520dataset%2520tailored%2520for%2520dynamic%250Afacial%2520expression%2520caption.%2520The%2520dataset%2520comprises%25205%252C033%2520high-quality%2520video%2520clips%250Aannotated%2520manually%252C%2520containing%2520over%2520700%252C000%2520tokens.%2520Its%2520purpose%2520is%2520to%2520improve%250Athe%2520capability%2520of%2520video%2520MLLMs%2520to%2520discern%2520subtle%2520facial%2520nuances.%2520Furthermore%252C%2520we%250Apropose%2520FaceTrack-MM%252C%2520which%2520leverages%2520a%2520limited%2520number%2520of%2520tokens%2520to%2520encode%2520the%250Amain%2520character%2527s%2520face.%2520This%2520model%2520demonstrates%2520superior%2520performance%2520in%2520tracking%250Afaces%2520and%2520focusing%2520on%2520the%2520facial%2520expressions%2520of%2520the%2520main%2520characters%252C%2520even%2520in%250Aintricate%2520multi-person%2520scenarios.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520evaluation%250Ametric%2520combining%2520event%2520extraction%252C%2520relation%2520classification%252C%2520and%2520the%2520longest%250Acommon%2520subsequence%2520%2528LCS%2529%2520algorithm%2520to%2520assess%2520the%2520content%2520consistency%2520and%250Atemporal%2520sequence%2520consistency%2520of%2520generated%2520text.%2520Moreover%252C%2520we%2520present%250AFEC-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520the%2520performance%2520of%2520existing%2520video%250AMLLMs%2520in%2520this%2520specific%2520task.%2520All%2520data%2520and%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facial%20Dynamics%20in%20Video%3A%20Instruction%20Tuning%20for%20Improved%20Facial%0A%20%20Expression%20Perception%20and%20Contextual%20Awareness&entry.906535625=Jiaxing%20Zhao%20and%20Boyuan%20Sun%20and%20Xiang%20Chen%20and%20Xihan%20Wei&entry.1292438233=%20%20Facial%20expression%20captioning%20has%20found%20widespread%20application%20across%20various%0Adomains.%20Recently%2C%20the%20emergence%20of%20video%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20has%20shown%20promise%20in%20general%20video%20understanding%20tasks.%20However%2C%0Adescribing%20facial%20expressions%20within%20videos%20poses%20two%20major%20challenges%20for%0Athese%20models%3A%20%281%29%20the%20lack%20of%20adequate%20datasets%20and%20benchmarks%2C%20and%20%282%29%20the%0Alimited%20visual%20token%20capacity%20of%20video%20MLLMs.%20To%20address%20these%20issues%2C%20this%0Apaper%20introduces%20a%20new%20instruction-following%20dataset%20tailored%20for%20dynamic%0Afacial%20expression%20caption.%20The%20dataset%20comprises%205%2C033%20high-quality%20video%20clips%0Aannotated%20manually%2C%20containing%20over%20700%2C000%20tokens.%20Its%20purpose%20is%20to%20improve%0Athe%20capability%20of%20video%20MLLMs%20to%20discern%20subtle%20facial%20nuances.%20Furthermore%2C%20we%0Apropose%20FaceTrack-MM%2C%20which%20leverages%20a%20limited%20number%20of%20tokens%20to%20encode%20the%0Amain%20character%27s%20face.%20This%20model%20demonstrates%20superior%20performance%20in%20tracking%0Afaces%20and%20focusing%20on%20the%20facial%20expressions%20of%20the%20main%20characters%2C%20even%20in%0Aintricate%20multi-person%20scenarios.%20Additionally%2C%20we%20introduce%20a%20novel%20evaluation%0Ametric%20combining%20event%20extraction%2C%20relation%20classification%2C%20and%20the%20longest%0Acommon%20subsequence%20%28LCS%29%20algorithm%20to%20assess%20the%20content%20consistency%20and%0Atemporal%20sequence%20consistency%20of%20generated%20text.%20Moreover%2C%20we%20present%0AFEC-Bench%2C%20a%20benchmark%20designed%20to%20assess%20the%20performance%20of%20existing%20video%0AMLLMs%20in%20this%20specific%20task.%20All%20data%20and%20source%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07978v1&entry.124074799=Read"},
{"title": "Exploring Narrative Clustering in Large Language Models: A Layerwise\n  Analysis of BERT", "author": "Awritrojit Banerjee and Achim Schilling and Patrick Krauss", "abstract": "  This study investigates the internal mechanisms of BERT, a transformer-based\nlarge language model, with a focus on its ability to cluster narrative content\nand authorial style across its layers. Using a dataset of narratives developed\nvia GPT-4, featuring diverse semantic content and stylistic variations, we\nanalyze BERT's layerwise activations to uncover patterns of localized neural\nprocessing. Through dimensionality reduction techniques such as Principal\nComponent Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that\nBERT exhibits strong clustering based on narrative content in its later layers,\nwith progressively compact and distinct clusters. While strong stylistic\nclustering might occur when narratives are rephrased into different text types\n(e.g., fables, sci-fi, kids' stories), minimal clustering is observed for\nauthorial style specific to individual writers. These findings highlight BERT's\nprioritization of semantic content over stylistic features, offering insights\ninto its representational capabilities and processing hierarchy. This study\ncontributes to understanding how transformer models like BERT encode linguistic\ninformation, paving the way for future interdisciplinary research in artificial\nintelligence and cognitive neuroscience.\n", "link": "http://arxiv.org/abs/2501.08053v1", "date": "2025-01-14", "relevancy": 2.3623, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Narrative%20Clustering%20in%20Large%20Language%20Models%3A%20A%20Layerwise%0A%20%20Analysis%20of%20BERT&body=Title%3A%20Exploring%20Narrative%20Clustering%20in%20Large%20Language%20Models%3A%20A%20Layerwise%0A%20%20Analysis%20of%20BERT%0AAuthor%3A%20Awritrojit%20Banerjee%20and%20Achim%20Schilling%20and%20Patrick%20Krauss%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20internal%20mechanisms%20of%20BERT%2C%20a%20transformer-based%0Alarge%20language%20model%2C%20with%20a%20focus%20on%20its%20ability%20to%20cluster%20narrative%20content%0Aand%20authorial%20style%20across%20its%20layers.%20Using%20a%20dataset%20of%20narratives%20developed%0Avia%20GPT-4%2C%20featuring%20diverse%20semantic%20content%20and%20stylistic%20variations%2C%20we%0Aanalyze%20BERT%27s%20layerwise%20activations%20to%20uncover%20patterns%20of%20localized%20neural%0Aprocessing.%20Through%20dimensionality%20reduction%20techniques%20such%20as%20Principal%0AComponent%20Analysis%20%28PCA%29%20and%20Multidimensional%20Scaling%20%28MDS%29%2C%20we%20reveal%20that%0ABERT%20exhibits%20strong%20clustering%20based%20on%20narrative%20content%20in%20its%20later%20layers%2C%0Awith%20progressively%20compact%20and%20distinct%20clusters.%20While%20strong%20stylistic%0Aclustering%20might%20occur%20when%20narratives%20are%20rephrased%20into%20different%20text%20types%0A%28e.g.%2C%20fables%2C%20sci-fi%2C%20kids%27%20stories%29%2C%20minimal%20clustering%20is%20observed%20for%0Aauthorial%20style%20specific%20to%20individual%20writers.%20These%20findings%20highlight%20BERT%27s%0Aprioritization%20of%20semantic%20content%20over%20stylistic%20features%2C%20offering%20insights%0Ainto%20its%20representational%20capabilities%20and%20processing%20hierarchy.%20This%20study%0Acontributes%20to%20understanding%20how%20transformer%20models%20like%20BERT%20encode%20linguistic%0Ainformation%2C%20paving%20the%20way%20for%20future%20interdisciplinary%20research%20in%20artificial%0Aintelligence%20and%20cognitive%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Narrative%2520Clustering%2520in%2520Large%2520Language%2520Models%253A%2520A%2520Layerwise%250A%2520%2520Analysis%2520of%2520BERT%26entry.906535625%3DAwritrojit%2520Banerjee%2520and%2520Achim%2520Schilling%2520and%2520Patrick%2520Krauss%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520internal%2520mechanisms%2520of%2520BERT%252C%2520a%2520transformer-based%250Alarge%2520language%2520model%252C%2520with%2520a%2520focus%2520on%2520its%2520ability%2520to%2520cluster%2520narrative%2520content%250Aand%2520authorial%2520style%2520across%2520its%2520layers.%2520Using%2520a%2520dataset%2520of%2520narratives%2520developed%250Avia%2520GPT-4%252C%2520featuring%2520diverse%2520semantic%2520content%2520and%2520stylistic%2520variations%252C%2520we%250Aanalyze%2520BERT%2527s%2520layerwise%2520activations%2520to%2520uncover%2520patterns%2520of%2520localized%2520neural%250Aprocessing.%2520Through%2520dimensionality%2520reduction%2520techniques%2520such%2520as%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529%2520and%2520Multidimensional%2520Scaling%2520%2528MDS%2529%252C%2520we%2520reveal%2520that%250ABERT%2520exhibits%2520strong%2520clustering%2520based%2520on%2520narrative%2520content%2520in%2520its%2520later%2520layers%252C%250Awith%2520progressively%2520compact%2520and%2520distinct%2520clusters.%2520While%2520strong%2520stylistic%250Aclustering%2520might%2520occur%2520when%2520narratives%2520are%2520rephrased%2520into%2520different%2520text%2520types%250A%2528e.g.%252C%2520fables%252C%2520sci-fi%252C%2520kids%2527%2520stories%2529%252C%2520minimal%2520clustering%2520is%2520observed%2520for%250Aauthorial%2520style%2520specific%2520to%2520individual%2520writers.%2520These%2520findings%2520highlight%2520BERT%2527s%250Aprioritization%2520of%2520semantic%2520content%2520over%2520stylistic%2520features%252C%2520offering%2520insights%250Ainto%2520its%2520representational%2520capabilities%2520and%2520processing%2520hierarchy.%2520This%2520study%250Acontributes%2520to%2520understanding%2520how%2520transformer%2520models%2520like%2520BERT%2520encode%2520linguistic%250Ainformation%252C%2520paving%2520the%2520way%2520for%2520future%2520interdisciplinary%2520research%2520in%2520artificial%250Aintelligence%2520and%2520cognitive%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Narrative%20Clustering%20in%20Large%20Language%20Models%3A%20A%20Layerwise%0A%20%20Analysis%20of%20BERT&entry.906535625=Awritrojit%20Banerjee%20and%20Achim%20Schilling%20and%20Patrick%20Krauss&entry.1292438233=%20%20This%20study%20investigates%20the%20internal%20mechanisms%20of%20BERT%2C%20a%20transformer-based%0Alarge%20language%20model%2C%20with%20a%20focus%20on%20its%20ability%20to%20cluster%20narrative%20content%0Aand%20authorial%20style%20across%20its%20layers.%20Using%20a%20dataset%20of%20narratives%20developed%0Avia%20GPT-4%2C%20featuring%20diverse%20semantic%20content%20and%20stylistic%20variations%2C%20we%0Aanalyze%20BERT%27s%20layerwise%20activations%20to%20uncover%20patterns%20of%20localized%20neural%0Aprocessing.%20Through%20dimensionality%20reduction%20techniques%20such%20as%20Principal%0AComponent%20Analysis%20%28PCA%29%20and%20Multidimensional%20Scaling%20%28MDS%29%2C%20we%20reveal%20that%0ABERT%20exhibits%20strong%20clustering%20based%20on%20narrative%20content%20in%20its%20later%20layers%2C%0Awith%20progressively%20compact%20and%20distinct%20clusters.%20While%20strong%20stylistic%0Aclustering%20might%20occur%20when%20narratives%20are%20rephrased%20into%20different%20text%20types%0A%28e.g.%2C%20fables%2C%20sci-fi%2C%20kids%27%20stories%29%2C%20minimal%20clustering%20is%20observed%20for%0Aauthorial%20style%20specific%20to%20individual%20writers.%20These%20findings%20highlight%20BERT%27s%0Aprioritization%20of%20semantic%20content%20over%20stylistic%20features%2C%20offering%20insights%0Ainto%20its%20representational%20capabilities%20and%20processing%20hierarchy.%20This%20study%0Acontributes%20to%20understanding%20how%20transformer%20models%20like%20BERT%20encode%20linguistic%0Ainformation%2C%20paving%20the%20way%20for%20future%20interdisciplinary%20research%20in%20artificial%0Aintelligence%20and%20cognitive%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08053v1&entry.124074799=Read"},
{"title": "D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models", "author": "Qian Zeng and Jie Song and Han Zheng and Hao Jiang and Mingli Song", "abstract": "  Diffusion models have achieved cutting-edge performance in image generation.\nHowever, their lengthy denoising process and computationally intensive score\nestimation network impede their scalability in low-latency and\nresource-constrained scenarios. Post-training quantization (PTQ) compresses and\naccelerates diffusion models without retraining, but it inevitably introduces\nadditional quantization noise, resulting in mean and variance deviations. In\nthis work, we propose D2-DPM, a dual denoising mechanism aimed at precisely\nmitigating the adverse effects of quantization noise on the noise estimation\nnetwork. Specifically, we first unravel the impact of quantization noise on the\nsampling equation into two components: the mean deviation and the variance\ndeviation. The mean deviation alters the drift coefficient of the sampling\nequation, influencing the trajectory trend, while the variance deviation\nmagnifies the diffusion coefficient, impacting the convergence of the sampling\ntrajectory. The proposed D2-DPM is thus devised to denoise the quantization\nnoise at each time step, and then denoise the noisy sample through the inverse\ndiffusion iterations. Experimental results demonstrate that D2-DPM achieves\nsuperior generation quality, yielding a 1.42 lower FID than the full-precision\nmodel while achieving 3.99x compression and 11.67x bit-operation acceleration.\n", "link": "http://arxiv.org/abs/2501.08180v1", "date": "2025-01-14", "relevancy": 2.3561, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6658}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.58}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D%24%5E2%24-DPM%3A%20Dual%20Denoising%20for%20Quantized%20Diffusion%20Probabilistic%20Models&body=Title%3A%20D%24%5E2%24-DPM%3A%20Dual%20Denoising%20for%20Quantized%20Diffusion%20Probabilistic%20Models%0AAuthor%3A%20Qian%20Zeng%20and%20Jie%20Song%20and%20Han%20Zheng%20and%20Hao%20Jiang%20and%20Mingli%20Song%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20cutting-edge%20performance%20in%20image%20generation.%0AHowever%2C%20their%20lengthy%20denoising%20process%20and%20computationally%20intensive%20score%0Aestimation%20network%20impede%20their%20scalability%20in%20low-latency%20and%0Aresource-constrained%20scenarios.%20Post-training%20quantization%20%28PTQ%29%20compresses%20and%0Aaccelerates%20diffusion%20models%20without%20retraining%2C%20but%20it%20inevitably%20introduces%0Aadditional%20quantization%20noise%2C%20resulting%20in%20mean%20and%20variance%20deviations.%20In%0Athis%20work%2C%20we%20propose%20D2-DPM%2C%20a%20dual%20denoising%20mechanism%20aimed%20at%20precisely%0Amitigating%20the%20adverse%20effects%20of%20quantization%20noise%20on%20the%20noise%20estimation%0Anetwork.%20Specifically%2C%20we%20first%20unravel%20the%20impact%20of%20quantization%20noise%20on%20the%0Asampling%20equation%20into%20two%20components%3A%20the%20mean%20deviation%20and%20the%20variance%0Adeviation.%20The%20mean%20deviation%20alters%20the%20drift%20coefficient%20of%20the%20sampling%0Aequation%2C%20influencing%20the%20trajectory%20trend%2C%20while%20the%20variance%20deviation%0Amagnifies%20the%20diffusion%20coefficient%2C%20impacting%20the%20convergence%20of%20the%20sampling%0Atrajectory.%20The%20proposed%20D2-DPM%20is%20thus%20devised%20to%20denoise%20the%20quantization%0Anoise%20at%20each%20time%20step%2C%20and%20then%20denoise%20the%20noisy%20sample%20through%20the%20inverse%0Adiffusion%20iterations.%20Experimental%20results%20demonstrate%20that%20D2-DPM%20achieves%0Asuperior%20generation%20quality%2C%20yielding%20a%201.42%20lower%20FID%20than%20the%20full-precision%0Amodel%20while%20achieving%203.99x%20compression%20and%2011.67x%20bit-operation%20acceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD%2524%255E2%2524-DPM%253A%2520Dual%2520Denoising%2520for%2520Quantized%2520Diffusion%2520Probabilistic%2520Models%26entry.906535625%3DQian%2520Zeng%2520and%2520Jie%2520Song%2520and%2520Han%2520Zheng%2520and%2520Hao%2520Jiang%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520achieved%2520cutting-edge%2520performance%2520in%2520image%2520generation.%250AHowever%252C%2520their%2520lengthy%2520denoising%2520process%2520and%2520computationally%2520intensive%2520score%250Aestimation%2520network%2520impede%2520their%2520scalability%2520in%2520low-latency%2520and%250Aresource-constrained%2520scenarios.%2520Post-training%2520quantization%2520%2528PTQ%2529%2520compresses%2520and%250Aaccelerates%2520diffusion%2520models%2520without%2520retraining%252C%2520but%2520it%2520inevitably%2520introduces%250Aadditional%2520quantization%2520noise%252C%2520resulting%2520in%2520mean%2520and%2520variance%2520deviations.%2520In%250Athis%2520work%252C%2520we%2520propose%2520D2-DPM%252C%2520a%2520dual%2520denoising%2520mechanism%2520aimed%2520at%2520precisely%250Amitigating%2520the%2520adverse%2520effects%2520of%2520quantization%2520noise%2520on%2520the%2520noise%2520estimation%250Anetwork.%2520Specifically%252C%2520we%2520first%2520unravel%2520the%2520impact%2520of%2520quantization%2520noise%2520on%2520the%250Asampling%2520equation%2520into%2520two%2520components%253A%2520the%2520mean%2520deviation%2520and%2520the%2520variance%250Adeviation.%2520The%2520mean%2520deviation%2520alters%2520the%2520drift%2520coefficient%2520of%2520the%2520sampling%250Aequation%252C%2520influencing%2520the%2520trajectory%2520trend%252C%2520while%2520the%2520variance%2520deviation%250Amagnifies%2520the%2520diffusion%2520coefficient%252C%2520impacting%2520the%2520convergence%2520of%2520the%2520sampling%250Atrajectory.%2520The%2520proposed%2520D2-DPM%2520is%2520thus%2520devised%2520to%2520denoise%2520the%2520quantization%250Anoise%2520at%2520each%2520time%2520step%252C%2520and%2520then%2520denoise%2520the%2520noisy%2520sample%2520through%2520the%2520inverse%250Adiffusion%2520iterations.%2520Experimental%2520results%2520demonstrate%2520that%2520D2-DPM%2520achieves%250Asuperior%2520generation%2520quality%252C%2520yielding%2520a%25201.42%2520lower%2520FID%2520than%2520the%2520full-precision%250Amodel%2520while%2520achieving%25203.99x%2520compression%2520and%252011.67x%2520bit-operation%2520acceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%24%5E2%24-DPM%3A%20Dual%20Denoising%20for%20Quantized%20Diffusion%20Probabilistic%20Models&entry.906535625=Qian%20Zeng%20and%20Jie%20Song%20and%20Han%20Zheng%20and%20Hao%20Jiang%20and%20Mingli%20Song&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20cutting-edge%20performance%20in%20image%20generation.%0AHowever%2C%20their%20lengthy%20denoising%20process%20and%20computationally%20intensive%20score%0Aestimation%20network%20impede%20their%20scalability%20in%20low-latency%20and%0Aresource-constrained%20scenarios.%20Post-training%20quantization%20%28PTQ%29%20compresses%20and%0Aaccelerates%20diffusion%20models%20without%20retraining%2C%20but%20it%20inevitably%20introduces%0Aadditional%20quantization%20noise%2C%20resulting%20in%20mean%20and%20variance%20deviations.%20In%0Athis%20work%2C%20we%20propose%20D2-DPM%2C%20a%20dual%20denoising%20mechanism%20aimed%20at%20precisely%0Amitigating%20the%20adverse%20effects%20of%20quantization%20noise%20on%20the%20noise%20estimation%0Anetwork.%20Specifically%2C%20we%20first%20unravel%20the%20impact%20of%20quantization%20noise%20on%20the%0Asampling%20equation%20into%20two%20components%3A%20the%20mean%20deviation%20and%20the%20variance%0Adeviation.%20The%20mean%20deviation%20alters%20the%20drift%20coefficient%20of%20the%20sampling%0Aequation%2C%20influencing%20the%20trajectory%20trend%2C%20while%20the%20variance%20deviation%0Amagnifies%20the%20diffusion%20coefficient%2C%20impacting%20the%20convergence%20of%20the%20sampling%0Atrajectory.%20The%20proposed%20D2-DPM%20is%20thus%20devised%20to%20denoise%20the%20quantization%0Anoise%20at%20each%20time%20step%2C%20and%20then%20denoise%20the%20noisy%20sample%20through%20the%20inverse%0Adiffusion%20iterations.%20Experimental%20results%20demonstrate%20that%20D2-DPM%20achieves%0Asuperior%20generation%20quality%2C%20yielding%20a%201.42%20lower%20FID%20than%20the%20full-precision%0Amodel%20while%20achieving%203.99x%20compression%20and%2011.67x%20bit-operation%20acceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08180v1&entry.124074799=Read"},
{"title": "Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded\n  Analysis", "author": "Jo\u00e3o Pedro Gandarela and Danilo S. Carvalho and Andr\u00e9 Freitas", "abstract": "  This work presents a novel systematic methodology to analyse the capabilities\nand limitations of Large Language Models (LLMs) with feedback from a formal\ninference engine, on logic theory induction. The analysis is complexity-graded\nw.r.t. rule dependency structure, allowing quantification of specific inference\nchallenges on LLM performance. Integrating LLMs with formal methods is a\npromising frontier in the Natural Language Processing field, as an important\navenue for improving model inference control and explainability. In particular,\ninductive learning over complex sets of facts and rules, poses unique\nchallenges for current autoregressive models, as they lack explicit symbolic\ngrounding. While they can be complemented by formal systems, the properties\ndelivered by LLMs regarding inductive learning, are not well understood and\nquantified. Empirical results indicate that the largest LLMs can achieve\ncompetitive results against a SOTA Inductive Logic Programming (ILP) system\nbaseline, but also that tracking long predicate relationship chains is a more\ndifficult obstacle than theory complexity for LLMs.\n", "link": "http://arxiv.org/abs/2408.16779v2", "date": "2025-01-14", "relevancy": 2.3552, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inductive%20Learning%20of%20Logical%20Theories%20with%20LLMs%3A%20An%20Expressivity-Graded%0A%20%20Analysis&body=Title%3A%20Inductive%20Learning%20of%20Logical%20Theories%20with%20LLMs%3A%20An%20Expressivity-Graded%0A%20%20Analysis%0AAuthor%3A%20Jo%C3%A3o%20Pedro%20Gandarela%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas%0AAbstract%3A%20%20%20This%20work%20presents%20a%20novel%20systematic%20methodology%20to%20analyse%20the%20capabilities%0Aand%20limitations%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20feedback%20from%20a%20formal%0Ainference%20engine%2C%20on%20logic%20theory%20induction.%20The%20analysis%20is%20complexity-graded%0Aw.r.t.%20rule%20dependency%20structure%2C%20allowing%20quantification%20of%20specific%20inference%0Achallenges%20on%20LLM%20performance.%20Integrating%20LLMs%20with%20formal%20methods%20is%20a%0Apromising%20frontier%20in%20the%20Natural%20Language%20Processing%20field%2C%20as%20an%20important%0Aavenue%20for%20improving%20model%20inference%20control%20and%20explainability.%20In%20particular%2C%0Ainductive%20learning%20over%20complex%20sets%20of%20facts%20and%20rules%2C%20poses%20unique%0Achallenges%20for%20current%20autoregressive%20models%2C%20as%20they%20lack%20explicit%20symbolic%0Agrounding.%20While%20they%20can%20be%20complemented%20by%20formal%20systems%2C%20the%20properties%0Adelivered%20by%20LLMs%20regarding%20inductive%20learning%2C%20are%20not%20well%20understood%20and%0Aquantified.%20Empirical%20results%20indicate%20that%20the%20largest%20LLMs%20can%20achieve%0Acompetitive%20results%20against%20a%20SOTA%20Inductive%20Logic%20Programming%20%28ILP%29%20system%0Abaseline%2C%20but%20also%20that%20tracking%20long%20predicate%20relationship%20chains%20is%20a%20more%0Adifficult%20obstacle%20than%20theory%20complexity%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16779v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInductive%2520Learning%2520of%2520Logical%2520Theories%2520with%2520LLMs%253A%2520An%2520Expressivity-Graded%250A%2520%2520Analysis%26entry.906535625%3DJo%25C3%25A3o%2520Pedro%2520Gandarela%2520and%2520Danilo%2520S.%2520Carvalho%2520and%2520Andr%25C3%25A9%2520Freitas%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520novel%2520systematic%2520methodology%2520to%2520analyse%2520the%2520capabilities%250Aand%2520limitations%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520feedback%2520from%2520a%2520formal%250Ainference%2520engine%252C%2520on%2520logic%2520theory%2520induction.%2520The%2520analysis%2520is%2520complexity-graded%250Aw.r.t.%2520rule%2520dependency%2520structure%252C%2520allowing%2520quantification%2520of%2520specific%2520inference%250Achallenges%2520on%2520LLM%2520performance.%2520Integrating%2520LLMs%2520with%2520formal%2520methods%2520is%2520a%250Apromising%2520frontier%2520in%2520the%2520Natural%2520Language%2520Processing%2520field%252C%2520as%2520an%2520important%250Aavenue%2520for%2520improving%2520model%2520inference%2520control%2520and%2520explainability.%2520In%2520particular%252C%250Ainductive%2520learning%2520over%2520complex%2520sets%2520of%2520facts%2520and%2520rules%252C%2520poses%2520unique%250Achallenges%2520for%2520current%2520autoregressive%2520models%252C%2520as%2520they%2520lack%2520explicit%2520symbolic%250Agrounding.%2520While%2520they%2520can%2520be%2520complemented%2520by%2520formal%2520systems%252C%2520the%2520properties%250Adelivered%2520by%2520LLMs%2520regarding%2520inductive%2520learning%252C%2520are%2520not%2520well%2520understood%2520and%250Aquantified.%2520Empirical%2520results%2520indicate%2520that%2520the%2520largest%2520LLMs%2520can%2520achieve%250Acompetitive%2520results%2520against%2520a%2520SOTA%2520Inductive%2520Logic%2520Programming%2520%2528ILP%2529%2520system%250Abaseline%252C%2520but%2520also%2520that%2520tracking%2520long%2520predicate%2520relationship%2520chains%2520is%2520a%2520more%250Adifficult%2520obstacle%2520than%2520theory%2520complexity%2520for%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16779v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inductive%20Learning%20of%20Logical%20Theories%20with%20LLMs%3A%20An%20Expressivity-Graded%0A%20%20Analysis&entry.906535625=Jo%C3%A3o%20Pedro%20Gandarela%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas&entry.1292438233=%20%20This%20work%20presents%20a%20novel%20systematic%20methodology%20to%20analyse%20the%20capabilities%0Aand%20limitations%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20feedback%20from%20a%20formal%0Ainference%20engine%2C%20on%20logic%20theory%20induction.%20The%20analysis%20is%20complexity-graded%0Aw.r.t.%20rule%20dependency%20structure%2C%20allowing%20quantification%20of%20specific%20inference%0Achallenges%20on%20LLM%20performance.%20Integrating%20LLMs%20with%20formal%20methods%20is%20a%0Apromising%20frontier%20in%20the%20Natural%20Language%20Processing%20field%2C%20as%20an%20important%0Aavenue%20for%20improving%20model%20inference%20control%20and%20explainability.%20In%20particular%2C%0Ainductive%20learning%20over%20complex%20sets%20of%20facts%20and%20rules%2C%20poses%20unique%0Achallenges%20for%20current%20autoregressive%20models%2C%20as%20they%20lack%20explicit%20symbolic%0Agrounding.%20While%20they%20can%20be%20complemented%20by%20formal%20systems%2C%20the%20properties%0Adelivered%20by%20LLMs%20regarding%20inductive%20learning%2C%20are%20not%20well%20understood%20and%0Aquantified.%20Empirical%20results%20indicate%20that%20the%20largest%20LLMs%20can%20achieve%0Acompetitive%20results%20against%20a%20SOTA%20Inductive%20Logic%20Programming%20%28ILP%29%20system%0Abaseline%2C%20but%20also%20that%20tracking%20long%20predicate%20relationship%20chains%20is%20a%20more%0Adifficult%20obstacle%20than%20theory%20complexity%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16779v2&entry.124074799=Read"},
{"title": "Benchmarking Multimodal Models for Fine-Grained Image Analysis: A\n  Comparative Study Across Diverse Visual Features", "author": "Evgenii Evstafev", "abstract": "  This article introduces a benchmark designed to evaluate the capabilities of\nmultimodal models in analyzing and interpreting images. The benchmark focuses\non seven key visual aspects: main object, additional objects, background,\ndetail, dominant colors, style, and viewpoint. A dataset of 14,580 images,\ngenerated from diverse text prompts, was used to assess the performance of\nseven leading multimodal models. These models were evaluated on their ability\nto accurately identify and describe each visual aspect, providing insights into\ntheir strengths and weaknesses for comprehensive image understanding. The\nfindings of this benchmark have significant implications for the development\nand selection of multimodal models for various image analysis tasks.\n", "link": "http://arxiv.org/abs/2501.08170v1", "date": "2025-01-14", "relevancy": 2.3474, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Multimodal%20Models%20for%20Fine-Grained%20Image%20Analysis%3A%20A%0A%20%20Comparative%20Study%20Across%20Diverse%20Visual%20Features&body=Title%3A%20Benchmarking%20Multimodal%20Models%20for%20Fine-Grained%20Image%20Analysis%3A%20A%0A%20%20Comparative%20Study%20Across%20Diverse%20Visual%20Features%0AAuthor%3A%20Evgenii%20Evstafev%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20benchmark%20designed%20to%20evaluate%20the%20capabilities%20of%0Amultimodal%20models%20in%20analyzing%20and%20interpreting%20images.%20The%20benchmark%20focuses%0Aon%20seven%20key%20visual%20aspects%3A%20main%20object%2C%20additional%20objects%2C%20background%2C%0Adetail%2C%20dominant%20colors%2C%20style%2C%20and%20viewpoint.%20A%20dataset%20of%2014%2C580%20images%2C%0Agenerated%20from%20diverse%20text%20prompts%2C%20was%20used%20to%20assess%20the%20performance%20of%0Aseven%20leading%20multimodal%20models.%20These%20models%20were%20evaluated%20on%20their%20ability%0Ato%20accurately%20identify%20and%20describe%20each%20visual%20aspect%2C%20providing%20insights%20into%0Atheir%20strengths%20and%20weaknesses%20for%20comprehensive%20image%20understanding.%20The%0Afindings%20of%20this%20benchmark%20have%20significant%20implications%20for%20the%20development%0Aand%20selection%20of%20multimodal%20models%20for%20various%20image%20analysis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Multimodal%2520Models%2520for%2520Fine-Grained%2520Image%2520Analysis%253A%2520A%250A%2520%2520Comparative%2520Study%2520Across%2520Diverse%2520Visual%2520Features%26entry.906535625%3DEvgenii%2520Evstafev%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520capabilities%2520of%250Amultimodal%2520models%2520in%2520analyzing%2520and%2520interpreting%2520images.%2520The%2520benchmark%2520focuses%250Aon%2520seven%2520key%2520visual%2520aspects%253A%2520main%2520object%252C%2520additional%2520objects%252C%2520background%252C%250Adetail%252C%2520dominant%2520colors%252C%2520style%252C%2520and%2520viewpoint.%2520A%2520dataset%2520of%252014%252C580%2520images%252C%250Agenerated%2520from%2520diverse%2520text%2520prompts%252C%2520was%2520used%2520to%2520assess%2520the%2520performance%2520of%250Aseven%2520leading%2520multimodal%2520models.%2520These%2520models%2520were%2520evaluated%2520on%2520their%2520ability%250Ato%2520accurately%2520identify%2520and%2520describe%2520each%2520visual%2520aspect%252C%2520providing%2520insights%2520into%250Atheir%2520strengths%2520and%2520weaknesses%2520for%2520comprehensive%2520image%2520understanding.%2520The%250Afindings%2520of%2520this%2520benchmark%2520have%2520significant%2520implications%2520for%2520the%2520development%250Aand%2520selection%2520of%2520multimodal%2520models%2520for%2520various%2520image%2520analysis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Multimodal%20Models%20for%20Fine-Grained%20Image%20Analysis%3A%20A%0A%20%20Comparative%20Study%20Across%20Diverse%20Visual%20Features&entry.906535625=Evgenii%20Evstafev&entry.1292438233=%20%20This%20article%20introduces%20a%20benchmark%20designed%20to%20evaluate%20the%20capabilities%20of%0Amultimodal%20models%20in%20analyzing%20and%20interpreting%20images.%20The%20benchmark%20focuses%0Aon%20seven%20key%20visual%20aspects%3A%20main%20object%2C%20additional%20objects%2C%20background%2C%0Adetail%2C%20dominant%20colors%2C%20style%2C%20and%20viewpoint.%20A%20dataset%20of%2014%2C580%20images%2C%0Agenerated%20from%20diverse%20text%20prompts%2C%20was%20used%20to%20assess%20the%20performance%20of%0Aseven%20leading%20multimodal%20models.%20These%20models%20were%20evaluated%20on%20their%20ability%0Ato%20accurately%20identify%20and%20describe%20each%20visual%20aspect%2C%20providing%20insights%20into%0Atheir%20strengths%20and%20weaknesses%20for%20comprehensive%20image%20understanding.%20The%0Afindings%20of%20this%20benchmark%20have%20significant%20implications%20for%20the%20development%0Aand%20selection%20of%20multimodal%20models%20for%20various%20image%20analysis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08170v1&entry.124074799=Read"},
{"title": "LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and\n  Dual-Process Thinking", "author": "Yukai Ma and Tiantian Wei and Naiting Zhong and Jianbiao Mei and Tao Hu and Licheng Wen and Xuemeng Yang and Botian Shi and Yong Liu", "abstract": "  While autonomous driving technology has made remarkable strides, data-driven\napproaches still struggle with complex scenarios due to their limited reasoning\ncapabilities. Meanwhile, knowledge-driven autonomous driving systems have\nevolved considerably with the popularization of visual language models. In this\npaper, we propose LeapVAD, a novel method based on cognitive perception and\ndual-process thinking. Our approach implements a human-attentional mechanism to\nidentify and focus on critical traffic elements that influence driving\ndecisions. By characterizing these objects through comprehensive attributes -\nincluding appearance, motion patterns, and associated risks - LeapVAD achieves\nmore effective environmental representation and streamlines the decision-making\nprocess. Furthermore, LeapVAD incorporates an innovative dual-process\ndecision-making module miming the human-driving learning process. The system\nconsists of an Analytic Process (System-II) that accumulates driving experience\nthrough logical reasoning and a Heuristic Process (System-I) that refines this\nknowledge via fine-tuning and few-shot learning. LeapVAD also includes\nreflective mechanisms and a growing memory bank, enabling it to learn from past\nmistakes and continuously improve its performance in a closed-loop environment.\nTo enhance efficiency, we develop a scene encoder network that generates\ncompact scene representations for rapid retrieval of relevant driving\nexperiences. Extensive evaluations conducted on two leading autonomous driving\nsimulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior\nperformance compared to camera-only approaches despite limited training data.\nComprehensive ablation studies further emphasize its effectiveness in\ncontinuous learning and domain adaptation. Project page:\nhttps://pjlab-adg.github.io/LeapVAD/.\n", "link": "http://arxiv.org/abs/2501.08168v1", "date": "2025-01-14", "relevancy": 2.3461, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5889}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeapVAD%3A%20A%20Leap%20in%20Autonomous%20Driving%20via%20Cognitive%20Perception%20and%0A%20%20Dual-Process%20Thinking&body=Title%3A%20LeapVAD%3A%20A%20Leap%20in%20Autonomous%20Driving%20via%20Cognitive%20Perception%20and%0A%20%20Dual-Process%20Thinking%0AAuthor%3A%20Yukai%20Ma%20and%20Tiantian%20Wei%20and%20Naiting%20Zhong%20and%20Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Botian%20Shi%20and%20Yong%20Liu%0AAbstract%3A%20%20%20While%20autonomous%20driving%20technology%20has%20made%20remarkable%20strides%2C%20data-driven%0Aapproaches%20still%20struggle%20with%20complex%20scenarios%20due%20to%20their%20limited%20reasoning%0Acapabilities.%20Meanwhile%2C%20knowledge-driven%20autonomous%20driving%20systems%20have%0Aevolved%20considerably%20with%20the%20popularization%20of%20visual%20language%20models.%20In%20this%0Apaper%2C%20we%20propose%20LeapVAD%2C%20a%20novel%20method%20based%20on%20cognitive%20perception%20and%0Adual-process%20thinking.%20Our%20approach%20implements%20a%20human-attentional%20mechanism%20to%0Aidentify%20and%20focus%20on%20critical%20traffic%20elements%20that%20influence%20driving%0Adecisions.%20By%20characterizing%20these%20objects%20through%20comprehensive%20attributes%20-%0Aincluding%20appearance%2C%20motion%20patterns%2C%20and%20associated%20risks%20-%20LeapVAD%20achieves%0Amore%20effective%20environmental%20representation%20and%20streamlines%20the%20decision-making%0Aprocess.%20Furthermore%2C%20LeapVAD%20incorporates%20an%20innovative%20dual-process%0Adecision-making%20module%20miming%20the%20human-driving%20learning%20process.%20The%20system%0Aconsists%20of%20an%20Analytic%20Process%20%28System-II%29%20that%20accumulates%20driving%20experience%0Athrough%20logical%20reasoning%20and%20a%20Heuristic%20Process%20%28System-I%29%20that%20refines%20this%0Aknowledge%20via%20fine-tuning%20and%20few-shot%20learning.%20LeapVAD%20also%20includes%0Areflective%20mechanisms%20and%20a%20growing%20memory%20bank%2C%20enabling%20it%20to%20learn%20from%20past%0Amistakes%20and%20continuously%20improve%20its%20performance%20in%20a%20closed-loop%20environment.%0ATo%20enhance%20efficiency%2C%20we%20develop%20a%20scene%20encoder%20network%20that%20generates%0Acompact%20scene%20representations%20for%20rapid%20retrieval%20of%20relevant%20driving%0Aexperiences.%20Extensive%20evaluations%20conducted%20on%20two%20leading%20autonomous%20driving%0Asimulators%2C%20CARLA%20and%20DriveArena%2C%20demonstrate%20that%20LeapVAD%20achieves%20superior%0Aperformance%20compared%20to%20camera-only%20approaches%20despite%20limited%20training%20data.%0AComprehensive%20ablation%20studies%20further%20emphasize%20its%20effectiveness%20in%0Acontinuous%20learning%20and%20domain%20adaptation.%20Project%20page%3A%0Ahttps%3A//pjlab-adg.github.io/LeapVAD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeapVAD%253A%2520A%2520Leap%2520in%2520Autonomous%2520Driving%2520via%2520Cognitive%2520Perception%2520and%250A%2520%2520Dual-Process%2520Thinking%26entry.906535625%3DYukai%2520Ma%2520and%2520Tiantian%2520Wei%2520and%2520Naiting%2520Zhong%2520and%2520Jianbiao%2520Mei%2520and%2520Tao%2520Hu%2520and%2520Licheng%2520Wen%2520and%2520Xuemeng%2520Yang%2520and%2520Botian%2520Shi%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520While%2520autonomous%2520driving%2520technology%2520has%2520made%2520remarkable%2520strides%252C%2520data-driven%250Aapproaches%2520still%2520struggle%2520with%2520complex%2520scenarios%2520due%2520to%2520their%2520limited%2520reasoning%250Acapabilities.%2520Meanwhile%252C%2520knowledge-driven%2520autonomous%2520driving%2520systems%2520have%250Aevolved%2520considerably%2520with%2520the%2520popularization%2520of%2520visual%2520language%2520models.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520LeapVAD%252C%2520a%2520novel%2520method%2520based%2520on%2520cognitive%2520perception%2520and%250Adual-process%2520thinking.%2520Our%2520approach%2520implements%2520a%2520human-attentional%2520mechanism%2520to%250Aidentify%2520and%2520focus%2520on%2520critical%2520traffic%2520elements%2520that%2520influence%2520driving%250Adecisions.%2520By%2520characterizing%2520these%2520objects%2520through%2520comprehensive%2520attributes%2520-%250Aincluding%2520appearance%252C%2520motion%2520patterns%252C%2520and%2520associated%2520risks%2520-%2520LeapVAD%2520achieves%250Amore%2520effective%2520environmental%2520representation%2520and%2520streamlines%2520the%2520decision-making%250Aprocess.%2520Furthermore%252C%2520LeapVAD%2520incorporates%2520an%2520innovative%2520dual-process%250Adecision-making%2520module%2520miming%2520the%2520human-driving%2520learning%2520process.%2520The%2520system%250Aconsists%2520of%2520an%2520Analytic%2520Process%2520%2528System-II%2529%2520that%2520accumulates%2520driving%2520experience%250Athrough%2520logical%2520reasoning%2520and%2520a%2520Heuristic%2520Process%2520%2528System-I%2529%2520that%2520refines%2520this%250Aknowledge%2520via%2520fine-tuning%2520and%2520few-shot%2520learning.%2520LeapVAD%2520also%2520includes%250Areflective%2520mechanisms%2520and%2520a%2520growing%2520memory%2520bank%252C%2520enabling%2520it%2520to%2520learn%2520from%2520past%250Amistakes%2520and%2520continuously%2520improve%2520its%2520performance%2520in%2520a%2520closed-loop%2520environment.%250ATo%2520enhance%2520efficiency%252C%2520we%2520develop%2520a%2520scene%2520encoder%2520network%2520that%2520generates%250Acompact%2520scene%2520representations%2520for%2520rapid%2520retrieval%2520of%2520relevant%2520driving%250Aexperiences.%2520Extensive%2520evaluations%2520conducted%2520on%2520two%2520leading%2520autonomous%2520driving%250Asimulators%252C%2520CARLA%2520and%2520DriveArena%252C%2520demonstrate%2520that%2520LeapVAD%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520camera-only%2520approaches%2520despite%2520limited%2520training%2520data.%250AComprehensive%2520ablation%2520studies%2520further%2520emphasize%2520its%2520effectiveness%2520in%250Acontinuous%2520learning%2520and%2520domain%2520adaptation.%2520Project%2520page%253A%250Ahttps%253A//pjlab-adg.github.io/LeapVAD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeapVAD%3A%20A%20Leap%20in%20Autonomous%20Driving%20via%20Cognitive%20Perception%20and%0A%20%20Dual-Process%20Thinking&entry.906535625=Yukai%20Ma%20and%20Tiantian%20Wei%20and%20Naiting%20Zhong%20and%20Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Botian%20Shi%20and%20Yong%20Liu&entry.1292438233=%20%20While%20autonomous%20driving%20technology%20has%20made%20remarkable%20strides%2C%20data-driven%0Aapproaches%20still%20struggle%20with%20complex%20scenarios%20due%20to%20their%20limited%20reasoning%0Acapabilities.%20Meanwhile%2C%20knowledge-driven%20autonomous%20driving%20systems%20have%0Aevolved%20considerably%20with%20the%20popularization%20of%20visual%20language%20models.%20In%20this%0Apaper%2C%20we%20propose%20LeapVAD%2C%20a%20novel%20method%20based%20on%20cognitive%20perception%20and%0Adual-process%20thinking.%20Our%20approach%20implements%20a%20human-attentional%20mechanism%20to%0Aidentify%20and%20focus%20on%20critical%20traffic%20elements%20that%20influence%20driving%0Adecisions.%20By%20characterizing%20these%20objects%20through%20comprehensive%20attributes%20-%0Aincluding%20appearance%2C%20motion%20patterns%2C%20and%20associated%20risks%20-%20LeapVAD%20achieves%0Amore%20effective%20environmental%20representation%20and%20streamlines%20the%20decision-making%0Aprocess.%20Furthermore%2C%20LeapVAD%20incorporates%20an%20innovative%20dual-process%0Adecision-making%20module%20miming%20the%20human-driving%20learning%20process.%20The%20system%0Aconsists%20of%20an%20Analytic%20Process%20%28System-II%29%20that%20accumulates%20driving%20experience%0Athrough%20logical%20reasoning%20and%20a%20Heuristic%20Process%20%28System-I%29%20that%20refines%20this%0Aknowledge%20via%20fine-tuning%20and%20few-shot%20learning.%20LeapVAD%20also%20includes%0Areflective%20mechanisms%20and%20a%20growing%20memory%20bank%2C%20enabling%20it%20to%20learn%20from%20past%0Amistakes%20and%20continuously%20improve%20its%20performance%20in%20a%20closed-loop%20environment.%0ATo%20enhance%20efficiency%2C%20we%20develop%20a%20scene%20encoder%20network%20that%20generates%0Acompact%20scene%20representations%20for%20rapid%20retrieval%20of%20relevant%20driving%0Aexperiences.%20Extensive%20evaluations%20conducted%20on%20two%20leading%20autonomous%20driving%0Asimulators%2C%20CARLA%20and%20DriveArena%2C%20demonstrate%20that%20LeapVAD%20achieves%20superior%0Aperformance%20compared%20to%20camera-only%20approaches%20despite%20limited%20training%20data.%0AComprehensive%20ablation%20studies%20further%20emphasize%20its%20effectiveness%20in%0Acontinuous%20learning%20and%20domain%20adaptation.%20Project%20page%3A%0Ahttps%3A//pjlab-adg.github.io/LeapVAD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08168v1&entry.124074799=Read"},
{"title": "Benchmarking Vision Foundation Models for Input Monitoring in Autonomous\n  Driving", "author": "Nert Keser and Halil Ibrahim Orhan and Niki Amini-Naieni and Gesina Schwalbe and Alois Knoll and Matthias Rottmann", "abstract": "  Deep neural networks (DNNs) remain challenged by distribution shifts in\ncomplex open-world domains like automated driving (AD): Absolute robustness\nagainst yet unknown novel objects (semantic shift) or styles like lighting\nconditions (covariate shift) cannot be guaranteed. Hence, reliable\noperation-time monitors for identification of out-of-training-data-distribution\n(OOD) scenarios are imperative. Current approaches for OOD classification are\nuntested for complex domains like AD, are limited in the kinds of shifts they\ndetect, or even require supervision with OOD samples. To prepare for\nunanticipated shifts, we instead establish a framework around a principled,\nunsupervised, and model-agnostic method that unifies detection of all kinds of\nshifts: Find a full model of the training data's feature distribution, to then\nuse its density at new points as in-distribution (ID) score. To implement this,\nwe propose to combine the newly available Vision Foundation Models (VFM) as\nfeature extractors with one of four alternative density modeling techniques. In\nan extensive benchmark of 4 VFMs against 20 baselines, we show the superior\nperformance of VFM feature encodings compared to shift-specific OOD monitors.\nAdditionally, we find that sophisticated architectures outperform larger latent\nspace dimensionality; and our method identifies samples with higher risk of\nerrors on downstream tasks, despite being model-agnostic. This suggests that\nVFMs are promising to realize model-agnostic, unsupervised, reliable safety\nmonitors in complex vision tasks.\n", "link": "http://arxiv.org/abs/2501.08083v1", "date": "2025-01-14", "relevancy": 2.3333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving&body=Title%3A%20Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Nert%20Keser%20and%20Halil%20Ibrahim%20Orhan%20and%20Niki%20Amini-Naieni%20and%20Gesina%20Schwalbe%20and%20Alois%20Knoll%20and%20Matthias%20Rottmann%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20remain%20challenged%20by%20distribution%20shifts%20in%0Acomplex%20open-world%20domains%20like%20automated%20driving%20%28AD%29%3A%20Absolute%20robustness%0Aagainst%20yet%20unknown%20novel%20objects%20%28semantic%20shift%29%20or%20styles%20like%20lighting%0Aconditions%20%28covariate%20shift%29%20cannot%20be%20guaranteed.%20Hence%2C%20reliable%0Aoperation-time%20monitors%20for%20identification%20of%20out-of-training-data-distribution%0A%28OOD%29%20scenarios%20are%20imperative.%20Current%20approaches%20for%20OOD%20classification%20are%0Auntested%20for%20complex%20domains%20like%20AD%2C%20are%20limited%20in%20the%20kinds%20of%20shifts%20they%0Adetect%2C%20or%20even%20require%20supervision%20with%20OOD%20samples.%20To%20prepare%20for%0Aunanticipated%20shifts%2C%20we%20instead%20establish%20a%20framework%20around%20a%20principled%2C%0Aunsupervised%2C%20and%20model-agnostic%20method%20that%20unifies%20detection%20of%20all%20kinds%20of%0Ashifts%3A%20Find%20a%20full%20model%20of%20the%20training%20data%27s%20feature%20distribution%2C%20to%20then%0Ause%20its%20density%20at%20new%20points%20as%20in-distribution%20%28ID%29%20score.%20To%20implement%20this%2C%0Awe%20propose%20to%20combine%20the%20newly%20available%20Vision%20Foundation%20Models%20%28VFM%29%20as%0Afeature%20extractors%20with%20one%20of%20four%20alternative%20density%20modeling%20techniques.%20In%0Aan%20extensive%20benchmark%20of%204%20VFMs%20against%2020%20baselines%2C%20we%20show%20the%20superior%0Aperformance%20of%20VFM%20feature%20encodings%20compared%20to%20shift-specific%20OOD%20monitors.%0AAdditionally%2C%20we%20find%20that%20sophisticated%20architectures%20outperform%20larger%20latent%0Aspace%20dimensionality%3B%20and%20our%20method%20identifies%20samples%20with%20higher%20risk%20of%0Aerrors%20on%20downstream%20tasks%2C%20despite%20being%20model-agnostic.%20This%20suggests%20that%0AVFMs%20are%20promising%20to%20realize%20model-agnostic%2C%20unsupervised%2C%20reliable%20safety%0Amonitors%20in%20complex%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision%2520Foundation%2520Models%2520for%2520Input%2520Monitoring%2520in%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DNert%2520Keser%2520and%2520Halil%2520Ibrahim%2520Orhan%2520and%2520Niki%2520Amini-Naieni%2520and%2520Gesina%2520Schwalbe%2520and%2520Alois%2520Knoll%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520remain%2520challenged%2520by%2520distribution%2520shifts%2520in%250Acomplex%2520open-world%2520domains%2520like%2520automated%2520driving%2520%2528AD%2529%253A%2520Absolute%2520robustness%250Aagainst%2520yet%2520unknown%2520novel%2520objects%2520%2528semantic%2520shift%2529%2520or%2520styles%2520like%2520lighting%250Aconditions%2520%2528covariate%2520shift%2529%2520cannot%2520be%2520guaranteed.%2520Hence%252C%2520reliable%250Aoperation-time%2520monitors%2520for%2520identification%2520of%2520out-of-training-data-distribution%250A%2528OOD%2529%2520scenarios%2520are%2520imperative.%2520Current%2520approaches%2520for%2520OOD%2520classification%2520are%250Auntested%2520for%2520complex%2520domains%2520like%2520AD%252C%2520are%2520limited%2520in%2520the%2520kinds%2520of%2520shifts%2520they%250Adetect%252C%2520or%2520even%2520require%2520supervision%2520with%2520OOD%2520samples.%2520To%2520prepare%2520for%250Aunanticipated%2520shifts%252C%2520we%2520instead%2520establish%2520a%2520framework%2520around%2520a%2520principled%252C%250Aunsupervised%252C%2520and%2520model-agnostic%2520method%2520that%2520unifies%2520detection%2520of%2520all%2520kinds%2520of%250Ashifts%253A%2520Find%2520a%2520full%2520model%2520of%2520the%2520training%2520data%2527s%2520feature%2520distribution%252C%2520to%2520then%250Ause%2520its%2520density%2520at%2520new%2520points%2520as%2520in-distribution%2520%2528ID%2529%2520score.%2520To%2520implement%2520this%252C%250Awe%2520propose%2520to%2520combine%2520the%2520newly%2520available%2520Vision%2520Foundation%2520Models%2520%2528VFM%2529%2520as%250Afeature%2520extractors%2520with%2520one%2520of%2520four%2520alternative%2520density%2520modeling%2520techniques.%2520In%250Aan%2520extensive%2520benchmark%2520of%25204%2520VFMs%2520against%252020%2520baselines%252C%2520we%2520show%2520the%2520superior%250Aperformance%2520of%2520VFM%2520feature%2520encodings%2520compared%2520to%2520shift-specific%2520OOD%2520monitors.%250AAdditionally%252C%2520we%2520find%2520that%2520sophisticated%2520architectures%2520outperform%2520larger%2520latent%250Aspace%2520dimensionality%253B%2520and%2520our%2520method%2520identifies%2520samples%2520with%2520higher%2520risk%2520of%250Aerrors%2520on%2520downstream%2520tasks%252C%2520despite%2520being%2520model-agnostic.%2520This%2520suggests%2520that%250AVFMs%2520are%2520promising%2520to%2520realize%2520model-agnostic%252C%2520unsupervised%252C%2520reliable%2520safety%250Amonitors%2520in%2520complex%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision%20Foundation%20Models%20for%20Input%20Monitoring%20in%20Autonomous%0A%20%20Driving&entry.906535625=Nert%20Keser%20and%20Halil%20Ibrahim%20Orhan%20and%20Niki%20Amini-Naieni%20and%20Gesina%20Schwalbe%20and%20Alois%20Knoll%20and%20Matthias%20Rottmann&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20remain%20challenged%20by%20distribution%20shifts%20in%0Acomplex%20open-world%20domains%20like%20automated%20driving%20%28AD%29%3A%20Absolute%20robustness%0Aagainst%20yet%20unknown%20novel%20objects%20%28semantic%20shift%29%20or%20styles%20like%20lighting%0Aconditions%20%28covariate%20shift%29%20cannot%20be%20guaranteed.%20Hence%2C%20reliable%0Aoperation-time%20monitors%20for%20identification%20of%20out-of-training-data-distribution%0A%28OOD%29%20scenarios%20are%20imperative.%20Current%20approaches%20for%20OOD%20classification%20are%0Auntested%20for%20complex%20domains%20like%20AD%2C%20are%20limited%20in%20the%20kinds%20of%20shifts%20they%0Adetect%2C%20or%20even%20require%20supervision%20with%20OOD%20samples.%20To%20prepare%20for%0Aunanticipated%20shifts%2C%20we%20instead%20establish%20a%20framework%20around%20a%20principled%2C%0Aunsupervised%2C%20and%20model-agnostic%20method%20that%20unifies%20detection%20of%20all%20kinds%20of%0Ashifts%3A%20Find%20a%20full%20model%20of%20the%20training%20data%27s%20feature%20distribution%2C%20to%20then%0Ause%20its%20density%20at%20new%20points%20as%20in-distribution%20%28ID%29%20score.%20To%20implement%20this%2C%0Awe%20propose%20to%20combine%20the%20newly%20available%20Vision%20Foundation%20Models%20%28VFM%29%20as%0Afeature%20extractors%20with%20one%20of%20four%20alternative%20density%20modeling%20techniques.%20In%0Aan%20extensive%20benchmark%20of%204%20VFMs%20against%2020%20baselines%2C%20we%20show%20the%20superior%0Aperformance%20of%20VFM%20feature%20encodings%20compared%20to%20shift-specific%20OOD%20monitors.%0AAdditionally%2C%20we%20find%20that%20sophisticated%20architectures%20outperform%20larger%20latent%0Aspace%20dimensionality%3B%20and%20our%20method%20identifies%20samples%20with%20higher%20risk%20of%0Aerrors%20on%20downstream%20tasks%2C%20despite%20being%20model-agnostic.%20This%20suggests%20that%0AVFMs%20are%20promising%20to%20realize%20model-agnostic%2C%20unsupervised%2C%20reliable%20safety%0Amonitors%20in%20complex%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08083v1&entry.124074799=Read"},
{"title": "A Roadmap to Guide the Integration of LLMs in Hierarchical Planning", "author": "Israel Puerta-Merino and Carlos N\u00fa\u00f1ez-Molina and Pablo Mesejo and Juan Fern\u00e1ndez-Olivares", "abstract": "  Recent advances in Large Language Models (LLMs) are fostering their\nintegration into several reasoning-related fields, including Automated Planning\n(AP). However, their integration into Hierarchical Planning (HP), a subfield of\nAP that leverages hierarchical knowledge to enhance planning performance,\nremains largely unexplored. In this preliminary work, we propose a roadmap to\naddress this gap and harness the potential of LLMs for HP. To this end, we\npresent a taxonomy of integration methods, exploring how LLMs can be utilized\nwithin the HP life cycle. Additionally, we provide a benchmark with a\nstandardized dataset for evaluating the performance of future LLM-based HP\napproaches, and present initial results for a state-of-the-art HP planner and\nLLM planner. As expected, the latter exhibits limited performance (3\\% correct\nplans, and none with a correct hierarchical decomposition) but serves as a\nvaluable baseline for future approaches.\n", "link": "http://arxiv.org/abs/2501.08068v1", "date": "2025-01-14", "relevancy": 2.3299, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4879}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Roadmap%20to%20Guide%20the%20Integration%20of%20LLMs%20in%20Hierarchical%20Planning&body=Title%3A%20A%20Roadmap%20to%20Guide%20the%20Integration%20of%20LLMs%20in%20Hierarchical%20Planning%0AAuthor%3A%20Israel%20Puerta-Merino%20and%20Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Pablo%20Mesejo%20and%20Juan%20Fern%C3%A1ndez-Olivares%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20fostering%20their%0Aintegration%20into%20several%20reasoning-related%20fields%2C%20including%20Automated%20Planning%0A%28AP%29.%20However%2C%20their%20integration%20into%20Hierarchical%20Planning%20%28HP%29%2C%20a%20subfield%20of%0AAP%20that%20leverages%20hierarchical%20knowledge%20to%20enhance%20planning%20performance%2C%0Aremains%20largely%20unexplored.%20In%20this%20preliminary%20work%2C%20we%20propose%20a%20roadmap%20to%0Aaddress%20this%20gap%20and%20harness%20the%20potential%20of%20LLMs%20for%20HP.%20To%20this%20end%2C%20we%0Apresent%20a%20taxonomy%20of%20integration%20methods%2C%20exploring%20how%20LLMs%20can%20be%20utilized%0Awithin%20the%20HP%20life%20cycle.%20Additionally%2C%20we%20provide%20a%20benchmark%20with%20a%0Astandardized%20dataset%20for%20evaluating%20the%20performance%20of%20future%20LLM-based%20HP%0Aapproaches%2C%20and%20present%20initial%20results%20for%20a%20state-of-the-art%20HP%20planner%20and%0ALLM%20planner.%20As%20expected%2C%20the%20latter%20exhibits%20limited%20performance%20%283%5C%25%20correct%0Aplans%2C%20and%20none%20with%20a%20correct%20hierarchical%20decomposition%29%20but%20serves%20as%20a%0Avaluable%20baseline%20for%20future%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Roadmap%2520to%2520Guide%2520the%2520Integration%2520of%2520LLMs%2520in%2520Hierarchical%2520Planning%26entry.906535625%3DIsrael%2520Puerta-Merino%2520and%2520Carlos%2520N%25C3%25BA%25C3%25B1ez-Molina%2520and%2520Pablo%2520Mesejo%2520and%2520Juan%2520Fern%25C3%25A1ndez-Olivares%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520fostering%2520their%250Aintegration%2520into%2520several%2520reasoning-related%2520fields%252C%2520including%2520Automated%2520Planning%250A%2528AP%2529.%2520However%252C%2520their%2520integration%2520into%2520Hierarchical%2520Planning%2520%2528HP%2529%252C%2520a%2520subfield%2520of%250AAP%2520that%2520leverages%2520hierarchical%2520knowledge%2520to%2520enhance%2520planning%2520performance%252C%250Aremains%2520largely%2520unexplored.%2520In%2520this%2520preliminary%2520work%252C%2520we%2520propose%2520a%2520roadmap%2520to%250Aaddress%2520this%2520gap%2520and%2520harness%2520the%2520potential%2520of%2520LLMs%2520for%2520HP.%2520To%2520this%2520end%252C%2520we%250Apresent%2520a%2520taxonomy%2520of%2520integration%2520methods%252C%2520exploring%2520how%2520LLMs%2520can%2520be%2520utilized%250Awithin%2520the%2520HP%2520life%2520cycle.%2520Additionally%252C%2520we%2520provide%2520a%2520benchmark%2520with%2520a%250Astandardized%2520dataset%2520for%2520evaluating%2520the%2520performance%2520of%2520future%2520LLM-based%2520HP%250Aapproaches%252C%2520and%2520present%2520initial%2520results%2520for%2520a%2520state-of-the-art%2520HP%2520planner%2520and%250ALLM%2520planner.%2520As%2520expected%252C%2520the%2520latter%2520exhibits%2520limited%2520performance%2520%25283%255C%2525%2520correct%250Aplans%252C%2520and%2520none%2520with%2520a%2520correct%2520hierarchical%2520decomposition%2529%2520but%2520serves%2520as%2520a%250Avaluable%2520baseline%2520for%2520future%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Roadmap%20to%20Guide%20the%20Integration%20of%20LLMs%20in%20Hierarchical%20Planning&entry.906535625=Israel%20Puerta-Merino%20and%20Carlos%20N%C3%BA%C3%B1ez-Molina%20and%20Pablo%20Mesejo%20and%20Juan%20Fern%C3%A1ndez-Olivares&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20fostering%20their%0Aintegration%20into%20several%20reasoning-related%20fields%2C%20including%20Automated%20Planning%0A%28AP%29.%20However%2C%20their%20integration%20into%20Hierarchical%20Planning%20%28HP%29%2C%20a%20subfield%20of%0AAP%20that%20leverages%20hierarchical%20knowledge%20to%20enhance%20planning%20performance%2C%0Aremains%20largely%20unexplored.%20In%20this%20preliminary%20work%2C%20we%20propose%20a%20roadmap%20to%0Aaddress%20this%20gap%20and%20harness%20the%20potential%20of%20LLMs%20for%20HP.%20To%20this%20end%2C%20we%0Apresent%20a%20taxonomy%20of%20integration%20methods%2C%20exploring%20how%20LLMs%20can%20be%20utilized%0Awithin%20the%20HP%20life%20cycle.%20Additionally%2C%20we%20provide%20a%20benchmark%20with%20a%0Astandardized%20dataset%20for%20evaluating%20the%20performance%20of%20future%20LLM-based%20HP%0Aapproaches%2C%20and%20present%20initial%20results%20for%20a%20state-of-the-art%20HP%20planner%20and%0ALLM%20planner.%20As%20expected%2C%20the%20latter%20exhibits%20limited%20performance%20%283%5C%25%20correct%0Aplans%2C%20and%20none%20with%20a%20correct%20hierarchical%20decomposition%29%20but%20serves%20as%20a%0Avaluable%20baseline%20for%20future%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08068v1&entry.124074799=Read"},
{"title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media", "author": "Wenlu Fan and Yuqi Zhu and Chenyang Wang and Bin Wang and Wentao Xu", "abstract": "  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.\n", "link": "http://arxiv.org/abs/2501.08102v1", "date": "2025-01-14", "relevancy": 2.3293, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&body=Title%3A%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media%0AAuthor%3A%20Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520of%2520Responses%2520and%2520Continuations%2520Generated%2520by%2520Large%2520Language%250A%2520%2520Models%2520on%2520Social%2520Media%26entry.906535625%3DWenlu%2520Fan%2520and%2520Yuqi%2520Zhu%2520and%2520Chenyang%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Wentao%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520capabilities%2520in%2520text%250Ageneration%252C%2520yet%2520their%2520emotional%2520consistency%2520and%2520semantic%2520coherence%2520in%2520social%250Amedia%2520contexts%2520remain%2520insufficiently%2520understood.%2520This%2520study%2520investigates%2520how%250ALLMs%2520handle%2520emotional%2520content%2520and%2520maintain%2520semantic%2520relationships%2520through%250Acontinuation%2520and%2520response%2520tasks%2520using%2520two%2520open-source%2520models%253A%2520Gemma%2520and%2520Llama.%250ABy%2520analyzing%2520climate%2520change%2520discussions%2520from%2520Twitter%2520and%2520Reddit%252C%2520we%2520examine%250Aemotional%2520transitions%252C%2520intensity%2520patterns%252C%2520and%2520semantic%2520similarity%2520between%250Ahuman-authored%2520and%2520LLM-generated%2520content.%2520Our%2520findings%2520reveal%2520that%2520while%2520both%250Amodels%2520maintain%2520high%2520semantic%2520coherence%252C%2520they%2520exhibit%2520distinct%2520emotional%250Apatterns%253A%2520Gemma%2520shows%2520a%2520tendency%2520toward%2520negative%2520emotion%2520amplification%252C%250Aparticularly%2520anger%252C%2520while%2520maintaining%2520certain%2520positive%2520emotions%2520like%2520optimism.%250ALlama%2520demonstrates%2520superior%2520emotional%2520preservation%2520across%2520a%2520broader%2520spectrum%2520of%250Aaffects.%2520Both%2520models%2520systematically%2520generate%2520responses%2520with%2520attenuated%250Aemotional%2520intensity%2520compared%2520to%2520human-authored%2520content%2520and%2520show%2520a%2520bias%2520toward%250Apositive%2520emotions%2520in%2520response%2520tasks.%2520Additionally%252C%2520both%2520models%2520maintain%2520strong%250Asemantic%2520similarity%2520with%2520original%2520texts%252C%2520though%2520performance%2520varies%2520between%250Acontinuation%2520and%2520response%2520tasks.%2520These%2520findings%2520provide%2520insights%2520into%2520LLMs%2527%250Aemotional%2520and%2520semantic%2520processing%2520capabilities%252C%2520with%2520implications%2520for%2520their%250Adeployment%2520in%2520social%2520media%2520contexts%2520and%2520human-AI%2520interaction%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&entry.906535625=Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08102v1&entry.124074799=Read"},
{"title": "Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition\n  via Foundation Models", "author": "Anjith George and Sebastien Marcel", "abstract": "  The accuracy of face recognition systems has improved significantly in the\npast few years, thanks to the large amount of data collected and advancements\nin neural network architectures. However, these large-scale datasets are often\ncollected without explicit consent, raising ethical and privacy concerns. To\naddress this, there have been proposals to use synthetic datasets for training\nface recognition models. Yet, such models still rely on real data to train the\ngenerative models and generally exhibit inferior performance compared to those\ntrained on real datasets. One of these datasets, DigiFace, uses a graphics\npipeline to generate different identities and intra-class variations without\nusing real data in model training. However, the performance of this approach is\npoor on face recognition benchmarks, possibly due to the lack of realism in the\nimages generated by the graphics pipeline. In this work, we introduce a novel\nframework for realism transfer aimed at enhancing the realism of synthetically\ngenerated face images. Our method leverages the large-scale face foundation\nmodel, and we adapt the pipeline for realism enhancement. By integrating the\ncontrollable aspects of the graphics pipeline with our realism enhancement\ntechnique, we generate a large amount of realistic variations, combining the\nadvantages of both approaches. Our empirical evaluations demonstrate that\nmodels trained using our enhanced dataset significantly improve the performance\nof face recognition systems over the baseline. The source code and dataset will\nbe publicly accessible at the following link:\nhttps://www.idiap.ch/paper/digi2real\n", "link": "http://arxiv.org/abs/2411.02188v4", "date": "2025-01-14", "relevancy": 2.3268, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5864}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5807}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models&body=Title%3A%20Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20The%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%20the%0Apast%20few%20years%2C%20thanks%20to%20the%20large%20amount%20of%20data%20collected%20and%20advancements%0Ain%20neural%20network%20architectures.%20However%2C%20these%20large-scale%20datasets%20are%20often%0Acollected%20without%20explicit%20consent%2C%20raising%20ethical%20and%20privacy%20concerns.%20To%0Aaddress%20this%2C%20there%20have%20been%20proposals%20to%20use%20synthetic%20datasets%20for%20training%0Aface%20recognition%20models.%20Yet%2C%20such%20models%20still%20rely%20on%20real%20data%20to%20train%20the%0Agenerative%20models%20and%20generally%20exhibit%20inferior%20performance%20compared%20to%20those%0Atrained%20on%20real%20datasets.%20One%20of%20these%20datasets%2C%20DigiFace%2C%20uses%20a%20graphics%0Apipeline%20to%20generate%20different%20identities%20and%20intra-class%20variations%20without%0Ausing%20real%20data%20in%20model%20training.%20However%2C%20the%20performance%20of%20this%20approach%20is%0Apoor%20on%20face%20recognition%20benchmarks%2C%20possibly%20due%20to%20the%20lack%20of%20realism%20in%20the%0Aimages%20generated%20by%20the%20graphics%20pipeline.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aframework%20for%20realism%20transfer%20aimed%20at%20enhancing%20the%20realism%20of%20synthetically%0Agenerated%20face%20images.%20Our%20method%20leverages%20the%20large-scale%20face%20foundation%0Amodel%2C%20and%20we%20adapt%20the%20pipeline%20for%20realism%20enhancement.%20By%20integrating%20the%0Acontrollable%20aspects%20of%20the%20graphics%20pipeline%20with%20our%20realism%20enhancement%0Atechnique%2C%20we%20generate%20a%20large%20amount%20of%20realistic%20variations%2C%20combining%20the%0Aadvantages%20of%20both%20approaches.%20Our%20empirical%20evaluations%20demonstrate%20that%0Amodels%20trained%20using%20our%20enhanced%20dataset%20significantly%20improve%20the%20performance%0Aof%20face%20recognition%20systems%20over%20the%20baseline.%20The%20source%20code%20and%20dataset%20will%0Abe%20publicly%20accessible%20at%20the%20following%20link%3A%0Ahttps%3A//www.idiap.ch/paper/digi2real%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02188v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigi2Real%253A%2520Bridging%2520the%2520Realism%2520Gap%2520in%2520Synthetic%2520Data%2520Face%2520Recognition%250A%2520%2520via%2520Foundation%2520Models%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520The%2520accuracy%2520of%2520face%2520recognition%2520systems%2520has%2520improved%2520significantly%2520in%2520the%250Apast%2520few%2520years%252C%2520thanks%2520to%2520the%2520large%2520amount%2520of%2520data%2520collected%2520and%2520advancements%250Ain%2520neural%2520network%2520architectures.%2520However%252C%2520these%2520large-scale%2520datasets%2520are%2520often%250Acollected%2520without%2520explicit%2520consent%252C%2520raising%2520ethical%2520and%2520privacy%2520concerns.%2520To%250Aaddress%2520this%252C%2520there%2520have%2520been%2520proposals%2520to%2520use%2520synthetic%2520datasets%2520for%2520training%250Aface%2520recognition%2520models.%2520Yet%252C%2520such%2520models%2520still%2520rely%2520on%2520real%2520data%2520to%2520train%2520the%250Agenerative%2520models%2520and%2520generally%2520exhibit%2520inferior%2520performance%2520compared%2520to%2520those%250Atrained%2520on%2520real%2520datasets.%2520One%2520of%2520these%2520datasets%252C%2520DigiFace%252C%2520uses%2520a%2520graphics%250Apipeline%2520to%2520generate%2520different%2520identities%2520and%2520intra-class%2520variations%2520without%250Ausing%2520real%2520data%2520in%2520model%2520training.%2520However%252C%2520the%2520performance%2520of%2520this%2520approach%2520is%250Apoor%2520on%2520face%2520recognition%2520benchmarks%252C%2520possibly%2520due%2520to%2520the%2520lack%2520of%2520realism%2520in%2520the%250Aimages%2520generated%2520by%2520the%2520graphics%2520pipeline.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aframework%2520for%2520realism%2520transfer%2520aimed%2520at%2520enhancing%2520the%2520realism%2520of%2520synthetically%250Agenerated%2520face%2520images.%2520Our%2520method%2520leverages%2520the%2520large-scale%2520face%2520foundation%250Amodel%252C%2520and%2520we%2520adapt%2520the%2520pipeline%2520for%2520realism%2520enhancement.%2520By%2520integrating%2520the%250Acontrollable%2520aspects%2520of%2520the%2520graphics%2520pipeline%2520with%2520our%2520realism%2520enhancement%250Atechnique%252C%2520we%2520generate%2520a%2520large%2520amount%2520of%2520realistic%2520variations%252C%2520combining%2520the%250Aadvantages%2520of%2520both%2520approaches.%2520Our%2520empirical%2520evaluations%2520demonstrate%2520that%250Amodels%2520trained%2520using%2520our%2520enhanced%2520dataset%2520significantly%2520improve%2520the%2520performance%250Aof%2520face%2520recognition%2520systems%2520over%2520the%2520baseline.%2520The%2520source%2520code%2520and%2520dataset%2520will%250Abe%2520publicly%2520accessible%2520at%2520the%2520following%2520link%253A%250Ahttps%253A//www.idiap.ch/paper/digi2real%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02188v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digi2Real%3A%20Bridging%20the%20Realism%20Gap%20in%20Synthetic%20Data%20Face%20Recognition%0A%20%20via%20Foundation%20Models&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20The%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%20the%0Apast%20few%20years%2C%20thanks%20to%20the%20large%20amount%20of%20data%20collected%20and%20advancements%0Ain%20neural%20network%20architectures.%20However%2C%20these%20large-scale%20datasets%20are%20often%0Acollected%20without%20explicit%20consent%2C%20raising%20ethical%20and%20privacy%20concerns.%20To%0Aaddress%20this%2C%20there%20have%20been%20proposals%20to%20use%20synthetic%20datasets%20for%20training%0Aface%20recognition%20models.%20Yet%2C%20such%20models%20still%20rely%20on%20real%20data%20to%20train%20the%0Agenerative%20models%20and%20generally%20exhibit%20inferior%20performance%20compared%20to%20those%0Atrained%20on%20real%20datasets.%20One%20of%20these%20datasets%2C%20DigiFace%2C%20uses%20a%20graphics%0Apipeline%20to%20generate%20different%20identities%20and%20intra-class%20variations%20without%0Ausing%20real%20data%20in%20model%20training.%20However%2C%20the%20performance%20of%20this%20approach%20is%0Apoor%20on%20face%20recognition%20benchmarks%2C%20possibly%20due%20to%20the%20lack%20of%20realism%20in%20the%0Aimages%20generated%20by%20the%20graphics%20pipeline.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aframework%20for%20realism%20transfer%20aimed%20at%20enhancing%20the%20realism%20of%20synthetically%0Agenerated%20face%20images.%20Our%20method%20leverages%20the%20large-scale%20face%20foundation%0Amodel%2C%20and%20we%20adapt%20the%20pipeline%20for%20realism%20enhancement.%20By%20integrating%20the%0Acontrollable%20aspects%20of%20the%20graphics%20pipeline%20with%20our%20realism%20enhancement%0Atechnique%2C%20we%20generate%20a%20large%20amount%20of%20realistic%20variations%2C%20combining%20the%0Aadvantages%20of%20both%20approaches.%20Our%20empirical%20evaluations%20demonstrate%20that%0Amodels%20trained%20using%20our%20enhanced%20dataset%20significantly%20improve%20the%20performance%0Aof%20face%20recognition%20systems%20over%20the%20baseline.%20The%20source%20code%20and%20dataset%20will%0Abe%20publicly%20accessible%20at%20the%20following%20link%3A%0Ahttps%3A//www.idiap.ch/paper/digi2real%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02188v4&entry.124074799=Read"},
{"title": "PokerBench: Training Large Language Models to become Professional Poker\n  Players", "author": "Richard Zhuang and Akshat Gupta and Richard Yang and Aniket Rahane and Zhengyu Li and Gopala Anumanchipalli", "abstract": "  We introduce PokerBench - a benchmark for evaluating the poker-playing\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\ntasks, their application to complex, strategic games like poker poses a new\nchallenge. Poker, an incomplete information game, demands a multitude of skills\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\ngame theory and human psychology. This makes Poker the ideal next frontier for\nlarge language models. PokerBench consists of a comprehensive compilation of\n11,000 most important scenarios, split between pre-flop and post-flop play,\ndeveloped in collaboration with trained poker players. We evaluate prominent\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\nHowever, after fine-tuning, these models show marked improvements. We validate\nPokerBench by having models with different scores compete with each other,\ndemonstrating that higher scores on PokerBench lead to higher win rates in\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\nalso identify limitations of simple supervised fine-tuning for learning optimal\nplaying strategy, suggesting the need for more advanced methodologies for\neffectively training language models to excel in games. PokerBench thus\npresents a unique benchmark for a quick and reliable evaluation of the\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\nbe made available at: \\url{https://github.com/pokerllm/pokerbench}.\n", "link": "http://arxiv.org/abs/2501.08328v1", "date": "2025-01-14", "relevancy": 2.3214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PokerBench%3A%20Training%20Large%20Language%20Models%20to%20become%20Professional%20Poker%0A%20%20Players&body=Title%3A%20PokerBench%3A%20Training%20Large%20Language%20Models%20to%20become%20Professional%20Poker%0A%20%20Players%0AAuthor%3A%20Richard%20Zhuang%20and%20Akshat%20Gupta%20and%20Richard%20Yang%20and%20Aniket%20Rahane%20and%20Zhengyu%20Li%20and%20Gopala%20Anumanchipalli%0AAbstract%3A%20%20%20We%20introduce%20PokerBench%20-%20a%20benchmark%20for%20evaluating%20the%20poker-playing%0Aabilities%20of%20large%20language%20models%20%28LLMs%29.%20As%20LLMs%20excel%20in%20traditional%20NLP%0Atasks%2C%20their%20application%20to%20complex%2C%20strategic%20games%20like%20poker%20poses%20a%20new%0Achallenge.%20Poker%2C%20an%20incomplete%20information%20game%2C%20demands%20a%20multitude%20of%20skills%0Asuch%20as%20mathematics%2C%20reasoning%2C%20planning%2C%20strategy%2C%20and%20a%20deep%20understanding%20of%0Agame%20theory%20and%20human%20psychology.%20This%20makes%20Poker%20the%20ideal%20next%20frontier%20for%0Alarge%20language%20models.%20PokerBench%20consists%20of%20a%20comprehensive%20compilation%20of%0A11%2C000%20most%20important%20scenarios%2C%20split%20between%20pre-flop%20and%20post-flop%20play%2C%0Adeveloped%20in%20collaboration%20with%20trained%20poker%20players.%20We%20evaluate%20prominent%0Amodels%20including%20GPT-4%2C%20ChatGPT%203.5%2C%20and%20various%20Llama%20and%20Gemma%20series%20models%2C%0Afinding%20that%20all%20state-of-the-art%20LLMs%20underperform%20in%20playing%20optimal%20poker.%0AHowever%2C%20after%20fine-tuning%2C%20these%20models%20show%20marked%20improvements.%20We%20validate%0APokerBench%20by%20having%20models%20with%20different%20scores%20compete%20with%20each%20other%2C%0Ademonstrating%20that%20higher%20scores%20on%20PokerBench%20lead%20to%20higher%20win%20rates%20in%0Aactual%20poker%20games.%20Through%20gameplay%20between%20our%20fine-tuned%20model%20and%20GPT-4%2C%20we%0Aalso%20identify%20limitations%20of%20simple%20supervised%20fine-tuning%20for%20learning%20optimal%0Aplaying%20strategy%2C%20suggesting%20the%20need%20for%20more%20advanced%20methodologies%20for%0Aeffectively%20training%20language%20models%20to%20excel%20in%20games.%20PokerBench%20thus%0Apresents%20a%20unique%20benchmark%20for%20a%20quick%20and%20reliable%20evaluation%20of%20the%0Apoker-playing%20ability%20of%20LLMs%20as%20well%20as%20a%20comprehensive%20benchmark%20to%20study%20the%0Aprogress%20of%20LLMs%20in%20complex%20game-playing%20scenarios.%20The%20dataset%20and%20code%20will%0Abe%20made%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/pokerllm/pokerbench%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPokerBench%253A%2520Training%2520Large%2520Language%2520Models%2520to%2520become%2520Professional%2520Poker%250A%2520%2520Players%26entry.906535625%3DRichard%2520Zhuang%2520and%2520Akshat%2520Gupta%2520and%2520Richard%2520Yang%2520and%2520Aniket%2520Rahane%2520and%2520Zhengyu%2520Li%2520and%2520Gopala%2520Anumanchipalli%26entry.1292438233%3D%2520%2520We%2520introduce%2520PokerBench%2520-%2520a%2520benchmark%2520for%2520evaluating%2520the%2520poker-playing%250Aabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520As%2520LLMs%2520excel%2520in%2520traditional%2520NLP%250Atasks%252C%2520their%2520application%2520to%2520complex%252C%2520strategic%2520games%2520like%2520poker%2520poses%2520a%2520new%250Achallenge.%2520Poker%252C%2520an%2520incomplete%2520information%2520game%252C%2520demands%2520a%2520multitude%2520of%2520skills%250Asuch%2520as%2520mathematics%252C%2520reasoning%252C%2520planning%252C%2520strategy%252C%2520and%2520a%2520deep%2520understanding%2520of%250Agame%2520theory%2520and%2520human%2520psychology.%2520This%2520makes%2520Poker%2520the%2520ideal%2520next%2520frontier%2520for%250Alarge%2520language%2520models.%2520PokerBench%2520consists%2520of%2520a%2520comprehensive%2520compilation%2520of%250A11%252C000%2520most%2520important%2520scenarios%252C%2520split%2520between%2520pre-flop%2520and%2520post-flop%2520play%252C%250Adeveloped%2520in%2520collaboration%2520with%2520trained%2520poker%2520players.%2520We%2520evaluate%2520prominent%250Amodels%2520including%2520GPT-4%252C%2520ChatGPT%25203.5%252C%2520and%2520various%2520Llama%2520and%2520Gemma%2520series%2520models%252C%250Afinding%2520that%2520all%2520state-of-the-art%2520LLMs%2520underperform%2520in%2520playing%2520optimal%2520poker.%250AHowever%252C%2520after%2520fine-tuning%252C%2520these%2520models%2520show%2520marked%2520improvements.%2520We%2520validate%250APokerBench%2520by%2520having%2520models%2520with%2520different%2520scores%2520compete%2520with%2520each%2520other%252C%250Ademonstrating%2520that%2520higher%2520scores%2520on%2520PokerBench%2520lead%2520to%2520higher%2520win%2520rates%2520in%250Aactual%2520poker%2520games.%2520Through%2520gameplay%2520between%2520our%2520fine-tuned%2520model%2520and%2520GPT-4%252C%2520we%250Aalso%2520identify%2520limitations%2520of%2520simple%2520supervised%2520fine-tuning%2520for%2520learning%2520optimal%250Aplaying%2520strategy%252C%2520suggesting%2520the%2520need%2520for%2520more%2520advanced%2520methodologies%2520for%250Aeffectively%2520training%2520language%2520models%2520to%2520excel%2520in%2520games.%2520PokerBench%2520thus%250Apresents%2520a%2520unique%2520benchmark%2520for%2520a%2520quick%2520and%2520reliable%2520evaluation%2520of%2520the%250Apoker-playing%2520ability%2520of%2520LLMs%2520as%2520well%2520as%2520a%2520comprehensive%2520benchmark%2520to%2520study%2520the%250Aprogress%2520of%2520LLMs%2520in%2520complex%2520game-playing%2520scenarios.%2520The%2520dataset%2520and%2520code%2520will%250Abe%2520made%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/pokerllm/pokerbench%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PokerBench%3A%20Training%20Large%20Language%20Models%20to%20become%20Professional%20Poker%0A%20%20Players&entry.906535625=Richard%20Zhuang%20and%20Akshat%20Gupta%20and%20Richard%20Yang%20and%20Aniket%20Rahane%20and%20Zhengyu%20Li%20and%20Gopala%20Anumanchipalli&entry.1292438233=%20%20We%20introduce%20PokerBench%20-%20a%20benchmark%20for%20evaluating%20the%20poker-playing%0Aabilities%20of%20large%20language%20models%20%28LLMs%29.%20As%20LLMs%20excel%20in%20traditional%20NLP%0Atasks%2C%20their%20application%20to%20complex%2C%20strategic%20games%20like%20poker%20poses%20a%20new%0Achallenge.%20Poker%2C%20an%20incomplete%20information%20game%2C%20demands%20a%20multitude%20of%20skills%0Asuch%20as%20mathematics%2C%20reasoning%2C%20planning%2C%20strategy%2C%20and%20a%20deep%20understanding%20of%0Agame%20theory%20and%20human%20psychology.%20This%20makes%20Poker%20the%20ideal%20next%20frontier%20for%0Alarge%20language%20models.%20PokerBench%20consists%20of%20a%20comprehensive%20compilation%20of%0A11%2C000%20most%20important%20scenarios%2C%20split%20between%20pre-flop%20and%20post-flop%20play%2C%0Adeveloped%20in%20collaboration%20with%20trained%20poker%20players.%20We%20evaluate%20prominent%0Amodels%20including%20GPT-4%2C%20ChatGPT%203.5%2C%20and%20various%20Llama%20and%20Gemma%20series%20models%2C%0Afinding%20that%20all%20state-of-the-art%20LLMs%20underperform%20in%20playing%20optimal%20poker.%0AHowever%2C%20after%20fine-tuning%2C%20these%20models%20show%20marked%20improvements.%20We%20validate%0APokerBench%20by%20having%20models%20with%20different%20scores%20compete%20with%20each%20other%2C%0Ademonstrating%20that%20higher%20scores%20on%20PokerBench%20lead%20to%20higher%20win%20rates%20in%0Aactual%20poker%20games.%20Through%20gameplay%20between%20our%20fine-tuned%20model%20and%20GPT-4%2C%20we%0Aalso%20identify%20limitations%20of%20simple%20supervised%20fine-tuning%20for%20learning%20optimal%0Aplaying%20strategy%2C%20suggesting%20the%20need%20for%20more%20advanced%20methodologies%20for%0Aeffectively%20training%20language%20models%20to%20excel%20in%20games.%20PokerBench%20thus%0Apresents%20a%20unique%20benchmark%20for%20a%20quick%20and%20reliable%20evaluation%20of%20the%0Apoker-playing%20ability%20of%20LLMs%20as%20well%20as%20a%20comprehensive%20benchmark%20to%20study%20the%0Aprogress%20of%20LLMs%20in%20complex%20game-playing%20scenarios.%20The%20dataset%20and%20code%20will%0Abe%20made%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/pokerllm/pokerbench%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08328v1&entry.124074799=Read"},
{"title": "LayerAnimate: Layer-specific Control for Animation", "author": "Yuxue Yang and Lue Fan and Zuzen Lin and Feng Wang and Zhaoxiang Zhang", "abstract": "  Animated video separates foreground and background elements into layers, with\ndistinct processes for sketching, refining, coloring, and in-betweening.\nExisting video generation methods typically treat animation as a monolithic\ndata domain, lacking fine-grained control over individual layers. In this\npaper, we introduce LayerAnimate, a novel architectural approach that enhances\nfine-grained control over individual animation layers within a video diffusion\nmodel, allowing users to independently manipulate foreground and background\nelements in distinct layers. To address the challenge of limited layer-specific\ndata, we propose a data curation pipeline that features automated element\nsegmentation, motion-state hierarchical merging, and motion coherence\nrefinement. Through quantitative and qualitative comparisons, and user study,\nwe demonstrate that LayerAnimate outperforms current methods in terms of\nanimation quality, control precision, and usability, making it an ideal tool\nfor both professional animators and amateur enthusiasts. This framework opens\nup new possibilities for layer-specific animation applications and creative\nflexibility. Our code is available at https://layeranimate.github.io.\n", "link": "http://arxiv.org/abs/2501.08295v1", "date": "2025-01-14", "relevancy": 2.3205, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5884}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5814}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation&body=Title%3A%20LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation%0AAuthor%3A%20Yuxue%20Yang%20and%20Lue%20Fan%20and%20Zuzen%20Lin%20and%20Feng%20Wang%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Animated%20video%20separates%20foreground%20and%20background%20elements%20into%20layers%2C%20with%0Adistinct%20processes%20for%20sketching%2C%20refining%2C%20coloring%2C%20and%20in-betweening.%0AExisting%20video%20generation%20methods%20typically%20treat%20animation%20as%20a%20monolithic%0Adata%20domain%2C%20lacking%20fine-grained%20control%20over%20individual%20layers.%20In%20this%0Apaper%2C%20we%20introduce%20LayerAnimate%2C%20a%20novel%20architectural%20approach%20that%20enhances%0Afine-grained%20control%20over%20individual%20animation%20layers%20within%20a%20video%20diffusion%0Amodel%2C%20allowing%20users%20to%20independently%20manipulate%20foreground%20and%20background%0Aelements%20in%20distinct%20layers.%20To%20address%20the%20challenge%20of%20limited%20layer-specific%0Adata%2C%20we%20propose%20a%20data%20curation%20pipeline%20that%20features%20automated%20element%0Asegmentation%2C%20motion-state%20hierarchical%20merging%2C%20and%20motion%20coherence%0Arefinement.%20Through%20quantitative%20and%20qualitative%20comparisons%2C%20and%20user%20study%2C%0Awe%20demonstrate%20that%20LayerAnimate%20outperforms%20current%20methods%20in%20terms%20of%0Aanimation%20quality%2C%20control%20precision%2C%20and%20usability%2C%20making%20it%20an%20ideal%20tool%0Afor%20both%20professional%20animators%20and%20amateur%20enthusiasts.%20This%20framework%20opens%0Aup%20new%20possibilities%20for%20layer-specific%20animation%20applications%20and%20creative%0Aflexibility.%20Our%20code%20is%20available%20at%20https%3A//layeranimate.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerAnimate%253A%2520Layer-specific%2520Control%2520for%2520Animation%26entry.906535625%3DYuxue%2520Yang%2520and%2520Lue%2520Fan%2520and%2520Zuzen%2520Lin%2520and%2520Feng%2520Wang%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Animated%2520video%2520separates%2520foreground%2520and%2520background%2520elements%2520into%2520layers%252C%2520with%250Adistinct%2520processes%2520for%2520sketching%252C%2520refining%252C%2520coloring%252C%2520and%2520in-betweening.%250AExisting%2520video%2520generation%2520methods%2520typically%2520treat%2520animation%2520as%2520a%2520monolithic%250Adata%2520domain%252C%2520lacking%2520fine-grained%2520control%2520over%2520individual%2520layers.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520LayerAnimate%252C%2520a%2520novel%2520architectural%2520approach%2520that%2520enhances%250Afine-grained%2520control%2520over%2520individual%2520animation%2520layers%2520within%2520a%2520video%2520diffusion%250Amodel%252C%2520allowing%2520users%2520to%2520independently%2520manipulate%2520foreground%2520and%2520background%250Aelements%2520in%2520distinct%2520layers.%2520To%2520address%2520the%2520challenge%2520of%2520limited%2520layer-specific%250Adata%252C%2520we%2520propose%2520a%2520data%2520curation%2520pipeline%2520that%2520features%2520automated%2520element%250Asegmentation%252C%2520motion-state%2520hierarchical%2520merging%252C%2520and%2520motion%2520coherence%250Arefinement.%2520Through%2520quantitative%2520and%2520qualitative%2520comparisons%252C%2520and%2520user%2520study%252C%250Awe%2520demonstrate%2520that%2520LayerAnimate%2520outperforms%2520current%2520methods%2520in%2520terms%2520of%250Aanimation%2520quality%252C%2520control%2520precision%252C%2520and%2520usability%252C%2520making%2520it%2520an%2520ideal%2520tool%250Afor%2520both%2520professional%2520animators%2520and%2520amateur%2520enthusiasts.%2520This%2520framework%2520opens%250Aup%2520new%2520possibilities%2520for%2520layer-specific%2520animation%2520applications%2520and%2520creative%250Aflexibility.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//layeranimate.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerAnimate%3A%20Layer-specific%20Control%20for%20Animation&entry.906535625=Yuxue%20Yang%20and%20Lue%20Fan%20and%20Zuzen%20Lin%20and%20Feng%20Wang%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Animated%20video%20separates%20foreground%20and%20background%20elements%20into%20layers%2C%20with%0Adistinct%20processes%20for%20sketching%2C%20refining%2C%20coloring%2C%20and%20in-betweening.%0AExisting%20video%20generation%20methods%20typically%20treat%20animation%20as%20a%20monolithic%0Adata%20domain%2C%20lacking%20fine-grained%20control%20over%20individual%20layers.%20In%20this%0Apaper%2C%20we%20introduce%20LayerAnimate%2C%20a%20novel%20architectural%20approach%20that%20enhances%0Afine-grained%20control%20over%20individual%20animation%20layers%20within%20a%20video%20diffusion%0Amodel%2C%20allowing%20users%20to%20independently%20manipulate%20foreground%20and%20background%0Aelements%20in%20distinct%20layers.%20To%20address%20the%20challenge%20of%20limited%20layer-specific%0Adata%2C%20we%20propose%20a%20data%20curation%20pipeline%20that%20features%20automated%20element%0Asegmentation%2C%20motion-state%20hierarchical%20merging%2C%20and%20motion%20coherence%0Arefinement.%20Through%20quantitative%20and%20qualitative%20comparisons%2C%20and%20user%20study%2C%0Awe%20demonstrate%20that%20LayerAnimate%20outperforms%20current%20methods%20in%20terms%20of%0Aanimation%20quality%2C%20control%20precision%2C%20and%20usability%2C%20making%20it%20an%20ideal%20tool%0Afor%20both%20professional%20animators%20and%20amateur%20enthusiasts.%20This%20framework%20opens%0Aup%20new%20possibilities%20for%20layer-specific%20animation%20applications%20and%20creative%0Aflexibility.%20Our%20code%20is%20available%20at%20https%3A//layeranimate.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08295v1&entry.124074799=Read"},
{"title": "Optimization of Link Configuration for Satellite Communication Using\n  Reinforcement Learning", "author": "Tobias Rohe and Michael K\u00f6lle and Jan Matheis and R\u00fcdiger H\u00f6pfl and Leo S\u00fcnkel and Claudia Linnhoff-Popien", "abstract": "  Satellite communication is a key technology in our modern connected world.\nWith increasingly complex hardware, one challenge is to efficiently configure\nlinks (connections) on a satellite transponder. Planning an optimal link\nconfiguration is extremely complex and depends on many parameters and metrics.\nThe optimal use of the limited resources, bandwidth and power of the\ntransponder is crucial. Such an optimization problem can be approximated using\nmetaheuristic methods such as simulated annealing, but recent research results\nalso show that reinforcement learning can achieve comparable or even better\nperformance in optimization methods. However, there have not yet been any\nstudies on link configuration on satellite transponders. In order to close this\nresearch gap, a transponder environment was developed as part of this work. For\nthis environment, the performance of the reinforcement learning algorithm PPO\nwas compared with the metaheuristic simulated annealing in two experiments. The\nresults show that Simulated Annealing delivers better results for this static\nproblem than the PPO algorithm, however, the research in turn also underlines\nthe potential of reinforcement learning for optimization problems.\n", "link": "http://arxiv.org/abs/2501.08220v1", "date": "2025-01-14", "relevancy": 2.3181, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4915}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20of%20Link%20Configuration%20for%20Satellite%20Communication%20Using%0A%20%20Reinforcement%20Learning&body=Title%3A%20Optimization%20of%20Link%20Configuration%20for%20Satellite%20Communication%20Using%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Tobias%20Rohe%20and%20Michael%20K%C3%B6lle%20and%20Jan%20Matheis%20and%20R%C3%BCdiger%20H%C3%B6pfl%20and%20Leo%20S%C3%BCnkel%20and%20Claudia%20Linnhoff-Popien%0AAbstract%3A%20%20%20Satellite%20communication%20is%20a%20key%20technology%20in%20our%20modern%20connected%20world.%0AWith%20increasingly%20complex%20hardware%2C%20one%20challenge%20is%20to%20efficiently%20configure%0Alinks%20%28connections%29%20on%20a%20satellite%20transponder.%20Planning%20an%20optimal%20link%0Aconfiguration%20is%20extremely%20complex%20and%20depends%20on%20many%20parameters%20and%20metrics.%0AThe%20optimal%20use%20of%20the%20limited%20resources%2C%20bandwidth%20and%20power%20of%20the%0Atransponder%20is%20crucial.%20Such%20an%20optimization%20problem%20can%20be%20approximated%20using%0Ametaheuristic%20methods%20such%20as%20simulated%20annealing%2C%20but%20recent%20research%20results%0Aalso%20show%20that%20reinforcement%20learning%20can%20achieve%20comparable%20or%20even%20better%0Aperformance%20in%20optimization%20methods.%20However%2C%20there%20have%20not%20yet%20been%20any%0Astudies%20on%20link%20configuration%20on%20satellite%20transponders.%20In%20order%20to%20close%20this%0Aresearch%20gap%2C%20a%20transponder%20environment%20was%20developed%20as%20part%20of%20this%20work.%20For%0Athis%20environment%2C%20the%20performance%20of%20the%20reinforcement%20learning%20algorithm%20PPO%0Awas%20compared%20with%20the%20metaheuristic%20simulated%20annealing%20in%20two%20experiments.%20The%0Aresults%20show%20that%20Simulated%20Annealing%20delivers%20better%20results%20for%20this%20static%0Aproblem%20than%20the%20PPO%20algorithm%2C%20however%2C%20the%20research%20in%20turn%20also%20underlines%0Athe%20potential%20of%20reinforcement%20learning%20for%20optimization%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520of%2520Link%2520Configuration%2520for%2520Satellite%2520Communication%2520Using%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DTobias%2520Rohe%2520and%2520Michael%2520K%25C3%25B6lle%2520and%2520Jan%2520Matheis%2520and%2520R%25C3%25BCdiger%2520H%25C3%25B6pfl%2520and%2520Leo%2520S%25C3%25BCnkel%2520and%2520Claudia%2520Linnhoff-Popien%26entry.1292438233%3D%2520%2520Satellite%2520communication%2520is%2520a%2520key%2520technology%2520in%2520our%2520modern%2520connected%2520world.%250AWith%2520increasingly%2520complex%2520hardware%252C%2520one%2520challenge%2520is%2520to%2520efficiently%2520configure%250Alinks%2520%2528connections%2529%2520on%2520a%2520satellite%2520transponder.%2520Planning%2520an%2520optimal%2520link%250Aconfiguration%2520is%2520extremely%2520complex%2520and%2520depends%2520on%2520many%2520parameters%2520and%2520metrics.%250AThe%2520optimal%2520use%2520of%2520the%2520limited%2520resources%252C%2520bandwidth%2520and%2520power%2520of%2520the%250Atransponder%2520is%2520crucial.%2520Such%2520an%2520optimization%2520problem%2520can%2520be%2520approximated%2520using%250Ametaheuristic%2520methods%2520such%2520as%2520simulated%2520annealing%252C%2520but%2520recent%2520research%2520results%250Aalso%2520show%2520that%2520reinforcement%2520learning%2520can%2520achieve%2520comparable%2520or%2520even%2520better%250Aperformance%2520in%2520optimization%2520methods.%2520However%252C%2520there%2520have%2520not%2520yet%2520been%2520any%250Astudies%2520on%2520link%2520configuration%2520on%2520satellite%2520transponders.%2520In%2520order%2520to%2520close%2520this%250Aresearch%2520gap%252C%2520a%2520transponder%2520environment%2520was%2520developed%2520as%2520part%2520of%2520this%2520work.%2520For%250Athis%2520environment%252C%2520the%2520performance%2520of%2520the%2520reinforcement%2520learning%2520algorithm%2520PPO%250Awas%2520compared%2520with%2520the%2520metaheuristic%2520simulated%2520annealing%2520in%2520two%2520experiments.%2520The%250Aresults%2520show%2520that%2520Simulated%2520Annealing%2520delivers%2520better%2520results%2520for%2520this%2520static%250Aproblem%2520than%2520the%2520PPO%2520algorithm%252C%2520however%252C%2520the%2520research%2520in%2520turn%2520also%2520underlines%250Athe%2520potential%2520of%2520reinforcement%2520learning%2520for%2520optimization%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20of%20Link%20Configuration%20for%20Satellite%20Communication%20Using%0A%20%20Reinforcement%20Learning&entry.906535625=Tobias%20Rohe%20and%20Michael%20K%C3%B6lle%20and%20Jan%20Matheis%20and%20R%C3%BCdiger%20H%C3%B6pfl%20and%20Leo%20S%C3%BCnkel%20and%20Claudia%20Linnhoff-Popien&entry.1292438233=%20%20Satellite%20communication%20is%20a%20key%20technology%20in%20our%20modern%20connected%20world.%0AWith%20increasingly%20complex%20hardware%2C%20one%20challenge%20is%20to%20efficiently%20configure%0Alinks%20%28connections%29%20on%20a%20satellite%20transponder.%20Planning%20an%20optimal%20link%0Aconfiguration%20is%20extremely%20complex%20and%20depends%20on%20many%20parameters%20and%20metrics.%0AThe%20optimal%20use%20of%20the%20limited%20resources%2C%20bandwidth%20and%20power%20of%20the%0Atransponder%20is%20crucial.%20Such%20an%20optimization%20problem%20can%20be%20approximated%20using%0Ametaheuristic%20methods%20such%20as%20simulated%20annealing%2C%20but%20recent%20research%20results%0Aalso%20show%20that%20reinforcement%20learning%20can%20achieve%20comparable%20or%20even%20better%0Aperformance%20in%20optimization%20methods.%20However%2C%20there%20have%20not%20yet%20been%20any%0Astudies%20on%20link%20configuration%20on%20satellite%20transponders.%20In%20order%20to%20close%20this%0Aresearch%20gap%2C%20a%20transponder%20environment%20was%20developed%20as%20part%20of%20this%20work.%20For%0Athis%20environment%2C%20the%20performance%20of%20the%20reinforcement%20learning%20algorithm%20PPO%0Awas%20compared%20with%20the%20metaheuristic%20simulated%20annealing%20in%20two%20experiments.%20The%0Aresults%20show%20that%20Simulated%20Annealing%20delivers%20better%20results%20for%20this%20static%0Aproblem%20than%20the%20PPO%20algorithm%2C%20however%2C%20the%20research%20in%20turn%20also%20underlines%0Athe%20potential%20of%20reinforcement%20learning%20for%20optimization%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08220v1&entry.124074799=Read"},
{"title": "Predicting 4D Hand Trajectory from Monocular Videos", "author": "Yufei Ye and Yao Feng and Omid Taheri and Haiwen Feng and Shubham Tulsiani and Michael J. Black", "abstract": "  We present HaPTIC, an approach that infers coherent 4D hand trajectories from\nmonocular videos. Current video-based hand pose reconstruction methods\nprimarily focus on improving frame-wise 3D pose using adjacent frames rather\nthan studying consistent 4D hand trajectories in space. Despite the additional\ntemporal cues, they generally underperform compared to image-based methods due\nto the scarcity of annotated video data. To address these issues, we repurpose\na state-of-the-art image-based transformer to take in multiple frames and\ndirectly predict a coherent trajectory. We introduce two types of lightweight\nattention layers: cross-view self-attention to fuse temporal information, and\nglobal cross-attention to bring in larger spatial context. Our method infers 4D\nhand trajectories similar to the ground truth while maintaining strong 2D\nreprojection alignment. We apply the method to both egocentric and allocentric\nvideos. It significantly outperforms existing methods in global trajectory\naccuracy while being comparable to the state-of-the-art in single-image pose\nestimation. Project website: https://judyye.github.io/haptic-www\n", "link": "http://arxiv.org/abs/2501.08329v1", "date": "2025-01-14", "relevancy": 2.3051, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%204D%20Hand%20Trajectory%20from%20Monocular%20Videos&body=Title%3A%20Predicting%204D%20Hand%20Trajectory%20from%20Monocular%20Videos%0AAuthor%3A%20Yufei%20Ye%20and%20Yao%20Feng%20and%20Omid%20Taheri%20and%20Haiwen%20Feng%20and%20Shubham%20Tulsiani%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20We%20present%20HaPTIC%2C%20an%20approach%20that%20infers%20coherent%204D%20hand%20trajectories%20from%0Amonocular%20videos.%20Current%20video-based%20hand%20pose%20reconstruction%20methods%0Aprimarily%20focus%20on%20improving%20frame-wise%203D%20pose%20using%20adjacent%20frames%20rather%0Athan%20studying%20consistent%204D%20hand%20trajectories%20in%20space.%20Despite%20the%20additional%0Atemporal%20cues%2C%20they%20generally%20underperform%20compared%20to%20image-based%20methods%20due%0Ato%20the%20scarcity%20of%20annotated%20video%20data.%20To%20address%20these%20issues%2C%20we%20repurpose%0Aa%20state-of-the-art%20image-based%20transformer%20to%20take%20in%20multiple%20frames%20and%0Adirectly%20predict%20a%20coherent%20trajectory.%20We%20introduce%20two%20types%20of%20lightweight%0Aattention%20layers%3A%20cross-view%20self-attention%20to%20fuse%20temporal%20information%2C%20and%0Aglobal%20cross-attention%20to%20bring%20in%20larger%20spatial%20context.%20Our%20method%20infers%204D%0Ahand%20trajectories%20similar%20to%20the%20ground%20truth%20while%20maintaining%20strong%202D%0Areprojection%20alignment.%20We%20apply%20the%20method%20to%20both%20egocentric%20and%20allocentric%0Avideos.%20It%20significantly%20outperforms%20existing%20methods%20in%20global%20trajectory%0Aaccuracy%20while%20being%20comparable%20to%20the%20state-of-the-art%20in%20single-image%20pose%0Aestimation.%20Project%20website%3A%20https%3A//judyye.github.io/haptic-www%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%25204D%2520Hand%2520Trajectory%2520from%2520Monocular%2520Videos%26entry.906535625%3DYufei%2520Ye%2520and%2520Yao%2520Feng%2520and%2520Omid%2520Taheri%2520and%2520Haiwen%2520Feng%2520and%2520Shubham%2520Tulsiani%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520We%2520present%2520HaPTIC%252C%2520an%2520approach%2520that%2520infers%2520coherent%25204D%2520hand%2520trajectories%2520from%250Amonocular%2520videos.%2520Current%2520video-based%2520hand%2520pose%2520reconstruction%2520methods%250Aprimarily%2520focus%2520on%2520improving%2520frame-wise%25203D%2520pose%2520using%2520adjacent%2520frames%2520rather%250Athan%2520studying%2520consistent%25204D%2520hand%2520trajectories%2520in%2520space.%2520Despite%2520the%2520additional%250Atemporal%2520cues%252C%2520they%2520generally%2520underperform%2520compared%2520to%2520image-based%2520methods%2520due%250Ato%2520the%2520scarcity%2520of%2520annotated%2520video%2520data.%2520To%2520address%2520these%2520issues%252C%2520we%2520repurpose%250Aa%2520state-of-the-art%2520image-based%2520transformer%2520to%2520take%2520in%2520multiple%2520frames%2520and%250Adirectly%2520predict%2520a%2520coherent%2520trajectory.%2520We%2520introduce%2520two%2520types%2520of%2520lightweight%250Aattention%2520layers%253A%2520cross-view%2520self-attention%2520to%2520fuse%2520temporal%2520information%252C%2520and%250Aglobal%2520cross-attention%2520to%2520bring%2520in%2520larger%2520spatial%2520context.%2520Our%2520method%2520infers%25204D%250Ahand%2520trajectories%2520similar%2520to%2520the%2520ground%2520truth%2520while%2520maintaining%2520strong%25202D%250Areprojection%2520alignment.%2520We%2520apply%2520the%2520method%2520to%2520both%2520egocentric%2520and%2520allocentric%250Avideos.%2520It%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520global%2520trajectory%250Aaccuracy%2520while%2520being%2520comparable%2520to%2520the%2520state-of-the-art%2520in%2520single-image%2520pose%250Aestimation.%2520Project%2520website%253A%2520https%253A//judyye.github.io/haptic-www%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%204D%20Hand%20Trajectory%20from%20Monocular%20Videos&entry.906535625=Yufei%20Ye%20and%20Yao%20Feng%20and%20Omid%20Taheri%20and%20Haiwen%20Feng%20and%20Shubham%20Tulsiani%20and%20Michael%20J.%20Black&entry.1292438233=%20%20We%20present%20HaPTIC%2C%20an%20approach%20that%20infers%20coherent%204D%20hand%20trajectories%20from%0Amonocular%20videos.%20Current%20video-based%20hand%20pose%20reconstruction%20methods%0Aprimarily%20focus%20on%20improving%20frame-wise%203D%20pose%20using%20adjacent%20frames%20rather%0Athan%20studying%20consistent%204D%20hand%20trajectories%20in%20space.%20Despite%20the%20additional%0Atemporal%20cues%2C%20they%20generally%20underperform%20compared%20to%20image-based%20methods%20due%0Ato%20the%20scarcity%20of%20annotated%20video%20data.%20To%20address%20these%20issues%2C%20we%20repurpose%0Aa%20state-of-the-art%20image-based%20transformer%20to%20take%20in%20multiple%20frames%20and%0Adirectly%20predict%20a%20coherent%20trajectory.%20We%20introduce%20two%20types%20of%20lightweight%0Aattention%20layers%3A%20cross-view%20self-attention%20to%20fuse%20temporal%20information%2C%20and%0Aglobal%20cross-attention%20to%20bring%20in%20larger%20spatial%20context.%20Our%20method%20infers%204D%0Ahand%20trajectories%20similar%20to%20the%20ground%20truth%20while%20maintaining%20strong%202D%0Areprojection%20alignment.%20We%20apply%20the%20method%20to%20both%20egocentric%20and%20allocentric%0Avideos.%20It%20significantly%20outperforms%20existing%20methods%20in%20global%20trajectory%0Aaccuracy%20while%20being%20comparable%20to%20the%20state-of-the-art%20in%20single-image%20pose%0Aestimation.%20Project%20website%3A%20https%3A//judyye.github.io/haptic-www%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08329v1&entry.124074799=Read"},
{"title": "I Can Find You in Seconds! Leveraging Large Language Models for Code\n  Authorship Attribution", "author": "Soohyeon Choi and Yong Kiam Tan and Mark Huasong Meng and Mohamed Ragab and Soumik Mondal and David Mohaisen and Khin Mi Mi Aung", "abstract": "  Source code authorship attribution is important in software forensics,\nplagiarism detection, and protecting software patch integrity. Existing\ntechniques often rely on supervised machine learning, which struggles with\ngeneralization across different programming languages and coding styles due to\nthe need for large labeled datasets. Inspired by recent advances in natural\nlanguage authorship analysis using large language models (LLMs), which have\nshown exceptional performance without task-specific tuning, this paper explores\nthe use of LLMs for source code authorship attribution.\n  We present a comprehensive study demonstrating that state-of-the-art LLMs can\nsuccessfully attribute source code authorship across different languages. LLMs\ncan determine whether two code snippets are written by the same author with\nzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of\n0.78, and can attribute code authorship from a small set of reference code\nsnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show\nsome adversarial robustness against misattribution attacks.\n  Despite these capabilities, we found that naive prompting of LLMs does not\nscale well with a large number of authors due to input token limitations. To\naddress this, we propose a tournament-style approach for large-scale\nattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355\nsamples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve\nclassification accuracy of up to 65% for C++ and 68.7% for Java using only one\nreference per author. These results open new possibilities for applying LLMs to\ncode authorship attribution in cybersecurity and software engineering.\n", "link": "http://arxiv.org/abs/2501.08165v1", "date": "2025-01-14", "relevancy": 2.3001, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Can%20Find%20You%20in%20Seconds%21%20Leveraging%20Large%20Language%20Models%20for%20Code%0A%20%20Authorship%20Attribution&body=Title%3A%20I%20Can%20Find%20You%20in%20Seconds%21%20Leveraging%20Large%20Language%20Models%20for%20Code%0A%20%20Authorship%20Attribution%0AAuthor%3A%20Soohyeon%20Choi%20and%20Yong%20Kiam%20Tan%20and%20Mark%20Huasong%20Meng%20and%20Mohamed%20Ragab%20and%20Soumik%20Mondal%20and%20David%20Mohaisen%20and%20Khin%20Mi%20Mi%20Aung%0AAbstract%3A%20%20%20Source%20code%20authorship%20attribution%20is%20important%20in%20software%20forensics%2C%0Aplagiarism%20detection%2C%20and%20protecting%20software%20patch%20integrity.%20Existing%0Atechniques%20often%20rely%20on%20supervised%20machine%20learning%2C%20which%20struggles%20with%0Ageneralization%20across%20different%20programming%20languages%20and%20coding%20styles%20due%20to%0Athe%20need%20for%20large%20labeled%20datasets.%20Inspired%20by%20recent%20advances%20in%20natural%0Alanguage%20authorship%20analysis%20using%20large%20language%20models%20%28LLMs%29%2C%20which%20have%0Ashown%20exceptional%20performance%20without%20task-specific%20tuning%2C%20this%20paper%20explores%0Athe%20use%20of%20LLMs%20for%20source%20code%20authorship%20attribution.%0A%20%20We%20present%20a%20comprehensive%20study%20demonstrating%20that%20state-of-the-art%20LLMs%20can%0Asuccessfully%20attribute%20source%20code%20authorship%20across%20different%20languages.%20LLMs%0Acan%20determine%20whether%20two%20code%20snippets%20are%20written%20by%20the%20same%20author%20with%0Azero-shot%20prompting%2C%20achieving%20a%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%0A0.78%2C%20and%20can%20attribute%20code%20authorship%20from%20a%20small%20set%20of%20reference%20code%0Asnippets%20via%20few-shot%20learning%2C%20achieving%20MCC%20of%200.77.%20Additionally%2C%20LLMs%20show%0Asome%20adversarial%20robustness%20against%20misattribution%20attacks.%0A%20%20Despite%20these%20capabilities%2C%20we%20found%20that%20naive%20prompting%20of%20LLMs%20does%20not%0Ascale%20well%20with%20a%20large%20number%20of%20authors%20due%20to%20input%20token%20limitations.%20To%0Aaddress%20this%2C%20we%20propose%20a%20tournament-style%20approach%20for%20large-scale%0Aattribution.%20Evaluating%20this%20approach%20on%20datasets%20of%20C%2B%2B%20%28500%20authors%2C%2026%2C355%0Asamples%29%20and%20Java%20%28686%20authors%2C%2055%2C267%20samples%29%20code%20from%20GitHub%2C%20we%20achieve%0Aclassification%20accuracy%20of%20up%20to%2065%25%20for%20C%2B%2B%20and%2068.7%25%20for%20Java%20using%20only%20one%0Areference%20per%20author.%20These%20results%20open%20new%20possibilities%20for%20applying%20LLMs%20to%0Acode%20authorship%20attribution%20in%20cybersecurity%20and%20software%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Can%2520Find%2520You%2520in%2520Seconds%2521%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Code%250A%2520%2520Authorship%2520Attribution%26entry.906535625%3DSoohyeon%2520Choi%2520and%2520Yong%2520Kiam%2520Tan%2520and%2520Mark%2520Huasong%2520Meng%2520and%2520Mohamed%2520Ragab%2520and%2520Soumik%2520Mondal%2520and%2520David%2520Mohaisen%2520and%2520Khin%2520Mi%2520Mi%2520Aung%26entry.1292438233%3D%2520%2520Source%2520code%2520authorship%2520attribution%2520is%2520important%2520in%2520software%2520forensics%252C%250Aplagiarism%2520detection%252C%2520and%2520protecting%2520software%2520patch%2520integrity.%2520Existing%250Atechniques%2520often%2520rely%2520on%2520supervised%2520machine%2520learning%252C%2520which%2520struggles%2520with%250Ageneralization%2520across%2520different%2520programming%2520languages%2520and%2520coding%2520styles%2520due%2520to%250Athe%2520need%2520for%2520large%2520labeled%2520datasets.%2520Inspired%2520by%2520recent%2520advances%2520in%2520natural%250Alanguage%2520authorship%2520analysis%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520have%250Ashown%2520exceptional%2520performance%2520without%2520task-specific%2520tuning%252C%2520this%2520paper%2520explores%250Athe%2520use%2520of%2520LLMs%2520for%2520source%2520code%2520authorship%2520attribution.%250A%2520%2520We%2520present%2520a%2520comprehensive%2520study%2520demonstrating%2520that%2520state-of-the-art%2520LLMs%2520can%250Asuccessfully%2520attribute%2520source%2520code%2520authorship%2520across%2520different%2520languages.%2520LLMs%250Acan%2520determine%2520whether%2520two%2520code%2520snippets%2520are%2520written%2520by%2520the%2520same%2520author%2520with%250Azero-shot%2520prompting%252C%2520achieving%2520a%2520Matthews%2520Correlation%2520Coefficient%2520%2528MCC%2529%2520of%250A0.78%252C%2520and%2520can%2520attribute%2520code%2520authorship%2520from%2520a%2520small%2520set%2520of%2520reference%2520code%250Asnippets%2520via%2520few-shot%2520learning%252C%2520achieving%2520MCC%2520of%25200.77.%2520Additionally%252C%2520LLMs%2520show%250Asome%2520adversarial%2520robustness%2520against%2520misattribution%2520attacks.%250A%2520%2520Despite%2520these%2520capabilities%252C%2520we%2520found%2520that%2520naive%2520prompting%2520of%2520LLMs%2520does%2520not%250Ascale%2520well%2520with%2520a%2520large%2520number%2520of%2520authors%2520due%2520to%2520input%2520token%2520limitations.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520tournament-style%2520approach%2520for%2520large-scale%250Aattribution.%2520Evaluating%2520this%2520approach%2520on%2520datasets%2520of%2520C%252B%252B%2520%2528500%2520authors%252C%252026%252C355%250Asamples%2529%2520and%2520Java%2520%2528686%2520authors%252C%252055%252C267%2520samples%2529%2520code%2520from%2520GitHub%252C%2520we%2520achieve%250Aclassification%2520accuracy%2520of%2520up%2520to%252065%2525%2520for%2520C%252B%252B%2520and%252068.7%2525%2520for%2520Java%2520using%2520only%2520one%250Areference%2520per%2520author.%2520These%2520results%2520open%2520new%2520possibilities%2520for%2520applying%2520LLMs%2520to%250Acode%2520authorship%2520attribution%2520in%2520cybersecurity%2520and%2520software%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Can%20Find%20You%20in%20Seconds%21%20Leveraging%20Large%20Language%20Models%20for%20Code%0A%20%20Authorship%20Attribution&entry.906535625=Soohyeon%20Choi%20and%20Yong%20Kiam%20Tan%20and%20Mark%20Huasong%20Meng%20and%20Mohamed%20Ragab%20and%20Soumik%20Mondal%20and%20David%20Mohaisen%20and%20Khin%20Mi%20Mi%20Aung&entry.1292438233=%20%20Source%20code%20authorship%20attribution%20is%20important%20in%20software%20forensics%2C%0Aplagiarism%20detection%2C%20and%20protecting%20software%20patch%20integrity.%20Existing%0Atechniques%20often%20rely%20on%20supervised%20machine%20learning%2C%20which%20struggles%20with%0Ageneralization%20across%20different%20programming%20languages%20and%20coding%20styles%20due%20to%0Athe%20need%20for%20large%20labeled%20datasets.%20Inspired%20by%20recent%20advances%20in%20natural%0Alanguage%20authorship%20analysis%20using%20large%20language%20models%20%28LLMs%29%2C%20which%20have%0Ashown%20exceptional%20performance%20without%20task-specific%20tuning%2C%20this%20paper%20explores%0Athe%20use%20of%20LLMs%20for%20source%20code%20authorship%20attribution.%0A%20%20We%20present%20a%20comprehensive%20study%20demonstrating%20that%20state-of-the-art%20LLMs%20can%0Asuccessfully%20attribute%20source%20code%20authorship%20across%20different%20languages.%20LLMs%0Acan%20determine%20whether%20two%20code%20snippets%20are%20written%20by%20the%20same%20author%20with%0Azero-shot%20prompting%2C%20achieving%20a%20Matthews%20Correlation%20Coefficient%20%28MCC%29%20of%0A0.78%2C%20and%20can%20attribute%20code%20authorship%20from%20a%20small%20set%20of%20reference%20code%0Asnippets%20via%20few-shot%20learning%2C%20achieving%20MCC%20of%200.77.%20Additionally%2C%20LLMs%20show%0Asome%20adversarial%20robustness%20against%20misattribution%20attacks.%0A%20%20Despite%20these%20capabilities%2C%20we%20found%20that%20naive%20prompting%20of%20LLMs%20does%20not%0Ascale%20well%20with%20a%20large%20number%20of%20authors%20due%20to%20input%20token%20limitations.%20To%0Aaddress%20this%2C%20we%20propose%20a%20tournament-style%20approach%20for%20large-scale%0Aattribution.%20Evaluating%20this%20approach%20on%20datasets%20of%20C%2B%2B%20%28500%20authors%2C%2026%2C355%0Asamples%29%20and%20Java%20%28686%20authors%2C%2055%2C267%20samples%29%20code%20from%20GitHub%2C%20we%20achieve%0Aclassification%20accuracy%20of%20up%20to%2065%25%20for%20C%2B%2B%20and%2068.7%25%20for%20Java%20using%20only%20one%0Areference%20per%20author.%20These%20results%20open%20new%20possibilities%20for%20applying%20LLMs%20to%0Acode%20authorship%20attribution%20in%20cybersecurity%20and%20software%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08165v1&entry.124074799=Read"},
{"title": "Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective", "author": "Qishuai Wen and Chun-Guang Li", "abstract": "  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n", "link": "http://arxiv.org/abs/2411.03033v3", "date": "2025-01-14", "relevancy": 2.2993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective&body=Title%3A%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective%0AAuthor%3A%20Qishuai%20Wen%20and%20Chun-Guang%20Li%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03033v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Decoders%2520for%2520Transformer-based%2520Semantic%2520Segmentation%253A%2520A%250A%2520%2520Compression%2520Perspective%26entry.906535625%3DQishuai%2520Wen%2520and%2520Chun-Guang%2520Li%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520Transformer-based%2520semantic%2520segmentation%250Atypically%2520adopt%2520Transformer%2520decoders%2520that%2520are%2520used%2520to%2520extract%2520additional%250Aembeddings%2520from%2520image%2520embeddings%2520via%2520cross-attention%252C%2520refine%2520either%2520or%2520both%250Atypes%2520of%2520embeddings%2520via%2520self-attention%252C%2520and%2520project%2520image%2520embeddings%2520onto%2520the%250Aadditional%2520embeddings%2520via%2520dot-product.%2520Despite%2520their%2520remarkable%2520success%252C%2520these%250Aempirical%2520designs%2520still%2520lack%2520theoretical%2520justifications%2520or%2520interpretations%252C%250Athus%2520hindering%2520potentially%2520principled%2520improvements.%2520In%2520this%2520paper%252C%2520we%2520argue%250Athat%2520there%2520are%2520fundamental%2520connections%2520between%2520semantic%2520segmentation%2520and%250Acompression%252C%2520especially%2520between%2520the%2520Transformer%2520decoders%2520and%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529.%2520From%2520such%2520a%2520perspective%252C%2520we%2520derive%2520a%2520white-box%252C%2520fully%250Aattentional%2520DEcoder%2520for%2520PrIncipled%2520semantiC%2520segemenTation%2520%2528DEPICT%2529%252C%2520with%2520the%250Ainterpretations%2520as%2520follows%253A%25201%2529%2520the%2520self-attention%2520operator%2520refines%2520image%250Aembeddings%2520to%2520construct%2520an%2520ideal%2520principal%2520subspace%2520that%2520aligns%2520with%2520the%250Asupervision%2520and%2520retains%2520most%2520information%253B%25202%2529%2520the%2520cross-attention%2520operator%2520seeks%250Ato%2520find%2520a%2520low-rank%2520approximation%2520of%2520the%2520refined%2520image%2520embeddings%252C%2520which%2520is%250Aexpected%2520to%2520be%2520a%2520set%2520of%2520orthonormal%2520bases%2520of%2520the%2520principal%2520subspace%2520and%250Acorresponds%2520to%2520the%2520predefined%2520classes%253B%25203%2529%2520the%2520dot-product%2520operation%2520yields%250Acompact%2520representation%2520for%2520image%2520embeddings%2520as%2520segmentation%2520masks.%2520Experiments%250Aconducted%2520on%2520dataset%2520ADE20K%2520find%2520that%2520DEPICT%2520consistently%2520outperforms%2520its%250Ablack-box%2520counterpart%252C%2520Segmenter%252C%2520and%2520it%2520is%2520light%2520weight%2520and%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03033v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective&entry.906535625=Qishuai%20Wen%20and%20Chun-Guang%20Li&entry.1292438233=%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03033v3&entry.124074799=Read"},
{"title": "Benchmarking Graph Representations and Graph Neural Networks for\n  Multivariate Time Series Classification", "author": "Wennuo Yang and Shiling Wu and Yuzhi Zhou and Weicheng Xie and Linlin Shen and Siyang Song", "abstract": "  Multivariate Time Series Classification (MTSC) enables the analysis if\ncomplex temporal data, and thus serves as a cornerstone in various real-world\napplications, ranging from healthcare to finance. Since the relationship among\nvariables in MTS usually contain crucial cues, a large number of graph-based\nMTSC approaches have been proposed, as the graph topology and edges can\nexplicitly represent relationships among variables (channels), where not only\nvarious MTS graph representation learning strategies but also different Graph\nNeural Networks (GNNs) have been explored. Despite such progresses, there is no\ncomprehensive study that fairly benchmarks and investigates the performances of\nexisting widely-used graph representation learning strategies/GNN classifiers\nin the application of different MTSC tasks. In this paper, we present the first\nbenchmark which systematically investigates the effectiveness of the\nwidely-used three node feature definition strategies, four edge feature\nlearning strategies and five GNN architecture, resulting in 60 different\nvariants for graph-based MTSC. These variants are developed and evaluated with\na standardized data pipeline and training/validation/testing strategy on 26\nwidely-used suspensor MTSC datasets. Our experiments highlight that node\nfeatures significantly influence MTSC performance, while the visualization of\nedge features illustrates why adaptive edge learning outperforms other edge\nfeature learning methods. The code of the proposed benchmark is publicly\navailable at\n\\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.\n", "link": "http://arxiv.org/abs/2501.08305v1", "date": "2025-01-14", "relevancy": 2.2909, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4825}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4476}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Graph%20Representations%20and%20Graph%20Neural%20Networks%20for%0A%20%20Multivariate%20Time%20Series%20Classification&body=Title%3A%20Benchmarking%20Graph%20Representations%20and%20Graph%20Neural%20Networks%20for%0A%20%20Multivariate%20Time%20Series%20Classification%0AAuthor%3A%20Wennuo%20Yang%20and%20Shiling%20Wu%20and%20Yuzhi%20Zhou%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%20and%20Siyang%20Song%0AAbstract%3A%20%20%20Multivariate%20Time%20Series%20Classification%20%28MTSC%29%20enables%20the%20analysis%20if%0Acomplex%20temporal%20data%2C%20and%20thus%20serves%20as%20a%20cornerstone%20in%20various%20real-world%0Aapplications%2C%20ranging%20from%20healthcare%20to%20finance.%20Since%20the%20relationship%20among%0Avariables%20in%20MTS%20usually%20contain%20crucial%20cues%2C%20a%20large%20number%20of%20graph-based%0AMTSC%20approaches%20have%20been%20proposed%2C%20as%20the%20graph%20topology%20and%20edges%20can%0Aexplicitly%20represent%20relationships%20among%20variables%20%28channels%29%2C%20where%20not%20only%0Avarious%20MTS%20graph%20representation%20learning%20strategies%20but%20also%20different%20Graph%0ANeural%20Networks%20%28GNNs%29%20have%20been%20explored.%20Despite%20such%20progresses%2C%20there%20is%20no%0Acomprehensive%20study%20that%20fairly%20benchmarks%20and%20investigates%20the%20performances%20of%0Aexisting%20widely-used%20graph%20representation%20learning%20strategies/GNN%20classifiers%0Ain%20the%20application%20of%20different%20MTSC%20tasks.%20In%20this%20paper%2C%20we%20present%20the%20first%0Abenchmark%20which%20systematically%20investigates%20the%20effectiveness%20of%20the%0Awidely-used%20three%20node%20feature%20definition%20strategies%2C%20four%20edge%20feature%0Alearning%20strategies%20and%20five%20GNN%20architecture%2C%20resulting%20in%2060%20different%0Avariants%20for%20graph-based%20MTSC.%20These%20variants%20are%20developed%20and%20evaluated%20with%0Aa%20standardized%20data%20pipeline%20and%20training/validation/testing%20strategy%20on%2026%0Awidely-used%20suspensor%20MTSC%20datasets.%20Our%20experiments%20highlight%20that%20node%0Afeatures%20significantly%20influence%20MTSC%20performance%2C%20while%20the%20visualization%20of%0Aedge%20features%20illustrates%20why%20adaptive%20edge%20learning%20outperforms%20other%20edge%0Afeature%20learning%20methods.%20The%20code%20of%20the%20proposed%20benchmark%20is%20publicly%0Aavailable%20at%0A%5Curl%7Bhttps%3A//github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Graph%2520Representations%2520and%2520Graph%2520Neural%2520Networks%2520for%250A%2520%2520Multivariate%2520Time%2520Series%2520Classification%26entry.906535625%3DWennuo%2520Yang%2520and%2520Shiling%2520Wu%2520and%2520Yuzhi%2520Zhou%2520and%2520Weicheng%2520Xie%2520and%2520Linlin%2520Shen%2520and%2520Siyang%2520Song%26entry.1292438233%3D%2520%2520Multivariate%2520Time%2520Series%2520Classification%2520%2528MTSC%2529%2520enables%2520the%2520analysis%2520if%250Acomplex%2520temporal%2520data%252C%2520and%2520thus%2520serves%2520as%2520a%2520cornerstone%2520in%2520various%2520real-world%250Aapplications%252C%2520ranging%2520from%2520healthcare%2520to%2520finance.%2520Since%2520the%2520relationship%2520among%250Avariables%2520in%2520MTS%2520usually%2520contain%2520crucial%2520cues%252C%2520a%2520large%2520number%2520of%2520graph-based%250AMTSC%2520approaches%2520have%2520been%2520proposed%252C%2520as%2520the%2520graph%2520topology%2520and%2520edges%2520can%250Aexplicitly%2520represent%2520relationships%2520among%2520variables%2520%2528channels%2529%252C%2520where%2520not%2520only%250Avarious%2520MTS%2520graph%2520representation%2520learning%2520strategies%2520but%2520also%2520different%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520have%2520been%2520explored.%2520Despite%2520such%2520progresses%252C%2520there%2520is%2520no%250Acomprehensive%2520study%2520that%2520fairly%2520benchmarks%2520and%2520investigates%2520the%2520performances%2520of%250Aexisting%2520widely-used%2520graph%2520representation%2520learning%2520strategies/GNN%2520classifiers%250Ain%2520the%2520application%2520of%2520different%2520MTSC%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%250Abenchmark%2520which%2520systematically%2520investigates%2520the%2520effectiveness%2520of%2520the%250Awidely-used%2520three%2520node%2520feature%2520definition%2520strategies%252C%2520four%2520edge%2520feature%250Alearning%2520strategies%2520and%2520five%2520GNN%2520architecture%252C%2520resulting%2520in%252060%2520different%250Avariants%2520for%2520graph-based%2520MTSC.%2520These%2520variants%2520are%2520developed%2520and%2520evaluated%2520with%250Aa%2520standardized%2520data%2520pipeline%2520and%2520training/validation/testing%2520strategy%2520on%252026%250Awidely-used%2520suspensor%2520MTSC%2520datasets.%2520Our%2520experiments%2520highlight%2520that%2520node%250Afeatures%2520significantly%2520influence%2520MTSC%2520performance%252C%2520while%2520the%2520visualization%2520of%250Aedge%2520features%2520illustrates%2520why%2520adaptive%2520edge%2520learning%2520outperforms%2520other%2520edge%250Afeature%2520learning%2520methods.%2520The%2520code%2520of%2520the%2520proposed%2520benchmark%2520is%2520publicly%250Aavailable%2520at%250A%255Curl%257Bhttps%253A//github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Graph%20Representations%20and%20Graph%20Neural%20Networks%20for%0A%20%20Multivariate%20Time%20Series%20Classification&entry.906535625=Wennuo%20Yang%20and%20Shiling%20Wu%20and%20Yuzhi%20Zhou%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%20and%20Siyang%20Song&entry.1292438233=%20%20Multivariate%20Time%20Series%20Classification%20%28MTSC%29%20enables%20the%20analysis%20if%0Acomplex%20temporal%20data%2C%20and%20thus%20serves%20as%20a%20cornerstone%20in%20various%20real-world%0Aapplications%2C%20ranging%20from%20healthcare%20to%20finance.%20Since%20the%20relationship%20among%0Avariables%20in%20MTS%20usually%20contain%20crucial%20cues%2C%20a%20large%20number%20of%20graph-based%0AMTSC%20approaches%20have%20been%20proposed%2C%20as%20the%20graph%20topology%20and%20edges%20can%0Aexplicitly%20represent%20relationships%20among%20variables%20%28channels%29%2C%20where%20not%20only%0Avarious%20MTS%20graph%20representation%20learning%20strategies%20but%20also%20different%20Graph%0ANeural%20Networks%20%28GNNs%29%20have%20been%20explored.%20Despite%20such%20progresses%2C%20there%20is%20no%0Acomprehensive%20study%20that%20fairly%20benchmarks%20and%20investigates%20the%20performances%20of%0Aexisting%20widely-used%20graph%20representation%20learning%20strategies/GNN%20classifiers%0Ain%20the%20application%20of%20different%20MTSC%20tasks.%20In%20this%20paper%2C%20we%20present%20the%20first%0Abenchmark%20which%20systematically%20investigates%20the%20effectiveness%20of%20the%0Awidely-used%20three%20node%20feature%20definition%20strategies%2C%20four%20edge%20feature%0Alearning%20strategies%20and%20five%20GNN%20architecture%2C%20resulting%20in%2060%20different%0Avariants%20for%20graph-based%20MTSC.%20These%20variants%20are%20developed%20and%20evaluated%20with%0Aa%20standardized%20data%20pipeline%20and%20training/validation/testing%20strategy%20on%2026%0Awidely-used%20suspensor%20MTSC%20datasets.%20Our%20experiments%20highlight%20that%20node%0Afeatures%20significantly%20influence%20MTSC%20performance%2C%20while%20the%20visualization%20of%0Aedge%20features%20illustrates%20why%20adaptive%20edge%20learning%20outperforms%20other%20edge%0Afeature%20learning%20methods.%20The%20code%20of%20the%20proposed%20benchmark%20is%20publicly%0Aavailable%20at%0A%5Curl%7Bhttps%3A//github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08305v1&entry.124074799=Read"},
{"title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban\n  Navigation", "author": "Ziyang Xie and Zhizheng Liu and Zhenghao Peng and Wayne Wu and Bolei Zhou", "abstract": "  Sim-to-real gap has long posed a significant challenge for robot learning in\nsimulation, preventing the deployment of learned models in the real world.\nPrevious work has primarily focused on domain randomization and system\nidentification to mitigate this gap. However, these methods are often limited\nby the inherent constraints of the simulation and graphics engines. In this\nwork, we propose Vid2Sim, a novel framework that effectively bridges the\nsim2real gap through a scalable and cost-efficient real2sim pipeline for neural\n3D scene reconstruction and simulation. Given a monocular video as input,\nVid2Sim can generate photorealistic and physically interactable 3D simulation\nenvironments to enable the reinforcement learning of visual navigation agents\nin complex urban environments. Extensive experiments demonstrate that Vid2Sim\nsignificantly improves the performance of urban navigation in the digital twins\nand real world by 31.2% and 68.3% in success rate compared with agents trained\nwith prior simulation methods.\n", "link": "http://arxiv.org/abs/2501.06693v2", "date": "2025-01-14", "relevancy": 2.2475, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.56}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vid2Sim%3A%20Realistic%20and%20Interactive%20Simulation%20from%20Video%20for%20Urban%0A%20%20Navigation&body=Title%3A%20Vid2Sim%3A%20Realistic%20and%20Interactive%20Simulation%20from%20Video%20for%20Urban%0A%20%20Navigation%0AAuthor%3A%20Ziyang%20Xie%20and%20Zhizheng%20Liu%20and%20Zhenghao%20Peng%20and%20Wayne%20Wu%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Sim-to-real%20gap%20has%20long%20posed%20a%20significant%20challenge%20for%20robot%20learning%20in%0Asimulation%2C%20preventing%20the%20deployment%20of%20learned%20models%20in%20the%20real%20world.%0APrevious%20work%20has%20primarily%20focused%20on%20domain%20randomization%20and%20system%0Aidentification%20to%20mitigate%20this%20gap.%20However%2C%20these%20methods%20are%20often%20limited%0Aby%20the%20inherent%20constraints%20of%20the%20simulation%20and%20graphics%20engines.%20In%20this%0Awork%2C%20we%20propose%20Vid2Sim%2C%20a%20novel%20framework%20that%20effectively%20bridges%20the%0Asim2real%20gap%20through%20a%20scalable%20and%20cost-efficient%20real2sim%20pipeline%20for%20neural%0A3D%20scene%20reconstruction%20and%20simulation.%20Given%20a%20monocular%20video%20as%20input%2C%0AVid2Sim%20can%20generate%20photorealistic%20and%20physically%20interactable%203D%20simulation%0Aenvironments%20to%20enable%20the%20reinforcement%20learning%20of%20visual%20navigation%20agents%0Ain%20complex%20urban%20environments.%20Extensive%20experiments%20demonstrate%20that%20Vid2Sim%0Asignificantly%20improves%20the%20performance%20of%20urban%20navigation%20in%20the%20digital%20twins%0Aand%20real%20world%20by%2031.2%25%20and%2068.3%25%20in%20success%20rate%20compared%20with%20agents%20trained%0Awith%20prior%20simulation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06693v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVid2Sim%253A%2520Realistic%2520and%2520Interactive%2520Simulation%2520from%2520Video%2520for%2520Urban%250A%2520%2520Navigation%26entry.906535625%3DZiyang%2520Xie%2520and%2520Zhizheng%2520Liu%2520and%2520Zhenghao%2520Peng%2520and%2520Wayne%2520Wu%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Sim-to-real%2520gap%2520has%2520long%2520posed%2520a%2520significant%2520challenge%2520for%2520robot%2520learning%2520in%250Asimulation%252C%2520preventing%2520the%2520deployment%2520of%2520learned%2520models%2520in%2520the%2520real%2520world.%250APrevious%2520work%2520has%2520primarily%2520focused%2520on%2520domain%2520randomization%2520and%2520system%250Aidentification%2520to%2520mitigate%2520this%2520gap.%2520However%252C%2520these%2520methods%2520are%2520often%2520limited%250Aby%2520the%2520inherent%2520constraints%2520of%2520the%2520simulation%2520and%2520graphics%2520engines.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Vid2Sim%252C%2520a%2520novel%2520framework%2520that%2520effectively%2520bridges%2520the%250Asim2real%2520gap%2520through%2520a%2520scalable%2520and%2520cost-efficient%2520real2sim%2520pipeline%2520for%2520neural%250A3D%2520scene%2520reconstruction%2520and%2520simulation.%2520Given%2520a%2520monocular%2520video%2520as%2520input%252C%250AVid2Sim%2520can%2520generate%2520photorealistic%2520and%2520physically%2520interactable%25203D%2520simulation%250Aenvironments%2520to%2520enable%2520the%2520reinforcement%2520learning%2520of%2520visual%2520navigation%2520agents%250Ain%2520complex%2520urban%2520environments.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Vid2Sim%250Asignificantly%2520improves%2520the%2520performance%2520of%2520urban%2520navigation%2520in%2520the%2520digital%2520twins%250Aand%2520real%2520world%2520by%252031.2%2525%2520and%252068.3%2525%2520in%2520success%2520rate%2520compared%2520with%2520agents%2520trained%250Awith%2520prior%2520simulation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06693v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vid2Sim%3A%20Realistic%20and%20Interactive%20Simulation%20from%20Video%20for%20Urban%0A%20%20Navigation&entry.906535625=Ziyang%20Xie%20and%20Zhizheng%20Liu%20and%20Zhenghao%20Peng%20and%20Wayne%20Wu%20and%20Bolei%20Zhou&entry.1292438233=%20%20Sim-to-real%20gap%20has%20long%20posed%20a%20significant%20challenge%20for%20robot%20learning%20in%0Asimulation%2C%20preventing%20the%20deployment%20of%20learned%20models%20in%20the%20real%20world.%0APrevious%20work%20has%20primarily%20focused%20on%20domain%20randomization%20and%20system%0Aidentification%20to%20mitigate%20this%20gap.%20However%2C%20these%20methods%20are%20often%20limited%0Aby%20the%20inherent%20constraints%20of%20the%20simulation%20and%20graphics%20engines.%20In%20this%0Awork%2C%20we%20propose%20Vid2Sim%2C%20a%20novel%20framework%20that%20effectively%20bridges%20the%0Asim2real%20gap%20through%20a%20scalable%20and%20cost-efficient%20real2sim%20pipeline%20for%20neural%0A3D%20scene%20reconstruction%20and%20simulation.%20Given%20a%20monocular%20video%20as%20input%2C%0AVid2Sim%20can%20generate%20photorealistic%20and%20physically%20interactable%203D%20simulation%0Aenvironments%20to%20enable%20the%20reinforcement%20learning%20of%20visual%20navigation%20agents%0Ain%20complex%20urban%20environments.%20Extensive%20experiments%20demonstrate%20that%20Vid2Sim%0Asignificantly%20improves%20the%20performance%20of%20urban%20navigation%20in%20the%20digital%20twins%0Aand%20real%20world%20by%2031.2%25%20and%2068.3%25%20in%20success%20rate%20compared%20with%20agents%20trained%0Awith%20prior%20simulation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06693v2&entry.124074799=Read"},
{"title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models", "author": "Zunnan Xu and Yukang Lin and Haonan Han and Sicheng Yang and Ronghui Li and Yachao Zhang and Xiu Li", "abstract": "  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models.\n", "link": "http://arxiv.org/abs/2403.09471v5", "date": "2025-01-14", "relevancy": 2.2418, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5874}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5424}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&body=Title%3A%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models%0AAuthor%3A%20Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09471v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaTalk%253A%2520Efficient%2520Holistic%2520Gesture%2520Synthesis%2520with%2520Selective%2520State%250A%2520%2520Space%2520Models%26entry.906535625%3DZunnan%2520Xu%2520and%2520Yukang%2520Lin%2520and%2520Haonan%2520Han%2520and%2520Sicheng%2520Yang%2520and%2520Ronghui%2520Li%2520and%2520Yachao%2520Zhang%2520and%2520Xiu%2520Li%26entry.1292438233%3D%2520%2520Gesture%2520synthesis%2520is%2520a%2520vital%2520realm%2520of%2520human-computer%2520interaction%252C%2520with%250Awide-ranging%2520applications%2520across%2520various%2520fields%2520like%2520film%252C%2520robotics%252C%2520and%250Avirtual%2520reality.%2520Recent%2520advancements%2520have%2520utilized%2520the%2520diffusion%2520model%2520and%250Aattention%2520mechanisms%2520to%2520improve%2520gesture%2520synthesis.%2520However%252C%2520due%2520to%2520the%2520high%250Acomputational%2520complexity%2520of%2520these%2520techniques%252C%2520generating%2520long%2520and%2520diverse%250Asequences%2520with%2520low%2520latency%2520remains%2520a%2520challenge.%2520We%2520explore%2520the%2520potential%2520of%250Astate%2520space%2520models%2520%2528SSMs%2529%2520to%2520address%2520the%2520challenge%252C%2520implementing%2520a%2520two-stage%250Amodeling%2520strategy%2520with%2520discrete%2520motion%2520priors%2520to%2520enhance%2520the%2520quality%2520of%250Agestures.%2520Leveraging%2520the%2520foundational%2520Mamba%2520block%252C%2520we%2520introduce%2520MambaTalk%252C%250Aenhancing%2520gesture%2520diversity%2520and%2520rhythm%2520through%2520multimodal%2520integration.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520matches%2520or%2520exceeds%2520the%250Aperformance%2520of%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09471v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&entry.906535625=Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li&entry.1292438233=%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09471v5&entry.124074799=Read"},
{"title": "Bootstrapping Corner Cases: High-Resolution Inpainting for Safety\n  Critical Detect and Avoid for Automated Flying", "author": "Jonathan Lyhs and Lars Hinneburg and Michael Fischer and Florian \u00d6lsner and Stefan Milz and Jeremy Tschirner and Patrick M\u00e4der", "abstract": "  Modern machine learning techniques have shown tremendous potential,\nespecially for object detection on camera images. For this reason, they are\nalso used to enable safety-critical automated processes such as autonomous\ndrone flights. We present a study on object detection for Detect and Avoid, a\nsafety critical function for drones that detects air traffic during automated\nflights for safety reasons. An ill-posed problem is the generation of good and\nespecially large data sets, since detection itself is the corner case. Most\nmodels suffer from limited ground truth in raw data, \\eg recorded air traffic\nor frontal flight with a small aircraft. It often leads to poor and critical\ndetection rates. We overcome this problem by using inpainting methods to\nbootstrap the dataset such that it explicitly contains the corner cases of the\nraw data. We provide an overview of inpainting methods and generative models\nand present an example pipeline given a small annotated dataset. We validate\nour method by generating a high-resolution dataset, which we make publicly\navailable and present it to an independent object detector that was fully\ntrained on real data.\n", "link": "http://arxiv.org/abs/2501.08142v1", "date": "2025-01-14", "relevancy": 2.2406, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5639}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bootstrapping%20Corner%20Cases%3A%20High-Resolution%20Inpainting%20for%20Safety%0A%20%20Critical%20Detect%20and%20Avoid%20for%20Automated%20Flying&body=Title%3A%20Bootstrapping%20Corner%20Cases%3A%20High-Resolution%20Inpainting%20for%20Safety%0A%20%20Critical%20Detect%20and%20Avoid%20for%20Automated%20Flying%0AAuthor%3A%20Jonathan%20Lyhs%20and%20Lars%20Hinneburg%20and%20Michael%20Fischer%20and%20Florian%20%C3%96lsner%20and%20Stefan%20Milz%20and%20Jeremy%20Tschirner%20and%20Patrick%20M%C3%A4der%0AAbstract%3A%20%20%20Modern%20machine%20learning%20techniques%20have%20shown%20tremendous%20potential%2C%0Aespecially%20for%20object%20detection%20on%20camera%20images.%20For%20this%20reason%2C%20they%20are%0Aalso%20used%20to%20enable%20safety-critical%20automated%20processes%20such%20as%20autonomous%0Adrone%20flights.%20We%20present%20a%20study%20on%20object%20detection%20for%20Detect%20and%20Avoid%2C%20a%0Asafety%20critical%20function%20for%20drones%20that%20detects%20air%20traffic%20during%20automated%0Aflights%20for%20safety%20reasons.%20An%20ill-posed%20problem%20is%20the%20generation%20of%20good%20and%0Aespecially%20large%20data%20sets%2C%20since%20detection%20itself%20is%20the%20corner%20case.%20Most%0Amodels%20suffer%20from%20limited%20ground%20truth%20in%20raw%20data%2C%20%5Ceg%20recorded%20air%20traffic%0Aor%20frontal%20flight%20with%20a%20small%20aircraft.%20It%20often%20leads%20to%20poor%20and%20critical%0Adetection%20rates.%20We%20overcome%20this%20problem%20by%20using%20inpainting%20methods%20to%0Abootstrap%20the%20dataset%20such%20that%20it%20explicitly%20contains%20the%20corner%20cases%20of%20the%0Araw%20data.%20We%20provide%20an%20overview%20of%20inpainting%20methods%20and%20generative%20models%0Aand%20present%20an%20example%20pipeline%20given%20a%20small%20annotated%20dataset.%20We%20validate%0Aour%20method%20by%20generating%20a%20high-resolution%20dataset%2C%20which%20we%20make%20publicly%0Aavailable%20and%20present%20it%20to%20an%20independent%20object%20detector%20that%20was%20fully%0Atrained%20on%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBootstrapping%2520Corner%2520Cases%253A%2520High-Resolution%2520Inpainting%2520for%2520Safety%250A%2520%2520Critical%2520Detect%2520and%2520Avoid%2520for%2520Automated%2520Flying%26entry.906535625%3DJonathan%2520Lyhs%2520and%2520Lars%2520Hinneburg%2520and%2520Michael%2520Fischer%2520and%2520Florian%2520%25C3%2596lsner%2520and%2520Stefan%2520Milz%2520and%2520Jeremy%2520Tschirner%2520and%2520Patrick%2520M%25C3%25A4der%26entry.1292438233%3D%2520%2520Modern%2520machine%2520learning%2520techniques%2520have%2520shown%2520tremendous%2520potential%252C%250Aespecially%2520for%2520object%2520detection%2520on%2520camera%2520images.%2520For%2520this%2520reason%252C%2520they%2520are%250Aalso%2520used%2520to%2520enable%2520safety-critical%2520automated%2520processes%2520such%2520as%2520autonomous%250Adrone%2520flights.%2520We%2520present%2520a%2520study%2520on%2520object%2520detection%2520for%2520Detect%2520and%2520Avoid%252C%2520a%250Asafety%2520critical%2520function%2520for%2520drones%2520that%2520detects%2520air%2520traffic%2520during%2520automated%250Aflights%2520for%2520safety%2520reasons.%2520An%2520ill-posed%2520problem%2520is%2520the%2520generation%2520of%2520good%2520and%250Aespecially%2520large%2520data%2520sets%252C%2520since%2520detection%2520itself%2520is%2520the%2520corner%2520case.%2520Most%250Amodels%2520suffer%2520from%2520limited%2520ground%2520truth%2520in%2520raw%2520data%252C%2520%255Ceg%2520recorded%2520air%2520traffic%250Aor%2520frontal%2520flight%2520with%2520a%2520small%2520aircraft.%2520It%2520often%2520leads%2520to%2520poor%2520and%2520critical%250Adetection%2520rates.%2520We%2520overcome%2520this%2520problem%2520by%2520using%2520inpainting%2520methods%2520to%250Abootstrap%2520the%2520dataset%2520such%2520that%2520it%2520explicitly%2520contains%2520the%2520corner%2520cases%2520of%2520the%250Araw%2520data.%2520We%2520provide%2520an%2520overview%2520of%2520inpainting%2520methods%2520and%2520generative%2520models%250Aand%2520present%2520an%2520example%2520pipeline%2520given%2520a%2520small%2520annotated%2520dataset.%2520We%2520validate%250Aour%2520method%2520by%2520generating%2520a%2520high-resolution%2520dataset%252C%2520which%2520we%2520make%2520publicly%250Aavailable%2520and%2520present%2520it%2520to%2520an%2520independent%2520object%2520detector%2520that%2520was%2520fully%250Atrained%2520on%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrapping%20Corner%20Cases%3A%20High-Resolution%20Inpainting%20for%20Safety%0A%20%20Critical%20Detect%20and%20Avoid%20for%20Automated%20Flying&entry.906535625=Jonathan%20Lyhs%20and%20Lars%20Hinneburg%20and%20Michael%20Fischer%20and%20Florian%20%C3%96lsner%20and%20Stefan%20Milz%20and%20Jeremy%20Tschirner%20and%20Patrick%20M%C3%A4der&entry.1292438233=%20%20Modern%20machine%20learning%20techniques%20have%20shown%20tremendous%20potential%2C%0Aespecially%20for%20object%20detection%20on%20camera%20images.%20For%20this%20reason%2C%20they%20are%0Aalso%20used%20to%20enable%20safety-critical%20automated%20processes%20such%20as%20autonomous%0Adrone%20flights.%20We%20present%20a%20study%20on%20object%20detection%20for%20Detect%20and%20Avoid%2C%20a%0Asafety%20critical%20function%20for%20drones%20that%20detects%20air%20traffic%20during%20automated%0Aflights%20for%20safety%20reasons.%20An%20ill-posed%20problem%20is%20the%20generation%20of%20good%20and%0Aespecially%20large%20data%20sets%2C%20since%20detection%20itself%20is%20the%20corner%20case.%20Most%0Amodels%20suffer%20from%20limited%20ground%20truth%20in%20raw%20data%2C%20%5Ceg%20recorded%20air%20traffic%0Aor%20frontal%20flight%20with%20a%20small%20aircraft.%20It%20often%20leads%20to%20poor%20and%20critical%0Adetection%20rates.%20We%20overcome%20this%20problem%20by%20using%20inpainting%20methods%20to%0Abootstrap%20the%20dataset%20such%20that%20it%20explicitly%20contains%20the%20corner%20cases%20of%20the%0Araw%20data.%20We%20provide%20an%20overview%20of%20inpainting%20methods%20and%20generative%20models%0Aand%20present%20an%20example%20pipeline%20given%20a%20small%20annotated%20dataset.%20We%20validate%0Aour%20method%20by%20generating%20a%20high-resolution%20dataset%2C%20which%20we%20make%20publicly%0Aavailable%20and%20present%20it%20to%20an%20independent%20object%20detector%20that%20was%20fully%0Atrained%20on%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08142v1&entry.124074799=Read"},
{"title": "ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection", "author": "Delyan Boychev and Radostin Cholakov", "abstract": "  Recent generative models produce images with a level of authenticity that\nmakes them nearly indistinguishable from real photos and artwork. Potential\nharmful use cases of these models, necessitate the creation of robust synthetic\nimage detectors. However, current datasets in the field contain generated\nimages with questionable quality or have examples from one predominant content\ntype which leads to poor generalizability of the underlying detectors. We find\nthat the curation of a balanced amount of high-resolution generated images\nacross various content types is crucial for the generalizability of detectors,\nand introduce ImagiNet, a dataset of 200K examples, spanning four categories:\nphotos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are\nproduced with both open-source and proprietary generators, whereas real\ncounterparts for each content type are collected from public datasets. The\nstructure of ImagiNet allows for a two-track evaluation system: i)\nclassification as real or synthetic and ii) identification of the generative\nmodel. To establish a strong baseline, we train a ResNet-50 model using a\nself-supervised contrastive objective (SelfCon) for each track which achieves\nevaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,\neven under conditions that involve compression and resizing. The provided model\nis generalizable enough to achieve zero-shot state-of-the-art performance on\nprevious synthetic detection benchmarks. We provide ablations to demonstrate\nthe importance of content types and publish code and data.\n", "link": "http://arxiv.org/abs/2407.20020v3", "date": "2025-01-14", "relevancy": 2.2259, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5825}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5518}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImagiNet%3A%20A%20Multi-Content%20Benchmark%20for%20Synthetic%20Image%20Detection&body=Title%3A%20ImagiNet%3A%20A%20Multi-Content%20Benchmark%20for%20Synthetic%20Image%20Detection%0AAuthor%3A%20Delyan%20Boychev%20and%20Radostin%20Cholakov%0AAbstract%3A%20%20%20Recent%20generative%20models%20produce%20images%20with%20a%20level%20of%20authenticity%20that%0Amakes%20them%20nearly%20indistinguishable%20from%20real%20photos%20and%20artwork.%20Potential%0Aharmful%20use%20cases%20of%20these%20models%2C%20necessitate%20the%20creation%20of%20robust%20synthetic%0Aimage%20detectors.%20However%2C%20current%20datasets%20in%20the%20field%20contain%20generated%0Aimages%20with%20questionable%20quality%20or%20have%20examples%20from%20one%20predominant%20content%0Atype%20which%20leads%20to%20poor%20generalizability%20of%20the%20underlying%20detectors.%20We%20find%0Athat%20the%20curation%20of%20a%20balanced%20amount%20of%20high-resolution%20generated%20images%0Aacross%20various%20content%20types%20is%20crucial%20for%20the%20generalizability%20of%20detectors%2C%0Aand%20introduce%20ImagiNet%2C%20a%20dataset%20of%20200K%20examples%2C%20spanning%20four%20categories%3A%0Aphotos%2C%20paintings%2C%20faces%2C%20and%20miscellaneous.%20Synthetic%20images%20in%20ImagiNet%20are%0Aproduced%20with%20both%20open-source%20and%20proprietary%20generators%2C%20whereas%20real%0Acounterparts%20for%20each%20content%20type%20are%20collected%20from%20public%20datasets.%20The%0Astructure%20of%20ImagiNet%20allows%20for%20a%20two-track%20evaluation%20system%3A%20i%29%0Aclassification%20as%20real%20or%20synthetic%20and%20ii%29%20identification%20of%20the%20generative%0Amodel.%20To%20establish%20a%20strong%20baseline%2C%20we%20train%20a%20ResNet-50%20model%20using%20a%0Aself-supervised%20contrastive%20objective%20%28SelfCon%29%20for%20each%20track%20which%20achieves%0Aevaluation%20AUC%20of%20up%20to%200.99%20and%20balanced%20accuracy%20ranging%20from%2086%25%20to%2095%25%2C%0Aeven%20under%20conditions%20that%20involve%20compression%20and%20resizing.%20The%20provided%20model%0Ais%20generalizable%20enough%20to%20achieve%20zero-shot%20state-of-the-art%20performance%20on%0Aprevious%20synthetic%20detection%20benchmarks.%20We%20provide%20ablations%20to%20demonstrate%0Athe%20importance%20of%20content%20types%20and%20publish%20code%20and%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20020v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagiNet%253A%2520A%2520Multi-Content%2520Benchmark%2520for%2520Synthetic%2520Image%2520Detection%26entry.906535625%3DDelyan%2520Boychev%2520and%2520Radostin%2520Cholakov%26entry.1292438233%3D%2520%2520Recent%2520generative%2520models%2520produce%2520images%2520with%2520a%2520level%2520of%2520authenticity%2520that%250Amakes%2520them%2520nearly%2520indistinguishable%2520from%2520real%2520photos%2520and%2520artwork.%2520Potential%250Aharmful%2520use%2520cases%2520of%2520these%2520models%252C%2520necessitate%2520the%2520creation%2520of%2520robust%2520synthetic%250Aimage%2520detectors.%2520However%252C%2520current%2520datasets%2520in%2520the%2520field%2520contain%2520generated%250Aimages%2520with%2520questionable%2520quality%2520or%2520have%2520examples%2520from%2520one%2520predominant%2520content%250Atype%2520which%2520leads%2520to%2520poor%2520generalizability%2520of%2520the%2520underlying%2520detectors.%2520We%2520find%250Athat%2520the%2520curation%2520of%2520a%2520balanced%2520amount%2520of%2520high-resolution%2520generated%2520images%250Aacross%2520various%2520content%2520types%2520is%2520crucial%2520for%2520the%2520generalizability%2520of%2520detectors%252C%250Aand%2520introduce%2520ImagiNet%252C%2520a%2520dataset%2520of%2520200K%2520examples%252C%2520spanning%2520four%2520categories%253A%250Aphotos%252C%2520paintings%252C%2520faces%252C%2520and%2520miscellaneous.%2520Synthetic%2520images%2520in%2520ImagiNet%2520are%250Aproduced%2520with%2520both%2520open-source%2520and%2520proprietary%2520generators%252C%2520whereas%2520real%250Acounterparts%2520for%2520each%2520content%2520type%2520are%2520collected%2520from%2520public%2520datasets.%2520The%250Astructure%2520of%2520ImagiNet%2520allows%2520for%2520a%2520two-track%2520evaluation%2520system%253A%2520i%2529%250Aclassification%2520as%2520real%2520or%2520synthetic%2520and%2520ii%2529%2520identification%2520of%2520the%2520generative%250Amodel.%2520To%2520establish%2520a%2520strong%2520baseline%252C%2520we%2520train%2520a%2520ResNet-50%2520model%2520using%2520a%250Aself-supervised%2520contrastive%2520objective%2520%2528SelfCon%2529%2520for%2520each%2520track%2520which%2520achieves%250Aevaluation%2520AUC%2520of%2520up%2520to%25200.99%2520and%2520balanced%2520accuracy%2520ranging%2520from%252086%2525%2520to%252095%2525%252C%250Aeven%2520under%2520conditions%2520that%2520involve%2520compression%2520and%2520resizing.%2520The%2520provided%2520model%250Ais%2520generalizable%2520enough%2520to%2520achieve%2520zero-shot%2520state-of-the-art%2520performance%2520on%250Aprevious%2520synthetic%2520detection%2520benchmarks.%2520We%2520provide%2520ablations%2520to%2520demonstrate%250Athe%2520importance%2520of%2520content%2520types%2520and%2520publish%2520code%2520and%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20020v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImagiNet%3A%20A%20Multi-Content%20Benchmark%20for%20Synthetic%20Image%20Detection&entry.906535625=Delyan%20Boychev%20and%20Radostin%20Cholakov&entry.1292438233=%20%20Recent%20generative%20models%20produce%20images%20with%20a%20level%20of%20authenticity%20that%0Amakes%20them%20nearly%20indistinguishable%20from%20real%20photos%20and%20artwork.%20Potential%0Aharmful%20use%20cases%20of%20these%20models%2C%20necessitate%20the%20creation%20of%20robust%20synthetic%0Aimage%20detectors.%20However%2C%20current%20datasets%20in%20the%20field%20contain%20generated%0Aimages%20with%20questionable%20quality%20or%20have%20examples%20from%20one%20predominant%20content%0Atype%20which%20leads%20to%20poor%20generalizability%20of%20the%20underlying%20detectors.%20We%20find%0Athat%20the%20curation%20of%20a%20balanced%20amount%20of%20high-resolution%20generated%20images%0Aacross%20various%20content%20types%20is%20crucial%20for%20the%20generalizability%20of%20detectors%2C%0Aand%20introduce%20ImagiNet%2C%20a%20dataset%20of%20200K%20examples%2C%20spanning%20four%20categories%3A%0Aphotos%2C%20paintings%2C%20faces%2C%20and%20miscellaneous.%20Synthetic%20images%20in%20ImagiNet%20are%0Aproduced%20with%20both%20open-source%20and%20proprietary%20generators%2C%20whereas%20real%0Acounterparts%20for%20each%20content%20type%20are%20collected%20from%20public%20datasets.%20The%0Astructure%20of%20ImagiNet%20allows%20for%20a%20two-track%20evaluation%20system%3A%20i%29%0Aclassification%20as%20real%20or%20synthetic%20and%20ii%29%20identification%20of%20the%20generative%0Amodel.%20To%20establish%20a%20strong%20baseline%2C%20we%20train%20a%20ResNet-50%20model%20using%20a%0Aself-supervised%20contrastive%20objective%20%28SelfCon%29%20for%20each%20track%20which%20achieves%0Aevaluation%20AUC%20of%20up%20to%200.99%20and%20balanced%20accuracy%20ranging%20from%2086%25%20to%2095%25%2C%0Aeven%20under%20conditions%20that%20involve%20compression%20and%20resizing.%20The%20provided%20model%0Ais%20generalizable%20enough%20to%20achieve%20zero-shot%20state-of-the-art%20performance%20on%0Aprevious%20synthetic%20detection%20benchmarks.%20We%20provide%20ablations%20to%20demonstrate%0Athe%20importance%20of%20content%20types%20and%20publish%20code%20and%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20020v3&entry.124074799=Read"},
{"title": "Robust Low-Light Human Pose Estimation through Illumination-Texture\n  Modulation", "author": "Feng Zhang and Ze Li and Xiatian Zhu and Lei Chen", "abstract": "  As critical visual details become obscured, the low visibility and high ISO\nnoise in extremely low-light images pose a significant challenge to human pose\nestimation. Current methods fail to provide high-quality representations due to\nreliance on pixel-level enhancements that compromise semantics and the\ninability to effectively handle extreme low-light conditions for robust feature\nlearning. In this work, we propose a frequency-based framework for low-light\nhuman pose estimation, rooted in the \"divide-and-conquer\" principle. Instead of\nuniformly enhancing the entire image, our method focuses on task-relevant\ninformation. By applying dynamic illumination correction to the low-frequency\ncomponents and low-rank denoising to the high-frequency components, we\neffectively enhance both the semantic and texture information essential for\naccurate pose estimation. As a result, this targeted enhancement method results\nin robust, high-quality representations, significantly improving pose\nestimation performance. Extensive experiments demonstrating its superiority\nover state-of-the-art methods in various challenging low-light scenarios.\n", "link": "http://arxiv.org/abs/2501.08038v1", "date": "2025-01-14", "relevancy": 2.2193, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5619}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5546}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Low-Light%20Human%20Pose%20Estimation%20through%20Illumination-Texture%0A%20%20Modulation&body=Title%3A%20Robust%20Low-Light%20Human%20Pose%20Estimation%20through%20Illumination-Texture%0A%20%20Modulation%0AAuthor%3A%20Feng%20Zhang%20and%20Ze%20Li%20and%20Xiatian%20Zhu%20and%20Lei%20Chen%0AAbstract%3A%20%20%20As%20critical%20visual%20details%20become%20obscured%2C%20the%20low%20visibility%20and%20high%20ISO%0Anoise%20in%20extremely%20low-light%20images%20pose%20a%20significant%20challenge%20to%20human%20pose%0Aestimation.%20Current%20methods%20fail%20to%20provide%20high-quality%20representations%20due%20to%0Areliance%20on%20pixel-level%20enhancements%20that%20compromise%20semantics%20and%20the%0Ainability%20to%20effectively%20handle%20extreme%20low-light%20conditions%20for%20robust%20feature%0Alearning.%20In%20this%20work%2C%20we%20propose%20a%20frequency-based%20framework%20for%20low-light%0Ahuman%20pose%20estimation%2C%20rooted%20in%20the%20%22divide-and-conquer%22%20principle.%20Instead%20of%0Auniformly%20enhancing%20the%20entire%20image%2C%20our%20method%20focuses%20on%20task-relevant%0Ainformation.%20By%20applying%20dynamic%20illumination%20correction%20to%20the%20low-frequency%0Acomponents%20and%20low-rank%20denoising%20to%20the%20high-frequency%20components%2C%20we%0Aeffectively%20enhance%20both%20the%20semantic%20and%20texture%20information%20essential%20for%0Aaccurate%20pose%20estimation.%20As%20a%20result%2C%20this%20targeted%20enhancement%20method%20results%0Ain%20robust%2C%20high-quality%20representations%2C%20significantly%20improving%20pose%0Aestimation%20performance.%20Extensive%20experiments%20demonstrating%20its%20superiority%0Aover%20state-of-the-art%20methods%20in%20various%20challenging%20low-light%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Low-Light%2520Human%2520Pose%2520Estimation%2520through%2520Illumination-Texture%250A%2520%2520Modulation%26entry.906535625%3DFeng%2520Zhang%2520and%2520Ze%2520Li%2520and%2520Xiatian%2520Zhu%2520and%2520Lei%2520Chen%26entry.1292438233%3D%2520%2520As%2520critical%2520visual%2520details%2520become%2520obscured%252C%2520the%2520low%2520visibility%2520and%2520high%2520ISO%250Anoise%2520in%2520extremely%2520low-light%2520images%2520pose%2520a%2520significant%2520challenge%2520to%2520human%2520pose%250Aestimation.%2520Current%2520methods%2520fail%2520to%2520provide%2520high-quality%2520representations%2520due%2520to%250Areliance%2520on%2520pixel-level%2520enhancements%2520that%2520compromise%2520semantics%2520and%2520the%250Ainability%2520to%2520effectively%2520handle%2520extreme%2520low-light%2520conditions%2520for%2520robust%2520feature%250Alearning.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520frequency-based%2520framework%2520for%2520low-light%250Ahuman%2520pose%2520estimation%252C%2520rooted%2520in%2520the%2520%2522divide-and-conquer%2522%2520principle.%2520Instead%2520of%250Auniformly%2520enhancing%2520the%2520entire%2520image%252C%2520our%2520method%2520focuses%2520on%2520task-relevant%250Ainformation.%2520By%2520applying%2520dynamic%2520illumination%2520correction%2520to%2520the%2520low-frequency%250Acomponents%2520and%2520low-rank%2520denoising%2520to%2520the%2520high-frequency%2520components%252C%2520we%250Aeffectively%2520enhance%2520both%2520the%2520semantic%2520and%2520texture%2520information%2520essential%2520for%250Aaccurate%2520pose%2520estimation.%2520As%2520a%2520result%252C%2520this%2520targeted%2520enhancement%2520method%2520results%250Ain%2520robust%252C%2520high-quality%2520representations%252C%2520significantly%2520improving%2520pose%250Aestimation%2520performance.%2520Extensive%2520experiments%2520demonstrating%2520its%2520superiority%250Aover%2520state-of-the-art%2520methods%2520in%2520various%2520challenging%2520low-light%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Low-Light%20Human%20Pose%20Estimation%20through%20Illumination-Texture%0A%20%20Modulation&entry.906535625=Feng%20Zhang%20and%20Ze%20Li%20and%20Xiatian%20Zhu%20and%20Lei%20Chen&entry.1292438233=%20%20As%20critical%20visual%20details%20become%20obscured%2C%20the%20low%20visibility%20and%20high%20ISO%0Anoise%20in%20extremely%20low-light%20images%20pose%20a%20significant%20challenge%20to%20human%20pose%0Aestimation.%20Current%20methods%20fail%20to%20provide%20high-quality%20representations%20due%20to%0Areliance%20on%20pixel-level%20enhancements%20that%20compromise%20semantics%20and%20the%0Ainability%20to%20effectively%20handle%20extreme%20low-light%20conditions%20for%20robust%20feature%0Alearning.%20In%20this%20work%2C%20we%20propose%20a%20frequency-based%20framework%20for%20low-light%0Ahuman%20pose%20estimation%2C%20rooted%20in%20the%20%22divide-and-conquer%22%20principle.%20Instead%20of%0Auniformly%20enhancing%20the%20entire%20image%2C%20our%20method%20focuses%20on%20task-relevant%0Ainformation.%20By%20applying%20dynamic%20illumination%20correction%20to%20the%20low-frequency%0Acomponents%20and%20low-rank%20denoising%20to%20the%20high-frequency%20components%2C%20we%0Aeffectively%20enhance%20both%20the%20semantic%20and%20texture%20information%20essential%20for%0Aaccurate%20pose%20estimation.%20As%20a%20result%2C%20this%20targeted%20enhancement%20method%20results%0Ain%20robust%2C%20high-quality%20representations%2C%20significantly%20improving%20pose%0Aestimation%20performance.%20Extensive%20experiments%20demonstrating%20its%20superiority%0Aover%20state-of-the-art%20methods%20in%20various%20challenging%20low-light%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08038v1&entry.124074799=Read"},
{"title": "Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal\n  MRI Datasets", "author": "Linxuan Han and Sa Xiao and Zimeng Li and Haidong Li and Xiuchao Zhao and Yeqing Han and Fumin Guo and Xin Zhou", "abstract": "  Multi-modal magnetic resonance imaging (MRI) provides information of lesions\nfor computer-aided diagnosis from different views. Deep learning algorithms are\nsuitable for identifying specific anatomical structures, segmenting lesions,\nand classifying diseases. Manual labels are limited due to the high expense,\nwhich hinders further improvement of accuracy. Self-supervised learning,\nparticularly masked image modeling (MIM), has shown promise in utilizing\nunlabeled data. However, we spot model collapse when applying MIM to\nmulti-modal MRI datasets. The performance of downstream tasks does not see any\nimprovement following the collapsed model. To solve model collapse, we analyze\nand address it in two types: complete collapse and dimensional collapse. We\nfind complete collapse occurs because the collapsed loss value in multi-modal\nMRI datasets falls below the normally converged loss value. Based on this, the\nhybrid mask pattern (HMP) masking strategy is introduced to elevate the\ncollapsed loss above the normally converged loss value and avoid complete\ncollapse. Additionally, we reveal that dimensional collapse stems from\ninsufficient feature uniformity in MIM. We mitigate dimensional collapse by\nintroducing the pyramid barlow twins (PBT) module as an explicit regularization\nmethod. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module\nto avoid model collapse multi-modal MRI. Experiments are conducted on three\nmulti-modal MRI datasets to validate the effectiveness of our approach in\npreventing both types of model collapse. By preventing model collapse, the\ntraining of the model becomes more stable, resulting in a decent improvement in\nperformance for segmentation and classification tasks. The code is available at\nhttps://github.com/LinxuanHan/E-MIM.\n", "link": "http://arxiv.org/abs/2407.10377v3", "date": "2025-01-14", "relevancy": 2.2127, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Masked%20Image%20Modeling%20to%20Avoid%20Model%20Collapse%20on%20Multi-modal%0A%20%20MRI%20Datasets&body=Title%3A%20Enhanced%20Masked%20Image%20Modeling%20to%20Avoid%20Model%20Collapse%20on%20Multi-modal%0A%20%20MRI%20Datasets%0AAuthor%3A%20Linxuan%20Han%20and%20Sa%20Xiao%20and%20Zimeng%20Li%20and%20Haidong%20Li%20and%20Xiuchao%20Zhao%20and%20Yeqing%20Han%20and%20Fumin%20Guo%20and%20Xin%20Zhou%0AAbstract%3A%20%20%20Multi-modal%20magnetic%20resonance%20imaging%20%28MRI%29%20provides%20information%20of%20lesions%0Afor%20computer-aided%20diagnosis%20from%20different%20views.%20Deep%20learning%20algorithms%20are%0Asuitable%20for%20identifying%20specific%20anatomical%20structures%2C%20segmenting%20lesions%2C%0Aand%20classifying%20diseases.%20Manual%20labels%20are%20limited%20due%20to%20the%20high%20expense%2C%0Awhich%20hinders%20further%20improvement%20of%20accuracy.%20Self-supervised%20learning%2C%0Aparticularly%20masked%20image%20modeling%20%28MIM%29%2C%20has%20shown%20promise%20in%20utilizing%0Aunlabeled%20data.%20However%2C%20we%20spot%20model%20collapse%20when%20applying%20MIM%20to%0Amulti-modal%20MRI%20datasets.%20The%20performance%20of%20downstream%20tasks%20does%20not%20see%20any%0Aimprovement%20following%20the%20collapsed%20model.%20To%20solve%20model%20collapse%2C%20we%20analyze%0Aand%20address%20it%20in%20two%20types%3A%20complete%20collapse%20and%20dimensional%20collapse.%20We%0Afind%20complete%20collapse%20occurs%20because%20the%20collapsed%20loss%20value%20in%20multi-modal%0AMRI%20datasets%20falls%20below%20the%20normally%20converged%20loss%20value.%20Based%20on%20this%2C%20the%0Ahybrid%20mask%20pattern%20%28HMP%29%20masking%20strategy%20is%20introduced%20to%20elevate%20the%0Acollapsed%20loss%20above%20the%20normally%20converged%20loss%20value%20and%20avoid%20complete%0Acollapse.%20Additionally%2C%20we%20reveal%20that%20dimensional%20collapse%20stems%20from%0Ainsufficient%20feature%20uniformity%20in%20MIM.%20We%20mitigate%20dimensional%20collapse%20by%0Aintroducing%20the%20pyramid%20barlow%20twins%20%28PBT%29%20module%20as%20an%20explicit%20regularization%0Amethod.%20Overall%2C%20we%20construct%20the%20enhanced%20MIM%20%28E-MIM%29%20with%20HMP%20and%20PBT%20module%0Ato%20avoid%20model%20collapse%20multi-modal%20MRI.%20Experiments%20are%20conducted%20on%20three%0Amulti-modal%20MRI%20datasets%20to%20validate%20the%20effectiveness%20of%20our%20approach%20in%0Apreventing%20both%20types%20of%20model%20collapse.%20By%20preventing%20model%20collapse%2C%20the%0Atraining%20of%20the%20model%20becomes%20more%20stable%2C%20resulting%20in%20a%20decent%20improvement%20in%0Aperformance%20for%20segmentation%20and%20classification%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LinxuanHan/E-MIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Masked%2520Image%2520Modeling%2520to%2520Avoid%2520Model%2520Collapse%2520on%2520Multi-modal%250A%2520%2520MRI%2520Datasets%26entry.906535625%3DLinxuan%2520Han%2520and%2520Sa%2520Xiao%2520and%2520Zimeng%2520Li%2520and%2520Haidong%2520Li%2520and%2520Xiuchao%2520Zhao%2520and%2520Yeqing%2520Han%2520and%2520Fumin%2520Guo%2520and%2520Xin%2520Zhou%26entry.1292438233%3D%2520%2520Multi-modal%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520provides%2520information%2520of%2520lesions%250Afor%2520computer-aided%2520diagnosis%2520from%2520different%2520views.%2520Deep%2520learning%2520algorithms%2520are%250Asuitable%2520for%2520identifying%2520specific%2520anatomical%2520structures%252C%2520segmenting%2520lesions%252C%250Aand%2520classifying%2520diseases.%2520Manual%2520labels%2520are%2520limited%2520due%2520to%2520the%2520high%2520expense%252C%250Awhich%2520hinders%2520further%2520improvement%2520of%2520accuracy.%2520Self-supervised%2520learning%252C%250Aparticularly%2520masked%2520image%2520modeling%2520%2528MIM%2529%252C%2520has%2520shown%2520promise%2520in%2520utilizing%250Aunlabeled%2520data.%2520However%252C%2520we%2520spot%2520model%2520collapse%2520when%2520applying%2520MIM%2520to%250Amulti-modal%2520MRI%2520datasets.%2520The%2520performance%2520of%2520downstream%2520tasks%2520does%2520not%2520see%2520any%250Aimprovement%2520following%2520the%2520collapsed%2520model.%2520To%2520solve%2520model%2520collapse%252C%2520we%2520analyze%250Aand%2520address%2520it%2520in%2520two%2520types%253A%2520complete%2520collapse%2520and%2520dimensional%2520collapse.%2520We%250Afind%2520complete%2520collapse%2520occurs%2520because%2520the%2520collapsed%2520loss%2520value%2520in%2520multi-modal%250AMRI%2520datasets%2520falls%2520below%2520the%2520normally%2520converged%2520loss%2520value.%2520Based%2520on%2520this%252C%2520the%250Ahybrid%2520mask%2520pattern%2520%2528HMP%2529%2520masking%2520strategy%2520is%2520introduced%2520to%2520elevate%2520the%250Acollapsed%2520loss%2520above%2520the%2520normally%2520converged%2520loss%2520value%2520and%2520avoid%2520complete%250Acollapse.%2520Additionally%252C%2520we%2520reveal%2520that%2520dimensional%2520collapse%2520stems%2520from%250Ainsufficient%2520feature%2520uniformity%2520in%2520MIM.%2520We%2520mitigate%2520dimensional%2520collapse%2520by%250Aintroducing%2520the%2520pyramid%2520barlow%2520twins%2520%2528PBT%2529%2520module%2520as%2520an%2520explicit%2520regularization%250Amethod.%2520Overall%252C%2520we%2520construct%2520the%2520enhanced%2520MIM%2520%2528E-MIM%2529%2520with%2520HMP%2520and%2520PBT%2520module%250Ato%2520avoid%2520model%2520collapse%2520multi-modal%2520MRI.%2520Experiments%2520are%2520conducted%2520on%2520three%250Amulti-modal%2520MRI%2520datasets%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Apreventing%2520both%2520types%2520of%2520model%2520collapse.%2520By%2520preventing%2520model%2520collapse%252C%2520the%250Atraining%2520of%2520the%2520model%2520becomes%2520more%2520stable%252C%2520resulting%2520in%2520a%2520decent%2520improvement%2520in%250Aperformance%2520for%2520segmentation%2520and%2520classification%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LinxuanHan/E-MIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Masked%20Image%20Modeling%20to%20Avoid%20Model%20Collapse%20on%20Multi-modal%0A%20%20MRI%20Datasets&entry.906535625=Linxuan%20Han%20and%20Sa%20Xiao%20and%20Zimeng%20Li%20and%20Haidong%20Li%20and%20Xiuchao%20Zhao%20and%20Yeqing%20Han%20and%20Fumin%20Guo%20and%20Xin%20Zhou&entry.1292438233=%20%20Multi-modal%20magnetic%20resonance%20imaging%20%28MRI%29%20provides%20information%20of%20lesions%0Afor%20computer-aided%20diagnosis%20from%20different%20views.%20Deep%20learning%20algorithms%20are%0Asuitable%20for%20identifying%20specific%20anatomical%20structures%2C%20segmenting%20lesions%2C%0Aand%20classifying%20diseases.%20Manual%20labels%20are%20limited%20due%20to%20the%20high%20expense%2C%0Awhich%20hinders%20further%20improvement%20of%20accuracy.%20Self-supervised%20learning%2C%0Aparticularly%20masked%20image%20modeling%20%28MIM%29%2C%20has%20shown%20promise%20in%20utilizing%0Aunlabeled%20data.%20However%2C%20we%20spot%20model%20collapse%20when%20applying%20MIM%20to%0Amulti-modal%20MRI%20datasets.%20The%20performance%20of%20downstream%20tasks%20does%20not%20see%20any%0Aimprovement%20following%20the%20collapsed%20model.%20To%20solve%20model%20collapse%2C%20we%20analyze%0Aand%20address%20it%20in%20two%20types%3A%20complete%20collapse%20and%20dimensional%20collapse.%20We%0Afind%20complete%20collapse%20occurs%20because%20the%20collapsed%20loss%20value%20in%20multi-modal%0AMRI%20datasets%20falls%20below%20the%20normally%20converged%20loss%20value.%20Based%20on%20this%2C%20the%0Ahybrid%20mask%20pattern%20%28HMP%29%20masking%20strategy%20is%20introduced%20to%20elevate%20the%0Acollapsed%20loss%20above%20the%20normally%20converged%20loss%20value%20and%20avoid%20complete%0Acollapse.%20Additionally%2C%20we%20reveal%20that%20dimensional%20collapse%20stems%20from%0Ainsufficient%20feature%20uniformity%20in%20MIM.%20We%20mitigate%20dimensional%20collapse%20by%0Aintroducing%20the%20pyramid%20barlow%20twins%20%28PBT%29%20module%20as%20an%20explicit%20regularization%0Amethod.%20Overall%2C%20we%20construct%20the%20enhanced%20MIM%20%28E-MIM%29%20with%20HMP%20and%20PBT%20module%0Ato%20avoid%20model%20collapse%20multi-modal%20MRI.%20Experiments%20are%20conducted%20on%20three%0Amulti-modal%20MRI%20datasets%20to%20validate%20the%20effectiveness%20of%20our%20approach%20in%0Apreventing%20both%20types%20of%20model%20collapse.%20By%20preventing%20model%20collapse%2C%20the%0Atraining%20of%20the%20model%20becomes%20more%20stable%2C%20resulting%20in%20a%20decent%20improvement%20in%0Aperformance%20for%20segmentation%20and%20classification%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LinxuanHan/E-MIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10377v3&entry.124074799=Read"},
{"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models", "author": "Yifu Qiu and Varun Embar and Yizhe Zhang and Navdeep Jaitly and Shay B. Cohen and Benjamin Han", "abstract": "  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n", "link": "http://arxiv.org/abs/2501.08248v1", "date": "2025-01-14", "relevancy": 2.2037, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eliciting%20In-context%20Retrieval%20and%20Reasoning%20for%20Long-context%20Large%0A%20%20Language%20Models&body=Title%3A%20Eliciting%20In-context%20Retrieval%20and%20Reasoning%20for%20Long-context%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yifu%20Qiu%20and%20Varun%20Embar%20and%20Yizhe%20Zhang%20and%20Navdeep%20Jaitly%20and%20Shay%20B.%20Cohen%20and%20Benjamin%20Han%0AAbstract%3A%20%20%20Recent%20advancements%20in%20long-context%20language%20models%20%28LCLMs%29%20promise%20to%0Atransform%20Retrieval-Augmented%20Generation%20%28RAG%29%20by%20simplifying%20pipelines.%20With%0Atheir%20expanded%20context%20windows%2C%20LCLMs%20can%20process%20entire%20knowledge%20bases%20and%0Aperform%20retrieval%20and%20reasoning%20directly%20--%20a%20capability%20we%20define%20as%0AIn-Context%20Retrieval%20and%20Reasoning%20%28ICR%5E2%29.%20However%2C%20existing%20benchmarks%20like%0ALOFT%20often%20overestimate%20LCLM%20performance%20by%20providing%20overly%20simplified%0Acontexts.%20To%20address%20this%2C%20we%20introduce%20ICR%5E2%2C%20a%20benchmark%20that%20evaluates%20LCLMs%0Ain%20more%20realistic%20scenarios%20by%20including%20confounding%20passages%20retrieved%20with%0Astrong%20retrievers.%20We%20then%20propose%20three%20methods%20to%20enhance%20LCLM%20performance%3A%0A%281%29%20retrieve-then-generate%20fine-tuning%2C%20%282%29%20retrieval-attention-probing%2C%20which%0Auses%20attention%20heads%20to%20filter%20and%20de-noise%20long%20contexts%20during%20decoding%2C%20and%0A%283%29%20joint%20retrieval%20head%20training%20alongside%20the%20generation%20head.%20Our%20evaluation%0Aof%20five%20well-known%20LCLMs%20on%20LOFT%20and%20ICR%5E2%20demonstrates%20significant%20gains%20with%0Aour%20best%20approach%20applied%20to%20Mistral-7B%3A%20%2B17%20and%20%2B15%20points%20by%20Exact%20Match%20on%0ALOFT%2C%20and%20%2B13%20and%20%2B2%20points%20on%20ICR%5E2%2C%20compared%20to%20vanilla%20RAG%20and%20supervised%0Afine-tuning%2C%20respectively.%20It%20even%20outperforms%20GPT-4-Turbo%20on%20most%20tasks%0Adespite%20being%20a%20much%20smaller%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEliciting%2520In-context%2520Retrieval%2520and%2520Reasoning%2520for%2520Long-context%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYifu%2520Qiu%2520and%2520Varun%2520Embar%2520and%2520Yizhe%2520Zhang%2520and%2520Navdeep%2520Jaitly%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Benjamin%2520Han%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520long-context%2520language%2520models%2520%2528LCLMs%2529%2520promise%2520to%250Atransform%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520by%2520simplifying%2520pipelines.%2520With%250Atheir%2520expanded%2520context%2520windows%252C%2520LCLMs%2520can%2520process%2520entire%2520knowledge%2520bases%2520and%250Aperform%2520retrieval%2520and%2520reasoning%2520directly%2520--%2520a%2520capability%2520we%2520define%2520as%250AIn-Context%2520Retrieval%2520and%2520Reasoning%2520%2528ICR%255E2%2529.%2520However%252C%2520existing%2520benchmarks%2520like%250ALOFT%2520often%2520overestimate%2520LCLM%2520performance%2520by%2520providing%2520overly%2520simplified%250Acontexts.%2520To%2520address%2520this%252C%2520we%2520introduce%2520ICR%255E2%252C%2520a%2520benchmark%2520that%2520evaluates%2520LCLMs%250Ain%2520more%2520realistic%2520scenarios%2520by%2520including%2520confounding%2520passages%2520retrieved%2520with%250Astrong%2520retrievers.%2520We%2520then%2520propose%2520three%2520methods%2520to%2520enhance%2520LCLM%2520performance%253A%250A%25281%2529%2520retrieve-then-generate%2520fine-tuning%252C%2520%25282%2529%2520retrieval-attention-probing%252C%2520which%250Auses%2520attention%2520heads%2520to%2520filter%2520and%2520de-noise%2520long%2520contexts%2520during%2520decoding%252C%2520and%250A%25283%2529%2520joint%2520retrieval%2520head%2520training%2520alongside%2520the%2520generation%2520head.%2520Our%2520evaluation%250Aof%2520five%2520well-known%2520LCLMs%2520on%2520LOFT%2520and%2520ICR%255E2%2520demonstrates%2520significant%2520gains%2520with%250Aour%2520best%2520approach%2520applied%2520to%2520Mistral-7B%253A%2520%252B17%2520and%2520%252B15%2520points%2520by%2520Exact%2520Match%2520on%250ALOFT%252C%2520and%2520%252B13%2520and%2520%252B2%2520points%2520on%2520ICR%255E2%252C%2520compared%2520to%2520vanilla%2520RAG%2520and%2520supervised%250Afine-tuning%252C%2520respectively.%2520It%2520even%2520outperforms%2520GPT-4-Turbo%2520on%2520most%2520tasks%250Adespite%2520being%2520a%2520much%2520smaller%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eliciting%20In-context%20Retrieval%20and%20Reasoning%20for%20Long-context%20Large%0A%20%20Language%20Models&entry.906535625=Yifu%20Qiu%20and%20Varun%20Embar%20and%20Yizhe%20Zhang%20and%20Navdeep%20Jaitly%20and%20Shay%20B.%20Cohen%20and%20Benjamin%20Han&entry.1292438233=%20%20Recent%20advancements%20in%20long-context%20language%20models%20%28LCLMs%29%20promise%20to%0Atransform%20Retrieval-Augmented%20Generation%20%28RAG%29%20by%20simplifying%20pipelines.%20With%0Atheir%20expanded%20context%20windows%2C%20LCLMs%20can%20process%20entire%20knowledge%20bases%20and%0Aperform%20retrieval%20and%20reasoning%20directly%20--%20a%20capability%20we%20define%20as%0AIn-Context%20Retrieval%20and%20Reasoning%20%28ICR%5E2%29.%20However%2C%20existing%20benchmarks%20like%0ALOFT%20often%20overestimate%20LCLM%20performance%20by%20providing%20overly%20simplified%0Acontexts.%20To%20address%20this%2C%20we%20introduce%20ICR%5E2%2C%20a%20benchmark%20that%20evaluates%20LCLMs%0Ain%20more%20realistic%20scenarios%20by%20including%20confounding%20passages%20retrieved%20with%0Astrong%20retrievers.%20We%20then%20propose%20three%20methods%20to%20enhance%20LCLM%20performance%3A%0A%281%29%20retrieve-then-generate%20fine-tuning%2C%20%282%29%20retrieval-attention-probing%2C%20which%0Auses%20attention%20heads%20to%20filter%20and%20de-noise%20long%20contexts%20during%20decoding%2C%20and%0A%283%29%20joint%20retrieval%20head%20training%20alongside%20the%20generation%20head.%20Our%20evaluation%0Aof%20five%20well-known%20LCLMs%20on%20LOFT%20and%20ICR%5E2%20demonstrates%20significant%20gains%20with%0Aour%20best%20approach%20applied%20to%20Mistral-7B%3A%20%2B17%20and%20%2B15%20points%20by%20Exact%20Match%20on%0ALOFT%2C%20and%20%2B13%20and%20%2B2%20points%20on%20ICR%5E2%2C%20compared%20to%20vanilla%20RAG%20and%20supervised%0Afine-tuning%2C%20respectively.%20It%20even%20outperforms%20GPT-4-Turbo%20on%20most%20tasks%0Adespite%20being%20a%20much%20smaller%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08248v1&entry.124074799=Read"},
{"title": "Towards Federated Graph Learning in One-shot Communication", "author": "Guochen Yan and Xunkai Li and Luyuan Xie and Wentao Zhang and Qingni Shen and Yuejian Fang and Zhonghai Wu", "abstract": "  Federated Graph Learning (FGL) has emerged as a promising paradigm for\nbreaking data silos among distributed private graphs. In practical scenarios\ninvolving heterogeneous distributed graph data, personalized Federated Graph\nLearning (pFGL) aims to enhance model utility by training personalized models\ntailored to client needs. However, existing pFGL methods often require numerous\ncommunication rounds under heterogeneous graphs, leading to significant\ncommunication overhead and security concerns. While One-shot Federated Learning\n(OFL) enables collaboration in a single round, existing OFL methods are\ndesigned for image-centric tasks and ineffective for graph data, leaving a\ncritical gap in the field. Additionally, personalized models derived from\nexisting methods suffer from bias, failing to effectively generalize to the\nminority. To address these challenges, we propose the first $\\textbf{O}$ne-shot\n$\\textbf{p}$ersonalized $\\textbf{F}$ederated $\\textbf{G}$raph\n$\\textbf{L}$earning method ($\\textbf{O-pFGL}$) for node classification,\ncompatible with Secure Aggregation protocols for privacy preservation.\nSpecifically, for effective graph learning in one communication round, our\nmethod estimates and aggregates class-wise feature distribution statistics to\nconstruct a global pseudo-graph on the server, facilitating the training of a\nglobal graph model. To mitigate bias, we introduce a two-stage personalized\ntraining approach that adaptively balances local personal information and\nglobal insights from the pseudo-graph, improving both personalization and\ngeneralization. Extensive experiments on 12 multi-scale graph datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines across various settings.\n", "link": "http://arxiv.org/abs/2411.11304v4", "date": "2025-01-14", "relevancy": 2.1955, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4856}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4163}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Federated%20Graph%20Learning%20in%20One-shot%20Communication&body=Title%3A%20Towards%20Federated%20Graph%20Learning%20in%20One-shot%20Communication%0AAuthor%3A%20Guochen%20Yan%20and%20Xunkai%20Li%20and%20Luyuan%20Xie%20and%20Wentao%20Zhang%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu%0AAbstract%3A%20%20%20Federated%20Graph%20Learning%20%28FGL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Abreaking%20data%20silos%20among%20distributed%20private%20graphs.%20In%20practical%20scenarios%0Ainvolving%20heterogeneous%20distributed%20graph%20data%2C%20personalized%20Federated%20Graph%0ALearning%20%28pFGL%29%20aims%20to%20enhance%20model%20utility%20by%20training%20personalized%20models%0Atailored%20to%20client%20needs.%20However%2C%20existing%20pFGL%20methods%20often%20require%20numerous%0Acommunication%20rounds%20under%20heterogeneous%20graphs%2C%20leading%20to%20significant%0Acommunication%20overhead%20and%20security%20concerns.%20While%20One-shot%20Federated%20Learning%0A%28OFL%29%20enables%20collaboration%20in%20a%20single%20round%2C%20existing%20OFL%20methods%20are%0Adesigned%20for%20image-centric%20tasks%20and%20ineffective%20for%20graph%20data%2C%20leaving%20a%0Acritical%20gap%20in%20the%20field.%20Additionally%2C%20personalized%20models%20derived%20from%0Aexisting%20methods%20suffer%20from%20bias%2C%20failing%20to%20effectively%20generalize%20to%20the%0Aminority.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20%24%5Ctextbf%7BO%7D%24ne-shot%0A%24%5Ctextbf%7Bp%7D%24ersonalized%20%24%5Ctextbf%7BF%7D%24ederated%20%24%5Ctextbf%7BG%7D%24raph%0A%24%5Ctextbf%7BL%7D%24earning%20method%20%28%24%5Ctextbf%7BO-pFGL%7D%24%29%20for%20node%20classification%2C%0Acompatible%20with%20Secure%20Aggregation%20protocols%20for%20privacy%20preservation.%0ASpecifically%2C%20for%20effective%20graph%20learning%20in%20one%20communication%20round%2C%20our%0Amethod%20estimates%20and%20aggregates%20class-wise%20feature%20distribution%20statistics%20to%0Aconstruct%20a%20global%20pseudo-graph%20on%20the%20server%2C%20facilitating%20the%20training%20of%20a%0Aglobal%20graph%20model.%20To%20mitigate%20bias%2C%20we%20introduce%20a%20two-stage%20personalized%0Atraining%20approach%20that%20adaptively%20balances%20local%20personal%20information%20and%0Aglobal%20insights%20from%20the%20pseudo-graph%2C%20improving%20both%20personalization%20and%0Ageneralization.%20Extensive%20experiments%20on%2012%20multi-scale%20graph%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Abaselines%20across%20various%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11304v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Federated%2520Graph%2520Learning%2520in%2520One-shot%2520Communication%26entry.906535625%3DGuochen%2520Yan%2520and%2520Xunkai%2520Li%2520and%2520Luyuan%2520Xie%2520and%2520Wentao%2520Zhang%2520and%2520Qingni%2520Shen%2520and%2520Yuejian%2520Fang%2520and%2520Zhonghai%2520Wu%26entry.1292438233%3D%2520%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Abreaking%2520data%2520silos%2520among%2520distributed%2520private%2520graphs.%2520In%2520practical%2520scenarios%250Ainvolving%2520heterogeneous%2520distributed%2520graph%2520data%252C%2520personalized%2520Federated%2520Graph%250ALearning%2520%2528pFGL%2529%2520aims%2520to%2520enhance%2520model%2520utility%2520by%2520training%2520personalized%2520models%250Atailored%2520to%2520client%2520needs.%2520However%252C%2520existing%2520pFGL%2520methods%2520often%2520require%2520numerous%250Acommunication%2520rounds%2520under%2520heterogeneous%2520graphs%252C%2520leading%2520to%2520significant%250Acommunication%2520overhead%2520and%2520security%2520concerns.%2520While%2520One-shot%2520Federated%2520Learning%250A%2528OFL%2529%2520enables%2520collaboration%2520in%2520a%2520single%2520round%252C%2520existing%2520OFL%2520methods%2520are%250Adesigned%2520for%2520image-centric%2520tasks%2520and%2520ineffective%2520for%2520graph%2520data%252C%2520leaving%2520a%250Acritical%2520gap%2520in%2520the%2520field.%2520Additionally%252C%2520personalized%2520models%2520derived%2520from%250Aexisting%2520methods%2520suffer%2520from%2520bias%252C%2520failing%2520to%2520effectively%2520generalize%2520to%2520the%250Aminority.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520first%2520%2524%255Ctextbf%257BO%257D%2524ne-shot%250A%2524%255Ctextbf%257Bp%257D%2524ersonalized%2520%2524%255Ctextbf%257BF%257D%2524ederated%2520%2524%255Ctextbf%257BG%257D%2524raph%250A%2524%255Ctextbf%257BL%257D%2524earning%2520method%2520%2528%2524%255Ctextbf%257BO-pFGL%257D%2524%2529%2520for%2520node%2520classification%252C%250Acompatible%2520with%2520Secure%2520Aggregation%2520protocols%2520for%2520privacy%2520preservation.%250ASpecifically%252C%2520for%2520effective%2520graph%2520learning%2520in%2520one%2520communication%2520round%252C%2520our%250Amethod%2520estimates%2520and%2520aggregates%2520class-wise%2520feature%2520distribution%2520statistics%2520to%250Aconstruct%2520a%2520global%2520pseudo-graph%2520on%2520the%2520server%252C%2520facilitating%2520the%2520training%2520of%2520a%250Aglobal%2520graph%2520model.%2520To%2520mitigate%2520bias%252C%2520we%2520introduce%2520a%2520two-stage%2520personalized%250Atraining%2520approach%2520that%2520adaptively%2520balances%2520local%2520personal%2520information%2520and%250Aglobal%2520insights%2520from%2520the%2520pseudo-graph%252C%2520improving%2520both%2520personalization%2520and%250Ageneralization.%2520Extensive%2520experiments%2520on%252012%2520multi-scale%2520graph%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Abaselines%2520across%2520various%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11304v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Federated%20Graph%20Learning%20in%20One-shot%20Communication&entry.906535625=Guochen%20Yan%20and%20Xunkai%20Li%20and%20Luyuan%20Xie%20and%20Wentao%20Zhang%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu&entry.1292438233=%20%20Federated%20Graph%20Learning%20%28FGL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Abreaking%20data%20silos%20among%20distributed%20private%20graphs.%20In%20practical%20scenarios%0Ainvolving%20heterogeneous%20distributed%20graph%20data%2C%20personalized%20Federated%20Graph%0ALearning%20%28pFGL%29%20aims%20to%20enhance%20model%20utility%20by%20training%20personalized%20models%0Atailored%20to%20client%20needs.%20However%2C%20existing%20pFGL%20methods%20often%20require%20numerous%0Acommunication%20rounds%20under%20heterogeneous%20graphs%2C%20leading%20to%20significant%0Acommunication%20overhead%20and%20security%20concerns.%20While%20One-shot%20Federated%20Learning%0A%28OFL%29%20enables%20collaboration%20in%20a%20single%20round%2C%20existing%20OFL%20methods%20are%0Adesigned%20for%20image-centric%20tasks%20and%20ineffective%20for%20graph%20data%2C%20leaving%20a%0Acritical%20gap%20in%20the%20field.%20Additionally%2C%20personalized%20models%20derived%20from%0Aexisting%20methods%20suffer%20from%20bias%2C%20failing%20to%20effectively%20generalize%20to%20the%0Aminority.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20%24%5Ctextbf%7BO%7D%24ne-shot%0A%24%5Ctextbf%7Bp%7D%24ersonalized%20%24%5Ctextbf%7BF%7D%24ederated%20%24%5Ctextbf%7BG%7D%24raph%0A%24%5Ctextbf%7BL%7D%24earning%20method%20%28%24%5Ctextbf%7BO-pFGL%7D%24%29%20for%20node%20classification%2C%0Acompatible%20with%20Secure%20Aggregation%20protocols%20for%20privacy%20preservation.%0ASpecifically%2C%20for%20effective%20graph%20learning%20in%20one%20communication%20round%2C%20our%0Amethod%20estimates%20and%20aggregates%20class-wise%20feature%20distribution%20statistics%20to%0Aconstruct%20a%20global%20pseudo-graph%20on%20the%20server%2C%20facilitating%20the%20training%20of%20a%0Aglobal%20graph%20model.%20To%20mitigate%20bias%2C%20we%20introduce%20a%20two-stage%20personalized%0Atraining%20approach%20that%20adaptively%20balances%20local%20personal%20information%20and%0Aglobal%20insights%20from%20the%20pseudo-graph%2C%20improving%20both%20personalization%20and%0Ageneralization.%20Extensive%20experiments%20on%2012%20multi-scale%20graph%20datasets%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Abaselines%20across%20various%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11304v4&entry.124074799=Read"},
{"title": "Exploring visual language models as a powerful tool in the diagnosis of\n  Ewing Sarcoma", "author": "Alvaro Pastor-Naranjo and Pablo Meseguer and Roc\u00edo del Amor and Jose Antonio Lopez-Guerrero and Samuel Navarro and Katia Scotlandi and Antonio Llombart-Bosch and Isidro Machado and Valery Naranjo", "abstract": "  Ewing's sarcoma (ES), characterized by a high density of small round blue\ncells without structural organization, presents a significant health concern,\nparticularly among adolescents aged 10 to 19. Artificial intelligence-based\nsystems for automated analysis of histopathological images are promising to\ncontribute to an accurate diagnosis of ES. In this context, this study explores\nthe feature extraction ability of different pre-training strategies for\ndistinguishing ES from other soft tissue or bone sarcomas with similar\nmorphology in digitized tissue microarrays for the first time, as far as we\nknow. Vision-language supervision (VLS) is compared to fully-supervised\nImageNet pre-training within a multiple instance learning paradigm. Our\nfindings indicate a substantial improvement in diagnostic accuracy with the\nadaption of VLS using an in-domain dataset. Notably, these models not only\nenhance the accuracy of predicted classes but also drastically reduce the\nnumber of trainable parameters and computational costs.\n", "link": "http://arxiv.org/abs/2501.08042v1", "date": "2025-01-14", "relevancy": 2.1912, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20visual%20language%20models%20as%20a%20powerful%20tool%20in%20the%20diagnosis%20of%0A%20%20Ewing%20Sarcoma&body=Title%3A%20Exploring%20visual%20language%20models%20as%20a%20powerful%20tool%20in%20the%20diagnosis%20of%0A%20%20Ewing%20Sarcoma%0AAuthor%3A%20Alvaro%20Pastor-Naranjo%20and%20Pablo%20Meseguer%20and%20Roc%C3%ADo%20del%20Amor%20and%20Jose%20Antonio%20Lopez-Guerrero%20and%20Samuel%20Navarro%20and%20Katia%20Scotlandi%20and%20Antonio%20Llombart-Bosch%20and%20Isidro%20Machado%20and%20Valery%20Naranjo%0AAbstract%3A%20%20%20Ewing%27s%20sarcoma%20%28ES%29%2C%20characterized%20by%20a%20high%20density%20of%20small%20round%20blue%0Acells%20without%20structural%20organization%2C%20presents%20a%20significant%20health%20concern%2C%0Aparticularly%20among%20adolescents%20aged%2010%20to%2019.%20Artificial%20intelligence-based%0Asystems%20for%20automated%20analysis%20of%20histopathological%20images%20are%20promising%20to%0Acontribute%20to%20an%20accurate%20diagnosis%20of%20ES.%20In%20this%20context%2C%20this%20study%20explores%0Athe%20feature%20extraction%20ability%20of%20different%20pre-training%20strategies%20for%0Adistinguishing%20ES%20from%20other%20soft%20tissue%20or%20bone%20sarcomas%20with%20similar%0Amorphology%20in%20digitized%20tissue%20microarrays%20for%20the%20first%20time%2C%20as%20far%20as%20we%0Aknow.%20Vision-language%20supervision%20%28VLS%29%20is%20compared%20to%20fully-supervised%0AImageNet%20pre-training%20within%20a%20multiple%20instance%20learning%20paradigm.%20Our%0Afindings%20indicate%20a%20substantial%20improvement%20in%20diagnostic%20accuracy%20with%20the%0Aadaption%20of%20VLS%20using%20an%20in-domain%20dataset.%20Notably%2C%20these%20models%20not%20only%0Aenhance%20the%20accuracy%20of%20predicted%20classes%20but%20also%20drastically%20reduce%20the%0Anumber%20of%20trainable%20parameters%20and%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520visual%2520language%2520models%2520as%2520a%2520powerful%2520tool%2520in%2520the%2520diagnosis%2520of%250A%2520%2520Ewing%2520Sarcoma%26entry.906535625%3DAlvaro%2520Pastor-Naranjo%2520and%2520Pablo%2520Meseguer%2520and%2520Roc%25C3%25ADo%2520del%2520Amor%2520and%2520Jose%2520Antonio%2520Lopez-Guerrero%2520and%2520Samuel%2520Navarro%2520and%2520Katia%2520Scotlandi%2520and%2520Antonio%2520Llombart-Bosch%2520and%2520Isidro%2520Machado%2520and%2520Valery%2520Naranjo%26entry.1292438233%3D%2520%2520Ewing%2527s%2520sarcoma%2520%2528ES%2529%252C%2520characterized%2520by%2520a%2520high%2520density%2520of%2520small%2520round%2520blue%250Acells%2520without%2520structural%2520organization%252C%2520presents%2520a%2520significant%2520health%2520concern%252C%250Aparticularly%2520among%2520adolescents%2520aged%252010%2520to%252019.%2520Artificial%2520intelligence-based%250Asystems%2520for%2520automated%2520analysis%2520of%2520histopathological%2520images%2520are%2520promising%2520to%250Acontribute%2520to%2520an%2520accurate%2520diagnosis%2520of%2520ES.%2520In%2520this%2520context%252C%2520this%2520study%2520explores%250Athe%2520feature%2520extraction%2520ability%2520of%2520different%2520pre-training%2520strategies%2520for%250Adistinguishing%2520ES%2520from%2520other%2520soft%2520tissue%2520or%2520bone%2520sarcomas%2520with%2520similar%250Amorphology%2520in%2520digitized%2520tissue%2520microarrays%2520for%2520the%2520first%2520time%252C%2520as%2520far%2520as%2520we%250Aknow.%2520Vision-language%2520supervision%2520%2528VLS%2529%2520is%2520compared%2520to%2520fully-supervised%250AImageNet%2520pre-training%2520within%2520a%2520multiple%2520instance%2520learning%2520paradigm.%2520Our%250Afindings%2520indicate%2520a%2520substantial%2520improvement%2520in%2520diagnostic%2520accuracy%2520with%2520the%250Aadaption%2520of%2520VLS%2520using%2520an%2520in-domain%2520dataset.%2520Notably%252C%2520these%2520models%2520not%2520only%250Aenhance%2520the%2520accuracy%2520of%2520predicted%2520classes%2520but%2520also%2520drastically%2520reduce%2520the%250Anumber%2520of%2520trainable%2520parameters%2520and%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20visual%20language%20models%20as%20a%20powerful%20tool%20in%20the%20diagnosis%20of%0A%20%20Ewing%20Sarcoma&entry.906535625=Alvaro%20Pastor-Naranjo%20and%20Pablo%20Meseguer%20and%20Roc%C3%ADo%20del%20Amor%20and%20Jose%20Antonio%20Lopez-Guerrero%20and%20Samuel%20Navarro%20and%20Katia%20Scotlandi%20and%20Antonio%20Llombart-Bosch%20and%20Isidro%20Machado%20and%20Valery%20Naranjo&entry.1292438233=%20%20Ewing%27s%20sarcoma%20%28ES%29%2C%20characterized%20by%20a%20high%20density%20of%20small%20round%20blue%0Acells%20without%20structural%20organization%2C%20presents%20a%20significant%20health%20concern%2C%0Aparticularly%20among%20adolescents%20aged%2010%20to%2019.%20Artificial%20intelligence-based%0Asystems%20for%20automated%20analysis%20of%20histopathological%20images%20are%20promising%20to%0Acontribute%20to%20an%20accurate%20diagnosis%20of%20ES.%20In%20this%20context%2C%20this%20study%20explores%0Athe%20feature%20extraction%20ability%20of%20different%20pre-training%20strategies%20for%0Adistinguishing%20ES%20from%20other%20soft%20tissue%20or%20bone%20sarcomas%20with%20similar%0Amorphology%20in%20digitized%20tissue%20microarrays%20for%20the%20first%20time%2C%20as%20far%20as%20we%0Aknow.%20Vision-language%20supervision%20%28VLS%29%20is%20compared%20to%20fully-supervised%0AImageNet%20pre-training%20within%20a%20multiple%20instance%20learning%20paradigm.%20Our%0Afindings%20indicate%20a%20substantial%20improvement%20in%20diagnostic%20accuracy%20with%20the%0Aadaption%20of%20VLS%20using%20an%20in-domain%20dataset.%20Notably%2C%20these%20models%20not%20only%0Aenhance%20the%20accuracy%20of%20predicted%20classes%20but%20also%20drastically%20reduce%20the%0Anumber%20of%20trainable%20parameters%20and%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08042v1&entry.124074799=Read"},
{"title": "Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition", "author": "Zixuan Wang and Chi-Keung Tang and Yu-Wing Tai", "abstract": "  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.\n", "link": "http://arxiv.org/abs/2410.03335v2", "date": "2025-01-14", "relevancy": 2.1899, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5677}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5477}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition&body=Title%3A%20Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition%0AAuthor%3A%20Zixuan%20Wang%20and%20Chi-Keung%20Tang%20and%20Yu-Wing%20Tai%0AAbstract%3A%20%20%20We%20introduce%20Audio-Agent%2C%20a%20multimodal%20framework%20for%20audio%20generation%2C%0Aediting%20and%20composition%20based%20on%20text%20or%20video%20inputs.%20Conventional%20approaches%0Afor%20text-to-audio%20%28TTA%29%20tasks%20often%20make%20single-pass%20inferences%20from%20text%0Adescriptions.%20While%20straightforward%2C%20this%20design%20struggles%20to%20produce%0Ahigh-quality%20audio%20when%20given%20complex%20text%20conditions.%20In%20our%20method%2C%20we%0Autilize%20a%20pre-trained%20TTA%20diffusion%20network%20as%20the%20audio%20generation%20agent%20to%0Awork%20in%20tandem%20with%20GPT-4%2C%20which%20decomposes%20the%20text%20condition%20into%20atomic%2C%0Aspecific%20instructions%20and%20calls%20the%20agent%20for%20audio%20generation.%20In%20doing%20so%2C%0AAudio-Agent%20can%20generate%20high-quality%20audio%20that%20is%20closely%20aligned%20with%20the%0Aprovided%20text%20or%20video%20exhibiting%20complex%20and%20multiple%20events%2C%20while%20supporting%0Avariable-length%20and%20variable-volume%20generation.%20For%20video-to-audio%20%28VTA%29%20tasks%2C%0Amost%20existing%20methods%20require%20training%20a%20timestamp%20detector%20to%20synchronize%0Avideo%20events%20with%20the%20generated%20audio%2C%20a%20process%20that%20can%20be%20tedious%20and%0Atime-consuming.%20Instead%2C%20we%20propose%20a%20simpler%20approach%20by%20fine-tuning%20a%0Apre-trained%20Large%20Language%20Model%20%28LLM%29%2C%20e.g.%2C%20Gemma2-2B-it%2C%20to%20obtain%20both%0Asemantic%20and%20temporal%20conditions%20that%20bridge%20the%20video%20and%20audio%20modality.%0AConsequently%2C%20our%20framework%20contributes%20a%20comprehensive%20solution%20for%20both%20TTA%0Aand%20VTA%20tasks%20without%20substantial%20computational%20overhead%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Agent%253A%2520Leveraging%2520LLMs%2520For%2520Audio%2520Generation%252C%2520Editing%2520and%250A%2520%2520Composition%26entry.906535625%3DZixuan%2520Wang%2520and%2520Chi-Keung%2520Tang%2520and%2520Yu-Wing%2520Tai%26entry.1292438233%3D%2520%2520We%2520introduce%2520Audio-Agent%252C%2520a%2520multimodal%2520framework%2520for%2520audio%2520generation%252C%250Aediting%2520and%2520composition%2520based%2520on%2520text%2520or%2520video%2520inputs.%2520Conventional%2520approaches%250Afor%2520text-to-audio%2520%2528TTA%2529%2520tasks%2520often%2520make%2520single-pass%2520inferences%2520from%2520text%250Adescriptions.%2520While%2520straightforward%252C%2520this%2520design%2520struggles%2520to%2520produce%250Ahigh-quality%2520audio%2520when%2520given%2520complex%2520text%2520conditions.%2520In%2520our%2520method%252C%2520we%250Autilize%2520a%2520pre-trained%2520TTA%2520diffusion%2520network%2520as%2520the%2520audio%2520generation%2520agent%2520to%250Awork%2520in%2520tandem%2520with%2520GPT-4%252C%2520which%2520decomposes%2520the%2520text%2520condition%2520into%2520atomic%252C%250Aspecific%2520instructions%2520and%2520calls%2520the%2520agent%2520for%2520audio%2520generation.%2520In%2520doing%2520so%252C%250AAudio-Agent%2520can%2520generate%2520high-quality%2520audio%2520that%2520is%2520closely%2520aligned%2520with%2520the%250Aprovided%2520text%2520or%2520video%2520exhibiting%2520complex%2520and%2520multiple%2520events%252C%2520while%2520supporting%250Avariable-length%2520and%2520variable-volume%2520generation.%2520For%2520video-to-audio%2520%2528VTA%2529%2520tasks%252C%250Amost%2520existing%2520methods%2520require%2520training%2520a%2520timestamp%2520detector%2520to%2520synchronize%250Avideo%2520events%2520with%2520the%2520generated%2520audio%252C%2520a%2520process%2520that%2520can%2520be%2520tedious%2520and%250Atime-consuming.%2520Instead%252C%2520we%2520propose%2520a%2520simpler%2520approach%2520by%2520fine-tuning%2520a%250Apre-trained%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520e.g.%252C%2520Gemma2-2B-it%252C%2520to%2520obtain%2520both%250Asemantic%2520and%2520temporal%2520conditions%2520that%2520bridge%2520the%2520video%2520and%2520audio%2520modality.%250AConsequently%252C%2520our%2520framework%2520contributes%2520a%2520comprehensive%2520solution%2520for%2520both%2520TTA%250Aand%2520VTA%2520tasks%2520without%2520substantial%2520computational%2520overhead%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Agent%3A%20Leveraging%20LLMs%20For%20Audio%20Generation%2C%20Editing%20and%0A%20%20Composition&entry.906535625=Zixuan%20Wang%20and%20Chi-Keung%20Tang%20and%20Yu-Wing%20Tai&entry.1292438233=%20%20We%20introduce%20Audio-Agent%2C%20a%20multimodal%20framework%20for%20audio%20generation%2C%0Aediting%20and%20composition%20based%20on%20text%20or%20video%20inputs.%20Conventional%20approaches%0Afor%20text-to-audio%20%28TTA%29%20tasks%20often%20make%20single-pass%20inferences%20from%20text%0Adescriptions.%20While%20straightforward%2C%20this%20design%20struggles%20to%20produce%0Ahigh-quality%20audio%20when%20given%20complex%20text%20conditions.%20In%20our%20method%2C%20we%0Autilize%20a%20pre-trained%20TTA%20diffusion%20network%20as%20the%20audio%20generation%20agent%20to%0Awork%20in%20tandem%20with%20GPT-4%2C%20which%20decomposes%20the%20text%20condition%20into%20atomic%2C%0Aspecific%20instructions%20and%20calls%20the%20agent%20for%20audio%20generation.%20In%20doing%20so%2C%0AAudio-Agent%20can%20generate%20high-quality%20audio%20that%20is%20closely%20aligned%20with%20the%0Aprovided%20text%20or%20video%20exhibiting%20complex%20and%20multiple%20events%2C%20while%20supporting%0Avariable-length%20and%20variable-volume%20generation.%20For%20video-to-audio%20%28VTA%29%20tasks%2C%0Amost%20existing%20methods%20require%20training%20a%20timestamp%20detector%20to%20synchronize%0Avideo%20events%20with%20the%20generated%20audio%2C%20a%20process%20that%20can%20be%20tedious%20and%0Atime-consuming.%20Instead%2C%20we%20propose%20a%20simpler%20approach%20by%20fine-tuning%20a%0Apre-trained%20Large%20Language%20Model%20%28LLM%29%2C%20e.g.%2C%20Gemma2-2B-it%2C%20to%20obtain%20both%0Asemantic%20and%20temporal%20conditions%20that%20bridge%20the%20video%20and%20audio%20modality.%0AConsequently%2C%20our%20framework%20contributes%20a%20comprehensive%20solution%20for%20both%20TTA%0Aand%20VTA%20tasks%20without%20substantial%20computational%20overhead%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03335v2&entry.124074799=Read"},
{"title": "Feedback-driven object detection and iterative model improvement", "author": "S\u00f6nke Tenckhoff and Mario Koddenbrock and Erik Rodner", "abstract": "  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To\nsupport the understanding of our labeling process, we have created an\nexplanatory video demonstrating the methodology using microscopy images of E.\ncoli bacteria as an example. The video is available on YouTube\n(https://www.youtube.com/watch?v=CM9uhE8NN5E).\n", "link": "http://arxiv.org/abs/2411.19835v2", "date": "2025-01-14", "relevancy": 2.1897, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5464}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement&body=Title%3A%20Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement%0AAuthor%3A%20S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner%0AAbstract%3A%20%20%20Automated%20object%20detection%20has%20become%20increasingly%20valuable%20across%20diverse%0Aapplications%2C%20yet%20efficient%2C%20high-quality%20annotation%20remains%20a%20persistent%0Achallenge.%20In%20this%20paper%2C%20we%20present%20the%20development%20and%20evaluation%20of%20a%0Aplatform%20designed%20to%20interactively%20improve%20object%20detection%20models.%20The%0Aplatform%20allows%20uploading%20and%20annotating%20images%20as%20well%20as%20fine-tuning%20object%0Adetection%20models.%20Users%20can%20then%20manually%20review%20and%20refine%20annotations%2C%0Afurther%20creating%20improved%20snapshots%20that%20are%20used%20for%20automatic%20object%0Adetection%20on%20subsequent%20image%20uploads%20-%20a%20process%20we%20refer%20to%20as%20semi-automatic%0Aannotation%20resulting%20in%20a%20significant%20gain%20in%20annotation%20efficiency.%0A%20%20Whereas%20iterative%20refinement%20of%20model%20results%20to%20speed%20up%20annotation%20has%0Abecome%20common%20practice%2C%20we%20are%20the%20first%20to%20quantitatively%20evaluate%20its%0Abenefits%20with%20respect%20to%20time%2C%20effort%2C%20and%20interaction%20savings.%20Our%0Aexperimental%20results%20show%20clear%20evidence%20for%20a%20significant%20time%20reduction%20of%20up%0Ato%2053%25%20for%20semi-automatic%20compared%20to%20manual%20annotation.%20Importantly%2C%20these%0Aefficiency%20gains%20did%20not%20compromise%20annotation%20quality%2C%20while%20matching%20or%0Aoccasionally%20even%20exceeding%20the%20accuracy%20of%20manual%20annotations.%20These%20findings%0Ademonstrate%20the%20potential%20of%20our%20lightweight%20annotation%20platform%20for%20creating%0Ahigh-quality%20object%20detection%20datasets%20and%20provide%20best%20practices%20to%20guide%0Afuture%20development%20of%20annotation%20platforms.%0A%20%20The%20platform%20is%20open-source%2C%20with%20the%20frontend%20and%20backend%20repositories%0Aavailable%20on%20GitHub%20%28https%3A//github.com/ml-lab-htw/iterative-annotate%29.%20To%0Asupport%20the%20understanding%20of%20our%20labeling%20process%2C%20we%20have%20created%20an%0Aexplanatory%20video%20demonstrating%20the%20methodology%20using%20microscopy%20images%20of%20E.%0Acoli%20bacteria%20as%20an%20example.%20The%20video%20is%20available%20on%20YouTube%0A%28https%3A//www.youtube.com/watch%3Fv%3DCM9uhE8NN5E%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback-driven%2520object%2520detection%2520and%2520iterative%2520model%2520improvement%26entry.906535625%3DS%25C3%25B6nke%2520Tenckhoff%2520and%2520Mario%2520Koddenbrock%2520and%2520Erik%2520Rodner%26entry.1292438233%3D%2520%2520Automated%2520object%2520detection%2520has%2520become%2520increasingly%2520valuable%2520across%2520diverse%250Aapplications%252C%2520yet%2520efficient%252C%2520high-quality%2520annotation%2520remains%2520a%2520persistent%250Achallenge.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520development%2520and%2520evaluation%2520of%2520a%250Aplatform%2520designed%2520to%2520interactively%2520improve%2520object%2520detection%2520models.%2520The%250Aplatform%2520allows%2520uploading%2520and%2520annotating%2520images%2520as%2520well%2520as%2520fine-tuning%2520object%250Adetection%2520models.%2520Users%2520can%2520then%2520manually%2520review%2520and%2520refine%2520annotations%252C%250Afurther%2520creating%2520improved%2520snapshots%2520that%2520are%2520used%2520for%2520automatic%2520object%250Adetection%2520on%2520subsequent%2520image%2520uploads%2520-%2520a%2520process%2520we%2520refer%2520to%2520as%2520semi-automatic%250Aannotation%2520resulting%2520in%2520a%2520significant%2520gain%2520in%2520annotation%2520efficiency.%250A%2520%2520Whereas%2520iterative%2520refinement%2520of%2520model%2520results%2520to%2520speed%2520up%2520annotation%2520has%250Abecome%2520common%2520practice%252C%2520we%2520are%2520the%2520first%2520to%2520quantitatively%2520evaluate%2520its%250Abenefits%2520with%2520respect%2520to%2520time%252C%2520effort%252C%2520and%2520interaction%2520savings.%2520Our%250Aexperimental%2520results%2520show%2520clear%2520evidence%2520for%2520a%2520significant%2520time%2520reduction%2520of%2520up%250Ato%252053%2525%2520for%2520semi-automatic%2520compared%2520to%2520manual%2520annotation.%2520Importantly%252C%2520these%250Aefficiency%2520gains%2520did%2520not%2520compromise%2520annotation%2520quality%252C%2520while%2520matching%2520or%250Aoccasionally%2520even%2520exceeding%2520the%2520accuracy%2520of%2520manual%2520annotations.%2520These%2520findings%250Ademonstrate%2520the%2520potential%2520of%2520our%2520lightweight%2520annotation%2520platform%2520for%2520creating%250Ahigh-quality%2520object%2520detection%2520datasets%2520and%2520provide%2520best%2520practices%2520to%2520guide%250Afuture%2520development%2520of%2520annotation%2520platforms.%250A%2520%2520The%2520platform%2520is%2520open-source%252C%2520with%2520the%2520frontend%2520and%2520backend%2520repositories%250Aavailable%2520on%2520GitHub%2520%2528https%253A//github.com/ml-lab-htw/iterative-annotate%2529.%2520To%250Asupport%2520the%2520understanding%2520of%2520our%2520labeling%2520process%252C%2520we%2520have%2520created%2520an%250Aexplanatory%2520video%2520demonstrating%2520the%2520methodology%2520using%2520microscopy%2520images%2520of%2520E.%250Acoli%2520bacteria%2520as%2520an%2520example.%2520The%2520video%2520is%2520available%2520on%2520YouTube%250A%2528https%253A//www.youtube.com/watch%253Fv%253DCM9uhE8NN5E%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement&entry.906535625=S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner&entry.1292438233=%20%20Automated%20object%20detection%20has%20become%20increasingly%20valuable%20across%20diverse%0Aapplications%2C%20yet%20efficient%2C%20high-quality%20annotation%20remains%20a%20persistent%0Achallenge.%20In%20this%20paper%2C%20we%20present%20the%20development%20and%20evaluation%20of%20a%0Aplatform%20designed%20to%20interactively%20improve%20object%20detection%20models.%20The%0Aplatform%20allows%20uploading%20and%20annotating%20images%20as%20well%20as%20fine-tuning%20object%0Adetection%20models.%20Users%20can%20then%20manually%20review%20and%20refine%20annotations%2C%0Afurther%20creating%20improved%20snapshots%20that%20are%20used%20for%20automatic%20object%0Adetection%20on%20subsequent%20image%20uploads%20-%20a%20process%20we%20refer%20to%20as%20semi-automatic%0Aannotation%20resulting%20in%20a%20significant%20gain%20in%20annotation%20efficiency.%0A%20%20Whereas%20iterative%20refinement%20of%20model%20results%20to%20speed%20up%20annotation%20has%0Abecome%20common%20practice%2C%20we%20are%20the%20first%20to%20quantitatively%20evaluate%20its%0Abenefits%20with%20respect%20to%20time%2C%20effort%2C%20and%20interaction%20savings.%20Our%0Aexperimental%20results%20show%20clear%20evidence%20for%20a%20significant%20time%20reduction%20of%20up%0Ato%2053%25%20for%20semi-automatic%20compared%20to%20manual%20annotation.%20Importantly%2C%20these%0Aefficiency%20gains%20did%20not%20compromise%20annotation%20quality%2C%20while%20matching%20or%0Aoccasionally%20even%20exceeding%20the%20accuracy%20of%20manual%20annotations.%20These%20findings%0Ademonstrate%20the%20potential%20of%20our%20lightweight%20annotation%20platform%20for%20creating%0Ahigh-quality%20object%20detection%20datasets%20and%20provide%20best%20practices%20to%20guide%0Afuture%20development%20of%20annotation%20platforms.%0A%20%20The%20platform%20is%20open-source%2C%20with%20the%20frontend%20and%20backend%20repositories%0Aavailable%20on%20GitHub%20%28https%3A//github.com/ml-lab-htw/iterative-annotate%29.%20To%0Asupport%20the%20understanding%20of%20our%20labeling%20process%2C%20we%20have%20created%20an%0Aexplanatory%20video%20demonstrating%20the%20methodology%20using%20microscopy%20images%20of%20E.%0Acoli%20bacteria%20as%20an%20example.%20The%20video%20is%20available%20on%20YouTube%0A%28https%3A//www.youtube.com/watch%3Fv%3DCM9uhE8NN5E%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19835v2&entry.124074799=Read"},
{"title": "Continual Deep Active Learning for Medical Imaging: Replay-Base\n  Architecture for Context Adaptation", "author": "Rui Daniel and M. Rita Verdelho and Catarina Barata and Carlos Santiago", "abstract": "  Deep Learning for medical imaging faces challenges in adapting and\ngeneralizing to new contexts. Additionally, it often lacks sufficient labeled\ndata for specific tasks requiring significant annotation effort. Continual\nLearning (CL) tackles adaptability and generalizability by enabling lifelong\nlearning from a data stream while mitigating forgetting of previously learned\nknowledge. Active Learning (AL) reduces the number of required annotations for\neffective training. This work explores both approaches (CAL) to develop a novel\nframework for robust medical image analysis. Based on the automatic recognition\nof shifts in image characteristics, Replay-Base Architecture for Context\nAdaptation (RBACA) employs a CL rehearsal method to continually learn from\ndiverse contexts, and an AL component to select the most informative instances\nfor annotation. A novel approach to evaluate CAL methods is established using a\ndefined metric denominated IL-Score, which allows for the simultaneous\nassessment of transfer learning, forgetting, and final model performance. We\nshow that RBACA works in domain and class-incremental learning scenarios, by\nassessing its IL-Score on the segmentation and diagnosis of cardiac images. The\nresults show that RBACA outperforms a baseline framework without CAL, and a\nstate-of-the-art CAL method across various memory sizes and annotation budgets.\nOur code is available in https://github.com/RuiDaniel/RBACA .\n", "link": "http://arxiv.org/abs/2501.08245v1", "date": "2025-01-14", "relevancy": 2.1581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Deep%20Active%20Learning%20for%20Medical%20Imaging%3A%20Replay-Base%0A%20%20Architecture%20for%20Context%20Adaptation&body=Title%3A%20Continual%20Deep%20Active%20Learning%20for%20Medical%20Imaging%3A%20Replay-Base%0A%20%20Architecture%20for%20Context%20Adaptation%0AAuthor%3A%20Rui%20Daniel%20and%20M.%20Rita%20Verdelho%20and%20Catarina%20Barata%20and%20Carlos%20Santiago%0AAbstract%3A%20%20%20Deep%20Learning%20for%20medical%20imaging%20faces%20challenges%20in%20adapting%20and%0Ageneralizing%20to%20new%20contexts.%20Additionally%2C%20it%20often%20lacks%20sufficient%20labeled%0Adata%20for%20specific%20tasks%20requiring%20significant%20annotation%20effort.%20Continual%0ALearning%20%28CL%29%20tackles%20adaptability%20and%20generalizability%20by%20enabling%20lifelong%0Alearning%20from%20a%20data%20stream%20while%20mitigating%20forgetting%20of%20previously%20learned%0Aknowledge.%20Active%20Learning%20%28AL%29%20reduces%20the%20number%20of%20required%20annotations%20for%0Aeffective%20training.%20This%20work%20explores%20both%20approaches%20%28CAL%29%20to%20develop%20a%20novel%0Aframework%20for%20robust%20medical%20image%20analysis.%20Based%20on%20the%20automatic%20recognition%0Aof%20shifts%20in%20image%20characteristics%2C%20Replay-Base%20Architecture%20for%20Context%0AAdaptation%20%28RBACA%29%20employs%20a%20CL%20rehearsal%20method%20to%20continually%20learn%20from%0Adiverse%20contexts%2C%20and%20an%20AL%20component%20to%20select%20the%20most%20informative%20instances%0Afor%20annotation.%20A%20novel%20approach%20to%20evaluate%20CAL%20methods%20is%20established%20using%20a%0Adefined%20metric%20denominated%20IL-Score%2C%20which%20allows%20for%20the%20simultaneous%0Aassessment%20of%20transfer%20learning%2C%20forgetting%2C%20and%20final%20model%20performance.%20We%0Ashow%20that%20RBACA%20works%20in%20domain%20and%20class-incremental%20learning%20scenarios%2C%20by%0Aassessing%20its%20IL-Score%20on%20the%20segmentation%20and%20diagnosis%20of%20cardiac%20images.%20The%0Aresults%20show%20that%20RBACA%20outperforms%20a%20baseline%20framework%20without%20CAL%2C%20and%20a%0Astate-of-the-art%20CAL%20method%20across%20various%20memory%20sizes%20and%20annotation%20budgets.%0AOur%20code%20is%20available%20in%20https%3A//github.com/RuiDaniel/RBACA%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Deep%2520Active%2520Learning%2520for%2520Medical%2520Imaging%253A%2520Replay-Base%250A%2520%2520Architecture%2520for%2520Context%2520Adaptation%26entry.906535625%3DRui%2520Daniel%2520and%2520M.%2520Rita%2520Verdelho%2520and%2520Catarina%2520Barata%2520and%2520Carlos%2520Santiago%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520for%2520medical%2520imaging%2520faces%2520challenges%2520in%2520adapting%2520and%250Ageneralizing%2520to%2520new%2520contexts.%2520Additionally%252C%2520it%2520often%2520lacks%2520sufficient%2520labeled%250Adata%2520for%2520specific%2520tasks%2520requiring%2520significant%2520annotation%2520effort.%2520Continual%250ALearning%2520%2528CL%2529%2520tackles%2520adaptability%2520and%2520generalizability%2520by%2520enabling%2520lifelong%250Alearning%2520from%2520a%2520data%2520stream%2520while%2520mitigating%2520forgetting%2520of%2520previously%2520learned%250Aknowledge.%2520Active%2520Learning%2520%2528AL%2529%2520reduces%2520the%2520number%2520of%2520required%2520annotations%2520for%250Aeffective%2520training.%2520This%2520work%2520explores%2520both%2520approaches%2520%2528CAL%2529%2520to%2520develop%2520a%2520novel%250Aframework%2520for%2520robust%2520medical%2520image%2520analysis.%2520Based%2520on%2520the%2520automatic%2520recognition%250Aof%2520shifts%2520in%2520image%2520characteristics%252C%2520Replay-Base%2520Architecture%2520for%2520Context%250AAdaptation%2520%2528RBACA%2529%2520employs%2520a%2520CL%2520rehearsal%2520method%2520to%2520continually%2520learn%2520from%250Adiverse%2520contexts%252C%2520and%2520an%2520AL%2520component%2520to%2520select%2520the%2520most%2520informative%2520instances%250Afor%2520annotation.%2520A%2520novel%2520approach%2520to%2520evaluate%2520CAL%2520methods%2520is%2520established%2520using%2520a%250Adefined%2520metric%2520denominated%2520IL-Score%252C%2520which%2520allows%2520for%2520the%2520simultaneous%250Aassessment%2520of%2520transfer%2520learning%252C%2520forgetting%252C%2520and%2520final%2520model%2520performance.%2520We%250Ashow%2520that%2520RBACA%2520works%2520in%2520domain%2520and%2520class-incremental%2520learning%2520scenarios%252C%2520by%250Aassessing%2520its%2520IL-Score%2520on%2520the%2520segmentation%2520and%2520diagnosis%2520of%2520cardiac%2520images.%2520The%250Aresults%2520show%2520that%2520RBACA%2520outperforms%2520a%2520baseline%2520framework%2520without%2520CAL%252C%2520and%2520a%250Astate-of-the-art%2520CAL%2520method%2520across%2520various%2520memory%2520sizes%2520and%2520annotation%2520budgets.%250AOur%2520code%2520is%2520available%2520in%2520https%253A//github.com/RuiDaniel/RBACA%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Deep%20Active%20Learning%20for%20Medical%20Imaging%3A%20Replay-Base%0A%20%20Architecture%20for%20Context%20Adaptation&entry.906535625=Rui%20Daniel%20and%20M.%20Rita%20Verdelho%20and%20Catarina%20Barata%20and%20Carlos%20Santiago&entry.1292438233=%20%20Deep%20Learning%20for%20medical%20imaging%20faces%20challenges%20in%20adapting%20and%0Ageneralizing%20to%20new%20contexts.%20Additionally%2C%20it%20often%20lacks%20sufficient%20labeled%0Adata%20for%20specific%20tasks%20requiring%20significant%20annotation%20effort.%20Continual%0ALearning%20%28CL%29%20tackles%20adaptability%20and%20generalizability%20by%20enabling%20lifelong%0Alearning%20from%20a%20data%20stream%20while%20mitigating%20forgetting%20of%20previously%20learned%0Aknowledge.%20Active%20Learning%20%28AL%29%20reduces%20the%20number%20of%20required%20annotations%20for%0Aeffective%20training.%20This%20work%20explores%20both%20approaches%20%28CAL%29%20to%20develop%20a%20novel%0Aframework%20for%20robust%20medical%20image%20analysis.%20Based%20on%20the%20automatic%20recognition%0Aof%20shifts%20in%20image%20characteristics%2C%20Replay-Base%20Architecture%20for%20Context%0AAdaptation%20%28RBACA%29%20employs%20a%20CL%20rehearsal%20method%20to%20continually%20learn%20from%0Adiverse%20contexts%2C%20and%20an%20AL%20component%20to%20select%20the%20most%20informative%20instances%0Afor%20annotation.%20A%20novel%20approach%20to%20evaluate%20CAL%20methods%20is%20established%20using%20a%0Adefined%20metric%20denominated%20IL-Score%2C%20which%20allows%20for%20the%20simultaneous%0Aassessment%20of%20transfer%20learning%2C%20forgetting%2C%20and%20final%20model%20performance.%20We%0Ashow%20that%20RBACA%20works%20in%20domain%20and%20class-incremental%20learning%20scenarios%2C%20by%0Aassessing%20its%20IL-Score%20on%20the%20segmentation%20and%20diagnosis%20of%20cardiac%20images.%20The%0Aresults%20show%20that%20RBACA%20outperforms%20a%20baseline%20framework%20without%20CAL%2C%20and%20a%0Astate-of-the-art%20CAL%20method%20across%20various%20memory%20sizes%20and%20annotation%20budgets.%0AOur%20code%20is%20available%20in%20https%3A//github.com/RuiDaniel/RBACA%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08245v1&entry.124074799=Read"},
{"title": "Scaling White-Box Transformers for Vision", "author": "Jinrui Yang and Xianhang Li and Druv Pai and Yuyin Zhou and Yi Ma and Yaodong Yu and Cihang Xie", "abstract": "  CRATE, a white-box transformer architecture designed to learn compressed and\nsparse representations, offers an intriguing alternative to standard vision\ntransformers (ViTs) due to its inherent mathematical interpretability. Despite\nextensive investigations into the scaling behaviors of language and vision\ntransformers, the scalability of CRATE remains an open question which this\npaper aims to address. Specifically, we propose CRATE-$\\alpha$, featuring\nstrategic yet minimal modifications to the sparse coding block in the CRATE\narchitecture design, and a light training recipe designed to improve the\nscalability of CRATE. Through extensive experiments, we demonstrate that\nCRATE-$\\alpha$ can effectively scale with larger model sizes and datasets. For\nexample, our CRATE-$\\alpha$-B substantially outperforms the prior best CRATE-B\nmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of\n83.2%. Meanwhile, when scaling further, our CRATE-$\\alpha$-L obtains an\nImageNet classification accuracy of 85.1%. More notably, these model\nperformance improvements are achieved while preserving, and potentially even\nenhancing the interpretability of learned CRATE models, as we demonstrate\nthrough showing that the learned token representations of increasingly larger\ntrained CRATE-$\\alpha$ models yield increasingly higher-quality unsupervised\nobject segmentation of images. The project page is\nhttps://rayjryang.github.io/CRATE-alpha/.\n", "link": "http://arxiv.org/abs/2405.20299v4", "date": "2025-01-14", "relevancy": 2.1491, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20White-Box%20Transformers%20for%20Vision&body=Title%3A%20Scaling%20White-Box%20Transformers%20for%20Vision%0AAuthor%3A%20Jinrui%20Yang%20and%20Xianhang%20Li%20and%20Druv%20Pai%20and%20Yuyin%20Zhou%20and%20Yi%20Ma%20and%20Yaodong%20Yu%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20CRATE%2C%20a%20white-box%20transformer%20architecture%20designed%20to%20learn%20compressed%20and%0Asparse%20representations%2C%20offers%20an%20intriguing%20alternative%20to%20standard%20vision%0Atransformers%20%28ViTs%29%20due%20to%20its%20inherent%20mathematical%20interpretability.%20Despite%0Aextensive%20investigations%20into%20the%20scaling%20behaviors%20of%20language%20and%20vision%0Atransformers%2C%20the%20scalability%20of%20CRATE%20remains%20an%20open%20question%20which%20this%0Apaper%20aims%20to%20address.%20Specifically%2C%20we%20propose%20CRATE-%24%5Calpha%24%2C%20featuring%0Astrategic%20yet%20minimal%20modifications%20to%20the%20sparse%20coding%20block%20in%20the%20CRATE%0Aarchitecture%20design%2C%20and%20a%20light%20training%20recipe%20designed%20to%20improve%20the%0Ascalability%20of%20CRATE.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACRATE-%24%5Calpha%24%20can%20effectively%20scale%20with%20larger%20model%20sizes%20and%20datasets.%20For%0Aexample%2C%20our%20CRATE-%24%5Calpha%24-B%20substantially%20outperforms%20the%20prior%20best%20CRATE-B%0Amodel%20accuracy%20on%20ImageNet%20classification%20by%203.7%25%2C%20achieving%20an%20accuracy%20of%0A83.2%25.%20Meanwhile%2C%20when%20scaling%20further%2C%20our%20CRATE-%24%5Calpha%24-L%20obtains%20an%0AImageNet%20classification%20accuracy%20of%2085.1%25.%20More%20notably%2C%20these%20model%0Aperformance%20improvements%20are%20achieved%20while%20preserving%2C%20and%20potentially%20even%0Aenhancing%20the%20interpretability%20of%20learned%20CRATE%20models%2C%20as%20we%20demonstrate%0Athrough%20showing%20that%20the%20learned%20token%20representations%20of%20increasingly%20larger%0Atrained%20CRATE-%24%5Calpha%24%20models%20yield%20increasingly%20higher-quality%20unsupervised%0Aobject%20segmentation%20of%20images.%20The%20project%20page%20is%0Ahttps%3A//rayjryang.github.io/CRATE-alpha/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20299v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520White-Box%2520Transformers%2520for%2520Vision%26entry.906535625%3DJinrui%2520Yang%2520and%2520Xianhang%2520Li%2520and%2520Druv%2520Pai%2520and%2520Yuyin%2520Zhou%2520and%2520Yi%2520Ma%2520and%2520Yaodong%2520Yu%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520CRATE%252C%2520a%2520white-box%2520transformer%2520architecture%2520designed%2520to%2520learn%2520compressed%2520and%250Asparse%2520representations%252C%2520offers%2520an%2520intriguing%2520alternative%2520to%2520standard%2520vision%250Atransformers%2520%2528ViTs%2529%2520due%2520to%2520its%2520inherent%2520mathematical%2520interpretability.%2520Despite%250Aextensive%2520investigations%2520into%2520the%2520scaling%2520behaviors%2520of%2520language%2520and%2520vision%250Atransformers%252C%2520the%2520scalability%2520of%2520CRATE%2520remains%2520an%2520open%2520question%2520which%2520this%250Apaper%2520aims%2520to%2520address.%2520Specifically%252C%2520we%2520propose%2520CRATE-%2524%255Calpha%2524%252C%2520featuring%250Astrategic%2520yet%2520minimal%2520modifications%2520to%2520the%2520sparse%2520coding%2520block%2520in%2520the%2520CRATE%250Aarchitecture%2520design%252C%2520and%2520a%2520light%2520training%2520recipe%2520designed%2520to%2520improve%2520the%250Ascalability%2520of%2520CRATE.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%250ACRATE-%2524%255Calpha%2524%2520can%2520effectively%2520scale%2520with%2520larger%2520model%2520sizes%2520and%2520datasets.%2520For%250Aexample%252C%2520our%2520CRATE-%2524%255Calpha%2524-B%2520substantially%2520outperforms%2520the%2520prior%2520best%2520CRATE-B%250Amodel%2520accuracy%2520on%2520ImageNet%2520classification%2520by%25203.7%2525%252C%2520achieving%2520an%2520accuracy%2520of%250A83.2%2525.%2520Meanwhile%252C%2520when%2520scaling%2520further%252C%2520our%2520CRATE-%2524%255Calpha%2524-L%2520obtains%2520an%250AImageNet%2520classification%2520accuracy%2520of%252085.1%2525.%2520More%2520notably%252C%2520these%2520model%250Aperformance%2520improvements%2520are%2520achieved%2520while%2520preserving%252C%2520and%2520potentially%2520even%250Aenhancing%2520the%2520interpretability%2520of%2520learned%2520CRATE%2520models%252C%2520as%2520we%2520demonstrate%250Athrough%2520showing%2520that%2520the%2520learned%2520token%2520representations%2520of%2520increasingly%2520larger%250Atrained%2520CRATE-%2524%255Calpha%2524%2520models%2520yield%2520increasingly%2520higher-quality%2520unsupervised%250Aobject%2520segmentation%2520of%2520images.%2520The%2520project%2520page%2520is%250Ahttps%253A//rayjryang.github.io/CRATE-alpha/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20299v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20White-Box%20Transformers%20for%20Vision&entry.906535625=Jinrui%20Yang%20and%20Xianhang%20Li%20and%20Druv%20Pai%20and%20Yuyin%20Zhou%20and%20Yi%20Ma%20and%20Yaodong%20Yu%20and%20Cihang%20Xie&entry.1292438233=%20%20CRATE%2C%20a%20white-box%20transformer%20architecture%20designed%20to%20learn%20compressed%20and%0Asparse%20representations%2C%20offers%20an%20intriguing%20alternative%20to%20standard%20vision%0Atransformers%20%28ViTs%29%20due%20to%20its%20inherent%20mathematical%20interpretability.%20Despite%0Aextensive%20investigations%20into%20the%20scaling%20behaviors%20of%20language%20and%20vision%0Atransformers%2C%20the%20scalability%20of%20CRATE%20remains%20an%20open%20question%20which%20this%0Apaper%20aims%20to%20address.%20Specifically%2C%20we%20propose%20CRATE-%24%5Calpha%24%2C%20featuring%0Astrategic%20yet%20minimal%20modifications%20to%20the%20sparse%20coding%20block%20in%20the%20CRATE%0Aarchitecture%20design%2C%20and%20a%20light%20training%20recipe%20designed%20to%20improve%20the%0Ascalability%20of%20CRATE.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%0ACRATE-%24%5Calpha%24%20can%20effectively%20scale%20with%20larger%20model%20sizes%20and%20datasets.%20For%0Aexample%2C%20our%20CRATE-%24%5Calpha%24-B%20substantially%20outperforms%20the%20prior%20best%20CRATE-B%0Amodel%20accuracy%20on%20ImageNet%20classification%20by%203.7%25%2C%20achieving%20an%20accuracy%20of%0A83.2%25.%20Meanwhile%2C%20when%20scaling%20further%2C%20our%20CRATE-%24%5Calpha%24-L%20obtains%20an%0AImageNet%20classification%20accuracy%20of%2085.1%25.%20More%20notably%2C%20these%20model%0Aperformance%20improvements%20are%20achieved%20while%20preserving%2C%20and%20potentially%20even%0Aenhancing%20the%20interpretability%20of%20learned%20CRATE%20models%2C%20as%20we%20demonstrate%0Athrough%20showing%20that%20the%20learned%20token%20representations%20of%20increasingly%20larger%0Atrained%20CRATE-%24%5Calpha%24%20models%20yield%20increasingly%20higher-quality%20unsupervised%0Aobject%20segmentation%20of%20images.%20The%20project%20page%20is%0Ahttps%3A//rayjryang.github.io/CRATE-alpha/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20299v4&entry.124074799=Read"},
{"title": "MiniMax-01: Scaling Foundation Models with Lightning Attention", "author": " MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu", "abstract": "  We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.\n", "link": "http://arxiv.org/abs/2501.08313v1", "date": "2025-01-14", "relevancy": 2.1431, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniMax-01%3A%20Scaling%20Foundation%20Models%20with%20Lightning%20Attention&body=Title%3A%20MiniMax-01%3A%20Scaling%20Foundation%20Models%20with%20Lightning%20Attention%0AAuthor%3A%20%20MiniMax%20and%20Aonian%20Li%20and%20Bangwei%20Gong%20and%20Bo%20Yang%20and%20Boji%20Shan%20and%20Chang%20Liu%20and%20Cheng%20Zhu%20and%20Chunhao%20Zhang%20and%20Congchao%20Guo%20and%20Da%20Chen%20and%20Dong%20Li%20and%20Enwei%20Jiao%20and%20Gengxin%20Li%20and%20Guojun%20Zhang%20and%20Haohai%20Sun%20and%20Houze%20Dong%20and%20Jiadai%20Zhu%20and%20Jiaqi%20Zhuang%20and%20Jiayuan%20Song%20and%20Jin%20Zhu%20and%20Jingtao%20Han%20and%20Jingyang%20Li%20and%20Junbin%20Xie%20and%20Junhao%20Xu%20and%20Junjie%20Yan%20and%20Kaishun%20Zhang%20and%20Kecheng%20Xiao%20and%20Kexi%20Kang%20and%20Le%20Han%20and%20Leyang%20Wang%20and%20Lianfei%20Yu%20and%20Liheng%20Feng%20and%20Lin%20Zheng%20and%20Linbo%20Chai%20and%20Long%20Xing%20and%20Meizhi%20Ju%20and%20Mingyuan%20Chi%20and%20Mozhi%20Zhang%20and%20Peikai%20Huang%20and%20Pengcheng%20Niu%20and%20Pengfei%20Li%20and%20Pengyu%20Zhao%20and%20Qi%20Yang%20and%20Qidi%20Xu%20and%20Qiexiang%20Wang%20and%20Qin%20Wang%20and%20Qiuhui%20Li%20and%20Ruitao%20Leng%20and%20Shengmin%20Shi%20and%20Shuqi%20Yu%20and%20Sichen%20Li%20and%20Songquan%20Zhu%20and%20Tao%20Huang%20and%20Tianrun%20Liang%20and%20Weigao%20Sun%20and%20Weixuan%20Sun%20and%20Weiyu%20Cheng%20and%20Wenkai%20Li%20and%20Xiangjun%20Song%20and%20Xiao%20Su%20and%20Xiaodong%20Han%20and%20Xinjie%20Zhang%20and%20Xinzhu%20Hou%20and%20Xu%20Min%20and%20Xun%20Zou%20and%20Xuyang%20Shen%20and%20Yan%20Gong%20and%20Yingjie%20Zhu%20and%20Yipeng%20Zhou%20and%20Yiran%20Zhong%20and%20Yongyi%20Hu%20and%20Yuanxiang%20Fan%20and%20Yue%20Yu%20and%20Yufeng%20Yang%20and%20Yuhao%20Li%20and%20Yunan%20Huang%20and%20Yunji%20Li%20and%20Yunpeng%20Huang%20and%20Yunzhi%20Xu%20and%20Yuxin%20Mao%20and%20Zehan%20Li%20and%20Zekang%20Li%20and%20Zewei%20Tao%20and%20Zewen%20Ying%20and%20Zhaoyang%20Cong%20and%20Zhen%20Qin%20and%20Zhenhua%20Fan%20and%20Zhihang%20Yu%20and%20Zhuo%20Jiang%20and%20Zijia%20Wu%0AAbstract%3A%20%20%20We%20introduce%20MiniMax-01%20series%2C%20including%20MiniMax-Text-01%20and%20MiniMax-VL-01%2C%0Awhich%20are%20comparable%20to%20top-tier%20models%20while%20offering%20superior%20capabilities%20in%0Aprocessing%20longer%20contexts.%20The%20core%20lies%20in%20lightning%20attention%20and%20its%0Aefficient%20scaling.%20To%20maximize%20computational%20capacity%2C%20we%20integrate%20it%20with%0AMixture%20of%20Experts%20%28MoE%29%2C%20creating%20a%20model%20with%2032%20experts%20and%20456%20billion%0Atotal%20parameters%2C%20of%20which%2045.9%20billion%20are%20activated%20for%20each%20token.%20We%0Adevelop%20an%20optimized%20parallel%20strategy%20and%20highly%20efficient%0Acomputation-communication%20overlap%20techniques%20for%20MoE%20and%20lightning%20attention.%0AThis%20approach%20enables%20us%20to%20conduct%20efficient%20training%20and%20inference%20on%20models%0Awith%20hundreds%20of%20billions%20of%20parameters%20across%20contexts%20spanning%20millions%20of%0Atokens.%20The%20context%20window%20of%20MiniMax-Text-01%20can%20reach%20up%20to%201%20million%20tokens%0Aduring%20training%20and%20extrapolate%20to%204%20million%20tokens%20during%20inference%20at%20an%0Aaffordable%20cost.%20Our%20vision-language%20model%2C%20MiniMax-VL-01%20is%20built%20through%0Acontinued%20training%20with%20512%20billion%20vision-language%20tokens.%20Experiments%20on%20both%0Astandard%20and%20in-house%20benchmarks%20show%20that%20our%20models%20match%20the%20performance%20of%0Astate-of-the-art%20models%20like%20GPT-4o%20and%20Claude-3.5-Sonnet%20while%20offering%2020-32%0Atimes%20longer%20context%20window.%20We%20publicly%20release%20MiniMax-01%20at%0Ahttps%3A//github.com/MiniMax-AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniMax-01%253A%2520Scaling%2520Foundation%2520Models%2520with%2520Lightning%2520Attention%26entry.906535625%3D%2520MiniMax%2520and%2520Aonian%2520Li%2520and%2520Bangwei%2520Gong%2520and%2520Bo%2520Yang%2520and%2520Boji%2520Shan%2520and%2520Chang%2520Liu%2520and%2520Cheng%2520Zhu%2520and%2520Chunhao%2520Zhang%2520and%2520Congchao%2520Guo%2520and%2520Da%2520Chen%2520and%2520Dong%2520Li%2520and%2520Enwei%2520Jiao%2520and%2520Gengxin%2520Li%2520and%2520Guojun%2520Zhang%2520and%2520Haohai%2520Sun%2520and%2520Houze%2520Dong%2520and%2520Jiadai%2520Zhu%2520and%2520Jiaqi%2520Zhuang%2520and%2520Jiayuan%2520Song%2520and%2520Jin%2520Zhu%2520and%2520Jingtao%2520Han%2520and%2520Jingyang%2520Li%2520and%2520Junbin%2520Xie%2520and%2520Junhao%2520Xu%2520and%2520Junjie%2520Yan%2520and%2520Kaishun%2520Zhang%2520and%2520Kecheng%2520Xiao%2520and%2520Kexi%2520Kang%2520and%2520Le%2520Han%2520and%2520Leyang%2520Wang%2520and%2520Lianfei%2520Yu%2520and%2520Liheng%2520Feng%2520and%2520Lin%2520Zheng%2520and%2520Linbo%2520Chai%2520and%2520Long%2520Xing%2520and%2520Meizhi%2520Ju%2520and%2520Mingyuan%2520Chi%2520and%2520Mozhi%2520Zhang%2520and%2520Peikai%2520Huang%2520and%2520Pengcheng%2520Niu%2520and%2520Pengfei%2520Li%2520and%2520Pengyu%2520Zhao%2520and%2520Qi%2520Yang%2520and%2520Qidi%2520Xu%2520and%2520Qiexiang%2520Wang%2520and%2520Qin%2520Wang%2520and%2520Qiuhui%2520Li%2520and%2520Ruitao%2520Leng%2520and%2520Shengmin%2520Shi%2520and%2520Shuqi%2520Yu%2520and%2520Sichen%2520Li%2520and%2520Songquan%2520Zhu%2520and%2520Tao%2520Huang%2520and%2520Tianrun%2520Liang%2520and%2520Weigao%2520Sun%2520and%2520Weixuan%2520Sun%2520and%2520Weiyu%2520Cheng%2520and%2520Wenkai%2520Li%2520and%2520Xiangjun%2520Song%2520and%2520Xiao%2520Su%2520and%2520Xiaodong%2520Han%2520and%2520Xinjie%2520Zhang%2520and%2520Xinzhu%2520Hou%2520and%2520Xu%2520Min%2520and%2520Xun%2520Zou%2520and%2520Xuyang%2520Shen%2520and%2520Yan%2520Gong%2520and%2520Yingjie%2520Zhu%2520and%2520Yipeng%2520Zhou%2520and%2520Yiran%2520Zhong%2520and%2520Yongyi%2520Hu%2520and%2520Yuanxiang%2520Fan%2520and%2520Yue%2520Yu%2520and%2520Yufeng%2520Yang%2520and%2520Yuhao%2520Li%2520and%2520Yunan%2520Huang%2520and%2520Yunji%2520Li%2520and%2520Yunpeng%2520Huang%2520and%2520Yunzhi%2520Xu%2520and%2520Yuxin%2520Mao%2520and%2520Zehan%2520Li%2520and%2520Zekang%2520Li%2520and%2520Zewei%2520Tao%2520and%2520Zewen%2520Ying%2520and%2520Zhaoyang%2520Cong%2520and%2520Zhen%2520Qin%2520and%2520Zhenhua%2520Fan%2520and%2520Zhihang%2520Yu%2520and%2520Zhuo%2520Jiang%2520and%2520Zijia%2520Wu%26entry.1292438233%3D%2520%2520We%2520introduce%2520MiniMax-01%2520series%252C%2520including%2520MiniMax-Text-01%2520and%2520MiniMax-VL-01%252C%250Awhich%2520are%2520comparable%2520to%2520top-tier%2520models%2520while%2520offering%2520superior%2520capabilities%2520in%250Aprocessing%2520longer%2520contexts.%2520The%2520core%2520lies%2520in%2520lightning%2520attention%2520and%2520its%250Aefficient%2520scaling.%2520To%2520maximize%2520computational%2520capacity%252C%2520we%2520integrate%2520it%2520with%250AMixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520creating%2520a%2520model%2520with%252032%2520experts%2520and%2520456%2520billion%250Atotal%2520parameters%252C%2520of%2520which%252045.9%2520billion%2520are%2520activated%2520for%2520each%2520token.%2520We%250Adevelop%2520an%2520optimized%2520parallel%2520strategy%2520and%2520highly%2520efficient%250Acomputation-communication%2520overlap%2520techniques%2520for%2520MoE%2520and%2520lightning%2520attention.%250AThis%2520approach%2520enables%2520us%2520to%2520conduct%2520efficient%2520training%2520and%2520inference%2520on%2520models%250Awith%2520hundreds%2520of%2520billions%2520of%2520parameters%2520across%2520contexts%2520spanning%2520millions%2520of%250Atokens.%2520The%2520context%2520window%2520of%2520MiniMax-Text-01%2520can%2520reach%2520up%2520to%25201%2520million%2520tokens%250Aduring%2520training%2520and%2520extrapolate%2520to%25204%2520million%2520tokens%2520during%2520inference%2520at%2520an%250Aaffordable%2520cost.%2520Our%2520vision-language%2520model%252C%2520MiniMax-VL-01%2520is%2520built%2520through%250Acontinued%2520training%2520with%2520512%2520billion%2520vision-language%2520tokens.%2520Experiments%2520on%2520both%250Astandard%2520and%2520in-house%2520benchmarks%2520show%2520that%2520our%2520models%2520match%2520the%2520performance%2520of%250Astate-of-the-art%2520models%2520like%2520GPT-4o%2520and%2520Claude-3.5-Sonnet%2520while%2520offering%252020-32%250Atimes%2520longer%2520context%2520window.%2520We%2520publicly%2520release%2520MiniMax-01%2520at%250Ahttps%253A//github.com/MiniMax-AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniMax-01%3A%20Scaling%20Foundation%20Models%20with%20Lightning%20Attention&entry.906535625=%20MiniMax%20and%20Aonian%20Li%20and%20Bangwei%20Gong%20and%20Bo%20Yang%20and%20Boji%20Shan%20and%20Chang%20Liu%20and%20Cheng%20Zhu%20and%20Chunhao%20Zhang%20and%20Congchao%20Guo%20and%20Da%20Chen%20and%20Dong%20Li%20and%20Enwei%20Jiao%20and%20Gengxin%20Li%20and%20Guojun%20Zhang%20and%20Haohai%20Sun%20and%20Houze%20Dong%20and%20Jiadai%20Zhu%20and%20Jiaqi%20Zhuang%20and%20Jiayuan%20Song%20and%20Jin%20Zhu%20and%20Jingtao%20Han%20and%20Jingyang%20Li%20and%20Junbin%20Xie%20and%20Junhao%20Xu%20and%20Junjie%20Yan%20and%20Kaishun%20Zhang%20and%20Kecheng%20Xiao%20and%20Kexi%20Kang%20and%20Le%20Han%20and%20Leyang%20Wang%20and%20Lianfei%20Yu%20and%20Liheng%20Feng%20and%20Lin%20Zheng%20and%20Linbo%20Chai%20and%20Long%20Xing%20and%20Meizhi%20Ju%20and%20Mingyuan%20Chi%20and%20Mozhi%20Zhang%20and%20Peikai%20Huang%20and%20Pengcheng%20Niu%20and%20Pengfei%20Li%20and%20Pengyu%20Zhao%20and%20Qi%20Yang%20and%20Qidi%20Xu%20and%20Qiexiang%20Wang%20and%20Qin%20Wang%20and%20Qiuhui%20Li%20and%20Ruitao%20Leng%20and%20Shengmin%20Shi%20and%20Shuqi%20Yu%20and%20Sichen%20Li%20and%20Songquan%20Zhu%20and%20Tao%20Huang%20and%20Tianrun%20Liang%20and%20Weigao%20Sun%20and%20Weixuan%20Sun%20and%20Weiyu%20Cheng%20and%20Wenkai%20Li%20and%20Xiangjun%20Song%20and%20Xiao%20Su%20and%20Xiaodong%20Han%20and%20Xinjie%20Zhang%20and%20Xinzhu%20Hou%20and%20Xu%20Min%20and%20Xun%20Zou%20and%20Xuyang%20Shen%20and%20Yan%20Gong%20and%20Yingjie%20Zhu%20and%20Yipeng%20Zhou%20and%20Yiran%20Zhong%20and%20Yongyi%20Hu%20and%20Yuanxiang%20Fan%20and%20Yue%20Yu%20and%20Yufeng%20Yang%20and%20Yuhao%20Li%20and%20Yunan%20Huang%20and%20Yunji%20Li%20and%20Yunpeng%20Huang%20and%20Yunzhi%20Xu%20and%20Yuxin%20Mao%20and%20Zehan%20Li%20and%20Zekang%20Li%20and%20Zewei%20Tao%20and%20Zewen%20Ying%20and%20Zhaoyang%20Cong%20and%20Zhen%20Qin%20and%20Zhenhua%20Fan%20and%20Zhihang%20Yu%20and%20Zhuo%20Jiang%20and%20Zijia%20Wu&entry.1292438233=%20%20We%20introduce%20MiniMax-01%20series%2C%20including%20MiniMax-Text-01%20and%20MiniMax-VL-01%2C%0Awhich%20are%20comparable%20to%20top-tier%20models%20while%20offering%20superior%20capabilities%20in%0Aprocessing%20longer%20contexts.%20The%20core%20lies%20in%20lightning%20attention%20and%20its%0Aefficient%20scaling.%20To%20maximize%20computational%20capacity%2C%20we%20integrate%20it%20with%0AMixture%20of%20Experts%20%28MoE%29%2C%20creating%20a%20model%20with%2032%20experts%20and%20456%20billion%0Atotal%20parameters%2C%20of%20which%2045.9%20billion%20are%20activated%20for%20each%20token.%20We%0Adevelop%20an%20optimized%20parallel%20strategy%20and%20highly%20efficient%0Acomputation-communication%20overlap%20techniques%20for%20MoE%20and%20lightning%20attention.%0AThis%20approach%20enables%20us%20to%20conduct%20efficient%20training%20and%20inference%20on%20models%0Awith%20hundreds%20of%20billions%20of%20parameters%20across%20contexts%20spanning%20millions%20of%0Atokens.%20The%20context%20window%20of%20MiniMax-Text-01%20can%20reach%20up%20to%201%20million%20tokens%0Aduring%20training%20and%20extrapolate%20to%204%20million%20tokens%20during%20inference%20at%20an%0Aaffordable%20cost.%20Our%20vision-language%20model%2C%20MiniMax-VL-01%20is%20built%20through%0Acontinued%20training%20with%20512%20billion%20vision-language%20tokens.%20Experiments%20on%20both%0Astandard%20and%20in-house%20benchmarks%20show%20that%20our%20models%20match%20the%20performance%20of%0Astate-of-the-art%20models%20like%20GPT-4o%20and%20Claude-3.5-Sonnet%20while%20offering%2020-32%0Atimes%20longer%20context%20window.%20We%20publicly%20release%20MiniMax-01%20at%0Ahttps%3A//github.com/MiniMax-AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08313v1&entry.124074799=Read"},
{"title": "WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity\n  Refinement", "author": "Chaewon Kim and Seung-Jun Moon and Gyeong-Moon Park", "abstract": "  Recent advanced GAN inversion models aim to convey high-fidelity information\nfrom original images to generators through methods using generator tuning or\nhigh-dimensional feature learning. Despite these efforts, accurately\nreconstructing image-specific details remains as a challenge due to the\ninherent limitations both in terms of training and structural aspects, leading\nto a bias towards low-frequency information. In this paper, we look into the\nwidely used pixel loss in GAN inversion, revealing its predominant focus on the\nreconstruction of low-frequency features. We then propose WINE, a\nWavelet-guided GAN Inversion aNd Editing model, which transfers the\nhigh-frequency information through wavelet coefficients via newly proposed\nwavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to\ninterpret GAN inversion in the frequency domain. Our experimental results\nshowcase the precision of WINE in preserving high-frequency details and\nenhancing image quality. Even in editing scenarios, WINE outperforms existing\nstate-of-the-art GAN inversion models with a fine balance between editability\nand reconstruction quality.\n", "link": "http://arxiv.org/abs/2210.09655v2", "date": "2025-01-14", "relevancy": 2.1392, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5772}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5051}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WINE%3A%20Wavelet-Guided%20GAN%20Inversion%20and%20Editing%20for%20High-Fidelity%0A%20%20Refinement&body=Title%3A%20WINE%3A%20Wavelet-Guided%20GAN%20Inversion%20and%20Editing%20for%20High-Fidelity%0A%20%20Refinement%0AAuthor%3A%20Chaewon%20Kim%20and%20Seung-Jun%20Moon%20and%20Gyeong-Moon%20Park%0AAbstract%3A%20%20%20Recent%20advanced%20GAN%20inversion%20models%20aim%20to%20convey%20high-fidelity%20information%0Afrom%20original%20images%20to%20generators%20through%20methods%20using%20generator%20tuning%20or%0Ahigh-dimensional%20feature%20learning.%20Despite%20these%20efforts%2C%20accurately%0Areconstructing%20image-specific%20details%20remains%20as%20a%20challenge%20due%20to%20the%0Ainherent%20limitations%20both%20in%20terms%20of%20training%20and%20structural%20aspects%2C%20leading%0Ato%20a%20bias%20towards%20low-frequency%20information.%20In%20this%20paper%2C%20we%20look%20into%20the%0Awidely%20used%20pixel%20loss%20in%20GAN%20inversion%2C%20revealing%20its%20predominant%20focus%20on%20the%0Areconstruction%20of%20low-frequency%20features.%20We%20then%20propose%20WINE%2C%20a%0AWavelet-guided%20GAN%20Inversion%20aNd%20Editing%20model%2C%20which%20transfers%20the%0Ahigh-frequency%20information%20through%20wavelet%20coefficients%20via%20newly%20proposed%0Awavelet%20loss%20and%20wavelet%20fusion%20scheme.%20Notably%2C%20WINE%20is%20the%20first%20attempt%20to%0Ainterpret%20GAN%20inversion%20in%20the%20frequency%20domain.%20Our%20experimental%20results%0Ashowcase%20the%20precision%20of%20WINE%20in%20preserving%20high-frequency%20details%20and%0Aenhancing%20image%20quality.%20Even%20in%20editing%20scenarios%2C%20WINE%20outperforms%20existing%0Astate-of-the-art%20GAN%20inversion%20models%20with%20a%20fine%20balance%20between%20editability%0Aand%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.09655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWINE%253A%2520Wavelet-Guided%2520GAN%2520Inversion%2520and%2520Editing%2520for%2520High-Fidelity%250A%2520%2520Refinement%26entry.906535625%3DChaewon%2520Kim%2520and%2520Seung-Jun%2520Moon%2520and%2520Gyeong-Moon%2520Park%26entry.1292438233%3D%2520%2520Recent%2520advanced%2520GAN%2520inversion%2520models%2520aim%2520to%2520convey%2520high-fidelity%2520information%250Afrom%2520original%2520images%2520to%2520generators%2520through%2520methods%2520using%2520generator%2520tuning%2520or%250Ahigh-dimensional%2520feature%2520learning.%2520Despite%2520these%2520efforts%252C%2520accurately%250Areconstructing%2520image-specific%2520details%2520remains%2520as%2520a%2520challenge%2520due%2520to%2520the%250Ainherent%2520limitations%2520both%2520in%2520terms%2520of%2520training%2520and%2520structural%2520aspects%252C%2520leading%250Ato%2520a%2520bias%2520towards%2520low-frequency%2520information.%2520In%2520this%2520paper%252C%2520we%2520look%2520into%2520the%250Awidely%2520used%2520pixel%2520loss%2520in%2520GAN%2520inversion%252C%2520revealing%2520its%2520predominant%2520focus%2520on%2520the%250Areconstruction%2520of%2520low-frequency%2520features.%2520We%2520then%2520propose%2520WINE%252C%2520a%250AWavelet-guided%2520GAN%2520Inversion%2520aNd%2520Editing%2520model%252C%2520which%2520transfers%2520the%250Ahigh-frequency%2520information%2520through%2520wavelet%2520coefficients%2520via%2520newly%2520proposed%250Awavelet%2520loss%2520and%2520wavelet%2520fusion%2520scheme.%2520Notably%252C%2520WINE%2520is%2520the%2520first%2520attempt%2520to%250Ainterpret%2520GAN%2520inversion%2520in%2520the%2520frequency%2520domain.%2520Our%2520experimental%2520results%250Ashowcase%2520the%2520precision%2520of%2520WINE%2520in%2520preserving%2520high-frequency%2520details%2520and%250Aenhancing%2520image%2520quality.%2520Even%2520in%2520editing%2520scenarios%252C%2520WINE%2520outperforms%2520existing%250Astate-of-the-art%2520GAN%2520inversion%2520models%2520with%2520a%2520fine%2520balance%2520between%2520editability%250Aand%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.09655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WINE%3A%20Wavelet-Guided%20GAN%20Inversion%20and%20Editing%20for%20High-Fidelity%0A%20%20Refinement&entry.906535625=Chaewon%20Kim%20and%20Seung-Jun%20Moon%20and%20Gyeong-Moon%20Park&entry.1292438233=%20%20Recent%20advanced%20GAN%20inversion%20models%20aim%20to%20convey%20high-fidelity%20information%0Afrom%20original%20images%20to%20generators%20through%20methods%20using%20generator%20tuning%20or%0Ahigh-dimensional%20feature%20learning.%20Despite%20these%20efforts%2C%20accurately%0Areconstructing%20image-specific%20details%20remains%20as%20a%20challenge%20due%20to%20the%0Ainherent%20limitations%20both%20in%20terms%20of%20training%20and%20structural%20aspects%2C%20leading%0Ato%20a%20bias%20towards%20low-frequency%20information.%20In%20this%20paper%2C%20we%20look%20into%20the%0Awidely%20used%20pixel%20loss%20in%20GAN%20inversion%2C%20revealing%20its%20predominant%20focus%20on%20the%0Areconstruction%20of%20low-frequency%20features.%20We%20then%20propose%20WINE%2C%20a%0AWavelet-guided%20GAN%20Inversion%20aNd%20Editing%20model%2C%20which%20transfers%20the%0Ahigh-frequency%20information%20through%20wavelet%20coefficients%20via%20newly%20proposed%0Awavelet%20loss%20and%20wavelet%20fusion%20scheme.%20Notably%2C%20WINE%20is%20the%20first%20attempt%20to%0Ainterpret%20GAN%20inversion%20in%20the%20frequency%20domain.%20Our%20experimental%20results%0Ashowcase%20the%20precision%20of%20WINE%20in%20preserving%20high-frequency%20details%20and%0Aenhancing%20image%20quality.%20Even%20in%20editing%20scenarios%2C%20WINE%20outperforms%20existing%0Astate-of-the-art%20GAN%20inversion%20models%20with%20a%20fine%20balance%20between%20editability%0Aand%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.09655v2&entry.124074799=Read"},
{"title": "Audio-visual Deepfake Detection With Local Temporal Inconsistencies", "author": "Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n", "link": "http://arxiv.org/abs/2501.08137v1", "date": "2025-01-14", "relevancy": 2.1371, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5433}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5302}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-visual%20Deepfake%20Detection%20With%20Local%20Temporal%20Inconsistencies&body=Title%3A%20Audio-visual%20Deepfake%20Detection%20With%20Local%20Temporal%20Inconsistencies%0AAuthor%3A%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20This%20paper%20proposes%20an%20audio-visual%20deepfake%20detection%20approach%20that%20aims%20to%0Acapture%20fine-grained%20temporal%20inconsistencies%20between%20audio%20and%20visual%0Amodalities.%20To%20achieve%20this%2C%20both%20architectural%20and%20data%20synthesis%20strategies%0Aare%20introduced.%20From%20an%20architectural%20perspective%2C%20a%20temporal%20distance%20map%2C%0Acoupled%20with%20an%20attention%20mechanism%2C%20is%20designed%20to%20capture%20these%0Ainconsistencies%20while%20minimizing%20the%20impact%20of%20irrelevant%20temporal%0Asubsequences.%20Moreover%2C%20we%20explore%20novel%20pseudo-fake%20generation%20techniques%20to%0Asynthesize%20local%20inconsistencies.%20Our%20approach%20is%20evaluated%20against%0Astate-of-the-art%20methods%20using%20the%20DFDC%20and%20FakeAVCeleb%20datasets%2C%20demonstrating%0Aits%20effectiveness%20in%20detecting%20audio-visual%20deepfakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-visual%2520Deepfake%2520Detection%2520With%2520Local%2520Temporal%2520Inconsistencies%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520an%2520audio-visual%2520deepfake%2520detection%2520approach%2520that%2520aims%2520to%250Acapture%2520fine-grained%2520temporal%2520inconsistencies%2520between%2520audio%2520and%2520visual%250Amodalities.%2520To%2520achieve%2520this%252C%2520both%2520architectural%2520and%2520data%2520synthesis%2520strategies%250Aare%2520introduced.%2520From%2520an%2520architectural%2520perspective%252C%2520a%2520temporal%2520distance%2520map%252C%250Acoupled%2520with%2520an%2520attention%2520mechanism%252C%2520is%2520designed%2520to%2520capture%2520these%250Ainconsistencies%2520while%2520minimizing%2520the%2520impact%2520of%2520irrelevant%2520temporal%250Asubsequences.%2520Moreover%252C%2520we%2520explore%2520novel%2520pseudo-fake%2520generation%2520techniques%2520to%250Asynthesize%2520local%2520inconsistencies.%2520Our%2520approach%2520is%2520evaluated%2520against%250Astate-of-the-art%2520methods%2520using%2520the%2520DFDC%2520and%2520FakeAVCeleb%2520datasets%252C%2520demonstrating%250Aits%2520effectiveness%2520in%2520detecting%2520audio-visual%2520deepfakes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-visual%20Deepfake%20Detection%20With%20Local%20Temporal%20Inconsistencies&entry.906535625=Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20This%20paper%20proposes%20an%20audio-visual%20deepfake%20detection%20approach%20that%20aims%20to%0Acapture%20fine-grained%20temporal%20inconsistencies%20between%20audio%20and%20visual%0Amodalities.%20To%20achieve%20this%2C%20both%20architectural%20and%20data%20synthesis%20strategies%0Aare%20introduced.%20From%20an%20architectural%20perspective%2C%20a%20temporal%20distance%20map%2C%0Acoupled%20with%20an%20attention%20mechanism%2C%20is%20designed%20to%20capture%20these%0Ainconsistencies%20while%20minimizing%20the%20impact%20of%20irrelevant%20temporal%0Asubsequences.%20Moreover%2C%20we%20explore%20novel%20pseudo-fake%20generation%20techniques%20to%0Asynthesize%20local%20inconsistencies.%20Our%20approach%20is%20evaluated%20against%0Astate-of-the-art%20methods%20using%20the%20DFDC%20and%20FakeAVCeleb%20datasets%2C%20demonstrating%0Aits%20effectiveness%20in%20detecting%20audio-visual%20deepfakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08137v1&entry.124074799=Read"},
{"title": "A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps", "author": "Ariel Larey and Eyal Rond and Omer Achrack", "abstract": "  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n", "link": "http://arxiv.org/abs/2410.24031v2", "date": "2025-01-14", "relevancy": 2.1358, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.533}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&body=Title%3A%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps%0AAuthor%3A%20Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack%0AAbstract%3A%20%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modal%2520Approach%2520for%2520Face%2520Anti-Spoofing%2520in%2520Non-Calibrated%2520Systems%250A%2520%2520using%2520Disparity%2520Maps%26entry.906535625%3DAriel%2520Larey%2520and%2520Eyal%2520Rond%2520and%2520Omer%2520Achrack%26entry.1292438233%3D%2520%2520Face%2520recognition%2520technologies%2520are%2520increasingly%2520used%2520in%2520various%2520applications%252C%250Ayet%2520they%2520are%2520vulnerable%2520to%2520face%2520spoofing%2520attacks.%2520These%2520spoofing%2520attacks%2520often%250Ainvolve%2520unique%25203D%2520structures%252C%2520such%2520as%2520printed%2520papers%2520or%2520mobile%2520device%2520screens.%250AAlthough%2520stereo-depth%2520cameras%2520can%2520detect%2520such%2520attacks%2520effectively%252C%2520their%250Ahigh-cost%2520limits%2520their%2520widespread%2520adoption.%2520Conversely%252C%2520two-sensor%2520systems%250Awithout%2520extrinsic%2520calibration%2520offer%2520a%2520cost-effective%2520alternative%2520but%2520are%2520unable%250Ato%2520calculate%2520depth%2520using%2520stereo%2520techniques.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%250Ato%2520overcome%2520this%2520challenge%2520by%2520leveraging%2520facial%2520attributes%2520to%2520derive%2520disparity%250Ainformation%2520and%2520estimate%2520relative%2520depth%2520for%2520anti-spoofing%2520purposes%252C%2520using%250Anon-calibrated%2520systems.%2520We%2520introduce%2520a%2520multi-modal%2520anti-spoofing%2520model%252C%2520coined%250ADisparity%2520Model%252C%2520that%2520incorporates%2520created%2520disparity%2520maps%2520as%2520a%2520third%2520modality%250Aalongside%2520the%2520two%2520original%2520sensor%2520modalities.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520Disparity%2520Model%2520in%2520countering%2520various%2520spoof%2520attacks%2520using%2520a%250Acomprehensive%2520dataset%2520collected%2520from%2520the%2520Intel%2520RealSense%2520ID%2520Solution%2520F455.%2520Our%250Amethod%2520outperformed%2520existing%2520methods%2520in%2520the%2520literature%252C%2520achieving%2520an%2520Equal%250AError%2520Rate%2520%2528EER%2529%2520of%25201.71%2525%2520and%2520a%2520False%2520Negative%2520Rate%2520%2528FNR%2529%2520of%25202.77%2525%2520at%2520a%2520False%250APositive%2520Rate%2520%2528FPR%2529%2520of%25201%2525.%2520These%2520errors%2520are%2520lower%2520by%25202.45%2525%2520and%25207.94%2525%2520than%2520the%250Aerrors%2520of%2520the%2520best%2520comparison%2520method%252C%2520respectively.%2520Additionally%252C%2520we%2520introduce%250Aa%2520model%2520ensemble%2520that%2520addresses%25203D%2520spoof%2520attacks%2520as%2520well%252C%2520achieving%2520an%2520EER%2520of%250A2.04%2525%2520and%2520an%2520FNR%2520of%25203.83%2525%2520at%2520an%2520FPR%2520of%25201%2525.%2520Overall%252C%2520our%2520work%2520provides%2520a%250Astate-of-the-art%2520solution%2520for%2520the%2520challenging%2520task%2520of%2520anti-spoofing%2520in%250Anon-calibrated%2520systems%2520that%2520lack%2520depth%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&entry.906535625=Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack&entry.1292438233=%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24031v2&entry.124074799=Read"},
{"title": "In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR", "author": "Markus J. Buehler", "abstract": "  The pursuit of automated scientific discovery has fueled progress from\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\nrecognition. Transformers function as potential systems, where every possible\nrelationship remains latent potentiality until tasks impose constraints, akin\nto measurement. Yet, refining their sampling requires more than probabilistic\nselection: solutions must conform to specific structures or rules, ensuring\nconsistency and the invocation of general principles. We present\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\nExploratory Optimization of Reasoning), a framework that combines graph\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\nultimately, final answers. Inspired by category theory, it encodes concepts as\nnodes and their relationships as edges, supporting hierarchical inference and\nadaptive learning through isomorphic representations. Demonstrations include\nhypothesis generation, materials design, and creative reasoning, such as\ndiscovering relationships between mythological concepts like 'thin places' with\nmaterials science. We propose a 'knowledge garden growth' strategy that\nintegrates insights across domains, promoting interdisciplinary connections.\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\nreasoning depth and adaptability, underscoring the potential for transparent,\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\nautonomous reasoning solutions.\n", "link": "http://arxiv.org/abs/2501.08120v1", "date": "2025-01-14", "relevancy": 2.1347, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-situ%20graph%20reasoning%20and%20knowledge%20expansion%20using%20Graph-PReFLexOR&body=Title%3A%20In-situ%20graph%20reasoning%20and%20knowledge%20expansion%20using%20Graph-PReFLexOR%0AAuthor%3A%20Markus%20J.%20Buehler%0AAbstract%3A%20%20%20The%20pursuit%20of%20automated%20scientific%20discovery%20has%20fueled%20progress%20from%0Asymbolic%20logic%20to%20modern%20AI%2C%20forging%20new%20frontiers%20in%20reasoning%20and%20pattern%0Arecognition.%20Transformers%20function%20as%20potential%20systems%2C%20where%20every%20possible%0Arelationship%20remains%20latent%20potentiality%20until%20tasks%20impose%20constraints%2C%20akin%0Ato%20measurement.%20Yet%2C%20refining%20their%20sampling%20requires%20more%20than%20probabilistic%0Aselection%3A%20solutions%20must%20conform%20to%20specific%20structures%20or%20rules%2C%20ensuring%0Aconsistency%20and%20the%20invocation%20of%20general%20principles.%20We%20present%0AGraph-PReFLexOR%20%28Graph-based%20Preference-based%20Recursive%20Language%20Modeling%20for%0AExploratory%20Optimization%20of%20Reasoning%29%2C%20a%20framework%20that%20combines%20graph%0Areasoning%20with%20symbolic%20abstraction%20to%20dynamically%20expand%20domain%20knowledge.%0AInspired%20by%20reinforcement%20learning%2C%20Graph-PReFLexOR%20defines%20reasoning%20as%20a%0Astructured%20mapping%2C%20where%20tasks%20yield%20knowledge%20graphs%2C%20abstract%20patterns%2C%20and%0Aultimately%2C%20final%20answers.%20Inspired%20by%20category%20theory%2C%20it%20encodes%20concepts%20as%0Anodes%20and%20their%20relationships%20as%20edges%2C%20supporting%20hierarchical%20inference%20and%0Aadaptive%20learning%20through%20isomorphic%20representations.%20Demonstrations%20include%0Ahypothesis%20generation%2C%20materials%20design%2C%20and%20creative%20reasoning%2C%20such%20as%0Adiscovering%20relationships%20between%20mythological%20concepts%20like%20%27thin%20places%27%20with%0Amaterials%20science.%20We%20propose%20a%20%27knowledge%20garden%20growth%27%20strategy%20that%0Aintegrates%20insights%20across%20domains%2C%20promoting%20interdisciplinary%20connections.%0AResults%20with%20a%203-billion-parameter%20Graph-PReFLexOR%20model%20show%20superior%0Areasoning%20depth%20and%20adaptability%2C%20underscoring%20the%20potential%20for%20transparent%2C%0Amultidisciplinary%20AI-driven%20discovery.%20It%20lays%20the%20groundwork%20for%20general%0Aautonomous%20reasoning%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-situ%2520graph%2520reasoning%2520and%2520knowledge%2520expansion%2520using%2520Graph-PReFLexOR%26entry.906535625%3DMarkus%2520J.%2520Buehler%26entry.1292438233%3D%2520%2520The%2520pursuit%2520of%2520automated%2520scientific%2520discovery%2520has%2520fueled%2520progress%2520from%250Asymbolic%2520logic%2520to%2520modern%2520AI%252C%2520forging%2520new%2520frontiers%2520in%2520reasoning%2520and%2520pattern%250Arecognition.%2520Transformers%2520function%2520as%2520potential%2520systems%252C%2520where%2520every%2520possible%250Arelationship%2520remains%2520latent%2520potentiality%2520until%2520tasks%2520impose%2520constraints%252C%2520akin%250Ato%2520measurement.%2520Yet%252C%2520refining%2520their%2520sampling%2520requires%2520more%2520than%2520probabilistic%250Aselection%253A%2520solutions%2520must%2520conform%2520to%2520specific%2520structures%2520or%2520rules%252C%2520ensuring%250Aconsistency%2520and%2520the%2520invocation%2520of%2520general%2520principles.%2520We%2520present%250AGraph-PReFLexOR%2520%2528Graph-based%2520Preference-based%2520Recursive%2520Language%2520Modeling%2520for%250AExploratory%2520Optimization%2520of%2520Reasoning%2529%252C%2520a%2520framework%2520that%2520combines%2520graph%250Areasoning%2520with%2520symbolic%2520abstraction%2520to%2520dynamically%2520expand%2520domain%2520knowledge.%250AInspired%2520by%2520reinforcement%2520learning%252C%2520Graph-PReFLexOR%2520defines%2520reasoning%2520as%2520a%250Astructured%2520mapping%252C%2520where%2520tasks%2520yield%2520knowledge%2520graphs%252C%2520abstract%2520patterns%252C%2520and%250Aultimately%252C%2520final%2520answers.%2520Inspired%2520by%2520category%2520theory%252C%2520it%2520encodes%2520concepts%2520as%250Anodes%2520and%2520their%2520relationships%2520as%2520edges%252C%2520supporting%2520hierarchical%2520inference%2520and%250Aadaptive%2520learning%2520through%2520isomorphic%2520representations.%2520Demonstrations%2520include%250Ahypothesis%2520generation%252C%2520materials%2520design%252C%2520and%2520creative%2520reasoning%252C%2520such%2520as%250Adiscovering%2520relationships%2520between%2520mythological%2520concepts%2520like%2520%2527thin%2520places%2527%2520with%250Amaterials%2520science.%2520We%2520propose%2520a%2520%2527knowledge%2520garden%2520growth%2527%2520strategy%2520that%250Aintegrates%2520insights%2520across%2520domains%252C%2520promoting%2520interdisciplinary%2520connections.%250AResults%2520with%2520a%25203-billion-parameter%2520Graph-PReFLexOR%2520model%2520show%2520superior%250Areasoning%2520depth%2520and%2520adaptability%252C%2520underscoring%2520the%2520potential%2520for%2520transparent%252C%250Amultidisciplinary%2520AI-driven%2520discovery.%2520It%2520lays%2520the%2520groundwork%2520for%2520general%250Aautonomous%2520reasoning%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-situ%20graph%20reasoning%20and%20knowledge%20expansion%20using%20Graph-PReFLexOR&entry.906535625=Markus%20J.%20Buehler&entry.1292438233=%20%20The%20pursuit%20of%20automated%20scientific%20discovery%20has%20fueled%20progress%20from%0Asymbolic%20logic%20to%20modern%20AI%2C%20forging%20new%20frontiers%20in%20reasoning%20and%20pattern%0Arecognition.%20Transformers%20function%20as%20potential%20systems%2C%20where%20every%20possible%0Arelationship%20remains%20latent%20potentiality%20until%20tasks%20impose%20constraints%2C%20akin%0Ato%20measurement.%20Yet%2C%20refining%20their%20sampling%20requires%20more%20than%20probabilistic%0Aselection%3A%20solutions%20must%20conform%20to%20specific%20structures%20or%20rules%2C%20ensuring%0Aconsistency%20and%20the%20invocation%20of%20general%20principles.%20We%20present%0AGraph-PReFLexOR%20%28Graph-based%20Preference-based%20Recursive%20Language%20Modeling%20for%0AExploratory%20Optimization%20of%20Reasoning%29%2C%20a%20framework%20that%20combines%20graph%0Areasoning%20with%20symbolic%20abstraction%20to%20dynamically%20expand%20domain%20knowledge.%0AInspired%20by%20reinforcement%20learning%2C%20Graph-PReFLexOR%20defines%20reasoning%20as%20a%0Astructured%20mapping%2C%20where%20tasks%20yield%20knowledge%20graphs%2C%20abstract%20patterns%2C%20and%0Aultimately%2C%20final%20answers.%20Inspired%20by%20category%20theory%2C%20it%20encodes%20concepts%20as%0Anodes%20and%20their%20relationships%20as%20edges%2C%20supporting%20hierarchical%20inference%20and%0Aadaptive%20learning%20through%20isomorphic%20representations.%20Demonstrations%20include%0Ahypothesis%20generation%2C%20materials%20design%2C%20and%20creative%20reasoning%2C%20such%20as%0Adiscovering%20relationships%20between%20mythological%20concepts%20like%20%27thin%20places%27%20with%0Amaterials%20science.%20We%20propose%20a%20%27knowledge%20garden%20growth%27%20strategy%20that%0Aintegrates%20insights%20across%20domains%2C%20promoting%20interdisciplinary%20connections.%0AResults%20with%20a%203-billion-parameter%20Graph-PReFLexOR%20model%20show%20superior%0Areasoning%20depth%20and%20adaptability%2C%20underscoring%20the%20potential%20for%20transparent%2C%0Amultidisciplinary%20AI-driven%20discovery.%20It%20lays%20the%20groundwork%20for%20general%0Aautonomous%20reasoning%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08120v1&entry.124074799=Read"},
{"title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation", "author": "Tal Zeevi and Ravid Shwartz-Ziv and Yann LeCun and Lawrence H. Staib and John A. Onofrey", "abstract": "  Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.\n", "link": "http://arxiv.org/abs/2412.07169v3", "date": "2025-01-14", "relevancy": 2.1331, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5397}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rate-In%3A%20Information-Driven%20Adaptive%20Dropout%20Rates%20for%20Improved%0A%20%20Inference-Time%20Uncertainty%20Estimation&body=Title%3A%20Rate-In%3A%20Information-Driven%20Adaptive%20Dropout%20Rates%20for%20Improved%0A%20%20Inference-Time%20Uncertainty%20Estimation%0AAuthor%3A%20Tal%20Zeevi%20and%20Ravid%20Shwartz-Ziv%20and%20Yann%20LeCun%20and%20Lawrence%20H.%20Staib%20and%20John%20A.%20Onofrey%0AAbstract%3A%20%20%20Accurate%20uncertainty%20estimation%20is%20crucial%20for%20deploying%20neural%20networks%20in%0Arisk-sensitive%20applications%20such%20as%20medical%20diagnosis.%20Monte%20Carlo%20Dropout%20is%20a%0Awidely%20used%20technique%20for%20approximating%20predictive%20uncertainty%20by%20performing%0Astochastic%20forward%20passes%20with%20dropout%20during%20inference.%20However%2C%20using%20static%0Adropout%20rates%20across%20all%20layers%20and%20inputs%20can%20lead%20to%20suboptimal%20uncertainty%0Aestimates%2C%20as%20it%20fails%20to%20adapt%20to%20the%20varying%20characteristics%20of%20individual%0Ainputs%20and%20network%20layers.%20Existing%20approaches%20optimize%20dropout%20rates%20during%0Atraining%20using%20labeled%20data%2C%20resulting%20in%20fixed%20inference-time%20parameters%20that%0Acannot%20adjust%20to%20new%20data%20distributions%2C%20compromising%20uncertainty%20estimates%20in%0AMonte%20Carlo%20simulations.%0A%20%20In%20this%20paper%2C%20we%20propose%20Rate-In%2C%20an%20algorithm%20that%20dynamically%20adjusts%0Adropout%20rates%20during%20inference%20by%20quantifying%20the%20information%20loss%20induced%20by%0Adropout%20in%20each%20layer%27s%20feature%20maps.%20By%20treating%20dropout%20as%20controlled%20noise%0Ainjection%20and%20leveraging%20information-theoretic%20principles%2C%20Rate-In%20adapts%0Adropout%20rates%20per%20layer%20and%20per%20input%20instance%20without%20requiring%20ground%20truth%0Alabels.%20By%20quantifying%20the%20functional%20information%20loss%20in%20feature%20maps%2C%20we%0Aadaptively%20tune%20dropout%20rates%20to%20maintain%20perceptual%20quality%20across%20diverse%0Amedical%20imaging%20tasks%20and%20architectural%20configurations.%20Our%20extensive%20empirical%0Astudy%20on%20synthetic%20data%20and%20real-world%20medical%20imaging%20tasks%20demonstrates%20that%0ARate-In%20improves%20calibration%20and%20sharpens%20uncertainty%20estimates%20compared%20to%0Afixed%20or%20heuristic%20dropout%20rates%20without%20compromising%20predictive%20performance.%0ARate-In%20offers%20a%20practical%2C%20unsupervised%2C%20inference-time%20approach%20to%20optimizing%0Adropout%20for%20more%20reliable%20predictive%20uncertainty%20estimation%20in%20critical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07169v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRate-In%253A%2520Information-Driven%2520Adaptive%2520Dropout%2520Rates%2520for%2520Improved%250A%2520%2520Inference-Time%2520Uncertainty%2520Estimation%26entry.906535625%3DTal%2520Zeevi%2520and%2520Ravid%2520Shwartz-Ziv%2520and%2520Yann%2520LeCun%2520and%2520Lawrence%2520H.%2520Staib%2520and%2520John%2520A.%2520Onofrey%26entry.1292438233%3D%2520%2520Accurate%2520uncertainty%2520estimation%2520is%2520crucial%2520for%2520deploying%2520neural%2520networks%2520in%250Arisk-sensitive%2520applications%2520such%2520as%2520medical%2520diagnosis.%2520Monte%2520Carlo%2520Dropout%2520is%2520a%250Awidely%2520used%2520technique%2520for%2520approximating%2520predictive%2520uncertainty%2520by%2520performing%250Astochastic%2520forward%2520passes%2520with%2520dropout%2520during%2520inference.%2520However%252C%2520using%2520static%250Adropout%2520rates%2520across%2520all%2520layers%2520and%2520inputs%2520can%2520lead%2520to%2520suboptimal%2520uncertainty%250Aestimates%252C%2520as%2520it%2520fails%2520to%2520adapt%2520to%2520the%2520varying%2520characteristics%2520of%2520individual%250Ainputs%2520and%2520network%2520layers.%2520Existing%2520approaches%2520optimize%2520dropout%2520rates%2520during%250Atraining%2520using%2520labeled%2520data%252C%2520resulting%2520in%2520fixed%2520inference-time%2520parameters%2520that%250Acannot%2520adjust%2520to%2520new%2520data%2520distributions%252C%2520compromising%2520uncertainty%2520estimates%2520in%250AMonte%2520Carlo%2520simulations.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Rate-In%252C%2520an%2520algorithm%2520that%2520dynamically%2520adjusts%250Adropout%2520rates%2520during%2520inference%2520by%2520quantifying%2520the%2520information%2520loss%2520induced%2520by%250Adropout%2520in%2520each%2520layer%2527s%2520feature%2520maps.%2520By%2520treating%2520dropout%2520as%2520controlled%2520noise%250Ainjection%2520and%2520leveraging%2520information-theoretic%2520principles%252C%2520Rate-In%2520adapts%250Adropout%2520rates%2520per%2520layer%2520and%2520per%2520input%2520instance%2520without%2520requiring%2520ground%2520truth%250Alabels.%2520By%2520quantifying%2520the%2520functional%2520information%2520loss%2520in%2520feature%2520maps%252C%2520we%250Aadaptively%2520tune%2520dropout%2520rates%2520to%2520maintain%2520perceptual%2520quality%2520across%2520diverse%250Amedical%2520imaging%2520tasks%2520and%2520architectural%2520configurations.%2520Our%2520extensive%2520empirical%250Astudy%2520on%2520synthetic%2520data%2520and%2520real-world%2520medical%2520imaging%2520tasks%2520demonstrates%2520that%250ARate-In%2520improves%2520calibration%2520and%2520sharpens%2520uncertainty%2520estimates%2520compared%2520to%250Afixed%2520or%2520heuristic%2520dropout%2520rates%2520without%2520compromising%2520predictive%2520performance.%250ARate-In%2520offers%2520a%2520practical%252C%2520unsupervised%252C%2520inference-time%2520approach%2520to%2520optimizing%250Adropout%2520for%2520more%2520reliable%2520predictive%2520uncertainty%2520estimation%2520in%2520critical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07169v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rate-In%3A%20Information-Driven%20Adaptive%20Dropout%20Rates%20for%20Improved%0A%20%20Inference-Time%20Uncertainty%20Estimation&entry.906535625=Tal%20Zeevi%20and%20Ravid%20Shwartz-Ziv%20and%20Yann%20LeCun%20and%20Lawrence%20H.%20Staib%20and%20John%20A.%20Onofrey&entry.1292438233=%20%20Accurate%20uncertainty%20estimation%20is%20crucial%20for%20deploying%20neural%20networks%20in%0Arisk-sensitive%20applications%20such%20as%20medical%20diagnosis.%20Monte%20Carlo%20Dropout%20is%20a%0Awidely%20used%20technique%20for%20approximating%20predictive%20uncertainty%20by%20performing%0Astochastic%20forward%20passes%20with%20dropout%20during%20inference.%20However%2C%20using%20static%0Adropout%20rates%20across%20all%20layers%20and%20inputs%20can%20lead%20to%20suboptimal%20uncertainty%0Aestimates%2C%20as%20it%20fails%20to%20adapt%20to%20the%20varying%20characteristics%20of%20individual%0Ainputs%20and%20network%20layers.%20Existing%20approaches%20optimize%20dropout%20rates%20during%0Atraining%20using%20labeled%20data%2C%20resulting%20in%20fixed%20inference-time%20parameters%20that%0Acannot%20adjust%20to%20new%20data%20distributions%2C%20compromising%20uncertainty%20estimates%20in%0AMonte%20Carlo%20simulations.%0A%20%20In%20this%20paper%2C%20we%20propose%20Rate-In%2C%20an%20algorithm%20that%20dynamically%20adjusts%0Adropout%20rates%20during%20inference%20by%20quantifying%20the%20information%20loss%20induced%20by%0Adropout%20in%20each%20layer%27s%20feature%20maps.%20By%20treating%20dropout%20as%20controlled%20noise%0Ainjection%20and%20leveraging%20information-theoretic%20principles%2C%20Rate-In%20adapts%0Adropout%20rates%20per%20layer%20and%20per%20input%20instance%20without%20requiring%20ground%20truth%0Alabels.%20By%20quantifying%20the%20functional%20information%20loss%20in%20feature%20maps%2C%20we%0Aadaptively%20tune%20dropout%20rates%20to%20maintain%20perceptual%20quality%20across%20diverse%0Amedical%20imaging%20tasks%20and%20architectural%20configurations.%20Our%20extensive%20empirical%0Astudy%20on%20synthetic%20data%20and%20real-world%20medical%20imaging%20tasks%20demonstrates%20that%0ARate-In%20improves%20calibration%20and%20sharpens%20uncertainty%20estimates%20compared%20to%0Afixed%20or%20heuristic%20dropout%20rates%20without%20compromising%20predictive%20performance.%0ARate-In%20offers%20a%20practical%2C%20unsupervised%2C%20inference-time%20approach%20to%20optimizing%0Adropout%20for%20more%20reliable%20predictive%20uncertainty%20estimation%20in%20critical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07169v3&entry.124074799=Read"},
{"title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data", "author": "Rewina Bedemariam and Natalie Perez and Sreyoshi Bhaduri and Satya Kapoor and Alex Gil and Elizabeth Conjar and Ikkei Itoku and David Theil and Aman Chadha and Naumaan Nayyar", "abstract": "  Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.\n", "link": "http://arxiv.org/abs/2501.08167v1", "date": "2025-01-14", "relevancy": 2.1296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5327}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potential%20and%20Perils%20of%20Large%20Language%20Models%20as%20Judges%20of%20Unstructured%0A%20%20Textual%20Data&body=Title%3A%20Potential%20and%20Perils%20of%20Large%20Language%20Models%20as%20Judges%20of%20Unstructured%0A%20%20Textual%20Data%0AAuthor%3A%20Rewina%20Bedemariam%20and%20Natalie%20Perez%20and%20Sreyoshi%20Bhaduri%20and%20Satya%20Kapoor%20and%20Alex%20Gil%20and%20Elizabeth%20Conjar%20and%20Ikkei%20Itoku%20and%20David%20Theil%20and%20Aman%20Chadha%20and%20Naumaan%20Nayyar%0AAbstract%3A%20%20%20Rapid%20advancements%20in%20large%20language%20models%20have%20unlocked%20remarkable%0Acapabilities%20when%20it%20comes%20to%20processing%20and%20summarizing%20unstructured%20text%0Adata.%20This%20has%20implications%20for%20the%20analysis%20of%20rich%2C%20open-ended%20datasets%2C%20such%0Aas%20survey%20responses%2C%20where%20LLMs%20hold%20the%20promise%20of%20efficiently%20distilling%20key%0Athemes%20and%20sentiments.%20However%2C%20as%20organizations%20increasingly%20turn%20to%20these%0Apowerful%20AI%20systems%20to%20make%20sense%20of%20textual%20feedback%2C%20a%20critical%20question%0Aarises%2C%20can%20we%20trust%20LLMs%20to%20accurately%20represent%20the%20perspectives%20contained%0Awithin%20these%20text%20based%20datasets%3F%20While%20LLMs%20excel%20at%20generating%20human-like%0Asummaries%2C%20there%20is%20a%20risk%20that%20their%20outputs%20may%20inadvertently%20diverge%20from%0Athe%20true%20substance%20of%20the%20original%20responses.%20Discrepancies%20between%20the%0ALLM-generated%20outputs%20and%20the%20actual%20themes%20present%20in%20the%20data%20could%20lead%20to%0Aflawed%20decision-making%2C%20with%20far-reaching%20consequences%20for%20organizations.%20This%0Aresearch%20investigates%20the%20effectiveness%20of%20LLMs%20as%20judge%20models%20to%20evaluate%20the%0Athematic%20alignment%20of%20summaries%20generated%20by%20other%20LLMs.%20We%20utilized%20an%0AAnthropic%20Claude%20model%20to%20generate%20thematic%20summaries%20from%20open-ended%20survey%0Aresponses%2C%20with%20Amazon%27s%20Titan%20Express%2C%20Nova%20Pro%2C%20and%20Meta%27s%20Llama%20serving%20as%0ALLM%20judges.%20The%20LLM-as-judge%20approach%20was%20compared%20to%20human%20evaluations%20using%0ACohen%27s%20kappa%2C%20Spearman%27s%20rho%2C%20and%20Krippendorff%27s%20alpha%2C%20validating%20a%20scalable%0Aalternative%20to%20traditional%20human%20centric%20evaluation%20methods.%20Our%20findings%0Areveal%20that%20while%20LLMs%20as%20judges%20offer%20a%20scalable%20solution%20comparable%20to%20human%0Araters%2C%20humans%20may%20still%20excel%20at%20detecting%20subtle%2C%20context-specific%20nuances.%0AThis%20research%20contributes%20to%20the%20growing%20body%20of%20knowledge%20on%20AI%20assisted%20text%0Aanalysis.%20We%20discuss%20limitations%20and%20provide%20recommendations%20for%20future%0Aresearch%2C%20emphasizing%20the%20need%20for%20careful%20consideration%20when%20generalizing%20LLM%0Ajudge%20models%20across%20various%20contexts%20and%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotential%2520and%2520Perils%2520of%2520Large%2520Language%2520Models%2520as%2520Judges%2520of%2520Unstructured%250A%2520%2520Textual%2520Data%26entry.906535625%3DRewina%2520Bedemariam%2520and%2520Natalie%2520Perez%2520and%2520Sreyoshi%2520Bhaduri%2520and%2520Satya%2520Kapoor%2520and%2520Alex%2520Gil%2520and%2520Elizabeth%2520Conjar%2520and%2520Ikkei%2520Itoku%2520and%2520David%2520Theil%2520and%2520Aman%2520Chadha%2520and%2520Naumaan%2520Nayyar%26entry.1292438233%3D%2520%2520Rapid%2520advancements%2520in%2520large%2520language%2520models%2520have%2520unlocked%2520remarkable%250Acapabilities%2520when%2520it%2520comes%2520to%2520processing%2520and%2520summarizing%2520unstructured%2520text%250Adata.%2520This%2520has%2520implications%2520for%2520the%2520analysis%2520of%2520rich%252C%2520open-ended%2520datasets%252C%2520such%250Aas%2520survey%2520responses%252C%2520where%2520LLMs%2520hold%2520the%2520promise%2520of%2520efficiently%2520distilling%2520key%250Athemes%2520and%2520sentiments.%2520However%252C%2520as%2520organizations%2520increasingly%2520turn%2520to%2520these%250Apowerful%2520AI%2520systems%2520to%2520make%2520sense%2520of%2520textual%2520feedback%252C%2520a%2520critical%2520question%250Aarises%252C%2520can%2520we%2520trust%2520LLMs%2520to%2520accurately%2520represent%2520the%2520perspectives%2520contained%250Awithin%2520these%2520text%2520based%2520datasets%253F%2520While%2520LLMs%2520excel%2520at%2520generating%2520human-like%250Asummaries%252C%2520there%2520is%2520a%2520risk%2520that%2520their%2520outputs%2520may%2520inadvertently%2520diverge%2520from%250Athe%2520true%2520substance%2520of%2520the%2520original%2520responses.%2520Discrepancies%2520between%2520the%250ALLM-generated%2520outputs%2520and%2520the%2520actual%2520themes%2520present%2520in%2520the%2520data%2520could%2520lead%2520to%250Aflawed%2520decision-making%252C%2520with%2520far-reaching%2520consequences%2520for%2520organizations.%2520This%250Aresearch%2520investigates%2520the%2520effectiveness%2520of%2520LLMs%2520as%2520judge%2520models%2520to%2520evaluate%2520the%250Athematic%2520alignment%2520of%2520summaries%2520generated%2520by%2520other%2520LLMs.%2520We%2520utilized%2520an%250AAnthropic%2520Claude%2520model%2520to%2520generate%2520thematic%2520summaries%2520from%2520open-ended%2520survey%250Aresponses%252C%2520with%2520Amazon%2527s%2520Titan%2520Express%252C%2520Nova%2520Pro%252C%2520and%2520Meta%2527s%2520Llama%2520serving%2520as%250ALLM%2520judges.%2520The%2520LLM-as-judge%2520approach%2520was%2520compared%2520to%2520human%2520evaluations%2520using%250ACohen%2527s%2520kappa%252C%2520Spearman%2527s%2520rho%252C%2520and%2520Krippendorff%2527s%2520alpha%252C%2520validating%2520a%2520scalable%250Aalternative%2520to%2520traditional%2520human%2520centric%2520evaluation%2520methods.%2520Our%2520findings%250Areveal%2520that%2520while%2520LLMs%2520as%2520judges%2520offer%2520a%2520scalable%2520solution%2520comparable%2520to%2520human%250Araters%252C%2520humans%2520may%2520still%2520excel%2520at%2520detecting%2520subtle%252C%2520context-specific%2520nuances.%250AThis%2520research%2520contributes%2520to%2520the%2520growing%2520body%2520of%2520knowledge%2520on%2520AI%2520assisted%2520text%250Aanalysis.%2520We%2520discuss%2520limitations%2520and%2520provide%2520recommendations%2520for%2520future%250Aresearch%252C%2520emphasizing%2520the%2520need%2520for%2520careful%2520consideration%2520when%2520generalizing%2520LLM%250Ajudge%2520models%2520across%2520various%2520contexts%2520and%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potential%20and%20Perils%20of%20Large%20Language%20Models%20as%20Judges%20of%20Unstructured%0A%20%20Textual%20Data&entry.906535625=Rewina%20Bedemariam%20and%20Natalie%20Perez%20and%20Sreyoshi%20Bhaduri%20and%20Satya%20Kapoor%20and%20Alex%20Gil%20and%20Elizabeth%20Conjar%20and%20Ikkei%20Itoku%20and%20David%20Theil%20and%20Aman%20Chadha%20and%20Naumaan%20Nayyar&entry.1292438233=%20%20Rapid%20advancements%20in%20large%20language%20models%20have%20unlocked%20remarkable%0Acapabilities%20when%20it%20comes%20to%20processing%20and%20summarizing%20unstructured%20text%0Adata.%20This%20has%20implications%20for%20the%20analysis%20of%20rich%2C%20open-ended%20datasets%2C%20such%0Aas%20survey%20responses%2C%20where%20LLMs%20hold%20the%20promise%20of%20efficiently%20distilling%20key%0Athemes%20and%20sentiments.%20However%2C%20as%20organizations%20increasingly%20turn%20to%20these%0Apowerful%20AI%20systems%20to%20make%20sense%20of%20textual%20feedback%2C%20a%20critical%20question%0Aarises%2C%20can%20we%20trust%20LLMs%20to%20accurately%20represent%20the%20perspectives%20contained%0Awithin%20these%20text%20based%20datasets%3F%20While%20LLMs%20excel%20at%20generating%20human-like%0Asummaries%2C%20there%20is%20a%20risk%20that%20their%20outputs%20may%20inadvertently%20diverge%20from%0Athe%20true%20substance%20of%20the%20original%20responses.%20Discrepancies%20between%20the%0ALLM-generated%20outputs%20and%20the%20actual%20themes%20present%20in%20the%20data%20could%20lead%20to%0Aflawed%20decision-making%2C%20with%20far-reaching%20consequences%20for%20organizations.%20This%0Aresearch%20investigates%20the%20effectiveness%20of%20LLMs%20as%20judge%20models%20to%20evaluate%20the%0Athematic%20alignment%20of%20summaries%20generated%20by%20other%20LLMs.%20We%20utilized%20an%0AAnthropic%20Claude%20model%20to%20generate%20thematic%20summaries%20from%20open-ended%20survey%0Aresponses%2C%20with%20Amazon%27s%20Titan%20Express%2C%20Nova%20Pro%2C%20and%20Meta%27s%20Llama%20serving%20as%0ALLM%20judges.%20The%20LLM-as-judge%20approach%20was%20compared%20to%20human%20evaluations%20using%0ACohen%27s%20kappa%2C%20Spearman%27s%20rho%2C%20and%20Krippendorff%27s%20alpha%2C%20validating%20a%20scalable%0Aalternative%20to%20traditional%20human%20centric%20evaluation%20methods.%20Our%20findings%0Areveal%20that%20while%20LLMs%20as%20judges%20offer%20a%20scalable%20solution%20comparable%20to%20human%0Araters%2C%20humans%20may%20still%20excel%20at%20detecting%20subtle%2C%20context-specific%20nuances.%0AThis%20research%20contributes%20to%20the%20growing%20body%20of%20knowledge%20on%20AI%20assisted%20text%0Aanalysis.%20We%20discuss%20limitations%20and%20provide%20recommendations%20for%20future%0Aresearch%2C%20emphasizing%20the%20need%20for%20careful%20consideration%20when%20generalizing%20LLM%0Ajudge%20models%20across%20various%20contexts%20and%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08167v1&entry.124074799=Read"},
{"title": "SAR Strikes Back: A New Hope for RSVQA", "author": "Lucrezia Tosato and Flora Weissgerber and Laurent Wendling and Sylvain Lobry", "abstract": "  Remote sensing visual question answering (RSVQA) is a task that automatically\nextracts information from satellite images and processes a question to predict\nthe answer from the images in textual form, helping with the interpretation of\nthe image. While different methods have been proposed to extract information\nfrom optical images with different spectral bands and resolutions, no method\nhas been proposed to answer questions from Synthetic Aperture Radar (SAR)\nimages. SAR images capture electromagnetic information from the scene, and are\nless affected by atmospheric conditions, such as clouds. In this work, our\nobjective is to introduce SAR in the RSVQA task, finding the best way to use\nthis modality. In our research, we carry out a study on different pipelines for\nthe task of RSVQA taking into account information from both SAR and optical\ndata. To this purpose, we also present a dataset that allows for the\nintroduction of SAR images in the RSVQA framework. We propose two different\nmodels to include the SAR modality. The first one is an end-to-end method in\nwhich we add an additional encoder for the SAR modality. In the second\napproach, we build on a two-stage framework. First, relevant information is\nextracted from SAR and, optionally, optical data. This information is then\ntranslated into natural language to be used in the second step which only\nrelies on a language model to provide the answer. We find that the second\npipeline allows us to obtain good results with SAR images alone. We then try\nvarious types of fusion methods to use SAR and optical images together, finding\nthat a fusion at the decision level achieves the best results on the proposed\ndataset. We show that SAR data offers additional information when fused with\nthe optical modality, particularly for questions related to specific land cover\nclasses, such as water areas.\n", "link": "http://arxiv.org/abs/2501.08131v1", "date": "2025-01-14", "relevancy": 2.1193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA&body=Title%3A%20SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA%0AAuthor%3A%20Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry%0AAbstract%3A%20%20%20Remote%20sensing%20visual%20question%20answering%20%28RSVQA%29%20is%20a%20task%20that%20automatically%0Aextracts%20information%20from%20satellite%20images%20and%20processes%20a%20question%20to%20predict%0Athe%20answer%20from%20the%20images%20in%20textual%20form%2C%20helping%20with%20the%20interpretation%20of%0Athe%20image.%20While%20different%20methods%20have%20been%20proposed%20to%20extract%20information%0Afrom%20optical%20images%20with%20different%20spectral%20bands%20and%20resolutions%2C%20no%20method%0Ahas%20been%20proposed%20to%20answer%20questions%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Aimages.%20SAR%20images%20capture%20electromagnetic%20information%20from%20the%20scene%2C%20and%20are%0Aless%20affected%20by%20atmospheric%20conditions%2C%20such%20as%20clouds.%20In%20this%20work%2C%20our%0Aobjective%20is%20to%20introduce%20SAR%20in%20the%20RSVQA%20task%2C%20finding%20the%20best%20way%20to%20use%0Athis%20modality.%20In%20our%20research%2C%20we%20carry%20out%20a%20study%20on%20different%20pipelines%20for%0Athe%20task%20of%20RSVQA%20taking%20into%20account%20information%20from%20both%20SAR%20and%20optical%0Adata.%20To%20this%20purpose%2C%20we%20also%20present%20a%20dataset%20that%20allows%20for%20the%0Aintroduction%20of%20SAR%20images%20in%20the%20RSVQA%20framework.%20We%20propose%20two%20different%0Amodels%20to%20include%20the%20SAR%20modality.%20The%20first%20one%20is%20an%20end-to-end%20method%20in%0Awhich%20we%20add%20an%20additional%20encoder%20for%20the%20SAR%20modality.%20In%20the%20second%0Aapproach%2C%20we%20build%20on%20a%20two-stage%20framework.%20First%2C%20relevant%20information%20is%0Aextracted%20from%20SAR%20and%2C%20optionally%2C%20optical%20data.%20This%20information%20is%20then%0Atranslated%20into%20natural%20language%20to%20be%20used%20in%20the%20second%20step%20which%20only%0Arelies%20on%20a%20language%20model%20to%20provide%20the%20answer.%20We%20find%20that%20the%20second%0Apipeline%20allows%20us%20to%20obtain%20good%20results%20with%20SAR%20images%20alone.%20We%20then%20try%0Avarious%20types%20of%20fusion%20methods%20to%20use%20SAR%20and%20optical%20images%20together%2C%20finding%0Athat%20a%20fusion%20at%20the%20decision%20level%20achieves%20the%20best%20results%20on%20the%20proposed%0Adataset.%20We%20show%20that%20SAR%20data%20offers%20additional%20information%20when%20fused%20with%0Athe%20optical%20modality%2C%20particularly%20for%20questions%20related%20to%20specific%20land%20cover%0Aclasses%2C%20such%20as%20water%20areas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAR%2520Strikes%2520Back%253A%2520A%2520New%2520Hope%2520for%2520RSVQA%26entry.906535625%3DLucrezia%2520Tosato%2520and%2520Flora%2520Weissgerber%2520and%2520Laurent%2520Wendling%2520and%2520Sylvain%2520Lobry%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520visual%2520question%2520answering%2520%2528RSVQA%2529%2520is%2520a%2520task%2520that%2520automatically%250Aextracts%2520information%2520from%2520satellite%2520images%2520and%2520processes%2520a%2520question%2520to%2520predict%250Athe%2520answer%2520from%2520the%2520images%2520in%2520textual%2520form%252C%2520helping%2520with%2520the%2520interpretation%2520of%250Athe%2520image.%2520While%2520different%2520methods%2520have%2520been%2520proposed%2520to%2520extract%2520information%250Afrom%2520optical%2520images%2520with%2520different%2520spectral%2520bands%2520and%2520resolutions%252C%2520no%2520method%250Ahas%2520been%2520proposed%2520to%2520answer%2520questions%2520from%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%250Aimages.%2520SAR%2520images%2520capture%2520electromagnetic%2520information%2520from%2520the%2520scene%252C%2520and%2520are%250Aless%2520affected%2520by%2520atmospheric%2520conditions%252C%2520such%2520as%2520clouds.%2520In%2520this%2520work%252C%2520our%250Aobjective%2520is%2520to%2520introduce%2520SAR%2520in%2520the%2520RSVQA%2520task%252C%2520finding%2520the%2520best%2520way%2520to%2520use%250Athis%2520modality.%2520In%2520our%2520research%252C%2520we%2520carry%2520out%2520a%2520study%2520on%2520different%2520pipelines%2520for%250Athe%2520task%2520of%2520RSVQA%2520taking%2520into%2520account%2520information%2520from%2520both%2520SAR%2520and%2520optical%250Adata.%2520To%2520this%2520purpose%252C%2520we%2520also%2520present%2520a%2520dataset%2520that%2520allows%2520for%2520the%250Aintroduction%2520of%2520SAR%2520images%2520in%2520the%2520RSVQA%2520framework.%2520We%2520propose%2520two%2520different%250Amodels%2520to%2520include%2520the%2520SAR%2520modality.%2520The%2520first%2520one%2520is%2520an%2520end-to-end%2520method%2520in%250Awhich%2520we%2520add%2520an%2520additional%2520encoder%2520for%2520the%2520SAR%2520modality.%2520In%2520the%2520second%250Aapproach%252C%2520we%2520build%2520on%2520a%2520two-stage%2520framework.%2520First%252C%2520relevant%2520information%2520is%250Aextracted%2520from%2520SAR%2520and%252C%2520optionally%252C%2520optical%2520data.%2520This%2520information%2520is%2520then%250Atranslated%2520into%2520natural%2520language%2520to%2520be%2520used%2520in%2520the%2520second%2520step%2520which%2520only%250Arelies%2520on%2520a%2520language%2520model%2520to%2520provide%2520the%2520answer.%2520We%2520find%2520that%2520the%2520second%250Apipeline%2520allows%2520us%2520to%2520obtain%2520good%2520results%2520with%2520SAR%2520images%2520alone.%2520We%2520then%2520try%250Avarious%2520types%2520of%2520fusion%2520methods%2520to%2520use%2520SAR%2520and%2520optical%2520images%2520together%252C%2520finding%250Athat%2520a%2520fusion%2520at%2520the%2520decision%2520level%2520achieves%2520the%2520best%2520results%2520on%2520the%2520proposed%250Adataset.%2520We%2520show%2520that%2520SAR%2520data%2520offers%2520additional%2520information%2520when%2520fused%2520with%250Athe%2520optical%2520modality%252C%2520particularly%2520for%2520questions%2520related%2520to%2520specific%2520land%2520cover%250Aclasses%252C%2520such%2520as%2520water%2520areas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAR%20Strikes%20Back%3A%20A%20New%20Hope%20for%20RSVQA&entry.906535625=Lucrezia%20Tosato%20and%20Flora%20Weissgerber%20and%20Laurent%20Wendling%20and%20Sylvain%20Lobry&entry.1292438233=%20%20Remote%20sensing%20visual%20question%20answering%20%28RSVQA%29%20is%20a%20task%20that%20automatically%0Aextracts%20information%20from%20satellite%20images%20and%20processes%20a%20question%20to%20predict%0Athe%20answer%20from%20the%20images%20in%20textual%20form%2C%20helping%20with%20the%20interpretation%20of%0Athe%20image.%20While%20different%20methods%20have%20been%20proposed%20to%20extract%20information%0Afrom%20optical%20images%20with%20different%20spectral%20bands%20and%20resolutions%2C%20no%20method%0Ahas%20been%20proposed%20to%20answer%20questions%20from%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Aimages.%20SAR%20images%20capture%20electromagnetic%20information%20from%20the%20scene%2C%20and%20are%0Aless%20affected%20by%20atmospheric%20conditions%2C%20such%20as%20clouds.%20In%20this%20work%2C%20our%0Aobjective%20is%20to%20introduce%20SAR%20in%20the%20RSVQA%20task%2C%20finding%20the%20best%20way%20to%20use%0Athis%20modality.%20In%20our%20research%2C%20we%20carry%20out%20a%20study%20on%20different%20pipelines%20for%0Athe%20task%20of%20RSVQA%20taking%20into%20account%20information%20from%20both%20SAR%20and%20optical%0Adata.%20To%20this%20purpose%2C%20we%20also%20present%20a%20dataset%20that%20allows%20for%20the%0Aintroduction%20of%20SAR%20images%20in%20the%20RSVQA%20framework.%20We%20propose%20two%20different%0Amodels%20to%20include%20the%20SAR%20modality.%20The%20first%20one%20is%20an%20end-to-end%20method%20in%0Awhich%20we%20add%20an%20additional%20encoder%20for%20the%20SAR%20modality.%20In%20the%20second%0Aapproach%2C%20we%20build%20on%20a%20two-stage%20framework.%20First%2C%20relevant%20information%20is%0Aextracted%20from%20SAR%20and%2C%20optionally%2C%20optical%20data.%20This%20information%20is%20then%0Atranslated%20into%20natural%20language%20to%20be%20used%20in%20the%20second%20step%20which%20only%0Arelies%20on%20a%20language%20model%20to%20provide%20the%20answer.%20We%20find%20that%20the%20second%0Apipeline%20allows%20us%20to%20obtain%20good%20results%20with%20SAR%20images%20alone.%20We%20then%20try%0Avarious%20types%20of%20fusion%20methods%20to%20use%20SAR%20and%20optical%20images%20together%2C%20finding%0Athat%20a%20fusion%20at%20the%20decision%20level%20achieves%20the%20best%20results%20on%20the%20proposed%0Adataset.%20We%20show%20that%20SAR%20data%20offers%20additional%20information%20when%20fused%20with%0Athe%20optical%20modality%2C%20particularly%20for%20questions%20related%20to%20specific%20land%20cover%0Aclasses%2C%20such%20as%20water%20areas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08131v1&entry.124074799=Read"},
{"title": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models", "author": "Xuemei Tang and Xufeng Duan and Zhenguang G. Cai", "abstract": "  The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.\n", "link": "http://arxiv.org/abs/2412.13612v2", "date": "2025-01-14", "relevancy": 2.1178, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4344}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Good%20Literature%20Review%20Writers%3F%20Evaluating%20the%20Literature%0A%20%20Review%20Writing%20Ability%20of%20Large%20Language%20Models&body=Title%3A%20Are%20LLMs%20Good%20Literature%20Review%20Writers%3F%20Evaluating%20the%20Literature%0A%20%20Review%20Writing%20Ability%20of%20Large%20Language%20Models%0AAuthor%3A%20Xuemei%20Tang%20and%20Xufeng%20Duan%20and%20Zhenguang%20G.%20Cai%0AAbstract%3A%20%20%20The%20literature%20review%20is%20a%20crucial%20form%20of%20academic%20writing%20that%20involves%0Acomplex%20processes%20of%20literature%20collection%2C%20organization%2C%20and%20summarization.%0AThe%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20introduced%20promising%20tools%20to%0Aautomate%20these%20processes.%20However%2C%20their%20actual%20capabilities%20in%20writing%0Acomprehensive%20literature%20reviews%20remain%20underexplored%2C%20such%20as%20whether%20they%20can%0Agenerate%20accurate%20and%20reliable%20references.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Aframework%20to%20assess%20the%20literature%20review%20writing%20ability%20of%20LLMs%0Aautomatically.%20We%20evaluate%20the%20performance%20of%20LLMs%20across%20three%20tasks%3A%0Agenerating%20references%2C%20writing%20abstracts%2C%20and%20writing%20literature%20reviews.%20We%0Aemploy%20external%20tools%20for%20a%20multidimensional%20evaluation%2C%20which%20includes%0Aassessing%20hallucination%20rates%20in%20references%2C%20semantic%20coverage%2C%20and%20factual%0Aconsistency%20with%20human-written%20context.%20By%20analyzing%20the%20experimental%20results%2C%0Awe%20find%20that%2C%20despite%20advancements%2C%20even%20the%20most%20sophisticated%20models%20still%0Acannot%20avoid%20generating%20hallucinated%20references.%20Additionally%2C%20different%20models%0Aexhibit%20varying%20performance%20in%20literature%20review%20writing%20across%20different%0Adisciplines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Good%2520Literature%2520Review%2520Writers%253F%2520Evaluating%2520the%2520Literature%250A%2520%2520Review%2520Writing%2520Ability%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DXuemei%2520Tang%2520and%2520Xufeng%2520Duan%2520and%2520Zhenguang%2520G.%2520Cai%26entry.1292438233%3D%2520%2520The%2520literature%2520review%2520is%2520a%2520crucial%2520form%2520of%2520academic%2520writing%2520that%2520involves%250Acomplex%2520processes%2520of%2520literature%2520collection%252C%2520organization%252C%2520and%2520summarization.%250AThe%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520introduced%2520promising%2520tools%2520to%250Aautomate%2520these%2520processes.%2520However%252C%2520their%2520actual%2520capabilities%2520in%2520writing%250Acomprehensive%2520literature%2520reviews%2520remain%2520underexplored%252C%2520such%2520as%2520whether%2520they%2520can%250Agenerate%2520accurate%2520and%2520reliable%2520references.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%250Aframework%2520to%2520assess%2520the%2520literature%2520review%2520writing%2520ability%2520of%2520LLMs%250Aautomatically.%2520We%2520evaluate%2520the%2520performance%2520of%2520LLMs%2520across%2520three%2520tasks%253A%250Agenerating%2520references%252C%2520writing%2520abstracts%252C%2520and%2520writing%2520literature%2520reviews.%2520We%250Aemploy%2520external%2520tools%2520for%2520a%2520multidimensional%2520evaluation%252C%2520which%2520includes%250Aassessing%2520hallucination%2520rates%2520in%2520references%252C%2520semantic%2520coverage%252C%2520and%2520factual%250Aconsistency%2520with%2520human-written%2520context.%2520By%2520analyzing%2520the%2520experimental%2520results%252C%250Awe%2520find%2520that%252C%2520despite%2520advancements%252C%2520even%2520the%2520most%2520sophisticated%2520models%2520still%250Acannot%2520avoid%2520generating%2520hallucinated%2520references.%2520Additionally%252C%2520different%2520models%250Aexhibit%2520varying%2520performance%2520in%2520literature%2520review%2520writing%2520across%2520different%250Adisciplines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Good%20Literature%20Review%20Writers%3F%20Evaluating%20the%20Literature%0A%20%20Review%20Writing%20Ability%20of%20Large%20Language%20Models&entry.906535625=Xuemei%20Tang%20and%20Xufeng%20Duan%20and%20Zhenguang%20G.%20Cai&entry.1292438233=%20%20The%20literature%20review%20is%20a%20crucial%20form%20of%20academic%20writing%20that%20involves%0Acomplex%20processes%20of%20literature%20collection%2C%20organization%2C%20and%20summarization.%0AThe%20emergence%20of%20large%20language%20models%20%28LLMs%29%20has%20introduced%20promising%20tools%20to%0Aautomate%20these%20processes.%20However%2C%20their%20actual%20capabilities%20in%20writing%0Acomprehensive%20literature%20reviews%20remain%20underexplored%2C%20such%20as%20whether%20they%20can%0Agenerate%20accurate%20and%20reliable%20references.%20To%20address%20this%20gap%2C%20we%20propose%20a%0Aframework%20to%20assess%20the%20literature%20review%20writing%20ability%20of%20LLMs%0Aautomatically.%20We%20evaluate%20the%20performance%20of%20LLMs%20across%20three%20tasks%3A%0Agenerating%20references%2C%20writing%20abstracts%2C%20and%20writing%20literature%20reviews.%20We%0Aemploy%20external%20tools%20for%20a%20multidimensional%20evaluation%2C%20which%20includes%0Aassessing%20hallucination%20rates%20in%20references%2C%20semantic%20coverage%2C%20and%20factual%0Aconsistency%20with%20human-written%20context.%20By%20analyzing%20the%20experimental%20results%2C%0Awe%20find%20that%2C%20despite%20advancements%2C%20even%20the%20most%20sophisticated%20models%20still%0Acannot%20avoid%20generating%20hallucinated%20references.%20Additionally%2C%20different%20models%0Aexhibit%20varying%20performance%20in%20literature%20review%20writing%20across%20different%0Adisciplines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13612v2&entry.124074799=Read"},
{"title": "Self-Attentive Spatio-Temporal Calibration for Precise Intermediate\n  Layer Matching in ANN-to-SNN Distillation", "author": "Di Hong and Yueming Wang", "abstract": "  Spiking Neural Networks (SNNs) are promising for low-power computation due to\ntheir event-driven mechanism but often suffer from lower accuracy compared to\nArtificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can\nimprove SNN performance, but previous methods either focus solely on label\ninformation, missing valuable intermediate layer features, or use a layer-wise\napproach that neglects spatial and temporal semantic inconsistencies, leading\nto performance degradation.To address these limitations, we propose a novel\nmethod called self-attentive spatio-temporal calibration (SASTC). SASTC uses\nself-attention to identify semantically aligned layer pairs between ANN and\nSNN, both spatially and temporally. This enables the autonomous transfer of\nrelevant semantic information. Extensive experiments show that SASTC\noutperforms existing methods, effectively solving the mismatching problem.\nSuperior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with\n2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and\n97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This\nmarks the first time SNNs have outperformed ANNs on both CIFAR-10 and\nCIFAR-100, shedding the new light on the potential applications of SNNs.\n", "link": "http://arxiv.org/abs/2501.08049v1", "date": "2025-01-14", "relevancy": 2.1079, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5345}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5239}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Attentive%20Spatio-Temporal%20Calibration%20for%20Precise%20Intermediate%0A%20%20Layer%20Matching%20in%20ANN-to-SNN%20Distillation&body=Title%3A%20Self-Attentive%20Spatio-Temporal%20Calibration%20for%20Precise%20Intermediate%0A%20%20Layer%20Matching%20in%20ANN-to-SNN%20Distillation%0AAuthor%3A%20Di%20Hong%20and%20Yueming%20Wang%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20promising%20for%20low-power%20computation%20due%20to%0Atheir%20event-driven%20mechanism%20but%20often%20suffer%20from%20lower%20accuracy%20compared%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29.%20ANN-to-SNN%20knowledge%20distillation%20can%0Aimprove%20SNN%20performance%2C%20but%20previous%20methods%20either%20focus%20solely%20on%20label%0Ainformation%2C%20missing%20valuable%20intermediate%20layer%20features%2C%20or%20use%20a%20layer-wise%0Aapproach%20that%20neglects%20spatial%20and%20temporal%20semantic%20inconsistencies%2C%20leading%0Ato%20performance%20degradation.To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20self-attentive%20spatio-temporal%20calibration%20%28SASTC%29.%20SASTC%20uses%0Aself-attention%20to%20identify%20semantically%20aligned%20layer%20pairs%20between%20ANN%20and%0ASNN%2C%20both%20spatially%20and%20temporally.%20This%20enables%20the%20autonomous%20transfer%20of%0Arelevant%20semantic%20information.%20Extensive%20experiments%20show%20that%20SASTC%0Aoutperforms%20existing%20methods%2C%20effectively%20solving%20the%20mismatching%20problem.%0ASuperior%20accuracy%20results%20include%2095.12%25%20on%20CIFAR-10%2C%2079.40%25%20on%20CIFAR-100%20with%0A2%20time%20steps%2C%20and%2068.69%25%20on%20ImageNet%20with%204%20time%20steps%20for%20static%20datasets%2C%20and%0A97.92%25%20on%20DVS-Gesture%20and%2083.60%25%20on%20DVS-CIFAR10%20for%20neuromorphic%20datasets.%20This%0Amarks%20the%20first%20time%20SNNs%20have%20outperformed%20ANNs%20on%20both%20CIFAR-10%20and%0ACIFAR-100%2C%20shedding%20the%20new%20light%20on%20the%20potential%20applications%20of%20SNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Attentive%2520Spatio-Temporal%2520Calibration%2520for%2520Precise%2520Intermediate%250A%2520%2520Layer%2520Matching%2520in%2520ANN-to-SNN%2520Distillation%26entry.906535625%3DDi%2520Hong%2520and%2520Yueming%2520Wang%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520promising%2520for%2520low-power%2520computation%2520due%2520to%250Atheir%2520event-driven%2520mechanism%2520but%2520often%2520suffer%2520from%2520lower%2520accuracy%2520compared%2520to%250AArtificial%2520Neural%2520Networks%2520%2528ANNs%2529.%2520ANN-to-SNN%2520knowledge%2520distillation%2520can%250Aimprove%2520SNN%2520performance%252C%2520but%2520previous%2520methods%2520either%2520focus%2520solely%2520on%2520label%250Ainformation%252C%2520missing%2520valuable%2520intermediate%2520layer%2520features%252C%2520or%2520use%2520a%2520layer-wise%250Aapproach%2520that%2520neglects%2520spatial%2520and%2520temporal%2520semantic%2520inconsistencies%252C%2520leading%250Ato%2520performance%2520degradation.To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%250Amethod%2520called%2520self-attentive%2520spatio-temporal%2520calibration%2520%2528SASTC%2529.%2520SASTC%2520uses%250Aself-attention%2520to%2520identify%2520semantically%2520aligned%2520layer%2520pairs%2520between%2520ANN%2520and%250ASNN%252C%2520both%2520spatially%2520and%2520temporally.%2520This%2520enables%2520the%2520autonomous%2520transfer%2520of%250Arelevant%2520semantic%2520information.%2520Extensive%2520experiments%2520show%2520that%2520SASTC%250Aoutperforms%2520existing%2520methods%252C%2520effectively%2520solving%2520the%2520mismatching%2520problem.%250ASuperior%2520accuracy%2520results%2520include%252095.12%2525%2520on%2520CIFAR-10%252C%252079.40%2525%2520on%2520CIFAR-100%2520with%250A2%2520time%2520steps%252C%2520and%252068.69%2525%2520on%2520ImageNet%2520with%25204%2520time%2520steps%2520for%2520static%2520datasets%252C%2520and%250A97.92%2525%2520on%2520DVS-Gesture%2520and%252083.60%2525%2520on%2520DVS-CIFAR10%2520for%2520neuromorphic%2520datasets.%2520This%250Amarks%2520the%2520first%2520time%2520SNNs%2520have%2520outperformed%2520ANNs%2520on%2520both%2520CIFAR-10%2520and%250ACIFAR-100%252C%2520shedding%2520the%2520new%2520light%2520on%2520the%2520potential%2520applications%2520of%2520SNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Attentive%20Spatio-Temporal%20Calibration%20for%20Precise%20Intermediate%0A%20%20Layer%20Matching%20in%20ANN-to-SNN%20Distillation&entry.906535625=Di%20Hong%20and%20Yueming%20Wang&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20promising%20for%20low-power%20computation%20due%20to%0Atheir%20event-driven%20mechanism%20but%20often%20suffer%20from%20lower%20accuracy%20compared%20to%0AArtificial%20Neural%20Networks%20%28ANNs%29.%20ANN-to-SNN%20knowledge%20distillation%20can%0Aimprove%20SNN%20performance%2C%20but%20previous%20methods%20either%20focus%20solely%20on%20label%0Ainformation%2C%20missing%20valuable%20intermediate%20layer%20features%2C%20or%20use%20a%20layer-wise%0Aapproach%20that%20neglects%20spatial%20and%20temporal%20semantic%20inconsistencies%2C%20leading%0Ato%20performance%20degradation.To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%0Amethod%20called%20self-attentive%20spatio-temporal%20calibration%20%28SASTC%29.%20SASTC%20uses%0Aself-attention%20to%20identify%20semantically%20aligned%20layer%20pairs%20between%20ANN%20and%0ASNN%2C%20both%20spatially%20and%20temporally.%20This%20enables%20the%20autonomous%20transfer%20of%0Arelevant%20semantic%20information.%20Extensive%20experiments%20show%20that%20SASTC%0Aoutperforms%20existing%20methods%2C%20effectively%20solving%20the%20mismatching%20problem.%0ASuperior%20accuracy%20results%20include%2095.12%25%20on%20CIFAR-10%2C%2079.40%25%20on%20CIFAR-100%20with%0A2%20time%20steps%2C%20and%2068.69%25%20on%20ImageNet%20with%204%20time%20steps%20for%20static%20datasets%2C%20and%0A97.92%25%20on%20DVS-Gesture%20and%2083.60%25%20on%20DVS-CIFAR10%20for%20neuromorphic%20datasets.%20This%0Amarks%20the%20first%20time%20SNNs%20have%20outperformed%20ANNs%20on%20both%20CIFAR-10%20and%0ACIFAR-100%2C%20shedding%20the%20new%20light%20on%20the%20potential%20applications%20of%20SNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08049v1&entry.124074799=Read"},
{"title": "Multiple-Input Variational Auto-Encoder for Anomaly Detection in\n  Heterogeneous Data", "author": "Phai Vu Dinh and Diep N. Nguyen and Dinh Thai Hoang and Quang Uy Nguyen and Eryk Dutkiewicz", "abstract": "  Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in\nclassification, and intrusion/threat detection in cybersecurity. However, most\nexisting methods face challenges of heterogeneity amongst feature subsets posed\nby non-independent and identically distributed (non-IID) data. We propose a\nnovel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)\nto address this. MIAEAD assigns an anomaly score to each feature subset of a\ndata sample to indicate its likelihood of being an anomaly. This is done by\nusing the reconstruction error of its sub-encoder as the anomaly score. All\nsub-encoders are then simultaneously trained using unsupervised learning to\ndetermine the anomaly scores of feature subsets. The final AUC of MIAEAD is\ncalculated for each sub-dataset, and the maximum AUC obtained among the\nsub-datasets is selected. To leverage the modelling of the distribution of\nnormal data to identify anomalies of the generative models, we develop a novel\nneural network architecture/model called Multiple-Input Variational\nAuto-Encoder (MIVAE). MIVAE can process feature subsets through its\nsub-encoders before learning distribution of normal data in the latent space.\nThis allows MIVAE to identify anomalies that deviate from the learned\ndistribution. We theoretically prove that the difference in the average anomaly\nscore between normal samples and anomalies obtained by the proposed MIVAE is\ngreater than that of the Variational Auto-Encoder (VAEAD), resulting in a\nhigher AUC for MIVAE. Extensive experiments on eight real-world anomaly\ndatasets demonstrate the superior performance of MIAEAD and MIVAE over\nconventional methods and the state-of-the-art unsupervised models, by up to 6%\nin terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when\napplied to feature subsets with low heterogeneity based on the coefficient of\nvariation (CV) score.\n", "link": "http://arxiv.org/abs/2501.08149v1", "date": "2025-01-14", "relevancy": 2.0884, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5329}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple-Input%20Variational%20Auto-Encoder%20for%20Anomaly%20Detection%20in%0A%20%20Heterogeneous%20Data&body=Title%3A%20Multiple-Input%20Variational%20Auto-Encoder%20for%20Anomaly%20Detection%20in%0A%20%20Heterogeneous%20Data%0AAuthor%3A%20Phai%20Vu%20Dinh%20and%20Diep%20N.%20Nguyen%20and%20Dinh%20Thai%20Hoang%20and%20Quang%20Uy%20Nguyen%20and%20Eryk%20Dutkiewicz%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20plays%20a%20pivotal%20role%20in%20AI%20applications%2C%20e.g.%2C%20in%0Aclassification%2C%20and%20intrusion/threat%20detection%20in%20cybersecurity.%20However%2C%20most%0Aexisting%20methods%20face%20challenges%20of%20heterogeneity%20amongst%20feature%20subsets%20posed%0Aby%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data.%20We%20propose%20a%0Anovel%20neural%20network%20model%20called%20Multiple-Input%20Auto-Encoder%20for%20AD%20%28MIAEAD%29%0Ato%20address%20this.%20MIAEAD%20assigns%20an%20anomaly%20score%20to%20each%20feature%20subset%20of%20a%0Adata%20sample%20to%20indicate%20its%20likelihood%20of%20being%20an%20anomaly.%20This%20is%20done%20by%0Ausing%20the%20reconstruction%20error%20of%20its%20sub-encoder%20as%20the%20anomaly%20score.%20All%0Asub-encoders%20are%20then%20simultaneously%20trained%20using%20unsupervised%20learning%20to%0Adetermine%20the%20anomaly%20scores%20of%20feature%20subsets.%20The%20final%20AUC%20of%20MIAEAD%20is%0Acalculated%20for%20each%20sub-dataset%2C%20and%20the%20maximum%20AUC%20obtained%20among%20the%0Asub-datasets%20is%20selected.%20To%20leverage%20the%20modelling%20of%20the%20distribution%20of%0Anormal%20data%20to%20identify%20anomalies%20of%20the%20generative%20models%2C%20we%20develop%20a%20novel%0Aneural%20network%20architecture/model%20called%20Multiple-Input%20Variational%0AAuto-Encoder%20%28MIVAE%29.%20MIVAE%20can%20process%20feature%20subsets%20through%20its%0Asub-encoders%20before%20learning%20distribution%20of%20normal%20data%20in%20the%20latent%20space.%0AThis%20allows%20MIVAE%20to%20identify%20anomalies%20that%20deviate%20from%20the%20learned%0Adistribution.%20We%20theoretically%20prove%20that%20the%20difference%20in%20the%20average%20anomaly%0Ascore%20between%20normal%20samples%20and%20anomalies%20obtained%20by%20the%20proposed%20MIVAE%20is%0Agreater%20than%20that%20of%20the%20Variational%20Auto-Encoder%20%28VAEAD%29%2C%20resulting%20in%20a%0Ahigher%20AUC%20for%20MIVAE.%20Extensive%20experiments%20on%20eight%20real-world%20anomaly%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20MIAEAD%20and%20MIVAE%20over%0Aconventional%20methods%20and%20the%20state-of-the-art%20unsupervised%20models%2C%20by%20up%20to%206%25%0Ain%20terms%20of%20AUC%20score.%20Alternatively%2C%20MIAEAD%20and%20MIVAE%20have%20a%20high%20AUC%20when%0Aapplied%20to%20feature%20subsets%20with%20low%20heterogeneity%20based%20on%20the%20coefficient%20of%0Avariation%20%28CV%29%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple-Input%2520Variational%2520Auto-Encoder%2520for%2520Anomaly%2520Detection%2520in%250A%2520%2520Heterogeneous%2520Data%26entry.906535625%3DPhai%2520Vu%2520Dinh%2520and%2520Diep%2520N.%2520Nguyen%2520and%2520Dinh%2520Thai%2520Hoang%2520and%2520Quang%2520Uy%2520Nguyen%2520and%2520Eryk%2520Dutkiewicz%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520%2528AD%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520AI%2520applications%252C%2520e.g.%252C%2520in%250Aclassification%252C%2520and%2520intrusion/threat%2520detection%2520in%2520cybersecurity.%2520However%252C%2520most%250Aexisting%2520methods%2520face%2520challenges%2520of%2520heterogeneity%2520amongst%2520feature%2520subsets%2520posed%250Aby%2520non-independent%2520and%2520identically%2520distributed%2520%2528non-IID%2529%2520data.%2520We%2520propose%2520a%250Anovel%2520neural%2520network%2520model%2520called%2520Multiple-Input%2520Auto-Encoder%2520for%2520AD%2520%2528MIAEAD%2529%250Ato%2520address%2520this.%2520MIAEAD%2520assigns%2520an%2520anomaly%2520score%2520to%2520each%2520feature%2520subset%2520of%2520a%250Adata%2520sample%2520to%2520indicate%2520its%2520likelihood%2520of%2520being%2520an%2520anomaly.%2520This%2520is%2520done%2520by%250Ausing%2520the%2520reconstruction%2520error%2520of%2520its%2520sub-encoder%2520as%2520the%2520anomaly%2520score.%2520All%250Asub-encoders%2520are%2520then%2520simultaneously%2520trained%2520using%2520unsupervised%2520learning%2520to%250Adetermine%2520the%2520anomaly%2520scores%2520of%2520feature%2520subsets.%2520The%2520final%2520AUC%2520of%2520MIAEAD%2520is%250Acalculated%2520for%2520each%2520sub-dataset%252C%2520and%2520the%2520maximum%2520AUC%2520obtained%2520among%2520the%250Asub-datasets%2520is%2520selected.%2520To%2520leverage%2520the%2520modelling%2520of%2520the%2520distribution%2520of%250Anormal%2520data%2520to%2520identify%2520anomalies%2520of%2520the%2520generative%2520models%252C%2520we%2520develop%2520a%2520novel%250Aneural%2520network%2520architecture/model%2520called%2520Multiple-Input%2520Variational%250AAuto-Encoder%2520%2528MIVAE%2529.%2520MIVAE%2520can%2520process%2520feature%2520subsets%2520through%2520its%250Asub-encoders%2520before%2520learning%2520distribution%2520of%2520normal%2520data%2520in%2520the%2520latent%2520space.%250AThis%2520allows%2520MIVAE%2520to%2520identify%2520anomalies%2520that%2520deviate%2520from%2520the%2520learned%250Adistribution.%2520We%2520theoretically%2520prove%2520that%2520the%2520difference%2520in%2520the%2520average%2520anomaly%250Ascore%2520between%2520normal%2520samples%2520and%2520anomalies%2520obtained%2520by%2520the%2520proposed%2520MIVAE%2520is%250Agreater%2520than%2520that%2520of%2520the%2520Variational%2520Auto-Encoder%2520%2528VAEAD%2529%252C%2520resulting%2520in%2520a%250Ahigher%2520AUC%2520for%2520MIVAE.%2520Extensive%2520experiments%2520on%2520eight%2520real-world%2520anomaly%250Adatasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520MIAEAD%2520and%2520MIVAE%2520over%250Aconventional%2520methods%2520and%2520the%2520state-of-the-art%2520unsupervised%2520models%252C%2520by%2520up%2520to%25206%2525%250Ain%2520terms%2520of%2520AUC%2520score.%2520Alternatively%252C%2520MIAEAD%2520and%2520MIVAE%2520have%2520a%2520high%2520AUC%2520when%250Aapplied%2520to%2520feature%2520subsets%2520with%2520low%2520heterogeneity%2520based%2520on%2520the%2520coefficient%2520of%250Avariation%2520%2528CV%2529%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple-Input%20Variational%20Auto-Encoder%20for%20Anomaly%20Detection%20in%0A%20%20Heterogeneous%20Data&entry.906535625=Phai%20Vu%20Dinh%20and%20Diep%20N.%20Nguyen%20and%20Dinh%20Thai%20Hoang%20and%20Quang%20Uy%20Nguyen%20and%20Eryk%20Dutkiewicz&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20plays%20a%20pivotal%20role%20in%20AI%20applications%2C%20e.g.%2C%20in%0Aclassification%2C%20and%20intrusion/threat%20detection%20in%20cybersecurity.%20However%2C%20most%0Aexisting%20methods%20face%20challenges%20of%20heterogeneity%20amongst%20feature%20subsets%20posed%0Aby%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data.%20We%20propose%20a%0Anovel%20neural%20network%20model%20called%20Multiple-Input%20Auto-Encoder%20for%20AD%20%28MIAEAD%29%0Ato%20address%20this.%20MIAEAD%20assigns%20an%20anomaly%20score%20to%20each%20feature%20subset%20of%20a%0Adata%20sample%20to%20indicate%20its%20likelihood%20of%20being%20an%20anomaly.%20This%20is%20done%20by%0Ausing%20the%20reconstruction%20error%20of%20its%20sub-encoder%20as%20the%20anomaly%20score.%20All%0Asub-encoders%20are%20then%20simultaneously%20trained%20using%20unsupervised%20learning%20to%0Adetermine%20the%20anomaly%20scores%20of%20feature%20subsets.%20The%20final%20AUC%20of%20MIAEAD%20is%0Acalculated%20for%20each%20sub-dataset%2C%20and%20the%20maximum%20AUC%20obtained%20among%20the%0Asub-datasets%20is%20selected.%20To%20leverage%20the%20modelling%20of%20the%20distribution%20of%0Anormal%20data%20to%20identify%20anomalies%20of%20the%20generative%20models%2C%20we%20develop%20a%20novel%0Aneural%20network%20architecture/model%20called%20Multiple-Input%20Variational%0AAuto-Encoder%20%28MIVAE%29.%20MIVAE%20can%20process%20feature%20subsets%20through%20its%0Asub-encoders%20before%20learning%20distribution%20of%20normal%20data%20in%20the%20latent%20space.%0AThis%20allows%20MIVAE%20to%20identify%20anomalies%20that%20deviate%20from%20the%20learned%0Adistribution.%20We%20theoretically%20prove%20that%20the%20difference%20in%20the%20average%20anomaly%0Ascore%20between%20normal%20samples%20and%20anomalies%20obtained%20by%20the%20proposed%20MIVAE%20is%0Agreater%20than%20that%20of%20the%20Variational%20Auto-Encoder%20%28VAEAD%29%2C%20resulting%20in%20a%0Ahigher%20AUC%20for%20MIVAE.%20Extensive%20experiments%20on%20eight%20real-world%20anomaly%0Adatasets%20demonstrate%20the%20superior%20performance%20of%20MIAEAD%20and%20MIVAE%20over%0Aconventional%20methods%20and%20the%20state-of-the-art%20unsupervised%20models%2C%20by%20up%20to%206%25%0Ain%20terms%20of%20AUC%20score.%20Alternatively%2C%20MIAEAD%20and%20MIVAE%20have%20a%20high%20AUC%20when%0Aapplied%20to%20feature%20subsets%20with%20low%20heterogeneity%20based%20on%20the%20coefficient%20of%0Avariation%20%28CV%29%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08149v1&entry.124074799=Read"},
{"title": "READ: Reinforcement-based Adversarial Learning for Text Classification\n  with Limited Labeled Data", "author": "Rohit Sharma and Shanu Kumar and Avinash Kumar", "abstract": "  Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets.\n", "link": "http://arxiv.org/abs/2501.08035v1", "date": "2025-01-14", "relevancy": 2.079, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5285}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20READ%3A%20Reinforcement-based%20Adversarial%20Learning%20for%20Text%20Classification%0A%20%20with%20Limited%20Labeled%20Data&body=Title%3A%20READ%3A%20Reinforcement-based%20Adversarial%20Learning%20for%20Text%20Classification%0A%20%20with%20Limited%20Labeled%20Data%0AAuthor%3A%20Rohit%20Sharma%20and%20Shanu%20Kumar%20and%20Avinash%20Kumar%0AAbstract%3A%20%20%20Pre-trained%20transformer%20models%20such%20as%20BERT%20have%20shown%20massive%20gains%20across%0Amany%20text%20classification%20tasks.%20However%2C%20these%20models%20usually%20need%20enormous%0Alabeled%20data%20to%20achieve%20impressive%20performances.%20Obtaining%20labeled%20data%20is%0Aoften%20expensive%20and%20time-consuming%2C%20whereas%20collecting%20unlabeled%20data%20using%0Asome%20heuristics%20is%20relatively%20much%20cheaper%20for%20any%20task.%20Therefore%2C%20this%20paper%0Aproposes%20a%20method%20that%20encapsulates%20reinforcement%20learning-based%20text%0Ageneration%20and%20semi-supervised%20adversarial%20learning%20approaches%20in%20a%20novel%20way%0Ato%20improve%20the%20model%27s%20performance.%20Our%20method%20READ%2C%20Reinforcement-based%0AAdversarial%20learning%2C%20utilizes%20an%20unlabeled%20dataset%20to%20generate%20diverse%0Asynthetic%20text%20through%20reinforcement%20learning%2C%20improving%20the%20model%27s%0Ageneralization%20capability%20using%20adversarial%20learning.%20Our%20experimental%20results%0Ashow%20that%20READ%20outperforms%20the%20existing%20state-of-art%20methods%20on%20multiple%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREAD%253A%2520Reinforcement-based%2520Adversarial%2520Learning%2520for%2520Text%2520Classification%250A%2520%2520with%2520Limited%2520Labeled%2520Data%26entry.906535625%3DRohit%2520Sharma%2520and%2520Shanu%2520Kumar%2520and%2520Avinash%2520Kumar%26entry.1292438233%3D%2520%2520Pre-trained%2520transformer%2520models%2520such%2520as%2520BERT%2520have%2520shown%2520massive%2520gains%2520across%250Amany%2520text%2520classification%2520tasks.%2520However%252C%2520these%2520models%2520usually%2520need%2520enormous%250Alabeled%2520data%2520to%2520achieve%2520impressive%2520performances.%2520Obtaining%2520labeled%2520data%2520is%250Aoften%2520expensive%2520and%2520time-consuming%252C%2520whereas%2520collecting%2520unlabeled%2520data%2520using%250Asome%2520heuristics%2520is%2520relatively%2520much%2520cheaper%2520for%2520any%2520task.%2520Therefore%252C%2520this%2520paper%250Aproposes%2520a%2520method%2520that%2520encapsulates%2520reinforcement%2520learning-based%2520text%250Ageneration%2520and%2520semi-supervised%2520adversarial%2520learning%2520approaches%2520in%2520a%2520novel%2520way%250Ato%2520improve%2520the%2520model%2527s%2520performance.%2520Our%2520method%2520READ%252C%2520Reinforcement-based%250AAdversarial%2520learning%252C%2520utilizes%2520an%2520unlabeled%2520dataset%2520to%2520generate%2520diverse%250Asynthetic%2520text%2520through%2520reinforcement%2520learning%252C%2520improving%2520the%2520model%2527s%250Ageneralization%2520capability%2520using%2520adversarial%2520learning.%2520Our%2520experimental%2520results%250Ashow%2520that%2520READ%2520outperforms%2520the%2520existing%2520state-of-art%2520methods%2520on%2520multiple%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=READ%3A%20Reinforcement-based%20Adversarial%20Learning%20for%20Text%20Classification%0A%20%20with%20Limited%20Labeled%20Data&entry.906535625=Rohit%20Sharma%20and%20Shanu%20Kumar%20and%20Avinash%20Kumar&entry.1292438233=%20%20Pre-trained%20transformer%20models%20such%20as%20BERT%20have%20shown%20massive%20gains%20across%0Amany%20text%20classification%20tasks.%20However%2C%20these%20models%20usually%20need%20enormous%0Alabeled%20data%20to%20achieve%20impressive%20performances.%20Obtaining%20labeled%20data%20is%0Aoften%20expensive%20and%20time-consuming%2C%20whereas%20collecting%20unlabeled%20data%20using%0Asome%20heuristics%20is%20relatively%20much%20cheaper%20for%20any%20task.%20Therefore%2C%20this%20paper%0Aproposes%20a%20method%20that%20encapsulates%20reinforcement%20learning-based%20text%0Ageneration%20and%20semi-supervised%20adversarial%20learning%20approaches%20in%20a%20novel%20way%0Ato%20improve%20the%20model%27s%20performance.%20Our%20method%20READ%2C%20Reinforcement-based%0AAdversarial%20learning%2C%20utilizes%20an%20unlabeled%20dataset%20to%20generate%20diverse%0Asynthetic%20text%20through%20reinforcement%20learning%2C%20improving%20the%20model%27s%0Ageneralization%20capability%20using%20adversarial%20learning.%20Our%20experimental%20results%0Ashow%20that%20READ%20outperforms%20the%20existing%20state-of-art%20methods%20on%20multiple%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08035v1&entry.124074799=Read"},
{"title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks", "author": "Fangru Lin and Shaoguang Mao and Emanuele La Malfa and Valentin Hofmann and Adrian de Wynter and Xun Wang and Si-Qing Chen and Michael Wooldridge and Janet B. Pierrehumbert and Furu Wei", "abstract": "  Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.\n", "link": "http://arxiv.org/abs/2410.11005v2", "date": "2025-01-14", "relevancy": 2.0784, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Language%2C%20Many%20Gaps%3A%20Evaluating%20Dialect%20Fairness%20and%20Robustness%20of%0A%20%20Large%20Language%20Models%20in%20Reasoning%20Tasks&body=Title%3A%20One%20Language%2C%20Many%20Gaps%3A%20Evaluating%20Dialect%20Fairness%20and%20Robustness%20of%0A%20%20Large%20Language%20Models%20in%20Reasoning%20Tasks%0AAuthor%3A%20Fangru%20Lin%20and%20Shaoguang%20Mao%20and%20Emanuele%20La%20Malfa%20and%20Valentin%20Hofmann%20and%20Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Si-Qing%20Chen%20and%20Michael%20Wooldridge%20and%20Janet%20B.%20Pierrehumbert%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Language%20is%20not%20monolithic.%20While%20benchmarks%2C%20including%20those%20designed%20for%0Amultiple%20languages%2C%20are%20often%20used%20as%20proxies%20to%20evaluate%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%2C%20they%20tend%20to%20overlook%20the%20nuances%20of%0Awithin-language%20variation%2C%20and%20thus%20fail%20to%20model%20the%20experience%20of%20speakers%20of%0Anon-standard%20dialects.%20Focusing%20on%20African%20American%20Vernacular%20English%20%28AAVE%29%2C%0Awe%20present%20the%20first%20study%20aimed%20at%20objectively%20assessing%20the%20fairness%20and%0Arobustness%20of%20LLMs%20in%20handling%20dialects%20in%20canonical%20reasoning%20tasks%2C%20including%0Aalgorithm%2C%20math%2C%20logic%2C%20and%20integrated%20reasoning.%20We%20introduce%20%5Ctextbf%7BReDial%7D%0A%28%5Ctextbf%7BRe%7Dasoning%20with%20%5Ctextbf%7BDial%7Dect%20Queries%29%2C%20a%20benchmark%20containing%0A1.2K%2B%20parallel%20query%20pairs%20in%20Standardized%20English%20and%20AAVE.%20We%20hire%20AAVE%0Aspeakers%2C%20including%20experts%20with%20computer%20science%20backgrounds%2C%20to%20rewrite%20seven%0Apopular%20benchmarks%2C%20such%20as%20HumanEval%20and%20GSM8K.%20With%20ReDial%2C%20we%20evaluate%0Awidely%20used%20LLMs%2C%20including%20GPT%2C%20Claude%2C%20Llama%2C%20Mistral%2C%20and%20the%20Phi%20model%0Afamilies.%20Our%20findings%20reveal%20that%20%5Ctextbf%7Balmost%20all%20of%20these%20widely%20used%0Amodels%20show%20significant%20brittleness%20and%20unfairness%20to%20queries%20in%20AAVE%7D.%20Our%0Awork%20establishes%20a%20systematic%20and%20objective%20framework%20for%20analyzing%20LLM%20bias%20in%0Adialectal%20queries.%20Moreover%2C%20it%20highlights%20how%20mainstream%20LLMs%20provide%20unfair%0Aservice%20to%20dialect%20speakers%20in%20reasoning%20tasks%2C%20laying%20a%20critical%20foundation%0Afor%20relevant%20future%20research.%20Code%20and%20data%20can%20be%20accessed%20at%0Ahttps%3A//github.com/fangru-lin/redial_dialect_robustness_fairness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11005v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Language%252C%2520Many%2520Gaps%253A%2520Evaluating%2520Dialect%2520Fairness%2520and%2520Robustness%2520of%250A%2520%2520Large%2520Language%2520Models%2520in%2520Reasoning%2520Tasks%26entry.906535625%3DFangru%2520Lin%2520and%2520Shaoguang%2520Mao%2520and%2520Emanuele%2520La%2520Malfa%2520and%2520Valentin%2520Hofmann%2520and%2520Adrian%2520de%2520Wynter%2520and%2520Xun%2520Wang%2520and%2520Si-Qing%2520Chen%2520and%2520Michael%2520Wooldridge%2520and%2520Janet%2520B.%2520Pierrehumbert%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Language%2520is%2520not%2520monolithic.%2520While%2520benchmarks%252C%2520including%2520those%2520designed%2520for%250Amultiple%2520languages%252C%2520are%2520often%2520used%2520as%2520proxies%2520to%2520evaluate%2520the%2520performance%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520they%2520tend%2520to%2520overlook%2520the%2520nuances%2520of%250Awithin-language%2520variation%252C%2520and%2520thus%2520fail%2520to%2520model%2520the%2520experience%2520of%2520speakers%2520of%250Anon-standard%2520dialects.%2520Focusing%2520on%2520African%2520American%2520Vernacular%2520English%2520%2528AAVE%2529%252C%250Awe%2520present%2520the%2520first%2520study%2520aimed%2520at%2520objectively%2520assessing%2520the%2520fairness%2520and%250Arobustness%2520of%2520LLMs%2520in%2520handling%2520dialects%2520in%2520canonical%2520reasoning%2520tasks%252C%2520including%250Aalgorithm%252C%2520math%252C%2520logic%252C%2520and%2520integrated%2520reasoning.%2520We%2520introduce%2520%255Ctextbf%257BReDial%257D%250A%2528%255Ctextbf%257BRe%257Dasoning%2520with%2520%255Ctextbf%257BDial%257Dect%2520Queries%2529%252C%2520a%2520benchmark%2520containing%250A1.2K%252B%2520parallel%2520query%2520pairs%2520in%2520Standardized%2520English%2520and%2520AAVE.%2520We%2520hire%2520AAVE%250Aspeakers%252C%2520including%2520experts%2520with%2520computer%2520science%2520backgrounds%252C%2520to%2520rewrite%2520seven%250Apopular%2520benchmarks%252C%2520such%2520as%2520HumanEval%2520and%2520GSM8K.%2520With%2520ReDial%252C%2520we%2520evaluate%250Awidely%2520used%2520LLMs%252C%2520including%2520GPT%252C%2520Claude%252C%2520Llama%252C%2520Mistral%252C%2520and%2520the%2520Phi%2520model%250Afamilies.%2520Our%2520findings%2520reveal%2520that%2520%255Ctextbf%257Balmost%2520all%2520of%2520these%2520widely%2520used%250Amodels%2520show%2520significant%2520brittleness%2520and%2520unfairness%2520to%2520queries%2520in%2520AAVE%257D.%2520Our%250Awork%2520establishes%2520a%2520systematic%2520and%2520objective%2520framework%2520for%2520analyzing%2520LLM%2520bias%2520in%250Adialectal%2520queries.%2520Moreover%252C%2520it%2520highlights%2520how%2520mainstream%2520LLMs%2520provide%2520unfair%250Aservice%2520to%2520dialect%2520speakers%2520in%2520reasoning%2520tasks%252C%2520laying%2520a%2520critical%2520foundation%250Afor%2520relevant%2520future%2520research.%2520Code%2520and%2520data%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/fangru-lin/redial_dialect_robustness_fairness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11005v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Language%2C%20Many%20Gaps%3A%20Evaluating%20Dialect%20Fairness%20and%20Robustness%20of%0A%20%20Large%20Language%20Models%20in%20Reasoning%20Tasks&entry.906535625=Fangru%20Lin%20and%20Shaoguang%20Mao%20and%20Emanuele%20La%20Malfa%20and%20Valentin%20Hofmann%20and%20Adrian%20de%20Wynter%20and%20Xun%20Wang%20and%20Si-Qing%20Chen%20and%20Michael%20Wooldridge%20and%20Janet%20B.%20Pierrehumbert%20and%20Furu%20Wei&entry.1292438233=%20%20Language%20is%20not%20monolithic.%20While%20benchmarks%2C%20including%20those%20designed%20for%0Amultiple%20languages%2C%20are%20often%20used%20as%20proxies%20to%20evaluate%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%2C%20they%20tend%20to%20overlook%20the%20nuances%20of%0Awithin-language%20variation%2C%20and%20thus%20fail%20to%20model%20the%20experience%20of%20speakers%20of%0Anon-standard%20dialects.%20Focusing%20on%20African%20American%20Vernacular%20English%20%28AAVE%29%2C%0Awe%20present%20the%20first%20study%20aimed%20at%20objectively%20assessing%20the%20fairness%20and%0Arobustness%20of%20LLMs%20in%20handling%20dialects%20in%20canonical%20reasoning%20tasks%2C%20including%0Aalgorithm%2C%20math%2C%20logic%2C%20and%20integrated%20reasoning.%20We%20introduce%20%5Ctextbf%7BReDial%7D%0A%28%5Ctextbf%7BRe%7Dasoning%20with%20%5Ctextbf%7BDial%7Dect%20Queries%29%2C%20a%20benchmark%20containing%0A1.2K%2B%20parallel%20query%20pairs%20in%20Standardized%20English%20and%20AAVE.%20We%20hire%20AAVE%0Aspeakers%2C%20including%20experts%20with%20computer%20science%20backgrounds%2C%20to%20rewrite%20seven%0Apopular%20benchmarks%2C%20such%20as%20HumanEval%20and%20GSM8K.%20With%20ReDial%2C%20we%20evaluate%0Awidely%20used%20LLMs%2C%20including%20GPT%2C%20Claude%2C%20Llama%2C%20Mistral%2C%20and%20the%20Phi%20model%0Afamilies.%20Our%20findings%20reveal%20that%20%5Ctextbf%7Balmost%20all%20of%20these%20widely%20used%0Amodels%20show%20significant%20brittleness%20and%20unfairness%20to%20queries%20in%20AAVE%7D.%20Our%0Awork%20establishes%20a%20systematic%20and%20objective%20framework%20for%20analyzing%20LLM%20bias%20in%0Adialectal%20queries.%20Moreover%2C%20it%20highlights%20how%20mainstream%20LLMs%20provide%20unfair%0Aservice%20to%20dialect%20speakers%20in%20reasoning%20tasks%2C%20laying%20a%20critical%20foundation%0Afor%20relevant%20future%20research.%20Code%20and%20data%20can%20be%20accessed%20at%0Ahttps%3A//github.com/fangru-lin/redial_dialect_robustness_fairness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11005v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Foundation Models in Medicine", "author": "Wasif Khan and Seowung Leem and Kyle B. See and Joshua K. Wong and Shaoting Zhang and Ruogu Fang", "abstract": "  Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.\n", "link": "http://arxiv.org/abs/2406.10729v2", "date": "2025-01-14", "relevancy": 2.0731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine%0AAuthor%3A%20Wasif%20Khan%20and%20Seowung%20Leem%20and%20Kyle%20B.%20See%20and%20Joshua%20K.%20Wong%20and%20Shaoting%20Zhang%20and%20Ruogu%20Fang%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20large-scale%20deep%20learning%20models%20that%20are%0Adeveloped%20using%20large%20datasets%20and%20self-supervised%20learning%20methods.%20These%0Amodels%20serve%20as%20a%20base%20for%20different%20downstream%20tasks%2C%20including%20healthcare.%0AFMs%20have%20been%20adopted%20with%20great%20success%20across%20various%20domains%20within%0Ahealthcare.%20Existing%20healthcare-based%20surveys%20have%20not%20yet%20included%20all%20of%0Athese%20domains.%20Therefore%2C%20we%20provide%20a%20detailed%20survey%20of%20FMs%20in%20healthcare.%20We%0Afocus%20on%20the%20history%2C%20learning%20strategies%2C%20flagship%20models%2C%20applications%2C%20and%0Achallenges%20of%20FMs.%20We%20explore%20how%20FMs%20such%20as%20the%20BERT%20and%20GPT%20families%20are%0Areshaping%20various%20healthcare%20domains%2C%20including%20clinical%20large%20language%20models%2C%0Amedical%20image%20analysis%2C%20and%20omics.%20Furthermore%2C%20we%20provide%20a%20detailed%20taxonomy%0Aof%20healthcare%20applications%20facilitated%20by%20FMs%2C%20such%20as%20clinical%20NLP%2C%20medical%0Acomputer%20vision%2C%20graph%20learning%2C%20and%20other%20biology-related%20tasks.%20Despite%20the%0Apromising%20opportunities%20FMs%20provide%2C%20they%20also%20have%20several%20associated%0Achallenges%2C%20which%20are%20explained%20in%20detail.%20We%20also%20outline%20open%20research%20issues%0Aand%20potential%20lessons%20learned%20to%20provide%20researchers%20and%20practitioners%20with%0Ainsights%20into%20the%20capabilities%20of%20FMs%20in%20healthcare%20to%20advance%20their%20deployment%0Aand%20mitigate%20associated%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10729v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Foundation%2520Models%2520in%2520Medicine%26entry.906535625%3DWasif%2520Khan%2520and%2520Seowung%2520Leem%2520and%2520Kyle%2520B.%2520See%2520and%2520Joshua%2520K.%2520Wong%2520and%2520Shaoting%2520Zhang%2520and%2520Ruogu%2520Fang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520large-scale%2520deep%2520learning%2520models%2520that%2520are%250Adeveloped%2520using%2520large%2520datasets%2520and%2520self-supervised%2520learning%2520methods.%2520These%250Amodels%2520serve%2520as%2520a%2520base%2520for%2520different%2520downstream%2520tasks%252C%2520including%2520healthcare.%250AFMs%2520have%2520been%2520adopted%2520with%2520great%2520success%2520across%2520various%2520domains%2520within%250Ahealthcare.%2520Existing%2520healthcare-based%2520surveys%2520have%2520not%2520yet%2520included%2520all%2520of%250Athese%2520domains.%2520Therefore%252C%2520we%2520provide%2520a%2520detailed%2520survey%2520of%2520FMs%2520in%2520healthcare.%2520We%250Afocus%2520on%2520the%2520history%252C%2520learning%2520strategies%252C%2520flagship%2520models%252C%2520applications%252C%2520and%250Achallenges%2520of%2520FMs.%2520We%2520explore%2520how%2520FMs%2520such%2520as%2520the%2520BERT%2520and%2520GPT%2520families%2520are%250Areshaping%2520various%2520healthcare%2520domains%252C%2520including%2520clinical%2520large%2520language%2520models%252C%250Amedical%2520image%2520analysis%252C%2520and%2520omics.%2520Furthermore%252C%2520we%2520provide%2520a%2520detailed%2520taxonomy%250Aof%2520healthcare%2520applications%2520facilitated%2520by%2520FMs%252C%2520such%2520as%2520clinical%2520NLP%252C%2520medical%250Acomputer%2520vision%252C%2520graph%2520learning%252C%2520and%2520other%2520biology-related%2520tasks.%2520Despite%2520the%250Apromising%2520opportunities%2520FMs%2520provide%252C%2520they%2520also%2520have%2520several%2520associated%250Achallenges%252C%2520which%2520are%2520explained%2520in%2520detail.%2520We%2520also%2520outline%2520open%2520research%2520issues%250Aand%2520potential%2520lessons%2520learned%2520to%2520provide%2520researchers%2520and%2520practitioners%2520with%250Ainsights%2520into%2520the%2520capabilities%2520of%2520FMs%2520in%2520healthcare%2520to%2520advance%2520their%2520deployment%250Aand%2520mitigate%2520associated%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10729v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Foundation%20Models%20in%20Medicine&entry.906535625=Wasif%20Khan%20and%20Seowung%20Leem%20and%20Kyle%20B.%20See%20and%20Joshua%20K.%20Wong%20and%20Shaoting%20Zhang%20and%20Ruogu%20Fang&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20large-scale%20deep%20learning%20models%20that%20are%0Adeveloped%20using%20large%20datasets%20and%20self-supervised%20learning%20methods.%20These%0Amodels%20serve%20as%20a%20base%20for%20different%20downstream%20tasks%2C%20including%20healthcare.%0AFMs%20have%20been%20adopted%20with%20great%20success%20across%20various%20domains%20within%0Ahealthcare.%20Existing%20healthcare-based%20surveys%20have%20not%20yet%20included%20all%20of%0Athese%20domains.%20Therefore%2C%20we%20provide%20a%20detailed%20survey%20of%20FMs%20in%20healthcare.%20We%0Afocus%20on%20the%20history%2C%20learning%20strategies%2C%20flagship%20models%2C%20applications%2C%20and%0Achallenges%20of%20FMs.%20We%20explore%20how%20FMs%20such%20as%20the%20BERT%20and%20GPT%20families%20are%0Areshaping%20various%20healthcare%20domains%2C%20including%20clinical%20large%20language%20models%2C%0Amedical%20image%20analysis%2C%20and%20omics.%20Furthermore%2C%20we%20provide%20a%20detailed%20taxonomy%0Aof%20healthcare%20applications%20facilitated%20by%20FMs%2C%20such%20as%20clinical%20NLP%2C%20medical%0Acomputer%20vision%2C%20graph%20learning%2C%20and%20other%20biology-related%20tasks.%20Despite%20the%0Apromising%20opportunities%20FMs%20provide%2C%20they%20also%20have%20several%20associated%0Achallenges%2C%20which%20are%20explained%20in%20detail.%20We%20also%20outline%20open%20research%20issues%0Aand%20potential%20lessons%20learned%20to%20provide%20researchers%20and%20practitioners%20with%0Ainsights%20into%20the%20capabilities%20of%20FMs%20in%20healthcare%20to%20advance%20their%20deployment%0Aand%20mitigate%20associated%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10729v2&entry.124074799=Read"},
{"title": "Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A\n  Single-Stage Transformer Approach", "author": "Yuduo Wang and Weikang Yu and Pedram Ghamisi", "abstract": "  Change captioning has become essential for accurately describing changes in\nmulti-temporal remote sensing data, providing an intuitive way to monitor\nEarth's dynamics through natural language. However, existing change captioning\nmethods face two key challenges: high computational demands due to multistage\nfusion strategy, and insufficient detail in object descriptions due to limited\nsemantic extraction from individual images. To solve these challenges, we\npropose SAT-Cap based on the transformers model with a single-stage feature\nfusion for remote sensing change captioning. In particular, SAT-Cap integrates\na Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a\nCaption Decoder. Compared to typical models that require multi-stage fusion in\ntransformer encoder and fusion module, SAT-Cap uses only a simple cosine\nsimilarity-based fusion module for information integration, reducing the\ncomplexity of the model architecture. By jointly modeling spatial and channel\ninformation in Spatial-Channel Attention Encoder, our approach significantly\nenhances the model's ability to extract semantic information from objects in\nmulti-temporal remote sensing images. Extensive experiments validate the\neffectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC\ndataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art\nmethods. The code and pre-trained models will be available online.\n", "link": "http://arxiv.org/abs/2501.08114v1", "date": "2025-01-14", "relevancy": 2.0717, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Change%20Captioning%20in%20Remote%20Sensing%3A%20Evolution%20to%20SAT-Cap%20--%20A%0A%20%20Single-Stage%20Transformer%20Approach&body=Title%3A%20Change%20Captioning%20in%20Remote%20Sensing%3A%20Evolution%20to%20SAT-Cap%20--%20A%0A%20%20Single-Stage%20Transformer%20Approach%0AAuthor%3A%20Yuduo%20Wang%20and%20Weikang%20Yu%20and%20Pedram%20Ghamisi%0AAbstract%3A%20%20%20Change%20captioning%20has%20become%20essential%20for%20accurately%20describing%20changes%20in%0Amulti-temporal%20remote%20sensing%20data%2C%20providing%20an%20intuitive%20way%20to%20monitor%0AEarth%27s%20dynamics%20through%20natural%20language.%20However%2C%20existing%20change%20captioning%0Amethods%20face%20two%20key%20challenges%3A%20high%20computational%20demands%20due%20to%20multistage%0Afusion%20strategy%2C%20and%20insufficient%20detail%20in%20object%20descriptions%20due%20to%20limited%0Asemantic%20extraction%20from%20individual%20images.%20To%20solve%20these%20challenges%2C%20we%0Apropose%20SAT-Cap%20based%20on%20the%20transformers%20model%20with%20a%20single-stage%20feature%0Afusion%20for%20remote%20sensing%20change%20captioning.%20In%20particular%2C%20SAT-Cap%20integrates%0Aa%20Spatial-Channel%20Attention%20Encoder%2C%20a%20Difference-Guided%20Fusion%20module%2C%20and%20a%0ACaption%20Decoder.%20Compared%20to%20typical%20models%20that%20require%20multi-stage%20fusion%20in%0Atransformer%20encoder%20and%20fusion%20module%2C%20SAT-Cap%20uses%20only%20a%20simple%20cosine%0Asimilarity-based%20fusion%20module%20for%20information%20integration%2C%20reducing%20the%0Acomplexity%20of%20the%20model%20architecture.%20By%20jointly%20modeling%20spatial%20and%20channel%0Ainformation%20in%20Spatial-Channel%20Attention%20Encoder%2C%20our%20approach%20significantly%0Aenhances%20the%20model%27s%20ability%20to%20extract%20semantic%20information%20from%20objects%20in%0Amulti-temporal%20remote%20sensing%20images.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20SAT-Cap%2C%20achieving%20CIDEr%20scores%20of%20140.23%25%20on%20the%20LEVIR-CC%0Adataset%20and%2097.74%25%20on%20the%20DUBAI-CC%20dataset%2C%20surpassing%20current%20state-of-the-art%0Amethods.%20The%20code%20and%20pre-trained%20models%20will%20be%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChange%2520Captioning%2520in%2520Remote%2520Sensing%253A%2520Evolution%2520to%2520SAT-Cap%2520--%2520A%250A%2520%2520Single-Stage%2520Transformer%2520Approach%26entry.906535625%3DYuduo%2520Wang%2520and%2520Weikang%2520Yu%2520and%2520Pedram%2520Ghamisi%26entry.1292438233%3D%2520%2520Change%2520captioning%2520has%2520become%2520essential%2520for%2520accurately%2520describing%2520changes%2520in%250Amulti-temporal%2520remote%2520sensing%2520data%252C%2520providing%2520an%2520intuitive%2520way%2520to%2520monitor%250AEarth%2527s%2520dynamics%2520through%2520natural%2520language.%2520However%252C%2520existing%2520change%2520captioning%250Amethods%2520face%2520two%2520key%2520challenges%253A%2520high%2520computational%2520demands%2520due%2520to%2520multistage%250Afusion%2520strategy%252C%2520and%2520insufficient%2520detail%2520in%2520object%2520descriptions%2520due%2520to%2520limited%250Asemantic%2520extraction%2520from%2520individual%2520images.%2520To%2520solve%2520these%2520challenges%252C%2520we%250Apropose%2520SAT-Cap%2520based%2520on%2520the%2520transformers%2520model%2520with%2520a%2520single-stage%2520feature%250Afusion%2520for%2520remote%2520sensing%2520change%2520captioning.%2520In%2520particular%252C%2520SAT-Cap%2520integrates%250Aa%2520Spatial-Channel%2520Attention%2520Encoder%252C%2520a%2520Difference-Guided%2520Fusion%2520module%252C%2520and%2520a%250ACaption%2520Decoder.%2520Compared%2520to%2520typical%2520models%2520that%2520require%2520multi-stage%2520fusion%2520in%250Atransformer%2520encoder%2520and%2520fusion%2520module%252C%2520SAT-Cap%2520uses%2520only%2520a%2520simple%2520cosine%250Asimilarity-based%2520fusion%2520module%2520for%2520information%2520integration%252C%2520reducing%2520the%250Acomplexity%2520of%2520the%2520model%2520architecture.%2520By%2520jointly%2520modeling%2520spatial%2520and%2520channel%250Ainformation%2520in%2520Spatial-Channel%2520Attention%2520Encoder%252C%2520our%2520approach%2520significantly%250Aenhances%2520the%2520model%2527s%2520ability%2520to%2520extract%2520semantic%2520information%2520from%2520objects%2520in%250Amulti-temporal%2520remote%2520sensing%2520images.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520SAT-Cap%252C%2520achieving%2520CIDEr%2520scores%2520of%2520140.23%2525%2520on%2520the%2520LEVIR-CC%250Adataset%2520and%252097.74%2525%2520on%2520the%2520DUBAI-CC%2520dataset%252C%2520surpassing%2520current%2520state-of-the-art%250Amethods.%2520The%2520code%2520and%2520pre-trained%2520models%2520will%2520be%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Change%20Captioning%20in%20Remote%20Sensing%3A%20Evolution%20to%20SAT-Cap%20--%20A%0A%20%20Single-Stage%20Transformer%20Approach&entry.906535625=Yuduo%20Wang%20and%20Weikang%20Yu%20and%20Pedram%20Ghamisi&entry.1292438233=%20%20Change%20captioning%20has%20become%20essential%20for%20accurately%20describing%20changes%20in%0Amulti-temporal%20remote%20sensing%20data%2C%20providing%20an%20intuitive%20way%20to%20monitor%0AEarth%27s%20dynamics%20through%20natural%20language.%20However%2C%20existing%20change%20captioning%0Amethods%20face%20two%20key%20challenges%3A%20high%20computational%20demands%20due%20to%20multistage%0Afusion%20strategy%2C%20and%20insufficient%20detail%20in%20object%20descriptions%20due%20to%20limited%0Asemantic%20extraction%20from%20individual%20images.%20To%20solve%20these%20challenges%2C%20we%0Apropose%20SAT-Cap%20based%20on%20the%20transformers%20model%20with%20a%20single-stage%20feature%0Afusion%20for%20remote%20sensing%20change%20captioning.%20In%20particular%2C%20SAT-Cap%20integrates%0Aa%20Spatial-Channel%20Attention%20Encoder%2C%20a%20Difference-Guided%20Fusion%20module%2C%20and%20a%0ACaption%20Decoder.%20Compared%20to%20typical%20models%20that%20require%20multi-stage%20fusion%20in%0Atransformer%20encoder%20and%20fusion%20module%2C%20SAT-Cap%20uses%20only%20a%20simple%20cosine%0Asimilarity-based%20fusion%20module%20for%20information%20integration%2C%20reducing%20the%0Acomplexity%20of%20the%20model%20architecture.%20By%20jointly%20modeling%20spatial%20and%20channel%0Ainformation%20in%20Spatial-Channel%20Attention%20Encoder%2C%20our%20approach%20significantly%0Aenhances%20the%20model%27s%20ability%20to%20extract%20semantic%20information%20from%20objects%20in%0Amulti-temporal%20remote%20sensing%20images.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20SAT-Cap%2C%20achieving%20CIDEr%20scores%20of%20140.23%25%20on%20the%20LEVIR-CC%0Adataset%20and%2097.74%25%20on%20the%20DUBAI-CC%20dataset%2C%20surpassing%20current%20state-of-the-art%0Amethods.%20The%20code%20and%20pre-trained%20models%20will%20be%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08114v1&entry.124074799=Read"},
{"title": "Optimizing Speech Multi-View Feature Fusion through Conditional\n  Computation", "author": "Weiqiao Shan and Yuhao Zhang and Yuchen Han and Bei Li and Xiaofeng Zhao and Yuang Li and Min Zhang and Hao Yang and Tong Xiao and Jingbo Zhu", "abstract": "  Recent advancements have highlighted the efficacy of self-supervised learning\n(SSL) features in various speech-related tasks, providing lightweight and\nversatile multi-view speech representations. However, our study reveals that\nwhile SSL features expedite model convergence, they conflict with traditional\nspectral features like FBanks in terms of update directions. In response, we\npropose a novel generalized feature fusion framework grounded in conditional\ncomputation, featuring a gradient-sensitive gating network and a multi-stage\ndropout strategy. This framework mitigates feature conflicts and bolsters model\nrobustness to multi-view input features. By integrating SSL and spectral\nfeatures, our approach accelerates convergence and maintains performance on par\nwith spectral models across multiple speech translation tasks on the MUSTC\ndataset.\n", "link": "http://arxiv.org/abs/2501.08057v1", "date": "2025-01-14", "relevancy": 2.0634, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Speech%20Multi-View%20Feature%20Fusion%20through%20Conditional%0A%20%20Computation&body=Title%3A%20Optimizing%20Speech%20Multi-View%20Feature%20Fusion%20through%20Conditional%0A%20%20Computation%0AAuthor%3A%20Weiqiao%20Shan%20and%20Yuhao%20Zhang%20and%20Yuchen%20Han%20and%20Bei%20Li%20and%20Xiaofeng%20Zhao%20and%20Yuang%20Li%20and%20Min%20Zhang%20and%20Hao%20Yang%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu%0AAbstract%3A%20%20%20Recent%20advancements%20have%20highlighted%20the%20efficacy%20of%20self-supervised%20learning%0A%28SSL%29%20features%20in%20various%20speech-related%20tasks%2C%20providing%20lightweight%20and%0Aversatile%20multi-view%20speech%20representations.%20However%2C%20our%20study%20reveals%20that%0Awhile%20SSL%20features%20expedite%20model%20convergence%2C%20they%20conflict%20with%20traditional%0Aspectral%20features%20like%20FBanks%20in%20terms%20of%20update%20directions.%20In%20response%2C%20we%0Apropose%20a%20novel%20generalized%20feature%20fusion%20framework%20grounded%20in%20conditional%0Acomputation%2C%20featuring%20a%20gradient-sensitive%20gating%20network%20and%20a%20multi-stage%0Adropout%20strategy.%20This%20framework%20mitigates%20feature%20conflicts%20and%20bolsters%20model%0Arobustness%20to%20multi-view%20input%20features.%20By%20integrating%20SSL%20and%20spectral%0Afeatures%2C%20our%20approach%20accelerates%20convergence%20and%20maintains%20performance%20on%20par%0Awith%20spectral%20models%20across%20multiple%20speech%20translation%20tasks%20on%20the%20MUSTC%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Speech%2520Multi-View%2520Feature%2520Fusion%2520through%2520Conditional%250A%2520%2520Computation%26entry.906535625%3DWeiqiao%2520Shan%2520and%2520Yuhao%2520Zhang%2520and%2520Yuchen%2520Han%2520and%2520Bei%2520Li%2520and%2520Xiaofeng%2520Zhao%2520and%2520Yuang%2520Li%2520and%2520Min%2520Zhang%2520and%2520Hao%2520Yang%2520and%2520Tong%2520Xiao%2520and%2520Jingbo%2520Zhu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520highlighted%2520the%2520efficacy%2520of%2520self-supervised%2520learning%250A%2528SSL%2529%2520features%2520in%2520various%2520speech-related%2520tasks%252C%2520providing%2520lightweight%2520and%250Aversatile%2520multi-view%2520speech%2520representations.%2520However%252C%2520our%2520study%2520reveals%2520that%250Awhile%2520SSL%2520features%2520expedite%2520model%2520convergence%252C%2520they%2520conflict%2520with%2520traditional%250Aspectral%2520features%2520like%2520FBanks%2520in%2520terms%2520of%2520update%2520directions.%2520In%2520response%252C%2520we%250Apropose%2520a%2520novel%2520generalized%2520feature%2520fusion%2520framework%2520grounded%2520in%2520conditional%250Acomputation%252C%2520featuring%2520a%2520gradient-sensitive%2520gating%2520network%2520and%2520a%2520multi-stage%250Adropout%2520strategy.%2520This%2520framework%2520mitigates%2520feature%2520conflicts%2520and%2520bolsters%2520model%250Arobustness%2520to%2520multi-view%2520input%2520features.%2520By%2520integrating%2520SSL%2520and%2520spectral%250Afeatures%252C%2520our%2520approach%2520accelerates%2520convergence%2520and%2520maintains%2520performance%2520on%2520par%250Awith%2520spectral%2520models%2520across%2520multiple%2520speech%2520translation%2520tasks%2520on%2520the%2520MUSTC%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Speech%20Multi-View%20Feature%20Fusion%20through%20Conditional%0A%20%20Computation&entry.906535625=Weiqiao%20Shan%20and%20Yuhao%20Zhang%20and%20Yuchen%20Han%20and%20Bei%20Li%20and%20Xiaofeng%20Zhao%20and%20Yuang%20Li%20and%20Min%20Zhang%20and%20Hao%20Yang%20and%20Tong%20Xiao%20and%20Jingbo%20Zhu&entry.1292438233=%20%20Recent%20advancements%20have%20highlighted%20the%20efficacy%20of%20self-supervised%20learning%0A%28SSL%29%20features%20in%20various%20speech-related%20tasks%2C%20providing%20lightweight%20and%0Aversatile%20multi-view%20speech%20representations.%20However%2C%20our%20study%20reveals%20that%0Awhile%20SSL%20features%20expedite%20model%20convergence%2C%20they%20conflict%20with%20traditional%0Aspectral%20features%20like%20FBanks%20in%20terms%20of%20update%20directions.%20In%20response%2C%20we%0Apropose%20a%20novel%20generalized%20feature%20fusion%20framework%20grounded%20in%20conditional%0Acomputation%2C%20featuring%20a%20gradient-sensitive%20gating%20network%20and%20a%20multi-stage%0Adropout%20strategy.%20This%20framework%20mitigates%20feature%20conflicts%20and%20bolsters%20model%0Arobustness%20to%20multi-view%20input%20features.%20By%20integrating%20SSL%20and%20spectral%0Afeatures%2C%20our%20approach%20accelerates%20convergence%20and%20maintains%20performance%20on%20par%0Awith%20spectral%20models%20across%20multiple%20speech%20translation%20tasks%20on%20the%20MUSTC%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08057v1&entry.124074799=Read"},
{"title": "Logic Augmented Generation", "author": "Aldo Gangemi and Andrea Giovanni Nuzzolese", "abstract": "  Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.\n", "link": "http://arxiv.org/abs/2411.14012v2", "date": "2025-01-14", "relevancy": 2.0606, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5338}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logic%20Augmented%20Generation&body=Title%3A%20Logic%20Augmented%20Generation%0AAuthor%3A%20Aldo%20Gangemi%20and%20Andrea%20Giovanni%20Nuzzolese%0AAbstract%3A%20%20%20Semantic%20Knowledge%20Graphs%20%28SKG%29%20face%20challenges%20with%20scalability%2C%0Aflexibility%2C%20contextual%20understanding%2C%20and%20handling%20unstructured%20or%20ambiguous%0Ainformation.%20However%2C%20they%20offer%20formal%20and%20structured%20knowledge%20enabling%0Ahighly%20interpretable%20and%20reliable%20results%20by%20means%20of%20reasoning%20and%20querying.%0ALarge%20Language%20Models%20%28LLMs%29%20overcome%20those%20limitations%20making%20them%20suitable%20in%0Aopen-ended%20tasks%20and%20unstructured%20environments.%20Nevertheless%2C%20LLMs%20are%20neither%0Ainterpretable%20nor%20reliable.%20To%20solve%20the%20dichotomy%20between%20LLMs%20and%20SKGs%20we%0Aenvision%20Logic%20Augmented%20Generation%20%28LAG%29%20that%20combines%20the%20benefits%20of%20the%20two%0Aworlds.%20LAG%20uses%20LLMs%20as%20Reactive%20Continuous%20Knowledge%20Graphs%20that%20can%20generate%0Apotentially%20infinite%20relations%20and%20tacit%20knowledge%20on-demand.%20SKGs%20are%20key%20for%0Ainjecting%20a%20discrete%20heuristic%20dimension%20with%20clear%20logical%20and%20factual%0Aboundaries.%20We%20exemplify%20LAG%20in%20two%20tasks%20of%20collective%20intelligence%2C%20i.e.%2C%0Amedical%20diagnostics%20and%20climate%20projections.%20Understanding%20the%20properties%20and%0Alimitations%20of%20LAG%2C%20which%20are%20still%20mostly%20unknown%2C%20is%20of%20utmost%20importance%20for%0Aenabling%20a%20variety%20of%20tasks%20involving%20tacit%20knowledge%20in%20order%20to%20provide%0Ainterpretable%20and%20effective%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogic%2520Augmented%2520Generation%26entry.906535625%3DAldo%2520Gangemi%2520and%2520Andrea%2520Giovanni%2520Nuzzolese%26entry.1292438233%3D%2520%2520Semantic%2520Knowledge%2520Graphs%2520%2528SKG%2529%2520face%2520challenges%2520with%2520scalability%252C%250Aflexibility%252C%2520contextual%2520understanding%252C%2520and%2520handling%2520unstructured%2520or%2520ambiguous%250Ainformation.%2520However%252C%2520they%2520offer%2520formal%2520and%2520structured%2520knowledge%2520enabling%250Ahighly%2520interpretable%2520and%2520reliable%2520results%2520by%2520means%2520of%2520reasoning%2520and%2520querying.%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520overcome%2520those%2520limitations%2520making%2520them%2520suitable%2520in%250Aopen-ended%2520tasks%2520and%2520unstructured%2520environments.%2520Nevertheless%252C%2520LLMs%2520are%2520neither%250Ainterpretable%2520nor%2520reliable.%2520To%2520solve%2520the%2520dichotomy%2520between%2520LLMs%2520and%2520SKGs%2520we%250Aenvision%2520Logic%2520Augmented%2520Generation%2520%2528LAG%2529%2520that%2520combines%2520the%2520benefits%2520of%2520the%2520two%250Aworlds.%2520LAG%2520uses%2520LLMs%2520as%2520Reactive%2520Continuous%2520Knowledge%2520Graphs%2520that%2520can%2520generate%250Apotentially%2520infinite%2520relations%2520and%2520tacit%2520knowledge%2520on-demand.%2520SKGs%2520are%2520key%2520for%250Ainjecting%2520a%2520discrete%2520heuristic%2520dimension%2520with%2520clear%2520logical%2520and%2520factual%250Aboundaries.%2520We%2520exemplify%2520LAG%2520in%2520two%2520tasks%2520of%2520collective%2520intelligence%252C%2520i.e.%252C%250Amedical%2520diagnostics%2520and%2520climate%2520projections.%2520Understanding%2520the%2520properties%2520and%250Alimitations%2520of%2520LAG%252C%2520which%2520are%2520still%2520mostly%2520unknown%252C%2520is%2520of%2520utmost%2520importance%2520for%250Aenabling%2520a%2520variety%2520of%2520tasks%2520involving%2520tacit%2520knowledge%2520in%2520order%2520to%2520provide%250Ainterpretable%2520and%2520effective%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logic%20Augmented%20Generation&entry.906535625=Aldo%20Gangemi%20and%20Andrea%20Giovanni%20Nuzzolese&entry.1292438233=%20%20Semantic%20Knowledge%20Graphs%20%28SKG%29%20face%20challenges%20with%20scalability%2C%0Aflexibility%2C%20contextual%20understanding%2C%20and%20handling%20unstructured%20or%20ambiguous%0Ainformation.%20However%2C%20they%20offer%20formal%20and%20structured%20knowledge%20enabling%0Ahighly%20interpretable%20and%20reliable%20results%20by%20means%20of%20reasoning%20and%20querying.%0ALarge%20Language%20Models%20%28LLMs%29%20overcome%20those%20limitations%20making%20them%20suitable%20in%0Aopen-ended%20tasks%20and%20unstructured%20environments.%20Nevertheless%2C%20LLMs%20are%20neither%0Ainterpretable%20nor%20reliable.%20To%20solve%20the%20dichotomy%20between%20LLMs%20and%20SKGs%20we%0Aenvision%20Logic%20Augmented%20Generation%20%28LAG%29%20that%20combines%20the%20benefits%20of%20the%20two%0Aworlds.%20LAG%20uses%20LLMs%20as%20Reactive%20Continuous%20Knowledge%20Graphs%20that%20can%20generate%0Apotentially%20infinite%20relations%20and%20tacit%20knowledge%20on-demand.%20SKGs%20are%20key%20for%0Ainjecting%20a%20discrete%20heuristic%20dimension%20with%20clear%20logical%20and%20factual%0Aboundaries.%20We%20exemplify%20LAG%20in%20two%20tasks%20of%20collective%20intelligence%2C%20i.e.%2C%0Amedical%20diagnostics%20and%20climate%20projections.%20Understanding%20the%20properties%20and%0Alimitations%20of%20LAG%2C%20which%20are%20still%20mostly%20unknown%2C%20is%20of%20utmost%20importance%20for%0Aenabling%20a%20variety%20of%20tasks%20involving%20tacit%20knowledge%20in%20order%20to%20provide%0Ainterpretable%20and%20effective%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14012v2&entry.124074799=Read"},
{"title": "Energy Backdoor Attack to Deep Neural Networks", "author": "Hanene F. Z. Brachemi Meftah and Wassim Hamidouche and Sid Ahmed Fezza and Olivier D\u00e9forges and Kassem Kallas", "abstract": "  The rise of deep learning (DL) has increased computing complexity and energy\nuse, prompting the adoption of application specific integrated circuits (ASICs)\nfor energy-efficient edge and mobile deployment. However, recent studies have\ndemonstrated the vulnerability of these accelerators to energy attacks. Despite\nthe development of various inference time energy attacks in prior research,\nbackdoor energy attacks remain unexplored. In this paper, we design an\ninnovative energy backdoor attack against deep neural networks (DNNs) operating\non sparsity-based accelerators. Our attack is carried out in two distinct\nphases: backdoor injection and backdoor stealthiness. Experimental results\nusing ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet\ndatasets show the effectiveness of our proposed attack in increasing energy\nconsumption on trigger samples while preserving the model's performance for\nclean/regular inputs. This demonstrates the vulnerability of DNNs to energy\nbackdoor attacks. The source code of our attack is available at:\nhttps://github.com/hbrachemi/energy_backdoor.\n", "link": "http://arxiv.org/abs/2501.08152v1", "date": "2025-01-14", "relevancy": 2.0563, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5449}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5133}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20Backdoor%20Attack%20to%20Deep%20Neural%20Networks&body=Title%3A%20Energy%20Backdoor%20Attack%20to%20Deep%20Neural%20Networks%0AAuthor%3A%20Hanene%20F.%20Z.%20Brachemi%20Meftah%20and%20Wassim%20Hamidouche%20and%20Sid%20Ahmed%20Fezza%20and%20Olivier%20D%C3%A9forges%20and%20Kassem%20Kallas%0AAbstract%3A%20%20%20The%20rise%20of%20deep%20learning%20%28DL%29%20has%20increased%20computing%20complexity%20and%20energy%0Ause%2C%20prompting%20the%20adoption%20of%20application%20specific%20integrated%20circuits%20%28ASICs%29%0Afor%20energy-efficient%20edge%20and%20mobile%20deployment.%20However%2C%20recent%20studies%20have%0Ademonstrated%20the%20vulnerability%20of%20these%20accelerators%20to%20energy%20attacks.%20Despite%0Athe%20development%20of%20various%20inference%20time%20energy%20attacks%20in%20prior%20research%2C%0Abackdoor%20energy%20attacks%20remain%20unexplored.%20In%20this%20paper%2C%20we%20design%20an%0Ainnovative%20energy%20backdoor%20attack%20against%20deep%20neural%20networks%20%28DNNs%29%20operating%0Aon%20sparsity-based%20accelerators.%20Our%20attack%20is%20carried%20out%20in%20two%20distinct%0Aphases%3A%20backdoor%20injection%20and%20backdoor%20stealthiness.%20Experimental%20results%0Ausing%20ResNet-18%20and%20MobileNet-V2%20models%20trained%20on%20CIFAR-10%20and%20Tiny%20ImageNet%0Adatasets%20show%20the%20effectiveness%20of%20our%20proposed%20attack%20in%20increasing%20energy%0Aconsumption%20on%20trigger%20samples%20while%20preserving%20the%20model%27s%20performance%20for%0Aclean/regular%20inputs.%20This%20demonstrates%20the%20vulnerability%20of%20DNNs%20to%20energy%0Abackdoor%20attacks.%20The%20source%20code%20of%20our%20attack%20is%20available%20at%3A%0Ahttps%3A//github.com/hbrachemi/energy_backdoor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520Backdoor%2520Attack%2520to%2520Deep%2520Neural%2520Networks%26entry.906535625%3DHanene%2520F.%2520Z.%2520Brachemi%2520Meftah%2520and%2520Wassim%2520Hamidouche%2520and%2520Sid%2520Ahmed%2520Fezza%2520and%2520Olivier%2520D%25C3%25A9forges%2520and%2520Kassem%2520Kallas%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520deep%2520learning%2520%2528DL%2529%2520has%2520increased%2520computing%2520complexity%2520and%2520energy%250Ause%252C%2520prompting%2520the%2520adoption%2520of%2520application%2520specific%2520integrated%2520circuits%2520%2528ASICs%2529%250Afor%2520energy-efficient%2520edge%2520and%2520mobile%2520deployment.%2520However%252C%2520recent%2520studies%2520have%250Ademonstrated%2520the%2520vulnerability%2520of%2520these%2520accelerators%2520to%2520energy%2520attacks.%2520Despite%250Athe%2520development%2520of%2520various%2520inference%2520time%2520energy%2520attacks%2520in%2520prior%2520research%252C%250Abackdoor%2520energy%2520attacks%2520remain%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520design%2520an%250Ainnovative%2520energy%2520backdoor%2520attack%2520against%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520operating%250Aon%2520sparsity-based%2520accelerators.%2520Our%2520attack%2520is%2520carried%2520out%2520in%2520two%2520distinct%250Aphases%253A%2520backdoor%2520injection%2520and%2520backdoor%2520stealthiness.%2520Experimental%2520results%250Ausing%2520ResNet-18%2520and%2520MobileNet-V2%2520models%2520trained%2520on%2520CIFAR-10%2520and%2520Tiny%2520ImageNet%250Adatasets%2520show%2520the%2520effectiveness%2520of%2520our%2520proposed%2520attack%2520in%2520increasing%2520energy%250Aconsumption%2520on%2520trigger%2520samples%2520while%2520preserving%2520the%2520model%2527s%2520performance%2520for%250Aclean/regular%2520inputs.%2520This%2520demonstrates%2520the%2520vulnerability%2520of%2520DNNs%2520to%2520energy%250Abackdoor%2520attacks.%2520The%2520source%2520code%2520of%2520our%2520attack%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/hbrachemi/energy_backdoor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20Backdoor%20Attack%20to%20Deep%20Neural%20Networks&entry.906535625=Hanene%20F.%20Z.%20Brachemi%20Meftah%20and%20Wassim%20Hamidouche%20and%20Sid%20Ahmed%20Fezza%20and%20Olivier%20D%C3%A9forges%20and%20Kassem%20Kallas&entry.1292438233=%20%20The%20rise%20of%20deep%20learning%20%28DL%29%20has%20increased%20computing%20complexity%20and%20energy%0Ause%2C%20prompting%20the%20adoption%20of%20application%20specific%20integrated%20circuits%20%28ASICs%29%0Afor%20energy-efficient%20edge%20and%20mobile%20deployment.%20However%2C%20recent%20studies%20have%0Ademonstrated%20the%20vulnerability%20of%20these%20accelerators%20to%20energy%20attacks.%20Despite%0Athe%20development%20of%20various%20inference%20time%20energy%20attacks%20in%20prior%20research%2C%0Abackdoor%20energy%20attacks%20remain%20unexplored.%20In%20this%20paper%2C%20we%20design%20an%0Ainnovative%20energy%20backdoor%20attack%20against%20deep%20neural%20networks%20%28DNNs%29%20operating%0Aon%20sparsity-based%20accelerators.%20Our%20attack%20is%20carried%20out%20in%20two%20distinct%0Aphases%3A%20backdoor%20injection%20and%20backdoor%20stealthiness.%20Experimental%20results%0Ausing%20ResNet-18%20and%20MobileNet-V2%20models%20trained%20on%20CIFAR-10%20and%20Tiny%20ImageNet%0Adatasets%20show%20the%20effectiveness%20of%20our%20proposed%20attack%20in%20increasing%20energy%0Aconsumption%20on%20trigger%20samples%20while%20preserving%20the%20model%27s%20performance%20for%0Aclean/regular%20inputs.%20This%20demonstrates%20the%20vulnerability%20of%20DNNs%20to%20energy%0Abackdoor%20attacks.%20The%20source%20code%20of%20our%20attack%20is%20available%20at%3A%0Ahttps%3A//github.com/hbrachemi/energy_backdoor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08152v1&entry.124074799=Read"},
{"title": "Towards an End-to-End (E2E) Adversarial Learning and Application in the\n  Physical World", "author": "Dudi Biton and Jacob Shams and Koda Satoru and Asaf Shabtai and Yuval Elovici and Ben Nassi", "abstract": "  The traditional learning process of patch-based adversarial attacks,\nconducted in the digital domain and then applied in the physical domain (e.g.,\nvia printed stickers), may suffer from reduced performance due to adversarial\npatches' limited transferability from the digital domain to the physical\ndomain. Given that previous studies have considered using projectors to apply\nadversarial attacks, we raise the following question: can adversarial learning\n(i.e., patch generation) be performed entirely in the physical domain with a\nprojector? In this work, we propose the Physical-domain Adversarial Patch\nLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework\nthat converts adversarial learning from the digital domain to the physical\ndomain using a projector. We evaluate PAPLA across multiple scenarios,\nincluding controlled laboratory settings and realistic outdoor environments,\ndemonstrating its ability to ensure attack success compared to conventional\ndigital learning-physical application (DL-PA) methods. We also analyze the\nimpact of environmental factors, such as projection surface color, projector\nstrength, ambient light, distance, and angle of the target object relative to\nthe camera, on the effectiveness of projected patches. Finally, we demonstrate\nthe feasibility of the attack against a parked car and a stop sign in a\nreal-world outdoor environment. Our results show that under specific\nconditions, E2E adversarial learning in the physical domain eliminates the\ntransferability issue and ensures evasion by object detectors. Finally, we\nprovide insights into the challenges and opportunities of applying adversarial\nlearning in the physical domain and explain where such an approach is more\neffective than using a sticker.\n", "link": "http://arxiv.org/abs/2501.08258v1", "date": "2025-01-14", "relevancy": 2.0527, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5174}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World&body=Title%3A%20Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World%0AAuthor%3A%20Dudi%20Biton%20and%20Jacob%20Shams%20and%20Koda%20Satoru%20and%20Asaf%20Shabtai%20and%20Yuval%20Elovici%20and%20Ben%20Nassi%0AAbstract%3A%20%20%20The%20traditional%20learning%20process%20of%20patch-based%20adversarial%20attacks%2C%0Aconducted%20in%20the%20digital%20domain%20and%20then%20applied%20in%20the%20physical%20domain%20%28e.g.%2C%0Avia%20printed%20stickers%29%2C%20may%20suffer%20from%20reduced%20performance%20due%20to%20adversarial%0Apatches%27%20limited%20transferability%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain.%20Given%20that%20previous%20studies%20have%20considered%20using%20projectors%20to%20apply%0Aadversarial%20attacks%2C%20we%20raise%20the%20following%20question%3A%20can%20adversarial%20learning%0A%28i.e.%2C%20patch%20generation%29%20be%20performed%20entirely%20in%20the%20physical%20domain%20with%20a%0Aprojector%3F%20In%20this%20work%2C%20we%20propose%20the%20Physical-domain%20Adversarial%20Patch%0ALearning%20Augmentation%20%28PAPLA%29%20framework%2C%20a%20novel%20end-to-end%20%28E2E%29%20framework%0Athat%20converts%20adversarial%20learning%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain%20using%20a%20projector.%20We%20evaluate%20PAPLA%20across%20multiple%20scenarios%2C%0Aincluding%20controlled%20laboratory%20settings%20and%20realistic%20outdoor%20environments%2C%0Ademonstrating%20its%20ability%20to%20ensure%20attack%20success%20compared%20to%20conventional%0Adigital%20learning-physical%20application%20%28DL-PA%29%20methods.%20We%20also%20analyze%20the%0Aimpact%20of%20environmental%20factors%2C%20such%20as%20projection%20surface%20color%2C%20projector%0Astrength%2C%20ambient%20light%2C%20distance%2C%20and%20angle%20of%20the%20target%20object%20relative%20to%0Athe%20camera%2C%20on%20the%20effectiveness%20of%20projected%20patches.%20Finally%2C%20we%20demonstrate%0Athe%20feasibility%20of%20the%20attack%20against%20a%20parked%20car%20and%20a%20stop%20sign%20in%20a%0Areal-world%20outdoor%20environment.%20Our%20results%20show%20that%20under%20specific%0Aconditions%2C%20E2E%20adversarial%20learning%20in%20the%20physical%20domain%20eliminates%20the%0Atransferability%20issue%20and%20ensures%20evasion%20by%20object%20detectors.%20Finally%2C%20we%0Aprovide%20insights%20into%20the%20challenges%20and%20opportunities%20of%20applying%20adversarial%0Alearning%20in%20the%20physical%20domain%20and%20explain%20where%20such%20an%20approach%20is%20more%0Aeffective%20than%20using%20a%20sticker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520an%2520End-to-End%2520%2528E2E%2529%2520Adversarial%2520Learning%2520and%2520Application%2520in%2520the%250A%2520%2520Physical%2520World%26entry.906535625%3DDudi%2520Biton%2520and%2520Jacob%2520Shams%2520and%2520Koda%2520Satoru%2520and%2520Asaf%2520Shabtai%2520and%2520Yuval%2520Elovici%2520and%2520Ben%2520Nassi%26entry.1292438233%3D%2520%2520The%2520traditional%2520learning%2520process%2520of%2520patch-based%2520adversarial%2520attacks%252C%250Aconducted%2520in%2520the%2520digital%2520domain%2520and%2520then%2520applied%2520in%2520the%2520physical%2520domain%2520%2528e.g.%252C%250Avia%2520printed%2520stickers%2529%252C%2520may%2520suffer%2520from%2520reduced%2520performance%2520due%2520to%2520adversarial%250Apatches%2527%2520limited%2520transferability%2520from%2520the%2520digital%2520domain%2520to%2520the%2520physical%250Adomain.%2520Given%2520that%2520previous%2520studies%2520have%2520considered%2520using%2520projectors%2520to%2520apply%250Aadversarial%2520attacks%252C%2520we%2520raise%2520the%2520following%2520question%253A%2520can%2520adversarial%2520learning%250A%2528i.e.%252C%2520patch%2520generation%2529%2520be%2520performed%2520entirely%2520in%2520the%2520physical%2520domain%2520with%2520a%250Aprojector%253F%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Physical-domain%2520Adversarial%2520Patch%250ALearning%2520Augmentation%2520%2528PAPLA%2529%2520framework%252C%2520a%2520novel%2520end-to-end%2520%2528E2E%2529%2520framework%250Athat%2520converts%2520adversarial%2520learning%2520from%2520the%2520digital%2520domain%2520to%2520the%2520physical%250Adomain%2520using%2520a%2520projector.%2520We%2520evaluate%2520PAPLA%2520across%2520multiple%2520scenarios%252C%250Aincluding%2520controlled%2520laboratory%2520settings%2520and%2520realistic%2520outdoor%2520environments%252C%250Ademonstrating%2520its%2520ability%2520to%2520ensure%2520attack%2520success%2520compared%2520to%2520conventional%250Adigital%2520learning-physical%2520application%2520%2528DL-PA%2529%2520methods.%2520We%2520also%2520analyze%2520the%250Aimpact%2520of%2520environmental%2520factors%252C%2520such%2520as%2520projection%2520surface%2520color%252C%2520projector%250Astrength%252C%2520ambient%2520light%252C%2520distance%252C%2520and%2520angle%2520of%2520the%2520target%2520object%2520relative%2520to%250Athe%2520camera%252C%2520on%2520the%2520effectiveness%2520of%2520projected%2520patches.%2520Finally%252C%2520we%2520demonstrate%250Athe%2520feasibility%2520of%2520the%2520attack%2520against%2520a%2520parked%2520car%2520and%2520a%2520stop%2520sign%2520in%2520a%250Areal-world%2520outdoor%2520environment.%2520Our%2520results%2520show%2520that%2520under%2520specific%250Aconditions%252C%2520E2E%2520adversarial%2520learning%2520in%2520the%2520physical%2520domain%2520eliminates%2520the%250Atransferability%2520issue%2520and%2520ensures%2520evasion%2520by%2520object%2520detectors.%2520Finally%252C%2520we%250Aprovide%2520insights%2520into%2520the%2520challenges%2520and%2520opportunities%2520of%2520applying%2520adversarial%250Alearning%2520in%2520the%2520physical%2520domain%2520and%2520explain%2520where%2520such%2520an%2520approach%2520is%2520more%250Aeffective%2520than%2520using%2520a%2520sticker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20an%20End-to-End%20%28E2E%29%20Adversarial%20Learning%20and%20Application%20in%20the%0A%20%20Physical%20World&entry.906535625=Dudi%20Biton%20and%20Jacob%20Shams%20and%20Koda%20Satoru%20and%20Asaf%20Shabtai%20and%20Yuval%20Elovici%20and%20Ben%20Nassi&entry.1292438233=%20%20The%20traditional%20learning%20process%20of%20patch-based%20adversarial%20attacks%2C%0Aconducted%20in%20the%20digital%20domain%20and%20then%20applied%20in%20the%20physical%20domain%20%28e.g.%2C%0Avia%20printed%20stickers%29%2C%20may%20suffer%20from%20reduced%20performance%20due%20to%20adversarial%0Apatches%27%20limited%20transferability%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain.%20Given%20that%20previous%20studies%20have%20considered%20using%20projectors%20to%20apply%0Aadversarial%20attacks%2C%20we%20raise%20the%20following%20question%3A%20can%20adversarial%20learning%0A%28i.e.%2C%20patch%20generation%29%20be%20performed%20entirely%20in%20the%20physical%20domain%20with%20a%0Aprojector%3F%20In%20this%20work%2C%20we%20propose%20the%20Physical-domain%20Adversarial%20Patch%0ALearning%20Augmentation%20%28PAPLA%29%20framework%2C%20a%20novel%20end-to-end%20%28E2E%29%20framework%0Athat%20converts%20adversarial%20learning%20from%20the%20digital%20domain%20to%20the%20physical%0Adomain%20using%20a%20projector.%20We%20evaluate%20PAPLA%20across%20multiple%20scenarios%2C%0Aincluding%20controlled%20laboratory%20settings%20and%20realistic%20outdoor%20environments%2C%0Ademonstrating%20its%20ability%20to%20ensure%20attack%20success%20compared%20to%20conventional%0Adigital%20learning-physical%20application%20%28DL-PA%29%20methods.%20We%20also%20analyze%20the%0Aimpact%20of%20environmental%20factors%2C%20such%20as%20projection%20surface%20color%2C%20projector%0Astrength%2C%20ambient%20light%2C%20distance%2C%20and%20angle%20of%20the%20target%20object%20relative%20to%0Athe%20camera%2C%20on%20the%20effectiveness%20of%20projected%20patches.%20Finally%2C%20we%20demonstrate%0Athe%20feasibility%20of%20the%20attack%20against%20a%20parked%20car%20and%20a%20stop%20sign%20in%20a%0Areal-world%20outdoor%20environment.%20Our%20results%20show%20that%20under%20specific%0Aconditions%2C%20E2E%20adversarial%20learning%20in%20the%20physical%20domain%20eliminates%20the%0Atransferability%20issue%20and%20ensures%20evasion%20by%20object%20detectors.%20Finally%2C%20we%0Aprovide%20insights%20into%20the%20challenges%20and%20opportunities%20of%20applying%20adversarial%0Alearning%20in%20the%20physical%20domain%20and%20explain%20where%20such%20an%20approach%20is%20more%0Aeffective%20than%20using%20a%20sticker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08258v1&entry.124074799=Read"},
{"title": "Combining imaging and shape features for prediction tasks of Alzheimer's\n  disease classification and brain age regression", "author": "Nairouz Shehata and Carolina Pi\u00e7arra and Ben Glocker", "abstract": "  We investigate combining imaging and shape features extracted from MRI for\nthe clinically relevant tasks of brain age prediction and Alzheimer's disease\nclassification. Our proposed model fuses ResNet-extracted image embeddings with\nshape embeddings from a bespoke graph neural network. The shape embeddings are\nderived from surface meshes of 15 brain structures, capturing detailed\ngeometric information. Combined with the appearance features from T1-weighted\nimages, we observe improvements in the prediction performance on both tasks,\nwith substantial gains for classification. We evaluate the model using public\ndatasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of\nfusing imaging and shape features for brain analysis.\n", "link": "http://arxiv.org/abs/2501.07994v1", "date": "2025-01-14", "relevancy": 2.0494, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20imaging%20and%20shape%20features%20for%20prediction%20tasks%20of%20Alzheimer%27s%0A%20%20disease%20classification%20and%20brain%20age%20regression&body=Title%3A%20Combining%20imaging%20and%20shape%20features%20for%20prediction%20tasks%20of%20Alzheimer%27s%0A%20%20disease%20classification%20and%20brain%20age%20regression%0AAuthor%3A%20Nairouz%20Shehata%20and%20Carolina%20Pi%C3%A7arra%20and%20Ben%20Glocker%0AAbstract%3A%20%20%20We%20investigate%20combining%20imaging%20and%20shape%20features%20extracted%20from%20MRI%20for%0Athe%20clinically%20relevant%20tasks%20of%20brain%20age%20prediction%20and%20Alzheimer%27s%20disease%0Aclassification.%20Our%20proposed%20model%20fuses%20ResNet-extracted%20image%20embeddings%20with%0Ashape%20embeddings%20from%20a%20bespoke%20graph%20neural%20network.%20The%20shape%20embeddings%20are%0Aderived%20from%20surface%20meshes%20of%2015%20brain%20structures%2C%20capturing%20detailed%0Ageometric%20information.%20Combined%20with%20the%20appearance%20features%20from%20T1-weighted%0Aimages%2C%20we%20observe%20improvements%20in%20the%20prediction%20performance%20on%20both%20tasks%2C%0Awith%20substantial%20gains%20for%20classification.%20We%20evaluate%20the%20model%20using%20public%0Adatasets%2C%20including%20CamCAN%2C%20IXI%2C%20and%20OASIS3%2C%20demonstrating%20the%20effectiveness%20of%0Afusing%20imaging%20and%20shape%20features%20for%20brain%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520imaging%2520and%2520shape%2520features%2520for%2520prediction%2520tasks%2520of%2520Alzheimer%2527s%250A%2520%2520disease%2520classification%2520and%2520brain%2520age%2520regression%26entry.906535625%3DNairouz%2520Shehata%2520and%2520Carolina%2520Pi%25C3%25A7arra%2520and%2520Ben%2520Glocker%26entry.1292438233%3D%2520%2520We%2520investigate%2520combining%2520imaging%2520and%2520shape%2520features%2520extracted%2520from%2520MRI%2520for%250Athe%2520clinically%2520relevant%2520tasks%2520of%2520brain%2520age%2520prediction%2520and%2520Alzheimer%2527s%2520disease%250Aclassification.%2520Our%2520proposed%2520model%2520fuses%2520ResNet-extracted%2520image%2520embeddings%2520with%250Ashape%2520embeddings%2520from%2520a%2520bespoke%2520graph%2520neural%2520network.%2520The%2520shape%2520embeddings%2520are%250Aderived%2520from%2520surface%2520meshes%2520of%252015%2520brain%2520structures%252C%2520capturing%2520detailed%250Ageometric%2520information.%2520Combined%2520with%2520the%2520appearance%2520features%2520from%2520T1-weighted%250Aimages%252C%2520we%2520observe%2520improvements%2520in%2520the%2520prediction%2520performance%2520on%2520both%2520tasks%252C%250Awith%2520substantial%2520gains%2520for%2520classification.%2520We%2520evaluate%2520the%2520model%2520using%2520public%250Adatasets%252C%2520including%2520CamCAN%252C%2520IXI%252C%2520and%2520OASIS3%252C%2520demonstrating%2520the%2520effectiveness%2520of%250Afusing%2520imaging%2520and%2520shape%2520features%2520for%2520brain%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20imaging%20and%20shape%20features%20for%20prediction%20tasks%20of%20Alzheimer%27s%0A%20%20disease%20classification%20and%20brain%20age%20regression&entry.906535625=Nairouz%20Shehata%20and%20Carolina%20Pi%C3%A7arra%20and%20Ben%20Glocker&entry.1292438233=%20%20We%20investigate%20combining%20imaging%20and%20shape%20features%20extracted%20from%20MRI%20for%0Athe%20clinically%20relevant%20tasks%20of%20brain%20age%20prediction%20and%20Alzheimer%27s%20disease%0Aclassification.%20Our%20proposed%20model%20fuses%20ResNet-extracted%20image%20embeddings%20with%0Ashape%20embeddings%20from%20a%20bespoke%20graph%20neural%20network.%20The%20shape%20embeddings%20are%0Aderived%20from%20surface%20meshes%20of%2015%20brain%20structures%2C%20capturing%20detailed%0Ageometric%20information.%20Combined%20with%20the%20appearance%20features%20from%20T1-weighted%0Aimages%2C%20we%20observe%20improvements%20in%20the%20prediction%20performance%20on%20both%20tasks%2C%0Awith%20substantial%20gains%20for%20classification.%20We%20evaluate%20the%20model%20using%20public%0Adatasets%2C%20including%20CamCAN%2C%20IXI%2C%20and%20OASIS3%2C%20demonstrating%20the%20effectiveness%20of%0Afusing%20imaging%20and%20shape%20features%20for%20brain%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07994v1&entry.124074799=Read"},
{"title": "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models", "author": "Saad Mashkoor Siddiqui and Mohammad Ali Sheikh and Muhammad Aleem and Kajol R Singh", "abstract": "  In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.\n", "link": "http://arxiv.org/abs/2501.08271v1", "date": "2025-01-14", "relevancy": 2.0468, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5557}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5284}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Efficient%20Adapter-Based%20Fine-Tuning%20of%0A%20%20State-of-the-Art%20Transformer%20Models&body=Title%3A%20Comparative%20Analysis%20of%20Efficient%20Adapter-Based%20Fine-Tuning%20of%0A%20%20State-of-the-Art%20Transformer%20Models%0AAuthor%3A%20Saad%20Mashkoor%20Siddiqui%20and%20Mohammad%20Ali%20Sheikh%20and%20Muhammad%20Aleem%20and%20Kajol%20R%20Singh%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20the%20efficacy%20of%20various%20adapter%20architectures%20on%0Asupervised%20binary%20classification%20tasks%20from%20the%20SuperGLUE%20benchmark%20as%20well%20as%0Aa%20supervised%20multi-class%20news%20category%20classification%20task%20from%20Kaggle.%0ASpecifically%2C%20we%20compare%20classification%20performance%20and%20time%20complexity%20of%0Athree%20transformer%20models%2C%20namely%20DistilBERT%2C%20ELECTRA%2C%20and%20BART%2C%20using%0Aconventional%20fine-tuning%20as%20well%20as%20nine%20state-of-the-art%20%28SoTA%29%20adapter%0Aarchitectures.%20Our%20analysis%20reveals%20performance%20differences%20across%20adapter%0Aarchitectures%2C%20highlighting%20their%20ability%20to%20achieve%20comparable%20or%20better%0Aperformance%20relative%20to%20fine-tuning%20at%20a%20fraction%20of%20the%20training%20time.%20Similar%0Aresults%20are%20observed%20on%20the%20new%20classification%20task%2C%20further%20supporting%20our%0Afindings%20and%20demonstrating%20adapters%20as%20efficient%20and%20flexible%20alternatives%20to%0Afine-tuning.%20This%20study%20provides%20valuable%20insights%20and%20guidelines%20for%20selecting%0Aand%20implementing%20adapters%20in%20diverse%20natural%20language%20processing%20%28NLP%29%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Efficient%2520Adapter-Based%2520Fine-Tuning%2520of%250A%2520%2520State-of-the-Art%2520Transformer%2520Models%26entry.906535625%3DSaad%2520Mashkoor%2520Siddiqui%2520and%2520Mohammad%2520Ali%2520Sheikh%2520and%2520Muhammad%2520Aleem%2520and%2520Kajol%2520R%2520Singh%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520efficacy%2520of%2520various%2520adapter%2520architectures%2520on%250Asupervised%2520binary%2520classification%2520tasks%2520from%2520the%2520SuperGLUE%2520benchmark%2520as%2520well%2520as%250Aa%2520supervised%2520multi-class%2520news%2520category%2520classification%2520task%2520from%2520Kaggle.%250ASpecifically%252C%2520we%2520compare%2520classification%2520performance%2520and%2520time%2520complexity%2520of%250Athree%2520transformer%2520models%252C%2520namely%2520DistilBERT%252C%2520ELECTRA%252C%2520and%2520BART%252C%2520using%250Aconventional%2520fine-tuning%2520as%2520well%2520as%2520nine%2520state-of-the-art%2520%2528SoTA%2529%2520adapter%250Aarchitectures.%2520Our%2520analysis%2520reveals%2520performance%2520differences%2520across%2520adapter%250Aarchitectures%252C%2520highlighting%2520their%2520ability%2520to%2520achieve%2520comparable%2520or%2520better%250Aperformance%2520relative%2520to%2520fine-tuning%2520at%2520a%2520fraction%2520of%2520the%2520training%2520time.%2520Similar%250Aresults%2520are%2520observed%2520on%2520the%2520new%2520classification%2520task%252C%2520further%2520supporting%2520our%250Afindings%2520and%2520demonstrating%2520adapters%2520as%2520efficient%2520and%2520flexible%2520alternatives%2520to%250Afine-tuning.%2520This%2520study%2520provides%2520valuable%2520insights%2520and%2520guidelines%2520for%2520selecting%250Aand%2520implementing%2520adapters%2520in%2520diverse%2520natural%2520language%2520processing%2520%2528NLP%2529%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Efficient%20Adapter-Based%20Fine-Tuning%20of%0A%20%20State-of-the-Art%20Transformer%20Models&entry.906535625=Saad%20Mashkoor%20Siddiqui%20and%20Mohammad%20Ali%20Sheikh%20and%20Muhammad%20Aleem%20and%20Kajol%20R%20Singh&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20the%20efficacy%20of%20various%20adapter%20architectures%20on%0Asupervised%20binary%20classification%20tasks%20from%20the%20SuperGLUE%20benchmark%20as%20well%20as%0Aa%20supervised%20multi-class%20news%20category%20classification%20task%20from%20Kaggle.%0ASpecifically%2C%20we%20compare%20classification%20performance%20and%20time%20complexity%20of%0Athree%20transformer%20models%2C%20namely%20DistilBERT%2C%20ELECTRA%2C%20and%20BART%2C%20using%0Aconventional%20fine-tuning%20as%20well%20as%20nine%20state-of-the-art%20%28SoTA%29%20adapter%0Aarchitectures.%20Our%20analysis%20reveals%20performance%20differences%20across%20adapter%0Aarchitectures%2C%20highlighting%20their%20ability%20to%20achieve%20comparable%20or%20better%0Aperformance%20relative%20to%20fine-tuning%20at%20a%20fraction%20of%20the%20training%20time.%20Similar%0Aresults%20are%20observed%20on%20the%20new%20classification%20task%2C%20further%20supporting%20our%0Afindings%20and%20demonstrating%20adapters%20as%20efficient%20and%20flexible%20alternatives%20to%0Afine-tuning.%20This%20study%20provides%20valuable%20insights%20and%20guidelines%20for%20selecting%0Aand%20implementing%20adapters%20in%20diverse%20natural%20language%20processing%20%28NLP%29%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08271v1&entry.124074799=Read"},
{"title": "Efficient Distribution Matching of Representations via Noise-Injected\n  Deep InfoMax", "author": "Ivan Butakov and Alexander Semenenko and Alexander Tolmachev and Andrey Gladkov and Marina Munkhoeva and Alexey Frolov", "abstract": "  Deep InfoMax (DIM) is a well-established method for self-supervised\nrepresentation learning (SSRL) based on maximization of the mutual information\nbetween the input and the output of a deep neural network encoder. Despite the\nDIM and contrastive SSRL in general being well-explored, the task of learning\nrepresentations conforming to a specific distribution (i.e., distribution\nmatching, DM) is still under-addressed. Motivated by the importance of DM to\nseveral downstream tasks (including generative modeling, disentanglement,\noutliers detection and other), we enhance DIM to enable automatic matching of\nlearned representations to a selected prior distribution. To achieve this, we\npropose injecting an independent noise into the normalized outputs of the\nencoder, while keeping the same InfoMax training objective. We show that such\nmodification allows for learning uniformly and normally distributed\nrepresentations, as well as representations of other absolutely continuous\ndistributions. Our approach is tested on various downstream tasks. The results\nindicate a moderate trade-off between the performance on the downstream tasks\nand quality of DM.\n", "link": "http://arxiv.org/abs/2410.06993v2", "date": "2025-01-14", "relevancy": 2.0356, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Distribution%20Matching%20of%20Representations%20via%20Noise-Injected%0A%20%20Deep%20InfoMax&body=Title%3A%20Efficient%20Distribution%20Matching%20of%20Representations%20via%20Noise-Injected%0A%20%20Deep%20InfoMax%0AAuthor%3A%20Ivan%20Butakov%20and%20Alexander%20Semenenko%20and%20Alexander%20Tolmachev%20and%20Andrey%20Gladkov%20and%20Marina%20Munkhoeva%20and%20Alexey%20Frolov%0AAbstract%3A%20%20%20Deep%20InfoMax%20%28DIM%29%20is%20a%20well-established%20method%20for%20self-supervised%0Arepresentation%20learning%20%28SSRL%29%20based%20on%20maximization%20of%20the%20mutual%20information%0Abetween%20the%20input%20and%20the%20output%20of%20a%20deep%20neural%20network%20encoder.%20Despite%20the%0ADIM%20and%20contrastive%20SSRL%20in%20general%20being%20well-explored%2C%20the%20task%20of%20learning%0Arepresentations%20conforming%20to%20a%20specific%20distribution%20%28i.e.%2C%20distribution%0Amatching%2C%20DM%29%20is%20still%20under-addressed.%20Motivated%20by%20the%20importance%20of%20DM%20to%0Aseveral%20downstream%20tasks%20%28including%20generative%20modeling%2C%20disentanglement%2C%0Aoutliers%20detection%20and%20other%29%2C%20we%20enhance%20DIM%20to%20enable%20automatic%20matching%20of%0Alearned%20representations%20to%20a%20selected%20prior%20distribution.%20To%20achieve%20this%2C%20we%0Apropose%20injecting%20an%20independent%20noise%20into%20the%20normalized%20outputs%20of%20the%0Aencoder%2C%20while%20keeping%20the%20same%20InfoMax%20training%20objective.%20We%20show%20that%20such%0Amodification%20allows%20for%20learning%20uniformly%20and%20normally%20distributed%0Arepresentations%2C%20as%20well%20as%20representations%20of%20other%20absolutely%20continuous%0Adistributions.%20Our%20approach%20is%20tested%20on%20various%20downstream%20tasks.%20The%20results%0Aindicate%20a%20moderate%20trade-off%20between%20the%20performance%20on%20the%20downstream%20tasks%0Aand%20quality%20of%20DM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Distribution%2520Matching%2520of%2520Representations%2520via%2520Noise-Injected%250A%2520%2520Deep%2520InfoMax%26entry.906535625%3DIvan%2520Butakov%2520and%2520Alexander%2520Semenenko%2520and%2520Alexander%2520Tolmachev%2520and%2520Andrey%2520Gladkov%2520and%2520Marina%2520Munkhoeva%2520and%2520Alexey%2520Frolov%26entry.1292438233%3D%2520%2520Deep%2520InfoMax%2520%2528DIM%2529%2520is%2520a%2520well-established%2520method%2520for%2520self-supervised%250Arepresentation%2520learning%2520%2528SSRL%2529%2520based%2520on%2520maximization%2520of%2520the%2520mutual%2520information%250Abetween%2520the%2520input%2520and%2520the%2520output%2520of%2520a%2520deep%2520neural%2520network%2520encoder.%2520Despite%2520the%250ADIM%2520and%2520contrastive%2520SSRL%2520in%2520general%2520being%2520well-explored%252C%2520the%2520task%2520of%2520learning%250Arepresentations%2520conforming%2520to%2520a%2520specific%2520distribution%2520%2528i.e.%252C%2520distribution%250Amatching%252C%2520DM%2529%2520is%2520still%2520under-addressed.%2520Motivated%2520by%2520the%2520importance%2520of%2520DM%2520to%250Aseveral%2520downstream%2520tasks%2520%2528including%2520generative%2520modeling%252C%2520disentanglement%252C%250Aoutliers%2520detection%2520and%2520other%2529%252C%2520we%2520enhance%2520DIM%2520to%2520enable%2520automatic%2520matching%2520of%250Alearned%2520representations%2520to%2520a%2520selected%2520prior%2520distribution.%2520To%2520achieve%2520this%252C%2520we%250Apropose%2520injecting%2520an%2520independent%2520noise%2520into%2520the%2520normalized%2520outputs%2520of%2520the%250Aencoder%252C%2520while%2520keeping%2520the%2520same%2520InfoMax%2520training%2520objective.%2520We%2520show%2520that%2520such%250Amodification%2520allows%2520for%2520learning%2520uniformly%2520and%2520normally%2520distributed%250Arepresentations%252C%2520as%2520well%2520as%2520representations%2520of%2520other%2520absolutely%2520continuous%250Adistributions.%2520Our%2520approach%2520is%2520tested%2520on%2520various%2520downstream%2520tasks.%2520The%2520results%250Aindicate%2520a%2520moderate%2520trade-off%2520between%2520the%2520performance%2520on%2520the%2520downstream%2520tasks%250Aand%2520quality%2520of%2520DM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Distribution%20Matching%20of%20Representations%20via%20Noise-Injected%0A%20%20Deep%20InfoMax&entry.906535625=Ivan%20Butakov%20and%20Alexander%20Semenenko%20and%20Alexander%20Tolmachev%20and%20Andrey%20Gladkov%20and%20Marina%20Munkhoeva%20and%20Alexey%20Frolov&entry.1292438233=%20%20Deep%20InfoMax%20%28DIM%29%20is%20a%20well-established%20method%20for%20self-supervised%0Arepresentation%20learning%20%28SSRL%29%20based%20on%20maximization%20of%20the%20mutual%20information%0Abetween%20the%20input%20and%20the%20output%20of%20a%20deep%20neural%20network%20encoder.%20Despite%20the%0ADIM%20and%20contrastive%20SSRL%20in%20general%20being%20well-explored%2C%20the%20task%20of%20learning%0Arepresentations%20conforming%20to%20a%20specific%20distribution%20%28i.e.%2C%20distribution%0Amatching%2C%20DM%29%20is%20still%20under-addressed.%20Motivated%20by%20the%20importance%20of%20DM%20to%0Aseveral%20downstream%20tasks%20%28including%20generative%20modeling%2C%20disentanglement%2C%0Aoutliers%20detection%20and%20other%29%2C%20we%20enhance%20DIM%20to%20enable%20automatic%20matching%20of%0Alearned%20representations%20to%20a%20selected%20prior%20distribution.%20To%20achieve%20this%2C%20we%0Apropose%20injecting%20an%20independent%20noise%20into%20the%20normalized%20outputs%20of%20the%0Aencoder%2C%20while%20keeping%20the%20same%20InfoMax%20training%20objective.%20We%20show%20that%20such%0Amodification%20allows%20for%20learning%20uniformly%20and%20normally%20distributed%0Arepresentations%2C%20as%20well%20as%20representations%20of%20other%20absolutely%20continuous%0Adistributions.%20Our%20approach%20is%20tested%20on%20various%20downstream%20tasks.%20The%20results%0Aindicate%20a%20moderate%20trade-off%20between%20the%20performance%20on%20the%20downstream%20tasks%0Aand%20quality%20of%20DM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06993v2&entry.124074799=Read"},
{"title": "Big Batch Bayesian Active Learning by Considering Predictive\n  Probabilities", "author": "Sebastian W. Ober and Samuel Power and Tom Diethe and Henry B. Moss", "abstract": "  We observe that BatchBALD, a popular acquisition function for batch Bayesian\nactive learning for classification, can conflate epistemic and aleatoric\nuncertainty, leading to suboptimal performance. Motivated by this observation,\nwe propose to focus on the predictive probabilities, which only exhibit\nepistemic uncertainty. The result is an acquisition function that not only\nperforms better, but is also faster to evaluate, allowing for larger batches\nthan before.\n", "link": "http://arxiv.org/abs/2501.08223v1", "date": "2025-01-14", "relevancy": 2.0354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.552}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5112}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Big%20Batch%20Bayesian%20Active%20Learning%20by%20Considering%20Predictive%0A%20%20Probabilities&body=Title%3A%20Big%20Batch%20Bayesian%20Active%20Learning%20by%20Considering%20Predictive%0A%20%20Probabilities%0AAuthor%3A%20Sebastian%20W.%20Ober%20and%20Samuel%20Power%20and%20Tom%20Diethe%20and%20Henry%20B.%20Moss%0AAbstract%3A%20%20%20We%20observe%20that%20BatchBALD%2C%20a%20popular%20acquisition%20function%20for%20batch%20Bayesian%0Aactive%20learning%20for%20classification%2C%20can%20conflate%20epistemic%20and%20aleatoric%0Auncertainty%2C%20leading%20to%20suboptimal%20performance.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20to%20focus%20on%20the%20predictive%20probabilities%2C%20which%20only%20exhibit%0Aepistemic%20uncertainty.%20The%20result%20is%20an%20acquisition%20function%20that%20not%20only%0Aperforms%20better%2C%20but%20is%20also%20faster%20to%20evaluate%2C%20allowing%20for%20larger%20batches%0Athan%20before.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBig%2520Batch%2520Bayesian%2520Active%2520Learning%2520by%2520Considering%2520Predictive%250A%2520%2520Probabilities%26entry.906535625%3DSebastian%2520W.%2520Ober%2520and%2520Samuel%2520Power%2520and%2520Tom%2520Diethe%2520and%2520Henry%2520B.%2520Moss%26entry.1292438233%3D%2520%2520We%2520observe%2520that%2520BatchBALD%252C%2520a%2520popular%2520acquisition%2520function%2520for%2520batch%2520Bayesian%250Aactive%2520learning%2520for%2520classification%252C%2520can%2520conflate%2520epistemic%2520and%2520aleatoric%250Auncertainty%252C%2520leading%2520to%2520suboptimal%2520performance.%2520Motivated%2520by%2520this%2520observation%252C%250Awe%2520propose%2520to%2520focus%2520on%2520the%2520predictive%2520probabilities%252C%2520which%2520only%2520exhibit%250Aepistemic%2520uncertainty.%2520The%2520result%2520is%2520an%2520acquisition%2520function%2520that%2520not%2520only%250Aperforms%2520better%252C%2520but%2520is%2520also%2520faster%2520to%2520evaluate%252C%2520allowing%2520for%2520larger%2520batches%250Athan%2520before.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Big%20Batch%20Bayesian%20Active%20Learning%20by%20Considering%20Predictive%0A%20%20Probabilities&entry.906535625=Sebastian%20W.%20Ober%20and%20Samuel%20Power%20and%20Tom%20Diethe%20and%20Henry%20B.%20Moss&entry.1292438233=%20%20We%20observe%20that%20BatchBALD%2C%20a%20popular%20acquisition%20function%20for%20batch%20Bayesian%0Aactive%20learning%20for%20classification%2C%20can%20conflate%20epistemic%20and%20aleatoric%0Auncertainty%2C%20leading%20to%20suboptimal%20performance.%20Motivated%20by%20this%20observation%2C%0Awe%20propose%20to%20focus%20on%20the%20predictive%20probabilities%2C%20which%20only%20exhibit%0Aepistemic%20uncertainty.%20The%20result%20is%20an%20acquisition%20function%20that%20not%20only%0Aperforms%20better%2C%20but%20is%20also%20faster%20to%20evaluate%2C%20allowing%20for%20larger%20batches%0Athan%20before.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08223v1&entry.124074799=Read"},
{"title": "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them", "author": "Abhilasha Ravichander and Shrusti Ghela and David Wadden and Yejin Choi", "abstract": "  Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.\n", "link": "http://arxiv.org/abs/2501.08292v1", "date": "2025-01-14", "relevancy": 2.0346, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5279}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HALoGEN%3A%20Fantastic%20LLM%20Hallucinations%20and%20Where%20to%20Find%20Them&body=Title%3A%20HALoGEN%3A%20Fantastic%20LLM%20Hallucinations%20and%20Where%20to%20Find%20Them%0AAuthor%3A%20Abhilasha%20Ravichander%20and%20Shrusti%20Ghela%20and%20David%20Wadden%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20Despite%20their%20impressive%20ability%20to%20generate%20high-quality%20and%20fluent%20text%2C%0Agenerative%20large%20language%20models%20%28LLMs%29%20also%20produce%20hallucinations%3A%20statements%0Athat%20are%20misaligned%20with%20established%20world%20knowledge%20or%20provided%20input%20context.%0AHowever%2C%20measuring%20hallucination%20can%20be%20challenging%2C%20as%20having%20humans%20verify%0Amodel%20generations%20on-the-fly%20is%20both%20expensive%20and%20time-consuming.%20In%20this%0Awork%2C%20we%20release%20HALoGEN%2C%20a%20comprehensive%20hallucination%20benchmark%20consisting%0Aof%3A%20%281%29%2010%2C923%20prompts%20for%20generative%20models%20spanning%20nine%20domains%20including%0Aprogramming%2C%20scientific%20attribution%2C%20and%20summarization%2C%20and%20%282%29%20automatic%0Ahigh-precision%20verifiers%20for%20each%20use%20case%20that%20decompose%20LLM%20generations%20into%0Aatomic%20units%2C%20and%20verify%20each%20unit%20against%20a%20high-quality%20knowledge%20source.%20We%0Ause%20this%20framework%20to%20evaluate%20~150%2C000%20generations%20from%2014%20language%20models%2C%0Afinding%20that%20even%20the%20best-performing%20models%20are%20riddled%20with%20hallucinations%0A%28sometimes%20up%20to%2086%25%20of%20generated%20atomic%20facts%20depending%20on%20the%20domain%29.%20We%0Afurther%20define%20a%20novel%20error%20classification%20for%20LLM%20hallucinations%20based%20on%0Awhether%20they%20likely%20stem%20from%20incorrect%20recollection%20of%20training%20data%20%28Type%20A%0Aerrors%29%2C%20or%20incorrect%20knowledge%20in%20training%20data%20%28Type%20B%20errors%29%2C%20or%20are%0Afabrication%20%28Type%20C%20errors%29.%20We%20hope%20our%20framework%20provides%20a%20foundation%20to%0Aenable%20the%20principled%20study%20of%20why%20generative%20models%20hallucinate%2C%20and%20advances%0Athe%20development%20of%20trustworthy%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHALoGEN%253A%2520Fantastic%2520LLM%2520Hallucinations%2520and%2520Where%2520to%2520Find%2520Them%26entry.906535625%3DAbhilasha%2520Ravichander%2520and%2520Shrusti%2520Ghela%2520and%2520David%2520Wadden%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520Despite%2520their%2520impressive%2520ability%2520to%2520generate%2520high-quality%2520and%2520fluent%2520text%252C%250Agenerative%2520large%2520language%2520models%2520%2528LLMs%2529%2520also%2520produce%2520hallucinations%253A%2520statements%250Athat%2520are%2520misaligned%2520with%2520established%2520world%2520knowledge%2520or%2520provided%2520input%2520context.%250AHowever%252C%2520measuring%2520hallucination%2520can%2520be%2520challenging%252C%2520as%2520having%2520humans%2520verify%250Amodel%2520generations%2520on-the-fly%2520is%2520both%2520expensive%2520and%2520time-consuming.%2520In%2520this%250Awork%252C%2520we%2520release%2520HALoGEN%252C%2520a%2520comprehensive%2520hallucination%2520benchmark%2520consisting%250Aof%253A%2520%25281%2529%252010%252C923%2520prompts%2520for%2520generative%2520models%2520spanning%2520nine%2520domains%2520including%250Aprogramming%252C%2520scientific%2520attribution%252C%2520and%2520summarization%252C%2520and%2520%25282%2529%2520automatic%250Ahigh-precision%2520verifiers%2520for%2520each%2520use%2520case%2520that%2520decompose%2520LLM%2520generations%2520into%250Aatomic%2520units%252C%2520and%2520verify%2520each%2520unit%2520against%2520a%2520high-quality%2520knowledge%2520source.%2520We%250Ause%2520this%2520framework%2520to%2520evaluate%2520~150%252C000%2520generations%2520from%252014%2520language%2520models%252C%250Afinding%2520that%2520even%2520the%2520best-performing%2520models%2520are%2520riddled%2520with%2520hallucinations%250A%2528sometimes%2520up%2520to%252086%2525%2520of%2520generated%2520atomic%2520facts%2520depending%2520on%2520the%2520domain%2529.%2520We%250Afurther%2520define%2520a%2520novel%2520error%2520classification%2520for%2520LLM%2520hallucinations%2520based%2520on%250Awhether%2520they%2520likely%2520stem%2520from%2520incorrect%2520recollection%2520of%2520training%2520data%2520%2528Type%2520A%250Aerrors%2529%252C%2520or%2520incorrect%2520knowledge%2520in%2520training%2520data%2520%2528Type%2520B%2520errors%2529%252C%2520or%2520are%250Afabrication%2520%2528Type%2520C%2520errors%2529.%2520We%2520hope%2520our%2520framework%2520provides%2520a%2520foundation%2520to%250Aenable%2520the%2520principled%2520study%2520of%2520why%2520generative%2520models%2520hallucinate%252C%2520and%2520advances%250Athe%2520development%2520of%2520trustworthy%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HALoGEN%3A%20Fantastic%20LLM%20Hallucinations%20and%20Where%20to%20Find%20Them&entry.906535625=Abhilasha%20Ravichander%20and%20Shrusti%20Ghela%20and%20David%20Wadden%20and%20Yejin%20Choi&entry.1292438233=%20%20Despite%20their%20impressive%20ability%20to%20generate%20high-quality%20and%20fluent%20text%2C%0Agenerative%20large%20language%20models%20%28LLMs%29%20also%20produce%20hallucinations%3A%20statements%0Athat%20are%20misaligned%20with%20established%20world%20knowledge%20or%20provided%20input%20context.%0AHowever%2C%20measuring%20hallucination%20can%20be%20challenging%2C%20as%20having%20humans%20verify%0Amodel%20generations%20on-the-fly%20is%20both%20expensive%20and%20time-consuming.%20In%20this%0Awork%2C%20we%20release%20HALoGEN%2C%20a%20comprehensive%20hallucination%20benchmark%20consisting%0Aof%3A%20%281%29%2010%2C923%20prompts%20for%20generative%20models%20spanning%20nine%20domains%20including%0Aprogramming%2C%20scientific%20attribution%2C%20and%20summarization%2C%20and%20%282%29%20automatic%0Ahigh-precision%20verifiers%20for%20each%20use%20case%20that%20decompose%20LLM%20generations%20into%0Aatomic%20units%2C%20and%20verify%20each%20unit%20against%20a%20high-quality%20knowledge%20source.%20We%0Ause%20this%20framework%20to%20evaluate%20~150%2C000%20generations%20from%2014%20language%20models%2C%0Afinding%20that%20even%20the%20best-performing%20models%20are%20riddled%20with%20hallucinations%0A%28sometimes%20up%20to%2086%25%20of%20generated%20atomic%20facts%20depending%20on%20the%20domain%29.%20We%0Afurther%20define%20a%20novel%20error%20classification%20for%20LLM%20hallucinations%20based%20on%0Awhether%20they%20likely%20stem%20from%20incorrect%20recollection%20of%20training%20data%20%28Type%20A%0Aerrors%29%2C%20or%20incorrect%20knowledge%20in%20training%20data%20%28Type%20B%20errors%29%2C%20or%20are%0Afabrication%20%28Type%20C%20errors%29.%20We%20hope%20our%20framework%20provides%20a%20foundation%20to%0Aenable%20the%20principled%20study%20of%20why%20generative%20models%20hallucinate%2C%20and%20advances%0Athe%20development%20of%20trustworthy%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08292v1&entry.124074799=Read"},
{"title": "ASTRID -- An Automated and Scalable TRIaD for the Evaluation of\n  RAG-based Clinical Question Answering Systems", "author": "Mohita Chowdhury and Yajie Vera He and Aisling Higham and Ernest Lim", "abstract": "  Large Language Models (LLMs) have shown impressive potential in clinical\nquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging as\na leading approach for ensuring the factual accuracy of model responses.\nHowever, current automated RAG metrics perform poorly in clinical and\nconversational use cases. Using clinical human evaluations of responses is\nexpensive, unscalable, and not conducive to the continuous iterative\ndevelopment of RAG systems. To address these challenges, we introduce ASTRID -\nan Automated and Scalable TRIaD for evaluating clinical QA systems leveraging\nRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy\n(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is\ndesigned to better capture the faithfulness of a model's response to the\nknowledge base without penalising conversational elements. To validate our\ntriad, we curate a dataset of over 200 real-world patient questions posed to an\nLLM-based QA agent during surgical follow-up for cataract surgery - the highest\nvolume operation in the world - augmented with clinician-selected questions for\nemergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate\nthat CF can predict human ratings of faithfulness better than existing\ndefinitions for conversational use cases. Furthermore, we show that evaluation\nusing our triad consisting of CF, RA, and CR exhibits alignment with clinician\nassessment for inappropriate, harmful, or unhelpful responses. Finally, using\nnine different LLMs, we demonstrate that the three metrics can closely agree\nwith human evaluations, highlighting the potential of these metrics for use in\nLLM-driven automated evaluation pipelines. We also publish the prompts and\ndatasets for these experiments, providing valuable resources for further\nresearch and development.\n", "link": "http://arxiv.org/abs/2501.08208v1", "date": "2025-01-14", "relevancy": 1.4843, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASTRID%20--%20An%20Automated%20and%20Scalable%20TRIaD%20for%20the%20Evaluation%20of%0A%20%20RAG-based%20Clinical%20Question%20Answering%20Systems&body=Title%3A%20ASTRID%20--%20An%20Automated%20and%20Scalable%20TRIaD%20for%20the%20Evaluation%20of%0A%20%20RAG-based%20Clinical%20Question%20Answering%20Systems%0AAuthor%3A%20Mohita%20Chowdhury%20and%20Yajie%20Vera%20He%20and%20Aisling%20Higham%20and%20Ernest%20Lim%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20potential%20in%20clinical%0Aquestion%20answering%20%28QA%29%2C%20with%20Retrieval%20Augmented%20Generation%20%28RAG%29%20emerging%20as%0Aa%20leading%20approach%20for%20ensuring%20the%20factual%20accuracy%20of%20model%20responses.%0AHowever%2C%20current%20automated%20RAG%20metrics%20perform%20poorly%20in%20clinical%20and%0Aconversational%20use%20cases.%20Using%20clinical%20human%20evaluations%20of%20responses%20is%0Aexpensive%2C%20unscalable%2C%20and%20not%20conducive%20to%20the%20continuous%20iterative%0Adevelopment%20of%20RAG%20systems.%20To%20address%20these%20challenges%2C%20we%20introduce%20ASTRID%20-%0Aan%20Automated%20and%20Scalable%20TRIaD%20for%20evaluating%20clinical%20QA%20systems%20leveraging%0ARAG%20-%20consisting%20of%20three%20metrics%3A%20Context%20Relevance%20%28CR%29%2C%20Refusal%20Accuracy%0A%28RA%29%2C%20and%20Conversational%20Faithfulness%20%28CF%29.%20Our%20novel%20evaluation%20metric%2C%20CF%2C%20is%0Adesigned%20to%20better%20capture%20the%20faithfulness%20of%20a%20model%27s%20response%20to%20the%0Aknowledge%20base%20without%20penalising%20conversational%20elements.%20To%20validate%20our%0Atriad%2C%20we%20curate%20a%20dataset%20of%20over%20200%20real-world%20patient%20questions%20posed%20to%20an%0ALLM-based%20QA%20agent%20during%20surgical%20follow-up%20for%20cataract%20surgery%20-%20the%20highest%0Avolume%20operation%20in%20the%20world%20-%20augmented%20with%20clinician-selected%20questions%20for%0Aemergency%2C%20clinical%2C%20and%20non-clinical%20out-of-domain%20scenarios.%20We%20demonstrate%0Athat%20CF%20can%20predict%20human%20ratings%20of%20faithfulness%20better%20than%20existing%0Adefinitions%20for%20conversational%20use%20cases.%20Furthermore%2C%20we%20show%20that%20evaluation%0Ausing%20our%20triad%20consisting%20of%20CF%2C%20RA%2C%20and%20CR%20exhibits%20alignment%20with%20clinician%0Aassessment%20for%20inappropriate%2C%20harmful%2C%20or%20unhelpful%20responses.%20Finally%2C%20using%0Anine%20different%20LLMs%2C%20we%20demonstrate%20that%20the%20three%20metrics%20can%20closely%20agree%0Awith%20human%20evaluations%2C%20highlighting%20the%20potential%20of%20these%20metrics%20for%20use%20in%0ALLM-driven%20automated%20evaluation%20pipelines.%20We%20also%20publish%20the%20prompts%20and%0Adatasets%20for%20these%20experiments%2C%20providing%20valuable%20resources%20for%20further%0Aresearch%20and%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASTRID%2520--%2520An%2520Automated%2520and%2520Scalable%2520TRIaD%2520for%2520the%2520Evaluation%2520of%250A%2520%2520RAG-based%2520Clinical%2520Question%2520Answering%2520Systems%26entry.906535625%3DMohita%2520Chowdhury%2520and%2520Yajie%2520Vera%2520He%2520and%2520Aisling%2520Higham%2520and%2520Ernest%2520Lim%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520potential%2520in%2520clinical%250Aquestion%2520answering%2520%2528QA%2529%252C%2520with%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520emerging%2520as%250Aa%2520leading%2520approach%2520for%2520ensuring%2520the%2520factual%2520accuracy%2520of%2520model%2520responses.%250AHowever%252C%2520current%2520automated%2520RAG%2520metrics%2520perform%2520poorly%2520in%2520clinical%2520and%250Aconversational%2520use%2520cases.%2520Using%2520clinical%2520human%2520evaluations%2520of%2520responses%2520is%250Aexpensive%252C%2520unscalable%252C%2520and%2520not%2520conducive%2520to%2520the%2520continuous%2520iterative%250Adevelopment%2520of%2520RAG%2520systems.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520ASTRID%2520-%250Aan%2520Automated%2520and%2520Scalable%2520TRIaD%2520for%2520evaluating%2520clinical%2520QA%2520systems%2520leveraging%250ARAG%2520-%2520consisting%2520of%2520three%2520metrics%253A%2520Context%2520Relevance%2520%2528CR%2529%252C%2520Refusal%2520Accuracy%250A%2528RA%2529%252C%2520and%2520Conversational%2520Faithfulness%2520%2528CF%2529.%2520Our%2520novel%2520evaluation%2520metric%252C%2520CF%252C%2520is%250Adesigned%2520to%2520better%2520capture%2520the%2520faithfulness%2520of%2520a%2520model%2527s%2520response%2520to%2520the%250Aknowledge%2520base%2520without%2520penalising%2520conversational%2520elements.%2520To%2520validate%2520our%250Atriad%252C%2520we%2520curate%2520a%2520dataset%2520of%2520over%2520200%2520real-world%2520patient%2520questions%2520posed%2520to%2520an%250ALLM-based%2520QA%2520agent%2520during%2520surgical%2520follow-up%2520for%2520cataract%2520surgery%2520-%2520the%2520highest%250Avolume%2520operation%2520in%2520the%2520world%2520-%2520augmented%2520with%2520clinician-selected%2520questions%2520for%250Aemergency%252C%2520clinical%252C%2520and%2520non-clinical%2520out-of-domain%2520scenarios.%2520We%2520demonstrate%250Athat%2520CF%2520can%2520predict%2520human%2520ratings%2520of%2520faithfulness%2520better%2520than%2520existing%250Adefinitions%2520for%2520conversational%2520use%2520cases.%2520Furthermore%252C%2520we%2520show%2520that%2520evaluation%250Ausing%2520our%2520triad%2520consisting%2520of%2520CF%252C%2520RA%252C%2520and%2520CR%2520exhibits%2520alignment%2520with%2520clinician%250Aassessment%2520for%2520inappropriate%252C%2520harmful%252C%2520or%2520unhelpful%2520responses.%2520Finally%252C%2520using%250Anine%2520different%2520LLMs%252C%2520we%2520demonstrate%2520that%2520the%2520three%2520metrics%2520can%2520closely%2520agree%250Awith%2520human%2520evaluations%252C%2520highlighting%2520the%2520potential%2520of%2520these%2520metrics%2520for%2520use%2520in%250ALLM-driven%2520automated%2520evaluation%2520pipelines.%2520We%2520also%2520publish%2520the%2520prompts%2520and%250Adatasets%2520for%2520these%2520experiments%252C%2520providing%2520valuable%2520resources%2520for%2520further%250Aresearch%2520and%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASTRID%20--%20An%20Automated%20and%20Scalable%20TRIaD%20for%20the%20Evaluation%20of%0A%20%20RAG-based%20Clinical%20Question%20Answering%20Systems&entry.906535625=Mohita%20Chowdhury%20and%20Yajie%20Vera%20He%20and%20Aisling%20Higham%20and%20Ernest%20Lim&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20potential%20in%20clinical%0Aquestion%20answering%20%28QA%29%2C%20with%20Retrieval%20Augmented%20Generation%20%28RAG%29%20emerging%20as%0Aa%20leading%20approach%20for%20ensuring%20the%20factual%20accuracy%20of%20model%20responses.%0AHowever%2C%20current%20automated%20RAG%20metrics%20perform%20poorly%20in%20clinical%20and%0Aconversational%20use%20cases.%20Using%20clinical%20human%20evaluations%20of%20responses%20is%0Aexpensive%2C%20unscalable%2C%20and%20not%20conducive%20to%20the%20continuous%20iterative%0Adevelopment%20of%20RAG%20systems.%20To%20address%20these%20challenges%2C%20we%20introduce%20ASTRID%20-%0Aan%20Automated%20and%20Scalable%20TRIaD%20for%20evaluating%20clinical%20QA%20systems%20leveraging%0ARAG%20-%20consisting%20of%20three%20metrics%3A%20Context%20Relevance%20%28CR%29%2C%20Refusal%20Accuracy%0A%28RA%29%2C%20and%20Conversational%20Faithfulness%20%28CF%29.%20Our%20novel%20evaluation%20metric%2C%20CF%2C%20is%0Adesigned%20to%20better%20capture%20the%20faithfulness%20of%20a%20model%27s%20response%20to%20the%0Aknowledge%20base%20without%20penalising%20conversational%20elements.%20To%20validate%20our%0Atriad%2C%20we%20curate%20a%20dataset%20of%20over%20200%20real-world%20patient%20questions%20posed%20to%20an%0ALLM-based%20QA%20agent%20during%20surgical%20follow-up%20for%20cataract%20surgery%20-%20the%20highest%0Avolume%20operation%20in%20the%20world%20-%20augmented%20with%20clinician-selected%20questions%20for%0Aemergency%2C%20clinical%2C%20and%20non-clinical%20out-of-domain%20scenarios.%20We%20demonstrate%0Athat%20CF%20can%20predict%20human%20ratings%20of%20faithfulness%20better%20than%20existing%0Adefinitions%20for%20conversational%20use%20cases.%20Furthermore%2C%20we%20show%20that%20evaluation%0Ausing%20our%20triad%20consisting%20of%20CF%2C%20RA%2C%20and%20CR%20exhibits%20alignment%20with%20clinician%0Aassessment%20for%20inappropriate%2C%20harmful%2C%20or%20unhelpful%20responses.%20Finally%2C%20using%0Anine%20different%20LLMs%2C%20we%20demonstrate%20that%20the%20three%20metrics%20can%20closely%20agree%0Awith%20human%20evaluations%2C%20highlighting%20the%20potential%20of%20these%20metrics%20for%20use%20in%0ALLM-driven%20automated%20evaluation%20pipelines.%20We%20also%20publish%20the%20prompts%20and%0Adatasets%20for%20these%20experiments%2C%20providing%20valuable%20resources%20for%20further%0Aresearch%20and%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08208v1&entry.124074799=Read"},
{"title": "Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I\n  Networks", "author": "Xiao Xu and Qiong Wu and Pingyi Fan and Kezhi Wang", "abstract": "  Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme\n", "link": "http://arxiv.org/abs/2501.08037v1", "date": "2025-01-14", "relevancy": 1.5815, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4085}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.3954}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20SPS%20Velocity-adaptive%20Scheme%3A%20Access%20Fariness%20in%205G%20NR%20V2I%0A%20%20Networks&body=Title%3A%20Enhanced%20SPS%20Velocity-adaptive%20Scheme%3A%20Access%20Fariness%20in%205G%20NR%20V2I%0A%20%20Networks%0AAuthor%3A%20Xiao%20Xu%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang%0AAbstract%3A%20%20%20Vehicle-to-Infrastructure%20%28V2I%29%20technology%20enables%20information%20exchange%0Abetween%20vehicles%20and%20road%20infrastructure.%20Specifically%2C%20when%20a%20vehicle%0Aapproaches%20a%20roadside%20unit%20%28RSU%29%2C%20it%20can%20exchange%20information%20with%20the%20RSU%20to%0Aobtain%20accurate%20data%20that%20assists%20in%20driving.%20With%20the%20release%20of%20the%203rd%0AGeneration%20Partnership%20Project%20%283GPP%29%20Release%2016%2C%20which%20includes%20the%205G%20New%0ARadio%20%28NR%29%20Vehicle-to-Everything%20%28V2X%29%20standards%2C%20vehicles%20typically%20adopt%0Amode-2%20communication%20using%20sensing-based%20semi-persistent%20scheduling%20%28SPS%29%20for%0Aresource%20allocation.%20In%20this%20approach%2C%20vehicles%20identify%20candidate%20resources%0Awithin%20a%20selection%20window%20and%20exclude%20ineligible%20resources%20based%20on%20information%0Afrom%20a%20sensing%20window.%20However%2C%20vehicles%20often%20drive%20at%20different%20speeds%2C%0Aresulting%20in%20varying%20amounts%20of%20data%20transmission%20with%20RSUs%20as%20they%20pass%20by%2C%0Awhich%20leads%20to%20unfair%20access.%20Therefore%2C%20it%20is%20essential%20to%20design%20an%20access%0Ascheme%20that%20accounts%20for%20different%20vehicle%20speeds%20to%20achieve%20fair%20access%20across%0Athe%20network.%20This%20paper%20formulates%20an%20optimization%20problem%20for%20vehicular%0Anetworks%20and%20proposes%20a%20multi-objective%20optimization%20scheme%20to%20address%20it%20by%0Aadjusting%20the%20selection%20window%20in%20the%20SPS%20mechanism%20of%205G%20NR%20V2I%20mode-2.%0ASimulation%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20scheme%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520SPS%2520Velocity-adaptive%2520Scheme%253A%2520Access%2520Fariness%2520in%25205G%2520NR%2520V2I%250A%2520%2520Networks%26entry.906535625%3DXiao%2520Xu%2520and%2520Qiong%2520Wu%2520and%2520Pingyi%2520Fan%2520and%2520Kezhi%2520Wang%26entry.1292438233%3D%2520%2520Vehicle-to-Infrastructure%2520%2528V2I%2529%2520technology%2520enables%2520information%2520exchange%250Abetween%2520vehicles%2520and%2520road%2520infrastructure.%2520Specifically%252C%2520when%2520a%2520vehicle%250Aapproaches%2520a%2520roadside%2520unit%2520%2528RSU%2529%252C%2520it%2520can%2520exchange%2520information%2520with%2520the%2520RSU%2520to%250Aobtain%2520accurate%2520data%2520that%2520assists%2520in%2520driving.%2520With%2520the%2520release%2520of%2520the%25203rd%250AGeneration%2520Partnership%2520Project%2520%25283GPP%2529%2520Release%252016%252C%2520which%2520includes%2520the%25205G%2520New%250ARadio%2520%2528NR%2529%2520Vehicle-to-Everything%2520%2528V2X%2529%2520standards%252C%2520vehicles%2520typically%2520adopt%250Amode-2%2520communication%2520using%2520sensing-based%2520semi-persistent%2520scheduling%2520%2528SPS%2529%2520for%250Aresource%2520allocation.%2520In%2520this%2520approach%252C%2520vehicles%2520identify%2520candidate%2520resources%250Awithin%2520a%2520selection%2520window%2520and%2520exclude%2520ineligible%2520resources%2520based%2520on%2520information%250Afrom%2520a%2520sensing%2520window.%2520However%252C%2520vehicles%2520often%2520drive%2520at%2520different%2520speeds%252C%250Aresulting%2520in%2520varying%2520amounts%2520of%2520data%2520transmission%2520with%2520RSUs%2520as%2520they%2520pass%2520by%252C%250Awhich%2520leads%2520to%2520unfair%2520access.%2520Therefore%252C%2520it%2520is%2520essential%2520to%2520design%2520an%2520access%250Ascheme%2520that%2520accounts%2520for%2520different%2520vehicle%2520speeds%2520to%2520achieve%2520fair%2520access%2520across%250Athe%2520network.%2520This%2520paper%2520formulates%2520an%2520optimization%2520problem%2520for%2520vehicular%250Anetworks%2520and%2520proposes%2520a%2520multi-objective%2520optimization%2520scheme%2520to%2520address%2520it%2520by%250Aadjusting%2520the%2520selection%2520window%2520in%2520the%2520SPS%2520mechanism%2520of%25205G%2520NR%2520V2I%2520mode-2.%250ASimulation%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520scheme%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20SPS%20Velocity-adaptive%20Scheme%3A%20Access%20Fariness%20in%205G%20NR%20V2I%0A%20%20Networks&entry.906535625=Xiao%20Xu%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Kezhi%20Wang&entry.1292438233=%20%20Vehicle-to-Infrastructure%20%28V2I%29%20technology%20enables%20information%20exchange%0Abetween%20vehicles%20and%20road%20infrastructure.%20Specifically%2C%20when%20a%20vehicle%0Aapproaches%20a%20roadside%20unit%20%28RSU%29%2C%20it%20can%20exchange%20information%20with%20the%20RSU%20to%0Aobtain%20accurate%20data%20that%20assists%20in%20driving.%20With%20the%20release%20of%20the%203rd%0AGeneration%20Partnership%20Project%20%283GPP%29%20Release%2016%2C%20which%20includes%20the%205G%20New%0ARadio%20%28NR%29%20Vehicle-to-Everything%20%28V2X%29%20standards%2C%20vehicles%20typically%20adopt%0Amode-2%20communication%20using%20sensing-based%20semi-persistent%20scheduling%20%28SPS%29%20for%0Aresource%20allocation.%20In%20this%20approach%2C%20vehicles%20identify%20candidate%20resources%0Awithin%20a%20selection%20window%20and%20exclude%20ineligible%20resources%20based%20on%20information%0Afrom%20a%20sensing%20window.%20However%2C%20vehicles%20often%20drive%20at%20different%20speeds%2C%0Aresulting%20in%20varying%20amounts%20of%20data%20transmission%20with%20RSUs%20as%20they%20pass%20by%2C%0Awhich%20leads%20to%20unfair%20access.%20Therefore%2C%20it%20is%20essential%20to%20design%20an%20access%0Ascheme%20that%20accounts%20for%20different%20vehicle%20speeds%20to%20achieve%20fair%20access%20across%0Athe%20network.%20This%20paper%20formulates%20an%20optimization%20problem%20for%20vehicular%0Anetworks%20and%20proposes%20a%20multi-objective%20optimization%20scheme%20to%20address%20it%20by%0Aadjusting%20the%20selection%20window%20in%20the%20SPS%20mechanism%20of%205G%20NR%20V2I%20mode-2.%0ASimulation%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20scheme%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08037v1&entry.124074799=Read"},
{"title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models", "author": "Junyu Chen and Han Cai and Junsong Chen and Enze Xie and Shang Yang and Haotian Tang and Muyang Li and Yao Lu and Song Han", "abstract": "  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.\n", "link": "http://arxiv.org/abs/2410.10733v4", "date": "2025-01-14", "relevancy": 1.8166, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6656}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5886}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models&body=Title%3A%20Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models%0AAuthor%3A%20Junyu%20Chen%20and%20Han%20Cai%20and%20Junsong%20Chen%20and%20Enze%20Xie%20and%20Shang%20Yang%20and%20Haotian%20Tang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20We%20present%20Deep%20Compression%20Autoencoder%20%28DC-AE%29%2C%20a%20new%20family%20of%20autoencoder%0Amodels%20for%20accelerating%20high-resolution%20diffusion%20models.%20Existing%20autoencoder%0Amodels%20have%20demonstrated%20impressive%20results%20at%20a%20moderate%20spatial%20compression%0Aratio%20%28e.g.%2C%208x%29%2C%20but%20fail%20to%20maintain%20satisfactory%20reconstruction%20accuracy%20for%0Ahigh%20spatial%20compression%20ratios%20%28e.g.%2C%2064x%29.%20We%20address%20this%20challenge%20by%0Aintroducing%20two%20key%20techniques%3A%20%281%29%20Residual%20Autoencoding%2C%20where%20we%20design%20our%0Amodels%20to%20learn%20residuals%20based%20on%20the%20space-to-channel%20transformed%20features%20to%0Aalleviate%20the%20optimization%20difficulty%20of%20high%20spatial-compression%20autoencoders%3B%0A%282%29%20Decoupled%20High-Resolution%20Adaptation%2C%20an%20efficient%20decoupled%20three-phases%0Atraining%20strategy%20for%20mitigating%20the%20generalization%20penalty%20of%20high%0Aspatial-compression%20autoencoders.%20With%20these%20designs%2C%20we%20improve%20the%0Aautoencoder%27s%20spatial%20compression%20ratio%20up%20to%20128%20while%20maintaining%20the%0Areconstruction%20quality.%20Applying%20our%20DC-AE%20to%20latent%20diffusion%20models%2C%20we%0Aachieve%20significant%20speedup%20without%20accuracy%20drop.%20For%20example%2C%20on%20ImageNet%0A512x512%2C%20our%20DC-AE%20provides%2019.1x%20inference%20speedup%20and%2017.9x%20training%20speedup%0Aon%20H100%20GPU%20for%20UViT-H%20while%20achieving%20a%20better%20FID%2C%20compared%20with%20the%20widely%0Aused%20SD-VAE-f8%20autoencoder.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-han-lab/efficientvit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10733v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Compression%2520Autoencoder%2520for%2520Efficient%2520High-Resolution%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DJunyu%2520Chen%2520and%2520Han%2520Cai%2520and%2520Junsong%2520Chen%2520and%2520Enze%2520Xie%2520and%2520Shang%2520Yang%2520and%2520Haotian%2520Tang%2520and%2520Muyang%2520Li%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520We%2520present%2520Deep%2520Compression%2520Autoencoder%2520%2528DC-AE%2529%252C%2520a%2520new%2520family%2520of%2520autoencoder%250Amodels%2520for%2520accelerating%2520high-resolution%2520diffusion%2520models.%2520Existing%2520autoencoder%250Amodels%2520have%2520demonstrated%2520impressive%2520results%2520at%2520a%2520moderate%2520spatial%2520compression%250Aratio%2520%2528e.g.%252C%25208x%2529%252C%2520but%2520fail%2520to%2520maintain%2520satisfactory%2520reconstruction%2520accuracy%2520for%250Ahigh%2520spatial%2520compression%2520ratios%2520%2528e.g.%252C%252064x%2529.%2520We%2520address%2520this%2520challenge%2520by%250Aintroducing%2520two%2520key%2520techniques%253A%2520%25281%2529%2520Residual%2520Autoencoding%252C%2520where%2520we%2520design%2520our%250Amodels%2520to%2520learn%2520residuals%2520based%2520on%2520the%2520space-to-channel%2520transformed%2520features%2520to%250Aalleviate%2520the%2520optimization%2520difficulty%2520of%2520high%2520spatial-compression%2520autoencoders%253B%250A%25282%2529%2520Decoupled%2520High-Resolution%2520Adaptation%252C%2520an%2520efficient%2520decoupled%2520three-phases%250Atraining%2520strategy%2520for%2520mitigating%2520the%2520generalization%2520penalty%2520of%2520high%250Aspatial-compression%2520autoencoders.%2520With%2520these%2520designs%252C%2520we%2520improve%2520the%250Aautoencoder%2527s%2520spatial%2520compression%2520ratio%2520up%2520to%2520128%2520while%2520maintaining%2520the%250Areconstruction%2520quality.%2520Applying%2520our%2520DC-AE%2520to%2520latent%2520diffusion%2520models%252C%2520we%250Aachieve%2520significant%2520speedup%2520without%2520accuracy%2520drop.%2520For%2520example%252C%2520on%2520ImageNet%250A512x512%252C%2520our%2520DC-AE%2520provides%252019.1x%2520inference%2520speedup%2520and%252017.9x%2520training%2520speedup%250Aon%2520H100%2520GPU%2520for%2520UViT-H%2520while%2520achieving%2520a%2520better%2520FID%252C%2520compared%2520with%2520the%2520widely%250Aused%2520SD-VAE-f8%2520autoencoder.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mit-han-lab/efficientvit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10733v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Compression%20Autoencoder%20for%20Efficient%20High-Resolution%20Diffusion%0A%20%20Models&entry.906535625=Junyu%20Chen%20and%20Han%20Cai%20and%20Junsong%20Chen%20and%20Enze%20Xie%20and%20Shang%20Yang%20and%20Haotian%20Tang%20and%20Muyang%20Li%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20We%20present%20Deep%20Compression%20Autoencoder%20%28DC-AE%29%2C%20a%20new%20family%20of%20autoencoder%0Amodels%20for%20accelerating%20high-resolution%20diffusion%20models.%20Existing%20autoencoder%0Amodels%20have%20demonstrated%20impressive%20results%20at%20a%20moderate%20spatial%20compression%0Aratio%20%28e.g.%2C%208x%29%2C%20but%20fail%20to%20maintain%20satisfactory%20reconstruction%20accuracy%20for%0Ahigh%20spatial%20compression%20ratios%20%28e.g.%2C%2064x%29.%20We%20address%20this%20challenge%20by%0Aintroducing%20two%20key%20techniques%3A%20%281%29%20Residual%20Autoencoding%2C%20where%20we%20design%20our%0Amodels%20to%20learn%20residuals%20based%20on%20the%20space-to-channel%20transformed%20features%20to%0Aalleviate%20the%20optimization%20difficulty%20of%20high%20spatial-compression%20autoencoders%3B%0A%282%29%20Decoupled%20High-Resolution%20Adaptation%2C%20an%20efficient%20decoupled%20three-phases%0Atraining%20strategy%20for%20mitigating%20the%20generalization%20penalty%20of%20high%0Aspatial-compression%20autoencoders.%20With%20these%20designs%2C%20we%20improve%20the%0Aautoencoder%27s%20spatial%20compression%20ratio%20up%20to%20128%20while%20maintaining%20the%0Areconstruction%20quality.%20Applying%20our%20DC-AE%20to%20latent%20diffusion%20models%2C%20we%0Aachieve%20significant%20speedup%20without%20accuracy%20drop.%20For%20example%2C%20on%20ImageNet%0A512x512%2C%20our%20DC-AE%20provides%2019.1x%20inference%20speedup%20and%2017.9x%20training%20speedup%0Aon%20H100%20GPU%20for%20UViT-H%20while%20achieving%20a%20better%20FID%2C%20compared%20with%20the%20widely%0Aused%20SD-VAE-f8%20autoencoder.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-han-lab/efficientvit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10733v4&entry.124074799=Read"},
{"title": "PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning", "author": "Marta Andronic and Jiawen Li and George A. Constantinides", "abstract": "  Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST.\n", "link": "http://arxiv.org/abs/2501.08043v1", "date": "2025-01-14", "relevancy": 1.8509, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4581}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyLUT%3A%20Ultra-low%20Latency%20Polynomial%20Inference%20with%20Hardware-Aware%0A%20%20Structured%20Pruning&body=Title%3A%20PolyLUT%3A%20Ultra-low%20Latency%20Polynomial%20Inference%20with%20Hardware-Aware%0A%20%20Structured%20Pruning%0AAuthor%3A%20Marta%20Andronic%20and%20Jiawen%20Li%20and%20George%20A.%20Constantinides%0AAbstract%3A%20%20%20Standard%20deep%20neural%20network%20inference%20involves%20the%20computation%20of%0Ainterleaved%20linear%20maps%20and%20nonlinear%20activation%20functions.%20Prior%20work%20for%0Aultra-low%20latency%20implementations%20has%20hardcoded%20these%20operations%20inside%20FPGA%0Alookup%20tables%20%28LUTs%29.%20However%2C%20FPGA%20LUTs%20can%20implement%20a%20much%20greater%20variety%0Aof%20functions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20training%20DNNs%20for%0AFPGA%20deployment%20using%20multivariate%20polynomials%20as%20the%20basic%20building%20block.%20Our%0Amethod%20takes%20advantage%20of%20the%20flexibility%20offered%20by%20the%20soft%20logic%2C%20hiding%20the%0Apolynomial%20evaluation%20inside%20the%20LUTs%20with%20minimal%20overhead.%20By%20using%0Apolynomial%20building%20blocks%2C%20we%20achieve%20the%20same%20accuracy%20using%20considerably%0Afewer%20layers%20of%20soft%20logic%20than%20by%20using%20linear%20functions%2C%20leading%20to%0Asignificant%20latency%20and%20area%20improvements.%20LUT-based%20implementations%20also%20face%0Aa%20significant%20challenge%3A%20the%20LUT%20size%20grows%20exponentially%20with%20the%20number%20of%0Ainputs.%20Prior%20work%20relies%20on%20a%20priori%20fixed%20sparsity%2C%20with%20results%20heavily%0Adependent%20on%20seed%20selection.%20To%20address%20this%2C%20we%20propose%20a%20structured%20pruning%0Astrategy%20using%20a%20bespoke%20hardware-aware%20group%20regularizer%20that%20encourages%20a%0Aparticular%20sparsity%20pattern%20that%20leads%20to%20a%20small%20number%20of%20inputs%20per%20neuron.%0AWe%20demonstrate%20the%20effectiveness%20of%20PolyLUT%20on%20three%20tasks%3A%20network%20intrusion%0Adetection%2C%20jet%20identification%20at%20the%20CERN%20Large%20Hadron%20Collider%2C%20and%20MNIST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyLUT%253A%2520Ultra-low%2520Latency%2520Polynomial%2520Inference%2520with%2520Hardware-Aware%250A%2520%2520Structured%2520Pruning%26entry.906535625%3DMarta%2520Andronic%2520and%2520Jiawen%2520Li%2520and%2520George%2520A.%2520Constantinides%26entry.1292438233%3D%2520%2520Standard%2520deep%2520neural%2520network%2520inference%2520involves%2520the%2520computation%2520of%250Ainterleaved%2520linear%2520maps%2520and%2520nonlinear%2520activation%2520functions.%2520Prior%2520work%2520for%250Aultra-low%2520latency%2520implementations%2520has%2520hardcoded%2520these%2520operations%2520inside%2520FPGA%250Alookup%2520tables%2520%2528LUTs%2529.%2520However%252C%2520FPGA%2520LUTs%2520can%2520implement%2520a%2520much%2520greater%2520variety%250Aof%2520functions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520training%2520DNNs%2520for%250AFPGA%2520deployment%2520using%2520multivariate%2520polynomials%2520as%2520the%2520basic%2520building%2520block.%2520Our%250Amethod%2520takes%2520advantage%2520of%2520the%2520flexibility%2520offered%2520by%2520the%2520soft%2520logic%252C%2520hiding%2520the%250Apolynomial%2520evaluation%2520inside%2520the%2520LUTs%2520with%2520minimal%2520overhead.%2520By%2520using%250Apolynomial%2520building%2520blocks%252C%2520we%2520achieve%2520the%2520same%2520accuracy%2520using%2520considerably%250Afewer%2520layers%2520of%2520soft%2520logic%2520than%2520by%2520using%2520linear%2520functions%252C%2520leading%2520to%250Asignificant%2520latency%2520and%2520area%2520improvements.%2520LUT-based%2520implementations%2520also%2520face%250Aa%2520significant%2520challenge%253A%2520the%2520LUT%2520size%2520grows%2520exponentially%2520with%2520the%2520number%2520of%250Ainputs.%2520Prior%2520work%2520relies%2520on%2520a%2520priori%2520fixed%2520sparsity%252C%2520with%2520results%2520heavily%250Adependent%2520on%2520seed%2520selection.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520structured%2520pruning%250Astrategy%2520using%2520a%2520bespoke%2520hardware-aware%2520group%2520regularizer%2520that%2520encourages%2520a%250Aparticular%2520sparsity%2520pattern%2520that%2520leads%2520to%2520a%2520small%2520number%2520of%2520inputs%2520per%2520neuron.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520PolyLUT%2520on%2520three%2520tasks%253A%2520network%2520intrusion%250Adetection%252C%2520jet%2520identification%2520at%2520the%2520CERN%2520Large%2520Hadron%2520Collider%252C%2520and%2520MNIST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyLUT%3A%20Ultra-low%20Latency%20Polynomial%20Inference%20with%20Hardware-Aware%0A%20%20Structured%20Pruning&entry.906535625=Marta%20Andronic%20and%20Jiawen%20Li%20and%20George%20A.%20Constantinides&entry.1292438233=%20%20Standard%20deep%20neural%20network%20inference%20involves%20the%20computation%20of%0Ainterleaved%20linear%20maps%20and%20nonlinear%20activation%20functions.%20Prior%20work%20for%0Aultra-low%20latency%20implementations%20has%20hardcoded%20these%20operations%20inside%20FPGA%0Alookup%20tables%20%28LUTs%29.%20However%2C%20FPGA%20LUTs%20can%20implement%20a%20much%20greater%20variety%0Aof%20functions.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20training%20DNNs%20for%0AFPGA%20deployment%20using%20multivariate%20polynomials%20as%20the%20basic%20building%20block.%20Our%0Amethod%20takes%20advantage%20of%20the%20flexibility%20offered%20by%20the%20soft%20logic%2C%20hiding%20the%0Apolynomial%20evaluation%20inside%20the%20LUTs%20with%20minimal%20overhead.%20By%20using%0Apolynomial%20building%20blocks%2C%20we%20achieve%20the%20same%20accuracy%20using%20considerably%0Afewer%20layers%20of%20soft%20logic%20than%20by%20using%20linear%20functions%2C%20leading%20to%0Asignificant%20latency%20and%20area%20improvements.%20LUT-based%20implementations%20also%20face%0Aa%20significant%20challenge%3A%20the%20LUT%20size%20grows%20exponentially%20with%20the%20number%20of%0Ainputs.%20Prior%20work%20relies%20on%20a%20priori%20fixed%20sparsity%2C%20with%20results%20heavily%0Adependent%20on%20seed%20selection.%20To%20address%20this%2C%20we%20propose%20a%20structured%20pruning%0Astrategy%20using%20a%20bespoke%20hardware-aware%20group%20regularizer%20that%20encourages%20a%0Aparticular%20sparsity%20pattern%20that%20leads%20to%20a%20small%20number%20of%20inputs%20per%20neuron.%0AWe%20demonstrate%20the%20effectiveness%20of%20PolyLUT%20on%20three%20tasks%3A%20network%20intrusion%0Adetection%2C%20jet%20identification%20at%20the%20CERN%20Large%20Hadron%20Collider%2C%20and%20MNIST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08043v1&entry.124074799=Read"},
{"title": "Artificial Liver Classifier: A New Alternative to Conventional Machine\n  Learning Models", "author": "Mahmood A. Jumaah and Yossra H. Ali and Tarik A. Rashid", "abstract": "  Supervised machine learning classifiers often encounter challenges related to\nperformance, accuracy, and overfitting. This paper introduces the Artificial\nLiver Classifier (ALC), a novel supervised learning classifier inspired by the\nhuman liver's detoxification function. The ALC is characterized by its\nsimplicity, speed, hyperparameters-free, ability to reduce overfitting, and\neffectiveness in addressing multi-classification problems through\nstraightforward mathematical operations. To optimize the ALC's parameters, an\nimproved FOX optimization algorithm (IFOX) is employed as the training method.\nThe proposed ALC was evaluated on five benchmark machine learning datasets:\nIris Flower, Breast Cancer Wisconsin, Wine, Voice Gender, and MNIST. The\nresults demonstrated competitive performance, with the ALC achieving 100%\naccuracy on the Iris dataset, surpassing logistic regression, multilayer\nperceptron, and support vector machine. Similarly, on the Breast Cancer\ndataset, it achieved 99.12% accuracy, outperforming XGBoost and logistic\nregression. Across all datasets, the ALC consistently exhibited lower\noverfitting gaps and loss compared to conventional classifiers. These findings\nhighlight the potential of leveraging biological process simulations to develop\nefficient machine learning models and open new avenues for innovation in the\nfield.\n", "link": "http://arxiv.org/abs/2501.08074v1", "date": "2025-01-14", "relevancy": 1.8189, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4551}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.455}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Liver%20Classifier%3A%20A%20New%20Alternative%20to%20Conventional%20Machine%0A%20%20Learning%20Models&body=Title%3A%20Artificial%20Liver%20Classifier%3A%20A%20New%20Alternative%20to%20Conventional%20Machine%0A%20%20Learning%20Models%0AAuthor%3A%20Mahmood%20A.%20Jumaah%20and%20Yossra%20H.%20Ali%20and%20Tarik%20A.%20Rashid%0AAbstract%3A%20%20%20Supervised%20machine%20learning%20classifiers%20often%20encounter%20challenges%20related%20to%0Aperformance%2C%20accuracy%2C%20and%20overfitting.%20This%20paper%20introduces%20the%20Artificial%0ALiver%20Classifier%20%28ALC%29%2C%20a%20novel%20supervised%20learning%20classifier%20inspired%20by%20the%0Ahuman%20liver%27s%20detoxification%20function.%20The%20ALC%20is%20characterized%20by%20its%0Asimplicity%2C%20speed%2C%20hyperparameters-free%2C%20ability%20to%20reduce%20overfitting%2C%20and%0Aeffectiveness%20in%20addressing%20multi-classification%20problems%20through%0Astraightforward%20mathematical%20operations.%20To%20optimize%20the%20ALC%27s%20parameters%2C%20an%0Aimproved%20FOX%20optimization%20algorithm%20%28IFOX%29%20is%20employed%20as%20the%20training%20method.%0AThe%20proposed%20ALC%20was%20evaluated%20on%20five%20benchmark%20machine%20learning%20datasets%3A%0AIris%20Flower%2C%20Breast%20Cancer%20Wisconsin%2C%20Wine%2C%20Voice%20Gender%2C%20and%20MNIST.%20The%0Aresults%20demonstrated%20competitive%20performance%2C%20with%20the%20ALC%20achieving%20100%25%0Aaccuracy%20on%20the%20Iris%20dataset%2C%20surpassing%20logistic%20regression%2C%20multilayer%0Aperceptron%2C%20and%20support%20vector%20machine.%20Similarly%2C%20on%20the%20Breast%20Cancer%0Adataset%2C%20it%20achieved%2099.12%25%20accuracy%2C%20outperforming%20XGBoost%20and%20logistic%0Aregression.%20Across%20all%20datasets%2C%20the%20ALC%20consistently%20exhibited%20lower%0Aoverfitting%20gaps%20and%20loss%20compared%20to%20conventional%20classifiers.%20These%20findings%0Ahighlight%20the%20potential%20of%20leveraging%20biological%20process%20simulations%20to%20develop%0Aefficient%20machine%20learning%20models%20and%20open%20new%20avenues%20for%20innovation%20in%20the%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Liver%2520Classifier%253A%2520A%2520New%2520Alternative%2520to%2520Conventional%2520Machine%250A%2520%2520Learning%2520Models%26entry.906535625%3DMahmood%2520A.%2520Jumaah%2520and%2520Yossra%2520H.%2520Ali%2520and%2520Tarik%2520A.%2520Rashid%26entry.1292438233%3D%2520%2520Supervised%2520machine%2520learning%2520classifiers%2520often%2520encounter%2520challenges%2520related%2520to%250Aperformance%252C%2520accuracy%252C%2520and%2520overfitting.%2520This%2520paper%2520introduces%2520the%2520Artificial%250ALiver%2520Classifier%2520%2528ALC%2529%252C%2520a%2520novel%2520supervised%2520learning%2520classifier%2520inspired%2520by%2520the%250Ahuman%2520liver%2527s%2520detoxification%2520function.%2520The%2520ALC%2520is%2520characterized%2520by%2520its%250Asimplicity%252C%2520speed%252C%2520hyperparameters-free%252C%2520ability%2520to%2520reduce%2520overfitting%252C%2520and%250Aeffectiveness%2520in%2520addressing%2520multi-classification%2520problems%2520through%250Astraightforward%2520mathematical%2520operations.%2520To%2520optimize%2520the%2520ALC%2527s%2520parameters%252C%2520an%250Aimproved%2520FOX%2520optimization%2520algorithm%2520%2528IFOX%2529%2520is%2520employed%2520as%2520the%2520training%2520method.%250AThe%2520proposed%2520ALC%2520was%2520evaluated%2520on%2520five%2520benchmark%2520machine%2520learning%2520datasets%253A%250AIris%2520Flower%252C%2520Breast%2520Cancer%2520Wisconsin%252C%2520Wine%252C%2520Voice%2520Gender%252C%2520and%2520MNIST.%2520The%250Aresults%2520demonstrated%2520competitive%2520performance%252C%2520with%2520the%2520ALC%2520achieving%2520100%2525%250Aaccuracy%2520on%2520the%2520Iris%2520dataset%252C%2520surpassing%2520logistic%2520regression%252C%2520multilayer%250Aperceptron%252C%2520and%2520support%2520vector%2520machine.%2520Similarly%252C%2520on%2520the%2520Breast%2520Cancer%250Adataset%252C%2520it%2520achieved%252099.12%2525%2520accuracy%252C%2520outperforming%2520XGBoost%2520and%2520logistic%250Aregression.%2520Across%2520all%2520datasets%252C%2520the%2520ALC%2520consistently%2520exhibited%2520lower%250Aoverfitting%2520gaps%2520and%2520loss%2520compared%2520to%2520conventional%2520classifiers.%2520These%2520findings%250Ahighlight%2520the%2520potential%2520of%2520leveraging%2520biological%2520process%2520simulations%2520to%2520develop%250Aefficient%2520machine%2520learning%2520models%2520and%2520open%2520new%2520avenues%2520for%2520innovation%2520in%2520the%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Liver%20Classifier%3A%20A%20New%20Alternative%20to%20Conventional%20Machine%0A%20%20Learning%20Models&entry.906535625=Mahmood%20A.%20Jumaah%20and%20Yossra%20H.%20Ali%20and%20Tarik%20A.%20Rashid&entry.1292438233=%20%20Supervised%20machine%20learning%20classifiers%20often%20encounter%20challenges%20related%20to%0Aperformance%2C%20accuracy%2C%20and%20overfitting.%20This%20paper%20introduces%20the%20Artificial%0ALiver%20Classifier%20%28ALC%29%2C%20a%20novel%20supervised%20learning%20classifier%20inspired%20by%20the%0Ahuman%20liver%27s%20detoxification%20function.%20The%20ALC%20is%20characterized%20by%20its%0Asimplicity%2C%20speed%2C%20hyperparameters-free%2C%20ability%20to%20reduce%20overfitting%2C%20and%0Aeffectiveness%20in%20addressing%20multi-classification%20problems%20through%0Astraightforward%20mathematical%20operations.%20To%20optimize%20the%20ALC%27s%20parameters%2C%20an%0Aimproved%20FOX%20optimization%20algorithm%20%28IFOX%29%20is%20employed%20as%20the%20training%20method.%0AThe%20proposed%20ALC%20was%20evaluated%20on%20five%20benchmark%20machine%20learning%20datasets%3A%0AIris%20Flower%2C%20Breast%20Cancer%20Wisconsin%2C%20Wine%2C%20Voice%20Gender%2C%20and%20MNIST.%20The%0Aresults%20demonstrated%20competitive%20performance%2C%20with%20the%20ALC%20achieving%20100%25%0Aaccuracy%20on%20the%20Iris%20dataset%2C%20surpassing%20logistic%20regression%2C%20multilayer%0Aperceptron%2C%20and%20support%20vector%20machine.%20Similarly%2C%20on%20the%20Breast%20Cancer%0Adataset%2C%20it%20achieved%2099.12%25%20accuracy%2C%20outperforming%20XGBoost%20and%20logistic%0Aregression.%20Across%20all%20datasets%2C%20the%20ALC%20consistently%20exhibited%20lower%0Aoverfitting%20gaps%20and%20loss%20compared%20to%20conventional%20classifiers.%20These%20findings%0Ahighlight%20the%20potential%20of%20leveraging%20biological%20process%20simulations%20to%20develop%0Aefficient%20machine%20learning%20models%20and%20open%20new%20avenues%20for%20innovation%20in%20the%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08074v1&entry.124074799=Read"},
{"title": "Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and\n  Deep Image Prior Models", "author": "Shuo Li and Mehrdad Yaghoobi", "abstract": "  Hyperspectral images are typically composed of hundreds of narrow and\ncontiguous spectral bands, each containing information regarding the material\ncomposition of the imaged scene. However, these images can be affected by\nvarious sources of noise, distortions, or data loss, which can significantly\ndegrade their quality and usefulness. This paper introduces a convergent\nguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the\ninstability issue of DHP that has been reported before. The proposed algorithm\nextends the successful joint low-rank and sparse model to further exploit the\nunderlying data structures beyond the conventional and sometimes restrictive\nunions of subspace models. A stability analysis guarantees the convergence of\nthe proposed algorithm under mild assumptions , which is crucial for its\napplication in real-world scenarios. Extensive experiments demonstrate that the\nproposed solution consistently delivers visually and quantitatively superior\ninpainting results, establishing state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2501.08195v1", "date": "2025-01-14", "relevancy": 1.6874, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5795}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5582}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Deep%20Hyperspectral%20Inpainting%20with%20the%20Plug%20and%20Play%20and%0A%20%20Deep%20Image%20Prior%20Models&body=Title%3A%20Self-supervised%20Deep%20Hyperspectral%20Inpainting%20with%20the%20Plug%20and%20Play%20and%0A%20%20Deep%20Image%20Prior%20Models%0AAuthor%3A%20Shuo%20Li%20and%20Mehrdad%20Yaghoobi%0AAbstract%3A%20%20%20Hyperspectral%20images%20are%20typically%20composed%20of%20hundreds%20of%20narrow%20and%0Acontiguous%20spectral%20bands%2C%20each%20containing%20information%20regarding%20the%20material%0Acomposition%20of%20the%20imaged%20scene.%20However%2C%20these%20images%20can%20be%20affected%20by%0Avarious%20sources%20of%20noise%2C%20distortions%2C%20or%20data%20loss%2C%20which%20can%20significantly%0Adegrade%20their%20quality%20and%20usefulness.%20This%20paper%20introduces%20a%20convergent%0Aguaranteed%20algorithm%2C%20LRS-PnP-DIP%281-Lip%29%2C%20which%20successfully%20addresses%20the%0Ainstability%20issue%20of%20DHP%20that%20has%20been%20reported%20before.%20The%20proposed%20algorithm%0Aextends%20the%20successful%20joint%20low-rank%20and%20sparse%20model%20to%20further%20exploit%20the%0Aunderlying%20data%20structures%20beyond%20the%20conventional%20and%20sometimes%20restrictive%0Aunions%20of%20subspace%20models.%20A%20stability%20analysis%20guarantees%20the%20convergence%20of%0Athe%20proposed%20algorithm%20under%20mild%20assumptions%20%2C%20which%20is%20crucial%20for%20its%0Aapplication%20in%20real-world%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20solution%20consistently%20delivers%20visually%20and%20quantitatively%20superior%0Ainpainting%20results%2C%20establishing%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Deep%2520Hyperspectral%2520Inpainting%2520with%2520the%2520Plug%2520and%2520Play%2520and%250A%2520%2520Deep%2520Image%2520Prior%2520Models%26entry.906535625%3DShuo%2520Li%2520and%2520Mehrdad%2520Yaghoobi%26entry.1292438233%3D%2520%2520Hyperspectral%2520images%2520are%2520typically%2520composed%2520of%2520hundreds%2520of%2520narrow%2520and%250Acontiguous%2520spectral%2520bands%252C%2520each%2520containing%2520information%2520regarding%2520the%2520material%250Acomposition%2520of%2520the%2520imaged%2520scene.%2520However%252C%2520these%2520images%2520can%2520be%2520affected%2520by%250Avarious%2520sources%2520of%2520noise%252C%2520distortions%252C%2520or%2520data%2520loss%252C%2520which%2520can%2520significantly%250Adegrade%2520their%2520quality%2520and%2520usefulness.%2520This%2520paper%2520introduces%2520a%2520convergent%250Aguaranteed%2520algorithm%252C%2520LRS-PnP-DIP%25281-Lip%2529%252C%2520which%2520successfully%2520addresses%2520the%250Ainstability%2520issue%2520of%2520DHP%2520that%2520has%2520been%2520reported%2520before.%2520The%2520proposed%2520algorithm%250Aextends%2520the%2520successful%2520joint%2520low-rank%2520and%2520sparse%2520model%2520to%2520further%2520exploit%2520the%250Aunderlying%2520data%2520structures%2520beyond%2520the%2520conventional%2520and%2520sometimes%2520restrictive%250Aunions%2520of%2520subspace%2520models.%2520A%2520stability%2520analysis%2520guarantees%2520the%2520convergence%2520of%250Athe%2520proposed%2520algorithm%2520under%2520mild%2520assumptions%2520%252C%2520which%2520is%2520crucial%2520for%2520its%250Aapplication%2520in%2520real-world%2520scenarios.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%250Aproposed%2520solution%2520consistently%2520delivers%2520visually%2520and%2520quantitatively%2520superior%250Ainpainting%2520results%252C%2520establishing%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Deep%20Hyperspectral%20Inpainting%20with%20the%20Plug%20and%20Play%20and%0A%20%20Deep%20Image%20Prior%20Models&entry.906535625=Shuo%20Li%20and%20Mehrdad%20Yaghoobi&entry.1292438233=%20%20Hyperspectral%20images%20are%20typically%20composed%20of%20hundreds%20of%20narrow%20and%0Acontiguous%20spectral%20bands%2C%20each%20containing%20information%20regarding%20the%20material%0Acomposition%20of%20the%20imaged%20scene.%20However%2C%20these%20images%20can%20be%20affected%20by%0Avarious%20sources%20of%20noise%2C%20distortions%2C%20or%20data%20loss%2C%20which%20can%20significantly%0Adegrade%20their%20quality%20and%20usefulness.%20This%20paper%20introduces%20a%20convergent%0Aguaranteed%20algorithm%2C%20LRS-PnP-DIP%281-Lip%29%2C%20which%20successfully%20addresses%20the%0Ainstability%20issue%20of%20DHP%20that%20has%20been%20reported%20before.%20The%20proposed%20algorithm%0Aextends%20the%20successful%20joint%20low-rank%20and%20sparse%20model%20to%20further%20exploit%20the%0Aunderlying%20data%20structures%20beyond%20the%20conventional%20and%20sometimes%20restrictive%0Aunions%20of%20subspace%20models.%20A%20stability%20analysis%20guarantees%20the%20convergence%20of%0Athe%20proposed%20algorithm%20under%20mild%20assumptions%20%2C%20which%20is%20crucial%20for%20its%0Aapplication%20in%20real-world%20scenarios.%20Extensive%20experiments%20demonstrate%20that%20the%0Aproposed%20solution%20consistently%20delivers%20visually%20and%20quantitatively%20superior%0Ainpainting%20results%2C%20establishing%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08195v1&entry.124074799=Read"},
{"title": "An AI-driven framework for rapid and localized optimizations of urban\n  open spaces", "author": "Pegah Eshraghi and Arman Nikkhah Dehnavi and Maedeh Mirdamadi and Riccardo Talami and Zahra-Sadat Zomorodian", "abstract": "  As urbanization accelerates, open spaces are increasingly recognized for\ntheir role in enhancing sustainability and well-being, yet they remain\nunderexplored compared to built spaces. This study introduces an AI-driven\nframework that integrates machine learning models (MLMs) and explainable AI\ntechniques to optimize Sky View Factor (SVF) and visibility, key spatial\nmetrics influencing thermal comfort and perceived safety in urban spaces.\nUnlike global optimization methods, which are computationally intensive and\nimpractical for localized adjustments, this framework supports incremental\ndesign improvements with lower computational costs and greater flexibility. The\nframework employs SHapley Adaptive Explanations (SHAP) to analyze feature\nimportance and Counterfactual Explanations (CFXs) to propose minimal design\nchanges. Simulations tested five MLMs, identifying XGBoost as the most\naccurate, with building width, park area, and heights of surrounding buildings\nas critical for SVF, and distances from southern buildings as key for\nvisibility. Compared to Genetic Algorithms, which required approximately 15/30\nminutes across 3/4 generations to converge, the tested CFX approach achieved\noptimized results in 1 minute with a 5% RMSE error, demonstrating significantly\nfaster performance and suitability for scalable retrofitting strategies. This\ninterpretable and computationally efficient framework advances urban\nperformance optimization, providing data-driven insights and practical\nretrofitting solutions for enhancing usability and environmental quality across\ndiverse urban contexts.\n", "link": "http://arxiv.org/abs/2501.08019v1", "date": "2025-01-14", "relevancy": 1.4984, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20AI-driven%20framework%20for%20rapid%20and%20localized%20optimizations%20of%20urban%0A%20%20open%20spaces&body=Title%3A%20An%20AI-driven%20framework%20for%20rapid%20and%20localized%20optimizations%20of%20urban%0A%20%20open%20spaces%0AAuthor%3A%20Pegah%20Eshraghi%20and%20Arman%20Nikkhah%20Dehnavi%20and%20Maedeh%20Mirdamadi%20and%20Riccardo%20Talami%20and%20Zahra-Sadat%20Zomorodian%0AAbstract%3A%20%20%20As%20urbanization%20accelerates%2C%20open%20spaces%20are%20increasingly%20recognized%20for%0Atheir%20role%20in%20enhancing%20sustainability%20and%20well-being%2C%20yet%20they%20remain%0Aunderexplored%20compared%20to%20built%20spaces.%20This%20study%20introduces%20an%20AI-driven%0Aframework%20that%20integrates%20machine%20learning%20models%20%28MLMs%29%20and%20explainable%20AI%0Atechniques%20to%20optimize%20Sky%20View%20Factor%20%28SVF%29%20and%20visibility%2C%20key%20spatial%0Ametrics%20influencing%20thermal%20comfort%20and%20perceived%20safety%20in%20urban%20spaces.%0AUnlike%20global%20optimization%20methods%2C%20which%20are%20computationally%20intensive%20and%0Aimpractical%20for%20localized%20adjustments%2C%20this%20framework%20supports%20incremental%0Adesign%20improvements%20with%20lower%20computational%20costs%20and%20greater%20flexibility.%20The%0Aframework%20employs%20SHapley%20Adaptive%20Explanations%20%28SHAP%29%20to%20analyze%20feature%0Aimportance%20and%20Counterfactual%20Explanations%20%28CFXs%29%20to%20propose%20minimal%20design%0Achanges.%20Simulations%20tested%20five%20MLMs%2C%20identifying%20XGBoost%20as%20the%20most%0Aaccurate%2C%20with%20building%20width%2C%20park%20area%2C%20and%20heights%20of%20surrounding%20buildings%0Aas%20critical%20for%20SVF%2C%20and%20distances%20from%20southern%20buildings%20as%20key%20for%0Avisibility.%20Compared%20to%20Genetic%20Algorithms%2C%20which%20required%20approximately%2015/30%0Aminutes%20across%203/4%20generations%20to%20converge%2C%20the%20tested%20CFX%20approach%20achieved%0Aoptimized%20results%20in%201%20minute%20with%20a%205%25%20RMSE%20error%2C%20demonstrating%20significantly%0Afaster%20performance%20and%20suitability%20for%20scalable%20retrofitting%20strategies.%20This%0Ainterpretable%20and%20computationally%20efficient%20framework%20advances%20urban%0Aperformance%20optimization%2C%20providing%20data-driven%20insights%20and%20practical%0Aretrofitting%20solutions%20for%20enhancing%20usability%20and%20environmental%20quality%20across%0Adiverse%20urban%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520AI-driven%2520framework%2520for%2520rapid%2520and%2520localized%2520optimizations%2520of%2520urban%250A%2520%2520open%2520spaces%26entry.906535625%3DPegah%2520Eshraghi%2520and%2520Arman%2520Nikkhah%2520Dehnavi%2520and%2520Maedeh%2520Mirdamadi%2520and%2520Riccardo%2520Talami%2520and%2520Zahra-Sadat%2520Zomorodian%26entry.1292438233%3D%2520%2520As%2520urbanization%2520accelerates%252C%2520open%2520spaces%2520are%2520increasingly%2520recognized%2520for%250Atheir%2520role%2520in%2520enhancing%2520sustainability%2520and%2520well-being%252C%2520yet%2520they%2520remain%250Aunderexplored%2520compared%2520to%2520built%2520spaces.%2520This%2520study%2520introduces%2520an%2520AI-driven%250Aframework%2520that%2520integrates%2520machine%2520learning%2520models%2520%2528MLMs%2529%2520and%2520explainable%2520AI%250Atechniques%2520to%2520optimize%2520Sky%2520View%2520Factor%2520%2528SVF%2529%2520and%2520visibility%252C%2520key%2520spatial%250Ametrics%2520influencing%2520thermal%2520comfort%2520and%2520perceived%2520safety%2520in%2520urban%2520spaces.%250AUnlike%2520global%2520optimization%2520methods%252C%2520which%2520are%2520computationally%2520intensive%2520and%250Aimpractical%2520for%2520localized%2520adjustments%252C%2520this%2520framework%2520supports%2520incremental%250Adesign%2520improvements%2520with%2520lower%2520computational%2520costs%2520and%2520greater%2520flexibility.%2520The%250Aframework%2520employs%2520SHapley%2520Adaptive%2520Explanations%2520%2528SHAP%2529%2520to%2520analyze%2520feature%250Aimportance%2520and%2520Counterfactual%2520Explanations%2520%2528CFXs%2529%2520to%2520propose%2520minimal%2520design%250Achanges.%2520Simulations%2520tested%2520five%2520MLMs%252C%2520identifying%2520XGBoost%2520as%2520the%2520most%250Aaccurate%252C%2520with%2520building%2520width%252C%2520park%2520area%252C%2520and%2520heights%2520of%2520surrounding%2520buildings%250Aas%2520critical%2520for%2520SVF%252C%2520and%2520distances%2520from%2520southern%2520buildings%2520as%2520key%2520for%250Avisibility.%2520Compared%2520to%2520Genetic%2520Algorithms%252C%2520which%2520required%2520approximately%252015/30%250Aminutes%2520across%25203/4%2520generations%2520to%2520converge%252C%2520the%2520tested%2520CFX%2520approach%2520achieved%250Aoptimized%2520results%2520in%25201%2520minute%2520with%2520a%25205%2525%2520RMSE%2520error%252C%2520demonstrating%2520significantly%250Afaster%2520performance%2520and%2520suitability%2520for%2520scalable%2520retrofitting%2520strategies.%2520This%250Ainterpretable%2520and%2520computationally%2520efficient%2520framework%2520advances%2520urban%250Aperformance%2520optimization%252C%2520providing%2520data-driven%2520insights%2520and%2520practical%250Aretrofitting%2520solutions%2520for%2520enhancing%2520usability%2520and%2520environmental%2520quality%2520across%250Adiverse%2520urban%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20AI-driven%20framework%20for%20rapid%20and%20localized%20optimizations%20of%20urban%0A%20%20open%20spaces&entry.906535625=Pegah%20Eshraghi%20and%20Arman%20Nikkhah%20Dehnavi%20and%20Maedeh%20Mirdamadi%20and%20Riccardo%20Talami%20and%20Zahra-Sadat%20Zomorodian&entry.1292438233=%20%20As%20urbanization%20accelerates%2C%20open%20spaces%20are%20increasingly%20recognized%20for%0Atheir%20role%20in%20enhancing%20sustainability%20and%20well-being%2C%20yet%20they%20remain%0Aunderexplored%20compared%20to%20built%20spaces.%20This%20study%20introduces%20an%20AI-driven%0Aframework%20that%20integrates%20machine%20learning%20models%20%28MLMs%29%20and%20explainable%20AI%0Atechniques%20to%20optimize%20Sky%20View%20Factor%20%28SVF%29%20and%20visibility%2C%20key%20spatial%0Ametrics%20influencing%20thermal%20comfort%20and%20perceived%20safety%20in%20urban%20spaces.%0AUnlike%20global%20optimization%20methods%2C%20which%20are%20computationally%20intensive%20and%0Aimpractical%20for%20localized%20adjustments%2C%20this%20framework%20supports%20incremental%0Adesign%20improvements%20with%20lower%20computational%20costs%20and%20greater%20flexibility.%20The%0Aframework%20employs%20SHapley%20Adaptive%20Explanations%20%28SHAP%29%20to%20analyze%20feature%0Aimportance%20and%20Counterfactual%20Explanations%20%28CFXs%29%20to%20propose%20minimal%20design%0Achanges.%20Simulations%20tested%20five%20MLMs%2C%20identifying%20XGBoost%20as%20the%20most%0Aaccurate%2C%20with%20building%20width%2C%20park%20area%2C%20and%20heights%20of%20surrounding%20buildings%0Aas%20critical%20for%20SVF%2C%20and%20distances%20from%20southern%20buildings%20as%20key%20for%0Avisibility.%20Compared%20to%20Genetic%20Algorithms%2C%20which%20required%20approximately%2015/30%0Aminutes%20across%203/4%20generations%20to%20converge%2C%20the%20tested%20CFX%20approach%20achieved%0Aoptimized%20results%20in%201%20minute%20with%20a%205%25%20RMSE%20error%2C%20demonstrating%20significantly%0Afaster%20performance%20and%20suitability%20for%20scalable%20retrofitting%20strategies.%20This%0Ainterpretable%20and%20computationally%20efficient%20framework%20advances%20urban%0Aperformance%20optimization%2C%20providing%20data-driven%20insights%20and%20practical%0Aretrofitting%20solutions%20for%20enhancing%20usability%20and%20environmental%20quality%20across%0Adiverse%20urban%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08019v1&entry.124074799=Read"},
{"title": "DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them", "author": "Francisco Caetano and Christiaan Viviers and Luis A. Zavala-Mondrag\u00f3n and Peter H. N. de With and Fons van der Sommen", "abstract": "  Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available\n", "link": "http://arxiv.org/abs/2501.08005v1", "date": "2025-01-14", "relevancy": 1.5706, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5321}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5148}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them&body=Title%3A%20DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them%0AAuthor%3A%20Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%0Aapplications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%0Athis%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%0Adistribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%0Adetecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%0Aboundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%0Atrained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%0Adistinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%0Adetection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%0AAutoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%0Abatches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%0Adistribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%0Athe%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%0Ato%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%0Aboundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%0Athis%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%0Adetection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%0Ashifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%0Amethods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%0A25MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%0Aexisting%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%0AOOD%20detection%20applications.%20The%20code%20will%20be%20made%20publicly%20available%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCoPatch%253A%2520Batch%2520Statistics%2520Are%2520All%2520You%2520Need%2520For%2520OOD%2520Detection%252C%2520But%250A%2520%2520Only%2520If%2520You%2520Can%2520Trust%2520Them%26entry.906535625%3DFrancisco%2520Caetano%2520and%2520Christiaan%2520Viviers%2520and%2520Luis%2520A.%2520Zavala-Mondrag%25C3%25B3n%2520and%2520Peter%2520H.%2520N.%2520de%2520With%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520holds%2520significant%2520importance%2520across%2520many%250Aapplications.%2520While%2520semantic%2520and%2520domain-shift%2520OOD%2520problems%2520are%2520well-studied%252C%250Athis%2520work%2520focuses%2520on%2520covariate%2520shifts%2520-%2520subtle%2520variations%2520in%2520the%2520data%250Adistribution%2520that%2520can%2520degrade%2520machine%2520learning%2520performance.%2520We%2520hypothesize%2520that%250Adetecting%2520these%2520subtle%2520shifts%2520can%2520improve%2520our%2520understanding%2520of%2520in-distribution%250Aboundaries%252C%2520ultimately%2520improving%2520OOD%2520detection.%2520In%2520adversarial%2520discriminators%250Atrained%2520with%2520Batch%2520Normalization%2520%2528BN%2529%252C%2520real%2520and%2520adversarial%2520samples%2520form%250Adistinct%2520domains%2520with%2520unique%2520batch%2520statistics%2520-%2520a%2520property%2520we%2520exploit%2520for%2520OOD%250Adetection.%2520We%2520introduce%2520DisCoPatch%252C%2520an%2520unsupervised%2520Adversarial%2520Variational%250AAutoencoder%2520%2528VAE%2529%2520framework%2520that%2520harnesses%2520this%2520mechanism.%2520During%2520inference%252C%250Abatches%2520consist%2520of%2520patches%2520from%2520the%2520same%2520image%252C%2520ensuring%2520a%2520consistent%2520data%250Adistribution%2520that%2520allows%2520the%2520model%2520to%2520rely%2520on%2520batch%2520statistics.%2520DisCoPatch%2520uses%250Athe%2520VAE%2527s%2520suboptimal%2520outputs%2520%2528generated%2520and%2520reconstructed%2529%2520as%2520negative%2520samples%250Ato%2520train%2520the%2520discriminator%252C%2520thereby%2520improving%2520its%2520ability%2520to%2520delineate%2520the%250Aboundary%2520between%2520in-distribution%2520samples%2520and%2520covariate%2520shifts.%2520By%2520tightening%250Athis%2520boundary%252C%2520DisCoPatch%2520achieves%2520state-of-the-art%2520results%2520in%2520public%2520OOD%250Adetection%2520benchmarks.%2520The%2520proposed%2520model%2520not%2520only%2520excels%2520in%2520detecting%2520covariate%250Ashifts%252C%2520achieving%252095.5%2525%2520AUROC%2520on%2520ImageNet-1K%2528-C%2529%2520but%2520also%2520outperforms%2520all%2520prior%250Amethods%2520on%2520public%2520Near-OOD%2520%252895.0%2525%2529%2520benchmarks.%2520With%2520a%2520compact%2520model%2520size%2520of%250A25MB%252C%2520it%2520achieves%2520high%2520OOD%2520detection%2520performance%2520at%2520notably%2520lower%2520latency%2520than%250Aexisting%2520methods%252C%2520making%2520it%2520an%2520efficient%2520and%2520practical%2520solution%2520for%2520real-world%250AOOD%2520detection%2520applications.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCoPatch%3A%20Batch%20Statistics%20Are%20All%20You%20Need%20For%20OOD%20Detection%2C%20But%0A%20%20Only%20If%20You%20Can%20Trust%20Them&entry.906535625=Francisco%20Caetano%20and%20Christiaan%20Viviers%20and%20Luis%20A.%20Zavala-Mondrag%C3%B3n%20and%20Peter%20H.%20N.%20de%20With%20and%20Fons%20van%20der%20Sommen&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20holds%20significant%20importance%20across%20many%0Aapplications.%20While%20semantic%20and%20domain-shift%20OOD%20problems%20are%20well-studied%2C%0Athis%20work%20focuses%20on%20covariate%20shifts%20-%20subtle%20variations%20in%20the%20data%0Adistribution%20that%20can%20degrade%20machine%20learning%20performance.%20We%20hypothesize%20that%0Adetecting%20these%20subtle%20shifts%20can%20improve%20our%20understanding%20of%20in-distribution%0Aboundaries%2C%20ultimately%20improving%20OOD%20detection.%20In%20adversarial%20discriminators%0Atrained%20with%20Batch%20Normalization%20%28BN%29%2C%20real%20and%20adversarial%20samples%20form%0Adistinct%20domains%20with%20unique%20batch%20statistics%20-%20a%20property%20we%20exploit%20for%20OOD%0Adetection.%20We%20introduce%20DisCoPatch%2C%20an%20unsupervised%20Adversarial%20Variational%0AAutoencoder%20%28VAE%29%20framework%20that%20harnesses%20this%20mechanism.%20During%20inference%2C%0Abatches%20consist%20of%20patches%20from%20the%20same%20image%2C%20ensuring%20a%20consistent%20data%0Adistribution%20that%20allows%20the%20model%20to%20rely%20on%20batch%20statistics.%20DisCoPatch%20uses%0Athe%20VAE%27s%20suboptimal%20outputs%20%28generated%20and%20reconstructed%29%20as%20negative%20samples%0Ato%20train%20the%20discriminator%2C%20thereby%20improving%20its%20ability%20to%20delineate%20the%0Aboundary%20between%20in-distribution%20samples%20and%20covariate%20shifts.%20By%20tightening%0Athis%20boundary%2C%20DisCoPatch%20achieves%20state-of-the-art%20results%20in%20public%20OOD%0Adetection%20benchmarks.%20The%20proposed%20model%20not%20only%20excels%20in%20detecting%20covariate%0Ashifts%2C%20achieving%2095.5%25%20AUROC%20on%20ImageNet-1K%28-C%29%20but%20also%20outperforms%20all%20prior%0Amethods%20on%20public%20Near-OOD%20%2895.0%25%29%20benchmarks.%20With%20a%20compact%20model%20size%20of%0A25MB%2C%20it%20achieves%20high%20OOD%20detection%20performance%20at%20notably%20lower%20latency%20than%0Aexisting%20methods%2C%20making%20it%20an%20efficient%20and%20practical%20solution%20for%20real-world%0AOOD%20detection%20applications.%20The%20code%20will%20be%20made%20publicly%20available%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08005v1&entry.124074799=Read"},
{"title": "Gradient Equilibrium in Online Learning: Theory and Applications", "author": "Anastasios N. Angelopoulos and Michael I. Jordan and Ryan J. Tibshirani", "abstract": "  We present a new perspective on online learning that we refer to as gradient\nequilibrium: a sequence of iterates achieves gradient equilibrium if the\naverage of gradients of losses along the sequence converges to zero. In\ngeneral, this condition is not implied by nor implies sublinear regret. It\nturns out that gradient equilibrium is achievable by standard online learning\nmethods such as gradient descent and mirror descent with constant step sizes\n(rather than decaying step sizes, as is usually required for no regret).\nFurther, as we show through examples, gradient equilibrium translates into an\ninterpretable and meaningful property in online prediction problems spanning\nregression, classification, quantile estimation, and others. Notably, we show\nthat the gradient equilibrium framework can be used to develop a debiasing\nscheme for black-box predictions under arbitrary distribution shift, based on\nsimple post hoc online descent updates. We also show that post hoc gradient\nupdates can be used to calibrate predicted quantiles under distribution shift,\nand that the framework leads to unbiased Elo scores for pairwise preference\nprediction.\n", "link": "http://arxiv.org/abs/2501.08330v1", "date": "2025-01-14", "relevancy": 1.8604, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Equilibrium%20in%20Online%20Learning%3A%20Theory%20and%20Applications&body=Title%3A%20Gradient%20Equilibrium%20in%20Online%20Learning%3A%20Theory%20and%20Applications%0AAuthor%3A%20Anastasios%20N.%20Angelopoulos%20and%20Michael%20I.%20Jordan%20and%20Ryan%20J.%20Tibshirani%0AAbstract%3A%20%20%20We%20present%20a%20new%20perspective%20on%20online%20learning%20that%20we%20refer%20to%20as%20gradient%0Aequilibrium%3A%20a%20sequence%20of%20iterates%20achieves%20gradient%20equilibrium%20if%20the%0Aaverage%20of%20gradients%20of%20losses%20along%20the%20sequence%20converges%20to%20zero.%20In%0Ageneral%2C%20this%20condition%20is%20not%20implied%20by%20nor%20implies%20sublinear%20regret.%20It%0Aturns%20out%20that%20gradient%20equilibrium%20is%20achievable%20by%20standard%20online%20learning%0Amethods%20such%20as%20gradient%20descent%20and%20mirror%20descent%20with%20constant%20step%20sizes%0A%28rather%20than%20decaying%20step%20sizes%2C%20as%20is%20usually%20required%20for%20no%20regret%29.%0AFurther%2C%20as%20we%20show%20through%20examples%2C%20gradient%20equilibrium%20translates%20into%20an%0Ainterpretable%20and%20meaningful%20property%20in%20online%20prediction%20problems%20spanning%0Aregression%2C%20classification%2C%20quantile%20estimation%2C%20and%20others.%20Notably%2C%20we%20show%0Athat%20the%20gradient%20equilibrium%20framework%20can%20be%20used%20to%20develop%20a%20debiasing%0Ascheme%20for%20black-box%20predictions%20under%20arbitrary%20distribution%20shift%2C%20based%20on%0Asimple%20post%20hoc%20online%20descent%20updates.%20We%20also%20show%20that%20post%20hoc%20gradient%0Aupdates%20can%20be%20used%20to%20calibrate%20predicted%20quantiles%20under%20distribution%20shift%2C%0Aand%20that%20the%20framework%20leads%20to%20unbiased%20Elo%20scores%20for%20pairwise%20preference%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Equilibrium%2520in%2520Online%2520Learning%253A%2520Theory%2520and%2520Applications%26entry.906535625%3DAnastasios%2520N.%2520Angelopoulos%2520and%2520Michael%2520I.%2520Jordan%2520and%2520Ryan%2520J.%2520Tibshirani%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520perspective%2520on%2520online%2520learning%2520that%2520we%2520refer%2520to%2520as%2520gradient%250Aequilibrium%253A%2520a%2520sequence%2520of%2520iterates%2520achieves%2520gradient%2520equilibrium%2520if%2520the%250Aaverage%2520of%2520gradients%2520of%2520losses%2520along%2520the%2520sequence%2520converges%2520to%2520zero.%2520In%250Ageneral%252C%2520this%2520condition%2520is%2520not%2520implied%2520by%2520nor%2520implies%2520sublinear%2520regret.%2520It%250Aturns%2520out%2520that%2520gradient%2520equilibrium%2520is%2520achievable%2520by%2520standard%2520online%2520learning%250Amethods%2520such%2520as%2520gradient%2520descent%2520and%2520mirror%2520descent%2520with%2520constant%2520step%2520sizes%250A%2528rather%2520than%2520decaying%2520step%2520sizes%252C%2520as%2520is%2520usually%2520required%2520for%2520no%2520regret%2529.%250AFurther%252C%2520as%2520we%2520show%2520through%2520examples%252C%2520gradient%2520equilibrium%2520translates%2520into%2520an%250Ainterpretable%2520and%2520meaningful%2520property%2520in%2520online%2520prediction%2520problems%2520spanning%250Aregression%252C%2520classification%252C%2520quantile%2520estimation%252C%2520and%2520others.%2520Notably%252C%2520we%2520show%250Athat%2520the%2520gradient%2520equilibrium%2520framework%2520can%2520be%2520used%2520to%2520develop%2520a%2520debiasing%250Ascheme%2520for%2520black-box%2520predictions%2520under%2520arbitrary%2520distribution%2520shift%252C%2520based%2520on%250Asimple%2520post%2520hoc%2520online%2520descent%2520updates.%2520We%2520also%2520show%2520that%2520post%2520hoc%2520gradient%250Aupdates%2520can%2520be%2520used%2520to%2520calibrate%2520predicted%2520quantiles%2520under%2520distribution%2520shift%252C%250Aand%2520that%2520the%2520framework%2520leads%2520to%2520unbiased%2520Elo%2520scores%2520for%2520pairwise%2520preference%250Aprediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Equilibrium%20in%20Online%20Learning%3A%20Theory%20and%20Applications&entry.906535625=Anastasios%20N.%20Angelopoulos%20and%20Michael%20I.%20Jordan%20and%20Ryan%20J.%20Tibshirani&entry.1292438233=%20%20We%20present%20a%20new%20perspective%20on%20online%20learning%20that%20we%20refer%20to%20as%20gradient%0Aequilibrium%3A%20a%20sequence%20of%20iterates%20achieves%20gradient%20equilibrium%20if%20the%0Aaverage%20of%20gradients%20of%20losses%20along%20the%20sequence%20converges%20to%20zero.%20In%0Ageneral%2C%20this%20condition%20is%20not%20implied%20by%20nor%20implies%20sublinear%20regret.%20It%0Aturns%20out%20that%20gradient%20equilibrium%20is%20achievable%20by%20standard%20online%20learning%0Amethods%20such%20as%20gradient%20descent%20and%20mirror%20descent%20with%20constant%20step%20sizes%0A%28rather%20than%20decaying%20step%20sizes%2C%20as%20is%20usually%20required%20for%20no%20regret%29.%0AFurther%2C%20as%20we%20show%20through%20examples%2C%20gradient%20equilibrium%20translates%20into%20an%0Ainterpretable%20and%20meaningful%20property%20in%20online%20prediction%20problems%20spanning%0Aregression%2C%20classification%2C%20quantile%20estimation%2C%20and%20others.%20Notably%2C%20we%20show%0Athat%20the%20gradient%20equilibrium%20framework%20can%20be%20used%20to%20develop%20a%20debiasing%0Ascheme%20for%20black-box%20predictions%20under%20arbitrary%20distribution%20shift%2C%20based%20on%0Asimple%20post%20hoc%20online%20descent%20updates.%20We%20also%20show%20that%20post%20hoc%20gradient%0Aupdates%20can%20be%20used%20to%20calibrate%20predicted%20quantiles%20under%20distribution%20shift%2C%0Aand%20that%20the%20framework%20leads%20to%20unbiased%20Elo%20scores%20for%20pairwise%20preference%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08330v1&entry.124074799=Read"},
{"title": "Fair CoVariance Neural Networks", "author": "Andrea Cavallo and Madeline Navarro and Santiago Segarra and Elvin Isufi", "abstract": "  Covariance-based data processing is widespread across signal processing and\nmachine learning applications due to its ability to model data\ninterconnectivities and dependencies. However, harmful biases in the data may\nbecome encoded in the sample covariance matrix and cause data-driven methods to\ntreat different subpopulations unfairly. Existing works such as fair principal\ncomponent analysis (PCA) mitigate these effects, but remain unstable in low\nsample regimes, which in turn may jeopardize the fairness goal. To address both\nbiases and instability, we propose Fair coVariance Neural Networks (FVNNs),\nwhich perform graph convolutions on the covariance matrix for both fair and\naccurate predictions. Our FVNNs provide a flexible model compatible with\nseveral existing bias mitigation techniques. In particular, FVNNs allow for\nmitigating the bias in two ways: first, they operate on fair covariance\nestimates that remove biases from their principal components; second, they are\ntrained in an end-to-end fashion via a fairness regularizer in the loss\nfunction so that the model parameters are tailored to solve the task directly\nin a fair manner. We prove that FVNNs are intrinsically fairer than analogous\nPCA approaches thanks to their stability in low sample regimes. We validate the\nrobustness and fairness of our model on synthetic and real-world data,\nshowcasing the flexibility of FVNNs along with the tradeoff between fair and\naccurate performance.\n", "link": "http://arxiv.org/abs/2409.08558v2", "date": "2025-01-14", "relevancy": 1.9437, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.493}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20CoVariance%20Neural%20Networks&body=Title%3A%20Fair%20CoVariance%20Neural%20Networks%0AAuthor%3A%20Andrea%20Cavallo%20and%20Madeline%20Navarro%20and%20Santiago%20Segarra%20and%20Elvin%20Isufi%0AAbstract%3A%20%20%20Covariance-based%20data%20processing%20is%20widespread%20across%20signal%20processing%20and%0Amachine%20learning%20applications%20due%20to%20its%20ability%20to%20model%20data%0Ainterconnectivities%20and%20dependencies.%20However%2C%20harmful%20biases%20in%20the%20data%20may%0Abecome%20encoded%20in%20the%20sample%20covariance%20matrix%20and%20cause%20data-driven%20methods%20to%0Atreat%20different%20subpopulations%20unfairly.%20Existing%20works%20such%20as%20fair%20principal%0Acomponent%20analysis%20%28PCA%29%20mitigate%20these%20effects%2C%20but%20remain%20unstable%20in%20low%0Asample%20regimes%2C%20which%20in%20turn%20may%20jeopardize%20the%20fairness%20goal.%20To%20address%20both%0Abiases%20and%20instability%2C%20we%20propose%20Fair%20coVariance%20Neural%20Networks%20%28FVNNs%29%2C%0Awhich%20perform%20graph%20convolutions%20on%20the%20covariance%20matrix%20for%20both%20fair%20and%0Aaccurate%20predictions.%20Our%20FVNNs%20provide%20a%20flexible%20model%20compatible%20with%0Aseveral%20existing%20bias%20mitigation%20techniques.%20In%20particular%2C%20FVNNs%20allow%20for%0Amitigating%20the%20bias%20in%20two%20ways%3A%20first%2C%20they%20operate%20on%20fair%20covariance%0Aestimates%20that%20remove%20biases%20from%20their%20principal%20components%3B%20second%2C%20they%20are%0Atrained%20in%20an%20end-to-end%20fashion%20via%20a%20fairness%20regularizer%20in%20the%20loss%0Afunction%20so%20that%20the%20model%20parameters%20are%20tailored%20to%20solve%20the%20task%20directly%0Ain%20a%20fair%20manner.%20We%20prove%20that%20FVNNs%20are%20intrinsically%20fairer%20than%20analogous%0APCA%20approaches%20thanks%20to%20their%20stability%20in%20low%20sample%20regimes.%20We%20validate%20the%0Arobustness%20and%20fairness%20of%20our%20model%20on%20synthetic%20and%20real-world%20data%2C%0Ashowcasing%20the%20flexibility%20of%20FVNNs%20along%20with%20the%20tradeoff%20between%20fair%20and%0Aaccurate%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520CoVariance%2520Neural%2520Networks%26entry.906535625%3DAndrea%2520Cavallo%2520and%2520Madeline%2520Navarro%2520and%2520Santiago%2520Segarra%2520and%2520Elvin%2520Isufi%26entry.1292438233%3D%2520%2520Covariance-based%2520data%2520processing%2520is%2520widespread%2520across%2520signal%2520processing%2520and%250Amachine%2520learning%2520applications%2520due%2520to%2520its%2520ability%2520to%2520model%2520data%250Ainterconnectivities%2520and%2520dependencies.%2520However%252C%2520harmful%2520biases%2520in%2520the%2520data%2520may%250Abecome%2520encoded%2520in%2520the%2520sample%2520covariance%2520matrix%2520and%2520cause%2520data-driven%2520methods%2520to%250Atreat%2520different%2520subpopulations%2520unfairly.%2520Existing%2520works%2520such%2520as%2520fair%2520principal%250Acomponent%2520analysis%2520%2528PCA%2529%2520mitigate%2520these%2520effects%252C%2520but%2520remain%2520unstable%2520in%2520low%250Asample%2520regimes%252C%2520which%2520in%2520turn%2520may%2520jeopardize%2520the%2520fairness%2520goal.%2520To%2520address%2520both%250Abiases%2520and%2520instability%252C%2520we%2520propose%2520Fair%2520coVariance%2520Neural%2520Networks%2520%2528FVNNs%2529%252C%250Awhich%2520perform%2520graph%2520convolutions%2520on%2520the%2520covariance%2520matrix%2520for%2520both%2520fair%2520and%250Aaccurate%2520predictions.%2520Our%2520FVNNs%2520provide%2520a%2520flexible%2520model%2520compatible%2520with%250Aseveral%2520existing%2520bias%2520mitigation%2520techniques.%2520In%2520particular%252C%2520FVNNs%2520allow%2520for%250Amitigating%2520the%2520bias%2520in%2520two%2520ways%253A%2520first%252C%2520they%2520operate%2520on%2520fair%2520covariance%250Aestimates%2520that%2520remove%2520biases%2520from%2520their%2520principal%2520components%253B%2520second%252C%2520they%2520are%250Atrained%2520in%2520an%2520end-to-end%2520fashion%2520via%2520a%2520fairness%2520regularizer%2520in%2520the%2520loss%250Afunction%2520so%2520that%2520the%2520model%2520parameters%2520are%2520tailored%2520to%2520solve%2520the%2520task%2520directly%250Ain%2520a%2520fair%2520manner.%2520We%2520prove%2520that%2520FVNNs%2520are%2520intrinsically%2520fairer%2520than%2520analogous%250APCA%2520approaches%2520thanks%2520to%2520their%2520stability%2520in%2520low%2520sample%2520regimes.%2520We%2520validate%2520the%250Arobustness%2520and%2520fairness%2520of%2520our%2520model%2520on%2520synthetic%2520and%2520real-world%2520data%252C%250Ashowcasing%2520the%2520flexibility%2520of%2520FVNNs%2520along%2520with%2520the%2520tradeoff%2520between%2520fair%2520and%250Aaccurate%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20CoVariance%20Neural%20Networks&entry.906535625=Andrea%20Cavallo%20and%20Madeline%20Navarro%20and%20Santiago%20Segarra%20and%20Elvin%20Isufi&entry.1292438233=%20%20Covariance-based%20data%20processing%20is%20widespread%20across%20signal%20processing%20and%0Amachine%20learning%20applications%20due%20to%20its%20ability%20to%20model%20data%0Ainterconnectivities%20and%20dependencies.%20However%2C%20harmful%20biases%20in%20the%20data%20may%0Abecome%20encoded%20in%20the%20sample%20covariance%20matrix%20and%20cause%20data-driven%20methods%20to%0Atreat%20different%20subpopulations%20unfairly.%20Existing%20works%20such%20as%20fair%20principal%0Acomponent%20analysis%20%28PCA%29%20mitigate%20these%20effects%2C%20but%20remain%20unstable%20in%20low%0Asample%20regimes%2C%20which%20in%20turn%20may%20jeopardize%20the%20fairness%20goal.%20To%20address%20both%0Abiases%20and%20instability%2C%20we%20propose%20Fair%20coVariance%20Neural%20Networks%20%28FVNNs%29%2C%0Awhich%20perform%20graph%20convolutions%20on%20the%20covariance%20matrix%20for%20both%20fair%20and%0Aaccurate%20predictions.%20Our%20FVNNs%20provide%20a%20flexible%20model%20compatible%20with%0Aseveral%20existing%20bias%20mitigation%20techniques.%20In%20particular%2C%20FVNNs%20allow%20for%0Amitigating%20the%20bias%20in%20two%20ways%3A%20first%2C%20they%20operate%20on%20fair%20covariance%0Aestimates%20that%20remove%20biases%20from%20their%20principal%20components%3B%20second%2C%20they%20are%0Atrained%20in%20an%20end-to-end%20fashion%20via%20a%20fairness%20regularizer%20in%20the%20loss%0Afunction%20so%20that%20the%20model%20parameters%20are%20tailored%20to%20solve%20the%20task%20directly%0Ain%20a%20fair%20manner.%20We%20prove%20that%20FVNNs%20are%20intrinsically%20fairer%20than%20analogous%0APCA%20approaches%20thanks%20to%20their%20stability%20in%20low%20sample%20regimes.%20We%20validate%20the%0Arobustness%20and%20fairness%20of%20our%20model%20on%20synthetic%20and%20real-world%20data%2C%0Ashowcasing%20the%20flexibility%20of%20FVNNs%20along%20with%20the%20tradeoff%20between%20fair%20and%0Aaccurate%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08558v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


