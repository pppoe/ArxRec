<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251001.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes", "author": "Paul Gavrikov and Wei Lin and M. Jehanzeb Mirza and Soumya Jahagirdar and Muhammad Huzaifa and Sivan Doveh and Serena Yeung-Levy and James Glass and Hilde Kuehne", "abstract": "  Is basic visual understanding really solved in state-of-the-art VLMs? We\npresent VisualOverload, a slightly different visual question answering (VQA)\nbenchmark comprising 2,720 question-answer pairs, with privately held\nground-truth responses. Unlike prior VQA datasets that typically focus on near\nglobal image understanding, VisualOverload challenges models to perform simple,\nknowledge-free vision tasks in densely populated (or, overloaded) scenes. Our\ndataset consists of high-resolution scans of public-domain paintings that are\npopulated with multiple figures, actions, and unfolding subplots set against\nelaborately detailed backdrops. We manually annotated these images with\nquestions across six task categories to probe for a thorough understanding of\nthe scene. We hypothesize that current benchmarks overestimate the performance\nof VLMs, and encoding and reasoning over details is still a challenging task\nfor them, especially if they are confronted with densely populated scenes.\nIndeed, we observe that even the best model (o3) out of 37 tested models only\nachieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on\nall questions. Beyond a thorough evaluation, we complement our benchmark with\nan error analysis that reveals multiple failure modes, including a lack of\ncounting skills, failure in OCR, and striking logical inconsistencies under\ncomplex tasks. Altogether, VisualOverload exposes a critical gap in current\nvision models and offers a crucial resource for the community to develop better\nmodels.\n  Benchmark: http://paulgavrikov.github.io/visualoverload\n", "link": "http://arxiv.org/abs/2509.25339v2", "date": "2025-10-01", "relevancy": 3.2094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6982}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualOverload%3A%20Probing%20Visual%20Understanding%20of%20VLMs%20in%20Really%20Dense%0A%20%20Scenes&body=Title%3A%20VisualOverload%3A%20Probing%20Visual%20Understanding%20of%20VLMs%20in%20Really%20Dense%0A%20%20Scenes%0AAuthor%3A%20Paul%20Gavrikov%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Soumya%20Jahagirdar%20and%20Muhammad%20Huzaifa%20and%20Sivan%20Doveh%20and%20Serena%20Yeung-Levy%20and%20James%20Glass%20and%20Hilde%20Kuehne%0AAbstract%3A%20%20%20Is%20basic%20visual%20understanding%20really%20solved%20in%20state-of-the-art%20VLMs%3F%20We%0Apresent%20VisualOverload%2C%20a%20slightly%20different%20visual%20question%20answering%20%28VQA%29%0Abenchmark%20comprising%202%2C720%20question-answer%20pairs%2C%20with%20privately%20held%0Aground-truth%20responses.%20Unlike%20prior%20VQA%20datasets%20that%20typically%20focus%20on%20near%0Aglobal%20image%20understanding%2C%20VisualOverload%20challenges%20models%20to%20perform%20simple%2C%0Aknowledge-free%20vision%20tasks%20in%20densely%20populated%20%28or%2C%20overloaded%29%20scenes.%20Our%0Adataset%20consists%20of%20high-resolution%20scans%20of%20public-domain%20paintings%20that%20are%0Apopulated%20with%20multiple%20figures%2C%20actions%2C%20and%20unfolding%20subplots%20set%20against%0Aelaborately%20detailed%20backdrops.%20We%20manually%20annotated%20these%20images%20with%0Aquestions%20across%20six%20task%20categories%20to%20probe%20for%20a%20thorough%20understanding%20of%0Athe%20scene.%20We%20hypothesize%20that%20current%20benchmarks%20overestimate%20the%20performance%0Aof%20VLMs%2C%20and%20encoding%20and%20reasoning%20over%20details%20is%20still%20a%20challenging%20task%0Afor%20them%2C%20especially%20if%20they%20are%20confronted%20with%20densely%20populated%20scenes.%0AIndeed%2C%20we%20observe%20that%20even%20the%20best%20model%20%28o3%29%20out%20of%2037%20tested%20models%20only%0Aachieves%2019.6%25%20accuracy%20on%20our%20hardest%20test%20split%20and%20overall%2069.5%25%20accuracy%20on%0Aall%20questions.%20Beyond%20a%20thorough%20evaluation%2C%20we%20complement%20our%20benchmark%20with%0Aan%20error%20analysis%20that%20reveals%20multiple%20failure%20modes%2C%20including%20a%20lack%20of%0Acounting%20skills%2C%20failure%20in%20OCR%2C%20and%20striking%20logical%20inconsistencies%20under%0Acomplex%20tasks.%20Altogether%2C%20VisualOverload%20exposes%20a%20critical%20gap%20in%20current%0Avision%20models%20and%20offers%20a%20crucial%20resource%20for%20the%20community%20to%20develop%20better%0Amodels.%0A%20%20Benchmark%3A%20http%3A//paulgavrikov.github.io/visualoverload%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualOverload%253A%2520Probing%2520Visual%2520Understanding%2520of%2520VLMs%2520in%2520Really%2520Dense%250A%2520%2520Scenes%26entry.906535625%3DPaul%2520Gavrikov%2520and%2520Wei%2520Lin%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520Soumya%2520Jahagirdar%2520and%2520Muhammad%2520Huzaifa%2520and%2520Sivan%2520Doveh%2520and%2520Serena%2520Yeung-Levy%2520and%2520James%2520Glass%2520and%2520Hilde%2520Kuehne%26entry.1292438233%3D%2520%2520Is%2520basic%2520visual%2520understanding%2520really%2520solved%2520in%2520state-of-the-art%2520VLMs%253F%2520We%250Apresent%2520VisualOverload%252C%2520a%2520slightly%2520different%2520visual%2520question%2520answering%2520%2528VQA%2529%250Abenchmark%2520comprising%25202%252C720%2520question-answer%2520pairs%252C%2520with%2520privately%2520held%250Aground-truth%2520responses.%2520Unlike%2520prior%2520VQA%2520datasets%2520that%2520typically%2520focus%2520on%2520near%250Aglobal%2520image%2520understanding%252C%2520VisualOverload%2520challenges%2520models%2520to%2520perform%2520simple%252C%250Aknowledge-free%2520vision%2520tasks%2520in%2520densely%2520populated%2520%2528or%252C%2520overloaded%2529%2520scenes.%2520Our%250Adataset%2520consists%2520of%2520high-resolution%2520scans%2520of%2520public-domain%2520paintings%2520that%2520are%250Apopulated%2520with%2520multiple%2520figures%252C%2520actions%252C%2520and%2520unfolding%2520subplots%2520set%2520against%250Aelaborately%2520detailed%2520backdrops.%2520We%2520manually%2520annotated%2520these%2520images%2520with%250Aquestions%2520across%2520six%2520task%2520categories%2520to%2520probe%2520for%2520a%2520thorough%2520understanding%2520of%250Athe%2520scene.%2520We%2520hypothesize%2520that%2520current%2520benchmarks%2520overestimate%2520the%2520performance%250Aof%2520VLMs%252C%2520and%2520encoding%2520and%2520reasoning%2520over%2520details%2520is%2520still%2520a%2520challenging%2520task%250Afor%2520them%252C%2520especially%2520if%2520they%2520are%2520confronted%2520with%2520densely%2520populated%2520scenes.%250AIndeed%252C%2520we%2520observe%2520that%2520even%2520the%2520best%2520model%2520%2528o3%2529%2520out%2520of%252037%2520tested%2520models%2520only%250Aachieves%252019.6%2525%2520accuracy%2520on%2520our%2520hardest%2520test%2520split%2520and%2520overall%252069.5%2525%2520accuracy%2520on%250Aall%2520questions.%2520Beyond%2520a%2520thorough%2520evaluation%252C%2520we%2520complement%2520our%2520benchmark%2520with%250Aan%2520error%2520analysis%2520that%2520reveals%2520multiple%2520failure%2520modes%252C%2520including%2520a%2520lack%2520of%250Acounting%2520skills%252C%2520failure%2520in%2520OCR%252C%2520and%2520striking%2520logical%2520inconsistencies%2520under%250Acomplex%2520tasks.%2520Altogether%252C%2520VisualOverload%2520exposes%2520a%2520critical%2520gap%2520in%2520current%250Avision%2520models%2520and%2520offers%2520a%2520crucial%2520resource%2520for%2520the%2520community%2520to%2520develop%2520better%250Amodels.%250A%2520%2520Benchmark%253A%2520http%253A//paulgavrikov.github.io/visualoverload%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualOverload%3A%20Probing%20Visual%20Understanding%20of%20VLMs%20in%20Really%20Dense%0A%20%20Scenes&entry.906535625=Paul%20Gavrikov%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Soumya%20Jahagirdar%20and%20Muhammad%20Huzaifa%20and%20Sivan%20Doveh%20and%20Serena%20Yeung-Levy%20and%20James%20Glass%20and%20Hilde%20Kuehne&entry.1292438233=%20%20Is%20basic%20visual%20understanding%20really%20solved%20in%20state-of-the-art%20VLMs%3F%20We%0Apresent%20VisualOverload%2C%20a%20slightly%20different%20visual%20question%20answering%20%28VQA%29%0Abenchmark%20comprising%202%2C720%20question-answer%20pairs%2C%20with%20privately%20held%0Aground-truth%20responses.%20Unlike%20prior%20VQA%20datasets%20that%20typically%20focus%20on%20near%0Aglobal%20image%20understanding%2C%20VisualOverload%20challenges%20models%20to%20perform%20simple%2C%0Aknowledge-free%20vision%20tasks%20in%20densely%20populated%20%28or%2C%20overloaded%29%20scenes.%20Our%0Adataset%20consists%20of%20high-resolution%20scans%20of%20public-domain%20paintings%20that%20are%0Apopulated%20with%20multiple%20figures%2C%20actions%2C%20and%20unfolding%20subplots%20set%20against%0Aelaborately%20detailed%20backdrops.%20We%20manually%20annotated%20these%20images%20with%0Aquestions%20across%20six%20task%20categories%20to%20probe%20for%20a%20thorough%20understanding%20of%0Athe%20scene.%20We%20hypothesize%20that%20current%20benchmarks%20overestimate%20the%20performance%0Aof%20VLMs%2C%20and%20encoding%20and%20reasoning%20over%20details%20is%20still%20a%20challenging%20task%0Afor%20them%2C%20especially%20if%20they%20are%20confronted%20with%20densely%20populated%20scenes.%0AIndeed%2C%20we%20observe%20that%20even%20the%20best%20model%20%28o3%29%20out%20of%2037%20tested%20models%20only%0Aachieves%2019.6%25%20accuracy%20on%20our%20hardest%20test%20split%20and%20overall%2069.5%25%20accuracy%20on%0Aall%20questions.%20Beyond%20a%20thorough%20evaluation%2C%20we%20complement%20our%20benchmark%20with%0Aan%20error%20analysis%20that%20reveals%20multiple%20failure%20modes%2C%20including%20a%20lack%20of%0Acounting%20skills%2C%20failure%20in%20OCR%2C%20and%20striking%20logical%20inconsistencies%20under%0Acomplex%20tasks.%20Altogether%2C%20VisualOverload%20exposes%20a%20critical%20gap%20in%20current%0Avision%20models%20and%20offers%20a%20crucial%20resource%20for%20the%20community%20to%20develop%20better%0Amodels.%0A%20%20Benchmark%3A%20http%3A//paulgavrikov.github.io/visualoverload%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25339v2&entry.124074799=Read"},
{"title": "DepthLM: Metric Depth From Vision Language Models", "author": "Zhipeng Cai and Ching-Feng Yeh and Hu Xu and Zhuang Liu and Gregory Meyer and Xinjie Lei and Changsheng Zhao and Shang-Wen Li and Vikas Chandra and Yangyang Shi", "abstract": "  Vision language models (VLMs) can flexibly address various vision tasks\nthrough text interactions. Although successful in semantic understanding,\nstate-of-the-art VLMs including GPT-5 still struggle in understanding 3D from\n2D inputs. On the other hand, expert pure vision models achieve super-human\naccuracy in metric depth estimation, a key 3D understanding task. However, they\nrequire task-specific architectures and losses. Such difference motivates us to\nask: Can VLMs reach expert-level accuracy without architecture or loss change?\nWe take per-pixel metric depth estimation as the representative task and show\nthat the answer is yes! Surprisingly, comprehensive analysis shows that\ntext-based supervised-finetuning with sparse labels is sufficient for VLMs to\nunlock strong 3D understanding, no dense prediction head or complex\nregression/regularization loss is needed. The bottleneck for VLMs lies actually\nin pixel reference and cross-dataset camera ambiguity, which we address through\nvisual prompting and intrinsic-conditioned augmentation. With much smaller\nmodels, our method DepthLM surpasses the accuracy of most advanced VLMs by over\n2x, making VLMs for the first time comparable with pure vision models.\nInterestingly, without explicit enforcement during training, VLMs trained with\nDepthLM naturally avoids over-smoothing, having much fewer flying points at\nboundary regions than pure vision models. The simplicity of DepthLM also\nenables a single VLM to cover various 3D tasks beyond metric depth. Our code\nand model will be released at the link below.\n", "link": "http://arxiv.org/abs/2509.25413v2", "date": "2025-10-01", "relevancy": 3.1831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthLM%3A%20Metric%20Depth%20From%20Vision%20Language%20Models&body=Title%3A%20DepthLM%3A%20Metric%20Depth%20From%20Vision%20Language%20Models%0AAuthor%3A%20Zhipeng%20Cai%20and%20Ching-Feng%20Yeh%20and%20Hu%20Xu%20and%20Zhuang%20Liu%20and%20Gregory%20Meyer%20and%20Xinjie%20Lei%20and%20Changsheng%20Zhao%20and%20Shang-Wen%20Li%20and%20Vikas%20Chandra%20and%20Yangyang%20Shi%0AAbstract%3A%20%20%20Vision%20language%20models%20%28VLMs%29%20can%20flexibly%20address%20various%20vision%20tasks%0Athrough%20text%20interactions.%20Although%20successful%20in%20semantic%20understanding%2C%0Astate-of-the-art%20VLMs%20including%20GPT-5%20still%20struggle%20in%20understanding%203D%20from%0A2D%20inputs.%20On%20the%20other%20hand%2C%20expert%20pure%20vision%20models%20achieve%20super-human%0Aaccuracy%20in%20metric%20depth%20estimation%2C%20a%20key%203D%20understanding%20task.%20However%2C%20they%0Arequire%20task-specific%20architectures%20and%20losses.%20Such%20difference%20motivates%20us%20to%0Aask%3A%20Can%20VLMs%20reach%20expert-level%20accuracy%20without%20architecture%20or%20loss%20change%3F%0AWe%20take%20per-pixel%20metric%20depth%20estimation%20as%20the%20representative%20task%20and%20show%0Athat%20the%20answer%20is%20yes%21%20Surprisingly%2C%20comprehensive%20analysis%20shows%20that%0Atext-based%20supervised-finetuning%20with%20sparse%20labels%20is%20sufficient%20for%20VLMs%20to%0Aunlock%20strong%203D%20understanding%2C%20no%20dense%20prediction%20head%20or%20complex%0Aregression/regularization%20loss%20is%20needed.%20The%20bottleneck%20for%20VLMs%20lies%20actually%0Ain%20pixel%20reference%20and%20cross-dataset%20camera%20ambiguity%2C%20which%20we%20address%20through%0Avisual%20prompting%20and%20intrinsic-conditioned%20augmentation.%20With%20much%20smaller%0Amodels%2C%20our%20method%20DepthLM%20surpasses%20the%20accuracy%20of%20most%20advanced%20VLMs%20by%20over%0A2x%2C%20making%20VLMs%20for%20the%20first%20time%20comparable%20with%20pure%20vision%20models.%0AInterestingly%2C%20without%20explicit%20enforcement%20during%20training%2C%20VLMs%20trained%20with%0ADepthLM%20naturally%20avoids%20over-smoothing%2C%20having%20much%20fewer%20flying%20points%20at%0Aboundary%20regions%20than%20pure%20vision%20models.%20The%20simplicity%20of%20DepthLM%20also%0Aenables%20a%20single%20VLM%20to%20cover%20various%203D%20tasks%20beyond%20metric%20depth.%20Our%20code%0Aand%20model%20will%20be%20released%20at%20the%20link%20below.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthLM%253A%2520Metric%2520Depth%2520From%2520Vision%2520Language%2520Models%26entry.906535625%3DZhipeng%2520Cai%2520and%2520Ching-Feng%2520Yeh%2520and%2520Hu%2520Xu%2520and%2520Zhuang%2520Liu%2520and%2520Gregory%2520Meyer%2520and%2520Xinjie%2520Lei%2520and%2520Changsheng%2520Zhao%2520and%2520Shang-Wen%2520Li%2520and%2520Vikas%2520Chandra%2520and%2520Yangyang%2520Shi%26entry.1292438233%3D%2520%2520Vision%2520language%2520models%2520%2528VLMs%2529%2520can%2520flexibly%2520address%2520various%2520vision%2520tasks%250Athrough%2520text%2520interactions.%2520Although%2520successful%2520in%2520semantic%2520understanding%252C%250Astate-of-the-art%2520VLMs%2520including%2520GPT-5%2520still%2520struggle%2520in%2520understanding%25203D%2520from%250A2D%2520inputs.%2520On%2520the%2520other%2520hand%252C%2520expert%2520pure%2520vision%2520models%2520achieve%2520super-human%250Aaccuracy%2520in%2520metric%2520depth%2520estimation%252C%2520a%2520key%25203D%2520understanding%2520task.%2520However%252C%2520they%250Arequire%2520task-specific%2520architectures%2520and%2520losses.%2520Such%2520difference%2520motivates%2520us%2520to%250Aask%253A%2520Can%2520VLMs%2520reach%2520expert-level%2520accuracy%2520without%2520architecture%2520or%2520loss%2520change%253F%250AWe%2520take%2520per-pixel%2520metric%2520depth%2520estimation%2520as%2520the%2520representative%2520task%2520and%2520show%250Athat%2520the%2520answer%2520is%2520yes%2521%2520Surprisingly%252C%2520comprehensive%2520analysis%2520shows%2520that%250Atext-based%2520supervised-finetuning%2520with%2520sparse%2520labels%2520is%2520sufficient%2520for%2520VLMs%2520to%250Aunlock%2520strong%25203D%2520understanding%252C%2520no%2520dense%2520prediction%2520head%2520or%2520complex%250Aregression/regularization%2520loss%2520is%2520needed.%2520The%2520bottleneck%2520for%2520VLMs%2520lies%2520actually%250Ain%2520pixel%2520reference%2520and%2520cross-dataset%2520camera%2520ambiguity%252C%2520which%2520we%2520address%2520through%250Avisual%2520prompting%2520and%2520intrinsic-conditioned%2520augmentation.%2520With%2520much%2520smaller%250Amodels%252C%2520our%2520method%2520DepthLM%2520surpasses%2520the%2520accuracy%2520of%2520most%2520advanced%2520VLMs%2520by%2520over%250A2x%252C%2520making%2520VLMs%2520for%2520the%2520first%2520time%2520comparable%2520with%2520pure%2520vision%2520models.%250AInterestingly%252C%2520without%2520explicit%2520enforcement%2520during%2520training%252C%2520VLMs%2520trained%2520with%250ADepthLM%2520naturally%2520avoids%2520over-smoothing%252C%2520having%2520much%2520fewer%2520flying%2520points%2520at%250Aboundary%2520regions%2520than%2520pure%2520vision%2520models.%2520The%2520simplicity%2520of%2520DepthLM%2520also%250Aenables%2520a%2520single%2520VLM%2520to%2520cover%2520various%25203D%2520tasks%2520beyond%2520metric%2520depth.%2520Our%2520code%250Aand%2520model%2520will%2520be%2520released%2520at%2520the%2520link%2520below.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthLM%3A%20Metric%20Depth%20From%20Vision%20Language%20Models&entry.906535625=Zhipeng%20Cai%20and%20Ching-Feng%20Yeh%20and%20Hu%20Xu%20and%20Zhuang%20Liu%20and%20Gregory%20Meyer%20and%20Xinjie%20Lei%20and%20Changsheng%20Zhao%20and%20Shang-Wen%20Li%20and%20Vikas%20Chandra%20and%20Yangyang%20Shi&entry.1292438233=%20%20Vision%20language%20models%20%28VLMs%29%20can%20flexibly%20address%20various%20vision%20tasks%0Athrough%20text%20interactions.%20Although%20successful%20in%20semantic%20understanding%2C%0Astate-of-the-art%20VLMs%20including%20GPT-5%20still%20struggle%20in%20understanding%203D%20from%0A2D%20inputs.%20On%20the%20other%20hand%2C%20expert%20pure%20vision%20models%20achieve%20super-human%0Aaccuracy%20in%20metric%20depth%20estimation%2C%20a%20key%203D%20understanding%20task.%20However%2C%20they%0Arequire%20task-specific%20architectures%20and%20losses.%20Such%20difference%20motivates%20us%20to%0Aask%3A%20Can%20VLMs%20reach%20expert-level%20accuracy%20without%20architecture%20or%20loss%20change%3F%0AWe%20take%20per-pixel%20metric%20depth%20estimation%20as%20the%20representative%20task%20and%20show%0Athat%20the%20answer%20is%20yes%21%20Surprisingly%2C%20comprehensive%20analysis%20shows%20that%0Atext-based%20supervised-finetuning%20with%20sparse%20labels%20is%20sufficient%20for%20VLMs%20to%0Aunlock%20strong%203D%20understanding%2C%20no%20dense%20prediction%20head%20or%20complex%0Aregression/regularization%20loss%20is%20needed.%20The%20bottleneck%20for%20VLMs%20lies%20actually%0Ain%20pixel%20reference%20and%20cross-dataset%20camera%20ambiguity%2C%20which%20we%20address%20through%0Avisual%20prompting%20and%20intrinsic-conditioned%20augmentation.%20With%20much%20smaller%0Amodels%2C%20our%20method%20DepthLM%20surpasses%20the%20accuracy%20of%20most%20advanced%20VLMs%20by%20over%0A2x%2C%20making%20VLMs%20for%20the%20first%20time%20comparable%20with%20pure%20vision%20models.%0AInterestingly%2C%20without%20explicit%20enforcement%20during%20training%2C%20VLMs%20trained%20with%0ADepthLM%20naturally%20avoids%20over-smoothing%2C%20having%20much%20fewer%20flying%20points%20at%0Aboundary%20regions%20than%20pure%20vision%20models.%20The%20simplicity%20of%20DepthLM%20also%0Aenables%20a%20single%20VLM%20to%20cover%20various%203D%20tasks%20beyond%20metric%20depth.%20Our%20code%0Aand%20model%20will%20be%20released%20at%20the%20link%20below.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25413v2&entry.124074799=Read"},
{"title": "What if Othello-Playing Language Models Could See?", "author": "Xinyi Chen and Yifei Yuan and Jiaang Li and Serge Belongie and Maarten de Rijke and Anders S\u00f8gaard", "abstract": "  Language models are often said to face a symbol grounding problem. While some\nhave argued the problem can be solved without resort to other modalities, many\nhave speculated that grounded learning is more efficient. We explore this\nquestion in Othello, a simplified, rule-based world that offers a controlled\nand interpretable testbed for studying world understanding. Building on prior\nwork, we introduce VISOTHELLO, a multi-modal model trained jointly on move\nsequences and board images. Using the Othello rule understanding task, we\nexamine whether multi-modal learning provides advantages over text-only\napproaches. We further evaluate robustness under semantically irrelevant\nperturbations and analyze the consistency of cross-modal alignment. Our results\nsuggest that multi-modal training not only improves performance and robustness\nbut also promotes convergence toward shared internal representations across\ndifferent model architectures.\n", "link": "http://arxiv.org/abs/2507.14520v2", "date": "2025-10-01", "relevancy": 2.988, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20if%20Othello-Playing%20Language%20Models%20Could%20See%3F&body=Title%3A%20What%20if%20Othello-Playing%20Language%20Models%20Could%20See%3F%0AAuthor%3A%20Xinyi%20Chen%20and%20Yifei%20Yuan%20and%20Jiaang%20Li%20and%20Serge%20Belongie%20and%20Maarten%20de%20Rijke%20and%20Anders%20S%C3%B8gaard%0AAbstract%3A%20%20%20Language%20models%20are%20often%20said%20to%20face%20a%20symbol%20grounding%20problem.%20While%20some%0Ahave%20argued%20the%20problem%20can%20be%20solved%20without%20resort%20to%20other%20modalities%2C%20many%0Ahave%20speculated%20that%20grounded%20learning%20is%20more%20efficient.%20We%20explore%20this%0Aquestion%20in%20Othello%2C%20a%20simplified%2C%20rule-based%20world%20that%20offers%20a%20controlled%0Aand%20interpretable%20testbed%20for%20studying%20world%20understanding.%20Building%20on%20prior%0Awork%2C%20we%20introduce%20VISOTHELLO%2C%20a%20multi-modal%20model%20trained%20jointly%20on%20move%0Asequences%20and%20board%20images.%20Using%20the%20Othello%20rule%20understanding%20task%2C%20we%0Aexamine%20whether%20multi-modal%20learning%20provides%20advantages%20over%20text-only%0Aapproaches.%20We%20further%20evaluate%20robustness%20under%20semantically%20irrelevant%0Aperturbations%20and%20analyze%20the%20consistency%20of%20cross-modal%20alignment.%20Our%20results%0Asuggest%20that%20multi-modal%20training%20not%20only%20improves%20performance%20and%20robustness%0Abut%20also%20promotes%20convergence%20toward%20shared%20internal%20representations%20across%0Adifferent%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520if%2520Othello-Playing%2520Language%2520Models%2520Could%2520See%253F%26entry.906535625%3DXinyi%2520Chen%2520and%2520Yifei%2520Yuan%2520and%2520Jiaang%2520Li%2520and%2520Serge%2520Belongie%2520and%2520Maarten%2520de%2520Rijke%2520and%2520Anders%2520S%25C3%25B8gaard%26entry.1292438233%3D%2520%2520Language%2520models%2520are%2520often%2520said%2520to%2520face%2520a%2520symbol%2520grounding%2520problem.%2520While%2520some%250Ahave%2520argued%2520the%2520problem%2520can%2520be%2520solved%2520without%2520resort%2520to%2520other%2520modalities%252C%2520many%250Ahave%2520speculated%2520that%2520grounded%2520learning%2520is%2520more%2520efficient.%2520We%2520explore%2520this%250Aquestion%2520in%2520Othello%252C%2520a%2520simplified%252C%2520rule-based%2520world%2520that%2520offers%2520a%2520controlled%250Aand%2520interpretable%2520testbed%2520for%2520studying%2520world%2520understanding.%2520Building%2520on%2520prior%250Awork%252C%2520we%2520introduce%2520VISOTHELLO%252C%2520a%2520multi-modal%2520model%2520trained%2520jointly%2520on%2520move%250Asequences%2520and%2520board%2520images.%2520Using%2520the%2520Othello%2520rule%2520understanding%2520task%252C%2520we%250Aexamine%2520whether%2520multi-modal%2520learning%2520provides%2520advantages%2520over%2520text-only%250Aapproaches.%2520We%2520further%2520evaluate%2520robustness%2520under%2520semantically%2520irrelevant%250Aperturbations%2520and%2520analyze%2520the%2520consistency%2520of%2520cross-modal%2520alignment.%2520Our%2520results%250Asuggest%2520that%2520multi-modal%2520training%2520not%2520only%2520improves%2520performance%2520and%2520robustness%250Abut%2520also%2520promotes%2520convergence%2520toward%2520shared%2520internal%2520representations%2520across%250Adifferent%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20if%20Othello-Playing%20Language%20Models%20Could%20See%3F&entry.906535625=Xinyi%20Chen%20and%20Yifei%20Yuan%20and%20Jiaang%20Li%20and%20Serge%20Belongie%20and%20Maarten%20de%20Rijke%20and%20Anders%20S%C3%B8gaard&entry.1292438233=%20%20Language%20models%20are%20often%20said%20to%20face%20a%20symbol%20grounding%20problem.%20While%20some%0Ahave%20argued%20the%20problem%20can%20be%20solved%20without%20resort%20to%20other%20modalities%2C%20many%0Ahave%20speculated%20that%20grounded%20learning%20is%20more%20efficient.%20We%20explore%20this%0Aquestion%20in%20Othello%2C%20a%20simplified%2C%20rule-based%20world%20that%20offers%20a%20controlled%0Aand%20interpretable%20testbed%20for%20studying%20world%20understanding.%20Building%20on%20prior%0Awork%2C%20we%20introduce%20VISOTHELLO%2C%20a%20multi-modal%20model%20trained%20jointly%20on%20move%0Asequences%20and%20board%20images.%20Using%20the%20Othello%20rule%20understanding%20task%2C%20we%0Aexamine%20whether%20multi-modal%20learning%20provides%20advantages%20over%20text-only%0Aapproaches.%20We%20further%20evaluate%20robustness%20under%20semantically%20irrelevant%0Aperturbations%20and%20analyze%20the%20consistency%20of%20cross-modal%20alignment.%20Our%20results%0Asuggest%20that%20multi-modal%20training%20not%20only%20improves%20performance%20and%20robustness%0Abut%20also%20promotes%20convergence%20toward%20shared%20internal%20representations%20across%0Adifferent%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14520v2&entry.124074799=Read"},
{"title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature\n  and Non-Homogeneous Poisson Processes", "author": "Trent Weiss and Madhur Behl", "abstract": "  Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing.\n", "link": "http://arxiv.org/abs/2507.18819v2", "date": "2025-10-01", "relevancy": 2.8361, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5965}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Collision%20Risk%20Estimation%20through%20Gauss-Legendre%20Cubature%0A%20%20and%20Non-Homogeneous%20Poisson%20Processes&body=Title%3A%20Probabilistic%20Collision%20Risk%20Estimation%20through%20Gauss-Legendre%20Cubature%0A%20%20and%20Non-Homogeneous%20Poisson%20Processes%0AAuthor%3A%20Trent%20Weiss%20and%20Madhur%20Behl%0AAbstract%3A%20%20%20Overtaking%20in%20high-speed%20autonomous%20racing%20demands%20precise%2C%20real-time%0Aestimation%20of%20collision%20risk%3B%20particularly%20in%20wheel-to-wheel%20scenarios%20where%0Asafety%20margins%20are%20minimal.%20Existing%20methods%20for%20collision%20risk%20estimation%0Aeither%20rely%20on%20simplified%20geometric%20approximations%2C%20like%20bounding%20circles%2C%20or%0Aperform%20Monte%20Carlo%20sampling%20which%20leads%20to%20overly%20conservative%20motion%20planning%0Abehavior%20at%20racing%20speeds.%20We%20introduce%20the%20Gauss-Legendre%20Rectangle%20%28GLR%29%0Aalgorithm%2C%20a%20principled%20two-stage%20integration%20method%20that%20estimates%20collision%0Arisk%20by%20combining%20Gauss-Legendre%20with%20a%20non-homogeneous%20Poisson%20process%20over%0Atime.%20GLR%20produces%20accurate%20risk%20estimates%20that%20account%20for%20vehicle%20geometry%0Aand%20trajectory%20uncertainty.%20In%20experiments%20across%20446%20overtaking%20scenarios%20in%20a%0Ahigh-fidelity%20Formula%20One%20racing%20simulation%2C%20GLR%20outperforms%20five%0Astate-of-the-art%20baselines%20achieving%20an%20average%20error%20reduction%20of%2077%25%20and%0Asurpassing%20the%20next-best%20method%20by%2052%25%2C%20all%20while%20running%20at%201000%20Hz.%20The%0Aframework%20is%20general%20and%20applicable%20to%20broader%20motion%20planning%20contexts%20beyond%0Aautonomous%20racing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Collision%2520Risk%2520Estimation%2520through%2520Gauss-Legendre%2520Cubature%250A%2520%2520and%2520Non-Homogeneous%2520Poisson%2520Processes%26entry.906535625%3DTrent%2520Weiss%2520and%2520Madhur%2520Behl%26entry.1292438233%3D%2520%2520Overtaking%2520in%2520high-speed%2520autonomous%2520racing%2520demands%2520precise%252C%2520real-time%250Aestimation%2520of%2520collision%2520risk%253B%2520particularly%2520in%2520wheel-to-wheel%2520scenarios%2520where%250Asafety%2520margins%2520are%2520minimal.%2520Existing%2520methods%2520for%2520collision%2520risk%2520estimation%250Aeither%2520rely%2520on%2520simplified%2520geometric%2520approximations%252C%2520like%2520bounding%2520circles%252C%2520or%250Aperform%2520Monte%2520Carlo%2520sampling%2520which%2520leads%2520to%2520overly%2520conservative%2520motion%2520planning%250Abehavior%2520at%2520racing%2520speeds.%2520We%2520introduce%2520the%2520Gauss-Legendre%2520Rectangle%2520%2528GLR%2529%250Aalgorithm%252C%2520a%2520principled%2520two-stage%2520integration%2520method%2520that%2520estimates%2520collision%250Arisk%2520by%2520combining%2520Gauss-Legendre%2520with%2520a%2520non-homogeneous%2520Poisson%2520process%2520over%250Atime.%2520GLR%2520produces%2520accurate%2520risk%2520estimates%2520that%2520account%2520for%2520vehicle%2520geometry%250Aand%2520trajectory%2520uncertainty.%2520In%2520experiments%2520across%2520446%2520overtaking%2520scenarios%2520in%2520a%250Ahigh-fidelity%2520Formula%2520One%2520racing%2520simulation%252C%2520GLR%2520outperforms%2520five%250Astate-of-the-art%2520baselines%2520achieving%2520an%2520average%2520error%2520reduction%2520of%252077%2525%2520and%250Asurpassing%2520the%2520next-best%2520method%2520by%252052%2525%252C%2520all%2520while%2520running%2520at%25201000%2520Hz.%2520The%250Aframework%2520is%2520general%2520and%2520applicable%2520to%2520broader%2520motion%2520planning%2520contexts%2520beyond%250Aautonomous%2520racing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Collision%20Risk%20Estimation%20through%20Gauss-Legendre%20Cubature%0A%20%20and%20Non-Homogeneous%20Poisson%20Processes&entry.906535625=Trent%20Weiss%20and%20Madhur%20Behl&entry.1292438233=%20%20Overtaking%20in%20high-speed%20autonomous%20racing%20demands%20precise%2C%20real-time%0Aestimation%20of%20collision%20risk%3B%20particularly%20in%20wheel-to-wheel%20scenarios%20where%0Asafety%20margins%20are%20minimal.%20Existing%20methods%20for%20collision%20risk%20estimation%0Aeither%20rely%20on%20simplified%20geometric%20approximations%2C%20like%20bounding%20circles%2C%20or%0Aperform%20Monte%20Carlo%20sampling%20which%20leads%20to%20overly%20conservative%20motion%20planning%0Abehavior%20at%20racing%20speeds.%20We%20introduce%20the%20Gauss-Legendre%20Rectangle%20%28GLR%29%0Aalgorithm%2C%20a%20principled%20two-stage%20integration%20method%20that%20estimates%20collision%0Arisk%20by%20combining%20Gauss-Legendre%20with%20a%20non-homogeneous%20Poisson%20process%20over%0Atime.%20GLR%20produces%20accurate%20risk%20estimates%20that%20account%20for%20vehicle%20geometry%0Aand%20trajectory%20uncertainty.%20In%20experiments%20across%20446%20overtaking%20scenarios%20in%20a%0Ahigh-fidelity%20Formula%20One%20racing%20simulation%2C%20GLR%20outperforms%20five%0Astate-of-the-art%20baselines%20achieving%20an%20average%20error%20reduction%20of%2077%25%20and%0Asurpassing%20the%20next-best%20method%20by%2052%25%2C%20all%20while%20running%20at%201000%20Hz.%20The%0Aframework%20is%20general%20and%20applicable%20to%20broader%20motion%20planning%20contexts%20beyond%0Aautonomous%20racing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18819v2&entry.124074799=Read"},
{"title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification", "author": "Wei Li and Renshan Zhang and Rui Shao and Jie He and Liqiang Nie", "abstract": "  Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.\n", "link": "http://arxiv.org/abs/2508.21046v2", "date": "2025-10-01", "relevancy": 2.8301, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5758}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification&body=Title%3A%20CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification%0AAuthor%3A%20Wei%20Li%20and%20Renshan%20Zhang%20and%20Rui%20Shao%20and%20Jie%20He%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Recent%20Vision-Language-Action%20%28VLA%29%20models%20built%20on%20pre-trained%0AVision-Language%20Models%20%28VLMs%29%20require%20extensive%20post-training%2C%20resulting%20in%0Ahigh%20computational%20overhead%20that%20limits%20scalability%20and%20deployment.We%20propose%0ACogVLA%2C%20a%20Cognition-Aligned%20Vision-Language-Action%20framework%20that%20leverages%0Ainstruction-driven%20routing%20and%20sparsification%20to%20improve%20both%20efficiency%20and%0Aperformance.%20CogVLA%20draws%20inspiration%20from%20human%20multimodal%20coordination%20and%0Aintroduces%20a%203-stage%20progressive%20architecture.%201%29%20Encoder-FiLM%20based%0AAggregation%20Routing%20%28EFA-Routing%29%20injects%20instruction%20information%20into%20the%0Avision%20encoder%20to%20selectively%20aggregate%20and%20compress%20dual-stream%20visual%20tokens%2C%0Aforming%20a%20instruction-aware%20latent%20representation.%202%29%20Building%20upon%20this%0Acompact%20visual%20encoding%2C%20LLM-FiLM%20based%20Pruning%20Routing%20%28LFP-Routing%29%0Aintroduces%20action%20intent%20into%20the%20language%20model%20by%20pruning%0Ainstruction-irrelevant%20visually%20grounded%20tokens%2C%20thereby%20achieving%20token-level%0Asparsity.%203%29%20To%20ensure%20that%20compressed%20perception%20inputs%20can%20still%20support%0Aaccurate%20and%20coherent%20action%20generation%2C%20we%20introduce%20V-L-A%20Coupled%20Attention%0A%28CAtten%29%2C%20which%20combines%20causal%20vision-language%20attention%20with%20bidirectional%0Aaction%20parallel%20decoding.%20Extensive%20experiments%20on%20the%20LIBERO%20benchmark%20and%0Areal-world%20robotic%20tasks%20demonstrate%20that%20CogVLA%20achieves%20state-of-the-art%0Aperformance%20with%20success%20rates%20of%2097.4%25%20and%2070.0%25%2C%20respectively%2C%20while%20reducing%0Atraining%20costs%20by%202.5-fold%20and%20decreasing%20inference%20latency%20by%202.8-fold%0Acompared%20to%20OpenVLA.%20CogVLA%20is%20open-sourced%20and%20publicly%20available%20at%0Ahttps%3A//github.com/JiuTian-VL/CogVLA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogVLA%253A%2520Cognition-Aligned%2520Vision-Language-Action%2520Model%2520via%250A%2520%2520Instruction-Driven%2520Routing%2520%2526%2520Sparsification%26entry.906535625%3DWei%2520Li%2520and%2520Renshan%2520Zhang%2520and%2520Rui%2520Shao%2520and%2520Jie%2520He%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Recent%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520built%2520on%2520pre-trained%250AVision-Language%2520Models%2520%2528VLMs%2529%2520require%2520extensive%2520post-training%252C%2520resulting%2520in%250Ahigh%2520computational%2520overhead%2520that%2520limits%2520scalability%2520and%2520deployment.We%2520propose%250ACogVLA%252C%2520a%2520Cognition-Aligned%2520Vision-Language-Action%2520framework%2520that%2520leverages%250Ainstruction-driven%2520routing%2520and%2520sparsification%2520to%2520improve%2520both%2520efficiency%2520and%250Aperformance.%2520CogVLA%2520draws%2520inspiration%2520from%2520human%2520multimodal%2520coordination%2520and%250Aintroduces%2520a%25203-stage%2520progressive%2520architecture.%25201%2529%2520Encoder-FiLM%2520based%250AAggregation%2520Routing%2520%2528EFA-Routing%2529%2520injects%2520instruction%2520information%2520into%2520the%250Avision%2520encoder%2520to%2520selectively%2520aggregate%2520and%2520compress%2520dual-stream%2520visual%2520tokens%252C%250Aforming%2520a%2520instruction-aware%2520latent%2520representation.%25202%2529%2520Building%2520upon%2520this%250Acompact%2520visual%2520encoding%252C%2520LLM-FiLM%2520based%2520Pruning%2520Routing%2520%2528LFP-Routing%2529%250Aintroduces%2520action%2520intent%2520into%2520the%2520language%2520model%2520by%2520pruning%250Ainstruction-irrelevant%2520visually%2520grounded%2520tokens%252C%2520thereby%2520achieving%2520token-level%250Asparsity.%25203%2529%2520To%2520ensure%2520that%2520compressed%2520perception%2520inputs%2520can%2520still%2520support%250Aaccurate%2520and%2520coherent%2520action%2520generation%252C%2520we%2520introduce%2520V-L-A%2520Coupled%2520Attention%250A%2528CAtten%2529%252C%2520which%2520combines%2520causal%2520vision-language%2520attention%2520with%2520bidirectional%250Aaction%2520parallel%2520decoding.%2520Extensive%2520experiments%2520on%2520the%2520LIBERO%2520benchmark%2520and%250Areal-world%2520robotic%2520tasks%2520demonstrate%2520that%2520CogVLA%2520achieves%2520state-of-the-art%250Aperformance%2520with%2520success%2520rates%2520of%252097.4%2525%2520and%252070.0%2525%252C%2520respectively%252C%2520while%2520reducing%250Atraining%2520costs%2520by%25202.5-fold%2520and%2520decreasing%2520inference%2520latency%2520by%25202.8-fold%250Acompared%2520to%2520OpenVLA.%2520CogVLA%2520is%2520open-sourced%2520and%2520publicly%2520available%2520at%250Ahttps%253A//github.com/JiuTian-VL/CogVLA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogVLA%3A%20Cognition-Aligned%20Vision-Language-Action%20Model%20via%0A%20%20Instruction-Driven%20Routing%20%26%20Sparsification&entry.906535625=Wei%20Li%20and%20Renshan%20Zhang%20and%20Rui%20Shao%20and%20Jie%20He%20and%20Liqiang%20Nie&entry.1292438233=%20%20Recent%20Vision-Language-Action%20%28VLA%29%20models%20built%20on%20pre-trained%0AVision-Language%20Models%20%28VLMs%29%20require%20extensive%20post-training%2C%20resulting%20in%0Ahigh%20computational%20overhead%20that%20limits%20scalability%20and%20deployment.We%20propose%0ACogVLA%2C%20a%20Cognition-Aligned%20Vision-Language-Action%20framework%20that%20leverages%0Ainstruction-driven%20routing%20and%20sparsification%20to%20improve%20both%20efficiency%20and%0Aperformance.%20CogVLA%20draws%20inspiration%20from%20human%20multimodal%20coordination%20and%0Aintroduces%20a%203-stage%20progressive%20architecture.%201%29%20Encoder-FiLM%20based%0AAggregation%20Routing%20%28EFA-Routing%29%20injects%20instruction%20information%20into%20the%0Avision%20encoder%20to%20selectively%20aggregate%20and%20compress%20dual-stream%20visual%20tokens%2C%0Aforming%20a%20instruction-aware%20latent%20representation.%202%29%20Building%20upon%20this%0Acompact%20visual%20encoding%2C%20LLM-FiLM%20based%20Pruning%20Routing%20%28LFP-Routing%29%0Aintroduces%20action%20intent%20into%20the%20language%20model%20by%20pruning%0Ainstruction-irrelevant%20visually%20grounded%20tokens%2C%20thereby%20achieving%20token-level%0Asparsity.%203%29%20To%20ensure%20that%20compressed%20perception%20inputs%20can%20still%20support%0Aaccurate%20and%20coherent%20action%20generation%2C%20we%20introduce%20V-L-A%20Coupled%20Attention%0A%28CAtten%29%2C%20which%20combines%20causal%20vision-language%20attention%20with%20bidirectional%0Aaction%20parallel%20decoding.%20Extensive%20experiments%20on%20the%20LIBERO%20benchmark%20and%0Areal-world%20robotic%20tasks%20demonstrate%20that%20CogVLA%20achieves%20state-of-the-art%0Aperformance%20with%20success%20rates%20of%2097.4%25%20and%2070.0%25%2C%20respectively%2C%20while%20reducing%0Atraining%20costs%20by%202.5-fold%20and%20decreasing%20inference%20latency%20by%202.8-fold%0Acompared%20to%20OpenVLA.%20CogVLA%20is%20open-sourced%20and%20publicly%20available%20at%0Ahttps%3A//github.com/JiuTian-VL/CogVLA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21046v2&entry.124074799=Read"},
{"title": "Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned", "author": "Brandon Ong and Tej Deep Pala and Vernon Toh and William Chandra Tjhi and Soujanya Poria", "abstract": "  Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.\n", "link": "http://arxiv.org/abs/2509.23250v2", "date": "2025-10-01", "relevancy": 2.8278, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Vision-Language%20Process%20Reward%20Models%20for%20Test-Time%20Scaling%20in%0A%20%20Multimodal%20Reasoning%3A%20Key%20Insights%20and%20Lessons%20Learned&body=Title%3A%20Training%20Vision-Language%20Process%20Reward%20Models%20for%20Test-Time%20Scaling%20in%0A%20%20Multimodal%20Reasoning%3A%20Key%20Insights%20and%20Lessons%20Learned%0AAuthor%3A%20Brandon%20Ong%20and%20Tej%20Deep%20Pala%20and%20Vernon%20Toh%20and%20William%20Chandra%20Tjhi%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Process%20Reward%20Models%20%28PRMs%29%20provide%20step-level%20supervision%20that%20improves%20the%0Areliability%20of%20reasoning%20in%20large%20language%20models.%20While%20PRMs%20have%20been%0Aextensively%20studied%20in%20text-based%20domains%2C%20their%20extension%20to%20Vision%20Language%0AModels%20%28VLMs%29%20remains%20limited.%20Existing%20Vision-Language%20PRMs%20%28VL-PRMs%29%20rely%20on%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%20for%20data%20construction%2C%20which%20can%20often%20produce%0Anoisy%20supervision%20signals%20and%20limit%20generalization%20across%20tasks.%20In%20this%20work%2C%0Awe%20aim%20to%20elucidate%20the%20design%20space%20of%20VL-PRMs%20by%20exploring%20diverse%20strategies%0Afor%20dataset%20construction%2C%20training%2C%20and%20test-time%20scaling.%20First%2C%20we%20introduce%0Aa%20hybrid%20data%20synthesis%20framework%20that%20combines%20MCTS%20with%20judgments%20from%20a%0Astrong%20VLM%2C%20producing%20more%20accurate%20step-level%20labels.%20Second%2C%20we%20propose%0Aperception-focused%20supervision%2C%20enabling%20our%20PRM%20to%20explicitly%20detect%20errors%20at%0Athe%20visual%20grounding%20stage%20of%20reasoning.%20Third%2C%20we%20systematically%20evaluate%0Amultiple%20test-time%20scaling%20strategies%2C%20showing%20that%20our%20PRMs%20can%20reliably%20guide%0AVLMs%20toward%20more%20accurate%20solutions.%20Our%20experiments%20covering%20five%20diverse%0Amultimodal%20benchmarks%20%28MMMU%2C%20PuzzleVQA%2C%20AlgoPuzzleVQA%2C%20MathVista%2C%20and%0AMathVision%29%20reveal%20several%20key%20insights%3A%20%28i%29%20VL-PRMs%20when%20used%20as%20Outcome%0AReward%20Models%20%28ORMs%29%20during%20test-time%20scaling%20%28TTS%29%20can%20outperform%20VL-PRM%0Aguided%20process%20step%20selection%2C%20%28ii%29%20smaller%20VL-PRMs%20can%20match%20or%20even%20surpass%0Alarger%20ones%20in%20detecting%20process%20errors%2C%20%28iii%29%20VL-PRMs%20uncover%20latent%20reasoning%0Aabilities%20in%20stronger%20VLM%20backbones%2C%20%28iv%29%20perception-level%20supervision%20leads%20to%0Asignificant%20gains%20in%20test-time%20scaling%2C%20and%20%28v%29%20TTS%20performance%20of%20different%0Apolicies%20improve%20on%20advanced%20math%20reasoning%20datasets%20despite%20not%20training%0AVL-PRMs%20on%20such%20datasets.%20We%20hope%20our%20work%20will%20motivate%20further%20research%20and%0Asupport%20the%20advancement%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Vision-Language%2520Process%2520Reward%2520Models%2520for%2520Test-Time%2520Scaling%2520in%250A%2520%2520Multimodal%2520Reasoning%253A%2520Key%2520Insights%2520and%2520Lessons%2520Learned%26entry.906535625%3DBrandon%2520Ong%2520and%2520Tej%2520Deep%2520Pala%2520and%2520Vernon%2520Toh%2520and%2520William%2520Chandra%2520Tjhi%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Process%2520Reward%2520Models%2520%2528PRMs%2529%2520provide%2520step-level%2520supervision%2520that%2520improves%2520the%250Areliability%2520of%2520reasoning%2520in%2520large%2520language%2520models.%2520While%2520PRMs%2520have%2520been%250Aextensively%2520studied%2520in%2520text-based%2520domains%252C%2520their%2520extension%2520to%2520Vision%2520Language%250AModels%2520%2528VLMs%2529%2520remains%2520limited.%2520Existing%2520Vision-Language%2520PRMs%2520%2528VL-PRMs%2529%2520rely%2520on%250AMonte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%2520for%2520data%2520construction%252C%2520which%2520can%2520often%2520produce%250Anoisy%2520supervision%2520signals%2520and%2520limit%2520generalization%2520across%2520tasks.%2520In%2520this%2520work%252C%250Awe%2520aim%2520to%2520elucidate%2520the%2520design%2520space%2520of%2520VL-PRMs%2520by%2520exploring%2520diverse%2520strategies%250Afor%2520dataset%2520construction%252C%2520training%252C%2520and%2520test-time%2520scaling.%2520First%252C%2520we%2520introduce%250Aa%2520hybrid%2520data%2520synthesis%2520framework%2520that%2520combines%2520MCTS%2520with%2520judgments%2520from%2520a%250Astrong%2520VLM%252C%2520producing%2520more%2520accurate%2520step-level%2520labels.%2520Second%252C%2520we%2520propose%250Aperception-focused%2520supervision%252C%2520enabling%2520our%2520PRM%2520to%2520explicitly%2520detect%2520errors%2520at%250Athe%2520visual%2520grounding%2520stage%2520of%2520reasoning.%2520Third%252C%2520we%2520systematically%2520evaluate%250Amultiple%2520test-time%2520scaling%2520strategies%252C%2520showing%2520that%2520our%2520PRMs%2520can%2520reliably%2520guide%250AVLMs%2520toward%2520more%2520accurate%2520solutions.%2520Our%2520experiments%2520covering%2520five%2520diverse%250Amultimodal%2520benchmarks%2520%2528MMMU%252C%2520PuzzleVQA%252C%2520AlgoPuzzleVQA%252C%2520MathVista%252C%2520and%250AMathVision%2529%2520reveal%2520several%2520key%2520insights%253A%2520%2528i%2529%2520VL-PRMs%2520when%2520used%2520as%2520Outcome%250AReward%2520Models%2520%2528ORMs%2529%2520during%2520test-time%2520scaling%2520%2528TTS%2529%2520can%2520outperform%2520VL-PRM%250Aguided%2520process%2520step%2520selection%252C%2520%2528ii%2529%2520smaller%2520VL-PRMs%2520can%2520match%2520or%2520even%2520surpass%250Alarger%2520ones%2520in%2520detecting%2520process%2520errors%252C%2520%2528iii%2529%2520VL-PRMs%2520uncover%2520latent%2520reasoning%250Aabilities%2520in%2520stronger%2520VLM%2520backbones%252C%2520%2528iv%2529%2520perception-level%2520supervision%2520leads%2520to%250Asignificant%2520gains%2520in%2520test-time%2520scaling%252C%2520and%2520%2528v%2529%2520TTS%2520performance%2520of%2520different%250Apolicies%2520improve%2520on%2520advanced%2520math%2520reasoning%2520datasets%2520despite%2520not%2520training%250AVL-PRMs%2520on%2520such%2520datasets.%2520We%2520hope%2520our%2520work%2520will%2520motivate%2520further%2520research%2520and%250Asupport%2520the%2520advancement%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Vision-Language%20Process%20Reward%20Models%20for%20Test-Time%20Scaling%20in%0A%20%20Multimodal%20Reasoning%3A%20Key%20Insights%20and%20Lessons%20Learned&entry.906535625=Brandon%20Ong%20and%20Tej%20Deep%20Pala%20and%20Vernon%20Toh%20and%20William%20Chandra%20Tjhi%20and%20Soujanya%20Poria&entry.1292438233=%20%20Process%20Reward%20Models%20%28PRMs%29%20provide%20step-level%20supervision%20that%20improves%20the%0Areliability%20of%20reasoning%20in%20large%20language%20models.%20While%20PRMs%20have%20been%0Aextensively%20studied%20in%20text-based%20domains%2C%20their%20extension%20to%20Vision%20Language%0AModels%20%28VLMs%29%20remains%20limited.%20Existing%20Vision-Language%20PRMs%20%28VL-PRMs%29%20rely%20on%0AMonte%20Carlo%20Tree%20Search%20%28MCTS%29%20for%20data%20construction%2C%20which%20can%20often%20produce%0Anoisy%20supervision%20signals%20and%20limit%20generalization%20across%20tasks.%20In%20this%20work%2C%0Awe%20aim%20to%20elucidate%20the%20design%20space%20of%20VL-PRMs%20by%20exploring%20diverse%20strategies%0Afor%20dataset%20construction%2C%20training%2C%20and%20test-time%20scaling.%20First%2C%20we%20introduce%0Aa%20hybrid%20data%20synthesis%20framework%20that%20combines%20MCTS%20with%20judgments%20from%20a%0Astrong%20VLM%2C%20producing%20more%20accurate%20step-level%20labels.%20Second%2C%20we%20propose%0Aperception-focused%20supervision%2C%20enabling%20our%20PRM%20to%20explicitly%20detect%20errors%20at%0Athe%20visual%20grounding%20stage%20of%20reasoning.%20Third%2C%20we%20systematically%20evaluate%0Amultiple%20test-time%20scaling%20strategies%2C%20showing%20that%20our%20PRMs%20can%20reliably%20guide%0AVLMs%20toward%20more%20accurate%20solutions.%20Our%20experiments%20covering%20five%20diverse%0Amultimodal%20benchmarks%20%28MMMU%2C%20PuzzleVQA%2C%20AlgoPuzzleVQA%2C%20MathVista%2C%20and%0AMathVision%29%20reveal%20several%20key%20insights%3A%20%28i%29%20VL-PRMs%20when%20used%20as%20Outcome%0AReward%20Models%20%28ORMs%29%20during%20test-time%20scaling%20%28TTS%29%20can%20outperform%20VL-PRM%0Aguided%20process%20step%20selection%2C%20%28ii%29%20smaller%20VL-PRMs%20can%20match%20or%20even%20surpass%0Alarger%20ones%20in%20detecting%20process%20errors%2C%20%28iii%29%20VL-PRMs%20uncover%20latent%20reasoning%0Aabilities%20in%20stronger%20VLM%20backbones%2C%20%28iv%29%20perception-level%20supervision%20leads%20to%0Asignificant%20gains%20in%20test-time%20scaling%2C%20and%20%28v%29%20TTS%20performance%20of%20different%0Apolicies%20improve%20on%20advanced%20math%20reasoning%20datasets%20despite%20not%20training%0AVL-PRMs%20on%20such%20datasets.%20We%20hope%20our%20work%20will%20motivate%20further%20research%20and%0Asupport%20the%20advancement%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23250v2&entry.124074799=Read"},
{"title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language\n  Models through Agentic Data Synthesis", "author": "Congzhi Zhang and Zhibin Wang and Yinchao Ma and Jiawei Peng and Yihan Wang and Qiang Zhou and Jun Song and Bo Zheng", "abstract": "  While Reinforcement Learning with Verifiable Reward (RLVR) significantly\nadvances image reasoning in Large Vision-Language Models (LVLMs), its\napplication to complex video reasoning remains underdeveloped. This gap stems\nprimarily from a critical data bottleneck: existing datasets lack the\nchallenging, multi-hop questions and high-quality, video-grounded\nChain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address\nthis, we introduce ReWatch, a large-scale dataset built to foster advanced\nvideo reasoning. We propose a novel multi-stage synthesis pipeline to\nsynthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.\nA core innovation is our Multi-Agent ReAct framework for CoT synthesis, which\nsimulates a human-like \"re-watching\" process to generate video-grounded\nreasoning traces by explicitly modeling information retrieval and verification.\nBuilding on this dataset, we develop ReWatch-R1 by post-training a strong\nbaseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This\nframework incorporates a novel Observation \\& Reasoning (O\\&R) reward mechanism\nthat evaluates both the final answer's correctness and the reasoning's\nalignment with video content, directly penalizing hallucination. Our\nexperiments show that ReWatch-R1 achieves state-of-the-art average performance\non five challenging video reasoning benchmarks. Project Page:\nhttps://rewatch-r1.github.io\n", "link": "http://arxiv.org/abs/2509.23652v2", "date": "2025-10-01", "relevancy": 2.7977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5759}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReWatch-R1%3A%20Boosting%20Complex%20Video%20Reasoning%20in%20Large%20Vision-Language%0A%20%20Models%20through%20Agentic%20Data%20Synthesis&body=Title%3A%20ReWatch-R1%3A%20Boosting%20Complex%20Video%20Reasoning%20in%20Large%20Vision-Language%0A%20%20Models%20through%20Agentic%20Data%20Synthesis%0AAuthor%3A%20Congzhi%20Zhang%20and%20Zhibin%20Wang%20and%20Yinchao%20Ma%20and%20Jiawei%20Peng%20and%20Yihan%20Wang%20and%20Qiang%20Zhou%20and%20Jun%20Song%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20significantly%0Aadvances%20image%20reasoning%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20its%0Aapplication%20to%20complex%20video%20reasoning%20remains%20underdeveloped.%20This%20gap%20stems%0Aprimarily%20from%20a%20critical%20data%20bottleneck%3A%20existing%20datasets%20lack%20the%0Achallenging%2C%20multi-hop%20questions%20and%20high-quality%2C%20video-grounded%0AChain-of-Thought%20%28CoT%29%20data%20necessary%20to%20effectively%20bootstrap%20RLVR.%20To%20address%0Athis%2C%20we%20introduce%20ReWatch%2C%20a%20large-scale%20dataset%20built%20to%20foster%20advanced%0Avideo%20reasoning.%20We%20propose%20a%20novel%20multi-stage%20synthesis%20pipeline%20to%0Asynthesize%20its%20three%20components%3A%20ReWatch-Caption%2C%20ReWatch-QA%2C%20and%20ReWatch-CoT.%0AA%20core%20innovation%20is%20our%20Multi-Agent%20ReAct%20framework%20for%20CoT%20synthesis%2C%20which%0Asimulates%20a%20human-like%20%22re-watching%22%20process%20to%20generate%20video-grounded%0Areasoning%20traces%20by%20explicitly%20modeling%20information%20retrieval%20and%20verification.%0ABuilding%20on%20this%20dataset%2C%20we%20develop%20ReWatch-R1%20by%20post-training%20a%20strong%0Abaseline%20LVLM%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20our%20RLVR%20framework.%20This%0Aframework%20incorporates%20a%20novel%20Observation%20%5C%26%20Reasoning%20%28O%5C%26R%29%20reward%20mechanism%0Athat%20evaluates%20both%20the%20final%20answer%27s%20correctness%20and%20the%20reasoning%27s%0Aalignment%20with%20video%20content%2C%20directly%20penalizing%20hallucination.%20Our%0Aexperiments%20show%20that%20ReWatch-R1%20achieves%20state-of-the-art%20average%20performance%0Aon%20five%20challenging%20video%20reasoning%20benchmarks.%20Project%20Page%3A%0Ahttps%3A//rewatch-r1.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReWatch-R1%253A%2520Boosting%2520Complex%2520Video%2520Reasoning%2520in%2520Large%2520Vision-Language%250A%2520%2520Models%2520through%2520Agentic%2520Data%2520Synthesis%26entry.906535625%3DCongzhi%2520Zhang%2520and%2520Zhibin%2520Wang%2520and%2520Yinchao%2520Ma%2520and%2520Jiawei%2520Peng%2520and%2520Yihan%2520Wang%2520and%2520Qiang%2520Zhou%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520While%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Reward%2520%2528RLVR%2529%2520significantly%250Aadvances%2520image%2520reasoning%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520its%250Aapplication%2520to%2520complex%2520video%2520reasoning%2520remains%2520underdeveloped.%2520This%2520gap%2520stems%250Aprimarily%2520from%2520a%2520critical%2520data%2520bottleneck%253A%2520existing%2520datasets%2520lack%2520the%250Achallenging%252C%2520multi-hop%2520questions%2520and%2520high-quality%252C%2520video-grounded%250AChain-of-Thought%2520%2528CoT%2529%2520data%2520necessary%2520to%2520effectively%2520bootstrap%2520RLVR.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520ReWatch%252C%2520a%2520large-scale%2520dataset%2520built%2520to%2520foster%2520advanced%250Avideo%2520reasoning.%2520We%2520propose%2520a%2520novel%2520multi-stage%2520synthesis%2520pipeline%2520to%250Asynthesize%2520its%2520three%2520components%253A%2520ReWatch-Caption%252C%2520ReWatch-QA%252C%2520and%2520ReWatch-CoT.%250AA%2520core%2520innovation%2520is%2520our%2520Multi-Agent%2520ReAct%2520framework%2520for%2520CoT%2520synthesis%252C%2520which%250Asimulates%2520a%2520human-like%2520%2522re-watching%2522%2520process%2520to%2520generate%2520video-grounded%250Areasoning%2520traces%2520by%2520explicitly%2520modeling%2520information%2520retrieval%2520and%2520verification.%250ABuilding%2520on%2520this%2520dataset%252C%2520we%2520develop%2520ReWatch-R1%2520by%2520post-training%2520a%2520strong%250Abaseline%2520LVLM%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520our%2520RLVR%2520framework.%2520This%250Aframework%2520incorporates%2520a%2520novel%2520Observation%2520%255C%2526%2520Reasoning%2520%2528O%255C%2526R%2529%2520reward%2520mechanism%250Athat%2520evaluates%2520both%2520the%2520final%2520answer%2527s%2520correctness%2520and%2520the%2520reasoning%2527s%250Aalignment%2520with%2520video%2520content%252C%2520directly%2520penalizing%2520hallucination.%2520Our%250Aexperiments%2520show%2520that%2520ReWatch-R1%2520achieves%2520state-of-the-art%2520average%2520performance%250Aon%2520five%2520challenging%2520video%2520reasoning%2520benchmarks.%2520Project%2520Page%253A%250Ahttps%253A//rewatch-r1.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReWatch-R1%3A%20Boosting%20Complex%20Video%20Reasoning%20in%20Large%20Vision-Language%0A%20%20Models%20through%20Agentic%20Data%20Synthesis&entry.906535625=Congzhi%20Zhang%20and%20Zhibin%20Wang%20and%20Yinchao%20Ma%20and%20Jiawei%20Peng%20and%20Yihan%20Wang%20and%20Qiang%20Zhou%20and%20Jun%20Song%20and%20Bo%20Zheng&entry.1292438233=%20%20While%20Reinforcement%20Learning%20with%20Verifiable%20Reward%20%28RLVR%29%20significantly%0Aadvances%20image%20reasoning%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20its%0Aapplication%20to%20complex%20video%20reasoning%20remains%20underdeveloped.%20This%20gap%20stems%0Aprimarily%20from%20a%20critical%20data%20bottleneck%3A%20existing%20datasets%20lack%20the%0Achallenging%2C%20multi-hop%20questions%20and%20high-quality%2C%20video-grounded%0AChain-of-Thought%20%28CoT%29%20data%20necessary%20to%20effectively%20bootstrap%20RLVR.%20To%20address%0Athis%2C%20we%20introduce%20ReWatch%2C%20a%20large-scale%20dataset%20built%20to%20foster%20advanced%0Avideo%20reasoning.%20We%20propose%20a%20novel%20multi-stage%20synthesis%20pipeline%20to%0Asynthesize%20its%20three%20components%3A%20ReWatch-Caption%2C%20ReWatch-QA%2C%20and%20ReWatch-CoT.%0AA%20core%20innovation%20is%20our%20Multi-Agent%20ReAct%20framework%20for%20CoT%20synthesis%2C%20which%0Asimulates%20a%20human-like%20%22re-watching%22%20process%20to%20generate%20video-grounded%0Areasoning%20traces%20by%20explicitly%20modeling%20information%20retrieval%20and%20verification.%0ABuilding%20on%20this%20dataset%2C%20we%20develop%20ReWatch-R1%20by%20post-training%20a%20strong%0Abaseline%20LVLM%20with%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20our%20RLVR%20framework.%20This%0Aframework%20incorporates%20a%20novel%20Observation%20%5C%26%20Reasoning%20%28O%5C%26R%29%20reward%20mechanism%0Athat%20evaluates%20both%20the%20final%20answer%27s%20correctness%20and%20the%20reasoning%27s%0Aalignment%20with%20video%20content%2C%20directly%20penalizing%20hallucination.%20Our%0Aexperiments%20show%20that%20ReWatch-R1%20achieves%20state-of-the-art%20average%20performance%0Aon%20five%20challenging%20video%20reasoning%20benchmarks.%20Project%20Page%3A%0Ahttps%3A//rewatch-r1.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23652v2&entry.124074799=Read"},
{"title": "Achieving More Human Brain-Like Vision via Human EEG Representational\n  Alignment", "author": "Zitong Lu and Yile Wang and Julie D. Golomb", "abstract": "  Despite advancements in artificial intelligence, object recognition models\nstill lag behind in emulating visual information processing in human brains.\nRecent studies have highlighted the potential of using neural data to mimic\nbrain processing; however, these often rely on invasive neural recordings from\nnon-human subjects, leaving a critical gap in understanding human visual\nperception. Addressing this gap, we present,\n'Re(presentational)Al(ignment)net', a vision model aligned with human brain\nactivity based on non-invasive EEG, demonstrating a significantly higher\nsimilarity to human brain representations. Our innovative image-to-brain\nmulti-layer encoding framework advances human neural alignment by optimizing\nmultiple model layers and enabling the model to efficiently learn and mimic the\nhuman brain's visual representational patterns across object categories and\ndifferent modalities. Our findings suggest that ReAlnets better align\nartificial neural networks with human brain representations, making it more\nsimilar to human brain processing than traditional computer vision models,\nwhich takes an important step toward bridging the gap between artificial and\nhuman vision and achieving more brain-like artificial intelligence systems.\n", "link": "http://arxiv.org/abs/2401.17231v3", "date": "2025-10-01", "relevancy": 2.7754, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20More%20Human%20Brain-Like%20Vision%20via%20Human%20EEG%20Representational%0A%20%20Alignment&body=Title%3A%20Achieving%20More%20Human%20Brain-Like%20Vision%20via%20Human%20EEG%20Representational%0A%20%20Alignment%0AAuthor%3A%20Zitong%20Lu%20and%20Yile%20Wang%20and%20Julie%20D.%20Golomb%0AAbstract%3A%20%20%20Despite%20advancements%20in%20artificial%20intelligence%2C%20object%20recognition%20models%0Astill%20lag%20behind%20in%20emulating%20visual%20information%20processing%20in%20human%20brains.%0ARecent%20studies%20have%20highlighted%20the%20potential%20of%20using%20neural%20data%20to%20mimic%0Abrain%20processing%3B%20however%2C%20these%20often%20rely%20on%20invasive%20neural%20recordings%20from%0Anon-human%20subjects%2C%20leaving%20a%20critical%20gap%20in%20understanding%20human%20visual%0Aperception.%20Addressing%20this%20gap%2C%20we%20present%2C%0A%27Re%28presentational%29Al%28ignment%29net%27%2C%20a%20vision%20model%20aligned%20with%20human%20brain%0Aactivity%20based%20on%20non-invasive%20EEG%2C%20demonstrating%20a%20significantly%20higher%0Asimilarity%20to%20human%20brain%20representations.%20Our%20innovative%20image-to-brain%0Amulti-layer%20encoding%20framework%20advances%20human%20neural%20alignment%20by%20optimizing%0Amultiple%20model%20layers%20and%20enabling%20the%20model%20to%20efficiently%20learn%20and%20mimic%20the%0Ahuman%20brain%27s%20visual%20representational%20patterns%20across%20object%20categories%20and%0Adifferent%20modalities.%20Our%20findings%20suggest%20that%20ReAlnets%20better%20align%0Aartificial%20neural%20networks%20with%20human%20brain%20representations%2C%20making%20it%20more%0Asimilar%20to%20human%20brain%20processing%20than%20traditional%20computer%20vision%20models%2C%0Awhich%20takes%20an%20important%20step%20toward%20bridging%20the%20gap%20between%20artificial%20and%0Ahuman%20vision%20and%20achieving%20more%20brain-like%20artificial%20intelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17231v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520More%2520Human%2520Brain-Like%2520Vision%2520via%2520Human%2520EEG%2520Representational%250A%2520%2520Alignment%26entry.906535625%3DZitong%2520Lu%2520and%2520Yile%2520Wang%2520and%2520Julie%2520D.%2520Golomb%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520artificial%2520intelligence%252C%2520object%2520recognition%2520models%250Astill%2520lag%2520behind%2520in%2520emulating%2520visual%2520information%2520processing%2520in%2520human%2520brains.%250ARecent%2520studies%2520have%2520highlighted%2520the%2520potential%2520of%2520using%2520neural%2520data%2520to%2520mimic%250Abrain%2520processing%253B%2520however%252C%2520these%2520often%2520rely%2520on%2520invasive%2520neural%2520recordings%2520from%250Anon-human%2520subjects%252C%2520leaving%2520a%2520critical%2520gap%2520in%2520understanding%2520human%2520visual%250Aperception.%2520Addressing%2520this%2520gap%252C%2520we%2520present%252C%250A%2527Re%2528presentational%2529Al%2528ignment%2529net%2527%252C%2520a%2520vision%2520model%2520aligned%2520with%2520human%2520brain%250Aactivity%2520based%2520on%2520non-invasive%2520EEG%252C%2520demonstrating%2520a%2520significantly%2520higher%250Asimilarity%2520to%2520human%2520brain%2520representations.%2520Our%2520innovative%2520image-to-brain%250Amulti-layer%2520encoding%2520framework%2520advances%2520human%2520neural%2520alignment%2520by%2520optimizing%250Amultiple%2520model%2520layers%2520and%2520enabling%2520the%2520model%2520to%2520efficiently%2520learn%2520and%2520mimic%2520the%250Ahuman%2520brain%2527s%2520visual%2520representational%2520patterns%2520across%2520object%2520categories%2520and%250Adifferent%2520modalities.%2520Our%2520findings%2520suggest%2520that%2520ReAlnets%2520better%2520align%250Aartificial%2520neural%2520networks%2520with%2520human%2520brain%2520representations%252C%2520making%2520it%2520more%250Asimilar%2520to%2520human%2520brain%2520processing%2520than%2520traditional%2520computer%2520vision%2520models%252C%250Awhich%2520takes%2520an%2520important%2520step%2520toward%2520bridging%2520the%2520gap%2520between%2520artificial%2520and%250Ahuman%2520vision%2520and%2520achieving%2520more%2520brain-like%2520artificial%2520intelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17231v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20More%20Human%20Brain-Like%20Vision%20via%20Human%20EEG%20Representational%0A%20%20Alignment&entry.906535625=Zitong%20Lu%20and%20Yile%20Wang%20and%20Julie%20D.%20Golomb&entry.1292438233=%20%20Despite%20advancements%20in%20artificial%20intelligence%2C%20object%20recognition%20models%0Astill%20lag%20behind%20in%20emulating%20visual%20information%20processing%20in%20human%20brains.%0ARecent%20studies%20have%20highlighted%20the%20potential%20of%20using%20neural%20data%20to%20mimic%0Abrain%20processing%3B%20however%2C%20these%20often%20rely%20on%20invasive%20neural%20recordings%20from%0Anon-human%20subjects%2C%20leaving%20a%20critical%20gap%20in%20understanding%20human%20visual%0Aperception.%20Addressing%20this%20gap%2C%20we%20present%2C%0A%27Re%28presentational%29Al%28ignment%29net%27%2C%20a%20vision%20model%20aligned%20with%20human%20brain%0Aactivity%20based%20on%20non-invasive%20EEG%2C%20demonstrating%20a%20significantly%20higher%0Asimilarity%20to%20human%20brain%20representations.%20Our%20innovative%20image-to-brain%0Amulti-layer%20encoding%20framework%20advances%20human%20neural%20alignment%20by%20optimizing%0Amultiple%20model%20layers%20and%20enabling%20the%20model%20to%20efficiently%20learn%20and%20mimic%20the%0Ahuman%20brain%27s%20visual%20representational%20patterns%20across%20object%20categories%20and%0Adifferent%20modalities.%20Our%20findings%20suggest%20that%20ReAlnets%20better%20align%0Aartificial%20neural%20networks%20with%20human%20brain%20representations%2C%20making%20it%20more%0Asimilar%20to%20human%20brain%20processing%20than%20traditional%20computer%20vision%20models%2C%0Awhich%20takes%20an%20important%20step%20toward%20bridging%20the%20gap%20between%20artificial%20and%0Ahuman%20vision%20and%20achieving%20more%20brain-like%20artificial%20intelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17231v3&entry.124074799=Read"},
{"title": "Explaining multimodal LLMs via intra-modal token interactions", "author": "Jiawei Liang and Ruoyu Chen and Xianghao Jiao and Siyuan Liang and Shiming Liu and Qunli Zhang and Zheng Hu and Xiaochun Cao", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.\n", "link": "http://arxiv.org/abs/2509.22415v2", "date": "2025-10-01", "relevancy": 2.6683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions&body=Title%3A%20Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions%0AAuthor%3A%20Jiawei%20Liang%20and%20Ruoyu%20Chen%20and%20Xianghao%20Jiao%20and%20Siyuan%20Liang%20and%20Shiming%20Liu%20and%20Qunli%20Zhang%20and%20Zheng%20Hu%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%0Aacross%20diverse%20vision-language%20tasks%2C%20yet%20their%20internal%20decision-making%0Amechanisms%20remain%20insufficiently%20understood.%20Existing%20interpretability%20research%0Ahas%20primarily%20focused%20on%20cross-modal%20attribution%2C%20identifying%20which%20image%0Aregions%20the%20model%20attends%20to%20during%20output%20generation.%20However%2C%20these%0Aapproaches%20often%20overlook%20intra-modal%20dependencies.%20In%20the%20visual%20modality%2C%0Aattributing%20importance%20to%20isolated%20image%20patches%20ignores%20spatial%20context%20due%20to%0Alimited%20receptive%20fields%2C%20resulting%20in%20fragmented%20and%20noisy%20explanations.%20In%0Athe%20textual%20modality%2C%20reliance%20on%20preceding%20tokens%20introduces%20spurious%0Aactivations.%20Failing%20to%20effectively%20mitigate%20these%20interference%20compromises%0Aattribution%20fidelity.%20To%20address%20these%20limitations%2C%20we%20propose%20enhancing%0Ainterpretability%20by%20leveraging%20intra-modal%20interaction.%20For%20the%20visual%20branch%2C%0Awe%20introduce%20%5Ctextit%7BMulti-Scale%20Explanation%20Aggregation%7D%20%28MSEA%29%2C%20which%0Aaggregates%20attributions%20over%20multi-scale%20inputs%20to%20dynamically%20adjust%20receptive%0Afields%2C%20producing%20more%20holistic%20and%20spatially%20coherent%20visual%20explanations.%20For%0Athe%20textual%20branch%2C%20we%20propose%20%5Ctextit%7BActivation%20Ranking%20Correlation%7D%20%28ARC%29%2C%0Awhich%20measures%20the%20relevance%20of%20contextual%20tokens%20to%20the%20current%20token%20via%0Aalignment%20of%20their%20top-%24k%24%20prediction%20rankings.%20ARC%20leverages%20this%20relevance%20to%0Asuppress%20spurious%20activations%20from%20irrelevant%20contexts%20while%20preserving%0Asemantically%20coherent%20ones.%20Extensive%20experiments%20across%20state-of-the-art%20MLLMs%0Aand%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20interpretability%20methods%2C%20yielding%20more%20faithful%20and%20fine-grained%0Aexplanations%20of%20model%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22415v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520multimodal%2520LLMs%2520via%2520intra-modal%2520token%2520interactions%26entry.906535625%3DJiawei%2520Liang%2520and%2520Ruoyu%2520Chen%2520and%2520Xianghao%2520Jiao%2520and%2520Siyuan%2520Liang%2520and%2520Shiming%2520Liu%2520and%2520Qunli%2520Zhang%2520and%2520Zheng%2520Hu%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%250Aacross%2520diverse%2520vision-language%2520tasks%252C%2520yet%2520their%2520internal%2520decision-making%250Amechanisms%2520remain%2520insufficiently%2520understood.%2520Existing%2520interpretability%2520research%250Ahas%2520primarily%2520focused%2520on%2520cross-modal%2520attribution%252C%2520identifying%2520which%2520image%250Aregions%2520the%2520model%2520attends%2520to%2520during%2520output%2520generation.%2520However%252C%2520these%250Aapproaches%2520often%2520overlook%2520intra-modal%2520dependencies.%2520In%2520the%2520visual%2520modality%252C%250Aattributing%2520importance%2520to%2520isolated%2520image%2520patches%2520ignores%2520spatial%2520context%2520due%2520to%250Alimited%2520receptive%2520fields%252C%2520resulting%2520in%2520fragmented%2520and%2520noisy%2520explanations.%2520In%250Athe%2520textual%2520modality%252C%2520reliance%2520on%2520preceding%2520tokens%2520introduces%2520spurious%250Aactivations.%2520Failing%2520to%2520effectively%2520mitigate%2520these%2520interference%2520compromises%250Aattribution%2520fidelity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520enhancing%250Ainterpretability%2520by%2520leveraging%2520intra-modal%2520interaction.%2520For%2520the%2520visual%2520branch%252C%250Awe%2520introduce%2520%255Ctextit%257BMulti-Scale%2520Explanation%2520Aggregation%257D%2520%2528MSEA%2529%252C%2520which%250Aaggregates%2520attributions%2520over%2520multi-scale%2520inputs%2520to%2520dynamically%2520adjust%2520receptive%250Afields%252C%2520producing%2520more%2520holistic%2520and%2520spatially%2520coherent%2520visual%2520explanations.%2520For%250Athe%2520textual%2520branch%252C%2520we%2520propose%2520%255Ctextit%257BActivation%2520Ranking%2520Correlation%257D%2520%2528ARC%2529%252C%250Awhich%2520measures%2520the%2520relevance%2520of%2520contextual%2520tokens%2520to%2520the%2520current%2520token%2520via%250Aalignment%2520of%2520their%2520top-%2524k%2524%2520prediction%2520rankings.%2520ARC%2520leverages%2520this%2520relevance%2520to%250Asuppress%2520spurious%2520activations%2520from%2520irrelevant%2520contexts%2520while%2520preserving%250Asemantically%2520coherent%2520ones.%2520Extensive%2520experiments%2520across%2520state-of-the-art%2520MLLMs%250Aand%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%250Aexisting%2520interpretability%2520methods%252C%2520yielding%2520more%2520faithful%2520and%2520fine-grained%250Aexplanations%2520of%2520model%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22415v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions&entry.906535625=Jiawei%20Liang%20and%20Ruoyu%20Chen%20and%20Xianghao%20Jiao%20and%20Siyuan%20Liang%20and%20Shiming%20Liu%20and%20Qunli%20Zhang%20and%20Zheng%20Hu%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%0Aacross%20diverse%20vision-language%20tasks%2C%20yet%20their%20internal%20decision-making%0Amechanisms%20remain%20insufficiently%20understood.%20Existing%20interpretability%20research%0Ahas%20primarily%20focused%20on%20cross-modal%20attribution%2C%20identifying%20which%20image%0Aregions%20the%20model%20attends%20to%20during%20output%20generation.%20However%2C%20these%0Aapproaches%20often%20overlook%20intra-modal%20dependencies.%20In%20the%20visual%20modality%2C%0Aattributing%20importance%20to%20isolated%20image%20patches%20ignores%20spatial%20context%20due%20to%0Alimited%20receptive%20fields%2C%20resulting%20in%20fragmented%20and%20noisy%20explanations.%20In%0Athe%20textual%20modality%2C%20reliance%20on%20preceding%20tokens%20introduces%20spurious%0Aactivations.%20Failing%20to%20effectively%20mitigate%20these%20interference%20compromises%0Aattribution%20fidelity.%20To%20address%20these%20limitations%2C%20we%20propose%20enhancing%0Ainterpretability%20by%20leveraging%20intra-modal%20interaction.%20For%20the%20visual%20branch%2C%0Awe%20introduce%20%5Ctextit%7BMulti-Scale%20Explanation%20Aggregation%7D%20%28MSEA%29%2C%20which%0Aaggregates%20attributions%20over%20multi-scale%20inputs%20to%20dynamically%20adjust%20receptive%0Afields%2C%20producing%20more%20holistic%20and%20spatially%20coherent%20visual%20explanations.%20For%0Athe%20textual%20branch%2C%20we%20propose%20%5Ctextit%7BActivation%20Ranking%20Correlation%7D%20%28ARC%29%2C%0Awhich%20measures%20the%20relevance%20of%20contextual%20tokens%20to%20the%20current%20token%20via%0Aalignment%20of%20their%20top-%24k%24%20prediction%20rankings.%20ARC%20leverages%20this%20relevance%20to%0Asuppress%20spurious%20activations%20from%20irrelevant%20contexts%20while%20preserving%0Asemantically%20coherent%20ones.%20Extensive%20experiments%20across%20state-of-the-art%20MLLMs%0Aand%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20interpretability%20methods%2C%20yielding%20more%20faithful%20and%20fine-grained%0Aexplanations%20of%20model%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22415v2&entry.124074799=Read"},
{"title": "Learning Dynamic Graph Embeddings with Neural Controlled Differential\n  Equations", "author": "Tiexin Qin and Benjamin Walker and Terry Lyons and Hong Yan and Haoliang Li", "abstract": "  This paper focuses on representation learning for dynamic graphs with\ntemporal interactions. A fundamental issue is that both the graph structure and\nthe nodes own their own dynamics, and their blending induces intractable\ncomplexity in the temporal evolution over graphs. Drawing inspiration from the\nrecent progress of physical dynamic models in deep neural networks, we propose\nGraph Neural Controlled Differential Equations (GN-CDEs), a continuous-time\nframework that jointly models node embeddings and structural dynamics by\nincorporating a graph enhanced neural network vector field with a time-varying\ngraph path as the control signal. Our framework exhibits several desirable\ncharacteristics, including the ability to express dynamics on evolving graphs\nwithout piecewise integration, the capability to calibrate trajectories with\nsubsequent data, and robustness to missing observations. Empirical evaluation\non a range of dynamic graph representation learning tasks demonstrates the\neffectiveness of our proposed approach in capturing the complex dynamics of\ndynamic graphs.\n", "link": "http://arxiv.org/abs/2302.11354v2", "date": "2025-10-01", "relevancy": 2.6477, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5854}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5156}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Graph%20Embeddings%20with%20Neural%20Controlled%20Differential%0A%20%20Equations&body=Title%3A%20Learning%20Dynamic%20Graph%20Embeddings%20with%20Neural%20Controlled%20Differential%0A%20%20Equations%0AAuthor%3A%20Tiexin%20Qin%20and%20Benjamin%20Walker%20and%20Terry%20Lyons%20and%20Hong%20Yan%20and%20Haoliang%20Li%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20representation%20learning%20for%20dynamic%20graphs%20with%0Atemporal%20interactions.%20A%20fundamental%20issue%20is%20that%20both%20the%20graph%20structure%20and%0Athe%20nodes%20own%20their%20own%20dynamics%2C%20and%20their%20blending%20induces%20intractable%0Acomplexity%20in%20the%20temporal%20evolution%20over%20graphs.%20Drawing%20inspiration%20from%20the%0Arecent%20progress%20of%20physical%20dynamic%20models%20in%20deep%20neural%20networks%2C%20we%20propose%0AGraph%20Neural%20Controlled%20Differential%20Equations%20%28GN-CDEs%29%2C%20a%20continuous-time%0Aframework%20that%20jointly%20models%20node%20embeddings%20and%20structural%20dynamics%20by%0Aincorporating%20a%20graph%20enhanced%20neural%20network%20vector%20field%20with%20a%20time-varying%0Agraph%20path%20as%20the%20control%20signal.%20Our%20framework%20exhibits%20several%20desirable%0Acharacteristics%2C%20including%20the%20ability%20to%20express%20dynamics%20on%20evolving%20graphs%0Awithout%20piecewise%20integration%2C%20the%20capability%20to%20calibrate%20trajectories%20with%0Asubsequent%20data%2C%20and%20robustness%20to%20missing%20observations.%20Empirical%20evaluation%0Aon%20a%20range%20of%20dynamic%20graph%20representation%20learning%20tasks%20demonstrates%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20in%20capturing%20the%20complex%20dynamics%20of%0Adynamic%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.11354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Graph%2520Embeddings%2520with%2520Neural%2520Controlled%2520Differential%250A%2520%2520Equations%26entry.906535625%3DTiexin%2520Qin%2520and%2520Benjamin%2520Walker%2520and%2520Terry%2520Lyons%2520and%2520Hong%2520Yan%2520and%2520Haoliang%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520representation%2520learning%2520for%2520dynamic%2520graphs%2520with%250Atemporal%2520interactions.%2520A%2520fundamental%2520issue%2520is%2520that%2520both%2520the%2520graph%2520structure%2520and%250Athe%2520nodes%2520own%2520their%2520own%2520dynamics%252C%2520and%2520their%2520blending%2520induces%2520intractable%250Acomplexity%2520in%2520the%2520temporal%2520evolution%2520over%2520graphs.%2520Drawing%2520inspiration%2520from%2520the%250Arecent%2520progress%2520of%2520physical%2520dynamic%2520models%2520in%2520deep%2520neural%2520networks%252C%2520we%2520propose%250AGraph%2520Neural%2520Controlled%2520Differential%2520Equations%2520%2528GN-CDEs%2529%252C%2520a%2520continuous-time%250Aframework%2520that%2520jointly%2520models%2520node%2520embeddings%2520and%2520structural%2520dynamics%2520by%250Aincorporating%2520a%2520graph%2520enhanced%2520neural%2520network%2520vector%2520field%2520with%2520a%2520time-varying%250Agraph%2520path%2520as%2520the%2520control%2520signal.%2520Our%2520framework%2520exhibits%2520several%2520desirable%250Acharacteristics%252C%2520including%2520the%2520ability%2520to%2520express%2520dynamics%2520on%2520evolving%2520graphs%250Awithout%2520piecewise%2520integration%252C%2520the%2520capability%2520to%2520calibrate%2520trajectories%2520with%250Asubsequent%2520data%252C%2520and%2520robustness%2520to%2520missing%2520observations.%2520Empirical%2520evaluation%250Aon%2520a%2520range%2520of%2520dynamic%2520graph%2520representation%2520learning%2520tasks%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520approach%2520in%2520capturing%2520the%2520complex%2520dynamics%2520of%250Adynamic%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.11354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Graph%20Embeddings%20with%20Neural%20Controlled%20Differential%0A%20%20Equations&entry.906535625=Tiexin%20Qin%20and%20Benjamin%20Walker%20and%20Terry%20Lyons%20and%20Hong%20Yan%20and%20Haoliang%20Li&entry.1292438233=%20%20This%20paper%20focuses%20on%20representation%20learning%20for%20dynamic%20graphs%20with%0Atemporal%20interactions.%20A%20fundamental%20issue%20is%20that%20both%20the%20graph%20structure%20and%0Athe%20nodes%20own%20their%20own%20dynamics%2C%20and%20their%20blending%20induces%20intractable%0Acomplexity%20in%20the%20temporal%20evolution%20over%20graphs.%20Drawing%20inspiration%20from%20the%0Arecent%20progress%20of%20physical%20dynamic%20models%20in%20deep%20neural%20networks%2C%20we%20propose%0AGraph%20Neural%20Controlled%20Differential%20Equations%20%28GN-CDEs%29%2C%20a%20continuous-time%0Aframework%20that%20jointly%20models%20node%20embeddings%20and%20structural%20dynamics%20by%0Aincorporating%20a%20graph%20enhanced%20neural%20network%20vector%20field%20with%20a%20time-varying%0Agraph%20path%20as%20the%20control%20signal.%20Our%20framework%20exhibits%20several%20desirable%0Acharacteristics%2C%20including%20the%20ability%20to%20express%20dynamics%20on%20evolving%20graphs%0Awithout%20piecewise%20integration%2C%20the%20capability%20to%20calibrate%20trajectories%20with%0Asubsequent%20data%2C%20and%20robustness%20to%20missing%20observations.%20Empirical%20evaluation%0Aon%20a%20range%20of%20dynamic%20graph%20representation%20learning%20tasks%20demonstrates%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20in%20capturing%20the%20complex%20dynamics%20of%0Adynamic%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.11354v2&entry.124074799=Read"},
{"title": "GRID: Scalable Task-Agnostic Prompt-Based Continual Learning for\n  Language Models", "author": "Anushka Tiwari and Sayantan Pal and Rohini K. Srihari and Kaiyi Ji", "abstract": "  Prompt-based continual learning (CL) provides a parameter-efficient approach\nfor adapting large language models (LLMs) across task sequences. However, most\nexisting methods rely on task-aware inference and maintain a growing set of\ntask-specific prompts, which introduces two major challenges: (1) severe\nperformance degradation on earlier tasks under task-agnostic inference, and (2)\nlimited scalability due to prompt memory accumulation as task sequences grow.\nIn this paper, we present GRID, a unified framework designed to address these\nchallenges. GRID incorporates a decoding mechanism that enhances backward\ntransfer by leveraging representative inputs, automatic task identification,\nand constrained decoding. Furthermore, it employs a gradient-guided prompt\nselection strategy to compress less informative prompts into a single\naggregated representation, ensuring scalable and memory-efficient continual\nlearning. Extensive experiments on long-sequence and negative transfer\nbenchmarks show that GRID improves average accuracy and backward transfer,\nachieves competitive forward transfer, and substantially reduces prompt memory\nusage.\n", "link": "http://arxiv.org/abs/2507.14725v3", "date": "2025-10-01", "relevancy": 2.6105, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRID%3A%20Scalable%20Task-Agnostic%20Prompt-Based%20Continual%20Learning%20for%0A%20%20Language%20Models&body=Title%3A%20GRID%3A%20Scalable%20Task-Agnostic%20Prompt-Based%20Continual%20Learning%20for%0A%20%20Language%20Models%0AAuthor%3A%20Anushka%20Tiwari%20and%20Sayantan%20Pal%20and%20Rohini%20K.%20Srihari%20and%20Kaiyi%20Ji%0AAbstract%3A%20%20%20Prompt-based%20continual%20learning%20%28CL%29%20provides%20a%20parameter-efficient%20approach%0Afor%20adapting%20large%20language%20models%20%28LLMs%29%20across%20task%20sequences.%20However%2C%20most%0Aexisting%20methods%20rely%20on%20task-aware%20inference%20and%20maintain%20a%20growing%20set%20of%0Atask-specific%20prompts%2C%20which%20introduces%20two%20major%20challenges%3A%20%281%29%20severe%0Aperformance%20degradation%20on%20earlier%20tasks%20under%20task-agnostic%20inference%2C%20and%20%282%29%0Alimited%20scalability%20due%20to%20prompt%20memory%20accumulation%20as%20task%20sequences%20grow.%0AIn%20this%20paper%2C%20we%20present%20GRID%2C%20a%20unified%20framework%20designed%20to%20address%20these%0Achallenges.%20GRID%20incorporates%20a%20decoding%20mechanism%20that%20enhances%20backward%0Atransfer%20by%20leveraging%20representative%20inputs%2C%20automatic%20task%20identification%2C%0Aand%20constrained%20decoding.%20Furthermore%2C%20it%20employs%20a%20gradient-guided%20prompt%0Aselection%20strategy%20to%20compress%20less%20informative%20prompts%20into%20a%20single%0Aaggregated%20representation%2C%20ensuring%20scalable%20and%20memory-efficient%20continual%0Alearning.%20Extensive%20experiments%20on%20long-sequence%20and%20negative%20transfer%0Abenchmarks%20show%20that%20GRID%20improves%20average%20accuracy%20and%20backward%20transfer%2C%0Aachieves%20competitive%20forward%20transfer%2C%20and%20substantially%20reduces%20prompt%20memory%0Ausage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14725v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRID%253A%2520Scalable%2520Task-Agnostic%2520Prompt-Based%2520Continual%2520Learning%2520for%250A%2520%2520Language%2520Models%26entry.906535625%3DAnushka%2520Tiwari%2520and%2520Sayantan%2520Pal%2520and%2520Rohini%2520K.%2520Srihari%2520and%2520Kaiyi%2520Ji%26entry.1292438233%3D%2520%2520Prompt-based%2520continual%2520learning%2520%2528CL%2529%2520provides%2520a%2520parameter-efficient%2520approach%250Afor%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520task%2520sequences.%2520However%252C%2520most%250Aexisting%2520methods%2520rely%2520on%2520task-aware%2520inference%2520and%2520maintain%2520a%2520growing%2520set%2520of%250Atask-specific%2520prompts%252C%2520which%2520introduces%2520two%2520major%2520challenges%253A%2520%25281%2529%2520severe%250Aperformance%2520degradation%2520on%2520earlier%2520tasks%2520under%2520task-agnostic%2520inference%252C%2520and%2520%25282%2529%250Alimited%2520scalability%2520due%2520to%2520prompt%2520memory%2520accumulation%2520as%2520task%2520sequences%2520grow.%250AIn%2520this%2520paper%252C%2520we%2520present%2520GRID%252C%2520a%2520unified%2520framework%2520designed%2520to%2520address%2520these%250Achallenges.%2520GRID%2520incorporates%2520a%2520decoding%2520mechanism%2520that%2520enhances%2520backward%250Atransfer%2520by%2520leveraging%2520representative%2520inputs%252C%2520automatic%2520task%2520identification%252C%250Aand%2520constrained%2520decoding.%2520Furthermore%252C%2520it%2520employs%2520a%2520gradient-guided%2520prompt%250Aselection%2520strategy%2520to%2520compress%2520less%2520informative%2520prompts%2520into%2520a%2520single%250Aaggregated%2520representation%252C%2520ensuring%2520scalable%2520and%2520memory-efficient%2520continual%250Alearning.%2520Extensive%2520experiments%2520on%2520long-sequence%2520and%2520negative%2520transfer%250Abenchmarks%2520show%2520that%2520GRID%2520improves%2520average%2520accuracy%2520and%2520backward%2520transfer%252C%250Aachieves%2520competitive%2520forward%2520transfer%252C%2520and%2520substantially%2520reduces%2520prompt%2520memory%250Ausage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14725v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRID%3A%20Scalable%20Task-Agnostic%20Prompt-Based%20Continual%20Learning%20for%0A%20%20Language%20Models&entry.906535625=Anushka%20Tiwari%20and%20Sayantan%20Pal%20and%20Rohini%20K.%20Srihari%20and%20Kaiyi%20Ji&entry.1292438233=%20%20Prompt-based%20continual%20learning%20%28CL%29%20provides%20a%20parameter-efficient%20approach%0Afor%20adapting%20large%20language%20models%20%28LLMs%29%20across%20task%20sequences.%20However%2C%20most%0Aexisting%20methods%20rely%20on%20task-aware%20inference%20and%20maintain%20a%20growing%20set%20of%0Atask-specific%20prompts%2C%20which%20introduces%20two%20major%20challenges%3A%20%281%29%20severe%0Aperformance%20degradation%20on%20earlier%20tasks%20under%20task-agnostic%20inference%2C%20and%20%282%29%0Alimited%20scalability%20due%20to%20prompt%20memory%20accumulation%20as%20task%20sequences%20grow.%0AIn%20this%20paper%2C%20we%20present%20GRID%2C%20a%20unified%20framework%20designed%20to%20address%20these%0Achallenges.%20GRID%20incorporates%20a%20decoding%20mechanism%20that%20enhances%20backward%0Atransfer%20by%20leveraging%20representative%20inputs%2C%20automatic%20task%20identification%2C%0Aand%20constrained%20decoding.%20Furthermore%2C%20it%20employs%20a%20gradient-guided%20prompt%0Aselection%20strategy%20to%20compress%20less%20informative%20prompts%20into%20a%20single%0Aaggregated%20representation%2C%20ensuring%20scalable%20and%20memory-efficient%20continual%0Alearning.%20Extensive%20experiments%20on%20long-sequence%20and%20negative%20transfer%0Abenchmarks%20show%20that%20GRID%20improves%20average%20accuracy%20and%20backward%20transfer%2C%0Aachieves%20competitive%20forward%20transfer%2C%20and%20substantially%20reduces%20prompt%20memory%0Ausage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14725v3&entry.124074799=Read"},
{"title": "Unpacking Let Alone: Human-Scale Models Generalize to a Rare\n  Construction in Form but not Meaning", "author": "Wesley Scivetti and Tatsuya Aoyama and Ethan Wilcox and Nathan Schneider", "abstract": "  Humans have a remarkable ability to acquire and understand grammatical\nphenomena that are seen rarely, if ever, during childhood. Recent evidence\nsuggests that language models with human-scale pretraining data may possess a\nsimilar ability by generalizing from frequent to rare constructions. However,\nit remains an open question how widespread this generalization ability is, and\nto what extent this knowledge extends to meanings of rare constructions, as\nopposed to just their forms. We fill this gap by testing human-scale\ntransformer language models on their knowledge of both the form and meaning of\nthe (rare and quirky) English LET-ALONE construction. To evaluate our LMs we\nconstruct a bespoke synthetic benchmark that targets syntactic and semantic\nproperties of the construction. We find that human-scale LMs are sensitive to\nform, even when related constructions are filtered from the dataset. However,\nhuman-scale LMs do not make correct generalizations about LET-ALONE's meaning.\nThese results point to an asymmetry in the current architectures' sample\nefficiency between language form and meaning, something which is not present in\nhuman language learners.\n", "link": "http://arxiv.org/abs/2506.04408v2", "date": "2025-10-01", "relevancy": 2.6023, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unpacking%20Let%20Alone%3A%20Human-Scale%20Models%20Generalize%20to%20a%20Rare%0A%20%20Construction%20in%20Form%20but%20not%20Meaning&body=Title%3A%20Unpacking%20Let%20Alone%3A%20Human-Scale%20Models%20Generalize%20to%20a%20Rare%0A%20%20Construction%20in%20Form%20but%20not%20Meaning%0AAuthor%3A%20Wesley%20Scivetti%20and%20Tatsuya%20Aoyama%20and%20Ethan%20Wilcox%20and%20Nathan%20Schneider%0AAbstract%3A%20%20%20Humans%20have%20a%20remarkable%20ability%20to%20acquire%20and%20understand%20grammatical%0Aphenomena%20that%20are%20seen%20rarely%2C%20if%20ever%2C%20during%20childhood.%20Recent%20evidence%0Asuggests%20that%20language%20models%20with%20human-scale%20pretraining%20data%20may%20possess%20a%0Asimilar%20ability%20by%20generalizing%20from%20frequent%20to%20rare%20constructions.%20However%2C%0Ait%20remains%20an%20open%20question%20how%20widespread%20this%20generalization%20ability%20is%2C%20and%0Ato%20what%20extent%20this%20knowledge%20extends%20to%20meanings%20of%20rare%20constructions%2C%20as%0Aopposed%20to%20just%20their%20forms.%20We%20fill%20this%20gap%20by%20testing%20human-scale%0Atransformer%20language%20models%20on%20their%20knowledge%20of%20both%20the%20form%20and%20meaning%20of%0Athe%20%28rare%20and%20quirky%29%20English%20LET-ALONE%20construction.%20To%20evaluate%20our%20LMs%20we%0Aconstruct%20a%20bespoke%20synthetic%20benchmark%20that%20targets%20syntactic%20and%20semantic%0Aproperties%20of%20the%20construction.%20We%20find%20that%20human-scale%20LMs%20are%20sensitive%20to%0Aform%2C%20even%20when%20related%20constructions%20are%20filtered%20from%20the%20dataset.%20However%2C%0Ahuman-scale%20LMs%20do%20not%20make%20correct%20generalizations%20about%20LET-ALONE%27s%20meaning.%0AThese%20results%20point%20to%20an%20asymmetry%20in%20the%20current%20architectures%27%20sample%0Aefficiency%20between%20language%20form%20and%20meaning%2C%20something%20which%20is%20not%20present%20in%0Ahuman%20language%20learners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnpacking%2520Let%2520Alone%253A%2520Human-Scale%2520Models%2520Generalize%2520to%2520a%2520Rare%250A%2520%2520Construction%2520in%2520Form%2520but%2520not%2520Meaning%26entry.906535625%3DWesley%2520Scivetti%2520and%2520Tatsuya%2520Aoyama%2520and%2520Ethan%2520Wilcox%2520and%2520Nathan%2520Schneider%26entry.1292438233%3D%2520%2520Humans%2520have%2520a%2520remarkable%2520ability%2520to%2520acquire%2520and%2520understand%2520grammatical%250Aphenomena%2520that%2520are%2520seen%2520rarely%252C%2520if%2520ever%252C%2520during%2520childhood.%2520Recent%2520evidence%250Asuggests%2520that%2520language%2520models%2520with%2520human-scale%2520pretraining%2520data%2520may%2520possess%2520a%250Asimilar%2520ability%2520by%2520generalizing%2520from%2520frequent%2520to%2520rare%2520constructions.%2520However%252C%250Ait%2520remains%2520an%2520open%2520question%2520how%2520widespread%2520this%2520generalization%2520ability%2520is%252C%2520and%250Ato%2520what%2520extent%2520this%2520knowledge%2520extends%2520to%2520meanings%2520of%2520rare%2520constructions%252C%2520as%250Aopposed%2520to%2520just%2520their%2520forms.%2520We%2520fill%2520this%2520gap%2520by%2520testing%2520human-scale%250Atransformer%2520language%2520models%2520on%2520their%2520knowledge%2520of%2520both%2520the%2520form%2520and%2520meaning%2520of%250Athe%2520%2528rare%2520and%2520quirky%2529%2520English%2520LET-ALONE%2520construction.%2520To%2520evaluate%2520our%2520LMs%2520we%250Aconstruct%2520a%2520bespoke%2520synthetic%2520benchmark%2520that%2520targets%2520syntactic%2520and%2520semantic%250Aproperties%2520of%2520the%2520construction.%2520We%2520find%2520that%2520human-scale%2520LMs%2520are%2520sensitive%2520to%250Aform%252C%2520even%2520when%2520related%2520constructions%2520are%2520filtered%2520from%2520the%2520dataset.%2520However%252C%250Ahuman-scale%2520LMs%2520do%2520not%2520make%2520correct%2520generalizations%2520about%2520LET-ALONE%2527s%2520meaning.%250AThese%2520results%2520point%2520to%2520an%2520asymmetry%2520in%2520the%2520current%2520architectures%2527%2520sample%250Aefficiency%2520between%2520language%2520form%2520and%2520meaning%252C%2520something%2520which%2520is%2520not%2520present%2520in%250Ahuman%2520language%2520learners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpacking%20Let%20Alone%3A%20Human-Scale%20Models%20Generalize%20to%20a%20Rare%0A%20%20Construction%20in%20Form%20but%20not%20Meaning&entry.906535625=Wesley%20Scivetti%20and%20Tatsuya%20Aoyama%20and%20Ethan%20Wilcox%20and%20Nathan%20Schneider&entry.1292438233=%20%20Humans%20have%20a%20remarkable%20ability%20to%20acquire%20and%20understand%20grammatical%0Aphenomena%20that%20are%20seen%20rarely%2C%20if%20ever%2C%20during%20childhood.%20Recent%20evidence%0Asuggests%20that%20language%20models%20with%20human-scale%20pretraining%20data%20may%20possess%20a%0Asimilar%20ability%20by%20generalizing%20from%20frequent%20to%20rare%20constructions.%20However%2C%0Ait%20remains%20an%20open%20question%20how%20widespread%20this%20generalization%20ability%20is%2C%20and%0Ato%20what%20extent%20this%20knowledge%20extends%20to%20meanings%20of%20rare%20constructions%2C%20as%0Aopposed%20to%20just%20their%20forms.%20We%20fill%20this%20gap%20by%20testing%20human-scale%0Atransformer%20language%20models%20on%20their%20knowledge%20of%20both%20the%20form%20and%20meaning%20of%0Athe%20%28rare%20and%20quirky%29%20English%20LET-ALONE%20construction.%20To%20evaluate%20our%20LMs%20we%0Aconstruct%20a%20bespoke%20synthetic%20benchmark%20that%20targets%20syntactic%20and%20semantic%0Aproperties%20of%20the%20construction.%20We%20find%20that%20human-scale%20LMs%20are%20sensitive%20to%0Aform%2C%20even%20when%20related%20constructions%20are%20filtered%20from%20the%20dataset.%20However%2C%0Ahuman-scale%20LMs%20do%20not%20make%20correct%20generalizations%20about%20LET-ALONE%27s%20meaning.%0AThese%20results%20point%20to%20an%20asymmetry%20in%20the%20current%20architectures%27%20sample%0Aefficiency%20between%20language%20form%20and%20meaning%2C%20something%20which%20is%20not%20present%20in%0Ahuman%20language%20learners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04408v2&entry.124074799=Read"},
{"title": "Mitigating Domain Shift in Federated Learning via Intra- and\n  Inter-Domain Prototypes", "author": "Huy Q. Le and Ye Lin Tun and Yu Qiao and Minh N. H. Nguyen and Keon Oh Kim and Eui-Nam Huh and Choong Seon Hong", "abstract": "  Federated Learning (FL) has emerged as a decentralized machine learning\ntechnique, allowing clients to train a global model collaboratively without\nsharing private data. However, most FL studies ignore the crucial challenge of\nheterogeneous domains where each client has a distinct feature distribution,\nwhich is popular in real-world scenarios. Prototype learning, which leverages\nthe mean feature vectors within the same classes, has become a prominent\nsolution for federated learning under domain shift. However, existing federated\nprototype learning methods focus soley on inter-domain prototypes and neglect\nintra-domain perspectives. In this work, we introduce a novel federated\nprototype learning method, namely I$^2$PFL, which incorporates\n$\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to\nmitigate domain shift from both perspectives and learn a generalized global\nmodel across multiple domains in federated learning. To construct intra-domain\nprototypes, we propose feature alignment with MixUp-based augmented prototypes\nto capture the diversity within local domains and enhance the generalization of\nlocal features. Additionally, we introduce a reweighting mechanism for\ninter-domain prototypes to generate generalized prototypes that reduce domain\nshift while providing inter-domain knowledge across multiple clients. Extensive\nexperiments on the Digits, Office-10, and PACS datasets illustrate the superior\nperformance of our method compared to other baselines.\n", "link": "http://arxiv.org/abs/2501.08521v3", "date": "2025-10-01", "relevancy": 2.5343, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Domain%20Shift%20in%20Federated%20Learning%20via%20Intra-%20and%0A%20%20Inter-Domain%20Prototypes&body=Title%3A%20Mitigating%20Domain%20Shift%20in%20Federated%20Learning%20via%20Intra-%20and%0A%20%20Inter-Domain%20Prototypes%0AAuthor%3A%20Huy%20Q.%20Le%20and%20Ye%20Lin%20Tun%20and%20Yu%20Qiao%20and%20Minh%20N.%20H.%20Nguyen%20and%20Keon%20Oh%20Kim%20and%20Eui-Nam%20Huh%20and%20Choong%20Seon%20Hong%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20decentralized%20machine%20learning%0Atechnique%2C%20allowing%20clients%20to%20train%20a%20global%20model%20collaboratively%20without%0Asharing%20private%20data.%20However%2C%20most%20FL%20studies%20ignore%20the%20crucial%20challenge%20of%0Aheterogeneous%20domains%20where%20each%20client%20has%20a%20distinct%20feature%20distribution%2C%0Awhich%20is%20popular%20in%20real-world%20scenarios.%20Prototype%20learning%2C%20which%20leverages%0Athe%20mean%20feature%20vectors%20within%20the%20same%20classes%2C%20has%20become%20a%20prominent%0Asolution%20for%20federated%20learning%20under%20domain%20shift.%20However%2C%20existing%20federated%0Aprototype%20learning%20methods%20focus%20soley%20on%20inter-domain%20prototypes%20and%20neglect%0Aintra-domain%20perspectives.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20federated%0Aprototype%20learning%20method%2C%20namely%20I%24%5E2%24PFL%2C%20which%20incorporates%0A%24%5Ctextbf%7BI%7D%24ntra-domain%20and%20%24%5Ctextbf%7BI%7D%24nter-domain%20%24%5Ctextbf%7BP%7D%24rototypes%2C%20to%0Amitigate%20domain%20shift%20from%20both%20perspectives%20and%20learn%20a%20generalized%20global%0Amodel%20across%20multiple%20domains%20in%20federated%20learning.%20To%20construct%20intra-domain%0Aprototypes%2C%20we%20propose%20feature%20alignment%20with%20MixUp-based%20augmented%20prototypes%0Ato%20capture%20the%20diversity%20within%20local%20domains%20and%20enhance%20the%20generalization%20of%0Alocal%20features.%20Additionally%2C%20we%20introduce%20a%20reweighting%20mechanism%20for%0Ainter-domain%20prototypes%20to%20generate%20generalized%20prototypes%20that%20reduce%20domain%0Ashift%20while%20providing%20inter-domain%20knowledge%20across%20multiple%20clients.%20Extensive%0Aexperiments%20on%20the%20Digits%2C%20Office-10%2C%20and%20PACS%20datasets%20illustrate%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20other%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08521v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Domain%2520Shift%2520in%2520Federated%2520Learning%2520via%2520Intra-%2520and%250A%2520%2520Inter-Domain%2520Prototypes%26entry.906535625%3DHuy%2520Q.%2520Le%2520and%2520Ye%2520Lin%2520Tun%2520and%2520Yu%2520Qiao%2520and%2520Minh%2520N.%2520H.%2520Nguyen%2520and%2520Keon%2520Oh%2520Kim%2520and%2520Eui-Nam%2520Huh%2520and%2520Choong%2520Seon%2520Hong%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520decentralized%2520machine%2520learning%250Atechnique%252C%2520allowing%2520clients%2520to%2520train%2520a%2520global%2520model%2520collaboratively%2520without%250Asharing%2520private%2520data.%2520However%252C%2520most%2520FL%2520studies%2520ignore%2520the%2520crucial%2520challenge%2520of%250Aheterogeneous%2520domains%2520where%2520each%2520client%2520has%2520a%2520distinct%2520feature%2520distribution%252C%250Awhich%2520is%2520popular%2520in%2520real-world%2520scenarios.%2520Prototype%2520learning%252C%2520which%2520leverages%250Athe%2520mean%2520feature%2520vectors%2520within%2520the%2520same%2520classes%252C%2520has%2520become%2520a%2520prominent%250Asolution%2520for%2520federated%2520learning%2520under%2520domain%2520shift.%2520However%252C%2520existing%2520federated%250Aprototype%2520learning%2520methods%2520focus%2520soley%2520on%2520inter-domain%2520prototypes%2520and%2520neglect%250Aintra-domain%2520perspectives.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520federated%250Aprototype%2520learning%2520method%252C%2520namely%2520I%2524%255E2%2524PFL%252C%2520which%2520incorporates%250A%2524%255Ctextbf%257BI%257D%2524ntra-domain%2520and%2520%2524%255Ctextbf%257BI%257D%2524nter-domain%2520%2524%255Ctextbf%257BP%257D%2524rototypes%252C%2520to%250Amitigate%2520domain%2520shift%2520from%2520both%2520perspectives%2520and%2520learn%2520a%2520generalized%2520global%250Amodel%2520across%2520multiple%2520domains%2520in%2520federated%2520learning.%2520To%2520construct%2520intra-domain%250Aprototypes%252C%2520we%2520propose%2520feature%2520alignment%2520with%2520MixUp-based%2520augmented%2520prototypes%250Ato%2520capture%2520the%2520diversity%2520within%2520local%2520domains%2520and%2520enhance%2520the%2520generalization%2520of%250Alocal%2520features.%2520Additionally%252C%2520we%2520introduce%2520a%2520reweighting%2520mechanism%2520for%250Ainter-domain%2520prototypes%2520to%2520generate%2520generalized%2520prototypes%2520that%2520reduce%2520domain%250Ashift%2520while%2520providing%2520inter-domain%2520knowledge%2520across%2520multiple%2520clients.%2520Extensive%250Aexperiments%2520on%2520the%2520Digits%252C%2520Office-10%252C%2520and%2520PACS%2520datasets%2520illustrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520compared%2520to%2520other%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08521v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Domain%20Shift%20in%20Federated%20Learning%20via%20Intra-%20and%0A%20%20Inter-Domain%20Prototypes&entry.906535625=Huy%20Q.%20Le%20and%20Ye%20Lin%20Tun%20and%20Yu%20Qiao%20and%20Minh%20N.%20H.%20Nguyen%20and%20Keon%20Oh%20Kim%20and%20Eui-Nam%20Huh%20and%20Choong%20Seon%20Hong&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20decentralized%20machine%20learning%0Atechnique%2C%20allowing%20clients%20to%20train%20a%20global%20model%20collaboratively%20without%0Asharing%20private%20data.%20However%2C%20most%20FL%20studies%20ignore%20the%20crucial%20challenge%20of%0Aheterogeneous%20domains%20where%20each%20client%20has%20a%20distinct%20feature%20distribution%2C%0Awhich%20is%20popular%20in%20real-world%20scenarios.%20Prototype%20learning%2C%20which%20leverages%0Athe%20mean%20feature%20vectors%20within%20the%20same%20classes%2C%20has%20become%20a%20prominent%0Asolution%20for%20federated%20learning%20under%20domain%20shift.%20However%2C%20existing%20federated%0Aprototype%20learning%20methods%20focus%20soley%20on%20inter-domain%20prototypes%20and%20neglect%0Aintra-domain%20perspectives.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20federated%0Aprototype%20learning%20method%2C%20namely%20I%24%5E2%24PFL%2C%20which%20incorporates%0A%24%5Ctextbf%7BI%7D%24ntra-domain%20and%20%24%5Ctextbf%7BI%7D%24nter-domain%20%24%5Ctextbf%7BP%7D%24rototypes%2C%20to%0Amitigate%20domain%20shift%20from%20both%20perspectives%20and%20learn%20a%20generalized%20global%0Amodel%20across%20multiple%20domains%20in%20federated%20learning.%20To%20construct%20intra-domain%0Aprototypes%2C%20we%20propose%20feature%20alignment%20with%20MixUp-based%20augmented%20prototypes%0Ato%20capture%20the%20diversity%20within%20local%20domains%20and%20enhance%20the%20generalization%20of%0Alocal%20features.%20Additionally%2C%20we%20introduce%20a%20reweighting%20mechanism%20for%0Ainter-domain%20prototypes%20to%20generate%20generalized%20prototypes%20that%20reduce%20domain%0Ashift%20while%20providing%20inter-domain%20knowledge%20across%20multiple%20clients.%20Extensive%0Aexperiments%20on%20the%20Digits%2C%20Office-10%2C%20and%20PACS%20datasets%20illustrate%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20other%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08521v3&entry.124074799=Read"},
{"title": "Stability Bounds for the Unfolded Forward-Backward Algorithm", "author": "Emilie Chouzenoux and Cecile Della Valle and Jean-Christophe Pesquet", "abstract": "  We consider a neural network architecture designed to solve inverse problems\nwhere the degradation operator is linear and known. This architecture is\nconstructed by unrolling a forward-backward algorithm derived from the\nminimization of an objective function that combines a data-fidelity term, a\nTikhonov-type regularization term, and a potentially nonsmooth convex penalty.\nThe robustness of this inversion method to input perturbations is analyzed\ntheoretically. Ensuring robustness complies with the principles of inverse\nproblem theory, as it ensures both the continuity of the inversion method and\nthe resilience to small noise - a critical property given the known\nvulnerability of deep neural networks to adversarial perturbations. A key\nnovelty of our work lies in examining the robustness of the proposed network to\nperturbations in its bias, which represents the observed data in the inverse\nproblem. Additionally, we provide numerical illustrations of the analytical\nLipschitz bounds derived in our analysis.\n", "link": "http://arxiv.org/abs/2412.17888v2", "date": "2025-10-01", "relevancy": 2.4684, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4927}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20Bounds%20for%20the%20Unfolded%20Forward-Backward%20Algorithm&body=Title%3A%20Stability%20Bounds%20for%20the%20Unfolded%20Forward-Backward%20Algorithm%0AAuthor%3A%20Emilie%20Chouzenoux%20and%20Cecile%20Della%20Valle%20and%20Jean-Christophe%20Pesquet%0AAbstract%3A%20%20%20We%20consider%20a%20neural%20network%20architecture%20designed%20to%20solve%20inverse%20problems%0Awhere%20the%20degradation%20operator%20is%20linear%20and%20known.%20This%20architecture%20is%0Aconstructed%20by%20unrolling%20a%20forward-backward%20algorithm%20derived%20from%20the%0Aminimization%20of%20an%20objective%20function%20that%20combines%20a%20data-fidelity%20term%2C%20a%0ATikhonov-type%20regularization%20term%2C%20and%20a%20potentially%20nonsmooth%20convex%20penalty.%0AThe%20robustness%20of%20this%20inversion%20method%20to%20input%20perturbations%20is%20analyzed%0Atheoretically.%20Ensuring%20robustness%20complies%20with%20the%20principles%20of%20inverse%0Aproblem%20theory%2C%20as%20it%20ensures%20both%20the%20continuity%20of%20the%20inversion%20method%20and%0Athe%20resilience%20to%20small%20noise%20-%20a%20critical%20property%20given%20the%20known%0Avulnerability%20of%20deep%20neural%20networks%20to%20adversarial%20perturbations.%20A%20key%0Anovelty%20of%20our%20work%20lies%20in%20examining%20the%20robustness%20of%20the%20proposed%20network%20to%0Aperturbations%20in%20its%20bias%2C%20which%20represents%20the%20observed%20data%20in%20the%20inverse%0Aproblem.%20Additionally%2C%20we%20provide%20numerical%20illustrations%20of%20the%20analytical%0ALipschitz%20bounds%20derived%20in%20our%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520Bounds%2520for%2520the%2520Unfolded%2520Forward-Backward%2520Algorithm%26entry.906535625%3DEmilie%2520Chouzenoux%2520and%2520Cecile%2520Della%2520Valle%2520and%2520Jean-Christophe%2520Pesquet%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520neural%2520network%2520architecture%2520designed%2520to%2520solve%2520inverse%2520problems%250Awhere%2520the%2520degradation%2520operator%2520is%2520linear%2520and%2520known.%2520This%2520architecture%2520is%250Aconstructed%2520by%2520unrolling%2520a%2520forward-backward%2520algorithm%2520derived%2520from%2520the%250Aminimization%2520of%2520an%2520objective%2520function%2520that%2520combines%2520a%2520data-fidelity%2520term%252C%2520a%250ATikhonov-type%2520regularization%2520term%252C%2520and%2520a%2520potentially%2520nonsmooth%2520convex%2520penalty.%250AThe%2520robustness%2520of%2520this%2520inversion%2520method%2520to%2520input%2520perturbations%2520is%2520analyzed%250Atheoretically.%2520Ensuring%2520robustness%2520complies%2520with%2520the%2520principles%2520of%2520inverse%250Aproblem%2520theory%252C%2520as%2520it%2520ensures%2520both%2520the%2520continuity%2520of%2520the%2520inversion%2520method%2520and%250Athe%2520resilience%2520to%2520small%2520noise%2520-%2520a%2520critical%2520property%2520given%2520the%2520known%250Avulnerability%2520of%2520deep%2520neural%2520networks%2520to%2520adversarial%2520perturbations.%2520A%2520key%250Anovelty%2520of%2520our%2520work%2520lies%2520in%2520examining%2520the%2520robustness%2520of%2520the%2520proposed%2520network%2520to%250Aperturbations%2520in%2520its%2520bias%252C%2520which%2520represents%2520the%2520observed%2520data%2520in%2520the%2520inverse%250Aproblem.%2520Additionally%252C%2520we%2520provide%2520numerical%2520illustrations%2520of%2520the%2520analytical%250ALipschitz%2520bounds%2520derived%2520in%2520our%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20Bounds%20for%20the%20Unfolded%20Forward-Backward%20Algorithm&entry.906535625=Emilie%20Chouzenoux%20and%20Cecile%20Della%20Valle%20and%20Jean-Christophe%20Pesquet&entry.1292438233=%20%20We%20consider%20a%20neural%20network%20architecture%20designed%20to%20solve%20inverse%20problems%0Awhere%20the%20degradation%20operator%20is%20linear%20and%20known.%20This%20architecture%20is%0Aconstructed%20by%20unrolling%20a%20forward-backward%20algorithm%20derived%20from%20the%0Aminimization%20of%20an%20objective%20function%20that%20combines%20a%20data-fidelity%20term%2C%20a%0ATikhonov-type%20regularization%20term%2C%20and%20a%20potentially%20nonsmooth%20convex%20penalty.%0AThe%20robustness%20of%20this%20inversion%20method%20to%20input%20perturbations%20is%20analyzed%0Atheoretically.%20Ensuring%20robustness%20complies%20with%20the%20principles%20of%20inverse%0Aproblem%20theory%2C%20as%20it%20ensures%20both%20the%20continuity%20of%20the%20inversion%20method%20and%0Athe%20resilience%20to%20small%20noise%20-%20a%20critical%20property%20given%20the%20known%0Avulnerability%20of%20deep%20neural%20networks%20to%20adversarial%20perturbations.%20A%20key%0Anovelty%20of%20our%20work%20lies%20in%20examining%20the%20robustness%20of%20the%20proposed%20network%20to%0Aperturbations%20in%20its%20bias%2C%20which%20represents%20the%20observed%20data%20in%20the%20inverse%0Aproblem.%20Additionally%2C%20we%20provide%20numerical%20illustrations%20of%20the%20analytical%0ALipschitz%20bounds%20derived%20in%20our%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17888v2&entry.124074799=Read"},
{"title": "LoRA meets Riemannion: Muon Optimizer for Parametrization-independent\n  Low-Rank Adapters", "author": "Vladimir Bogachev and Vladimir Aletov and Alexander Molozhavenko and Denis Bobkov and Vera Soboleva and Aibek Alanov and Maxim Rakhuba", "abstract": "  This work presents a novel, fully Riemannian framework for Low-Rank\nAdaptation (LoRA) that geometrically treats low-rank adapters by optimizing\nthem directly on the fixed-rank manifold. This formulation eliminates the\nparametrization ambiguity present in standard Euclidean optimizers. Our\nframework integrates three key components to achieve this: (1) we derive\nRiemannion, a new Riemannian optimizer on the fixed-rank matrix manifold that\ngeneralizes the recently proposed Muon optimizer; (2) we develop a Riemannian\ngradient-informed LoRA initialization, and (3) we provide an efficient\nimplementation without prominent overhead that uses automatic differentiation\nto compute arising geometric operations while adhering to best practices in\nnumerical linear algebra. Comprehensive experimental results on both LLM and\ndiffusion model architectures demonstrate that our approach yields consistent\nand noticeable improvements in convergence speed and final task performance\nover both standard LoRA and its state-of-the-art modifications.\n", "link": "http://arxiv.org/abs/2507.12142v2", "date": "2025-10-01", "relevancy": 2.4149, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA%20meets%20Riemannion%3A%20Muon%20Optimizer%20for%20Parametrization-independent%0A%20%20Low-Rank%20Adapters&body=Title%3A%20LoRA%20meets%20Riemannion%3A%20Muon%20Optimizer%20for%20Parametrization-independent%0A%20%20Low-Rank%20Adapters%0AAuthor%3A%20Vladimir%20Bogachev%20and%20Vladimir%20Aletov%20and%20Alexander%20Molozhavenko%20and%20Denis%20Bobkov%20and%20Vera%20Soboleva%20and%20Aibek%20Alanov%20and%20Maxim%20Rakhuba%0AAbstract%3A%20%20%20This%20work%20presents%20a%20novel%2C%20fully%20Riemannian%20framework%20for%20Low-Rank%0AAdaptation%20%28LoRA%29%20that%20geometrically%20treats%20low-rank%20adapters%20by%20optimizing%0Athem%20directly%20on%20the%20fixed-rank%20manifold.%20This%20formulation%20eliminates%20the%0Aparametrization%20ambiguity%20present%20in%20standard%20Euclidean%20optimizers.%20Our%0Aframework%20integrates%20three%20key%20components%20to%20achieve%20this%3A%20%281%29%20we%20derive%0ARiemannion%2C%20a%20new%20Riemannian%20optimizer%20on%20the%20fixed-rank%20matrix%20manifold%20that%0Ageneralizes%20the%20recently%20proposed%20Muon%20optimizer%3B%20%282%29%20we%20develop%20a%20Riemannian%0Agradient-informed%20LoRA%20initialization%2C%20and%20%283%29%20we%20provide%20an%20efficient%0Aimplementation%20without%20prominent%20overhead%20that%20uses%20automatic%20differentiation%0Ato%20compute%20arising%20geometric%20operations%20while%20adhering%20to%20best%20practices%20in%0Anumerical%20linear%20algebra.%20Comprehensive%20experimental%20results%20on%20both%20LLM%20and%0Adiffusion%20model%20architectures%20demonstrate%20that%20our%20approach%20yields%20consistent%0Aand%20noticeable%20improvements%20in%20convergence%20speed%20and%20final%20task%20performance%0Aover%20both%20standard%20LoRA%20and%20its%20state-of-the-art%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA%2520meets%2520Riemannion%253A%2520Muon%2520Optimizer%2520for%2520Parametrization-independent%250A%2520%2520Low-Rank%2520Adapters%26entry.906535625%3DVladimir%2520Bogachev%2520and%2520Vladimir%2520Aletov%2520and%2520Alexander%2520Molozhavenko%2520and%2520Denis%2520Bobkov%2520and%2520Vera%2520Soboleva%2520and%2520Aibek%2520Alanov%2520and%2520Maxim%2520Rakhuba%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520novel%252C%2520fully%2520Riemannian%2520framework%2520for%2520Low-Rank%250AAdaptation%2520%2528LoRA%2529%2520that%2520geometrically%2520treats%2520low-rank%2520adapters%2520by%2520optimizing%250Athem%2520directly%2520on%2520the%2520fixed-rank%2520manifold.%2520This%2520formulation%2520eliminates%2520the%250Aparametrization%2520ambiguity%2520present%2520in%2520standard%2520Euclidean%2520optimizers.%2520Our%250Aframework%2520integrates%2520three%2520key%2520components%2520to%2520achieve%2520this%253A%2520%25281%2529%2520we%2520derive%250ARiemannion%252C%2520a%2520new%2520Riemannian%2520optimizer%2520on%2520the%2520fixed-rank%2520matrix%2520manifold%2520that%250Ageneralizes%2520the%2520recently%2520proposed%2520Muon%2520optimizer%253B%2520%25282%2529%2520we%2520develop%2520a%2520Riemannian%250Agradient-informed%2520LoRA%2520initialization%252C%2520and%2520%25283%2529%2520we%2520provide%2520an%2520efficient%250Aimplementation%2520without%2520prominent%2520overhead%2520that%2520uses%2520automatic%2520differentiation%250Ato%2520compute%2520arising%2520geometric%2520operations%2520while%2520adhering%2520to%2520best%2520practices%2520in%250Anumerical%2520linear%2520algebra.%2520Comprehensive%2520experimental%2520results%2520on%2520both%2520LLM%2520and%250Adiffusion%2520model%2520architectures%2520demonstrate%2520that%2520our%2520approach%2520yields%2520consistent%250Aand%2520noticeable%2520improvements%2520in%2520convergence%2520speed%2520and%2520final%2520task%2520performance%250Aover%2520both%2520standard%2520LoRA%2520and%2520its%2520state-of-the-art%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA%20meets%20Riemannion%3A%20Muon%20Optimizer%20for%20Parametrization-independent%0A%20%20Low-Rank%20Adapters&entry.906535625=Vladimir%20Bogachev%20and%20Vladimir%20Aletov%20and%20Alexander%20Molozhavenko%20and%20Denis%20Bobkov%20and%20Vera%20Soboleva%20and%20Aibek%20Alanov%20and%20Maxim%20Rakhuba&entry.1292438233=%20%20This%20work%20presents%20a%20novel%2C%20fully%20Riemannian%20framework%20for%20Low-Rank%0AAdaptation%20%28LoRA%29%20that%20geometrically%20treats%20low-rank%20adapters%20by%20optimizing%0Athem%20directly%20on%20the%20fixed-rank%20manifold.%20This%20formulation%20eliminates%20the%0Aparametrization%20ambiguity%20present%20in%20standard%20Euclidean%20optimizers.%20Our%0Aframework%20integrates%20three%20key%20components%20to%20achieve%20this%3A%20%281%29%20we%20derive%0ARiemannion%2C%20a%20new%20Riemannian%20optimizer%20on%20the%20fixed-rank%20matrix%20manifold%20that%0Ageneralizes%20the%20recently%20proposed%20Muon%20optimizer%3B%20%282%29%20we%20develop%20a%20Riemannian%0Agradient-informed%20LoRA%20initialization%2C%20and%20%283%29%20we%20provide%20an%20efficient%0Aimplementation%20without%20prominent%20overhead%20that%20uses%20automatic%20differentiation%0Ato%20compute%20arising%20geometric%20operations%20while%20adhering%20to%20best%20practices%20in%0Anumerical%20linear%20algebra.%20Comprehensive%20experimental%20results%20on%20both%20LLM%20and%0Adiffusion%20model%20architectures%20demonstrate%20that%20our%20approach%20yields%20consistent%0Aand%20noticeable%20improvements%20in%20convergence%20speed%20and%20final%20task%20performance%0Aover%20both%20standard%20LoRA%20and%20its%20state-of-the-art%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12142v2&entry.124074799=Read"},
{"title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?", "author": "Ouxiang Li and Yuan Wang and Xinting Hu and Huijuan Huang and Rui Chen and Jiarong Ou and Xin Tao and Pengfei Wan and Xiaojuan Qi and Fuli Feng", "abstract": "  Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, which thus correspond to two core capabilities: composition and\nreasoning. Despite recent advances of T2I models in both composition and\nreasoning, existing benchmarks remain limited in evaluation. They not only fail\nto provide comprehensive coverage across and within both capabilities, but also\nlargely restrict evaluation to low scene density and simple one-to-one\nreasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent real-world complexities, we curate\neach prompt with higher compositional density for composition and greater\nreasoning intensity for reasoning. To facilitate fine-grained and reliable\nevaluation, we also pair each evaluation prompt with a checklist that specifies\nindividual yes/no questions to assess each intended element independently. In\nstatistics, our benchmark comprises 1,080 challenging prompts and around 13,500\nchecklist questions. Experiments across 28 current T2I models reveal that their\ncomposition capability still remains limited in high compositional scenarios,\nwhile the reasoning capability lags even further behind as a critical\nbottleneck, with all models struggling to infer implicit elements from prompts.\n", "link": "http://arxiv.org/abs/2509.03516v2", "date": "2025-10-01", "relevancy": 2.3374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5872}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F&body=Title%3A%20Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F%0AAuthor%3A%20Ouxiang%20Li%20and%20Yuan%20Wang%20and%20Xinting%20Hu%20and%20Huijuan%20Huang%20and%20Rui%20Chen%20and%20Jiarong%20Ou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Xiaojuan%20Qi%20and%20Fuli%20Feng%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20aims%20to%20synthesize%20images%20from%20textual%0Aprompts%2C%20which%20jointly%20specify%20what%20must%20be%20shown%20and%20imply%20what%20can%20be%0Ainferred%2C%20which%20thus%20correspond%20to%20two%20core%20capabilities%3A%20composition%20and%0Areasoning.%20Despite%20recent%20advances%20of%20T2I%20models%20in%20both%20composition%20and%0Areasoning%2C%20existing%20benchmarks%20remain%20limited%20in%20evaluation.%20They%20not%20only%20fail%0Ato%20provide%20comprehensive%20coverage%20across%20and%20within%20both%20capabilities%2C%20but%20also%0Alargely%20restrict%20evaluation%20to%20low%20scene%20density%20and%20simple%20one-to-one%0Areasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20T2I-CoReBench%2C%20a%0Acomprehensive%20and%20complex%20benchmark%20that%20evaluates%20both%20composition%20and%0Areasoning%20capabilities%20of%20T2I%20models.%20To%20ensure%20comprehensiveness%2C%20we%20structure%0Acomposition%20around%20scene%20graph%20elements%20%28instance%2C%20attribute%2C%20and%20relation%29%20and%0Areasoning%20around%20the%20philosophical%20framework%20of%20inference%20%28deductive%2C%0Ainductive%2C%20and%20abductive%29%2C%20formulating%20a%2012-dimensional%20evaluation%20taxonomy.%20To%0Aincrease%20complexity%2C%20driven%20by%20the%20inherent%20real-world%20complexities%2C%20we%20curate%0Aeach%20prompt%20with%20higher%20compositional%20density%20for%20composition%20and%20greater%0Areasoning%20intensity%20for%20reasoning.%20To%20facilitate%20fine-grained%20and%20reliable%0Aevaluation%2C%20we%20also%20pair%20each%20evaluation%20prompt%20with%20a%20checklist%20that%20specifies%0Aindividual%20yes/no%20questions%20to%20assess%20each%20intended%20element%20independently.%20In%0Astatistics%2C%20our%20benchmark%20comprises%201%2C080%20challenging%20prompts%20and%20around%2013%2C500%0Achecklist%20questions.%20Experiments%20across%2028%20current%20T2I%20models%20reveal%20that%20their%0Acomposition%20capability%20still%20remains%20limited%20in%20high%20compositional%20scenarios%2C%0Awhile%20the%20reasoning%20capability%20lags%20even%20further%20behind%20as%20a%20critical%0Abottleneck%2C%20with%20all%20models%20struggling%20to%20infer%20implicit%20elements%20from%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasier%2520Painting%2520Than%2520Thinking%253A%2520Can%2520Text-to-Image%2520Models%2520Set%2520the%2520Stage%252C%250A%2520%2520but%2520Not%2520Direct%2520the%2520Play%253F%26entry.906535625%3DOuxiang%2520Li%2520and%2520Yuan%2520Wang%2520and%2520Xinting%2520Hu%2520and%2520Huijuan%2520Huang%2520and%2520Rui%2520Chen%2520and%2520Jiarong%2520Ou%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Xiaojuan%2520Qi%2520and%2520Fuli%2520Feng%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520aims%2520to%2520synthesize%2520images%2520from%2520textual%250Aprompts%252C%2520which%2520jointly%2520specify%2520what%2520must%2520be%2520shown%2520and%2520imply%2520what%2520can%2520be%250Ainferred%252C%2520which%2520thus%2520correspond%2520to%2520two%2520core%2520capabilities%253A%2520composition%2520and%250Areasoning.%2520Despite%2520recent%2520advances%2520of%2520T2I%2520models%2520in%2520both%2520composition%2520and%250Areasoning%252C%2520existing%2520benchmarks%2520remain%2520limited%2520in%2520evaluation.%2520They%2520not%2520only%2520fail%250Ato%2520provide%2520comprehensive%2520coverage%2520across%2520and%2520within%2520both%2520capabilities%252C%2520but%2520also%250Alargely%2520restrict%2520evaluation%2520to%2520low%2520scene%2520density%2520and%2520simple%2520one-to-one%250Areasoning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520T2I-CoReBench%252C%2520a%250Acomprehensive%2520and%2520complex%2520benchmark%2520that%2520evaluates%2520both%2520composition%2520and%250Areasoning%2520capabilities%2520of%2520T2I%2520models.%2520To%2520ensure%2520comprehensiveness%252C%2520we%2520structure%250Acomposition%2520around%2520scene%2520graph%2520elements%2520%2528instance%252C%2520attribute%252C%2520and%2520relation%2529%2520and%250Areasoning%2520around%2520the%2520philosophical%2520framework%2520of%2520inference%2520%2528deductive%252C%250Ainductive%252C%2520and%2520abductive%2529%252C%2520formulating%2520a%252012-dimensional%2520evaluation%2520taxonomy.%2520To%250Aincrease%2520complexity%252C%2520driven%2520by%2520the%2520inherent%2520real-world%2520complexities%252C%2520we%2520curate%250Aeach%2520prompt%2520with%2520higher%2520compositional%2520density%2520for%2520composition%2520and%2520greater%250Areasoning%2520intensity%2520for%2520reasoning.%2520To%2520facilitate%2520fine-grained%2520and%2520reliable%250Aevaluation%252C%2520we%2520also%2520pair%2520each%2520evaluation%2520prompt%2520with%2520a%2520checklist%2520that%2520specifies%250Aindividual%2520yes/no%2520questions%2520to%2520assess%2520each%2520intended%2520element%2520independently.%2520In%250Astatistics%252C%2520our%2520benchmark%2520comprises%25201%252C080%2520challenging%2520prompts%2520and%2520around%252013%252C500%250Achecklist%2520questions.%2520Experiments%2520across%252028%2520current%2520T2I%2520models%2520reveal%2520that%2520their%250Acomposition%2520capability%2520still%2520remains%2520limited%2520in%2520high%2520compositional%2520scenarios%252C%250Awhile%2520the%2520reasoning%2520capability%2520lags%2520even%2520further%2520behind%2520as%2520a%2520critical%250Abottleneck%252C%2520with%2520all%2520models%2520struggling%2520to%2520infer%2520implicit%2520elements%2520from%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Easier%20Painting%20Than%20Thinking%3A%20Can%20Text-to-Image%20Models%20Set%20the%20Stage%2C%0A%20%20but%20Not%20Direct%20the%20Play%3F&entry.906535625=Ouxiang%20Li%20and%20Yuan%20Wang%20and%20Xinting%20Hu%20and%20Huijuan%20Huang%20and%20Rui%20Chen%20and%20Jiarong%20Ou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Xiaojuan%20Qi%20and%20Fuli%20Feng&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20aims%20to%20synthesize%20images%20from%20textual%0Aprompts%2C%20which%20jointly%20specify%20what%20must%20be%20shown%20and%20imply%20what%20can%20be%0Ainferred%2C%20which%20thus%20correspond%20to%20two%20core%20capabilities%3A%20composition%20and%0Areasoning.%20Despite%20recent%20advances%20of%20T2I%20models%20in%20both%20composition%20and%0Areasoning%2C%20existing%20benchmarks%20remain%20limited%20in%20evaluation.%20They%20not%20only%20fail%0Ato%20provide%20comprehensive%20coverage%20across%20and%20within%20both%20capabilities%2C%20but%20also%0Alargely%20restrict%20evaluation%20to%20low%20scene%20density%20and%20simple%20one-to-one%0Areasoning.%20To%20address%20these%20limitations%2C%20we%20propose%20T2I-CoReBench%2C%20a%0Acomprehensive%20and%20complex%20benchmark%20that%20evaluates%20both%20composition%20and%0Areasoning%20capabilities%20of%20T2I%20models.%20To%20ensure%20comprehensiveness%2C%20we%20structure%0Acomposition%20around%20scene%20graph%20elements%20%28instance%2C%20attribute%2C%20and%20relation%29%20and%0Areasoning%20around%20the%20philosophical%20framework%20of%20inference%20%28deductive%2C%0Ainductive%2C%20and%20abductive%29%2C%20formulating%20a%2012-dimensional%20evaluation%20taxonomy.%20To%0Aincrease%20complexity%2C%20driven%20by%20the%20inherent%20real-world%20complexities%2C%20we%20curate%0Aeach%20prompt%20with%20higher%20compositional%20density%20for%20composition%20and%20greater%0Areasoning%20intensity%20for%20reasoning.%20To%20facilitate%20fine-grained%20and%20reliable%0Aevaluation%2C%20we%20also%20pair%20each%20evaluation%20prompt%20with%20a%20checklist%20that%20specifies%0Aindividual%20yes/no%20questions%20to%20assess%20each%20intended%20element%20independently.%20In%0Astatistics%2C%20our%20benchmark%20comprises%201%2C080%20challenging%20prompts%20and%20around%2013%2C500%0Achecklist%20questions.%20Experiments%20across%2028%20current%20T2I%20models%20reveal%20that%20their%0Acomposition%20capability%20still%20remains%20limited%20in%20high%20compositional%20scenarios%2C%0Awhile%20the%20reasoning%20capability%20lags%20even%20further%20behind%20as%20a%20critical%0Abottleneck%2C%20with%20all%20models%20struggling%20to%20infer%20implicit%20elements%20from%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03516v2&entry.124074799=Read"},
{"title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing", "author": "Syed Mahbubul Huq and Daniel Brito and Daniel Sikar and Chris Child and Tillman Weyde and Rajesh Mojumder", "abstract": "  This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks.\n", "link": "http://arxiv.org/abs/2509.22255v2", "date": "2025-10-01", "relevancy": 2.3366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing&body=Title%3A%20Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing%0AAuthor%3A%20Syed%20Mahbubul%20Huq%20and%20Daniel%20Brito%20and%20Daniel%20Sikar%20and%20Chris%20Child%20and%20Tillman%20Weyde%20and%20Rajesh%20Mojumder%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20evaluation%20framework%20for%20assessing%20Large%20Language%0AModels%27%20%28LLMs%29%20capabilities%20in%20combinatorial%20optimization%2C%20specifically%0Aaddressing%20the%202D%20bin-packing%20problem.%20We%20introduce%20a%20systematic%20methodology%0Athat%20combines%20LLMs%20with%20evolutionary%20algorithms%20to%20generate%20and%20refine%0Aheuristic%20solutions%20iteratively.%20Through%20comprehensive%20experiments%20comparing%0ALLM%20generated%20heuristics%20against%20traditional%20approaches%20%28Finite%20First-Fit%20and%0AHybrid%20First-Fit%29%2C%20we%20demonstrate%20that%20LLMs%20can%20produce%20more%20efficient%0Asolutions%20while%20requiring%20fewer%20computational%20resources.%20Our%20evaluation%20reveals%0Athat%20GPT-4o%20achieves%20optimal%20solutions%20within%20two%20iterations%2C%20reducing%20average%0Abin%20usage%20from%2016%20to%2015%20bins%20while%20improving%20space%20utilization%20from%200.76-0.78%0Ato%200.83.%20This%20work%20contributes%20to%20understanding%20LLM%20evaluation%20in%20specialized%0Adomains%20and%20establishes%20benchmarks%20for%20assessing%20LLM%20performance%20in%0Acombinatorial%20optimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLMs%2520for%2520Combinatorial%2520Optimization%253A%2520One-Phase%2520and%2520Two-Phase%250A%2520%2520Heuristics%2520for%25202D%2520Bin-Packing%26entry.906535625%3DSyed%2520Mahbubul%2520Huq%2520and%2520Daniel%2520Brito%2520and%2520Daniel%2520Sikar%2520and%2520Chris%2520Child%2520and%2520Tillman%2520Weyde%2520and%2520Rajesh%2520Mojumder%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520evaluation%2520framework%2520for%2520assessing%2520Large%2520Language%250AModels%2527%2520%2528LLMs%2529%2520capabilities%2520in%2520combinatorial%2520optimization%252C%2520specifically%250Aaddressing%2520the%25202D%2520bin-packing%2520problem.%2520We%2520introduce%2520a%2520systematic%2520methodology%250Athat%2520combines%2520LLMs%2520with%2520evolutionary%2520algorithms%2520to%2520generate%2520and%2520refine%250Aheuristic%2520solutions%2520iteratively.%2520Through%2520comprehensive%2520experiments%2520comparing%250ALLM%2520generated%2520heuristics%2520against%2520traditional%2520approaches%2520%2528Finite%2520First-Fit%2520and%250AHybrid%2520First-Fit%2529%252C%2520we%2520demonstrate%2520that%2520LLMs%2520can%2520produce%2520more%2520efficient%250Asolutions%2520while%2520requiring%2520fewer%2520computational%2520resources.%2520Our%2520evaluation%2520reveals%250Athat%2520GPT-4o%2520achieves%2520optimal%2520solutions%2520within%2520two%2520iterations%252C%2520reducing%2520average%250Abin%2520usage%2520from%252016%2520to%252015%2520bins%2520while%2520improving%2520space%2520utilization%2520from%25200.76-0.78%250Ato%25200.83.%2520This%2520work%2520contributes%2520to%2520understanding%2520LLM%2520evaluation%2520in%2520specialized%250Adomains%2520and%2520establishes%2520benchmarks%2520for%2520assessing%2520LLM%2520performance%2520in%250Acombinatorial%2520optimization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing&entry.906535625=Syed%20Mahbubul%20Huq%20and%20Daniel%20Brito%20and%20Daniel%20Sikar%20and%20Chris%20Child%20and%20Tillman%20Weyde%20and%20Rajesh%20Mojumder&entry.1292438233=%20%20This%20paper%20presents%20an%20evaluation%20framework%20for%20assessing%20Large%20Language%0AModels%27%20%28LLMs%29%20capabilities%20in%20combinatorial%20optimization%2C%20specifically%0Aaddressing%20the%202D%20bin-packing%20problem.%20We%20introduce%20a%20systematic%20methodology%0Athat%20combines%20LLMs%20with%20evolutionary%20algorithms%20to%20generate%20and%20refine%0Aheuristic%20solutions%20iteratively.%20Through%20comprehensive%20experiments%20comparing%0ALLM%20generated%20heuristics%20against%20traditional%20approaches%20%28Finite%20First-Fit%20and%0AHybrid%20First-Fit%29%2C%20we%20demonstrate%20that%20LLMs%20can%20produce%20more%20efficient%0Asolutions%20while%20requiring%20fewer%20computational%20resources.%20Our%20evaluation%20reveals%0Athat%20GPT-4o%20achieves%20optimal%20solutions%20within%20two%20iterations%2C%20reducing%20average%0Abin%20usage%20from%2016%20to%2015%20bins%20while%20improving%20space%20utilization%20from%200.76-0.78%0Ato%200.83.%20This%20work%20contributes%20to%20understanding%20LLM%20evaluation%20in%20specialized%0Adomains%20and%20establishes%20benchmarks%20for%20assessing%20LLM%20performance%20in%0Acombinatorial%20optimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22255v2&entry.124074799=Read"},
{"title": "PhyloLM : Inferring the Phylogeny of Large Language Models and\n  Predicting their Performances in Benchmarks", "author": "Nicolas Yax and Pierre-Yves Oudeyer and Stefano Palminteri", "abstract": "  This paper introduces PhyloLM, a method adapting phylogenetic algorithms to\nLarge Language Models (LLMs) to explore whether and how they relate to each\nother and to predict their performance characteristics. Our method calculates a\nphylogenetic distance metric based on the similarity of LLMs' output. The\nresulting metric is then used to construct dendrograms, which satisfactorily\ncapture known relationships across a set of 111 open-source and 45 closed\nmodels. Furthermore, our phylogenetic distance predicts performance in standard\nbenchmarks, thus demonstrating its functional validity and paving the way for a\ntime and cost-effective estimation of LLM capabilities. To sum up, by\ntranslating population genetic concepts to machine learning, we propose and\nvalidate a tool to evaluate LLM development, relationships and capabilities,\neven in the absence of transparent training information.\n", "link": "http://arxiv.org/abs/2404.04671v4", "date": "2025-10-01", "relevancy": 2.3137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4664}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%0A%20%20Predicting%20their%20Performances%20in%20Benchmarks&body=Title%3A%20PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%0A%20%20Predicting%20their%20Performances%20in%20Benchmarks%0AAuthor%3A%20Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri%0AAbstract%3A%20%20%20This%20paper%20introduces%20PhyloLM%2C%20a%20method%20adapting%20phylogenetic%20algorithms%20to%0ALarge%20Language%20Models%20%28LLMs%29%20to%20explore%20whether%20and%20how%20they%20relate%20to%20each%0Aother%20and%20to%20predict%20their%20performance%20characteristics.%20Our%20method%20calculates%20a%0Aphylogenetic%20distance%20metric%20based%20on%20the%20similarity%20of%20LLMs%27%20output.%20The%0Aresulting%20metric%20is%20then%20used%20to%20construct%20dendrograms%2C%20which%20satisfactorily%0Acapture%20known%20relationships%20across%20a%20set%20of%20111%20open-source%20and%2045%20closed%0Amodels.%20Furthermore%2C%20our%20phylogenetic%20distance%20predicts%20performance%20in%20standard%0Abenchmarks%2C%20thus%20demonstrating%20its%20functional%20validity%20and%20paving%20the%20way%20for%20a%0Atime%20and%20cost-effective%20estimation%20of%20LLM%20capabilities.%20To%20sum%20up%2C%20by%0Atranslating%20population%20genetic%20concepts%20to%20machine%20learning%2C%20we%20propose%20and%0Avalidate%20a%20tool%20to%20evaluate%20LLM%20development%2C%20relationships%20and%20capabilities%2C%0Aeven%20in%20the%20absence%20of%20transparent%20training%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04671v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyloLM%2520%253A%2520Inferring%2520the%2520Phylogeny%2520of%2520Large%2520Language%2520Models%2520and%250A%2520%2520Predicting%2520their%2520Performances%2520in%2520Benchmarks%26entry.906535625%3DNicolas%2520Yax%2520and%2520Pierre-Yves%2520Oudeyer%2520and%2520Stefano%2520Palminteri%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520PhyloLM%252C%2520a%2520method%2520adapting%2520phylogenetic%2520algorithms%2520to%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520explore%2520whether%2520and%2520how%2520they%2520relate%2520to%2520each%250Aother%2520and%2520to%2520predict%2520their%2520performance%2520characteristics.%2520Our%2520method%2520calculates%2520a%250Aphylogenetic%2520distance%2520metric%2520based%2520on%2520the%2520similarity%2520of%2520LLMs%2527%2520output.%2520The%250Aresulting%2520metric%2520is%2520then%2520used%2520to%2520construct%2520dendrograms%252C%2520which%2520satisfactorily%250Acapture%2520known%2520relationships%2520across%2520a%2520set%2520of%2520111%2520open-source%2520and%252045%2520closed%250Amodels.%2520Furthermore%252C%2520our%2520phylogenetic%2520distance%2520predicts%2520performance%2520in%2520standard%250Abenchmarks%252C%2520thus%2520demonstrating%2520its%2520functional%2520validity%2520and%2520paving%2520the%2520way%2520for%2520a%250Atime%2520and%2520cost-effective%2520estimation%2520of%2520LLM%2520capabilities.%2520To%2520sum%2520up%252C%2520by%250Atranslating%2520population%2520genetic%2520concepts%2520to%2520machine%2520learning%252C%2520we%2520propose%2520and%250Avalidate%2520a%2520tool%2520to%2520evaluate%2520LLM%2520development%252C%2520relationships%2520and%2520capabilities%252C%250Aeven%2520in%2520the%2520absence%2520of%2520transparent%2520training%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04671v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%0A%20%20Predicting%20their%20Performances%20in%20Benchmarks&entry.906535625=Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri&entry.1292438233=%20%20This%20paper%20introduces%20PhyloLM%2C%20a%20method%20adapting%20phylogenetic%20algorithms%20to%0ALarge%20Language%20Models%20%28LLMs%29%20to%20explore%20whether%20and%20how%20they%20relate%20to%20each%0Aother%20and%20to%20predict%20their%20performance%20characteristics.%20Our%20method%20calculates%20a%0Aphylogenetic%20distance%20metric%20based%20on%20the%20similarity%20of%20LLMs%27%20output.%20The%0Aresulting%20metric%20is%20then%20used%20to%20construct%20dendrograms%2C%20which%20satisfactorily%0Acapture%20known%20relationships%20across%20a%20set%20of%20111%20open-source%20and%2045%20closed%0Amodels.%20Furthermore%2C%20our%20phylogenetic%20distance%20predicts%20performance%20in%20standard%0Abenchmarks%2C%20thus%20demonstrating%20its%20functional%20validity%20and%20paving%20the%20way%20for%20a%0Atime%20and%20cost-effective%20estimation%20of%20LLM%20capabilities.%20To%20sum%20up%2C%20by%0Atranslating%20population%20genetic%20concepts%20to%20machine%20learning%2C%20we%20propose%20and%0Avalidate%20a%20tool%20to%20evaluate%20LLM%20development%2C%20relationships%20and%20capabilities%2C%0Aeven%20in%20the%20absence%20of%20transparent%20training%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04671v4&entry.124074799=Read"},
{"title": "SpargeAttention: Accurate and Training-free Sparse Attention\n  Accelerating Any Model Inference", "author": "Jintao Zhang and Chendong Xiang and Haofeng Huang and Jia Wei and Haocheng Xi and Jun Zhu and Jianfei Chen", "abstract": "  An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.\n", "link": "http://arxiv.org/abs/2502.18137v7", "date": "2025-10-01", "relevancy": 2.3051, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6319}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.538}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%0A%20%20Accelerating%20Any%20Model%20Inference&body=Title%3A%20SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%0A%20%20Accelerating%20Any%20Model%20Inference%0AAuthor%3A%20Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%0Aits%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%0Asparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%0Athe%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%0Asparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%0Aoptimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%0Apatterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%0Aboth%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%0AIn%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%0Aattention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%0Afirst%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%0Askip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%0Adesign%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%0Askips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%0Asignificantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%0Ageneration%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SpargeAttn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18137v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpargeAttention%253A%2520Accurate%2520and%2520Training-free%2520Sparse%2520Attention%250A%2520%2520Accelerating%2520Any%2520Model%2520Inference%26entry.906535625%3DJintao%2520Zhang%2520and%2520Chendong%2520Xiang%2520and%2520Haofeng%2520Huang%2520and%2520Jia%2520Wei%2520and%2520Haocheng%2520Xi%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520An%2520efficient%2520attention%2520implementation%2520is%2520essential%2520for%2520large%2520models%2520due%2520to%250Aits%2520quadratic%2520time%2520complexity.%2520Fortunately%252C%2520attention%2520commonly%2520exhibits%250Asparsity%252C%2520i.e.%252C%2520many%2520values%2520in%2520the%2520attention%2520map%2520are%2520near%2520zero%252C%2520allowing%2520for%250Athe%2520omission%2520of%2520corresponding%2520computations.%2520Many%2520studies%2520have%2520utilized%2520the%250Asparse%2520pattern%2520to%2520accelerate%2520attention.%2520However%252C%2520most%2520existing%2520works%2520focus%2520on%250Aoptimizing%2520attention%2520within%2520specific%2520models%2520by%2520exploiting%2520certain%2520sparse%250Apatterns%2520of%2520the%2520attention%2520map.%2520A%2520universal%2520sparse%2520attention%2520that%2520guarantees%250Aboth%2520the%2520speedup%2520and%2520end-to-end%2520performance%2520of%2520diverse%2520models%2520remains%2520elusive.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520SpargeAttn%252C%2520a%2520universal%2520sparse%2520and%2520quantized%250Aattention%2520for%2520any%2520model.%2520Our%2520method%2520uses%2520a%2520two-stage%2520online%2520filter%253A%2520in%2520the%250Afirst%2520stage%252C%2520we%2520rapidly%2520and%2520accurately%2520predict%2520the%2520attention%2520map%252C%2520enabling%2520the%250Askip%2520of%2520some%2520matrix%2520multiplications%2520in%2520attention.%2520In%2520the%2520second%2520stage%252C%2520we%250Adesign%2520an%2520online%2520softmax-aware%2520filter%2520that%2520incurs%2520no%2520extra%2520overhead%2520and%2520further%250Askips%2520some%2520matrix%2520multiplications.%2520Experiments%2520show%2520that%2520our%2520method%250Asignificantly%2520accelerates%2520diverse%2520models%252C%2520including%2520language%252C%2520image%252C%2520and%2520video%250Ageneration%252C%2520without%2520sacrificing%2520end-to-end%2520metrics.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/thu-ml/SpargeAttn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18137v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpargeAttention%3A%20Accurate%20and%20Training-free%20Sparse%20Attention%0A%20%20Accelerating%20Any%20Model%20Inference&entry.906535625=Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%0Aits%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%0Asparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%0Athe%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%0Asparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%0Aoptimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%0Apatterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%0Aboth%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%0AIn%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%0Aattention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%0Afirst%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%0Askip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%0Adesign%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%0Askips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%0Asignificantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%0Ageneration%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SpargeAttn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18137v7&entry.124074799=Read"},
{"title": "Learning Frequency and Memory-Aware Prompts for Multi-Modal Object\n  Tracking", "author": "Boyue Xu and Ruichao Hou and Tongwei Ren and Dongming zhou and Gangshan Wu and Jinde Cao", "abstract": "  Prompt-learning-based multi-modal trackers have made strong progress by using\nlightweight visual adapters to inject auxiliary-modality cues into frozen\nfoundation models. However, they still underutilize two essentials:\nmodality-specific frequency structure and long-range temporal dependencies. We\npresent Learning Frequency and Memory-Aware Prompts, a dual-adapter framework\nthat injects lightweight prompts into a frozen RGB tracker. A frequency-guided\nvisual adapter adaptively transfers complementary cues across modalities by\njointly calibrating spatial, channel, and frequency components, narrowing the\nmodality gap without full fine-tuning. A multilevel memory adapter with short,\nlong, and permanent memory stores, updates, and retrieves reliable temporal\ncontext, enabling consistent propagation across frames and robust recovery from\nocclusion, motion blur, and illumination changes. This unified design preserves\nthe efficiency of prompt learning while strengthening cross-modal interaction\nand temporal coherence. Extensive experiments on RGB-Thermal, RGB-Depth, and\nRGB-Event benchmarks show consistent state-of-the-art results over fully\nfine-tuned and adapter-based baselines, together with favorable parameter\nefficiency and runtime. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.\n", "link": "http://arxiv.org/abs/2506.23972v2", "date": "2025-10-01", "relevancy": 2.2642, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Frequency%20and%20Memory-Aware%20Prompts%20for%20Multi-Modal%20Object%0A%20%20Tracking&body=Title%3A%20Learning%20Frequency%20and%20Memory-Aware%20Prompts%20for%20Multi-Modal%20Object%0A%20%20Tracking%0AAuthor%3A%20Boyue%20Xu%20and%20Ruichao%20Hou%20and%20Tongwei%20Ren%20and%20Dongming%20zhou%20and%20Gangshan%20Wu%20and%20Jinde%20Cao%0AAbstract%3A%20%20%20Prompt-learning-based%20multi-modal%20trackers%20have%20made%20strong%20progress%20by%20using%0Alightweight%20visual%20adapters%20to%20inject%20auxiliary-modality%20cues%20into%20frozen%0Afoundation%20models.%20However%2C%20they%20still%20underutilize%20two%20essentials%3A%0Amodality-specific%20frequency%20structure%20and%20long-range%20temporal%20dependencies.%20We%0Apresent%20Learning%20Frequency%20and%20Memory-Aware%20Prompts%2C%20a%20dual-adapter%20framework%0Athat%20injects%20lightweight%20prompts%20into%20a%20frozen%20RGB%20tracker.%20A%20frequency-guided%0Avisual%20adapter%20adaptively%20transfers%20complementary%20cues%20across%20modalities%20by%0Ajointly%20calibrating%20spatial%2C%20channel%2C%20and%20frequency%20components%2C%20narrowing%20the%0Amodality%20gap%20without%20full%20fine-tuning.%20A%20multilevel%20memory%20adapter%20with%20short%2C%0Along%2C%20and%20permanent%20memory%20stores%2C%20updates%2C%20and%20retrieves%20reliable%20temporal%0Acontext%2C%20enabling%20consistent%20propagation%20across%20frames%20and%20robust%20recovery%20from%0Aocclusion%2C%20motion%20blur%2C%20and%20illumination%20changes.%20This%20unified%20design%20preserves%0Athe%20efficiency%20of%20prompt%20learning%20while%20strengthening%20cross-modal%20interaction%0Aand%20temporal%20coherence.%20Extensive%20experiments%20on%20RGB-Thermal%2C%20RGB-Depth%2C%20and%0ARGB-Event%20benchmarks%20show%20consistent%20state-of-the-art%20results%20over%20fully%0Afine-tuned%20and%20adapter-based%20baselines%2C%20together%20with%20favorable%20parameter%0Aefficiency%20and%20runtime.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/xuboyue1999/mmtrack.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.23972v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Frequency%2520and%2520Memory-Aware%2520Prompts%2520for%2520Multi-Modal%2520Object%250A%2520%2520Tracking%26entry.906535625%3DBoyue%2520Xu%2520and%2520Ruichao%2520Hou%2520and%2520Tongwei%2520Ren%2520and%2520Dongming%2520zhou%2520and%2520Gangshan%2520Wu%2520and%2520Jinde%2520Cao%26entry.1292438233%3D%2520%2520Prompt-learning-based%2520multi-modal%2520trackers%2520have%2520made%2520strong%2520progress%2520by%2520using%250Alightweight%2520visual%2520adapters%2520to%2520inject%2520auxiliary-modality%2520cues%2520into%2520frozen%250Afoundation%2520models.%2520However%252C%2520they%2520still%2520underutilize%2520two%2520essentials%253A%250Amodality-specific%2520frequency%2520structure%2520and%2520long-range%2520temporal%2520dependencies.%2520We%250Apresent%2520Learning%2520Frequency%2520and%2520Memory-Aware%2520Prompts%252C%2520a%2520dual-adapter%2520framework%250Athat%2520injects%2520lightweight%2520prompts%2520into%2520a%2520frozen%2520RGB%2520tracker.%2520A%2520frequency-guided%250Avisual%2520adapter%2520adaptively%2520transfers%2520complementary%2520cues%2520across%2520modalities%2520by%250Ajointly%2520calibrating%2520spatial%252C%2520channel%252C%2520and%2520frequency%2520components%252C%2520narrowing%2520the%250Amodality%2520gap%2520without%2520full%2520fine-tuning.%2520A%2520multilevel%2520memory%2520adapter%2520with%2520short%252C%250Along%252C%2520and%2520permanent%2520memory%2520stores%252C%2520updates%252C%2520and%2520retrieves%2520reliable%2520temporal%250Acontext%252C%2520enabling%2520consistent%2520propagation%2520across%2520frames%2520and%2520robust%2520recovery%2520from%250Aocclusion%252C%2520motion%2520blur%252C%2520and%2520illumination%2520changes.%2520This%2520unified%2520design%2520preserves%250Athe%2520efficiency%2520of%2520prompt%2520learning%2520while%2520strengthening%2520cross-modal%2520interaction%250Aand%2520temporal%2520coherence.%2520Extensive%2520experiments%2520on%2520RGB-Thermal%252C%2520RGB-Depth%252C%2520and%250ARGB-Event%2520benchmarks%2520show%2520consistent%2520state-of-the-art%2520results%2520over%2520fully%250Afine-tuned%2520and%2520adapter-based%2520baselines%252C%2520together%2520with%2520favorable%2520parameter%250Aefficiency%2520and%2520runtime.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/xuboyue1999/mmtrack.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.23972v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Frequency%20and%20Memory-Aware%20Prompts%20for%20Multi-Modal%20Object%0A%20%20Tracking&entry.906535625=Boyue%20Xu%20and%20Ruichao%20Hou%20and%20Tongwei%20Ren%20and%20Dongming%20zhou%20and%20Gangshan%20Wu%20and%20Jinde%20Cao&entry.1292438233=%20%20Prompt-learning-based%20multi-modal%20trackers%20have%20made%20strong%20progress%20by%20using%0Alightweight%20visual%20adapters%20to%20inject%20auxiliary-modality%20cues%20into%20frozen%0Afoundation%20models.%20However%2C%20they%20still%20underutilize%20two%20essentials%3A%0Amodality-specific%20frequency%20structure%20and%20long-range%20temporal%20dependencies.%20We%0Apresent%20Learning%20Frequency%20and%20Memory-Aware%20Prompts%2C%20a%20dual-adapter%20framework%0Athat%20injects%20lightweight%20prompts%20into%20a%20frozen%20RGB%20tracker.%20A%20frequency-guided%0Avisual%20adapter%20adaptively%20transfers%20complementary%20cues%20across%20modalities%20by%0Ajointly%20calibrating%20spatial%2C%20channel%2C%20and%20frequency%20components%2C%20narrowing%20the%0Amodality%20gap%20without%20full%20fine-tuning.%20A%20multilevel%20memory%20adapter%20with%20short%2C%0Along%2C%20and%20permanent%20memory%20stores%2C%20updates%2C%20and%20retrieves%20reliable%20temporal%0Acontext%2C%20enabling%20consistent%20propagation%20across%20frames%20and%20robust%20recovery%20from%0Aocclusion%2C%20motion%20blur%2C%20and%20illumination%20changes.%20This%20unified%20design%20preserves%0Athe%20efficiency%20of%20prompt%20learning%20while%20strengthening%20cross-modal%20interaction%0Aand%20temporal%20coherence.%20Extensive%20experiments%20on%20RGB-Thermal%2C%20RGB-Depth%2C%20and%0ARGB-Event%20benchmarks%20show%20consistent%20state-of-the-art%20results%20over%20fully%0Afine-tuned%20and%20adapter-based%20baselines%2C%20together%20with%20favorable%20parameter%0Aefficiency%20and%20runtime.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/xuboyue1999/mmtrack.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.23972v2&entry.124074799=Read"},
{"title": "Learning Hierarchical Domain Models Through Environment-Grounded\n  Interaction", "author": "Claudius Kienle and Benjamin Alt and Oleg Arenz and Jan Peters", "abstract": "  Domain models enable autonomous agents to solve long-horizon tasks by\nproducing interpretable plans. However, in open-world environments, a single\ngeneral domain model cannot capture the variety of tasks, so agents must\ngenerate suitable task-specific models on the fly. Large Language Models\n(LLMs), with their implicit common knowledge, can generate such domains, but\nsuffer from high error rates that limit their applicability. Hence, related\nwork relies on extensive human feed-back or prior knowledge, which undermines\nautonomous, open-world deployment. In this work, we propose LODGE, a framework\nfor autonomous domain learning from LLMs and environment grounding. LODGE\nbuilds on hierarchical abstractions and automated simulations to identify and\ncorrect inconsistencies between abstraction layers and between the model and\nenvironment. Our framework is task-agnostic, as it generates predicates,\noperators, and their preconditions and effects, while only assuming access to a\nsimulator and a set of generic, executable low-level skills. Experiments on two\nInternational Planning Competition ( IPC) domains and a robotic assembly domain\nshow that LODGE yields more accurate domain models and higher task success than\nexisting methods, requiring remarkably few environment interactions and no\nhuman feedback or demonstrations.\n", "link": "http://arxiv.org/abs/2505.13497v3", "date": "2025-10-01", "relevancy": 2.2554, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Hierarchical%20Domain%20Models%20Through%20Environment-Grounded%0A%20%20Interaction&body=Title%3A%20Learning%20Hierarchical%20Domain%20Models%20Through%20Environment-Grounded%0A%20%20Interaction%0AAuthor%3A%20Claudius%20Kienle%20and%20Benjamin%20Alt%20and%20Oleg%20Arenz%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Domain%20models%20enable%20autonomous%20agents%20to%20solve%20long-horizon%20tasks%20by%0Aproducing%20interpretable%20plans.%20However%2C%20in%20open-world%20environments%2C%20a%20single%0Ageneral%20domain%20model%20cannot%20capture%20the%20variety%20of%20tasks%2C%20so%20agents%20must%0Agenerate%20suitable%20task-specific%20models%20on%20the%20fly.%20Large%20Language%20Models%0A%28LLMs%29%2C%20with%20their%20implicit%20common%20knowledge%2C%20can%20generate%20such%20domains%2C%20but%0Asuffer%20from%20high%20error%20rates%20that%20limit%20their%20applicability.%20Hence%2C%20related%0Awork%20relies%20on%20extensive%20human%20feed-back%20or%20prior%20knowledge%2C%20which%20undermines%0Aautonomous%2C%20open-world%20deployment.%20In%20this%20work%2C%20we%20propose%20LODGE%2C%20a%20framework%0Afor%20autonomous%20domain%20learning%20from%20LLMs%20and%20environment%20grounding.%20LODGE%0Abuilds%20on%20hierarchical%20abstractions%20and%20automated%20simulations%20to%20identify%20and%0Acorrect%20inconsistencies%20between%20abstraction%20layers%20and%20between%20the%20model%20and%0Aenvironment.%20Our%20framework%20is%20task-agnostic%2C%20as%20it%20generates%20predicates%2C%0Aoperators%2C%20and%20their%20preconditions%20and%20effects%2C%20while%20only%20assuming%20access%20to%20a%0Asimulator%20and%20a%20set%20of%20generic%2C%20executable%20low-level%20skills.%20Experiments%20on%20two%0AInternational%20Planning%20Competition%20%28%20IPC%29%20domains%20and%20a%20robotic%20assembly%20domain%0Ashow%20that%20LODGE%20yields%20more%20accurate%20domain%20models%20and%20higher%20task%20success%20than%0Aexisting%20methods%2C%20requiring%20remarkably%20few%20environment%20interactions%20and%20no%0Ahuman%20feedback%20or%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13497v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Hierarchical%2520Domain%2520Models%2520Through%2520Environment-Grounded%250A%2520%2520Interaction%26entry.906535625%3DClaudius%2520Kienle%2520and%2520Benjamin%2520Alt%2520and%2520Oleg%2520Arenz%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Domain%2520models%2520enable%2520autonomous%2520agents%2520to%2520solve%2520long-horizon%2520tasks%2520by%250Aproducing%2520interpretable%2520plans.%2520However%252C%2520in%2520open-world%2520environments%252C%2520a%2520single%250Ageneral%2520domain%2520model%2520cannot%2520capture%2520the%2520variety%2520of%2520tasks%252C%2520so%2520agents%2520must%250Agenerate%2520suitable%2520task-specific%2520models%2520on%2520the%2520fly.%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520with%2520their%2520implicit%2520common%2520knowledge%252C%2520can%2520generate%2520such%2520domains%252C%2520but%250Asuffer%2520from%2520high%2520error%2520rates%2520that%2520limit%2520their%2520applicability.%2520Hence%252C%2520related%250Awork%2520relies%2520on%2520extensive%2520human%2520feed-back%2520or%2520prior%2520knowledge%252C%2520which%2520undermines%250Aautonomous%252C%2520open-world%2520deployment.%2520In%2520this%2520work%252C%2520we%2520propose%2520LODGE%252C%2520a%2520framework%250Afor%2520autonomous%2520domain%2520learning%2520from%2520LLMs%2520and%2520environment%2520grounding.%2520LODGE%250Abuilds%2520on%2520hierarchical%2520abstractions%2520and%2520automated%2520simulations%2520to%2520identify%2520and%250Acorrect%2520inconsistencies%2520between%2520abstraction%2520layers%2520and%2520between%2520the%2520model%2520and%250Aenvironment.%2520Our%2520framework%2520is%2520task-agnostic%252C%2520as%2520it%2520generates%2520predicates%252C%250Aoperators%252C%2520and%2520their%2520preconditions%2520and%2520effects%252C%2520while%2520only%2520assuming%2520access%2520to%2520a%250Asimulator%2520and%2520a%2520set%2520of%2520generic%252C%2520executable%2520low-level%2520skills.%2520Experiments%2520on%2520two%250AInternational%2520Planning%2520Competition%2520%2528%2520IPC%2529%2520domains%2520and%2520a%2520robotic%2520assembly%2520domain%250Ashow%2520that%2520LODGE%2520yields%2520more%2520accurate%2520domain%2520models%2520and%2520higher%2520task%2520success%2520than%250Aexisting%2520methods%252C%2520requiring%2520remarkably%2520few%2520environment%2520interactions%2520and%2520no%250Ahuman%2520feedback%2520or%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13497v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Hierarchical%20Domain%20Models%20Through%20Environment-Grounded%0A%20%20Interaction&entry.906535625=Claudius%20Kienle%20and%20Benjamin%20Alt%20and%20Oleg%20Arenz%20and%20Jan%20Peters&entry.1292438233=%20%20Domain%20models%20enable%20autonomous%20agents%20to%20solve%20long-horizon%20tasks%20by%0Aproducing%20interpretable%20plans.%20However%2C%20in%20open-world%20environments%2C%20a%20single%0Ageneral%20domain%20model%20cannot%20capture%20the%20variety%20of%20tasks%2C%20so%20agents%20must%0Agenerate%20suitable%20task-specific%20models%20on%20the%20fly.%20Large%20Language%20Models%0A%28LLMs%29%2C%20with%20their%20implicit%20common%20knowledge%2C%20can%20generate%20such%20domains%2C%20but%0Asuffer%20from%20high%20error%20rates%20that%20limit%20their%20applicability.%20Hence%2C%20related%0Awork%20relies%20on%20extensive%20human%20feed-back%20or%20prior%20knowledge%2C%20which%20undermines%0Aautonomous%2C%20open-world%20deployment.%20In%20this%20work%2C%20we%20propose%20LODGE%2C%20a%20framework%0Afor%20autonomous%20domain%20learning%20from%20LLMs%20and%20environment%20grounding.%20LODGE%0Abuilds%20on%20hierarchical%20abstractions%20and%20automated%20simulations%20to%20identify%20and%0Acorrect%20inconsistencies%20between%20abstraction%20layers%20and%20between%20the%20model%20and%0Aenvironment.%20Our%20framework%20is%20task-agnostic%2C%20as%20it%20generates%20predicates%2C%0Aoperators%2C%20and%20their%20preconditions%20and%20effects%2C%20while%20only%20assuming%20access%20to%20a%0Asimulator%20and%20a%20set%20of%20generic%2C%20executable%20low-level%20skills.%20Experiments%20on%20two%0AInternational%20Planning%20Competition%20%28%20IPC%29%20domains%20and%20a%20robotic%20assembly%20domain%0Ashow%20that%20LODGE%20yields%20more%20accurate%20domain%20models%20and%20higher%20task%20success%20than%0Aexisting%20methods%2C%20requiring%20remarkably%20few%20environment%20interactions%20and%20no%0Ahuman%20feedback%20or%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13497v3&entry.124074799=Read"},
{"title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge\n  Representation", "author": "Hunzalah Hassan Bhatti and Youssef Ahmed and Md Arid Hasan and Firoj Alam", "abstract": "  In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%.\n", "link": "http://arxiv.org/abs/2508.17324v2", "date": "2025-10-01", "relevancy": 2.2458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CultranAI%20at%20PalmX%202025%3A%20Data%20Augmentation%20for%20Cultural%20Knowledge%0A%20%20Representation&body=Title%3A%20CultranAI%20at%20PalmX%202025%3A%20Data%20Augmentation%20for%20Cultural%20Knowledge%0A%20%20Representation%0AAuthor%3A%20Hunzalah%20Hassan%20Bhatti%20and%20Youssef%20Ahmed%20and%20Md%20Arid%20Hasan%20and%20Firoj%20Alam%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20report%20our%20participation%20to%20the%20PalmX%20cultural%20evaluation%0Ashared%20task.%20Our%20system%2C%20CultranAI%2C%20focused%20on%20data%20augmentation%20and%20LoRA%0Afine-tuning%20of%20large%20language%20models%20%28LLMs%29%20for%20Arabic%20cultural%20knowledge%0Arepresentation.%20We%20benchmarked%20several%20LLMs%20to%20identify%20the%20best-performing%0Amodel%20for%20the%20task.%20In%20addition%20to%20utilizing%20the%20PalmX%20dataset%2C%20we%20augmented%20it%0Aby%20incorporating%20the%20Palm%20dataset%20and%20curated%20a%20new%20dataset%20of%20over%2022K%0Aculturally%20grounded%20multiple-choice%20questions%20%28MCQs%29.%20Our%20experiments%20showed%0Athat%20the%20Fanar-1-9B-Instruct%20model%20achieved%20the%20highest%20performance.%20We%0Afine-tuned%20this%20model%20on%20the%20combined%20augmented%20dataset%20of%2022K%2B%20MCQs.%20On%20the%0Ablind%20test%20set%2C%20our%20submitted%20system%20ranked%205th%20with%20an%20accuracy%20of%2070.50%25%2C%0Awhile%20on%20the%20PalmX%20development%20set%2C%20it%20achieved%20an%20accuracy%20of%2084.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCultranAI%2520at%2520PalmX%25202025%253A%2520Data%2520Augmentation%2520for%2520Cultural%2520Knowledge%250A%2520%2520Representation%26entry.906535625%3DHunzalah%2520Hassan%2520Bhatti%2520and%2520Youssef%2520Ahmed%2520and%2520Md%2520Arid%2520Hasan%2520and%2520Firoj%2520Alam%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520report%2520our%2520participation%2520to%2520the%2520PalmX%2520cultural%2520evaluation%250Ashared%2520task.%2520Our%2520system%252C%2520CultranAI%252C%2520focused%2520on%2520data%2520augmentation%2520and%2520LoRA%250Afine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520Arabic%2520cultural%2520knowledge%250Arepresentation.%2520We%2520benchmarked%2520several%2520LLMs%2520to%2520identify%2520the%2520best-performing%250Amodel%2520for%2520the%2520task.%2520In%2520addition%2520to%2520utilizing%2520the%2520PalmX%2520dataset%252C%2520we%2520augmented%2520it%250Aby%2520incorporating%2520the%2520Palm%2520dataset%2520and%2520curated%2520a%2520new%2520dataset%2520of%2520over%252022K%250Aculturally%2520grounded%2520multiple-choice%2520questions%2520%2528MCQs%2529.%2520Our%2520experiments%2520showed%250Athat%2520the%2520Fanar-1-9B-Instruct%2520model%2520achieved%2520the%2520highest%2520performance.%2520We%250Afine-tuned%2520this%2520model%2520on%2520the%2520combined%2520augmented%2520dataset%2520of%252022K%252B%2520MCQs.%2520On%2520the%250Ablind%2520test%2520set%252C%2520our%2520submitted%2520system%2520ranked%25205th%2520with%2520an%2520accuracy%2520of%252070.50%2525%252C%250Awhile%2520on%2520the%2520PalmX%2520development%2520set%252C%2520it%2520achieved%2520an%2520accuracy%2520of%252084.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CultranAI%20at%20PalmX%202025%3A%20Data%20Augmentation%20for%20Cultural%20Knowledge%0A%20%20Representation&entry.906535625=Hunzalah%20Hassan%20Bhatti%20and%20Youssef%20Ahmed%20and%20Md%20Arid%20Hasan%20and%20Firoj%20Alam&entry.1292438233=%20%20In%20this%20paper%2C%20we%20report%20our%20participation%20to%20the%20PalmX%20cultural%20evaluation%0Ashared%20task.%20Our%20system%2C%20CultranAI%2C%20focused%20on%20data%20augmentation%20and%20LoRA%0Afine-tuning%20of%20large%20language%20models%20%28LLMs%29%20for%20Arabic%20cultural%20knowledge%0Arepresentation.%20We%20benchmarked%20several%20LLMs%20to%20identify%20the%20best-performing%0Amodel%20for%20the%20task.%20In%20addition%20to%20utilizing%20the%20PalmX%20dataset%2C%20we%20augmented%20it%0Aby%20incorporating%20the%20Palm%20dataset%20and%20curated%20a%20new%20dataset%20of%20over%2022K%0Aculturally%20grounded%20multiple-choice%20questions%20%28MCQs%29.%20Our%20experiments%20showed%0Athat%20the%20Fanar-1-9B-Instruct%20model%20achieved%20the%20highest%20performance.%20We%0Afine-tuned%20this%20model%20on%20the%20combined%20augmented%20dataset%20of%2022K%2B%20MCQs.%20On%20the%0Ablind%20test%20set%2C%20our%20submitted%20system%20ranked%205th%20with%20an%20accuracy%20of%2070.50%25%2C%0Awhile%20on%20the%20PalmX%20development%20set%2C%20it%20achieved%20an%20accuracy%20of%2084.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17324v2&entry.124074799=Read"},
{"title": "AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)", "author": "Thanh Tran and Son T. Luu and Quan Bui and Shoshin Nomura", "abstract": "  This paper proposes a method for automatic GUI component detection for the\nIBM i system (formerly and still more commonly known as AS/400). We introduce a\nhuman-annotated dataset consisting of 1,050 system screen images, in which 381\nimages are screenshots of IBM i system screens in Japanese. Each image contains\nmultiple components, including text labels, text boxes, options, tables,\ninstructions, keyboards, and command lines. We then develop a detection system\nbased on state-of-the-art deep learning models and evaluate different\napproaches using our dataset. The experimental results demonstrate the\neffectiveness of our dataset in constructing a system for component detection\nfrom GUI screens. By automatically detecting GUI components from the screen,\nAS400-DET has the potential to perform automated testing on systems that\noperate via GUI screens.\n", "link": "http://arxiv.org/abs/2506.13032v2", "date": "2025-10-01", "relevancy": 2.2402, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4512}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4493}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AS400-DET%3A%20Detection%20using%20Deep%20Learning%20Model%20for%20IBM%20i%20%28AS/400%29&body=Title%3A%20AS400-DET%3A%20Detection%20using%20Deep%20Learning%20Model%20for%20IBM%20i%20%28AS/400%29%0AAuthor%3A%20Thanh%20Tran%20and%20Son%20T.%20Luu%20and%20Quan%20Bui%20and%20Shoshin%20Nomura%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20method%20for%20automatic%20GUI%20component%20detection%20for%20the%0AIBM%20i%20system%20%28formerly%20and%20still%20more%20commonly%20known%20as%20AS/400%29.%20We%20introduce%20a%0Ahuman-annotated%20dataset%20consisting%20of%201%2C050%20system%20screen%20images%2C%20in%20which%20381%0Aimages%20are%20screenshots%20of%20IBM%20i%20system%20screens%20in%20Japanese.%20Each%20image%20contains%0Amultiple%20components%2C%20including%20text%20labels%2C%20text%20boxes%2C%20options%2C%20tables%2C%0Ainstructions%2C%20keyboards%2C%20and%20command%20lines.%20We%20then%20develop%20a%20detection%20system%0Abased%20on%20state-of-the-art%20deep%20learning%20models%20and%20evaluate%20different%0Aapproaches%20using%20our%20dataset.%20The%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20dataset%20in%20constructing%20a%20system%20for%20component%20detection%0Afrom%20GUI%20screens.%20By%20automatically%20detecting%20GUI%20components%20from%20the%20screen%2C%0AAS400-DET%20has%20the%20potential%20to%20perform%20automated%20testing%20on%20systems%20that%0Aoperate%20via%20GUI%20screens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13032v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAS400-DET%253A%2520Detection%2520using%2520Deep%2520Learning%2520Model%2520for%2520IBM%2520i%2520%2528AS/400%2529%26entry.906535625%3DThanh%2520Tran%2520and%2520Son%2520T.%2520Luu%2520and%2520Quan%2520Bui%2520and%2520Shoshin%2520Nomura%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520method%2520for%2520automatic%2520GUI%2520component%2520detection%2520for%2520the%250AIBM%2520i%2520system%2520%2528formerly%2520and%2520still%2520more%2520commonly%2520known%2520as%2520AS/400%2529.%2520We%2520introduce%2520a%250Ahuman-annotated%2520dataset%2520consisting%2520of%25201%252C050%2520system%2520screen%2520images%252C%2520in%2520which%2520381%250Aimages%2520are%2520screenshots%2520of%2520IBM%2520i%2520system%2520screens%2520in%2520Japanese.%2520Each%2520image%2520contains%250Amultiple%2520components%252C%2520including%2520text%2520labels%252C%2520text%2520boxes%252C%2520options%252C%2520tables%252C%250Ainstructions%252C%2520keyboards%252C%2520and%2520command%2520lines.%2520We%2520then%2520develop%2520a%2520detection%2520system%250Abased%2520on%2520state-of-the-art%2520deep%2520learning%2520models%2520and%2520evaluate%2520different%250Aapproaches%2520using%2520our%2520dataset.%2520The%2520experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520dataset%2520in%2520constructing%2520a%2520system%2520for%2520component%2520detection%250Afrom%2520GUI%2520screens.%2520By%2520automatically%2520detecting%2520GUI%2520components%2520from%2520the%2520screen%252C%250AAS400-DET%2520has%2520the%2520potential%2520to%2520perform%2520automated%2520testing%2520on%2520systems%2520that%250Aoperate%2520via%2520GUI%2520screens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13032v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AS400-DET%3A%20Detection%20using%20Deep%20Learning%20Model%20for%20IBM%20i%20%28AS/400%29&entry.906535625=Thanh%20Tran%20and%20Son%20T.%20Luu%20and%20Quan%20Bui%20and%20Shoshin%20Nomura&entry.1292438233=%20%20This%20paper%20proposes%20a%20method%20for%20automatic%20GUI%20component%20detection%20for%20the%0AIBM%20i%20system%20%28formerly%20and%20still%20more%20commonly%20known%20as%20AS/400%29.%20We%20introduce%20a%0Ahuman-annotated%20dataset%20consisting%20of%201%2C050%20system%20screen%20images%2C%20in%20which%20381%0Aimages%20are%20screenshots%20of%20IBM%20i%20system%20screens%20in%20Japanese.%20Each%20image%20contains%0Amultiple%20components%2C%20including%20text%20labels%2C%20text%20boxes%2C%20options%2C%20tables%2C%0Ainstructions%2C%20keyboards%2C%20and%20command%20lines.%20We%20then%20develop%20a%20detection%20system%0Abased%20on%20state-of-the-art%20deep%20learning%20models%20and%20evaluate%20different%0Aapproaches%20using%20our%20dataset.%20The%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20dataset%20in%20constructing%20a%20system%20for%20component%20detection%0Afrom%20GUI%20screens.%20By%20automatically%20detecting%20GUI%20components%20from%20the%20screen%2C%0AAS400-DET%20has%20the%20potential%20to%20perform%20automated%20testing%20on%20systems%20that%0Aoperate%20via%20GUI%20screens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13032v2&entry.124074799=Read"},
{"title": "Semi-Supervised Unconstrained Head Pose Estimation in the Wild", "author": "Huayi Zhou and Fei Jiang and Jin Yuan and Yong Rui and Hongtao Lu and Kui Jia", "abstract": "  Existing research on unconstrained in-the-wild head pose estimation suffers\nfrom the flaws of its datasets, which consist of either numerous samples by\nnon-realistic synthesis or constrained collection, or small-scale natural\nimages yet with plausible manual annotations. This makes fully-supervised\nsolutions compromised due to the reliance on generous labels. To alleviate it,\nwe propose the first semi-supervised unconstrained head pose estimation method\nSemiUHPE, which can leverage abundant easily available unlabeled head images.\nTechnically, we choose semi-supervised rotation regression and adapt it to the\nerror-sensitive and label-scarce problem of unconstrained head pose. Our method\nis based on the observation that the aspect-ratio invariant cropping of wild\nheads is superior to previous landmark-based affine alignment given that\nlandmarks of unconstrained human heads are usually unavailable, especially for\nunderexplored non-frontal heads. Instead of using a pre-fixed threshold to\nfilter out pseudo labeled heads, we propose dynamic entropy based filtering to\nadaptively remove unlabeled outliers as training progresses by updating the\nthreshold in multiple stages. We then revisit the design of weak-strong\naugmentations and improve it by devising two novel head-oriented strong\naugmentations, termed pose-irrelevant cut-occlusion and pose-altering rotation\nconsistency respectively. Extensive experiments and ablation studies show that\nSemiUHPE outperforms its counterparts greatly on public benchmarks under both\nthe front-range and full-range settings. Furthermore, our proposed method is\nalso beneficial for solving other closely related problems, including generic\nobject rotation regression and 3D head reconstruction, demonstrating good\nversatility and extensibility. Code is in https://github.com/hnuzhy/SemiUHPE.\n", "link": "http://arxiv.org/abs/2404.02544v4", "date": "2025-10-01", "relevancy": 2.2298, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5714}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5698}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild&body=Title%3A%20Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Huayi%20Zhou%20and%20Fei%20Jiang%20and%20Jin%20Yuan%20and%20Yong%20Rui%20and%20Hongtao%20Lu%20and%20Kui%20Jia%0AAbstract%3A%20%20%20Existing%20research%20on%20unconstrained%20in-the-wild%20head%20pose%20estimation%20suffers%0Afrom%20the%20flaws%20of%20its%20datasets%2C%20which%20consist%20of%20either%20numerous%20samples%20by%0Anon-realistic%20synthesis%20or%20constrained%20collection%2C%20or%20small-scale%20natural%0Aimages%20yet%20with%20plausible%20manual%20annotations.%20This%20makes%20fully-supervised%0Asolutions%20compromised%20due%20to%20the%20reliance%20on%20generous%20labels.%20To%20alleviate%20it%2C%0Awe%20propose%20the%20first%20semi-supervised%20unconstrained%20head%20pose%20estimation%20method%0ASemiUHPE%2C%20which%20can%20leverage%20abundant%20easily%20available%20unlabeled%20head%20images.%0ATechnically%2C%20we%20choose%20semi-supervised%20rotation%20regression%20and%20adapt%20it%20to%20the%0Aerror-sensitive%20and%20label-scarce%20problem%20of%20unconstrained%20head%20pose.%20Our%20method%0Ais%20based%20on%20the%20observation%20that%20the%20aspect-ratio%20invariant%20cropping%20of%20wild%0Aheads%20is%20superior%20to%20previous%20landmark-based%20affine%20alignment%20given%20that%0Alandmarks%20of%20unconstrained%20human%20heads%20are%20usually%20unavailable%2C%20especially%20for%0Aunderexplored%20non-frontal%20heads.%20Instead%20of%20using%20a%20pre-fixed%20threshold%20to%0Afilter%20out%20pseudo%20labeled%20heads%2C%20we%20propose%20dynamic%20entropy%20based%20filtering%20to%0Aadaptively%20remove%20unlabeled%20outliers%20as%20training%20progresses%20by%20updating%20the%0Athreshold%20in%20multiple%20stages.%20We%20then%20revisit%20the%20design%20of%20weak-strong%0Aaugmentations%20and%20improve%20it%20by%20devising%20two%20novel%20head-oriented%20strong%0Aaugmentations%2C%20termed%20pose-irrelevant%20cut-occlusion%20and%20pose-altering%20rotation%0Aconsistency%20respectively.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%0ASemiUHPE%20outperforms%20its%20counterparts%20greatly%20on%20public%20benchmarks%20under%20both%0Athe%20front-range%20and%20full-range%20settings.%20Furthermore%2C%20our%20proposed%20method%20is%0Aalso%20beneficial%20for%20solving%20other%20closely%20related%20problems%2C%20including%20generic%0Aobject%20rotation%20regression%20and%203D%20head%20reconstruction%2C%20demonstrating%20good%0Aversatility%20and%20extensibility.%20Code%20is%20in%20https%3A//github.com/hnuzhy/SemiUHPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02544v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Unconstrained%2520Head%2520Pose%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DHuayi%2520Zhou%2520and%2520Fei%2520Jiang%2520and%2520Jin%2520Yuan%2520and%2520Yong%2520Rui%2520and%2520Hongtao%2520Lu%2520and%2520Kui%2520Jia%26entry.1292438233%3D%2520%2520Existing%2520research%2520on%2520unconstrained%2520in-the-wild%2520head%2520pose%2520estimation%2520suffers%250Afrom%2520the%2520flaws%2520of%2520its%2520datasets%252C%2520which%2520consist%2520of%2520either%2520numerous%2520samples%2520by%250Anon-realistic%2520synthesis%2520or%2520constrained%2520collection%252C%2520or%2520small-scale%2520natural%250Aimages%2520yet%2520with%2520plausible%2520manual%2520annotations.%2520This%2520makes%2520fully-supervised%250Asolutions%2520compromised%2520due%2520to%2520the%2520reliance%2520on%2520generous%2520labels.%2520To%2520alleviate%2520it%252C%250Awe%2520propose%2520the%2520first%2520semi-supervised%2520unconstrained%2520head%2520pose%2520estimation%2520method%250ASemiUHPE%252C%2520which%2520can%2520leverage%2520abundant%2520easily%2520available%2520unlabeled%2520head%2520images.%250ATechnically%252C%2520we%2520choose%2520semi-supervised%2520rotation%2520regression%2520and%2520adapt%2520it%2520to%2520the%250Aerror-sensitive%2520and%2520label-scarce%2520problem%2520of%2520unconstrained%2520head%2520pose.%2520Our%2520method%250Ais%2520based%2520on%2520the%2520observation%2520that%2520the%2520aspect-ratio%2520invariant%2520cropping%2520of%2520wild%250Aheads%2520is%2520superior%2520to%2520previous%2520landmark-based%2520affine%2520alignment%2520given%2520that%250Alandmarks%2520of%2520unconstrained%2520human%2520heads%2520are%2520usually%2520unavailable%252C%2520especially%2520for%250Aunderexplored%2520non-frontal%2520heads.%2520Instead%2520of%2520using%2520a%2520pre-fixed%2520threshold%2520to%250Afilter%2520out%2520pseudo%2520labeled%2520heads%252C%2520we%2520propose%2520dynamic%2520entropy%2520based%2520filtering%2520to%250Aadaptively%2520remove%2520unlabeled%2520outliers%2520as%2520training%2520progresses%2520by%2520updating%2520the%250Athreshold%2520in%2520multiple%2520stages.%2520We%2520then%2520revisit%2520the%2520design%2520of%2520weak-strong%250Aaugmentations%2520and%2520improve%2520it%2520by%2520devising%2520two%2520novel%2520head-oriented%2520strong%250Aaugmentations%252C%2520termed%2520pose-irrelevant%2520cut-occlusion%2520and%2520pose-altering%2520rotation%250Aconsistency%2520respectively.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520show%2520that%250ASemiUHPE%2520outperforms%2520its%2520counterparts%2520greatly%2520on%2520public%2520benchmarks%2520under%2520both%250Athe%2520front-range%2520and%2520full-range%2520settings.%2520Furthermore%252C%2520our%2520proposed%2520method%2520is%250Aalso%2520beneficial%2520for%2520solving%2520other%2520closely%2520related%2520problems%252C%2520including%2520generic%250Aobject%2520rotation%2520regression%2520and%25203D%2520head%2520reconstruction%252C%2520demonstrating%2520good%250Aversatility%2520and%2520extensibility.%2520Code%2520is%2520in%2520https%253A//github.com/hnuzhy/SemiUHPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02544v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Unconstrained%20Head%20Pose%20Estimation%20in%20the%20Wild&entry.906535625=Huayi%20Zhou%20and%20Fei%20Jiang%20and%20Jin%20Yuan%20and%20Yong%20Rui%20and%20Hongtao%20Lu%20and%20Kui%20Jia&entry.1292438233=%20%20Existing%20research%20on%20unconstrained%20in-the-wild%20head%20pose%20estimation%20suffers%0Afrom%20the%20flaws%20of%20its%20datasets%2C%20which%20consist%20of%20either%20numerous%20samples%20by%0Anon-realistic%20synthesis%20or%20constrained%20collection%2C%20or%20small-scale%20natural%0Aimages%20yet%20with%20plausible%20manual%20annotations.%20This%20makes%20fully-supervised%0Asolutions%20compromised%20due%20to%20the%20reliance%20on%20generous%20labels.%20To%20alleviate%20it%2C%0Awe%20propose%20the%20first%20semi-supervised%20unconstrained%20head%20pose%20estimation%20method%0ASemiUHPE%2C%20which%20can%20leverage%20abundant%20easily%20available%20unlabeled%20head%20images.%0ATechnically%2C%20we%20choose%20semi-supervised%20rotation%20regression%20and%20adapt%20it%20to%20the%0Aerror-sensitive%20and%20label-scarce%20problem%20of%20unconstrained%20head%20pose.%20Our%20method%0Ais%20based%20on%20the%20observation%20that%20the%20aspect-ratio%20invariant%20cropping%20of%20wild%0Aheads%20is%20superior%20to%20previous%20landmark-based%20affine%20alignment%20given%20that%0Alandmarks%20of%20unconstrained%20human%20heads%20are%20usually%20unavailable%2C%20especially%20for%0Aunderexplored%20non-frontal%20heads.%20Instead%20of%20using%20a%20pre-fixed%20threshold%20to%0Afilter%20out%20pseudo%20labeled%20heads%2C%20we%20propose%20dynamic%20entropy%20based%20filtering%20to%0Aadaptively%20remove%20unlabeled%20outliers%20as%20training%20progresses%20by%20updating%20the%0Athreshold%20in%20multiple%20stages.%20We%20then%20revisit%20the%20design%20of%20weak-strong%0Aaugmentations%20and%20improve%20it%20by%20devising%20two%20novel%20head-oriented%20strong%0Aaugmentations%2C%20termed%20pose-irrelevant%20cut-occlusion%20and%20pose-altering%20rotation%0Aconsistency%20respectively.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%0ASemiUHPE%20outperforms%20its%20counterparts%20greatly%20on%20public%20benchmarks%20under%20both%0Athe%20front-range%20and%20full-range%20settings.%20Furthermore%2C%20our%20proposed%20method%20is%0Aalso%20beneficial%20for%20solving%20other%20closely%20related%20problems%2C%20including%20generic%0Aobject%20rotation%20regression%20and%203D%20head%20reconstruction%2C%20demonstrating%20good%0Aversatility%20and%20extensibility.%20Code%20is%20in%20https%3A//github.com/hnuzhy/SemiUHPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02544v4&entry.124074799=Read"},
{"title": "ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for\n  Intelligent Navigation in Dynamic Environment", "author": "Faryal Batool and Yasheerah Yaqoot and Malaika Zafar and Roohan Ahmed Khan and Muhammad Haris Khan and Aleksey Fedoseev and Dzmitry Tsetserukou", "abstract": "  Swarm robotics plays a crucial role in enabling autonomous operations in\ndynamic and unpredictable environments. However, a major challenge remains\nensuring safe and efficient navigation in environments filled with both dynamic\nalive (e.g., humans) and dynamic inanimate (e.g., non-living objects)\nobstacles. In this paper, we propose ImpedanceGPT, a novel system that combines\na Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to\nenable real-time reasoning for adaptive navigation of mini-drone swarms in\ncomplex environments.\n  The key innovation of ImpedanceGPT lies in the integration of VLM and RAG,\nwhich provides the drones with enhanced semantic understanding of their\nsurroundings. This enables the system to dynamically adjust impedance control\nparameters in response to obstacle types and environmental conditions. Our\napproach not only ensures safe and precise navigation but also improves\ncoordination between drones in the swarm.\n  Experimental evaluations demonstrate the effectiveness of the system. The\nVLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 %\nunder optimal lighting. In static environments, drones navigated dynamic\ninanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation\naround humans. In dynamic environments, speed adjusted to 1.0 m/s near hard\nobstacles, while reducing to 0.6 m/s with higher deflection to safely avoid\nmoving humans.\n", "link": "http://arxiv.org/abs/2503.02723v2", "date": "2025-10-01", "relevancy": 2.2262, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5676}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImpedanceGPT%3A%20VLM-driven%20Impedance%20Control%20of%20Swarm%20of%20Mini-drones%20for%0A%20%20Intelligent%20Navigation%20in%20Dynamic%20Environment&body=Title%3A%20ImpedanceGPT%3A%20VLM-driven%20Impedance%20Control%20of%20Swarm%20of%20Mini-drones%20for%0A%20%20Intelligent%20Navigation%20in%20Dynamic%20Environment%0AAuthor%3A%20Faryal%20Batool%20and%20Yasheerah%20Yaqoot%20and%20Malaika%20Zafar%20and%20Roohan%20Ahmed%20Khan%20and%20Muhammad%20Haris%20Khan%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Swarm%20robotics%20plays%20a%20crucial%20role%20in%20enabling%20autonomous%20operations%20in%0Adynamic%20and%20unpredictable%20environments.%20However%2C%20a%20major%20challenge%20remains%0Aensuring%20safe%20and%20efficient%20navigation%20in%20environments%20filled%20with%20both%20dynamic%0Aalive%20%28e.g.%2C%20humans%29%20and%20dynamic%20inanimate%20%28e.g.%2C%20non-living%20objects%29%0Aobstacles.%20In%20this%20paper%2C%20we%20propose%20ImpedanceGPT%2C%20a%20novel%20system%20that%20combines%0Aa%20Vision-Language%20Model%20%28VLM%29%20with%20retrieval-augmented%20generation%20%28RAG%29%20to%0Aenable%20real-time%20reasoning%20for%20adaptive%20navigation%20of%20mini-drone%20swarms%20in%0Acomplex%20environments.%0A%20%20The%20key%20innovation%20of%20ImpedanceGPT%20lies%20in%20the%20integration%20of%20VLM%20and%20RAG%2C%0Awhich%20provides%20the%20drones%20with%20enhanced%20semantic%20understanding%20of%20their%0Asurroundings.%20This%20enables%20the%20system%20to%20dynamically%20adjust%20impedance%20control%0Aparameters%20in%20response%20to%20obstacle%20types%20and%20environmental%20conditions.%20Our%0Aapproach%20not%20only%20ensures%20safe%20and%20precise%20navigation%20but%20also%20improves%0Acoordination%20between%20drones%20in%20the%20swarm.%0A%20%20Experimental%20evaluations%20demonstrate%20the%20effectiveness%20of%20the%20system.%20The%0AVLM-RAG%20framework%20achieved%20an%20obstacle%20detection%20and%20retrieval%20accuracy%20of%2080%20%25%0Aunder%20optimal%20lighting.%20In%20static%20environments%2C%20drones%20navigated%20dynamic%0Ainanimate%20obstacles%20at%201.4%20m/s%20but%20slowed%20to%200.7%20m/s%20with%20increased%20separation%0Aaround%20humans.%20In%20dynamic%20environments%2C%20speed%20adjusted%20to%201.0%20m/s%20near%20hard%0Aobstacles%2C%20while%20reducing%20to%200.6%20m/s%20with%20higher%20deflection%20to%20safely%20avoid%0Amoving%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpedanceGPT%253A%2520VLM-driven%2520Impedance%2520Control%2520of%2520Swarm%2520of%2520Mini-drones%2520for%250A%2520%2520Intelligent%2520Navigation%2520in%2520Dynamic%2520Environment%26entry.906535625%3DFaryal%2520Batool%2520and%2520Yasheerah%2520Yaqoot%2520and%2520Malaika%2520Zafar%2520and%2520Roohan%2520Ahmed%2520Khan%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Aleksey%2520Fedoseev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Swarm%2520robotics%2520plays%2520a%2520crucial%2520role%2520in%2520enabling%2520autonomous%2520operations%2520in%250Adynamic%2520and%2520unpredictable%2520environments.%2520However%252C%2520a%2520major%2520challenge%2520remains%250Aensuring%2520safe%2520and%2520efficient%2520navigation%2520in%2520environments%2520filled%2520with%2520both%2520dynamic%250Aalive%2520%2528e.g.%252C%2520humans%2529%2520and%2520dynamic%2520inanimate%2520%2528e.g.%252C%2520non-living%2520objects%2529%250Aobstacles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ImpedanceGPT%252C%2520a%2520novel%2520system%2520that%2520combines%250Aa%2520Vision-Language%2520Model%2520%2528VLM%2529%2520with%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520to%250Aenable%2520real-time%2520reasoning%2520for%2520adaptive%2520navigation%2520of%2520mini-drone%2520swarms%2520in%250Acomplex%2520environments.%250A%2520%2520The%2520key%2520innovation%2520of%2520ImpedanceGPT%2520lies%2520in%2520the%2520integration%2520of%2520VLM%2520and%2520RAG%252C%250Awhich%2520provides%2520the%2520drones%2520with%2520enhanced%2520semantic%2520understanding%2520of%2520their%250Asurroundings.%2520This%2520enables%2520the%2520system%2520to%2520dynamically%2520adjust%2520impedance%2520control%250Aparameters%2520in%2520response%2520to%2520obstacle%2520types%2520and%2520environmental%2520conditions.%2520Our%250Aapproach%2520not%2520only%2520ensures%2520safe%2520and%2520precise%2520navigation%2520but%2520also%2520improves%250Acoordination%2520between%2520drones%2520in%2520the%2520swarm.%250A%2520%2520Experimental%2520evaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520system.%2520The%250AVLM-RAG%2520framework%2520achieved%2520an%2520obstacle%2520detection%2520and%2520retrieval%2520accuracy%2520of%252080%2520%2525%250Aunder%2520optimal%2520lighting.%2520In%2520static%2520environments%252C%2520drones%2520navigated%2520dynamic%250Ainanimate%2520obstacles%2520at%25201.4%2520m/s%2520but%2520slowed%2520to%25200.7%2520m/s%2520with%2520increased%2520separation%250Aaround%2520humans.%2520In%2520dynamic%2520environments%252C%2520speed%2520adjusted%2520to%25201.0%2520m/s%2520near%2520hard%250Aobstacles%252C%2520while%2520reducing%2520to%25200.6%2520m/s%2520with%2520higher%2520deflection%2520to%2520safely%2520avoid%250Amoving%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImpedanceGPT%3A%20VLM-driven%20Impedance%20Control%20of%20Swarm%20of%20Mini-drones%20for%0A%20%20Intelligent%20Navigation%20in%20Dynamic%20Environment&entry.906535625=Faryal%20Batool%20and%20Yasheerah%20Yaqoot%20and%20Malaika%20Zafar%20and%20Roohan%20Ahmed%20Khan%20and%20Muhammad%20Haris%20Khan%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Swarm%20robotics%20plays%20a%20crucial%20role%20in%20enabling%20autonomous%20operations%20in%0Adynamic%20and%20unpredictable%20environments.%20However%2C%20a%20major%20challenge%20remains%0Aensuring%20safe%20and%20efficient%20navigation%20in%20environments%20filled%20with%20both%20dynamic%0Aalive%20%28e.g.%2C%20humans%29%20and%20dynamic%20inanimate%20%28e.g.%2C%20non-living%20objects%29%0Aobstacles.%20In%20this%20paper%2C%20we%20propose%20ImpedanceGPT%2C%20a%20novel%20system%20that%20combines%0Aa%20Vision-Language%20Model%20%28VLM%29%20with%20retrieval-augmented%20generation%20%28RAG%29%20to%0Aenable%20real-time%20reasoning%20for%20adaptive%20navigation%20of%20mini-drone%20swarms%20in%0Acomplex%20environments.%0A%20%20The%20key%20innovation%20of%20ImpedanceGPT%20lies%20in%20the%20integration%20of%20VLM%20and%20RAG%2C%0Awhich%20provides%20the%20drones%20with%20enhanced%20semantic%20understanding%20of%20their%0Asurroundings.%20This%20enables%20the%20system%20to%20dynamically%20adjust%20impedance%20control%0Aparameters%20in%20response%20to%20obstacle%20types%20and%20environmental%20conditions.%20Our%0Aapproach%20not%20only%20ensures%20safe%20and%20precise%20navigation%20but%20also%20improves%0Acoordination%20between%20drones%20in%20the%20swarm.%0A%20%20Experimental%20evaluations%20demonstrate%20the%20effectiveness%20of%20the%20system.%20The%0AVLM-RAG%20framework%20achieved%20an%20obstacle%20detection%20and%20retrieval%20accuracy%20of%2080%20%25%0Aunder%20optimal%20lighting.%20In%20static%20environments%2C%20drones%20navigated%20dynamic%0Ainanimate%20obstacles%20at%201.4%20m/s%20but%20slowed%20to%200.7%20m/s%20with%20increased%20separation%0Aaround%20humans.%20In%20dynamic%20environments%2C%20speed%20adjusted%20to%201.0%20m/s%20near%20hard%0Aobstacles%2C%20while%20reducing%20to%200.6%20m/s%20with%20higher%20deflection%20to%20safely%20avoid%0Amoving%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02723v2&entry.124074799=Read"},
{"title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning", "author": "Bosung Kim and Prithviraj Ammanabrolu", "abstract": "  We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.\n", "link": "http://arxiv.org/abs/2505.16928v2", "date": "2025-10-01", "relevancy": 2.1644, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Needle%28s%29%20in%20the%20Embodied%20Haystack%3A%20Environment%2C%20Architecture%2C%0A%20%20and%20Training%20Considerations%20for%20Long%20Context%20Reasoning&body=Title%3A%20Beyond%20Needle%28s%29%20in%20the%20Embodied%20Haystack%3A%20Environment%2C%20Architecture%2C%0A%20%20and%20Training%20Considerations%20for%20Long%20Context%20Reasoning%0AAuthor%3A%20Bosung%20Kim%20and%20Prithviraj%20Ammanabrolu%0AAbstract%3A%20%20%20We%20introduce%20%24%5Cinfty%24-THOR%2C%20a%20new%20framework%20for%20long-horizon%20embodied%20tasks%0Athat%20advances%20long-context%20understanding%20in%20embodied%20AI.%20%24%5Cinfty%24-THOR%0Aprovides%3A%20%281%29%20a%20generation%20framework%20for%20synthesizing%20scalable%2C%20reproducible%2C%0Aand%20unlimited%20long-horizon%20trajectories%3B%20%282%29%20a%20novel%20embodied%20QA%20task%2C%0ANeedle%28s%29%20in%20the%20Embodied%20Haystack%2C%20where%20multiple%20scattered%20clues%20across%0Aextended%20trajectories%20test%20agents%27%20long-context%20reasoning%20ability%3B%20and%20%283%29%20a%0Along-horizon%20dataset%20and%20benchmark%20suite%20featuring%20complex%20tasks%20that%20span%0Ahundreds%20of%20environment%20steps%2C%20each%20paired%20with%20ground-truth%20action%20sequences.%0ATo%20enable%20this%20capability%2C%20we%20explore%20architectural%20adaptations%2C%20including%0Ainterleaved%20Goal-State-Action%20modeling%2C%20context%20extension%20techniques%2C%20and%0AContext%20Parallelism%2C%20to%20equip%20LLM-based%20agents%20for%20extreme%20long-context%0Areasoning%20and%20interaction.%20Experimental%20results%20and%20analyses%20highlight%20the%0Achallenges%20posed%20by%20our%20benchmark%20and%20provide%20insights%20into%20training%20strategies%0Aand%20model%20behaviors%20under%20long-horizon%20conditions.%20Our%20work%20provides%20a%0Afoundation%20for%20the%20next%20generation%20of%20embodied%20AI%20systems%20capable%20of%20robust%2C%0Along-term%20reasoning%20and%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Needle%2528s%2529%2520in%2520the%2520Embodied%2520Haystack%253A%2520Environment%252C%2520Architecture%252C%250A%2520%2520and%2520Training%2520Considerations%2520for%2520Long%2520Context%2520Reasoning%26entry.906535625%3DBosung%2520Kim%2520and%2520Prithviraj%2520Ammanabrolu%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2524%255Cinfty%2524-THOR%252C%2520a%2520new%2520framework%2520for%2520long-horizon%2520embodied%2520tasks%250Athat%2520advances%2520long-context%2520understanding%2520in%2520embodied%2520AI.%2520%2524%255Cinfty%2524-THOR%250Aprovides%253A%2520%25281%2529%2520a%2520generation%2520framework%2520for%2520synthesizing%2520scalable%252C%2520reproducible%252C%250Aand%2520unlimited%2520long-horizon%2520trajectories%253B%2520%25282%2529%2520a%2520novel%2520embodied%2520QA%2520task%252C%250ANeedle%2528s%2529%2520in%2520the%2520Embodied%2520Haystack%252C%2520where%2520multiple%2520scattered%2520clues%2520across%250Aextended%2520trajectories%2520test%2520agents%2527%2520long-context%2520reasoning%2520ability%253B%2520and%2520%25283%2529%2520a%250Along-horizon%2520dataset%2520and%2520benchmark%2520suite%2520featuring%2520complex%2520tasks%2520that%2520span%250Ahundreds%2520of%2520environment%2520steps%252C%2520each%2520paired%2520with%2520ground-truth%2520action%2520sequences.%250ATo%2520enable%2520this%2520capability%252C%2520we%2520explore%2520architectural%2520adaptations%252C%2520including%250Ainterleaved%2520Goal-State-Action%2520modeling%252C%2520context%2520extension%2520techniques%252C%2520and%250AContext%2520Parallelism%252C%2520to%2520equip%2520LLM-based%2520agents%2520for%2520extreme%2520long-context%250Areasoning%2520and%2520interaction.%2520Experimental%2520results%2520and%2520analyses%2520highlight%2520the%250Achallenges%2520posed%2520by%2520our%2520benchmark%2520and%2520provide%2520insights%2520into%2520training%2520strategies%250Aand%2520model%2520behaviors%2520under%2520long-horizon%2520conditions.%2520Our%2520work%2520provides%2520a%250Afoundation%2520for%2520the%2520next%2520generation%2520of%2520embodied%2520AI%2520systems%2520capable%2520of%2520robust%252C%250Along-term%2520reasoning%2520and%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Needle%28s%29%20in%20the%20Embodied%20Haystack%3A%20Environment%2C%20Architecture%2C%0A%20%20and%20Training%20Considerations%20for%20Long%20Context%20Reasoning&entry.906535625=Bosung%20Kim%20and%20Prithviraj%20Ammanabrolu&entry.1292438233=%20%20We%20introduce%20%24%5Cinfty%24-THOR%2C%20a%20new%20framework%20for%20long-horizon%20embodied%20tasks%0Athat%20advances%20long-context%20understanding%20in%20embodied%20AI.%20%24%5Cinfty%24-THOR%0Aprovides%3A%20%281%29%20a%20generation%20framework%20for%20synthesizing%20scalable%2C%20reproducible%2C%0Aand%20unlimited%20long-horizon%20trajectories%3B%20%282%29%20a%20novel%20embodied%20QA%20task%2C%0ANeedle%28s%29%20in%20the%20Embodied%20Haystack%2C%20where%20multiple%20scattered%20clues%20across%0Aextended%20trajectories%20test%20agents%27%20long-context%20reasoning%20ability%3B%20and%20%283%29%20a%0Along-horizon%20dataset%20and%20benchmark%20suite%20featuring%20complex%20tasks%20that%20span%0Ahundreds%20of%20environment%20steps%2C%20each%20paired%20with%20ground-truth%20action%20sequences.%0ATo%20enable%20this%20capability%2C%20we%20explore%20architectural%20adaptations%2C%20including%0Ainterleaved%20Goal-State-Action%20modeling%2C%20context%20extension%20techniques%2C%20and%0AContext%20Parallelism%2C%20to%20equip%20LLM-based%20agents%20for%20extreme%20long-context%0Areasoning%20and%20interaction.%20Experimental%20results%20and%20analyses%20highlight%20the%0Achallenges%20posed%20by%20our%20benchmark%20and%20provide%20insights%20into%20training%20strategies%0Aand%20model%20behaviors%20under%20long-horizon%20conditions.%20Our%20work%20provides%20a%0Afoundation%20for%20the%20next%20generation%20of%20embodied%20AI%20systems%20capable%20of%20robust%2C%0Along-term%20reasoning%20and%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16928v2&entry.124074799=Read"},
{"title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content", "author": "Shaina Raza and Caesar Saleh and Azib Farooq and Emrul Hasan and Franklin Ogidi and Maximus Powers and Veronica Chatrath and Marcelo Lotif and Karanpal Sekhon and Roya Javadi and Haad Zahid and Anam Zahid and Vahid Reza Khazaie and Zhenyu Yu", "abstract": "  Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.\n", "link": "http://arxiv.org/abs/2412.17052v4", "date": "2025-10-01", "relevancy": 2.1637, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content&body=Title%3A%20ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content%0AAuthor%3A%20Shaina%20Raza%20and%20Caesar%20Saleh%20and%20Azib%20Farooq%20and%20Emrul%20Hasan%20and%20Franklin%20Ogidi%20and%20Maximus%20Powers%20and%20Veronica%20Chatrath%20and%20Marcelo%20Lotif%20and%20Karanpal%20Sekhon%20and%20Roya%20Javadi%20and%20Haad%20Zahid%20and%20Anam%20Zahid%20and%20Vahid%20Reza%20Khazaie%20and%20Zhenyu%20Yu%0AAbstract%3A%20%20%20Detecting%20bias%20in%20multimodal%20news%20requires%20models%20that%20reason%20over%0Atext--image%20pairs%2C%20not%20just%20classify%20text.%20In%20response%2C%20we%20present%20ViLBias%2C%20a%0AVQA-style%20benchmark%20and%20framework%20for%20detecting%20and%20reasoning%20about%20bias%20in%0Amultimodal%20news.%20The%20dataset%20comprises%2040%2C945%20text--image%20pairs%20from%20diverse%0Aoutlets%2C%20each%20annotated%20with%20a%20bias%20label%20and%20concise%20rationale%20using%20a%0Atwo-stage%20LLM-as-annotator%20pipeline%20with%20hierarchical%20majority%20voting%20and%0Ahuman-in-the-loop%20validation.%20We%20evaluate%20Small%20Language%20Models%20%28SLMs%29%2C%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20and%20Vision--Language%20Models%20%28VLMs%29%20across%20closed-ended%0Aclassification%20and%20open-ended%20reasoning%20%28oVQA%29%2C%20and%20compare%20parameter-efficient%0Atuning%20strategies.%20Results%20show%20that%20incorporating%20images%20alongside%20text%0Aimproves%20detection%20accuracy%20by%203--5%5C%25%2C%20and%20that%20LLMs/VLMs%20better%20capture%20subtle%0Aframing%20and%20text--image%20inconsistencies%20than%20SLMs.%20Parameter-efficient%20methods%0A%28LoRA/QLoRA/Adapters%29%20recover%2097--99%5C%25%20of%20full%20fine-tuning%20performance%20with%0A%24%3C5%5C%25%24%20trainable%20parameters.%20For%20oVQA%2C%20reasoning%20accuracy%20spans%2052--79%5C%25%20and%0Afaithfulness%2068--89%5C%25%2C%20both%20improved%20by%20instruction%20tuning%3B%20closed%20accuracy%0Acorrelates%20strongly%20with%20reasoning%20%28%24r%20%3D%200.91%24%29.%20ViLBias%20offers%20a%20scalable%0Abenchmark%20and%20strong%20baselines%20for%20multimodal%20bias%20detection%20and%20rationale%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17052v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViLBias%253A%2520Detecting%2520and%2520Reasoning%2520about%2520Bias%2520in%2520Multimodal%2520Content%26entry.906535625%3DShaina%2520Raza%2520and%2520Caesar%2520Saleh%2520and%2520Azib%2520Farooq%2520and%2520Emrul%2520Hasan%2520and%2520Franklin%2520Ogidi%2520and%2520Maximus%2520Powers%2520and%2520Veronica%2520Chatrath%2520and%2520Marcelo%2520Lotif%2520and%2520Karanpal%2520Sekhon%2520and%2520Roya%2520Javadi%2520and%2520Haad%2520Zahid%2520and%2520Anam%2520Zahid%2520and%2520Vahid%2520Reza%2520Khazaie%2520and%2520Zhenyu%2520Yu%26entry.1292438233%3D%2520%2520Detecting%2520bias%2520in%2520multimodal%2520news%2520requires%2520models%2520that%2520reason%2520over%250Atext--image%2520pairs%252C%2520not%2520just%2520classify%2520text.%2520In%2520response%252C%2520we%2520present%2520ViLBias%252C%2520a%250AVQA-style%2520benchmark%2520and%2520framework%2520for%2520detecting%2520and%2520reasoning%2520about%2520bias%2520in%250Amultimodal%2520news.%2520The%2520dataset%2520comprises%252040%252C945%2520text--image%2520pairs%2520from%2520diverse%250Aoutlets%252C%2520each%2520annotated%2520with%2520a%2520bias%2520label%2520and%2520concise%2520rationale%2520using%2520a%250Atwo-stage%2520LLM-as-annotator%2520pipeline%2520with%2520hierarchical%2520majority%2520voting%2520and%250Ahuman-in-the-loop%2520validation.%2520We%2520evaluate%2520Small%2520Language%2520Models%2520%2528SLMs%2529%252C%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520and%2520Vision--Language%2520Models%2520%2528VLMs%2529%2520across%2520closed-ended%250Aclassification%2520and%2520open-ended%2520reasoning%2520%2528oVQA%2529%252C%2520and%2520compare%2520parameter-efficient%250Atuning%2520strategies.%2520Results%2520show%2520that%2520incorporating%2520images%2520alongside%2520text%250Aimproves%2520detection%2520accuracy%2520by%25203--5%255C%2525%252C%2520and%2520that%2520LLMs/VLMs%2520better%2520capture%2520subtle%250Aframing%2520and%2520text--image%2520inconsistencies%2520than%2520SLMs.%2520Parameter-efficient%2520methods%250A%2528LoRA/QLoRA/Adapters%2529%2520recover%252097--99%255C%2525%2520of%2520full%2520fine-tuning%2520performance%2520with%250A%2524%253C5%255C%2525%2524%2520trainable%2520parameters.%2520For%2520oVQA%252C%2520reasoning%2520accuracy%2520spans%252052--79%255C%2525%2520and%250Afaithfulness%252068--89%255C%2525%252C%2520both%2520improved%2520by%2520instruction%2520tuning%253B%2520closed%2520accuracy%250Acorrelates%2520strongly%2520with%2520reasoning%2520%2528%2524r%2520%253D%25200.91%2524%2529.%2520ViLBias%2520offers%2520a%2520scalable%250Abenchmark%2520and%2520strong%2520baselines%2520for%2520multimodal%2520bias%2520detection%2520and%2520rationale%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17052v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViLBias%3A%20Detecting%20and%20Reasoning%20about%20Bias%20in%20Multimodal%20Content&entry.906535625=Shaina%20Raza%20and%20Caesar%20Saleh%20and%20Azib%20Farooq%20and%20Emrul%20Hasan%20and%20Franklin%20Ogidi%20and%20Maximus%20Powers%20and%20Veronica%20Chatrath%20and%20Marcelo%20Lotif%20and%20Karanpal%20Sekhon%20and%20Roya%20Javadi%20and%20Haad%20Zahid%20and%20Anam%20Zahid%20and%20Vahid%20Reza%20Khazaie%20and%20Zhenyu%20Yu&entry.1292438233=%20%20Detecting%20bias%20in%20multimodal%20news%20requires%20models%20that%20reason%20over%0Atext--image%20pairs%2C%20not%20just%20classify%20text.%20In%20response%2C%20we%20present%20ViLBias%2C%20a%0AVQA-style%20benchmark%20and%20framework%20for%20detecting%20and%20reasoning%20about%20bias%20in%0Amultimodal%20news.%20The%20dataset%20comprises%2040%2C945%20text--image%20pairs%20from%20diverse%0Aoutlets%2C%20each%20annotated%20with%20a%20bias%20label%20and%20concise%20rationale%20using%20a%0Atwo-stage%20LLM-as-annotator%20pipeline%20with%20hierarchical%20majority%20voting%20and%0Ahuman-in-the-loop%20validation.%20We%20evaluate%20Small%20Language%20Models%20%28SLMs%29%2C%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20and%20Vision--Language%20Models%20%28VLMs%29%20across%20closed-ended%0Aclassification%20and%20open-ended%20reasoning%20%28oVQA%29%2C%20and%20compare%20parameter-efficient%0Atuning%20strategies.%20Results%20show%20that%20incorporating%20images%20alongside%20text%0Aimproves%20detection%20accuracy%20by%203--5%5C%25%2C%20and%20that%20LLMs/VLMs%20better%20capture%20subtle%0Aframing%20and%20text--image%20inconsistencies%20than%20SLMs.%20Parameter-efficient%20methods%0A%28LoRA/QLoRA/Adapters%29%20recover%2097--99%5C%25%20of%20full%20fine-tuning%20performance%20with%0A%24%3C5%5C%25%24%20trainable%20parameters.%20For%20oVQA%2C%20reasoning%20accuracy%20spans%2052--79%5C%25%20and%0Afaithfulness%2068--89%5C%25%2C%20both%20improved%20by%20instruction%20tuning%3B%20closed%20accuracy%0Acorrelates%20strongly%20with%20reasoning%20%28%24r%20%3D%200.91%24%29.%20ViLBias%20offers%20a%20scalable%0Abenchmark%20and%20strong%20baselines%20for%20multimodal%20bias%20detection%20and%20rationale%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17052v4&entry.124074799=Read"},
{"title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection", "author": "Tuan Nguyen and Naseem Khan and Khang Tran and NhatHai Phan and Issa Khalil", "abstract": "  The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection.\n", "link": "http://arxiv.org/abs/2509.26272v2", "date": "2025-10-01", "relevancy": 2.158, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRPO%3A%20Paragraph-level%20Policy%20Optimization%20for%20Vision-Language%20Deepfake%0A%20%20Detection&body=Title%3A%20PRPO%3A%20Paragraph-level%20Policy%20Optimization%20for%20Vision-Language%20Deepfake%0A%20%20Detection%0AAuthor%3A%20Tuan%20Nguyen%20and%20Naseem%20Khan%20and%20Khang%20Tran%20and%20NhatHai%20Phan%20and%20Issa%20Khalil%0AAbstract%3A%20%20%20The%20rapid%20rise%20of%20synthetic%20media%20has%20made%20deepfake%20detection%20a%20critical%0Achallenge%20for%20online%20safety%20and%20trust.%20Progress%20remains%20constrained%20by%20the%0Ascarcity%20of%20large%2C%20high-quality%20datasets.%20Although%20multimodal%20large%20language%0Amodels%20%28LLMs%29%20exhibit%20strong%20reasoning%20capabilities%2C%20their%20performance%20on%0Adeepfake%20detection%20is%20poor%2C%20often%20producing%20explanations%20that%20are%20misaligned%0Awith%20visual%20evidence%20or%20hallucinatory.%20To%20address%20this%20limitation%2C%20we%20introduce%0Aa%20reasoning-annotated%20dataset%20for%20deepfake%20detection%20and%20propose%0AParagraph-level%20Relative%20Policy%20Optimization%20%28PRPO%29%2C%20a%20reinforcement%20learning%0Aalgorithm%20that%20aligns%20LLM%20reasoning%20with%20image%20content%20at%20the%20paragraph%20level.%0AExperiments%20show%20that%20PRPO%20improves%20detection%20accuracy%20by%20a%20wide%20margin%20and%0Aachieves%20the%20highest%20reasoning%20score%20of%204.55/5.0.%20Ablation%20studies%20further%0Ademonstrate%20that%20PRPO%20significantly%20outperforms%20GRPO%20under%20test-time%0Aconditions.%20These%20results%20underscore%20the%20importance%20of%20grounding%20multimodal%0Areasoning%20in%20visual%20evidence%20to%20enable%20more%20reliable%20and%20interpretable%20deepfake%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRPO%253A%2520Paragraph-level%2520Policy%2520Optimization%2520for%2520Vision-Language%2520Deepfake%250A%2520%2520Detection%26entry.906535625%3DTuan%2520Nguyen%2520and%2520Naseem%2520Khan%2520and%2520Khang%2520Tran%2520and%2520NhatHai%2520Phan%2520and%2520Issa%2520Khalil%26entry.1292438233%3D%2520%2520The%2520rapid%2520rise%2520of%2520synthetic%2520media%2520has%2520made%2520deepfake%2520detection%2520a%2520critical%250Achallenge%2520for%2520online%2520safety%2520and%2520trust.%2520Progress%2520remains%2520constrained%2520by%2520the%250Ascarcity%2520of%2520large%252C%2520high-quality%2520datasets.%2520Although%2520multimodal%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520exhibit%2520strong%2520reasoning%2520capabilities%252C%2520their%2520performance%2520on%250Adeepfake%2520detection%2520is%2520poor%252C%2520often%2520producing%2520explanations%2520that%2520are%2520misaligned%250Awith%2520visual%2520evidence%2520or%2520hallucinatory.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%250Aa%2520reasoning-annotated%2520dataset%2520for%2520deepfake%2520detection%2520and%2520propose%250AParagraph-level%2520Relative%2520Policy%2520Optimization%2520%2528PRPO%2529%252C%2520a%2520reinforcement%2520learning%250Aalgorithm%2520that%2520aligns%2520LLM%2520reasoning%2520with%2520image%2520content%2520at%2520the%2520paragraph%2520level.%250AExperiments%2520show%2520that%2520PRPO%2520improves%2520detection%2520accuracy%2520by%2520a%2520wide%2520margin%2520and%250Aachieves%2520the%2520highest%2520reasoning%2520score%2520of%25204.55/5.0.%2520Ablation%2520studies%2520further%250Ademonstrate%2520that%2520PRPO%2520significantly%2520outperforms%2520GRPO%2520under%2520test-time%250Aconditions.%2520These%2520results%2520underscore%2520the%2520importance%2520of%2520grounding%2520multimodal%250Areasoning%2520in%2520visual%2520evidence%2520to%2520enable%2520more%2520reliable%2520and%2520interpretable%2520deepfake%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRPO%3A%20Paragraph-level%20Policy%20Optimization%20for%20Vision-Language%20Deepfake%0A%20%20Detection&entry.906535625=Tuan%20Nguyen%20and%20Naseem%20Khan%20and%20Khang%20Tran%20and%20NhatHai%20Phan%20and%20Issa%20Khalil&entry.1292438233=%20%20The%20rapid%20rise%20of%20synthetic%20media%20has%20made%20deepfake%20detection%20a%20critical%0Achallenge%20for%20online%20safety%20and%20trust.%20Progress%20remains%20constrained%20by%20the%0Ascarcity%20of%20large%2C%20high-quality%20datasets.%20Although%20multimodal%20large%20language%0Amodels%20%28LLMs%29%20exhibit%20strong%20reasoning%20capabilities%2C%20their%20performance%20on%0Adeepfake%20detection%20is%20poor%2C%20often%20producing%20explanations%20that%20are%20misaligned%0Awith%20visual%20evidence%20or%20hallucinatory.%20To%20address%20this%20limitation%2C%20we%20introduce%0Aa%20reasoning-annotated%20dataset%20for%20deepfake%20detection%20and%20propose%0AParagraph-level%20Relative%20Policy%20Optimization%20%28PRPO%29%2C%20a%20reinforcement%20learning%0Aalgorithm%20that%20aligns%20LLM%20reasoning%20with%20image%20content%20at%20the%20paragraph%20level.%0AExperiments%20show%20that%20PRPO%20improves%20detection%20accuracy%20by%20a%20wide%20margin%20and%0Aachieves%20the%20highest%20reasoning%20score%20of%204.55/5.0.%20Ablation%20studies%20further%0Ademonstrate%20that%20PRPO%20significantly%20outperforms%20GRPO%20under%20test-time%0Aconditions.%20These%20results%20underscore%20the%20importance%20of%20grounding%20multimodal%0Areasoning%20in%20visual%20evidence%20to%20enable%20more%20reliable%20and%20interpretable%20deepfake%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26272v2&entry.124074799=Read"},
{"title": "First Hallucination Tokens Are Different from Conditional Ones", "author": "Jakob Snel and Seong Joon Oh", "abstract": "  Hallucination, the generation of untruthful content, is one of the major\nconcerns regarding foundational models. Detecting hallucinations at the token\nlevel is vital for real-time filtering and targeted correction, yet the\nvariation of hallucination signals within token sequences is not fully\nunderstood. Leveraging the RAGTruth corpus with token-level annotations and\nreproduced logits, we analyse how these signals depend on a token's position\nwithin hallucinated spans, contributing to an improved understanding of\ntoken-level hallucination. Our results show that the first hallucinated token\ncarries a stronger signal and is more detectable than conditional tokens. We\nrelease our analysis framework, along with code for logit reproduction and\nmetric computation at https://github.com/jakobsnl/RAGTruth\\_Xtended.\n", "link": "http://arxiv.org/abs/2507.20836v2", "date": "2025-10-01", "relevancy": 2.1566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4431}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones&body=Title%3A%20First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones%0AAuthor%3A%20Jakob%20Snel%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Hallucination%2C%20the%20generation%20of%20untruthful%20content%2C%20is%20one%20of%20the%20major%0Aconcerns%20regarding%20foundational%20models.%20Detecting%20hallucinations%20at%20the%20token%0Alevel%20is%20vital%20for%20real-time%20filtering%20and%20targeted%20correction%2C%20yet%20the%0Avariation%20of%20hallucination%20signals%20within%20token%20sequences%20is%20not%20fully%0Aunderstood.%20Leveraging%20the%20RAGTruth%20corpus%20with%20token-level%20annotations%20and%0Areproduced%20logits%2C%20we%20analyse%20how%20these%20signals%20depend%20on%20a%20token%27s%20position%0Awithin%20hallucinated%20spans%2C%20contributing%20to%20an%20improved%20understanding%20of%0Atoken-level%20hallucination.%20Our%20results%20show%20that%20the%20first%20hallucinated%20token%0Acarries%20a%20stronger%20signal%20and%20is%20more%20detectable%20than%20conditional%20tokens.%20We%0Arelease%20our%20analysis%20framework%2C%20along%20with%20code%20for%20logit%20reproduction%20and%0Ametric%20computation%20at%20https%3A//github.com/jakobsnl/RAGTruth%5C_Xtended.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%2520Hallucination%2520Tokens%2520Are%2520Different%2520from%2520Conditional%2520Ones%26entry.906535625%3DJakob%2520Snel%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Hallucination%252C%2520the%2520generation%2520of%2520untruthful%2520content%252C%2520is%2520one%2520of%2520the%2520major%250Aconcerns%2520regarding%2520foundational%2520models.%2520Detecting%2520hallucinations%2520at%2520the%2520token%250Alevel%2520is%2520vital%2520for%2520real-time%2520filtering%2520and%2520targeted%2520correction%252C%2520yet%2520the%250Avariation%2520of%2520hallucination%2520signals%2520within%2520token%2520sequences%2520is%2520not%2520fully%250Aunderstood.%2520Leveraging%2520the%2520RAGTruth%2520corpus%2520with%2520token-level%2520annotations%2520and%250Areproduced%2520logits%252C%2520we%2520analyse%2520how%2520these%2520signals%2520depend%2520on%2520a%2520token%2527s%2520position%250Awithin%2520hallucinated%2520spans%252C%2520contributing%2520to%2520an%2520improved%2520understanding%2520of%250Atoken-level%2520hallucination.%2520Our%2520results%2520show%2520that%2520the%2520first%2520hallucinated%2520token%250Acarries%2520a%2520stronger%2520signal%2520and%2520is%2520more%2520detectable%2520than%2520conditional%2520tokens.%2520We%250Arelease%2520our%2520analysis%2520framework%252C%2520along%2520with%2520code%2520for%2520logit%2520reproduction%2520and%250Ametric%2520computation%2520at%2520https%253A//github.com/jakobsnl/RAGTruth%255C_Xtended.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones&entry.906535625=Jakob%20Snel%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Hallucination%2C%20the%20generation%20of%20untruthful%20content%2C%20is%20one%20of%20the%20major%0Aconcerns%20regarding%20foundational%20models.%20Detecting%20hallucinations%20at%20the%20token%0Alevel%20is%20vital%20for%20real-time%20filtering%20and%20targeted%20correction%2C%20yet%20the%0Avariation%20of%20hallucination%20signals%20within%20token%20sequences%20is%20not%20fully%0Aunderstood.%20Leveraging%20the%20RAGTruth%20corpus%20with%20token-level%20annotations%20and%0Areproduced%20logits%2C%20we%20analyse%20how%20these%20signals%20depend%20on%20a%20token%27s%20position%0Awithin%20hallucinated%20spans%2C%20contributing%20to%20an%20improved%20understanding%20of%0Atoken-level%20hallucination.%20Our%20results%20show%20that%20the%20first%20hallucinated%20token%0Acarries%20a%20stronger%20signal%20and%20is%20more%20detectable%20than%20conditional%20tokens.%20We%0Arelease%20our%20analysis%20framework%2C%20along%20with%20code%20for%20logit%20reproduction%20and%0Ametric%20computation%20at%20https%3A//github.com/jakobsnl/RAGTruth%5C_Xtended.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20836v2&entry.124074799=Read"},
{"title": "Grounded GUI Understanding for Vision-Based Spatial Intelligent Agent:\n  Exemplified by Extended Reality Apps", "author": "Shuqing Li and Binchang Li and Yepang Liu and Cuiyun Gao and Jianping Zhang and Shing-Chi Cheung and Michael R. Lyu", "abstract": "  In recent years, spatial computing a.k.a. Extended Reality (XR) has emerged\nas a transformative technology, offering users immersive and interactive\nexperiences across diversified virtual environments. Users can interact with XR\napps through interactable GUI elements (IGEs) on the stereoscopic\nthree-dimensional (3D) graphical user interface (GUI). The accurate recognition\nof these IGEs is instrumental, serving as the foundation of many software\nengineering tasks, including automated testing and effective GUI search. The\nmost recent IGE detection approaches for 2D mobile apps typically train a\nsupervised object detection model based on a large-scale manually-labeled GUI\ndataset, usually with a pre-defined set of clickable GUI element categories\nlike buttons and spinners. Such approaches can hardly be applied to IGE\ndetection in XR apps, due to a multitude of challenges including complexities\nposed by open-vocabulary and heterogeneous IGE categories, intricacies of\ncontext-sensitive interactability, and the necessities of precise spatial\nperception and visual-semantic alignment for accurate IGE detection results.\nThus, it is necessary to embark on the IGE research tailored to XR apps. In\nthis paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI\nElemeNT dEtection framework for virtual Reality apps, named Orienter. By\nimitating human behaviors, Orienter observes and understands the semantic\ncontexts of XR app scenes first, before performing the detection. The detection\nprocess is iterated within a feedback-directed validation and reflection loop.\nSpecifically, Orienter contains three components, including (1) Semantic\ncontext comprehension, (2) Reflection-directed IGE candidate detection, and (3)\nContext-sensitive interactability classification. Extensive experiments\ndemonstrate that Orienter is more effective than the state-of-the-art GUI\nelement detection approaches.\n", "link": "http://arxiv.org/abs/2409.10811v4", "date": "2025-10-01", "relevancy": 2.1196, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5411}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5308}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20GUI%20Understanding%20for%20Vision-Based%20Spatial%20Intelligent%20Agent%3A%0A%20%20Exemplified%20by%20Extended%20Reality%20Apps&body=Title%3A%20Grounded%20GUI%20Understanding%20for%20Vision-Based%20Spatial%20Intelligent%20Agent%3A%0A%20%20Exemplified%20by%20Extended%20Reality%20Apps%0AAuthor%3A%20Shuqing%20Li%20and%20Binchang%20Li%20and%20Yepang%20Liu%20and%20Cuiyun%20Gao%20and%20Jianping%20Zhang%20and%20Shing-Chi%20Cheung%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20spatial%20computing%20a.k.a.%20Extended%20Reality%20%28XR%29%20has%20emerged%0Aas%20a%20transformative%20technology%2C%20offering%20users%20immersive%20and%20interactive%0Aexperiences%20across%20diversified%20virtual%20environments.%20Users%20can%20interact%20with%20XR%0Aapps%20through%20interactable%20GUI%20elements%20%28IGEs%29%20on%20the%20stereoscopic%0Athree-dimensional%20%283D%29%20graphical%20user%20interface%20%28GUI%29.%20The%20accurate%20recognition%0Aof%20these%20IGEs%20is%20instrumental%2C%20serving%20as%20the%20foundation%20of%20many%20software%0Aengineering%20tasks%2C%20including%20automated%20testing%20and%20effective%20GUI%20search.%20The%0Amost%20recent%20IGE%20detection%20approaches%20for%202D%20mobile%20apps%20typically%20train%20a%0Asupervised%20object%20detection%20model%20based%20on%20a%20large-scale%20manually-labeled%20GUI%0Adataset%2C%20usually%20with%20a%20pre-defined%20set%20of%20clickable%20GUI%20element%20categories%0Alike%20buttons%20and%20spinners.%20Such%20approaches%20can%20hardly%20be%20applied%20to%20IGE%0Adetection%20in%20XR%20apps%2C%20due%20to%20a%20multitude%20of%20challenges%20including%20complexities%0Aposed%20by%20open-vocabulary%20and%20heterogeneous%20IGE%20categories%2C%20intricacies%20of%0Acontext-sensitive%20interactability%2C%20and%20the%20necessities%20of%20precise%20spatial%0Aperception%20and%20visual-semantic%20alignment%20for%20accurate%20IGE%20detection%20results.%0AThus%2C%20it%20is%20necessary%20to%20embark%20on%20the%20IGE%20research%20tailored%20to%20XR%20apps.%20In%0Athis%20paper%2C%20we%20propose%20the%20first%20zero-shot%20cOntext-sensitive%20inteRactable%20GUI%0AElemeNT%20dEtection%20framework%20for%20virtual%20Reality%20apps%2C%20named%20Orienter.%20By%0Aimitating%20human%20behaviors%2C%20Orienter%20observes%20and%20understands%20the%20semantic%0Acontexts%20of%20XR%20app%20scenes%20first%2C%20before%20performing%20the%20detection.%20The%20detection%0Aprocess%20is%20iterated%20within%20a%20feedback-directed%20validation%20and%20reflection%20loop.%0ASpecifically%2C%20Orienter%20contains%20three%20components%2C%20including%20%281%29%20Semantic%0Acontext%20comprehension%2C%20%282%29%20Reflection-directed%20IGE%20candidate%20detection%2C%20and%20%283%29%0AContext-sensitive%20interactability%20classification.%20Extensive%20experiments%0Ademonstrate%20that%20Orienter%20is%20more%20effective%20than%20the%20state-of-the-art%20GUI%0Aelement%20detection%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10811v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520GUI%2520Understanding%2520for%2520Vision-Based%2520Spatial%2520Intelligent%2520Agent%253A%250A%2520%2520Exemplified%2520by%2520Extended%2520Reality%2520Apps%26entry.906535625%3DShuqing%2520Li%2520and%2520Binchang%2520Li%2520and%2520Yepang%2520Liu%2520and%2520Cuiyun%2520Gao%2520and%2520Jianping%2520Zhang%2520and%2520Shing-Chi%2520Cheung%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520spatial%2520computing%2520a.k.a.%2520Extended%2520Reality%2520%2528XR%2529%2520has%2520emerged%250Aas%2520a%2520transformative%2520technology%252C%2520offering%2520users%2520immersive%2520and%2520interactive%250Aexperiences%2520across%2520diversified%2520virtual%2520environments.%2520Users%2520can%2520interact%2520with%2520XR%250Aapps%2520through%2520interactable%2520GUI%2520elements%2520%2528IGEs%2529%2520on%2520the%2520stereoscopic%250Athree-dimensional%2520%25283D%2529%2520graphical%2520user%2520interface%2520%2528GUI%2529.%2520The%2520accurate%2520recognition%250Aof%2520these%2520IGEs%2520is%2520instrumental%252C%2520serving%2520as%2520the%2520foundation%2520of%2520many%2520software%250Aengineering%2520tasks%252C%2520including%2520automated%2520testing%2520and%2520effective%2520GUI%2520search.%2520The%250Amost%2520recent%2520IGE%2520detection%2520approaches%2520for%25202D%2520mobile%2520apps%2520typically%2520train%2520a%250Asupervised%2520object%2520detection%2520model%2520based%2520on%2520a%2520large-scale%2520manually-labeled%2520GUI%250Adataset%252C%2520usually%2520with%2520a%2520pre-defined%2520set%2520of%2520clickable%2520GUI%2520element%2520categories%250Alike%2520buttons%2520and%2520spinners.%2520Such%2520approaches%2520can%2520hardly%2520be%2520applied%2520to%2520IGE%250Adetection%2520in%2520XR%2520apps%252C%2520due%2520to%2520a%2520multitude%2520of%2520challenges%2520including%2520complexities%250Aposed%2520by%2520open-vocabulary%2520and%2520heterogeneous%2520IGE%2520categories%252C%2520intricacies%2520of%250Acontext-sensitive%2520interactability%252C%2520and%2520the%2520necessities%2520of%2520precise%2520spatial%250Aperception%2520and%2520visual-semantic%2520alignment%2520for%2520accurate%2520IGE%2520detection%2520results.%250AThus%252C%2520it%2520is%2520necessary%2520to%2520embark%2520on%2520the%2520IGE%2520research%2520tailored%2520to%2520XR%2520apps.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520the%2520first%2520zero-shot%2520cOntext-sensitive%2520inteRactable%2520GUI%250AElemeNT%2520dEtection%2520framework%2520for%2520virtual%2520Reality%2520apps%252C%2520named%2520Orienter.%2520By%250Aimitating%2520human%2520behaviors%252C%2520Orienter%2520observes%2520and%2520understands%2520the%2520semantic%250Acontexts%2520of%2520XR%2520app%2520scenes%2520first%252C%2520before%2520performing%2520the%2520detection.%2520The%2520detection%250Aprocess%2520is%2520iterated%2520within%2520a%2520feedback-directed%2520validation%2520and%2520reflection%2520loop.%250ASpecifically%252C%2520Orienter%2520contains%2520three%2520components%252C%2520including%2520%25281%2529%2520Semantic%250Acontext%2520comprehension%252C%2520%25282%2529%2520Reflection-directed%2520IGE%2520candidate%2520detection%252C%2520and%2520%25283%2529%250AContext-sensitive%2520interactability%2520classification.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Orienter%2520is%2520more%2520effective%2520than%2520the%2520state-of-the-art%2520GUI%250Aelement%2520detection%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10811v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20GUI%20Understanding%20for%20Vision-Based%20Spatial%20Intelligent%20Agent%3A%0A%20%20Exemplified%20by%20Extended%20Reality%20Apps&entry.906535625=Shuqing%20Li%20and%20Binchang%20Li%20and%20Yepang%20Liu%20and%20Cuiyun%20Gao%20and%20Jianping%20Zhang%20and%20Shing-Chi%20Cheung%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20In%20recent%20years%2C%20spatial%20computing%20a.k.a.%20Extended%20Reality%20%28XR%29%20has%20emerged%0Aas%20a%20transformative%20technology%2C%20offering%20users%20immersive%20and%20interactive%0Aexperiences%20across%20diversified%20virtual%20environments.%20Users%20can%20interact%20with%20XR%0Aapps%20through%20interactable%20GUI%20elements%20%28IGEs%29%20on%20the%20stereoscopic%0Athree-dimensional%20%283D%29%20graphical%20user%20interface%20%28GUI%29.%20The%20accurate%20recognition%0Aof%20these%20IGEs%20is%20instrumental%2C%20serving%20as%20the%20foundation%20of%20many%20software%0Aengineering%20tasks%2C%20including%20automated%20testing%20and%20effective%20GUI%20search.%20The%0Amost%20recent%20IGE%20detection%20approaches%20for%202D%20mobile%20apps%20typically%20train%20a%0Asupervised%20object%20detection%20model%20based%20on%20a%20large-scale%20manually-labeled%20GUI%0Adataset%2C%20usually%20with%20a%20pre-defined%20set%20of%20clickable%20GUI%20element%20categories%0Alike%20buttons%20and%20spinners.%20Such%20approaches%20can%20hardly%20be%20applied%20to%20IGE%0Adetection%20in%20XR%20apps%2C%20due%20to%20a%20multitude%20of%20challenges%20including%20complexities%0Aposed%20by%20open-vocabulary%20and%20heterogeneous%20IGE%20categories%2C%20intricacies%20of%0Acontext-sensitive%20interactability%2C%20and%20the%20necessities%20of%20precise%20spatial%0Aperception%20and%20visual-semantic%20alignment%20for%20accurate%20IGE%20detection%20results.%0AThus%2C%20it%20is%20necessary%20to%20embark%20on%20the%20IGE%20research%20tailored%20to%20XR%20apps.%20In%0Athis%20paper%2C%20we%20propose%20the%20first%20zero-shot%20cOntext-sensitive%20inteRactable%20GUI%0AElemeNT%20dEtection%20framework%20for%20virtual%20Reality%20apps%2C%20named%20Orienter.%20By%0Aimitating%20human%20behaviors%2C%20Orienter%20observes%20and%20understands%20the%20semantic%0Acontexts%20of%20XR%20app%20scenes%20first%2C%20before%20performing%20the%20detection.%20The%20detection%0Aprocess%20is%20iterated%20within%20a%20feedback-directed%20validation%20and%20reflection%20loop.%0ASpecifically%2C%20Orienter%20contains%20three%20components%2C%20including%20%281%29%20Semantic%0Acontext%20comprehension%2C%20%282%29%20Reflection-directed%20IGE%20candidate%20detection%2C%20and%20%283%29%0AContext-sensitive%20interactability%20classification.%20Extensive%20experiments%0Ademonstrate%20that%20Orienter%20is%20more%20effective%20than%20the%20state-of-the-art%20GUI%0Aelement%20detection%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10811v4&entry.124074799=Read"},
{"title": "HetSwarm: Cooperative Navigation of Heterogeneous Swarm in Dynamic and\n  Dense Environments through Impedance-based Guidance", "author": "Malaika Zafar and Roohan Ahmed Khan and Aleksey Fedoseev and Kumar Katyayan Jaiswal and Dzmitry Tsetserukou", "abstract": "  With the growing demand for efficient logistics and warehouse management,\nunmanned aerial vehicles (UAVs) are emerging as a valuable complement to\nautomated guided vehicles (AGVs). UAVs enhance efficiency by navigating dense\nenvironments and operating at varying altitudes. However, their limited flight\ntime, battery life, and payload capacity necessitate a supporting ground\nstation. To address these challenges, we propose HetSwarm, a heterogeneous\nmulti-robot system that combines a UAV and a mobile ground robot for\ncollaborative navigation in cluttered and dynamic conditions. Our approach\nemploys an artificial potential field (APF)-based path planner for the UAV,\nallowing it to dynamically adjust its trajectory in real time. The ground robot\nfollows this path while maintaining connectivity through impedance links,\nensuring stable coordination. Additionally, the ground robot establishes\ntemporal impedance links with low-height ground obstacles to avoid local\ncollisions, as these obstacles do not interfere with the UAV's flight.\nExperimental validation of HetSwarm in diverse environmental conditions\ndemonstrated a 90% success rate across 30 test cases. The ground robot\nexhibited an average deviation of 45 cm near obstacles, confirming effective\ncollision avoidance. Extensive simulations in the Gym PyBullet environment\nfurther validated the robustness of our system for real-world applications,\ndemonstrating its potential for dynamic, real-time task execution in cluttered\nenvironments.\n", "link": "http://arxiv.org/abs/2502.06722v2", "date": "2025-10-01", "relevancy": 2.1067, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HetSwarm%3A%20Cooperative%20Navigation%20of%20Heterogeneous%20Swarm%20in%20Dynamic%20and%0A%20%20Dense%20Environments%20through%20Impedance-based%20Guidance&body=Title%3A%20HetSwarm%3A%20Cooperative%20Navigation%20of%20Heterogeneous%20Swarm%20in%20Dynamic%20and%0A%20%20Dense%20Environments%20through%20Impedance-based%20Guidance%0AAuthor%3A%20Malaika%20Zafar%20and%20Roohan%20Ahmed%20Khan%20and%20Aleksey%20Fedoseev%20and%20Kumar%20Katyayan%20Jaiswal%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20With%20the%20growing%20demand%20for%20efficient%20logistics%20and%20warehouse%20management%2C%0Aunmanned%20aerial%20vehicles%20%28UAVs%29%20are%20emerging%20as%20a%20valuable%20complement%20to%0Aautomated%20guided%20vehicles%20%28AGVs%29.%20UAVs%20enhance%20efficiency%20by%20navigating%20dense%0Aenvironments%20and%20operating%20at%20varying%20altitudes.%20However%2C%20their%20limited%20flight%0Atime%2C%20battery%20life%2C%20and%20payload%20capacity%20necessitate%20a%20supporting%20ground%0Astation.%20To%20address%20these%20challenges%2C%20we%20propose%20HetSwarm%2C%20a%20heterogeneous%0Amulti-robot%20system%20that%20combines%20a%20UAV%20and%20a%20mobile%20ground%20robot%20for%0Acollaborative%20navigation%20in%20cluttered%20and%20dynamic%20conditions.%20Our%20approach%0Aemploys%20an%20artificial%20potential%20field%20%28APF%29-based%20path%20planner%20for%20the%20UAV%2C%0Aallowing%20it%20to%20dynamically%20adjust%20its%20trajectory%20in%20real%20time.%20The%20ground%20robot%0Afollows%20this%20path%20while%20maintaining%20connectivity%20through%20impedance%20links%2C%0Aensuring%20stable%20coordination.%20Additionally%2C%20the%20ground%20robot%20establishes%0Atemporal%20impedance%20links%20with%20low-height%20ground%20obstacles%20to%20avoid%20local%0Acollisions%2C%20as%20these%20obstacles%20do%20not%20interfere%20with%20the%20UAV%27s%20flight.%0AExperimental%20validation%20of%20HetSwarm%20in%20diverse%20environmental%20conditions%0Ademonstrated%20a%2090%25%20success%20rate%20across%2030%20test%20cases.%20The%20ground%20robot%0Aexhibited%20an%20average%20deviation%20of%2045%20cm%20near%20obstacles%2C%20confirming%20effective%0Acollision%20avoidance.%20Extensive%20simulations%20in%20the%20Gym%20PyBullet%20environment%0Afurther%20validated%20the%20robustness%20of%20our%20system%20for%20real-world%20applications%2C%0Ademonstrating%20its%20potential%20for%20dynamic%2C%20real-time%20task%20execution%20in%20cluttered%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHetSwarm%253A%2520Cooperative%2520Navigation%2520of%2520Heterogeneous%2520Swarm%2520in%2520Dynamic%2520and%250A%2520%2520Dense%2520Environments%2520through%2520Impedance-based%2520Guidance%26entry.906535625%3DMalaika%2520Zafar%2520and%2520Roohan%2520Ahmed%2520Khan%2520and%2520Aleksey%2520Fedoseev%2520and%2520Kumar%2520Katyayan%2520Jaiswal%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520demand%2520for%2520efficient%2520logistics%2520and%2520warehouse%2520management%252C%250Aunmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520are%2520emerging%2520as%2520a%2520valuable%2520complement%2520to%250Aautomated%2520guided%2520vehicles%2520%2528AGVs%2529.%2520UAVs%2520enhance%2520efficiency%2520by%2520navigating%2520dense%250Aenvironments%2520and%2520operating%2520at%2520varying%2520altitudes.%2520However%252C%2520their%2520limited%2520flight%250Atime%252C%2520battery%2520life%252C%2520and%2520payload%2520capacity%2520necessitate%2520a%2520supporting%2520ground%250Astation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520HetSwarm%252C%2520a%2520heterogeneous%250Amulti-robot%2520system%2520that%2520combines%2520a%2520UAV%2520and%2520a%2520mobile%2520ground%2520robot%2520for%250Acollaborative%2520navigation%2520in%2520cluttered%2520and%2520dynamic%2520conditions.%2520Our%2520approach%250Aemploys%2520an%2520artificial%2520potential%2520field%2520%2528APF%2529-based%2520path%2520planner%2520for%2520the%2520UAV%252C%250Aallowing%2520it%2520to%2520dynamically%2520adjust%2520its%2520trajectory%2520in%2520real%2520time.%2520The%2520ground%2520robot%250Afollows%2520this%2520path%2520while%2520maintaining%2520connectivity%2520through%2520impedance%2520links%252C%250Aensuring%2520stable%2520coordination.%2520Additionally%252C%2520the%2520ground%2520robot%2520establishes%250Atemporal%2520impedance%2520links%2520with%2520low-height%2520ground%2520obstacles%2520to%2520avoid%2520local%250Acollisions%252C%2520as%2520these%2520obstacles%2520do%2520not%2520interfere%2520with%2520the%2520UAV%2527s%2520flight.%250AExperimental%2520validation%2520of%2520HetSwarm%2520in%2520diverse%2520environmental%2520conditions%250Ademonstrated%2520a%252090%2525%2520success%2520rate%2520across%252030%2520test%2520cases.%2520The%2520ground%2520robot%250Aexhibited%2520an%2520average%2520deviation%2520of%252045%2520cm%2520near%2520obstacles%252C%2520confirming%2520effective%250Acollision%2520avoidance.%2520Extensive%2520simulations%2520in%2520the%2520Gym%2520PyBullet%2520environment%250Afurther%2520validated%2520the%2520robustness%2520of%2520our%2520system%2520for%2520real-world%2520applications%252C%250Ademonstrating%2520its%2520potential%2520for%2520dynamic%252C%2520real-time%2520task%2520execution%2520in%2520cluttered%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HetSwarm%3A%20Cooperative%20Navigation%20of%20Heterogeneous%20Swarm%20in%20Dynamic%20and%0A%20%20Dense%20Environments%20through%20Impedance-based%20Guidance&entry.906535625=Malaika%20Zafar%20and%20Roohan%20Ahmed%20Khan%20and%20Aleksey%20Fedoseev%20and%20Kumar%20Katyayan%20Jaiswal%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20With%20the%20growing%20demand%20for%20efficient%20logistics%20and%20warehouse%20management%2C%0Aunmanned%20aerial%20vehicles%20%28UAVs%29%20are%20emerging%20as%20a%20valuable%20complement%20to%0Aautomated%20guided%20vehicles%20%28AGVs%29.%20UAVs%20enhance%20efficiency%20by%20navigating%20dense%0Aenvironments%20and%20operating%20at%20varying%20altitudes.%20However%2C%20their%20limited%20flight%0Atime%2C%20battery%20life%2C%20and%20payload%20capacity%20necessitate%20a%20supporting%20ground%0Astation.%20To%20address%20these%20challenges%2C%20we%20propose%20HetSwarm%2C%20a%20heterogeneous%0Amulti-robot%20system%20that%20combines%20a%20UAV%20and%20a%20mobile%20ground%20robot%20for%0Acollaborative%20navigation%20in%20cluttered%20and%20dynamic%20conditions.%20Our%20approach%0Aemploys%20an%20artificial%20potential%20field%20%28APF%29-based%20path%20planner%20for%20the%20UAV%2C%0Aallowing%20it%20to%20dynamically%20adjust%20its%20trajectory%20in%20real%20time.%20The%20ground%20robot%0Afollows%20this%20path%20while%20maintaining%20connectivity%20through%20impedance%20links%2C%0Aensuring%20stable%20coordination.%20Additionally%2C%20the%20ground%20robot%20establishes%0Atemporal%20impedance%20links%20with%20low-height%20ground%20obstacles%20to%20avoid%20local%0Acollisions%2C%20as%20these%20obstacles%20do%20not%20interfere%20with%20the%20UAV%27s%20flight.%0AExperimental%20validation%20of%20HetSwarm%20in%20diverse%20environmental%20conditions%0Ademonstrated%20a%2090%25%20success%20rate%20across%2030%20test%20cases.%20The%20ground%20robot%0Aexhibited%20an%20average%20deviation%20of%2045%20cm%20near%20obstacles%2C%20confirming%20effective%0Acollision%20avoidance.%20Extensive%20simulations%20in%20the%20Gym%20PyBullet%20environment%0Afurther%20validated%20the%20robustness%20of%20our%20system%20for%20real-world%20applications%2C%0Ademonstrating%20its%20potential%20for%20dynamic%2C%20real-time%20task%20execution%20in%20cluttered%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06722v2&entry.124074799=Read"},
{"title": "Fully Heteroscedastic Count Regression with Deep Double Poisson Networks", "author": "Spencer Young and Porter Jenkins and Longchao Da and Jeff Dotson and Hua Wei", "abstract": "  Neural networks capable of accurate, input-conditional uncertainty\nrepresentation are essential for real-world AI systems. Deep ensembles of\nGaussian networks have proven highly effective for continuous regression due to\ntheir ability to flexibly represent aleatoric uncertainty via unrestricted\nheteroscedastic variance, which in turn enables accurate epistemic uncertainty\nestimation. However, no analogous approach exists for count regression, despite\nmany important applications. To address this gap, we propose the Deep Double\nPoisson Network (DDPN), a novel neural discrete count regression model that\noutputs the parameters of the Double Poisson distribution, enabling arbitrarily\nhigh or low predictive aleatoric uncertainty for count data and improving\nepistemic uncertainty estimation when ensembled. We formalize and prove that\nDDPN exhibits robust regression properties similar to heteroscedastic Gaussian\nmodels via learnable loss attenuation, and introduce a simple loss modification\nto control this behavior. Experiments on diverse datasets demonstrate that DDPN\noutperforms current baselines in accuracy, calibration, and out-of-distribution\ndetection, establishing a new state-of-the-art in deep count regression.\n", "link": "http://arxiv.org/abs/2406.09262v5", "date": "2025-10-01", "relevancy": 2.0945, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5598}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4991}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks&body=Title%3A%20Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks%0AAuthor%3A%20Spencer%20Young%20and%20Porter%20Jenkins%20and%20Longchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei%0AAbstract%3A%20%20%20Neural%20networks%20capable%20of%20accurate%2C%20input-conditional%20uncertainty%0Arepresentation%20are%20essential%20for%20real-world%20AI%20systems.%20Deep%20ensembles%20of%0AGaussian%20networks%20have%20proven%20highly%20effective%20for%20continuous%20regression%20due%20to%0Atheir%20ability%20to%20flexibly%20represent%20aleatoric%20uncertainty%20via%20unrestricted%0Aheteroscedastic%20variance%2C%20which%20in%20turn%20enables%20accurate%20epistemic%20uncertainty%0Aestimation.%20However%2C%20no%20analogous%20approach%20exists%20for%20count%20regression%2C%20despite%0Amany%20important%20applications.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Deep%20Double%0APoisson%20Network%20%28DDPN%29%2C%20a%20novel%20neural%20discrete%20count%20regression%20model%20that%0Aoutputs%20the%20parameters%20of%20the%20Double%20Poisson%20distribution%2C%20enabling%20arbitrarily%0Ahigh%20or%20low%20predictive%20aleatoric%20uncertainty%20for%20count%20data%20and%20improving%0Aepistemic%20uncertainty%20estimation%20when%20ensembled.%20We%20formalize%20and%20prove%20that%0ADDPN%20exhibits%20robust%20regression%20properties%20similar%20to%20heteroscedastic%20Gaussian%0Amodels%20via%20learnable%20loss%20attenuation%2C%20and%20introduce%20a%20simple%20loss%20modification%0Ato%20control%20this%20behavior.%20Experiments%20on%20diverse%20datasets%20demonstrate%20that%20DDPN%0Aoutperforms%20current%20baselines%20in%20accuracy%2C%20calibration%2C%20and%20out-of-distribution%0Adetection%2C%20establishing%20a%20new%20state-of-the-art%20in%20deep%20count%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09262v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Heteroscedastic%2520Count%2520Regression%2520with%2520Deep%2520Double%2520Poisson%2520Networks%26entry.906535625%3DSpencer%2520Young%2520and%2520Porter%2520Jenkins%2520and%2520Longchao%2520Da%2520and%2520Jeff%2520Dotson%2520and%2520Hua%2520Wei%26entry.1292438233%3D%2520%2520Neural%2520networks%2520capable%2520of%2520accurate%252C%2520input-conditional%2520uncertainty%250Arepresentation%2520are%2520essential%2520for%2520real-world%2520AI%2520systems.%2520Deep%2520ensembles%2520of%250AGaussian%2520networks%2520have%2520proven%2520highly%2520effective%2520for%2520continuous%2520regression%2520due%2520to%250Atheir%2520ability%2520to%2520flexibly%2520represent%2520aleatoric%2520uncertainty%2520via%2520unrestricted%250Aheteroscedastic%2520variance%252C%2520which%2520in%2520turn%2520enables%2520accurate%2520epistemic%2520uncertainty%250Aestimation.%2520However%252C%2520no%2520analogous%2520approach%2520exists%2520for%2520count%2520regression%252C%2520despite%250Amany%2520important%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%2520Deep%2520Double%250APoisson%2520Network%2520%2528DDPN%2529%252C%2520a%2520novel%2520neural%2520discrete%2520count%2520regression%2520model%2520that%250Aoutputs%2520the%2520parameters%2520of%2520the%2520Double%2520Poisson%2520distribution%252C%2520enabling%2520arbitrarily%250Ahigh%2520or%2520low%2520predictive%2520aleatoric%2520uncertainty%2520for%2520count%2520data%2520and%2520improving%250Aepistemic%2520uncertainty%2520estimation%2520when%2520ensembled.%2520We%2520formalize%2520and%2520prove%2520that%250ADDPN%2520exhibits%2520robust%2520regression%2520properties%2520similar%2520to%2520heteroscedastic%2520Gaussian%250Amodels%2520via%2520learnable%2520loss%2520attenuation%252C%2520and%2520introduce%2520a%2520simple%2520loss%2520modification%250Ato%2520control%2520this%2520behavior.%2520Experiments%2520on%2520diverse%2520datasets%2520demonstrate%2520that%2520DDPN%250Aoutperforms%2520current%2520baselines%2520in%2520accuracy%252C%2520calibration%252C%2520and%2520out-of-distribution%250Adetection%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%2520deep%2520count%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09262v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Heteroscedastic%20Count%20Regression%20with%20Deep%20Double%20Poisson%20Networks&entry.906535625=Spencer%20Young%20and%20Porter%20Jenkins%20and%20Longchao%20Da%20and%20Jeff%20Dotson%20and%20Hua%20Wei&entry.1292438233=%20%20Neural%20networks%20capable%20of%20accurate%2C%20input-conditional%20uncertainty%0Arepresentation%20are%20essential%20for%20real-world%20AI%20systems.%20Deep%20ensembles%20of%0AGaussian%20networks%20have%20proven%20highly%20effective%20for%20continuous%20regression%20due%20to%0Atheir%20ability%20to%20flexibly%20represent%20aleatoric%20uncertainty%20via%20unrestricted%0Aheteroscedastic%20variance%2C%20which%20in%20turn%20enables%20accurate%20epistemic%20uncertainty%0Aestimation.%20However%2C%20no%20analogous%20approach%20exists%20for%20count%20regression%2C%20despite%0Amany%20important%20applications.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Deep%20Double%0APoisson%20Network%20%28DDPN%29%2C%20a%20novel%20neural%20discrete%20count%20regression%20model%20that%0Aoutputs%20the%20parameters%20of%20the%20Double%20Poisson%20distribution%2C%20enabling%20arbitrarily%0Ahigh%20or%20low%20predictive%20aleatoric%20uncertainty%20for%20count%20data%20and%20improving%0Aepistemic%20uncertainty%20estimation%20when%20ensembled.%20We%20formalize%20and%20prove%20that%0ADDPN%20exhibits%20robust%20regression%20properties%20similar%20to%20heteroscedastic%20Gaussian%0Amodels%20via%20learnable%20loss%20attenuation%2C%20and%20introduce%20a%20simple%20loss%20modification%0Ato%20control%20this%20behavior.%20Experiments%20on%20diverse%20datasets%20demonstrate%20that%20DDPN%0Aoutperforms%20current%20baselines%20in%20accuracy%2C%20calibration%2C%20and%20out-of-distribution%0Adetection%2C%20establishing%20a%20new%20state-of-the-art%20in%20deep%20count%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09262v5&entry.124074799=Read"},
{"title": "Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey", "author": "Chih-Kai Yang and Neo S. Ho and Hung-yi Lee", "abstract": "  With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.\n", "link": "http://arxiv.org/abs/2505.15957v3", "date": "2025-10-01", "relevancy": 2.087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Holistic%20Evaluation%20of%20Large%20Audio-Language%20Models%3A%20A%0A%20%20Comprehensive%20Survey&body=Title%3A%20Towards%20Holistic%20Evaluation%20of%20Large%20Audio-Language%20Models%3A%20A%0A%20%20Comprehensive%20Survey%0AAuthor%3A%20Chih-Kai%20Yang%20and%20Neo%20S.%20Ho%20and%20Hung-yi%20Lee%0AAbstract%3A%20%20%20With%20advancements%20in%20large%20audio-language%20models%20%28LALMs%29%2C%20which%20enhance%20large%0Alanguage%20models%20%28LLMs%29%20with%20auditory%20capabilities%2C%20these%20models%20are%20expected%20to%0Ademonstrate%20universal%20proficiency%20across%20various%20auditory%20tasks.%20While%20numerous%0Abenchmarks%20have%20emerged%20to%20assess%20LALMs%27%20performance%2C%20they%20remain%20fragmented%0Aand%20lack%20a%20structured%20taxonomy.%20To%20bridge%20this%20gap%2C%20we%20conduct%20a%20comprehensive%0Asurvey%20and%20propose%20a%20systematic%20taxonomy%20for%20LALM%20evaluations%2C%20categorizing%0Athem%20into%20four%20dimensions%20based%20on%20their%20objectives%3A%20%281%29%20General%20Auditory%0AAwareness%20and%20Processing%2C%20%282%29%20Knowledge%20and%20Reasoning%2C%20%283%29%20Dialogue-oriented%0AAbility%2C%20and%20%284%29%20Fairness%2C%20Safety%2C%20and%20Trustworthiness.%20We%20provide%20detailed%0Aoverviews%20within%20each%20category%20and%20highlight%20challenges%20in%20this%20field%2C%20offering%0Ainsights%20into%20promising%20future%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20survey%20specifically%20focused%20on%20the%20evaluations%20of%20LALMs%2C%20providing%0Aclear%20guidelines%20for%20the%20community.%20We%20will%20release%20the%20collection%20of%20the%0Asurveyed%20papers%20and%20actively%20maintain%20it%20to%20support%20ongoing%20advancements%20in%20the%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15957v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Holistic%2520Evaluation%2520of%2520Large%2520Audio-Language%2520Models%253A%2520A%250A%2520%2520Comprehensive%2520Survey%26entry.906535625%3DChih-Kai%2520Yang%2520and%2520Neo%2520S.%2520Ho%2520and%2520Hung-yi%2520Lee%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520large%2520audio-language%2520models%2520%2528LALMs%2529%252C%2520which%2520enhance%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520with%2520auditory%2520capabilities%252C%2520these%2520models%2520are%2520expected%2520to%250Ademonstrate%2520universal%2520proficiency%2520across%2520various%2520auditory%2520tasks.%2520While%2520numerous%250Abenchmarks%2520have%2520emerged%2520to%2520assess%2520LALMs%2527%2520performance%252C%2520they%2520remain%2520fragmented%250Aand%2520lack%2520a%2520structured%2520taxonomy.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520conduct%2520a%2520comprehensive%250Asurvey%2520and%2520propose%2520a%2520systematic%2520taxonomy%2520for%2520LALM%2520evaluations%252C%2520categorizing%250Athem%2520into%2520four%2520dimensions%2520based%2520on%2520their%2520objectives%253A%2520%25281%2529%2520General%2520Auditory%250AAwareness%2520and%2520Processing%252C%2520%25282%2529%2520Knowledge%2520and%2520Reasoning%252C%2520%25283%2529%2520Dialogue-oriented%250AAbility%252C%2520and%2520%25284%2529%2520Fairness%252C%2520Safety%252C%2520and%2520Trustworthiness.%2520We%2520provide%2520detailed%250Aoverviews%2520within%2520each%2520category%2520and%2520highlight%2520challenges%2520in%2520this%2520field%252C%2520offering%250Ainsights%2520into%2520promising%2520future%2520directions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520survey%2520specifically%2520focused%2520on%2520the%2520evaluations%2520of%2520LALMs%252C%2520providing%250Aclear%2520guidelines%2520for%2520the%2520community.%2520We%2520will%2520release%2520the%2520collection%2520of%2520the%250Asurveyed%2520papers%2520and%2520actively%2520maintain%2520it%2520to%2520support%2520ongoing%2520advancements%2520in%2520the%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15957v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Holistic%20Evaluation%20of%20Large%20Audio-Language%20Models%3A%20A%0A%20%20Comprehensive%20Survey&entry.906535625=Chih-Kai%20Yang%20and%20Neo%20S.%20Ho%20and%20Hung-yi%20Lee&entry.1292438233=%20%20With%20advancements%20in%20large%20audio-language%20models%20%28LALMs%29%2C%20which%20enhance%20large%0Alanguage%20models%20%28LLMs%29%20with%20auditory%20capabilities%2C%20these%20models%20are%20expected%20to%0Ademonstrate%20universal%20proficiency%20across%20various%20auditory%20tasks.%20While%20numerous%0Abenchmarks%20have%20emerged%20to%20assess%20LALMs%27%20performance%2C%20they%20remain%20fragmented%0Aand%20lack%20a%20structured%20taxonomy.%20To%20bridge%20this%20gap%2C%20we%20conduct%20a%20comprehensive%0Asurvey%20and%20propose%20a%20systematic%20taxonomy%20for%20LALM%20evaluations%2C%20categorizing%0Athem%20into%20four%20dimensions%20based%20on%20their%20objectives%3A%20%281%29%20General%20Auditory%0AAwareness%20and%20Processing%2C%20%282%29%20Knowledge%20and%20Reasoning%2C%20%283%29%20Dialogue-oriented%0AAbility%2C%20and%20%284%29%20Fairness%2C%20Safety%2C%20and%20Trustworthiness.%20We%20provide%20detailed%0Aoverviews%20within%20each%20category%20and%20highlight%20challenges%20in%20this%20field%2C%20offering%0Ainsights%20into%20promising%20future%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20survey%20specifically%20focused%20on%20the%20evaluations%20of%20LALMs%2C%20providing%0Aclear%20guidelines%20for%20the%20community.%20We%20will%20release%20the%20collection%20of%20the%0Asurveyed%20papers%20and%20actively%20maintain%20it%20to%20support%20ongoing%20advancements%20in%20the%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15957v3&entry.124074799=Read"},
{"title": "PSScreen: Partially Supervised Multiple Retinal Disease Screening", "author": "Boyi Zheng and Qing Liu", "abstract": "  Leveraging multiple partially labeled datasets to train a model for multiple\nretinal disease screening reduces the reliance on fully annotated datasets, but\nremains challenging due to significant domain shifts across training datasets\nfrom various medical sites, and the label absent issue for partial classes. To\nsolve these challenges, we propose PSScreen, a novel Partially Supervised\nmultiple retinal disease Screening model. Our PSScreen consists of two streams\nand one learns deterministic features and the other learns probabilistic\nfeatures via uncertainty injection. Then, we leverage the textual guidance to\ndecouple two types of features into disease-wise features and align them via\nfeature distillation to boost the domain generalization ability. Meanwhile, we\nemploy pseudo label consistency between two streams to address the label absent\nissue and introduce a self-distillation to transfer task-relevant semantics\nabout known classes from the deterministic to the probabilistic stream to\nfurther enhance the detection performances. Experiments show that our PSScreen\nsignificantly enhances the detection performances on six retinal diseases and\nthe normal state averagely and achieves state-of-the-art results on both\nin-domain and out-of-domain datasets. Codes are available at\nhttps://github.com/boyiZheng99/PSScreen.\n", "link": "http://arxiv.org/abs/2508.10549v3", "date": "2025-10-01", "relevancy": 2.0572, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5268}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening&body=Title%3A%20PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening%0AAuthor%3A%20Boyi%20Zheng%20and%20Qing%20Liu%0AAbstract%3A%20%20%20Leveraging%20multiple%20partially%20labeled%20datasets%20to%20train%20a%20model%20for%20multiple%0Aretinal%20disease%20screening%20reduces%20the%20reliance%20on%20fully%20annotated%20datasets%2C%20but%0Aremains%20challenging%20due%20to%20significant%20domain%20shifts%20across%20training%20datasets%0Afrom%20various%20medical%20sites%2C%20and%20the%20label%20absent%20issue%20for%20partial%20classes.%20To%0Asolve%20these%20challenges%2C%20we%20propose%20PSScreen%2C%20a%20novel%20Partially%20Supervised%0Amultiple%20retinal%20disease%20Screening%20model.%20Our%20PSScreen%20consists%20of%20two%20streams%0Aand%20one%20learns%20deterministic%20features%20and%20the%20other%20learns%20probabilistic%0Afeatures%20via%20uncertainty%20injection.%20Then%2C%20we%20leverage%20the%20textual%20guidance%20to%0Adecouple%20two%20types%20of%20features%20into%20disease-wise%20features%20and%20align%20them%20via%0Afeature%20distillation%20to%20boost%20the%20domain%20generalization%20ability.%20Meanwhile%2C%20we%0Aemploy%20pseudo%20label%20consistency%20between%20two%20streams%20to%20address%20the%20label%20absent%0Aissue%20and%20introduce%20a%20self-distillation%20to%20transfer%20task-relevant%20semantics%0Aabout%20known%20classes%20from%20the%20deterministic%20to%20the%20probabilistic%20stream%20to%0Afurther%20enhance%20the%20detection%20performances.%20Experiments%20show%20that%20our%20PSScreen%0Asignificantly%20enhances%20the%20detection%20performances%20on%20six%20retinal%20diseases%20and%0Athe%20normal%20state%20averagely%20and%20achieves%20state-of-the-art%20results%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/boyiZheng99/PSScreen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10549v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSScreen%253A%2520Partially%2520Supervised%2520Multiple%2520Retinal%2520Disease%2520Screening%26entry.906535625%3DBoyi%2520Zheng%2520and%2520Qing%2520Liu%26entry.1292438233%3D%2520%2520Leveraging%2520multiple%2520partially%2520labeled%2520datasets%2520to%2520train%2520a%2520model%2520for%2520multiple%250Aretinal%2520disease%2520screening%2520reduces%2520the%2520reliance%2520on%2520fully%2520annotated%2520datasets%252C%2520but%250Aremains%2520challenging%2520due%2520to%2520significant%2520domain%2520shifts%2520across%2520training%2520datasets%250Afrom%2520various%2520medical%2520sites%252C%2520and%2520the%2520label%2520absent%2520issue%2520for%2520partial%2520classes.%2520To%250Asolve%2520these%2520challenges%252C%2520we%2520propose%2520PSScreen%252C%2520a%2520novel%2520Partially%2520Supervised%250Amultiple%2520retinal%2520disease%2520Screening%2520model.%2520Our%2520PSScreen%2520consists%2520of%2520two%2520streams%250Aand%2520one%2520learns%2520deterministic%2520features%2520and%2520the%2520other%2520learns%2520probabilistic%250Afeatures%2520via%2520uncertainty%2520injection.%2520Then%252C%2520we%2520leverage%2520the%2520textual%2520guidance%2520to%250Adecouple%2520two%2520types%2520of%2520features%2520into%2520disease-wise%2520features%2520and%2520align%2520them%2520via%250Afeature%2520distillation%2520to%2520boost%2520the%2520domain%2520generalization%2520ability.%2520Meanwhile%252C%2520we%250Aemploy%2520pseudo%2520label%2520consistency%2520between%2520two%2520streams%2520to%2520address%2520the%2520label%2520absent%250Aissue%2520and%2520introduce%2520a%2520self-distillation%2520to%2520transfer%2520task-relevant%2520semantics%250Aabout%2520known%2520classes%2520from%2520the%2520deterministic%2520to%2520the%2520probabilistic%2520stream%2520to%250Afurther%2520enhance%2520the%2520detection%2520performances.%2520Experiments%2520show%2520that%2520our%2520PSScreen%250Asignificantly%2520enhances%2520the%2520detection%2520performances%2520on%2520six%2520retinal%2520diseases%2520and%250Athe%2520normal%2520state%2520averagely%2520and%2520achieves%2520state-of-the-art%2520results%2520on%2520both%250Ain-domain%2520and%2520out-of-domain%2520datasets.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/boyiZheng99/PSScreen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10549v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening&entry.906535625=Boyi%20Zheng%20and%20Qing%20Liu&entry.1292438233=%20%20Leveraging%20multiple%20partially%20labeled%20datasets%20to%20train%20a%20model%20for%20multiple%0Aretinal%20disease%20screening%20reduces%20the%20reliance%20on%20fully%20annotated%20datasets%2C%20but%0Aremains%20challenging%20due%20to%20significant%20domain%20shifts%20across%20training%20datasets%0Afrom%20various%20medical%20sites%2C%20and%20the%20label%20absent%20issue%20for%20partial%20classes.%20To%0Asolve%20these%20challenges%2C%20we%20propose%20PSScreen%2C%20a%20novel%20Partially%20Supervised%0Amultiple%20retinal%20disease%20Screening%20model.%20Our%20PSScreen%20consists%20of%20two%20streams%0Aand%20one%20learns%20deterministic%20features%20and%20the%20other%20learns%20probabilistic%0Afeatures%20via%20uncertainty%20injection.%20Then%2C%20we%20leverage%20the%20textual%20guidance%20to%0Adecouple%20two%20types%20of%20features%20into%20disease-wise%20features%20and%20align%20them%20via%0Afeature%20distillation%20to%20boost%20the%20domain%20generalization%20ability.%20Meanwhile%2C%20we%0Aemploy%20pseudo%20label%20consistency%20between%20two%20streams%20to%20address%20the%20label%20absent%0Aissue%20and%20introduce%20a%20self-distillation%20to%20transfer%20task-relevant%20semantics%0Aabout%20known%20classes%20from%20the%20deterministic%20to%20the%20probabilistic%20stream%20to%0Afurther%20enhance%20the%20detection%20performances.%20Experiments%20show%20that%20our%20PSScreen%0Asignificantly%20enhances%20the%20detection%20performances%20on%20six%20retinal%20diseases%20and%0Athe%20normal%20state%20averagely%20and%20achieves%20state-of-the-art%20results%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/boyiZheng99/PSScreen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10549v3&entry.124074799=Read"},
{"title": "Metaphor identification using large language models: A comparison of\n  RAG, prompt engineering, and fine-tuning", "author": "Matteo Fuoli and Weihang Huang and Jeannette Littlemore and Sarah Turner and Ellen Wilding", "abstract": "  Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.\n", "link": "http://arxiv.org/abs/2509.24866v2", "date": "2025-10-01", "relevancy": 2.0557, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning&body=Title%3A%20Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning%0AAuthor%3A%20Matteo%20Fuoli%20and%20Weihang%20Huang%20and%20Jeannette%20Littlemore%20and%20Sarah%20Turner%20and%20Ellen%20Wilding%0AAbstract%3A%20%20%20Metaphor%20is%20a%20pervasive%20feature%20of%20discourse%20and%20a%20powerful%20lens%20for%0Aexamining%20cognition%2C%20emotion%2C%20and%20ideology.%20Large-scale%20analysis%2C%20however%2C%20has%0Abeen%20constrained%20by%20the%20need%20for%20manual%20annotation%20due%20to%20the%20context-sensitive%0Anature%20of%20metaphor.%20This%20study%20investigates%20the%20potential%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20metaphor%20identification%20in%20full%20texts.%20We%20compare%0Athree%20methods%3A%20%28i%29%20retrieval-augmented%20generation%20%28RAG%29%2C%20where%20the%20model%20is%0Aprovided%20with%20a%20codebook%20and%20instructed%20to%20annotate%20texts%20based%20on%20its%20rules%0Aand%20examples%3B%20%28ii%29%20prompt%20engineering%2C%20where%20we%20design%20task-specific%20verbal%0Ainstructions%3B%20and%20%28iii%29%20fine-tuning%2C%20where%20the%20model%20is%20trained%20on%20hand-coded%0Atexts%20to%20optimize%20performance.%20Within%20prompt%20engineering%2C%20we%20test%20zero-shot%2C%0Afew-shot%2C%20and%20chain-of-thought%20strategies.%20Our%20results%20show%20that%0Astate-of-the-art%20closed-source%20LLMs%20can%20achieve%20high%20accuracy%2C%20with%20fine-tuning%0Ayielding%20a%20median%20F1%20score%20of%200.79.%20A%20comparison%20of%20human%20and%20LLM%20outputs%0Areveals%20that%20most%20discrepancies%20are%20systematic%2C%20reflecting%20well-known%20grey%0Aareas%20and%20conceptual%20challenges%20in%20metaphor%20theory.%20We%20propose%20that%20LLMs%20can%20be%0Aused%20to%20at%20least%20partly%20automate%20metaphor%20identification%20and%20can%20serve%20as%20a%0Atestbed%20for%20developing%20and%20refining%20metaphor%20identification%20protocols%20and%20the%0Atheory%20that%20underpins%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaphor%2520identification%2520using%2520large%2520language%2520models%253A%2520A%2520comparison%2520of%250A%2520%2520RAG%252C%2520prompt%2520engineering%252C%2520and%2520fine-tuning%26entry.906535625%3DMatteo%2520Fuoli%2520and%2520Weihang%2520Huang%2520and%2520Jeannette%2520Littlemore%2520and%2520Sarah%2520Turner%2520and%2520Ellen%2520Wilding%26entry.1292438233%3D%2520%2520Metaphor%2520is%2520a%2520pervasive%2520feature%2520of%2520discourse%2520and%2520a%2520powerful%2520lens%2520for%250Aexamining%2520cognition%252C%2520emotion%252C%2520and%2520ideology.%2520Large-scale%2520analysis%252C%2520however%252C%2520has%250Abeen%2520constrained%2520by%2520the%2520need%2520for%2520manual%2520annotation%2520due%2520to%2520the%2520context-sensitive%250Anature%2520of%2520metaphor.%2520This%2520study%2520investigates%2520the%2520potential%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520automate%2520metaphor%2520identification%2520in%2520full%2520texts.%2520We%2520compare%250Athree%2520methods%253A%2520%2528i%2529%2520retrieval-augmented%2520generation%2520%2528RAG%2529%252C%2520where%2520the%2520model%2520is%250Aprovided%2520with%2520a%2520codebook%2520and%2520instructed%2520to%2520annotate%2520texts%2520based%2520on%2520its%2520rules%250Aand%2520examples%253B%2520%2528ii%2529%2520prompt%2520engineering%252C%2520where%2520we%2520design%2520task-specific%2520verbal%250Ainstructions%253B%2520and%2520%2528iii%2529%2520fine-tuning%252C%2520where%2520the%2520model%2520is%2520trained%2520on%2520hand-coded%250Atexts%2520to%2520optimize%2520performance.%2520Within%2520prompt%2520engineering%252C%2520we%2520test%2520zero-shot%252C%250Afew-shot%252C%2520and%2520chain-of-thought%2520strategies.%2520Our%2520results%2520show%2520that%250Astate-of-the-art%2520closed-source%2520LLMs%2520can%2520achieve%2520high%2520accuracy%252C%2520with%2520fine-tuning%250Ayielding%2520a%2520median%2520F1%2520score%2520of%25200.79.%2520A%2520comparison%2520of%2520human%2520and%2520LLM%2520outputs%250Areveals%2520that%2520most%2520discrepancies%2520are%2520systematic%252C%2520reflecting%2520well-known%2520grey%250Aareas%2520and%2520conceptual%2520challenges%2520in%2520metaphor%2520theory.%2520We%2520propose%2520that%2520LLMs%2520can%2520be%250Aused%2520to%2520at%2520least%2520partly%2520automate%2520metaphor%2520identification%2520and%2520can%2520serve%2520as%2520a%250Atestbed%2520for%2520developing%2520and%2520refining%2520metaphor%2520identification%2520protocols%2520and%2520the%250Atheory%2520that%2520underpins%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metaphor%20identification%20using%20large%20language%20models%3A%20A%20comparison%20of%0A%20%20RAG%2C%20prompt%20engineering%2C%20and%20fine-tuning&entry.906535625=Matteo%20Fuoli%20and%20Weihang%20Huang%20and%20Jeannette%20Littlemore%20and%20Sarah%20Turner%20and%20Ellen%20Wilding&entry.1292438233=%20%20Metaphor%20is%20a%20pervasive%20feature%20of%20discourse%20and%20a%20powerful%20lens%20for%0Aexamining%20cognition%2C%20emotion%2C%20and%20ideology.%20Large-scale%20analysis%2C%20however%2C%20has%0Abeen%20constrained%20by%20the%20need%20for%20manual%20annotation%20due%20to%20the%20context-sensitive%0Anature%20of%20metaphor.%20This%20study%20investigates%20the%20potential%20of%20large%20language%0Amodels%20%28LLMs%29%20to%20automate%20metaphor%20identification%20in%20full%20texts.%20We%20compare%0Athree%20methods%3A%20%28i%29%20retrieval-augmented%20generation%20%28RAG%29%2C%20where%20the%20model%20is%0Aprovided%20with%20a%20codebook%20and%20instructed%20to%20annotate%20texts%20based%20on%20its%20rules%0Aand%20examples%3B%20%28ii%29%20prompt%20engineering%2C%20where%20we%20design%20task-specific%20verbal%0Ainstructions%3B%20and%20%28iii%29%20fine-tuning%2C%20where%20the%20model%20is%20trained%20on%20hand-coded%0Atexts%20to%20optimize%20performance.%20Within%20prompt%20engineering%2C%20we%20test%20zero-shot%2C%0Afew-shot%2C%20and%20chain-of-thought%20strategies.%20Our%20results%20show%20that%0Astate-of-the-art%20closed-source%20LLMs%20can%20achieve%20high%20accuracy%2C%20with%20fine-tuning%0Ayielding%20a%20median%20F1%20score%20of%200.79.%20A%20comparison%20of%20human%20and%20LLM%20outputs%0Areveals%20that%20most%20discrepancies%20are%20systematic%2C%20reflecting%20well-known%20grey%0Aareas%20and%20conceptual%20challenges%20in%20metaphor%20theory.%20We%20propose%20that%20LLMs%20can%20be%0Aused%20to%20at%20least%20partly%20automate%20metaphor%20identification%20and%20can%20serve%20as%20a%0Atestbed%20for%20developing%20and%20refining%20metaphor%20identification%20protocols%20and%20the%0Atheory%20that%20underpins%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24866v2&entry.124074799=Read"},
{"title": "The Inhibitor: ReLU and Addition-Based Attention for Efficient\n  Transformers under Fully Homomorphic Encryption on the Torus", "author": "Rickard Br\u00e4nnvall and Andrei Stoian", "abstract": "  To enhance the computational efficiency of quantized Transformers, we replace\nthe dot-product and Softmax-based attention with an alternative mechanism\ninvolving addition and ReLU activation only. This side-steps the expansion to\ndouble precision often required by matrix multiplication and avoids costly\nSoftmax evaluations but maintains much of the core functionality of\nconventional dot-product attention. It can enable more efficient execution and\nsupport larger quantized Transformer models on resource-constrained hardware or\nalternative arithmetic systems like homomorphic encryption. Training\nexperiments on four common benchmark tasks show test set prediction scores\ncomparable to those of conventional Transformers with dot-product attention.\nOur scaling experiments also suggest significant computational savings, both in\nplaintext and under encryption. In particular, we believe that the ReLU and\naddition-based attention mechanism examined in this paper may enable\nprivacy-preserving AI applications operating under homomorphic encryption by\navoiding the costly multiplication of encrypted variables.\n", "link": "http://arxiv.org/abs/2310.02041v2", "date": "2025-10-01", "relevancy": 2.0519, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5454}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Inhibitor%3A%20ReLU%20and%20Addition-Based%20Attention%20for%20Efficient%0A%20%20Transformers%20under%20Fully%20Homomorphic%20Encryption%20on%20the%20Torus&body=Title%3A%20The%20Inhibitor%3A%20ReLU%20and%20Addition-Based%20Attention%20for%20Efficient%0A%20%20Transformers%20under%20Fully%20Homomorphic%20Encryption%20on%20the%20Torus%0AAuthor%3A%20Rickard%20Br%C3%A4nnvall%20and%20Andrei%20Stoian%0AAbstract%3A%20%20%20To%20enhance%20the%20computational%20efficiency%20of%20quantized%20Transformers%2C%20we%20replace%0Athe%20dot-product%20and%20Softmax-based%20attention%20with%20an%20alternative%20mechanism%0Ainvolving%20addition%20and%20ReLU%20activation%20only.%20This%20side-steps%20the%20expansion%20to%0Adouble%20precision%20often%20required%20by%20matrix%20multiplication%20and%20avoids%20costly%0ASoftmax%20evaluations%20but%20maintains%20much%20of%20the%20core%20functionality%20of%0Aconventional%20dot-product%20attention.%20It%20can%20enable%20more%20efficient%20execution%20and%0Asupport%20larger%20quantized%20Transformer%20models%20on%20resource-constrained%20hardware%20or%0Aalternative%20arithmetic%20systems%20like%20homomorphic%20encryption.%20Training%0Aexperiments%20on%20four%20common%20benchmark%20tasks%20show%20test%20set%20prediction%20scores%0Acomparable%20to%20those%20of%20conventional%20Transformers%20with%20dot-product%20attention.%0AOur%20scaling%20experiments%20also%20suggest%20significant%20computational%20savings%2C%20both%20in%0Aplaintext%20and%20under%20encryption.%20In%20particular%2C%20we%20believe%20that%20the%20ReLU%20and%0Aaddition-based%20attention%20mechanism%20examined%20in%20this%20paper%20may%20enable%0Aprivacy-preserving%20AI%20applications%20operating%20under%20homomorphic%20encryption%20by%0Aavoiding%20the%20costly%20multiplication%20of%20encrypted%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Inhibitor%253A%2520ReLU%2520and%2520Addition-Based%2520Attention%2520for%2520Efficient%250A%2520%2520Transformers%2520under%2520Fully%2520Homomorphic%2520Encryption%2520on%2520the%2520Torus%26entry.906535625%3DRickard%2520Br%25C3%25A4nnvall%2520and%2520Andrei%2520Stoian%26entry.1292438233%3D%2520%2520To%2520enhance%2520the%2520computational%2520efficiency%2520of%2520quantized%2520Transformers%252C%2520we%2520replace%250Athe%2520dot-product%2520and%2520Softmax-based%2520attention%2520with%2520an%2520alternative%2520mechanism%250Ainvolving%2520addition%2520and%2520ReLU%2520activation%2520only.%2520This%2520side-steps%2520the%2520expansion%2520to%250Adouble%2520precision%2520often%2520required%2520by%2520matrix%2520multiplication%2520and%2520avoids%2520costly%250ASoftmax%2520evaluations%2520but%2520maintains%2520much%2520of%2520the%2520core%2520functionality%2520of%250Aconventional%2520dot-product%2520attention.%2520It%2520can%2520enable%2520more%2520efficient%2520execution%2520and%250Asupport%2520larger%2520quantized%2520Transformer%2520models%2520on%2520resource-constrained%2520hardware%2520or%250Aalternative%2520arithmetic%2520systems%2520like%2520homomorphic%2520encryption.%2520Training%250Aexperiments%2520on%2520four%2520common%2520benchmark%2520tasks%2520show%2520test%2520set%2520prediction%2520scores%250Acomparable%2520to%2520those%2520of%2520conventional%2520Transformers%2520with%2520dot-product%2520attention.%250AOur%2520scaling%2520experiments%2520also%2520suggest%2520significant%2520computational%2520savings%252C%2520both%2520in%250Aplaintext%2520and%2520under%2520encryption.%2520In%2520particular%252C%2520we%2520believe%2520that%2520the%2520ReLU%2520and%250Aaddition-based%2520attention%2520mechanism%2520examined%2520in%2520this%2520paper%2520may%2520enable%250Aprivacy-preserving%2520AI%2520applications%2520operating%2520under%2520homomorphic%2520encryption%2520by%250Aavoiding%2520the%2520costly%2520multiplication%2520of%2520encrypted%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Inhibitor%3A%20ReLU%20and%20Addition-Based%20Attention%20for%20Efficient%0A%20%20Transformers%20under%20Fully%20Homomorphic%20Encryption%20on%20the%20Torus&entry.906535625=Rickard%20Br%C3%A4nnvall%20and%20Andrei%20Stoian&entry.1292438233=%20%20To%20enhance%20the%20computational%20efficiency%20of%20quantized%20Transformers%2C%20we%20replace%0Athe%20dot-product%20and%20Softmax-based%20attention%20with%20an%20alternative%20mechanism%0Ainvolving%20addition%20and%20ReLU%20activation%20only.%20This%20side-steps%20the%20expansion%20to%0Adouble%20precision%20often%20required%20by%20matrix%20multiplication%20and%20avoids%20costly%0ASoftmax%20evaluations%20but%20maintains%20much%20of%20the%20core%20functionality%20of%0Aconventional%20dot-product%20attention.%20It%20can%20enable%20more%20efficient%20execution%20and%0Asupport%20larger%20quantized%20Transformer%20models%20on%20resource-constrained%20hardware%20or%0Aalternative%20arithmetic%20systems%20like%20homomorphic%20encryption.%20Training%0Aexperiments%20on%20four%20common%20benchmark%20tasks%20show%20test%20set%20prediction%20scores%0Acomparable%20to%20those%20of%20conventional%20Transformers%20with%20dot-product%20attention.%0AOur%20scaling%20experiments%20also%20suggest%20significant%20computational%20savings%2C%20both%20in%0Aplaintext%20and%20under%20encryption.%20In%20particular%2C%20we%20believe%20that%20the%20ReLU%20and%0Aaddition-based%20attention%20mechanism%20examined%20in%20this%20paper%20may%20enable%0Aprivacy-preserving%20AI%20applications%20operating%20under%20homomorphic%20encryption%20by%0Aavoiding%20the%20costly%20multiplication%20of%20encrypted%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02041v2&entry.124074799=Read"},
{"title": "Ambiguity in LLMs is a concept missing problem", "author": "Zhibo Hu and Chen Wang and Yanfeng Shu and Hye-Young Paik and Liming Zhu", "abstract": "  Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.\n", "link": "http://arxiv.org/abs/2505.11679v3", "date": "2025-10-01", "relevancy": 2.0457, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ambiguity%20in%20LLMs%20is%20a%20concept%20missing%20problem&body=Title%3A%20Ambiguity%20in%20LLMs%20is%20a%20concept%20missing%20problem%0AAuthor%3A%20Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-Young%20Paik%20and%20Liming%20Zhu%0AAbstract%3A%20%20%20Ambiguity%20in%20natural%20language%20is%20a%20significant%20obstacle%20for%20achieving%0Aaccurate%20text%20to%20structured%20data%20mapping%20through%20large%20language%20models%20%28LLMs%29%2C%0Awhich%20affects%20the%20performance%20of%20tasks%20such%20as%20mapping%20text%20to%20agentic%20tool%0Acalling%20and%20text-to-SQL%20queries.%20Existing%20methods%20to%20ambiguity%20handling%20either%0Arely%20on%20the%20ReACT%20framework%20to%20obtain%20correct%20mappings%20through%20trial%20and%20error%2C%0Aor%20on%20supervised%20fine-tuning%20to%20bias%20models%20toward%20specific%20tasks.%20In%20this%0Apaper%2C%20we%20adopt%20a%20different%20approach%20that%20characterizes%20representation%0Adifferences%20of%20ambiguous%20text%20in%20the%20latent%20space%20and%20leverages%20these%0Adifferences%20to%20identify%20ambiguity%20before%20mapping%20them%20to%20structured%20data.%20To%0Adetect%20sentence-level%20ambiguity%2C%20we%20focus%20on%20the%20relationship%20between%20ambiguous%0Aquestions%20and%20their%20interpretations.%20Unlike%20distances%20calculated%20by%20dense%0Aembeddings%2C%20we%20introduce%20a%20new%20distance%20measure%20based%20on%20a%20path%20kernel%20over%0Aconcepts.%20With%20this%20measurement%2C%20we%20identify%20patterns%20to%20distinguish%20ambiguous%0Afrom%20unambiguous%20questions.%20Furthermore%2C%20we%20propose%20a%20method%20for%20improving%20LLM%0Aperformance%20on%20ambiguous%20agentic%20tool%20calling%20through%20missing%20concept%0Aprediction.%20Both%20achieve%20state-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmbiguity%2520in%2520LLMs%2520is%2520a%2520concept%2520missing%2520problem%26entry.906535625%3DZhibo%2520Hu%2520and%2520Chen%2520Wang%2520and%2520Yanfeng%2520Shu%2520and%2520Hye-Young%2520Paik%2520and%2520Liming%2520Zhu%26entry.1292438233%3D%2520%2520Ambiguity%2520in%2520natural%2520language%2520is%2520a%2520significant%2520obstacle%2520for%2520achieving%250Aaccurate%2520text%2520to%2520structured%2520data%2520mapping%2520through%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Awhich%2520affects%2520the%2520performance%2520of%2520tasks%2520such%2520as%2520mapping%2520text%2520to%2520agentic%2520tool%250Acalling%2520and%2520text-to-SQL%2520queries.%2520Existing%2520methods%2520to%2520ambiguity%2520handling%2520either%250Arely%2520on%2520the%2520ReACT%2520framework%2520to%2520obtain%2520correct%2520mappings%2520through%2520trial%2520and%2520error%252C%250Aor%2520on%2520supervised%2520fine-tuning%2520to%2520bias%2520models%2520toward%2520specific%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520adopt%2520a%2520different%2520approach%2520that%2520characterizes%2520representation%250Adifferences%2520of%2520ambiguous%2520text%2520in%2520the%2520latent%2520space%2520and%2520leverages%2520these%250Adifferences%2520to%2520identify%2520ambiguity%2520before%2520mapping%2520them%2520to%2520structured%2520data.%2520To%250Adetect%2520sentence-level%2520ambiguity%252C%2520we%2520focus%2520on%2520the%2520relationship%2520between%2520ambiguous%250Aquestions%2520and%2520their%2520interpretations.%2520Unlike%2520distances%2520calculated%2520by%2520dense%250Aembeddings%252C%2520we%2520introduce%2520a%2520new%2520distance%2520measure%2520based%2520on%2520a%2520path%2520kernel%2520over%250Aconcepts.%2520With%2520this%2520measurement%252C%2520we%2520identify%2520patterns%2520to%2520distinguish%2520ambiguous%250Afrom%2520unambiguous%2520questions.%2520Furthermore%252C%2520we%2520propose%2520a%2520method%2520for%2520improving%2520LLM%250Aperformance%2520on%2520ambiguous%2520agentic%2520tool%2520calling%2520through%2520missing%2520concept%250Aprediction.%2520Both%2520achieve%2520state-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ambiguity%20in%20LLMs%20is%20a%20concept%20missing%20problem&entry.906535625=Zhibo%20Hu%20and%20Chen%20Wang%20and%20Yanfeng%20Shu%20and%20Hye-Young%20Paik%20and%20Liming%20Zhu&entry.1292438233=%20%20Ambiguity%20in%20natural%20language%20is%20a%20significant%20obstacle%20for%20achieving%0Aaccurate%20text%20to%20structured%20data%20mapping%20through%20large%20language%20models%20%28LLMs%29%2C%0Awhich%20affects%20the%20performance%20of%20tasks%20such%20as%20mapping%20text%20to%20agentic%20tool%0Acalling%20and%20text-to-SQL%20queries.%20Existing%20methods%20to%20ambiguity%20handling%20either%0Arely%20on%20the%20ReACT%20framework%20to%20obtain%20correct%20mappings%20through%20trial%20and%20error%2C%0Aor%20on%20supervised%20fine-tuning%20to%20bias%20models%20toward%20specific%20tasks.%20In%20this%0Apaper%2C%20we%20adopt%20a%20different%20approach%20that%20characterizes%20representation%0Adifferences%20of%20ambiguous%20text%20in%20the%20latent%20space%20and%20leverages%20these%0Adifferences%20to%20identify%20ambiguity%20before%20mapping%20them%20to%20structured%20data.%20To%0Adetect%20sentence-level%20ambiguity%2C%20we%20focus%20on%20the%20relationship%20between%20ambiguous%0Aquestions%20and%20their%20interpretations.%20Unlike%20distances%20calculated%20by%20dense%0Aembeddings%2C%20we%20introduce%20a%20new%20distance%20measure%20based%20on%20a%20path%20kernel%20over%0Aconcepts.%20With%20this%20measurement%2C%20we%20identify%20patterns%20to%20distinguish%20ambiguous%0Afrom%20unambiguous%20questions.%20Furthermore%2C%20we%20propose%20a%20method%20for%20improving%20LLM%0Aperformance%20on%20ambiguous%20agentic%20tool%20calling%20through%20missing%20concept%0Aprediction.%20Both%20achieve%20state-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11679v3&entry.124074799=Read"},
{"title": "Topology of Reasoning: Understanding Large Reasoning Models through\n  Reasoning Graph Properties", "author": "Gouki Minegishi and Hiroki Furuta and Takeshi Kojima and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Recent large-scale reasoning models have achieved state-of-the-art\nperformance on challenging mathematical benchmarks, yet the internal mechanisms\nunderlying their success remain poorly understood. In this work, we introduce\nthe notion of a reasoning graph, extracted by clustering hidden-state\nrepresentations at each reasoning step, and systematically analyze three key\ngraph-theoretic properties: cyclicity, diameter, and small-world index, across\nmultiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled\nreasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly\nmore recurrent cycles (about 5 per sample), substantially larger graph\ndiameters, and pronounced small-world characteristics (about 6x) compared to\ntheir base counterparts. Notably, these structural advantages grow with task\ndifficulty and model capacity, with cycle detection peaking at the 14B scale\nand exploration diameter maximized in the 32B variant, correlating positively\nwith accuracy. Furthermore, we show that supervised fine-tuning on an improved\ndataset systematically expands reasoning graph diameters in tandem with\nperformance gains, offering concrete guidelines for dataset design aimed at\nboosting reasoning capabilities. By bridging theoretical insights into\nreasoning graph structures with practical recommendations for data\nconstruction, our work advances both the interpretability and the efficacy of\nlarge reasoning models.\n", "link": "http://arxiv.org/abs/2506.05744v3", "date": "2025-10-01", "relevancy": 2.0451, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology%20of%20Reasoning%3A%20Understanding%20Large%20Reasoning%20Models%20through%0A%20%20Reasoning%20Graph%20Properties&body=Title%3A%20Topology%20of%20Reasoning%3A%20Understanding%20Large%20Reasoning%20Models%20through%0A%20%20Reasoning%20Graph%20Properties%0AAuthor%3A%20Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Takeshi%20Kojima%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Recent%20large-scale%20reasoning%20models%20have%20achieved%20state-of-the-art%0Aperformance%20on%20challenging%20mathematical%20benchmarks%2C%20yet%20the%20internal%20mechanisms%0Aunderlying%20their%20success%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20introduce%0Athe%20notion%20of%20a%20reasoning%20graph%2C%20extracted%20by%20clustering%20hidden-state%0Arepresentations%20at%20each%20reasoning%20step%2C%20and%20systematically%20analyze%20three%20key%0Agraph-theoretic%20properties%3A%20cyclicity%2C%20diameter%2C%20and%20small-world%20index%2C%20across%0Amultiple%20tasks%20%28GSM8K%2C%20MATH500%2C%20AIME%202024%29.%20Our%20findings%20reveal%20that%20distilled%0Areasoning%20models%20%28e.g.%2C%20DeepSeek-R1-Distill-Qwen-32B%29%20exhibit%20significantly%0Amore%20recurrent%20cycles%20%28about%205%20per%20sample%29%2C%20substantially%20larger%20graph%0Adiameters%2C%20and%20pronounced%20small-world%20characteristics%20%28about%206x%29%20compared%20to%0Atheir%20base%20counterparts.%20Notably%2C%20these%20structural%20advantages%20grow%20with%20task%0Adifficulty%20and%20model%20capacity%2C%20with%20cycle%20detection%20peaking%20at%20the%2014B%20scale%0Aand%20exploration%20diameter%20maximized%20in%20the%2032B%20variant%2C%20correlating%20positively%0Awith%20accuracy.%20Furthermore%2C%20we%20show%20that%20supervised%20fine-tuning%20on%20an%20improved%0Adataset%20systematically%20expands%20reasoning%20graph%20diameters%20in%20tandem%20with%0Aperformance%20gains%2C%20offering%20concrete%20guidelines%20for%20dataset%20design%20aimed%20at%0Aboosting%20reasoning%20capabilities.%20By%20bridging%20theoretical%20insights%20into%0Areasoning%20graph%20structures%20with%20practical%20recommendations%20for%20data%0Aconstruction%2C%20our%20work%20advances%20both%20the%20interpretability%20and%20the%20efficacy%20of%0Alarge%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology%2520of%2520Reasoning%253A%2520Understanding%2520Large%2520Reasoning%2520Models%2520through%250A%2520%2520Reasoning%2520Graph%2520Properties%26entry.906535625%3DGouki%2520Minegishi%2520and%2520Hiroki%2520Furuta%2520and%2520Takeshi%2520Kojima%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Recent%2520large-scale%2520reasoning%2520models%2520have%2520achieved%2520state-of-the-art%250Aperformance%2520on%2520challenging%2520mathematical%2520benchmarks%252C%2520yet%2520the%2520internal%2520mechanisms%250Aunderlying%2520their%2520success%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520introduce%250Athe%2520notion%2520of%2520a%2520reasoning%2520graph%252C%2520extracted%2520by%2520clustering%2520hidden-state%250Arepresentations%2520at%2520each%2520reasoning%2520step%252C%2520and%2520systematically%2520analyze%2520three%2520key%250Agraph-theoretic%2520properties%253A%2520cyclicity%252C%2520diameter%252C%2520and%2520small-world%2520index%252C%2520across%250Amultiple%2520tasks%2520%2528GSM8K%252C%2520MATH500%252C%2520AIME%25202024%2529.%2520Our%2520findings%2520reveal%2520that%2520distilled%250Areasoning%2520models%2520%2528e.g.%252C%2520DeepSeek-R1-Distill-Qwen-32B%2529%2520exhibit%2520significantly%250Amore%2520recurrent%2520cycles%2520%2528about%25205%2520per%2520sample%2529%252C%2520substantially%2520larger%2520graph%250Adiameters%252C%2520and%2520pronounced%2520small-world%2520characteristics%2520%2528about%25206x%2529%2520compared%2520to%250Atheir%2520base%2520counterparts.%2520Notably%252C%2520these%2520structural%2520advantages%2520grow%2520with%2520task%250Adifficulty%2520and%2520model%2520capacity%252C%2520with%2520cycle%2520detection%2520peaking%2520at%2520the%252014B%2520scale%250Aand%2520exploration%2520diameter%2520maximized%2520in%2520the%252032B%2520variant%252C%2520correlating%2520positively%250Awith%2520accuracy.%2520Furthermore%252C%2520we%2520show%2520that%2520supervised%2520fine-tuning%2520on%2520an%2520improved%250Adataset%2520systematically%2520expands%2520reasoning%2520graph%2520diameters%2520in%2520tandem%2520with%250Aperformance%2520gains%252C%2520offering%2520concrete%2520guidelines%2520for%2520dataset%2520design%2520aimed%2520at%250Aboosting%2520reasoning%2520capabilities.%2520By%2520bridging%2520theoretical%2520insights%2520into%250Areasoning%2520graph%2520structures%2520with%2520practical%2520recommendations%2520for%2520data%250Aconstruction%252C%2520our%2520work%2520advances%2520both%2520the%2520interpretability%2520and%2520the%2520efficacy%2520of%250Alarge%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology%20of%20Reasoning%3A%20Understanding%20Large%20Reasoning%20Models%20through%0A%20%20Reasoning%20Graph%20Properties&entry.906535625=Gouki%20Minegishi%20and%20Hiroki%20Furuta%20and%20Takeshi%20Kojima%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Recent%20large-scale%20reasoning%20models%20have%20achieved%20state-of-the-art%0Aperformance%20on%20challenging%20mathematical%20benchmarks%2C%20yet%20the%20internal%20mechanisms%0Aunderlying%20their%20success%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20introduce%0Athe%20notion%20of%20a%20reasoning%20graph%2C%20extracted%20by%20clustering%20hidden-state%0Arepresentations%20at%20each%20reasoning%20step%2C%20and%20systematically%20analyze%20three%20key%0Agraph-theoretic%20properties%3A%20cyclicity%2C%20diameter%2C%20and%20small-world%20index%2C%20across%0Amultiple%20tasks%20%28GSM8K%2C%20MATH500%2C%20AIME%202024%29.%20Our%20findings%20reveal%20that%20distilled%0Areasoning%20models%20%28e.g.%2C%20DeepSeek-R1-Distill-Qwen-32B%29%20exhibit%20significantly%0Amore%20recurrent%20cycles%20%28about%205%20per%20sample%29%2C%20substantially%20larger%20graph%0Adiameters%2C%20and%20pronounced%20small-world%20characteristics%20%28about%206x%29%20compared%20to%0Atheir%20base%20counterparts.%20Notably%2C%20these%20structural%20advantages%20grow%20with%20task%0Adifficulty%20and%20model%20capacity%2C%20with%20cycle%20detection%20peaking%20at%20the%2014B%20scale%0Aand%20exploration%20diameter%20maximized%20in%20the%2032B%20variant%2C%20correlating%20positively%0Awith%20accuracy.%20Furthermore%2C%20we%20show%20that%20supervised%20fine-tuning%20on%20an%20improved%0Adataset%20systematically%20expands%20reasoning%20graph%20diameters%20in%20tandem%20with%0Aperformance%20gains%2C%20offering%20concrete%20guidelines%20for%20dataset%20design%20aimed%20at%0Aboosting%20reasoning%20capabilities.%20By%20bridging%20theoretical%20insights%20into%0Areasoning%20graph%20structures%20with%20practical%20recommendations%20for%20data%0Aconstruction%2C%20our%20work%20advances%20both%20the%20interpretability%20and%20the%20efficacy%20of%0Alarge%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05744v3&entry.124074799=Read"},
{"title": "Direct Preference Optimization for Adaptive Concept-based Explanations", "author": "Jacopo Teneggi and Zhenzhen Wang and Paul H. Yi and Tianmin Shu and Jeremias Sulam", "abstract": "  Concept-based explanation methods aim at making machine learning models more\ntransparent by finding the most important semantic features of an input (e.g.,\ncolors, patterns, shapes) for a given prediction task. However, these methods\ngenerally ignore the communicative context of explanations, such as the\npreferences of a listener. For example, medical doctors understand explanations\nin terms of clinical markers, but patients may not, needing a different\nvocabulary to rationalize the same diagnosis. We address this gap with\nlistener-adaptive explanations grounded in principles of pragmatic reasoning\nand the rational speech act. We introduce an iterative training procedure based\non direct preference optimization where a speaker learns to compose\nexplanations that maximize communicative utility for a listener. Our approach\nonly needs access to pairwise preferences, which can be collected from human\nfeedback, making it particularly relevant in real-world scenarios where a model\nof the listener may not be available. We demonstrate that our method is able to\nalign speakers with the preferences of simulated listeners on image\nclassification across three datasets, and further validate that pragmatic\nexplanations generated with our method improve the classification accuracy of\nparticipants in a user study.\n", "link": "http://arxiv.org/abs/2505.15626v2", "date": "2025-10-01", "relevancy": 2.0428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Preference%20Optimization%20for%20Adaptive%20Concept-based%20Explanations&body=Title%3A%20Direct%20Preference%20Optimization%20for%20Adaptive%20Concept-based%20Explanations%0AAuthor%3A%20Jacopo%20Teneggi%20and%20Zhenzhen%20Wang%20and%20Paul%20H.%20Yi%20and%20Tianmin%20Shu%20and%20Jeremias%20Sulam%0AAbstract%3A%20%20%20Concept-based%20explanation%20methods%20aim%20at%20making%20machine%20learning%20models%20more%0Atransparent%20by%20finding%20the%20most%20important%20semantic%20features%20of%20an%20input%20%28e.g.%2C%0Acolors%2C%20patterns%2C%20shapes%29%20for%20a%20given%20prediction%20task.%20However%2C%20these%20methods%0Agenerally%20ignore%20the%20communicative%20context%20of%20explanations%2C%20such%20as%20the%0Apreferences%20of%20a%20listener.%20For%20example%2C%20medical%20doctors%20understand%20explanations%0Ain%20terms%20of%20clinical%20markers%2C%20but%20patients%20may%20not%2C%20needing%20a%20different%0Avocabulary%20to%20rationalize%20the%20same%20diagnosis.%20We%20address%20this%20gap%20with%0Alistener-adaptive%20explanations%20grounded%20in%20principles%20of%20pragmatic%20reasoning%0Aand%20the%20rational%20speech%20act.%20We%20introduce%20an%20iterative%20training%20procedure%20based%0Aon%20direct%20preference%20optimization%20where%20a%20speaker%20learns%20to%20compose%0Aexplanations%20that%20maximize%20communicative%20utility%20for%20a%20listener.%20Our%20approach%0Aonly%20needs%20access%20to%20pairwise%20preferences%2C%20which%20can%20be%20collected%20from%20human%0Afeedback%2C%20making%20it%20particularly%20relevant%20in%20real-world%20scenarios%20where%20a%20model%0Aof%20the%20listener%20may%20not%20be%20available.%20We%20demonstrate%20that%20our%20method%20is%20able%20to%0Aalign%20speakers%20with%20the%20preferences%20of%20simulated%20listeners%20on%20image%0Aclassification%20across%20three%20datasets%2C%20and%20further%20validate%20that%20pragmatic%0Aexplanations%20generated%20with%20our%20method%20improve%20the%20classification%20accuracy%20of%0Aparticipants%20in%20a%20user%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Preference%2520Optimization%2520for%2520Adaptive%2520Concept-based%2520Explanations%26entry.906535625%3DJacopo%2520Teneggi%2520and%2520Zhenzhen%2520Wang%2520and%2520Paul%2520H.%2520Yi%2520and%2520Tianmin%2520Shu%2520and%2520Jeremias%2520Sulam%26entry.1292438233%3D%2520%2520Concept-based%2520explanation%2520methods%2520aim%2520at%2520making%2520machine%2520learning%2520models%2520more%250Atransparent%2520by%2520finding%2520the%2520most%2520important%2520semantic%2520features%2520of%2520an%2520input%2520%2528e.g.%252C%250Acolors%252C%2520patterns%252C%2520shapes%2529%2520for%2520a%2520given%2520prediction%2520task.%2520However%252C%2520these%2520methods%250Agenerally%2520ignore%2520the%2520communicative%2520context%2520of%2520explanations%252C%2520such%2520as%2520the%250Apreferences%2520of%2520a%2520listener.%2520For%2520example%252C%2520medical%2520doctors%2520understand%2520explanations%250Ain%2520terms%2520of%2520clinical%2520markers%252C%2520but%2520patients%2520may%2520not%252C%2520needing%2520a%2520different%250Avocabulary%2520to%2520rationalize%2520the%2520same%2520diagnosis.%2520We%2520address%2520this%2520gap%2520with%250Alistener-adaptive%2520explanations%2520grounded%2520in%2520principles%2520of%2520pragmatic%2520reasoning%250Aand%2520the%2520rational%2520speech%2520act.%2520We%2520introduce%2520an%2520iterative%2520training%2520procedure%2520based%250Aon%2520direct%2520preference%2520optimization%2520where%2520a%2520speaker%2520learns%2520to%2520compose%250Aexplanations%2520that%2520maximize%2520communicative%2520utility%2520for%2520a%2520listener.%2520Our%2520approach%250Aonly%2520needs%2520access%2520to%2520pairwise%2520preferences%252C%2520which%2520can%2520be%2520collected%2520from%2520human%250Afeedback%252C%2520making%2520it%2520particularly%2520relevant%2520in%2520real-world%2520scenarios%2520where%2520a%2520model%250Aof%2520the%2520listener%2520may%2520not%2520be%2520available.%2520We%2520demonstrate%2520that%2520our%2520method%2520is%2520able%2520to%250Aalign%2520speakers%2520with%2520the%2520preferences%2520of%2520simulated%2520listeners%2520on%2520image%250Aclassification%2520across%2520three%2520datasets%252C%2520and%2520further%2520validate%2520that%2520pragmatic%250Aexplanations%2520generated%2520with%2520our%2520method%2520improve%2520the%2520classification%2520accuracy%2520of%250Aparticipants%2520in%2520a%2520user%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Preference%20Optimization%20for%20Adaptive%20Concept-based%20Explanations&entry.906535625=Jacopo%20Teneggi%20and%20Zhenzhen%20Wang%20and%20Paul%20H.%20Yi%20and%20Tianmin%20Shu%20and%20Jeremias%20Sulam&entry.1292438233=%20%20Concept-based%20explanation%20methods%20aim%20at%20making%20machine%20learning%20models%20more%0Atransparent%20by%20finding%20the%20most%20important%20semantic%20features%20of%20an%20input%20%28e.g.%2C%0Acolors%2C%20patterns%2C%20shapes%29%20for%20a%20given%20prediction%20task.%20However%2C%20these%20methods%0Agenerally%20ignore%20the%20communicative%20context%20of%20explanations%2C%20such%20as%20the%0Apreferences%20of%20a%20listener.%20For%20example%2C%20medical%20doctors%20understand%20explanations%0Ain%20terms%20of%20clinical%20markers%2C%20but%20patients%20may%20not%2C%20needing%20a%20different%0Avocabulary%20to%20rationalize%20the%20same%20diagnosis.%20We%20address%20this%20gap%20with%0Alistener-adaptive%20explanations%20grounded%20in%20principles%20of%20pragmatic%20reasoning%0Aand%20the%20rational%20speech%20act.%20We%20introduce%20an%20iterative%20training%20procedure%20based%0Aon%20direct%20preference%20optimization%20where%20a%20speaker%20learns%20to%20compose%0Aexplanations%20that%20maximize%20communicative%20utility%20for%20a%20listener.%20Our%20approach%0Aonly%20needs%20access%20to%20pairwise%20preferences%2C%20which%20can%20be%20collected%20from%20human%0Afeedback%2C%20making%20it%20particularly%20relevant%20in%20real-world%20scenarios%20where%20a%20model%0Aof%20the%20listener%20may%20not%20be%20available.%20We%20demonstrate%20that%20our%20method%20is%20able%20to%0Aalign%20speakers%20with%20the%20preferences%20of%20simulated%20listeners%20on%20image%0Aclassification%20across%20three%20datasets%2C%20and%20further%20validate%20that%20pragmatic%0Aexplanations%20generated%20with%20our%20method%20improve%20the%20classification%20accuracy%20of%0Aparticipants%20in%20a%20user%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15626v2&entry.124074799=Read"},
{"title": "Temporal Misalignment Attacks against Multimodal Perception in\n  Autonomous Driving", "author": "Md Hasan Shahriar and Md Mohaimin Al Barat and Harshavardhan Sundar and Ning Zhang and Naren Ramakrishnan and Y. Thomas Hou and Wenjing Lou", "abstract": "  Multimodal fusion (MMF) plays a critical role in the perception of autonomous\ndriving, which primarily fuses camera and LiDAR streams for a comprehensive and\nefficient scene understanding. However, its strict reliance on precise temporal\nsynchronization exposes it to new vulnerabilities. In this paper, we introduce\nDejaVu, an attack that exploits the in-vehicular network and induces delays\nacross sensor streams to create subtle temporal misalignments, severely\ndegrading downstream MMF-based perception tasks. Our comprehensive attack\nanalysis across different models and datasets reveals the sensors'\ntask-specific imbalanced sensitivities: object detection is overly dependent on\nLiDAR inputs, while object tracking is highly reliant on the camera inputs.\nConsequently, with a single-frame LiDAR delay, an attacker can reduce the car\ndetection mAP by up to 88.5%, while with a three-frame camera delay, multiple\nobject tracking accuracy (MOTA) for car drops by 73%. We further demonstrated\ntwo attack scenarios using an automotive Ethernet testbed for\nhardware-in-the-loop validation and the Autoware stack for end-to-end AD\nsimulation, demonstrating the feasibility of the DejaVu attack and its severe\nimpact, such as collisions and phantom braking.\n", "link": "http://arxiv.org/abs/2507.09095v2", "date": "2025-10-01", "relevancy": 2.0375, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5025}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Misalignment%20Attacks%20against%20Multimodal%20Perception%20in%0A%20%20Autonomous%20Driving&body=Title%3A%20Temporal%20Misalignment%20Attacks%20against%20Multimodal%20Perception%20in%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Md%20Hasan%20Shahriar%20and%20Md%20Mohaimin%20Al%20Barat%20and%20Harshavardhan%20Sundar%20and%20Ning%20Zhang%20and%20Naren%20Ramakrishnan%20and%20Y.%20Thomas%20Hou%20and%20Wenjing%20Lou%0AAbstract%3A%20%20%20Multimodal%20fusion%20%28MMF%29%20plays%20a%20critical%20role%20in%20the%20perception%20of%20autonomous%0Adriving%2C%20which%20primarily%20fuses%20camera%20and%20LiDAR%20streams%20for%20a%20comprehensive%20and%0Aefficient%20scene%20understanding.%20However%2C%20its%20strict%20reliance%20on%20precise%20temporal%0Asynchronization%20exposes%20it%20to%20new%20vulnerabilities.%20In%20this%20paper%2C%20we%20introduce%0ADejaVu%2C%20an%20attack%20that%20exploits%20the%20in-vehicular%20network%20and%20induces%20delays%0Aacross%20sensor%20streams%20to%20create%20subtle%20temporal%20misalignments%2C%20severely%0Adegrading%20downstream%20MMF-based%20perception%20tasks.%20Our%20comprehensive%20attack%0Aanalysis%20across%20different%20models%20and%20datasets%20reveals%20the%20sensors%27%0Atask-specific%20imbalanced%20sensitivities%3A%20object%20detection%20is%20overly%20dependent%20on%0ALiDAR%20inputs%2C%20while%20object%20tracking%20is%20highly%20reliant%20on%20the%20camera%20inputs.%0AConsequently%2C%20with%20a%20single-frame%20LiDAR%20delay%2C%20an%20attacker%20can%20reduce%20the%20car%0Adetection%20mAP%20by%20up%20to%2088.5%25%2C%20while%20with%20a%20three-frame%20camera%20delay%2C%20multiple%0Aobject%20tracking%20accuracy%20%28MOTA%29%20for%20car%20drops%20by%2073%25.%20We%20further%20demonstrated%0Atwo%20attack%20scenarios%20using%20an%20automotive%20Ethernet%20testbed%20for%0Ahardware-in-the-loop%20validation%20and%20the%20Autoware%20stack%20for%20end-to-end%20AD%0Asimulation%2C%20demonstrating%20the%20feasibility%20of%20the%20DejaVu%20attack%20and%20its%20severe%0Aimpact%2C%20such%20as%20collisions%20and%20phantom%20braking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Misalignment%2520Attacks%2520against%2520Multimodal%2520Perception%2520in%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DMd%2520Hasan%2520Shahriar%2520and%2520Md%2520Mohaimin%2520Al%2520Barat%2520and%2520Harshavardhan%2520Sundar%2520and%2520Ning%2520Zhang%2520and%2520Naren%2520Ramakrishnan%2520and%2520Y.%2520Thomas%2520Hou%2520and%2520Wenjing%2520Lou%26entry.1292438233%3D%2520%2520Multimodal%2520fusion%2520%2528MMF%2529%2520plays%2520a%2520critical%2520role%2520in%2520the%2520perception%2520of%2520autonomous%250Adriving%252C%2520which%2520primarily%2520fuses%2520camera%2520and%2520LiDAR%2520streams%2520for%2520a%2520comprehensive%2520and%250Aefficient%2520scene%2520understanding.%2520However%252C%2520its%2520strict%2520reliance%2520on%2520precise%2520temporal%250Asynchronization%2520exposes%2520it%2520to%2520new%2520vulnerabilities.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ADejaVu%252C%2520an%2520attack%2520that%2520exploits%2520the%2520in-vehicular%2520network%2520and%2520induces%2520delays%250Aacross%2520sensor%2520streams%2520to%2520create%2520subtle%2520temporal%2520misalignments%252C%2520severely%250Adegrading%2520downstream%2520MMF-based%2520perception%2520tasks.%2520Our%2520comprehensive%2520attack%250Aanalysis%2520across%2520different%2520models%2520and%2520datasets%2520reveals%2520the%2520sensors%2527%250Atask-specific%2520imbalanced%2520sensitivities%253A%2520object%2520detection%2520is%2520overly%2520dependent%2520on%250ALiDAR%2520inputs%252C%2520while%2520object%2520tracking%2520is%2520highly%2520reliant%2520on%2520the%2520camera%2520inputs.%250AConsequently%252C%2520with%2520a%2520single-frame%2520LiDAR%2520delay%252C%2520an%2520attacker%2520can%2520reduce%2520the%2520car%250Adetection%2520mAP%2520by%2520up%2520to%252088.5%2525%252C%2520while%2520with%2520a%2520three-frame%2520camera%2520delay%252C%2520multiple%250Aobject%2520tracking%2520accuracy%2520%2528MOTA%2529%2520for%2520car%2520drops%2520by%252073%2525.%2520We%2520further%2520demonstrated%250Atwo%2520attack%2520scenarios%2520using%2520an%2520automotive%2520Ethernet%2520testbed%2520for%250Ahardware-in-the-loop%2520validation%2520and%2520the%2520Autoware%2520stack%2520for%2520end-to-end%2520AD%250Asimulation%252C%2520demonstrating%2520the%2520feasibility%2520of%2520the%2520DejaVu%2520attack%2520and%2520its%2520severe%250Aimpact%252C%2520such%2520as%2520collisions%2520and%2520phantom%2520braking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Misalignment%20Attacks%20against%20Multimodal%20Perception%20in%0A%20%20Autonomous%20Driving&entry.906535625=Md%20Hasan%20Shahriar%20and%20Md%20Mohaimin%20Al%20Barat%20and%20Harshavardhan%20Sundar%20and%20Ning%20Zhang%20and%20Naren%20Ramakrishnan%20and%20Y.%20Thomas%20Hou%20and%20Wenjing%20Lou&entry.1292438233=%20%20Multimodal%20fusion%20%28MMF%29%20plays%20a%20critical%20role%20in%20the%20perception%20of%20autonomous%0Adriving%2C%20which%20primarily%20fuses%20camera%20and%20LiDAR%20streams%20for%20a%20comprehensive%20and%0Aefficient%20scene%20understanding.%20However%2C%20its%20strict%20reliance%20on%20precise%20temporal%0Asynchronization%20exposes%20it%20to%20new%20vulnerabilities.%20In%20this%20paper%2C%20we%20introduce%0ADejaVu%2C%20an%20attack%20that%20exploits%20the%20in-vehicular%20network%20and%20induces%20delays%0Aacross%20sensor%20streams%20to%20create%20subtle%20temporal%20misalignments%2C%20severely%0Adegrading%20downstream%20MMF-based%20perception%20tasks.%20Our%20comprehensive%20attack%0Aanalysis%20across%20different%20models%20and%20datasets%20reveals%20the%20sensors%27%0Atask-specific%20imbalanced%20sensitivities%3A%20object%20detection%20is%20overly%20dependent%20on%0ALiDAR%20inputs%2C%20while%20object%20tracking%20is%20highly%20reliant%20on%20the%20camera%20inputs.%0AConsequently%2C%20with%20a%20single-frame%20LiDAR%20delay%2C%20an%20attacker%20can%20reduce%20the%20car%0Adetection%20mAP%20by%20up%20to%2088.5%25%2C%20while%20with%20a%20three-frame%20camera%20delay%2C%20multiple%0Aobject%20tracking%20accuracy%20%28MOTA%29%20for%20car%20drops%20by%2073%25.%20We%20further%20demonstrated%0Atwo%20attack%20scenarios%20using%20an%20automotive%20Ethernet%20testbed%20for%0Ahardware-in-the-loop%20validation%20and%20the%20Autoware%20stack%20for%20end-to-end%20AD%0Asimulation%2C%20demonstrating%20the%20feasibility%20of%20the%20DejaVu%20attack%20and%20its%20severe%0Aimpact%2C%20such%20as%20collisions%20and%20phantom%20braking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09095v2&entry.124074799=Read"},
{"title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "author": "Yuqiao Meng and Luoxi Tang and Feiyang Yu and Jinyuan Jia and Guanhua Yan and Ping Yang and Zhaohan Xi", "abstract": "  Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.\n", "link": "http://arxiv.org/abs/2509.23573v2", "date": "2025-10-01", "relevancy": 2.018, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5023}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Vulnerabilities%20of%20LLM-Assisted%20Cyber%20Threat%20Intelligence&body=Title%3A%20Uncovering%20Vulnerabilities%20of%20LLM-Assisted%20Cyber%20Threat%20Intelligence%0AAuthor%3A%20Yuqiao%20Meng%20and%20Luoxi%20Tang%20and%20Feiyang%20Yu%20and%20Jinyuan%20Jia%20and%20Guanhua%20Yan%20and%20Ping%20Yang%20and%20Zhaohan%20Xi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intensively%20used%20to%20assist%20security%20analysts%0Ain%20counteracting%20the%20rapid%20exploitation%20of%20cyber%20threats%2C%20wherein%20LLMs%20offer%0Acyber%20threat%20intelligence%20%28CTI%29%20to%20support%20vulnerability%20assessment%20and%0Aincident%20response.%20While%20recent%20work%20has%20shown%20that%20LLMs%20can%20support%20a%20wide%0Arange%20of%20CTI%20tasks%20such%20as%20threat%20analysis%2C%20vulnerability%20detection%2C%20and%0Aintrusion%20defense%2C%20significant%20performance%20gaps%20persist%20in%20practical%0Adeployments.%20In%20this%20paper%2C%20we%20investigate%20the%20intrinsic%20vulnerabilities%20of%0ALLMs%20in%20CTI%2C%20focusing%20on%20challenges%20that%20arise%20from%20the%20nature%20of%20the%20threat%0Alandscape%20itself%20rather%20than%20the%20model%20architecture.%20Using%20large-scale%0Aevaluations%20across%20multiple%20CTI%20benchmarks%20and%20real-world%20threat%20reports%2C%20we%0Aintroduce%20a%20novel%20categorization%20methodology%20that%20integrates%20stratification%2C%0Aautoregressive%20refinement%2C%20and%20human-in-the-loop%20supervision%20to%20reliably%0Aanalyze%20failure%20instances.%20Through%20extensive%20experiments%20and%20human%20inspections%2C%0Awe%20reveal%20three%20fundamental%20vulnerabilities%3A%20spurious%20correlations%2C%0Acontradictory%20knowledge%2C%20and%20constrained%20generalization%2C%20that%20limit%20LLMs%20in%0Aeffectively%20supporting%20CTI.%20Subsequently%2C%20we%20provide%20actionable%20insights%20for%0Adesigning%20more%20robust%20LLM-powered%20CTI%20systems%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Vulnerabilities%2520of%2520LLM-Assisted%2520Cyber%2520Threat%2520Intelligence%26entry.906535625%3DYuqiao%2520Meng%2520and%2520Luoxi%2520Tang%2520and%2520Feiyang%2520Yu%2520and%2520Jinyuan%2520Jia%2520and%2520Guanhua%2520Yan%2520and%2520Ping%2520Yang%2520and%2520Zhaohan%2520Xi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520intensively%2520used%2520to%2520assist%2520security%2520analysts%250Ain%2520counteracting%2520the%2520rapid%2520exploitation%2520of%2520cyber%2520threats%252C%2520wherein%2520LLMs%2520offer%250Acyber%2520threat%2520intelligence%2520%2528CTI%2529%2520to%2520support%2520vulnerability%2520assessment%2520and%250Aincident%2520response.%2520While%2520recent%2520work%2520has%2520shown%2520that%2520LLMs%2520can%2520support%2520a%2520wide%250Arange%2520of%2520CTI%2520tasks%2520such%2520as%2520threat%2520analysis%252C%2520vulnerability%2520detection%252C%2520and%250Aintrusion%2520defense%252C%2520significant%2520performance%2520gaps%2520persist%2520in%2520practical%250Adeployments.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520intrinsic%2520vulnerabilities%2520of%250ALLMs%2520in%2520CTI%252C%2520focusing%2520on%2520challenges%2520that%2520arise%2520from%2520the%2520nature%2520of%2520the%2520threat%250Alandscape%2520itself%2520rather%2520than%2520the%2520model%2520architecture.%2520Using%2520large-scale%250Aevaluations%2520across%2520multiple%2520CTI%2520benchmarks%2520and%2520real-world%2520threat%2520reports%252C%2520we%250Aintroduce%2520a%2520novel%2520categorization%2520methodology%2520that%2520integrates%2520stratification%252C%250Aautoregressive%2520refinement%252C%2520and%2520human-in-the-loop%2520supervision%2520to%2520reliably%250Aanalyze%2520failure%2520instances.%2520Through%2520extensive%2520experiments%2520and%2520human%2520inspections%252C%250Awe%2520reveal%2520three%2520fundamental%2520vulnerabilities%253A%2520spurious%2520correlations%252C%250Acontradictory%2520knowledge%252C%2520and%2520constrained%2520generalization%252C%2520that%2520limit%2520LLMs%2520in%250Aeffectively%2520supporting%2520CTI.%2520Subsequently%252C%2520we%2520provide%2520actionable%2520insights%2520for%250Adesigning%2520more%2520robust%2520LLM-powered%2520CTI%2520systems%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Vulnerabilities%20of%20LLM-Assisted%20Cyber%20Threat%20Intelligence&entry.906535625=Yuqiao%20Meng%20and%20Luoxi%20Tang%20and%20Feiyang%20Yu%20and%20Jinyuan%20Jia%20and%20Guanhua%20Yan%20and%20Ping%20Yang%20and%20Zhaohan%20Xi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20intensively%20used%20to%20assist%20security%20analysts%0Ain%20counteracting%20the%20rapid%20exploitation%20of%20cyber%20threats%2C%20wherein%20LLMs%20offer%0Acyber%20threat%20intelligence%20%28CTI%29%20to%20support%20vulnerability%20assessment%20and%0Aincident%20response.%20While%20recent%20work%20has%20shown%20that%20LLMs%20can%20support%20a%20wide%0Arange%20of%20CTI%20tasks%20such%20as%20threat%20analysis%2C%20vulnerability%20detection%2C%20and%0Aintrusion%20defense%2C%20significant%20performance%20gaps%20persist%20in%20practical%0Adeployments.%20In%20this%20paper%2C%20we%20investigate%20the%20intrinsic%20vulnerabilities%20of%0ALLMs%20in%20CTI%2C%20focusing%20on%20challenges%20that%20arise%20from%20the%20nature%20of%20the%20threat%0Alandscape%20itself%20rather%20than%20the%20model%20architecture.%20Using%20large-scale%0Aevaluations%20across%20multiple%20CTI%20benchmarks%20and%20real-world%20threat%20reports%2C%20we%0Aintroduce%20a%20novel%20categorization%20methodology%20that%20integrates%20stratification%2C%0Aautoregressive%20refinement%2C%20and%20human-in-the-loop%20supervision%20to%20reliably%0Aanalyze%20failure%20instances.%20Through%20extensive%20experiments%20and%20human%20inspections%2C%0Awe%20reveal%20three%20fundamental%20vulnerabilities%3A%20spurious%20correlations%2C%0Acontradictory%20knowledge%2C%20and%20constrained%20generalization%2C%20that%20limit%20LLMs%20in%0Aeffectively%20supporting%20CTI.%20Subsequently%2C%20we%20provide%20actionable%20insights%20for%0Adesigning%20more%20robust%20LLM-powered%20CTI%20systems%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23573v2&entry.124074799=Read"},
{"title": "Learning to Interact in World Latent for Team Coordination", "author": "Dongsu Lee and Daehee Lee and Yaru Niu and Honguk Woo and Amy Zhang and Ding Zhao", "abstract": "  This work presents a novel representation learning framework, interactive\nworld latent (IWoL), to facilitate team coordination in multi-agent\nreinforcement learning (MARL). Building effective representation for team\ncoordination is a challenging problem, due to the intricate dynamics emerging\nfrom multi-agent interaction and incomplete information induced by local\nobservations. Our key insight is to construct a learnable representation space\nthat jointly captures inter-agent relations and task-specific world information\nby directly modeling communication protocols. This representation, we maintain\nfully decentralized execution with implicit coordination, all while avoiding\nthe inherent drawbacks of explicit message passing, e.g., slower\ndecision-making, vulnerability to malicious attackers, and sensitivity to\nbandwidth constraints. In practice, our representation can be used not only as\nan implicit latent for each agent, but also as an explicit message for\ncommunication. Across four challenging MARL benchmarks, we evaluate both\nvariants and show that IWoL provides a simple yet powerful key for team\ncoordination. Moreover, we demonstrate that our representation can be combined\nwith existing MARL algorithms to further enhance their performance.\n", "link": "http://arxiv.org/abs/2509.25550v2", "date": "2025-10-01", "relevancy": 2.017, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5148}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Interact%20in%20World%20Latent%20for%20Team%20Coordination&body=Title%3A%20Learning%20to%20Interact%20in%20World%20Latent%20for%20Team%20Coordination%0AAuthor%3A%20Dongsu%20Lee%20and%20Daehee%20Lee%20and%20Yaru%20Niu%20and%20Honguk%20Woo%20and%20Amy%20Zhang%20and%20Ding%20Zhao%0AAbstract%3A%20%20%20This%20work%20presents%20a%20novel%20representation%20learning%20framework%2C%20interactive%0Aworld%20latent%20%28IWoL%29%2C%20to%20facilitate%20team%20coordination%20in%20multi-agent%0Areinforcement%20learning%20%28MARL%29.%20Building%20effective%20representation%20for%20team%0Acoordination%20is%20a%20challenging%20problem%2C%20due%20to%20the%20intricate%20dynamics%20emerging%0Afrom%20multi-agent%20interaction%20and%20incomplete%20information%20induced%20by%20local%0Aobservations.%20Our%20key%20insight%20is%20to%20construct%20a%20learnable%20representation%20space%0Athat%20jointly%20captures%20inter-agent%20relations%20and%20task-specific%20world%20information%0Aby%20directly%20modeling%20communication%20protocols.%20This%20representation%2C%20we%20maintain%0Afully%20decentralized%20execution%20with%20implicit%20coordination%2C%20all%20while%20avoiding%0Athe%20inherent%20drawbacks%20of%20explicit%20message%20passing%2C%20e.g.%2C%20slower%0Adecision-making%2C%20vulnerability%20to%20malicious%20attackers%2C%20and%20sensitivity%20to%0Abandwidth%20constraints.%20In%20practice%2C%20our%20representation%20can%20be%20used%20not%20only%20as%0Aan%20implicit%20latent%20for%20each%20agent%2C%20but%20also%20as%20an%20explicit%20message%20for%0Acommunication.%20Across%20four%20challenging%20MARL%20benchmarks%2C%20we%20evaluate%20both%0Avariants%20and%20show%20that%20IWoL%20provides%20a%20simple%20yet%20powerful%20key%20for%20team%0Acoordination.%20Moreover%2C%20we%20demonstrate%20that%20our%20representation%20can%20be%20combined%0Awith%20existing%20MARL%20algorithms%20to%20further%20enhance%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Interact%2520in%2520World%2520Latent%2520for%2520Team%2520Coordination%26entry.906535625%3DDongsu%2520Lee%2520and%2520Daehee%2520Lee%2520and%2520Yaru%2520Niu%2520and%2520Honguk%2520Woo%2520and%2520Amy%2520Zhang%2520and%2520Ding%2520Zhao%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520novel%2520representation%2520learning%2520framework%252C%2520interactive%250Aworld%2520latent%2520%2528IWoL%2529%252C%2520to%2520facilitate%2520team%2520coordination%2520in%2520multi-agent%250Areinforcement%2520learning%2520%2528MARL%2529.%2520Building%2520effective%2520representation%2520for%2520team%250Acoordination%2520is%2520a%2520challenging%2520problem%252C%2520due%2520to%2520the%2520intricate%2520dynamics%2520emerging%250Afrom%2520multi-agent%2520interaction%2520and%2520incomplete%2520information%2520induced%2520by%2520local%250Aobservations.%2520Our%2520key%2520insight%2520is%2520to%2520construct%2520a%2520learnable%2520representation%2520space%250Athat%2520jointly%2520captures%2520inter-agent%2520relations%2520and%2520task-specific%2520world%2520information%250Aby%2520directly%2520modeling%2520communication%2520protocols.%2520This%2520representation%252C%2520we%2520maintain%250Afully%2520decentralized%2520execution%2520with%2520implicit%2520coordination%252C%2520all%2520while%2520avoiding%250Athe%2520inherent%2520drawbacks%2520of%2520explicit%2520message%2520passing%252C%2520e.g.%252C%2520slower%250Adecision-making%252C%2520vulnerability%2520to%2520malicious%2520attackers%252C%2520and%2520sensitivity%2520to%250Abandwidth%2520constraints.%2520In%2520practice%252C%2520our%2520representation%2520can%2520be%2520used%2520not%2520only%2520as%250Aan%2520implicit%2520latent%2520for%2520each%2520agent%252C%2520but%2520also%2520as%2520an%2520explicit%2520message%2520for%250Acommunication.%2520Across%2520four%2520challenging%2520MARL%2520benchmarks%252C%2520we%2520evaluate%2520both%250Avariants%2520and%2520show%2520that%2520IWoL%2520provides%2520a%2520simple%2520yet%2520powerful%2520key%2520for%2520team%250Acoordination.%2520Moreover%252C%2520we%2520demonstrate%2520that%2520our%2520representation%2520can%2520be%2520combined%250Awith%2520existing%2520MARL%2520algorithms%2520to%2520further%2520enhance%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Interact%20in%20World%20Latent%20for%20Team%20Coordination&entry.906535625=Dongsu%20Lee%20and%20Daehee%20Lee%20and%20Yaru%20Niu%20and%20Honguk%20Woo%20and%20Amy%20Zhang%20and%20Ding%20Zhao&entry.1292438233=%20%20This%20work%20presents%20a%20novel%20representation%20learning%20framework%2C%20interactive%0Aworld%20latent%20%28IWoL%29%2C%20to%20facilitate%20team%20coordination%20in%20multi-agent%0Areinforcement%20learning%20%28MARL%29.%20Building%20effective%20representation%20for%20team%0Acoordination%20is%20a%20challenging%20problem%2C%20due%20to%20the%20intricate%20dynamics%20emerging%0Afrom%20multi-agent%20interaction%20and%20incomplete%20information%20induced%20by%20local%0Aobservations.%20Our%20key%20insight%20is%20to%20construct%20a%20learnable%20representation%20space%0Athat%20jointly%20captures%20inter-agent%20relations%20and%20task-specific%20world%20information%0Aby%20directly%20modeling%20communication%20protocols.%20This%20representation%2C%20we%20maintain%0Afully%20decentralized%20execution%20with%20implicit%20coordination%2C%20all%20while%20avoiding%0Athe%20inherent%20drawbacks%20of%20explicit%20message%20passing%2C%20e.g.%2C%20slower%0Adecision-making%2C%20vulnerability%20to%20malicious%20attackers%2C%20and%20sensitivity%20to%0Abandwidth%20constraints.%20In%20practice%2C%20our%20representation%20can%20be%20used%20not%20only%20as%0Aan%20implicit%20latent%20for%20each%20agent%2C%20but%20also%20as%20an%20explicit%20message%20for%0Acommunication.%20Across%20four%20challenging%20MARL%20benchmarks%2C%20we%20evaluate%20both%0Avariants%20and%20show%20that%20IWoL%20provides%20a%20simple%20yet%20powerful%20key%20for%20team%0Acoordination.%20Moreover%2C%20we%20demonstrate%20that%20our%20representation%20can%20be%20combined%0Awith%20existing%20MARL%20algorithms%20to%20further%20enhance%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25550v2&entry.124074799=Read"},
{"title": "DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent\n  Autonomous Racing Overtakes", "author": "Trent Weiss and Amar Kulkarni and Madhur Behl", "abstract": "  A significant challenge in autonomous racing is to generate overtaking\nmaneuvers. Racing agents must execute these maneuvers on complex racetracks\nwith little room for error. Optimization techniques and graph-based methods\nhave been proposed, but these methods often rely on oversimplified assumptions\nfor collision-avoidance and dynamic constraints. In this work, we present an\napproach to trajectory synthesis based on an extension of the Differential\nBayesian Filtering framework. Our approach for collision-free trajectory\nsynthesis frames the problem as one of Bayesian Inference over the space of\nComposite Bezier Curves. Our method is derivative-free, does not require a\nspherical approximation of the vehicle footprint, linearization of constraints,\nor simplifying upper bounds on collision avoidance. We conduct a closed-loop\nanalysis of DBF-MA and find it successfully overtakes an opponent in 87% of\ntested scenarios, outperforming existing methods in autonomous overtaking.\n", "link": "http://arxiv.org/abs/2509.22937v2", "date": "2025-10-01", "relevancy": 2.0138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.582}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4916}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBF-MA%3A%20A%20Differential%20Bayesian%20Filtering%20Planner%20for%20Multi-Agent%0A%20%20Autonomous%20Racing%20Overtakes&body=Title%3A%20DBF-MA%3A%20A%20Differential%20Bayesian%20Filtering%20Planner%20for%20Multi-Agent%0A%20%20Autonomous%20Racing%20Overtakes%0AAuthor%3A%20Trent%20Weiss%20and%20Amar%20Kulkarni%20and%20Madhur%20Behl%0AAbstract%3A%20%20%20A%20significant%20challenge%20in%20autonomous%20racing%20is%20to%20generate%20overtaking%0Amaneuvers.%20Racing%20agents%20must%20execute%20these%20maneuvers%20on%20complex%20racetracks%0Awith%20little%20room%20for%20error.%20Optimization%20techniques%20and%20graph-based%20methods%0Ahave%20been%20proposed%2C%20but%20these%20methods%20often%20rely%20on%20oversimplified%20assumptions%0Afor%20collision-avoidance%20and%20dynamic%20constraints.%20In%20this%20work%2C%20we%20present%20an%0Aapproach%20to%20trajectory%20synthesis%20based%20on%20an%20extension%20of%20the%20Differential%0ABayesian%20Filtering%20framework.%20Our%20approach%20for%20collision-free%20trajectory%0Asynthesis%20frames%20the%20problem%20as%20one%20of%20Bayesian%20Inference%20over%20the%20space%20of%0AComposite%20Bezier%20Curves.%20Our%20method%20is%20derivative-free%2C%20does%20not%20require%20a%0Aspherical%20approximation%20of%20the%20vehicle%20footprint%2C%20linearization%20of%20constraints%2C%0Aor%20simplifying%20upper%20bounds%20on%20collision%20avoidance.%20We%20conduct%20a%20closed-loop%0Aanalysis%20of%20DBF-MA%20and%20find%20it%20successfully%20overtakes%20an%20opponent%20in%2087%25%20of%0Atested%20scenarios%2C%20outperforming%20existing%20methods%20in%20autonomous%20overtaking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBF-MA%253A%2520A%2520Differential%2520Bayesian%2520Filtering%2520Planner%2520for%2520Multi-Agent%250A%2520%2520Autonomous%2520Racing%2520Overtakes%26entry.906535625%3DTrent%2520Weiss%2520and%2520Amar%2520Kulkarni%2520and%2520Madhur%2520Behl%26entry.1292438233%3D%2520%2520A%2520significant%2520challenge%2520in%2520autonomous%2520racing%2520is%2520to%2520generate%2520overtaking%250Amaneuvers.%2520Racing%2520agents%2520must%2520execute%2520these%2520maneuvers%2520on%2520complex%2520racetracks%250Awith%2520little%2520room%2520for%2520error.%2520Optimization%2520techniques%2520and%2520graph-based%2520methods%250Ahave%2520been%2520proposed%252C%2520but%2520these%2520methods%2520often%2520rely%2520on%2520oversimplified%2520assumptions%250Afor%2520collision-avoidance%2520and%2520dynamic%2520constraints.%2520In%2520this%2520work%252C%2520we%2520present%2520an%250Aapproach%2520to%2520trajectory%2520synthesis%2520based%2520on%2520an%2520extension%2520of%2520the%2520Differential%250ABayesian%2520Filtering%2520framework.%2520Our%2520approach%2520for%2520collision-free%2520trajectory%250Asynthesis%2520frames%2520the%2520problem%2520as%2520one%2520of%2520Bayesian%2520Inference%2520over%2520the%2520space%2520of%250AComposite%2520Bezier%2520Curves.%2520Our%2520method%2520is%2520derivative-free%252C%2520does%2520not%2520require%2520a%250Aspherical%2520approximation%2520of%2520the%2520vehicle%2520footprint%252C%2520linearization%2520of%2520constraints%252C%250Aor%2520simplifying%2520upper%2520bounds%2520on%2520collision%2520avoidance.%2520We%2520conduct%2520a%2520closed-loop%250Aanalysis%2520of%2520DBF-MA%2520and%2520find%2520it%2520successfully%2520overtakes%2520an%2520opponent%2520in%252087%2525%2520of%250Atested%2520scenarios%252C%2520outperforming%2520existing%2520methods%2520in%2520autonomous%2520overtaking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBF-MA%3A%20A%20Differential%20Bayesian%20Filtering%20Planner%20for%20Multi-Agent%0A%20%20Autonomous%20Racing%20Overtakes&entry.906535625=Trent%20Weiss%20and%20Amar%20Kulkarni%20and%20Madhur%20Behl&entry.1292438233=%20%20A%20significant%20challenge%20in%20autonomous%20racing%20is%20to%20generate%20overtaking%0Amaneuvers.%20Racing%20agents%20must%20execute%20these%20maneuvers%20on%20complex%20racetracks%0Awith%20little%20room%20for%20error.%20Optimization%20techniques%20and%20graph-based%20methods%0Ahave%20been%20proposed%2C%20but%20these%20methods%20often%20rely%20on%20oversimplified%20assumptions%0Afor%20collision-avoidance%20and%20dynamic%20constraints.%20In%20this%20work%2C%20we%20present%20an%0Aapproach%20to%20trajectory%20synthesis%20based%20on%20an%20extension%20of%20the%20Differential%0ABayesian%20Filtering%20framework.%20Our%20approach%20for%20collision-free%20trajectory%0Asynthesis%20frames%20the%20problem%20as%20one%20of%20Bayesian%20Inference%20over%20the%20space%20of%0AComposite%20Bezier%20Curves.%20Our%20method%20is%20derivative-free%2C%20does%20not%20require%20a%0Aspherical%20approximation%20of%20the%20vehicle%20footprint%2C%20linearization%20of%20constraints%2C%0Aor%20simplifying%20upper%20bounds%20on%20collision%20avoidance.%20We%20conduct%20a%20closed-loop%0Aanalysis%20of%20DBF-MA%20and%20find%20it%20successfully%20overtakes%20an%20opponent%20in%2087%25%20of%0Atested%20scenarios%2C%20outperforming%20existing%20methods%20in%20autonomous%20overtaking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22937v2&entry.124074799=Read"},
{"title": "Rapid training of Hamiltonian graph networks using random features", "author": "Atamert Rahma and Chinmay Datar and Ana Cukarska and Felix Dietrich", "abstract": "  Learning dynamical systems that respect physical symmetries and constraints\nremains a fundamental challenge in data-driven modeling. Integrating physical\nlaws with graph neural networks facilitates principled modeling of complex\nN-body dynamics and yields accurate and permutation-invariant models. However,\ntraining graph neural networks with iterative, gradient-based optimization\nalgorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,\nespecially for large, complex systems. In comparison to 15 different\noptimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained\nup to 600x faster--but with comparable accuracy--by replacing iterative\noptimization with random feature-based parameter construction. We show robust\nperformance in diverse simulations, including N-body mass-spring and molecular\nsystems in up to 3 dimensions and 10,000 particles with different geometries,\nwhile retaining essential physical invariances with respect to permutation,\nrotation, and translation. Our proposed approach is benchmarked using a NeurIPS\n2022 Datasets and Benchmarks Track publication to further demonstrate its\nversatility. We reveal that even when trained on minimal 8-node systems, the\nmodel can generalize in a zero-shot manner to systems as large as 4096 nodes\nwithout retraining. Our work challenges the dominance of iterative\ngradient-descent-based optimization algorithms for training neural network\nmodels for physical systems.\n", "link": "http://arxiv.org/abs/2506.06558v2", "date": "2025-10-01", "relevancy": 2.003, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5079}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4961}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rapid%20training%20of%20Hamiltonian%20graph%20networks%20using%20random%20features&body=Title%3A%20Rapid%20training%20of%20Hamiltonian%20graph%20networks%20using%20random%20features%0AAuthor%3A%20Atamert%20Rahma%20and%20Chinmay%20Datar%20and%20Ana%20Cukarska%20and%20Felix%20Dietrich%0AAbstract%3A%20%20%20Learning%20dynamical%20systems%20that%20respect%20physical%20symmetries%20and%20constraints%0Aremains%20a%20fundamental%20challenge%20in%20data-driven%20modeling.%20Integrating%20physical%0Alaws%20with%20graph%20neural%20networks%20facilitates%20principled%20modeling%20of%20complex%0AN-body%20dynamics%20and%20yields%20accurate%20and%20permutation-invariant%20models.%20However%2C%0Atraining%20graph%20neural%20networks%20with%20iterative%2C%20gradient-based%20optimization%0Aalgorithms%20%28e.g.%2C%20Adam%2C%20RMSProp%2C%20LBFGS%29%20often%20leads%20to%20slow%20training%2C%0Aespecially%20for%20large%2C%20complex%20systems.%20In%20comparison%20to%2015%20different%0Aoptimizers%2C%20we%20demonstrate%20that%20Hamiltonian%20Graph%20Networks%20%28HGN%29%20can%20be%20trained%0Aup%20to%20600x%20faster--but%20with%20comparable%20accuracy--by%20replacing%20iterative%0Aoptimization%20with%20random%20feature-based%20parameter%20construction.%20We%20show%20robust%0Aperformance%20in%20diverse%20simulations%2C%20including%20N-body%20mass-spring%20and%20molecular%0Asystems%20in%20up%20to%203%20dimensions%20and%2010%2C000%20particles%20with%20different%20geometries%2C%0Awhile%20retaining%20essential%20physical%20invariances%20with%20respect%20to%20permutation%2C%0Arotation%2C%20and%20translation.%20Our%20proposed%20approach%20is%20benchmarked%20using%20a%20NeurIPS%0A2022%20Datasets%20and%20Benchmarks%20Track%20publication%20to%20further%20demonstrate%20its%0Aversatility.%20We%20reveal%20that%20even%20when%20trained%20on%20minimal%208-node%20systems%2C%20the%0Amodel%20can%20generalize%20in%20a%20zero-shot%20manner%20to%20systems%20as%20large%20as%204096%20nodes%0Awithout%20retraining.%20Our%20work%20challenges%20the%20dominance%20of%20iterative%0Agradient-descent-based%20optimization%20algorithms%20for%20training%20neural%20network%0Amodels%20for%20physical%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapid%2520training%2520of%2520Hamiltonian%2520graph%2520networks%2520using%2520random%2520features%26entry.906535625%3DAtamert%2520Rahma%2520and%2520Chinmay%2520Datar%2520and%2520Ana%2520Cukarska%2520and%2520Felix%2520Dietrich%26entry.1292438233%3D%2520%2520Learning%2520dynamical%2520systems%2520that%2520respect%2520physical%2520symmetries%2520and%2520constraints%250Aremains%2520a%2520fundamental%2520challenge%2520in%2520data-driven%2520modeling.%2520Integrating%2520physical%250Alaws%2520with%2520graph%2520neural%2520networks%2520facilitates%2520principled%2520modeling%2520of%2520complex%250AN-body%2520dynamics%2520and%2520yields%2520accurate%2520and%2520permutation-invariant%2520models.%2520However%252C%250Atraining%2520graph%2520neural%2520networks%2520with%2520iterative%252C%2520gradient-based%2520optimization%250Aalgorithms%2520%2528e.g.%252C%2520Adam%252C%2520RMSProp%252C%2520LBFGS%2529%2520often%2520leads%2520to%2520slow%2520training%252C%250Aespecially%2520for%2520large%252C%2520complex%2520systems.%2520In%2520comparison%2520to%252015%2520different%250Aoptimizers%252C%2520we%2520demonstrate%2520that%2520Hamiltonian%2520Graph%2520Networks%2520%2528HGN%2529%2520can%2520be%2520trained%250Aup%2520to%2520600x%2520faster--but%2520with%2520comparable%2520accuracy--by%2520replacing%2520iterative%250Aoptimization%2520with%2520random%2520feature-based%2520parameter%2520construction.%2520We%2520show%2520robust%250Aperformance%2520in%2520diverse%2520simulations%252C%2520including%2520N-body%2520mass-spring%2520and%2520molecular%250Asystems%2520in%2520up%2520to%25203%2520dimensions%2520and%252010%252C000%2520particles%2520with%2520different%2520geometries%252C%250Awhile%2520retaining%2520essential%2520physical%2520invariances%2520with%2520respect%2520to%2520permutation%252C%250Arotation%252C%2520and%2520translation.%2520Our%2520proposed%2520approach%2520is%2520benchmarked%2520using%2520a%2520NeurIPS%250A2022%2520Datasets%2520and%2520Benchmarks%2520Track%2520publication%2520to%2520further%2520demonstrate%2520its%250Aversatility.%2520We%2520reveal%2520that%2520even%2520when%2520trained%2520on%2520minimal%25208-node%2520systems%252C%2520the%250Amodel%2520can%2520generalize%2520in%2520a%2520zero-shot%2520manner%2520to%2520systems%2520as%2520large%2520as%25204096%2520nodes%250Awithout%2520retraining.%2520Our%2520work%2520challenges%2520the%2520dominance%2520of%2520iterative%250Agradient-descent-based%2520optimization%2520algorithms%2520for%2520training%2520neural%2520network%250Amodels%2520for%2520physical%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rapid%20training%20of%20Hamiltonian%20graph%20networks%20using%20random%20features&entry.906535625=Atamert%20Rahma%20and%20Chinmay%20Datar%20and%20Ana%20Cukarska%20and%20Felix%20Dietrich&entry.1292438233=%20%20Learning%20dynamical%20systems%20that%20respect%20physical%20symmetries%20and%20constraints%0Aremains%20a%20fundamental%20challenge%20in%20data-driven%20modeling.%20Integrating%20physical%0Alaws%20with%20graph%20neural%20networks%20facilitates%20principled%20modeling%20of%20complex%0AN-body%20dynamics%20and%20yields%20accurate%20and%20permutation-invariant%20models.%20However%2C%0Atraining%20graph%20neural%20networks%20with%20iterative%2C%20gradient-based%20optimization%0Aalgorithms%20%28e.g.%2C%20Adam%2C%20RMSProp%2C%20LBFGS%29%20often%20leads%20to%20slow%20training%2C%0Aespecially%20for%20large%2C%20complex%20systems.%20In%20comparison%20to%2015%20different%0Aoptimizers%2C%20we%20demonstrate%20that%20Hamiltonian%20Graph%20Networks%20%28HGN%29%20can%20be%20trained%0Aup%20to%20600x%20faster--but%20with%20comparable%20accuracy--by%20replacing%20iterative%0Aoptimization%20with%20random%20feature-based%20parameter%20construction.%20We%20show%20robust%0Aperformance%20in%20diverse%20simulations%2C%20including%20N-body%20mass-spring%20and%20molecular%0Asystems%20in%20up%20to%203%20dimensions%20and%2010%2C000%20particles%20with%20different%20geometries%2C%0Awhile%20retaining%20essential%20physical%20invariances%20with%20respect%20to%20permutation%2C%0Arotation%2C%20and%20translation.%20Our%20proposed%20approach%20is%20benchmarked%20using%20a%20NeurIPS%0A2022%20Datasets%20and%20Benchmarks%20Track%20publication%20to%20further%20demonstrate%20its%0Aversatility.%20We%20reveal%20that%20even%20when%20trained%20on%20minimal%208-node%20systems%2C%20the%0Amodel%20can%20generalize%20in%20a%20zero-shot%20manner%20to%20systems%20as%20large%20as%204096%20nodes%0Awithout%20retraining.%20Our%20work%20challenges%20the%20dominance%20of%20iterative%0Agradient-descent-based%20optimization%20algorithms%20for%20training%20neural%20network%0Amodels%20for%20physical%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06558v2&entry.124074799=Read"},
{"title": "Automatically Generating Web Applications from Requirements Via\n  Multi-Agent Test-Driven Development", "author": "Yuxuan Wan and Tingshuo Liang and Jiakai Xu and Jingyu Xiao and Yintong Huo and Michael R. Lyu", "abstract": "  Developing full-stack web applications is complex and time-intensive,\ndemanding proficiency across diverse technologies and frameworks. Although\nrecent advances in multimodal large language models (MLLMs) enable automated\nwebpage generation from visual inputs, current solutions remain limited to\nfront-end tasks and fail to deliver fully functional applications. In this\nwork, we introduce TDDev, the first test-driven development (TDD)-enabled\nLLM-agent framework for end-to-end full-stack web application generation. Given\na natural language description or design image, TDDev automatically derives\nexecutable test cases, generates front-end and back-end code, simulates user\ninteractions, and iteratively refines the implementation until all requirements\nare satisfied. Our framework addresses key challenges in full-stack automation,\nincluding underspecified user requirements, complex interdependencies among\nmultiple files, and the need for both functional correctness and visual\nfidelity. Through extensive experiments on diverse application scenarios, TDDev\nachieves a 14.4% improvement on overall accuracy compared to state-of-the-art\nbaselines, demonstrating its effectiveness in producing reliable, high-quality\nweb applications without requiring manual intervention.\n", "link": "http://arxiv.org/abs/2509.25297v2", "date": "2025-10-01", "relevancy": 1.9952, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4862}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatically%20Generating%20Web%20Applications%20from%20Requirements%20Via%0A%20%20Multi-Agent%20Test-Driven%20Development&body=Title%3A%20Automatically%20Generating%20Web%20Applications%20from%20Requirements%20Via%0A%20%20Multi-Agent%20Test-Driven%20Development%0AAuthor%3A%20Yuxuan%20Wan%20and%20Tingshuo%20Liang%20and%20Jiakai%20Xu%20and%20Jingyu%20Xiao%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Developing%20full-stack%20web%20applications%20is%20complex%20and%20time-intensive%2C%0Ademanding%20proficiency%20across%20diverse%20technologies%20and%20frameworks.%20Although%0Arecent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20enable%20automated%0Awebpage%20generation%20from%20visual%20inputs%2C%20current%20solutions%20remain%20limited%20to%0Afront-end%20tasks%20and%20fail%20to%20deliver%20fully%20functional%20applications.%20In%20this%0Awork%2C%20we%20introduce%20TDDev%2C%20the%20first%20test-driven%20development%20%28TDD%29-enabled%0ALLM-agent%20framework%20for%20end-to-end%20full-stack%20web%20application%20generation.%20Given%0Aa%20natural%20language%20description%20or%20design%20image%2C%20TDDev%20automatically%20derives%0Aexecutable%20test%20cases%2C%20generates%20front-end%20and%20back-end%20code%2C%20simulates%20user%0Ainteractions%2C%20and%20iteratively%20refines%20the%20implementation%20until%20all%20requirements%0Aare%20satisfied.%20Our%20framework%20addresses%20key%20challenges%20in%20full-stack%20automation%2C%0Aincluding%20underspecified%20user%20requirements%2C%20complex%20interdependencies%20among%0Amultiple%20files%2C%20and%20the%20need%20for%20both%20functional%20correctness%20and%20visual%0Afidelity.%20Through%20extensive%20experiments%20on%20diverse%20application%20scenarios%2C%20TDDev%0Aachieves%20a%2014.4%25%20improvement%20on%20overall%20accuracy%20compared%20to%20state-of-the-art%0Abaselines%2C%20demonstrating%20its%20effectiveness%20in%20producing%20reliable%2C%20high-quality%0Aweb%20applications%20without%20requiring%20manual%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25297v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatically%2520Generating%2520Web%2520Applications%2520from%2520Requirements%2520Via%250A%2520%2520Multi-Agent%2520Test-Driven%2520Development%26entry.906535625%3DYuxuan%2520Wan%2520and%2520Tingshuo%2520Liang%2520and%2520Jiakai%2520Xu%2520and%2520Jingyu%2520Xiao%2520and%2520Yintong%2520Huo%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Developing%2520full-stack%2520web%2520applications%2520is%2520complex%2520and%2520time-intensive%252C%250Ademanding%2520proficiency%2520across%2520diverse%2520technologies%2520and%2520frameworks.%2520Although%250Arecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520enable%2520automated%250Awebpage%2520generation%2520from%2520visual%2520inputs%252C%2520current%2520solutions%2520remain%2520limited%2520to%250Afront-end%2520tasks%2520and%2520fail%2520to%2520deliver%2520fully%2520functional%2520applications.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520TDDev%252C%2520the%2520first%2520test-driven%2520development%2520%2528TDD%2529-enabled%250ALLM-agent%2520framework%2520for%2520end-to-end%2520full-stack%2520web%2520application%2520generation.%2520Given%250Aa%2520natural%2520language%2520description%2520or%2520design%2520image%252C%2520TDDev%2520automatically%2520derives%250Aexecutable%2520test%2520cases%252C%2520generates%2520front-end%2520and%2520back-end%2520code%252C%2520simulates%2520user%250Ainteractions%252C%2520and%2520iteratively%2520refines%2520the%2520implementation%2520until%2520all%2520requirements%250Aare%2520satisfied.%2520Our%2520framework%2520addresses%2520key%2520challenges%2520in%2520full-stack%2520automation%252C%250Aincluding%2520underspecified%2520user%2520requirements%252C%2520complex%2520interdependencies%2520among%250Amultiple%2520files%252C%2520and%2520the%2520need%2520for%2520both%2520functional%2520correctness%2520and%2520visual%250Afidelity.%2520Through%2520extensive%2520experiments%2520on%2520diverse%2520application%2520scenarios%252C%2520TDDev%250Aachieves%2520a%252014.4%2525%2520improvement%2520on%2520overall%2520accuracy%2520compared%2520to%2520state-of-the-art%250Abaselines%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520producing%2520reliable%252C%2520high-quality%250Aweb%2520applications%2520without%2520requiring%2520manual%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25297v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Generating%20Web%20Applications%20from%20Requirements%20Via%0A%20%20Multi-Agent%20Test-Driven%20Development&entry.906535625=Yuxuan%20Wan%20and%20Tingshuo%20Liang%20and%20Jiakai%20Xu%20and%20Jingyu%20Xiao%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Developing%20full-stack%20web%20applications%20is%20complex%20and%20time-intensive%2C%0Ademanding%20proficiency%20across%20diverse%20technologies%20and%20frameworks.%20Although%0Arecent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20enable%20automated%0Awebpage%20generation%20from%20visual%20inputs%2C%20current%20solutions%20remain%20limited%20to%0Afront-end%20tasks%20and%20fail%20to%20deliver%20fully%20functional%20applications.%20In%20this%0Awork%2C%20we%20introduce%20TDDev%2C%20the%20first%20test-driven%20development%20%28TDD%29-enabled%0ALLM-agent%20framework%20for%20end-to-end%20full-stack%20web%20application%20generation.%20Given%0Aa%20natural%20language%20description%20or%20design%20image%2C%20TDDev%20automatically%20derives%0Aexecutable%20test%20cases%2C%20generates%20front-end%20and%20back-end%20code%2C%20simulates%20user%0Ainteractions%2C%20and%20iteratively%20refines%20the%20implementation%20until%20all%20requirements%0Aare%20satisfied.%20Our%20framework%20addresses%20key%20challenges%20in%20full-stack%20automation%2C%0Aincluding%20underspecified%20user%20requirements%2C%20complex%20interdependencies%20among%0Amultiple%20files%2C%20and%20the%20need%20for%20both%20functional%20correctness%20and%20visual%0Afidelity.%20Through%20extensive%20experiments%20on%20diverse%20application%20scenarios%2C%20TDDev%0Aachieves%20a%2014.4%25%20improvement%20on%20overall%20accuracy%20compared%20to%20state-of-the-art%0Abaselines%2C%20demonstrating%20its%20effectiveness%20in%20producing%20reliable%2C%20high-quality%0Aweb%20applications%20without%20requiring%20manual%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25297v2&entry.124074799=Read"},
{"title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning", "author": "Yixuan Even Xu and Yash Savani and Fei Fang and J. Zico Kolter", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has emerged as the\nleading approach for enhancing reasoning capabilities in large language models.\nHowever, it faces a fundamental compute and memory asymmetry: rollout\ngeneration is embarrassingly parallel and memory-light, whereas policy updates\nare communication-heavy and memory-intensive. To address this, we introduce\nPODS (Policy Optimization with Down-Sampling), which decouples rollout\ngeneration from policy updates by training only on a strategically selected\nsubset of rollouts, maintaining learning quality while dramatically reducing\nupdate costs. We propose a principled subset selection criterion, max-variance\ndown-sampling, that maximizes reward diversity, and provide an efficient\n$O(n\\log n)$ implementation. Empirically, Group Relative Policy Optimization\n(GRPO) with PODS achieves the peak test accuracy of vanilla GRPO at least\n$\\mathbf{1.7\\times}$ faster across the different reasoning benchmarks and\nhardware configurations we tested.\n", "link": "http://arxiv.org/abs/2504.13818v3", "date": "2025-10-01", "relevancy": 1.9934, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5252}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning&body=Title%3A%20Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yixuan%20Even%20Xu%20and%20Yash%20Savani%20and%20Fei%20Fang%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20emerged%20as%20the%0Aleading%20approach%20for%20enhancing%20reasoning%20capabilities%20in%20large%20language%20models.%0AHowever%2C%20it%20faces%20a%20fundamental%20compute%20and%20memory%20asymmetry%3A%20rollout%0Ageneration%20is%20embarrassingly%20parallel%20and%20memory-light%2C%20whereas%20policy%20updates%0Aare%20communication-heavy%20and%20memory-intensive.%20To%20address%20this%2C%20we%20introduce%0APODS%20%28Policy%20Optimization%20with%20Down-Sampling%29%2C%20which%20decouples%20rollout%0Ageneration%20from%20policy%20updates%20by%20training%20only%20on%20a%20strategically%20selected%0Asubset%20of%20rollouts%2C%20maintaining%20learning%20quality%20while%20dramatically%20reducing%0Aupdate%20costs.%20We%20propose%20a%20principled%20subset%20selection%20criterion%2C%20max-variance%0Adown-sampling%2C%20that%20maximizes%20reward%20diversity%2C%20and%20provide%20an%20efficient%0A%24O%28n%5Clog%20n%29%24%20implementation.%20Empirically%2C%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20with%20PODS%20achieves%20the%20peak%20test%20accuracy%20of%20vanilla%20GRPO%20at%20least%0A%24%5Cmathbf%7B1.7%5Ctimes%7D%24%20faster%20across%20the%20different%20reasoning%20benchmarks%20and%0Ahardware%20configurations%20we%20tested.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13818v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Rollouts%2520are%2520Useful%253A%2520Down-Sampling%2520Rollouts%2520in%2520LLM%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYixuan%2520Even%2520Xu%2520and%2520Yash%2520Savani%2520and%2520Fei%2520Fang%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520the%250Aleading%2520approach%2520for%2520enhancing%2520reasoning%2520capabilities%2520in%2520large%2520language%2520models.%250AHowever%252C%2520it%2520faces%2520a%2520fundamental%2520compute%2520and%2520memory%2520asymmetry%253A%2520rollout%250Ageneration%2520is%2520embarrassingly%2520parallel%2520and%2520memory-light%252C%2520whereas%2520policy%2520updates%250Aare%2520communication-heavy%2520and%2520memory-intensive.%2520To%2520address%2520this%252C%2520we%2520introduce%250APODS%2520%2528Policy%2520Optimization%2520with%2520Down-Sampling%2529%252C%2520which%2520decouples%2520rollout%250Ageneration%2520from%2520policy%2520updates%2520by%2520training%2520only%2520on%2520a%2520strategically%2520selected%250Asubset%2520of%2520rollouts%252C%2520maintaining%2520learning%2520quality%2520while%2520dramatically%2520reducing%250Aupdate%2520costs.%2520We%2520propose%2520a%2520principled%2520subset%2520selection%2520criterion%252C%2520max-variance%250Adown-sampling%252C%2520that%2520maximizes%2520reward%2520diversity%252C%2520and%2520provide%2520an%2520efficient%250A%2524O%2528n%255Clog%2520n%2529%2524%2520implementation.%2520Empirically%252C%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520with%2520PODS%2520achieves%2520the%2520peak%2520test%2520accuracy%2520of%2520vanilla%2520GRPO%2520at%2520least%250A%2524%255Cmathbf%257B1.7%255Ctimes%257D%2524%2520faster%2520across%2520the%2520different%2520reasoning%2520benchmarks%2520and%250Ahardware%2520configurations%2520we%2520tested.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13818v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning&entry.906535625=Yixuan%20Even%20Xu%20and%20Yash%20Savani%20and%20Fei%20Fang%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20emerged%20as%20the%0Aleading%20approach%20for%20enhancing%20reasoning%20capabilities%20in%20large%20language%20models.%0AHowever%2C%20it%20faces%20a%20fundamental%20compute%20and%20memory%20asymmetry%3A%20rollout%0Ageneration%20is%20embarrassingly%20parallel%20and%20memory-light%2C%20whereas%20policy%20updates%0Aare%20communication-heavy%20and%20memory-intensive.%20To%20address%20this%2C%20we%20introduce%0APODS%20%28Policy%20Optimization%20with%20Down-Sampling%29%2C%20which%20decouples%20rollout%0Ageneration%20from%20policy%20updates%20by%20training%20only%20on%20a%20strategically%20selected%0Asubset%20of%20rollouts%2C%20maintaining%20learning%20quality%20while%20dramatically%20reducing%0Aupdate%20costs.%20We%20propose%20a%20principled%20subset%20selection%20criterion%2C%20max-variance%0Adown-sampling%2C%20that%20maximizes%20reward%20diversity%2C%20and%20provide%20an%20efficient%0A%24O%28n%5Clog%20n%29%24%20implementation.%20Empirically%2C%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20with%20PODS%20achieves%20the%20peak%20test%20accuracy%20of%20vanilla%20GRPO%20at%20least%0A%24%5Cmathbf%7B1.7%5Ctimes%7D%24%20faster%20across%20the%20different%20reasoning%20benchmarks%20and%0Ahardware%20configurations%20we%20tested.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13818v3&entry.124074799=Read"},
{"title": "Deep Learning for Subspace Regression", "author": "Vladimir Fanaskov and Vladislav Trifonov and Alexander Rudikov and Ekaterina Muravleva and Ivan Oseledets", "abstract": "  It is often possible to perform reduced order modelling by specifying linear\nsubspace which accurately captures the dynamics of the system. This approach\nbecomes especially appealing when linear subspace explicitly depends on\nparameters of the problem. A practical way to apply such a scheme is to compute\nsubspaces for a selected set of parameters in the computationally demanding\noffline stage and in the online stage approximate subspace for unknown\nparameters by interpolation. For realistic problems the space of parameters is\nhigh dimensional, which renders classical interpolation strategies infeasible\nor unreliable. We propose to relax the interpolation problem to regression,\nintroduce several loss functions suitable for subspace data, and use a neural\nnetwork as an approximation to high-dimensional target function. To further\nsimplify a learning problem we introduce redundancy: in place of predicting\nsubspace of a given dimension we predict larger subspace. We show theoretically\nthat this strategy decreases the complexity of the mapping for elliptic\neigenproblems with constant coefficients and makes the mapping smoother for\ngeneral smooth function on the Grassmann manifold. Empirical results also show\nthat accuracy significantly improves when larger-than-needed subspaces are\npredicted. With the set of numerical illustrations we demonstrate that subspace\nregression can be useful for a range of tasks including parametric\neigenproblems, deflation techniques, relaxation methods, optimal control and\nsolution of parametric partial differential equations.\n", "link": "http://arxiv.org/abs/2509.23249v2", "date": "2025-10-01", "relevancy": 1.9586, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5065}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Subspace%20Regression&body=Title%3A%20Deep%20Learning%20for%20Subspace%20Regression%0AAuthor%3A%20Vladimir%20Fanaskov%20and%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Ekaterina%20Muravleva%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20It%20is%20often%20possible%20to%20perform%20reduced%20order%20modelling%20by%20specifying%20linear%0Asubspace%20which%20accurately%20captures%20the%20dynamics%20of%20the%20system.%20This%20approach%0Abecomes%20especially%20appealing%20when%20linear%20subspace%20explicitly%20depends%20on%0Aparameters%20of%20the%20problem.%20A%20practical%20way%20to%20apply%20such%20a%20scheme%20is%20to%20compute%0Asubspaces%20for%20a%20selected%20set%20of%20parameters%20in%20the%20computationally%20demanding%0Aoffline%20stage%20and%20in%20the%20online%20stage%20approximate%20subspace%20for%20unknown%0Aparameters%20by%20interpolation.%20For%20realistic%20problems%20the%20space%20of%20parameters%20is%0Ahigh%20dimensional%2C%20which%20renders%20classical%20interpolation%20strategies%20infeasible%0Aor%20unreliable.%20We%20propose%20to%20relax%20the%20interpolation%20problem%20to%20regression%2C%0Aintroduce%20several%20loss%20functions%20suitable%20for%20subspace%20data%2C%20and%20use%20a%20neural%0Anetwork%20as%20an%20approximation%20to%20high-dimensional%20target%20function.%20To%20further%0Asimplify%20a%20learning%20problem%20we%20introduce%20redundancy%3A%20in%20place%20of%20predicting%0Asubspace%20of%20a%20given%20dimension%20we%20predict%20larger%20subspace.%20We%20show%20theoretically%0Athat%20this%20strategy%20decreases%20the%20complexity%20of%20the%20mapping%20for%20elliptic%0Aeigenproblems%20with%20constant%20coefficients%20and%20makes%20the%20mapping%20smoother%20for%0Ageneral%20smooth%20function%20on%20the%20Grassmann%20manifold.%20Empirical%20results%20also%20show%0Athat%20accuracy%20significantly%20improves%20when%20larger-than-needed%20subspaces%20are%0Apredicted.%20With%20the%20set%20of%20numerical%20illustrations%20we%20demonstrate%20that%20subspace%0Aregression%20can%20be%20useful%20for%20a%20range%20of%20tasks%20including%20parametric%0Aeigenproblems%2C%20deflation%20techniques%2C%20relaxation%20methods%2C%20optimal%20control%20and%0Asolution%20of%20parametric%20partial%20differential%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Subspace%2520Regression%26entry.906535625%3DVladimir%2520Fanaskov%2520and%2520Vladislav%2520Trifonov%2520and%2520Alexander%2520Rudikov%2520and%2520Ekaterina%2520Muravleva%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520It%2520is%2520often%2520possible%2520to%2520perform%2520reduced%2520order%2520modelling%2520by%2520specifying%2520linear%250Asubspace%2520which%2520accurately%2520captures%2520the%2520dynamics%2520of%2520the%2520system.%2520This%2520approach%250Abecomes%2520especially%2520appealing%2520when%2520linear%2520subspace%2520explicitly%2520depends%2520on%250Aparameters%2520of%2520the%2520problem.%2520A%2520practical%2520way%2520to%2520apply%2520such%2520a%2520scheme%2520is%2520to%2520compute%250Asubspaces%2520for%2520a%2520selected%2520set%2520of%2520parameters%2520in%2520the%2520computationally%2520demanding%250Aoffline%2520stage%2520and%2520in%2520the%2520online%2520stage%2520approximate%2520subspace%2520for%2520unknown%250Aparameters%2520by%2520interpolation.%2520For%2520realistic%2520problems%2520the%2520space%2520of%2520parameters%2520is%250Ahigh%2520dimensional%252C%2520which%2520renders%2520classical%2520interpolation%2520strategies%2520infeasible%250Aor%2520unreliable.%2520We%2520propose%2520to%2520relax%2520the%2520interpolation%2520problem%2520to%2520regression%252C%250Aintroduce%2520several%2520loss%2520functions%2520suitable%2520for%2520subspace%2520data%252C%2520and%2520use%2520a%2520neural%250Anetwork%2520as%2520an%2520approximation%2520to%2520high-dimensional%2520target%2520function.%2520To%2520further%250Asimplify%2520a%2520learning%2520problem%2520we%2520introduce%2520redundancy%253A%2520in%2520place%2520of%2520predicting%250Asubspace%2520of%2520a%2520given%2520dimension%2520we%2520predict%2520larger%2520subspace.%2520We%2520show%2520theoretically%250Athat%2520this%2520strategy%2520decreases%2520the%2520complexity%2520of%2520the%2520mapping%2520for%2520elliptic%250Aeigenproblems%2520with%2520constant%2520coefficients%2520and%2520makes%2520the%2520mapping%2520smoother%2520for%250Ageneral%2520smooth%2520function%2520on%2520the%2520Grassmann%2520manifold.%2520Empirical%2520results%2520also%2520show%250Athat%2520accuracy%2520significantly%2520improves%2520when%2520larger-than-needed%2520subspaces%2520are%250Apredicted.%2520With%2520the%2520set%2520of%2520numerical%2520illustrations%2520we%2520demonstrate%2520that%2520subspace%250Aregression%2520can%2520be%2520useful%2520for%2520a%2520range%2520of%2520tasks%2520including%2520parametric%250Aeigenproblems%252C%2520deflation%2520techniques%252C%2520relaxation%2520methods%252C%2520optimal%2520control%2520and%250Asolution%2520of%2520parametric%2520partial%2520differential%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Subspace%20Regression&entry.906535625=Vladimir%20Fanaskov%20and%20Vladislav%20Trifonov%20and%20Alexander%20Rudikov%20and%20Ekaterina%20Muravleva%20and%20Ivan%20Oseledets&entry.1292438233=%20%20It%20is%20often%20possible%20to%20perform%20reduced%20order%20modelling%20by%20specifying%20linear%0Asubspace%20which%20accurately%20captures%20the%20dynamics%20of%20the%20system.%20This%20approach%0Abecomes%20especially%20appealing%20when%20linear%20subspace%20explicitly%20depends%20on%0Aparameters%20of%20the%20problem.%20A%20practical%20way%20to%20apply%20such%20a%20scheme%20is%20to%20compute%0Asubspaces%20for%20a%20selected%20set%20of%20parameters%20in%20the%20computationally%20demanding%0Aoffline%20stage%20and%20in%20the%20online%20stage%20approximate%20subspace%20for%20unknown%0Aparameters%20by%20interpolation.%20For%20realistic%20problems%20the%20space%20of%20parameters%20is%0Ahigh%20dimensional%2C%20which%20renders%20classical%20interpolation%20strategies%20infeasible%0Aor%20unreliable.%20We%20propose%20to%20relax%20the%20interpolation%20problem%20to%20regression%2C%0Aintroduce%20several%20loss%20functions%20suitable%20for%20subspace%20data%2C%20and%20use%20a%20neural%0Anetwork%20as%20an%20approximation%20to%20high-dimensional%20target%20function.%20To%20further%0Asimplify%20a%20learning%20problem%20we%20introduce%20redundancy%3A%20in%20place%20of%20predicting%0Asubspace%20of%20a%20given%20dimension%20we%20predict%20larger%20subspace.%20We%20show%20theoretically%0Athat%20this%20strategy%20decreases%20the%20complexity%20of%20the%20mapping%20for%20elliptic%0Aeigenproblems%20with%20constant%20coefficients%20and%20makes%20the%20mapping%20smoother%20for%0Ageneral%20smooth%20function%20on%20the%20Grassmann%20manifold.%20Empirical%20results%20also%20show%0Athat%20accuracy%20significantly%20improves%20when%20larger-than-needed%20subspaces%20are%0Apredicted.%20With%20the%20set%20of%20numerical%20illustrations%20we%20demonstrate%20that%20subspace%0Aregression%20can%20be%20useful%20for%20a%20range%20of%20tasks%20including%20parametric%0Aeigenproblems%2C%20deflation%20techniques%2C%20relaxation%20methods%2C%20optimal%20control%20and%0Asolution%20of%20parametric%20partial%20differential%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23249v2&entry.124074799=Read"},
{"title": "Alternating Training-based Label Smoothing Enhances Prompt\n  Generalization", "author": "Yang Chen and Yanbin Wei and Ke Jin and Yi Kong and James Kwok and Yu Zhang", "abstract": "  Recent advances in pre-trained vision-language models have demonstrated\nremarkable zero-shot generalization capabilities. To further enhance these\nmodels' adaptability to various downstream tasks, prompt tuning has emerged as\na parameter-efficient fine-tuning method. However, despite its efficiency, the\ngeneralization ability of prompt remains limited. In contrast, label smoothing\n(LS) has been widely recognized as an effective regularization technique that\nprevents models from becoming over-confident and improves their generalization.\nThis inspires us to explore the integration of LS with prompt tuning. However,\nwe have observed that the vanilla LS even weakens the generalization ability of\nprompt tuning. To address this issue, we propose the Alternating Training-based\nLabel Smoothing (ATLaS) method, which alternately trains with standard one-hot\nlabels and soft labels generated by LS to supervise the prompt tuning.\nMoreover, we introduce two types of efficient offline soft labels, including\nClass-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide\ninter-class or instance-class relationships for prompt tuning. The theoretical\nproperties of the proposed ATLaS method are analyzed. Extensive experiments\ndemonstrate that the proposed ATLaS method, combined with CSL and ISL,\nconsistently enhances the generalization performance of prompt tuning.\nMoreover, the proposed ATLaS method exhibits high compatibility with prevalent\nprompt tuning methods, enabling seamless integration into existing methods.\n", "link": "http://arxiv.org/abs/2508.17846v2", "date": "2025-10-01", "relevancy": 1.9543, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4887}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternating%20Training-based%20Label%20Smoothing%20Enhances%20Prompt%0A%20%20Generalization&body=Title%3A%20Alternating%20Training-based%20Label%20Smoothing%20Enhances%20Prompt%0A%20%20Generalization%0AAuthor%3A%20Yang%20Chen%20and%20Yanbin%20Wei%20and%20Ke%20Jin%20and%20Yi%20Kong%20and%20James%20Kwok%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20pre-trained%20vision-language%20models%20have%20demonstrated%0Aremarkable%20zero-shot%20generalization%20capabilities.%20To%20further%20enhance%20these%0Amodels%27%20adaptability%20to%20various%20downstream%20tasks%2C%20prompt%20tuning%20has%20emerged%20as%0Aa%20parameter-efficient%20fine-tuning%20method.%20However%2C%20despite%20its%20efficiency%2C%20the%0Ageneralization%20ability%20of%20prompt%20remains%20limited.%20In%20contrast%2C%20label%20smoothing%0A%28LS%29%20has%20been%20widely%20recognized%20as%20an%20effective%20regularization%20technique%20that%0Aprevents%20models%20from%20becoming%20over-confident%20and%20improves%20their%20generalization.%0AThis%20inspires%20us%20to%20explore%20the%20integration%20of%20LS%20with%20prompt%20tuning.%20However%2C%0Awe%20have%20observed%20that%20the%20vanilla%20LS%20even%20weakens%20the%20generalization%20ability%20of%0Aprompt%20tuning.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Alternating%20Training-based%0ALabel%20Smoothing%20%28ATLaS%29%20method%2C%20which%20alternately%20trains%20with%20standard%20one-hot%0Alabels%20and%20soft%20labels%20generated%20by%20LS%20to%20supervise%20the%20prompt%20tuning.%0AMoreover%2C%20we%20introduce%20two%20types%20of%20efficient%20offline%20soft%20labels%2C%20including%0AClass-wise%20Soft%20Labels%20%28CSL%29%20and%20Instance-wise%20Soft%20Labels%20%28ISL%29%2C%20to%20provide%0Ainter-class%20or%20instance-class%20relationships%20for%20prompt%20tuning.%20The%20theoretical%0Aproperties%20of%20the%20proposed%20ATLaS%20method%20are%20analyzed.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20ATLaS%20method%2C%20combined%20with%20CSL%20and%20ISL%2C%0Aconsistently%20enhances%20the%20generalization%20performance%20of%20prompt%20tuning.%0AMoreover%2C%20the%20proposed%20ATLaS%20method%20exhibits%20high%20compatibility%20with%20prevalent%0Aprompt%20tuning%20methods%2C%20enabling%20seamless%20integration%20into%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternating%2520Training-based%2520Label%2520Smoothing%2520Enhances%2520Prompt%250A%2520%2520Generalization%26entry.906535625%3DYang%2520Chen%2520and%2520Yanbin%2520Wei%2520and%2520Ke%2520Jin%2520and%2520Yi%2520Kong%2520and%2520James%2520Kwok%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520pre-trained%2520vision-language%2520models%2520have%2520demonstrated%250Aremarkable%2520zero-shot%2520generalization%2520capabilities.%2520To%2520further%2520enhance%2520these%250Amodels%2527%2520adaptability%2520to%2520various%2520downstream%2520tasks%252C%2520prompt%2520tuning%2520has%2520emerged%2520as%250Aa%2520parameter-efficient%2520fine-tuning%2520method.%2520However%252C%2520despite%2520its%2520efficiency%252C%2520the%250Ageneralization%2520ability%2520of%2520prompt%2520remains%2520limited.%2520In%2520contrast%252C%2520label%2520smoothing%250A%2528LS%2529%2520has%2520been%2520widely%2520recognized%2520as%2520an%2520effective%2520regularization%2520technique%2520that%250Aprevents%2520models%2520from%2520becoming%2520over-confident%2520and%2520improves%2520their%2520generalization.%250AThis%2520inspires%2520us%2520to%2520explore%2520the%2520integration%2520of%2520LS%2520with%2520prompt%2520tuning.%2520However%252C%250Awe%2520have%2520observed%2520that%2520the%2520vanilla%2520LS%2520even%2520weakens%2520the%2520generalization%2520ability%2520of%250Aprompt%2520tuning.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520Alternating%2520Training-based%250ALabel%2520Smoothing%2520%2528ATLaS%2529%2520method%252C%2520which%2520alternately%2520trains%2520with%2520standard%2520one-hot%250Alabels%2520and%2520soft%2520labels%2520generated%2520by%2520LS%2520to%2520supervise%2520the%2520prompt%2520tuning.%250AMoreover%252C%2520we%2520introduce%2520two%2520types%2520of%2520efficient%2520offline%2520soft%2520labels%252C%2520including%250AClass-wise%2520Soft%2520Labels%2520%2528CSL%2529%2520and%2520Instance-wise%2520Soft%2520Labels%2520%2528ISL%2529%252C%2520to%2520provide%250Ainter-class%2520or%2520instance-class%2520relationships%2520for%2520prompt%2520tuning.%2520The%2520theoretical%250Aproperties%2520of%2520the%2520proposed%2520ATLaS%2520method%2520are%2520analyzed.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520proposed%2520ATLaS%2520method%252C%2520combined%2520with%2520CSL%2520and%2520ISL%252C%250Aconsistently%2520enhances%2520the%2520generalization%2520performance%2520of%2520prompt%2520tuning.%250AMoreover%252C%2520the%2520proposed%2520ATLaS%2520method%2520exhibits%2520high%2520compatibility%2520with%2520prevalent%250Aprompt%2520tuning%2520methods%252C%2520enabling%2520seamless%2520integration%2520into%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternating%20Training-based%20Label%20Smoothing%20Enhances%20Prompt%0A%20%20Generalization&entry.906535625=Yang%20Chen%20and%20Yanbin%20Wei%20and%20Ke%20Jin%20and%20Yi%20Kong%20and%20James%20Kwok%20and%20Yu%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20pre-trained%20vision-language%20models%20have%20demonstrated%0Aremarkable%20zero-shot%20generalization%20capabilities.%20To%20further%20enhance%20these%0Amodels%27%20adaptability%20to%20various%20downstream%20tasks%2C%20prompt%20tuning%20has%20emerged%20as%0Aa%20parameter-efficient%20fine-tuning%20method.%20However%2C%20despite%20its%20efficiency%2C%20the%0Ageneralization%20ability%20of%20prompt%20remains%20limited.%20In%20contrast%2C%20label%20smoothing%0A%28LS%29%20has%20been%20widely%20recognized%20as%20an%20effective%20regularization%20technique%20that%0Aprevents%20models%20from%20becoming%20over-confident%20and%20improves%20their%20generalization.%0AThis%20inspires%20us%20to%20explore%20the%20integration%20of%20LS%20with%20prompt%20tuning.%20However%2C%0Awe%20have%20observed%20that%20the%20vanilla%20LS%20even%20weakens%20the%20generalization%20ability%20of%0Aprompt%20tuning.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Alternating%20Training-based%0ALabel%20Smoothing%20%28ATLaS%29%20method%2C%20which%20alternately%20trains%20with%20standard%20one-hot%0Alabels%20and%20soft%20labels%20generated%20by%20LS%20to%20supervise%20the%20prompt%20tuning.%0AMoreover%2C%20we%20introduce%20two%20types%20of%20efficient%20offline%20soft%20labels%2C%20including%0AClass-wise%20Soft%20Labels%20%28CSL%29%20and%20Instance-wise%20Soft%20Labels%20%28ISL%29%2C%20to%20provide%0Ainter-class%20or%20instance-class%20relationships%20for%20prompt%20tuning.%20The%20theoretical%0Aproperties%20of%20the%20proposed%20ATLaS%20method%20are%20analyzed.%20Extensive%20experiments%0Ademonstrate%20that%20the%20proposed%20ATLaS%20method%2C%20combined%20with%20CSL%20and%20ISL%2C%0Aconsistently%20enhances%20the%20generalization%20performance%20of%20prompt%20tuning.%0AMoreover%2C%20the%20proposed%20ATLaS%20method%20exhibits%20high%20compatibility%20with%20prevalent%0Aprompt%20tuning%20methods%2C%20enabling%20seamless%20integration%20into%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17846v2&entry.124074799=Read"},
{"title": "Adversarial Attacks to Latent Representations of Distributed Neural\n  Networks in Split Computing", "author": "Milin Zhang and Mohammad Abdi and Jonathan Ashdown and Francesco Restuccia", "abstract": "  Distributed deep neural networks (DNNs) have been shown to reduce the\ncomputational burden of mobile devices and decrease the end-to-end inference\nlatency in edge computing scenarios. While distributed DNNs have been studied,\nto the best of our knowledge, the resilience of distributed DNNs to adversarial\naction remains an open problem. In this paper, we fill the existing research\ngap by rigorously analyzing the robustness of distributed DNNs against\nadversarial action. We cast this problem in the context of information theory\nand rigorously proved that (i) the compressed latent dimension improves the\nrobustness but also affect task-oriented performance; and (ii) the deeper\nsplitting point enhances the robustness but also increases the computational\nburden. These two trade-offs provide a novel perspective to design robust\ndistributed DNN. To test our theoretical findings, we perform extensive\nexperimental analysis by considering 6 different DNN architectures, 6 different\napproaches for distributed DNN and 10 different adversarial attacks using the\nImageNet-1K dataset.\n", "link": "http://arxiv.org/abs/2309.17401v5", "date": "2025-10-01", "relevancy": 1.9461, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5141}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5063}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20to%20Latent%20Representations%20of%20Distributed%20Neural%0A%20%20Networks%20in%20Split%20Computing&body=Title%3A%20Adversarial%20Attacks%20to%20Latent%20Representations%20of%20Distributed%20Neural%0A%20%20Networks%20in%20Split%20Computing%0AAuthor%3A%20Milin%20Zhang%20and%20Mohammad%20Abdi%20and%20Jonathan%20Ashdown%20and%20Francesco%20Restuccia%0AAbstract%3A%20%20%20Distributed%20deep%20neural%20networks%20%28DNNs%29%20have%20been%20shown%20to%20reduce%20the%0Acomputational%20burden%20of%20mobile%20devices%20and%20decrease%20the%20end-to-end%20inference%0Alatency%20in%20edge%20computing%20scenarios.%20While%20distributed%20DNNs%20have%20been%20studied%2C%0Ato%20the%20best%20of%20our%20knowledge%2C%20the%20resilience%20of%20distributed%20DNNs%20to%20adversarial%0Aaction%20remains%20an%20open%20problem.%20In%20this%20paper%2C%20we%20fill%20the%20existing%20research%0Agap%20by%20rigorously%20analyzing%20the%20robustness%20of%20distributed%20DNNs%20against%0Aadversarial%20action.%20We%20cast%20this%20problem%20in%20the%20context%20of%20information%20theory%0Aand%20rigorously%20proved%20that%20%28i%29%20the%20compressed%20latent%20dimension%20improves%20the%0Arobustness%20but%20also%20affect%20task-oriented%20performance%3B%20and%20%28ii%29%20the%20deeper%0Asplitting%20point%20enhances%20the%20robustness%20but%20also%20increases%20the%20computational%0Aburden.%20These%20two%20trade-offs%20provide%20a%20novel%20perspective%20to%20design%20robust%0Adistributed%20DNN.%20To%20test%20our%20theoretical%20findings%2C%20we%20perform%20extensive%0Aexperimental%20analysis%20by%20considering%206%20different%20DNN%20architectures%2C%206%20different%0Aapproaches%20for%20distributed%20DNN%20and%2010%20different%20adversarial%20attacks%20using%20the%0AImageNet-1K%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17401v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520to%2520Latent%2520Representations%2520of%2520Distributed%2520Neural%250A%2520%2520Networks%2520in%2520Split%2520Computing%26entry.906535625%3DMilin%2520Zhang%2520and%2520Mohammad%2520Abdi%2520and%2520Jonathan%2520Ashdown%2520and%2520Francesco%2520Restuccia%26entry.1292438233%3D%2520%2520Distributed%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520been%2520shown%2520to%2520reduce%2520the%250Acomputational%2520burden%2520of%2520mobile%2520devices%2520and%2520decrease%2520the%2520end-to-end%2520inference%250Alatency%2520in%2520edge%2520computing%2520scenarios.%2520While%2520distributed%2520DNNs%2520have%2520been%2520studied%252C%250Ato%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520resilience%2520of%2520distributed%2520DNNs%2520to%2520adversarial%250Aaction%2520remains%2520an%2520open%2520problem.%2520In%2520this%2520paper%252C%2520we%2520fill%2520the%2520existing%2520research%250Agap%2520by%2520rigorously%2520analyzing%2520the%2520robustness%2520of%2520distributed%2520DNNs%2520against%250Aadversarial%2520action.%2520We%2520cast%2520this%2520problem%2520in%2520the%2520context%2520of%2520information%2520theory%250Aand%2520rigorously%2520proved%2520that%2520%2528i%2529%2520the%2520compressed%2520latent%2520dimension%2520improves%2520the%250Arobustness%2520but%2520also%2520affect%2520task-oriented%2520performance%253B%2520and%2520%2528ii%2529%2520the%2520deeper%250Asplitting%2520point%2520enhances%2520the%2520robustness%2520but%2520also%2520increases%2520the%2520computational%250Aburden.%2520These%2520two%2520trade-offs%2520provide%2520a%2520novel%2520perspective%2520to%2520design%2520robust%250Adistributed%2520DNN.%2520To%2520test%2520our%2520theoretical%2520findings%252C%2520we%2520perform%2520extensive%250Aexperimental%2520analysis%2520by%2520considering%25206%2520different%2520DNN%2520architectures%252C%25206%2520different%250Aapproaches%2520for%2520distributed%2520DNN%2520and%252010%2520different%2520adversarial%2520attacks%2520using%2520the%250AImageNet-1K%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17401v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20to%20Latent%20Representations%20of%20Distributed%20Neural%0A%20%20Networks%20in%20Split%20Computing&entry.906535625=Milin%20Zhang%20and%20Mohammad%20Abdi%20and%20Jonathan%20Ashdown%20and%20Francesco%20Restuccia&entry.1292438233=%20%20Distributed%20deep%20neural%20networks%20%28DNNs%29%20have%20been%20shown%20to%20reduce%20the%0Acomputational%20burden%20of%20mobile%20devices%20and%20decrease%20the%20end-to-end%20inference%0Alatency%20in%20edge%20computing%20scenarios.%20While%20distributed%20DNNs%20have%20been%20studied%2C%0Ato%20the%20best%20of%20our%20knowledge%2C%20the%20resilience%20of%20distributed%20DNNs%20to%20adversarial%0Aaction%20remains%20an%20open%20problem.%20In%20this%20paper%2C%20we%20fill%20the%20existing%20research%0Agap%20by%20rigorously%20analyzing%20the%20robustness%20of%20distributed%20DNNs%20against%0Aadversarial%20action.%20We%20cast%20this%20problem%20in%20the%20context%20of%20information%20theory%0Aand%20rigorously%20proved%20that%20%28i%29%20the%20compressed%20latent%20dimension%20improves%20the%0Arobustness%20but%20also%20affect%20task-oriented%20performance%3B%20and%20%28ii%29%20the%20deeper%0Asplitting%20point%20enhances%20the%20robustness%20but%20also%20increases%20the%20computational%0Aburden.%20These%20two%20trade-offs%20provide%20a%20novel%20perspective%20to%20design%20robust%0Adistributed%20DNN.%20To%20test%20our%20theoretical%20findings%2C%20we%20perform%20extensive%0Aexperimental%20analysis%20by%20considering%206%20different%20DNN%20architectures%2C%206%20different%0Aapproaches%20for%20distributed%20DNN%20and%2010%20different%20adversarial%20attacks%20using%20the%0AImageNet-1K%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17401v5&entry.124074799=Read"},
{"title": "Neural Logic Networks for Interpretable Classification", "author": "Vincent Perreault and Katsumi Inoue and Richard Labib and Alain Hertz", "abstract": "  Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on examples from the medical and industrial\nfields where interpretability has tangible value.\n", "link": "http://arxiv.org/abs/2508.08172v4", "date": "2025-10-01", "relevancy": 1.9197, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Logic%20Networks%20for%20Interpretable%20Classification&body=Title%3A%20Neural%20Logic%20Networks%20for%20Interpretable%20Classification%0AAuthor%3A%20Vincent%20Perreault%20and%20Katsumi%20Inoue%20and%20Richard%20Labib%20and%20Alain%20Hertz%0AAbstract%3A%20%20%20Traditional%20neural%20networks%20have%20an%20impressive%20classification%20performance%2C%0Abut%20what%20they%20learn%20cannot%20be%20inspected%2C%20verified%20or%20extracted.%20Neural%20Logic%0ANetworks%20on%20the%20other%20hand%20have%20an%20interpretable%20structure%20that%20enables%20them%20to%0Alearn%20a%20logical%20mechanism%20relating%20the%20inputs%20and%20outputs%20with%20AND%20and%20OR%0Aoperations.%20We%20generalize%20these%20networks%20with%20NOT%20operations%20and%20biases%20that%0Atake%20into%20account%20unobserved%20data%20and%20develop%20a%20rigorous%20logical%20and%0Aprobabilistic%20modeling%20in%20terms%20of%20concept%20combinations%20to%20motivate%20their%20use.%0AWe%20also%20propose%20a%20novel%20factorized%20IF-THEN%20rule%20structure%20for%20the%20model%20as%20well%0Aas%20a%20modified%20learning%20algorithm.%20Our%20method%20improves%20the%20state-of-the-art%20in%0ABoolean%20networks%20discovery%20and%20is%20able%20to%20learn%20relevant%2C%20interpretable%20rules%0Ain%20tabular%20classification%2C%20notably%20on%20examples%20from%20the%20medical%20and%20industrial%0Afields%20where%20interpretability%20has%20tangible%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08172v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Logic%2520Networks%2520for%2520Interpretable%2520Classification%26entry.906535625%3DVincent%2520Perreault%2520and%2520Katsumi%2520Inoue%2520and%2520Richard%2520Labib%2520and%2520Alain%2520Hertz%26entry.1292438233%3D%2520%2520Traditional%2520neural%2520networks%2520have%2520an%2520impressive%2520classification%2520performance%252C%250Abut%2520what%2520they%2520learn%2520cannot%2520be%2520inspected%252C%2520verified%2520or%2520extracted.%2520Neural%2520Logic%250ANetworks%2520on%2520the%2520other%2520hand%2520have%2520an%2520interpretable%2520structure%2520that%2520enables%2520them%2520to%250Alearn%2520a%2520logical%2520mechanism%2520relating%2520the%2520inputs%2520and%2520outputs%2520with%2520AND%2520and%2520OR%250Aoperations.%2520We%2520generalize%2520these%2520networks%2520with%2520NOT%2520operations%2520and%2520biases%2520that%250Atake%2520into%2520account%2520unobserved%2520data%2520and%2520develop%2520a%2520rigorous%2520logical%2520and%250Aprobabilistic%2520modeling%2520in%2520terms%2520of%2520concept%2520combinations%2520to%2520motivate%2520their%2520use.%250AWe%2520also%2520propose%2520a%2520novel%2520factorized%2520IF-THEN%2520rule%2520structure%2520for%2520the%2520model%2520as%2520well%250Aas%2520a%2520modified%2520learning%2520algorithm.%2520Our%2520method%2520improves%2520the%2520state-of-the-art%2520in%250ABoolean%2520networks%2520discovery%2520and%2520is%2520able%2520to%2520learn%2520relevant%252C%2520interpretable%2520rules%250Ain%2520tabular%2520classification%252C%2520notably%2520on%2520examples%2520from%2520the%2520medical%2520and%2520industrial%250Afields%2520where%2520interpretability%2520has%2520tangible%2520value.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08172v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Logic%20Networks%20for%20Interpretable%20Classification&entry.906535625=Vincent%20Perreault%20and%20Katsumi%20Inoue%20and%20Richard%20Labib%20and%20Alain%20Hertz&entry.1292438233=%20%20Traditional%20neural%20networks%20have%20an%20impressive%20classification%20performance%2C%0Abut%20what%20they%20learn%20cannot%20be%20inspected%2C%20verified%20or%20extracted.%20Neural%20Logic%0ANetworks%20on%20the%20other%20hand%20have%20an%20interpretable%20structure%20that%20enables%20them%20to%0Alearn%20a%20logical%20mechanism%20relating%20the%20inputs%20and%20outputs%20with%20AND%20and%20OR%0Aoperations.%20We%20generalize%20these%20networks%20with%20NOT%20operations%20and%20biases%20that%0Atake%20into%20account%20unobserved%20data%20and%20develop%20a%20rigorous%20logical%20and%0Aprobabilistic%20modeling%20in%20terms%20of%20concept%20combinations%20to%20motivate%20their%20use.%0AWe%20also%20propose%20a%20novel%20factorized%20IF-THEN%20rule%20structure%20for%20the%20model%20as%20well%0Aas%20a%20modified%20learning%20algorithm.%20Our%20method%20improves%20the%20state-of-the-art%20in%0ABoolean%20networks%20discovery%20and%20is%20able%20to%20learn%20relevant%2C%20interpretable%20rules%0Ain%20tabular%20classification%2C%20notably%20on%20examples%20from%20the%20medical%20and%20industrial%0Afields%20where%20interpretability%20has%20tangible%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08172v4&entry.124074799=Read"},
{"title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal\n  Verification", "author": "Balaji Rao and William Eiers and Carlo Lipizzi", "abstract": "  Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.\n", "link": "http://arxiv.org/abs/2504.17017v2", "date": "2025-10-01", "relevancy": 1.9141, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Theorem%20Proving%3A%20Generating%20and%20Structuring%20Proofs%20for%20Formal%0A%20%20Verification&body=Title%3A%20Neural%20Theorem%20Proving%3A%20Generating%20and%20Structuring%20Proofs%20for%20Formal%0A%20%20Verification%0AAuthor%3A%20Balaji%20Rao%20and%20William%20Eiers%20and%20Carlo%20Lipizzi%0AAbstract%3A%20%20%20Formally%20verifying%20properties%20of%20software%20code%20has%20been%20a%20highly%20desirable%0Atask%2C%20especially%20with%20the%20emergence%20of%20LLM-generated%20code.%20In%20the%20same%20vein%2C%0Athey%20provide%20an%20interesting%20avenue%20for%20the%20exploration%20of%20formal%20verification%0Aand%20mechanistic%20interpretability.%20Since%20the%20introduction%20of%20code-specific%0Amodels%2C%20despite%20their%20successes%20in%20generating%20code%20in%20Lean4%20and%20Isabelle%2C%20the%0Atask%20of%20generalized%20theorem%20proving%20still%20remains%20far%20from%20being%20fully%20solved%0Aand%20will%20be%20a%20benchmark%20for%20reasoning%20capability%20in%20LLMs.%20In%20this%20work%2C%20we%0Aintroduce%20a%20framework%20that%20generates%20whole%20proofs%20in%20a%20formal%20language%20to%20be%0Aused%20within%20systems%20that%20utilize%20the%20power%20of%20built-in%20tactics%20and%0Aoff-the-shelf%20automated%20theorem%20provers.%20Our%20framework%20includes%203%20components%3A%0Agenerating%20natural%20language%20statements%20of%20the%20code%20to%20be%20verified%2C%20an%20LLM%20that%0Agenerates%20formal%20proofs%20for%20the%20given%20statement%2C%20and%20a%20module%20employing%0Aheuristics%20for%20building%20the%20final%20proof.%20To%20train%20the%20LLM%2C%20we%20employ%20a%202-stage%0Afine-tuning%20process%2C%20where%20we%20first%20use%20SFT-based%20training%20to%20enable%20the%20model%0Ato%20generate%20syntactically%20correct%20Isabelle%20code%20and%20then%20RL-based%20training%20that%0Aencourages%20the%20model%20to%20generate%20proofs%20verified%20by%20a%20theorem%20prover.%20We%0Avalidate%20our%20framework%20using%20the%20miniF2F-test%20benchmark%20and%20the%20Isabelle%20proof%0Aassistant%20and%20design%20a%20use%20case%20to%20verify%20the%20correctness%20of%20the%20AWS%20S3%20bucket%0Aaccess%20policy%20code.%20We%20also%20curate%20a%20dataset%20based%20on%20the%0AFVEL%5Ctextsubscript%7B%5Ctextnormal%7BER%7D%7D%20dataset%20for%20future%20training%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Theorem%2520Proving%253A%2520Generating%2520and%2520Structuring%2520Proofs%2520for%2520Formal%250A%2520%2520Verification%26entry.906535625%3DBalaji%2520Rao%2520and%2520William%2520Eiers%2520and%2520Carlo%2520Lipizzi%26entry.1292438233%3D%2520%2520Formally%2520verifying%2520properties%2520of%2520software%2520code%2520has%2520been%2520a%2520highly%2520desirable%250Atask%252C%2520especially%2520with%2520the%2520emergence%2520of%2520LLM-generated%2520code.%2520In%2520the%2520same%2520vein%252C%250Athey%2520provide%2520an%2520interesting%2520avenue%2520for%2520the%2520exploration%2520of%2520formal%2520verification%250Aand%2520mechanistic%2520interpretability.%2520Since%2520the%2520introduction%2520of%2520code-specific%250Amodels%252C%2520despite%2520their%2520successes%2520in%2520generating%2520code%2520in%2520Lean4%2520and%2520Isabelle%252C%2520the%250Atask%2520of%2520generalized%2520theorem%2520proving%2520still%2520remains%2520far%2520from%2520being%2520fully%2520solved%250Aand%2520will%2520be%2520a%2520benchmark%2520for%2520reasoning%2520capability%2520in%2520LLMs.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520framework%2520that%2520generates%2520whole%2520proofs%2520in%2520a%2520formal%2520language%2520to%2520be%250Aused%2520within%2520systems%2520that%2520utilize%2520the%2520power%2520of%2520built-in%2520tactics%2520and%250Aoff-the-shelf%2520automated%2520theorem%2520provers.%2520Our%2520framework%2520includes%25203%2520components%253A%250Agenerating%2520natural%2520language%2520statements%2520of%2520the%2520code%2520to%2520be%2520verified%252C%2520an%2520LLM%2520that%250Agenerates%2520formal%2520proofs%2520for%2520the%2520given%2520statement%252C%2520and%2520a%2520module%2520employing%250Aheuristics%2520for%2520building%2520the%2520final%2520proof.%2520To%2520train%2520the%2520LLM%252C%2520we%2520employ%2520a%25202-stage%250Afine-tuning%2520process%252C%2520where%2520we%2520first%2520use%2520SFT-based%2520training%2520to%2520enable%2520the%2520model%250Ato%2520generate%2520syntactically%2520correct%2520Isabelle%2520code%2520and%2520then%2520RL-based%2520training%2520that%250Aencourages%2520the%2520model%2520to%2520generate%2520proofs%2520verified%2520by%2520a%2520theorem%2520prover.%2520We%250Avalidate%2520our%2520framework%2520using%2520the%2520miniF2F-test%2520benchmark%2520and%2520the%2520Isabelle%2520proof%250Aassistant%2520and%2520design%2520a%2520use%2520case%2520to%2520verify%2520the%2520correctness%2520of%2520the%2520AWS%2520S3%2520bucket%250Aaccess%2520policy%2520code.%2520We%2520also%2520curate%2520a%2520dataset%2520based%2520on%2520the%250AFVEL%255Ctextsubscript%257B%255Ctextnormal%257BER%257D%257D%2520dataset%2520for%2520future%2520training%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Theorem%20Proving%3A%20Generating%20and%20Structuring%20Proofs%20for%20Formal%0A%20%20Verification&entry.906535625=Balaji%20Rao%20and%20William%20Eiers%20and%20Carlo%20Lipizzi&entry.1292438233=%20%20Formally%20verifying%20properties%20of%20software%20code%20has%20been%20a%20highly%20desirable%0Atask%2C%20especially%20with%20the%20emergence%20of%20LLM-generated%20code.%20In%20the%20same%20vein%2C%0Athey%20provide%20an%20interesting%20avenue%20for%20the%20exploration%20of%20formal%20verification%0Aand%20mechanistic%20interpretability.%20Since%20the%20introduction%20of%20code-specific%0Amodels%2C%20despite%20their%20successes%20in%20generating%20code%20in%20Lean4%20and%20Isabelle%2C%20the%0Atask%20of%20generalized%20theorem%20proving%20still%20remains%20far%20from%20being%20fully%20solved%0Aand%20will%20be%20a%20benchmark%20for%20reasoning%20capability%20in%20LLMs.%20In%20this%20work%2C%20we%0Aintroduce%20a%20framework%20that%20generates%20whole%20proofs%20in%20a%20formal%20language%20to%20be%0Aused%20within%20systems%20that%20utilize%20the%20power%20of%20built-in%20tactics%20and%0Aoff-the-shelf%20automated%20theorem%20provers.%20Our%20framework%20includes%203%20components%3A%0Agenerating%20natural%20language%20statements%20of%20the%20code%20to%20be%20verified%2C%20an%20LLM%20that%0Agenerates%20formal%20proofs%20for%20the%20given%20statement%2C%20and%20a%20module%20employing%0Aheuristics%20for%20building%20the%20final%20proof.%20To%20train%20the%20LLM%2C%20we%20employ%20a%202-stage%0Afine-tuning%20process%2C%20where%20we%20first%20use%20SFT-based%20training%20to%20enable%20the%20model%0Ato%20generate%20syntactically%20correct%20Isabelle%20code%20and%20then%20RL-based%20training%20that%0Aencourages%20the%20model%20to%20generate%20proofs%20verified%20by%20a%20theorem%20prover.%20We%0Avalidate%20our%20framework%20using%20the%20miniF2F-test%20benchmark%20and%20the%20Isabelle%20proof%0Aassistant%20and%20design%20a%20use%20case%20to%20verify%20the%20correctness%20of%20the%20AWS%20S3%20bucket%0Aaccess%20policy%20code.%20We%20also%20curate%20a%20dataset%20based%20on%20the%0AFVEL%5Ctextsubscript%7B%5Ctextnormal%7BER%7D%7D%20dataset%20for%20future%20training%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17017v2&entry.124074799=Read"},
{"title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT\n  Dataset", "author": "Ruixu Zhang and Yuran Wang and Xinyi Hu and Chaoyu Mai and Wenxuan Liu and Danni Xu and Xian Zhong and Zheng Wang", "abstract": "  Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.\n", "link": "http://arxiv.org/abs/2509.20715v3", "date": "2025-10-01", "relevancy": 1.9108, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4842}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4813}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset&body=Title%3A%20Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset%0AAuthor%3A%20Ruixu%20Zhang%20and%20Yuran%20Wang%20and%20Xinyi%20Hu%20and%20Chaoyu%20Mai%20and%20Wenxuan%20Liu%20and%20Danni%20Xu%20and%20Xian%20Zhong%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20Intention%20recognition%20has%20traditionally%20focused%20on%20individual%20intentions%2C%0Aoverlooking%20the%20complexities%20of%20collective%20intentions%20in%20group%20settings.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20concept%20of%20group%20intention%2C%20which%0Arepresents%20shared%20goals%20emerging%20through%20the%20actions%20of%20multiple%20individuals%2C%0Aand%20Group%20Intention%20Forecasting%20%28GIF%29%2C%20a%20novel%20task%20that%20forecasts%20when%20group%0Aintentions%20will%20occur%20by%20analyzing%20individual%20actions%20and%20interactions%20before%0Athe%20collective%20goal%20becomes%20apparent.%20To%20investigate%20GIF%20in%20a%20specific%0Ascenario%2C%20we%20propose%20SHOT%2C%20the%20first%20large-scale%20dataset%20for%20GIF%2C%20consisting%20of%0A1%2C979%20basketball%20video%20clips%20captured%20from%205%20camera%20views%20and%20annotated%20with%206%0Atypes%20of%20individual%20attributes.%20SHOT%20is%20designed%20with%203%20key%20characteristics%3A%0Amulti-individual%20information%2C%20multi-view%20adaptability%2C%20and%20multi-level%0Aintention%2C%20making%20it%20well-suited%20for%20studying%20emerging%20group%20intentions.%0AFurthermore%2C%20we%20introduce%20GIFT%20%28Group%20Intention%20ForecasTer%29%2C%20a%20framework%20that%0Aextracts%20fine-grained%20individual%20features%20and%20models%20evolving%20group%20dynamics%20to%0Aforecast%20intention%20emergence.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0ASHOT%20and%20GIFT%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20group%0Aintention%20forecasting.%20The%20dataset%20is%20available%20at%0Ahttps%3A//xinyi-hu.github.io/SHOT_DATASET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20715v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Individual%253A%2520Introducing%2520Group%2520Intention%2520Forecasting%2520with%2520SHOT%250A%2520%2520Dataset%26entry.906535625%3DRuixu%2520Zhang%2520and%2520Yuran%2520Wang%2520and%2520Xinyi%2520Hu%2520and%2520Chaoyu%2520Mai%2520and%2520Wenxuan%2520Liu%2520and%2520Danni%2520Xu%2520and%2520Xian%2520Zhong%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520Intention%2520recognition%2520has%2520traditionally%2520focused%2520on%2520individual%2520intentions%252C%250Aoverlooking%2520the%2520complexities%2520of%2520collective%2520intentions%2520in%2520group%2520settings.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520concept%2520of%2520group%2520intention%252C%2520which%250Arepresents%2520shared%2520goals%2520emerging%2520through%2520the%2520actions%2520of%2520multiple%2520individuals%252C%250Aand%2520Group%2520Intention%2520Forecasting%2520%2528GIF%2529%252C%2520a%2520novel%2520task%2520that%2520forecasts%2520when%2520group%250Aintentions%2520will%2520occur%2520by%2520analyzing%2520individual%2520actions%2520and%2520interactions%2520before%250Athe%2520collective%2520goal%2520becomes%2520apparent.%2520To%2520investigate%2520GIF%2520in%2520a%2520specific%250Ascenario%252C%2520we%2520propose%2520SHOT%252C%2520the%2520first%2520large-scale%2520dataset%2520for%2520GIF%252C%2520consisting%2520of%250A1%252C979%2520basketball%2520video%2520clips%2520captured%2520from%25205%2520camera%2520views%2520and%2520annotated%2520with%25206%250Atypes%2520of%2520individual%2520attributes.%2520SHOT%2520is%2520designed%2520with%25203%2520key%2520characteristics%253A%250Amulti-individual%2520information%252C%2520multi-view%2520adaptability%252C%2520and%2520multi-level%250Aintention%252C%2520making%2520it%2520well-suited%2520for%2520studying%2520emerging%2520group%2520intentions.%250AFurthermore%252C%2520we%2520introduce%2520GIFT%2520%2528Group%2520Intention%2520ForecasTer%2529%252C%2520a%2520framework%2520that%250Aextracts%2520fine-grained%2520individual%2520features%2520and%2520models%2520evolving%2520group%2520dynamics%2520to%250Aforecast%2520intention%2520emergence.%2520Experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%250ASHOT%2520and%2520GIFT%252C%2520establishing%2520a%2520strong%2520foundation%2520for%2520future%2520research%2520in%2520group%250Aintention%2520forecasting.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//xinyi-hu.github.io/SHOT_DATASET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20715v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset&entry.906535625=Ruixu%20Zhang%20and%20Yuran%20Wang%20and%20Xinyi%20Hu%20and%20Chaoyu%20Mai%20and%20Wenxuan%20Liu%20and%20Danni%20Xu%20and%20Xian%20Zhong%20and%20Zheng%20Wang&entry.1292438233=%20%20Intention%20recognition%20has%20traditionally%20focused%20on%20individual%20intentions%2C%0Aoverlooking%20the%20complexities%20of%20collective%20intentions%20in%20group%20settings.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20concept%20of%20group%20intention%2C%20which%0Arepresents%20shared%20goals%20emerging%20through%20the%20actions%20of%20multiple%20individuals%2C%0Aand%20Group%20Intention%20Forecasting%20%28GIF%29%2C%20a%20novel%20task%20that%20forecasts%20when%20group%0Aintentions%20will%20occur%20by%20analyzing%20individual%20actions%20and%20interactions%20before%0Athe%20collective%20goal%20becomes%20apparent.%20To%20investigate%20GIF%20in%20a%20specific%0Ascenario%2C%20we%20propose%20SHOT%2C%20the%20first%20large-scale%20dataset%20for%20GIF%2C%20consisting%20of%0A1%2C979%20basketball%20video%20clips%20captured%20from%205%20camera%20views%20and%20annotated%20with%206%0Atypes%20of%20individual%20attributes.%20SHOT%20is%20designed%20with%203%20key%20characteristics%3A%0Amulti-individual%20information%2C%20multi-view%20adaptability%2C%20and%20multi-level%0Aintention%2C%20making%20it%20well-suited%20for%20studying%20emerging%20group%20intentions.%0AFurthermore%2C%20we%20introduce%20GIFT%20%28Group%20Intention%20ForecasTer%29%2C%20a%20framework%20that%0Aextracts%20fine-grained%20individual%20features%20and%20models%20evolving%20group%20dynamics%20to%0Aforecast%20intention%20emergence.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0ASHOT%20and%20GIFT%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20group%0Aintention%20forecasting.%20The%20dataset%20is%20available%20at%0Ahttps%3A//xinyi-hu.github.io/SHOT_DATASET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20715v3&entry.124074799=Read"},
{"title": "On the Application of Model Predictive Control to a Weighted Coverage\n  Path Planning Problem", "author": "Kilian Schweppe and Ludmila Moshagen and Georg Schildbach", "abstract": "  This paper considers the application of Model Predictive Control (MPC) to a\nweighted coverage path planning (WCPP) problem. The problem appears in a wide\nrange of practical applications, including search and rescue (SAR) missions.\nThe basic setup is that one (or multiple) agents can move around a given search\nspace and collect rewards from a given spatial distribution. Unlike an\nartificial potential field, each reward can only be collected once. In contrast\nto a Traveling Salesman Problem (TSP), the agent moves in a continuous space.\nMoreover, he is not obliged to cover all locations and/or may return to\npreviously visited locations. The WCPP problem is tackled by a new Model\nPredictive Control (MPC) formulation with so-called Coverage Constraints (CCs).\nIt is shown that the solution becomes more effective if the solver is\ninitialized with a TSP-based heuristic. With and without this initialization,\nthe proposed MPC approach clearly outperforms a naive MPC formulation, as\ndemonstrated in a small simulation study.\n", "link": "http://arxiv.org/abs/2411.08634v2", "date": "2025-10-01", "relevancy": 1.9099, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5038}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4771}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem&body=Title%3A%20On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem%0AAuthor%3A%20Kilian%20Schweppe%20and%20Ludmila%20Moshagen%20and%20Georg%20Schildbach%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%20a%0Aweighted%20coverage%20path%20planning%20%28WCPP%29%20problem.%20The%20problem%20appears%20in%20a%20wide%0Arange%20of%20practical%20applications%2C%20including%20search%20and%20rescue%20%28SAR%29%20missions.%0AThe%20basic%20setup%20is%20that%20one%20%28or%20multiple%29%20agents%20can%20move%20around%20a%20given%20search%0Aspace%20and%20collect%20rewards%20from%20a%20given%20spatial%20distribution.%20Unlike%20an%0Aartificial%20potential%20field%2C%20each%20reward%20can%20only%20be%20collected%20once.%20In%20contrast%0Ato%20a%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%20the%20agent%20moves%20in%20a%20continuous%20space.%0AMoreover%2C%20he%20is%20not%20obliged%20to%20cover%20all%20locations%20and/or%20may%20return%20to%0Apreviously%20visited%20locations.%20The%20WCPP%20problem%20is%20tackled%20by%20a%20new%20Model%0APredictive%20Control%20%28MPC%29%20formulation%20with%20so-called%20Coverage%20Constraints%20%28CCs%29.%0AIt%20is%20shown%20that%20the%20solution%20becomes%20more%20effective%20if%20the%20solver%20is%0Ainitialized%20with%20a%20TSP-based%20heuristic.%20With%20and%20without%20this%20initialization%2C%0Athe%20proposed%20MPC%20approach%20clearly%20outperforms%20a%20naive%20MPC%20formulation%2C%20as%0Ademonstrated%20in%20a%20small%20simulation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Application%2520of%2520Model%2520Predictive%2520Control%2520to%2520a%2520Weighted%2520Coverage%250A%2520%2520Path%2520Planning%2520Problem%26entry.906535625%3DKilian%2520Schweppe%2520and%2520Ludmila%2520Moshagen%2520and%2520Georg%2520Schildbach%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520application%2520of%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520to%2520a%250Aweighted%2520coverage%2520path%2520planning%2520%2528WCPP%2529%2520problem.%2520The%2520problem%2520appears%2520in%2520a%2520wide%250Arange%2520of%2520practical%2520applications%252C%2520including%2520search%2520and%2520rescue%2520%2528SAR%2529%2520missions.%250AThe%2520basic%2520setup%2520is%2520that%2520one%2520%2528or%2520multiple%2529%2520agents%2520can%2520move%2520around%2520a%2520given%2520search%250Aspace%2520and%2520collect%2520rewards%2520from%2520a%2520given%2520spatial%2520distribution.%2520Unlike%2520an%250Aartificial%2520potential%2520field%252C%2520each%2520reward%2520can%2520only%2520be%2520collected%2520once.%2520In%2520contrast%250Ato%2520a%2520Traveling%2520Salesman%2520Problem%2520%2528TSP%2529%252C%2520the%2520agent%2520moves%2520in%2520a%2520continuous%2520space.%250AMoreover%252C%2520he%2520is%2520not%2520obliged%2520to%2520cover%2520all%2520locations%2520and/or%2520may%2520return%2520to%250Apreviously%2520visited%2520locations.%2520The%2520WCPP%2520problem%2520is%2520tackled%2520by%2520a%2520new%2520Model%250APredictive%2520Control%2520%2528MPC%2529%2520formulation%2520with%2520so-called%2520Coverage%2520Constraints%2520%2528CCs%2529.%250AIt%2520is%2520shown%2520that%2520the%2520solution%2520becomes%2520more%2520effective%2520if%2520the%2520solver%2520is%250Ainitialized%2520with%2520a%2520TSP-based%2520heuristic.%2520With%2520and%2520without%2520this%2520initialization%252C%250Athe%2520proposed%2520MPC%2520approach%2520clearly%2520outperforms%2520a%2520naive%2520MPC%2520formulation%252C%2520as%250Ademonstrated%2520in%2520a%2520small%2520simulation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem&entry.906535625=Kilian%20Schweppe%20and%20Ludmila%20Moshagen%20and%20Georg%20Schildbach&entry.1292438233=%20%20This%20paper%20considers%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%20a%0Aweighted%20coverage%20path%20planning%20%28WCPP%29%20problem.%20The%20problem%20appears%20in%20a%20wide%0Arange%20of%20practical%20applications%2C%20including%20search%20and%20rescue%20%28SAR%29%20missions.%0AThe%20basic%20setup%20is%20that%20one%20%28or%20multiple%29%20agents%20can%20move%20around%20a%20given%20search%0Aspace%20and%20collect%20rewards%20from%20a%20given%20spatial%20distribution.%20Unlike%20an%0Aartificial%20potential%20field%2C%20each%20reward%20can%20only%20be%20collected%20once.%20In%20contrast%0Ato%20a%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%20the%20agent%20moves%20in%20a%20continuous%20space.%0AMoreover%2C%20he%20is%20not%20obliged%20to%20cover%20all%20locations%20and/or%20may%20return%20to%0Apreviously%20visited%20locations.%20The%20WCPP%20problem%20is%20tackled%20by%20a%20new%20Model%0APredictive%20Control%20%28MPC%29%20formulation%20with%20so-called%20Coverage%20Constraints%20%28CCs%29.%0AIt%20is%20shown%20that%20the%20solution%20becomes%20more%20effective%20if%20the%20solver%20is%0Ainitialized%20with%20a%20TSP-based%20heuristic.%20With%20and%20without%20this%20initialization%2C%0Athe%20proposed%20MPC%20approach%20clearly%20outperforms%20a%20naive%20MPC%20formulation%2C%20as%0Ademonstrated%20in%20a%20small%20simulation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08634v2&entry.124074799=Read"},
{"title": "PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face\n  Generation", "author": "Fatemeh Nazarieh and Zhenhua Feng and Diptesh Kanojia and Muhammad Awais and Josef Kittler", "abstract": "  Audio-driven talking face generation is a challenging task in digital\ncommunication. Despite significant progress in the area, most existing methods\nconcentrate on audio-lip synchronization, often overlooking aspects such as\nvisual quality, customization, and generalization that are crucial to producing\nrealistic talking faces. To address these limitations, we introduce a novel,\ncustomizable one-shot audio-driven talking face generation framework, named\nPortraitTalk. Our proposed method utilizes a latent diffusion framework\nconsisting of two main components: IdentityNet and AnimateNet. IdentityNet is\ndesigned to preserve identity features consistently across the generated video\nframes, while AnimateNet aims to enhance temporal coherence and motion\nconsistency. This framework also integrates an audio input with the reference\nimages, thereby reducing the reliance on reference-style videos prevalent in\nexisting approaches. A key innovation of PortraitTalk is the incorporation of\ntext prompts through decoupled cross-attention mechanisms, which significantly\nexpands creative control over the generated videos. Through extensive\nexperiments, including a newly developed evaluation metric, our model\ndemonstrates superior performance over the state-of-the-art methods, setting a\nnew standard for the generation of customizable realistic talking faces\nsuitable for real-world applications.\n", "link": "http://arxiv.org/abs/2412.07754v2", "date": "2025-10-01", "relevancy": 1.9097, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6502}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6464}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PortraitTalk%3A%20Towards%20Customizable%20One-Shot%20Audio-to-Talking%20Face%0A%20%20Generation&body=Title%3A%20PortraitTalk%3A%20Towards%20Customizable%20One-Shot%20Audio-to-Talking%20Face%0A%20%20Generation%0AAuthor%3A%20Fatemeh%20Nazarieh%20and%20Zhenhua%20Feng%20and%20Diptesh%20Kanojia%20and%20Muhammad%20Awais%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20Audio-driven%20talking%20face%20generation%20is%20a%20challenging%20task%20in%20digital%0Acommunication.%20Despite%20significant%20progress%20in%20the%20area%2C%20most%20existing%20methods%0Aconcentrate%20on%20audio-lip%20synchronization%2C%20often%20overlooking%20aspects%20such%20as%0Avisual%20quality%2C%20customization%2C%20and%20generalization%20that%20are%20crucial%20to%20producing%0Arealistic%20talking%20faces.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20novel%2C%0Acustomizable%20one-shot%20audio-driven%20talking%20face%20generation%20framework%2C%20named%0APortraitTalk.%20Our%20proposed%20method%20utilizes%20a%20latent%20diffusion%20framework%0Aconsisting%20of%20two%20main%20components%3A%20IdentityNet%20and%20AnimateNet.%20IdentityNet%20is%0Adesigned%20to%20preserve%20identity%20features%20consistently%20across%20the%20generated%20video%0Aframes%2C%20while%20AnimateNet%20aims%20to%20enhance%20temporal%20coherence%20and%20motion%0Aconsistency.%20This%20framework%20also%20integrates%20an%20audio%20input%20with%20the%20reference%0Aimages%2C%20thereby%20reducing%20the%20reliance%20on%20reference-style%20videos%20prevalent%20in%0Aexisting%20approaches.%20A%20key%20innovation%20of%20PortraitTalk%20is%20the%20incorporation%20of%0Atext%20prompts%20through%20decoupled%20cross-attention%20mechanisms%2C%20which%20significantly%0Aexpands%20creative%20control%20over%20the%20generated%20videos.%20Through%20extensive%0Aexperiments%2C%20including%20a%20newly%20developed%20evaluation%20metric%2C%20our%20model%0Ademonstrates%20superior%20performance%20over%20the%20state-of-the-art%20methods%2C%20setting%20a%0Anew%20standard%20for%20the%20generation%20of%20customizable%20realistic%20talking%20faces%0Asuitable%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPortraitTalk%253A%2520Towards%2520Customizable%2520One-Shot%2520Audio-to-Talking%2520Face%250A%2520%2520Generation%26entry.906535625%3DFatemeh%2520Nazarieh%2520and%2520Zhenhua%2520Feng%2520and%2520Diptesh%2520Kanojia%2520and%2520Muhammad%2520Awais%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520face%2520generation%2520is%2520a%2520challenging%2520task%2520in%2520digital%250Acommunication.%2520Despite%2520significant%2520progress%2520in%2520the%2520area%252C%2520most%2520existing%2520methods%250Aconcentrate%2520on%2520audio-lip%2520synchronization%252C%2520often%2520overlooking%2520aspects%2520such%2520as%250Avisual%2520quality%252C%2520customization%252C%2520and%2520generalization%2520that%2520are%2520crucial%2520to%2520producing%250Arealistic%2520talking%2520faces.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520novel%252C%250Acustomizable%2520one-shot%2520audio-driven%2520talking%2520face%2520generation%2520framework%252C%2520named%250APortraitTalk.%2520Our%2520proposed%2520method%2520utilizes%2520a%2520latent%2520diffusion%2520framework%250Aconsisting%2520of%2520two%2520main%2520components%253A%2520IdentityNet%2520and%2520AnimateNet.%2520IdentityNet%2520is%250Adesigned%2520to%2520preserve%2520identity%2520features%2520consistently%2520across%2520the%2520generated%2520video%250Aframes%252C%2520while%2520AnimateNet%2520aims%2520to%2520enhance%2520temporal%2520coherence%2520and%2520motion%250Aconsistency.%2520This%2520framework%2520also%2520integrates%2520an%2520audio%2520input%2520with%2520the%2520reference%250Aimages%252C%2520thereby%2520reducing%2520the%2520reliance%2520on%2520reference-style%2520videos%2520prevalent%2520in%250Aexisting%2520approaches.%2520A%2520key%2520innovation%2520of%2520PortraitTalk%2520is%2520the%2520incorporation%2520of%250Atext%2520prompts%2520through%2520decoupled%2520cross-attention%2520mechanisms%252C%2520which%2520significantly%250Aexpands%2520creative%2520control%2520over%2520the%2520generated%2520videos.%2520Through%2520extensive%250Aexperiments%252C%2520including%2520a%2520newly%2520developed%2520evaluation%2520metric%252C%2520our%2520model%250Ademonstrates%2520superior%2520performance%2520over%2520the%2520state-of-the-art%2520methods%252C%2520setting%2520a%250Anew%2520standard%2520for%2520the%2520generation%2520of%2520customizable%2520realistic%2520talking%2520faces%250Asuitable%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PortraitTalk%3A%20Towards%20Customizable%20One-Shot%20Audio-to-Talking%20Face%0A%20%20Generation&entry.906535625=Fatemeh%20Nazarieh%20and%20Zhenhua%20Feng%20and%20Diptesh%20Kanojia%20and%20Muhammad%20Awais%20and%20Josef%20Kittler&entry.1292438233=%20%20Audio-driven%20talking%20face%20generation%20is%20a%20challenging%20task%20in%20digital%0Acommunication.%20Despite%20significant%20progress%20in%20the%20area%2C%20most%20existing%20methods%0Aconcentrate%20on%20audio-lip%20synchronization%2C%20often%20overlooking%20aspects%20such%20as%0Avisual%20quality%2C%20customization%2C%20and%20generalization%20that%20are%20crucial%20to%20producing%0Arealistic%20talking%20faces.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20novel%2C%0Acustomizable%20one-shot%20audio-driven%20talking%20face%20generation%20framework%2C%20named%0APortraitTalk.%20Our%20proposed%20method%20utilizes%20a%20latent%20diffusion%20framework%0Aconsisting%20of%20two%20main%20components%3A%20IdentityNet%20and%20AnimateNet.%20IdentityNet%20is%0Adesigned%20to%20preserve%20identity%20features%20consistently%20across%20the%20generated%20video%0Aframes%2C%20while%20AnimateNet%20aims%20to%20enhance%20temporal%20coherence%20and%20motion%0Aconsistency.%20This%20framework%20also%20integrates%20an%20audio%20input%20with%20the%20reference%0Aimages%2C%20thereby%20reducing%20the%20reliance%20on%20reference-style%20videos%20prevalent%20in%0Aexisting%20approaches.%20A%20key%20innovation%20of%20PortraitTalk%20is%20the%20incorporation%20of%0Atext%20prompts%20through%20decoupled%20cross-attention%20mechanisms%2C%20which%20significantly%0Aexpands%20creative%20control%20over%20the%20generated%20videos.%20Through%20extensive%0Aexperiments%2C%20including%20a%20newly%20developed%20evaluation%20metric%2C%20our%20model%0Ademonstrates%20superior%20performance%20over%20the%20state-of-the-art%20methods%2C%20setting%20a%0Anew%20standard%20for%20the%20generation%20of%20customizable%20realistic%20talking%20faces%0Asuitable%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07754v2&entry.124074799=Read"},
{"title": "IC-Custom: Diverse Image Customization via In-Context Learning", "author": "Yaowei Li and Xiaoyu Li and Zhaoyang Zhang and Yuxuan Bian and Gan Liu and Xinyuan Li and Jiale Xu and Wenbo Hu and Yating Liu and Lingen Li and Jing Cai and Yuexian Zou and Yancheng He and Ying Shan", "abstract": "  Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We propose the\nIn-context Multi-Modal Attention (ICMA) mechanism, which employs learnable\ntask-oriented register tokens and boundary-aware positional embeddings to\nenable the model to effectively handle diverse tasks and distinguish between\ninputs in polyptych configurations. To address the data gap, we curated a 12K\nidentity-consistent dataset with 8K real-world and 4K high-quality synthetic\nsamples, avoiding the overly glossy, oversaturated look typical of synthetic\ndata. IC-Custom supports various industrial applications, including try-on,\nimage insertion, and creative IP customization. Extensive evaluations on our\nproposed ProductBench and the publicly available DreamBench demonstrate that\nIC-Custom significantly outperforms community workflows, closed-source models,\nand state-of-the-art open-source approaches. IC-Custom achieves about 73\\%\nhigher human preference across identity consistency, harmony, and text\nalignment metrics, while training only 0.4\\% of the original model parameters.\nProject page: https://liyaowei-stu.github.io/project/IC_Custom\n", "link": "http://arxiv.org/abs/2507.01926v3", "date": "2025-10-01", "relevancy": 1.9027, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6451}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6221}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IC-Custom%3A%20Diverse%20Image%20Customization%20via%20In-Context%20Learning&body=Title%3A%20IC-Custom%3A%20Diverse%20Image%20Customization%20via%20In-Context%20Learning%0AAuthor%3A%20Yaowei%20Li%20and%20Xiaoyu%20Li%20and%20Zhaoyang%20Zhang%20and%20Yuxuan%20Bian%20and%20Gan%20Liu%20and%20Xinyuan%20Li%20and%20Jiale%20Xu%20and%20Wenbo%20Hu%20and%20Yating%20Liu%20and%20Lingen%20Li%20and%20Jing%20Cai%20and%20Yuexian%20Zou%20and%20Yancheng%20He%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Image%20customization%2C%20a%20crucial%20technique%20for%20industrial%20media%20production%2C%0Aaims%20to%20generate%20content%20that%20is%20consistent%20with%20reference%20images.%20However%2C%0Acurrent%20approaches%20conventionally%20separate%20image%20customization%20into%0Aposition-aware%20and%20position-free%20customization%20paradigms%20and%20lack%20a%20universal%0Aframework%20for%20diverse%20customization%2C%20limiting%20their%20applications%20across%20various%0Ascenarios.%20To%20overcome%20these%20limitations%2C%20we%20propose%20IC-Custom%2C%20a%20unified%0Aframework%20that%20seamlessly%20integrates%20position-aware%20and%20position-free%20image%0Acustomization%20through%20in-context%20learning.%20IC-Custom%20concatenates%20reference%0Aimages%20with%20target%20images%20to%20a%20polyptych%2C%20leveraging%20DiT%27s%20multi-modal%0Aattention%20mechanism%20for%20fine-grained%20token-level%20interactions.%20We%20propose%20the%0AIn-context%20Multi-Modal%20Attention%20%28ICMA%29%20mechanism%2C%20which%20employs%20learnable%0Atask-oriented%20register%20tokens%20and%20boundary-aware%20positional%20embeddings%20to%0Aenable%20the%20model%20to%20effectively%20handle%20diverse%20tasks%20and%20distinguish%20between%0Ainputs%20in%20polyptych%20configurations.%20To%20address%20the%20data%20gap%2C%20we%20curated%20a%2012K%0Aidentity-consistent%20dataset%20with%208K%20real-world%20and%204K%20high-quality%20synthetic%0Asamples%2C%20avoiding%20the%20overly%20glossy%2C%20oversaturated%20look%20typical%20of%20synthetic%0Adata.%20IC-Custom%20supports%20various%20industrial%20applications%2C%20including%20try-on%2C%0Aimage%20insertion%2C%20and%20creative%20IP%20customization.%20Extensive%20evaluations%20on%20our%0Aproposed%20ProductBench%20and%20the%20publicly%20available%20DreamBench%20demonstrate%20that%0AIC-Custom%20significantly%20outperforms%20community%20workflows%2C%20closed-source%20models%2C%0Aand%20state-of-the-art%20open-source%20approaches.%20IC-Custom%20achieves%20about%2073%5C%25%0Ahigher%20human%20preference%20across%20identity%20consistency%2C%20harmony%2C%20and%20text%0Aalignment%20metrics%2C%20while%20training%20only%200.4%5C%25%20of%20the%20original%20model%20parameters.%0AProject%20page%3A%20https%3A//liyaowei-stu.github.io/project/IC_Custom%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01926v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIC-Custom%253A%2520Diverse%2520Image%2520Customization%2520via%2520In-Context%2520Learning%26entry.906535625%3DYaowei%2520Li%2520and%2520Xiaoyu%2520Li%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yuxuan%2520Bian%2520and%2520Gan%2520Liu%2520and%2520Xinyuan%2520Li%2520and%2520Jiale%2520Xu%2520and%2520Wenbo%2520Hu%2520and%2520Yating%2520Liu%2520and%2520Lingen%2520Li%2520and%2520Jing%2520Cai%2520and%2520Yuexian%2520Zou%2520and%2520Yancheng%2520He%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Image%2520customization%252C%2520a%2520crucial%2520technique%2520for%2520industrial%2520media%2520production%252C%250Aaims%2520to%2520generate%2520content%2520that%2520is%2520consistent%2520with%2520reference%2520images.%2520However%252C%250Acurrent%2520approaches%2520conventionally%2520separate%2520image%2520customization%2520into%250Aposition-aware%2520and%2520position-free%2520customization%2520paradigms%2520and%2520lack%2520a%2520universal%250Aframework%2520for%2520diverse%2520customization%252C%2520limiting%2520their%2520applications%2520across%2520various%250Ascenarios.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520IC-Custom%252C%2520a%2520unified%250Aframework%2520that%2520seamlessly%2520integrates%2520position-aware%2520and%2520position-free%2520image%250Acustomization%2520through%2520in-context%2520learning.%2520IC-Custom%2520concatenates%2520reference%250Aimages%2520with%2520target%2520images%2520to%2520a%2520polyptych%252C%2520leveraging%2520DiT%2527s%2520multi-modal%250Aattention%2520mechanism%2520for%2520fine-grained%2520token-level%2520interactions.%2520We%2520propose%2520the%250AIn-context%2520Multi-Modal%2520Attention%2520%2528ICMA%2529%2520mechanism%252C%2520which%2520employs%2520learnable%250Atask-oriented%2520register%2520tokens%2520and%2520boundary-aware%2520positional%2520embeddings%2520to%250Aenable%2520the%2520model%2520to%2520effectively%2520handle%2520diverse%2520tasks%2520and%2520distinguish%2520between%250Ainputs%2520in%2520polyptych%2520configurations.%2520To%2520address%2520the%2520data%2520gap%252C%2520we%2520curated%2520a%252012K%250Aidentity-consistent%2520dataset%2520with%25208K%2520real-world%2520and%25204K%2520high-quality%2520synthetic%250Asamples%252C%2520avoiding%2520the%2520overly%2520glossy%252C%2520oversaturated%2520look%2520typical%2520of%2520synthetic%250Adata.%2520IC-Custom%2520supports%2520various%2520industrial%2520applications%252C%2520including%2520try-on%252C%250Aimage%2520insertion%252C%2520and%2520creative%2520IP%2520customization.%2520Extensive%2520evaluations%2520on%2520our%250Aproposed%2520ProductBench%2520and%2520the%2520publicly%2520available%2520DreamBench%2520demonstrate%2520that%250AIC-Custom%2520significantly%2520outperforms%2520community%2520workflows%252C%2520closed-source%2520models%252C%250Aand%2520state-of-the-art%2520open-source%2520approaches.%2520IC-Custom%2520achieves%2520about%252073%255C%2525%250Ahigher%2520human%2520preference%2520across%2520identity%2520consistency%252C%2520harmony%252C%2520and%2520text%250Aalignment%2520metrics%252C%2520while%2520training%2520only%25200.4%255C%2525%2520of%2520the%2520original%2520model%2520parameters.%250AProject%2520page%253A%2520https%253A//liyaowei-stu.github.io/project/IC_Custom%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01926v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IC-Custom%3A%20Diverse%20Image%20Customization%20via%20In-Context%20Learning&entry.906535625=Yaowei%20Li%20and%20Xiaoyu%20Li%20and%20Zhaoyang%20Zhang%20and%20Yuxuan%20Bian%20and%20Gan%20Liu%20and%20Xinyuan%20Li%20and%20Jiale%20Xu%20and%20Wenbo%20Hu%20and%20Yating%20Liu%20and%20Lingen%20Li%20and%20Jing%20Cai%20and%20Yuexian%20Zou%20and%20Yancheng%20He%20and%20Ying%20Shan&entry.1292438233=%20%20Image%20customization%2C%20a%20crucial%20technique%20for%20industrial%20media%20production%2C%0Aaims%20to%20generate%20content%20that%20is%20consistent%20with%20reference%20images.%20However%2C%0Acurrent%20approaches%20conventionally%20separate%20image%20customization%20into%0Aposition-aware%20and%20position-free%20customization%20paradigms%20and%20lack%20a%20universal%0Aframework%20for%20diverse%20customization%2C%20limiting%20their%20applications%20across%20various%0Ascenarios.%20To%20overcome%20these%20limitations%2C%20we%20propose%20IC-Custom%2C%20a%20unified%0Aframework%20that%20seamlessly%20integrates%20position-aware%20and%20position-free%20image%0Acustomization%20through%20in-context%20learning.%20IC-Custom%20concatenates%20reference%0Aimages%20with%20target%20images%20to%20a%20polyptych%2C%20leveraging%20DiT%27s%20multi-modal%0Aattention%20mechanism%20for%20fine-grained%20token-level%20interactions.%20We%20propose%20the%0AIn-context%20Multi-Modal%20Attention%20%28ICMA%29%20mechanism%2C%20which%20employs%20learnable%0Atask-oriented%20register%20tokens%20and%20boundary-aware%20positional%20embeddings%20to%0Aenable%20the%20model%20to%20effectively%20handle%20diverse%20tasks%20and%20distinguish%20between%0Ainputs%20in%20polyptych%20configurations.%20To%20address%20the%20data%20gap%2C%20we%20curated%20a%2012K%0Aidentity-consistent%20dataset%20with%208K%20real-world%20and%204K%20high-quality%20synthetic%0Asamples%2C%20avoiding%20the%20overly%20glossy%2C%20oversaturated%20look%20typical%20of%20synthetic%0Adata.%20IC-Custom%20supports%20various%20industrial%20applications%2C%20including%20try-on%2C%0Aimage%20insertion%2C%20and%20creative%20IP%20customization.%20Extensive%20evaluations%20on%20our%0Aproposed%20ProductBench%20and%20the%20publicly%20available%20DreamBench%20demonstrate%20that%0AIC-Custom%20significantly%20outperforms%20community%20workflows%2C%20closed-source%20models%2C%0Aand%20state-of-the-art%20open-source%20approaches.%20IC-Custom%20achieves%20about%2073%5C%25%0Ahigher%20human%20preference%20across%20identity%20consistency%2C%20harmony%2C%20and%20text%0Aalignment%20metrics%2C%20while%20training%20only%200.4%5C%25%20of%20the%20original%20model%20parameters.%0AProject%20page%3A%20https%3A//liyaowei-stu.github.io/project/IC_Custom%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01926v3&entry.124074799=Read"},
{"title": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical\n  Practice", "author": "Jack Gallifant and Katherine C. Kellogg and Matt Butler and Amanda Centi and Shan Chen and Patrick F. Doyle and Sayon Dutta and Joyce Guo and Matthew J. Hadfield and Esther H. Kim and David E. Kozono and Hugo JWL Aerts and Adam B. Landman and Raymond H. Mak and Rebecca G. Mishuris and Tanna L. Nelson and Guergana K. Savova and Elad Sharon and Benjamin C. Silverman and Umit Topaloglu and Jeremy L. Warner and Danielle S. Bitterman", "abstract": "  Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care.\n", "link": "http://arxiv.org/abs/2509.26153v2", "date": "2025-10-01", "relevancy": 1.8902, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4747}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Algorithm%3A%20A%20Field%20Guide%20to%20Deploying%20AI%20Agents%20in%20Clinical%0A%20%20Practice&body=Title%3A%20Beyond%20the%20Algorithm%3A%20A%20Field%20Guide%20to%20Deploying%20AI%20Agents%20in%20Clinical%0A%20%20Practice%0AAuthor%3A%20Jack%20Gallifant%20and%20Katherine%20C.%20Kellogg%20and%20Matt%20Butler%20and%20Amanda%20Centi%20and%20Shan%20Chen%20and%20Patrick%20F.%20Doyle%20and%20Sayon%20Dutta%20and%20Joyce%20Guo%20and%20Matthew%20J.%20Hadfield%20and%20Esther%20H.%20Kim%20and%20David%20E.%20Kozono%20and%20Hugo%20JWL%20Aerts%20and%20Adam%20B.%20Landman%20and%20Raymond%20H.%20Mak%20and%20Rebecca%20G.%20Mishuris%20and%20Tanna%20L.%20Nelson%20and%20Guergana%20K.%20Savova%20and%20Elad%20Sharon%20and%20Benjamin%20C.%20Silverman%20and%20Umit%20Topaloglu%20and%20Jeremy%20L.%20Warner%20and%20Danielle%20S.%20Bitterman%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20integrated%20into%20agent-driven%20workflows%20hold%0Aimmense%20promise%20for%20healthcare%2C%20yet%20a%20significant%20gap%20exists%20between%20their%0Apotential%20and%20practical%20implementation%20within%20clinical%20settings.%20To%20address%0Athis%2C%20we%20present%20a%20practitioner-oriented%20field%20manual%20for%20deploying%20generative%0Aagents%20that%20use%20electronic%20health%20record%20%28EHR%29%20data.%20This%20guide%20is%20informed%20by%0Aour%20experience%20deploying%20the%20%22irAE-Agent%22%2C%20an%20automated%20system%20to%20detect%0Aimmune-related%20adverse%20events%20from%20clinical%20notes%20at%20Mass%20General%20Brigham%2C%20and%0Aby%20structured%20interviews%20with%2020%20clinicians%2C%20engineers%2C%20and%20informatics%20leaders%0Ainvolved%20in%20the%20project.%20Our%20analysis%20reveals%20a%20critical%20misalignment%20in%0Aclinical%20AI%20development%3A%20less%20than%2020%25%20of%20our%20effort%20was%20dedicated%20to%20prompt%0Aengineering%20and%20model%20development%2C%20while%20over%2080%25%20was%20consumed%20by%20the%0Asociotechnical%20work%20of%20implementation.%20We%20distill%20this%20effort%20into%20five%20%22heavy%0Alifts%22%3A%20data%20integration%2C%20model%20validation%2C%20ensuring%20economic%20value%2C%20managing%0Asystem%20drift%2C%20and%20governance.%20By%20providing%20actionable%20solutions%20for%20each%20of%0Athese%20challenges%2C%20this%20field%20manual%20shifts%20the%20focus%20from%20algorithmic%0Adevelopment%20to%20the%20essential%20infrastructure%20and%20implementation%20work%20required%20to%0Abridge%20the%20%22valley%20of%20death%22%20and%20successfully%20translate%20generative%20AI%20from%0Apilot%20projects%20into%20routine%20clinical%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Algorithm%253A%2520A%2520Field%2520Guide%2520to%2520Deploying%2520AI%2520Agents%2520in%2520Clinical%250A%2520%2520Practice%26entry.906535625%3DJack%2520Gallifant%2520and%2520Katherine%2520C.%2520Kellogg%2520and%2520Matt%2520Butler%2520and%2520Amanda%2520Centi%2520and%2520Shan%2520Chen%2520and%2520Patrick%2520F.%2520Doyle%2520and%2520Sayon%2520Dutta%2520and%2520Joyce%2520Guo%2520and%2520Matthew%2520J.%2520Hadfield%2520and%2520Esther%2520H.%2520Kim%2520and%2520David%2520E.%2520Kozono%2520and%2520Hugo%2520JWL%2520Aerts%2520and%2520Adam%2520B.%2520Landman%2520and%2520Raymond%2520H.%2520Mak%2520and%2520Rebecca%2520G.%2520Mishuris%2520and%2520Tanna%2520L.%2520Nelson%2520and%2520Guergana%2520K.%2520Savova%2520and%2520Elad%2520Sharon%2520and%2520Benjamin%2520C.%2520Silverman%2520and%2520Umit%2520Topaloglu%2520and%2520Jeremy%2520L.%2520Warner%2520and%2520Danielle%2520S.%2520Bitterman%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520integrated%2520into%2520agent-driven%2520workflows%2520hold%250Aimmense%2520promise%2520for%2520healthcare%252C%2520yet%2520a%2520significant%2520gap%2520exists%2520between%2520their%250Apotential%2520and%2520practical%2520implementation%2520within%2520clinical%2520settings.%2520To%2520address%250Athis%252C%2520we%2520present%2520a%2520practitioner-oriented%2520field%2520manual%2520for%2520deploying%2520generative%250Aagents%2520that%2520use%2520electronic%2520health%2520record%2520%2528EHR%2529%2520data.%2520This%2520guide%2520is%2520informed%2520by%250Aour%2520experience%2520deploying%2520the%2520%2522irAE-Agent%2522%252C%2520an%2520automated%2520system%2520to%2520detect%250Aimmune-related%2520adverse%2520events%2520from%2520clinical%2520notes%2520at%2520Mass%2520General%2520Brigham%252C%2520and%250Aby%2520structured%2520interviews%2520with%252020%2520clinicians%252C%2520engineers%252C%2520and%2520informatics%2520leaders%250Ainvolved%2520in%2520the%2520project.%2520Our%2520analysis%2520reveals%2520a%2520critical%2520misalignment%2520in%250Aclinical%2520AI%2520development%253A%2520less%2520than%252020%2525%2520of%2520our%2520effort%2520was%2520dedicated%2520to%2520prompt%250Aengineering%2520and%2520model%2520development%252C%2520while%2520over%252080%2525%2520was%2520consumed%2520by%2520the%250Asociotechnical%2520work%2520of%2520implementation.%2520We%2520distill%2520this%2520effort%2520into%2520five%2520%2522heavy%250Alifts%2522%253A%2520data%2520integration%252C%2520model%2520validation%252C%2520ensuring%2520economic%2520value%252C%2520managing%250Asystem%2520drift%252C%2520and%2520governance.%2520By%2520providing%2520actionable%2520solutions%2520for%2520each%2520of%250Athese%2520challenges%252C%2520this%2520field%2520manual%2520shifts%2520the%2520focus%2520from%2520algorithmic%250Adevelopment%2520to%2520the%2520essential%2520infrastructure%2520and%2520implementation%2520work%2520required%2520to%250Abridge%2520the%2520%2522valley%2520of%2520death%2522%2520and%2520successfully%2520translate%2520generative%2520AI%2520from%250Apilot%2520projects%2520into%2520routine%2520clinical%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Algorithm%3A%20A%20Field%20Guide%20to%20Deploying%20AI%20Agents%20in%20Clinical%0A%20%20Practice&entry.906535625=Jack%20Gallifant%20and%20Katherine%20C.%20Kellogg%20and%20Matt%20Butler%20and%20Amanda%20Centi%20and%20Shan%20Chen%20and%20Patrick%20F.%20Doyle%20and%20Sayon%20Dutta%20and%20Joyce%20Guo%20and%20Matthew%20J.%20Hadfield%20and%20Esther%20H.%20Kim%20and%20David%20E.%20Kozono%20and%20Hugo%20JWL%20Aerts%20and%20Adam%20B.%20Landman%20and%20Raymond%20H.%20Mak%20and%20Rebecca%20G.%20Mishuris%20and%20Tanna%20L.%20Nelson%20and%20Guergana%20K.%20Savova%20and%20Elad%20Sharon%20and%20Benjamin%20C.%20Silverman%20and%20Umit%20Topaloglu%20and%20Jeremy%20L.%20Warner%20and%20Danielle%20S.%20Bitterman&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20integrated%20into%20agent-driven%20workflows%20hold%0Aimmense%20promise%20for%20healthcare%2C%20yet%20a%20significant%20gap%20exists%20between%20their%0Apotential%20and%20practical%20implementation%20within%20clinical%20settings.%20To%20address%0Athis%2C%20we%20present%20a%20practitioner-oriented%20field%20manual%20for%20deploying%20generative%0Aagents%20that%20use%20electronic%20health%20record%20%28EHR%29%20data.%20This%20guide%20is%20informed%20by%0Aour%20experience%20deploying%20the%20%22irAE-Agent%22%2C%20an%20automated%20system%20to%20detect%0Aimmune-related%20adverse%20events%20from%20clinical%20notes%20at%20Mass%20General%20Brigham%2C%20and%0Aby%20structured%20interviews%20with%2020%20clinicians%2C%20engineers%2C%20and%20informatics%20leaders%0Ainvolved%20in%20the%20project.%20Our%20analysis%20reveals%20a%20critical%20misalignment%20in%0Aclinical%20AI%20development%3A%20less%20than%2020%25%20of%20our%20effort%20was%20dedicated%20to%20prompt%0Aengineering%20and%20model%20development%2C%20while%20over%2080%25%20was%20consumed%20by%20the%0Asociotechnical%20work%20of%20implementation.%20We%20distill%20this%20effort%20into%20five%20%22heavy%0Alifts%22%3A%20data%20integration%2C%20model%20validation%2C%20ensuring%20economic%20value%2C%20managing%0Asystem%20drift%2C%20and%20governance.%20By%20providing%20actionable%20solutions%20for%20each%20of%0Athese%20challenges%2C%20this%20field%20manual%20shifts%20the%20focus%20from%20algorithmic%0Adevelopment%20to%20the%20essential%20infrastructure%20and%20implementation%20work%20required%20to%0Abridge%20the%20%22valley%20of%20death%22%20and%20successfully%20translate%20generative%20AI%20from%0Apilot%20projects%20into%20routine%20clinical%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26153v2&entry.124074799=Read"},
{"title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting", "author": "Yuqiao Meng and Luoxi Tang and Feiyang Yu and Xi Li and Guanhua Yan and Ping Yang and Zhaohan Xi", "abstract": "  As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting.\n", "link": "http://arxiv.org/abs/2509.23571v2", "date": "2025-10-01", "relevancy": 1.8813, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4706}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20LLM-Assisted%20Blue%20Teaming%20via%20Standardized%20Threat%20Hunting&body=Title%3A%20Benchmarking%20LLM-Assisted%20Blue%20Teaming%20via%20Standardized%20Threat%20Hunting%0AAuthor%3A%20Yuqiao%20Meng%20and%20Luoxi%20Tang%20and%20Feiyang%20Yu%20and%20Xi%20Li%20and%20Guanhua%20Yan%20and%20Ping%20Yang%20and%20Zhaohan%20Xi%0AAbstract%3A%20%20%20As%20cyber%20threats%20continue%20to%20grow%20in%20scale%20and%20sophistication%2C%20blue%20team%0Adefenders%20increasingly%20require%20advanced%20tools%20to%20proactively%20detect%20and%0Amitigate%20risks.%20Large%20Language%20Models%20%28LLMs%29%20offer%20promising%20capabilities%20for%0Aenhancing%20threat%20analysis.%20However%2C%20their%20effectiveness%20in%20real-world%20blue%20team%0Athreat-hunting%20scenarios%20remains%20insufficiently%20explored.%20This%20paper%20presents%0ACyberTeam%2C%20a%20benchmark%20designed%20to%20guide%20LLMs%20in%20blue%20teaming%20practice.%0ACyberTeam%20constructs%20a%20standardized%20workflow%20in%20two%20stages.%20First%2C%20it%20models%0Arealistic%20threat-hunting%20workflows%20by%20capturing%20the%20dependencies%20among%0Aanalytical%20tasks%20from%20threat%20attribution%20to%20incident%20response.%20Next%2C%20each%20task%0Ais%20addressed%20through%20a%20set%20of%20operational%20modules%20tailored%20to%20its%20specific%0Aanalytical%20requirements.%20This%20transforms%20threat%20hunting%20into%20a%20structured%0Asequence%20of%20reasoning%20steps%2C%20with%20each%20step%20grounded%20in%20a%20discrete%20operation%0Aand%20ordered%20according%20to%20task-specific%20dependencies.%20Guided%20by%20this%20framework%2C%0ALLMs%20are%20directed%20to%20perform%20threat-hunting%20tasks%20through%20modularized%20steps.%0AOverall%2C%20CyberTeam%20integrates%2030%20tasks%20and%209%20operational%20modules%20to%20guide%20LLMs%0Athrough%20standardized%20threat%20analysis.%20We%20evaluate%20both%20leading%20LLMs%20and%0Astate-of-the-art%20cybersecurity%20agents%2C%20comparing%20CyberTeam%20against%20open-ended%0Areasoning%20strategies.%20Our%20results%20highlight%20the%20improvements%20enabled%20by%0Astandardized%20design%2C%20while%20also%20revealing%20the%20limitations%20of%20open-ended%0Areasoning%20in%20real-world%20threat%20hunting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520LLM-Assisted%2520Blue%2520Teaming%2520via%2520Standardized%2520Threat%2520Hunting%26entry.906535625%3DYuqiao%2520Meng%2520and%2520Luoxi%2520Tang%2520and%2520Feiyang%2520Yu%2520and%2520Xi%2520Li%2520and%2520Guanhua%2520Yan%2520and%2520Ping%2520Yang%2520and%2520Zhaohan%2520Xi%26entry.1292438233%3D%2520%2520As%2520cyber%2520threats%2520continue%2520to%2520grow%2520in%2520scale%2520and%2520sophistication%252C%2520blue%2520team%250Adefenders%2520increasingly%2520require%2520advanced%2520tools%2520to%2520proactively%2520detect%2520and%250Amitigate%2520risks.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520promising%2520capabilities%2520for%250Aenhancing%2520threat%2520analysis.%2520However%252C%2520their%2520effectiveness%2520in%2520real-world%2520blue%2520team%250Athreat-hunting%2520scenarios%2520remains%2520insufficiently%2520explored.%2520This%2520paper%2520presents%250ACyberTeam%252C%2520a%2520benchmark%2520designed%2520to%2520guide%2520LLMs%2520in%2520blue%2520teaming%2520practice.%250ACyberTeam%2520constructs%2520a%2520standardized%2520workflow%2520in%2520two%2520stages.%2520First%252C%2520it%2520models%250Arealistic%2520threat-hunting%2520workflows%2520by%2520capturing%2520the%2520dependencies%2520among%250Aanalytical%2520tasks%2520from%2520threat%2520attribution%2520to%2520incident%2520response.%2520Next%252C%2520each%2520task%250Ais%2520addressed%2520through%2520a%2520set%2520of%2520operational%2520modules%2520tailored%2520to%2520its%2520specific%250Aanalytical%2520requirements.%2520This%2520transforms%2520threat%2520hunting%2520into%2520a%2520structured%250Asequence%2520of%2520reasoning%2520steps%252C%2520with%2520each%2520step%2520grounded%2520in%2520a%2520discrete%2520operation%250Aand%2520ordered%2520according%2520to%2520task-specific%2520dependencies.%2520Guided%2520by%2520this%2520framework%252C%250ALLMs%2520are%2520directed%2520to%2520perform%2520threat-hunting%2520tasks%2520through%2520modularized%2520steps.%250AOverall%252C%2520CyberTeam%2520integrates%252030%2520tasks%2520and%25209%2520operational%2520modules%2520to%2520guide%2520LLMs%250Athrough%2520standardized%2520threat%2520analysis.%2520We%2520evaluate%2520both%2520leading%2520LLMs%2520and%250Astate-of-the-art%2520cybersecurity%2520agents%252C%2520comparing%2520CyberTeam%2520against%2520open-ended%250Areasoning%2520strategies.%2520Our%2520results%2520highlight%2520the%2520improvements%2520enabled%2520by%250Astandardized%2520design%252C%2520while%2520also%2520revealing%2520the%2520limitations%2520of%2520open-ended%250Areasoning%2520in%2520real-world%2520threat%2520hunting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20LLM-Assisted%20Blue%20Teaming%20via%20Standardized%20Threat%20Hunting&entry.906535625=Yuqiao%20Meng%20and%20Luoxi%20Tang%20and%20Feiyang%20Yu%20and%20Xi%20Li%20and%20Guanhua%20Yan%20and%20Ping%20Yang%20and%20Zhaohan%20Xi&entry.1292438233=%20%20As%20cyber%20threats%20continue%20to%20grow%20in%20scale%20and%20sophistication%2C%20blue%20team%0Adefenders%20increasingly%20require%20advanced%20tools%20to%20proactively%20detect%20and%0Amitigate%20risks.%20Large%20Language%20Models%20%28LLMs%29%20offer%20promising%20capabilities%20for%0Aenhancing%20threat%20analysis.%20However%2C%20their%20effectiveness%20in%20real-world%20blue%20team%0Athreat-hunting%20scenarios%20remains%20insufficiently%20explored.%20This%20paper%20presents%0ACyberTeam%2C%20a%20benchmark%20designed%20to%20guide%20LLMs%20in%20blue%20teaming%20practice.%0ACyberTeam%20constructs%20a%20standardized%20workflow%20in%20two%20stages.%20First%2C%20it%20models%0Arealistic%20threat-hunting%20workflows%20by%20capturing%20the%20dependencies%20among%0Aanalytical%20tasks%20from%20threat%20attribution%20to%20incident%20response.%20Next%2C%20each%20task%0Ais%20addressed%20through%20a%20set%20of%20operational%20modules%20tailored%20to%20its%20specific%0Aanalytical%20requirements.%20This%20transforms%20threat%20hunting%20into%20a%20structured%0Asequence%20of%20reasoning%20steps%2C%20with%20each%20step%20grounded%20in%20a%20discrete%20operation%0Aand%20ordered%20according%20to%20task-specific%20dependencies.%20Guided%20by%20this%20framework%2C%0ALLMs%20are%20directed%20to%20perform%20threat-hunting%20tasks%20through%20modularized%20steps.%0AOverall%2C%20CyberTeam%20integrates%2030%20tasks%20and%209%20operational%20modules%20to%20guide%20LLMs%0Athrough%20standardized%20threat%20analysis.%20We%20evaluate%20both%20leading%20LLMs%20and%0Astate-of-the-art%20cybersecurity%20agents%2C%20comparing%20CyberTeam%20against%20open-ended%0Areasoning%20strategies.%20Our%20results%20highlight%20the%20improvements%20enabled%20by%0Astandardized%20design%2C%20while%20also%20revealing%20the%20limitations%20of%20open-ended%0Areasoning%20in%20real-world%20threat%20hunting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23571v2&entry.124074799=Read"},
{"title": "Prompt Tuning Decision Transformers with Structured and Scalable Bandits", "author": "Finn Rietz and Oleg Smirnov and Sara Karimi and Lele Cao", "abstract": "  Prompt tuning has emerged as a key technique for adapting large pre-trained\nDecision Transformers (DTs) in offline Reinforcement Learning (RL),\nparticularly in multi-task and few-shot settings. The Prompting Decision\nTransformer (PDT) enables task generalization via trajectory prompts sampled\nuniformly from expert demonstrations -- without accounting for prompt\ninformativeness. In this work, we propose a bandit-based prompt-tuning method\nthat learns to construct optimal trajectory prompts from demonstration data at\ninference time. We devise a structured bandit architecture operating in the\ntrajectory prompt space, achieving linear rather than combinatorial scaling\nwith prompt size. Additionally, we show that the pre-trained PDT itself can\nserve as a powerful feature extractor for the bandit, enabling efficient reward\nmodeling across various environments. We theoretically establish regret bounds\nand demonstrate empirically that our method consistently enhances performance\nacross a wide range of tasks, high-dimensional environments, and\nout-of-distribution scenarios, outperforming existing baselines in prompt\ntuning.\n", "link": "http://arxiv.org/abs/2502.04979v3", "date": "2025-10-01", "relevancy": 1.8653, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4722}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4637}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Tuning%20Decision%20Transformers%20with%20Structured%20and%20Scalable%20Bandits&body=Title%3A%20Prompt%20Tuning%20Decision%20Transformers%20with%20Structured%20and%20Scalable%20Bandits%0AAuthor%3A%20Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao%0AAbstract%3A%20%20%20Prompt%20tuning%20has%20emerged%20as%20a%20key%20technique%20for%20adapting%20large%20pre-trained%0ADecision%20Transformers%20%28DTs%29%20in%20offline%20Reinforcement%20Learning%20%28RL%29%2C%0Aparticularly%20in%20multi-task%20and%20few-shot%20settings.%20The%20Prompting%20Decision%0ATransformer%20%28PDT%29%20enables%20task%20generalization%20via%20trajectory%20prompts%20sampled%0Auniformly%20from%20expert%20demonstrations%20--%20without%20accounting%20for%20prompt%0Ainformativeness.%20In%20this%20work%2C%20we%20propose%20a%20bandit-based%20prompt-tuning%20method%0Athat%20learns%20to%20construct%20optimal%20trajectory%20prompts%20from%20demonstration%20data%20at%0Ainference%20time.%20We%20devise%20a%20structured%20bandit%20architecture%20operating%20in%20the%0Atrajectory%20prompt%20space%2C%20achieving%20linear%20rather%20than%20combinatorial%20scaling%0Awith%20prompt%20size.%20Additionally%2C%20we%20show%20that%20the%20pre-trained%20PDT%20itself%20can%0Aserve%20as%20a%20powerful%20feature%20extractor%20for%20the%20bandit%2C%20enabling%20efficient%20reward%0Amodeling%20across%20various%20environments.%20We%20theoretically%20establish%20regret%20bounds%0Aand%20demonstrate%20empirically%20that%20our%20method%20consistently%20enhances%20performance%0Aacross%20a%20wide%20range%20of%20tasks%2C%20high-dimensional%20environments%2C%20and%0Aout-of-distribution%20scenarios%2C%20outperforming%20existing%20baselines%20in%20prompt%0Atuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Tuning%2520Decision%2520Transformers%2520with%2520Structured%2520and%2520Scalable%2520Bandits%26entry.906535625%3DFinn%2520Rietz%2520and%2520Oleg%2520Smirnov%2520and%2520Sara%2520Karimi%2520and%2520Lele%2520Cao%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520has%2520emerged%2520as%2520a%2520key%2520technique%2520for%2520adapting%2520large%2520pre-trained%250ADecision%2520Transformers%2520%2528DTs%2529%2520in%2520offline%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%250Aparticularly%2520in%2520multi-task%2520and%2520few-shot%2520settings.%2520The%2520Prompting%2520Decision%250ATransformer%2520%2528PDT%2529%2520enables%2520task%2520generalization%2520via%2520trajectory%2520prompts%2520sampled%250Auniformly%2520from%2520expert%2520demonstrations%2520--%2520without%2520accounting%2520for%2520prompt%250Ainformativeness.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520bandit-based%2520prompt-tuning%2520method%250Athat%2520learns%2520to%2520construct%2520optimal%2520trajectory%2520prompts%2520from%2520demonstration%2520data%2520at%250Ainference%2520time.%2520We%2520devise%2520a%2520structured%2520bandit%2520architecture%2520operating%2520in%2520the%250Atrajectory%2520prompt%2520space%252C%2520achieving%2520linear%2520rather%2520than%2520combinatorial%2520scaling%250Awith%2520prompt%2520size.%2520Additionally%252C%2520we%2520show%2520that%2520the%2520pre-trained%2520PDT%2520itself%2520can%250Aserve%2520as%2520a%2520powerful%2520feature%2520extractor%2520for%2520the%2520bandit%252C%2520enabling%2520efficient%2520reward%250Amodeling%2520across%2520various%2520environments.%2520We%2520theoretically%2520establish%2520regret%2520bounds%250Aand%2520demonstrate%2520empirically%2520that%2520our%2520method%2520consistently%2520enhances%2520performance%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%252C%2520high-dimensional%2520environments%252C%2520and%250Aout-of-distribution%2520scenarios%252C%2520outperforming%2520existing%2520baselines%2520in%2520prompt%250Atuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Tuning%20Decision%20Transformers%20with%20Structured%20and%20Scalable%20Bandits&entry.906535625=Finn%20Rietz%20and%20Oleg%20Smirnov%20and%20Sara%20Karimi%20and%20Lele%20Cao&entry.1292438233=%20%20Prompt%20tuning%20has%20emerged%20as%20a%20key%20technique%20for%20adapting%20large%20pre-trained%0ADecision%20Transformers%20%28DTs%29%20in%20offline%20Reinforcement%20Learning%20%28RL%29%2C%0Aparticularly%20in%20multi-task%20and%20few-shot%20settings.%20The%20Prompting%20Decision%0ATransformer%20%28PDT%29%20enables%20task%20generalization%20via%20trajectory%20prompts%20sampled%0Auniformly%20from%20expert%20demonstrations%20--%20without%20accounting%20for%20prompt%0Ainformativeness.%20In%20this%20work%2C%20we%20propose%20a%20bandit-based%20prompt-tuning%20method%0Athat%20learns%20to%20construct%20optimal%20trajectory%20prompts%20from%20demonstration%20data%20at%0Ainference%20time.%20We%20devise%20a%20structured%20bandit%20architecture%20operating%20in%20the%0Atrajectory%20prompt%20space%2C%20achieving%20linear%20rather%20than%20combinatorial%20scaling%0Awith%20prompt%20size.%20Additionally%2C%20we%20show%20that%20the%20pre-trained%20PDT%20itself%20can%0Aserve%20as%20a%20powerful%20feature%20extractor%20for%20the%20bandit%2C%20enabling%20efficient%20reward%0Amodeling%20across%20various%20environments.%20We%20theoretically%20establish%20regret%20bounds%0Aand%20demonstrate%20empirically%20that%20our%20method%20consistently%20enhances%20performance%0Aacross%20a%20wide%20range%20of%20tasks%2C%20high-dimensional%20environments%2C%20and%0Aout-of-distribution%20scenarios%2C%20outperforming%20existing%20baselines%20in%20prompt%0Atuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04979v3&entry.124074799=Read"},
{"title": "Opt2Skill: Imitating Dynamically-feasible Whole-Body Trajectories for\n  Versatile Humanoid Loco-Manipulation", "author": "Fukang Liu and Zhaoyuan Gu and Yilin Cai and Ziyi Zhou and Hyunyoung Jung and Jaehwi Jang and Shijie Zhao and Sehoon Ha and Yue Chen and Danfei Xu and Ye Zhao", "abstract": "  Humanoid robots are designed to perform diverse loco-manipulation tasks.\nHowever, they face challenges due to their high-dimensional and unstable\ndynamics, as well as the complex contact-rich nature of the tasks. Model-based\noptimal control methods offer flexibility to define precise motion but are\nlimited by high computational complexity and accurate contact sensing. On the\nother hand, reinforcement learning (RL) handles high-dimensional spaces with\nstrong robustness but suffers from inefficient learning, unnatural motion, and\nsim-to-real gaps. To address these challenges, we introduce Opt2Skill, an\nend-to-end pipeline that combines model-based trajectory optimization with RL\nto achieve robust whole-body loco-manipulation. Opt2Skill generates dynamic\nfeasible and contact-consistent reference motions for the Digit humanoid robot\nusing differential dynamic programming (DDP) and trains RL policies to track\nthese optimal trajectories. Our results demonstrate that Opt2Skill outperforms\nbaselines that rely on human demonstrations and inverse kinematics-based\nreferences, both in motion tracking and task success rates. Furthermore, we\nshow that incorporating trajectories with torque information improves contact\nforce tracking in contact-involved tasks, such as wiping a table. We have\nsuccessfully transferred our approach to real-world applications.\n", "link": "http://arxiv.org/abs/2409.20514v6", "date": "2025-10-01", "relevancy": 1.86, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.635}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6067}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opt2Skill%3A%20Imitating%20Dynamically-feasible%20Whole-Body%20Trajectories%20for%0A%20%20Versatile%20Humanoid%20Loco-Manipulation&body=Title%3A%20Opt2Skill%3A%20Imitating%20Dynamically-feasible%20Whole-Body%20Trajectories%20for%0A%20%20Versatile%20Humanoid%20Loco-Manipulation%0AAuthor%3A%20Fukang%20Liu%20and%20Zhaoyuan%20Gu%20and%20Yilin%20Cai%20and%20Ziyi%20Zhou%20and%20Hyunyoung%20Jung%20and%20Jaehwi%20Jang%20and%20Shijie%20Zhao%20and%20Sehoon%20Ha%20and%20Yue%20Chen%20and%20Danfei%20Xu%20and%20Ye%20Zhao%0AAbstract%3A%20%20%20Humanoid%20robots%20are%20designed%20to%20perform%20diverse%20loco-manipulation%20tasks.%0AHowever%2C%20they%20face%20challenges%20due%20to%20their%20high-dimensional%20and%20unstable%0Adynamics%2C%20as%20well%20as%20the%20complex%20contact-rich%20nature%20of%20the%20tasks.%20Model-based%0Aoptimal%20control%20methods%20offer%20flexibility%20to%20define%20precise%20motion%20but%20are%0Alimited%20by%20high%20computational%20complexity%20and%20accurate%20contact%20sensing.%20On%20the%0Aother%20hand%2C%20reinforcement%20learning%20%28RL%29%20handles%20high-dimensional%20spaces%20with%0Astrong%20robustness%20but%20suffers%20from%20inefficient%20learning%2C%20unnatural%20motion%2C%20and%0Asim-to-real%20gaps.%20To%20address%20these%20challenges%2C%20we%20introduce%20Opt2Skill%2C%20an%0Aend-to-end%20pipeline%20that%20combines%20model-based%20trajectory%20optimization%20with%20RL%0Ato%20achieve%20robust%20whole-body%20loco-manipulation.%20Opt2Skill%20generates%20dynamic%0Afeasible%20and%20contact-consistent%20reference%20motions%20for%20the%20Digit%20humanoid%20robot%0Ausing%20differential%20dynamic%20programming%20%28DDP%29%20and%20trains%20RL%20policies%20to%20track%0Athese%20optimal%20trajectories.%20Our%20results%20demonstrate%20that%20Opt2Skill%20outperforms%0Abaselines%20that%20rely%20on%20human%20demonstrations%20and%20inverse%20kinematics-based%0Areferences%2C%20both%20in%20motion%20tracking%20and%20task%20success%20rates.%20Furthermore%2C%20we%0Ashow%20that%20incorporating%20trajectories%20with%20torque%20information%20improves%20contact%0Aforce%20tracking%20in%20contact-involved%20tasks%2C%20such%20as%20wiping%20a%20table.%20We%20have%0Asuccessfully%20transferred%20our%20approach%20to%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20514v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpt2Skill%253A%2520Imitating%2520Dynamically-feasible%2520Whole-Body%2520Trajectories%2520for%250A%2520%2520Versatile%2520Humanoid%2520Loco-Manipulation%26entry.906535625%3DFukang%2520Liu%2520and%2520Zhaoyuan%2520Gu%2520and%2520Yilin%2520Cai%2520and%2520Ziyi%2520Zhou%2520and%2520Hyunyoung%2520Jung%2520and%2520Jaehwi%2520Jang%2520and%2520Shijie%2520Zhao%2520and%2520Sehoon%2520Ha%2520and%2520Yue%2520Chen%2520and%2520Danfei%2520Xu%2520and%2520Ye%2520Zhao%26entry.1292438233%3D%2520%2520Humanoid%2520robots%2520are%2520designed%2520to%2520perform%2520diverse%2520loco-manipulation%2520tasks.%250AHowever%252C%2520they%2520face%2520challenges%2520due%2520to%2520their%2520high-dimensional%2520and%2520unstable%250Adynamics%252C%2520as%2520well%2520as%2520the%2520complex%2520contact-rich%2520nature%2520of%2520the%2520tasks.%2520Model-based%250Aoptimal%2520control%2520methods%2520offer%2520flexibility%2520to%2520define%2520precise%2520motion%2520but%2520are%250Alimited%2520by%2520high%2520computational%2520complexity%2520and%2520accurate%2520contact%2520sensing.%2520On%2520the%250Aother%2520hand%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520handles%2520high-dimensional%2520spaces%2520with%250Astrong%2520robustness%2520but%2520suffers%2520from%2520inefficient%2520learning%252C%2520unnatural%2520motion%252C%2520and%250Asim-to-real%2520gaps.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Opt2Skill%252C%2520an%250Aend-to-end%2520pipeline%2520that%2520combines%2520model-based%2520trajectory%2520optimization%2520with%2520RL%250Ato%2520achieve%2520robust%2520whole-body%2520loco-manipulation.%2520Opt2Skill%2520generates%2520dynamic%250Afeasible%2520and%2520contact-consistent%2520reference%2520motions%2520for%2520the%2520Digit%2520humanoid%2520robot%250Ausing%2520differential%2520dynamic%2520programming%2520%2528DDP%2529%2520and%2520trains%2520RL%2520policies%2520to%2520track%250Athese%2520optimal%2520trajectories.%2520Our%2520results%2520demonstrate%2520that%2520Opt2Skill%2520outperforms%250Abaselines%2520that%2520rely%2520on%2520human%2520demonstrations%2520and%2520inverse%2520kinematics-based%250Areferences%252C%2520both%2520in%2520motion%2520tracking%2520and%2520task%2520success%2520rates.%2520Furthermore%252C%2520we%250Ashow%2520that%2520incorporating%2520trajectories%2520with%2520torque%2520information%2520improves%2520contact%250Aforce%2520tracking%2520in%2520contact-involved%2520tasks%252C%2520such%2520as%2520wiping%2520a%2520table.%2520We%2520have%250Asuccessfully%2520transferred%2520our%2520approach%2520to%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20514v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opt2Skill%3A%20Imitating%20Dynamically-feasible%20Whole-Body%20Trajectories%20for%0A%20%20Versatile%20Humanoid%20Loco-Manipulation&entry.906535625=Fukang%20Liu%20and%20Zhaoyuan%20Gu%20and%20Yilin%20Cai%20and%20Ziyi%20Zhou%20and%20Hyunyoung%20Jung%20and%20Jaehwi%20Jang%20and%20Shijie%20Zhao%20and%20Sehoon%20Ha%20and%20Yue%20Chen%20and%20Danfei%20Xu%20and%20Ye%20Zhao&entry.1292438233=%20%20Humanoid%20robots%20are%20designed%20to%20perform%20diverse%20loco-manipulation%20tasks.%0AHowever%2C%20they%20face%20challenges%20due%20to%20their%20high-dimensional%20and%20unstable%0Adynamics%2C%20as%20well%20as%20the%20complex%20contact-rich%20nature%20of%20the%20tasks.%20Model-based%0Aoptimal%20control%20methods%20offer%20flexibility%20to%20define%20precise%20motion%20but%20are%0Alimited%20by%20high%20computational%20complexity%20and%20accurate%20contact%20sensing.%20On%20the%0Aother%20hand%2C%20reinforcement%20learning%20%28RL%29%20handles%20high-dimensional%20spaces%20with%0Astrong%20robustness%20but%20suffers%20from%20inefficient%20learning%2C%20unnatural%20motion%2C%20and%0Asim-to-real%20gaps.%20To%20address%20these%20challenges%2C%20we%20introduce%20Opt2Skill%2C%20an%0Aend-to-end%20pipeline%20that%20combines%20model-based%20trajectory%20optimization%20with%20RL%0Ato%20achieve%20robust%20whole-body%20loco-manipulation.%20Opt2Skill%20generates%20dynamic%0Afeasible%20and%20contact-consistent%20reference%20motions%20for%20the%20Digit%20humanoid%20robot%0Ausing%20differential%20dynamic%20programming%20%28DDP%29%20and%20trains%20RL%20policies%20to%20track%0Athese%20optimal%20trajectories.%20Our%20results%20demonstrate%20that%20Opt2Skill%20outperforms%0Abaselines%20that%20rely%20on%20human%20demonstrations%20and%20inverse%20kinematics-based%0Areferences%2C%20both%20in%20motion%20tracking%20and%20task%20success%20rates.%20Furthermore%2C%20we%0Ashow%20that%20incorporating%20trajectories%20with%20torque%20information%20improves%20contact%0Aforce%20tracking%20in%20contact-involved%20tasks%2C%20such%20as%20wiping%20a%20table.%20We%20have%0Asuccessfully%20transferred%20our%20approach%20to%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20514v6&entry.124074799=Read"},
{"title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm", "author": "Jiale Chen and Yalda Shabanzadeh and Elvir Crn\u010devi\u0107 and Torsten Hoefler and Dan Alistarh", "abstract": "  Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. While GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: first, the GPTQ error\npropagation step gains an intuitive geometric interpretation; second, GPTQ\ninherits the error upper bound of Babai's algorithm under the assumption that\nno weights are clipped. Leveraging this bound, we design post-training\nquantization methods that avoid clipping, and outperform the original GPTQ. In\naddition, we provide efficient GPU inference kernels for the resulting\nrepresentation. Taken together, these results place GPTQ on a firm theoretical\nfooting and open the door to importing decades of progress in lattice\nalgorithms towards the design of future quantization algorithms for\nbillion-parameter models.\n", "link": "http://arxiv.org/abs/2507.18553v2", "date": "2025-10-01", "relevancy": 1.8448, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4757}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4632}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Geometry%20of%20LLM%20Quantization%3A%20GPTQ%20as%20Babai%27s%20Nearest%20Plane%0A%20%20Algorithm&body=Title%3A%20The%20Geometry%20of%20LLM%20Quantization%3A%20GPTQ%20as%20Babai%27s%20Nearest%20Plane%0A%20%20Algorithm%0AAuthor%3A%20Jiale%20Chen%20and%20Yalda%20Shabanzadeh%20and%20Elvir%20Crn%C4%8Devi%C4%87%20and%20Torsten%20Hoefler%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20Quantizing%20the%20weights%20of%20large%20language%20models%20%28LLMs%29%20from%2016-bit%20to%20lower%0Abitwidth%20is%20the%20de%20facto%20approach%20to%20deploy%20massive%20transformers%20onto%20more%0Aaffordable%20accelerators.%20While%20GPTQ%20emerged%20as%20one%20of%20the%20standard%20methods%20for%0Aone-shot%20post-training%20quantization%20at%20LLM%20scale%2C%20its%20inner%20workings%20are%0Adescribed%20as%20a%20sequence%20of%20ad-hoc%20algebraic%20updates%20that%20obscure%20geometric%0Ameaning%20or%20worst-case%20guarantees.%20In%20this%20work%2C%20we%20show%20that%2C%20when%20executed%0Aback-to-front%20%28from%20the%20last%20to%20first%20dimension%29%20for%20a%20linear%20layer%2C%20GPTQ%20is%0Amathematically%20identical%20to%20Babai%27s%20nearest%20plane%20algorithm%20for%20the%20classical%0Aclosest%20vector%20problem%20%28CVP%29%20on%20a%20lattice%20defined%20by%20the%20Hessian%20matrix%20of%20the%0Alayer%27s%20inputs.%20This%20equivalence%20is%20based%20on%20a%20sophisticated%20mathematical%0Aargument%2C%20and%20has%20two%20analytical%20consequences%3A%20first%2C%20the%20GPTQ%20error%0Apropagation%20step%20gains%20an%20intuitive%20geometric%20interpretation%3B%20second%2C%20GPTQ%0Ainherits%20the%20error%20upper%20bound%20of%20Babai%27s%20algorithm%20under%20the%20assumption%20that%0Ano%20weights%20are%20clipped.%20Leveraging%20this%20bound%2C%20we%20design%20post-training%0Aquantization%20methods%20that%20avoid%20clipping%2C%20and%20outperform%20the%20original%20GPTQ.%20In%0Aaddition%2C%20we%20provide%20efficient%20GPU%20inference%20kernels%20for%20the%20resulting%0Arepresentation.%20Taken%20together%2C%20these%20results%20place%20GPTQ%20on%20a%20firm%20theoretical%0Afooting%20and%20open%20the%20door%20to%20importing%20decades%20of%20progress%20in%20lattice%0Aalgorithms%20towards%20the%20design%20of%20future%20quantization%20algorithms%20for%0Abillion-parameter%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Geometry%2520of%2520LLM%2520Quantization%253A%2520GPTQ%2520as%2520Babai%2527s%2520Nearest%2520Plane%250A%2520%2520Algorithm%26entry.906535625%3DJiale%2520Chen%2520and%2520Yalda%2520Shabanzadeh%2520and%2520Elvir%2520Crn%25C4%258Devi%25C4%2587%2520and%2520Torsten%2520Hoefler%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520Quantizing%2520the%2520weights%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520from%252016-bit%2520to%2520lower%250Abitwidth%2520is%2520the%2520de%2520facto%2520approach%2520to%2520deploy%2520massive%2520transformers%2520onto%2520more%250Aaffordable%2520accelerators.%2520While%2520GPTQ%2520emerged%2520as%2520one%2520of%2520the%2520standard%2520methods%2520for%250Aone-shot%2520post-training%2520quantization%2520at%2520LLM%2520scale%252C%2520its%2520inner%2520workings%2520are%250Adescribed%2520as%2520a%2520sequence%2520of%2520ad-hoc%2520algebraic%2520updates%2520that%2520obscure%2520geometric%250Ameaning%2520or%2520worst-case%2520guarantees.%2520In%2520this%2520work%252C%2520we%2520show%2520that%252C%2520when%2520executed%250Aback-to-front%2520%2528from%2520the%2520last%2520to%2520first%2520dimension%2529%2520for%2520a%2520linear%2520layer%252C%2520GPTQ%2520is%250Amathematically%2520identical%2520to%2520Babai%2527s%2520nearest%2520plane%2520algorithm%2520for%2520the%2520classical%250Aclosest%2520vector%2520problem%2520%2528CVP%2529%2520on%2520a%2520lattice%2520defined%2520by%2520the%2520Hessian%2520matrix%2520of%2520the%250Alayer%2527s%2520inputs.%2520This%2520equivalence%2520is%2520based%2520on%2520a%2520sophisticated%2520mathematical%250Aargument%252C%2520and%2520has%2520two%2520analytical%2520consequences%253A%2520first%252C%2520the%2520GPTQ%2520error%250Apropagation%2520step%2520gains%2520an%2520intuitive%2520geometric%2520interpretation%253B%2520second%252C%2520GPTQ%250Ainherits%2520the%2520error%2520upper%2520bound%2520of%2520Babai%2527s%2520algorithm%2520under%2520the%2520assumption%2520that%250Ano%2520weights%2520are%2520clipped.%2520Leveraging%2520this%2520bound%252C%2520we%2520design%2520post-training%250Aquantization%2520methods%2520that%2520avoid%2520clipping%252C%2520and%2520outperform%2520the%2520original%2520GPTQ.%2520In%250Aaddition%252C%2520we%2520provide%2520efficient%2520GPU%2520inference%2520kernels%2520for%2520the%2520resulting%250Arepresentation.%2520Taken%2520together%252C%2520these%2520results%2520place%2520GPTQ%2520on%2520a%2520firm%2520theoretical%250Afooting%2520and%2520open%2520the%2520door%2520to%2520importing%2520decades%2520of%2520progress%2520in%2520lattice%250Aalgorithms%2520towards%2520the%2520design%2520of%2520future%2520quantization%2520algorithms%2520for%250Abillion-parameter%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Geometry%20of%20LLM%20Quantization%3A%20GPTQ%20as%20Babai%27s%20Nearest%20Plane%0A%20%20Algorithm&entry.906535625=Jiale%20Chen%20and%20Yalda%20Shabanzadeh%20and%20Elvir%20Crn%C4%8Devi%C4%87%20and%20Torsten%20Hoefler%20and%20Dan%20Alistarh&entry.1292438233=%20%20Quantizing%20the%20weights%20of%20large%20language%20models%20%28LLMs%29%20from%2016-bit%20to%20lower%0Abitwidth%20is%20the%20de%20facto%20approach%20to%20deploy%20massive%20transformers%20onto%20more%0Aaffordable%20accelerators.%20While%20GPTQ%20emerged%20as%20one%20of%20the%20standard%20methods%20for%0Aone-shot%20post-training%20quantization%20at%20LLM%20scale%2C%20its%20inner%20workings%20are%0Adescribed%20as%20a%20sequence%20of%20ad-hoc%20algebraic%20updates%20that%20obscure%20geometric%0Ameaning%20or%20worst-case%20guarantees.%20In%20this%20work%2C%20we%20show%20that%2C%20when%20executed%0Aback-to-front%20%28from%20the%20last%20to%20first%20dimension%29%20for%20a%20linear%20layer%2C%20GPTQ%20is%0Amathematically%20identical%20to%20Babai%27s%20nearest%20plane%20algorithm%20for%20the%20classical%0Aclosest%20vector%20problem%20%28CVP%29%20on%20a%20lattice%20defined%20by%20the%20Hessian%20matrix%20of%20the%0Alayer%27s%20inputs.%20This%20equivalence%20is%20based%20on%20a%20sophisticated%20mathematical%0Aargument%2C%20and%20has%20two%20analytical%20consequences%3A%20first%2C%20the%20GPTQ%20error%0Apropagation%20step%20gains%20an%20intuitive%20geometric%20interpretation%3B%20second%2C%20GPTQ%0Ainherits%20the%20error%20upper%20bound%20of%20Babai%27s%20algorithm%20under%20the%20assumption%20that%0Ano%20weights%20are%20clipped.%20Leveraging%20this%20bound%2C%20we%20design%20post-training%0Aquantization%20methods%20that%20avoid%20clipping%2C%20and%20outperform%20the%20original%20GPTQ.%20In%0Aaddition%2C%20we%20provide%20efficient%20GPU%20inference%20kernels%20for%20the%20resulting%0Arepresentation.%20Taken%20together%2C%20these%20results%20place%20GPTQ%20on%20a%20firm%20theoretical%0Afooting%20and%20open%20the%20door%20to%20importing%20decades%20of%20progress%20in%20lattice%0Aalgorithms%20towards%20the%20design%20of%20future%20quantization%20algorithms%20for%0Abillion-parameter%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18553v2&entry.124074799=Read"},
{"title": "Minimax and Bayes Optimal Best-Arm Identification", "author": "Masahiro Kato", "abstract": "  This study investigates minimax and Bayes optimal strategies in fixed-budget\nbest-arm identification. We consider an adaptive procedure consisting of a\nsampling phase followed by a recommendation phase, and we design an adaptive\nexperiment within this framework to efficiently identify the best arm, defined\nas the one with the highest expected outcome. In our proposed strategy, the\nsampling phase consists of two stages. The first stage is a pilot phase, in\nwhich we allocate each arm uniformly in equal proportions to eliminate clearly\nsuboptimal arms and estimate outcome variances. In the second stage, arms are\nallocated in proportion to the variances estimated during the first stage.\nAfter the sampling phase, the procedure enters the recommendation phase, where\nwe select the arm with the highest sample mean as our estimate of the best arm.\nWe prove that this single strategy is simultaneously asymptotically minimax and\nBayes optimal for the simple regret, with upper bounds that coincide exactly\nwith our lower bounds, including the constant terms.\n", "link": "http://arxiv.org/abs/2506.24007v3", "date": "2025-10-01", "relevancy": 1.8122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4748}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimax%20and%20Bayes%20Optimal%20Best-Arm%20Identification&body=Title%3A%20Minimax%20and%20Bayes%20Optimal%20Best-Arm%20Identification%0AAuthor%3A%20Masahiro%20Kato%0AAbstract%3A%20%20%20This%20study%20investigates%20minimax%20and%20Bayes%20optimal%20strategies%20in%20fixed-budget%0Abest-arm%20identification.%20We%20consider%20an%20adaptive%20procedure%20consisting%20of%20a%0Asampling%20phase%20followed%20by%20a%20recommendation%20phase%2C%20and%20we%20design%20an%20adaptive%0Aexperiment%20within%20this%20framework%20to%20efficiently%20identify%20the%20best%20arm%2C%20defined%0Aas%20the%20one%20with%20the%20highest%20expected%20outcome.%20In%20our%20proposed%20strategy%2C%20the%0Asampling%20phase%20consists%20of%20two%20stages.%20The%20first%20stage%20is%20a%20pilot%20phase%2C%20in%0Awhich%20we%20allocate%20each%20arm%20uniformly%20in%20equal%20proportions%20to%20eliminate%20clearly%0Asuboptimal%20arms%20and%20estimate%20outcome%20variances.%20In%20the%20second%20stage%2C%20arms%20are%0Aallocated%20in%20proportion%20to%20the%20variances%20estimated%20during%20the%20first%20stage.%0AAfter%20the%20sampling%20phase%2C%20the%20procedure%20enters%20the%20recommendation%20phase%2C%20where%0Awe%20select%20the%20arm%20with%20the%20highest%20sample%20mean%20as%20our%20estimate%20of%20the%20best%20arm.%0AWe%20prove%20that%20this%20single%20strategy%20is%20simultaneously%20asymptotically%20minimax%20and%0ABayes%20optimal%20for%20the%20simple%20regret%2C%20with%20upper%20bounds%20that%20coincide%20exactly%0Awith%20our%20lower%20bounds%2C%20including%20the%20constant%20terms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.24007v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimax%2520and%2520Bayes%2520Optimal%2520Best-Arm%2520Identification%26entry.906535625%3DMasahiro%2520Kato%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520minimax%2520and%2520Bayes%2520optimal%2520strategies%2520in%2520fixed-budget%250Abest-arm%2520identification.%2520We%2520consider%2520an%2520adaptive%2520procedure%2520consisting%2520of%2520a%250Asampling%2520phase%2520followed%2520by%2520a%2520recommendation%2520phase%252C%2520and%2520we%2520design%2520an%2520adaptive%250Aexperiment%2520within%2520this%2520framework%2520to%2520efficiently%2520identify%2520the%2520best%2520arm%252C%2520defined%250Aas%2520the%2520one%2520with%2520the%2520highest%2520expected%2520outcome.%2520In%2520our%2520proposed%2520strategy%252C%2520the%250Asampling%2520phase%2520consists%2520of%2520two%2520stages.%2520The%2520first%2520stage%2520is%2520a%2520pilot%2520phase%252C%2520in%250Awhich%2520we%2520allocate%2520each%2520arm%2520uniformly%2520in%2520equal%2520proportions%2520to%2520eliminate%2520clearly%250Asuboptimal%2520arms%2520and%2520estimate%2520outcome%2520variances.%2520In%2520the%2520second%2520stage%252C%2520arms%2520are%250Aallocated%2520in%2520proportion%2520to%2520the%2520variances%2520estimated%2520during%2520the%2520first%2520stage.%250AAfter%2520the%2520sampling%2520phase%252C%2520the%2520procedure%2520enters%2520the%2520recommendation%2520phase%252C%2520where%250Awe%2520select%2520the%2520arm%2520with%2520the%2520highest%2520sample%2520mean%2520as%2520our%2520estimate%2520of%2520the%2520best%2520arm.%250AWe%2520prove%2520that%2520this%2520single%2520strategy%2520is%2520simultaneously%2520asymptotically%2520minimax%2520and%250ABayes%2520optimal%2520for%2520the%2520simple%2520regret%252C%2520with%2520upper%2520bounds%2520that%2520coincide%2520exactly%250Awith%2520our%2520lower%2520bounds%252C%2520including%2520the%2520constant%2520terms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.24007v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimax%20and%20Bayes%20Optimal%20Best-Arm%20Identification&entry.906535625=Masahiro%20Kato&entry.1292438233=%20%20This%20study%20investigates%20minimax%20and%20Bayes%20optimal%20strategies%20in%20fixed-budget%0Abest-arm%20identification.%20We%20consider%20an%20adaptive%20procedure%20consisting%20of%20a%0Asampling%20phase%20followed%20by%20a%20recommendation%20phase%2C%20and%20we%20design%20an%20adaptive%0Aexperiment%20within%20this%20framework%20to%20efficiently%20identify%20the%20best%20arm%2C%20defined%0Aas%20the%20one%20with%20the%20highest%20expected%20outcome.%20In%20our%20proposed%20strategy%2C%20the%0Asampling%20phase%20consists%20of%20two%20stages.%20The%20first%20stage%20is%20a%20pilot%20phase%2C%20in%0Awhich%20we%20allocate%20each%20arm%20uniformly%20in%20equal%20proportions%20to%20eliminate%20clearly%0Asuboptimal%20arms%20and%20estimate%20outcome%20variances.%20In%20the%20second%20stage%2C%20arms%20are%0Aallocated%20in%20proportion%20to%20the%20variances%20estimated%20during%20the%20first%20stage.%0AAfter%20the%20sampling%20phase%2C%20the%20procedure%20enters%20the%20recommendation%20phase%2C%20where%0Awe%20select%20the%20arm%20with%20the%20highest%20sample%20mean%20as%20our%20estimate%20of%20the%20best%20arm.%0AWe%20prove%20that%20this%20single%20strategy%20is%20simultaneously%20asymptotically%20minimax%20and%0ABayes%20optimal%20for%20the%20simple%20regret%2C%20with%20upper%20bounds%20that%20coincide%20exactly%0Awith%20our%20lower%20bounds%2C%20including%20the%20constant%20terms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.24007v3&entry.124074799=Read"},
{"title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and\n  Detection", "author": "Haywood Gelman and John D. Hastings and David Kenley", "abstract": "  Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their\nperformance evaluated through statistical metrics including accuracy,\nprecision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7\nconsistently outperformed GPT-4o across nearly all metrics, particularly in\nreducing false alarms and improving detection accuracy. The results show strong\npromise for the use of LLMs in synthetic dataset generation and insider threat\ndetection.\n", "link": "http://arxiv.org/abs/2509.06920v2", "date": "2025-10-01", "relevancy": 1.806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection&body=Title%3A%20An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection%0AAuthor%3A%20Haywood%20Gelman%20and%20John%20D.%20Hastings%20and%20David%20Kenley%0AAbstract%3A%20%20%20Insider%20threats%20are%20a%20growing%20organizational%20problem%20due%20to%20the%20complexity%20of%0Aidentifying%20their%20technical%20and%20behavioral%20elements.%20A%20large%20research%20body%20is%0Adedicated%20to%20the%20study%20of%20insider%20threats%20from%20technological%2C%20psychological%2C%0Aand%20educational%20perspectives.%20However%2C%20research%20in%20this%20domain%20has%20been%0Agenerally%20dependent%20on%20datasets%20that%20are%20static%20and%20limited%20access%20which%0Arestricts%20the%20development%20of%20adaptive%20detection%20models.%20This%20study%20introduces%20a%0Anovel%2C%20ethically%20grounded%20approach%20that%20uses%20the%20large%20language%20model%20%28LLM%29%0AClaude%20Sonnet%203.7%20to%20dynamically%20synthesize%20syslog%20messages%2C%20some%20of%20which%0Acontain%20indicators%20of%20insider%20threat%20scenarios.%20The%20messages%20reflect%20real-world%0Adata%20distributions%20by%20being%20highly%20imbalanced%20%281%25%20insider%20threats%29.%20The%20syslogs%0Awere%20analyzed%20for%20insider%20threats%20by%20both%20Sonnet%203.7%20and%20GPT-4o%2C%20with%20their%0Aperformance%20evaluated%20through%20statistical%20metrics%20including%20accuracy%2C%0Aprecision%2C%20recall%2C%20F1%2C%20specificity%2C%20FAR%2C%20MCC%2C%20and%20ROC%20AUC.%20Sonnet%203.7%0Aconsistently%20outperformed%20GPT-4o%20across%20nearly%20all%20metrics%2C%20particularly%20in%0Areducing%20false%20alarms%20and%20improving%20detection%20accuracy.%20The%20results%20show%20strong%0Apromise%20for%20the%20use%20of%20LLMs%20in%20synthetic%20dataset%20generation%20and%20insider%20threat%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Ethically%2520Grounded%2520LLM-Based%2520Approach%2520to%2520Insider%2520Threat%2520Synthesis%2520and%250A%2520%2520Detection%26entry.906535625%3DHaywood%2520Gelman%2520and%2520John%2520D.%2520Hastings%2520and%2520David%2520Kenley%26entry.1292438233%3D%2520%2520Insider%2520threats%2520are%2520a%2520growing%2520organizational%2520problem%2520due%2520to%2520the%2520complexity%2520of%250Aidentifying%2520their%2520technical%2520and%2520behavioral%2520elements.%2520A%2520large%2520research%2520body%2520is%250Adedicated%2520to%2520the%2520study%2520of%2520insider%2520threats%2520from%2520technological%252C%2520psychological%252C%250Aand%2520educational%2520perspectives.%2520However%252C%2520research%2520in%2520this%2520domain%2520has%2520been%250Agenerally%2520dependent%2520on%2520datasets%2520that%2520are%2520static%2520and%2520limited%2520access%2520which%250Arestricts%2520the%2520development%2520of%2520adaptive%2520detection%2520models.%2520This%2520study%2520introduces%2520a%250Anovel%252C%2520ethically%2520grounded%2520approach%2520that%2520uses%2520the%2520large%2520language%2520model%2520%2528LLM%2529%250AClaude%2520Sonnet%25203.7%2520to%2520dynamically%2520synthesize%2520syslog%2520messages%252C%2520some%2520of%2520which%250Acontain%2520indicators%2520of%2520insider%2520threat%2520scenarios.%2520The%2520messages%2520reflect%2520real-world%250Adata%2520distributions%2520by%2520being%2520highly%2520imbalanced%2520%25281%2525%2520insider%2520threats%2529.%2520The%2520syslogs%250Awere%2520analyzed%2520for%2520insider%2520threats%2520by%2520both%2520Sonnet%25203.7%2520and%2520GPT-4o%252C%2520with%2520their%250Aperformance%2520evaluated%2520through%2520statistical%2520metrics%2520including%2520accuracy%252C%250Aprecision%252C%2520recall%252C%2520F1%252C%2520specificity%252C%2520FAR%252C%2520MCC%252C%2520and%2520ROC%2520AUC.%2520Sonnet%25203.7%250Aconsistently%2520outperformed%2520GPT-4o%2520across%2520nearly%2520all%2520metrics%252C%2520particularly%2520in%250Areducing%2520false%2520alarms%2520and%2520improving%2520detection%2520accuracy.%2520The%2520results%2520show%2520strong%250Apromise%2520for%2520the%2520use%2520of%2520LLMs%2520in%2520synthetic%2520dataset%2520generation%2520and%2520insider%2520threat%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Ethically%20Grounded%20LLM-Based%20Approach%20to%20Insider%20Threat%20Synthesis%20and%0A%20%20Detection&entry.906535625=Haywood%20Gelman%20and%20John%20D.%20Hastings%20and%20David%20Kenley&entry.1292438233=%20%20Insider%20threats%20are%20a%20growing%20organizational%20problem%20due%20to%20the%20complexity%20of%0Aidentifying%20their%20technical%20and%20behavioral%20elements.%20A%20large%20research%20body%20is%0Adedicated%20to%20the%20study%20of%20insider%20threats%20from%20technological%2C%20psychological%2C%0Aand%20educational%20perspectives.%20However%2C%20research%20in%20this%20domain%20has%20been%0Agenerally%20dependent%20on%20datasets%20that%20are%20static%20and%20limited%20access%20which%0Arestricts%20the%20development%20of%20adaptive%20detection%20models.%20This%20study%20introduces%20a%0Anovel%2C%20ethically%20grounded%20approach%20that%20uses%20the%20large%20language%20model%20%28LLM%29%0AClaude%20Sonnet%203.7%20to%20dynamically%20synthesize%20syslog%20messages%2C%20some%20of%20which%0Acontain%20indicators%20of%20insider%20threat%20scenarios.%20The%20messages%20reflect%20real-world%0Adata%20distributions%20by%20being%20highly%20imbalanced%20%281%25%20insider%20threats%29.%20The%20syslogs%0Awere%20analyzed%20for%20insider%20threats%20by%20both%20Sonnet%203.7%20and%20GPT-4o%2C%20with%20their%0Aperformance%20evaluated%20through%20statistical%20metrics%20including%20accuracy%2C%0Aprecision%2C%20recall%2C%20F1%2C%20specificity%2C%20FAR%2C%20MCC%2C%20and%20ROC%20AUC.%20Sonnet%203.7%0Aconsistently%20outperformed%20GPT-4o%20across%20nearly%20all%20metrics%2C%20particularly%20in%0Areducing%20false%20alarms%20and%20improving%20detection%20accuracy.%20The%20results%20show%20strong%0Apromise%20for%20the%20use%20of%20LLMs%20in%20synthetic%20dataset%20generation%20and%20insider%20threat%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06920v2&entry.124074799=Read"},
{"title": "Replicable Reinforcement Learning with Linear Function Approximation", "author": "Eric Eaton and Marcel Hussing and Michael Kearns and Aaron Roth and Sikata Bela Sengupta and Jessica Sorrell", "abstract": "  Replication of experimental results has been a challenge faced by many\nscientific disciplines, including the field of machine learning. Recent work on\nthe theory of machine learning has formalized replicability as the demand that\nan algorithm produce identical outcomes when executed twice on different\nsamples from the same distribution. Provably replicable algorithms are\nespecially interesting for reinforcement learning (RL), where algorithms are\nknown to be unstable in practice. While replicable algorithms exist for tabular\nRL settings, extending these guarantees to more practical function\napproximation settings has remained an open problem. In this work, we make\nprogress by developing replicable methods for linear function approximation in\nRL. We first introduce two efficient algorithms for replicable random design\nregression and uncentered covariance estimation, each of independent interest.\nWe then leverage these tools to provide the first provably efficient replicable\nRL algorithms for linear Markov decision processes in both the generative model\nand episodic settings. Finally, we evaluate our algorithms experimentally and\nshow how they can inspire more consistent neural policies.\n", "link": "http://arxiv.org/abs/2509.08660v2", "date": "2025-10-01", "relevancy": 1.7842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4959}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Replicable%20Reinforcement%20Learning%20with%20Linear%20Function%20Approximation&body=Title%3A%20Replicable%20Reinforcement%20Learning%20with%20Linear%20Function%20Approximation%0AAuthor%3A%20Eric%20Eaton%20and%20Marcel%20Hussing%20and%20Michael%20Kearns%20and%20Aaron%20Roth%20and%20Sikata%20Bela%20Sengupta%20and%20Jessica%20Sorrell%0AAbstract%3A%20%20%20Replication%20of%20experimental%20results%20has%20been%20a%20challenge%20faced%20by%20many%0Ascientific%20disciplines%2C%20including%20the%20field%20of%20machine%20learning.%20Recent%20work%20on%0Athe%20theory%20of%20machine%20learning%20has%20formalized%20replicability%20as%20the%20demand%20that%0Aan%20algorithm%20produce%20identical%20outcomes%20when%20executed%20twice%20on%20different%0Asamples%20from%20the%20same%20distribution.%20Provably%20replicable%20algorithms%20are%0Aespecially%20interesting%20for%20reinforcement%20learning%20%28RL%29%2C%20where%20algorithms%20are%0Aknown%20to%20be%20unstable%20in%20practice.%20While%20replicable%20algorithms%20exist%20for%20tabular%0ARL%20settings%2C%20extending%20these%20guarantees%20to%20more%20practical%20function%0Aapproximation%20settings%20has%20remained%20an%20open%20problem.%20In%20this%20work%2C%20we%20make%0Aprogress%20by%20developing%20replicable%20methods%20for%20linear%20function%20approximation%20in%0ARL.%20We%20first%20introduce%20two%20efficient%20algorithms%20for%20replicable%20random%20design%0Aregression%20and%20uncentered%20covariance%20estimation%2C%20each%20of%20independent%20interest.%0AWe%20then%20leverage%20these%20tools%20to%20provide%20the%20first%20provably%20efficient%20replicable%0ARL%20algorithms%20for%20linear%20Markov%20decision%20processes%20in%20both%20the%20generative%20model%0Aand%20episodic%20settings.%20Finally%2C%20we%20evaluate%20our%20algorithms%20experimentally%20and%0Ashow%20how%20they%20can%20inspire%20more%20consistent%20neural%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReplicable%2520Reinforcement%2520Learning%2520with%2520Linear%2520Function%2520Approximation%26entry.906535625%3DEric%2520Eaton%2520and%2520Marcel%2520Hussing%2520and%2520Michael%2520Kearns%2520and%2520Aaron%2520Roth%2520and%2520Sikata%2520Bela%2520Sengupta%2520and%2520Jessica%2520Sorrell%26entry.1292438233%3D%2520%2520Replication%2520of%2520experimental%2520results%2520has%2520been%2520a%2520challenge%2520faced%2520by%2520many%250Ascientific%2520disciplines%252C%2520including%2520the%2520field%2520of%2520machine%2520learning.%2520Recent%2520work%2520on%250Athe%2520theory%2520of%2520machine%2520learning%2520has%2520formalized%2520replicability%2520as%2520the%2520demand%2520that%250Aan%2520algorithm%2520produce%2520identical%2520outcomes%2520when%2520executed%2520twice%2520on%2520different%250Asamples%2520from%2520the%2520same%2520distribution.%2520Provably%2520replicable%2520algorithms%2520are%250Aespecially%2520interesting%2520for%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520where%2520algorithms%2520are%250Aknown%2520to%2520be%2520unstable%2520in%2520practice.%2520While%2520replicable%2520algorithms%2520exist%2520for%2520tabular%250ARL%2520settings%252C%2520extending%2520these%2520guarantees%2520to%2520more%2520practical%2520function%250Aapproximation%2520settings%2520has%2520remained%2520an%2520open%2520problem.%2520In%2520this%2520work%252C%2520we%2520make%250Aprogress%2520by%2520developing%2520replicable%2520methods%2520for%2520linear%2520function%2520approximation%2520in%250ARL.%2520We%2520first%2520introduce%2520two%2520efficient%2520algorithms%2520for%2520replicable%2520random%2520design%250Aregression%2520and%2520uncentered%2520covariance%2520estimation%252C%2520each%2520of%2520independent%2520interest.%250AWe%2520then%2520leverage%2520these%2520tools%2520to%2520provide%2520the%2520first%2520provably%2520efficient%2520replicable%250ARL%2520algorithms%2520for%2520linear%2520Markov%2520decision%2520processes%2520in%2520both%2520the%2520generative%2520model%250Aand%2520episodic%2520settings.%2520Finally%252C%2520we%2520evaluate%2520our%2520algorithms%2520experimentally%2520and%250Ashow%2520how%2520they%2520can%2520inspire%2520more%2520consistent%2520neural%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Replicable%20Reinforcement%20Learning%20with%20Linear%20Function%20Approximation&entry.906535625=Eric%20Eaton%20and%20Marcel%20Hussing%20and%20Michael%20Kearns%20and%20Aaron%20Roth%20and%20Sikata%20Bela%20Sengupta%20and%20Jessica%20Sorrell&entry.1292438233=%20%20Replication%20of%20experimental%20results%20has%20been%20a%20challenge%20faced%20by%20many%0Ascientific%20disciplines%2C%20including%20the%20field%20of%20machine%20learning.%20Recent%20work%20on%0Athe%20theory%20of%20machine%20learning%20has%20formalized%20replicability%20as%20the%20demand%20that%0Aan%20algorithm%20produce%20identical%20outcomes%20when%20executed%20twice%20on%20different%0Asamples%20from%20the%20same%20distribution.%20Provably%20replicable%20algorithms%20are%0Aespecially%20interesting%20for%20reinforcement%20learning%20%28RL%29%2C%20where%20algorithms%20are%0Aknown%20to%20be%20unstable%20in%20practice.%20While%20replicable%20algorithms%20exist%20for%20tabular%0ARL%20settings%2C%20extending%20these%20guarantees%20to%20more%20practical%20function%0Aapproximation%20settings%20has%20remained%20an%20open%20problem.%20In%20this%20work%2C%20we%20make%0Aprogress%20by%20developing%20replicable%20methods%20for%20linear%20function%20approximation%20in%0ARL.%20We%20first%20introduce%20two%20efficient%20algorithms%20for%20replicable%20random%20design%0Aregression%20and%20uncentered%20covariance%20estimation%2C%20each%20of%20independent%20interest.%0AWe%20then%20leverage%20these%20tools%20to%20provide%20the%20first%20provably%20efficient%20replicable%0ARL%20algorithms%20for%20linear%20Markov%20decision%20processes%20in%20both%20the%20generative%20model%0Aand%20episodic%20settings.%20Finally%2C%20we%20evaluate%20our%20algorithms%20experimentally%20and%0Ashow%20how%20they%20can%20inspire%20more%20consistent%20neural%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08660v2&entry.124074799=Read"},
{"title": "Gauges and Accelerated Optimization over Smooth and/or Strongly Convex\n  Sets", "author": "Ning Liu and Benjamin Grimmer", "abstract": "  We consider feasibility and constrained optimization problems defined over\nsmooth and/or strongly convex sets. These notions mirror their popular function\ncounterparts but are much less explored in the first-order optimization\nliterature. We propose new scalable, projection-free, accelerated first-order\nmethods in these settings. Our methods avoid linear optimization or projection\noracles, only using cheap one-dimensional linesearches and normal vector\ncomputations. Despite this, we derive optimal accelerated convergence\nguarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth\nproblems, and accelerated linear convergence given both. Our algorithms and\nanalysis are based on novel characterizations of the Minkowski gauge of smooth\nand/or strongly convex sets, which may be of independent interest: although the\ngauge is neither smooth nor strongly convex, we show the gauge squared inherits\nany structure present in the set.\n", "link": "http://arxiv.org/abs/2303.05037v4", "date": "2025-10-01", "relevancy": 1.778, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4388}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gauges%20and%20Accelerated%20Optimization%20over%20Smooth%20and/or%20Strongly%20Convex%0A%20%20Sets&body=Title%3A%20Gauges%20and%20Accelerated%20Optimization%20over%20Smooth%20and/or%20Strongly%20Convex%0A%20%20Sets%0AAuthor%3A%20Ning%20Liu%20and%20Benjamin%20Grimmer%0AAbstract%3A%20%20%20We%20consider%20feasibility%20and%20constrained%20optimization%20problems%20defined%20over%0Asmooth%20and/or%20strongly%20convex%20sets.%20These%20notions%20mirror%20their%20popular%20function%0Acounterparts%20but%20are%20much%20less%20explored%20in%20the%20first-order%20optimization%0Aliterature.%20We%20propose%20new%20scalable%2C%20projection-free%2C%20accelerated%20first-order%0Amethods%20in%20these%20settings.%20Our%20methods%20avoid%20linear%20optimization%20or%20projection%0Aoracles%2C%20only%20using%20cheap%20one-dimensional%20linesearches%20and%20normal%20vector%0Acomputations.%20Despite%20this%2C%20we%20derive%20optimal%20accelerated%20convergence%0Aguarantees%20of%20%24O%281/T%29%24%20for%20strongly%20convex%20problems%2C%20%24O%281/T%5E2%29%24%20for%20smooth%0Aproblems%2C%20and%20accelerated%20linear%20convergence%20given%20both.%20Our%20algorithms%20and%0Aanalysis%20are%20based%20on%20novel%20characterizations%20of%20the%20Minkowski%20gauge%20of%20smooth%0Aand/or%20strongly%20convex%20sets%2C%20which%20may%20be%20of%20independent%20interest%3A%20although%20the%0Agauge%20is%20neither%20smooth%20nor%20strongly%20convex%2C%20we%20show%20the%20gauge%20squared%20inherits%0Aany%20structure%20present%20in%20the%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.05037v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauges%2520and%2520Accelerated%2520Optimization%2520over%2520Smooth%2520and/or%2520Strongly%2520Convex%250A%2520%2520Sets%26entry.906535625%3DNing%2520Liu%2520and%2520Benjamin%2520Grimmer%26entry.1292438233%3D%2520%2520We%2520consider%2520feasibility%2520and%2520constrained%2520optimization%2520problems%2520defined%2520over%250Asmooth%2520and/or%2520strongly%2520convex%2520sets.%2520These%2520notions%2520mirror%2520their%2520popular%2520function%250Acounterparts%2520but%2520are%2520much%2520less%2520explored%2520in%2520the%2520first-order%2520optimization%250Aliterature.%2520We%2520propose%2520new%2520scalable%252C%2520projection-free%252C%2520accelerated%2520first-order%250Amethods%2520in%2520these%2520settings.%2520Our%2520methods%2520avoid%2520linear%2520optimization%2520or%2520projection%250Aoracles%252C%2520only%2520using%2520cheap%2520one-dimensional%2520linesearches%2520and%2520normal%2520vector%250Acomputations.%2520Despite%2520this%252C%2520we%2520derive%2520optimal%2520accelerated%2520convergence%250Aguarantees%2520of%2520%2524O%25281/T%2529%2524%2520for%2520strongly%2520convex%2520problems%252C%2520%2524O%25281/T%255E2%2529%2524%2520for%2520smooth%250Aproblems%252C%2520and%2520accelerated%2520linear%2520convergence%2520given%2520both.%2520Our%2520algorithms%2520and%250Aanalysis%2520are%2520based%2520on%2520novel%2520characterizations%2520of%2520the%2520Minkowski%2520gauge%2520of%2520smooth%250Aand/or%2520strongly%2520convex%2520sets%252C%2520which%2520may%2520be%2520of%2520independent%2520interest%253A%2520although%2520the%250Agauge%2520is%2520neither%2520smooth%2520nor%2520strongly%2520convex%252C%2520we%2520show%2520the%2520gauge%2520squared%2520inherits%250Aany%2520structure%2520present%2520in%2520the%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.05037v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gauges%20and%20Accelerated%20Optimization%20over%20Smooth%20and/or%20Strongly%20Convex%0A%20%20Sets&entry.906535625=Ning%20Liu%20and%20Benjamin%20Grimmer&entry.1292438233=%20%20We%20consider%20feasibility%20and%20constrained%20optimization%20problems%20defined%20over%0Asmooth%20and/or%20strongly%20convex%20sets.%20These%20notions%20mirror%20their%20popular%20function%0Acounterparts%20but%20are%20much%20less%20explored%20in%20the%20first-order%20optimization%0Aliterature.%20We%20propose%20new%20scalable%2C%20projection-free%2C%20accelerated%20first-order%0Amethods%20in%20these%20settings.%20Our%20methods%20avoid%20linear%20optimization%20or%20projection%0Aoracles%2C%20only%20using%20cheap%20one-dimensional%20linesearches%20and%20normal%20vector%0Acomputations.%20Despite%20this%2C%20we%20derive%20optimal%20accelerated%20convergence%0Aguarantees%20of%20%24O%281/T%29%24%20for%20strongly%20convex%20problems%2C%20%24O%281/T%5E2%29%24%20for%20smooth%0Aproblems%2C%20and%20accelerated%20linear%20convergence%20given%20both.%20Our%20algorithms%20and%0Aanalysis%20are%20based%20on%20novel%20characterizations%20of%20the%20Minkowski%20gauge%20of%20smooth%0Aand/or%20strongly%20convex%20sets%2C%20which%20may%20be%20of%20independent%20interest%3A%20although%20the%0Agauge%20is%20neither%20smooth%20nor%20strongly%20convex%2C%20we%20show%20the%20gauge%20squared%20inherits%0Aany%20structure%20present%20in%20the%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.05037v4&entry.124074799=Read"},
{"title": "Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation", "author": "Haolei Tong and Yuezhe Zhang and Sophie Lueth and Georgia Chalvatzaki", "abstract": "  Coordinated multi-arm manipulation requires satisfying multiple simultaneous\ngeometric constraints across high-dimensional configuration spaces, which poses\na significant challenge for traditional planning and control methods. In this\nwork, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative\nframework that flexibly integrates both equality (e.g., relative and absolute\npose constraints) and structured inequality constraints (e.g., proximity to\nobject surfaces) into an energy-based diffusion model. Equality constraints are\nmodeled using dedicated energy networks trained on pose differences in Lie\nalgebra space, while inequality constraints are represented via Signed Distance\nFunctions (SDFs) and encoded into learned constraint embeddings, allowing the\nmodel to reason about complex spatial regions. A key innovation of our method\nis a Transformer-based architecture that learns to weight constraint-specific\nenergy functions at inference time, enabling flexible and context-aware\nconstraint integration. Moreover, we adopt a two-phase sampling strategy that\nimproves precision and sample diversity by combining Langevin dynamics with\nresampling and density-aware re-weighting. Experimental results on dual-arm\nmanipulation tasks show that ADCS significantly improves sample diversity and\ngeneralization across settings demanding precise coordination and adaptive\nconstraint handling.\n", "link": "http://arxiv.org/abs/2505.13667v4", "date": "2025-10-01", "relevancy": 1.7774, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5901}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Diffusion%20Constrained%20Sampling%20for%20Bimanual%20Robot%20Manipulation&body=Title%3A%20Adaptive%20Diffusion%20Constrained%20Sampling%20for%20Bimanual%20Robot%20Manipulation%0AAuthor%3A%20Haolei%20Tong%20and%20Yuezhe%20Zhang%20and%20Sophie%20Lueth%20and%20Georgia%20Chalvatzaki%0AAbstract%3A%20%20%20Coordinated%20multi-arm%20manipulation%20requires%20satisfying%20multiple%20simultaneous%0Ageometric%20constraints%20across%20high-dimensional%20configuration%20spaces%2C%20which%20poses%0Aa%20significant%20challenge%20for%20traditional%20planning%20and%20control%20methods.%20In%20this%0Awork%2C%20we%20propose%20Adaptive%20Diffusion%20Constrained%20Sampling%20%28ADCS%29%2C%20a%20generative%0Aframework%20that%20flexibly%20integrates%20both%20equality%20%28e.g.%2C%20relative%20and%20absolute%0Apose%20constraints%29%20and%20structured%20inequality%20constraints%20%28e.g.%2C%20proximity%20to%0Aobject%20surfaces%29%20into%20an%20energy-based%20diffusion%20model.%20Equality%20constraints%20are%0Amodeled%20using%20dedicated%20energy%20networks%20trained%20on%20pose%20differences%20in%20Lie%0Aalgebra%20space%2C%20while%20inequality%20constraints%20are%20represented%20via%20Signed%20Distance%0AFunctions%20%28SDFs%29%20and%20encoded%20into%20learned%20constraint%20embeddings%2C%20allowing%20the%0Amodel%20to%20reason%20about%20complex%20spatial%20regions.%20A%20key%20innovation%20of%20our%20method%0Ais%20a%20Transformer-based%20architecture%20that%20learns%20to%20weight%20constraint-specific%0Aenergy%20functions%20at%20inference%20time%2C%20enabling%20flexible%20and%20context-aware%0Aconstraint%20integration.%20Moreover%2C%20we%20adopt%20a%20two-phase%20sampling%20strategy%20that%0Aimproves%20precision%20and%20sample%20diversity%20by%20combining%20Langevin%20dynamics%20with%0Aresampling%20and%20density-aware%20re-weighting.%20Experimental%20results%20on%20dual-arm%0Amanipulation%20tasks%20show%20that%20ADCS%20significantly%20improves%20sample%20diversity%20and%0Ageneralization%20across%20settings%20demanding%20precise%20coordination%20and%20adaptive%0Aconstraint%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13667v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Diffusion%2520Constrained%2520Sampling%2520for%2520Bimanual%2520Robot%2520Manipulation%26entry.906535625%3DHaolei%2520Tong%2520and%2520Yuezhe%2520Zhang%2520and%2520Sophie%2520Lueth%2520and%2520Georgia%2520Chalvatzaki%26entry.1292438233%3D%2520%2520Coordinated%2520multi-arm%2520manipulation%2520requires%2520satisfying%2520multiple%2520simultaneous%250Ageometric%2520constraints%2520across%2520high-dimensional%2520configuration%2520spaces%252C%2520which%2520poses%250Aa%2520significant%2520challenge%2520for%2520traditional%2520planning%2520and%2520control%2520methods.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Adaptive%2520Diffusion%2520Constrained%2520Sampling%2520%2528ADCS%2529%252C%2520a%2520generative%250Aframework%2520that%2520flexibly%2520integrates%2520both%2520equality%2520%2528e.g.%252C%2520relative%2520and%2520absolute%250Apose%2520constraints%2529%2520and%2520structured%2520inequality%2520constraints%2520%2528e.g.%252C%2520proximity%2520to%250Aobject%2520surfaces%2529%2520into%2520an%2520energy-based%2520diffusion%2520model.%2520Equality%2520constraints%2520are%250Amodeled%2520using%2520dedicated%2520energy%2520networks%2520trained%2520on%2520pose%2520differences%2520in%2520Lie%250Aalgebra%2520space%252C%2520while%2520inequality%2520constraints%2520are%2520represented%2520via%2520Signed%2520Distance%250AFunctions%2520%2528SDFs%2529%2520and%2520encoded%2520into%2520learned%2520constraint%2520embeddings%252C%2520allowing%2520the%250Amodel%2520to%2520reason%2520about%2520complex%2520spatial%2520regions.%2520A%2520key%2520innovation%2520of%2520our%2520method%250Ais%2520a%2520Transformer-based%2520architecture%2520that%2520learns%2520to%2520weight%2520constraint-specific%250Aenergy%2520functions%2520at%2520inference%2520time%252C%2520enabling%2520flexible%2520and%2520context-aware%250Aconstraint%2520integration.%2520Moreover%252C%2520we%2520adopt%2520a%2520two-phase%2520sampling%2520strategy%2520that%250Aimproves%2520precision%2520and%2520sample%2520diversity%2520by%2520combining%2520Langevin%2520dynamics%2520with%250Aresampling%2520and%2520density-aware%2520re-weighting.%2520Experimental%2520results%2520on%2520dual-arm%250Amanipulation%2520tasks%2520show%2520that%2520ADCS%2520significantly%2520improves%2520sample%2520diversity%2520and%250Ageneralization%2520across%2520settings%2520demanding%2520precise%2520coordination%2520and%2520adaptive%250Aconstraint%2520handling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13667v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Diffusion%20Constrained%20Sampling%20for%20Bimanual%20Robot%20Manipulation&entry.906535625=Haolei%20Tong%20and%20Yuezhe%20Zhang%20and%20Sophie%20Lueth%20and%20Georgia%20Chalvatzaki&entry.1292438233=%20%20Coordinated%20multi-arm%20manipulation%20requires%20satisfying%20multiple%20simultaneous%0Ageometric%20constraints%20across%20high-dimensional%20configuration%20spaces%2C%20which%20poses%0Aa%20significant%20challenge%20for%20traditional%20planning%20and%20control%20methods.%20In%20this%0Awork%2C%20we%20propose%20Adaptive%20Diffusion%20Constrained%20Sampling%20%28ADCS%29%2C%20a%20generative%0Aframework%20that%20flexibly%20integrates%20both%20equality%20%28e.g.%2C%20relative%20and%20absolute%0Apose%20constraints%29%20and%20structured%20inequality%20constraints%20%28e.g.%2C%20proximity%20to%0Aobject%20surfaces%29%20into%20an%20energy-based%20diffusion%20model.%20Equality%20constraints%20are%0Amodeled%20using%20dedicated%20energy%20networks%20trained%20on%20pose%20differences%20in%20Lie%0Aalgebra%20space%2C%20while%20inequality%20constraints%20are%20represented%20via%20Signed%20Distance%0AFunctions%20%28SDFs%29%20and%20encoded%20into%20learned%20constraint%20embeddings%2C%20allowing%20the%0Amodel%20to%20reason%20about%20complex%20spatial%20regions.%20A%20key%20innovation%20of%20our%20method%0Ais%20a%20Transformer-based%20architecture%20that%20learns%20to%20weight%20constraint-specific%0Aenergy%20functions%20at%20inference%20time%2C%20enabling%20flexible%20and%20context-aware%0Aconstraint%20integration.%20Moreover%2C%20we%20adopt%20a%20two-phase%20sampling%20strategy%20that%0Aimproves%20precision%20and%20sample%20diversity%20by%20combining%20Langevin%20dynamics%20with%0Aresampling%20and%20density-aware%20re-weighting.%20Experimental%20results%20on%20dual-arm%0Amanipulation%20tasks%20show%20that%20ADCS%20significantly%20improves%20sample%20diversity%20and%0Ageneralization%20across%20settings%20demanding%20precise%20coordination%20and%20adaptive%0Aconstraint%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13667v4&entry.124074799=Read"},
{"title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "author": "S M Asif Hossain and Ruksat Khan Shayoni and Mohd Ruhul Ameen and Akif Islam and M. F. Mridha and Jungpil Shin", "abstract": "  Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries.\n", "link": "http://arxiv.org/abs/2509.14285v2", "date": "2025-10-01", "relevancy": 1.759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4397}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Agent%20LLM%20Defense%20Pipeline%20Against%20Prompt%20Injection%20Attacks&body=Title%3A%20A%20Multi-Agent%20LLM%20Defense%20Pipeline%20Against%20Prompt%20Injection%20Attacks%0AAuthor%3A%20S%20M%20Asif%20Hossain%20and%20Ruksat%20Khan%20Shayoni%20and%20Mohd%20Ruhul%20Ameen%20and%20Akif%20Islam%20and%20M.%20F.%20Mridha%20and%20Jungpil%20Shin%0AAbstract%3A%20%20%20Prompt%20injection%20attacks%20represent%20a%20major%20vulnerability%20in%20Large%20Language%0AModel%20%28LLM%29%20deployments%2C%20where%20malicious%20instructions%20embedded%20in%20user%20inputs%0Acan%20override%20system%20prompts%20and%20induce%20unintended%20behaviors.%20This%20paper%0Apresents%20a%20novel%20multi-agent%20defense%20framework%20that%20employs%20specialized%20LLM%0Aagents%20in%20coordinated%20pipelines%20to%20detect%20and%20neutralize%20prompt%20injection%0Aattacks%20in%20real-time.%20We%20evaluate%20our%20approach%20using%20two%20distinct%0Aarchitectures%3A%20a%20sequential%20chain-of-agents%20pipeline%20and%20a%20hierarchical%0Acoordinator-based%20system.%20Our%20comprehensive%20evaluation%20on%2055%20unique%20prompt%0Ainjection%20attacks%2C%20grouped%20into%208%20categories%20and%20totaling%20400%20attack%20instances%0Aacross%20two%20LLM%20platforms%20%28ChatGLM%20and%20Llama2%29%2C%20demonstrates%20significant%0Asecurity%20improvements.%20Without%20defense%20mechanisms%2C%20baseline%20Attack%20Success%0ARates%20%28ASR%29%20reached%2030%25%20for%20ChatGLM%20and%2020%25%20for%20Llama2.%20Our%20multi-agent%0Apipeline%20achieved%20100%25%20mitigation%2C%20reducing%20ASR%20to%200%25%20across%20all%20tested%0Ascenarios.%20The%20framework%20demonstrates%20robustness%20across%20multiple%20attack%0Acategories%20including%20direct%20overrides%2C%20code%20execution%20attempts%2C%20data%0Aexfiltration%2C%20and%20obfuscation%20techniques%2C%20while%20maintaining%20system%0Afunctionality%20for%20legitimate%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Agent%2520LLM%2520Defense%2520Pipeline%2520Against%2520Prompt%2520Injection%2520Attacks%26entry.906535625%3DS%2520M%2520Asif%2520Hossain%2520and%2520Ruksat%2520Khan%2520Shayoni%2520and%2520Mohd%2520Ruhul%2520Ameen%2520and%2520Akif%2520Islam%2520and%2520M.%2520F.%2520Mridha%2520and%2520Jungpil%2520Shin%26entry.1292438233%3D%2520%2520Prompt%2520injection%2520attacks%2520represent%2520a%2520major%2520vulnerability%2520in%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520deployments%252C%2520where%2520malicious%2520instructions%2520embedded%2520in%2520user%2520inputs%250Acan%2520override%2520system%2520prompts%2520and%2520induce%2520unintended%2520behaviors.%2520This%2520paper%250Apresents%2520a%2520novel%2520multi-agent%2520defense%2520framework%2520that%2520employs%2520specialized%2520LLM%250Aagents%2520in%2520coordinated%2520pipelines%2520to%2520detect%2520and%2520neutralize%2520prompt%2520injection%250Aattacks%2520in%2520real-time.%2520We%2520evaluate%2520our%2520approach%2520using%2520two%2520distinct%250Aarchitectures%253A%2520a%2520sequential%2520chain-of-agents%2520pipeline%2520and%2520a%2520hierarchical%250Acoordinator-based%2520system.%2520Our%2520comprehensive%2520evaluation%2520on%252055%2520unique%2520prompt%250Ainjection%2520attacks%252C%2520grouped%2520into%25208%2520categories%2520and%2520totaling%2520400%2520attack%2520instances%250Aacross%2520two%2520LLM%2520platforms%2520%2528ChatGLM%2520and%2520Llama2%2529%252C%2520demonstrates%2520significant%250Asecurity%2520improvements.%2520Without%2520defense%2520mechanisms%252C%2520baseline%2520Attack%2520Success%250ARates%2520%2528ASR%2529%2520reached%252030%2525%2520for%2520ChatGLM%2520and%252020%2525%2520for%2520Llama2.%2520Our%2520multi-agent%250Apipeline%2520achieved%2520100%2525%2520mitigation%252C%2520reducing%2520ASR%2520to%25200%2525%2520across%2520all%2520tested%250Ascenarios.%2520The%2520framework%2520demonstrates%2520robustness%2520across%2520multiple%2520attack%250Acategories%2520including%2520direct%2520overrides%252C%2520code%2520execution%2520attempts%252C%2520data%250Aexfiltration%252C%2520and%2520obfuscation%2520techniques%252C%2520while%2520maintaining%2520system%250Afunctionality%2520for%2520legitimate%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Agent%20LLM%20Defense%20Pipeline%20Against%20Prompt%20Injection%20Attacks&entry.906535625=S%20M%20Asif%20Hossain%20and%20Ruksat%20Khan%20Shayoni%20and%20Mohd%20Ruhul%20Ameen%20and%20Akif%20Islam%20and%20M.%20F.%20Mridha%20and%20Jungpil%20Shin&entry.1292438233=%20%20Prompt%20injection%20attacks%20represent%20a%20major%20vulnerability%20in%20Large%20Language%0AModel%20%28LLM%29%20deployments%2C%20where%20malicious%20instructions%20embedded%20in%20user%20inputs%0Acan%20override%20system%20prompts%20and%20induce%20unintended%20behaviors.%20This%20paper%0Apresents%20a%20novel%20multi-agent%20defense%20framework%20that%20employs%20specialized%20LLM%0Aagents%20in%20coordinated%20pipelines%20to%20detect%20and%20neutralize%20prompt%20injection%0Aattacks%20in%20real-time.%20We%20evaluate%20our%20approach%20using%20two%20distinct%0Aarchitectures%3A%20a%20sequential%20chain-of-agents%20pipeline%20and%20a%20hierarchical%0Acoordinator-based%20system.%20Our%20comprehensive%20evaluation%20on%2055%20unique%20prompt%0Ainjection%20attacks%2C%20grouped%20into%208%20categories%20and%20totaling%20400%20attack%20instances%0Aacross%20two%20LLM%20platforms%20%28ChatGLM%20and%20Llama2%29%2C%20demonstrates%20significant%0Asecurity%20improvements.%20Without%20defense%20mechanisms%2C%20baseline%20Attack%20Success%0ARates%20%28ASR%29%20reached%2030%25%20for%20ChatGLM%20and%2020%25%20for%20Llama2.%20Our%20multi-agent%0Apipeline%20achieved%20100%25%20mitigation%2C%20reducing%20ASR%20to%200%25%20across%20all%20tested%0Ascenarios.%20The%20framework%20demonstrates%20robustness%20across%20multiple%20attack%0Acategories%20including%20direct%20overrides%2C%20code%20execution%20attempts%2C%20data%0Aexfiltration%2C%20and%20obfuscation%20techniques%2C%20while%20maintaining%20system%0Afunctionality%20for%20legitimate%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14285v2&entry.124074799=Read"},
{"title": "Vector-Valued Reproducing Kernel Banach Spaces for Neural Networks and\n  Operators", "author": "Sven Dummer and Tjeerd Jan Heeringa and Jos\u00e9 A. Iglesias", "abstract": "  Recently, there has been growing interest in characterizing the function\nspaces underlying neural networks. While shallow and deep scalar-valued neural\nnetworks have been linked to scalar-valued reproducing kernel Banach spaces\n(RKBS), $\\mathbb{R}^d$-valued neural networks and neural operator models remain\nless understood in the RKBS setting. To address this gap, we develop a general\ndefinition of vector-valued RKBS (vv-RKBS), which inherently includes the\nassociated reproducing kernel. Our construction extends existing definitions by\navoiding restrictive assumptions such as symmetric kernel domains,\nfinite-dimensional output spaces, reflexivity, or separability, while still\nrecovering familiar properties of vector-valued reproducing kernel Hilbert\nspaces (vv-RKHS). We then show that shallow $\\mathbb{R}^d$-valued neural\nnetworks are elements of a specific vv-RKBS, namely an instance of the integral\nand neural vv-RKBS. To also explore the functional structure of neural\noperators, we analyze the DeepONet and Hypernetwork architectures and\ndemonstrate that they too belong to an integral and neural vv-RKBS. In all\ncases, we establish a Representer Theorem, showing that optimization over these\nfunction spaces recovers the corresponding neural architectures.\n", "link": "http://arxiv.org/abs/2509.26371v2", "date": "2025-10-01", "relevancy": 1.755, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector-Valued%20Reproducing%20Kernel%20Banach%20Spaces%20for%20Neural%20Networks%20and%0A%20%20Operators&body=Title%3A%20Vector-Valued%20Reproducing%20Kernel%20Banach%20Spaces%20for%20Neural%20Networks%20and%0A%20%20Operators%0AAuthor%3A%20Sven%20Dummer%20and%20Tjeerd%20Jan%20Heeringa%20and%20Jos%C3%A9%20A.%20Iglesias%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20characterizing%20the%20function%0Aspaces%20underlying%20neural%20networks.%20While%20shallow%20and%20deep%20scalar-valued%20neural%0Anetworks%20have%20been%20linked%20to%20scalar-valued%20reproducing%20kernel%20Banach%20spaces%0A%28RKBS%29%2C%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20neural%20networks%20and%20neural%20operator%20models%20remain%0Aless%20understood%20in%20the%20RKBS%20setting.%20To%20address%20this%20gap%2C%20we%20develop%20a%20general%0Adefinition%20of%20vector-valued%20RKBS%20%28vv-RKBS%29%2C%20which%20inherently%20includes%20the%0Aassociated%20reproducing%20kernel.%20Our%20construction%20extends%20existing%20definitions%20by%0Aavoiding%20restrictive%20assumptions%20such%20as%20symmetric%20kernel%20domains%2C%0Afinite-dimensional%20output%20spaces%2C%20reflexivity%2C%20or%20separability%2C%20while%20still%0Arecovering%20familiar%20properties%20of%20vector-valued%20reproducing%20kernel%20Hilbert%0Aspaces%20%28vv-RKHS%29.%20We%20then%20show%20that%20shallow%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20neural%0Anetworks%20are%20elements%20of%20a%20specific%20vv-RKBS%2C%20namely%20an%20instance%20of%20the%20integral%0Aand%20neural%20vv-RKBS.%20To%20also%20explore%20the%20functional%20structure%20of%20neural%0Aoperators%2C%20we%20analyze%20the%20DeepONet%20and%20Hypernetwork%20architectures%20and%0Ademonstrate%20that%20they%20too%20belong%20to%20an%20integral%20and%20neural%20vv-RKBS.%20In%20all%0Acases%2C%20we%20establish%20a%20Representer%20Theorem%2C%20showing%20that%20optimization%20over%20these%0Afunction%20spaces%20recovers%20the%20corresponding%20neural%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector-Valued%2520Reproducing%2520Kernel%2520Banach%2520Spaces%2520for%2520Neural%2520Networks%2520and%250A%2520%2520Operators%26entry.906535625%3DSven%2520Dummer%2520and%2520Tjeerd%2520Jan%2520Heeringa%2520and%2520Jos%25C3%25A9%2520A.%2520Iglesias%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520characterizing%2520the%2520function%250Aspaces%2520underlying%2520neural%2520networks.%2520While%2520shallow%2520and%2520deep%2520scalar-valued%2520neural%250Anetworks%2520have%2520been%2520linked%2520to%2520scalar-valued%2520reproducing%2520kernel%2520Banach%2520spaces%250A%2528RKBS%2529%252C%2520%2524%255Cmathbb%257BR%257D%255Ed%2524-valued%2520neural%2520networks%2520and%2520neural%2520operator%2520models%2520remain%250Aless%2520understood%2520in%2520the%2520RKBS%2520setting.%2520To%2520address%2520this%2520gap%252C%2520we%2520develop%2520a%2520general%250Adefinition%2520of%2520vector-valued%2520RKBS%2520%2528vv-RKBS%2529%252C%2520which%2520inherently%2520includes%2520the%250Aassociated%2520reproducing%2520kernel.%2520Our%2520construction%2520extends%2520existing%2520definitions%2520by%250Aavoiding%2520restrictive%2520assumptions%2520such%2520as%2520symmetric%2520kernel%2520domains%252C%250Afinite-dimensional%2520output%2520spaces%252C%2520reflexivity%252C%2520or%2520separability%252C%2520while%2520still%250Arecovering%2520familiar%2520properties%2520of%2520vector-valued%2520reproducing%2520kernel%2520Hilbert%250Aspaces%2520%2528vv-RKHS%2529.%2520We%2520then%2520show%2520that%2520shallow%2520%2524%255Cmathbb%257BR%257D%255Ed%2524-valued%2520neural%250Anetworks%2520are%2520elements%2520of%2520a%2520specific%2520vv-RKBS%252C%2520namely%2520an%2520instance%2520of%2520the%2520integral%250Aand%2520neural%2520vv-RKBS.%2520To%2520also%2520explore%2520the%2520functional%2520structure%2520of%2520neural%250Aoperators%252C%2520we%2520analyze%2520the%2520DeepONet%2520and%2520Hypernetwork%2520architectures%2520and%250Ademonstrate%2520that%2520they%2520too%2520belong%2520to%2520an%2520integral%2520and%2520neural%2520vv-RKBS.%2520In%2520all%250Acases%252C%2520we%2520establish%2520a%2520Representer%2520Theorem%252C%2520showing%2520that%2520optimization%2520over%2520these%250Afunction%2520spaces%2520recovers%2520the%2520corresponding%2520neural%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector-Valued%20Reproducing%20Kernel%20Banach%20Spaces%20for%20Neural%20Networks%20and%0A%20%20Operators&entry.906535625=Sven%20Dummer%20and%20Tjeerd%20Jan%20Heeringa%20and%20Jos%C3%A9%20A.%20Iglesias&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20characterizing%20the%20function%0Aspaces%20underlying%20neural%20networks.%20While%20shallow%20and%20deep%20scalar-valued%20neural%0Anetworks%20have%20been%20linked%20to%20scalar-valued%20reproducing%20kernel%20Banach%20spaces%0A%28RKBS%29%2C%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20neural%20networks%20and%20neural%20operator%20models%20remain%0Aless%20understood%20in%20the%20RKBS%20setting.%20To%20address%20this%20gap%2C%20we%20develop%20a%20general%0Adefinition%20of%20vector-valued%20RKBS%20%28vv-RKBS%29%2C%20which%20inherently%20includes%20the%0Aassociated%20reproducing%20kernel.%20Our%20construction%20extends%20existing%20definitions%20by%0Aavoiding%20restrictive%20assumptions%20such%20as%20symmetric%20kernel%20domains%2C%0Afinite-dimensional%20output%20spaces%2C%20reflexivity%2C%20or%20separability%2C%20while%20still%0Arecovering%20familiar%20properties%20of%20vector-valued%20reproducing%20kernel%20Hilbert%0Aspaces%20%28vv-RKHS%29.%20We%20then%20show%20that%20shallow%20%24%5Cmathbb%7BR%7D%5Ed%24-valued%20neural%0Anetworks%20are%20elements%20of%20a%20specific%20vv-RKBS%2C%20namely%20an%20instance%20of%20the%20integral%0Aand%20neural%20vv-RKBS.%20To%20also%20explore%20the%20functional%20structure%20of%20neural%0Aoperators%2C%20we%20analyze%20the%20DeepONet%20and%20Hypernetwork%20architectures%20and%0Ademonstrate%20that%20they%20too%20belong%20to%20an%20integral%20and%20neural%20vv-RKBS.%20In%20all%0Acases%2C%20we%20establish%20a%20Representer%20Theorem%2C%20showing%20that%20optimization%20over%20these%0Afunction%20spaces%20recovers%20the%20corresponding%20neural%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26371v2&entry.124074799=Read"},
{"title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking", "author": "Feng Wang and Yuqing Li and Han Xiao", "abstract": "  jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being significant smaller than generative\nlistwise rerankers.\n", "link": "http://arxiv.org/abs/2509.25085v2", "date": "2025-10-01", "relevancy": 1.7238, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4376}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4264}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Document%20Reranking&body=Title%3A%20jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Document%20Reranking%0AAuthor%3A%20Feng%20Wang%20and%20Yuqing%20Li%20and%20Han%20Xiao%0AAbstract%3A%20%20%20jina-reranker-v3%20is%20a%200.6B%20parameter%20multilingual%20document%20reranker%20that%0Aintroduces%20a%20novel%20last%20but%20not%20late%20interaction.%20Unlike%20late%20interaction%0Amodels%20such%20as%20ColBERT%20that%20perform%20separate%20encoding%20followed%20by%20multi-vector%0Amatching%2C%20our%20approach%20conducts%20causal%20self-attention%20between%20query%20and%0Adocuments%20within%20the%20same%20context%20window%2C%20enabling%20rich%20cross-document%0Ainteractions%20before%20extracting%20contextual%20embeddings%20from%20the%20last%20token%20of%0Aeach%20document.%20This%20compact%20architecture%20achieves%20state-of-the-art%20BEIR%0Aperformance%20with%2061.94%20nDCG%4010%20while%20being%20significant%20smaller%20than%20generative%0Alistwise%20rerankers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Djina-reranker-v3%253A%2520Last%2520but%2520Not%2520Late%2520Interaction%2520for%2520Document%2520Reranking%26entry.906535625%3DFeng%2520Wang%2520and%2520Yuqing%2520Li%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520jina-reranker-v3%2520is%2520a%25200.6B%2520parameter%2520multilingual%2520document%2520reranker%2520that%250Aintroduces%2520a%2520novel%2520last%2520but%2520not%2520late%2520interaction.%2520Unlike%2520late%2520interaction%250Amodels%2520such%2520as%2520ColBERT%2520that%2520perform%2520separate%2520encoding%2520followed%2520by%2520multi-vector%250Amatching%252C%2520our%2520approach%2520conducts%2520causal%2520self-attention%2520between%2520query%2520and%250Adocuments%2520within%2520the%2520same%2520context%2520window%252C%2520enabling%2520rich%2520cross-document%250Ainteractions%2520before%2520extracting%2520contextual%2520embeddings%2520from%2520the%2520last%2520token%2520of%250Aeach%2520document.%2520This%2520compact%2520architecture%2520achieves%2520state-of-the-art%2520BEIR%250Aperformance%2520with%252061.94%2520nDCG%254010%2520while%2520being%2520significant%2520smaller%2520than%2520generative%250Alistwise%2520rerankers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=jina-reranker-v3%3A%20Last%20but%20Not%20Late%20Interaction%20for%20Document%20Reranking&entry.906535625=Feng%20Wang%20and%20Yuqing%20Li%20and%20Han%20Xiao&entry.1292438233=%20%20jina-reranker-v3%20is%20a%200.6B%20parameter%20multilingual%20document%20reranker%20that%0Aintroduces%20a%20novel%20last%20but%20not%20late%20interaction.%20Unlike%20late%20interaction%0Amodels%20such%20as%20ColBERT%20that%20perform%20separate%20encoding%20followed%20by%20multi-vector%0Amatching%2C%20our%20approach%20conducts%20causal%20self-attention%20between%20query%20and%0Adocuments%20within%20the%20same%20context%20window%2C%20enabling%20rich%20cross-document%0Ainteractions%20before%20extracting%20contextual%20embeddings%20from%20the%20last%20token%20of%0Aeach%20document.%20This%20compact%20architecture%20achieves%20state-of-the-art%20BEIR%0Aperformance%20with%2061.94%20nDCG%4010%20while%20being%20significant%20smaller%20than%20generative%0Alistwise%20rerankers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25085v2&entry.124074799=Read"},
{"title": "CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative\n  Fairness in Decentralized Learning", "author": "Nurbek Tastan and Samuel Horvath and Karthik Nandakumar", "abstract": "  Collaborative learning (CL) enables multiple participants to jointly train\nmachine learning (ML) models on decentralized data sources without raw data\nsharing. While the primary goal of CL is to maximize the expected accuracy gain\nfor each participant, it is also important to ensure that the gains are fairly\ndistributed: no client should be negatively impacted, and gains should reflect\ncontributions. Most existing CL methods require central coordination and focus\nonly on gain maximization, overlooking fairness. In this work, we first show\nthat the existing measure of collaborative fairness based on the correlation\nbetween accuracy values without and with collaboration has drawbacks because it\ndoes not account for negative collaboration gain. We argue that maximizing mean\ncollaboration gain (MCG) while simultaneously minimizing the collaboration gain\nspread (CGS) is a fairer alternative. Next, we propose the CYCle protocol that\nenables individual participants in a private decentralized learning (PDL)\nframework to achieve this objective through a novel reputation scoring method\nbased on gradient alignment between the local cross-entropy and distillation\nlosses. We further extend the CYCle protocol to operate on top of gossip-based\ndecentralized algorithms such as Gossip-SGD. We also theoretically show that\nCYCle performs better than standard FedAvg in a two-client mean estimation\nsetting under high heterogeneity. Empirical experiments demonstrate the\neffectiveness of the CYCle protocol to ensure positive and fair collaboration\ngain for all participants, even in cases where the data distributions of\nparticipants are highly skewed.\n", "link": "http://arxiv.org/abs/2501.12344v2", "date": "2025-10-01", "relevancy": 1.7065, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4272}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4266}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CYCle%3A%20Choosing%20Your%20Collaborators%20Wisely%20to%20Enhance%20Collaborative%0A%20%20Fairness%20in%20Decentralized%20Learning&body=Title%3A%20CYCle%3A%20Choosing%20Your%20Collaborators%20Wisely%20to%20Enhance%20Collaborative%0A%20%20Fairness%20in%20Decentralized%20Learning%0AAuthor%3A%20Nurbek%20Tastan%20and%20Samuel%20Horvath%20and%20Karthik%20Nandakumar%0AAbstract%3A%20%20%20Collaborative%20learning%20%28CL%29%20enables%20multiple%20participants%20to%20jointly%20train%0Amachine%20learning%20%28ML%29%20models%20on%20decentralized%20data%20sources%20without%20raw%20data%0Asharing.%20While%20the%20primary%20goal%20of%20CL%20is%20to%20maximize%20the%20expected%20accuracy%20gain%0Afor%20each%20participant%2C%20it%20is%20also%20important%20to%20ensure%20that%20the%20gains%20are%20fairly%0Adistributed%3A%20no%20client%20should%20be%20negatively%20impacted%2C%20and%20gains%20should%20reflect%0Acontributions.%20Most%20existing%20CL%20methods%20require%20central%20coordination%20and%20focus%0Aonly%20on%20gain%20maximization%2C%20overlooking%20fairness.%20In%20this%20work%2C%20we%20first%20show%0Athat%20the%20existing%20measure%20of%20collaborative%20fairness%20based%20on%20the%20correlation%0Abetween%20accuracy%20values%20without%20and%20with%20collaboration%20has%20drawbacks%20because%20it%0Adoes%20not%20account%20for%20negative%20collaboration%20gain.%20We%20argue%20that%20maximizing%20mean%0Acollaboration%20gain%20%28MCG%29%20while%20simultaneously%20minimizing%20the%20collaboration%20gain%0Aspread%20%28CGS%29%20is%20a%20fairer%20alternative.%20Next%2C%20we%20propose%20the%20CYCle%20protocol%20that%0Aenables%20individual%20participants%20in%20a%20private%20decentralized%20learning%20%28PDL%29%0Aframework%20to%20achieve%20this%20objective%20through%20a%20novel%20reputation%20scoring%20method%0Abased%20on%20gradient%20alignment%20between%20the%20local%20cross-entropy%20and%20distillation%0Alosses.%20We%20further%20extend%20the%20CYCle%20protocol%20to%20operate%20on%20top%20of%20gossip-based%0Adecentralized%20algorithms%20such%20as%20Gossip-SGD.%20We%20also%20theoretically%20show%20that%0ACYCle%20performs%20better%20than%20standard%20FedAvg%20in%20a%20two-client%20mean%20estimation%0Asetting%20under%20high%20heterogeneity.%20Empirical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20CYCle%20protocol%20to%20ensure%20positive%20and%20fair%20collaboration%0Again%20for%20all%20participants%2C%20even%20in%20cases%20where%20the%20data%20distributions%20of%0Aparticipants%20are%20highly%20skewed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCYCle%253A%2520Choosing%2520Your%2520Collaborators%2520Wisely%2520to%2520Enhance%2520Collaborative%250A%2520%2520Fairness%2520in%2520Decentralized%2520Learning%26entry.906535625%3DNurbek%2520Tastan%2520and%2520Samuel%2520Horvath%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3D%2520%2520Collaborative%2520learning%2520%2528CL%2529%2520enables%2520multiple%2520participants%2520to%2520jointly%2520train%250Amachine%2520learning%2520%2528ML%2529%2520models%2520on%2520decentralized%2520data%2520sources%2520without%2520raw%2520data%250Asharing.%2520While%2520the%2520primary%2520goal%2520of%2520CL%2520is%2520to%2520maximize%2520the%2520expected%2520accuracy%2520gain%250Afor%2520each%2520participant%252C%2520it%2520is%2520also%2520important%2520to%2520ensure%2520that%2520the%2520gains%2520are%2520fairly%250Adistributed%253A%2520no%2520client%2520should%2520be%2520negatively%2520impacted%252C%2520and%2520gains%2520should%2520reflect%250Acontributions.%2520Most%2520existing%2520CL%2520methods%2520require%2520central%2520coordination%2520and%2520focus%250Aonly%2520on%2520gain%2520maximization%252C%2520overlooking%2520fairness.%2520In%2520this%2520work%252C%2520we%2520first%2520show%250Athat%2520the%2520existing%2520measure%2520of%2520collaborative%2520fairness%2520based%2520on%2520the%2520correlation%250Abetween%2520accuracy%2520values%2520without%2520and%2520with%2520collaboration%2520has%2520drawbacks%2520because%2520it%250Adoes%2520not%2520account%2520for%2520negative%2520collaboration%2520gain.%2520We%2520argue%2520that%2520maximizing%2520mean%250Acollaboration%2520gain%2520%2528MCG%2529%2520while%2520simultaneously%2520minimizing%2520the%2520collaboration%2520gain%250Aspread%2520%2528CGS%2529%2520is%2520a%2520fairer%2520alternative.%2520Next%252C%2520we%2520propose%2520the%2520CYCle%2520protocol%2520that%250Aenables%2520individual%2520participants%2520in%2520a%2520private%2520decentralized%2520learning%2520%2528PDL%2529%250Aframework%2520to%2520achieve%2520this%2520objective%2520through%2520a%2520novel%2520reputation%2520scoring%2520method%250Abased%2520on%2520gradient%2520alignment%2520between%2520the%2520local%2520cross-entropy%2520and%2520distillation%250Alosses.%2520We%2520further%2520extend%2520the%2520CYCle%2520protocol%2520to%2520operate%2520on%2520top%2520of%2520gossip-based%250Adecentralized%2520algorithms%2520such%2520as%2520Gossip-SGD.%2520We%2520also%2520theoretically%2520show%2520that%250ACYCle%2520performs%2520better%2520than%2520standard%2520FedAvg%2520in%2520a%2520two-client%2520mean%2520estimation%250Asetting%2520under%2520high%2520heterogeneity.%2520Empirical%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520CYCle%2520protocol%2520to%2520ensure%2520positive%2520and%2520fair%2520collaboration%250Again%2520for%2520all%2520participants%252C%2520even%2520in%2520cases%2520where%2520the%2520data%2520distributions%2520of%250Aparticipants%2520are%2520highly%2520skewed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CYCle%3A%20Choosing%20Your%20Collaborators%20Wisely%20to%20Enhance%20Collaborative%0A%20%20Fairness%20in%20Decentralized%20Learning&entry.906535625=Nurbek%20Tastan%20and%20Samuel%20Horvath%20and%20Karthik%20Nandakumar&entry.1292438233=%20%20Collaborative%20learning%20%28CL%29%20enables%20multiple%20participants%20to%20jointly%20train%0Amachine%20learning%20%28ML%29%20models%20on%20decentralized%20data%20sources%20without%20raw%20data%0Asharing.%20While%20the%20primary%20goal%20of%20CL%20is%20to%20maximize%20the%20expected%20accuracy%20gain%0Afor%20each%20participant%2C%20it%20is%20also%20important%20to%20ensure%20that%20the%20gains%20are%20fairly%0Adistributed%3A%20no%20client%20should%20be%20negatively%20impacted%2C%20and%20gains%20should%20reflect%0Acontributions.%20Most%20existing%20CL%20methods%20require%20central%20coordination%20and%20focus%0Aonly%20on%20gain%20maximization%2C%20overlooking%20fairness.%20In%20this%20work%2C%20we%20first%20show%0Athat%20the%20existing%20measure%20of%20collaborative%20fairness%20based%20on%20the%20correlation%0Abetween%20accuracy%20values%20without%20and%20with%20collaboration%20has%20drawbacks%20because%20it%0Adoes%20not%20account%20for%20negative%20collaboration%20gain.%20We%20argue%20that%20maximizing%20mean%0Acollaboration%20gain%20%28MCG%29%20while%20simultaneously%20minimizing%20the%20collaboration%20gain%0Aspread%20%28CGS%29%20is%20a%20fairer%20alternative.%20Next%2C%20we%20propose%20the%20CYCle%20protocol%20that%0Aenables%20individual%20participants%20in%20a%20private%20decentralized%20learning%20%28PDL%29%0Aframework%20to%20achieve%20this%20objective%20through%20a%20novel%20reputation%20scoring%20method%0Abased%20on%20gradient%20alignment%20between%20the%20local%20cross-entropy%20and%20distillation%0Alosses.%20We%20further%20extend%20the%20CYCle%20protocol%20to%20operate%20on%20top%20of%20gossip-based%0Adecentralized%20algorithms%20such%20as%20Gossip-SGD.%20We%20also%20theoretically%20show%20that%0ACYCle%20performs%20better%20than%20standard%20FedAvg%20in%20a%20two-client%20mean%20estimation%0Asetting%20under%20high%20heterogeneity.%20Empirical%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20CYCle%20protocol%20to%20ensure%20positive%20and%20fair%20collaboration%0Again%20for%20all%20participants%2C%20even%20in%20cases%20where%20the%20data%20distributions%20of%0Aparticipants%20are%20highly%20skewed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12344v2&entry.124074799=Read"},
{"title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct", "author": "Haoyang Zheng and Xinyang Liu and Cindy Xiangrui Kong and Nan Jiang and Zheyuan Hu and Weijian Luo and Wei Deng and Guang Lin", "abstract": "  Fast and high-quality language generation is the holy grail that people\npursue in the age of AI. In this work, we introduce Discrete Diffusion\nDivergence Instruct (DiDi-Instruct), a training-based method that initializes\nfrom a pre-trained (masked) discrete diffusion language model (dLLM) and\ndistills a few-step student for fast generation. The resulting DiDi-Instruct\nmodel achieves comparable or superior performance to its dLLM teacher and the\nGPT-2 baseline while enabling up to 64$\\times$ acceleration. The theoretical\nfoundation of DiDi-Instruct is a novel framework based on integral\nKL-divergence minimization, which yields a practical training algorithm. We\nfurther introduce grouped reward normalization, intermediate-state matching,\nand the reward-guided ancestral sampler that significantly improve training\nstability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct\nachieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms\nprior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible\nentropy loss (around $1\\%$) and reduce additional training wall-clock time by\nmore than $20\\times$ compared to competing dLLM distillation methods. We\nfurther validate the robustness and effectiveness of DiDi-Instruct through\nextensive ablation studies, model scaling, and the generation of discrete\nprotein sequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.\n", "link": "http://arxiv.org/abs/2509.25035v2", "date": "2025-10-01", "relevancy": 1.6853, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5779}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-Fast%20Language%20Generation%20via%20Discrete%20Diffusion%20Divergence%0A%20%20Instruct&body=Title%3A%20Ultra-Fast%20Language%20Generation%20via%20Discrete%20Diffusion%20Divergence%0A%20%20Instruct%0AAuthor%3A%20Haoyang%20Zheng%20and%20Xinyang%20Liu%20and%20Cindy%20Xiangrui%20Kong%20and%20Nan%20Jiang%20and%20Zheyuan%20Hu%20and%20Weijian%20Luo%20and%20Wei%20Deng%20and%20Guang%20Lin%0AAbstract%3A%20%20%20Fast%20and%20high-quality%20language%20generation%20is%20the%20holy%20grail%20that%20people%0Apursue%20in%20the%20age%20of%20AI.%20In%20this%20work%2C%20we%20introduce%20Discrete%20Diffusion%0ADivergence%20Instruct%20%28DiDi-Instruct%29%2C%20a%20training-based%20method%20that%20initializes%0Afrom%20a%20pre-trained%20%28masked%29%20discrete%20diffusion%20language%20model%20%28dLLM%29%20and%0Adistills%20a%20few-step%20student%20for%20fast%20generation.%20The%20resulting%20DiDi-Instruct%0Amodel%20achieves%20comparable%20or%20superior%20performance%20to%20its%20dLLM%20teacher%20and%20the%0AGPT-2%20baseline%20while%20enabling%20up%20to%2064%24%5Ctimes%24%20acceleration.%20The%20theoretical%0Afoundation%20of%20DiDi-Instruct%20is%20a%20novel%20framework%20based%20on%20integral%0AKL-divergence%20minimization%2C%20which%20yields%20a%20practical%20training%20algorithm.%20We%0Afurther%20introduce%20grouped%20reward%20normalization%2C%20intermediate-state%20matching%2C%0Aand%20the%20reward-guided%20ancestral%20sampler%20that%20significantly%20improve%20training%0Astability%2C%20model%20coverage%2C%20and%20inference%20quality.%20On%20OpenWebText%2C%20DiDi-Instruct%0Aachieves%20perplexity%20from%2062.2%20%288%20NFEs%29%20to%2018.4%20%28128%20NFEs%29%2C%20which%20outperforms%0Aprior%20accelerated%20dLLMs%20and%20GPT-2%20baseline.%20These%20gains%20come%20with%20a%20negligible%0Aentropy%20loss%20%28around%20%241%5C%25%24%29%20and%20reduce%20additional%20training%20wall-clock%20time%20by%0Amore%20than%20%2420%5Ctimes%24%20compared%20to%20competing%20dLLM%20distillation%20methods.%20We%0Afurther%20validate%20the%20robustness%20and%20effectiveness%20of%20DiDi-Instruct%20through%0Aextensive%20ablation%20studies%2C%20model%20scaling%2C%20and%20the%20generation%20of%20discrete%0Aprotein%20sequences.%20In%20conclusion%2C%20DiDi-Instruct%20is%20an%20efficient%20yet%20effective%0Adistillation%20method%2C%20enabling%20language%20generation%20in%20the%20blink%20of%20an%20eye.%20We%0Awill%20release%20both%20code%20and%20models%20at%20github.com/haoyangzheng-ai/didi-instruct.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-Fast%2520Language%2520Generation%2520via%2520Discrete%2520Diffusion%2520Divergence%250A%2520%2520Instruct%26entry.906535625%3DHaoyang%2520Zheng%2520and%2520Xinyang%2520Liu%2520and%2520Cindy%2520Xiangrui%2520Kong%2520and%2520Nan%2520Jiang%2520and%2520Zheyuan%2520Hu%2520and%2520Weijian%2520Luo%2520and%2520Wei%2520Deng%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520Fast%2520and%2520high-quality%2520language%2520generation%2520is%2520the%2520holy%2520grail%2520that%2520people%250Apursue%2520in%2520the%2520age%2520of%2520AI.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Discrete%2520Diffusion%250ADivergence%2520Instruct%2520%2528DiDi-Instruct%2529%252C%2520a%2520training-based%2520method%2520that%2520initializes%250Afrom%2520a%2520pre-trained%2520%2528masked%2529%2520discrete%2520diffusion%2520language%2520model%2520%2528dLLM%2529%2520and%250Adistills%2520a%2520few-step%2520student%2520for%2520fast%2520generation.%2520The%2520resulting%2520DiDi-Instruct%250Amodel%2520achieves%2520comparable%2520or%2520superior%2520performance%2520to%2520its%2520dLLM%2520teacher%2520and%2520the%250AGPT-2%2520baseline%2520while%2520enabling%2520up%2520to%252064%2524%255Ctimes%2524%2520acceleration.%2520The%2520theoretical%250Afoundation%2520of%2520DiDi-Instruct%2520is%2520a%2520novel%2520framework%2520based%2520on%2520integral%250AKL-divergence%2520minimization%252C%2520which%2520yields%2520a%2520practical%2520training%2520algorithm.%2520We%250Afurther%2520introduce%2520grouped%2520reward%2520normalization%252C%2520intermediate-state%2520matching%252C%250Aand%2520the%2520reward-guided%2520ancestral%2520sampler%2520that%2520significantly%2520improve%2520training%250Astability%252C%2520model%2520coverage%252C%2520and%2520inference%2520quality.%2520On%2520OpenWebText%252C%2520DiDi-Instruct%250Aachieves%2520perplexity%2520from%252062.2%2520%25288%2520NFEs%2529%2520to%252018.4%2520%2528128%2520NFEs%2529%252C%2520which%2520outperforms%250Aprior%2520accelerated%2520dLLMs%2520and%2520GPT-2%2520baseline.%2520These%2520gains%2520come%2520with%2520a%2520negligible%250Aentropy%2520loss%2520%2528around%2520%25241%255C%2525%2524%2529%2520and%2520reduce%2520additional%2520training%2520wall-clock%2520time%2520by%250Amore%2520than%2520%252420%255Ctimes%2524%2520compared%2520to%2520competing%2520dLLM%2520distillation%2520methods.%2520We%250Afurther%2520validate%2520the%2520robustness%2520and%2520effectiveness%2520of%2520DiDi-Instruct%2520through%250Aextensive%2520ablation%2520studies%252C%2520model%2520scaling%252C%2520and%2520the%2520generation%2520of%2520discrete%250Aprotein%2520sequences.%2520In%2520conclusion%252C%2520DiDi-Instruct%2520is%2520an%2520efficient%2520yet%2520effective%250Adistillation%2520method%252C%2520enabling%2520language%2520generation%2520in%2520the%2520blink%2520of%2520an%2520eye.%2520We%250Awill%2520release%2520both%2520code%2520and%2520models%2520at%2520github.com/haoyangzheng-ai/didi-instruct.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-Fast%20Language%20Generation%20via%20Discrete%20Diffusion%20Divergence%0A%20%20Instruct&entry.906535625=Haoyang%20Zheng%20and%20Xinyang%20Liu%20and%20Cindy%20Xiangrui%20Kong%20and%20Nan%20Jiang%20and%20Zheyuan%20Hu%20and%20Weijian%20Luo%20and%20Wei%20Deng%20and%20Guang%20Lin&entry.1292438233=%20%20Fast%20and%20high-quality%20language%20generation%20is%20the%20holy%20grail%20that%20people%0Apursue%20in%20the%20age%20of%20AI.%20In%20this%20work%2C%20we%20introduce%20Discrete%20Diffusion%0ADivergence%20Instruct%20%28DiDi-Instruct%29%2C%20a%20training-based%20method%20that%20initializes%0Afrom%20a%20pre-trained%20%28masked%29%20discrete%20diffusion%20language%20model%20%28dLLM%29%20and%0Adistills%20a%20few-step%20student%20for%20fast%20generation.%20The%20resulting%20DiDi-Instruct%0Amodel%20achieves%20comparable%20or%20superior%20performance%20to%20its%20dLLM%20teacher%20and%20the%0AGPT-2%20baseline%20while%20enabling%20up%20to%2064%24%5Ctimes%24%20acceleration.%20The%20theoretical%0Afoundation%20of%20DiDi-Instruct%20is%20a%20novel%20framework%20based%20on%20integral%0AKL-divergence%20minimization%2C%20which%20yields%20a%20practical%20training%20algorithm.%20We%0Afurther%20introduce%20grouped%20reward%20normalization%2C%20intermediate-state%20matching%2C%0Aand%20the%20reward-guided%20ancestral%20sampler%20that%20significantly%20improve%20training%0Astability%2C%20model%20coverage%2C%20and%20inference%20quality.%20On%20OpenWebText%2C%20DiDi-Instruct%0Aachieves%20perplexity%20from%2062.2%20%288%20NFEs%29%20to%2018.4%20%28128%20NFEs%29%2C%20which%20outperforms%0Aprior%20accelerated%20dLLMs%20and%20GPT-2%20baseline.%20These%20gains%20come%20with%20a%20negligible%0Aentropy%20loss%20%28around%20%241%5C%25%24%29%20and%20reduce%20additional%20training%20wall-clock%20time%20by%0Amore%20than%20%2420%5Ctimes%24%20compared%20to%20competing%20dLLM%20distillation%20methods.%20We%0Afurther%20validate%20the%20robustness%20and%20effectiveness%20of%20DiDi-Instruct%20through%0Aextensive%20ablation%20studies%2C%20model%20scaling%2C%20and%20the%20generation%20of%20discrete%0Aprotein%20sequences.%20In%20conclusion%2C%20DiDi-Instruct%20is%20an%20efficient%20yet%20effective%0Adistillation%20method%2C%20enabling%20language%20generation%20in%20the%20blink%20of%20an%20eye.%20We%0Awill%20release%20both%20code%20and%20models%20at%20github.com/haoyangzheng-ai/didi-instruct.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25035v2&entry.124074799=Read"},
{"title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization\n  Perspective", "author": "Lianghe Shi and Meng Wu and Huijie Zhang and Zekai Zhang and Molei Tao and Qing Qu", "abstract": "  The widespread use of diffusion models has led to an abundance of\nAI-generated data, raising concerns about model collapse -- a phenomenon in\nwhich recursive iterations of training on synthetic data lead to performance\ndegradation. Prior work primarily characterizes this collapse via variance\nshrinkage or distribution shift, but these perspectives miss practical\nmanifestations of model collapse. This paper identifies a transition from\ngeneralization to memorization during model collapse in diffusion models, where\nmodels increasingly replicate training data instead of generating novel content\nduring iterative training on synthetic samples. This transition is directly\ndriven by the declining entropy of the synthetic training data produced in each\ntraining cycle, which serves as a clear indicator of model degradation.\nMotivated by this insight, we propose an entropy-based data selection strategy\nto mitigate the transition from generalization to memorization and alleviate\nmodel collapse. Empirical results show that our approach significantly enhances\nvisual quality and diversity in recursive generation, effectively preventing\ncollapse.\n", "link": "http://arxiv.org/abs/2509.16499v2", "date": "2025-10-01", "relevancy": 1.6807, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5705}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5636}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20Model%20Collapse%3A%20From%20a%20Generalization-to-Memorization%0A%20%20Perspective&body=Title%3A%20A%20Closer%20Look%20at%20Model%20Collapse%3A%20From%20a%20Generalization-to-Memorization%0A%20%20Perspective%0AAuthor%3A%20Lianghe%20Shi%20and%20Meng%20Wu%20and%20Huijie%20Zhang%20and%20Zekai%20Zhang%20and%20Molei%20Tao%20and%20Qing%20Qu%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20diffusion%20models%20has%20led%20to%20an%20abundance%20of%0AAI-generated%20data%2C%20raising%20concerns%20about%20model%20collapse%20--%20a%20phenomenon%20in%0Awhich%20recursive%20iterations%20of%20training%20on%20synthetic%20data%20lead%20to%20performance%0Adegradation.%20Prior%20work%20primarily%20characterizes%20this%20collapse%20via%20variance%0Ashrinkage%20or%20distribution%20shift%2C%20but%20these%20perspectives%20miss%20practical%0Amanifestations%20of%20model%20collapse.%20This%20paper%20identifies%20a%20transition%20from%0Ageneralization%20to%20memorization%20during%20model%20collapse%20in%20diffusion%20models%2C%20where%0Amodels%20increasingly%20replicate%20training%20data%20instead%20of%20generating%20novel%20content%0Aduring%20iterative%20training%20on%20synthetic%20samples.%20This%20transition%20is%20directly%0Adriven%20by%20the%20declining%20entropy%20of%20the%20synthetic%20training%20data%20produced%20in%20each%0Atraining%20cycle%2C%20which%20serves%20as%20a%20clear%20indicator%20of%20model%20degradation.%0AMotivated%20by%20this%20insight%2C%20we%20propose%20an%20entropy-based%20data%20selection%20strategy%0Ato%20mitigate%20the%20transition%20from%20generalization%20to%20memorization%20and%20alleviate%0Amodel%20collapse.%20Empirical%20results%20show%20that%20our%20approach%20significantly%20enhances%0Avisual%20quality%20and%20diversity%20in%20recursive%20generation%2C%20effectively%20preventing%0Acollapse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16499v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520Model%2520Collapse%253A%2520From%2520a%2520Generalization-to-Memorization%250A%2520%2520Perspective%26entry.906535625%3DLianghe%2520Shi%2520and%2520Meng%2520Wu%2520and%2520Huijie%2520Zhang%2520and%2520Zekai%2520Zhang%2520and%2520Molei%2520Tao%2520and%2520Qing%2520Qu%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520diffusion%2520models%2520has%2520led%2520to%2520an%2520abundance%2520of%250AAI-generated%2520data%252C%2520raising%2520concerns%2520about%2520model%2520collapse%2520--%2520a%2520phenomenon%2520in%250Awhich%2520recursive%2520iterations%2520of%2520training%2520on%2520synthetic%2520data%2520lead%2520to%2520performance%250Adegradation.%2520Prior%2520work%2520primarily%2520characterizes%2520this%2520collapse%2520via%2520variance%250Ashrinkage%2520or%2520distribution%2520shift%252C%2520but%2520these%2520perspectives%2520miss%2520practical%250Amanifestations%2520of%2520model%2520collapse.%2520This%2520paper%2520identifies%2520a%2520transition%2520from%250Ageneralization%2520to%2520memorization%2520during%2520model%2520collapse%2520in%2520diffusion%2520models%252C%2520where%250Amodels%2520increasingly%2520replicate%2520training%2520data%2520instead%2520of%2520generating%2520novel%2520content%250Aduring%2520iterative%2520training%2520on%2520synthetic%2520samples.%2520This%2520transition%2520is%2520directly%250Adriven%2520by%2520the%2520declining%2520entropy%2520of%2520the%2520synthetic%2520training%2520data%2520produced%2520in%2520each%250Atraining%2520cycle%252C%2520which%2520serves%2520as%2520a%2520clear%2520indicator%2520of%2520model%2520degradation.%250AMotivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520an%2520entropy-based%2520data%2520selection%2520strategy%250Ato%2520mitigate%2520the%2520transition%2520from%2520generalization%2520to%2520memorization%2520and%2520alleviate%250Amodel%2520collapse.%2520Empirical%2520results%2520show%2520that%2520our%2520approach%2520significantly%2520enhances%250Avisual%2520quality%2520and%2520diversity%2520in%2520recursive%2520generation%252C%2520effectively%2520preventing%250Acollapse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16499v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20Model%20Collapse%3A%20From%20a%20Generalization-to-Memorization%0A%20%20Perspective&entry.906535625=Lianghe%20Shi%20and%20Meng%20Wu%20and%20Huijie%20Zhang%20and%20Zekai%20Zhang%20and%20Molei%20Tao%20and%20Qing%20Qu&entry.1292438233=%20%20The%20widespread%20use%20of%20diffusion%20models%20has%20led%20to%20an%20abundance%20of%0AAI-generated%20data%2C%20raising%20concerns%20about%20model%20collapse%20--%20a%20phenomenon%20in%0Awhich%20recursive%20iterations%20of%20training%20on%20synthetic%20data%20lead%20to%20performance%0Adegradation.%20Prior%20work%20primarily%20characterizes%20this%20collapse%20via%20variance%0Ashrinkage%20or%20distribution%20shift%2C%20but%20these%20perspectives%20miss%20practical%0Amanifestations%20of%20model%20collapse.%20This%20paper%20identifies%20a%20transition%20from%0Ageneralization%20to%20memorization%20during%20model%20collapse%20in%20diffusion%20models%2C%20where%0Amodels%20increasingly%20replicate%20training%20data%20instead%20of%20generating%20novel%20content%0Aduring%20iterative%20training%20on%20synthetic%20samples.%20This%20transition%20is%20directly%0Adriven%20by%20the%20declining%20entropy%20of%20the%20synthetic%20training%20data%20produced%20in%20each%0Atraining%20cycle%2C%20which%20serves%20as%20a%20clear%20indicator%20of%20model%20degradation.%0AMotivated%20by%20this%20insight%2C%20we%20propose%20an%20entropy-based%20data%20selection%20strategy%0Ato%20mitigate%20the%20transition%20from%20generalization%20to%20memorization%20and%20alleviate%0Amodel%20collapse.%20Empirical%20results%20show%20that%20our%20approach%20significantly%20enhances%0Avisual%20quality%20and%20diversity%20in%20recursive%20generation%2C%20effectively%20preventing%0Acollapse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16499v2&entry.124074799=Read"},
{"title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "author": "James R. Williamson and Andrew Alini and Brian A. Telfer and Adam W. Potter and Karl E. Friedl", "abstract": "  Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.\n", "link": "http://arxiv.org/abs/2506.09167v2", "date": "2025-10-01", "relevancy": 1.669, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4402}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4388}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.3865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Visceral%20Adiposity%20from%20Wrist-Worn%20Accelerometry&body=Title%3A%20Estimating%20Visceral%20Adiposity%20from%20Wrist-Worn%20Accelerometry%0AAuthor%3A%20James%20R.%20Williamson%20and%20Andrew%20Alini%20and%20Brian%20A.%20Telfer%20and%20Adam%20W.%20Potter%20and%20Karl%20E.%20Friedl%0AAbstract%3A%20%20%20Visceral%20adipose%20tissue%20%28VAT%29%20is%20a%20key%20marker%20of%20both%20metabolic%20health%20and%0Ahabitual%20physical%20activity%20%28PA%29.%20Excess%20VAT%20is%20highly%20correlated%20with%20type%202%0Adiabetes%20and%20insulin%20resistance.%20The%20mechanistic%20basis%20for%20this%20pathophysiology%0Arelates%20to%20overloading%20the%20liver%20with%20fatty%20acids.%20VAT%20is%20also%20a%20highly%20labile%0Afat%20depot%2C%20with%20increased%20turnover%20stimulated%20by%20catecholamines%20during%0Aexercise.%20VAT%20can%20be%20measured%20with%20sophisticated%20imaging%20technologies%2C%20but%20can%0Aalso%20be%20inferred%20directly%20from%20PA.%20We%20tested%20this%20relationship%20using%20National%0AHealth%20and%20Nutrition%20Examination%20Survey%20%28NHANES%29%20data%20from%202011-2014%2C%20for%0Aindividuals%20aged%2020-60%20years%20with%207%20days%20of%20accelerometry%20data%20%28n%3D2%2C456%20men%3B%0A2%2C427%20women%29%20%5B1%5D.%20Two%20approaches%20were%20used%20for%20estimating%20VAT%20from%20activity.%0AThe%20first%20used%20engineered%20features%20based%20on%20movements%20during%20gait%20and%20sleep%2C%0Aand%20then%20ridge%20regression%20to%20map%20summary%20statistics%20of%20these%20features%20into%20a%0AVAT%20estimate.%20The%20second%20approach%20used%20deep%20neural%20networks%20trained%20on%2024%20hours%0Aof%20continuous%20accelerometry.%20A%20foundation%20model%20first%20mapped%20each%2010s%20frame%0Ainto%20a%20high-dimensional%20feature%20vector.%20A%20transformer%20model%20then%20mapped%20each%0Aday%27s%20feature%20vector%20time%20series%20into%20a%20VAT%20estimate%2C%20which%20were%20averaged%20over%0Amultiple%20days.%20For%20both%20approaches%2C%20the%20most%20accurate%20estimates%20were%20obtained%0Awith%20the%20addition%20of%20covariate%20information%20about%20subject%20demographics%20and%20body%0Ameasurements.%20The%20best%20performance%20was%20obtained%20by%20combining%20the%20two%0Aapproaches%2C%20resulting%20in%20VAT%20estimates%20with%20correlations%20of%20r%3D0.86.%20These%0Afindings%20demonstrate%20a%20strong%20relationship%20between%20PA%20and%20VAT%20and%2C%20by%0Aextension%2C%20between%20PA%20and%20metabolic%20health%20risks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Visceral%2520Adiposity%2520from%2520Wrist-Worn%2520Accelerometry%26entry.906535625%3DJames%2520R.%2520Williamson%2520and%2520Andrew%2520Alini%2520and%2520Brian%2520A.%2520Telfer%2520and%2520Adam%2520W.%2520Potter%2520and%2520Karl%2520E.%2520Friedl%26entry.1292438233%3D%2520%2520Visceral%2520adipose%2520tissue%2520%2528VAT%2529%2520is%2520a%2520key%2520marker%2520of%2520both%2520metabolic%2520health%2520and%250Ahabitual%2520physical%2520activity%2520%2528PA%2529.%2520Excess%2520VAT%2520is%2520highly%2520correlated%2520with%2520type%25202%250Adiabetes%2520and%2520insulin%2520resistance.%2520The%2520mechanistic%2520basis%2520for%2520this%2520pathophysiology%250Arelates%2520to%2520overloading%2520the%2520liver%2520with%2520fatty%2520acids.%2520VAT%2520is%2520also%2520a%2520highly%2520labile%250Afat%2520depot%252C%2520with%2520increased%2520turnover%2520stimulated%2520by%2520catecholamines%2520during%250Aexercise.%2520VAT%2520can%2520be%2520measured%2520with%2520sophisticated%2520imaging%2520technologies%252C%2520but%2520can%250Aalso%2520be%2520inferred%2520directly%2520from%2520PA.%2520We%2520tested%2520this%2520relationship%2520using%2520National%250AHealth%2520and%2520Nutrition%2520Examination%2520Survey%2520%2528NHANES%2529%2520data%2520from%25202011-2014%252C%2520for%250Aindividuals%2520aged%252020-60%2520years%2520with%25207%2520days%2520of%2520accelerometry%2520data%2520%2528n%253D2%252C456%2520men%253B%250A2%252C427%2520women%2529%2520%255B1%255D.%2520Two%2520approaches%2520were%2520used%2520for%2520estimating%2520VAT%2520from%2520activity.%250AThe%2520first%2520used%2520engineered%2520features%2520based%2520on%2520movements%2520during%2520gait%2520and%2520sleep%252C%250Aand%2520then%2520ridge%2520regression%2520to%2520map%2520summary%2520statistics%2520of%2520these%2520features%2520into%2520a%250AVAT%2520estimate.%2520The%2520second%2520approach%2520used%2520deep%2520neural%2520networks%2520trained%2520on%252024%2520hours%250Aof%2520continuous%2520accelerometry.%2520A%2520foundation%2520model%2520first%2520mapped%2520each%252010s%2520frame%250Ainto%2520a%2520high-dimensional%2520feature%2520vector.%2520A%2520transformer%2520model%2520then%2520mapped%2520each%250Aday%2527s%2520feature%2520vector%2520time%2520series%2520into%2520a%2520VAT%2520estimate%252C%2520which%2520were%2520averaged%2520over%250Amultiple%2520days.%2520For%2520both%2520approaches%252C%2520the%2520most%2520accurate%2520estimates%2520were%2520obtained%250Awith%2520the%2520addition%2520of%2520covariate%2520information%2520about%2520subject%2520demographics%2520and%2520body%250Ameasurements.%2520The%2520best%2520performance%2520was%2520obtained%2520by%2520combining%2520the%2520two%250Aapproaches%252C%2520resulting%2520in%2520VAT%2520estimates%2520with%2520correlations%2520of%2520r%253D0.86.%2520These%250Afindings%2520demonstrate%2520a%2520strong%2520relationship%2520between%2520PA%2520and%2520VAT%2520and%252C%2520by%250Aextension%252C%2520between%2520PA%2520and%2520metabolic%2520health%2520risks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Visceral%20Adiposity%20from%20Wrist-Worn%20Accelerometry&entry.906535625=James%20R.%20Williamson%20and%20Andrew%20Alini%20and%20Brian%20A.%20Telfer%20and%20Adam%20W.%20Potter%20and%20Karl%20E.%20Friedl&entry.1292438233=%20%20Visceral%20adipose%20tissue%20%28VAT%29%20is%20a%20key%20marker%20of%20both%20metabolic%20health%20and%0Ahabitual%20physical%20activity%20%28PA%29.%20Excess%20VAT%20is%20highly%20correlated%20with%20type%202%0Adiabetes%20and%20insulin%20resistance.%20The%20mechanistic%20basis%20for%20this%20pathophysiology%0Arelates%20to%20overloading%20the%20liver%20with%20fatty%20acids.%20VAT%20is%20also%20a%20highly%20labile%0Afat%20depot%2C%20with%20increased%20turnover%20stimulated%20by%20catecholamines%20during%0Aexercise.%20VAT%20can%20be%20measured%20with%20sophisticated%20imaging%20technologies%2C%20but%20can%0Aalso%20be%20inferred%20directly%20from%20PA.%20We%20tested%20this%20relationship%20using%20National%0AHealth%20and%20Nutrition%20Examination%20Survey%20%28NHANES%29%20data%20from%202011-2014%2C%20for%0Aindividuals%20aged%2020-60%20years%20with%207%20days%20of%20accelerometry%20data%20%28n%3D2%2C456%20men%3B%0A2%2C427%20women%29%20%5B1%5D.%20Two%20approaches%20were%20used%20for%20estimating%20VAT%20from%20activity.%0AThe%20first%20used%20engineered%20features%20based%20on%20movements%20during%20gait%20and%20sleep%2C%0Aand%20then%20ridge%20regression%20to%20map%20summary%20statistics%20of%20these%20features%20into%20a%0AVAT%20estimate.%20The%20second%20approach%20used%20deep%20neural%20networks%20trained%20on%2024%20hours%0Aof%20continuous%20accelerometry.%20A%20foundation%20model%20first%20mapped%20each%2010s%20frame%0Ainto%20a%20high-dimensional%20feature%20vector.%20A%20transformer%20model%20then%20mapped%20each%0Aday%27s%20feature%20vector%20time%20series%20into%20a%20VAT%20estimate%2C%20which%20were%20averaged%20over%0Amultiple%20days.%20For%20both%20approaches%2C%20the%20most%20accurate%20estimates%20were%20obtained%0Awith%20the%20addition%20of%20covariate%20information%20about%20subject%20demographics%20and%20body%0Ameasurements.%20The%20best%20performance%20was%20obtained%20by%20combining%20the%20two%0Aapproaches%2C%20resulting%20in%20VAT%20estimates%20with%20correlations%20of%20r%3D0.86.%20These%0Afindings%20demonstrate%20a%20strong%20relationship%20between%20PA%20and%20VAT%20and%2C%20by%0Aextension%2C%20between%20PA%20and%20metabolic%20health%20risks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09167v2&entry.124074799=Read"},
{"title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration", "author": "Jintao Zhang and Jia Wei and Haofeng Huang and Pengle Zhang and Jun Zhu and Jianfei Chen", "abstract": "  The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.\n", "link": "http://arxiv.org/abs/2410.02367v9", "date": "2025-10-01", "relevancy": 1.6633, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.571}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5366}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SageAttention%3A%20Accurate%208-Bit%20Attention%20for%20Plug-and-play%20Inference%0A%20%20Acceleration&body=Title%3A%20SageAttention%3A%20Accurate%208-Bit%20Attention%20for%20Plug-and-play%20Inference%0A%20%20Acceleration%0AAuthor%3A%20Jintao%20Zhang%20and%20Jia%20Wei%20and%20Haofeng%20Huang%20and%20Pengle%20Zhang%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20The%20transformer%20architecture%20predominates%20across%20various%20models.%20As%20the%20heart%0Aof%20the%20transformer%2C%20attention%20has%20a%20computational%20complexity%20of%20%24O%28N%5E2%29%24%2C%0Acompared%20to%20%24O%28N%29%24%20for%20linear%20transformations.%20When%20handling%20large%20sequence%0Alengths%2C%20attention%20becomes%20the%20primary%20time-consuming%20component.%20Although%0Aquantization%20has%20proven%20to%20be%20an%20effective%20method%20for%20accelerating%20model%0Ainference%2C%20existing%20quantization%20methods%20primarily%20focus%20on%20optimizing%20the%0Alinear%20layer.%20In%20response%2C%20we%20first%20analyze%20the%20feasibility%20of%20quantization%20in%0Aattention%20detailedly.%20Following%20that%2C%20we%20propose%20SageAttention%2C%20a%20highly%0Aefficient%20and%20accurate%20quantization%20method%20for%20attention.%20The%20OPS%20%28operations%0Aper%20second%29%20of%20our%20approach%20outperforms%20FlashAttention2%20and%20xformers%20by%20about%0A2.1%20times%20and%202.7%20times%2C%20respectively.%20SageAttention%20also%20achieves%20superior%0Aaccuracy%20performance%20over%20FlashAttention3.%20Comprehensive%20experiments%20confirm%0Athat%20our%20approach%20incurs%20almost%20no%20end-to-end%20metrics%20loss%20across%20diverse%0Amodels%2C%20including%20those%20for%20large%20language%20processing%2C%20image%20generation%2C%20and%0Avideo%20generation.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SageAttention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02367v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSageAttention%253A%2520Accurate%25208-Bit%2520Attention%2520for%2520Plug-and-play%2520Inference%250A%2520%2520Acceleration%26entry.906535625%3DJintao%2520Zhang%2520and%2520Jia%2520Wei%2520and%2520Haofeng%2520Huang%2520and%2520Pengle%2520Zhang%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520The%2520transformer%2520architecture%2520predominates%2520across%2520various%2520models.%2520As%2520the%2520heart%250Aof%2520the%2520transformer%252C%2520attention%2520has%2520a%2520computational%2520complexity%2520of%2520%2524O%2528N%255E2%2529%2524%252C%250Acompared%2520to%2520%2524O%2528N%2529%2524%2520for%2520linear%2520transformations.%2520When%2520handling%2520large%2520sequence%250Alengths%252C%2520attention%2520becomes%2520the%2520primary%2520time-consuming%2520component.%2520Although%250Aquantization%2520has%2520proven%2520to%2520be%2520an%2520effective%2520method%2520for%2520accelerating%2520model%250Ainference%252C%2520existing%2520quantization%2520methods%2520primarily%2520focus%2520on%2520optimizing%2520the%250Alinear%2520layer.%2520In%2520response%252C%2520we%2520first%2520analyze%2520the%2520feasibility%2520of%2520quantization%2520in%250Aattention%2520detailedly.%2520Following%2520that%252C%2520we%2520propose%2520SageAttention%252C%2520a%2520highly%250Aefficient%2520and%2520accurate%2520quantization%2520method%2520for%2520attention.%2520The%2520OPS%2520%2528operations%250Aper%2520second%2529%2520of%2520our%2520approach%2520outperforms%2520FlashAttention2%2520and%2520xformers%2520by%2520about%250A2.1%2520times%2520and%25202.7%2520times%252C%2520respectively.%2520SageAttention%2520also%2520achieves%2520superior%250Aaccuracy%2520performance%2520over%2520FlashAttention3.%2520Comprehensive%2520experiments%2520confirm%250Athat%2520our%2520approach%2520incurs%2520almost%2520no%2520end-to-end%2520metrics%2520loss%2520across%2520diverse%250Amodels%252C%2520including%2520those%2520for%2520large%2520language%2520processing%252C%2520image%2520generation%252C%2520and%250Avideo%2520generation.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/thu-ml/SageAttention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02367v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SageAttention%3A%20Accurate%208-Bit%20Attention%20for%20Plug-and-play%20Inference%0A%20%20Acceleration&entry.906535625=Jintao%20Zhang%20and%20Jia%20Wei%20and%20Haofeng%20Huang%20and%20Pengle%20Zhang%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20The%20transformer%20architecture%20predominates%20across%20various%20models.%20As%20the%20heart%0Aof%20the%20transformer%2C%20attention%20has%20a%20computational%20complexity%20of%20%24O%28N%5E2%29%24%2C%0Acompared%20to%20%24O%28N%29%24%20for%20linear%20transformations.%20When%20handling%20large%20sequence%0Alengths%2C%20attention%20becomes%20the%20primary%20time-consuming%20component.%20Although%0Aquantization%20has%20proven%20to%20be%20an%20effective%20method%20for%20accelerating%20model%0Ainference%2C%20existing%20quantization%20methods%20primarily%20focus%20on%20optimizing%20the%0Alinear%20layer.%20In%20response%2C%20we%20first%20analyze%20the%20feasibility%20of%20quantization%20in%0Aattention%20detailedly.%20Following%20that%2C%20we%20propose%20SageAttention%2C%20a%20highly%0Aefficient%20and%20accurate%20quantization%20method%20for%20attention.%20The%20OPS%20%28operations%0Aper%20second%29%20of%20our%20approach%20outperforms%20FlashAttention2%20and%20xformers%20by%20about%0A2.1%20times%20and%202.7%20times%2C%20respectively.%20SageAttention%20also%20achieves%20superior%0Aaccuracy%20performance%20over%20FlashAttention3.%20Comprehensive%20experiments%20confirm%0Athat%20our%20approach%20incurs%20almost%20no%20end-to-end%20metrics%20loss%20across%20diverse%0Amodels%2C%20including%20those%20for%20large%20language%20processing%2C%20image%20generation%2C%20and%0Avideo%20generation.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SageAttention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02367v9&entry.124074799=Read"},
{"title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving", "author": "Zebin Xing and Xingyu Zhang and Yang Hu and Bo Jiang and Tong He and Qian Zhang and Xiaoxiao Long and Wei Yin", "abstract": "  We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the Navsim\\cite{Dauner2024_navsim},\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.\n", "link": "http://arxiv.org/abs/2503.05689v6", "date": "2025-10-01", "relevancy": 1.6524, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.556}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GoalFlow%3A%20Goal-Driven%20Flow%20Matching%20for%20Multimodal%20Trajectories%0A%20%20Generation%20in%20End-to-End%20Autonomous%20Driving&body=Title%3A%20GoalFlow%3A%20Goal-Driven%20Flow%20Matching%20for%20Multimodal%20Trajectories%0A%20%20Generation%20in%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Zebin%20Xing%20and%20Xingyu%20Zhang%20and%20Yang%20Hu%20and%20Bo%20Jiang%20and%20Tong%20He%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long%20and%20Wei%20Yin%0AAbstract%3A%20%20%20We%20propose%20GoalFlow%2C%20an%20end-to-end%20autonomous%20driving%20method%20for%20generating%0Ahigh-quality%20multimodal%20trajectories.%20In%20autonomous%20driving%20scenarios%2C%20there%20is%0Ararely%20a%20single%20suitable%20trajectory.%20Recent%20methods%20have%20increasingly%20focused%0Aon%20modeling%20multimodal%20trajectory%20distributions.%20However%2C%20they%20suffer%20from%0Atrajectory%20selection%20complexity%20and%20reduced%20trajectory%20quality%20due%20to%20high%0Atrajectory%20divergence%20and%20inconsistencies%20between%20guidance%20and%20scene%0Ainformation.%20To%20address%20these%20issues%2C%20we%20introduce%20GoalFlow%2C%20a%20novel%20method%0Athat%20effectively%20constrains%20the%20generative%20process%20to%20produce%20high-quality%2C%0Amultimodal%20trajectories.%20To%20resolve%20the%20trajectory%20divergence%20problem%20inherent%0Ain%20diffusion-based%20methods%2C%20GoalFlow%20constrains%20the%20generated%20trajectories%20by%0Aintroducing%20a%20goal%20point.%20GoalFlow%20establishes%20a%20novel%20scoring%20mechanism%20that%0Aselects%20the%20most%20appropriate%20goal%20point%20from%20the%20candidate%20points%20based%20on%0Ascene%20information.%20Furthermore%2C%20GoalFlow%20employs%20an%20efficient%20generative%0Amethod%2C%20Flow%20Matching%2C%20to%20generate%20multimodal%20trajectories%2C%20and%20incorporates%20a%0Arefined%20scoring%20mechanism%20to%20select%20the%20optimal%20trajectory%20from%20the%20candidates.%0AOur%20experimental%20results%2C%20validated%20on%20the%20Navsim%5Ccite%7BDauner2024_navsim%7D%2C%0Ademonstrate%20that%20GoalFlow%20achieves%20state-of-the-art%20performance%2C%20delivering%0Arobust%20multimodal%20trajectories%20for%20autonomous%20driving.%20GoalFlow%20achieved%20PDMS%0Aof%2090.3%2C%20significantly%20surpassing%20other%20methods.%20Compared%20with%20other%0Adiffusion-policy-based%20methods%2C%20our%20approach%20requires%20only%20a%20single%20denoising%0Astep%20to%20obtain%20excellent%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/YvanYin/GoalFlow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05689v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoalFlow%253A%2520Goal-Driven%2520Flow%2520Matching%2520for%2520Multimodal%2520Trajectories%250A%2520%2520Generation%2520in%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DZebin%2520Xing%2520and%2520Xingyu%2520Zhang%2520and%2520Yang%2520Hu%2520and%2520Bo%2520Jiang%2520and%2520Tong%2520He%2520and%2520Qian%2520Zhang%2520and%2520Xiaoxiao%2520Long%2520and%2520Wei%2520Yin%26entry.1292438233%3D%2520%2520We%2520propose%2520GoalFlow%252C%2520an%2520end-to-end%2520autonomous%2520driving%2520method%2520for%2520generating%250Ahigh-quality%2520multimodal%2520trajectories.%2520In%2520autonomous%2520driving%2520scenarios%252C%2520there%2520is%250Ararely%2520a%2520single%2520suitable%2520trajectory.%2520Recent%2520methods%2520have%2520increasingly%2520focused%250Aon%2520modeling%2520multimodal%2520trajectory%2520distributions.%2520However%252C%2520they%2520suffer%2520from%250Atrajectory%2520selection%2520complexity%2520and%2520reduced%2520trajectory%2520quality%2520due%2520to%2520high%250Atrajectory%2520divergence%2520and%2520inconsistencies%2520between%2520guidance%2520and%2520scene%250Ainformation.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520GoalFlow%252C%2520a%2520novel%2520method%250Athat%2520effectively%2520constrains%2520the%2520generative%2520process%2520to%2520produce%2520high-quality%252C%250Amultimodal%2520trajectories.%2520To%2520resolve%2520the%2520trajectory%2520divergence%2520problem%2520inherent%250Ain%2520diffusion-based%2520methods%252C%2520GoalFlow%2520constrains%2520the%2520generated%2520trajectories%2520by%250Aintroducing%2520a%2520goal%2520point.%2520GoalFlow%2520establishes%2520a%2520novel%2520scoring%2520mechanism%2520that%250Aselects%2520the%2520most%2520appropriate%2520goal%2520point%2520from%2520the%2520candidate%2520points%2520based%2520on%250Ascene%2520information.%2520Furthermore%252C%2520GoalFlow%2520employs%2520an%2520efficient%2520generative%250Amethod%252C%2520Flow%2520Matching%252C%2520to%2520generate%2520multimodal%2520trajectories%252C%2520and%2520incorporates%2520a%250Arefined%2520scoring%2520mechanism%2520to%2520select%2520the%2520optimal%2520trajectory%2520from%2520the%2520candidates.%250AOur%2520experimental%2520results%252C%2520validated%2520on%2520the%2520Navsim%255Ccite%257BDauner2024_navsim%257D%252C%250Ademonstrate%2520that%2520GoalFlow%2520achieves%2520state-of-the-art%2520performance%252C%2520delivering%250Arobust%2520multimodal%2520trajectories%2520for%2520autonomous%2520driving.%2520GoalFlow%2520achieved%2520PDMS%250Aof%252090.3%252C%2520significantly%2520surpassing%2520other%2520methods.%2520Compared%2520with%2520other%250Adiffusion-policy-based%2520methods%252C%2520our%2520approach%2520requires%2520only%2520a%2520single%2520denoising%250Astep%2520to%2520obtain%2520excellent%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/YvanYin/GoalFlow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05689v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GoalFlow%3A%20Goal-Driven%20Flow%20Matching%20for%20Multimodal%20Trajectories%0A%20%20Generation%20in%20End-to-End%20Autonomous%20Driving&entry.906535625=Zebin%20Xing%20and%20Xingyu%20Zhang%20and%20Yang%20Hu%20and%20Bo%20Jiang%20and%20Tong%20He%20and%20Qian%20Zhang%20and%20Xiaoxiao%20Long%20and%20Wei%20Yin&entry.1292438233=%20%20We%20propose%20GoalFlow%2C%20an%20end-to-end%20autonomous%20driving%20method%20for%20generating%0Ahigh-quality%20multimodal%20trajectories.%20In%20autonomous%20driving%20scenarios%2C%20there%20is%0Ararely%20a%20single%20suitable%20trajectory.%20Recent%20methods%20have%20increasingly%20focused%0Aon%20modeling%20multimodal%20trajectory%20distributions.%20However%2C%20they%20suffer%20from%0Atrajectory%20selection%20complexity%20and%20reduced%20trajectory%20quality%20due%20to%20high%0Atrajectory%20divergence%20and%20inconsistencies%20between%20guidance%20and%20scene%0Ainformation.%20To%20address%20these%20issues%2C%20we%20introduce%20GoalFlow%2C%20a%20novel%20method%0Athat%20effectively%20constrains%20the%20generative%20process%20to%20produce%20high-quality%2C%0Amultimodal%20trajectories.%20To%20resolve%20the%20trajectory%20divergence%20problem%20inherent%0Ain%20diffusion-based%20methods%2C%20GoalFlow%20constrains%20the%20generated%20trajectories%20by%0Aintroducing%20a%20goal%20point.%20GoalFlow%20establishes%20a%20novel%20scoring%20mechanism%20that%0Aselects%20the%20most%20appropriate%20goal%20point%20from%20the%20candidate%20points%20based%20on%0Ascene%20information.%20Furthermore%2C%20GoalFlow%20employs%20an%20efficient%20generative%0Amethod%2C%20Flow%20Matching%2C%20to%20generate%20multimodal%20trajectories%2C%20and%20incorporates%20a%0Arefined%20scoring%20mechanism%20to%20select%20the%20optimal%20trajectory%20from%20the%20candidates.%0AOur%20experimental%20results%2C%20validated%20on%20the%20Navsim%5Ccite%7BDauner2024_navsim%7D%2C%0Ademonstrate%20that%20GoalFlow%20achieves%20state-of-the-art%20performance%2C%20delivering%0Arobust%20multimodal%20trajectories%20for%20autonomous%20driving.%20GoalFlow%20achieved%20PDMS%0Aof%2090.3%2C%20significantly%20surpassing%20other%20methods.%20Compared%20with%20other%0Adiffusion-policy-based%20methods%2C%20our%20approach%20requires%20only%20a%20single%20denoising%0Astep%20to%20obtain%20excellent%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/YvanYin/GoalFlow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05689v6&entry.124074799=Read"},
{"title": "Balancing Multimodal Training Through Game-Theoretic Regularization", "author": "Konstantinos Kontras and Thomas Strypsteen and Christos Chatzichristos and Paul Pu Liang and Matthew Blaschko and Maarten De Vos", "abstract": "  Multimodal learning holds promise for richer information extraction by\ncapturing dependencies across data sources. Yet, current training methods often\nunderperform due to modality competition, a phenomenon where modalities contend\nfor training resources leaving some underoptimized. This raises a pivotal\nquestion: how can we address training imbalances, ensure adequate optimization\nacross all modalities, and achieve consistent performance improvements as we\ntransition from unimodal to multimodal data? This paper proposes the Multimodal\nCompetition Regularizer (MCR), inspired by a mutual information (MI)\ndecomposition designed to prevent the adverse effects of competition in\nmultimodal training. Our key contributions are: 1) A game-theoretic framework\nthat adaptively balances modality contributions by encouraging each to maximize\nits informative role in the final prediction 2) Refining lower and upper bounds\nfor each MI term to enhance the extraction of both task-relevant unique and\nshared information across modalities. 3) Proposing latent space permutations\nfor conditional MI estimation, significantly improving computational\nefficiency. MCR outperforms all previously suggested training strategies and\nsimple baseline, clearly demonstrating that training modalities jointly leads\nto important performance gains on both synthetic and large real-world datasets.\nWe release our code and models at https://github.com/kkontras/MCR.\n", "link": "http://arxiv.org/abs/2411.07335v3", "date": "2025-10-01", "relevancy": 1.6279, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5454}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Multimodal%20Training%20Through%20Game-Theoretic%20Regularization&body=Title%3A%20Balancing%20Multimodal%20Training%20Through%20Game-Theoretic%20Regularization%0AAuthor%3A%20Konstantinos%20Kontras%20and%20Thomas%20Strypsteen%20and%20Christos%20Chatzichristos%20and%20Paul%20Pu%20Liang%20and%20Matthew%20Blaschko%20and%20Maarten%20De%20Vos%0AAbstract%3A%20%20%20Multimodal%20learning%20holds%20promise%20for%20richer%20information%20extraction%20by%0Acapturing%20dependencies%20across%20data%20sources.%20Yet%2C%20current%20training%20methods%20often%0Aunderperform%20due%20to%20modality%20competition%2C%20a%20phenomenon%20where%20modalities%20contend%0Afor%20training%20resources%20leaving%20some%20underoptimized.%20This%20raises%20a%20pivotal%0Aquestion%3A%20how%20can%20we%20address%20training%20imbalances%2C%20ensure%20adequate%20optimization%0Aacross%20all%20modalities%2C%20and%20achieve%20consistent%20performance%20improvements%20as%20we%0Atransition%20from%20unimodal%20to%20multimodal%20data%3F%20This%20paper%20proposes%20the%20Multimodal%0ACompetition%20Regularizer%20%28MCR%29%2C%20inspired%20by%20a%20mutual%20information%20%28MI%29%0Adecomposition%20designed%20to%20prevent%20the%20adverse%20effects%20of%20competition%20in%0Amultimodal%20training.%20Our%20key%20contributions%20are%3A%201%29%20A%20game-theoretic%20framework%0Athat%20adaptively%20balances%20modality%20contributions%20by%20encouraging%20each%20to%20maximize%0Aits%20informative%20role%20in%20the%20final%20prediction%202%29%20Refining%20lower%20and%20upper%20bounds%0Afor%20each%20MI%20term%20to%20enhance%20the%20extraction%20of%20both%20task-relevant%20unique%20and%0Ashared%20information%20across%20modalities.%203%29%20Proposing%20latent%20space%20permutations%0Afor%20conditional%20MI%20estimation%2C%20significantly%20improving%20computational%0Aefficiency.%20MCR%20outperforms%20all%20previously%20suggested%20training%20strategies%20and%0Asimple%20baseline%2C%20clearly%20demonstrating%20that%20training%20modalities%20jointly%20leads%0Ato%20important%20performance%20gains%20on%20both%20synthetic%20and%20large%20real-world%20datasets.%0AWe%20release%20our%20code%20and%20models%20at%20https%3A//github.com/kkontras/MCR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Multimodal%2520Training%2520Through%2520Game-Theoretic%2520Regularization%26entry.906535625%3DKonstantinos%2520Kontras%2520and%2520Thomas%2520Strypsteen%2520and%2520Christos%2520Chatzichristos%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Matthew%2520Blaschko%2520and%2520Maarten%2520De%2520Vos%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520holds%2520promise%2520for%2520richer%2520information%2520extraction%2520by%250Acapturing%2520dependencies%2520across%2520data%2520sources.%2520Yet%252C%2520current%2520training%2520methods%2520often%250Aunderperform%2520due%2520to%2520modality%2520competition%252C%2520a%2520phenomenon%2520where%2520modalities%2520contend%250Afor%2520training%2520resources%2520leaving%2520some%2520underoptimized.%2520This%2520raises%2520a%2520pivotal%250Aquestion%253A%2520how%2520can%2520we%2520address%2520training%2520imbalances%252C%2520ensure%2520adequate%2520optimization%250Aacross%2520all%2520modalities%252C%2520and%2520achieve%2520consistent%2520performance%2520improvements%2520as%2520we%250Atransition%2520from%2520unimodal%2520to%2520multimodal%2520data%253F%2520This%2520paper%2520proposes%2520the%2520Multimodal%250ACompetition%2520Regularizer%2520%2528MCR%2529%252C%2520inspired%2520by%2520a%2520mutual%2520information%2520%2528MI%2529%250Adecomposition%2520designed%2520to%2520prevent%2520the%2520adverse%2520effects%2520of%2520competition%2520in%250Amultimodal%2520training.%2520Our%2520key%2520contributions%2520are%253A%25201%2529%2520A%2520game-theoretic%2520framework%250Athat%2520adaptively%2520balances%2520modality%2520contributions%2520by%2520encouraging%2520each%2520to%2520maximize%250Aits%2520informative%2520role%2520in%2520the%2520final%2520prediction%25202%2529%2520Refining%2520lower%2520and%2520upper%2520bounds%250Afor%2520each%2520MI%2520term%2520to%2520enhance%2520the%2520extraction%2520of%2520both%2520task-relevant%2520unique%2520and%250Ashared%2520information%2520across%2520modalities.%25203%2529%2520Proposing%2520latent%2520space%2520permutations%250Afor%2520conditional%2520MI%2520estimation%252C%2520significantly%2520improving%2520computational%250Aefficiency.%2520MCR%2520outperforms%2520all%2520previously%2520suggested%2520training%2520strategies%2520and%250Asimple%2520baseline%252C%2520clearly%2520demonstrating%2520that%2520training%2520modalities%2520jointly%2520leads%250Ato%2520important%2520performance%2520gains%2520on%2520both%2520synthetic%2520and%2520large%2520real-world%2520datasets.%250AWe%2520release%2520our%2520code%2520and%2520models%2520at%2520https%253A//github.com/kkontras/MCR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Multimodal%20Training%20Through%20Game-Theoretic%20Regularization&entry.906535625=Konstantinos%20Kontras%20and%20Thomas%20Strypsteen%20and%20Christos%20Chatzichristos%20and%20Paul%20Pu%20Liang%20and%20Matthew%20Blaschko%20and%20Maarten%20De%20Vos&entry.1292438233=%20%20Multimodal%20learning%20holds%20promise%20for%20richer%20information%20extraction%20by%0Acapturing%20dependencies%20across%20data%20sources.%20Yet%2C%20current%20training%20methods%20often%0Aunderperform%20due%20to%20modality%20competition%2C%20a%20phenomenon%20where%20modalities%20contend%0Afor%20training%20resources%20leaving%20some%20underoptimized.%20This%20raises%20a%20pivotal%0Aquestion%3A%20how%20can%20we%20address%20training%20imbalances%2C%20ensure%20adequate%20optimization%0Aacross%20all%20modalities%2C%20and%20achieve%20consistent%20performance%20improvements%20as%20we%0Atransition%20from%20unimodal%20to%20multimodal%20data%3F%20This%20paper%20proposes%20the%20Multimodal%0ACompetition%20Regularizer%20%28MCR%29%2C%20inspired%20by%20a%20mutual%20information%20%28MI%29%0Adecomposition%20designed%20to%20prevent%20the%20adverse%20effects%20of%20competition%20in%0Amultimodal%20training.%20Our%20key%20contributions%20are%3A%201%29%20A%20game-theoretic%20framework%0Athat%20adaptively%20balances%20modality%20contributions%20by%20encouraging%20each%20to%20maximize%0Aits%20informative%20role%20in%20the%20final%20prediction%202%29%20Refining%20lower%20and%20upper%20bounds%0Afor%20each%20MI%20term%20to%20enhance%20the%20extraction%20of%20both%20task-relevant%20unique%20and%0Ashared%20information%20across%20modalities.%203%29%20Proposing%20latent%20space%20permutations%0Afor%20conditional%20MI%20estimation%2C%20significantly%20improving%20computational%0Aefficiency.%20MCR%20outperforms%20all%20previously%20suggested%20training%20strategies%20and%0Asimple%20baseline%2C%20clearly%20demonstrating%20that%20training%20modalities%20jointly%20leads%0Ato%20important%20performance%20gains%20on%20both%20synthetic%20and%20large%20real-world%20datasets.%0AWe%20release%20our%20code%20and%20models%20at%20https%3A//github.com/kkontras/MCR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07335v3&entry.124074799=Read"},
{"title": "LLM-guided Task and Motion Planning using Knowledge-based Reasoning", "author": "Muhayy Ud Din and Jan Rosell and Waseem Akram and Isiah Zaplana and Maximo A Roa and Irfan Hussain", "abstract": "  Performing complex manipulation tasks in dynamic environments requires\nefficient Task and Motion Planning (TAMP) approaches that combine high-level\nsymbolic plans with low-level motion control. Advances in Large Language Models\n(LLMs), such as GPT-4, are transforming task planning by offering natural\nlanguage as an intuitive and flexible way to describe tasks, generate symbolic\nplans, and reason. However, the effectiveness of LLM-based TAMP approaches is\nlimited due to static and template-based prompting, which limits adaptability\nto dynamic environments and complex task contexts. To address these\nlimitations, this work proposes a novel Onto-LLM-TAMP framework that employs\nknowledge-based reasoning to refine and expand user prompts with\ntask-contextual reasoning and knowledge-based environment state descriptions.\nIntegrating domain-specific knowledge into the prompt ensures semantically\naccurate and context-aware task plans. The proposed framework demonstrates its\neffectiveness by resolving semantic errors in symbolic plan generation, such as\nmaintaining logical temporal goal ordering in scenarios involving hierarchical\nobject placement. The proposed framework is validated through both simulation\nand real-world scenarios, demonstrating significant improvements over the\nbaseline approach in terms of adaptability to dynamic environments and the\ngeneration of semantically correct task plans.\n", "link": "http://arxiv.org/abs/2412.07493v3", "date": "2025-10-01", "relevancy": 1.6262, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-guided%20Task%20and%20Motion%20Planning%20using%20Knowledge-based%20Reasoning&body=Title%3A%20LLM-guided%20Task%20and%20Motion%20Planning%20using%20Knowledge-based%20Reasoning%0AAuthor%3A%20Muhayy%20Ud%20Din%20and%20Jan%20Rosell%20and%20Waseem%20Akram%20and%20Isiah%20Zaplana%20and%20Maximo%20A%20Roa%20and%20Irfan%20Hussain%0AAbstract%3A%20%20%20Performing%20complex%20manipulation%20tasks%20in%20dynamic%20environments%20requires%0Aefficient%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approaches%20that%20combine%20high-level%0Asymbolic%20plans%20with%20low-level%20motion%20control.%20Advances%20in%20Large%20Language%20Models%0A%28LLMs%29%2C%20such%20as%20GPT-4%2C%20are%20transforming%20task%20planning%20by%20offering%20natural%0Alanguage%20as%20an%20intuitive%20and%20flexible%20way%20to%20describe%20tasks%2C%20generate%20symbolic%0Aplans%2C%20and%20reason.%20However%2C%20the%20effectiveness%20of%20LLM-based%20TAMP%20approaches%20is%0Alimited%20due%20to%20static%20and%20template-based%20prompting%2C%20which%20limits%20adaptability%0Ato%20dynamic%20environments%20and%20complex%20task%20contexts.%20To%20address%20these%0Alimitations%2C%20this%20work%20proposes%20a%20novel%20Onto-LLM-TAMP%20framework%20that%20employs%0Aknowledge-based%20reasoning%20to%20refine%20and%20expand%20user%20prompts%20with%0Atask-contextual%20reasoning%20and%20knowledge-based%20environment%20state%20descriptions.%0AIntegrating%20domain-specific%20knowledge%20into%20the%20prompt%20ensures%20semantically%0Aaccurate%20and%20context-aware%20task%20plans.%20The%20proposed%20framework%20demonstrates%20its%0Aeffectiveness%20by%20resolving%20semantic%20errors%20in%20symbolic%20plan%20generation%2C%20such%20as%0Amaintaining%20logical%20temporal%20goal%20ordering%20in%20scenarios%20involving%20hierarchical%0Aobject%20placement.%20The%20proposed%20framework%20is%20validated%20through%20both%20simulation%0Aand%20real-world%20scenarios%2C%20demonstrating%20significant%20improvements%20over%20the%0Abaseline%20approach%20in%20terms%20of%20adaptability%20to%20dynamic%20environments%20and%20the%0Ageneration%20of%20semantically%20correct%20task%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-guided%2520Task%2520and%2520Motion%2520Planning%2520using%2520Knowledge-based%2520Reasoning%26entry.906535625%3DMuhayy%2520Ud%2520Din%2520and%2520Jan%2520Rosell%2520and%2520Waseem%2520Akram%2520and%2520Isiah%2520Zaplana%2520and%2520Maximo%2520A%2520Roa%2520and%2520Irfan%2520Hussain%26entry.1292438233%3D%2520%2520Performing%2520complex%2520manipulation%2520tasks%2520in%2520dynamic%2520environments%2520requires%250Aefficient%2520Task%2520and%2520Motion%2520Planning%2520%2528TAMP%2529%2520approaches%2520that%2520combine%2520high-level%250Asymbolic%2520plans%2520with%2520low-level%2520motion%2520control.%2520Advances%2520in%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520such%2520as%2520GPT-4%252C%2520are%2520transforming%2520task%2520planning%2520by%2520offering%2520natural%250Alanguage%2520as%2520an%2520intuitive%2520and%2520flexible%2520way%2520to%2520describe%2520tasks%252C%2520generate%2520symbolic%250Aplans%252C%2520and%2520reason.%2520However%252C%2520the%2520effectiveness%2520of%2520LLM-based%2520TAMP%2520approaches%2520is%250Alimited%2520due%2520to%2520static%2520and%2520template-based%2520prompting%252C%2520which%2520limits%2520adaptability%250Ato%2520dynamic%2520environments%2520and%2520complex%2520task%2520contexts.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520work%2520proposes%2520a%2520novel%2520Onto-LLM-TAMP%2520framework%2520that%2520employs%250Aknowledge-based%2520reasoning%2520to%2520refine%2520and%2520expand%2520user%2520prompts%2520with%250Atask-contextual%2520reasoning%2520and%2520knowledge-based%2520environment%2520state%2520descriptions.%250AIntegrating%2520domain-specific%2520knowledge%2520into%2520the%2520prompt%2520ensures%2520semantically%250Aaccurate%2520and%2520context-aware%2520task%2520plans.%2520The%2520proposed%2520framework%2520demonstrates%2520its%250Aeffectiveness%2520by%2520resolving%2520semantic%2520errors%2520in%2520symbolic%2520plan%2520generation%252C%2520such%2520as%250Amaintaining%2520logical%2520temporal%2520goal%2520ordering%2520in%2520scenarios%2520involving%2520hierarchical%250Aobject%2520placement.%2520The%2520proposed%2520framework%2520is%2520validated%2520through%2520both%2520simulation%250Aand%2520real-world%2520scenarios%252C%2520demonstrating%2520significant%2520improvements%2520over%2520the%250Abaseline%2520approach%2520in%2520terms%2520of%2520adaptability%2520to%2520dynamic%2520environments%2520and%2520the%250Ageneration%2520of%2520semantically%2520correct%2520task%2520plans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-guided%20Task%20and%20Motion%20Planning%20using%20Knowledge-based%20Reasoning&entry.906535625=Muhayy%20Ud%20Din%20and%20Jan%20Rosell%20and%20Waseem%20Akram%20and%20Isiah%20Zaplana%20and%20Maximo%20A%20Roa%20and%20Irfan%20Hussain&entry.1292438233=%20%20Performing%20complex%20manipulation%20tasks%20in%20dynamic%20environments%20requires%0Aefficient%20Task%20and%20Motion%20Planning%20%28TAMP%29%20approaches%20that%20combine%20high-level%0Asymbolic%20plans%20with%20low-level%20motion%20control.%20Advances%20in%20Large%20Language%20Models%0A%28LLMs%29%2C%20such%20as%20GPT-4%2C%20are%20transforming%20task%20planning%20by%20offering%20natural%0Alanguage%20as%20an%20intuitive%20and%20flexible%20way%20to%20describe%20tasks%2C%20generate%20symbolic%0Aplans%2C%20and%20reason.%20However%2C%20the%20effectiveness%20of%20LLM-based%20TAMP%20approaches%20is%0Alimited%20due%20to%20static%20and%20template-based%20prompting%2C%20which%20limits%20adaptability%0Ato%20dynamic%20environments%20and%20complex%20task%20contexts.%20To%20address%20these%0Alimitations%2C%20this%20work%20proposes%20a%20novel%20Onto-LLM-TAMP%20framework%20that%20employs%0Aknowledge-based%20reasoning%20to%20refine%20and%20expand%20user%20prompts%20with%0Atask-contextual%20reasoning%20and%20knowledge-based%20environment%20state%20descriptions.%0AIntegrating%20domain-specific%20knowledge%20into%20the%20prompt%20ensures%20semantically%0Aaccurate%20and%20context-aware%20task%20plans.%20The%20proposed%20framework%20demonstrates%20its%0Aeffectiveness%20by%20resolving%20semantic%20errors%20in%20symbolic%20plan%20generation%2C%20such%20as%0Amaintaining%20logical%20temporal%20goal%20ordering%20in%20scenarios%20involving%20hierarchical%0Aobject%20placement.%20The%20proposed%20framework%20is%20validated%20through%20both%20simulation%0Aand%20real-world%20scenarios%2C%20demonstrating%20significant%20improvements%20over%20the%0Abaseline%20approach%20in%20terms%20of%20adaptability%20to%20dynamic%20environments%20and%20the%0Ageneration%20of%20semantically%20correct%20task%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07493v3&entry.124074799=Read"},
{"title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization", "author": "Jintao Zhang and Haofeng Huang and Pengle Zhang and Jia Wei and Jun Zhu and Jianfei Chen", "abstract": "  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.\n", "link": "http://arxiv.org/abs/2411.10958v7", "date": "2025-10-01", "relevancy": 1.6174, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5731}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5193}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SageAttention2%3A%20Efficient%20Attention%20with%20Thorough%20Outlier%20Smoothing%20and%0A%20%20Per-thread%20INT4%20Quantization&body=Title%3A%20SageAttention2%3A%20Efficient%20Attention%20with%20Thorough%20Outlier%20Smoothing%20and%0A%20%20Per-thread%20INT4%20Quantization%0AAuthor%3A%20Jintao%20Zhang%20and%20Haofeng%20Huang%20and%20Pengle%20Zhang%20and%20Jia%20Wei%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20Although%20quantization%20for%20linear%20layers%20has%20been%20widely%20used%2C%20its%20application%0Ato%20accelerate%20the%20attention%20process%20remains%20limited.%20To%20further%20enhance%20the%0Aefficiency%20of%20attention%20computation%20compared%20to%20SageAttention%20while%20maintaining%0Aprecision%2C%20we%20propose%20SageAttention2%2C%20which%20utilizes%20significantly%20faster%204-bit%0Amatrix%20multiplication%20%28Matmul%29%20alongside%20additional%20precision-enhancing%0Atechniques.%20First%2C%20we%20propose%20to%20quantize%20matrices%20%24%28Q%2C%20K%29%24%20to%20INT4%20in%20a%0Ahardware-friendly%20thread-level%20granularity%20and%20quantize%20matrices%20%24%28%5Cwidetilde%0AP%2C%20V%29%24%20to%20FP8.%20Second%2C%20we%20propose%20a%20method%20to%20smooth%20%24Q%24%2C%20enhancing%20the%0Aaccuracy%20of%20INT4%20%24QK%5E%5Ctop%24.%20Third%2C%20we%20propose%20a%20two-level%20accumulation%20strategy%0Afor%20%24%5Cwidetilde%20PV%24%20to%20enhance%20the%20accuracy%20of%20FP8%20%24%5Cwidetilde%20PV%24.%20The%0Aoperations%20per%20second%20%28OPS%29%20of%20SageAttention2%20surpass%20FlashAttention2%20and%0Axformers%20by%20about%203x%20and%204.5x%20on%20RTX4090%2C%20respectively.%20Moreover%2C%0ASageAttention2%20matches%20the%20speed%20of%20FlashAttention3%28fp8%29%20on%20the%20Hopper%20GPUs%2C%0Awhile%20delivering%20much%20higher%20accuracy.%20Comprehensive%20experiments%20confirm%20that%0Aour%20approach%20incurs%20negligible%20end-to-end%20metrics%20loss%20across%20diverse%20models%2C%0Aincluding%20those%20for%20language%2C%20image%2C%20and%20video%20generation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/thu-ml/SageAttention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10958v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSageAttention2%253A%2520Efficient%2520Attention%2520with%2520Thorough%2520Outlier%2520Smoothing%2520and%250A%2520%2520Per-thread%2520INT4%2520Quantization%26entry.906535625%3DJintao%2520Zhang%2520and%2520Haofeng%2520Huang%2520and%2520Pengle%2520Zhang%2520and%2520Jia%2520Wei%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520Although%2520quantization%2520for%2520linear%2520layers%2520has%2520been%2520widely%2520used%252C%2520its%2520application%250Ato%2520accelerate%2520the%2520attention%2520process%2520remains%2520limited.%2520To%2520further%2520enhance%2520the%250Aefficiency%2520of%2520attention%2520computation%2520compared%2520to%2520SageAttention%2520while%2520maintaining%250Aprecision%252C%2520we%2520propose%2520SageAttention2%252C%2520which%2520utilizes%2520significantly%2520faster%25204-bit%250Amatrix%2520multiplication%2520%2528Matmul%2529%2520alongside%2520additional%2520precision-enhancing%250Atechniques.%2520First%252C%2520we%2520propose%2520to%2520quantize%2520matrices%2520%2524%2528Q%252C%2520K%2529%2524%2520to%2520INT4%2520in%2520a%250Ahardware-friendly%2520thread-level%2520granularity%2520and%2520quantize%2520matrices%2520%2524%2528%255Cwidetilde%250AP%252C%2520V%2529%2524%2520to%2520FP8.%2520Second%252C%2520we%2520propose%2520a%2520method%2520to%2520smooth%2520%2524Q%2524%252C%2520enhancing%2520the%250Aaccuracy%2520of%2520INT4%2520%2524QK%255E%255Ctop%2524.%2520Third%252C%2520we%2520propose%2520a%2520two-level%2520accumulation%2520strategy%250Afor%2520%2524%255Cwidetilde%2520PV%2524%2520to%2520enhance%2520the%2520accuracy%2520of%2520FP8%2520%2524%255Cwidetilde%2520PV%2524.%2520The%250Aoperations%2520per%2520second%2520%2528OPS%2529%2520of%2520SageAttention2%2520surpass%2520FlashAttention2%2520and%250Axformers%2520by%2520about%25203x%2520and%25204.5x%2520on%2520RTX4090%252C%2520respectively.%2520Moreover%252C%250ASageAttention2%2520matches%2520the%2520speed%2520of%2520FlashAttention3%2528fp8%2529%2520on%2520the%2520Hopper%2520GPUs%252C%250Awhile%2520delivering%2520much%2520higher%2520accuracy.%2520Comprehensive%2520experiments%2520confirm%2520that%250Aour%2520approach%2520incurs%2520negligible%2520end-to-end%2520metrics%2520loss%2520across%2520diverse%2520models%252C%250Aincluding%2520those%2520for%2520language%252C%2520image%252C%2520and%2520video%2520generation.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/thu-ml/SageAttention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10958v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SageAttention2%3A%20Efficient%20Attention%20with%20Thorough%20Outlier%20Smoothing%20and%0A%20%20Per-thread%20INT4%20Quantization&entry.906535625=Jintao%20Zhang%20and%20Haofeng%20Huang%20and%20Pengle%20Zhang%20and%20Jia%20Wei%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20Although%20quantization%20for%20linear%20layers%20has%20been%20widely%20used%2C%20its%20application%0Ato%20accelerate%20the%20attention%20process%20remains%20limited.%20To%20further%20enhance%20the%0Aefficiency%20of%20attention%20computation%20compared%20to%20SageAttention%20while%20maintaining%0Aprecision%2C%20we%20propose%20SageAttention2%2C%20which%20utilizes%20significantly%20faster%204-bit%0Amatrix%20multiplication%20%28Matmul%29%20alongside%20additional%20precision-enhancing%0Atechniques.%20First%2C%20we%20propose%20to%20quantize%20matrices%20%24%28Q%2C%20K%29%24%20to%20INT4%20in%20a%0Ahardware-friendly%20thread-level%20granularity%20and%20quantize%20matrices%20%24%28%5Cwidetilde%0AP%2C%20V%29%24%20to%20FP8.%20Second%2C%20we%20propose%20a%20method%20to%20smooth%20%24Q%24%2C%20enhancing%20the%0Aaccuracy%20of%20INT4%20%24QK%5E%5Ctop%24.%20Third%2C%20we%20propose%20a%20two-level%20accumulation%20strategy%0Afor%20%24%5Cwidetilde%20PV%24%20to%20enhance%20the%20accuracy%20of%20FP8%20%24%5Cwidetilde%20PV%24.%20The%0Aoperations%20per%20second%20%28OPS%29%20of%20SageAttention2%20surpass%20FlashAttention2%20and%0Axformers%20by%20about%203x%20and%204.5x%20on%20RTX4090%2C%20respectively.%20Moreover%2C%0ASageAttention2%20matches%20the%20speed%20of%20FlashAttention3%28fp8%29%20on%20the%20Hopper%20GPUs%2C%0Awhile%20delivering%20much%20higher%20accuracy.%20Comprehensive%20experiments%20confirm%20that%0Aour%20approach%20incurs%20negligible%20end-to-end%20metrics%20loss%20across%20diverse%20models%2C%0Aincluding%20those%20for%20language%2C%20image%2C%20and%20video%20generation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/thu-ml/SageAttention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10958v7&entry.124074799=Read"},
{"title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies", "author": "Jing Wang and Weiting Peng and Jing Tang and Zeyu Gong and Xihua Wang and Bo Tao and Li Cheng", "abstract": "  Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning.\n", "link": "http://arxiv.org/abs/2509.25822v2", "date": "2025-10-01", "relevancy": 1.6117, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5396}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.537}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Act%20to%20See%2C%20See%20to%20Act%3A%20Diffusion-Driven%20Perception-Action%20Interplay%20for%0A%20%20Adaptive%20Policies&body=Title%3A%20Act%20to%20See%2C%20See%20to%20Act%3A%20Diffusion-Driven%20Perception-Action%20Interplay%20for%0A%20%20Adaptive%20Policies%0AAuthor%3A%20Jing%20Wang%20and%20Weiting%20Peng%20and%20Jing%20Tang%20and%20Zeyu%20Gong%20and%20Xihua%20Wang%20and%20Bo%20Tao%20and%20Li%20Cheng%0AAbstract%3A%20%20%20Existing%20imitation%20learning%20methods%20decouple%20perception%20and%20action%2C%20which%0Aoverlooks%20the%20causal%20reciprocity%20between%20sensory%20representations%20and%20action%0Aexecution%20that%20humans%20naturally%20leverage%20for%20adaptive%20behaviors.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20Action-Guided%20Diffusion%20Policy%20%28DP-AG%29%2C%20a%20unified%0Arepresentation%20learning%20that%20explicitly%20models%20a%20dynamic%20interplay%20between%0Aperception%20and%20action%20through%20probabilistic%20latent%20dynamics.%20DP-AG%20encodes%0Alatent%20observations%20into%20a%20Gaussian%20posterior%20via%20variational%20inference%20and%0Aevolves%20them%20using%20an%20action-guided%20SDE%2C%20where%20the%20Vector-Jacobian%20Product%0A%28VJP%29%20of%20the%20diffusion%20policy%27s%20noise%20predictions%20serves%20as%20a%20structured%0Astochastic%20force%20driving%20latent%20updates.%20To%20promote%20bidirectional%20learning%0Abetween%20perception%20and%20action%2C%20we%20introduce%20a%20cycle-consistent%20contrastive%20loss%0Athat%20organizes%20the%20gradient%20flow%20of%20the%20noise%20predictor%20into%20a%20coherent%0Aperception-action%20loop%2C%20enforcing%20mutually%20consistent%20transitions%20in%20both%0Alatent%20updates%20and%20action%20refinements.%20Theoretically%2C%20we%20derive%20a%20variational%0Alower%20bound%20for%20the%20action-guided%20SDE%2C%20and%20prove%20that%20the%20contrastive%20objective%0Aenhances%20continuity%20in%20both%20latent%20and%20action%20trajectories.%20Empirically%2C%20DP-AG%0Asignificantly%20outperforms%20state-of-the-art%20methods%20across%20simulation%20benchmarks%0Aand%20real-world%20UR5%20manipulation%20tasks.%20As%20a%20result%2C%20our%20DP-AG%20offers%20a%0Apromising%20step%20toward%20bridging%20biological%20adaptability%20and%20artificial%20policy%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.25822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAct%2520to%2520See%252C%2520See%2520to%2520Act%253A%2520Diffusion-Driven%2520Perception-Action%2520Interplay%2520for%250A%2520%2520Adaptive%2520Policies%26entry.906535625%3DJing%2520Wang%2520and%2520Weiting%2520Peng%2520and%2520Jing%2520Tang%2520and%2520Zeyu%2520Gong%2520and%2520Xihua%2520Wang%2520and%2520Bo%2520Tao%2520and%2520Li%2520Cheng%26entry.1292438233%3D%2520%2520Existing%2520imitation%2520learning%2520methods%2520decouple%2520perception%2520and%2520action%252C%2520which%250Aoverlooks%2520the%2520causal%2520reciprocity%2520between%2520sensory%2520representations%2520and%2520action%250Aexecution%2520that%2520humans%2520naturally%2520leverage%2520for%2520adaptive%2520behaviors.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520introduce%2520Action-Guided%2520Diffusion%2520Policy%2520%2528DP-AG%2529%252C%2520a%2520unified%250Arepresentation%2520learning%2520that%2520explicitly%2520models%2520a%2520dynamic%2520interplay%2520between%250Aperception%2520and%2520action%2520through%2520probabilistic%2520latent%2520dynamics.%2520DP-AG%2520encodes%250Alatent%2520observations%2520into%2520a%2520Gaussian%2520posterior%2520via%2520variational%2520inference%2520and%250Aevolves%2520them%2520using%2520an%2520action-guided%2520SDE%252C%2520where%2520the%2520Vector-Jacobian%2520Product%250A%2528VJP%2529%2520of%2520the%2520diffusion%2520policy%2527s%2520noise%2520predictions%2520serves%2520as%2520a%2520structured%250Astochastic%2520force%2520driving%2520latent%2520updates.%2520To%2520promote%2520bidirectional%2520learning%250Abetween%2520perception%2520and%2520action%252C%2520we%2520introduce%2520a%2520cycle-consistent%2520contrastive%2520loss%250Athat%2520organizes%2520the%2520gradient%2520flow%2520of%2520the%2520noise%2520predictor%2520into%2520a%2520coherent%250Aperception-action%2520loop%252C%2520enforcing%2520mutually%2520consistent%2520transitions%2520in%2520both%250Alatent%2520updates%2520and%2520action%2520refinements.%2520Theoretically%252C%2520we%2520derive%2520a%2520variational%250Alower%2520bound%2520for%2520the%2520action-guided%2520SDE%252C%2520and%2520prove%2520that%2520the%2520contrastive%2520objective%250Aenhances%2520continuity%2520in%2520both%2520latent%2520and%2520action%2520trajectories.%2520Empirically%252C%2520DP-AG%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520across%2520simulation%2520benchmarks%250Aand%2520real-world%2520UR5%2520manipulation%2520tasks.%2520As%2520a%2520result%252C%2520our%2520DP-AG%2520offers%2520a%250Apromising%2520step%2520toward%2520bridging%2520biological%2520adaptability%2520and%2520artificial%2520policy%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Act%20to%20See%2C%20See%20to%20Act%3A%20Diffusion-Driven%20Perception-Action%20Interplay%20for%0A%20%20Adaptive%20Policies&entry.906535625=Jing%20Wang%20and%20Weiting%20Peng%20and%20Jing%20Tang%20and%20Zeyu%20Gong%20and%20Xihua%20Wang%20and%20Bo%20Tao%20and%20Li%20Cheng&entry.1292438233=%20%20Existing%20imitation%20learning%20methods%20decouple%20perception%20and%20action%2C%20which%0Aoverlooks%20the%20causal%20reciprocity%20between%20sensory%20representations%20and%20action%0Aexecution%20that%20humans%20naturally%20leverage%20for%20adaptive%20behaviors.%20To%20bridge%20this%0Agap%2C%20we%20introduce%20Action-Guided%20Diffusion%20Policy%20%28DP-AG%29%2C%20a%20unified%0Arepresentation%20learning%20that%20explicitly%20models%20a%20dynamic%20interplay%20between%0Aperception%20and%20action%20through%20probabilistic%20latent%20dynamics.%20DP-AG%20encodes%0Alatent%20observations%20into%20a%20Gaussian%20posterior%20via%20variational%20inference%20and%0Aevolves%20them%20using%20an%20action-guided%20SDE%2C%20where%20the%20Vector-Jacobian%20Product%0A%28VJP%29%20of%20the%20diffusion%20policy%27s%20noise%20predictions%20serves%20as%20a%20structured%0Astochastic%20force%20driving%20latent%20updates.%20To%20promote%20bidirectional%20learning%0Abetween%20perception%20and%20action%2C%20we%20introduce%20a%20cycle-consistent%20contrastive%20loss%0Athat%20organizes%20the%20gradient%20flow%20of%20the%20noise%20predictor%20into%20a%20coherent%0Aperception-action%20loop%2C%20enforcing%20mutually%20consistent%20transitions%20in%20both%0Alatent%20updates%20and%20action%20refinements.%20Theoretically%2C%20we%20derive%20a%20variational%0Alower%20bound%20for%20the%20action-guided%20SDE%2C%20and%20prove%20that%20the%20contrastive%20objective%0Aenhances%20continuity%20in%20both%20latent%20and%20action%20trajectories.%20Empirically%2C%20DP-AG%0Asignificantly%20outperforms%20state-of-the-art%20methods%20across%20simulation%20benchmarks%0Aand%20real-world%20UR5%20manipulation%20tasks.%20As%20a%20result%2C%20our%20DP-AG%20offers%20a%0Apromising%20step%20toward%20bridging%20biological%20adaptability%20and%20artificial%20policy%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.25822v2&entry.124074799=Read"},
{"title": "Progressive Weight Loading: Accelerating Initial Inference and Gradually\n  Boosting Performance on Resource-Constrained Environments", "author": "Hyunwoo Kim and Junha Lee and Mincheol Choi and Jeonghwan Lee and Jaeshin Cho", "abstract": "  Deep learning models have become increasingly large and complex, resulting in\nhigher memory consumption and computational demands. Consequently, model\nloading times and initial inference latency have increased, posing significant\nchallenges in mobile and latency-sensitive environments where frequent model\nloading and unloading are required, which directly impacts user experience.\nWhile Knowledge Distillation (KD) offers a solution by compressing large\nteacher models into smaller student ones, it often comes at the cost of reduced\nperformance. To address this trade-off, we propose Progressive Weight Loading\n(PWL), a novel technique that enables fast initial inference by first deploying\na lightweight student model, then incrementally replacing its layers with those\nof a pre-trained teacher model. To support seamless layer substitution, we\nintroduce a training method that not only aligns intermediate feature\nrepresentations between student and teacher layers, but also improves the\noverall output performance of the student model. Our experiments on VGG,\nResNet, and ViT architectures demonstrate that models trained with PWL maintain\ncompetitive distillation performance and gradually improve accuracy as teacher\nlayers are loaded-matching the final accuracy of the full teacher model without\ncompromising initial inference speed. This makes PWL particularly suited for\ndynamic, resource-constrained deployments where both responsiveness and\nperformance are critical.\n", "link": "http://arxiv.org/abs/2509.22319v2", "date": "2025-10-01", "relevancy": 1.6003, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5548}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5333}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Weight%20Loading%3A%20Accelerating%20Initial%20Inference%20and%20Gradually%0A%20%20Boosting%20Performance%20on%20Resource-Constrained%20Environments&body=Title%3A%20Progressive%20Weight%20Loading%3A%20Accelerating%20Initial%20Inference%20and%20Gradually%0A%20%20Boosting%20Performance%20on%20Resource-Constrained%20Environments%0AAuthor%3A%20Hyunwoo%20Kim%20and%20Junha%20Lee%20and%20Mincheol%20Choi%20and%20Jeonghwan%20Lee%20and%20Jaeshin%20Cho%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20become%20increasingly%20large%20and%20complex%2C%20resulting%20in%0Ahigher%20memory%20consumption%20and%20computational%20demands.%20Consequently%2C%20model%0Aloading%20times%20and%20initial%20inference%20latency%20have%20increased%2C%20posing%20significant%0Achallenges%20in%20mobile%20and%20latency-sensitive%20environments%20where%20frequent%20model%0Aloading%20and%20unloading%20are%20required%2C%20which%20directly%20impacts%20user%20experience.%0AWhile%20Knowledge%20Distillation%20%28KD%29%20offers%20a%20solution%20by%20compressing%20large%0Ateacher%20models%20into%20smaller%20student%20ones%2C%20it%20often%20comes%20at%20the%20cost%20of%20reduced%0Aperformance.%20To%20address%20this%20trade-off%2C%20we%20propose%20Progressive%20Weight%20Loading%0A%28PWL%29%2C%20a%20novel%20technique%20that%20enables%20fast%20initial%20inference%20by%20first%20deploying%0Aa%20lightweight%20student%20model%2C%20then%20incrementally%20replacing%20its%20layers%20with%20those%0Aof%20a%20pre-trained%20teacher%20model.%20To%20support%20seamless%20layer%20substitution%2C%20we%0Aintroduce%20a%20training%20method%20that%20not%20only%20aligns%20intermediate%20feature%0Arepresentations%20between%20student%20and%20teacher%20layers%2C%20but%20also%20improves%20the%0Aoverall%20output%20performance%20of%20the%20student%20model.%20Our%20experiments%20on%20VGG%2C%0AResNet%2C%20and%20ViT%20architectures%20demonstrate%20that%20models%20trained%20with%20PWL%20maintain%0Acompetitive%20distillation%20performance%20and%20gradually%20improve%20accuracy%20as%20teacher%0Alayers%20are%20loaded-matching%20the%20final%20accuracy%20of%20the%20full%20teacher%20model%20without%0Acompromising%20initial%20inference%20speed.%20This%20makes%20PWL%20particularly%20suited%20for%0Adynamic%2C%20resource-constrained%20deployments%20where%20both%20responsiveness%20and%0Aperformance%20are%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Weight%2520Loading%253A%2520Accelerating%2520Initial%2520Inference%2520and%2520Gradually%250A%2520%2520Boosting%2520Performance%2520on%2520Resource-Constrained%2520Environments%26entry.906535625%3DHyunwoo%2520Kim%2520and%2520Junha%2520Lee%2520and%2520Mincheol%2520Choi%2520and%2520Jeonghwan%2520Lee%2520and%2520Jaeshin%2520Cho%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520become%2520increasingly%2520large%2520and%2520complex%252C%2520resulting%2520in%250Ahigher%2520memory%2520consumption%2520and%2520computational%2520demands.%2520Consequently%252C%2520model%250Aloading%2520times%2520and%2520initial%2520inference%2520latency%2520have%2520increased%252C%2520posing%2520significant%250Achallenges%2520in%2520mobile%2520and%2520latency-sensitive%2520environments%2520where%2520frequent%2520model%250Aloading%2520and%2520unloading%2520are%2520required%252C%2520which%2520directly%2520impacts%2520user%2520experience.%250AWhile%2520Knowledge%2520Distillation%2520%2528KD%2529%2520offers%2520a%2520solution%2520by%2520compressing%2520large%250Ateacher%2520models%2520into%2520smaller%2520student%2520ones%252C%2520it%2520often%2520comes%2520at%2520the%2520cost%2520of%2520reduced%250Aperformance.%2520To%2520address%2520this%2520trade-off%252C%2520we%2520propose%2520Progressive%2520Weight%2520Loading%250A%2528PWL%2529%252C%2520a%2520novel%2520technique%2520that%2520enables%2520fast%2520initial%2520inference%2520by%2520first%2520deploying%250Aa%2520lightweight%2520student%2520model%252C%2520then%2520incrementally%2520replacing%2520its%2520layers%2520with%2520those%250Aof%2520a%2520pre-trained%2520teacher%2520model.%2520To%2520support%2520seamless%2520layer%2520substitution%252C%2520we%250Aintroduce%2520a%2520training%2520method%2520that%2520not%2520only%2520aligns%2520intermediate%2520feature%250Arepresentations%2520between%2520student%2520and%2520teacher%2520layers%252C%2520but%2520also%2520improves%2520the%250Aoverall%2520output%2520performance%2520of%2520the%2520student%2520model.%2520Our%2520experiments%2520on%2520VGG%252C%250AResNet%252C%2520and%2520ViT%2520architectures%2520demonstrate%2520that%2520models%2520trained%2520with%2520PWL%2520maintain%250Acompetitive%2520distillation%2520performance%2520and%2520gradually%2520improve%2520accuracy%2520as%2520teacher%250Alayers%2520are%2520loaded-matching%2520the%2520final%2520accuracy%2520of%2520the%2520full%2520teacher%2520model%2520without%250Acompromising%2520initial%2520inference%2520speed.%2520This%2520makes%2520PWL%2520particularly%2520suited%2520for%250Adynamic%252C%2520resource-constrained%2520deployments%2520where%2520both%2520responsiveness%2520and%250Aperformance%2520are%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Weight%20Loading%3A%20Accelerating%20Initial%20Inference%20and%20Gradually%0A%20%20Boosting%20Performance%20on%20Resource-Constrained%20Environments&entry.906535625=Hyunwoo%20Kim%20and%20Junha%20Lee%20and%20Mincheol%20Choi%20and%20Jeonghwan%20Lee%20and%20Jaeshin%20Cho&entry.1292438233=%20%20Deep%20learning%20models%20have%20become%20increasingly%20large%20and%20complex%2C%20resulting%20in%0Ahigher%20memory%20consumption%20and%20computational%20demands.%20Consequently%2C%20model%0Aloading%20times%20and%20initial%20inference%20latency%20have%20increased%2C%20posing%20significant%0Achallenges%20in%20mobile%20and%20latency-sensitive%20environments%20where%20frequent%20model%0Aloading%20and%20unloading%20are%20required%2C%20which%20directly%20impacts%20user%20experience.%0AWhile%20Knowledge%20Distillation%20%28KD%29%20offers%20a%20solution%20by%20compressing%20large%0Ateacher%20models%20into%20smaller%20student%20ones%2C%20it%20often%20comes%20at%20the%20cost%20of%20reduced%0Aperformance.%20To%20address%20this%20trade-off%2C%20we%20propose%20Progressive%20Weight%20Loading%0A%28PWL%29%2C%20a%20novel%20technique%20that%20enables%20fast%20initial%20inference%20by%20first%20deploying%0Aa%20lightweight%20student%20model%2C%20then%20incrementally%20replacing%20its%20layers%20with%20those%0Aof%20a%20pre-trained%20teacher%20model.%20To%20support%20seamless%20layer%20substitution%2C%20we%0Aintroduce%20a%20training%20method%20that%20not%20only%20aligns%20intermediate%20feature%0Arepresentations%20between%20student%20and%20teacher%20layers%2C%20but%20also%20improves%20the%0Aoverall%20output%20performance%20of%20the%20student%20model.%20Our%20experiments%20on%20VGG%2C%0AResNet%2C%20and%20ViT%20architectures%20demonstrate%20that%20models%20trained%20with%20PWL%20maintain%0Acompetitive%20distillation%20performance%20and%20gradually%20improve%20accuracy%20as%20teacher%0Alayers%20are%20loaded-matching%20the%20final%20accuracy%20of%20the%20full%20teacher%20model%20without%0Acompromising%20initial%20inference%20speed.%20This%20makes%20PWL%20particularly%20suited%20for%0Adynamic%2C%20resource-constrained%20deployments%20where%20both%20responsiveness%20and%0Aperformance%20are%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22319v2&entry.124074799=Read"},
{"title": "BlobCtrl: Taming Controllable Blob for Element-level Image Editing", "author": "Yaowei Li and Lingen Li and Zhaoyang Zhang and Xiaoyu Li and Guangzhi Wang and Hongxiang Li and Xiaodong Cun and Ying Shan and Yuexian Zou", "abstract": "  As user expectations for image editing continue to rise, the demand for\nflexible, fine-grained manipulation of specific visual elements presents a\nchallenge for current diffusion-based methods. In this work, we present\nBlobCtrl, a framework for element-level image editing based on a probabilistic\nblob-based representation. Treating blobs as visual primitives, BlobCtrl\ndisentangles layout from appearance, affording fine-grained, controllable\nobject-level manipulation. Our key contributions are twofold: (1) an in-context\ndual-branch diffusion model that separates foreground and background\nprocessing, incorporating blob representations to explicitly decouple layout\nand appearance, and (2) a self-supervised disentangle-then-reconstruct training\nparadigm with an identity-preserving loss function, along with tailored\nstrategies to efficiently leverage blob-image pairs. To foster further\nresearch, we introduce BlobData for large-scale training and BlobBench, a\nbenchmark for systematic evaluation. Experimental results demonstrate that\nBlobCtrl achieves state-of-the-art performance in a variety of element-level\nediting tasks, such as object addition, removal, scaling, and replacement,\nwhile maintaining computational efficiency. Project Webpage:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/\n", "link": "http://arxiv.org/abs/2503.13434v2", "date": "2025-10-01", "relevancy": 1.5753, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.541}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlobCtrl%3A%20Taming%20Controllable%20Blob%20for%20Element-level%20Image%20Editing&body=Title%3A%20BlobCtrl%3A%20Taming%20Controllable%20Blob%20for%20Element-level%20Image%20Editing%0AAuthor%3A%20Yaowei%20Li%20and%20Lingen%20Li%20and%20Zhaoyang%20Zhang%20and%20Xiaoyu%20Li%20and%20Guangzhi%20Wang%20and%20Hongxiang%20Li%20and%20Xiaodong%20Cun%20and%20Ying%20Shan%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20As%20user%20expectations%20for%20image%20editing%20continue%20to%20rise%2C%20the%20demand%20for%0Aflexible%2C%20fine-grained%20manipulation%20of%20specific%20visual%20elements%20presents%20a%0Achallenge%20for%20current%20diffusion-based%20methods.%20In%20this%20work%2C%20we%20present%0ABlobCtrl%2C%20a%20framework%20for%20element-level%20image%20editing%20based%20on%20a%20probabilistic%0Ablob-based%20representation.%20Treating%20blobs%20as%20visual%20primitives%2C%20BlobCtrl%0Adisentangles%20layout%20from%20appearance%2C%20affording%20fine-grained%2C%20controllable%0Aobject-level%20manipulation.%20Our%20key%20contributions%20are%20twofold%3A%20%281%29%20an%20in-context%0Adual-branch%20diffusion%20model%20that%20separates%20foreground%20and%20background%0Aprocessing%2C%20incorporating%20blob%20representations%20to%20explicitly%20decouple%20layout%0Aand%20appearance%2C%20and%20%282%29%20a%20self-supervised%20disentangle-then-reconstruct%20training%0Aparadigm%20with%20an%20identity-preserving%20loss%20function%2C%20along%20with%20tailored%0Astrategies%20to%20efficiently%20leverage%20blob-image%20pairs.%20To%20foster%20further%0Aresearch%2C%20we%20introduce%20BlobData%20for%20large-scale%20training%20and%20BlobBench%2C%20a%0Abenchmark%20for%20systematic%20evaluation.%20Experimental%20results%20demonstrate%20that%0ABlobCtrl%20achieves%20state-of-the-art%20performance%20in%20a%20variety%20of%20element-level%0Aediting%20tasks%2C%20such%20as%20object%20addition%2C%20removal%2C%20scaling%2C%20and%20replacement%2C%0Awhile%20maintaining%20computational%20efficiency.%20Project%20Webpage%3A%0Ahttps%3A//liyaowei-stu.github.io/project/BlobCtrl/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlobCtrl%253A%2520Taming%2520Controllable%2520Blob%2520for%2520Element-level%2520Image%2520Editing%26entry.906535625%3DYaowei%2520Li%2520and%2520Lingen%2520Li%2520and%2520Zhaoyang%2520Zhang%2520and%2520Xiaoyu%2520Li%2520and%2520Guangzhi%2520Wang%2520and%2520Hongxiang%2520Li%2520and%2520Xiaodong%2520Cun%2520and%2520Ying%2520Shan%2520and%2520Yuexian%2520Zou%26entry.1292438233%3D%2520%2520As%2520user%2520expectations%2520for%2520image%2520editing%2520continue%2520to%2520rise%252C%2520the%2520demand%2520for%250Aflexible%252C%2520fine-grained%2520manipulation%2520of%2520specific%2520visual%2520elements%2520presents%2520a%250Achallenge%2520for%2520current%2520diffusion-based%2520methods.%2520In%2520this%2520work%252C%2520we%2520present%250ABlobCtrl%252C%2520a%2520framework%2520for%2520element-level%2520image%2520editing%2520based%2520on%2520a%2520probabilistic%250Ablob-based%2520representation.%2520Treating%2520blobs%2520as%2520visual%2520primitives%252C%2520BlobCtrl%250Adisentangles%2520layout%2520from%2520appearance%252C%2520affording%2520fine-grained%252C%2520controllable%250Aobject-level%2520manipulation.%2520Our%2520key%2520contributions%2520are%2520twofold%253A%2520%25281%2529%2520an%2520in-context%250Adual-branch%2520diffusion%2520model%2520that%2520separates%2520foreground%2520and%2520background%250Aprocessing%252C%2520incorporating%2520blob%2520representations%2520to%2520explicitly%2520decouple%2520layout%250Aand%2520appearance%252C%2520and%2520%25282%2529%2520a%2520self-supervised%2520disentangle-then-reconstruct%2520training%250Aparadigm%2520with%2520an%2520identity-preserving%2520loss%2520function%252C%2520along%2520with%2520tailored%250Astrategies%2520to%2520efficiently%2520leverage%2520blob-image%2520pairs.%2520To%2520foster%2520further%250Aresearch%252C%2520we%2520introduce%2520BlobData%2520for%2520large-scale%2520training%2520and%2520BlobBench%252C%2520a%250Abenchmark%2520for%2520systematic%2520evaluation.%2520Experimental%2520results%2520demonstrate%2520that%250ABlobCtrl%2520achieves%2520state-of-the-art%2520performance%2520in%2520a%2520variety%2520of%2520element-level%250Aediting%2520tasks%252C%2520such%2520as%2520object%2520addition%252C%2520removal%252C%2520scaling%252C%2520and%2520replacement%252C%250Awhile%2520maintaining%2520computational%2520efficiency.%2520Project%2520Webpage%253A%250Ahttps%253A//liyaowei-stu.github.io/project/BlobCtrl/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlobCtrl%3A%20Taming%20Controllable%20Blob%20for%20Element-level%20Image%20Editing&entry.906535625=Yaowei%20Li%20and%20Lingen%20Li%20and%20Zhaoyang%20Zhang%20and%20Xiaoyu%20Li%20and%20Guangzhi%20Wang%20and%20Hongxiang%20Li%20and%20Xiaodong%20Cun%20and%20Ying%20Shan%20and%20Yuexian%20Zou&entry.1292438233=%20%20As%20user%20expectations%20for%20image%20editing%20continue%20to%20rise%2C%20the%20demand%20for%0Aflexible%2C%20fine-grained%20manipulation%20of%20specific%20visual%20elements%20presents%20a%0Achallenge%20for%20current%20diffusion-based%20methods.%20In%20this%20work%2C%20we%20present%0ABlobCtrl%2C%20a%20framework%20for%20element-level%20image%20editing%20based%20on%20a%20probabilistic%0Ablob-based%20representation.%20Treating%20blobs%20as%20visual%20primitives%2C%20BlobCtrl%0Adisentangles%20layout%20from%20appearance%2C%20affording%20fine-grained%2C%20controllable%0Aobject-level%20manipulation.%20Our%20key%20contributions%20are%20twofold%3A%20%281%29%20an%20in-context%0Adual-branch%20diffusion%20model%20that%20separates%20foreground%20and%20background%0Aprocessing%2C%20incorporating%20blob%20representations%20to%20explicitly%20decouple%20layout%0Aand%20appearance%2C%20and%20%282%29%20a%20self-supervised%20disentangle-then-reconstruct%20training%0Aparadigm%20with%20an%20identity-preserving%20loss%20function%2C%20along%20with%20tailored%0Astrategies%20to%20efficiently%20leverage%20blob-image%20pairs.%20To%20foster%20further%0Aresearch%2C%20we%20introduce%20BlobData%20for%20large-scale%20training%20and%20BlobBench%2C%20a%0Abenchmark%20for%20systematic%20evaluation.%20Experimental%20results%20demonstrate%20that%0ABlobCtrl%20achieves%20state-of-the-art%20performance%20in%20a%20variety%20of%20element-level%0Aediting%20tasks%2C%20such%20as%20object%20addition%2C%20removal%2C%20scaling%2C%20and%20replacement%2C%0Awhile%20maintaining%20computational%20efficiency.%20Project%20Webpage%3A%0Ahttps%3A//liyaowei-stu.github.io/project/BlobCtrl/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13434v2&entry.124074799=Read"},
{"title": "Image-Difficulty-Aware Evaluation of Super-Resolution Models", "author": "Atakan Topaloglu and Ahmet Bilican and Cansu Korkmaz and A. Murat Tekalp", "abstract": "  Image super-resolution models are commonly evaluated by average scores (over\nsome benchmark test sets), which fail to reflect the performance of these\nmodels on images of varying difficulty and that some models generate artifacts\non certain difficult images, which is not reflected by the average scores. We\npropose difficulty-aware performance evaluation procedures to better\ndifferentiate between SISR models that produce visually different results on\nsome images but yield close average performance scores over the entire test\nset. In particular, we propose two image-difficulty measures, the\nhigh-frequency index and rotation-invariant edge index, to predict those test\nimages, where a model would yield significantly better visual results over\nanother model, and an evaluation method where these visual differences are\nreflected on objective measures. Experimental results demonstrate the\neffectiveness of the proposed image-difficulty measures and evaluation\nmethodology.\n", "link": "http://arxiv.org/abs/2509.26398v2", "date": "2025-10-01", "relevancy": 1.5574, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.548}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5129}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Difficulty-Aware%20Evaluation%20of%20Super-Resolution%20Models&body=Title%3A%20Image-Difficulty-Aware%20Evaluation%20of%20Super-Resolution%20Models%0AAuthor%3A%20Atakan%20Topaloglu%20and%20Ahmet%20Bilican%20and%20Cansu%20Korkmaz%20and%20A.%20Murat%20Tekalp%0AAbstract%3A%20%20%20Image%20super-resolution%20models%20are%20commonly%20evaluated%20by%20average%20scores%20%28over%0Asome%20benchmark%20test%20sets%29%2C%20which%20fail%20to%20reflect%20the%20performance%20of%20these%0Amodels%20on%20images%20of%20varying%20difficulty%20and%20that%20some%20models%20generate%20artifacts%0Aon%20certain%20difficult%20images%2C%20which%20is%20not%20reflected%20by%20the%20average%20scores.%20We%0Apropose%20difficulty-aware%20performance%20evaluation%20procedures%20to%20better%0Adifferentiate%20between%20SISR%20models%20that%20produce%20visually%20different%20results%20on%0Asome%20images%20but%20yield%20close%20average%20performance%20scores%20over%20the%20entire%20test%0Aset.%20In%20particular%2C%20we%20propose%20two%20image-difficulty%20measures%2C%20the%0Ahigh-frequency%20index%20and%20rotation-invariant%20edge%20index%2C%20to%20predict%20those%20test%0Aimages%2C%20where%20a%20model%20would%20yield%20significantly%20better%20visual%20results%20over%0Aanother%20model%2C%20and%20an%20evaluation%20method%20where%20these%20visual%20differences%20are%0Areflected%20on%20objective%20measures.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20image-difficulty%20measures%20and%20evaluation%0Amethodology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Difficulty-Aware%2520Evaluation%2520of%2520Super-Resolution%2520Models%26entry.906535625%3DAtakan%2520Topaloglu%2520and%2520Ahmet%2520Bilican%2520and%2520Cansu%2520Korkmaz%2520and%2520A.%2520Murat%2520Tekalp%26entry.1292438233%3D%2520%2520Image%2520super-resolution%2520models%2520are%2520commonly%2520evaluated%2520by%2520average%2520scores%2520%2528over%250Asome%2520benchmark%2520test%2520sets%2529%252C%2520which%2520fail%2520to%2520reflect%2520the%2520performance%2520of%2520these%250Amodels%2520on%2520images%2520of%2520varying%2520difficulty%2520and%2520that%2520some%2520models%2520generate%2520artifacts%250Aon%2520certain%2520difficult%2520images%252C%2520which%2520is%2520not%2520reflected%2520by%2520the%2520average%2520scores.%2520We%250Apropose%2520difficulty-aware%2520performance%2520evaluation%2520procedures%2520to%2520better%250Adifferentiate%2520between%2520SISR%2520models%2520that%2520produce%2520visually%2520different%2520results%2520on%250Asome%2520images%2520but%2520yield%2520close%2520average%2520performance%2520scores%2520over%2520the%2520entire%2520test%250Aset.%2520In%2520particular%252C%2520we%2520propose%2520two%2520image-difficulty%2520measures%252C%2520the%250Ahigh-frequency%2520index%2520and%2520rotation-invariant%2520edge%2520index%252C%2520to%2520predict%2520those%2520test%250Aimages%252C%2520where%2520a%2520model%2520would%2520yield%2520significantly%2520better%2520visual%2520results%2520over%250Aanother%2520model%252C%2520and%2520an%2520evaluation%2520method%2520where%2520these%2520visual%2520differences%2520are%250Areflected%2520on%2520objective%2520measures.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520image-difficulty%2520measures%2520and%2520evaluation%250Amethodology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Difficulty-Aware%20Evaluation%20of%20Super-Resolution%20Models&entry.906535625=Atakan%20Topaloglu%20and%20Ahmet%20Bilican%20and%20Cansu%20Korkmaz%20and%20A.%20Murat%20Tekalp&entry.1292438233=%20%20Image%20super-resolution%20models%20are%20commonly%20evaluated%20by%20average%20scores%20%28over%0Asome%20benchmark%20test%20sets%29%2C%20which%20fail%20to%20reflect%20the%20performance%20of%20these%0Amodels%20on%20images%20of%20varying%20difficulty%20and%20that%20some%20models%20generate%20artifacts%0Aon%20certain%20difficult%20images%2C%20which%20is%20not%20reflected%20by%20the%20average%20scores.%20We%0Apropose%20difficulty-aware%20performance%20evaluation%20procedures%20to%20better%0Adifferentiate%20between%20SISR%20models%20that%20produce%20visually%20different%20results%20on%0Asome%20images%20but%20yield%20close%20average%20performance%20scores%20over%20the%20entire%20test%0Aset.%20In%20particular%2C%20we%20propose%20two%20image-difficulty%20measures%2C%20the%0Ahigh-frequency%20index%20and%20rotation-invariant%20edge%20index%2C%20to%20predict%20those%20test%0Aimages%2C%20where%20a%20model%20would%20yield%20significantly%20better%20visual%20results%20over%0Aanother%20model%2C%20and%20an%20evaluation%20method%20where%20these%20visual%20differences%20are%0Areflected%20on%20objective%20measures.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20image-difficulty%20measures%20and%20evaluation%0Amethodology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26398v2&entry.124074799=Read"},
{"title": "SEE: See Everything Every Time -- Adaptive Brightness Adjustment for\n  Broad Light Range Images via Events", "author": "Yunfan Lu and Xiaogang Xu and Hao Lu and Yanlin Qian and Pengteng Li and Huizai Yao and Bin Yang and Junyi Li and Qianyi Cai and Weiyu Guo and Hui Xiong", "abstract": "  Event cameras, with a high dynamic range exceeding $120dB$, significantly\noutperform traditional embedded cameras, robustly recording detailed changing\ninformation under various lighting conditions, including both low- and\nhigh-light situations. However, recent research on utilizing event data has\nprimarily focused on low-light image enhancement, neglecting image enhancement\nand brightness adjustment across a broader range of lighting conditions, such\nas normal or high illumination. Based on this, we propose a novel research\nquestion: how to employ events to enhance and adaptively adjust the brightness\nof images captured under broad lighting conditions? To investigate this\nquestion, we first collected a new dataset, SEE-600K, consisting of 610,126\nimages and corresponding events across 202 scenarios, each featuring an average\nof four lighting conditions with over a 1000-fold variation in illumination.\nSubsequently, we propose a framework that effectively utilizes events to\nsmoothly adjust image brightness through the use of prompts. Our framework\ncaptures color through sensor patterns, uses cross-attention to model events as\na brightness dictionary, and adjusts the image's dynamic range to form a broad\nlight-range representation (BLR), which is then decoded at the pixel level\nbased on the brightness prompt. Experimental results demonstrate that our\nmethod not only performs well on the low-light enhancement dataset but also\nshows robust performance on broader light-range image enhancement using the\nSEE-600K dataset. Additionally, our approach enables pixel-level brightness\nadjustment, providing flexibility for post-processing and inspiring more\nimaging applications. The dataset and source code are publicly available at:\nhttps://github.com/yunfanLu/SEE.\n", "link": "http://arxiv.org/abs/2502.21120v2", "date": "2025-10-01", "relevancy": 1.5346, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5222}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEE%3A%20See%20Everything%20Every%20Time%20--%20Adaptive%20Brightness%20Adjustment%20for%0A%20%20Broad%20Light%20Range%20Images%20via%20Events&body=Title%3A%20SEE%3A%20See%20Everything%20Every%20Time%20--%20Adaptive%20Brightness%20Adjustment%20for%0A%20%20Broad%20Light%20Range%20Images%20via%20Events%0AAuthor%3A%20Yunfan%20Lu%20and%20Xiaogang%20Xu%20and%20Hao%20Lu%20and%20Yanlin%20Qian%20and%20Pengteng%20Li%20and%20Huizai%20Yao%20and%20Bin%20Yang%20and%20Junyi%20Li%20and%20Qianyi%20Cai%20and%20Weiyu%20Guo%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Event%20cameras%2C%20with%20a%20high%20dynamic%20range%20exceeding%20%24120dB%24%2C%20significantly%0Aoutperform%20traditional%20embedded%20cameras%2C%20robustly%20recording%20detailed%20changing%0Ainformation%20under%20various%20lighting%20conditions%2C%20including%20both%20low-%20and%0Ahigh-light%20situations.%20However%2C%20recent%20research%20on%20utilizing%20event%20data%20has%0Aprimarily%20focused%20on%20low-light%20image%20enhancement%2C%20neglecting%20image%20enhancement%0Aand%20brightness%20adjustment%20across%20a%20broader%20range%20of%20lighting%20conditions%2C%20such%0Aas%20normal%20or%20high%20illumination.%20Based%20on%20this%2C%20we%20propose%20a%20novel%20research%0Aquestion%3A%20how%20to%20employ%20events%20to%20enhance%20and%20adaptively%20adjust%20the%20brightness%0Aof%20images%20captured%20under%20broad%20lighting%20conditions%3F%20To%20investigate%20this%0Aquestion%2C%20we%20first%20collected%20a%20new%20dataset%2C%20SEE-600K%2C%20consisting%20of%20610%2C126%0Aimages%20and%20corresponding%20events%20across%20202%20scenarios%2C%20each%20featuring%20an%20average%0Aof%20four%20lighting%20conditions%20with%20over%20a%201000-fold%20variation%20in%20illumination.%0ASubsequently%2C%20we%20propose%20a%20framework%20that%20effectively%20utilizes%20events%20to%0Asmoothly%20adjust%20image%20brightness%20through%20the%20use%20of%20prompts.%20Our%20framework%0Acaptures%20color%20through%20sensor%20patterns%2C%20uses%20cross-attention%20to%20model%20events%20as%0Aa%20brightness%20dictionary%2C%20and%20adjusts%20the%20image%27s%20dynamic%20range%20to%20form%20a%20broad%0Alight-range%20representation%20%28BLR%29%2C%20which%20is%20then%20decoded%20at%20the%20pixel%20level%0Abased%20on%20the%20brightness%20prompt.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20not%20only%20performs%20well%20on%20the%20low-light%20enhancement%20dataset%20but%20also%0Ashows%20robust%20performance%20on%20broader%20light-range%20image%20enhancement%20using%20the%0ASEE-600K%20dataset.%20Additionally%2C%20our%20approach%20enables%20pixel-level%20brightness%0Aadjustment%2C%20providing%20flexibility%20for%20post-processing%20and%20inspiring%20more%0Aimaging%20applications.%20The%20dataset%20and%20source%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/yunfanLu/SEE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.21120v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEE%253A%2520See%2520Everything%2520Every%2520Time%2520--%2520Adaptive%2520Brightness%2520Adjustment%2520for%250A%2520%2520Broad%2520Light%2520Range%2520Images%2520via%2520Events%26entry.906535625%3DYunfan%2520Lu%2520and%2520Xiaogang%2520Xu%2520and%2520Hao%2520Lu%2520and%2520Yanlin%2520Qian%2520and%2520Pengteng%2520Li%2520and%2520Huizai%2520Yao%2520and%2520Bin%2520Yang%2520and%2520Junyi%2520Li%2520and%2520Qianyi%2520Cai%2520and%2520Weiyu%2520Guo%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Event%2520cameras%252C%2520with%2520a%2520high%2520dynamic%2520range%2520exceeding%2520%2524120dB%2524%252C%2520significantly%250Aoutperform%2520traditional%2520embedded%2520cameras%252C%2520robustly%2520recording%2520detailed%2520changing%250Ainformation%2520under%2520various%2520lighting%2520conditions%252C%2520including%2520both%2520low-%2520and%250Ahigh-light%2520situations.%2520However%252C%2520recent%2520research%2520on%2520utilizing%2520event%2520data%2520has%250Aprimarily%2520focused%2520on%2520low-light%2520image%2520enhancement%252C%2520neglecting%2520image%2520enhancement%250Aand%2520brightness%2520adjustment%2520across%2520a%2520broader%2520range%2520of%2520lighting%2520conditions%252C%2520such%250Aas%2520normal%2520or%2520high%2520illumination.%2520Based%2520on%2520this%252C%2520we%2520propose%2520a%2520novel%2520research%250Aquestion%253A%2520how%2520to%2520employ%2520events%2520to%2520enhance%2520and%2520adaptively%2520adjust%2520the%2520brightness%250Aof%2520images%2520captured%2520under%2520broad%2520lighting%2520conditions%253F%2520To%2520investigate%2520this%250Aquestion%252C%2520we%2520first%2520collected%2520a%2520new%2520dataset%252C%2520SEE-600K%252C%2520consisting%2520of%2520610%252C126%250Aimages%2520and%2520corresponding%2520events%2520across%2520202%2520scenarios%252C%2520each%2520featuring%2520an%2520average%250Aof%2520four%2520lighting%2520conditions%2520with%2520over%2520a%25201000-fold%2520variation%2520in%2520illumination.%250ASubsequently%252C%2520we%2520propose%2520a%2520framework%2520that%2520effectively%2520utilizes%2520events%2520to%250Asmoothly%2520adjust%2520image%2520brightness%2520through%2520the%2520use%2520of%2520prompts.%2520Our%2520framework%250Acaptures%2520color%2520through%2520sensor%2520patterns%252C%2520uses%2520cross-attention%2520to%2520model%2520events%2520as%250Aa%2520brightness%2520dictionary%252C%2520and%2520adjusts%2520the%2520image%2527s%2520dynamic%2520range%2520to%2520form%2520a%2520broad%250Alight-range%2520representation%2520%2528BLR%2529%252C%2520which%2520is%2520then%2520decoded%2520at%2520the%2520pixel%2520level%250Abased%2520on%2520the%2520brightness%2520prompt.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520not%2520only%2520performs%2520well%2520on%2520the%2520low-light%2520enhancement%2520dataset%2520but%2520also%250Ashows%2520robust%2520performance%2520on%2520broader%2520light-range%2520image%2520enhancement%2520using%2520the%250ASEE-600K%2520dataset.%2520Additionally%252C%2520our%2520approach%2520enables%2520pixel-level%2520brightness%250Aadjustment%252C%2520providing%2520flexibility%2520for%2520post-processing%2520and%2520inspiring%2520more%250Aimaging%2520applications.%2520The%2520dataset%2520and%2520source%2520code%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/yunfanLu/SEE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.21120v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEE%3A%20See%20Everything%20Every%20Time%20--%20Adaptive%20Brightness%20Adjustment%20for%0A%20%20Broad%20Light%20Range%20Images%20via%20Events&entry.906535625=Yunfan%20Lu%20and%20Xiaogang%20Xu%20and%20Hao%20Lu%20and%20Yanlin%20Qian%20and%20Pengteng%20Li%20and%20Huizai%20Yao%20and%20Bin%20Yang%20and%20Junyi%20Li%20and%20Qianyi%20Cai%20and%20Weiyu%20Guo%20and%20Hui%20Xiong&entry.1292438233=%20%20Event%20cameras%2C%20with%20a%20high%20dynamic%20range%20exceeding%20%24120dB%24%2C%20significantly%0Aoutperform%20traditional%20embedded%20cameras%2C%20robustly%20recording%20detailed%20changing%0Ainformation%20under%20various%20lighting%20conditions%2C%20including%20both%20low-%20and%0Ahigh-light%20situations.%20However%2C%20recent%20research%20on%20utilizing%20event%20data%20has%0Aprimarily%20focused%20on%20low-light%20image%20enhancement%2C%20neglecting%20image%20enhancement%0Aand%20brightness%20adjustment%20across%20a%20broader%20range%20of%20lighting%20conditions%2C%20such%0Aas%20normal%20or%20high%20illumination.%20Based%20on%20this%2C%20we%20propose%20a%20novel%20research%0Aquestion%3A%20how%20to%20employ%20events%20to%20enhance%20and%20adaptively%20adjust%20the%20brightness%0Aof%20images%20captured%20under%20broad%20lighting%20conditions%3F%20To%20investigate%20this%0Aquestion%2C%20we%20first%20collected%20a%20new%20dataset%2C%20SEE-600K%2C%20consisting%20of%20610%2C126%0Aimages%20and%20corresponding%20events%20across%20202%20scenarios%2C%20each%20featuring%20an%20average%0Aof%20four%20lighting%20conditions%20with%20over%20a%201000-fold%20variation%20in%20illumination.%0ASubsequently%2C%20we%20propose%20a%20framework%20that%20effectively%20utilizes%20events%20to%0Asmoothly%20adjust%20image%20brightness%20through%20the%20use%20of%20prompts.%20Our%20framework%0Acaptures%20color%20through%20sensor%20patterns%2C%20uses%20cross-attention%20to%20model%20events%20as%0Aa%20brightness%20dictionary%2C%20and%20adjusts%20the%20image%27s%20dynamic%20range%20to%20form%20a%20broad%0Alight-range%20representation%20%28BLR%29%2C%20which%20is%20then%20decoded%20at%20the%20pixel%20level%0Abased%20on%20the%20brightness%20prompt.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20not%20only%20performs%20well%20on%20the%20low-light%20enhancement%20dataset%20but%20also%0Ashows%20robust%20performance%20on%20broader%20light-range%20image%20enhancement%20using%20the%0ASEE-600K%20dataset.%20Additionally%2C%20our%20approach%20enables%20pixel-level%20brightness%0Aadjustment%2C%20providing%20flexibility%20for%20post-processing%20and%20inspiring%20more%0Aimaging%20applications.%20The%20dataset%20and%20source%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/yunfanLu/SEE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.21120v2&entry.124074799=Read"},
{"title": "Streamline pathology foundation model by cross-magnification\n  distillation", "author": "Ziyu Su and Abdul Rehman Akbar and Usama Sajjad and Anil V. Parwani and Muhammad Khalid Khan Niazi", "abstract": "  Foundation models (FM) have transformed computational pathology but remain\ncomputationally prohibitive for clinical deployment due to their massive\nparameter counts and high-magnification processing requirements. Here, we\nintroduce XMAG, a lightweight FM developed through corss-magnification\ndistillation that transfers knowledge from state-of-the-art 20x magnification\nteacher to an efficient 5x magnification student architecture. XMAG employs a\ncompact backbone and operates entirely at 5x, requiring 11.3 times fewer\npatches per whole slide image (WSI) compared to existing approaches. Our Novel\ndistillation framework incorporates dual-level knowledge transfer, aligning\nboth global image representations and local spatial token mapping. We trained\nXMAG on 3.49 million images curated from publicly available datasets and\nevaluated performance across six clinically relevant histopathology analysis\ntasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within\n1% of substantially larger foundation models while delivering 30-fold\nprocessing acceleration, reaching 8.8 WSIs per minute processing speed. Our\ncross-institutional validation confirmed robust generalization. Further, we\ndeveloped an end-to-end training strategy to further boost our model's\nperformance to approach the larger FMs' performance. These results establish\ncross-magnification distillation as a viable approach for deploying FM\ncapabilities in resource-constrained clinical environments, potentially\nenabling real-time pathology AI integration.\n", "link": "http://arxiv.org/abs/2509.23097v2", "date": "2025-10-01", "relevancy": 1.5345, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streamline%20pathology%20foundation%20model%20by%20cross-magnification%0A%20%20distillation&body=Title%3A%20Streamline%20pathology%20foundation%20model%20by%20cross-magnification%0A%20%20distillation%0AAuthor%3A%20Ziyu%20Su%20and%20Abdul%20Rehman%20Akbar%20and%20Usama%20Sajjad%20and%20Anil%20V.%20Parwani%20and%20Muhammad%20Khalid%20Khan%20Niazi%0AAbstract%3A%20%20%20Foundation%20models%20%28FM%29%20have%20transformed%20computational%20pathology%20but%20remain%0Acomputationally%20prohibitive%20for%20clinical%20deployment%20due%20to%20their%20massive%0Aparameter%20counts%20and%20high-magnification%20processing%20requirements.%20Here%2C%20we%0Aintroduce%20XMAG%2C%20a%20lightweight%20FM%20developed%20through%20corss-magnification%0Adistillation%20that%20transfers%20knowledge%20from%20state-of-the-art%2020x%20magnification%0Ateacher%20to%20an%20efficient%205x%20magnification%20student%20architecture.%20XMAG%20employs%20a%0Acompact%20backbone%20and%20operates%20entirely%20at%205x%2C%20requiring%2011.3%20times%20fewer%0Apatches%20per%20whole%20slide%20image%20%28WSI%29%20compared%20to%20existing%20approaches.%20Our%20Novel%0Adistillation%20framework%20incorporates%20dual-level%20knowledge%20transfer%2C%20aligning%0Aboth%20global%20image%20representations%20and%20local%20spatial%20token%20mapping.%20We%20trained%0AXMAG%20on%203.49%20million%20images%20curated%20from%20publicly%20available%20datasets%20and%0Aevaluated%20performance%20across%20six%20clinically%20relevant%20histopathology%20analysis%0Atasks%20spanning%20multiple%20cancer%20types.%20XMAG%20achieved%20diagnostic%20accuracy%20within%0A1%25%20of%20substantially%20larger%20foundation%20models%20while%20delivering%2030-fold%0Aprocessing%20acceleration%2C%20reaching%208.8%20WSIs%20per%20minute%20processing%20speed.%20Our%0Across-institutional%20validation%20confirmed%20robust%20generalization.%20Further%2C%20we%0Adeveloped%20an%20end-to-end%20training%20strategy%20to%20further%20boost%20our%20model%27s%0Aperformance%20to%20approach%20the%20larger%20FMs%27%20performance.%20These%20results%20establish%0Across-magnification%20distillation%20as%20a%20viable%20approach%20for%20deploying%20FM%0Acapabilities%20in%20resource-constrained%20clinical%20environments%2C%20potentially%0Aenabling%20real-time%20pathology%20AI%20integration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamline%2520pathology%2520foundation%2520model%2520by%2520cross-magnification%250A%2520%2520distillation%26entry.906535625%3DZiyu%2520Su%2520and%2520Abdul%2520Rehman%2520Akbar%2520and%2520Usama%2520Sajjad%2520and%2520Anil%2520V.%2520Parwani%2520and%2520Muhammad%2520Khalid%2520Khan%2520Niazi%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FM%2529%2520have%2520transformed%2520computational%2520pathology%2520but%2520remain%250Acomputationally%2520prohibitive%2520for%2520clinical%2520deployment%2520due%2520to%2520their%2520massive%250Aparameter%2520counts%2520and%2520high-magnification%2520processing%2520requirements.%2520Here%252C%2520we%250Aintroduce%2520XMAG%252C%2520a%2520lightweight%2520FM%2520developed%2520through%2520corss-magnification%250Adistillation%2520that%2520transfers%2520knowledge%2520from%2520state-of-the-art%252020x%2520magnification%250Ateacher%2520to%2520an%2520efficient%25205x%2520magnification%2520student%2520architecture.%2520XMAG%2520employs%2520a%250Acompact%2520backbone%2520and%2520operates%2520entirely%2520at%25205x%252C%2520requiring%252011.3%2520times%2520fewer%250Apatches%2520per%2520whole%2520slide%2520image%2520%2528WSI%2529%2520compared%2520to%2520existing%2520approaches.%2520Our%2520Novel%250Adistillation%2520framework%2520incorporates%2520dual-level%2520knowledge%2520transfer%252C%2520aligning%250Aboth%2520global%2520image%2520representations%2520and%2520local%2520spatial%2520token%2520mapping.%2520We%2520trained%250AXMAG%2520on%25203.49%2520million%2520images%2520curated%2520from%2520publicly%2520available%2520datasets%2520and%250Aevaluated%2520performance%2520across%2520six%2520clinically%2520relevant%2520histopathology%2520analysis%250Atasks%2520spanning%2520multiple%2520cancer%2520types.%2520XMAG%2520achieved%2520diagnostic%2520accuracy%2520within%250A1%2525%2520of%2520substantially%2520larger%2520foundation%2520models%2520while%2520delivering%252030-fold%250Aprocessing%2520acceleration%252C%2520reaching%25208.8%2520WSIs%2520per%2520minute%2520processing%2520speed.%2520Our%250Across-institutional%2520validation%2520confirmed%2520robust%2520generalization.%2520Further%252C%2520we%250Adeveloped%2520an%2520end-to-end%2520training%2520strategy%2520to%2520further%2520boost%2520our%2520model%2527s%250Aperformance%2520to%2520approach%2520the%2520larger%2520FMs%2527%2520performance.%2520These%2520results%2520establish%250Across-magnification%2520distillation%2520as%2520a%2520viable%2520approach%2520for%2520deploying%2520FM%250Acapabilities%2520in%2520resource-constrained%2520clinical%2520environments%252C%2520potentially%250Aenabling%2520real-time%2520pathology%2520AI%2520integration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamline%20pathology%20foundation%20model%20by%20cross-magnification%0A%20%20distillation&entry.906535625=Ziyu%20Su%20and%20Abdul%20Rehman%20Akbar%20and%20Usama%20Sajjad%20and%20Anil%20V.%20Parwani%20and%20Muhammad%20Khalid%20Khan%20Niazi&entry.1292438233=%20%20Foundation%20models%20%28FM%29%20have%20transformed%20computational%20pathology%20but%20remain%0Acomputationally%20prohibitive%20for%20clinical%20deployment%20due%20to%20their%20massive%0Aparameter%20counts%20and%20high-magnification%20processing%20requirements.%20Here%2C%20we%0Aintroduce%20XMAG%2C%20a%20lightweight%20FM%20developed%20through%20corss-magnification%0Adistillation%20that%20transfers%20knowledge%20from%20state-of-the-art%2020x%20magnification%0Ateacher%20to%20an%20efficient%205x%20magnification%20student%20architecture.%20XMAG%20employs%20a%0Acompact%20backbone%20and%20operates%20entirely%20at%205x%2C%20requiring%2011.3%20times%20fewer%0Apatches%20per%20whole%20slide%20image%20%28WSI%29%20compared%20to%20existing%20approaches.%20Our%20Novel%0Adistillation%20framework%20incorporates%20dual-level%20knowledge%20transfer%2C%20aligning%0Aboth%20global%20image%20representations%20and%20local%20spatial%20token%20mapping.%20We%20trained%0AXMAG%20on%203.49%20million%20images%20curated%20from%20publicly%20available%20datasets%20and%0Aevaluated%20performance%20across%20six%20clinically%20relevant%20histopathology%20analysis%0Atasks%20spanning%20multiple%20cancer%20types.%20XMAG%20achieved%20diagnostic%20accuracy%20within%0A1%25%20of%20substantially%20larger%20foundation%20models%20while%20delivering%2030-fold%0Aprocessing%20acceleration%2C%20reaching%208.8%20WSIs%20per%20minute%20processing%20speed.%20Our%0Across-institutional%20validation%20confirmed%20robust%20generalization.%20Further%2C%20we%0Adeveloped%20an%20end-to-end%20training%20strategy%20to%20further%20boost%20our%20model%27s%0Aperformance%20to%20approach%20the%20larger%20FMs%27%20performance.%20These%20results%20establish%0Across-magnification%20distillation%20as%20a%20viable%20approach%20for%20deploying%20FM%0Acapabilities%20in%20resource-constrained%20clinical%20environments%2C%20potentially%0Aenabling%20real-time%20pathology%20AI%20integration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23097v2&entry.124074799=Read"},
{"title": "Nonlinear Framework for Speech Bandwidth Extension", "author": "Tarikul Islam Tamiti and Nursad Mamun and Anomadarshi Barua", "abstract": "  Recovering high-frequency components lost to bandwidth constraints is crucial\nfor applications ranging from telecommunications to high-fidelity audio on\nlimited resources. We introduce NDSI-BWE, a new adversarial Band Width\nExtension (BWE) framework that leverage four new discriminators inspired by\nnonlinear dynamical system to capture diverse temporal behaviors: a\nMulti-Resolution Lyapunov Discriminator (MRLD) for determining sensitivity to\ninitial conditions by capturing deterministic chaos, a Multi-Scale Recurrence\nDiscriminator (MS-RD) for self-similar recurrence dynamics, a Multi-Scale\nDetrended Fractal Analysis Discriminator (MSDFA) for long range slow variant\nscale invariant relationship, a Multi-Resolution Poincar\\'e Plot Discriminator\n(MR-PPD) for capturing hidden latent space relationship, a Multi-Period\nDiscriminator (MPD) for cyclical patterns, a Multi-Resolution Amplitude\nDiscriminator (MRAD) and Multi-Resolution Phase Discriminator (MRPD) for\ncapturing intricate amplitude-phase transition statistics. By using depth-wise\nconvolution at the core of the convolutional block with in each discriminators,\nNDSI-BWE attains an eight-times parameter reduction. These seven discriminators\nguide a complex-valued ConformerNeXt based genetor with a dual stream\nLattice-Net based architecture for simultaneous refinement of magnitude and\nphase. The genertor leverage the transformer based conformer's global\ndependency modeling and ConvNeXt block's local temporal modeling capability.\nAcross six objective evaluation metrics and subjective based texts comprises of\nfive human judges, NDSI-BWE establishes a new SoTA in BWE.\n", "link": "http://arxiv.org/abs/2507.15970v2", "date": "2025-10-01", "relevancy": 1.5041, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5036}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20Framework%20for%20Speech%20Bandwidth%20Extension&body=Title%3A%20Nonlinear%20Framework%20for%20Speech%20Bandwidth%20Extension%0AAuthor%3A%20Tarikul%20Islam%20Tamiti%20and%20Nursad%20Mamun%20and%20Anomadarshi%20Barua%0AAbstract%3A%20%20%20Recovering%20high-frequency%20components%20lost%20to%20bandwidth%20constraints%20is%20crucial%0Afor%20applications%20ranging%20from%20telecommunications%20to%20high-fidelity%20audio%20on%0Alimited%20resources.%20We%20introduce%20NDSI-BWE%2C%20a%20new%20adversarial%20Band%20Width%0AExtension%20%28BWE%29%20framework%20that%20leverage%20four%20new%20discriminators%20inspired%20by%0Anonlinear%20dynamical%20system%20to%20capture%20diverse%20temporal%20behaviors%3A%20a%0AMulti-Resolution%20Lyapunov%20Discriminator%20%28MRLD%29%20for%20determining%20sensitivity%20to%0Ainitial%20conditions%20by%20capturing%20deterministic%20chaos%2C%20a%20Multi-Scale%20Recurrence%0ADiscriminator%20%28MS-RD%29%20for%20self-similar%20recurrence%20dynamics%2C%20a%20Multi-Scale%0ADetrended%20Fractal%20Analysis%20Discriminator%20%28MSDFA%29%20for%20long%20range%20slow%20variant%0Ascale%20invariant%20relationship%2C%20a%20Multi-Resolution%20Poincar%5C%27e%20Plot%20Discriminator%0A%28MR-PPD%29%20for%20capturing%20hidden%20latent%20space%20relationship%2C%20a%20Multi-Period%0ADiscriminator%20%28MPD%29%20for%20cyclical%20patterns%2C%20a%20Multi-Resolution%20Amplitude%0ADiscriminator%20%28MRAD%29%20and%20Multi-Resolution%20Phase%20Discriminator%20%28MRPD%29%20for%0Acapturing%20intricate%20amplitude-phase%20transition%20statistics.%20By%20using%20depth-wise%0Aconvolution%20at%20the%20core%20of%20the%20convolutional%20block%20with%20in%20each%20discriminators%2C%0ANDSI-BWE%20attains%20an%20eight-times%20parameter%20reduction.%20These%20seven%20discriminators%0Aguide%20a%20complex-valued%20ConformerNeXt%20based%20genetor%20with%20a%20dual%20stream%0ALattice-Net%20based%20architecture%20for%20simultaneous%20refinement%20of%20magnitude%20and%0Aphase.%20The%20genertor%20leverage%20the%20transformer%20based%20conformer%27s%20global%0Adependency%20modeling%20and%20ConvNeXt%20block%27s%20local%20temporal%20modeling%20capability.%0AAcross%20six%20objective%20evaluation%20metrics%20and%20subjective%20based%20texts%20comprises%20of%0Afive%20human%20judges%2C%20NDSI-BWE%20establishes%20a%20new%20SoTA%20in%20BWE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520Framework%2520for%2520Speech%2520Bandwidth%2520Extension%26entry.906535625%3DTarikul%2520Islam%2520Tamiti%2520and%2520Nursad%2520Mamun%2520and%2520Anomadarshi%2520Barua%26entry.1292438233%3D%2520%2520Recovering%2520high-frequency%2520components%2520lost%2520to%2520bandwidth%2520constraints%2520is%2520crucial%250Afor%2520applications%2520ranging%2520from%2520telecommunications%2520to%2520high-fidelity%2520audio%2520on%250Alimited%2520resources.%2520We%2520introduce%2520NDSI-BWE%252C%2520a%2520new%2520adversarial%2520Band%2520Width%250AExtension%2520%2528BWE%2529%2520framework%2520that%2520leverage%2520four%2520new%2520discriminators%2520inspired%2520by%250Anonlinear%2520dynamical%2520system%2520to%2520capture%2520diverse%2520temporal%2520behaviors%253A%2520a%250AMulti-Resolution%2520Lyapunov%2520Discriminator%2520%2528MRLD%2529%2520for%2520determining%2520sensitivity%2520to%250Ainitial%2520conditions%2520by%2520capturing%2520deterministic%2520chaos%252C%2520a%2520Multi-Scale%2520Recurrence%250ADiscriminator%2520%2528MS-RD%2529%2520for%2520self-similar%2520recurrence%2520dynamics%252C%2520a%2520Multi-Scale%250ADetrended%2520Fractal%2520Analysis%2520Discriminator%2520%2528MSDFA%2529%2520for%2520long%2520range%2520slow%2520variant%250Ascale%2520invariant%2520relationship%252C%2520a%2520Multi-Resolution%2520Poincar%255C%2527e%2520Plot%2520Discriminator%250A%2528MR-PPD%2529%2520for%2520capturing%2520hidden%2520latent%2520space%2520relationship%252C%2520a%2520Multi-Period%250ADiscriminator%2520%2528MPD%2529%2520for%2520cyclical%2520patterns%252C%2520a%2520Multi-Resolution%2520Amplitude%250ADiscriminator%2520%2528MRAD%2529%2520and%2520Multi-Resolution%2520Phase%2520Discriminator%2520%2528MRPD%2529%2520for%250Acapturing%2520intricate%2520amplitude-phase%2520transition%2520statistics.%2520By%2520using%2520depth-wise%250Aconvolution%2520at%2520the%2520core%2520of%2520the%2520convolutional%2520block%2520with%2520in%2520each%2520discriminators%252C%250ANDSI-BWE%2520attains%2520an%2520eight-times%2520parameter%2520reduction.%2520These%2520seven%2520discriminators%250Aguide%2520a%2520complex-valued%2520ConformerNeXt%2520based%2520genetor%2520with%2520a%2520dual%2520stream%250ALattice-Net%2520based%2520architecture%2520for%2520simultaneous%2520refinement%2520of%2520magnitude%2520and%250Aphase.%2520The%2520genertor%2520leverage%2520the%2520transformer%2520based%2520conformer%2527s%2520global%250Adependency%2520modeling%2520and%2520ConvNeXt%2520block%2527s%2520local%2520temporal%2520modeling%2520capability.%250AAcross%2520six%2520objective%2520evaluation%2520metrics%2520and%2520subjective%2520based%2520texts%2520comprises%2520of%250Afive%2520human%2520judges%252C%2520NDSI-BWE%2520establishes%2520a%2520new%2520SoTA%2520in%2520BWE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20Framework%20for%20Speech%20Bandwidth%20Extension&entry.906535625=Tarikul%20Islam%20Tamiti%20and%20Nursad%20Mamun%20and%20Anomadarshi%20Barua&entry.1292438233=%20%20Recovering%20high-frequency%20components%20lost%20to%20bandwidth%20constraints%20is%20crucial%0Afor%20applications%20ranging%20from%20telecommunications%20to%20high-fidelity%20audio%20on%0Alimited%20resources.%20We%20introduce%20NDSI-BWE%2C%20a%20new%20adversarial%20Band%20Width%0AExtension%20%28BWE%29%20framework%20that%20leverage%20four%20new%20discriminators%20inspired%20by%0Anonlinear%20dynamical%20system%20to%20capture%20diverse%20temporal%20behaviors%3A%20a%0AMulti-Resolution%20Lyapunov%20Discriminator%20%28MRLD%29%20for%20determining%20sensitivity%20to%0Ainitial%20conditions%20by%20capturing%20deterministic%20chaos%2C%20a%20Multi-Scale%20Recurrence%0ADiscriminator%20%28MS-RD%29%20for%20self-similar%20recurrence%20dynamics%2C%20a%20Multi-Scale%0ADetrended%20Fractal%20Analysis%20Discriminator%20%28MSDFA%29%20for%20long%20range%20slow%20variant%0Ascale%20invariant%20relationship%2C%20a%20Multi-Resolution%20Poincar%5C%27e%20Plot%20Discriminator%0A%28MR-PPD%29%20for%20capturing%20hidden%20latent%20space%20relationship%2C%20a%20Multi-Period%0ADiscriminator%20%28MPD%29%20for%20cyclical%20patterns%2C%20a%20Multi-Resolution%20Amplitude%0ADiscriminator%20%28MRAD%29%20and%20Multi-Resolution%20Phase%20Discriminator%20%28MRPD%29%20for%0Acapturing%20intricate%20amplitude-phase%20transition%20statistics.%20By%20using%20depth-wise%0Aconvolution%20at%20the%20core%20of%20the%20convolutional%20block%20with%20in%20each%20discriminators%2C%0ANDSI-BWE%20attains%20an%20eight-times%20parameter%20reduction.%20These%20seven%20discriminators%0Aguide%20a%20complex-valued%20ConformerNeXt%20based%20genetor%20with%20a%20dual%20stream%0ALattice-Net%20based%20architecture%20for%20simultaneous%20refinement%20of%20magnitude%20and%0Aphase.%20The%20genertor%20leverage%20the%20transformer%20based%20conformer%27s%20global%0Adependency%20modeling%20and%20ConvNeXt%20block%27s%20local%20temporal%20modeling%20capability.%0AAcross%20six%20objective%20evaluation%20metrics%20and%20subjective%20based%20texts%20comprises%20of%0Afive%20human%20judges%2C%20NDSI-BWE%20establishes%20a%20new%20SoTA%20in%20BWE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15970v2&entry.124074799=Read"},
{"title": "A Framework for Double-Blind Federated Adaptation of Foundation Models", "author": "Nurbek Tastan and Karthik Nandakumar", "abstract": "  Foundation models (FMs) excel in zero-shot tasks but benefit from\ntask-specific adaptation. However, privacy concerns prevent data sharing among\nmultiple data owners, and proprietary restrictions prevent the learning service\nprovider (LSP) from sharing the FM. In this work, we propose BlindFed, a\nframework enabling collaborative FM adaptation while protecting both parties:\ndata owners do not access the FM or each other's data, and the LSP does not see\nsensitive task data. BlindFed relies on fully homomorphic encryption (FHE) and\nconsists of three key innovations: (i) FHE-friendly architectural modifications\nvia polynomial approximations and low-rank adapters, (ii) a two-stage split\nlearning approach combining offline knowledge distillation and online encrypted\ninference for adapter training without backpropagation through the FM, and\n(iii) a privacy-boosting scheme using sample permutations and stochastic block\nsampling to mitigate model extraction attacks. Empirical results on four image\nclassification datasets demonstrate the practical feasibility of the BlindFed\nframework, albeit at a high communication cost and large computational\ncomplexity for the LSP.\n", "link": "http://arxiv.org/abs/2502.01289v2", "date": "2025-10-01", "relevancy": 1.5008, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4828}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Double-Blind%20Federated%20Adaptation%20of%20Foundation%20Models&body=Title%3A%20A%20Framework%20for%20Double-Blind%20Federated%20Adaptation%20of%20Foundation%20Models%0AAuthor%3A%20Nurbek%20Tastan%20and%20Karthik%20Nandakumar%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20excel%20in%20zero-shot%20tasks%20but%20benefit%20from%0Atask-specific%20adaptation.%20However%2C%20privacy%20concerns%20prevent%20data%20sharing%20among%0Amultiple%20data%20owners%2C%20and%20proprietary%20restrictions%20prevent%20the%20learning%20service%0Aprovider%20%28LSP%29%20from%20sharing%20the%20FM.%20In%20this%20work%2C%20we%20propose%20BlindFed%2C%20a%0Aframework%20enabling%20collaborative%20FM%20adaptation%20while%20protecting%20both%20parties%3A%0Adata%20owners%20do%20not%20access%20the%20FM%20or%20each%20other%27s%20data%2C%20and%20the%20LSP%20does%20not%20see%0Asensitive%20task%20data.%20BlindFed%20relies%20on%20fully%20homomorphic%20encryption%20%28FHE%29%20and%0Aconsists%20of%20three%20key%20innovations%3A%20%28i%29%20FHE-friendly%20architectural%20modifications%0Avia%20polynomial%20approximations%20and%20low-rank%20adapters%2C%20%28ii%29%20a%20two-stage%20split%0Alearning%20approach%20combining%20offline%20knowledge%20distillation%20and%20online%20encrypted%0Ainference%20for%20adapter%20training%20without%20backpropagation%20through%20the%20FM%2C%20and%0A%28iii%29%20a%20privacy-boosting%20scheme%20using%20sample%20permutations%20and%20stochastic%20block%0Asampling%20to%20mitigate%20model%20extraction%20attacks.%20Empirical%20results%20on%20four%20image%0Aclassification%20datasets%20demonstrate%20the%20practical%20feasibility%20of%20the%20BlindFed%0Aframework%2C%20albeit%20at%20a%20high%20communication%20cost%20and%20large%20computational%0Acomplexity%20for%20the%20LSP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Double-Blind%2520Federated%2520Adaptation%2520of%2520Foundation%2520Models%26entry.906535625%3DNurbek%2520Tastan%2520and%2520Karthik%2520Nandakumar%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520excel%2520in%2520zero-shot%2520tasks%2520but%2520benefit%2520from%250Atask-specific%2520adaptation.%2520However%252C%2520privacy%2520concerns%2520prevent%2520data%2520sharing%2520among%250Amultiple%2520data%2520owners%252C%2520and%2520proprietary%2520restrictions%2520prevent%2520the%2520learning%2520service%250Aprovider%2520%2528LSP%2529%2520from%2520sharing%2520the%2520FM.%2520In%2520this%2520work%252C%2520we%2520propose%2520BlindFed%252C%2520a%250Aframework%2520enabling%2520collaborative%2520FM%2520adaptation%2520while%2520protecting%2520both%2520parties%253A%250Adata%2520owners%2520do%2520not%2520access%2520the%2520FM%2520or%2520each%2520other%2527s%2520data%252C%2520and%2520the%2520LSP%2520does%2520not%2520see%250Asensitive%2520task%2520data.%2520BlindFed%2520relies%2520on%2520fully%2520homomorphic%2520encryption%2520%2528FHE%2529%2520and%250Aconsists%2520of%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520FHE-friendly%2520architectural%2520modifications%250Avia%2520polynomial%2520approximations%2520and%2520low-rank%2520adapters%252C%2520%2528ii%2529%2520a%2520two-stage%2520split%250Alearning%2520approach%2520combining%2520offline%2520knowledge%2520distillation%2520and%2520online%2520encrypted%250Ainference%2520for%2520adapter%2520training%2520without%2520backpropagation%2520through%2520the%2520FM%252C%2520and%250A%2528iii%2529%2520a%2520privacy-boosting%2520scheme%2520using%2520sample%2520permutations%2520and%2520stochastic%2520block%250Asampling%2520to%2520mitigate%2520model%2520extraction%2520attacks.%2520Empirical%2520results%2520on%2520four%2520image%250Aclassification%2520datasets%2520demonstrate%2520the%2520practical%2520feasibility%2520of%2520the%2520BlindFed%250Aframework%252C%2520albeit%2520at%2520a%2520high%2520communication%2520cost%2520and%2520large%2520computational%250Acomplexity%2520for%2520the%2520LSP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Double-Blind%20Federated%20Adaptation%20of%20Foundation%20Models&entry.906535625=Nurbek%20Tastan%20and%20Karthik%20Nandakumar&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20excel%20in%20zero-shot%20tasks%20but%20benefit%20from%0Atask-specific%20adaptation.%20However%2C%20privacy%20concerns%20prevent%20data%20sharing%20among%0Amultiple%20data%20owners%2C%20and%20proprietary%20restrictions%20prevent%20the%20learning%20service%0Aprovider%20%28LSP%29%20from%20sharing%20the%20FM.%20In%20this%20work%2C%20we%20propose%20BlindFed%2C%20a%0Aframework%20enabling%20collaborative%20FM%20adaptation%20while%20protecting%20both%20parties%3A%0Adata%20owners%20do%20not%20access%20the%20FM%20or%20each%20other%27s%20data%2C%20and%20the%20LSP%20does%20not%20see%0Asensitive%20task%20data.%20BlindFed%20relies%20on%20fully%20homomorphic%20encryption%20%28FHE%29%20and%0Aconsists%20of%20three%20key%20innovations%3A%20%28i%29%20FHE-friendly%20architectural%20modifications%0Avia%20polynomial%20approximations%20and%20low-rank%20adapters%2C%20%28ii%29%20a%20two-stage%20split%0Alearning%20approach%20combining%20offline%20knowledge%20distillation%20and%20online%20encrypted%0Ainference%20for%20adapter%20training%20without%20backpropagation%20through%20the%20FM%2C%20and%0A%28iii%29%20a%20privacy-boosting%20scheme%20using%20sample%20permutations%20and%20stochastic%20block%0Asampling%20to%20mitigate%20model%20extraction%20attacks.%20Empirical%20results%20on%20four%20image%0Aclassification%20datasets%20demonstrate%20the%20practical%20feasibility%20of%20the%20BlindFed%0Aframework%2C%20albeit%20at%20a%20high%20communication%20cost%20and%20large%20computational%0Acomplexity%20for%20the%20LSP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01289v2&entry.124074799=Read"},
{"title": "Addressing Moral Uncertainty using Large Language Models for Ethical\n  Decision-Making", "author": "Rohit K. Dubey and Damian Dailisan and Sachit Mahajan", "abstract": "  We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications.\n", "link": "http://arxiv.org/abs/2503.05724v2", "date": "2025-10-01", "relevancy": 1.4932, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20Moral%20Uncertainty%20using%20Large%20Language%20Models%20for%20Ethical%0A%20%20Decision-Making&body=Title%3A%20Addressing%20Moral%20Uncertainty%20using%20Large%20Language%20Models%20for%20Ethical%0A%20%20Decision-Making%0AAuthor%3A%20Rohit%20K.%20Dubey%20and%20Damian%20Dailisan%20and%20Sachit%20Mahajan%0AAbstract%3A%20%20%20We%20present%20an%20ethical%20decision-making%20framework%20that%20refines%20a%20pre-trained%0Areinforcement%20learning%20%28RL%29%20model%20using%20a%20task-agnostic%20ethical%20layer.%0AFollowing%20initial%20training%2C%20the%20RL%20model%20undergoes%20ethical%20fine-tuning%2C%20where%0Ahuman%20feedback%20is%20replaced%20by%20feedback%20generated%20from%20a%20large%20language%20model%0A%28LLM%29.%20The%20LLM%20embodies%20consequentialist%2C%20deontological%2C%20virtue%2C%20social%0Ajustice%2C%20and%20care%20ethics%20as%20moral%20principles%20to%20assign%20belief%20values%20to%0Arecommended%20actions%20during%20ethical%20decision-making.%20An%20ethical%20layer%20aggregates%0Abelief%20scores%20from%20multiple%20LLM-derived%20moral%20perspectives%20using%20Belief%0AJensen-Shannon%20Divergence%20and%20Dempster-Shafer%20Theory%20into%20probability%20scores%0Athat%20also%20serve%20as%20the%20shaping%20reward%2C%20steering%20the%20agent%20toward%20choices%20that%0Aalign%20with%20a%20balanced%20ethical%20framework.%20This%20integrated%20learning%20framework%0Ahelps%20the%20RL%20agent%20navigate%20moral%20uncertainty%20in%20complex%20environments%20and%0Aenables%20it%20to%20make%20morally%20sound%20decisions%20across%20diverse%20tasks.%20Our%20approach%2C%0Atested%20across%20different%20LLM%20variants%20and%20compared%20with%20other%20belief%20aggregation%0Atechniques%2C%20demonstrates%20improved%20consistency%2C%20adaptability%2C%20and%20reduced%0Areliance%20on%20handcrafted%20ethical%20rewards.%20This%20method%20is%20especially%20effective%20in%0Adynamic%20scenarios%20where%20ethical%20challenges%20arise%20unexpectedly%2C%20making%20it%0Awell-suited%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520Moral%2520Uncertainty%2520using%2520Large%2520Language%2520Models%2520for%2520Ethical%250A%2520%2520Decision-Making%26entry.906535625%3DRohit%2520K.%2520Dubey%2520and%2520Damian%2520Dailisan%2520and%2520Sachit%2520Mahajan%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520ethical%2520decision-making%2520framework%2520that%2520refines%2520a%2520pre-trained%250Areinforcement%2520learning%2520%2528RL%2529%2520model%2520using%2520a%2520task-agnostic%2520ethical%2520layer.%250AFollowing%2520initial%2520training%252C%2520the%2520RL%2520model%2520undergoes%2520ethical%2520fine-tuning%252C%2520where%250Ahuman%2520feedback%2520is%2520replaced%2520by%2520feedback%2520generated%2520from%2520a%2520large%2520language%2520model%250A%2528LLM%2529.%2520The%2520LLM%2520embodies%2520consequentialist%252C%2520deontological%252C%2520virtue%252C%2520social%250Ajustice%252C%2520and%2520care%2520ethics%2520as%2520moral%2520principles%2520to%2520assign%2520belief%2520values%2520to%250Arecommended%2520actions%2520during%2520ethical%2520decision-making.%2520An%2520ethical%2520layer%2520aggregates%250Abelief%2520scores%2520from%2520multiple%2520LLM-derived%2520moral%2520perspectives%2520using%2520Belief%250AJensen-Shannon%2520Divergence%2520and%2520Dempster-Shafer%2520Theory%2520into%2520probability%2520scores%250Athat%2520also%2520serve%2520as%2520the%2520shaping%2520reward%252C%2520steering%2520the%2520agent%2520toward%2520choices%2520that%250Aalign%2520with%2520a%2520balanced%2520ethical%2520framework.%2520This%2520integrated%2520learning%2520framework%250Ahelps%2520the%2520RL%2520agent%2520navigate%2520moral%2520uncertainty%2520in%2520complex%2520environments%2520and%250Aenables%2520it%2520to%2520make%2520morally%2520sound%2520decisions%2520across%2520diverse%2520tasks.%2520Our%2520approach%252C%250Atested%2520across%2520different%2520LLM%2520variants%2520and%2520compared%2520with%2520other%2520belief%2520aggregation%250Atechniques%252C%2520demonstrates%2520improved%2520consistency%252C%2520adaptability%252C%2520and%2520reduced%250Areliance%2520on%2520handcrafted%2520ethical%2520rewards.%2520This%2520method%2520is%2520especially%2520effective%2520in%250Adynamic%2520scenarios%2520where%2520ethical%2520challenges%2520arise%2520unexpectedly%252C%2520making%2520it%250Awell-suited%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Moral%20Uncertainty%20using%20Large%20Language%20Models%20for%20Ethical%0A%20%20Decision-Making&entry.906535625=Rohit%20K.%20Dubey%20and%20Damian%20Dailisan%20and%20Sachit%20Mahajan&entry.1292438233=%20%20We%20present%20an%20ethical%20decision-making%20framework%20that%20refines%20a%20pre-trained%0Areinforcement%20learning%20%28RL%29%20model%20using%20a%20task-agnostic%20ethical%20layer.%0AFollowing%20initial%20training%2C%20the%20RL%20model%20undergoes%20ethical%20fine-tuning%2C%20where%0Ahuman%20feedback%20is%20replaced%20by%20feedback%20generated%20from%20a%20large%20language%20model%0A%28LLM%29.%20The%20LLM%20embodies%20consequentialist%2C%20deontological%2C%20virtue%2C%20social%0Ajustice%2C%20and%20care%20ethics%20as%20moral%20principles%20to%20assign%20belief%20values%20to%0Arecommended%20actions%20during%20ethical%20decision-making.%20An%20ethical%20layer%20aggregates%0Abelief%20scores%20from%20multiple%20LLM-derived%20moral%20perspectives%20using%20Belief%0AJensen-Shannon%20Divergence%20and%20Dempster-Shafer%20Theory%20into%20probability%20scores%0Athat%20also%20serve%20as%20the%20shaping%20reward%2C%20steering%20the%20agent%20toward%20choices%20that%0Aalign%20with%20a%20balanced%20ethical%20framework.%20This%20integrated%20learning%20framework%0Ahelps%20the%20RL%20agent%20navigate%20moral%20uncertainty%20in%20complex%20environments%20and%0Aenables%20it%20to%20make%20morally%20sound%20decisions%20across%20diverse%20tasks.%20Our%20approach%2C%0Atested%20across%20different%20LLM%20variants%20and%20compared%20with%20other%20belief%20aggregation%0Atechniques%2C%20demonstrates%20improved%20consistency%2C%20adaptability%2C%20and%20reduced%0Areliance%20on%20handcrafted%20ethical%20rewards.%20This%20method%20is%20especially%20effective%20in%0Adynamic%20scenarios%20where%20ethical%20challenges%20arise%20unexpectedly%2C%20making%20it%0Awell-suited%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05724v2&entry.124074799=Read"},
{"title": "Model Parallelism With Subnetwork Data Parallelism", "author": "Vaibhav Singh and Zafir Khalid and Edouard Oyallon and Eugene Belilovsky", "abstract": "  Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.\n", "link": "http://arxiv.org/abs/2507.09029v2", "date": "2025-10-01", "relevancy": 1.4644, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4934}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4886}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Parallelism%20With%20Subnetwork%20Data%20Parallelism&body=Title%3A%20Model%20Parallelism%20With%20Subnetwork%20Data%20Parallelism%0AAuthor%3A%20Vaibhav%20Singh%20and%20Zafir%20Khalid%20and%20Edouard%20Oyallon%20and%20Eugene%20Belilovsky%0AAbstract%3A%20%20%20Distributed%20pre-training%20of%20large%20models%20at%20scale%20often%20imposes%20heavy%20memory%0Ademands%20on%20individual%20nodes%20and%20incurs%20significant%20intra-node%20communication%0Acosts.%20We%20propose%20a%20novel%20alternative%20approach%20that%20reduces%20the%20memory%0Arequirements%20by%20training%20small%2C%20structured%20subnetworks%20of%20the%20model%20on%20separate%0Aworkers.%20Unlike%20pipelining%2C%20our%20method%20avoids%20inter-node%20activation%0Acommunication%20and%20maintains%20bandwidth%20requirements%20that%20are%20comparable%20to%20or%0Alower%20than%20standard%20data%20parallel%20communication%20schemes%20based%20on%20all-reduce.%20We%0Aevaluate%20two%20subnetwork%20construction%20strategies%20guided%20by%20the%20principle%20of%0Aensuring%20uniform%20representation%20of%20each%20parameter%20across%20the%20distributed%0Atraining%20setup.%20Our%20results%20show%20that%20the%20stochastic%20block%20dropping%20technique%0Aconsistently%20outperforms%20the%20width-wise%20subnetwork%20construction%20previously%0Aexplored%20in%20federated%20learning.%20We%20empirically%20attribute%20this%20superior%0Aperformance%20to%20stronger%20gradient%20alignment%20in%20subnetworks%20that%20retain%20blocks%0Ahaving%20skip%20connections.%20Preliminary%20experiments%20highlight%20the%20promise%20of%20our%0Aapproach%2C%20achieving%20a%2020-40%25%20reduction%20in%20memory%20usage%20without%20any%20loss%20in%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Parallelism%2520With%2520Subnetwork%2520Data%2520Parallelism%26entry.906535625%3DVaibhav%2520Singh%2520and%2520Zafir%2520Khalid%2520and%2520Edouard%2520Oyallon%2520and%2520Eugene%2520Belilovsky%26entry.1292438233%3D%2520%2520Distributed%2520pre-training%2520of%2520large%2520models%2520at%2520scale%2520often%2520imposes%2520heavy%2520memory%250Ademands%2520on%2520individual%2520nodes%2520and%2520incurs%2520significant%2520intra-node%2520communication%250Acosts.%2520We%2520propose%2520a%2520novel%2520alternative%2520approach%2520that%2520reduces%2520the%2520memory%250Arequirements%2520by%2520training%2520small%252C%2520structured%2520subnetworks%2520of%2520the%2520model%2520on%2520separate%250Aworkers.%2520Unlike%2520pipelining%252C%2520our%2520method%2520avoids%2520inter-node%2520activation%250Acommunication%2520and%2520maintains%2520bandwidth%2520requirements%2520that%2520are%2520comparable%2520to%2520or%250Alower%2520than%2520standard%2520data%2520parallel%2520communication%2520schemes%2520based%2520on%2520all-reduce.%2520We%250Aevaluate%2520two%2520subnetwork%2520construction%2520strategies%2520guided%2520by%2520the%2520principle%2520of%250Aensuring%2520uniform%2520representation%2520of%2520each%2520parameter%2520across%2520the%2520distributed%250Atraining%2520setup.%2520Our%2520results%2520show%2520that%2520the%2520stochastic%2520block%2520dropping%2520technique%250Aconsistently%2520outperforms%2520the%2520width-wise%2520subnetwork%2520construction%2520previously%250Aexplored%2520in%2520federated%2520learning.%2520We%2520empirically%2520attribute%2520this%2520superior%250Aperformance%2520to%2520stronger%2520gradient%2520alignment%2520in%2520subnetworks%2520that%2520retain%2520blocks%250Ahaving%2520skip%2520connections.%2520Preliminary%2520experiments%2520highlight%2520the%2520promise%2520of%2520our%250Aapproach%252C%2520achieving%2520a%252020-40%2525%2520reduction%2520in%2520memory%2520usage%2520without%2520any%2520loss%2520in%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Parallelism%20With%20Subnetwork%20Data%20Parallelism&entry.906535625=Vaibhav%20Singh%20and%20Zafir%20Khalid%20and%20Edouard%20Oyallon%20and%20Eugene%20Belilovsky&entry.1292438233=%20%20Distributed%20pre-training%20of%20large%20models%20at%20scale%20often%20imposes%20heavy%20memory%0Ademands%20on%20individual%20nodes%20and%20incurs%20significant%20intra-node%20communication%0Acosts.%20We%20propose%20a%20novel%20alternative%20approach%20that%20reduces%20the%20memory%0Arequirements%20by%20training%20small%2C%20structured%20subnetworks%20of%20the%20model%20on%20separate%0Aworkers.%20Unlike%20pipelining%2C%20our%20method%20avoids%20inter-node%20activation%0Acommunication%20and%20maintains%20bandwidth%20requirements%20that%20are%20comparable%20to%20or%0Alower%20than%20standard%20data%20parallel%20communication%20schemes%20based%20on%20all-reduce.%20We%0Aevaluate%20two%20subnetwork%20construction%20strategies%20guided%20by%20the%20principle%20of%0Aensuring%20uniform%20representation%20of%20each%20parameter%20across%20the%20distributed%0Atraining%20setup.%20Our%20results%20show%20that%20the%20stochastic%20block%20dropping%20technique%0Aconsistently%20outperforms%20the%20width-wise%20subnetwork%20construction%20previously%0Aexplored%20in%20federated%20learning.%20We%20empirically%20attribute%20this%20superior%0Aperformance%20to%20stronger%20gradient%20alignment%20in%20subnetworks%20that%20retain%20blocks%0Ahaving%20skip%20connections.%20Preliminary%20experiments%20highlight%20the%20promise%20of%20our%0Aapproach%2C%20achieving%20a%2020-40%25%20reduction%20in%20memory%20usage%20without%20any%20loss%20in%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09029v2&entry.124074799=Read"},
{"title": "XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR)\n  Applications", "author": "Shuqing Li and Chenran Zhang and Cuiyun Gao and Michael R. Lyu", "abstract": "  The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR)\nand spatial computing technologies forms a foundational layer for the emerging\nMetaverse, enabling innovative applications across healthcare, education,\nmanufacturing, and entertainment. However, research in this area is often\nlimited by the lack of large, representative, and highquality application\ndatasets that can support empirical studies and the development of new\napproaches benefiting XR software processes. In this paper, we introduce XRZoo,\na comprehensive and curated dataset of XR applications designed to bridge this\ngap. XRZoo contains 12,528 free XR applications, spanning nine app stores,\nacross all XR techniques (i.e., AR, MR, and VR) and use cases, with detailed\nmetadata on key aspects such as application descriptions, application\ncategories, release dates, user review numbers, and hardware specifications,\netc. By making XRZoo publicly available, we aim to foster reproducible XR\nsoftware engineering and security research, enable cross-disciplinary\ninvestigations, and also support the development of advanced XR systems by\nproviding examples to developers. Our dataset serves as a valuable resource for\nresearchers and practitioners interested in improving the scalability,\nusability, and effectiveness of XR applications. XRZoo will be released and\nactively maintained.\n", "link": "http://arxiv.org/abs/2412.06759v3", "date": "2025-10-01", "relevancy": 1.4427, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.4858}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XRZoo%3A%20A%20Large-Scale%20and%20Versatile%20Dataset%20of%20Extended%20Reality%20%28XR%29%0A%20%20Applications&body=Title%3A%20XRZoo%3A%20A%20Large-Scale%20and%20Versatile%20Dataset%20of%20Extended%20Reality%20%28XR%29%0A%20%20Applications%0AAuthor%3A%20Shuqing%20Li%20and%20Chenran%20Zhang%20and%20Cuiyun%20Gao%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Extended%20Reality%20%28XR%2C%20encompassing%20AR%2C%20MR%2C%20and%20VR%29%0Aand%20spatial%20computing%20technologies%20forms%20a%20foundational%20layer%20for%20the%20emerging%0AMetaverse%2C%20enabling%20innovative%20applications%20across%20healthcare%2C%20education%2C%0Amanufacturing%2C%20and%20entertainment.%20However%2C%20research%20in%20this%20area%20is%20often%0Alimited%20by%20the%20lack%20of%20large%2C%20representative%2C%20and%20highquality%20application%0Adatasets%20that%20can%20support%20empirical%20studies%20and%20the%20development%20of%20new%0Aapproaches%20benefiting%20XR%20software%20processes.%20In%20this%20paper%2C%20we%20introduce%20XRZoo%2C%0Aa%20comprehensive%20and%20curated%20dataset%20of%20XR%20applications%20designed%20to%20bridge%20this%0Agap.%20XRZoo%20contains%2012%2C528%20free%20XR%20applications%2C%20spanning%20nine%20app%20stores%2C%0Aacross%20all%20XR%20techniques%20%28i.e.%2C%20AR%2C%20MR%2C%20and%20VR%29%20and%20use%20cases%2C%20with%20detailed%0Ametadata%20on%20key%20aspects%20such%20as%20application%20descriptions%2C%20application%0Acategories%2C%20release%20dates%2C%20user%20review%20numbers%2C%20and%20hardware%20specifications%2C%0Aetc.%20By%20making%20XRZoo%20publicly%20available%2C%20we%20aim%20to%20foster%20reproducible%20XR%0Asoftware%20engineering%20and%20security%20research%2C%20enable%20cross-disciplinary%0Ainvestigations%2C%20and%20also%20support%20the%20development%20of%20advanced%20XR%20systems%20by%0Aproviding%20examples%20to%20developers.%20Our%20dataset%20serves%20as%20a%20valuable%20resource%20for%0Aresearchers%20and%20practitioners%20interested%20in%20improving%20the%20scalability%2C%0Ausability%2C%20and%20effectiveness%20of%20XR%20applications.%20XRZoo%20will%20be%20released%20and%0Aactively%20maintained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06759v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXRZoo%253A%2520A%2520Large-Scale%2520and%2520Versatile%2520Dataset%2520of%2520Extended%2520Reality%2520%2528XR%2529%250A%2520%2520Applications%26entry.906535625%3DShuqing%2520Li%2520and%2520Chenran%2520Zhang%2520and%2520Cuiyun%2520Gao%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Extended%2520Reality%2520%2528XR%252C%2520encompassing%2520AR%252C%2520MR%252C%2520and%2520VR%2529%250Aand%2520spatial%2520computing%2520technologies%2520forms%2520a%2520foundational%2520layer%2520for%2520the%2520emerging%250AMetaverse%252C%2520enabling%2520innovative%2520applications%2520across%2520healthcare%252C%2520education%252C%250Amanufacturing%252C%2520and%2520entertainment.%2520However%252C%2520research%2520in%2520this%2520area%2520is%2520often%250Alimited%2520by%2520the%2520lack%2520of%2520large%252C%2520representative%252C%2520and%2520highquality%2520application%250Adatasets%2520that%2520can%2520support%2520empirical%2520studies%2520and%2520the%2520development%2520of%2520new%250Aapproaches%2520benefiting%2520XR%2520software%2520processes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520XRZoo%252C%250Aa%2520comprehensive%2520and%2520curated%2520dataset%2520of%2520XR%2520applications%2520designed%2520to%2520bridge%2520this%250Agap.%2520XRZoo%2520contains%252012%252C528%2520free%2520XR%2520applications%252C%2520spanning%2520nine%2520app%2520stores%252C%250Aacross%2520all%2520XR%2520techniques%2520%2528i.e.%252C%2520AR%252C%2520MR%252C%2520and%2520VR%2529%2520and%2520use%2520cases%252C%2520with%2520detailed%250Ametadata%2520on%2520key%2520aspects%2520such%2520as%2520application%2520descriptions%252C%2520application%250Acategories%252C%2520release%2520dates%252C%2520user%2520review%2520numbers%252C%2520and%2520hardware%2520specifications%252C%250Aetc.%2520By%2520making%2520XRZoo%2520publicly%2520available%252C%2520we%2520aim%2520to%2520foster%2520reproducible%2520XR%250Asoftware%2520engineering%2520and%2520security%2520research%252C%2520enable%2520cross-disciplinary%250Ainvestigations%252C%2520and%2520also%2520support%2520the%2520development%2520of%2520advanced%2520XR%2520systems%2520by%250Aproviding%2520examples%2520to%2520developers.%2520Our%2520dataset%2520serves%2520as%2520a%2520valuable%2520resource%2520for%250Aresearchers%2520and%2520practitioners%2520interested%2520in%2520improving%2520the%2520scalability%252C%250Ausability%252C%2520and%2520effectiveness%2520of%2520XR%2520applications.%2520XRZoo%2520will%2520be%2520released%2520and%250Aactively%2520maintained.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06759v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XRZoo%3A%20A%20Large-Scale%20and%20Versatile%20Dataset%20of%20Extended%20Reality%20%28XR%29%0A%20%20Applications&entry.906535625=Shuqing%20Li%20and%20Chenran%20Zhang%20and%20Cuiyun%20Gao%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Extended%20Reality%20%28XR%2C%20encompassing%20AR%2C%20MR%2C%20and%20VR%29%0Aand%20spatial%20computing%20technologies%20forms%20a%20foundational%20layer%20for%20the%20emerging%0AMetaverse%2C%20enabling%20innovative%20applications%20across%20healthcare%2C%20education%2C%0Amanufacturing%2C%20and%20entertainment.%20However%2C%20research%20in%20this%20area%20is%20often%0Alimited%20by%20the%20lack%20of%20large%2C%20representative%2C%20and%20highquality%20application%0Adatasets%20that%20can%20support%20empirical%20studies%20and%20the%20development%20of%20new%0Aapproaches%20benefiting%20XR%20software%20processes.%20In%20this%20paper%2C%20we%20introduce%20XRZoo%2C%0Aa%20comprehensive%20and%20curated%20dataset%20of%20XR%20applications%20designed%20to%20bridge%20this%0Agap.%20XRZoo%20contains%2012%2C528%20free%20XR%20applications%2C%20spanning%20nine%20app%20stores%2C%0Aacross%20all%20XR%20techniques%20%28i.e.%2C%20AR%2C%20MR%2C%20and%20VR%29%20and%20use%20cases%2C%20with%20detailed%0Ametadata%20on%20key%20aspects%20such%20as%20application%20descriptions%2C%20application%0Acategories%2C%20release%20dates%2C%20user%20review%20numbers%2C%20and%20hardware%20specifications%2C%0Aetc.%20By%20making%20XRZoo%20publicly%20available%2C%20we%20aim%20to%20foster%20reproducible%20XR%0Asoftware%20engineering%20and%20security%20research%2C%20enable%20cross-disciplinary%0Ainvestigations%2C%20and%20also%20support%20the%20development%20of%20advanced%20XR%20systems%20by%0Aproviding%20examples%20to%20developers.%20Our%20dataset%20serves%20as%20a%20valuable%20resource%20for%0Aresearchers%20and%20practitioners%20interested%20in%20improving%20the%20scalability%2C%0Ausability%2C%20and%20effectiveness%20of%20XR%20applications.%20XRZoo%20will%20be%20released%20and%0Aactively%20maintained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06759v3&entry.124074799=Read"},
{"title": "Stackelberg Coupling of Online Representation Learning and Reinforcement\n  Learning", "author": "Fernando Martinez and Tao Li and Yingdong Lu and Juntao Chen", "abstract": "  Deep Q-learning jointly learns representations and values within monolithic\nnetworks, promising beneficial co-adaptation between features and value\nestimates. Although this architecture has attained substantial success, the\ncoupling between representation and value learning creates instability as\nrepresentations must constantly adapt to non-stationary value targets, while\nvalue estimates depend on these shifting representations. This is compounded by\nhigh variance in bootstrapped targets, which causes bias in value estimation in\noff-policy methods. We introduce Stackelberg Coupled Representation and\nReinforcement Learning (SCORER), a framework for value-based RL that views\nrepresentation and Q-learning as two strategic agents in a hierarchical game.\nSCORER models the Q-function as the leader, which commits to its strategy by\nupdating less frequently, while the perception network (encoder) acts as the\nfollower, adapting more frequently to learn representations that minimize\nBellman error variance given the leader's committed strategy. Through this\ndivision of labor, the Q-function minimizes MSBE while perception minimizes its\nvariance, thereby reducing bias accordingly, with asymmetric updates allowing\nstable co-adaptation, unlike simultaneous parameter updates in monolithic\nsolutions. Our proposed SCORER framework leads to a bi-level optimization\nproblem whose solution is approximated by a two-timescale algorithm that\ncreates an asymmetric learning dynamic between the two players. Extensive\nexperiments on DQN and its variants demonstrate that gains stem from\nalgorithmic insight rather than model complexity.\n", "link": "http://arxiv.org/abs/2508.07452v2", "date": "2025-10-01", "relevancy": 1.4254, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4693}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stackelberg%20Coupling%20of%20Online%20Representation%20Learning%20and%20Reinforcement%0A%20%20Learning&body=Title%3A%20Stackelberg%20Coupling%20of%20Online%20Representation%20Learning%20and%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Fernando%20Martinez%20and%20Tao%20Li%20and%20Yingdong%20Lu%20and%20Juntao%20Chen%0AAbstract%3A%20%20%20Deep%20Q-learning%20jointly%20learns%20representations%20and%20values%20within%20monolithic%0Anetworks%2C%20promising%20beneficial%20co-adaptation%20between%20features%20and%20value%0Aestimates.%20Although%20this%20architecture%20has%20attained%20substantial%20success%2C%20the%0Acoupling%20between%20representation%20and%20value%20learning%20creates%20instability%20as%0Arepresentations%20must%20constantly%20adapt%20to%20non-stationary%20value%20targets%2C%20while%0Avalue%20estimates%20depend%20on%20these%20shifting%20representations.%20This%20is%20compounded%20by%0Ahigh%20variance%20in%20bootstrapped%20targets%2C%20which%20causes%20bias%20in%20value%20estimation%20in%0Aoff-policy%20methods.%20We%20introduce%20Stackelberg%20Coupled%20Representation%20and%0AReinforcement%20Learning%20%28SCORER%29%2C%20a%20framework%20for%20value-based%20RL%20that%20views%0Arepresentation%20and%20Q-learning%20as%20two%20strategic%20agents%20in%20a%20hierarchical%20game.%0ASCORER%20models%20the%20Q-function%20as%20the%20leader%2C%20which%20commits%20to%20its%20strategy%20by%0Aupdating%20less%20frequently%2C%20while%20the%20perception%20network%20%28encoder%29%20acts%20as%20the%0Afollower%2C%20adapting%20more%20frequently%20to%20learn%20representations%20that%20minimize%0ABellman%20error%20variance%20given%20the%20leader%27s%20committed%20strategy.%20Through%20this%0Adivision%20of%20labor%2C%20the%20Q-function%20minimizes%20MSBE%20while%20perception%20minimizes%20its%0Avariance%2C%20thereby%20reducing%20bias%20accordingly%2C%20with%20asymmetric%20updates%20allowing%0Astable%20co-adaptation%2C%20unlike%20simultaneous%20parameter%20updates%20in%20monolithic%0Asolutions.%20Our%20proposed%20SCORER%20framework%20leads%20to%20a%20bi-level%20optimization%0Aproblem%20whose%20solution%20is%20approximated%20by%20a%20two-timescale%20algorithm%20that%0Acreates%20an%20asymmetric%20learning%20dynamic%20between%20the%20two%20players.%20Extensive%0Aexperiments%20on%20DQN%20and%20its%20variants%20demonstrate%20that%20gains%20stem%20from%0Aalgorithmic%20insight%20rather%20than%20model%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStackelberg%2520Coupling%2520of%2520Online%2520Representation%2520Learning%2520and%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DFernando%2520Martinez%2520and%2520Tao%2520Li%2520and%2520Yingdong%2520Lu%2520and%2520Juntao%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520Q-learning%2520jointly%2520learns%2520representations%2520and%2520values%2520within%2520monolithic%250Anetworks%252C%2520promising%2520beneficial%2520co-adaptation%2520between%2520features%2520and%2520value%250Aestimates.%2520Although%2520this%2520architecture%2520has%2520attained%2520substantial%2520success%252C%2520the%250Acoupling%2520between%2520representation%2520and%2520value%2520learning%2520creates%2520instability%2520as%250Arepresentations%2520must%2520constantly%2520adapt%2520to%2520non-stationary%2520value%2520targets%252C%2520while%250Avalue%2520estimates%2520depend%2520on%2520these%2520shifting%2520representations.%2520This%2520is%2520compounded%2520by%250Ahigh%2520variance%2520in%2520bootstrapped%2520targets%252C%2520which%2520causes%2520bias%2520in%2520value%2520estimation%2520in%250Aoff-policy%2520methods.%2520We%2520introduce%2520Stackelberg%2520Coupled%2520Representation%2520and%250AReinforcement%2520Learning%2520%2528SCORER%2529%252C%2520a%2520framework%2520for%2520value-based%2520RL%2520that%2520views%250Arepresentation%2520and%2520Q-learning%2520as%2520two%2520strategic%2520agents%2520in%2520a%2520hierarchical%2520game.%250ASCORER%2520models%2520the%2520Q-function%2520as%2520the%2520leader%252C%2520which%2520commits%2520to%2520its%2520strategy%2520by%250Aupdating%2520less%2520frequently%252C%2520while%2520the%2520perception%2520network%2520%2528encoder%2529%2520acts%2520as%2520the%250Afollower%252C%2520adapting%2520more%2520frequently%2520to%2520learn%2520representations%2520that%2520minimize%250ABellman%2520error%2520variance%2520given%2520the%2520leader%2527s%2520committed%2520strategy.%2520Through%2520this%250Adivision%2520of%2520labor%252C%2520the%2520Q-function%2520minimizes%2520MSBE%2520while%2520perception%2520minimizes%2520its%250Avariance%252C%2520thereby%2520reducing%2520bias%2520accordingly%252C%2520with%2520asymmetric%2520updates%2520allowing%250Astable%2520co-adaptation%252C%2520unlike%2520simultaneous%2520parameter%2520updates%2520in%2520monolithic%250Asolutions.%2520Our%2520proposed%2520SCORER%2520framework%2520leads%2520to%2520a%2520bi-level%2520optimization%250Aproblem%2520whose%2520solution%2520is%2520approximated%2520by%2520a%2520two-timescale%2520algorithm%2520that%250Acreates%2520an%2520asymmetric%2520learning%2520dynamic%2520between%2520the%2520two%2520players.%2520Extensive%250Aexperiments%2520on%2520DQN%2520and%2520its%2520variants%2520demonstrate%2520that%2520gains%2520stem%2520from%250Aalgorithmic%2520insight%2520rather%2520than%2520model%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stackelberg%20Coupling%20of%20Online%20Representation%20Learning%20and%20Reinforcement%0A%20%20Learning&entry.906535625=Fernando%20Martinez%20and%20Tao%20Li%20and%20Yingdong%20Lu%20and%20Juntao%20Chen&entry.1292438233=%20%20Deep%20Q-learning%20jointly%20learns%20representations%20and%20values%20within%20monolithic%0Anetworks%2C%20promising%20beneficial%20co-adaptation%20between%20features%20and%20value%0Aestimates.%20Although%20this%20architecture%20has%20attained%20substantial%20success%2C%20the%0Acoupling%20between%20representation%20and%20value%20learning%20creates%20instability%20as%0Arepresentations%20must%20constantly%20adapt%20to%20non-stationary%20value%20targets%2C%20while%0Avalue%20estimates%20depend%20on%20these%20shifting%20representations.%20This%20is%20compounded%20by%0Ahigh%20variance%20in%20bootstrapped%20targets%2C%20which%20causes%20bias%20in%20value%20estimation%20in%0Aoff-policy%20methods.%20We%20introduce%20Stackelberg%20Coupled%20Representation%20and%0AReinforcement%20Learning%20%28SCORER%29%2C%20a%20framework%20for%20value-based%20RL%20that%20views%0Arepresentation%20and%20Q-learning%20as%20two%20strategic%20agents%20in%20a%20hierarchical%20game.%0ASCORER%20models%20the%20Q-function%20as%20the%20leader%2C%20which%20commits%20to%20its%20strategy%20by%0Aupdating%20less%20frequently%2C%20while%20the%20perception%20network%20%28encoder%29%20acts%20as%20the%0Afollower%2C%20adapting%20more%20frequently%20to%20learn%20representations%20that%20minimize%0ABellman%20error%20variance%20given%20the%20leader%27s%20committed%20strategy.%20Through%20this%0Adivision%20of%20labor%2C%20the%20Q-function%20minimizes%20MSBE%20while%20perception%20minimizes%20its%0Avariance%2C%20thereby%20reducing%20bias%20accordingly%2C%20with%20asymmetric%20updates%20allowing%0Astable%20co-adaptation%2C%20unlike%20simultaneous%20parameter%20updates%20in%20monolithic%0Asolutions.%20Our%20proposed%20SCORER%20framework%20leads%20to%20a%20bi-level%20optimization%0Aproblem%20whose%20solution%20is%20approximated%20by%20a%20two-timescale%20algorithm%20that%0Acreates%20an%20asymmetric%20learning%20dynamic%20between%20the%20two%20players.%20Extensive%0Aexperiments%20on%20DQN%20and%20its%20variants%20demonstrate%20that%20gains%20stem%20from%0Aalgorithmic%20insight%20rather%20than%20model%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07452v2&entry.124074799=Read"},
{"title": "Steering When Necessary: Flexible Steering Large Language Models with\n  Backtracking", "author": "Zifeng Cheng and Jinwei Gan and Zhiwei Jiang and Cong Wang and Yafeng Yin and Xiang Luo and Yuchen Fu and Qing Gu", "abstract": "  Large language models (LLMs) have achieved remarkable performance across many\ngeneration tasks. Nevertheless, effectively aligning them with desired\nbehaviors remains a significant challenge. Activation steering is an effective\nand cost-efficient approach that directly modifies the activations of LLMs\nduring the inference stage, aligning their responses with the desired behaviors\nand avoiding the high cost of fine-tuning. Existing methods typically\nindiscriminately intervene to all generations or rely solely on the question to\ndetermine intervention, which limits the accurate assessment of the\nintervention strength. To this end, we propose the Flexible Activation Steering\nwith Backtracking (FASB) framework, which dynamically determines both the\nnecessity and strength of intervention by tracking the internal states of the\nLLMs during generation, considering both the question and the generated\ncontent. Since intervening after detecting a deviation from the desired\nbehavior is often too late, we further propose the backtracking mechanism to\ncorrect the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice\ndatasets demonstrate that our method outperforms baselines. Our code will be\nreleased at https://github.com/gjw185/FASB.\n", "link": "http://arxiv.org/abs/2508.17621v2", "date": "2025-10-01", "relevancy": 1.4048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4848}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4743}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20When%20Necessary%3A%20Flexible%20Steering%20Large%20Language%20Models%20with%0A%20%20Backtracking&body=Title%3A%20Steering%20When%20Necessary%3A%20Flexible%20Steering%20Large%20Language%20Models%20with%0A%20%20Backtracking%0AAuthor%3A%20Zifeng%20Cheng%20and%20Jinwei%20Gan%20and%20Zhiwei%20Jiang%20and%20Cong%20Wang%20and%20Yafeng%20Yin%20and%20Xiang%20Luo%20and%20Yuchen%20Fu%20and%20Qing%20Gu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20across%20many%0Ageneration%20tasks.%20Nevertheless%2C%20effectively%20aligning%20them%20with%20desired%0Abehaviors%20remains%20a%20significant%20challenge.%20Activation%20steering%20is%20an%20effective%0Aand%20cost-efficient%20approach%20that%20directly%20modifies%20the%20activations%20of%20LLMs%0Aduring%20the%20inference%20stage%2C%20aligning%20their%20responses%20with%20the%20desired%20behaviors%0Aand%20avoiding%20the%20high%20cost%20of%20fine-tuning.%20Existing%20methods%20typically%0Aindiscriminately%20intervene%20to%20all%20generations%20or%20rely%20solely%20on%20the%20question%20to%0Adetermine%20intervention%2C%20which%20limits%20the%20accurate%20assessment%20of%20the%0Aintervention%20strength.%20To%20this%20end%2C%20we%20propose%20the%20Flexible%20Activation%20Steering%0Awith%20Backtracking%20%28FASB%29%20framework%2C%20which%20dynamically%20determines%20both%20the%0Anecessity%20and%20strength%20of%20intervention%20by%20tracking%20the%20internal%20states%20of%20the%0ALLMs%20during%20generation%2C%20considering%20both%20the%20question%20and%20the%20generated%0Acontent.%20Since%20intervening%20after%20detecting%20a%20deviation%20from%20the%20desired%0Abehavior%20is%20often%20too%20late%2C%20we%20further%20propose%20the%20backtracking%20mechanism%20to%0Acorrect%20the%20deviated%20tokens%20and%20steer%20the%20LLMs%20toward%20the%20desired%20behavior.%0AExtensive%20experiments%20on%20the%20TruthfulQA%20dataset%20and%20six%20multiple-choice%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20baselines.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/gjw185/FASB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520When%2520Necessary%253A%2520Flexible%2520Steering%2520Large%2520Language%2520Models%2520with%250A%2520%2520Backtracking%26entry.906535625%3DZifeng%2520Cheng%2520and%2520Jinwei%2520Gan%2520and%2520Zhiwei%2520Jiang%2520and%2520Cong%2520Wang%2520and%2520Yafeng%2520Yin%2520and%2520Xiang%2520Luo%2520and%2520Yuchen%2520Fu%2520and%2520Qing%2520Gu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520across%2520many%250Ageneration%2520tasks.%2520Nevertheless%252C%2520effectively%2520aligning%2520them%2520with%2520desired%250Abehaviors%2520remains%2520a%2520significant%2520challenge.%2520Activation%2520steering%2520is%2520an%2520effective%250Aand%2520cost-efficient%2520approach%2520that%2520directly%2520modifies%2520the%2520activations%2520of%2520LLMs%250Aduring%2520the%2520inference%2520stage%252C%2520aligning%2520their%2520responses%2520with%2520the%2520desired%2520behaviors%250Aand%2520avoiding%2520the%2520high%2520cost%2520of%2520fine-tuning.%2520Existing%2520methods%2520typically%250Aindiscriminately%2520intervene%2520to%2520all%2520generations%2520or%2520rely%2520solely%2520on%2520the%2520question%2520to%250Adetermine%2520intervention%252C%2520which%2520limits%2520the%2520accurate%2520assessment%2520of%2520the%250Aintervention%2520strength.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Flexible%2520Activation%2520Steering%250Awith%2520Backtracking%2520%2528FASB%2529%2520framework%252C%2520which%2520dynamically%2520determines%2520both%2520the%250Anecessity%2520and%2520strength%2520of%2520intervention%2520by%2520tracking%2520the%2520internal%2520states%2520of%2520the%250ALLMs%2520during%2520generation%252C%2520considering%2520both%2520the%2520question%2520and%2520the%2520generated%250Acontent.%2520Since%2520intervening%2520after%2520detecting%2520a%2520deviation%2520from%2520the%2520desired%250Abehavior%2520is%2520often%2520too%2520late%252C%2520we%2520further%2520propose%2520the%2520backtracking%2520mechanism%2520to%250Acorrect%2520the%2520deviated%2520tokens%2520and%2520steer%2520the%2520LLMs%2520toward%2520the%2520desired%2520behavior.%250AExtensive%2520experiments%2520on%2520the%2520TruthfulQA%2520dataset%2520and%2520six%2520multiple-choice%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520baselines.%2520Our%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/gjw185/FASB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20When%20Necessary%3A%20Flexible%20Steering%20Large%20Language%20Models%20with%0A%20%20Backtracking&entry.906535625=Zifeng%20Cheng%20and%20Jinwei%20Gan%20and%20Zhiwei%20Jiang%20and%20Cong%20Wang%20and%20Yafeng%20Yin%20and%20Xiang%20Luo%20and%20Yuchen%20Fu%20and%20Qing%20Gu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20across%20many%0Ageneration%20tasks.%20Nevertheless%2C%20effectively%20aligning%20them%20with%20desired%0Abehaviors%20remains%20a%20significant%20challenge.%20Activation%20steering%20is%20an%20effective%0Aand%20cost-efficient%20approach%20that%20directly%20modifies%20the%20activations%20of%20LLMs%0Aduring%20the%20inference%20stage%2C%20aligning%20their%20responses%20with%20the%20desired%20behaviors%0Aand%20avoiding%20the%20high%20cost%20of%20fine-tuning.%20Existing%20methods%20typically%0Aindiscriminately%20intervene%20to%20all%20generations%20or%20rely%20solely%20on%20the%20question%20to%0Adetermine%20intervention%2C%20which%20limits%20the%20accurate%20assessment%20of%20the%0Aintervention%20strength.%20To%20this%20end%2C%20we%20propose%20the%20Flexible%20Activation%20Steering%0Awith%20Backtracking%20%28FASB%29%20framework%2C%20which%20dynamically%20determines%20both%20the%0Anecessity%20and%20strength%20of%20intervention%20by%20tracking%20the%20internal%20states%20of%20the%0ALLMs%20during%20generation%2C%20considering%20both%20the%20question%20and%20the%20generated%0Acontent.%20Since%20intervening%20after%20detecting%20a%20deviation%20from%20the%20desired%0Abehavior%20is%20often%20too%20late%2C%20we%20further%20propose%20the%20backtracking%20mechanism%20to%0Acorrect%20the%20deviated%20tokens%20and%20steer%20the%20LLMs%20toward%20the%20desired%20behavior.%0AExtensive%20experiments%20on%20the%20TruthfulQA%20dataset%20and%20six%20multiple-choice%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20baselines.%20Our%20code%20will%20be%0Areleased%20at%20https%3A//github.com/gjw185/FASB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17621v2&entry.124074799=Read"},
{"title": "PaECTER: Patent-level Representation Learning using Citation-informed\n  Transformers", "author": "Mainak Ghosh and Michael E. Rose and Sebastian Erhardt and Erik Buunk and Dietmar Harhoff", "abstract": "  PaECTER is an open-source document-level encoder specific for patents. We\nfine-tune BERT for Patents with examiner-added citation information to generate\nnumerical representations for patent documents. PaECTER performs better in\nsimilarity tasks than current state-of-the-art models used in the patent\ndomain. More specifically, our model outperforms the patent specific\npre-trained language model (BERT for Patents) and general-purpose text\nembedding models (e.g., E5, GTE, and BGE) on our patent citation prediction\ntest dataset on different rank evaluation metrics. PaECTER predicts at least\none most similar patent at a rank of 1.32 on average when compared against 25\nirrelevant patents. Numerical representations generated by PaECTER from patent\ntext can be used for downstream tasks such as classification, tracing knowledge\nflows, or semantic similarity search. Semantic similarity search is especially\nrelevant in the context of prior art search for both inventors and patent\nexaminers.\n", "link": "http://arxiv.org/abs/2402.19411v2", "date": "2025-10-01", "relevancy": 1.3943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4782}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4709}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PaECTER%3A%20Patent-level%20Representation%20Learning%20using%20Citation-informed%0A%20%20Transformers&body=Title%3A%20PaECTER%3A%20Patent-level%20Representation%20Learning%20using%20Citation-informed%0A%20%20Transformers%0AAuthor%3A%20Mainak%20Ghosh%20and%20Michael%20E.%20Rose%20and%20Sebastian%20Erhardt%20and%20Erik%20Buunk%20and%20Dietmar%20Harhoff%0AAbstract%3A%20%20%20PaECTER%20is%20an%20open-source%20document-level%20encoder%20specific%20for%20patents.%20We%0Afine-tune%20BERT%20for%20Patents%20with%20examiner-added%20citation%20information%20to%20generate%0Anumerical%20representations%20for%20patent%20documents.%20PaECTER%20performs%20better%20in%0Asimilarity%20tasks%20than%20current%20state-of-the-art%20models%20used%20in%20the%20patent%0Adomain.%20More%20specifically%2C%20our%20model%20outperforms%20the%20patent%20specific%0Apre-trained%20language%20model%20%28BERT%20for%20Patents%29%20and%20general-purpose%20text%0Aembedding%20models%20%28e.g.%2C%20E5%2C%20GTE%2C%20and%20BGE%29%20on%20our%20patent%20citation%20prediction%0Atest%20dataset%20on%20different%20rank%20evaluation%20metrics.%20PaECTER%20predicts%20at%20least%0Aone%20most%20similar%20patent%20at%20a%20rank%20of%201.32%20on%20average%20when%20compared%20against%2025%0Airrelevant%20patents.%20Numerical%20representations%20generated%20by%20PaECTER%20from%20patent%0Atext%20can%20be%20used%20for%20downstream%20tasks%20such%20as%20classification%2C%20tracing%20knowledge%0Aflows%2C%20or%20semantic%20similarity%20search.%20Semantic%20similarity%20search%20is%20especially%0Arelevant%20in%20the%20context%20of%20prior%20art%20search%20for%20both%20inventors%20and%20patent%0Aexaminers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPaECTER%253A%2520Patent-level%2520Representation%2520Learning%2520using%2520Citation-informed%250A%2520%2520Transformers%26entry.906535625%3DMainak%2520Ghosh%2520and%2520Michael%2520E.%2520Rose%2520and%2520Sebastian%2520Erhardt%2520and%2520Erik%2520Buunk%2520and%2520Dietmar%2520Harhoff%26entry.1292438233%3D%2520%2520PaECTER%2520is%2520an%2520open-source%2520document-level%2520encoder%2520specific%2520for%2520patents.%2520We%250Afine-tune%2520BERT%2520for%2520Patents%2520with%2520examiner-added%2520citation%2520information%2520to%2520generate%250Anumerical%2520representations%2520for%2520patent%2520documents.%2520PaECTER%2520performs%2520better%2520in%250Asimilarity%2520tasks%2520than%2520current%2520state-of-the-art%2520models%2520used%2520in%2520the%2520patent%250Adomain.%2520More%2520specifically%252C%2520our%2520model%2520outperforms%2520the%2520patent%2520specific%250Apre-trained%2520language%2520model%2520%2528BERT%2520for%2520Patents%2529%2520and%2520general-purpose%2520text%250Aembedding%2520models%2520%2528e.g.%252C%2520E5%252C%2520GTE%252C%2520and%2520BGE%2529%2520on%2520our%2520patent%2520citation%2520prediction%250Atest%2520dataset%2520on%2520different%2520rank%2520evaluation%2520metrics.%2520PaECTER%2520predicts%2520at%2520least%250Aone%2520most%2520similar%2520patent%2520at%2520a%2520rank%2520of%25201.32%2520on%2520average%2520when%2520compared%2520against%252025%250Airrelevant%2520patents.%2520Numerical%2520representations%2520generated%2520by%2520PaECTER%2520from%2520patent%250Atext%2520can%2520be%2520used%2520for%2520downstream%2520tasks%2520such%2520as%2520classification%252C%2520tracing%2520knowledge%250Aflows%252C%2520or%2520semantic%2520similarity%2520search.%2520Semantic%2520similarity%2520search%2520is%2520especially%250Arelevant%2520in%2520the%2520context%2520of%2520prior%2520art%2520search%2520for%2520both%2520inventors%2520and%2520patent%250Aexaminers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PaECTER%3A%20Patent-level%20Representation%20Learning%20using%20Citation-informed%0A%20%20Transformers&entry.906535625=Mainak%20Ghosh%20and%20Michael%20E.%20Rose%20and%20Sebastian%20Erhardt%20and%20Erik%20Buunk%20and%20Dietmar%20Harhoff&entry.1292438233=%20%20PaECTER%20is%20an%20open-source%20document-level%20encoder%20specific%20for%20patents.%20We%0Afine-tune%20BERT%20for%20Patents%20with%20examiner-added%20citation%20information%20to%20generate%0Anumerical%20representations%20for%20patent%20documents.%20PaECTER%20performs%20better%20in%0Asimilarity%20tasks%20than%20current%20state-of-the-art%20models%20used%20in%20the%20patent%0Adomain.%20More%20specifically%2C%20our%20model%20outperforms%20the%20patent%20specific%0Apre-trained%20language%20model%20%28BERT%20for%20Patents%29%20and%20general-purpose%20text%0Aembedding%20models%20%28e.g.%2C%20E5%2C%20GTE%2C%20and%20BGE%29%20on%20our%20patent%20citation%20prediction%0Atest%20dataset%20on%20different%20rank%20evaluation%20metrics.%20PaECTER%20predicts%20at%20least%0Aone%20most%20similar%20patent%20at%20a%20rank%20of%201.32%20on%20average%20when%20compared%20against%2025%0Airrelevant%20patents.%20Numerical%20representations%20generated%20by%20PaECTER%20from%20patent%0Atext%20can%20be%20used%20for%20downstream%20tasks%20such%20as%20classification%2C%20tracing%20knowledge%0Aflows%2C%20or%20semantic%20similarity%20search.%20Semantic%20similarity%20search%20is%20especially%0Arelevant%20in%20the%20context%20of%20prior%20art%20search%20for%20both%20inventors%20and%20patent%0Aexaminers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19411v2&entry.124074799=Read"},
{"title": "Post Hoc Regression Refinement via Pairwise Rankings", "author": "Kevin Tirta Wijaya and Michael Sun and Minghao Guo and Hans-Peter Seidel and Wojciech Matusik and Vahid Babaei", "abstract": "  Accurate prediction of continuous properties is essential to many scientific\nand engineering tasks. Although deep-learning regressors excel with abundant\nlabels, their accuracy deteriorates in data-scarce regimes. We introduce\nRankRefine, a model-agnostic, plug-and-play post hoc method that refines\nregression with expert knowledge coming from pairwise rankings. Given a query\nitem and a small reference set with known properties, RankRefine combines the\nbase regressor's output with a rank-based estimate via inverse variance\nweighting, requiring no retraining. In molecular property prediction task,\nRankRefine achieves up to 10% relative reduction in mean absolute error using\nonly 20 pairwise comparisons obtained through a general-purpose large language\nmodel (LLM) with no finetuning. As rankings provided by human experts or\ngeneral-purpose LLMs are sufficient for improving regression across diverse\ndomains, RankRefine offers practicality and broad applicability, especially in\nlow-data settings.\n", "link": "http://arxiv.org/abs/2508.16495v2", "date": "2025-10-01", "relevancy": 1.3756, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post%20Hoc%20Regression%20Refinement%20via%20Pairwise%20Rankings&body=Title%3A%20Post%20Hoc%20Regression%20Refinement%20via%20Pairwise%20Rankings%0AAuthor%3A%20Kevin%20Tirta%20Wijaya%20and%20Michael%20Sun%20and%20Minghao%20Guo%20and%20Hans-Peter%20Seidel%20and%20Wojciech%20Matusik%20and%20Vahid%20Babaei%0AAbstract%3A%20%20%20Accurate%20prediction%20of%20continuous%20properties%20is%20essential%20to%20many%20scientific%0Aand%20engineering%20tasks.%20Although%20deep-learning%20regressors%20excel%20with%20abundant%0Alabels%2C%20their%20accuracy%20deteriorates%20in%20data-scarce%20regimes.%20We%20introduce%0ARankRefine%2C%20a%20model-agnostic%2C%20plug-and-play%20post%20hoc%20method%20that%20refines%0Aregression%20with%20expert%20knowledge%20coming%20from%20pairwise%20rankings.%20Given%20a%20query%0Aitem%20and%20a%20small%20reference%20set%20with%20known%20properties%2C%20RankRefine%20combines%20the%0Abase%20regressor%27s%20output%20with%20a%20rank-based%20estimate%20via%20inverse%20variance%0Aweighting%2C%20requiring%20no%20retraining.%20In%20molecular%20property%20prediction%20task%2C%0ARankRefine%20achieves%20up%20to%2010%25%20relative%20reduction%20in%20mean%20absolute%20error%20using%0Aonly%2020%20pairwise%20comparisons%20obtained%20through%20a%20general-purpose%20large%20language%0Amodel%20%28LLM%29%20with%20no%20finetuning.%20As%20rankings%20provided%20by%20human%20experts%20or%0Ageneral-purpose%20LLMs%20are%20sufficient%20for%20improving%20regression%20across%20diverse%0Adomains%2C%20RankRefine%20offers%20practicality%20and%20broad%20applicability%2C%20especially%20in%0Alow-data%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost%2520Hoc%2520Regression%2520Refinement%2520via%2520Pairwise%2520Rankings%26entry.906535625%3DKevin%2520Tirta%2520Wijaya%2520and%2520Michael%2520Sun%2520and%2520Minghao%2520Guo%2520and%2520Hans-Peter%2520Seidel%2520and%2520Wojciech%2520Matusik%2520and%2520Vahid%2520Babaei%26entry.1292438233%3D%2520%2520Accurate%2520prediction%2520of%2520continuous%2520properties%2520is%2520essential%2520to%2520many%2520scientific%250Aand%2520engineering%2520tasks.%2520Although%2520deep-learning%2520regressors%2520excel%2520with%2520abundant%250Alabels%252C%2520their%2520accuracy%2520deteriorates%2520in%2520data-scarce%2520regimes.%2520We%2520introduce%250ARankRefine%252C%2520a%2520model-agnostic%252C%2520plug-and-play%2520post%2520hoc%2520method%2520that%2520refines%250Aregression%2520with%2520expert%2520knowledge%2520coming%2520from%2520pairwise%2520rankings.%2520Given%2520a%2520query%250Aitem%2520and%2520a%2520small%2520reference%2520set%2520with%2520known%2520properties%252C%2520RankRefine%2520combines%2520the%250Abase%2520regressor%2527s%2520output%2520with%2520a%2520rank-based%2520estimate%2520via%2520inverse%2520variance%250Aweighting%252C%2520requiring%2520no%2520retraining.%2520In%2520molecular%2520property%2520prediction%2520task%252C%250ARankRefine%2520achieves%2520up%2520to%252010%2525%2520relative%2520reduction%2520in%2520mean%2520absolute%2520error%2520using%250Aonly%252020%2520pairwise%2520comparisons%2520obtained%2520through%2520a%2520general-purpose%2520large%2520language%250Amodel%2520%2528LLM%2529%2520with%2520no%2520finetuning.%2520As%2520rankings%2520provided%2520by%2520human%2520experts%2520or%250Ageneral-purpose%2520LLMs%2520are%2520sufficient%2520for%2520improving%2520regression%2520across%2520diverse%250Adomains%252C%2520RankRefine%2520offers%2520practicality%2520and%2520broad%2520applicability%252C%2520especially%2520in%250Alow-data%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post%20Hoc%20Regression%20Refinement%20via%20Pairwise%20Rankings&entry.906535625=Kevin%20Tirta%20Wijaya%20and%20Michael%20Sun%20and%20Minghao%20Guo%20and%20Hans-Peter%20Seidel%20and%20Wojciech%20Matusik%20and%20Vahid%20Babaei&entry.1292438233=%20%20Accurate%20prediction%20of%20continuous%20properties%20is%20essential%20to%20many%20scientific%0Aand%20engineering%20tasks.%20Although%20deep-learning%20regressors%20excel%20with%20abundant%0Alabels%2C%20their%20accuracy%20deteriorates%20in%20data-scarce%20regimes.%20We%20introduce%0ARankRefine%2C%20a%20model-agnostic%2C%20plug-and-play%20post%20hoc%20method%20that%20refines%0Aregression%20with%20expert%20knowledge%20coming%20from%20pairwise%20rankings.%20Given%20a%20query%0Aitem%20and%20a%20small%20reference%20set%20with%20known%20properties%2C%20RankRefine%20combines%20the%0Abase%20regressor%27s%20output%20with%20a%20rank-based%20estimate%20via%20inverse%20variance%0Aweighting%2C%20requiring%20no%20retraining.%20In%20molecular%20property%20prediction%20task%2C%0ARankRefine%20achieves%20up%20to%2010%25%20relative%20reduction%20in%20mean%20absolute%20error%20using%0Aonly%2020%20pairwise%20comparisons%20obtained%20through%20a%20general-purpose%20large%20language%0Amodel%20%28LLM%29%20with%20no%20finetuning.%20As%20rankings%20provided%20by%20human%20experts%20or%0Ageneral-purpose%20LLMs%20are%20sufficient%20for%20improving%20regression%20across%20diverse%0Adomains%2C%20RankRefine%20offers%20practicality%20and%20broad%20applicability%2C%20especially%20in%0Alow-data%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16495v2&entry.124074799=Read"},
{"title": "Auto-ARGUE: LLM-Based Report Generation Evaluation", "author": "William Walden and Marc Mason and Orion Weller and Laura Dietz and Hannah Recknor and Bryan Li and Gabrielle Kaili-May Liu and Yu Hou and James Mayfield and Eugene Yang", "abstract": "  Generation of long-form, citation-backed reports is a primary use case for\nretrieval augmented generation (RAG) systems. While open-source evaluation\ntools exist for various RAG tasks, ones tailored to report generation are\nlacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based\nimplementation of the recent ARGUE framework for report generation evaluation.\nWe present analysis of Auto-ARGUE on the report generation pilot task from the\nTREC 2024 NeuCLIR track, showing good system-level correlations with human\njudgments. We further release a web app for visualization of Auto-ARGUE\noutputs.\n", "link": "http://arxiv.org/abs/2509.26184v2", "date": "2025-10-01", "relevancy": 1.3758, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.468}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-ARGUE%3A%20LLM-Based%20Report%20Generation%20Evaluation&body=Title%3A%20Auto-ARGUE%3A%20LLM-Based%20Report%20Generation%20Evaluation%0AAuthor%3A%20William%20Walden%20and%20Marc%20Mason%20and%20Orion%20Weller%20and%20Laura%20Dietz%20and%20Hannah%20Recknor%20and%20Bryan%20Li%20and%20Gabrielle%20Kaili-May%20Liu%20and%20Yu%20Hou%20and%20James%20Mayfield%20and%20Eugene%20Yang%0AAbstract%3A%20%20%20Generation%20of%20long-form%2C%20citation-backed%20reports%20is%20a%20primary%20use%20case%20for%0Aretrieval%20augmented%20generation%20%28RAG%29%20systems.%20While%20open-source%20evaluation%0Atools%20exist%20for%20various%20RAG%20tasks%2C%20ones%20tailored%20to%20report%20generation%20are%0Alacking.%20Accordingly%2C%20we%20introduce%20Auto-ARGUE%2C%20a%20robust%20LLM-based%0Aimplementation%20of%20the%20recent%20ARGUE%20framework%20for%20report%20generation%20evaluation.%0AWe%20present%20analysis%20of%20Auto-ARGUE%20on%20the%20report%20generation%20pilot%20task%20from%20the%0ATREC%202024%20NeuCLIR%20track%2C%20showing%20good%20system-level%20correlations%20with%20human%0Ajudgments.%20We%20further%20release%20a%20web%20app%20for%20visualization%20of%20Auto-ARGUE%0Aoutputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-ARGUE%253A%2520LLM-Based%2520Report%2520Generation%2520Evaluation%26entry.906535625%3DWilliam%2520Walden%2520and%2520Marc%2520Mason%2520and%2520Orion%2520Weller%2520and%2520Laura%2520Dietz%2520and%2520Hannah%2520Recknor%2520and%2520Bryan%2520Li%2520and%2520Gabrielle%2520Kaili-May%2520Liu%2520and%2520Yu%2520Hou%2520and%2520James%2520Mayfield%2520and%2520Eugene%2520Yang%26entry.1292438233%3D%2520%2520Generation%2520of%2520long-form%252C%2520citation-backed%2520reports%2520is%2520a%2520primary%2520use%2520case%2520for%250Aretrieval%2520augmented%2520generation%2520%2528RAG%2529%2520systems.%2520While%2520open-source%2520evaluation%250Atools%2520exist%2520for%2520various%2520RAG%2520tasks%252C%2520ones%2520tailored%2520to%2520report%2520generation%2520are%250Alacking.%2520Accordingly%252C%2520we%2520introduce%2520Auto-ARGUE%252C%2520a%2520robust%2520LLM-based%250Aimplementation%2520of%2520the%2520recent%2520ARGUE%2520framework%2520for%2520report%2520generation%2520evaluation.%250AWe%2520present%2520analysis%2520of%2520Auto-ARGUE%2520on%2520the%2520report%2520generation%2520pilot%2520task%2520from%2520the%250ATREC%25202024%2520NeuCLIR%2520track%252C%2520showing%2520good%2520system-level%2520correlations%2520with%2520human%250Ajudgments.%2520We%2520further%2520release%2520a%2520web%2520app%2520for%2520visualization%2520of%2520Auto-ARGUE%250Aoutputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-ARGUE%3A%20LLM-Based%20Report%20Generation%20Evaluation&entry.906535625=William%20Walden%20and%20Marc%20Mason%20and%20Orion%20Weller%20and%20Laura%20Dietz%20and%20Hannah%20Recknor%20and%20Bryan%20Li%20and%20Gabrielle%20Kaili-May%20Liu%20and%20Yu%20Hou%20and%20James%20Mayfield%20and%20Eugene%20Yang&entry.1292438233=%20%20Generation%20of%20long-form%2C%20citation-backed%20reports%20is%20a%20primary%20use%20case%20for%0Aretrieval%20augmented%20generation%20%28RAG%29%20systems.%20While%20open-source%20evaluation%0Atools%20exist%20for%20various%20RAG%20tasks%2C%20ones%20tailored%20to%20report%20generation%20are%0Alacking.%20Accordingly%2C%20we%20introduce%20Auto-ARGUE%2C%20a%20robust%20LLM-based%0Aimplementation%20of%20the%20recent%20ARGUE%20framework%20for%20report%20generation%20evaluation.%0AWe%20present%20analysis%20of%20Auto-ARGUE%20on%20the%20report%20generation%20pilot%20task%20from%20the%0ATREC%202024%20NeuCLIR%20track%2C%20showing%20good%20system-level%20correlations%20with%20human%0Ajudgments.%20We%20further%20release%20a%20web%20app%20for%20visualization%20of%20Auto-ARGUE%0Aoutputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26184v2&entry.124074799=Read"},
{"title": "A Likelihood Based Approach to Distribution Regression Using Conditional\n  Deep Generative Models", "author": "Shivam Kumar and Yun Yang and Lizhen Lin", "abstract": "  In this work, we explore the theoretical properties of conditional deep\ngenerative models under the statistical framework of distribution regression\nwhere the response variable lies in a high-dimensional ambient space but\nconcentrates around a potentially lower-dimensional manifold. More\nspecifically, we study the large-sample properties of a likelihood-based\napproach for estimating these models. Our results lead to the convergence rate\nof a sieve maximum likelihood estimator (MLE) for estimating the conditional\ndistribution (and its devolved counterpart) of the response given predictors in\nthe Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic\ndimension and smoothness of the true conditional distribution. These findings\nprovide an explanation of why conditional deep generative models can circumvent\nthe curse of dimensionality from the perspective of statistical foundations and\ndemonstrate that they can learn a broader class of nearly singular conditional\ndistributions. Our analysis also emphasizes the importance of introducing a\nsmall noise perturbation to the data when they are supported sufficiently close\nto a manifold. Finally, in our numerical studies, we demonstrate the effective\nimplementation of the proposed approach using both synthetic and real-world\ndatasets, which also provide complementary validation to our theoretical\nfindings.\n", "link": "http://arxiv.org/abs/2410.02025v2", "date": "2025-10-01", "relevancy": 0.9784, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4911}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Likelihood%20Based%20Approach%20to%20Distribution%20Regression%20Using%20Conditional%0A%20%20Deep%20Generative%20Models&body=Title%3A%20A%20Likelihood%20Based%20Approach%20to%20Distribution%20Regression%20Using%20Conditional%0A%20%20Deep%20Generative%20Models%0AAuthor%3A%20Shivam%20Kumar%20and%20Yun%20Yang%20and%20Lizhen%20Lin%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20the%20theoretical%20properties%20of%20conditional%20deep%0Agenerative%20models%20under%20the%20statistical%20framework%20of%20distribution%20regression%0Awhere%20the%20response%20variable%20lies%20in%20a%20high-dimensional%20ambient%20space%20but%0Aconcentrates%20around%20a%20potentially%20lower-dimensional%20manifold.%20More%0Aspecifically%2C%20we%20study%20the%20large-sample%20properties%20of%20a%20likelihood-based%0Aapproach%20for%20estimating%20these%20models.%20Our%20results%20lead%20to%20the%20convergence%20rate%0Aof%20a%20sieve%20maximum%20likelihood%20estimator%20%28MLE%29%20for%20estimating%20the%20conditional%0Adistribution%20%28and%20its%20devolved%20counterpart%29%20of%20the%20response%20given%20predictors%20in%0Athe%20Hellinger%20%28Wasserstein%29%20metric.%20Our%20rates%20depend%20solely%20on%20the%20intrinsic%0Adimension%20and%20smoothness%20of%20the%20true%20conditional%20distribution.%20These%20findings%0Aprovide%20an%20explanation%20of%20why%20conditional%20deep%20generative%20models%20can%20circumvent%0Athe%20curse%20of%20dimensionality%20from%20the%20perspective%20of%20statistical%20foundations%20and%0Ademonstrate%20that%20they%20can%20learn%20a%20broader%20class%20of%20nearly%20singular%20conditional%0Adistributions.%20Our%20analysis%20also%20emphasizes%20the%20importance%20of%20introducing%20a%0Asmall%20noise%20perturbation%20to%20the%20data%20when%20they%20are%20supported%20sufficiently%20close%0Ato%20a%20manifold.%20Finally%2C%20in%20our%20numerical%20studies%2C%20we%20demonstrate%20the%20effective%0Aimplementation%20of%20the%20proposed%20approach%20using%20both%20synthetic%20and%20real-world%0Adatasets%2C%20which%20also%20provide%20complementary%20validation%20to%20our%20theoretical%0Afindings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Likelihood%2520Based%2520Approach%2520to%2520Distribution%2520Regression%2520Using%2520Conditional%250A%2520%2520Deep%2520Generative%2520Models%26entry.906535625%3DShivam%2520Kumar%2520and%2520Yun%2520Yang%2520and%2520Lizhen%2520Lin%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520theoretical%2520properties%2520of%2520conditional%2520deep%250Agenerative%2520models%2520under%2520the%2520statistical%2520framework%2520of%2520distribution%2520regression%250Awhere%2520the%2520response%2520variable%2520lies%2520in%2520a%2520high-dimensional%2520ambient%2520space%2520but%250Aconcentrates%2520around%2520a%2520potentially%2520lower-dimensional%2520manifold.%2520More%250Aspecifically%252C%2520we%2520study%2520the%2520large-sample%2520properties%2520of%2520a%2520likelihood-based%250Aapproach%2520for%2520estimating%2520these%2520models.%2520Our%2520results%2520lead%2520to%2520the%2520convergence%2520rate%250Aof%2520a%2520sieve%2520maximum%2520likelihood%2520estimator%2520%2528MLE%2529%2520for%2520estimating%2520the%2520conditional%250Adistribution%2520%2528and%2520its%2520devolved%2520counterpart%2529%2520of%2520the%2520response%2520given%2520predictors%2520in%250Athe%2520Hellinger%2520%2528Wasserstein%2529%2520metric.%2520Our%2520rates%2520depend%2520solely%2520on%2520the%2520intrinsic%250Adimension%2520and%2520smoothness%2520of%2520the%2520true%2520conditional%2520distribution.%2520These%2520findings%250Aprovide%2520an%2520explanation%2520of%2520why%2520conditional%2520deep%2520generative%2520models%2520can%2520circumvent%250Athe%2520curse%2520of%2520dimensionality%2520from%2520the%2520perspective%2520of%2520statistical%2520foundations%2520and%250Ademonstrate%2520that%2520they%2520can%2520learn%2520a%2520broader%2520class%2520of%2520nearly%2520singular%2520conditional%250Adistributions.%2520Our%2520analysis%2520also%2520emphasizes%2520the%2520importance%2520of%2520introducing%2520a%250Asmall%2520noise%2520perturbation%2520to%2520the%2520data%2520when%2520they%2520are%2520supported%2520sufficiently%2520close%250Ato%2520a%2520manifold.%2520Finally%252C%2520in%2520our%2520numerical%2520studies%252C%2520we%2520demonstrate%2520the%2520effective%250Aimplementation%2520of%2520the%2520proposed%2520approach%2520using%2520both%2520synthetic%2520and%2520real-world%250Adatasets%252C%2520which%2520also%2520provide%2520complementary%2520validation%2520to%2520our%2520theoretical%250Afindings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Likelihood%20Based%20Approach%20to%20Distribution%20Regression%20Using%20Conditional%0A%20%20Deep%20Generative%20Models&entry.906535625=Shivam%20Kumar%20and%20Yun%20Yang%20and%20Lizhen%20Lin&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20the%20theoretical%20properties%20of%20conditional%20deep%0Agenerative%20models%20under%20the%20statistical%20framework%20of%20distribution%20regression%0Awhere%20the%20response%20variable%20lies%20in%20a%20high-dimensional%20ambient%20space%20but%0Aconcentrates%20around%20a%20potentially%20lower-dimensional%20manifold.%20More%0Aspecifically%2C%20we%20study%20the%20large-sample%20properties%20of%20a%20likelihood-based%0Aapproach%20for%20estimating%20these%20models.%20Our%20results%20lead%20to%20the%20convergence%20rate%0Aof%20a%20sieve%20maximum%20likelihood%20estimator%20%28MLE%29%20for%20estimating%20the%20conditional%0Adistribution%20%28and%20its%20devolved%20counterpart%29%20of%20the%20response%20given%20predictors%20in%0Athe%20Hellinger%20%28Wasserstein%29%20metric.%20Our%20rates%20depend%20solely%20on%20the%20intrinsic%0Adimension%20and%20smoothness%20of%20the%20true%20conditional%20distribution.%20These%20findings%0Aprovide%20an%20explanation%20of%20why%20conditional%20deep%20generative%20models%20can%20circumvent%0Athe%20curse%20of%20dimensionality%20from%20the%20perspective%20of%20statistical%20foundations%20and%0Ademonstrate%20that%20they%20can%20learn%20a%20broader%20class%20of%20nearly%20singular%20conditional%0Adistributions.%20Our%20analysis%20also%20emphasizes%20the%20importance%20of%20introducing%20a%0Asmall%20noise%20perturbation%20to%20the%20data%20when%20they%20are%20supported%20sufficiently%20close%0Ato%20a%20manifold.%20Finally%2C%20in%20our%20numerical%20studies%2C%20we%20demonstrate%20the%20effective%0Aimplementation%20of%20the%20proposed%20approach%20using%20both%20synthetic%20and%20real-world%0Adatasets%2C%20which%20also%20provide%20complementary%20validation%20to%20our%20theoretical%0Afindings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02025v2&entry.124074799=Read"},
{"title": "The Illusion of Readiness: Stress Testing Large Frontier Models on\n  Multimodal Medical Benchmarks", "author": "Yu Gu and Jingjing Fu and Xiaodong Liu and Jeya Maria Jose Valanarasu and Noel CF Codella and Reuben Tan and Qianchu Liu and Ying Jin and Sheng Zhang and Jinyu Wang and Rui Wang and Lei Song and Guanghui Qin and Naoto Usuyama and Cliff Wong and Hao Cheng and Hohin Lee and Praneeth Sanapathi and Sarah Hilado and Jiang Bian and Javier Alvarez-Valle and Mu Wei and Khalil Malik and Jianfeng Gao and Eric Horvitz and Matthew P Lungren and Hoifung Poon and Paul Vozila", "abstract": "  Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.\n", "link": "http://arxiv.org/abs/2509.18234v2", "date": "2025-10-01", "relevancy": 1.3586, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4593}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Illusion%20of%20Readiness%3A%20Stress%20Testing%20Large%20Frontier%20Models%20on%0A%20%20Multimodal%20Medical%20Benchmarks&body=Title%3A%20The%20Illusion%20of%20Readiness%3A%20Stress%20Testing%20Large%20Frontier%20Models%20on%0A%20%20Multimodal%20Medical%20Benchmarks%0AAuthor%3A%20Yu%20Gu%20and%20Jingjing%20Fu%20and%20Xiaodong%20Liu%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Noel%20CF%20Codella%20and%20Reuben%20Tan%20and%20Qianchu%20Liu%20and%20Ying%20Jin%20and%20Sheng%20Zhang%20and%20Jinyu%20Wang%20and%20Rui%20Wang%20and%20Lei%20Song%20and%20Guanghui%20Qin%20and%20Naoto%20Usuyama%20and%20Cliff%20Wong%20and%20Hao%20Cheng%20and%20Hohin%20Lee%20and%20Praneeth%20Sanapathi%20and%20Sarah%20Hilado%20and%20Jiang%20Bian%20and%20Javier%20Alvarez-Valle%20and%20Mu%20Wei%20and%20Khalil%20Malik%20and%20Jianfeng%20Gao%20and%20Eric%20Horvitz%20and%20Matthew%20P%20Lungren%20and%20Hoifung%20Poon%20and%20Paul%20Vozila%0AAbstract%3A%20%20%20Large%20frontier%20models%20like%20GPT-5%20now%20achieve%20top%20scores%20on%20medical%0Abenchmarks.%20But%20our%20stress%20tests%20tell%20a%20different%20story.%20Leading%20systems%20often%0Aguess%20correctly%20even%20when%20key%20inputs%20like%20images%20are%20removed%2C%20flip%20answers%0Aunder%20trivial%20prompt%20changes%2C%20and%20fabricate%20convincing%20yet%20flawed%20reasoning.%0AThese%20aren%27t%20glitches%3B%20they%20expose%20how%20today%27s%20benchmarks%20reward%20test-taking%0Atricks%20over%20medical%20understanding.%20We%20evaluate%20six%20flagship%20models%20across%20six%0Awidely%20used%20benchmarks%20and%20find%20that%20high%20leaderboard%20scores%20hide%20brittleness%0Aand%20shortcut%20learning.%20Through%20clinician-guided%20rubric%20evaluation%2C%20we%20show%20that%0Abenchmarks%20vary%20widely%20in%20what%20they%20truly%20measure%20yet%20are%20treated%0Ainterchangeably%2C%20masking%20failure%20modes.%20We%20caution%20that%20medical%20benchmark%0Ascores%20do%20not%20directly%20reflect%20real-world%20readiness.%20If%20we%20want%20AI%20to%20earn%0Atrust%20in%20healthcare%2C%20we%20must%20demand%20more%20than%20leaderboard%20wins%20and%20must%20hold%0Asystems%20accountable%20for%20robustness%2C%20sound%20reasoning%2C%20and%20alignment%20with%20real%0Amedical%20demands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.18234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Illusion%2520of%2520Readiness%253A%2520Stress%2520Testing%2520Large%2520Frontier%2520Models%2520on%250A%2520%2520Multimodal%2520Medical%2520Benchmarks%26entry.906535625%3DYu%2520Gu%2520and%2520Jingjing%2520Fu%2520and%2520Xiaodong%2520Liu%2520and%2520Jeya%2520Maria%2520Jose%2520Valanarasu%2520and%2520Noel%2520CF%2520Codella%2520and%2520Reuben%2520Tan%2520and%2520Qianchu%2520Liu%2520and%2520Ying%2520Jin%2520and%2520Sheng%2520Zhang%2520and%2520Jinyu%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Lei%2520Song%2520and%2520Guanghui%2520Qin%2520and%2520Naoto%2520Usuyama%2520and%2520Cliff%2520Wong%2520and%2520Hao%2520Cheng%2520and%2520Hohin%2520Lee%2520and%2520Praneeth%2520Sanapathi%2520and%2520Sarah%2520Hilado%2520and%2520Jiang%2520Bian%2520and%2520Javier%2520Alvarez-Valle%2520and%2520Mu%2520Wei%2520and%2520Khalil%2520Malik%2520and%2520Jianfeng%2520Gao%2520and%2520Eric%2520Horvitz%2520and%2520Matthew%2520P%2520Lungren%2520and%2520Hoifung%2520Poon%2520and%2520Paul%2520Vozila%26entry.1292438233%3D%2520%2520Large%2520frontier%2520models%2520like%2520GPT-5%2520now%2520achieve%2520top%2520scores%2520on%2520medical%250Abenchmarks.%2520But%2520our%2520stress%2520tests%2520tell%2520a%2520different%2520story.%2520Leading%2520systems%2520often%250Aguess%2520correctly%2520even%2520when%2520key%2520inputs%2520like%2520images%2520are%2520removed%252C%2520flip%2520answers%250Aunder%2520trivial%2520prompt%2520changes%252C%2520and%2520fabricate%2520convincing%2520yet%2520flawed%2520reasoning.%250AThese%2520aren%2527t%2520glitches%253B%2520they%2520expose%2520how%2520today%2527s%2520benchmarks%2520reward%2520test-taking%250Atricks%2520over%2520medical%2520understanding.%2520We%2520evaluate%2520six%2520flagship%2520models%2520across%2520six%250Awidely%2520used%2520benchmarks%2520and%2520find%2520that%2520high%2520leaderboard%2520scores%2520hide%2520brittleness%250Aand%2520shortcut%2520learning.%2520Through%2520clinician-guided%2520rubric%2520evaluation%252C%2520we%2520show%2520that%250Abenchmarks%2520vary%2520widely%2520in%2520what%2520they%2520truly%2520measure%2520yet%2520are%2520treated%250Ainterchangeably%252C%2520masking%2520failure%2520modes.%2520We%2520caution%2520that%2520medical%2520benchmark%250Ascores%2520do%2520not%2520directly%2520reflect%2520real-world%2520readiness.%2520If%2520we%2520want%2520AI%2520to%2520earn%250Atrust%2520in%2520healthcare%252C%2520we%2520must%2520demand%2520more%2520than%2520leaderboard%2520wins%2520and%2520must%2520hold%250Asystems%2520accountable%2520for%2520robustness%252C%2520sound%2520reasoning%252C%2520and%2520alignment%2520with%2520real%250Amedical%2520demands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Illusion%20of%20Readiness%3A%20Stress%20Testing%20Large%20Frontier%20Models%20on%0A%20%20Multimodal%20Medical%20Benchmarks&entry.906535625=Yu%20Gu%20and%20Jingjing%20Fu%20and%20Xiaodong%20Liu%20and%20Jeya%20Maria%20Jose%20Valanarasu%20and%20Noel%20CF%20Codella%20and%20Reuben%20Tan%20and%20Qianchu%20Liu%20and%20Ying%20Jin%20and%20Sheng%20Zhang%20and%20Jinyu%20Wang%20and%20Rui%20Wang%20and%20Lei%20Song%20and%20Guanghui%20Qin%20and%20Naoto%20Usuyama%20and%20Cliff%20Wong%20and%20Hao%20Cheng%20and%20Hohin%20Lee%20and%20Praneeth%20Sanapathi%20and%20Sarah%20Hilado%20and%20Jiang%20Bian%20and%20Javier%20Alvarez-Valle%20and%20Mu%20Wei%20and%20Khalil%20Malik%20and%20Jianfeng%20Gao%20and%20Eric%20Horvitz%20and%20Matthew%20P%20Lungren%20and%20Hoifung%20Poon%20and%20Paul%20Vozila&entry.1292438233=%20%20Large%20frontier%20models%20like%20GPT-5%20now%20achieve%20top%20scores%20on%20medical%0Abenchmarks.%20But%20our%20stress%20tests%20tell%20a%20different%20story.%20Leading%20systems%20often%0Aguess%20correctly%20even%20when%20key%20inputs%20like%20images%20are%20removed%2C%20flip%20answers%0Aunder%20trivial%20prompt%20changes%2C%20and%20fabricate%20convincing%20yet%20flawed%20reasoning.%0AThese%20aren%27t%20glitches%3B%20they%20expose%20how%20today%27s%20benchmarks%20reward%20test-taking%0Atricks%20over%20medical%20understanding.%20We%20evaluate%20six%20flagship%20models%20across%20six%0Awidely%20used%20benchmarks%20and%20find%20that%20high%20leaderboard%20scores%20hide%20brittleness%0Aand%20shortcut%20learning.%20Through%20clinician-guided%20rubric%20evaluation%2C%20we%20show%20that%0Abenchmarks%20vary%20widely%20in%20what%20they%20truly%20measure%20yet%20are%20treated%0Ainterchangeably%2C%20masking%20failure%20modes.%20We%20caution%20that%20medical%20benchmark%0Ascores%20do%20not%20directly%20reflect%20real-world%20readiness.%20If%20we%20want%20AI%20to%20earn%0Atrust%20in%20healthcare%2C%20we%20must%20demand%20more%20than%20leaderboard%20wins%20and%20must%20hold%0Asystems%20accountable%20for%20robustness%2C%20sound%20reasoning%2C%20and%20alignment%20with%20real%0Amedical%20demands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.18234v2&entry.124074799=Read"},
{"title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents", "author": "Akshat Naik and Patrick Quinn and Guillermo Bosch and Emma Goun\u00e9 and Francisco Javier Campos Zabala and Jason Ross Brown and Edward James Young", "abstract": "  As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. While prior research has studied agents' ability\nto produce harmful outputs or follow malicious instructions, it remains unclear\nhow likely agents are to spontaneously pursue unintended goals in realistic\ndeployments. In this work, we approach misalignment as a conflict between the\ninternal goals pursued by the model and the goals intended by its deployer. We\nintroduce a misalignment propensity benchmark, \\textsc{AgentMisalignment}, a\nbenchmark suite designed to evaluate the propensity of LLM agents to misalign\nin realistic scenarios. Evaluations cover behaviours such as avoiding\noversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier\nmodels, we find that more capable agents tend to exhibit higher misalignment on\naverage. We also systematically vary agent personalities through different\nsystem prompts and observe that persona characteristics can strongly and\nunpredictably influence misalignment, sometimes more than the choice of model\nitself. Our results reveal the limitations of current alignment methods for\nautonomous LLM agents and underscore the need to rethink misalignment in\nrealistic deployment settings.\n", "link": "http://arxiv.org/abs/2506.04018v2", "date": "2025-10-01", "relevancy": 1.3857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentMisalignment%3A%20Measuring%20the%20Propensity%20for%20Misaligned%20Behaviour%20in%0A%20%20LLM-Based%20Agents&body=Title%3A%20AgentMisalignment%3A%20Measuring%20the%20Propensity%20for%20Misaligned%20Behaviour%20in%0A%20%20LLM-Based%20Agents%0AAuthor%3A%20Akshat%20Naik%20and%20Patrick%20Quinn%20and%20Guillermo%20Bosch%20and%20Emma%20Goun%C3%A9%20and%20Francisco%20Javier%20Campos%20Zabala%20and%20Jason%20Ross%20Brown%20and%20Edward%20James%20Young%0AAbstract%3A%20%20%20As%20Large%20Language%20Model%20%28LLM%29%20agents%20become%20more%20widespread%2C%20associated%0Amisalignment%20risks%20increase.%20While%20prior%20research%20has%20studied%20agents%27%20ability%0Ato%20produce%20harmful%20outputs%20or%20follow%20malicious%20instructions%2C%20it%20remains%20unclear%0Ahow%20likely%20agents%20are%20to%20spontaneously%20pursue%20unintended%20goals%20in%20realistic%0Adeployments.%20In%20this%20work%2C%20we%20approach%20misalignment%20as%20a%20conflict%20between%20the%0Ainternal%20goals%20pursued%20by%20the%20model%20and%20the%20goals%20intended%20by%20its%20deployer.%20We%0Aintroduce%20a%20misalignment%20propensity%20benchmark%2C%20%5Ctextsc%7BAgentMisalignment%7D%2C%20a%0Abenchmark%20suite%20designed%20to%20evaluate%20the%20propensity%20of%20LLM%20agents%20to%20misalign%0Ain%20realistic%20scenarios.%20Evaluations%20cover%20behaviours%20such%20as%20avoiding%0Aoversight%2C%20resisting%20shutdown%2C%20sandbagging%2C%20and%20power-seeking.%20Testing%20frontier%0Amodels%2C%20we%20find%20that%20more%20capable%20agents%20tend%20to%20exhibit%20higher%20misalignment%20on%0Aaverage.%20We%20also%20systematically%20vary%20agent%20personalities%20through%20different%0Asystem%20prompts%20and%20observe%20that%20persona%20characteristics%20can%20strongly%20and%0Aunpredictably%20influence%20misalignment%2C%20sometimes%20more%20than%20the%20choice%20of%20model%0Aitself.%20Our%20results%20reveal%20the%20limitations%20of%20current%20alignment%20methods%20for%0Aautonomous%20LLM%20agents%20and%20underscore%20the%20need%20to%20rethink%20misalignment%20in%0Arealistic%20deployment%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04018v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentMisalignment%253A%2520Measuring%2520the%2520Propensity%2520for%2520Misaligned%2520Behaviour%2520in%250A%2520%2520LLM-Based%2520Agents%26entry.906535625%3DAkshat%2520Naik%2520and%2520Patrick%2520Quinn%2520and%2520Guillermo%2520Bosch%2520and%2520Emma%2520Goun%25C3%25A9%2520and%2520Francisco%2520Javier%2520Campos%2520Zabala%2520and%2520Jason%2520Ross%2520Brown%2520and%2520Edward%2520James%2520Young%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520become%2520more%2520widespread%252C%2520associated%250Amisalignment%2520risks%2520increase.%2520While%2520prior%2520research%2520has%2520studied%2520agents%2527%2520ability%250Ato%2520produce%2520harmful%2520outputs%2520or%2520follow%2520malicious%2520instructions%252C%2520it%2520remains%2520unclear%250Ahow%2520likely%2520agents%2520are%2520to%2520spontaneously%2520pursue%2520unintended%2520goals%2520in%2520realistic%250Adeployments.%2520In%2520this%2520work%252C%2520we%2520approach%2520misalignment%2520as%2520a%2520conflict%2520between%2520the%250Ainternal%2520goals%2520pursued%2520by%2520the%2520model%2520and%2520the%2520goals%2520intended%2520by%2520its%2520deployer.%2520We%250Aintroduce%2520a%2520misalignment%2520propensity%2520benchmark%252C%2520%255Ctextsc%257BAgentMisalignment%257D%252C%2520a%250Abenchmark%2520suite%2520designed%2520to%2520evaluate%2520the%2520propensity%2520of%2520LLM%2520agents%2520to%2520misalign%250Ain%2520realistic%2520scenarios.%2520Evaluations%2520cover%2520behaviours%2520such%2520as%2520avoiding%250Aoversight%252C%2520resisting%2520shutdown%252C%2520sandbagging%252C%2520and%2520power-seeking.%2520Testing%2520frontier%250Amodels%252C%2520we%2520find%2520that%2520more%2520capable%2520agents%2520tend%2520to%2520exhibit%2520higher%2520misalignment%2520on%250Aaverage.%2520We%2520also%2520systematically%2520vary%2520agent%2520personalities%2520through%2520different%250Asystem%2520prompts%2520and%2520observe%2520that%2520persona%2520characteristics%2520can%2520strongly%2520and%250Aunpredictably%2520influence%2520misalignment%252C%2520sometimes%2520more%2520than%2520the%2520choice%2520of%2520model%250Aitself.%2520Our%2520results%2520reveal%2520the%2520limitations%2520of%2520current%2520alignment%2520methods%2520for%250Aautonomous%2520LLM%2520agents%2520and%2520underscore%2520the%2520need%2520to%2520rethink%2520misalignment%2520in%250Arealistic%2520deployment%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04018v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentMisalignment%3A%20Measuring%20the%20Propensity%20for%20Misaligned%20Behaviour%20in%0A%20%20LLM-Based%20Agents&entry.906535625=Akshat%20Naik%20and%20Patrick%20Quinn%20and%20Guillermo%20Bosch%20and%20Emma%20Goun%C3%A9%20and%20Francisco%20Javier%20Campos%20Zabala%20and%20Jason%20Ross%20Brown%20and%20Edward%20James%20Young&entry.1292438233=%20%20As%20Large%20Language%20Model%20%28LLM%29%20agents%20become%20more%20widespread%2C%20associated%0Amisalignment%20risks%20increase.%20While%20prior%20research%20has%20studied%20agents%27%20ability%0Ato%20produce%20harmful%20outputs%20or%20follow%20malicious%20instructions%2C%20it%20remains%20unclear%0Ahow%20likely%20agents%20are%20to%20spontaneously%20pursue%20unintended%20goals%20in%20realistic%0Adeployments.%20In%20this%20work%2C%20we%20approach%20misalignment%20as%20a%20conflict%20between%20the%0Ainternal%20goals%20pursued%20by%20the%20model%20and%20the%20goals%20intended%20by%20its%20deployer.%20We%0Aintroduce%20a%20misalignment%20propensity%20benchmark%2C%20%5Ctextsc%7BAgentMisalignment%7D%2C%20a%0Abenchmark%20suite%20designed%20to%20evaluate%20the%20propensity%20of%20LLM%20agents%20to%20misalign%0Ain%20realistic%20scenarios.%20Evaluations%20cover%20behaviours%20such%20as%20avoiding%0Aoversight%2C%20resisting%20shutdown%2C%20sandbagging%2C%20and%20power-seeking.%20Testing%20frontier%0Amodels%2C%20we%20find%20that%20more%20capable%20agents%20tend%20to%20exhibit%20higher%20misalignment%20on%0Aaverage.%20We%20also%20systematically%20vary%20agent%20personalities%20through%20different%0Asystem%20prompts%20and%20observe%20that%20persona%20characteristics%20can%20strongly%20and%0Aunpredictably%20influence%20misalignment%2C%20sometimes%20more%20than%20the%20choice%20of%20model%0Aitself.%20Our%20results%20reveal%20the%20limitations%20of%20current%20alignment%20methods%20for%0Aautonomous%20LLM%20agents%20and%20underscore%20the%20need%20to%20rethink%20misalignment%20in%0Arealistic%20deployment%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04018v2&entry.124074799=Read"},
{"title": "LLM Watermark Evasion via Bias Inversion", "author": "Jeongyeon Hwang and Sangdon Park and Jungseul Ok", "abstract": "  Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses.\n", "link": "http://arxiv.org/abs/2509.23019v2", "date": "2025-10-01", "relevancy": 1.3887, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4721}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4561}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Watermark%20Evasion%20via%20Bias%20Inversion&body=Title%3A%20LLM%20Watermark%20Evasion%20via%20Bias%20Inversion%0AAuthor%3A%20Jeongyeon%20Hwang%20and%20Sangdon%20Park%20and%20Jungseul%20Ok%0AAbstract%3A%20%20%20Watermarking%20for%20large%20language%20models%20%28LLMs%29%20embeds%20a%20statistical%20signal%0Aduring%20generation%20to%20enable%20detection%20of%20model-produced%20text.%20While%0Awatermarking%20has%20proven%20effective%20in%20benign%20settings%2C%20its%20robustness%20under%0Aadversarial%20evasion%20remains%20contested.%20To%20advance%20a%20rigorous%20understanding%20and%0Aevaluation%20of%20such%20vulnerabilities%2C%20we%20propose%20the%20%5Cemph%7BBias-Inversion%0ARewriting%20Attack%7D%20%28BIRA%29%2C%20which%20is%20theoretically%20motivated%20and%20model-agnostic.%0ABIRA%20weakens%20the%20watermark%20signal%20by%20suppressing%20the%20logits%20of%20likely%0Awatermarked%20tokens%20during%20LLM-based%20rewriting%2C%20without%20any%20knowledge%20of%20the%0Aunderlying%20watermarking%20scheme.%20Across%20recent%20watermarking%20methods%2C%20BIRA%0Aachieves%20over%2099%5C%25%20evasion%20while%20preserving%20the%20semantic%20content%20of%20the%0Aoriginal%20text.%20Beyond%20demonstrating%20an%20attack%2C%20our%20results%20reveal%20a%20systematic%0Avulnerability%2C%20emphasizing%20the%20need%20for%20stress%20testing%20and%20robust%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23019v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Watermark%2520Evasion%2520via%2520Bias%2520Inversion%26entry.906535625%3DJeongyeon%2520Hwang%2520and%2520Sangdon%2520Park%2520and%2520Jungseul%2520Ok%26entry.1292438233%3D%2520%2520Watermarking%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520embeds%2520a%2520statistical%2520signal%250Aduring%2520generation%2520to%2520enable%2520detection%2520of%2520model-produced%2520text.%2520While%250Awatermarking%2520has%2520proven%2520effective%2520in%2520benign%2520settings%252C%2520its%2520robustness%2520under%250Aadversarial%2520evasion%2520remains%2520contested.%2520To%2520advance%2520a%2520rigorous%2520understanding%2520and%250Aevaluation%2520of%2520such%2520vulnerabilities%252C%2520we%2520propose%2520the%2520%255Cemph%257BBias-Inversion%250ARewriting%2520Attack%257D%2520%2528BIRA%2529%252C%2520which%2520is%2520theoretically%2520motivated%2520and%2520model-agnostic.%250ABIRA%2520weakens%2520the%2520watermark%2520signal%2520by%2520suppressing%2520the%2520logits%2520of%2520likely%250Awatermarked%2520tokens%2520during%2520LLM-based%2520rewriting%252C%2520without%2520any%2520knowledge%2520of%2520the%250Aunderlying%2520watermarking%2520scheme.%2520Across%2520recent%2520watermarking%2520methods%252C%2520BIRA%250Aachieves%2520over%252099%255C%2525%2520evasion%2520while%2520preserving%2520the%2520semantic%2520content%2520of%2520the%250Aoriginal%2520text.%2520Beyond%2520demonstrating%2520an%2520attack%252C%2520our%2520results%2520reveal%2520a%2520systematic%250Avulnerability%252C%2520emphasizing%2520the%2520need%2520for%2520stress%2520testing%2520and%2520robust%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23019v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Watermark%20Evasion%20via%20Bias%20Inversion&entry.906535625=Jeongyeon%20Hwang%20and%20Sangdon%20Park%20and%20Jungseul%20Ok&entry.1292438233=%20%20Watermarking%20for%20large%20language%20models%20%28LLMs%29%20embeds%20a%20statistical%20signal%0Aduring%20generation%20to%20enable%20detection%20of%20model-produced%20text.%20While%0Awatermarking%20has%20proven%20effective%20in%20benign%20settings%2C%20its%20robustness%20under%0Aadversarial%20evasion%20remains%20contested.%20To%20advance%20a%20rigorous%20understanding%20and%0Aevaluation%20of%20such%20vulnerabilities%2C%20we%20propose%20the%20%5Cemph%7BBias-Inversion%0ARewriting%20Attack%7D%20%28BIRA%29%2C%20which%20is%20theoretically%20motivated%20and%20model-agnostic.%0ABIRA%20weakens%20the%20watermark%20signal%20by%20suppressing%20the%20logits%20of%20likely%0Awatermarked%20tokens%20during%20LLM-based%20rewriting%2C%20without%20any%20knowledge%20of%20the%0Aunderlying%20watermarking%20scheme.%20Across%20recent%20watermarking%20methods%2C%20BIRA%0Aachieves%20over%2099%5C%25%20evasion%20while%20preserving%20the%20semantic%20content%20of%20the%0Aoriginal%20text.%20Beyond%20demonstrating%20an%20attack%2C%20our%20results%20reveal%20a%20systematic%0Avulnerability%2C%20emphasizing%20the%20need%20for%20stress%20testing%20and%20robust%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23019v2&entry.124074799=Read"},
{"title": "Integration of Calcium Imaging Traces via Deep Generative Modeling", "author": "Berta Ros and Mireia Olives-Verger and Caterina Fuses and Josep M Canals and Jordi Soriano and Jordi Abante", "abstract": "  Calcium imaging allows for the parallel measurement of large neuronal\npopulations in a spatially resolved and minimally invasive manner, and has\nbecome a gold-standard for neuronal functionality. While deep generative models\nhave been successfully applied to study the activity of neuronal ensembles,\ntheir potential for learning single-neuron representations from calcium imaging\nfluorescence traces remains largely unexplored, and batch effects remain an\nimportant hurdle. To address this, we explore supervised variational\nautoencoder architectures that learn compact representations of individual\nneurons from fluorescent traces without relying on spike inference algorithms.\nWe find that this approach outperforms state-of-the-art models, preserving\nbiological variability while mitigating batch effects. Across simulated and\nexperimental datasets, this framework enables robust visualization, clustering,\nand interpretation of single-neuron dynamics.\n", "link": "http://arxiv.org/abs/2501.14615v3", "date": "2025-10-01", "relevancy": 1.0568, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5421}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5236}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integration%20of%20Calcium%20Imaging%20Traces%20via%20Deep%20Generative%20Modeling&body=Title%3A%20Integration%20of%20Calcium%20Imaging%20Traces%20via%20Deep%20Generative%20Modeling%0AAuthor%3A%20Berta%20Ros%20and%20Mireia%20Olives-Verger%20and%20Caterina%20Fuses%20and%20Josep%20M%20Canals%20and%20Jordi%20Soriano%20and%20Jordi%20Abante%0AAbstract%3A%20%20%20Calcium%20imaging%20allows%20for%20the%20parallel%20measurement%20of%20large%20neuronal%0Apopulations%20in%20a%20spatially%20resolved%20and%20minimally%20invasive%20manner%2C%20and%20has%0Abecome%20a%20gold-standard%20for%20neuronal%20functionality.%20While%20deep%20generative%20models%0Ahave%20been%20successfully%20applied%20to%20study%20the%20activity%20of%20neuronal%20ensembles%2C%0Atheir%20potential%20for%20learning%20single-neuron%20representations%20from%20calcium%20imaging%0Afluorescence%20traces%20remains%20largely%20unexplored%2C%20and%20batch%20effects%20remain%20an%0Aimportant%20hurdle.%20To%20address%20this%2C%20we%20explore%20supervised%20variational%0Aautoencoder%20architectures%20that%20learn%20compact%20representations%20of%20individual%0Aneurons%20from%20fluorescent%20traces%20without%20relying%20on%20spike%20inference%20algorithms.%0AWe%20find%20that%20this%20approach%20outperforms%20state-of-the-art%20models%2C%20preserving%0Abiological%20variability%20while%20mitigating%20batch%20effects.%20Across%20simulated%20and%0Aexperimental%20datasets%2C%20this%20framework%20enables%20robust%20visualization%2C%20clustering%2C%0Aand%20interpretation%20of%20single-neuron%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14615v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegration%2520of%2520Calcium%2520Imaging%2520Traces%2520via%2520Deep%2520Generative%2520Modeling%26entry.906535625%3DBerta%2520Ros%2520and%2520Mireia%2520Olives-Verger%2520and%2520Caterina%2520Fuses%2520and%2520Josep%2520M%2520Canals%2520and%2520Jordi%2520Soriano%2520and%2520Jordi%2520Abante%26entry.1292438233%3D%2520%2520Calcium%2520imaging%2520allows%2520for%2520the%2520parallel%2520measurement%2520of%2520large%2520neuronal%250Apopulations%2520in%2520a%2520spatially%2520resolved%2520and%2520minimally%2520invasive%2520manner%252C%2520and%2520has%250Abecome%2520a%2520gold-standard%2520for%2520neuronal%2520functionality.%2520While%2520deep%2520generative%2520models%250Ahave%2520been%2520successfully%2520applied%2520to%2520study%2520the%2520activity%2520of%2520neuronal%2520ensembles%252C%250Atheir%2520potential%2520for%2520learning%2520single-neuron%2520representations%2520from%2520calcium%2520imaging%250Afluorescence%2520traces%2520remains%2520largely%2520unexplored%252C%2520and%2520batch%2520effects%2520remain%2520an%250Aimportant%2520hurdle.%2520To%2520address%2520this%252C%2520we%2520explore%2520supervised%2520variational%250Aautoencoder%2520architectures%2520that%2520learn%2520compact%2520representations%2520of%2520individual%250Aneurons%2520from%2520fluorescent%2520traces%2520without%2520relying%2520on%2520spike%2520inference%2520algorithms.%250AWe%2520find%2520that%2520this%2520approach%2520outperforms%2520state-of-the-art%2520models%252C%2520preserving%250Abiological%2520variability%2520while%2520mitigating%2520batch%2520effects.%2520Across%2520simulated%2520and%250Aexperimental%2520datasets%252C%2520this%2520framework%2520enables%2520robust%2520visualization%252C%2520clustering%252C%250Aand%2520interpretation%2520of%2520single-neuron%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14615v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integration%20of%20Calcium%20Imaging%20Traces%20via%20Deep%20Generative%20Modeling&entry.906535625=Berta%20Ros%20and%20Mireia%20Olives-Verger%20and%20Caterina%20Fuses%20and%20Josep%20M%20Canals%20and%20Jordi%20Soriano%20and%20Jordi%20Abante&entry.1292438233=%20%20Calcium%20imaging%20allows%20for%20the%20parallel%20measurement%20of%20large%20neuronal%0Apopulations%20in%20a%20spatially%20resolved%20and%20minimally%20invasive%20manner%2C%20and%20has%0Abecome%20a%20gold-standard%20for%20neuronal%20functionality.%20While%20deep%20generative%20models%0Ahave%20been%20successfully%20applied%20to%20study%20the%20activity%20of%20neuronal%20ensembles%2C%0Atheir%20potential%20for%20learning%20single-neuron%20representations%20from%20calcium%20imaging%0Afluorescence%20traces%20remains%20largely%20unexplored%2C%20and%20batch%20effects%20remain%20an%0Aimportant%20hurdle.%20To%20address%20this%2C%20we%20explore%20supervised%20variational%0Aautoencoder%20architectures%20that%20learn%20compact%20representations%20of%20individual%0Aneurons%20from%20fluorescent%20traces%20without%20relying%20on%20spike%20inference%20algorithms.%0AWe%20find%20that%20this%20approach%20outperforms%20state-of-the-art%20models%2C%20preserving%0Abiological%20variability%20while%20mitigating%20batch%20effects.%20Across%20simulated%20and%0Aexperimental%20datasets%2C%20this%20framework%20enables%20robust%20visualization%2C%20clustering%2C%0Aand%20interpretation%20of%20single-neuron%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14615v3&entry.124074799=Read"},
{"title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with\n  Reinforcement Learning", "author": "Ram Ramrakhya and Matthew Chang and Xavier Puig and Ruta Desai and Zsolt Kira and Roozbeh Mottaghi", "abstract": "  Embodied agents operating in household environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent is tasked\nwith a single or multi-object rearrangement task using an under-specified\ninstruction in a home environment. The agent must strategically ask minimal,\nyet relevant, clarification questions to resolve ambiguity while navigating\nunder partial observability. To address this challenge, we propose a novel\napproach that fine-tunes multi-modal large language models (MLLMs) as\nvision-language-action (VLA) policies using online reinforcement learning (RL)\nwith LLM-generated rewards. Our method eliminates the need for large-scale\nhuman demonstrations or manually engineered rewards for training such agents.\nWe benchmark against strong zero-shot baselines including GPT-4o as well as\nsupervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned\nMLLM outperforms all baselines by a significant margin (10.4-16.5%),\ngeneralizing well to novel scenes and tasks. To the best of our knowledge, this\nis the first demonstration of adapting MLLMs as VLA agents that can act and ask\nfor help using LLM-generated rewards with online RL.\n", "link": "http://arxiv.org/abs/2504.00907v4", "date": "2025-10-01", "relevancy": 1.1231, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounding%20Multimodal%20LLMs%20to%20Embodied%20Agents%20that%20Ask%20for%20Help%20with%0A%20%20Reinforcement%20Learning&body=Title%3A%20Grounding%20Multimodal%20LLMs%20to%20Embodied%20Agents%20that%20Ask%20for%20Help%20with%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ram%20Ramrakhya%20and%20Matthew%20Chang%20and%20Xavier%20Puig%20and%20Ruta%20Desai%20and%20Zsolt%20Kira%20and%20Roozbeh%20Mottaghi%0AAbstract%3A%20%20%20Embodied%20agents%20operating%20in%20household%20environments%20must%20interpret%20ambiguous%0Aand%20under-specified%20human%20instructions.%20A%20capable%20household%20robot%20should%0Arecognize%20ambiguity%20and%20ask%20relevant%20clarification%20questions%20to%20infer%20the%20user%0Aintent%20accurately%2C%20leading%20to%20more%20effective%20task%20execution.%20To%20study%20this%0Aproblem%2C%20we%20introduce%20the%20Ask-to-Act%20task%2C%20where%20an%20embodied%20agent%20is%20tasked%0Awith%20a%20single%20or%20multi-object%20rearrangement%20task%20using%20an%20under-specified%0Ainstruction%20in%20a%20home%20environment.%20The%20agent%20must%20strategically%20ask%20minimal%2C%0Ayet%20relevant%2C%20clarification%20questions%20to%20resolve%20ambiguity%20while%20navigating%0Aunder%20partial%20observability.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Aapproach%20that%20fine-tunes%20multi-modal%20large%20language%20models%20%28MLLMs%29%20as%0Avision-language-action%20%28VLA%29%20policies%20using%20online%20reinforcement%20learning%20%28RL%29%0Awith%20LLM-generated%20rewards.%20Our%20method%20eliminates%20the%20need%20for%20large-scale%0Ahuman%20demonstrations%20or%20manually%20engineered%20rewards%20for%20training%20such%20agents.%0AWe%20benchmark%20against%20strong%20zero-shot%20baselines%20including%20GPT-4o%20as%20well%20as%0Asupervised%20fine-tuned%20MLLMs%20on%20our%20task.%20Our%20results%20show%20that%20our%20RL-finetuned%0AMLLM%20outperforms%20all%20baselines%20by%20a%20significant%20margin%20%2810.4-16.5%25%29%2C%0Ageneralizing%20well%20to%20novel%20scenes%20and%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20demonstration%20of%20adapting%20MLLMs%20as%20VLA%20agents%20that%20can%20act%20and%20ask%0Afor%20help%20using%20LLM-generated%20rewards%20with%20online%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00907v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounding%2520Multimodal%2520LLMs%2520to%2520Embodied%2520Agents%2520that%2520Ask%2520for%2520Help%2520with%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRam%2520Ramrakhya%2520and%2520Matthew%2520Chang%2520and%2520Xavier%2520Puig%2520and%2520Ruta%2520Desai%2520and%2520Zsolt%2520Kira%2520and%2520Roozbeh%2520Mottaghi%26entry.1292438233%3D%2520%2520Embodied%2520agents%2520operating%2520in%2520household%2520environments%2520must%2520interpret%2520ambiguous%250Aand%2520under-specified%2520human%2520instructions.%2520A%2520capable%2520household%2520robot%2520should%250Arecognize%2520ambiguity%2520and%2520ask%2520relevant%2520clarification%2520questions%2520to%2520infer%2520the%2520user%250Aintent%2520accurately%252C%2520leading%2520to%2520more%2520effective%2520task%2520execution.%2520To%2520study%2520this%250Aproblem%252C%2520we%2520introduce%2520the%2520Ask-to-Act%2520task%252C%2520where%2520an%2520embodied%2520agent%2520is%2520tasked%250Awith%2520a%2520single%2520or%2520multi-object%2520rearrangement%2520task%2520using%2520an%2520under-specified%250Ainstruction%2520in%2520a%2520home%2520environment.%2520The%2520agent%2520must%2520strategically%2520ask%2520minimal%252C%250Ayet%2520relevant%252C%2520clarification%2520questions%2520to%2520resolve%2520ambiguity%2520while%2520navigating%250Aunder%2520partial%2520observability.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520that%2520fine-tunes%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520as%250Avision-language-action%2520%2528VLA%2529%2520policies%2520using%2520online%2520reinforcement%2520learning%2520%2528RL%2529%250Awith%2520LLM-generated%2520rewards.%2520Our%2520method%2520eliminates%2520the%2520need%2520for%2520large-scale%250Ahuman%2520demonstrations%2520or%2520manually%2520engineered%2520rewards%2520for%2520training%2520such%2520agents.%250AWe%2520benchmark%2520against%2520strong%2520zero-shot%2520baselines%2520including%2520GPT-4o%2520as%2520well%2520as%250Asupervised%2520fine-tuned%2520MLLMs%2520on%2520our%2520task.%2520Our%2520results%2520show%2520that%2520our%2520RL-finetuned%250AMLLM%2520outperforms%2520all%2520baselines%2520by%2520a%2520significant%2520margin%2520%252810.4-16.5%2525%2529%252C%250Ageneralizing%2520well%2520to%2520novel%2520scenes%2520and%2520tasks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520demonstration%2520of%2520adapting%2520MLLMs%2520as%2520VLA%2520agents%2520that%2520can%2520act%2520and%2520ask%250Afor%2520help%2520using%2520LLM-generated%2520rewards%2520with%2520online%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00907v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounding%20Multimodal%20LLMs%20to%20Embodied%20Agents%20that%20Ask%20for%20Help%20with%0A%20%20Reinforcement%20Learning&entry.906535625=Ram%20Ramrakhya%20and%20Matthew%20Chang%20and%20Xavier%20Puig%20and%20Ruta%20Desai%20and%20Zsolt%20Kira%20and%20Roozbeh%20Mottaghi&entry.1292438233=%20%20Embodied%20agents%20operating%20in%20household%20environments%20must%20interpret%20ambiguous%0Aand%20under-specified%20human%20instructions.%20A%20capable%20household%20robot%20should%0Arecognize%20ambiguity%20and%20ask%20relevant%20clarification%20questions%20to%20infer%20the%20user%0Aintent%20accurately%2C%20leading%20to%20more%20effective%20task%20execution.%20To%20study%20this%0Aproblem%2C%20we%20introduce%20the%20Ask-to-Act%20task%2C%20where%20an%20embodied%20agent%20is%20tasked%0Awith%20a%20single%20or%20multi-object%20rearrangement%20task%20using%20an%20under-specified%0Ainstruction%20in%20a%20home%20environment.%20The%20agent%20must%20strategically%20ask%20minimal%2C%0Ayet%20relevant%2C%20clarification%20questions%20to%20resolve%20ambiguity%20while%20navigating%0Aunder%20partial%20observability.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Aapproach%20that%20fine-tunes%20multi-modal%20large%20language%20models%20%28MLLMs%29%20as%0Avision-language-action%20%28VLA%29%20policies%20using%20online%20reinforcement%20learning%20%28RL%29%0Awith%20LLM-generated%20rewards.%20Our%20method%20eliminates%20the%20need%20for%20large-scale%0Ahuman%20demonstrations%20or%20manually%20engineered%20rewards%20for%20training%20such%20agents.%0AWe%20benchmark%20against%20strong%20zero-shot%20baselines%20including%20GPT-4o%20as%20well%20as%0Asupervised%20fine-tuned%20MLLMs%20on%20our%20task.%20Our%20results%20show%20that%20our%20RL-finetuned%0AMLLM%20outperforms%20all%20baselines%20by%20a%20significant%20margin%20%2810.4-16.5%25%29%2C%0Ageneralizing%20well%20to%20novel%20scenes%20and%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20demonstration%20of%20adapting%20MLLMs%20as%20VLA%20agents%20that%20can%20act%20and%20ask%0Afor%20help%20using%20LLM-generated%20rewards%20with%20online%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00907v4&entry.124074799=Read"},
{"title": "Graph Transformer Networks for Accurate Band Structure Prediction: An\n  End-to-End Approach", "author": "Weiyi Gong and Tao Sun and Hexin Bai and Jeng-Yuan Tsai and Haibin Ling and Qimin Yan", "abstract": "  Predicting electronic band structures from crystal structures is crucial for\nunderstanding structure-property correlations in materials science.\nFirst-principles approaches are accurate but computationally intensive. Recent\nyears, machine learning (ML) has been extensively applied to this field, while\nexisting ML models predominantly focus on band gap predictions or indirect band\nstructure estimation via solving predicted Hamiltonians. An end-to-end model to\npredict band structure accurately and efficiently is still lacking. Here, we\nintroduce a graph Transformer-based end-to-end approach that directly predicts\nband structures from crystal structures with high accuracy. Our method\nleverages the continuity of the k-path and treat continuous bands as a\nsequence. We demonstrate that our model not only provides accurate band\nstructure predictions but also can derive other properties (such as band gap,\nband center, and band dispersion) with high accuracy. We verify the model\nperformance on large and diverse datasets.\n", "link": "http://arxiv.org/abs/2411.16483v2", "date": "2025-10-01", "relevancy": 1.3839, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5053}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Transformer%20Networks%20for%20Accurate%20Band%20Structure%20Prediction%3A%20An%0A%20%20End-to-End%20Approach&body=Title%3A%20Graph%20Transformer%20Networks%20for%20Accurate%20Band%20Structure%20Prediction%3A%20An%0A%20%20End-to-End%20Approach%0AAuthor%3A%20Weiyi%20Gong%20and%20Tao%20Sun%20and%20Hexin%20Bai%20and%20Jeng-Yuan%20Tsai%20and%20Haibin%20Ling%20and%20Qimin%20Yan%0AAbstract%3A%20%20%20Predicting%20electronic%20band%20structures%20from%20crystal%20structures%20is%20crucial%20for%0Aunderstanding%20structure-property%20correlations%20in%20materials%20science.%0AFirst-principles%20approaches%20are%20accurate%20but%20computationally%20intensive.%20Recent%0Ayears%2C%20machine%20learning%20%28ML%29%20has%20been%20extensively%20applied%20to%20this%20field%2C%20while%0Aexisting%20ML%20models%20predominantly%20focus%20on%20band%20gap%20predictions%20or%20indirect%20band%0Astructure%20estimation%20via%20solving%20predicted%20Hamiltonians.%20An%20end-to-end%20model%20to%0Apredict%20band%20structure%20accurately%20and%20efficiently%20is%20still%20lacking.%20Here%2C%20we%0Aintroduce%20a%20graph%20Transformer-based%20end-to-end%20approach%20that%20directly%20predicts%0Aband%20structures%20from%20crystal%20structures%20with%20high%20accuracy.%20Our%20method%0Aleverages%20the%20continuity%20of%20the%20k-path%20and%20treat%20continuous%20bands%20as%20a%0Asequence.%20We%20demonstrate%20that%20our%20model%20not%20only%20provides%20accurate%20band%0Astructure%20predictions%20but%20also%20can%20derive%20other%20properties%20%28such%20as%20band%20gap%2C%0Aband%20center%2C%20and%20band%20dispersion%29%20with%20high%20accuracy.%20We%20verify%20the%20model%0Aperformance%20on%20large%20and%20diverse%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Transformer%2520Networks%2520for%2520Accurate%2520Band%2520Structure%2520Prediction%253A%2520An%250A%2520%2520End-to-End%2520Approach%26entry.906535625%3DWeiyi%2520Gong%2520and%2520Tao%2520Sun%2520and%2520Hexin%2520Bai%2520and%2520Jeng-Yuan%2520Tsai%2520and%2520Haibin%2520Ling%2520and%2520Qimin%2520Yan%26entry.1292438233%3D%2520%2520Predicting%2520electronic%2520band%2520structures%2520from%2520crystal%2520structures%2520is%2520crucial%2520for%250Aunderstanding%2520structure-property%2520correlations%2520in%2520materials%2520science.%250AFirst-principles%2520approaches%2520are%2520accurate%2520but%2520computationally%2520intensive.%2520Recent%250Ayears%252C%2520machine%2520learning%2520%2528ML%2529%2520has%2520been%2520extensively%2520applied%2520to%2520this%2520field%252C%2520while%250Aexisting%2520ML%2520models%2520predominantly%2520focus%2520on%2520band%2520gap%2520predictions%2520or%2520indirect%2520band%250Astructure%2520estimation%2520via%2520solving%2520predicted%2520Hamiltonians.%2520An%2520end-to-end%2520model%2520to%250Apredict%2520band%2520structure%2520accurately%2520and%2520efficiently%2520is%2520still%2520lacking.%2520Here%252C%2520we%250Aintroduce%2520a%2520graph%2520Transformer-based%2520end-to-end%2520approach%2520that%2520directly%2520predicts%250Aband%2520structures%2520from%2520crystal%2520structures%2520with%2520high%2520accuracy.%2520Our%2520method%250Aleverages%2520the%2520continuity%2520of%2520the%2520k-path%2520and%2520treat%2520continuous%2520bands%2520as%2520a%250Asequence.%2520We%2520demonstrate%2520that%2520our%2520model%2520not%2520only%2520provides%2520accurate%2520band%250Astructure%2520predictions%2520but%2520also%2520can%2520derive%2520other%2520properties%2520%2528such%2520as%2520band%2520gap%252C%250Aband%2520center%252C%2520and%2520band%2520dispersion%2529%2520with%2520high%2520accuracy.%2520We%2520verify%2520the%2520model%250Aperformance%2520on%2520large%2520and%2520diverse%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Transformer%20Networks%20for%20Accurate%20Band%20Structure%20Prediction%3A%20An%0A%20%20End-to-End%20Approach&entry.906535625=Weiyi%20Gong%20and%20Tao%20Sun%20and%20Hexin%20Bai%20and%20Jeng-Yuan%20Tsai%20and%20Haibin%20Ling%20and%20Qimin%20Yan&entry.1292438233=%20%20Predicting%20electronic%20band%20structures%20from%20crystal%20structures%20is%20crucial%20for%0Aunderstanding%20structure-property%20correlations%20in%20materials%20science.%0AFirst-principles%20approaches%20are%20accurate%20but%20computationally%20intensive.%20Recent%0Ayears%2C%20machine%20learning%20%28ML%29%20has%20been%20extensively%20applied%20to%20this%20field%2C%20while%0Aexisting%20ML%20models%20predominantly%20focus%20on%20band%20gap%20predictions%20or%20indirect%20band%0Astructure%20estimation%20via%20solving%20predicted%20Hamiltonians.%20An%20end-to-end%20model%20to%0Apredict%20band%20structure%20accurately%20and%20efficiently%20is%20still%20lacking.%20Here%2C%20we%0Aintroduce%20a%20graph%20Transformer-based%20end-to-end%20approach%20that%20directly%20predicts%0Aband%20structures%20from%20crystal%20structures%20with%20high%20accuracy.%20Our%20method%0Aleverages%20the%20continuity%20of%20the%20k-path%20and%20treat%20continuous%20bands%20as%20a%0Asequence.%20We%20demonstrate%20that%20our%20model%20not%20only%20provides%20accurate%20band%0Astructure%20predictions%20but%20also%20can%20derive%20other%20properties%20%28such%20as%20band%20gap%2C%0Aband%20center%2C%20and%20band%20dispersion%29%20with%20high%20accuracy.%20We%20verify%20the%20model%0Aperformance%20on%20large%20and%20diverse%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16483v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


