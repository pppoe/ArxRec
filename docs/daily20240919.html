<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240918.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianHeads: End-to-End Learning of Drivable Gaussian Head Avatars\n  from Coarse-to-fine Representations", "author": "Kartik Teotia and Hyeongwoo Kim and Pablo Garrido and Marc Habermann and Mohamed Elgharib and Christian Theobalt", "abstract": "  Real-time rendering of human head avatars is a cornerstone of many computer\ngraphics applications, such as augmented reality, video games, and films, to\nname a few. Recent approaches address this challenge with computationally\nefficient geometry primitives in a carefully calibrated multi-view setup.\nAlbeit producing photorealistic head renderings, it often fails to represent\ncomplex motion changes such as the mouth interior and strongly varying head\nposes. We propose a new method to generate highly dynamic and deformable human\nhead avatars from multi-view imagery in real-time. At the core of our method is\na hierarchical representation of head models that allows to capture the complex\ndynamics of facial expressions and head movements. First, with rich facial\nfeatures extracted from raw input frames, we learn to deform the coarse facial\ngeometry of the template mesh. We then initialize 3D Gaussians on the deformed\nsurface and refine their positions in a fine step. We train this coarse-to-fine\nfacial avatar model along with the head pose as a learnable parameter in an\nend-to-end framework. This enables not only controllable facial animation via\nvideo inputs, but also high-fidelity novel view synthesis of challenging facial\nexpressions, such as tongue deformations and fine-grained teeth structure under\nlarge motion changes. Moreover, it encourages the learned head avatar to\ngeneralize towards new facial expressions and head poses at inference time. We\ndemonstrate the performance of our method with comparisons against the related\nmethods on different datasets, spanning challenging facial expression sequences\nacross multiple identities. We also show the potential application of our\napproach by demonstrating a cross-identity facial performance transfer\napplication.\n", "link": "http://arxiv.org/abs/2409.11951v1", "date": "2024-09-18", "relevancy": 3.7901, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7986}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7986}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianHeads%3A%20End-to-End%20Learning%20of%20Drivable%20Gaussian%20Head%20Avatars%0A%20%20from%20Coarse-to-fine%20Representations&body=Title%3A%20GaussianHeads%3A%20End-to-End%20Learning%20of%20Drivable%20Gaussian%20Head%20Avatars%0A%20%20from%20Coarse-to-fine%20Representations%0AAuthor%3A%20Kartik%20Teotia%20and%20Hyeongwoo%20Kim%20and%20Pablo%20Garrido%20and%20Marc%20Habermann%20and%20Mohamed%20Elgharib%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Real-time%20rendering%20of%20human%20head%20avatars%20is%20a%20cornerstone%20of%20many%20computer%0Agraphics%20applications%2C%20such%20as%20augmented%20reality%2C%20video%20games%2C%20and%20films%2C%20to%0Aname%20a%20few.%20Recent%20approaches%20address%20this%20challenge%20with%20computationally%0Aefficient%20geometry%20primitives%20in%20a%20carefully%20calibrated%20multi-view%20setup.%0AAlbeit%20producing%20photorealistic%20head%20renderings%2C%20it%20often%20fails%20to%20represent%0Acomplex%20motion%20changes%20such%20as%20the%20mouth%20interior%20and%20strongly%20varying%20head%0Aposes.%20We%20propose%20a%20new%20method%20to%20generate%20highly%20dynamic%20and%20deformable%20human%0Ahead%20avatars%20from%20multi-view%20imagery%20in%20real-time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20hierarchical%20representation%20of%20head%20models%20that%20allows%20to%20capture%20the%20complex%0Adynamics%20of%20facial%20expressions%20and%20head%20movements.%20First%2C%20with%20rich%20facial%0Afeatures%20extracted%20from%20raw%20input%20frames%2C%20we%20learn%20to%20deform%20the%20coarse%20facial%0Ageometry%20of%20the%20template%20mesh.%20We%20then%20initialize%203D%20Gaussians%20on%20the%20deformed%0Asurface%20and%20refine%20their%20positions%20in%20a%20fine%20step.%20We%20train%20this%20coarse-to-fine%0Afacial%20avatar%20model%20along%20with%20the%20head%20pose%20as%20a%20learnable%20parameter%20in%20an%0Aend-to-end%20framework.%20This%20enables%20not%20only%20controllable%20facial%20animation%20via%0Avideo%20inputs%2C%20but%20also%20high-fidelity%20novel%20view%20synthesis%20of%20challenging%20facial%0Aexpressions%2C%20such%20as%20tongue%20deformations%20and%20fine-grained%20teeth%20structure%20under%0Alarge%20motion%20changes.%20Moreover%2C%20it%20encourages%20the%20learned%20head%20avatar%20to%0Ageneralize%20towards%20new%20facial%20expressions%20and%20head%20poses%20at%20inference%20time.%20We%0Ademonstrate%20the%20performance%20of%20our%20method%20with%20comparisons%20against%20the%20related%0Amethods%20on%20different%20datasets%2C%20spanning%20challenging%20facial%20expression%20sequences%0Aacross%20multiple%20identities.%20We%20also%20show%20the%20potential%20application%20of%20our%0Aapproach%20by%20demonstrating%20a%20cross-identity%20facial%20performance%20transfer%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianHeads%253A%2520End-to-End%2520Learning%2520of%2520Drivable%2520Gaussian%2520Head%2520Avatars%250A%2520%2520from%2520Coarse-to-fine%2520Representations%26entry.906535625%3DKartik%2520Teotia%2520and%2520Hyeongwoo%2520Kim%2520and%2520Pablo%2520Garrido%2520and%2520Marc%2520Habermann%2520and%2520Mohamed%2520Elgharib%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Real-time%2520rendering%2520of%2520human%2520head%2520avatars%2520is%2520a%2520cornerstone%2520of%2520many%2520computer%250Agraphics%2520applications%252C%2520such%2520as%2520augmented%2520reality%252C%2520video%2520games%252C%2520and%2520films%252C%2520to%250Aname%2520a%2520few.%2520Recent%2520approaches%2520address%2520this%2520challenge%2520with%2520computationally%250Aefficient%2520geometry%2520primitives%2520in%2520a%2520carefully%2520calibrated%2520multi-view%2520setup.%250AAlbeit%2520producing%2520photorealistic%2520head%2520renderings%252C%2520it%2520often%2520fails%2520to%2520represent%250Acomplex%2520motion%2520changes%2520such%2520as%2520the%2520mouth%2520interior%2520and%2520strongly%2520varying%2520head%250Aposes.%2520We%2520propose%2520a%2520new%2520method%2520to%2520generate%2520highly%2520dynamic%2520and%2520deformable%2520human%250Ahead%2520avatars%2520from%2520multi-view%2520imagery%2520in%2520real-time.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%250Aa%2520hierarchical%2520representation%2520of%2520head%2520models%2520that%2520allows%2520to%2520capture%2520the%2520complex%250Adynamics%2520of%2520facial%2520expressions%2520and%2520head%2520movements.%2520First%252C%2520with%2520rich%2520facial%250Afeatures%2520extracted%2520from%2520raw%2520input%2520frames%252C%2520we%2520learn%2520to%2520deform%2520the%2520coarse%2520facial%250Ageometry%2520of%2520the%2520template%2520mesh.%2520We%2520then%2520initialize%25203D%2520Gaussians%2520on%2520the%2520deformed%250Asurface%2520and%2520refine%2520their%2520positions%2520in%2520a%2520fine%2520step.%2520We%2520train%2520this%2520coarse-to-fine%250Afacial%2520avatar%2520model%2520along%2520with%2520the%2520head%2520pose%2520as%2520a%2520learnable%2520parameter%2520in%2520an%250Aend-to-end%2520framework.%2520This%2520enables%2520not%2520only%2520controllable%2520facial%2520animation%2520via%250Avideo%2520inputs%252C%2520but%2520also%2520high-fidelity%2520novel%2520view%2520synthesis%2520of%2520challenging%2520facial%250Aexpressions%252C%2520such%2520as%2520tongue%2520deformations%2520and%2520fine-grained%2520teeth%2520structure%2520under%250Alarge%2520motion%2520changes.%2520Moreover%252C%2520it%2520encourages%2520the%2520learned%2520head%2520avatar%2520to%250Ageneralize%2520towards%2520new%2520facial%2520expressions%2520and%2520head%2520poses%2520at%2520inference%2520time.%2520We%250Ademonstrate%2520the%2520performance%2520of%2520our%2520method%2520with%2520comparisons%2520against%2520the%2520related%250Amethods%2520on%2520different%2520datasets%252C%2520spanning%2520challenging%2520facial%2520expression%2520sequences%250Aacross%2520multiple%2520identities.%2520We%2520also%2520show%2520the%2520potential%2520application%2520of%2520our%250Aapproach%2520by%2520demonstrating%2520a%2520cross-identity%2520facial%2520performance%2520transfer%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianHeads%3A%20End-to-End%20Learning%20of%20Drivable%20Gaussian%20Head%20Avatars%0A%20%20from%20Coarse-to-fine%20Representations&entry.906535625=Kartik%20Teotia%20and%20Hyeongwoo%20Kim%20and%20Pablo%20Garrido%20and%20Marc%20Habermann%20and%20Mohamed%20Elgharib%20and%20Christian%20Theobalt&entry.1292438233=%20%20Real-time%20rendering%20of%20human%20head%20avatars%20is%20a%20cornerstone%20of%20many%20computer%0Agraphics%20applications%2C%20such%20as%20augmented%20reality%2C%20video%20games%2C%20and%20films%2C%20to%0Aname%20a%20few.%20Recent%20approaches%20address%20this%20challenge%20with%20computationally%0Aefficient%20geometry%20primitives%20in%20a%20carefully%20calibrated%20multi-view%20setup.%0AAlbeit%20producing%20photorealistic%20head%20renderings%2C%20it%20often%20fails%20to%20represent%0Acomplex%20motion%20changes%20such%20as%20the%20mouth%20interior%20and%20strongly%20varying%20head%0Aposes.%20We%20propose%20a%20new%20method%20to%20generate%20highly%20dynamic%20and%20deformable%20human%0Ahead%20avatars%20from%20multi-view%20imagery%20in%20real-time.%20At%20the%20core%20of%20our%20method%20is%0Aa%20hierarchical%20representation%20of%20head%20models%20that%20allows%20to%20capture%20the%20complex%0Adynamics%20of%20facial%20expressions%20and%20head%20movements.%20First%2C%20with%20rich%20facial%0Afeatures%20extracted%20from%20raw%20input%20frames%2C%20we%20learn%20to%20deform%20the%20coarse%20facial%0Ageometry%20of%20the%20template%20mesh.%20We%20then%20initialize%203D%20Gaussians%20on%20the%20deformed%0Asurface%20and%20refine%20their%20positions%20in%20a%20fine%20step.%20We%20train%20this%20coarse-to-fine%0Afacial%20avatar%20model%20along%20with%20the%20head%20pose%20as%20a%20learnable%20parameter%20in%20an%0Aend-to-end%20framework.%20This%20enables%20not%20only%20controllable%20facial%20animation%20via%0Avideo%20inputs%2C%20but%20also%20high-fidelity%20novel%20view%20synthesis%20of%20challenging%20facial%0Aexpressions%2C%20such%20as%20tongue%20deformations%20and%20fine-grained%20teeth%20structure%20under%0Alarge%20motion%20changes.%20Moreover%2C%20it%20encourages%20the%20learned%20head%20avatar%20to%0Ageneralize%20towards%20new%20facial%20expressions%20and%20head%20poses%20at%20inference%20time.%20We%0Ademonstrate%20the%20performance%20of%20our%20method%20with%20comparisons%20against%20the%20related%0Amethods%20on%20different%20datasets%2C%20spanning%20challenging%20facial%20expression%20sequences%0Aacross%20multiple%20identities.%20We%20also%20show%20the%20potential%20application%20of%20our%0Aapproach%20by%20demonstrating%20a%20cross-identity%20facial%20performance%20transfer%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11951v1&entry.124074799=Read"},
{"title": "LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model\n  Priors", "author": "Hanyang Yu and Xiaoxiao Long and Ping Tan", "abstract": "  We aim to address sparse-view reconstruction of a 3D scene by leveraging\npriors from large-scale vision models. While recent advancements such as 3D\nGaussian Splatting (3DGS) have demonstrated remarkable successes in 3D\nreconstruction, these methods typically necessitate hundreds of input images\nthat densely capture the underlying scene, making them time-consuming and\nimpractical for real-world applications. However, sparse-view reconstruction is\ninherently ill-posed and under-constrained, often resulting in inferior and\nincomplete outcomes. This is due to issues such as failed initialization,\noverfitting on input images, and a lack of details. To mitigate these\nchallenges, we introduce LM-Gaussian, a method capable of generating\nhigh-quality reconstructions from a limited number of images. Specifically, we\npropose a robust initialization module that leverages stereo priors to aid in\nthe recovery of camera poses and the reliable point clouds. Additionally, a\ndiffusion-based refinement is iteratively applied to incorporate image\ndiffusion priors into the Gaussian optimization process to preserve intricate\nscene details. Finally, we utilize video diffusion priors to further enhance\nthe rendered images for realistic visual effects. Overall, our approach\nsignificantly reduces the data acquisition requirements compared to previous\n3DGS methods. We validate the effectiveness of our framework through\nexperiments on various public datasets, demonstrating its potential for\nhigh-quality 360-degree scene reconstruction. Visual results are on our\nwebsite.\n", "link": "http://arxiv.org/abs/2409.03456v2", "date": "2024-09-18", "relevancy": 3.6046, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7469}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7176}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors&body=Title%3A%20LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors%0AAuthor%3A%20Hanyang%20Yu%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20We%20aim%20to%20address%20sparse-view%20reconstruction%20of%20a%203D%20scene%20by%20leveraging%0Apriors%20from%20large-scale%20vision%20models.%20While%20recent%20advancements%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20successes%20in%203D%0Areconstruction%2C%20these%20methods%20typically%20necessitate%20hundreds%20of%20input%20images%0Athat%20densely%20capture%20the%20underlying%20scene%2C%20making%20them%20time-consuming%20and%0Aimpractical%20for%20real-world%20applications.%20However%2C%20sparse-view%20reconstruction%20is%0Ainherently%20ill-posed%20and%20under-constrained%2C%20often%20resulting%20in%20inferior%20and%0Aincomplete%20outcomes.%20This%20is%20due%20to%20issues%20such%20as%20failed%20initialization%2C%0Aoverfitting%20on%20input%20images%2C%20and%20a%20lack%20of%20details.%20To%20mitigate%20these%0Achallenges%2C%20we%20introduce%20LM-Gaussian%2C%20a%20method%20capable%20of%20generating%0Ahigh-quality%20reconstructions%20from%20a%20limited%20number%20of%20images.%20Specifically%2C%20we%0Apropose%20a%20robust%20initialization%20module%20that%20leverages%20stereo%20priors%20to%20aid%20in%0Athe%20recovery%20of%20camera%20poses%20and%20the%20reliable%20point%20clouds.%20Additionally%2C%20a%0Adiffusion-based%20refinement%20is%20iteratively%20applied%20to%20incorporate%20image%0Adiffusion%20priors%20into%20the%20Gaussian%20optimization%20process%20to%20preserve%20intricate%0Ascene%20details.%20Finally%2C%20we%20utilize%20video%20diffusion%20priors%20to%20further%20enhance%0Athe%20rendered%20images%20for%20realistic%20visual%20effects.%20Overall%2C%20our%20approach%0Asignificantly%20reduces%20the%20data%20acquisition%20requirements%20compared%20to%20previous%0A3DGS%20methods.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aexperiments%20on%20various%20public%20datasets%2C%20demonstrating%20its%20potential%20for%0Ahigh-quality%20360-degree%20scene%20reconstruction.%20Visual%20results%20are%20on%20our%0Awebsite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM-Gaussian%253A%2520Boost%2520Sparse-view%25203D%2520Gaussian%2520Splatting%2520with%2520Large%2520Model%250A%2520%2520Priors%26entry.906535625%3DHanyang%2520Yu%2520and%2520Xiaoxiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520address%2520sparse-view%2520reconstruction%2520of%2520a%25203D%2520scene%2520by%2520leveraging%250Apriors%2520from%2520large-scale%2520vision%2520models.%2520While%2520recent%2520advancements%2520such%2520as%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520have%2520demonstrated%2520remarkable%2520successes%2520in%25203D%250Areconstruction%252C%2520these%2520methods%2520typically%2520necessitate%2520hundreds%2520of%2520input%2520images%250Athat%2520densely%2520capture%2520the%2520underlying%2520scene%252C%2520making%2520them%2520time-consuming%2520and%250Aimpractical%2520for%2520real-world%2520applications.%2520However%252C%2520sparse-view%2520reconstruction%2520is%250Ainherently%2520ill-posed%2520and%2520under-constrained%252C%2520often%2520resulting%2520in%2520inferior%2520and%250Aincomplete%2520outcomes.%2520This%2520is%2520due%2520to%2520issues%2520such%2520as%2520failed%2520initialization%252C%250Aoverfitting%2520on%2520input%2520images%252C%2520and%2520a%2520lack%2520of%2520details.%2520To%2520mitigate%2520these%250Achallenges%252C%2520we%2520introduce%2520LM-Gaussian%252C%2520a%2520method%2520capable%2520of%2520generating%250Ahigh-quality%2520reconstructions%2520from%2520a%2520limited%2520number%2520of%2520images.%2520Specifically%252C%2520we%250Apropose%2520a%2520robust%2520initialization%2520module%2520that%2520leverages%2520stereo%2520priors%2520to%2520aid%2520in%250Athe%2520recovery%2520of%2520camera%2520poses%2520and%2520the%2520reliable%2520point%2520clouds.%2520Additionally%252C%2520a%250Adiffusion-based%2520refinement%2520is%2520iteratively%2520applied%2520to%2520incorporate%2520image%250Adiffusion%2520priors%2520into%2520the%2520Gaussian%2520optimization%2520process%2520to%2520preserve%2520intricate%250Ascene%2520details.%2520Finally%252C%2520we%2520utilize%2520video%2520diffusion%2520priors%2520to%2520further%2520enhance%250Athe%2520rendered%2520images%2520for%2520realistic%2520visual%2520effects.%2520Overall%252C%2520our%2520approach%250Asignificantly%2520reduces%2520the%2520data%2520acquisition%2520requirements%2520compared%2520to%2520previous%250A3DGS%2520methods.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework%2520through%250Aexperiments%2520on%2520various%2520public%2520datasets%252C%2520demonstrating%2520its%2520potential%2520for%250Ahigh-quality%2520360-degree%2520scene%2520reconstruction.%2520Visual%2520results%2520are%2520on%2520our%250Awebsite.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM-Gaussian%3A%20Boost%20Sparse-view%203D%20Gaussian%20Splatting%20with%20Large%20Model%0A%20%20Priors&entry.906535625=Hanyang%20Yu%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20We%20aim%20to%20address%20sparse-view%20reconstruction%20of%20a%203D%20scene%20by%20leveraging%0Apriors%20from%20large-scale%20vision%20models.%20While%20recent%20advancements%20such%20as%203D%0AGaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20successes%20in%203D%0Areconstruction%2C%20these%20methods%20typically%20necessitate%20hundreds%20of%20input%20images%0Athat%20densely%20capture%20the%20underlying%20scene%2C%20making%20them%20time-consuming%20and%0Aimpractical%20for%20real-world%20applications.%20However%2C%20sparse-view%20reconstruction%20is%0Ainherently%20ill-posed%20and%20under-constrained%2C%20often%20resulting%20in%20inferior%20and%0Aincomplete%20outcomes.%20This%20is%20due%20to%20issues%20such%20as%20failed%20initialization%2C%0Aoverfitting%20on%20input%20images%2C%20and%20a%20lack%20of%20details.%20To%20mitigate%20these%0Achallenges%2C%20we%20introduce%20LM-Gaussian%2C%20a%20method%20capable%20of%20generating%0Ahigh-quality%20reconstructions%20from%20a%20limited%20number%20of%20images.%20Specifically%2C%20we%0Apropose%20a%20robust%20initialization%20module%20that%20leverages%20stereo%20priors%20to%20aid%20in%0Athe%20recovery%20of%20camera%20poses%20and%20the%20reliable%20point%20clouds.%20Additionally%2C%20a%0Adiffusion-based%20refinement%20is%20iteratively%20applied%20to%20incorporate%20image%0Adiffusion%20priors%20into%20the%20Gaussian%20optimization%20process%20to%20preserve%20intricate%0Ascene%20details.%20Finally%2C%20we%20utilize%20video%20diffusion%20priors%20to%20further%20enhance%0Athe%20rendered%20images%20for%20realistic%20visual%20effects.%20Overall%2C%20our%20approach%0Asignificantly%20reduces%20the%20data%20acquisition%20requirements%20compared%20to%20previous%0A3DGS%20methods.%20We%20validate%20the%20effectiveness%20of%20our%20framework%20through%0Aexperiments%20on%20various%20public%20datasets%2C%20demonstrating%20its%20potential%20for%0Ahigh-quality%20360-degree%20scene%20reconstruction.%20Visual%20results%20are%20on%20our%0Awebsite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03456v2&entry.124074799=Read"},
{"title": "Vista3D: Unravel the 3D Darkside of a Single Image", "author": "Qiuhong Shen and Xingyi Yang and Michael Bi Mi and Xinchao Wang", "abstract": "  We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.\n", "link": "http://arxiv.org/abs/2409.12193v1", "date": "2024-09-18", "relevancy": 3.4128, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7191}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7191}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vista3D%3A%20Unravel%20the%203D%20Darkside%20of%20a%20Single%20Image&body=Title%3A%20Vista3D%3A%20Unravel%20the%203D%20Darkside%20of%20a%20Single%20Image%0AAuthor%3A%20Qiuhong%20Shen%20and%20Xingyi%20Yang%20and%20Michael%20Bi%20Mi%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20We%20embark%20on%20the%20age-old%20quest%3A%20unveiling%20the%20hidden%20dimensions%20of%20objects%0Afrom%20mere%20glimpses%20of%20their%20visible%20parts.%20To%20address%20this%2C%20we%20present%20Vista3D%2C%0Aa%20framework%20that%20realizes%20swift%20and%20consistent%203D%20generation%20within%20a%20mere%205%0Aminutes.%20At%20the%20heart%20of%20Vista3D%20lies%20a%20two-phase%20approach%3A%20the%20coarse%20phase%0Aand%20the%20fine%20phase.%20In%20the%20coarse%20phase%2C%20we%20rapidly%20generate%20initial%20geometry%0Awith%20Gaussian%20Splatting%20from%20a%20single%20image.%20In%20the%20fine%20phase%2C%20we%20extract%20a%0ASigned%20Distance%20Function%20%28SDF%29%20directly%20from%20learned%20Gaussian%20Splatting%2C%0Aoptimizing%20it%20with%20a%20differentiable%20isosurface%20representation.%20Furthermore%2C%20it%0Aelevates%20the%20quality%20of%20generation%20by%20using%20a%20disentangled%20representation%20with%0Atwo%20independent%20implicit%20functions%20to%20capture%20both%20visible%20and%20obscured%20aspects%0Aof%20objects.%20Additionally%2C%20it%20harmonizes%20gradients%20from%202D%20diffusion%20prior%20with%0A3D-aware%20diffusion%20priors%20by%20angular%20diffusion%20prior%20composition.%20Through%0Aextensive%20evaluation%2C%20we%20demonstrate%20that%20Vista3D%20effectively%20sustains%20a%0Abalance%20between%20the%20consistency%20and%20diversity%20of%20the%20generated%203D%20objects.%0ADemos%20and%20code%20will%20be%20available%20at%20https%3A//github.com/florinshen/Vista3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVista3D%253A%2520Unravel%2520the%25203D%2520Darkside%2520of%2520a%2520Single%2520Image%26entry.906535625%3DQiuhong%2520Shen%2520and%2520Xingyi%2520Yang%2520and%2520Michael%2520Bi%2520Mi%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520We%2520embark%2520on%2520the%2520age-old%2520quest%253A%2520unveiling%2520the%2520hidden%2520dimensions%2520of%2520objects%250Afrom%2520mere%2520glimpses%2520of%2520their%2520visible%2520parts.%2520To%2520address%2520this%252C%2520we%2520present%2520Vista3D%252C%250Aa%2520framework%2520that%2520realizes%2520swift%2520and%2520consistent%25203D%2520generation%2520within%2520a%2520mere%25205%250Aminutes.%2520At%2520the%2520heart%2520of%2520Vista3D%2520lies%2520a%2520two-phase%2520approach%253A%2520the%2520coarse%2520phase%250Aand%2520the%2520fine%2520phase.%2520In%2520the%2520coarse%2520phase%252C%2520we%2520rapidly%2520generate%2520initial%2520geometry%250Awith%2520Gaussian%2520Splatting%2520from%2520a%2520single%2520image.%2520In%2520the%2520fine%2520phase%252C%2520we%2520extract%2520a%250ASigned%2520Distance%2520Function%2520%2528SDF%2529%2520directly%2520from%2520learned%2520Gaussian%2520Splatting%252C%250Aoptimizing%2520it%2520with%2520a%2520differentiable%2520isosurface%2520representation.%2520Furthermore%252C%2520it%250Aelevates%2520the%2520quality%2520of%2520generation%2520by%2520using%2520a%2520disentangled%2520representation%2520with%250Atwo%2520independent%2520implicit%2520functions%2520to%2520capture%2520both%2520visible%2520and%2520obscured%2520aspects%250Aof%2520objects.%2520Additionally%252C%2520it%2520harmonizes%2520gradients%2520from%25202D%2520diffusion%2520prior%2520with%250A3D-aware%2520diffusion%2520priors%2520by%2520angular%2520diffusion%2520prior%2520composition.%2520Through%250Aextensive%2520evaluation%252C%2520we%2520demonstrate%2520that%2520Vista3D%2520effectively%2520sustains%2520a%250Abalance%2520between%2520the%2520consistency%2520and%2520diversity%2520of%2520the%2520generated%25203D%2520objects.%250ADemos%2520and%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/florinshen/Vista3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vista3D%3A%20Unravel%20the%203D%20Darkside%20of%20a%20Single%20Image&entry.906535625=Qiuhong%20Shen%20and%20Xingyi%20Yang%20and%20Michael%20Bi%20Mi%20and%20Xinchao%20Wang&entry.1292438233=%20%20We%20embark%20on%20the%20age-old%20quest%3A%20unveiling%20the%20hidden%20dimensions%20of%20objects%0Afrom%20mere%20glimpses%20of%20their%20visible%20parts.%20To%20address%20this%2C%20we%20present%20Vista3D%2C%0Aa%20framework%20that%20realizes%20swift%20and%20consistent%203D%20generation%20within%20a%20mere%205%0Aminutes.%20At%20the%20heart%20of%20Vista3D%20lies%20a%20two-phase%20approach%3A%20the%20coarse%20phase%0Aand%20the%20fine%20phase.%20In%20the%20coarse%20phase%2C%20we%20rapidly%20generate%20initial%20geometry%0Awith%20Gaussian%20Splatting%20from%20a%20single%20image.%20In%20the%20fine%20phase%2C%20we%20extract%20a%0ASigned%20Distance%20Function%20%28SDF%29%20directly%20from%20learned%20Gaussian%20Splatting%2C%0Aoptimizing%20it%20with%20a%20differentiable%20isosurface%20representation.%20Furthermore%2C%20it%0Aelevates%20the%20quality%20of%20generation%20by%20using%20a%20disentangled%20representation%20with%0Atwo%20independent%20implicit%20functions%20to%20capture%20both%20visible%20and%20obscured%20aspects%0Aof%20objects.%20Additionally%2C%20it%20harmonizes%20gradients%20from%202D%20diffusion%20prior%20with%0A3D-aware%20diffusion%20priors%20by%20angular%20diffusion%20prior%20composition.%20Through%0Aextensive%20evaluation%2C%20we%20demonstrate%20that%20Vista3D%20effectively%20sustains%20a%0Abalance%20between%20the%20consistency%20and%20diversity%20of%20the%20generated%203D%20objects.%0ADemos%20and%20code%20will%20be%20available%20at%20https%3A//github.com/florinshen/Vista3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12193v1&entry.124074799=Read"},
{"title": "Efficient 3D Instance Mapping and Localization with Neural Fields", "author": "George Tang and Krishna Murthy Jatavallabhula and Antonio Torralba", "abstract": "  We tackle the problem of learning an implicit scene representation for 3D\ninstance segmentation from a sequence of posed RGB images. Towards this, we\nintroduce 3DIML, a novel framework that efficiently learns a neural label field\nwhich can render 3D instance segmentation masks from novel viewpoints. Opposed\nto prior art that optimizes a neural field in a self-supervised manner,\nrequiring complicated training procedures and loss function design, 3DIML\nleverages a two-phase process. The first phase, InstanceMap, takes as input 2D\nsegmentation masks of the image sequence generated by a frontend instance\nsegmentation model, and associates corresponding masks across images to 3D\nlabels. These almost 3D-consistent pseudolabel masks are then used in the\nsecond phase, InstanceLift, to supervise the training of a neural label field,\nwhich interpolates regions missed by InstanceMap and resolves ambiguities.\nAdditionally, we introduce InstanceLoc, which enables near realtime\nlocalization of instance masks given a trained neural label field. We evaluate\n3DIML on sequences from the Replica and ScanNet datasets and demonstrate its\neffectiveness under mild assumptions for the image sequences. We achieve a\nlarge practical speedup over existing implicit scene representation methods\nwith comparable quality, showcasing its potential to facilitate faster and more\neffective 3D scene understanding.\n", "link": "http://arxiv.org/abs/2403.19797v3", "date": "2024-09-18", "relevancy": 3.1143, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields&body=Title%3A%20Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields%0AAuthor%3A%20George%20Tang%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Antonio%20Torralba%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20learning%20an%20implicit%20scene%20representation%20for%203D%0Ainstance%20segmentation%20from%20a%20sequence%20of%20posed%20RGB%20images.%20Towards%20this%2C%20we%0Aintroduce%203DIML%2C%20a%20novel%20framework%20that%20efficiently%20learns%20a%20neural%20label%20field%0Awhich%20can%20render%203D%20instance%20segmentation%20masks%20from%20novel%20viewpoints.%20Opposed%0Ato%20prior%20art%20that%20optimizes%20a%20neural%20field%20in%20a%20self-supervised%20manner%2C%0Arequiring%20complicated%20training%20procedures%20and%20loss%20function%20design%2C%203DIML%0Aleverages%20a%20two-phase%20process.%20The%20first%20phase%2C%20InstanceMap%2C%20takes%20as%20input%202D%0Asegmentation%20masks%20of%20the%20image%20sequence%20generated%20by%20a%20frontend%20instance%0Asegmentation%20model%2C%20and%20associates%20corresponding%20masks%20across%20images%20to%203D%0Alabels.%20These%20almost%203D-consistent%20pseudolabel%20masks%20are%20then%20used%20in%20the%0Asecond%20phase%2C%20InstanceLift%2C%20to%20supervise%20the%20training%20of%20a%20neural%20label%20field%2C%0Awhich%20interpolates%20regions%20missed%20by%20InstanceMap%20and%20resolves%20ambiguities.%0AAdditionally%2C%20we%20introduce%20InstanceLoc%2C%20which%20enables%20near%20realtime%0Alocalization%20of%20instance%20masks%20given%20a%20trained%20neural%20label%20field.%20We%20evaluate%0A3DIML%20on%20sequences%20from%20the%20Replica%20and%20ScanNet%20datasets%20and%20demonstrate%20its%0Aeffectiveness%20under%20mild%20assumptions%20for%20the%20image%20sequences.%20We%20achieve%20a%0Alarge%20practical%20speedup%20over%20existing%20implicit%20scene%20representation%20methods%0Awith%20comparable%20quality%2C%20showcasing%20its%20potential%20to%20facilitate%20faster%20and%20more%0Aeffective%203D%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Instance%2520Mapping%2520and%2520Localization%2520with%2520Neural%2520Fields%26entry.906535625%3DGeorge%2520Tang%2520and%2520Krishna%2520Murthy%2520Jatavallabhula%2520and%2520Antonio%2520Torralba%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520learning%2520an%2520implicit%2520scene%2520representation%2520for%25203D%250Ainstance%2520segmentation%2520from%2520a%2520sequence%2520of%2520posed%2520RGB%2520images.%2520Towards%2520this%252C%2520we%250Aintroduce%25203DIML%252C%2520a%2520novel%2520framework%2520that%2520efficiently%2520learns%2520a%2520neural%2520label%2520field%250Awhich%2520can%2520render%25203D%2520instance%2520segmentation%2520masks%2520from%2520novel%2520viewpoints.%2520Opposed%250Ato%2520prior%2520art%2520that%2520optimizes%2520a%2520neural%2520field%2520in%2520a%2520self-supervised%2520manner%252C%250Arequiring%2520complicated%2520training%2520procedures%2520and%2520loss%2520function%2520design%252C%25203DIML%250Aleverages%2520a%2520two-phase%2520process.%2520The%2520first%2520phase%252C%2520InstanceMap%252C%2520takes%2520as%2520input%25202D%250Asegmentation%2520masks%2520of%2520the%2520image%2520sequence%2520generated%2520by%2520a%2520frontend%2520instance%250Asegmentation%2520model%252C%2520and%2520associates%2520corresponding%2520masks%2520across%2520images%2520to%25203D%250Alabels.%2520These%2520almost%25203D-consistent%2520pseudolabel%2520masks%2520are%2520then%2520used%2520in%2520the%250Asecond%2520phase%252C%2520InstanceLift%252C%2520to%2520supervise%2520the%2520training%2520of%2520a%2520neural%2520label%2520field%252C%250Awhich%2520interpolates%2520regions%2520missed%2520by%2520InstanceMap%2520and%2520resolves%2520ambiguities.%250AAdditionally%252C%2520we%2520introduce%2520InstanceLoc%252C%2520which%2520enables%2520near%2520realtime%250Alocalization%2520of%2520instance%2520masks%2520given%2520a%2520trained%2520neural%2520label%2520field.%2520We%2520evaluate%250A3DIML%2520on%2520sequences%2520from%2520the%2520Replica%2520and%2520ScanNet%2520datasets%2520and%2520demonstrate%2520its%250Aeffectiveness%2520under%2520mild%2520assumptions%2520for%2520the%2520image%2520sequences.%2520We%2520achieve%2520a%250Alarge%2520practical%2520speedup%2520over%2520existing%2520implicit%2520scene%2520representation%2520methods%250Awith%2520comparable%2520quality%252C%2520showcasing%2520its%2520potential%2520to%2520facilitate%2520faster%2520and%2520more%250Aeffective%25203D%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Instance%20Mapping%20and%20Localization%20with%20Neural%20Fields&entry.906535625=George%20Tang%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Antonio%20Torralba&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20learning%20an%20implicit%20scene%20representation%20for%203D%0Ainstance%20segmentation%20from%20a%20sequence%20of%20posed%20RGB%20images.%20Towards%20this%2C%20we%0Aintroduce%203DIML%2C%20a%20novel%20framework%20that%20efficiently%20learns%20a%20neural%20label%20field%0Awhich%20can%20render%203D%20instance%20segmentation%20masks%20from%20novel%20viewpoints.%20Opposed%0Ato%20prior%20art%20that%20optimizes%20a%20neural%20field%20in%20a%20self-supervised%20manner%2C%0Arequiring%20complicated%20training%20procedures%20and%20loss%20function%20design%2C%203DIML%0Aleverages%20a%20two-phase%20process.%20The%20first%20phase%2C%20InstanceMap%2C%20takes%20as%20input%202D%0Asegmentation%20masks%20of%20the%20image%20sequence%20generated%20by%20a%20frontend%20instance%0Asegmentation%20model%2C%20and%20associates%20corresponding%20masks%20across%20images%20to%203D%0Alabels.%20These%20almost%203D-consistent%20pseudolabel%20masks%20are%20then%20used%20in%20the%0Asecond%20phase%2C%20InstanceLift%2C%20to%20supervise%20the%20training%20of%20a%20neural%20label%20field%2C%0Awhich%20interpolates%20regions%20missed%20by%20InstanceMap%20and%20resolves%20ambiguities.%0AAdditionally%2C%20we%20introduce%20InstanceLoc%2C%20which%20enables%20near%20realtime%0Alocalization%20of%20instance%20masks%20given%20a%20trained%20neural%20label%20field.%20We%20evaluate%0A3DIML%20on%20sequences%20from%20the%20Replica%20and%20ScanNet%20datasets%20and%20demonstrate%20its%0Aeffectiveness%20under%20mild%20assumptions%20for%20the%20image%20sequences.%20We%20achieve%20a%0Alarge%20practical%20speedup%20over%20existing%20implicit%20scene%20representation%20methods%0Awith%20comparable%20quality%2C%20showcasing%20its%20potential%20to%20facilitate%20faster%20and%20more%0Aeffective%203D%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19797v3&entry.124074799=Read"},
{"title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution", "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin", "abstract": "  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\n\\url{https://github.com/QwenLM/Qwen2-VL}.\n", "link": "http://arxiv.org/abs/2409.12191v1", "date": "2024-09-18", "relevancy": 3.044, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6332}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen2-VL%3A%20Enhancing%20Vision-Language%20Model%27s%20Perception%20of%20the%20World%20at%0A%20%20Any%20Resolution&body=Title%3A%20Qwen2-VL%3A%20Enhancing%20Vision-Language%20Model%27s%20Perception%20of%20the%20World%20at%0A%20%20Any%20Resolution%0AAuthor%3A%20Peng%20Wang%20and%20Shuai%20Bai%20and%20Sinan%20Tan%20and%20Shijie%20Wang%20and%20Zhihao%20Fan%20and%20Jinze%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Jialin%20Wang%20and%20Wenbin%20Ge%20and%20Yang%20Fan%20and%20Kai%20Dang%20and%20Mengfei%20Du%20and%20Xuancheng%20Ren%20and%20Rui%20Men%20and%20Dayiheng%20Liu%20and%20Chang%20Zhou%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%0AAbstract%3A%20%20%20We%20present%20the%20Qwen2-VL%20Series%2C%20an%20advanced%20upgrade%20of%20the%20previous%20Qwen-VL%0Amodels%20that%20redefines%20the%20conventional%20predetermined-resolution%20approach%20in%0Avisual%20processing.%20Qwen2-VL%20introduces%20the%20Naive%20Dynamic%20Resolution%20mechanism%2C%0Awhich%20enables%20the%20model%20to%20dynamically%20process%20images%20of%20varying%20resolutions%0Ainto%20different%20numbers%20of%20visual%20tokens.%20This%20approach%20allows%20the%20model%20to%0Agenerate%20more%20efficient%20and%20accurate%20visual%20representations%2C%20closely%20aligning%0Awith%20human%20perceptual%20processes.%20The%20model%20also%20integrates%20Multimodal%20Rotary%0APosition%20Embedding%20%28M-RoPE%29%2C%20facilitating%20the%20effective%20fusion%20of%20positional%0Ainformation%20across%20text%2C%20images%2C%20and%20videos.%20We%20employ%20a%20unified%20paradigm%20for%0Aprocessing%20both%20images%20and%20videos%2C%20enhancing%20the%20model%27s%20visual%20perception%0Acapabilities.%20To%20explore%20the%20potential%20of%20large%20multimodal%20models%2C%20Qwen2-VL%0Ainvestigates%20the%20scaling%20laws%20for%20large%20vision-language%20models%20%28LVLMs%29.%20By%0Ascaling%20both%20the%20model%20size-with%20versions%20at%202B%2C%208B%2C%20and%2072B%20parameters-and%20the%0Aamount%20of%20training%20data%2C%20the%20Qwen2-VL%20Series%20achieves%20highly%20competitive%0Aperformance.%20Notably%2C%20the%20Qwen2-VL-72B%20model%20achieves%20results%20comparable%20to%0Aleading%20models%20such%20as%20GPT-4o%20and%20Claude3.5-Sonnet%20across%20various%20multimodal%0Abenchmarks%2C%20outperforming%20other%20generalist%20models.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/QwenLM/Qwen2-VL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen2-VL%253A%2520Enhancing%2520Vision-Language%2520Model%2527s%2520Perception%2520of%2520the%2520World%2520at%250A%2520%2520Any%2520Resolution%26entry.906535625%3DPeng%2520Wang%2520and%2520Shuai%2520Bai%2520and%2520Sinan%2520Tan%2520and%2520Shijie%2520Wang%2520and%2520Zhihao%2520Fan%2520and%2520Jinze%2520Bai%2520and%2520Keqin%2520Chen%2520and%2520Xuejing%2520Liu%2520and%2520Jialin%2520Wang%2520and%2520Wenbin%2520Ge%2520and%2520Yang%2520Fan%2520and%2520Kai%2520Dang%2520and%2520Mengfei%2520Du%2520and%2520Xuancheng%2520Ren%2520and%2520Rui%2520Men%2520and%2520Dayiheng%2520Liu%2520and%2520Chang%2520Zhou%2520and%2520Jingren%2520Zhou%2520and%2520Junyang%2520Lin%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Qwen2-VL%2520Series%252C%2520an%2520advanced%2520upgrade%2520of%2520the%2520previous%2520Qwen-VL%250Amodels%2520that%2520redefines%2520the%2520conventional%2520predetermined-resolution%2520approach%2520in%250Avisual%2520processing.%2520Qwen2-VL%2520introduces%2520the%2520Naive%2520Dynamic%2520Resolution%2520mechanism%252C%250Awhich%2520enables%2520the%2520model%2520to%2520dynamically%2520process%2520images%2520of%2520varying%2520resolutions%250Ainto%2520different%2520numbers%2520of%2520visual%2520tokens.%2520This%2520approach%2520allows%2520the%2520model%2520to%250Agenerate%2520more%2520efficient%2520and%2520accurate%2520visual%2520representations%252C%2520closely%2520aligning%250Awith%2520human%2520perceptual%2520processes.%2520The%2520model%2520also%2520integrates%2520Multimodal%2520Rotary%250APosition%2520Embedding%2520%2528M-RoPE%2529%252C%2520facilitating%2520the%2520effective%2520fusion%2520of%2520positional%250Ainformation%2520across%2520text%252C%2520images%252C%2520and%2520videos.%2520We%2520employ%2520a%2520unified%2520paradigm%2520for%250Aprocessing%2520both%2520images%2520and%2520videos%252C%2520enhancing%2520the%2520model%2527s%2520visual%2520perception%250Acapabilities.%2520To%2520explore%2520the%2520potential%2520of%2520large%2520multimodal%2520models%252C%2520Qwen2-VL%250Ainvestigates%2520the%2520scaling%2520laws%2520for%2520large%2520vision-language%2520models%2520%2528LVLMs%2529.%2520By%250Ascaling%2520both%2520the%2520model%2520size-with%2520versions%2520at%25202B%252C%25208B%252C%2520and%252072B%2520parameters-and%2520the%250Aamount%2520of%2520training%2520data%252C%2520the%2520Qwen2-VL%2520Series%2520achieves%2520highly%2520competitive%250Aperformance.%2520Notably%252C%2520the%2520Qwen2-VL-72B%2520model%2520achieves%2520results%2520comparable%2520to%250Aleading%2520models%2520such%2520as%2520GPT-4o%2520and%2520Claude3.5-Sonnet%2520across%2520various%2520multimodal%250Abenchmarks%252C%2520outperforming%2520other%2520generalist%2520models.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/QwenLM/Qwen2-VL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen2-VL%3A%20Enhancing%20Vision-Language%20Model%27s%20Perception%20of%20the%20World%20at%0A%20%20Any%20Resolution&entry.906535625=Peng%20Wang%20and%20Shuai%20Bai%20and%20Sinan%20Tan%20and%20Shijie%20Wang%20and%20Zhihao%20Fan%20and%20Jinze%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Jialin%20Wang%20and%20Wenbin%20Ge%20and%20Yang%20Fan%20and%20Kai%20Dang%20and%20Mengfei%20Du%20and%20Xuancheng%20Ren%20and%20Rui%20Men%20and%20Dayiheng%20Liu%20and%20Chang%20Zhou%20and%20Jingren%20Zhou%20and%20Junyang%20Lin&entry.1292438233=%20%20We%20present%20the%20Qwen2-VL%20Series%2C%20an%20advanced%20upgrade%20of%20the%20previous%20Qwen-VL%0Amodels%20that%20redefines%20the%20conventional%20predetermined-resolution%20approach%20in%0Avisual%20processing.%20Qwen2-VL%20introduces%20the%20Naive%20Dynamic%20Resolution%20mechanism%2C%0Awhich%20enables%20the%20model%20to%20dynamically%20process%20images%20of%20varying%20resolutions%0Ainto%20different%20numbers%20of%20visual%20tokens.%20This%20approach%20allows%20the%20model%20to%0Agenerate%20more%20efficient%20and%20accurate%20visual%20representations%2C%20closely%20aligning%0Awith%20human%20perceptual%20processes.%20The%20model%20also%20integrates%20Multimodal%20Rotary%0APosition%20Embedding%20%28M-RoPE%29%2C%20facilitating%20the%20effective%20fusion%20of%20positional%0Ainformation%20across%20text%2C%20images%2C%20and%20videos.%20We%20employ%20a%20unified%20paradigm%20for%0Aprocessing%20both%20images%20and%20videos%2C%20enhancing%20the%20model%27s%20visual%20perception%0Acapabilities.%20To%20explore%20the%20potential%20of%20large%20multimodal%20models%2C%20Qwen2-VL%0Ainvestigates%20the%20scaling%20laws%20for%20large%20vision-language%20models%20%28LVLMs%29.%20By%0Ascaling%20both%20the%20model%20size-with%20versions%20at%202B%2C%208B%2C%20and%2072B%20parameters-and%20the%0Aamount%20of%20training%20data%2C%20the%20Qwen2-VL%20Series%20achieves%20highly%20competitive%0Aperformance.%20Notably%2C%20the%20Qwen2-VL-72B%20model%20achieves%20results%20comparable%20to%0Aleading%20models%20such%20as%20GPT-4o%20and%20Claude3.5-Sonnet%20across%20various%20multimodal%0Abenchmarks%2C%20outperforming%20other%20generalist%20models.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/QwenLM/Qwen2-VL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12191v1&entry.124074799=Read"},
{"title": "An Efficient Projection-Based Next-best-view Planning Framework for\n  Reconstruction of Unknown Objects", "author": "Zhizhou Jia and Shaohui Zhang and Qun Hao", "abstract": "  Efficiently and completely capturing the three-dimensional data of an object\nis a fundamental problem in industrial and robotic applications. The task of\nnext-best-view (NBV) planning is to infer the pose of the next viewpoint based\non the current data, and gradually realize the complete three-dimensional\nreconstruction. Many existing algorithms, however, suffer a large computational\nburden due to the use of ray-casting. To address this, this paper proposes a\nprojection-based NBV planning framework. It can select the next best view at an\nextremely fast speed while ensuring the complete scanning of the object.\nSpecifically, this framework refits different types of voxel clusters into\nellipsoids based on the voxel structure.Then, the next best view is selected\nfrom the candidate views using a projection-based viewpoint quality evaluation\nfunction in conjunction with a global partitioning strategy. This process\nreplaces the ray-casting in voxel structures, significantly improving the\ncomputational efficiency. Comparative experiments with other algorithms in a\nsimulation environment show that the framework proposed in this paper can\nachieve 10 times efficiency improvement on the basis of capturing roughly the\nsame coverage. The real-world experimental results also prove the efficiency\nand feasibility of the framework.\n", "link": "http://arxiv.org/abs/2409.12096v1", "date": "2024-09-18", "relevancy": 3.0059, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6016}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6009}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Projection-Based%20Next-best-view%20Planning%20Framework%20for%0A%20%20Reconstruction%20of%20Unknown%20Objects&body=Title%3A%20An%20Efficient%20Projection-Based%20Next-best-view%20Planning%20Framework%20for%0A%20%20Reconstruction%20of%20Unknown%20Objects%0AAuthor%3A%20Zhizhou%20Jia%20and%20Shaohui%20Zhang%20and%20Qun%20Hao%0AAbstract%3A%20%20%20Efficiently%20and%20completely%20capturing%20the%20three-dimensional%20data%20of%20an%20object%0Ais%20a%20fundamental%20problem%20in%20industrial%20and%20robotic%20applications.%20The%20task%20of%0Anext-best-view%20%28NBV%29%20planning%20is%20to%20infer%20the%20pose%20of%20the%20next%20viewpoint%20based%0Aon%20the%20current%20data%2C%20and%20gradually%20realize%20the%20complete%20three-dimensional%0Areconstruction.%20Many%20existing%20algorithms%2C%20however%2C%20suffer%20a%20large%20computational%0Aburden%20due%20to%20the%20use%20of%20ray-casting.%20To%20address%20this%2C%20this%20paper%20proposes%20a%0Aprojection-based%20NBV%20planning%20framework.%20It%20can%20select%20the%20next%20best%20view%20at%20an%0Aextremely%20fast%20speed%20while%20ensuring%20the%20complete%20scanning%20of%20the%20object.%0ASpecifically%2C%20this%20framework%20refits%20different%20types%20of%20voxel%20clusters%20into%0Aellipsoids%20based%20on%20the%20voxel%20structure.Then%2C%20the%20next%20best%20view%20is%20selected%0Afrom%20the%20candidate%20views%20using%20a%20projection-based%20viewpoint%20quality%20evaluation%0Afunction%20in%20conjunction%20with%20a%20global%20partitioning%20strategy.%20This%20process%0Areplaces%20the%20ray-casting%20in%20voxel%20structures%2C%20significantly%20improving%20the%0Acomputational%20efficiency.%20Comparative%20experiments%20with%20other%20algorithms%20in%20a%0Asimulation%20environment%20show%20that%20the%20framework%20proposed%20in%20this%20paper%20can%0Aachieve%2010%20times%20efficiency%20improvement%20on%20the%20basis%20of%20capturing%20roughly%20the%0Asame%20coverage.%20The%20real-world%20experimental%20results%20also%20prove%20the%20efficiency%0Aand%20feasibility%20of%20the%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Projection-Based%2520Next-best-view%2520Planning%2520Framework%2520for%250A%2520%2520Reconstruction%2520of%2520Unknown%2520Objects%26entry.906535625%3DZhizhou%2520Jia%2520and%2520Shaohui%2520Zhang%2520and%2520Qun%2520Hao%26entry.1292438233%3D%2520%2520Efficiently%2520and%2520completely%2520capturing%2520the%2520three-dimensional%2520data%2520of%2520an%2520object%250Ais%2520a%2520fundamental%2520problem%2520in%2520industrial%2520and%2520robotic%2520applications.%2520The%2520task%2520of%250Anext-best-view%2520%2528NBV%2529%2520planning%2520is%2520to%2520infer%2520the%2520pose%2520of%2520the%2520next%2520viewpoint%2520based%250Aon%2520the%2520current%2520data%252C%2520and%2520gradually%2520realize%2520the%2520complete%2520three-dimensional%250Areconstruction.%2520Many%2520existing%2520algorithms%252C%2520however%252C%2520suffer%2520a%2520large%2520computational%250Aburden%2520due%2520to%2520the%2520use%2520of%2520ray-casting.%2520To%2520address%2520this%252C%2520this%2520paper%2520proposes%2520a%250Aprojection-based%2520NBV%2520planning%2520framework.%2520It%2520can%2520select%2520the%2520next%2520best%2520view%2520at%2520an%250Aextremely%2520fast%2520speed%2520while%2520ensuring%2520the%2520complete%2520scanning%2520of%2520the%2520object.%250ASpecifically%252C%2520this%2520framework%2520refits%2520different%2520types%2520of%2520voxel%2520clusters%2520into%250Aellipsoids%2520based%2520on%2520the%2520voxel%2520structure.Then%252C%2520the%2520next%2520best%2520view%2520is%2520selected%250Afrom%2520the%2520candidate%2520views%2520using%2520a%2520projection-based%2520viewpoint%2520quality%2520evaluation%250Afunction%2520in%2520conjunction%2520with%2520a%2520global%2520partitioning%2520strategy.%2520This%2520process%250Areplaces%2520the%2520ray-casting%2520in%2520voxel%2520structures%252C%2520significantly%2520improving%2520the%250Acomputational%2520efficiency.%2520Comparative%2520experiments%2520with%2520other%2520algorithms%2520in%2520a%250Asimulation%2520environment%2520show%2520that%2520the%2520framework%2520proposed%2520in%2520this%2520paper%2520can%250Aachieve%252010%2520times%2520efficiency%2520improvement%2520on%2520the%2520basis%2520of%2520capturing%2520roughly%2520the%250Asame%2520coverage.%2520The%2520real-world%2520experimental%2520results%2520also%2520prove%2520the%2520efficiency%250Aand%2520feasibility%2520of%2520the%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Projection-Based%20Next-best-view%20Planning%20Framework%20for%0A%20%20Reconstruction%20of%20Unknown%20Objects&entry.906535625=Zhizhou%20Jia%20and%20Shaohui%20Zhang%20and%20Qun%20Hao&entry.1292438233=%20%20Efficiently%20and%20completely%20capturing%20the%20three-dimensional%20data%20of%20an%20object%0Ais%20a%20fundamental%20problem%20in%20industrial%20and%20robotic%20applications.%20The%20task%20of%0Anext-best-view%20%28NBV%29%20planning%20is%20to%20infer%20the%20pose%20of%20the%20next%20viewpoint%20based%0Aon%20the%20current%20data%2C%20and%20gradually%20realize%20the%20complete%20three-dimensional%0Areconstruction.%20Many%20existing%20algorithms%2C%20however%2C%20suffer%20a%20large%20computational%0Aburden%20due%20to%20the%20use%20of%20ray-casting.%20To%20address%20this%2C%20this%20paper%20proposes%20a%0Aprojection-based%20NBV%20planning%20framework.%20It%20can%20select%20the%20next%20best%20view%20at%20an%0Aextremely%20fast%20speed%20while%20ensuring%20the%20complete%20scanning%20of%20the%20object.%0ASpecifically%2C%20this%20framework%20refits%20different%20types%20of%20voxel%20clusters%20into%0Aellipsoids%20based%20on%20the%20voxel%20structure.Then%2C%20the%20next%20best%20view%20is%20selected%0Afrom%20the%20candidate%20views%20using%20a%20projection-based%20viewpoint%20quality%20evaluation%0Afunction%20in%20conjunction%20with%20a%20global%20partitioning%20strategy.%20This%20process%0Areplaces%20the%20ray-casting%20in%20voxel%20structures%2C%20significantly%20improving%20the%0Acomputational%20efficiency.%20Comparative%20experiments%20with%20other%20algorithms%20in%20a%0Asimulation%20environment%20show%20that%20the%20framework%20proposed%20in%20this%20paper%20can%0Aachieve%2010%20times%20efficiency%20improvement%20on%20the%20basis%20of%20capturing%20roughly%20the%0Asame%20coverage.%20The%20real-world%20experimental%20results%20also%20prove%20the%20efficiency%0Aand%20feasibility%20of%20the%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12096v1&entry.124074799=Read"},
{"title": "Towards Global Localization using Multi-Modal Object-Instance\n  Re-Identification", "author": "Aneesh Chavan and Vaibhav Agrawal and Vineeth Bhat and Sarthak Chittawar and Siddharth Srivastava and Chetan Arora and K Madhava Krishna", "abstract": "  Re-identification (ReID) is a critical challenge in computer vision,\npredominantly studied in the context of pedestrians and vehicles. However,\nrobust object-instance ReID, which has significant implications for tasks such\nas autonomous exploration, long-term perception, and scene understanding,\nremains underexplored. In this work, we address this gap by proposing a novel\ndual-path object-instance re-identification transformer architecture that\nintegrates multimodal RGB and depth information. By leveraging depth data, we\ndemonstrate improvements in ReID across scenes that are cluttered or have\nvarying illumination conditions. Additionally, we develop a ReID-based\nlocalization framework that enables accurate camera localization and pose\nidentification across different viewpoints. We validate our methods using two\ncustom-built RGB-D datasets, as well as multiple sequences from the open-source\nTUM RGB-D datasets. Our approach demonstrates significant improvements in both\nobject instance ReID (mAP of 75.18) and localization accuracy (success rate of\n83% on TUM-RGBD), highlighting the essential role of object ReID in advancing\nrobotic perception. Our models, frameworks, and datasets have been made\npublicly available.\n", "link": "http://arxiv.org/abs/2409.12002v1", "date": "2024-09-18", "relevancy": 2.9905, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.603}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification&body=Title%3A%20Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification%0AAuthor%3A%20Aneesh%20Chavan%20and%20Vaibhav%20Agrawal%20and%20Vineeth%20Bhat%20and%20Sarthak%20Chittawar%20and%20Siddharth%20Srivastava%20and%20Chetan%20Arora%20and%20K%20Madhava%20Krishna%0AAbstract%3A%20%20%20Re-identification%20%28ReID%29%20is%20a%20critical%20challenge%20in%20computer%20vision%2C%0Apredominantly%20studied%20in%20the%20context%20of%20pedestrians%20and%20vehicles.%20However%2C%0Arobust%20object-instance%20ReID%2C%20which%20has%20significant%20implications%20for%20tasks%20such%0Aas%20autonomous%20exploration%2C%20long-term%20perception%2C%20and%20scene%20understanding%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20proposing%20a%20novel%0Adual-path%20object-instance%20re-identification%20transformer%20architecture%20that%0Aintegrates%20multimodal%20RGB%20and%20depth%20information.%20By%20leveraging%20depth%20data%2C%20we%0Ademonstrate%20improvements%20in%20ReID%20across%20scenes%20that%20are%20cluttered%20or%20have%0Avarying%20illumination%20conditions.%20Additionally%2C%20we%20develop%20a%20ReID-based%0Alocalization%20framework%20that%20enables%20accurate%20camera%20localization%20and%20pose%0Aidentification%20across%20different%20viewpoints.%20We%20validate%20our%20methods%20using%20two%0Acustom-built%20RGB-D%20datasets%2C%20as%20well%20as%20multiple%20sequences%20from%20the%20open-source%0ATUM%20RGB-D%20datasets.%20Our%20approach%20demonstrates%20significant%20improvements%20in%20both%0Aobject%20instance%20ReID%20%28mAP%20of%2075.18%29%20and%20localization%20accuracy%20%28success%20rate%20of%0A83%25%20on%20TUM-RGBD%29%2C%20highlighting%20the%20essential%20role%20of%20object%20ReID%20in%20advancing%0Arobotic%20perception.%20Our%20models%2C%20frameworks%2C%20and%20datasets%20have%20been%20made%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Global%2520Localization%2520using%2520Multi-Modal%2520Object-Instance%250A%2520%2520Re-Identification%26entry.906535625%3DAneesh%2520Chavan%2520and%2520Vaibhav%2520Agrawal%2520and%2520Vineeth%2520Bhat%2520and%2520Sarthak%2520Chittawar%2520and%2520Siddharth%2520Srivastava%2520and%2520Chetan%2520Arora%2520and%2520K%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Re-identification%2520%2528ReID%2529%2520is%2520a%2520critical%2520challenge%2520in%2520computer%2520vision%252C%250Apredominantly%2520studied%2520in%2520the%2520context%2520of%2520pedestrians%2520and%2520vehicles.%2520However%252C%250Arobust%2520object-instance%2520ReID%252C%2520which%2520has%2520significant%2520implications%2520for%2520tasks%2520such%250Aas%2520autonomous%2520exploration%252C%2520long-term%2520perception%252C%2520and%2520scene%2520understanding%252C%250Aremains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%2520by%2520proposing%2520a%2520novel%250Adual-path%2520object-instance%2520re-identification%2520transformer%2520architecture%2520that%250Aintegrates%2520multimodal%2520RGB%2520and%2520depth%2520information.%2520By%2520leveraging%2520depth%2520data%252C%2520we%250Ademonstrate%2520improvements%2520in%2520ReID%2520across%2520scenes%2520that%2520are%2520cluttered%2520or%2520have%250Avarying%2520illumination%2520conditions.%2520Additionally%252C%2520we%2520develop%2520a%2520ReID-based%250Alocalization%2520framework%2520that%2520enables%2520accurate%2520camera%2520localization%2520and%2520pose%250Aidentification%2520across%2520different%2520viewpoints.%2520We%2520validate%2520our%2520methods%2520using%2520two%250Acustom-built%2520RGB-D%2520datasets%252C%2520as%2520well%2520as%2520multiple%2520sequences%2520from%2520the%2520open-source%250ATUM%2520RGB-D%2520datasets.%2520Our%2520approach%2520demonstrates%2520significant%2520improvements%2520in%2520both%250Aobject%2520instance%2520ReID%2520%2528mAP%2520of%252075.18%2529%2520and%2520localization%2520accuracy%2520%2528success%2520rate%2520of%250A83%2525%2520on%2520TUM-RGBD%2529%252C%2520highlighting%2520the%2520essential%2520role%2520of%2520object%2520ReID%2520in%2520advancing%250Arobotic%2520perception.%2520Our%2520models%252C%2520frameworks%252C%2520and%2520datasets%2520have%2520been%2520made%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Global%20Localization%20using%20Multi-Modal%20Object-Instance%0A%20%20Re-Identification&entry.906535625=Aneesh%20Chavan%20and%20Vaibhav%20Agrawal%20and%20Vineeth%20Bhat%20and%20Sarthak%20Chittawar%20and%20Siddharth%20Srivastava%20and%20Chetan%20Arora%20and%20K%20Madhava%20Krishna&entry.1292438233=%20%20Re-identification%20%28ReID%29%20is%20a%20critical%20challenge%20in%20computer%20vision%2C%0Apredominantly%20studied%20in%20the%20context%20of%20pedestrians%20and%20vehicles.%20However%2C%0Arobust%20object-instance%20ReID%2C%20which%20has%20significant%20implications%20for%20tasks%20such%0Aas%20autonomous%20exploration%2C%20long-term%20perception%2C%20and%20scene%20understanding%2C%0Aremains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%20by%20proposing%20a%20novel%0Adual-path%20object-instance%20re-identification%20transformer%20architecture%20that%0Aintegrates%20multimodal%20RGB%20and%20depth%20information.%20By%20leveraging%20depth%20data%2C%20we%0Ademonstrate%20improvements%20in%20ReID%20across%20scenes%20that%20are%20cluttered%20or%20have%0Avarying%20illumination%20conditions.%20Additionally%2C%20we%20develop%20a%20ReID-based%0Alocalization%20framework%20that%20enables%20accurate%20camera%20localization%20and%20pose%0Aidentification%20across%20different%20viewpoints.%20We%20validate%20our%20methods%20using%20two%0Acustom-built%20RGB-D%20datasets%2C%20as%20well%20as%20multiple%20sequences%20from%20the%20open-source%0ATUM%20RGB-D%20datasets.%20Our%20approach%20demonstrates%20significant%20improvements%20in%20both%0Aobject%20instance%20ReID%20%28mAP%20of%2075.18%29%20and%20localization%20accuracy%20%28success%20rate%20of%0A83%25%20on%20TUM-RGBD%29%2C%20highlighting%20the%20essential%20role%20of%20object%20ReID%20in%20advancing%0Arobotic%20perception.%20Our%20models%2C%20frameworks%2C%20and%20datasets%20have%20been%20made%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12002v1&entry.124074799=Read"},
{"title": "OneEncoder: A Lightweight Framework for Progressive Alignment of\n  Modalities", "author": "Bilal Faye and Hanane Azzag and Mustapha Lebbah", "abstract": "  Cross-modal alignment Learning integrates information from different\nmodalities like text, image, audio and video to create unified models. This\napproach develops shared representations and learns correlations between\nmodalities, enabling applications such as visual question answering and\naudiovisual content analysis. Current techniques rely on large\nmodality-specific encoders, necessitating fine-tuning or training from scratch\non vast aligned datasets (e.g., text-image, text-audio, image-audio). This\napproach has limitations: (i) it is very expensive due to the need for training\nlarge encoders on extensive datasets, (ii) acquiring aligned large paired\ndatasets is challenging, and (iii) adding new modalities requires retraining\nthe entire framework to incorporate these modalities. To address these issues,\nwe propose OneEncoder, a lightweight framework that progressively represents\nand aligns four modalities (image, text, audio, video). Initially, we train a\nlightweight Universal Projection module (UP) to align image and text\nmodalities. Then, we freeze the pretrained UP and progressively align future\nmodalities to those already aligned. OneEncoder operates efficiently and\ncost-effectively, even in scenarios where vast aligned datasets are\nunavailable, due to its lightweight design. Trained on small paired datasets,\nit shows strong performance in tasks like classification, querying, and visual\nquestion answering, surpassing methods that rely on large datasets and\nspecialized encoders.\n", "link": "http://arxiv.org/abs/2409.11059v2", "date": "2024-09-18", "relevancy": 2.949, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.59}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities&body=Title%3A%20OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities%0AAuthor%3A%20Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah%0AAbstract%3A%20%20%20Cross-modal%20alignment%20Learning%20integrates%20information%20from%20different%0Amodalities%20like%20text%2C%20image%2C%20audio%20and%20video%20to%20create%20unified%20models.%20This%0Aapproach%20develops%20shared%20representations%20and%20learns%20correlations%20between%0Amodalities%2C%20enabling%20applications%20such%20as%20visual%20question%20answering%20and%0Aaudiovisual%20content%20analysis.%20Current%20techniques%20rely%20on%20large%0Amodality-specific%20encoders%2C%20necessitating%20fine-tuning%20or%20training%20from%20scratch%0Aon%20vast%20aligned%20datasets%20%28e.g.%2C%20text-image%2C%20text-audio%2C%20image-audio%29.%20This%0Aapproach%20has%20limitations%3A%20%28i%29%20it%20is%20very%20expensive%20due%20to%20the%20need%20for%20training%0Alarge%20encoders%20on%20extensive%20datasets%2C%20%28ii%29%20acquiring%20aligned%20large%20paired%0Adatasets%20is%20challenging%2C%20and%20%28iii%29%20adding%20new%20modalities%20requires%20retraining%0Athe%20entire%20framework%20to%20incorporate%20these%20modalities.%20To%20address%20these%20issues%2C%0Awe%20propose%20OneEncoder%2C%20a%20lightweight%20framework%20that%20progressively%20represents%0Aand%20aligns%20four%20modalities%20%28image%2C%20text%2C%20audio%2C%20video%29.%20Initially%2C%20we%20train%20a%0Alightweight%20Universal%20Projection%20module%20%28UP%29%20to%20align%20image%20and%20text%0Amodalities.%20Then%2C%20we%20freeze%20the%20pretrained%20UP%20and%20progressively%20align%20future%0Amodalities%20to%20those%20already%20aligned.%20OneEncoder%20operates%20efficiently%20and%0Acost-effectively%2C%20even%20in%20scenarios%20where%20vast%20aligned%20datasets%20are%0Aunavailable%2C%20due%20to%20its%20lightweight%20design.%20Trained%20on%20small%20paired%20datasets%2C%0Ait%20shows%20strong%20performance%20in%20tasks%20like%20classification%2C%20querying%2C%20and%20visual%0Aquestion%20answering%2C%20surpassing%20methods%20that%20rely%20on%20large%20datasets%20and%0Aspecialized%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneEncoder%253A%2520A%2520Lightweight%2520Framework%2520for%2520Progressive%2520Alignment%2520of%250A%2520%2520Modalities%26entry.906535625%3DBilal%2520Faye%2520and%2520Hanane%2520Azzag%2520and%2520Mustapha%2520Lebbah%26entry.1292438233%3D%2520%2520Cross-modal%2520alignment%2520Learning%2520integrates%2520information%2520from%2520different%250Amodalities%2520like%2520text%252C%2520image%252C%2520audio%2520and%2520video%2520to%2520create%2520unified%2520models.%2520This%250Aapproach%2520develops%2520shared%2520representations%2520and%2520learns%2520correlations%2520between%250Amodalities%252C%2520enabling%2520applications%2520such%2520as%2520visual%2520question%2520answering%2520and%250Aaudiovisual%2520content%2520analysis.%2520Current%2520techniques%2520rely%2520on%2520large%250Amodality-specific%2520encoders%252C%2520necessitating%2520fine-tuning%2520or%2520training%2520from%2520scratch%250Aon%2520vast%2520aligned%2520datasets%2520%2528e.g.%252C%2520text-image%252C%2520text-audio%252C%2520image-audio%2529.%2520This%250Aapproach%2520has%2520limitations%253A%2520%2528i%2529%2520it%2520is%2520very%2520expensive%2520due%2520to%2520the%2520need%2520for%2520training%250Alarge%2520encoders%2520on%2520extensive%2520datasets%252C%2520%2528ii%2529%2520acquiring%2520aligned%2520large%2520paired%250Adatasets%2520is%2520challenging%252C%2520and%2520%2528iii%2529%2520adding%2520new%2520modalities%2520requires%2520retraining%250Athe%2520entire%2520framework%2520to%2520incorporate%2520these%2520modalities.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520OneEncoder%252C%2520a%2520lightweight%2520framework%2520that%2520progressively%2520represents%250Aand%2520aligns%2520four%2520modalities%2520%2528image%252C%2520text%252C%2520audio%252C%2520video%2529.%2520Initially%252C%2520we%2520train%2520a%250Alightweight%2520Universal%2520Projection%2520module%2520%2528UP%2529%2520to%2520align%2520image%2520and%2520text%250Amodalities.%2520Then%252C%2520we%2520freeze%2520the%2520pretrained%2520UP%2520and%2520progressively%2520align%2520future%250Amodalities%2520to%2520those%2520already%2520aligned.%2520OneEncoder%2520operates%2520efficiently%2520and%250Acost-effectively%252C%2520even%2520in%2520scenarios%2520where%2520vast%2520aligned%2520datasets%2520are%250Aunavailable%252C%2520due%2520to%2520its%2520lightweight%2520design.%2520Trained%2520on%2520small%2520paired%2520datasets%252C%250Ait%2520shows%2520strong%2520performance%2520in%2520tasks%2520like%2520classification%252C%2520querying%252C%2520and%2520visual%250Aquestion%2520answering%252C%2520surpassing%2520methods%2520that%2520rely%2520on%2520large%2520datasets%2520and%250Aspecialized%2520encoders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneEncoder%3A%20A%20Lightweight%20Framework%20for%20Progressive%20Alignment%20of%0A%20%20Modalities&entry.906535625=Bilal%20Faye%20and%20Hanane%20Azzag%20and%20Mustapha%20Lebbah&entry.1292438233=%20%20Cross-modal%20alignment%20Learning%20integrates%20information%20from%20different%0Amodalities%20like%20text%2C%20image%2C%20audio%20and%20video%20to%20create%20unified%20models.%20This%0Aapproach%20develops%20shared%20representations%20and%20learns%20correlations%20between%0Amodalities%2C%20enabling%20applications%20such%20as%20visual%20question%20answering%20and%0Aaudiovisual%20content%20analysis.%20Current%20techniques%20rely%20on%20large%0Amodality-specific%20encoders%2C%20necessitating%20fine-tuning%20or%20training%20from%20scratch%0Aon%20vast%20aligned%20datasets%20%28e.g.%2C%20text-image%2C%20text-audio%2C%20image-audio%29.%20This%0Aapproach%20has%20limitations%3A%20%28i%29%20it%20is%20very%20expensive%20due%20to%20the%20need%20for%20training%0Alarge%20encoders%20on%20extensive%20datasets%2C%20%28ii%29%20acquiring%20aligned%20large%20paired%0Adatasets%20is%20challenging%2C%20and%20%28iii%29%20adding%20new%20modalities%20requires%20retraining%0Athe%20entire%20framework%20to%20incorporate%20these%20modalities.%20To%20address%20these%20issues%2C%0Awe%20propose%20OneEncoder%2C%20a%20lightweight%20framework%20that%20progressively%20represents%0Aand%20aligns%20four%20modalities%20%28image%2C%20text%2C%20audio%2C%20video%29.%20Initially%2C%20we%20train%20a%0Alightweight%20Universal%20Projection%20module%20%28UP%29%20to%20align%20image%20and%20text%0Amodalities.%20Then%2C%20we%20freeze%20the%20pretrained%20UP%20and%20progressively%20align%20future%0Amodalities%20to%20those%20already%20aligned.%20OneEncoder%20operates%20efficiently%20and%0Acost-effectively%2C%20even%20in%20scenarios%20where%20vast%20aligned%20datasets%20are%0Aunavailable%2C%20due%20to%20its%20lightweight%20design.%20Trained%20on%20small%20paired%20datasets%2C%0Ait%20shows%20strong%20performance%20in%20tasks%20like%20classification%2C%20querying%2C%20and%20visual%0Aquestion%20answering%2C%20surpassing%20methods%20that%20rely%20on%20large%20datasets%20and%0Aspecialized%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11059v2&entry.124074799=Read"},
{"title": "Massively Multi-Person 3D Human Motion Forecasting with Scene Context", "author": "Felix B Mueller and Julian Tanke and Juergen Gall", "abstract": "  Forecasting long-term 3D human motion is challenging: the stochasticity of\nhuman behavior makes it hard to generate realistic human motion from the input\nsequence alone. Information on the scene environment and the motion of nearby\npeople can greatly aid the generation process. We propose a scene-aware social\ntransformer model (SAST) to forecast long-term (10s) human motion motion.\nUnlike previous models, our approach can model interactions between both widely\nvarying numbers of people and objects in a scene. We combine a temporal\nconvolutional encoder-decoder architecture with a Transformer-based bottleneck\nthat allows us to efficiently combine motion and scene information. We model\nthe conditional motion distribution using denoising diffusion models. We\nbenchmark our approach on the Humans in Kitchens dataset, which contains 1 to\n16 persons and 29 to 50 objects that are visible simultaneously. Our model\noutperforms other approaches in terms of realism and diversity on different\nmetrics and in a user study. Code is available at\nhttps://github.com/felixbmuller/SAST.\n", "link": "http://arxiv.org/abs/2409.12189v1", "date": "2024-09-18", "relevancy": 2.9294, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.631}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Massively%20Multi-Person%203D%20Human%20Motion%20Forecasting%20with%20Scene%20Context&body=Title%3A%20Massively%20Multi-Person%203D%20Human%20Motion%20Forecasting%20with%20Scene%20Context%0AAuthor%3A%20Felix%20B%20Mueller%20and%20Julian%20Tanke%20and%20Juergen%20Gall%0AAbstract%3A%20%20%20Forecasting%20long-term%203D%20human%20motion%20is%20challenging%3A%20the%20stochasticity%20of%0Ahuman%20behavior%20makes%20it%20hard%20to%20generate%20realistic%20human%20motion%20from%20the%20input%0Asequence%20alone.%20Information%20on%20the%20scene%20environment%20and%20the%20motion%20of%20nearby%0Apeople%20can%20greatly%20aid%20the%20generation%20process.%20We%20propose%20a%20scene-aware%20social%0Atransformer%20model%20%28SAST%29%20to%20forecast%20long-term%20%2810s%29%20human%20motion%20motion.%0AUnlike%20previous%20models%2C%20our%20approach%20can%20model%20interactions%20between%20both%20widely%0Avarying%20numbers%20of%20people%20and%20objects%20in%20a%20scene.%20We%20combine%20a%20temporal%0Aconvolutional%20encoder-decoder%20architecture%20with%20a%20Transformer-based%20bottleneck%0Athat%20allows%20us%20to%20efficiently%20combine%20motion%20and%20scene%20information.%20We%20model%0Athe%20conditional%20motion%20distribution%20using%20denoising%20diffusion%20models.%20We%0Abenchmark%20our%20approach%20on%20the%20Humans%20in%20Kitchens%20dataset%2C%20which%20contains%201%20to%0A16%20persons%20and%2029%20to%2050%20objects%20that%20are%20visible%20simultaneously.%20Our%20model%0Aoutperforms%20other%20approaches%20in%20terms%20of%20realism%20and%20diversity%20on%20different%0Ametrics%20and%20in%20a%20user%20study.%20Code%20is%20available%20at%0Ahttps%3A//github.com/felixbmuller/SAST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMassively%2520Multi-Person%25203D%2520Human%2520Motion%2520Forecasting%2520with%2520Scene%2520Context%26entry.906535625%3DFelix%2520B%2520Mueller%2520and%2520Julian%2520Tanke%2520and%2520Juergen%2520Gall%26entry.1292438233%3D%2520%2520Forecasting%2520long-term%25203D%2520human%2520motion%2520is%2520challenging%253A%2520the%2520stochasticity%2520of%250Ahuman%2520behavior%2520makes%2520it%2520hard%2520to%2520generate%2520realistic%2520human%2520motion%2520from%2520the%2520input%250Asequence%2520alone.%2520Information%2520on%2520the%2520scene%2520environment%2520and%2520the%2520motion%2520of%2520nearby%250Apeople%2520can%2520greatly%2520aid%2520the%2520generation%2520process.%2520We%2520propose%2520a%2520scene-aware%2520social%250Atransformer%2520model%2520%2528SAST%2529%2520to%2520forecast%2520long-term%2520%252810s%2529%2520human%2520motion%2520motion.%250AUnlike%2520previous%2520models%252C%2520our%2520approach%2520can%2520model%2520interactions%2520between%2520both%2520widely%250Avarying%2520numbers%2520of%2520people%2520and%2520objects%2520in%2520a%2520scene.%2520We%2520combine%2520a%2520temporal%250Aconvolutional%2520encoder-decoder%2520architecture%2520with%2520a%2520Transformer-based%2520bottleneck%250Athat%2520allows%2520us%2520to%2520efficiently%2520combine%2520motion%2520and%2520scene%2520information.%2520We%2520model%250Athe%2520conditional%2520motion%2520distribution%2520using%2520denoising%2520diffusion%2520models.%2520We%250Abenchmark%2520our%2520approach%2520on%2520the%2520Humans%2520in%2520Kitchens%2520dataset%252C%2520which%2520contains%25201%2520to%250A16%2520persons%2520and%252029%2520to%252050%2520objects%2520that%2520are%2520visible%2520simultaneously.%2520Our%2520model%250Aoutperforms%2520other%2520approaches%2520in%2520terms%2520of%2520realism%2520and%2520diversity%2520on%2520different%250Ametrics%2520and%2520in%2520a%2520user%2520study.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/felixbmuller/SAST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Massively%20Multi-Person%203D%20Human%20Motion%20Forecasting%20with%20Scene%20Context&entry.906535625=Felix%20B%20Mueller%20and%20Julian%20Tanke%20and%20Juergen%20Gall&entry.1292438233=%20%20Forecasting%20long-term%203D%20human%20motion%20is%20challenging%3A%20the%20stochasticity%20of%0Ahuman%20behavior%20makes%20it%20hard%20to%20generate%20realistic%20human%20motion%20from%20the%20input%0Asequence%20alone.%20Information%20on%20the%20scene%20environment%20and%20the%20motion%20of%20nearby%0Apeople%20can%20greatly%20aid%20the%20generation%20process.%20We%20propose%20a%20scene-aware%20social%0Atransformer%20model%20%28SAST%29%20to%20forecast%20long-term%20%2810s%29%20human%20motion%20motion.%0AUnlike%20previous%20models%2C%20our%20approach%20can%20model%20interactions%20between%20both%20widely%0Avarying%20numbers%20of%20people%20and%20objects%20in%20a%20scene.%20We%20combine%20a%20temporal%0Aconvolutional%20encoder-decoder%20architecture%20with%20a%20Transformer-based%20bottleneck%0Athat%20allows%20us%20to%20efficiently%20combine%20motion%20and%20scene%20information.%20We%20model%0Athe%20conditional%20motion%20distribution%20using%20denoising%20diffusion%20models.%20We%0Abenchmark%20our%20approach%20on%20the%20Humans%20in%20Kitchens%20dataset%2C%20which%20contains%201%20to%0A16%20persons%20and%2029%20to%2050%20objects%20that%20are%20visible%20simultaneously.%20Our%20model%0Aoutperforms%20other%20approaches%20in%20terms%20of%20realism%20and%20diversity%20on%20different%0Ametrics%20and%20in%20a%20user%20study.%20Code%20is%20available%20at%0Ahttps%3A//github.com/felixbmuller/SAST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12189v1&entry.124074799=Read"},
{"title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Foundation Models", "author": "Amaia Cardiel and Eloi Zablocki and Oriane Sim\u00e9oni and Elias Ramzi and Matthieu Cord", "abstract": "  Vision Language Models (VLMs) have shown impressive performances on numerous\ntasks but their zero-shot capabilities can be limited compared to dedicated or\nfine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requires\n`white-box' access to the model's architecture and weights as well as expertise\nto design the fine-tuning objectives and optimize the hyper-parameters, which\nare specific to each VLM and downstream task. In this work, we propose\nLLM-wrapper, a novel approach to adapt VLMs in a `black-box' manner by\nleveraging large language models (LLMs) so as to reason on their outputs. We\ndemonstrate the effectiveness of LLM-wrapper on Referring Expression\nComprehension (REC), a challenging open-vocabulary task that requires spatial\nand semantic reasoning. Our approach significantly boosts the performance of\noff-the-shelf models, resulting in competitive results when compared with\nclassic fine-tuning.\n", "link": "http://arxiv.org/abs/2409.11919v1", "date": "2024-09-18", "relevancy": 2.8711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Foundation%20Models&body=Title%3A%20LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Foundation%20Models%0AAuthor%3A%20Amaia%20Cardiel%20and%20Eloi%20Zablocki%20and%20Oriane%20Sim%C3%A9oni%20and%20Elias%20Ramzi%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20performances%20on%20numerous%0Atasks%20but%20their%20zero-shot%20capabilities%20can%20be%20limited%20compared%20to%20dedicated%20or%0Afine-tuned%20models.%20Yet%2C%20fine-tuning%20VLMs%20comes%20with%20limitations%20as%20it%20requires%0A%60white-box%27%20access%20to%20the%20model%27s%20architecture%20and%20weights%20as%20well%20as%20expertise%0Ato%20design%20the%20fine-tuning%20objectives%20and%20optimize%20the%20hyper-parameters%2C%20which%0Aare%20specific%20to%20each%20VLM%20and%20downstream%20task.%20In%20this%20work%2C%20we%20propose%0ALLM-wrapper%2C%20a%20novel%20approach%20to%20adapt%20VLMs%20in%20a%20%60black-box%27%20manner%20by%0Aleveraging%20large%20language%20models%20%28LLMs%29%20so%20as%20to%20reason%20on%20their%20outputs.%20We%0Ademonstrate%20the%20effectiveness%20of%20LLM-wrapper%20on%20Referring%20Expression%0AComprehension%20%28REC%29%2C%20a%20challenging%20open-vocabulary%20task%20that%20requires%20spatial%0Aand%20semantic%20reasoning.%20Our%20approach%20significantly%20boosts%20the%20performance%20of%0Aoff-the-shelf%20models%2C%20resulting%20in%20competitive%20results%20when%20compared%20with%0Aclassic%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-wrapper%253A%2520Black-Box%2520Semantic-Aware%2520Adaptation%2520of%2520Vision-Language%250A%2520%2520Foundation%2520Models%26entry.906535625%3DAmaia%2520Cardiel%2520and%2520Eloi%2520Zablocki%2520and%2520Oriane%2520Sim%25C3%25A9oni%2520and%2520Elias%2520Ramzi%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520impressive%2520performances%2520on%2520numerous%250Atasks%2520but%2520their%2520zero-shot%2520capabilities%2520can%2520be%2520limited%2520compared%2520to%2520dedicated%2520or%250Afine-tuned%2520models.%2520Yet%252C%2520fine-tuning%2520VLMs%2520comes%2520with%2520limitations%2520as%2520it%2520requires%250A%2560white-box%2527%2520access%2520to%2520the%2520model%2527s%2520architecture%2520and%2520weights%2520as%2520well%2520as%2520expertise%250Ato%2520design%2520the%2520fine-tuning%2520objectives%2520and%2520optimize%2520the%2520hyper-parameters%252C%2520which%250Aare%2520specific%2520to%2520each%2520VLM%2520and%2520downstream%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%250ALLM-wrapper%252C%2520a%2520novel%2520approach%2520to%2520adapt%2520VLMs%2520in%2520a%2520%2560black-box%2527%2520manner%2520by%250Aleveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520so%2520as%2520to%2520reason%2520on%2520their%2520outputs.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520LLM-wrapper%2520on%2520Referring%2520Expression%250AComprehension%2520%2528REC%2529%252C%2520a%2520challenging%2520open-vocabulary%2520task%2520that%2520requires%2520spatial%250Aand%2520semantic%2520reasoning.%2520Our%2520approach%2520significantly%2520boosts%2520the%2520performance%2520of%250Aoff-the-shelf%2520models%252C%2520resulting%2520in%2520competitive%2520results%2520when%2520compared%2520with%250Aclassic%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-wrapper%3A%20Black-Box%20Semantic-Aware%20Adaptation%20of%20Vision-Language%0A%20%20Foundation%20Models&entry.906535625=Amaia%20Cardiel%20and%20Eloi%20Zablocki%20and%20Oriane%20Sim%C3%A9oni%20and%20Elias%20Ramzi%20and%20Matthieu%20Cord&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20performances%20on%20numerous%0Atasks%20but%20their%20zero-shot%20capabilities%20can%20be%20limited%20compared%20to%20dedicated%20or%0Afine-tuned%20models.%20Yet%2C%20fine-tuning%20VLMs%20comes%20with%20limitations%20as%20it%20requires%0A%60white-box%27%20access%20to%20the%20model%27s%20architecture%20and%20weights%20as%20well%20as%20expertise%0Ato%20design%20the%20fine-tuning%20objectives%20and%20optimize%20the%20hyper-parameters%2C%20which%0Aare%20specific%20to%20each%20VLM%20and%20downstream%20task.%20In%20this%20work%2C%20we%20propose%0ALLM-wrapper%2C%20a%20novel%20approach%20to%20adapt%20VLMs%20in%20a%20%60black-box%27%20manner%20by%0Aleveraging%20large%20language%20models%20%28LLMs%29%20so%20as%20to%20reason%20on%20their%20outputs.%20We%0Ademonstrate%20the%20effectiveness%20of%20LLM-wrapper%20on%20Referring%20Expression%0AComprehension%20%28REC%29%2C%20a%20challenging%20open-vocabulary%20task%20that%20requires%20spatial%0Aand%20semantic%20reasoning.%20Our%20approach%20significantly%20boosts%20the%20performance%20of%0Aoff-the-shelf%20models%2C%20resulting%20in%20competitive%20results%20when%20compared%20with%0Aclassic%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11919v1&entry.124074799=Read"},
{"title": "Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance", "author": "Jaehoon Joo and Taejin Jeong and Seongjae Hwang", "abstract": "  Understanding how humans process visual information is one of the crucial\nsteps for unraveling the underlying mechanism of brain activity. Recently, this\ncuriosity has motivated the fMRI-to-image reconstruction task; given the fMRI\ndata from visual stimuli, it aims to reconstruct the corresponding visual\nstimuli. Surprisingly, leveraging powerful generative models such as the Latent\nDiffusion Model (LDM) has shown promising results in reconstructing complex\nvisual stimuli such as high-resolution natural images from vision datasets.\nDespite the impressive structural fidelity of these reconstructions, they often\nlack details of small objects, ambiguous shapes, and semantic nuances.\nConsequently, the incorporation of additional semantic knowledge, beyond mere\nvisuals, becomes imperative. In light of this, we exploit how modern LDMs\neffectively incorporate multi-modal guidance (text guidance, visual guidance,\nand image layout) for structurally and semantically plausible image\ngenerations. Specifically, inspired by the two-streams hypothesis suggesting\nthat perceptual and semantic information are processed in different brain\nregions, our framework, Brain-Streams, maps fMRI signals from these brain\nregions to appropriate embeddings. That is, by extracting textual guidance from\nsemantic information regions and visual guidance from perceptual information\nregions, Brain-Streams provides accurate multi-modal guidance to LDMs. We\nvalidate the reconstruction ability of Brain-Streams both quantitatively and\nqualitatively on a real fMRI dataset comprising natural image stimuli and fMRI\ndata.\n", "link": "http://arxiv.org/abs/2409.12099v1", "date": "2024-09-18", "relevancy": 2.8689, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5833}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-Streams%3A%20fMRI-to-Image%20Reconstruction%20with%20Multi-modal%20Guidance&body=Title%3A%20Brain-Streams%3A%20fMRI-to-Image%20Reconstruction%20with%20Multi-modal%20Guidance%0AAuthor%3A%20Jaehoon%20Joo%20and%20Taejin%20Jeong%20and%20Seongjae%20Hwang%0AAbstract%3A%20%20%20Understanding%20how%20humans%20process%20visual%20information%20is%20one%20of%20the%20crucial%0Asteps%20for%20unraveling%20the%20underlying%20mechanism%20of%20brain%20activity.%20Recently%2C%20this%0Acuriosity%20has%20motivated%20the%20fMRI-to-image%20reconstruction%20task%3B%20given%20the%20fMRI%0Adata%20from%20visual%20stimuli%2C%20it%20aims%20to%20reconstruct%20the%20corresponding%20visual%0Astimuli.%20Surprisingly%2C%20leveraging%20powerful%20generative%20models%20such%20as%20the%20Latent%0ADiffusion%20Model%20%28LDM%29%20has%20shown%20promising%20results%20in%20reconstructing%20complex%0Avisual%20stimuli%20such%20as%20high-resolution%20natural%20images%20from%20vision%20datasets.%0ADespite%20the%20impressive%20structural%20fidelity%20of%20these%20reconstructions%2C%20they%20often%0Alack%20details%20of%20small%20objects%2C%20ambiguous%20shapes%2C%20and%20semantic%20nuances.%0AConsequently%2C%20the%20incorporation%20of%20additional%20semantic%20knowledge%2C%20beyond%20mere%0Avisuals%2C%20becomes%20imperative.%20In%20light%20of%20this%2C%20we%20exploit%20how%20modern%20LDMs%0Aeffectively%20incorporate%20multi-modal%20guidance%20%28text%20guidance%2C%20visual%20guidance%2C%0Aand%20image%20layout%29%20for%20structurally%20and%20semantically%20plausible%20image%0Agenerations.%20Specifically%2C%20inspired%20by%20the%20two-streams%20hypothesis%20suggesting%0Athat%20perceptual%20and%20semantic%20information%20are%20processed%20in%20different%20brain%0Aregions%2C%20our%20framework%2C%20Brain-Streams%2C%20maps%20fMRI%20signals%20from%20these%20brain%0Aregions%20to%20appropriate%20embeddings.%20That%20is%2C%20by%20extracting%20textual%20guidance%20from%0Asemantic%20information%20regions%20and%20visual%20guidance%20from%20perceptual%20information%0Aregions%2C%20Brain-Streams%20provides%20accurate%20multi-modal%20guidance%20to%20LDMs.%20We%0Avalidate%20the%20reconstruction%20ability%20of%20Brain-Streams%20both%20quantitatively%20and%0Aqualitatively%20on%20a%20real%20fMRI%20dataset%20comprising%20natural%20image%20stimuli%20and%20fMRI%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-Streams%253A%2520fMRI-to-Image%2520Reconstruction%2520with%2520Multi-modal%2520Guidance%26entry.906535625%3DJaehoon%2520Joo%2520and%2520Taejin%2520Jeong%2520and%2520Seongjae%2520Hwang%26entry.1292438233%3D%2520%2520Understanding%2520how%2520humans%2520process%2520visual%2520information%2520is%2520one%2520of%2520the%2520crucial%250Asteps%2520for%2520unraveling%2520the%2520underlying%2520mechanism%2520of%2520brain%2520activity.%2520Recently%252C%2520this%250Acuriosity%2520has%2520motivated%2520the%2520fMRI-to-image%2520reconstruction%2520task%253B%2520given%2520the%2520fMRI%250Adata%2520from%2520visual%2520stimuli%252C%2520it%2520aims%2520to%2520reconstruct%2520the%2520corresponding%2520visual%250Astimuli.%2520Surprisingly%252C%2520leveraging%2520powerful%2520generative%2520models%2520such%2520as%2520the%2520Latent%250ADiffusion%2520Model%2520%2528LDM%2529%2520has%2520shown%2520promising%2520results%2520in%2520reconstructing%2520complex%250Avisual%2520stimuli%2520such%2520as%2520high-resolution%2520natural%2520images%2520from%2520vision%2520datasets.%250ADespite%2520the%2520impressive%2520structural%2520fidelity%2520of%2520these%2520reconstructions%252C%2520they%2520often%250Alack%2520details%2520of%2520small%2520objects%252C%2520ambiguous%2520shapes%252C%2520and%2520semantic%2520nuances.%250AConsequently%252C%2520the%2520incorporation%2520of%2520additional%2520semantic%2520knowledge%252C%2520beyond%2520mere%250Avisuals%252C%2520becomes%2520imperative.%2520In%2520light%2520of%2520this%252C%2520we%2520exploit%2520how%2520modern%2520LDMs%250Aeffectively%2520incorporate%2520multi-modal%2520guidance%2520%2528text%2520guidance%252C%2520visual%2520guidance%252C%250Aand%2520image%2520layout%2529%2520for%2520structurally%2520and%2520semantically%2520plausible%2520image%250Agenerations.%2520Specifically%252C%2520inspired%2520by%2520the%2520two-streams%2520hypothesis%2520suggesting%250Athat%2520perceptual%2520and%2520semantic%2520information%2520are%2520processed%2520in%2520different%2520brain%250Aregions%252C%2520our%2520framework%252C%2520Brain-Streams%252C%2520maps%2520fMRI%2520signals%2520from%2520these%2520brain%250Aregions%2520to%2520appropriate%2520embeddings.%2520That%2520is%252C%2520by%2520extracting%2520textual%2520guidance%2520from%250Asemantic%2520information%2520regions%2520and%2520visual%2520guidance%2520from%2520perceptual%2520information%250Aregions%252C%2520Brain-Streams%2520provides%2520accurate%2520multi-modal%2520guidance%2520to%2520LDMs.%2520We%250Avalidate%2520the%2520reconstruction%2520ability%2520of%2520Brain-Streams%2520both%2520quantitatively%2520and%250Aqualitatively%2520on%2520a%2520real%2520fMRI%2520dataset%2520comprising%2520natural%2520image%2520stimuli%2520and%2520fMRI%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-Streams%3A%20fMRI-to-Image%20Reconstruction%20with%20Multi-modal%20Guidance&entry.906535625=Jaehoon%20Joo%20and%20Taejin%20Jeong%20and%20Seongjae%20Hwang&entry.1292438233=%20%20Understanding%20how%20humans%20process%20visual%20information%20is%20one%20of%20the%20crucial%0Asteps%20for%20unraveling%20the%20underlying%20mechanism%20of%20brain%20activity.%20Recently%2C%20this%0Acuriosity%20has%20motivated%20the%20fMRI-to-image%20reconstruction%20task%3B%20given%20the%20fMRI%0Adata%20from%20visual%20stimuli%2C%20it%20aims%20to%20reconstruct%20the%20corresponding%20visual%0Astimuli.%20Surprisingly%2C%20leveraging%20powerful%20generative%20models%20such%20as%20the%20Latent%0ADiffusion%20Model%20%28LDM%29%20has%20shown%20promising%20results%20in%20reconstructing%20complex%0Avisual%20stimuli%20such%20as%20high-resolution%20natural%20images%20from%20vision%20datasets.%0ADespite%20the%20impressive%20structural%20fidelity%20of%20these%20reconstructions%2C%20they%20often%0Alack%20details%20of%20small%20objects%2C%20ambiguous%20shapes%2C%20and%20semantic%20nuances.%0AConsequently%2C%20the%20incorporation%20of%20additional%20semantic%20knowledge%2C%20beyond%20mere%0Avisuals%2C%20becomes%20imperative.%20In%20light%20of%20this%2C%20we%20exploit%20how%20modern%20LDMs%0Aeffectively%20incorporate%20multi-modal%20guidance%20%28text%20guidance%2C%20visual%20guidance%2C%0Aand%20image%20layout%29%20for%20structurally%20and%20semantically%20plausible%20image%0Agenerations.%20Specifically%2C%20inspired%20by%20the%20two-streams%20hypothesis%20suggesting%0Athat%20perceptual%20and%20semantic%20information%20are%20processed%20in%20different%20brain%0Aregions%2C%20our%20framework%2C%20Brain-Streams%2C%20maps%20fMRI%20signals%20from%20these%20brain%0Aregions%20to%20appropriate%20embeddings.%20That%20is%2C%20by%20extracting%20textual%20guidance%20from%0Asemantic%20information%20regions%20and%20visual%20guidance%20from%20perceptual%20information%0Aregions%2C%20Brain-Streams%20provides%20accurate%20multi-modal%20guidance%20to%20LDMs.%20We%0Avalidate%20the%20reconstruction%20ability%20of%20Brain-Streams%20both%20quantitatively%20and%0Aqualitatively%20on%20a%20real%20fMRI%20dataset%20comprising%20natural%20image%20stimuli%20and%20fMRI%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12099v1&entry.124074799=Read"},
{"title": "JEAN: Joint Expression and Audio-guided NeRF-based Talking Face\n  Generation", "author": "Sai Tanmay Reddy Chakkera and Aggelina Chatziagapi and Dimitris Samaras", "abstract": "  We introduce a novel method for joint expression and audio-guided talking\nface generation. Recent approaches either struggle to preserve the speaker\nidentity or fail to produce faithful facial expressions. To address these\nchallenges, we propose a NeRF-based network. Since we train our network on\nmonocular videos without any ground truth, it is essential to learn\ndisentangled representations for audio and expression. We first learn audio\nfeatures in a self-supervised manner, given utterances from multiple subjects.\nBy incorporating a contrastive learning technique, we ensure that the learned\naudio features are aligned to the lip motion and disentangled from the muscle\nmotion of the rest of the face. We then devise a transformer-based architecture\nthat learns expression features, capturing long-range facial expressions and\ndisentangling them from the speech-specific mouth movements. Through\nquantitative and qualitative evaluation, we demonstrate that our method can\nsynthesize high-fidelity talking face videos, achieving state-of-the-art facial\nexpression transfer along with lip synchronization to unseen audio.\n", "link": "http://arxiv.org/abs/2409.12156v1", "date": "2024-09-18", "relevancy": 2.8304, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5723}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.563}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JEAN%3A%20Joint%20Expression%20and%20Audio-guided%20NeRF-based%20Talking%20Face%0A%20%20Generation&body=Title%3A%20JEAN%3A%20Joint%20Expression%20and%20Audio-guided%20NeRF-based%20Talking%20Face%0A%20%20Generation%0AAuthor%3A%20Sai%20Tanmay%20Reddy%20Chakkera%20and%20Aggelina%20Chatziagapi%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20joint%20expression%20and%20audio-guided%20talking%0Aface%20generation.%20Recent%20approaches%20either%20struggle%20to%20preserve%20the%20speaker%0Aidentity%20or%20fail%20to%20produce%20faithful%20facial%20expressions.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20NeRF-based%20network.%20Since%20we%20train%20our%20network%20on%0Amonocular%20videos%20without%20any%20ground%20truth%2C%20it%20is%20essential%20to%20learn%0Adisentangled%20representations%20for%20audio%20and%20expression.%20We%20first%20learn%20audio%0Afeatures%20in%20a%20self-supervised%20manner%2C%20given%20utterances%20from%20multiple%20subjects.%0ABy%20incorporating%20a%20contrastive%20learning%20technique%2C%20we%20ensure%20that%20the%20learned%0Aaudio%20features%20are%20aligned%20to%20the%20lip%20motion%20and%20disentangled%20from%20the%20muscle%0Amotion%20of%20the%20rest%20of%20the%20face.%20We%20then%20devise%20a%20transformer-based%20architecture%0Athat%20learns%20expression%20features%2C%20capturing%20long-range%20facial%20expressions%20and%0Adisentangling%20them%20from%20the%20speech-specific%20mouth%20movements.%20Through%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20that%20our%20method%20can%0Asynthesize%20high-fidelity%20talking%20face%20videos%2C%20achieving%20state-of-the-art%20facial%0Aexpression%20transfer%20along%20with%20lip%20synchronization%20to%20unseen%20audio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJEAN%253A%2520Joint%2520Expression%2520and%2520Audio-guided%2520NeRF-based%2520Talking%2520Face%250A%2520%2520Generation%26entry.906535625%3DSai%2520Tanmay%2520Reddy%2520Chakkera%2520and%2520Aggelina%2520Chatziagapi%2520and%2520Dimitris%2520Samaras%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520joint%2520expression%2520and%2520audio-guided%2520talking%250Aface%2520generation.%2520Recent%2520approaches%2520either%2520struggle%2520to%2520preserve%2520the%2520speaker%250Aidentity%2520or%2520fail%2520to%2520produce%2520faithful%2520facial%2520expressions.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520NeRF-based%2520network.%2520Since%2520we%2520train%2520our%2520network%2520on%250Amonocular%2520videos%2520without%2520any%2520ground%2520truth%252C%2520it%2520is%2520essential%2520to%2520learn%250Adisentangled%2520representations%2520for%2520audio%2520and%2520expression.%2520We%2520first%2520learn%2520audio%250Afeatures%2520in%2520a%2520self-supervised%2520manner%252C%2520given%2520utterances%2520from%2520multiple%2520subjects.%250ABy%2520incorporating%2520a%2520contrastive%2520learning%2520technique%252C%2520we%2520ensure%2520that%2520the%2520learned%250Aaudio%2520features%2520are%2520aligned%2520to%2520the%2520lip%2520motion%2520and%2520disentangled%2520from%2520the%2520muscle%250Amotion%2520of%2520the%2520rest%2520of%2520the%2520face.%2520We%2520then%2520devise%2520a%2520transformer-based%2520architecture%250Athat%2520learns%2520expression%2520features%252C%2520capturing%2520long-range%2520facial%2520expressions%2520and%250Adisentangling%2520them%2520from%2520the%2520speech-specific%2520mouth%2520movements.%2520Through%250Aquantitative%2520and%2520qualitative%2520evaluation%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520can%250Asynthesize%2520high-fidelity%2520talking%2520face%2520videos%252C%2520achieving%2520state-of-the-art%2520facial%250Aexpression%2520transfer%2520along%2520with%2520lip%2520synchronization%2520to%2520unseen%2520audio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JEAN%3A%20Joint%20Expression%20and%20Audio-guided%20NeRF-based%20Talking%20Face%0A%20%20Generation&entry.906535625=Sai%20Tanmay%20Reddy%20Chakkera%20and%20Aggelina%20Chatziagapi%20and%20Dimitris%20Samaras&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20joint%20expression%20and%20audio-guided%20talking%0Aface%20generation.%20Recent%20approaches%20either%20struggle%20to%20preserve%20the%20speaker%0Aidentity%20or%20fail%20to%20produce%20faithful%20facial%20expressions.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20NeRF-based%20network.%20Since%20we%20train%20our%20network%20on%0Amonocular%20videos%20without%20any%20ground%20truth%2C%20it%20is%20essential%20to%20learn%0Adisentangled%20representations%20for%20audio%20and%20expression.%20We%20first%20learn%20audio%0Afeatures%20in%20a%20self-supervised%20manner%2C%20given%20utterances%20from%20multiple%20subjects.%0ABy%20incorporating%20a%20contrastive%20learning%20technique%2C%20we%20ensure%20that%20the%20learned%0Aaudio%20features%20are%20aligned%20to%20the%20lip%20motion%20and%20disentangled%20from%20the%20muscle%0Amotion%20of%20the%20rest%20of%20the%20face.%20We%20then%20devise%20a%20transformer-based%20architecture%0Athat%20learns%20expression%20features%2C%20capturing%20long-range%20facial%20expressions%20and%0Adisentangling%20them%20from%20the%20speech-specific%20mouth%20movements.%20Through%0Aquantitative%20and%20qualitative%20evaluation%2C%20we%20demonstrate%20that%20our%20method%20can%0Asynthesize%20high-fidelity%20talking%20face%20videos%2C%20achieving%20state-of-the-art%20facial%0Aexpression%20transfer%20along%20with%20lip%20synchronization%20to%20unseen%20audio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12156v1&entry.124074799=Read"},
{"title": "ViewActive: Active viewpoint optimization from a single image", "author": "Jiayi Wu and Xiaomin Lin and Botao He and Cornelia Fermuller and Yiannis Aloimonos", "abstract": "  When observing objects, humans benefit from their spatial visualization and\nmental rotation ability to envision potential optimal viewpoints based on the\ncurrent observation. This capability is crucial for enabling robots to achieve\nefficient and robust scene perception during operation, as optimal viewpoints\nprovide essential and informative features for accurately representing scenes\nin 2D images, thereby enhancing downstream tasks.\n  To endow robots with this human-like active viewpoint optimization\ncapability, we propose ViewActive, a modernized machine learning approach\ndrawing inspiration from aspect graph, which provides viewpoint optimization\nguidance based solely on the current 2D image input. Specifically, we introduce\nthe 3D Viewpoint Quality Field (VQF), a compact and consistent representation\nfor viewpoint quality distribution similar to an aspect graph, composed of\nthree general-purpose viewpoint quality metrics: self-occlusion ratio,\noccupancy-aware surface normal entropy, and visual entropy. We utilize\npre-trained image encoders to extract robust visual and semantic features,\nwhich are then decoded into the 3D VQF, allowing our model to generalize\neffectively across diverse objects, including unseen categories.The lightweight\nViewActive network (72 FPS on a single GPU) significantly enhances the\nperformance of state-of-the-art object recognition pipelines and can be\nintegrated into real-time motion planning for robotic applications. Our code\nand dataset are available here: https://github.com/jiayi-wu-umd/ViewActive\n", "link": "http://arxiv.org/abs/2409.09997v2", "date": "2024-09-18", "relevancy": 2.8292, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5667}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image&body=Title%3A%20ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image%0AAuthor%3A%20Jiayi%20Wu%20and%20Xiaomin%20Lin%20and%20Botao%20He%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20When%20observing%20objects%2C%20humans%20benefit%20from%20their%20spatial%20visualization%20and%0Amental%20rotation%20ability%20to%20envision%20potential%20optimal%20viewpoints%20based%20on%20the%0Acurrent%20observation.%20This%20capability%20is%20crucial%20for%20enabling%20robots%20to%20achieve%0Aefficient%20and%20robust%20scene%20perception%20during%20operation%2C%20as%20optimal%20viewpoints%0Aprovide%20essential%20and%20informative%20features%20for%20accurately%20representing%20scenes%0Ain%202D%20images%2C%20thereby%20enhancing%20downstream%20tasks.%0A%20%20To%20endow%20robots%20with%20this%20human-like%20active%20viewpoint%20optimization%0Acapability%2C%20we%20propose%20ViewActive%2C%20a%20modernized%20machine%20learning%20approach%0Adrawing%20inspiration%20from%20aspect%20graph%2C%20which%20provides%20viewpoint%20optimization%0Aguidance%20based%20solely%20on%20the%20current%202D%20image%20input.%20Specifically%2C%20we%20introduce%0Athe%203D%20Viewpoint%20Quality%20Field%20%28VQF%29%2C%20a%20compact%20and%20consistent%20representation%0Afor%20viewpoint%20quality%20distribution%20similar%20to%20an%20aspect%20graph%2C%20composed%20of%0Athree%20general-purpose%20viewpoint%20quality%20metrics%3A%20self-occlusion%20ratio%2C%0Aoccupancy-aware%20surface%20normal%20entropy%2C%20and%20visual%20entropy.%20We%20utilize%0Apre-trained%20image%20encoders%20to%20extract%20robust%20visual%20and%20semantic%20features%2C%0Awhich%20are%20then%20decoded%20into%20the%203D%20VQF%2C%20allowing%20our%20model%20to%20generalize%0Aeffectively%20across%20diverse%20objects%2C%20including%20unseen%20categories.The%20lightweight%0AViewActive%20network%20%2872%20FPS%20on%20a%20single%20GPU%29%20significantly%20enhances%20the%0Aperformance%20of%20state-of-the-art%20object%20recognition%20pipelines%20and%20can%20be%0Aintegrated%20into%20real-time%20motion%20planning%20for%20robotic%20applications.%20Our%20code%0Aand%20dataset%20are%20available%20here%3A%20https%3A//github.com/jiayi-wu-umd/ViewActive%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewActive%253A%2520Active%2520viewpoint%2520optimization%2520from%2520a%2520single%2520image%26entry.906535625%3DJiayi%2520Wu%2520and%2520Xiaomin%2520Lin%2520and%2520Botao%2520He%2520and%2520Cornelia%2520Fermuller%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520When%2520observing%2520objects%252C%2520humans%2520benefit%2520from%2520their%2520spatial%2520visualization%2520and%250Amental%2520rotation%2520ability%2520to%2520envision%2520potential%2520optimal%2520viewpoints%2520based%2520on%2520the%250Acurrent%2520observation.%2520This%2520capability%2520is%2520crucial%2520for%2520enabling%2520robots%2520to%2520achieve%250Aefficient%2520and%2520robust%2520scene%2520perception%2520during%2520operation%252C%2520as%2520optimal%2520viewpoints%250Aprovide%2520essential%2520and%2520informative%2520features%2520for%2520accurately%2520representing%2520scenes%250Ain%25202D%2520images%252C%2520thereby%2520enhancing%2520downstream%2520tasks.%250A%2520%2520To%2520endow%2520robots%2520with%2520this%2520human-like%2520active%2520viewpoint%2520optimization%250Acapability%252C%2520we%2520propose%2520ViewActive%252C%2520a%2520modernized%2520machine%2520learning%2520approach%250Adrawing%2520inspiration%2520from%2520aspect%2520graph%252C%2520which%2520provides%2520viewpoint%2520optimization%250Aguidance%2520based%2520solely%2520on%2520the%2520current%25202D%2520image%2520input.%2520Specifically%252C%2520we%2520introduce%250Athe%25203D%2520Viewpoint%2520Quality%2520Field%2520%2528VQF%2529%252C%2520a%2520compact%2520and%2520consistent%2520representation%250Afor%2520viewpoint%2520quality%2520distribution%2520similar%2520to%2520an%2520aspect%2520graph%252C%2520composed%2520of%250Athree%2520general-purpose%2520viewpoint%2520quality%2520metrics%253A%2520self-occlusion%2520ratio%252C%250Aoccupancy-aware%2520surface%2520normal%2520entropy%252C%2520and%2520visual%2520entropy.%2520We%2520utilize%250Apre-trained%2520image%2520encoders%2520to%2520extract%2520robust%2520visual%2520and%2520semantic%2520features%252C%250Awhich%2520are%2520then%2520decoded%2520into%2520the%25203D%2520VQF%252C%2520allowing%2520our%2520model%2520to%2520generalize%250Aeffectively%2520across%2520diverse%2520objects%252C%2520including%2520unseen%2520categories.The%2520lightweight%250AViewActive%2520network%2520%252872%2520FPS%2520on%2520a%2520single%2520GPU%2529%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520state-of-the-art%2520object%2520recognition%2520pipelines%2520and%2520can%2520be%250Aintegrated%2520into%2520real-time%2520motion%2520planning%2520for%2520robotic%2520applications.%2520Our%2520code%250Aand%2520dataset%2520are%2520available%2520here%253A%2520https%253A//github.com/jiayi-wu-umd/ViewActive%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image&entry.906535625=Jiayi%20Wu%20and%20Xiaomin%20Lin%20and%20Botao%20He%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20When%20observing%20objects%2C%20humans%20benefit%20from%20their%20spatial%20visualization%20and%0Amental%20rotation%20ability%20to%20envision%20potential%20optimal%20viewpoints%20based%20on%20the%0Acurrent%20observation.%20This%20capability%20is%20crucial%20for%20enabling%20robots%20to%20achieve%0Aefficient%20and%20robust%20scene%20perception%20during%20operation%2C%20as%20optimal%20viewpoints%0Aprovide%20essential%20and%20informative%20features%20for%20accurately%20representing%20scenes%0Ain%202D%20images%2C%20thereby%20enhancing%20downstream%20tasks.%0A%20%20To%20endow%20robots%20with%20this%20human-like%20active%20viewpoint%20optimization%0Acapability%2C%20we%20propose%20ViewActive%2C%20a%20modernized%20machine%20learning%20approach%0Adrawing%20inspiration%20from%20aspect%20graph%2C%20which%20provides%20viewpoint%20optimization%0Aguidance%20based%20solely%20on%20the%20current%202D%20image%20input.%20Specifically%2C%20we%20introduce%0Athe%203D%20Viewpoint%20Quality%20Field%20%28VQF%29%2C%20a%20compact%20and%20consistent%20representation%0Afor%20viewpoint%20quality%20distribution%20similar%20to%20an%20aspect%20graph%2C%20composed%20of%0Athree%20general-purpose%20viewpoint%20quality%20metrics%3A%20self-occlusion%20ratio%2C%0Aoccupancy-aware%20surface%20normal%20entropy%2C%20and%20visual%20entropy.%20We%20utilize%0Apre-trained%20image%20encoders%20to%20extract%20robust%20visual%20and%20semantic%20features%2C%0Awhich%20are%20then%20decoded%20into%20the%203D%20VQF%2C%20allowing%20our%20model%20to%20generalize%0Aeffectively%20across%20diverse%20objects%2C%20including%20unseen%20categories.The%20lightweight%0AViewActive%20network%20%2872%20FPS%20on%20a%20single%20GPU%29%20significantly%20enhances%20the%0Aperformance%20of%20state-of-the-art%20object%20recognition%20pipelines%20and%20can%20be%0Aintegrated%20into%20real-time%20motion%20planning%20for%20robotic%20applications.%20Our%20code%0Aand%20dataset%20are%20available%20here%3A%20https%3A//github.com/jiayi-wu-umd/ViewActive%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09997v2&entry.124074799=Read"},
{"title": "SpheriGait: Enriching Spatial Representation via Spherical Projection\n  for LiDAR-based Gait Recognition", "author": "Yanxi Wang and Zhigang Chang and Chen Wu and Zihao Cheng and Hongmin Gao", "abstract": "  Gait recognition is a rapidly progressing technique for the remote\nidentification of individuals. Prior research predominantly employing 2D\nsensors to gather gait data has achieved notable advancements; nonetheless,\nthey have unavoidably neglected the influence of 3D dynamic characteristics on\nrecognition. Gait recognition utilizing LiDAR 3D point clouds not only directly\ncaptures 3D spatial features but also diminishes the impact of lighting\nconditions while ensuring privacy protection.The essence of the problem lies in\nhow to effectively extract discriminative 3D dynamic representation from point\nclouds.In this paper, we proposes a method named SpheriGait for extracting and\nenhancing dynamic features from point clouds for Lidar-based gait recognition.\nSpecifically, it substitutes the conventional point cloud plane projection\nmethod with spherical projection to augment the perception of dynamic\nfeature.Additionally, a network block named DAM-L is proposed to extract gait\ncues from the projected point cloud data. We conducted extensive experiments\nand the results demonstrated the SpheriGait achieved state-of-the-art\nperformance on the SUSTech1K dataset, and verified that the spherical\nprojection method can serve as a universal data preprocessing technique to\nenhance the performance of other LiDAR-based gait recognition methods,\nexhibiting exceptional flexibility and practicality.\n", "link": "http://arxiv.org/abs/2409.11869v1", "date": "2024-09-18", "relevancy": 2.811, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5755}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5635}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpheriGait%3A%20Enriching%20Spatial%20Representation%20via%20Spherical%20Projection%0A%20%20for%20LiDAR-based%20Gait%20Recognition&body=Title%3A%20SpheriGait%3A%20Enriching%20Spatial%20Representation%20via%20Spherical%20Projection%0A%20%20for%20LiDAR-based%20Gait%20Recognition%0AAuthor%3A%20Yanxi%20Wang%20and%20Zhigang%20Chang%20and%20Chen%20Wu%20and%20Zihao%20Cheng%20and%20Hongmin%20Gao%0AAbstract%3A%20%20%20Gait%20recognition%20is%20a%20rapidly%20progressing%20technique%20for%20the%20remote%0Aidentification%20of%20individuals.%20Prior%20research%20predominantly%20employing%202D%0Asensors%20to%20gather%20gait%20data%20has%20achieved%20notable%20advancements%3B%20nonetheless%2C%0Athey%20have%20unavoidably%20neglected%20the%20influence%20of%203D%20dynamic%20characteristics%20on%0Arecognition.%20Gait%20recognition%20utilizing%20LiDAR%203D%20point%20clouds%20not%20only%20directly%0Acaptures%203D%20spatial%20features%20but%20also%20diminishes%20the%20impact%20of%20lighting%0Aconditions%20while%20ensuring%20privacy%20protection.The%20essence%20of%20the%20problem%20lies%20in%0Ahow%20to%20effectively%20extract%20discriminative%203D%20dynamic%20representation%20from%20point%0Aclouds.In%20this%20paper%2C%20we%20proposes%20a%20method%20named%20SpheriGait%20for%20extracting%20and%0Aenhancing%20dynamic%20features%20from%20point%20clouds%20for%20Lidar-based%20gait%20recognition.%0ASpecifically%2C%20it%20substitutes%20the%20conventional%20point%20cloud%20plane%20projection%0Amethod%20with%20spherical%20projection%20to%20augment%20the%20perception%20of%20dynamic%0Afeature.Additionally%2C%20a%20network%20block%20named%20DAM-L%20is%20proposed%20to%20extract%20gait%0Acues%20from%20the%20projected%20point%20cloud%20data.%20We%20conducted%20extensive%20experiments%0Aand%20the%20results%20demonstrated%20the%20SpheriGait%20achieved%20state-of-the-art%0Aperformance%20on%20the%20SUSTech1K%20dataset%2C%20and%20verified%20that%20the%20spherical%0Aprojection%20method%20can%20serve%20as%20a%20universal%20data%20preprocessing%20technique%20to%0Aenhance%20the%20performance%20of%20other%20LiDAR-based%20gait%20recognition%20methods%2C%0Aexhibiting%20exceptional%20flexibility%20and%20practicality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpheriGait%253A%2520Enriching%2520Spatial%2520Representation%2520via%2520Spherical%2520Projection%250A%2520%2520for%2520LiDAR-based%2520Gait%2520Recognition%26entry.906535625%3DYanxi%2520Wang%2520and%2520Zhigang%2520Chang%2520and%2520Chen%2520Wu%2520and%2520Zihao%2520Cheng%2520and%2520Hongmin%2520Gao%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520is%2520a%2520rapidly%2520progressing%2520technique%2520for%2520the%2520remote%250Aidentification%2520of%2520individuals.%2520Prior%2520research%2520predominantly%2520employing%25202D%250Asensors%2520to%2520gather%2520gait%2520data%2520has%2520achieved%2520notable%2520advancements%253B%2520nonetheless%252C%250Athey%2520have%2520unavoidably%2520neglected%2520the%2520influence%2520of%25203D%2520dynamic%2520characteristics%2520on%250Arecognition.%2520Gait%2520recognition%2520utilizing%2520LiDAR%25203D%2520point%2520clouds%2520not%2520only%2520directly%250Acaptures%25203D%2520spatial%2520features%2520but%2520also%2520diminishes%2520the%2520impact%2520of%2520lighting%250Aconditions%2520while%2520ensuring%2520privacy%2520protection.The%2520essence%2520of%2520the%2520problem%2520lies%2520in%250Ahow%2520to%2520effectively%2520extract%2520discriminative%25203D%2520dynamic%2520representation%2520from%2520point%250Aclouds.In%2520this%2520paper%252C%2520we%2520proposes%2520a%2520method%2520named%2520SpheriGait%2520for%2520extracting%2520and%250Aenhancing%2520dynamic%2520features%2520from%2520point%2520clouds%2520for%2520Lidar-based%2520gait%2520recognition.%250ASpecifically%252C%2520it%2520substitutes%2520the%2520conventional%2520point%2520cloud%2520plane%2520projection%250Amethod%2520with%2520spherical%2520projection%2520to%2520augment%2520the%2520perception%2520of%2520dynamic%250Afeature.Additionally%252C%2520a%2520network%2520block%2520named%2520DAM-L%2520is%2520proposed%2520to%2520extract%2520gait%250Acues%2520from%2520the%2520projected%2520point%2520cloud%2520data.%2520We%2520conducted%2520extensive%2520experiments%250Aand%2520the%2520results%2520demonstrated%2520the%2520SpheriGait%2520achieved%2520state-of-the-art%250Aperformance%2520on%2520the%2520SUSTech1K%2520dataset%252C%2520and%2520verified%2520that%2520the%2520spherical%250Aprojection%2520method%2520can%2520serve%2520as%2520a%2520universal%2520data%2520preprocessing%2520technique%2520to%250Aenhance%2520the%2520performance%2520of%2520other%2520LiDAR-based%2520gait%2520recognition%2520methods%252C%250Aexhibiting%2520exceptional%2520flexibility%2520and%2520practicality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpheriGait%3A%20Enriching%20Spatial%20Representation%20via%20Spherical%20Projection%0A%20%20for%20LiDAR-based%20Gait%20Recognition&entry.906535625=Yanxi%20Wang%20and%20Zhigang%20Chang%20and%20Chen%20Wu%20and%20Zihao%20Cheng%20and%20Hongmin%20Gao&entry.1292438233=%20%20Gait%20recognition%20is%20a%20rapidly%20progressing%20technique%20for%20the%20remote%0Aidentification%20of%20individuals.%20Prior%20research%20predominantly%20employing%202D%0Asensors%20to%20gather%20gait%20data%20has%20achieved%20notable%20advancements%3B%20nonetheless%2C%0Athey%20have%20unavoidably%20neglected%20the%20influence%20of%203D%20dynamic%20characteristics%20on%0Arecognition.%20Gait%20recognition%20utilizing%20LiDAR%203D%20point%20clouds%20not%20only%20directly%0Acaptures%203D%20spatial%20features%20but%20also%20diminishes%20the%20impact%20of%20lighting%0Aconditions%20while%20ensuring%20privacy%20protection.The%20essence%20of%20the%20problem%20lies%20in%0Ahow%20to%20effectively%20extract%20discriminative%203D%20dynamic%20representation%20from%20point%0Aclouds.In%20this%20paper%2C%20we%20proposes%20a%20method%20named%20SpheriGait%20for%20extracting%20and%0Aenhancing%20dynamic%20features%20from%20point%20clouds%20for%20Lidar-based%20gait%20recognition.%0ASpecifically%2C%20it%20substitutes%20the%20conventional%20point%20cloud%20plane%20projection%0Amethod%20with%20spherical%20projection%20to%20augment%20the%20perception%20of%20dynamic%0Afeature.Additionally%2C%20a%20network%20block%20named%20DAM-L%20is%20proposed%20to%20extract%20gait%0Acues%20from%20the%20projected%20point%20cloud%20data.%20We%20conducted%20extensive%20experiments%0Aand%20the%20results%20demonstrated%20the%20SpheriGait%20achieved%20state-of-the-art%0Aperformance%20on%20the%20SUSTech1K%20dataset%2C%20and%20verified%20that%20the%20spherical%0Aprojection%20method%20can%20serve%20as%20a%20universal%20data%20preprocessing%20technique%20to%0Aenhance%20the%20performance%20of%20other%20LiDAR-based%20gait%20recognition%20methods%2C%0Aexhibiting%20exceptional%20flexibility%20and%20practicality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11869v1&entry.124074799=Read"},
{"title": "Mitigating Urban-Rural Disparities in Contrastive Representation\n  Learning with Satellite Imagery", "author": "Miao Zhang and Rumi Chunara", "abstract": "  Satellite imagery is being leveraged for many societally critical tasks\nacross climate, economics, and public health. Yet, because of heterogeneity in\nlandscapes (e.g. how a road looks in different places), models can show\ndisparate performance across geographic areas. Given the important potential of\ndisparities in algorithmic systems used in societal contexts, here we consider\nthe risk of urban-rural disparities in identification of land-cover features.\nThis is via semantic segmentation (a common computer vision task in which image\nregions are labelled according to what is being shown) which uses pre-trained\nimage representations generated via contrastive self-supervised learning. We\npropose fair dense representation with contrastive learning (FairDCL) as a\nmethod for de-biasing the multi-level latent space of convolution neural\nnetwork models. The method improves feature identification by removing spurious\nmodel representations which are disparately distributed across urban and rural\nareas, and is achieved in an unsupervised way by contrastive pre-training. The\nobtained image representation mitigates downstream urban-rural prediction\ndisparities and outperforms state-of-the-art baselines on real-world satellite\nimages. Embedding space evaluation and ablation studies further demonstrate\nFairDCL's robustness. As generalizability and robustness in geographic imagery\nis a nascent topic, our work motivates researchers to consider metrics beyond\naverage accuracy in such applications.\n", "link": "http://arxiv.org/abs/2211.08672v3", "date": "2024-09-18", "relevancy": 2.7733, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Urban-Rural%20Disparities%20in%20Contrastive%20Representation%0A%20%20Learning%20with%20Satellite%20Imagery&body=Title%3A%20Mitigating%20Urban-Rural%20Disparities%20in%20Contrastive%20Representation%0A%20%20Learning%20with%20Satellite%20Imagery%0AAuthor%3A%20Miao%20Zhang%20and%20Rumi%20Chunara%0AAbstract%3A%20%20%20Satellite%20imagery%20is%20being%20leveraged%20for%20many%20societally%20critical%20tasks%0Aacross%20climate%2C%20economics%2C%20and%20public%20health.%20Yet%2C%20because%20of%20heterogeneity%20in%0Alandscapes%20%28e.g.%20how%20a%20road%20looks%20in%20different%20places%29%2C%20models%20can%20show%0Adisparate%20performance%20across%20geographic%20areas.%20Given%20the%20important%20potential%20of%0Adisparities%20in%20algorithmic%20systems%20used%20in%20societal%20contexts%2C%20here%20we%20consider%0Athe%20risk%20of%20urban-rural%20disparities%20in%20identification%20of%20land-cover%20features.%0AThis%20is%20via%20semantic%20segmentation%20%28a%20common%20computer%20vision%20task%20in%20which%20image%0Aregions%20are%20labelled%20according%20to%20what%20is%20being%20shown%29%20which%20uses%20pre-trained%0Aimage%20representations%20generated%20via%20contrastive%20self-supervised%20learning.%20We%0Apropose%20fair%20dense%20representation%20with%20contrastive%20learning%20%28FairDCL%29%20as%20a%0Amethod%20for%20de-biasing%20the%20multi-level%20latent%20space%20of%20convolution%20neural%0Anetwork%20models.%20The%20method%20improves%20feature%20identification%20by%20removing%20spurious%0Amodel%20representations%20which%20are%20disparately%20distributed%20across%20urban%20and%20rural%0Aareas%2C%20and%20is%20achieved%20in%20an%20unsupervised%20way%20by%20contrastive%20pre-training.%20The%0Aobtained%20image%20representation%20mitigates%20downstream%20urban-rural%20prediction%0Adisparities%20and%20outperforms%20state-of-the-art%20baselines%20on%20real-world%20satellite%0Aimages.%20Embedding%20space%20evaluation%20and%20ablation%20studies%20further%20demonstrate%0AFairDCL%27s%20robustness.%20As%20generalizability%20and%20robustness%20in%20geographic%20imagery%0Ais%20a%20nascent%20topic%2C%20our%20work%20motivates%20researchers%20to%20consider%20metrics%20beyond%0Aaverage%20accuracy%20in%20such%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.08672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Urban-Rural%2520Disparities%2520in%2520Contrastive%2520Representation%250A%2520%2520Learning%2520with%2520Satellite%2520Imagery%26entry.906535625%3DMiao%2520Zhang%2520and%2520Rumi%2520Chunara%26entry.1292438233%3D%2520%2520Satellite%2520imagery%2520is%2520being%2520leveraged%2520for%2520many%2520societally%2520critical%2520tasks%250Aacross%2520climate%252C%2520economics%252C%2520and%2520public%2520health.%2520Yet%252C%2520because%2520of%2520heterogeneity%2520in%250Alandscapes%2520%2528e.g.%2520how%2520a%2520road%2520looks%2520in%2520different%2520places%2529%252C%2520models%2520can%2520show%250Adisparate%2520performance%2520across%2520geographic%2520areas.%2520Given%2520the%2520important%2520potential%2520of%250Adisparities%2520in%2520algorithmic%2520systems%2520used%2520in%2520societal%2520contexts%252C%2520here%2520we%2520consider%250Athe%2520risk%2520of%2520urban-rural%2520disparities%2520in%2520identification%2520of%2520land-cover%2520features.%250AThis%2520is%2520via%2520semantic%2520segmentation%2520%2528a%2520common%2520computer%2520vision%2520task%2520in%2520which%2520image%250Aregions%2520are%2520labelled%2520according%2520to%2520what%2520is%2520being%2520shown%2529%2520which%2520uses%2520pre-trained%250Aimage%2520representations%2520generated%2520via%2520contrastive%2520self-supervised%2520learning.%2520We%250Apropose%2520fair%2520dense%2520representation%2520with%2520contrastive%2520learning%2520%2528FairDCL%2529%2520as%2520a%250Amethod%2520for%2520de-biasing%2520the%2520multi-level%2520latent%2520space%2520of%2520convolution%2520neural%250Anetwork%2520models.%2520The%2520method%2520improves%2520feature%2520identification%2520by%2520removing%2520spurious%250Amodel%2520representations%2520which%2520are%2520disparately%2520distributed%2520across%2520urban%2520and%2520rural%250Aareas%252C%2520and%2520is%2520achieved%2520in%2520an%2520unsupervised%2520way%2520by%2520contrastive%2520pre-training.%2520The%250Aobtained%2520image%2520representation%2520mitigates%2520downstream%2520urban-rural%2520prediction%250Adisparities%2520and%2520outperforms%2520state-of-the-art%2520baselines%2520on%2520real-world%2520satellite%250Aimages.%2520Embedding%2520space%2520evaluation%2520and%2520ablation%2520studies%2520further%2520demonstrate%250AFairDCL%2527s%2520robustness.%2520As%2520generalizability%2520and%2520robustness%2520in%2520geographic%2520imagery%250Ais%2520a%2520nascent%2520topic%252C%2520our%2520work%2520motivates%2520researchers%2520to%2520consider%2520metrics%2520beyond%250Aaverage%2520accuracy%2520in%2520such%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.08672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Urban-Rural%20Disparities%20in%20Contrastive%20Representation%0A%20%20Learning%20with%20Satellite%20Imagery&entry.906535625=Miao%20Zhang%20and%20Rumi%20Chunara&entry.1292438233=%20%20Satellite%20imagery%20is%20being%20leveraged%20for%20many%20societally%20critical%20tasks%0Aacross%20climate%2C%20economics%2C%20and%20public%20health.%20Yet%2C%20because%20of%20heterogeneity%20in%0Alandscapes%20%28e.g.%20how%20a%20road%20looks%20in%20different%20places%29%2C%20models%20can%20show%0Adisparate%20performance%20across%20geographic%20areas.%20Given%20the%20important%20potential%20of%0Adisparities%20in%20algorithmic%20systems%20used%20in%20societal%20contexts%2C%20here%20we%20consider%0Athe%20risk%20of%20urban-rural%20disparities%20in%20identification%20of%20land-cover%20features.%0AThis%20is%20via%20semantic%20segmentation%20%28a%20common%20computer%20vision%20task%20in%20which%20image%0Aregions%20are%20labelled%20according%20to%20what%20is%20being%20shown%29%20which%20uses%20pre-trained%0Aimage%20representations%20generated%20via%20contrastive%20self-supervised%20learning.%20We%0Apropose%20fair%20dense%20representation%20with%20contrastive%20learning%20%28FairDCL%29%20as%20a%0Amethod%20for%20de-biasing%20the%20multi-level%20latent%20space%20of%20convolution%20neural%0Anetwork%20models.%20The%20method%20improves%20feature%20identification%20by%20removing%20spurious%0Amodel%20representations%20which%20are%20disparately%20distributed%20across%20urban%20and%20rural%0Aareas%2C%20and%20is%20achieved%20in%20an%20unsupervised%20way%20by%20contrastive%20pre-training.%20The%0Aobtained%20image%20representation%20mitigates%20downstream%20urban-rural%20prediction%0Adisparities%20and%20outperforms%20state-of-the-art%20baselines%20on%20real-world%20satellite%0Aimages.%20Embedding%20space%20evaluation%20and%20ablation%20studies%20further%20demonstrate%0AFairDCL%27s%20robustness.%20As%20generalizability%20and%20robustness%20in%20geographic%20imagery%0Ais%20a%20nascent%20topic%2C%20our%20work%20motivates%20researchers%20to%20consider%20metrics%20beyond%0Aaverage%20accuracy%20in%20such%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.08672v3&entry.124074799=Read"},
{"title": "TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for\n  Dynamic UAV-based Scenes", "author": "Christopher Maxey and Jaehoon Choi and Yonghan Lee and Hyungtae Lee and Dinesh Manocha and Heesung Kwon", "abstract": "  In this paper, we present a new approach to bridge the domain gap between\nsynthetic and real-world data for unmanned aerial vehicle (UAV)-based\nperception. Our formulation is designed for dynamic scenes, consisting of small\nmoving objects or human actions. We propose an extension of K-Planes Neural\nRadiance Field (NeRF), wherein our algorithm stores a set of tiered feature\nvectors. The tiered feature vectors are generated to effectively model\nconceptual information about a scene as well as an image decoder that\ntransforms output feature maps into RGB images. Our technique leverages the\ninformation amongst both static and dynamic objects within a scene and is able\nto capture salient scene attributes of high altitude videos. We evaluate its\nperformance on challenging datasets, including Okutama Action and UG2, and\nobserve considerable improvement in accuracy over state of the art neural\nrendering methods.\n", "link": "http://arxiv.org/abs/2405.02762v2", "date": "2024-09-18", "relevancy": 2.7273, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TK-Planes%3A%20Tiered%20K-Planes%20with%20High%20Dimensional%20Feature%20Vectors%20for%0A%20%20Dynamic%20UAV-based%20Scenes&body=Title%3A%20TK-Planes%3A%20Tiered%20K-Planes%20with%20High%20Dimensional%20Feature%20Vectors%20for%0A%20%20Dynamic%20UAV-based%20Scenes%0AAuthor%3A%20Christopher%20Maxey%20and%20Jaehoon%20Choi%20and%20Yonghan%20Lee%20and%20Hyungtae%20Lee%20and%20Dinesh%20Manocha%20and%20Heesung%20Kwon%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20to%20bridge%20the%20domain%20gap%20between%0Asynthetic%20and%20real-world%20data%20for%20unmanned%20aerial%20vehicle%20%28UAV%29-based%0Aperception.%20Our%20formulation%20is%20designed%20for%20dynamic%20scenes%2C%20consisting%20of%20small%0Amoving%20objects%20or%20human%20actions.%20We%20propose%20an%20extension%20of%20K-Planes%20Neural%0ARadiance%20Field%20%28NeRF%29%2C%20wherein%20our%20algorithm%20stores%20a%20set%20of%20tiered%20feature%0Avectors.%20The%20tiered%20feature%20vectors%20are%20generated%20to%20effectively%20model%0Aconceptual%20information%20about%20a%20scene%20as%20well%20as%20an%20image%20decoder%20that%0Atransforms%20output%20feature%20maps%20into%20RGB%20images.%20Our%20technique%20leverages%20the%0Ainformation%20amongst%20both%20static%20and%20dynamic%20objects%20within%20a%20scene%20and%20is%20able%0Ato%20capture%20salient%20scene%20attributes%20of%20high%20altitude%20videos.%20We%20evaluate%20its%0Aperformance%20on%20challenging%20datasets%2C%20including%20Okutama%20Action%20and%20UG2%2C%20and%0Aobserve%20considerable%20improvement%20in%20accuracy%20over%20state%20of%20the%20art%20neural%0Arendering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTK-Planes%253A%2520Tiered%2520K-Planes%2520with%2520High%2520Dimensional%2520Feature%2520Vectors%2520for%250A%2520%2520Dynamic%2520UAV-based%2520Scenes%26entry.906535625%3DChristopher%2520Maxey%2520and%2520Jaehoon%2520Choi%2520and%2520Yonghan%2520Lee%2520and%2520Hyungtae%2520Lee%2520and%2520Dinesh%2520Manocha%2520and%2520Heesung%2520Kwon%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520approach%2520to%2520bridge%2520the%2520domain%2520gap%2520between%250Asynthetic%2520and%2520real-world%2520data%2520for%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529-based%250Aperception.%2520Our%2520formulation%2520is%2520designed%2520for%2520dynamic%2520scenes%252C%2520consisting%2520of%2520small%250Amoving%2520objects%2520or%2520human%2520actions.%2520We%2520propose%2520an%2520extension%2520of%2520K-Planes%2520Neural%250ARadiance%2520Field%2520%2528NeRF%2529%252C%2520wherein%2520our%2520algorithm%2520stores%2520a%2520set%2520of%2520tiered%2520feature%250Avectors.%2520The%2520tiered%2520feature%2520vectors%2520are%2520generated%2520to%2520effectively%2520model%250Aconceptual%2520information%2520about%2520a%2520scene%2520as%2520well%2520as%2520an%2520image%2520decoder%2520that%250Atransforms%2520output%2520feature%2520maps%2520into%2520RGB%2520images.%2520Our%2520technique%2520leverages%2520the%250Ainformation%2520amongst%2520both%2520static%2520and%2520dynamic%2520objects%2520within%2520a%2520scene%2520and%2520is%2520able%250Ato%2520capture%2520salient%2520scene%2520attributes%2520of%2520high%2520altitude%2520videos.%2520We%2520evaluate%2520its%250Aperformance%2520on%2520challenging%2520datasets%252C%2520including%2520Okutama%2520Action%2520and%2520UG2%252C%2520and%250Aobserve%2520considerable%2520improvement%2520in%2520accuracy%2520over%2520state%2520of%2520the%2520art%2520neural%250Arendering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TK-Planes%3A%20Tiered%20K-Planes%20with%20High%20Dimensional%20Feature%20Vectors%20for%0A%20%20Dynamic%20UAV-based%20Scenes&entry.906535625=Christopher%20Maxey%20and%20Jaehoon%20Choi%20and%20Yonghan%20Lee%20and%20Hyungtae%20Lee%20and%20Dinesh%20Manocha%20and%20Heesung%20Kwon&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20to%20bridge%20the%20domain%20gap%20between%0Asynthetic%20and%20real-world%20data%20for%20unmanned%20aerial%20vehicle%20%28UAV%29-based%0Aperception.%20Our%20formulation%20is%20designed%20for%20dynamic%20scenes%2C%20consisting%20of%20small%0Amoving%20objects%20or%20human%20actions.%20We%20propose%20an%20extension%20of%20K-Planes%20Neural%0ARadiance%20Field%20%28NeRF%29%2C%20wherein%20our%20algorithm%20stores%20a%20set%20of%20tiered%20feature%0Avectors.%20The%20tiered%20feature%20vectors%20are%20generated%20to%20effectively%20model%0Aconceptual%20information%20about%20a%20scene%20as%20well%20as%20an%20image%20decoder%20that%0Atransforms%20output%20feature%20maps%20into%20RGB%20images.%20Our%20technique%20leverages%20the%0Ainformation%20amongst%20both%20static%20and%20dynamic%20objects%20within%20a%20scene%20and%20is%20able%0Ato%20capture%20salient%20scene%20attributes%20of%20high%20altitude%20videos.%20We%20evaluate%20its%0Aperformance%20on%20challenging%20datasets%2C%20including%20Okutama%20Action%20and%20UG2%2C%20and%0Aobserve%20considerable%20improvement%20in%20accuracy%20over%20state%20of%20the%20art%20neural%0Arendering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02762v2&entry.124074799=Read"},
{"title": "A Controlled Study on Long Context Extension and Generalization in LLMs", "author": "Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush", "abstract": "  Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.\n", "link": "http://arxiv.org/abs/2409.12181v1", "date": "2024-09-18", "relevancy": 2.726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs&body=Title%3A%20A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs%0AAuthor%3A%20Yi%20Lu%20and%20Jing%20Nathan%20Yan%20and%20Songlin%20Yang%20and%20Justin%20T.%20Chiu%20and%20Siyu%20Ren%20and%20Fei%20Yuan%20and%20Wenting%20Zhao%20and%20Zhiyong%20Wu%20and%20Alexander%20M.%20Rush%0AAbstract%3A%20%20%20Broad%20textual%20understanding%20and%20in-context%20learning%20require%20language%20models%0Athat%20utilize%20full%20document%20contexts.%20Due%20to%20the%20implementation%20challenges%0Aassociated%20with%20directly%20training%20long-context%20models%2C%20many%20methods%20have%20been%0Aproposed%20for%20extending%20models%20to%20handle%20long%20contexts.%20However%2C%20owing%20to%0Adifferences%20in%20data%20and%20model%20classes%2C%20it%20has%20been%20challenging%20to%20compare%20these%0Aapproaches%2C%20leading%20to%20uncertainty%20as%20to%20how%20to%20evaluate%20long-context%0Aperformance%20and%20whether%20it%20differs%20from%20standard%20evaluation.%20We%20implement%20a%0Acontrolled%20protocol%20for%20extension%20methods%20with%20a%20standardized%20evaluation%2C%0Autilizing%20consistent%20base%20models%20and%20extension%20data.%20Our%20study%20yields%20several%0Ainsights%20into%20long-context%20behavior.%20First%2C%20we%20reaffirm%20the%20critical%20role%20of%0Aperplexity%20as%20a%20general-purpose%20performance%20indicator%20even%20in%20longer-context%0Atasks.%20Second%2C%20we%20find%20that%20current%20approximate%20attention%20methods%0Asystematically%20underperform%20across%20long-context%20tasks.%20Finally%2C%20we%20confirm%20that%0Aexact%20fine-tuning%20based%20methods%20are%20generally%20effective%20within%20the%20range%20of%0Atheir%20extension%2C%20whereas%20extrapolation%20remains%20challenging.%20All%20codebases%2C%0Amodels%2C%20and%20checkpoints%20will%20be%20made%20available%20open-source%2C%20promoting%0Atransparency%20and%20facilitating%20further%20research%20in%20this%20critical%20area%20of%20AI%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Controlled%2520Study%2520on%2520Long%2520Context%2520Extension%2520and%2520Generalization%2520in%2520LLMs%26entry.906535625%3DYi%2520Lu%2520and%2520Jing%2520Nathan%2520Yan%2520and%2520Songlin%2520Yang%2520and%2520Justin%2520T.%2520Chiu%2520and%2520Siyu%2520Ren%2520and%2520Fei%2520Yuan%2520and%2520Wenting%2520Zhao%2520and%2520Zhiyong%2520Wu%2520and%2520Alexander%2520M.%2520Rush%26entry.1292438233%3D%2520%2520Broad%2520textual%2520understanding%2520and%2520in-context%2520learning%2520require%2520language%2520models%250Athat%2520utilize%2520full%2520document%2520contexts.%2520Due%2520to%2520the%2520implementation%2520challenges%250Aassociated%2520with%2520directly%2520training%2520long-context%2520models%252C%2520many%2520methods%2520have%2520been%250Aproposed%2520for%2520extending%2520models%2520to%2520handle%2520long%2520contexts.%2520However%252C%2520owing%2520to%250Adifferences%2520in%2520data%2520and%2520model%2520classes%252C%2520it%2520has%2520been%2520challenging%2520to%2520compare%2520these%250Aapproaches%252C%2520leading%2520to%2520uncertainty%2520as%2520to%2520how%2520to%2520evaluate%2520long-context%250Aperformance%2520and%2520whether%2520it%2520differs%2520from%2520standard%2520evaluation.%2520We%2520implement%2520a%250Acontrolled%2520protocol%2520for%2520extension%2520methods%2520with%2520a%2520standardized%2520evaluation%252C%250Autilizing%2520consistent%2520base%2520models%2520and%2520extension%2520data.%2520Our%2520study%2520yields%2520several%250Ainsights%2520into%2520long-context%2520behavior.%2520First%252C%2520we%2520reaffirm%2520the%2520critical%2520role%2520of%250Aperplexity%2520as%2520a%2520general-purpose%2520performance%2520indicator%2520even%2520in%2520longer-context%250Atasks.%2520Second%252C%2520we%2520find%2520that%2520current%2520approximate%2520attention%2520methods%250Asystematically%2520underperform%2520across%2520long-context%2520tasks.%2520Finally%252C%2520we%2520confirm%2520that%250Aexact%2520fine-tuning%2520based%2520methods%2520are%2520generally%2520effective%2520within%2520the%2520range%2520of%250Atheir%2520extension%252C%2520whereas%2520extrapolation%2520remains%2520challenging.%2520All%2520codebases%252C%250Amodels%252C%2520and%2520checkpoints%2520will%2520be%2520made%2520available%2520open-source%252C%2520promoting%250Atransparency%2520and%2520facilitating%2520further%2520research%2520in%2520this%2520critical%2520area%2520of%2520AI%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Controlled%20Study%20on%20Long%20Context%20Extension%20and%20Generalization%20in%20LLMs&entry.906535625=Yi%20Lu%20and%20Jing%20Nathan%20Yan%20and%20Songlin%20Yang%20and%20Justin%20T.%20Chiu%20and%20Siyu%20Ren%20and%20Fei%20Yuan%20and%20Wenting%20Zhao%20and%20Zhiyong%20Wu%20and%20Alexander%20M.%20Rush&entry.1292438233=%20%20Broad%20textual%20understanding%20and%20in-context%20learning%20require%20language%20models%0Athat%20utilize%20full%20document%20contexts.%20Due%20to%20the%20implementation%20challenges%0Aassociated%20with%20directly%20training%20long-context%20models%2C%20many%20methods%20have%20been%0Aproposed%20for%20extending%20models%20to%20handle%20long%20contexts.%20However%2C%20owing%20to%0Adifferences%20in%20data%20and%20model%20classes%2C%20it%20has%20been%20challenging%20to%20compare%20these%0Aapproaches%2C%20leading%20to%20uncertainty%20as%20to%20how%20to%20evaluate%20long-context%0Aperformance%20and%20whether%20it%20differs%20from%20standard%20evaluation.%20We%20implement%20a%0Acontrolled%20protocol%20for%20extension%20methods%20with%20a%20standardized%20evaluation%2C%0Autilizing%20consistent%20base%20models%20and%20extension%20data.%20Our%20study%20yields%20several%0Ainsights%20into%20long-context%20behavior.%20First%2C%20we%20reaffirm%20the%20critical%20role%20of%0Aperplexity%20as%20a%20general-purpose%20performance%20indicator%20even%20in%20longer-context%0Atasks.%20Second%2C%20we%20find%20that%20current%20approximate%20attention%20methods%0Asystematically%20underperform%20across%20long-context%20tasks.%20Finally%2C%20we%20confirm%20that%0Aexact%20fine-tuning%20based%20methods%20are%20generally%20effective%20within%20the%20range%20of%0Atheir%20extension%2C%20whereas%20extrapolation%20remains%20challenging.%20All%20codebases%2C%0Amodels%2C%20and%20checkpoints%20will%20be%20made%20available%20open-source%2C%20promoting%0Atransparency%20and%20facilitating%20further%20research%20in%20this%20critical%20area%20of%20AI%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12181v1&entry.124074799=Read"},
{"title": "A New Era in Computational Pathology: A Survey on Foundation and\n  Vision-Language Models", "author": "Dibaloke Chanda and Milan Aryal and Nasim Yahya Soltani and Masoud Ganji", "abstract": "  Recent advances in deep learning have completely transformed the domain of\ncomputational pathology (CPath). More specifically, it has altered the\ndiagnostic workflow of pathologists by integrating foundation models (FMs) and\nvision-language models (VLMs) in their assessment and decision-making process.\nThe limitations of existing deep learning approaches in CPath can be overcome\nby FMs through learning a representation space that can be adapted to a wide\nvariety of downstream tasks without explicit supervision. Deploying VLMs allow\npathology reports written in natural language be used as rich semantic\ninformation sources to improve existing models as well as generate predictions\nin natural language form. In this survey, a holistic and systematic overview of\nrecent innovations in FMs and VLMs in CPath is presented. Furthermore, the\ntools, datasets and training schemes for these models are summarized in\naddition to categorizing them into distinct groups. This extensive survey\nhighlights the current trends in CPath and its possible revolution through the\nuse of FMs and VLMs in the future.\n", "link": "http://arxiv.org/abs/2408.14496v3", "date": "2024-09-18", "relevancy": 2.7072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Era%20in%20Computational%20Pathology%3A%20A%20Survey%20on%20Foundation%20and%0A%20%20Vision-Language%20Models&body=Title%3A%20A%20New%20Era%20in%20Computational%20Pathology%3A%20A%20Survey%20on%20Foundation%20and%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Dibaloke%20Chanda%20and%20Milan%20Aryal%20and%20Nasim%20Yahya%20Soltani%20and%20Masoud%20Ganji%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20have%20completely%20transformed%20the%20domain%20of%0Acomputational%20pathology%20%28CPath%29.%20More%20specifically%2C%20it%20has%20altered%20the%0Adiagnostic%20workflow%20of%20pathologists%20by%20integrating%20foundation%20models%20%28FMs%29%20and%0Avision-language%20models%20%28VLMs%29%20in%20their%20assessment%20and%20decision-making%20process.%0AThe%20limitations%20of%20existing%20deep%20learning%20approaches%20in%20CPath%20can%20be%20overcome%0Aby%20FMs%20through%20learning%20a%20representation%20space%20that%20can%20be%20adapted%20to%20a%20wide%0Avariety%20of%20downstream%20tasks%20without%20explicit%20supervision.%20Deploying%20VLMs%20allow%0Apathology%20reports%20written%20in%20natural%20language%20be%20used%20as%20rich%20semantic%0Ainformation%20sources%20to%20improve%20existing%20models%20as%20well%20as%20generate%20predictions%0Ain%20natural%20language%20form.%20In%20this%20survey%2C%20a%20holistic%20and%20systematic%20overview%20of%0Arecent%20innovations%20in%20FMs%20and%20VLMs%20in%20CPath%20is%20presented.%20Furthermore%2C%20the%0Atools%2C%20datasets%20and%20training%20schemes%20for%20these%20models%20are%20summarized%20in%0Aaddition%20to%20categorizing%20them%20into%20distinct%20groups.%20This%20extensive%20survey%0Ahighlights%20the%20current%20trends%20in%20CPath%20and%20its%20possible%20revolution%20through%20the%0Ause%20of%20FMs%20and%20VLMs%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14496v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Era%2520in%2520Computational%2520Pathology%253A%2520A%2520Survey%2520on%2520Foundation%2520and%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DDibaloke%2520Chanda%2520and%2520Milan%2520Aryal%2520and%2520Nasim%2520Yahya%2520Soltani%2520and%2520Masoud%2520Ganji%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520completely%2520transformed%2520the%2520domain%2520of%250Acomputational%2520pathology%2520%2528CPath%2529.%2520More%2520specifically%252C%2520it%2520has%2520altered%2520the%250Adiagnostic%2520workflow%2520of%2520pathologists%2520by%2520integrating%2520foundation%2520models%2520%2528FMs%2529%2520and%250Avision-language%2520models%2520%2528VLMs%2529%2520in%2520their%2520assessment%2520and%2520decision-making%2520process.%250AThe%2520limitations%2520of%2520existing%2520deep%2520learning%2520approaches%2520in%2520CPath%2520can%2520be%2520overcome%250Aby%2520FMs%2520through%2520learning%2520a%2520representation%2520space%2520that%2520can%2520be%2520adapted%2520to%2520a%2520wide%250Avariety%2520of%2520downstream%2520tasks%2520without%2520explicit%2520supervision.%2520Deploying%2520VLMs%2520allow%250Apathology%2520reports%2520written%2520in%2520natural%2520language%2520be%2520used%2520as%2520rich%2520semantic%250Ainformation%2520sources%2520to%2520improve%2520existing%2520models%2520as%2520well%2520as%2520generate%2520predictions%250Ain%2520natural%2520language%2520form.%2520In%2520this%2520survey%252C%2520a%2520holistic%2520and%2520systematic%2520overview%2520of%250Arecent%2520innovations%2520in%2520FMs%2520and%2520VLMs%2520in%2520CPath%2520is%2520presented.%2520Furthermore%252C%2520the%250Atools%252C%2520datasets%2520and%2520training%2520schemes%2520for%2520these%2520models%2520are%2520summarized%2520in%250Aaddition%2520to%2520categorizing%2520them%2520into%2520distinct%2520groups.%2520This%2520extensive%2520survey%250Ahighlights%2520the%2520current%2520trends%2520in%2520CPath%2520and%2520its%2520possible%2520revolution%2520through%2520the%250Ause%2520of%2520FMs%2520and%2520VLMs%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14496v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Era%20in%20Computational%20Pathology%3A%20A%20Survey%20on%20Foundation%20and%0A%20%20Vision-Language%20Models&entry.906535625=Dibaloke%20Chanda%20and%20Milan%20Aryal%20and%20Nasim%20Yahya%20Soltani%20and%20Masoud%20Ganji&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20have%20completely%20transformed%20the%20domain%20of%0Acomputational%20pathology%20%28CPath%29.%20More%20specifically%2C%20it%20has%20altered%20the%0Adiagnostic%20workflow%20of%20pathologists%20by%20integrating%20foundation%20models%20%28FMs%29%20and%0Avision-language%20models%20%28VLMs%29%20in%20their%20assessment%20and%20decision-making%20process.%0AThe%20limitations%20of%20existing%20deep%20learning%20approaches%20in%20CPath%20can%20be%20overcome%0Aby%20FMs%20through%20learning%20a%20representation%20space%20that%20can%20be%20adapted%20to%20a%20wide%0Avariety%20of%20downstream%20tasks%20without%20explicit%20supervision.%20Deploying%20VLMs%20allow%0Apathology%20reports%20written%20in%20natural%20language%20be%20used%20as%20rich%20semantic%0Ainformation%20sources%20to%20improve%20existing%20models%20as%20well%20as%20generate%20predictions%0Ain%20natural%20language%20form.%20In%20this%20survey%2C%20a%20holistic%20and%20systematic%20overview%20of%0Arecent%20innovations%20in%20FMs%20and%20VLMs%20in%20CPath%20is%20presented.%20Furthermore%2C%20the%0Atools%2C%20datasets%20and%20training%20schemes%20for%20these%20models%20are%20summarized%20in%0Aaddition%20to%20categorizing%20them%20into%20distinct%20groups.%20This%20extensive%20survey%0Ahighlights%20the%20current%20trends%20in%20CPath%20and%20its%20possible%20revolution%20through%20the%0Ause%20of%20FMs%20and%20VLMs%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14496v3&entry.124074799=Read"},
{"title": "Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary\n  Detection", "author": "Haoxuan Wang and Qingdong He and Jinlong Peng and Hao Yang and Mingmin Chi and Yabiao Wang", "abstract": "  Open-vocabulary detection (OVD) aims to detect objects beyond a predefined\nset of categories. As a pioneering model incorporating the YOLO series into\nOVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.\nHowever, its performance is hindered by its neck feature fusion mechanism,\nwhich causes the quadratic complexity and the limited guided receptive fields.\nTo address these limitations, we present Mamba-YOLO-World, a novel YOLO-based\nOVD model employing the proposed MambaFusion Path Aggregation Network\n(MambaFusion-PAN) as its neck architecture. Specifically, we introduce an\ninnovative State Space Model-based feature fusion mechanism consisting of a\nParallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan\nalgorithm with linear complexity and globally guided receptive fields. It\nleverages multi-modal input sequences and mamba hidden states to guide the\nselective scanning process. Experiments demonstrate that our model outperforms\nthe original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and\nfine-tuning settings while maintaining comparable parameters and FLOPs.\nAdditionally, it surpasses existing state-of-the-art OVD methods with fewer\nparameters and FLOPs.\n", "link": "http://arxiv.org/abs/2409.08513v3", "date": "2024-09-18", "relevancy": 2.707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-YOLO-World%3A%20Marrying%20YOLO-World%20with%20Mamba%20for%20Open-Vocabulary%0A%20%20Detection&body=Title%3A%20Mamba-YOLO-World%3A%20Marrying%20YOLO-World%20with%20Mamba%20for%20Open-Vocabulary%0A%20%20Detection%0AAuthor%3A%20Haoxuan%20Wang%20and%20Qingdong%20He%20and%20Jinlong%20Peng%20and%20Hao%20Yang%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%0AAbstract%3A%20%20%20Open-vocabulary%20detection%20%28OVD%29%20aims%20to%20detect%20objects%20beyond%20a%20predefined%0Aset%20of%20categories.%20As%20a%20pioneering%20model%20incorporating%20the%20YOLO%20series%20into%0AOVD%2C%20YOLO-World%20is%20well-suited%20for%20scenarios%20prioritizing%20speed%20and%20efficiency.%0AHowever%2C%20its%20performance%20is%20hindered%20by%20its%20neck%20feature%20fusion%20mechanism%2C%0Awhich%20causes%20the%20quadratic%20complexity%20and%20the%20limited%20guided%20receptive%20fields.%0ATo%20address%20these%20limitations%2C%20we%20present%20Mamba-YOLO-World%2C%20a%20novel%20YOLO-based%0AOVD%20model%20employing%20the%20proposed%20MambaFusion%20Path%20Aggregation%20Network%0A%28MambaFusion-PAN%29%20as%20its%20neck%20architecture.%20Specifically%2C%20we%20introduce%20an%0Ainnovative%20State%20Space%20Model-based%20feature%20fusion%20mechanism%20consisting%20of%20a%0AParallel-Guided%20Selective%20Scan%20algorithm%20and%20a%20Serial-Guided%20Selective%20Scan%0Aalgorithm%20with%20linear%20complexity%20and%20globally%20guided%20receptive%20fields.%20It%0Aleverages%20multi-modal%20input%20sequences%20and%20mamba%20hidden%20states%20to%20guide%20the%0Aselective%20scanning%20process.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%0Athe%20original%20YOLO-World%20on%20the%20COCO%20and%20LVIS%20benchmarks%20in%20both%20zero-shot%20and%0Afine-tuning%20settings%20while%20maintaining%20comparable%20parameters%20and%20FLOPs.%0AAdditionally%2C%20it%20surpasses%20existing%20state-of-the-art%20OVD%20methods%20with%20fewer%0Aparameters%20and%20FLOPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08513v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-YOLO-World%253A%2520Marrying%2520YOLO-World%2520with%2520Mamba%2520for%2520Open-Vocabulary%250A%2520%2520Detection%26entry.906535625%3DHaoxuan%2520Wang%2520and%2520Qingdong%2520He%2520and%2520Jinlong%2520Peng%2520and%2520Hao%2520Yang%2520and%2520Mingmin%2520Chi%2520and%2520Yabiao%2520Wang%26entry.1292438233%3D%2520%2520Open-vocabulary%2520detection%2520%2528OVD%2529%2520aims%2520to%2520detect%2520objects%2520beyond%2520a%2520predefined%250Aset%2520of%2520categories.%2520As%2520a%2520pioneering%2520model%2520incorporating%2520the%2520YOLO%2520series%2520into%250AOVD%252C%2520YOLO-World%2520is%2520well-suited%2520for%2520scenarios%2520prioritizing%2520speed%2520and%2520efficiency.%250AHowever%252C%2520its%2520performance%2520is%2520hindered%2520by%2520its%2520neck%2520feature%2520fusion%2520mechanism%252C%250Awhich%2520causes%2520the%2520quadratic%2520complexity%2520and%2520the%2520limited%2520guided%2520receptive%2520fields.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520present%2520Mamba-YOLO-World%252C%2520a%2520novel%2520YOLO-based%250AOVD%2520model%2520employing%2520the%2520proposed%2520MambaFusion%2520Path%2520Aggregation%2520Network%250A%2528MambaFusion-PAN%2529%2520as%2520its%2520neck%2520architecture.%2520Specifically%252C%2520we%2520introduce%2520an%250Ainnovative%2520State%2520Space%2520Model-based%2520feature%2520fusion%2520mechanism%2520consisting%2520of%2520a%250AParallel-Guided%2520Selective%2520Scan%2520algorithm%2520and%2520a%2520Serial-Guided%2520Selective%2520Scan%250Aalgorithm%2520with%2520linear%2520complexity%2520and%2520globally%2520guided%2520receptive%2520fields.%2520It%250Aleverages%2520multi-modal%2520input%2520sequences%2520and%2520mamba%2520hidden%2520states%2520to%2520guide%2520the%250Aselective%2520scanning%2520process.%2520Experiments%2520demonstrate%2520that%2520our%2520model%2520outperforms%250Athe%2520original%2520YOLO-World%2520on%2520the%2520COCO%2520and%2520LVIS%2520benchmarks%2520in%2520both%2520zero-shot%2520and%250Afine-tuning%2520settings%2520while%2520maintaining%2520comparable%2520parameters%2520and%2520FLOPs.%250AAdditionally%252C%2520it%2520surpasses%2520existing%2520state-of-the-art%2520OVD%2520methods%2520with%2520fewer%250Aparameters%2520and%2520FLOPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08513v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-YOLO-World%3A%20Marrying%20YOLO-World%20with%20Mamba%20for%20Open-Vocabulary%0A%20%20Detection&entry.906535625=Haoxuan%20Wang%20and%20Qingdong%20He%20and%20Jinlong%20Peng%20and%20Hao%20Yang%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang&entry.1292438233=%20%20Open-vocabulary%20detection%20%28OVD%29%20aims%20to%20detect%20objects%20beyond%20a%20predefined%0Aset%20of%20categories.%20As%20a%20pioneering%20model%20incorporating%20the%20YOLO%20series%20into%0AOVD%2C%20YOLO-World%20is%20well-suited%20for%20scenarios%20prioritizing%20speed%20and%20efficiency.%0AHowever%2C%20its%20performance%20is%20hindered%20by%20its%20neck%20feature%20fusion%20mechanism%2C%0Awhich%20causes%20the%20quadratic%20complexity%20and%20the%20limited%20guided%20receptive%20fields.%0ATo%20address%20these%20limitations%2C%20we%20present%20Mamba-YOLO-World%2C%20a%20novel%20YOLO-based%0AOVD%20model%20employing%20the%20proposed%20MambaFusion%20Path%20Aggregation%20Network%0A%28MambaFusion-PAN%29%20as%20its%20neck%20architecture.%20Specifically%2C%20we%20introduce%20an%0Ainnovative%20State%20Space%20Model-based%20feature%20fusion%20mechanism%20consisting%20of%20a%0AParallel-Guided%20Selective%20Scan%20algorithm%20and%20a%20Serial-Guided%20Selective%20Scan%0Aalgorithm%20with%20linear%20complexity%20and%20globally%20guided%20receptive%20fields.%20It%0Aleverages%20multi-modal%20input%20sequences%20and%20mamba%20hidden%20states%20to%20guide%20the%0Aselective%20scanning%20process.%20Experiments%20demonstrate%20that%20our%20model%20outperforms%0Athe%20original%20YOLO-World%20on%20the%20COCO%20and%20LVIS%20benchmarks%20in%20both%20zero-shot%20and%0Afine-tuning%20settings%20while%20maintaining%20comparable%20parameters%20and%20FLOPs.%0AAdditionally%2C%20it%20surpasses%20existing%20state-of-the-art%20OVD%20methods%20with%20fewer%0Aparameters%20and%20FLOPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08513v3&entry.124074799=Read"},
{"title": "Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping", "author": "Jaehyung Jung and Simon Boche and Sebastian Barbas Laina and Stefan Leutenegger", "abstract": "  We propose visual-inertial simultaneous localization and mapping that tightly\ncouples sparse reprojection errors, inertial measurement unit pre-integrals,\nand relative pose factors with dense volumetric occupancy mapping. Hereby depth\npredictions from a deep neural network are fused in a fully probabilistic\nmanner. Specifically, our method is rigorously uncertainty-aware: first, we use\ndepth and uncertainty predictions from a deep network not only from the robot's\nstereo rig, but we further probabilistically fuse motion stereo that provides\ndepth information across a range of baselines, therefore drastically increasing\nmapping accuracy. Next, predicted and fused depth uncertainty propagates not\nonly into occupancy probabilities but also into alignment factors between\ngenerated dense submaps that enter the probabilistic nonlinear least squares\nestimator. This submap representation offers globally consistent geometry at\nscale. Our method is thoroughly evaluated in two benchmark datasets, resulting\nin localization and mapping accuracy that exceeds the state of the art, while\nsimultaneously offering volumetric occupancy directly usable for downstream\nrobotic planning and control in real-time.\n", "link": "http://arxiv.org/abs/2409.12051v1", "date": "2024-09-18", "relevancy": 2.7005, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7416}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6598}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Visual-Inertial%20SLAM%20with%20Volumetric%20Occupancy%20Mapping&body=Title%3A%20Uncertainty-Aware%20Visual-Inertial%20SLAM%20with%20Volumetric%20Occupancy%20Mapping%0AAuthor%3A%20Jaehyung%20Jung%20and%20Simon%20Boche%20and%20Sebastian%20Barbas%20Laina%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20We%20propose%20visual-inertial%20simultaneous%20localization%20and%20mapping%20that%20tightly%0Acouples%20sparse%20reprojection%20errors%2C%20inertial%20measurement%20unit%20pre-integrals%2C%0Aand%20relative%20pose%20factors%20with%20dense%20volumetric%20occupancy%20mapping.%20Hereby%20depth%0Apredictions%20from%20a%20deep%20neural%20network%20are%20fused%20in%20a%20fully%20probabilistic%0Amanner.%20Specifically%2C%20our%20method%20is%20rigorously%20uncertainty-aware%3A%20first%2C%20we%20use%0Adepth%20and%20uncertainty%20predictions%20from%20a%20deep%20network%20not%20only%20from%20the%20robot%27s%0Astereo%20rig%2C%20but%20we%20further%20probabilistically%20fuse%20motion%20stereo%20that%20provides%0Adepth%20information%20across%20a%20range%20of%20baselines%2C%20therefore%20drastically%20increasing%0Amapping%20accuracy.%20Next%2C%20predicted%20and%20fused%20depth%20uncertainty%20propagates%20not%0Aonly%20into%20occupancy%20probabilities%20but%20also%20into%20alignment%20factors%20between%0Agenerated%20dense%20submaps%20that%20enter%20the%20probabilistic%20nonlinear%20least%20squares%0Aestimator.%20This%20submap%20representation%20offers%20globally%20consistent%20geometry%20at%0Ascale.%20Our%20method%20is%20thoroughly%20evaluated%20in%20two%20benchmark%20datasets%2C%20resulting%0Ain%20localization%20and%20mapping%20accuracy%20that%20exceeds%20the%20state%20of%20the%20art%2C%20while%0Asimultaneously%20offering%20volumetric%20occupancy%20directly%20usable%20for%20downstream%0Arobotic%20planning%20and%20control%20in%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Visual-Inertial%2520SLAM%2520with%2520Volumetric%2520Occupancy%2520Mapping%26entry.906535625%3DJaehyung%2520Jung%2520and%2520Simon%2520Boche%2520and%2520Sebastian%2520Barbas%2520Laina%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520We%2520propose%2520visual-inertial%2520simultaneous%2520localization%2520and%2520mapping%2520that%2520tightly%250Acouples%2520sparse%2520reprojection%2520errors%252C%2520inertial%2520measurement%2520unit%2520pre-integrals%252C%250Aand%2520relative%2520pose%2520factors%2520with%2520dense%2520volumetric%2520occupancy%2520mapping.%2520Hereby%2520depth%250Apredictions%2520from%2520a%2520deep%2520neural%2520network%2520are%2520fused%2520in%2520a%2520fully%2520probabilistic%250Amanner.%2520Specifically%252C%2520our%2520method%2520is%2520rigorously%2520uncertainty-aware%253A%2520first%252C%2520we%2520use%250Adepth%2520and%2520uncertainty%2520predictions%2520from%2520a%2520deep%2520network%2520not%2520only%2520from%2520the%2520robot%2527s%250Astereo%2520rig%252C%2520but%2520we%2520further%2520probabilistically%2520fuse%2520motion%2520stereo%2520that%2520provides%250Adepth%2520information%2520across%2520a%2520range%2520of%2520baselines%252C%2520therefore%2520drastically%2520increasing%250Amapping%2520accuracy.%2520Next%252C%2520predicted%2520and%2520fused%2520depth%2520uncertainty%2520propagates%2520not%250Aonly%2520into%2520occupancy%2520probabilities%2520but%2520also%2520into%2520alignment%2520factors%2520between%250Agenerated%2520dense%2520submaps%2520that%2520enter%2520the%2520probabilistic%2520nonlinear%2520least%2520squares%250Aestimator.%2520This%2520submap%2520representation%2520offers%2520globally%2520consistent%2520geometry%2520at%250Ascale.%2520Our%2520method%2520is%2520thoroughly%2520evaluated%2520in%2520two%2520benchmark%2520datasets%252C%2520resulting%250Ain%2520localization%2520and%2520mapping%2520accuracy%2520that%2520exceeds%2520the%2520state%2520of%2520the%2520art%252C%2520while%250Asimultaneously%2520offering%2520volumetric%2520occupancy%2520directly%2520usable%2520for%2520downstream%250Arobotic%2520planning%2520and%2520control%2520in%2520real-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Visual-Inertial%20SLAM%20with%20Volumetric%20Occupancy%20Mapping&entry.906535625=Jaehyung%20Jung%20and%20Simon%20Boche%20and%20Sebastian%20Barbas%20Laina%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20We%20propose%20visual-inertial%20simultaneous%20localization%20and%20mapping%20that%20tightly%0Acouples%20sparse%20reprojection%20errors%2C%20inertial%20measurement%20unit%20pre-integrals%2C%0Aand%20relative%20pose%20factors%20with%20dense%20volumetric%20occupancy%20mapping.%20Hereby%20depth%0Apredictions%20from%20a%20deep%20neural%20network%20are%20fused%20in%20a%20fully%20probabilistic%0Amanner.%20Specifically%2C%20our%20method%20is%20rigorously%20uncertainty-aware%3A%20first%2C%20we%20use%0Adepth%20and%20uncertainty%20predictions%20from%20a%20deep%20network%20not%20only%20from%20the%20robot%27s%0Astereo%20rig%2C%20but%20we%20further%20probabilistically%20fuse%20motion%20stereo%20that%20provides%0Adepth%20information%20across%20a%20range%20of%20baselines%2C%20therefore%20drastically%20increasing%0Amapping%20accuracy.%20Next%2C%20predicted%20and%20fused%20depth%20uncertainty%20propagates%20not%0Aonly%20into%20occupancy%20probabilities%20but%20also%20into%20alignment%20factors%20between%0Agenerated%20dense%20submaps%20that%20enter%20the%20probabilistic%20nonlinear%20least%20squares%0Aestimator.%20This%20submap%20representation%20offers%20globally%20consistent%20geometry%20at%0Ascale.%20Our%20method%20is%20thoroughly%20evaluated%20in%20two%20benchmark%20datasets%2C%20resulting%0Ain%20localization%20and%20mapping%20accuracy%20that%20exceeds%20the%20state%20of%20the%20art%2C%20while%0Asimultaneously%20offering%20volumetric%20occupancy%20directly%20usable%20for%20downstream%0Arobotic%20planning%20and%20control%20in%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12051v1&entry.124074799=Read"},
{"title": "A Chinese Continuous Sign Language Dataset Based on Complex Environments", "author": "Qidan Zhu and Jing Li and Fei Yuan and Jiaojiao Fan and Quan Gan", "abstract": "  The current bottleneck in continuous sign language recognition (CSLR)\nresearch lies in the fact that most publicly available datasets are limited to\nlaboratory environments or television program recordings, resulting in a single\nbackground environment with uniform lighting, which significantly deviates from\nthe diversity and complexity found in real-life scenarios. To address this\nchallenge, we have constructed a new, large-scale dataset for Chinese\ncontinuous sign language (CSL) based on complex environments, termed the\ncomplex environment - chinese sign language dataset (CE-CSL). This dataset\nencompasses 5,988 continuous CSL video clips collected from daily life scenes,\nfeaturing more than 70 different complex backgrounds to ensure\nrepresentativeness and generalization capability. To tackle the impact of\ncomplex backgrounds on CSLR performance, we propose a time-frequency network\n(TFNet) model for continuous sign language recognition. This model extracts\nframe-level features and then utilizes both temporal and spectral information\nto separately derive sequence features before fusion, aiming to achieve\nefficient and accurate CSLR. Experimental results demonstrate that our approach\nachieves significant performance improvements on the CE-CSL, validating its\neffectiveness under complex background conditions. Additionally, our proposed\nmethod has also yielded highly competitive results when applied to three\npublicly available CSL datasets.\n", "link": "http://arxiv.org/abs/2409.11960v1", "date": "2024-09-18", "relevancy": 2.697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Chinese%20Continuous%20Sign%20Language%20Dataset%20Based%20on%20Complex%20Environments&body=Title%3A%20A%20Chinese%20Continuous%20Sign%20Language%20Dataset%20Based%20on%20Complex%20Environments%0AAuthor%3A%20Qidan%20Zhu%20and%20Jing%20Li%20and%20Fei%20Yuan%20and%20Jiaojiao%20Fan%20and%20Quan%20Gan%0AAbstract%3A%20%20%20The%20current%20bottleneck%20in%20continuous%20sign%20language%20recognition%20%28CSLR%29%0Aresearch%20lies%20in%20the%20fact%20that%20most%20publicly%20available%20datasets%20are%20limited%20to%0Alaboratory%20environments%20or%20television%20program%20recordings%2C%20resulting%20in%20a%20single%0Abackground%20environment%20with%20uniform%20lighting%2C%20which%20significantly%20deviates%20from%0Athe%20diversity%20and%20complexity%20found%20in%20real-life%20scenarios.%20To%20address%20this%0Achallenge%2C%20we%20have%20constructed%20a%20new%2C%20large-scale%20dataset%20for%20Chinese%0Acontinuous%20sign%20language%20%28CSL%29%20based%20on%20complex%20environments%2C%20termed%20the%0Acomplex%20environment%20-%20chinese%20sign%20language%20dataset%20%28CE-CSL%29.%20This%20dataset%0Aencompasses%205%2C988%20continuous%20CSL%20video%20clips%20collected%20from%20daily%20life%20scenes%2C%0Afeaturing%20more%20than%2070%20different%20complex%20backgrounds%20to%20ensure%0Arepresentativeness%20and%20generalization%20capability.%20To%20tackle%20the%20impact%20of%0Acomplex%20backgrounds%20on%20CSLR%20performance%2C%20we%20propose%20a%20time-frequency%20network%0A%28TFNet%29%20model%20for%20continuous%20sign%20language%20recognition.%20This%20model%20extracts%0Aframe-level%20features%20and%20then%20utilizes%20both%20temporal%20and%20spectral%20information%0Ato%20separately%20derive%20sequence%20features%20before%20fusion%2C%20aiming%20to%20achieve%0Aefficient%20and%20accurate%20CSLR.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Aachieves%20significant%20performance%20improvements%20on%20the%20CE-CSL%2C%20validating%20its%0Aeffectiveness%20under%20complex%20background%20conditions.%20Additionally%2C%20our%20proposed%0Amethod%20has%20also%20yielded%20highly%20competitive%20results%20when%20applied%20to%20three%0Apublicly%20available%20CSL%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Chinese%2520Continuous%2520Sign%2520Language%2520Dataset%2520Based%2520on%2520Complex%2520Environments%26entry.906535625%3DQidan%2520Zhu%2520and%2520Jing%2520Li%2520and%2520Fei%2520Yuan%2520and%2520Jiaojiao%2520Fan%2520and%2520Quan%2520Gan%26entry.1292438233%3D%2520%2520The%2520current%2520bottleneck%2520in%2520continuous%2520sign%2520language%2520recognition%2520%2528CSLR%2529%250Aresearch%2520lies%2520in%2520the%2520fact%2520that%2520most%2520publicly%2520available%2520datasets%2520are%2520limited%2520to%250Alaboratory%2520environments%2520or%2520television%2520program%2520recordings%252C%2520resulting%2520in%2520a%2520single%250Abackground%2520environment%2520with%2520uniform%2520lighting%252C%2520which%2520significantly%2520deviates%2520from%250Athe%2520diversity%2520and%2520complexity%2520found%2520in%2520real-life%2520scenarios.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520have%2520constructed%2520a%2520new%252C%2520large-scale%2520dataset%2520for%2520Chinese%250Acontinuous%2520sign%2520language%2520%2528CSL%2529%2520based%2520on%2520complex%2520environments%252C%2520termed%2520the%250Acomplex%2520environment%2520-%2520chinese%2520sign%2520language%2520dataset%2520%2528CE-CSL%2529.%2520This%2520dataset%250Aencompasses%25205%252C988%2520continuous%2520CSL%2520video%2520clips%2520collected%2520from%2520daily%2520life%2520scenes%252C%250Afeaturing%2520more%2520than%252070%2520different%2520complex%2520backgrounds%2520to%2520ensure%250Arepresentativeness%2520and%2520generalization%2520capability.%2520To%2520tackle%2520the%2520impact%2520of%250Acomplex%2520backgrounds%2520on%2520CSLR%2520performance%252C%2520we%2520propose%2520a%2520time-frequency%2520network%250A%2528TFNet%2529%2520model%2520for%2520continuous%2520sign%2520language%2520recognition.%2520This%2520model%2520extracts%250Aframe-level%2520features%2520and%2520then%2520utilizes%2520both%2520temporal%2520and%2520spectral%2520information%250Ato%2520separately%2520derive%2520sequence%2520features%2520before%2520fusion%252C%2520aiming%2520to%2520achieve%250Aefficient%2520and%2520accurate%2520CSLR.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%250Aachieves%2520significant%2520performance%2520improvements%2520on%2520the%2520CE-CSL%252C%2520validating%2520its%250Aeffectiveness%2520under%2520complex%2520background%2520conditions.%2520Additionally%252C%2520our%2520proposed%250Amethod%2520has%2520also%2520yielded%2520highly%2520competitive%2520results%2520when%2520applied%2520to%2520three%250Apublicly%2520available%2520CSL%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Chinese%20Continuous%20Sign%20Language%20Dataset%20Based%20on%20Complex%20Environments&entry.906535625=Qidan%20Zhu%20and%20Jing%20Li%20and%20Fei%20Yuan%20and%20Jiaojiao%20Fan%20and%20Quan%20Gan&entry.1292438233=%20%20The%20current%20bottleneck%20in%20continuous%20sign%20language%20recognition%20%28CSLR%29%0Aresearch%20lies%20in%20the%20fact%20that%20most%20publicly%20available%20datasets%20are%20limited%20to%0Alaboratory%20environments%20or%20television%20program%20recordings%2C%20resulting%20in%20a%20single%0Abackground%20environment%20with%20uniform%20lighting%2C%20which%20significantly%20deviates%20from%0Athe%20diversity%20and%20complexity%20found%20in%20real-life%20scenarios.%20To%20address%20this%0Achallenge%2C%20we%20have%20constructed%20a%20new%2C%20large-scale%20dataset%20for%20Chinese%0Acontinuous%20sign%20language%20%28CSL%29%20based%20on%20complex%20environments%2C%20termed%20the%0Acomplex%20environment%20-%20chinese%20sign%20language%20dataset%20%28CE-CSL%29.%20This%20dataset%0Aencompasses%205%2C988%20continuous%20CSL%20video%20clips%20collected%20from%20daily%20life%20scenes%2C%0Afeaturing%20more%20than%2070%20different%20complex%20backgrounds%20to%20ensure%0Arepresentativeness%20and%20generalization%20capability.%20To%20tackle%20the%20impact%20of%0Acomplex%20backgrounds%20on%20CSLR%20performance%2C%20we%20propose%20a%20time-frequency%20network%0A%28TFNet%29%20model%20for%20continuous%20sign%20language%20recognition.%20This%20model%20extracts%0Aframe-level%20features%20and%20then%20utilizes%20both%20temporal%20and%20spectral%20information%0Ato%20separately%20derive%20sequence%20features%20before%20fusion%2C%20aiming%20to%20achieve%0Aefficient%20and%20accurate%20CSLR.%20Experimental%20results%20demonstrate%20that%20our%20approach%0Aachieves%20significant%20performance%20improvements%20on%20the%20CE-CSL%2C%20validating%20its%0Aeffectiveness%20under%20complex%20background%20conditions.%20Additionally%2C%20our%20proposed%0Amethod%20has%20also%20yielded%20highly%20competitive%20results%20when%20applied%20to%20three%0Apublicly%20available%20CSL%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11960v1&entry.124074799=Read"},
{"title": "Differentiable Collision-Supervised Tooth Arrangement Network with a\n  Decoupling Perspective", "author": "Zhihui He and Chengyuan Wang and Shidong Yang and Li Chen and Yanheng Zhou and Shuo Wang", "abstract": "  Tooth arrangement is an essential step in the digital orthodontic planning\nprocess. Existing learning-based methods use hidden teeth features to directly\nregress teeth motions, which couples target pose perception and motion\nregression. It could lead to poor perceptions of three-dimensional\ntransformation. They also ignore the possible overlaps or gaps between teeth of\npredicted dentition, which is generally unacceptable. Therefore, we propose\nDTAN, a differentiable collision-supervised tooth arrangement network,\ndecoupling predicting tasks and feature modeling. DTAN decouples the tooth\narrangement task by first predicting the hidden features of the final teeth\nposes and then using them to assist in regressing the motions between the\nbeginning and target teeth. To learn the hidden features better, DTAN also\ndecouples the teeth-hidden features into geometric and positional features,\nwhich are further supervised by feature consistency constraints. Furthermore,\nwe propose a novel differentiable collision loss function for point cloud data\nto constrain the related gestures between teeth, which can be easily extended\nto other 3D point cloud tasks. We propose an arch-width guided tooth\narrangement network, named C-DTAN, to make the results controllable. We\nconstruct three different tooth arrangement datasets and achieve drastically\nimproved performance on accuracy and speed compared with existing methods.\n", "link": "http://arxiv.org/abs/2409.11937v1", "date": "2024-09-18", "relevancy": 2.6815, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5572}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Collision-Supervised%20Tooth%20Arrangement%20Network%20with%20a%0A%20%20Decoupling%20Perspective&body=Title%3A%20Differentiable%20Collision-Supervised%20Tooth%20Arrangement%20Network%20with%20a%0A%20%20Decoupling%20Perspective%0AAuthor%3A%20Zhihui%20He%20and%20Chengyuan%20Wang%20and%20Shidong%20Yang%20and%20Li%20Chen%20and%20Yanheng%20Zhou%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20Tooth%20arrangement%20is%20an%20essential%20step%20in%20the%20digital%20orthodontic%20planning%0Aprocess.%20Existing%20learning-based%20methods%20use%20hidden%20teeth%20features%20to%20directly%0Aregress%20teeth%20motions%2C%20which%20couples%20target%20pose%20perception%20and%20motion%0Aregression.%20It%20could%20lead%20to%20poor%20perceptions%20of%20three-dimensional%0Atransformation.%20They%20also%20ignore%20the%20possible%20overlaps%20or%20gaps%20between%20teeth%20of%0Apredicted%20dentition%2C%20which%20is%20generally%20unacceptable.%20Therefore%2C%20we%20propose%0ADTAN%2C%20a%20differentiable%20collision-supervised%20tooth%20arrangement%20network%2C%0Adecoupling%20predicting%20tasks%20and%20feature%20modeling.%20DTAN%20decouples%20the%20tooth%0Aarrangement%20task%20by%20first%20predicting%20the%20hidden%20features%20of%20the%20final%20teeth%0Aposes%20and%20then%20using%20them%20to%20assist%20in%20regressing%20the%20motions%20between%20the%0Abeginning%20and%20target%20teeth.%20To%20learn%20the%20hidden%20features%20better%2C%20DTAN%20also%0Adecouples%20the%20teeth-hidden%20features%20into%20geometric%20and%20positional%20features%2C%0Awhich%20are%20further%20supervised%20by%20feature%20consistency%20constraints.%20Furthermore%2C%0Awe%20propose%20a%20novel%20differentiable%20collision%20loss%20function%20for%20point%20cloud%20data%0Ato%20constrain%20the%20related%20gestures%20between%20teeth%2C%20which%20can%20be%20easily%20extended%0Ato%20other%203D%20point%20cloud%20tasks.%20We%20propose%20an%20arch-width%20guided%20tooth%0Aarrangement%20network%2C%20named%20C-DTAN%2C%20to%20make%20the%20results%20controllable.%20We%0Aconstruct%20three%20different%20tooth%20arrangement%20datasets%20and%20achieve%20drastically%0Aimproved%20performance%20on%20accuracy%20and%20speed%20compared%20with%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Collision-Supervised%2520Tooth%2520Arrangement%2520Network%2520with%2520a%250A%2520%2520Decoupling%2520Perspective%26entry.906535625%3DZhihui%2520He%2520and%2520Chengyuan%2520Wang%2520and%2520Shidong%2520Yang%2520and%2520Li%2520Chen%2520and%2520Yanheng%2520Zhou%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520Tooth%2520arrangement%2520is%2520an%2520essential%2520step%2520in%2520the%2520digital%2520orthodontic%2520planning%250Aprocess.%2520Existing%2520learning-based%2520methods%2520use%2520hidden%2520teeth%2520features%2520to%2520directly%250Aregress%2520teeth%2520motions%252C%2520which%2520couples%2520target%2520pose%2520perception%2520and%2520motion%250Aregression.%2520It%2520could%2520lead%2520to%2520poor%2520perceptions%2520of%2520three-dimensional%250Atransformation.%2520They%2520also%2520ignore%2520the%2520possible%2520overlaps%2520or%2520gaps%2520between%2520teeth%2520of%250Apredicted%2520dentition%252C%2520which%2520is%2520generally%2520unacceptable.%2520Therefore%252C%2520we%2520propose%250ADTAN%252C%2520a%2520differentiable%2520collision-supervised%2520tooth%2520arrangement%2520network%252C%250Adecoupling%2520predicting%2520tasks%2520and%2520feature%2520modeling.%2520DTAN%2520decouples%2520the%2520tooth%250Aarrangement%2520task%2520by%2520first%2520predicting%2520the%2520hidden%2520features%2520of%2520the%2520final%2520teeth%250Aposes%2520and%2520then%2520using%2520them%2520to%2520assist%2520in%2520regressing%2520the%2520motions%2520between%2520the%250Abeginning%2520and%2520target%2520teeth.%2520To%2520learn%2520the%2520hidden%2520features%2520better%252C%2520DTAN%2520also%250Adecouples%2520the%2520teeth-hidden%2520features%2520into%2520geometric%2520and%2520positional%2520features%252C%250Awhich%2520are%2520further%2520supervised%2520by%2520feature%2520consistency%2520constraints.%2520Furthermore%252C%250Awe%2520propose%2520a%2520novel%2520differentiable%2520collision%2520loss%2520function%2520for%2520point%2520cloud%2520data%250Ato%2520constrain%2520the%2520related%2520gestures%2520between%2520teeth%252C%2520which%2520can%2520be%2520easily%2520extended%250Ato%2520other%25203D%2520point%2520cloud%2520tasks.%2520We%2520propose%2520an%2520arch-width%2520guided%2520tooth%250Aarrangement%2520network%252C%2520named%2520C-DTAN%252C%2520to%2520make%2520the%2520results%2520controllable.%2520We%250Aconstruct%2520three%2520different%2520tooth%2520arrangement%2520datasets%2520and%2520achieve%2520drastically%250Aimproved%2520performance%2520on%2520accuracy%2520and%2520speed%2520compared%2520with%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Collision-Supervised%20Tooth%20Arrangement%20Network%20with%20a%0A%20%20Decoupling%20Perspective&entry.906535625=Zhihui%20He%20and%20Chengyuan%20Wang%20and%20Shidong%20Yang%20and%20Li%20Chen%20and%20Yanheng%20Zhou%20and%20Shuo%20Wang&entry.1292438233=%20%20Tooth%20arrangement%20is%20an%20essential%20step%20in%20the%20digital%20orthodontic%20planning%0Aprocess.%20Existing%20learning-based%20methods%20use%20hidden%20teeth%20features%20to%20directly%0Aregress%20teeth%20motions%2C%20which%20couples%20target%20pose%20perception%20and%20motion%0Aregression.%20It%20could%20lead%20to%20poor%20perceptions%20of%20three-dimensional%0Atransformation.%20They%20also%20ignore%20the%20possible%20overlaps%20or%20gaps%20between%20teeth%20of%0Apredicted%20dentition%2C%20which%20is%20generally%20unacceptable.%20Therefore%2C%20we%20propose%0ADTAN%2C%20a%20differentiable%20collision-supervised%20tooth%20arrangement%20network%2C%0Adecoupling%20predicting%20tasks%20and%20feature%20modeling.%20DTAN%20decouples%20the%20tooth%0Aarrangement%20task%20by%20first%20predicting%20the%20hidden%20features%20of%20the%20final%20teeth%0Aposes%20and%20then%20using%20them%20to%20assist%20in%20regressing%20the%20motions%20between%20the%0Abeginning%20and%20target%20teeth.%20To%20learn%20the%20hidden%20features%20better%2C%20DTAN%20also%0Adecouples%20the%20teeth-hidden%20features%20into%20geometric%20and%20positional%20features%2C%0Awhich%20are%20further%20supervised%20by%20feature%20consistency%20constraints.%20Furthermore%2C%0Awe%20propose%20a%20novel%20differentiable%20collision%20loss%20function%20for%20point%20cloud%20data%0Ato%20constrain%20the%20related%20gestures%20between%20teeth%2C%20which%20can%20be%20easily%20extended%0Ato%20other%203D%20point%20cloud%20tasks.%20We%20propose%20an%20arch-width%20guided%20tooth%0Aarrangement%20network%2C%20named%20C-DTAN%2C%20to%20make%20the%20results%20controllable.%20We%0Aconstruct%20three%20different%20tooth%20arrangement%20datasets%20and%20achieve%20drastically%0Aimproved%20performance%20on%20accuracy%20and%20speed%20compared%20with%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11937v1&entry.124074799=Read"},
{"title": "Dual-Layer Training and Decoding of Large Language Model with\n  Simultaneously Thinking and Speaking", "author": "Ningyuan Xi and Xiaoyu Wang and Yetao Wu and Teng Chen and Qingqing Gu and Jinxian Qu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.\n", "link": "http://arxiv.org/abs/2409.12059v1", "date": "2024-09-18", "relevancy": 2.6349, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking&body=Title%3A%20Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking%0AAuthor%3A%20Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Layer%2520Training%2520and%2520Decoding%2520of%2520Large%2520Language%2520Model%2520with%250A%2520%2520Simultaneously%2520Thinking%2520and%2520Speaking%26entry.906535625%3DNingyuan%2520Xi%2520and%2520Xiaoyu%2520Wang%2520and%2520Yetao%2520Wu%2520and%2520Teng%2520Chen%2520and%2520Qingqing%2520Gu%2520and%2520Jinxian%2520Qu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520can%2520reasonably%2520understand%2520and%2520generate%2520human%2520expressions%250Abut%2520may%2520lack%2520of%2520thorough%2520thinking%2520and%2520reasoning%2520mechanisms.%2520Recently%2520there%2520have%250Abeen%2520several%2520studies%2520which%2520enhance%2520the%2520thinking%2520ability%2520of%2520language%2520models%2520but%250Amost%2520of%2520them%2520are%2520not%2520data-driven%2520or%2520training-based.%2520In%2520this%2520paper%252C%2520we%2520are%250Amotivated%2520by%2520the%2520cognitive%2520mechanism%2520in%2520the%2520natural%2520world%252C%2520and%2520design%2520a%2520novel%250Amodel%2520architecture%2520called%2520TaS%2520which%2520allows%2520it%2520to%2520first%2520consider%2520the%2520thoughts%250Aand%2520then%2520express%2520the%2520response%2520based%2520upon%2520the%2520query.%2520We%2520design%2520several%2520pipelines%250Ato%2520annotate%2520or%2520generate%2520the%2520thought%2520contents%2520from%2520prompt-response%2520samples%252C%2520then%250Aadd%2520language%2520heads%2520in%2520a%2520middle%2520layer%2520which%2520behaves%2520as%2520the%2520thinking%2520layer.%2520We%250Atrain%2520the%2520language%2520model%2520by%2520the%2520thoughts-augmented%2520data%2520and%2520successfully%2520let%250Athe%2520thinking%2520layer%2520automatically%2520generate%2520reasonable%2520thoughts%2520and%2520finally%250Aoutput%2520more%2520reasonable%2520responses.%2520Both%2520qualitative%2520examples%2520and%2520quantitative%250Aresults%2520validate%2520the%2520effectiveness%2520and%2520performance%2520of%2520TaS.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//anonymous.4open.science/r/TadE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Layer%20Training%20and%20Decoding%20of%20Large%20Language%20Model%20with%0A%20%20Simultaneously%20Thinking%20and%20Speaking&entry.906535625=Ningyuan%20Xi%20and%20Xiaoyu%20Wang%20and%20Yetao%20Wu%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Jinxian%20Qu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Large%20Language%20Model%20can%20reasonably%20understand%20and%20generate%20human%20expressions%0Abut%20may%20lack%20of%20thorough%20thinking%20and%20reasoning%20mechanisms.%20Recently%20there%20have%0Abeen%20several%20studies%20which%20enhance%20the%20thinking%20ability%20of%20language%20models%20but%0Amost%20of%20them%20are%20not%20data-driven%20or%20training-based.%20In%20this%20paper%2C%20we%20are%0Amotivated%20by%20the%20cognitive%20mechanism%20in%20the%20natural%20world%2C%20and%20design%20a%20novel%0Amodel%20architecture%20called%20TaS%20which%20allows%20it%20to%20first%20consider%20the%20thoughts%0Aand%20then%20express%20the%20response%20based%20upon%20the%20query.%20We%20design%20several%20pipelines%0Ato%20annotate%20or%20generate%20the%20thought%20contents%20from%20prompt-response%20samples%2C%20then%0Aadd%20language%20heads%20in%20a%20middle%20layer%20which%20behaves%20as%20the%20thinking%20layer.%20We%0Atrain%20the%20language%20model%20by%20the%20thoughts-augmented%20data%20and%20successfully%20let%0Athe%20thinking%20layer%20automatically%20generate%20reasonable%20thoughts%20and%20finally%0Aoutput%20more%20reasonable%20responses.%20Both%20qualitative%20examples%20and%20quantitative%0Aresults%20validate%20the%20effectiveness%20and%20performance%20of%20TaS.%20Our%20code%20is%0Aavailable%20at%20https%3A//anonymous.4open.science/r/TadE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12059v1&entry.124074799=Read"},
{"title": "Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations", "author": "Haeyong Kang and Jaehong Yoon and Sung Ju Hwang and Chang D. Yoo", "abstract": "  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n", "link": "http://arxiv.org/abs/2312.11973v5", "date": "2024-09-18", "relevancy": 2.6281, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5392}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%3A%20Forget-free%20Winning%20Subnetworks%20for%20Video%0A%20%20Representations&body=Title%3A%20Continual%20Learning%3A%20Forget-free%20Winning%20Subnetworks%20for%20Video%0A%20%20Representations%0AAuthor%3A%20Haeyong%20Kang%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20Inspired%20by%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%2C%20which%20highlights%20the%0Aexistence%20of%20efficient%20subnetworks%20within%20larger%2C%20dense%20networks%2C%20a%0Ahigh-performing%20Winning%20Subnetwork%20%28WSN%29%20in%20terms%20of%20task%20performance%20under%0Aappropriate%20sparsity%20conditions%20is%20considered%20for%20various%20continual%20learning%0Atasks.%20It%20leverages%20pre-existing%20weights%20from%20dense%20networks%20to%20achieve%0Aefficient%20learning%20in%20Task%20Incremental%20Learning%20%28TIL%29%20and%20Task-agnostic%0AIncremental%20Learning%20%28TaIL%29%20scenarios.%20In%20Few-Shot%20Class%20Incremental%20Learning%0A%28FSCIL%29%2C%20a%20variation%20of%20WSN%20referred%20to%20as%20the%20Soft%20subnetwork%20%28SoftNet%29%20is%0Adesigned%20to%20prevent%20overfitting%20when%20the%20data%20samples%20are%20scarce.%20Furthermore%2C%0Athe%20sparse%20reuse%20of%20WSN%20weights%20is%20considered%20for%20Video%20Incremental%20Learning%0A%28VIL%29.%20The%20use%20of%20Fourier%20Subneural%20Operator%20%28FSO%29%20within%20WSN%20is%20considered.%20It%0Aenables%20compact%20encoding%20of%20videos%20and%20identifies%20reusable%20subnetworks%20across%0Avarying%20bandwidths.%20We%20have%20integrated%20FSO%20into%20different%20architectural%0Aframeworks%20for%20continual%20learning%2C%20including%20VIL%2C%20TIL%2C%20and%20FSCIL.%20Our%0Acomprehensive%20experiments%20demonstrate%20FSO%27s%20effectiveness%2C%20significantly%0Aimproving%20task%20performance%20at%20various%20convolutional%20representational%20levels.%0ASpecifically%2C%20FSO%20enhances%20higher-layer%20performance%20in%20TIL%20and%20FSCIL%20and%0Alower-layer%20performance%20in%20VIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11973v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%253A%2520Forget-free%2520Winning%2520Subnetworks%2520for%2520Video%250A%2520%2520Representations%26entry.906535625%3DHaeyong%2520Kang%2520and%2520Jaehong%2520Yoon%2520and%2520Sung%2520Ju%2520Hwang%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520Lottery%2520Ticket%2520Hypothesis%2520%2528LTH%2529%252C%2520which%2520highlights%2520the%250Aexistence%2520of%2520efficient%2520subnetworks%2520within%2520larger%252C%2520dense%2520networks%252C%2520a%250Ahigh-performing%2520Winning%2520Subnetwork%2520%2528WSN%2529%2520in%2520terms%2520of%2520task%2520performance%2520under%250Aappropriate%2520sparsity%2520conditions%2520is%2520considered%2520for%2520various%2520continual%2520learning%250Atasks.%2520It%2520leverages%2520pre-existing%2520weights%2520from%2520dense%2520networks%2520to%2520achieve%250Aefficient%2520learning%2520in%2520Task%2520Incremental%2520Learning%2520%2528TIL%2529%2520and%2520Task-agnostic%250AIncremental%2520Learning%2520%2528TaIL%2529%2520scenarios.%2520In%2520Few-Shot%2520Class%2520Incremental%2520Learning%250A%2528FSCIL%2529%252C%2520a%2520variation%2520of%2520WSN%2520referred%2520to%2520as%2520the%2520Soft%2520subnetwork%2520%2528SoftNet%2529%2520is%250Adesigned%2520to%2520prevent%2520overfitting%2520when%2520the%2520data%2520samples%2520are%2520scarce.%2520Furthermore%252C%250Athe%2520sparse%2520reuse%2520of%2520WSN%2520weights%2520is%2520considered%2520for%2520Video%2520Incremental%2520Learning%250A%2528VIL%2529.%2520The%2520use%2520of%2520Fourier%2520Subneural%2520Operator%2520%2528FSO%2529%2520within%2520WSN%2520is%2520considered.%2520It%250Aenables%2520compact%2520encoding%2520of%2520videos%2520and%2520identifies%2520reusable%2520subnetworks%2520across%250Avarying%2520bandwidths.%2520We%2520have%2520integrated%2520FSO%2520into%2520different%2520architectural%250Aframeworks%2520for%2520continual%2520learning%252C%2520including%2520VIL%252C%2520TIL%252C%2520and%2520FSCIL.%2520Our%250Acomprehensive%2520experiments%2520demonstrate%2520FSO%2527s%2520effectiveness%252C%2520significantly%250Aimproving%2520task%2520performance%2520at%2520various%2520convolutional%2520representational%2520levels.%250ASpecifically%252C%2520FSO%2520enhances%2520higher-layer%2520performance%2520in%2520TIL%2520and%2520FSCIL%2520and%250Alower-layer%2520performance%2520in%2520VIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11973v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%3A%20Forget-free%20Winning%20Subnetworks%20for%20Video%0A%20%20Representations&entry.906535625=Haeyong%20Kang%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20Inspired%20by%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29%2C%20which%20highlights%20the%0Aexistence%20of%20efficient%20subnetworks%20within%20larger%2C%20dense%20networks%2C%20a%0Ahigh-performing%20Winning%20Subnetwork%20%28WSN%29%20in%20terms%20of%20task%20performance%20under%0Aappropriate%20sparsity%20conditions%20is%20considered%20for%20various%20continual%20learning%0Atasks.%20It%20leverages%20pre-existing%20weights%20from%20dense%20networks%20to%20achieve%0Aefficient%20learning%20in%20Task%20Incremental%20Learning%20%28TIL%29%20and%20Task-agnostic%0AIncremental%20Learning%20%28TaIL%29%20scenarios.%20In%20Few-Shot%20Class%20Incremental%20Learning%0A%28FSCIL%29%2C%20a%20variation%20of%20WSN%20referred%20to%20as%20the%20Soft%20subnetwork%20%28SoftNet%29%20is%0Adesigned%20to%20prevent%20overfitting%20when%20the%20data%20samples%20are%20scarce.%20Furthermore%2C%0Athe%20sparse%20reuse%20of%20WSN%20weights%20is%20considered%20for%20Video%20Incremental%20Learning%0A%28VIL%29.%20The%20use%20of%20Fourier%20Subneural%20Operator%20%28FSO%29%20within%20WSN%20is%20considered.%20It%0Aenables%20compact%20encoding%20of%20videos%20and%20identifies%20reusable%20subnetworks%20across%0Avarying%20bandwidths.%20We%20have%20integrated%20FSO%20into%20different%20architectural%0Aframeworks%20for%20continual%20learning%2C%20including%20VIL%2C%20TIL%2C%20and%20FSCIL.%20Our%0Acomprehensive%20experiments%20demonstrate%20FSO%27s%20effectiveness%2C%20significantly%0Aimproving%20task%20performance%20at%20various%20convolutional%20representational%20levels.%0ASpecifically%2C%20FSO%20enhances%20higher-layer%20performance%20in%20TIL%20and%20FSCIL%20and%0Alower-layer%20performance%20in%20VIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11973v5&entry.124074799=Read"},
{"title": "PhysMamba: Efficient Remote Physiological Measurement with SlowFast\n  Temporal Difference Mamba", "author": "Chaoqi Luo and Yiping Xie and Zitong Yu", "abstract": "  Facial-video based Remote photoplethysmography (rPPG) aims at measuring\nphysiological signals and monitoring heart activity without any contact,\nshowing significant potential in various applications. Previous deep learning\nbased rPPG measurement are primarily based on CNNs and Transformers. However,\nthe limited receptive fields of CNNs restrict their ability to capture\nlong-range spatio-temporal dependencies, while Transformers also struggle with\nmodeling long video sequences with high complexity. Recently, the state space\nmodels (SSMs) represented by Mamba are known for their impressive performance\non capturing long-range dependencies from long sequences. In this paper, we\npropose the PhysMamba, a Mamba-based framework, to efficiently represent\nlong-range physiological dependencies from facial videos. Specifically, we\nintroduce the Temporal Difference Mamba block to first enhance local dynamic\ndifferences and further model the long-range spatio-temporal context. Moreover,\na dual-stream SlowFast architecture is utilized to fuse the multi-scale\ntemporal features. Extensive experiments are conducted on three benchmark\ndatasets to demonstrate the superiority and efficiency of PhysMamba. The codes\nare available at https://github.com/Chaoqi31/PhysMamba\n", "link": "http://arxiv.org/abs/2409.12031v1", "date": "2024-09-18", "relevancy": 2.6253, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5288}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5256}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysMamba%3A%20Efficient%20Remote%20Physiological%20Measurement%20with%20SlowFast%0A%20%20Temporal%20Difference%20Mamba&body=Title%3A%20PhysMamba%3A%20Efficient%20Remote%20Physiological%20Measurement%20with%20SlowFast%0A%20%20Temporal%20Difference%20Mamba%0AAuthor%3A%20Chaoqi%20Luo%20and%20Yiping%20Xie%20and%20Zitong%20Yu%0AAbstract%3A%20%20%20Facial-video%20based%20Remote%20photoplethysmography%20%28rPPG%29%20aims%20at%20measuring%0Aphysiological%20signals%20and%20monitoring%20heart%20activity%20without%20any%20contact%2C%0Ashowing%20significant%20potential%20in%20various%20applications.%20Previous%20deep%20learning%0Abased%20rPPG%20measurement%20are%20primarily%20based%20on%20CNNs%20and%20Transformers.%20However%2C%0Athe%20limited%20receptive%20fields%20of%20CNNs%20restrict%20their%20ability%20to%20capture%0Along-range%20spatio-temporal%20dependencies%2C%20while%20Transformers%20also%20struggle%20with%0Amodeling%20long%20video%20sequences%20with%20high%20complexity.%20Recently%2C%20the%20state%20space%0Amodels%20%28SSMs%29%20represented%20by%20Mamba%20are%20known%20for%20their%20impressive%20performance%0Aon%20capturing%20long-range%20dependencies%20from%20long%20sequences.%20In%20this%20paper%2C%20we%0Apropose%20the%20PhysMamba%2C%20a%20Mamba-based%20framework%2C%20to%20efficiently%20represent%0Along-range%20physiological%20dependencies%20from%20facial%20videos.%20Specifically%2C%20we%0Aintroduce%20the%20Temporal%20Difference%20Mamba%20block%20to%20first%20enhance%20local%20dynamic%0Adifferences%20and%20further%20model%20the%20long-range%20spatio-temporal%20context.%20Moreover%2C%0Aa%20dual-stream%20SlowFast%20architecture%20is%20utilized%20to%20fuse%20the%20multi-scale%0Atemporal%20features.%20Extensive%20experiments%20are%20conducted%20on%20three%20benchmark%0Adatasets%20to%20demonstrate%20the%20superiority%20and%20efficiency%20of%20PhysMamba.%20The%20codes%0Aare%20available%20at%20https%3A//github.com/Chaoqi31/PhysMamba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysMamba%253A%2520Efficient%2520Remote%2520Physiological%2520Measurement%2520with%2520SlowFast%250A%2520%2520Temporal%2520Difference%2520Mamba%26entry.906535625%3DChaoqi%2520Luo%2520and%2520Yiping%2520Xie%2520and%2520Zitong%2520Yu%26entry.1292438233%3D%2520%2520Facial-video%2520based%2520Remote%2520photoplethysmography%2520%2528rPPG%2529%2520aims%2520at%2520measuring%250Aphysiological%2520signals%2520and%2520monitoring%2520heart%2520activity%2520without%2520any%2520contact%252C%250Ashowing%2520significant%2520potential%2520in%2520various%2520applications.%2520Previous%2520deep%2520learning%250Abased%2520rPPG%2520measurement%2520are%2520primarily%2520based%2520on%2520CNNs%2520and%2520Transformers.%2520However%252C%250Athe%2520limited%2520receptive%2520fields%2520of%2520CNNs%2520restrict%2520their%2520ability%2520to%2520capture%250Along-range%2520spatio-temporal%2520dependencies%252C%2520while%2520Transformers%2520also%2520struggle%2520with%250Amodeling%2520long%2520video%2520sequences%2520with%2520high%2520complexity.%2520Recently%252C%2520the%2520state%2520space%250Amodels%2520%2528SSMs%2529%2520represented%2520by%2520Mamba%2520are%2520known%2520for%2520their%2520impressive%2520performance%250Aon%2520capturing%2520long-range%2520dependencies%2520from%2520long%2520sequences.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520the%2520PhysMamba%252C%2520a%2520Mamba-based%2520framework%252C%2520to%2520efficiently%2520represent%250Along-range%2520physiological%2520dependencies%2520from%2520facial%2520videos.%2520Specifically%252C%2520we%250Aintroduce%2520the%2520Temporal%2520Difference%2520Mamba%2520block%2520to%2520first%2520enhance%2520local%2520dynamic%250Adifferences%2520and%2520further%2520model%2520the%2520long-range%2520spatio-temporal%2520context.%2520Moreover%252C%250Aa%2520dual-stream%2520SlowFast%2520architecture%2520is%2520utilized%2520to%2520fuse%2520the%2520multi-scale%250Atemporal%2520features.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520three%2520benchmark%250Adatasets%2520to%2520demonstrate%2520the%2520superiority%2520and%2520efficiency%2520of%2520PhysMamba.%2520The%2520codes%250Aare%2520available%2520at%2520https%253A//github.com/Chaoqi31/PhysMamba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysMamba%3A%20Efficient%20Remote%20Physiological%20Measurement%20with%20SlowFast%0A%20%20Temporal%20Difference%20Mamba&entry.906535625=Chaoqi%20Luo%20and%20Yiping%20Xie%20and%20Zitong%20Yu&entry.1292438233=%20%20Facial-video%20based%20Remote%20photoplethysmography%20%28rPPG%29%20aims%20at%20measuring%0Aphysiological%20signals%20and%20monitoring%20heart%20activity%20without%20any%20contact%2C%0Ashowing%20significant%20potential%20in%20various%20applications.%20Previous%20deep%20learning%0Abased%20rPPG%20measurement%20are%20primarily%20based%20on%20CNNs%20and%20Transformers.%20However%2C%0Athe%20limited%20receptive%20fields%20of%20CNNs%20restrict%20their%20ability%20to%20capture%0Along-range%20spatio-temporal%20dependencies%2C%20while%20Transformers%20also%20struggle%20with%0Amodeling%20long%20video%20sequences%20with%20high%20complexity.%20Recently%2C%20the%20state%20space%0Amodels%20%28SSMs%29%20represented%20by%20Mamba%20are%20known%20for%20their%20impressive%20performance%0Aon%20capturing%20long-range%20dependencies%20from%20long%20sequences.%20In%20this%20paper%2C%20we%0Apropose%20the%20PhysMamba%2C%20a%20Mamba-based%20framework%2C%20to%20efficiently%20represent%0Along-range%20physiological%20dependencies%20from%20facial%20videos.%20Specifically%2C%20we%0Aintroduce%20the%20Temporal%20Difference%20Mamba%20block%20to%20first%20enhance%20local%20dynamic%0Adifferences%20and%20further%20model%20the%20long-range%20spatio-temporal%20context.%20Moreover%2C%0Aa%20dual-stream%20SlowFast%20architecture%20is%20utilized%20to%20fuse%20the%20multi-scale%0Atemporal%20features.%20Extensive%20experiments%20are%20conducted%20on%20three%20benchmark%0Adatasets%20to%20demonstrate%20the%20superiority%20and%20efficiency%20of%20PhysMamba.%20The%20codes%0Aare%20available%20at%20https%3A//github.com/Chaoqi31/PhysMamba%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12031v1&entry.124074799=Read"},
{"title": "DocMamba: Efficient Document Pre-training with State Space Model", "author": "Pengfei Hu and Zhenrong Zhang and Jiefeng Ma and Shuhang Liu and Jun Du and Jianshu Zhang", "abstract": "  In recent years, visually-rich document understanding has attracted\nincreasing attention. Transformer-based pre-trained models have become the\nmainstream approach, yielding significant performance gains in this field.\nHowever, the self-attention mechanism's quadratic computational complexity\nhinders their efficiency and ability to process long documents. In this paper,\nwe present DocMamba, a novel framework based on the state space model. It is\ndesigned to reduce computational complexity to linear while preserving global\nmodeling capabilities. To further enhance its effectiveness in document\nprocessing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture\ncontiguous semantic information. Experimental results demonstrate that DocMamba\nachieves new state-of-the-art results on downstream datasets such as FUNSD,\nCORD, and SORIE, while significantly improving speed and reducing memory usage.\nNotably, experiments on the HRDoc confirm DocMamba's potential for length\nextrapolation. The code will be available online.\n", "link": "http://arxiv.org/abs/2409.11887v1", "date": "2024-09-18", "relevancy": 2.6196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5354}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocMamba%3A%20Efficient%20Document%20Pre-training%20with%20State%20Space%20Model&body=Title%3A%20DocMamba%3A%20Efficient%20Document%20Pre-training%20with%20State%20Space%20Model%0AAuthor%3A%20Pengfei%20Hu%20and%20Zhenrong%20Zhang%20and%20Jiefeng%20Ma%20and%20Shuhang%20Liu%20and%20Jun%20Du%20and%20Jianshu%20Zhang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20visually-rich%20document%20understanding%20has%20attracted%0Aincreasing%20attention.%20Transformer-based%20pre-trained%20models%20have%20become%20the%0Amainstream%20approach%2C%20yielding%20significant%20performance%20gains%20in%20this%20field.%0AHowever%2C%20the%20self-attention%20mechanism%27s%20quadratic%20computational%20complexity%0Ahinders%20their%20efficiency%20and%20ability%20to%20process%20long%20documents.%20In%20this%20paper%2C%0Awe%20present%20DocMamba%2C%20a%20novel%20framework%20based%20on%20the%20state%20space%20model.%20It%20is%0Adesigned%20to%20reduce%20computational%20complexity%20to%20linear%20while%20preserving%20global%0Amodeling%20capabilities.%20To%20further%20enhance%20its%20effectiveness%20in%20document%0Aprocessing%2C%20we%20introduce%20the%20Segment-First%20Bidirectional%20Scan%20%28SFBS%29%20to%20capture%0Acontiguous%20semantic%20information.%20Experimental%20results%20demonstrate%20that%20DocMamba%0Aachieves%20new%20state-of-the-art%20results%20on%20downstream%20datasets%20such%20as%20FUNSD%2C%0ACORD%2C%20and%20SORIE%2C%20while%20significantly%20improving%20speed%20and%20reducing%20memory%20usage.%0ANotably%2C%20experiments%20on%20the%20HRDoc%20confirm%20DocMamba%27s%20potential%20for%20length%0Aextrapolation.%20The%20code%20will%20be%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocMamba%253A%2520Efficient%2520Document%2520Pre-training%2520with%2520State%2520Space%2520Model%26entry.906535625%3DPengfei%2520Hu%2520and%2520Zhenrong%2520Zhang%2520and%2520Jiefeng%2520Ma%2520and%2520Shuhang%2520Liu%2520and%2520Jun%2520Du%2520and%2520Jianshu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520visually-rich%2520document%2520understanding%2520has%2520attracted%250Aincreasing%2520attention.%2520Transformer-based%2520pre-trained%2520models%2520have%2520become%2520the%250Amainstream%2520approach%252C%2520yielding%2520significant%2520performance%2520gains%2520in%2520this%2520field.%250AHowever%252C%2520the%2520self-attention%2520mechanism%2527s%2520quadratic%2520computational%2520complexity%250Ahinders%2520their%2520efficiency%2520and%2520ability%2520to%2520process%2520long%2520documents.%2520In%2520this%2520paper%252C%250Awe%2520present%2520DocMamba%252C%2520a%2520novel%2520framework%2520based%2520on%2520the%2520state%2520space%2520model.%2520It%2520is%250Adesigned%2520to%2520reduce%2520computational%2520complexity%2520to%2520linear%2520while%2520preserving%2520global%250Amodeling%2520capabilities.%2520To%2520further%2520enhance%2520its%2520effectiveness%2520in%2520document%250Aprocessing%252C%2520we%2520introduce%2520the%2520Segment-First%2520Bidirectional%2520Scan%2520%2528SFBS%2529%2520to%2520capture%250Acontiguous%2520semantic%2520information.%2520Experimental%2520results%2520demonstrate%2520that%2520DocMamba%250Aachieves%2520new%2520state-of-the-art%2520results%2520on%2520downstream%2520datasets%2520such%2520as%2520FUNSD%252C%250ACORD%252C%2520and%2520SORIE%252C%2520while%2520significantly%2520improving%2520speed%2520and%2520reducing%2520memory%2520usage.%250ANotably%252C%2520experiments%2520on%2520the%2520HRDoc%2520confirm%2520DocMamba%2527s%2520potential%2520for%2520length%250Aextrapolation.%2520The%2520code%2520will%2520be%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocMamba%3A%20Efficient%20Document%20Pre-training%20with%20State%20Space%20Model&entry.906535625=Pengfei%20Hu%20and%20Zhenrong%20Zhang%20and%20Jiefeng%20Ma%20and%20Shuhang%20Liu%20and%20Jun%20Du%20and%20Jianshu%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20visually-rich%20document%20understanding%20has%20attracted%0Aincreasing%20attention.%20Transformer-based%20pre-trained%20models%20have%20become%20the%0Amainstream%20approach%2C%20yielding%20significant%20performance%20gains%20in%20this%20field.%0AHowever%2C%20the%20self-attention%20mechanism%27s%20quadratic%20computational%20complexity%0Ahinders%20their%20efficiency%20and%20ability%20to%20process%20long%20documents.%20In%20this%20paper%2C%0Awe%20present%20DocMamba%2C%20a%20novel%20framework%20based%20on%20the%20state%20space%20model.%20It%20is%0Adesigned%20to%20reduce%20computational%20complexity%20to%20linear%20while%20preserving%20global%0Amodeling%20capabilities.%20To%20further%20enhance%20its%20effectiveness%20in%20document%0Aprocessing%2C%20we%20introduce%20the%20Segment-First%20Bidirectional%20Scan%20%28SFBS%29%20to%20capture%0Acontiguous%20semantic%20information.%20Experimental%20results%20demonstrate%20that%20DocMamba%0Aachieves%20new%20state-of-the-art%20results%20on%20downstream%20datasets%20such%20as%20FUNSD%2C%0ACORD%2C%20and%20SORIE%2C%20while%20significantly%20improving%20speed%20and%20reducing%20memory%20usage.%0ANotably%2C%20experiments%20on%20the%20HRDoc%20confirm%20DocMamba%27s%20potential%20for%20length%0Aextrapolation.%20The%20code%20will%20be%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11887v1&entry.124074799=Read"},
{"title": "Tracking Any Point with Frame-Event Fusion Network at High Frame Rate", "author": "Jiaxiong Liu and Bo Wang and Zhen Tan and Jinpu Zhang and Hui Shen and Dewen Hu", "abstract": "  Tracking any point based on image frames is constrained by frame rates,\nleading to instability in high-speed scenarios and limited generalization in\nreal-world applications. To overcome these limitations, we propose an\nimage-event fusion point tracker, FE-TAP, which combines the contextual\ninformation from image frames with the high temporal resolution of events,\nachieving high frame rate and robust point tracking under various challenging\nconditions. Specifically, we designed an Evolution Fusion module (EvoFusion) to\nmodel the image generation process guided by events. This module can\neffectively integrate valuable information from both modalities operating at\ndifferent frequencies. To achieve smoother point trajectories, we employed a\ntransformer-based refinement strategy that updates the point's trajectories and\nfeatures iteratively. Extensive experiments demonstrate that our method\noutperforms state-of-the-art approaches, particularly improving expected\nfeature age by 24$\\%$ on EDS datasets. Finally, we qualitatively validated the\nrobustness of our algorithm in real driving scenarios using our custom-designed\nhigh-resolution image-event synchronization device. Our source code will be\nreleased at https://github.com/ljx1002/FE-TAP.\n", "link": "http://arxiv.org/abs/2409.11953v1", "date": "2024-09-18", "relevancy": 2.6173, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5289}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5209}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20Any%20Point%20with%20Frame-Event%20Fusion%20Network%20at%20High%20Frame%20Rate&body=Title%3A%20Tracking%20Any%20Point%20with%20Frame-Event%20Fusion%20Network%20at%20High%20Frame%20Rate%0AAuthor%3A%20Jiaxiong%20Liu%20and%20Bo%20Wang%20and%20Zhen%20Tan%20and%20Jinpu%20Zhang%20and%20Hui%20Shen%20and%20Dewen%20Hu%0AAbstract%3A%20%20%20Tracking%20any%20point%20based%20on%20image%20frames%20is%20constrained%20by%20frame%20rates%2C%0Aleading%20to%20instability%20in%20high-speed%20scenarios%20and%20limited%20generalization%20in%0Areal-world%20applications.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Aimage-event%20fusion%20point%20tracker%2C%20FE-TAP%2C%20which%20combines%20the%20contextual%0Ainformation%20from%20image%20frames%20with%20the%20high%20temporal%20resolution%20of%20events%2C%0Aachieving%20high%20frame%20rate%20and%20robust%20point%20tracking%20under%20various%20challenging%0Aconditions.%20Specifically%2C%20we%20designed%20an%20Evolution%20Fusion%20module%20%28EvoFusion%29%20to%0Amodel%20the%20image%20generation%20process%20guided%20by%20events.%20This%20module%20can%0Aeffectively%20integrate%20valuable%20information%20from%20both%20modalities%20operating%20at%0Adifferent%20frequencies.%20To%20achieve%20smoother%20point%20trajectories%2C%20we%20employed%20a%0Atransformer-based%20refinement%20strategy%20that%20updates%20the%20point%27s%20trajectories%20and%0Afeatures%20iteratively.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20approaches%2C%20particularly%20improving%20expected%0Afeature%20age%20by%2024%24%5C%25%24%20on%20EDS%20datasets.%20Finally%2C%20we%20qualitatively%20validated%20the%0Arobustness%20of%20our%20algorithm%20in%20real%20driving%20scenarios%20using%20our%20custom-designed%0Ahigh-resolution%20image-event%20synchronization%20device.%20Our%20source%20code%20will%20be%0Areleased%20at%20https%3A//github.com/ljx1002/FE-TAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520Any%2520Point%2520with%2520Frame-Event%2520Fusion%2520Network%2520at%2520High%2520Frame%2520Rate%26entry.906535625%3DJiaxiong%2520Liu%2520and%2520Bo%2520Wang%2520and%2520Zhen%2520Tan%2520and%2520Jinpu%2520Zhang%2520and%2520Hui%2520Shen%2520and%2520Dewen%2520Hu%26entry.1292438233%3D%2520%2520Tracking%2520any%2520point%2520based%2520on%2520image%2520frames%2520is%2520constrained%2520by%2520frame%2520rates%252C%250Aleading%2520to%2520instability%2520in%2520high-speed%2520scenarios%2520and%2520limited%2520generalization%2520in%250Areal-world%2520applications.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520an%250Aimage-event%2520fusion%2520point%2520tracker%252C%2520FE-TAP%252C%2520which%2520combines%2520the%2520contextual%250Ainformation%2520from%2520image%2520frames%2520with%2520the%2520high%2520temporal%2520resolution%2520of%2520events%252C%250Aachieving%2520high%2520frame%2520rate%2520and%2520robust%2520point%2520tracking%2520under%2520various%2520challenging%250Aconditions.%2520Specifically%252C%2520we%2520designed%2520an%2520Evolution%2520Fusion%2520module%2520%2528EvoFusion%2529%2520to%250Amodel%2520the%2520image%2520generation%2520process%2520guided%2520by%2520events.%2520This%2520module%2520can%250Aeffectively%2520integrate%2520valuable%2520information%2520from%2520both%2520modalities%2520operating%2520at%250Adifferent%2520frequencies.%2520To%2520achieve%2520smoother%2520point%2520trajectories%252C%2520we%2520employed%2520a%250Atransformer-based%2520refinement%2520strategy%2520that%2520updates%2520the%2520point%2527s%2520trajectories%2520and%250Afeatures%2520iteratively.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520approaches%252C%2520particularly%2520improving%2520expected%250Afeature%2520age%2520by%252024%2524%255C%2525%2524%2520on%2520EDS%2520datasets.%2520Finally%252C%2520we%2520qualitatively%2520validated%2520the%250Arobustness%2520of%2520our%2520algorithm%2520in%2520real%2520driving%2520scenarios%2520using%2520our%2520custom-designed%250Ahigh-resolution%2520image-event%2520synchronization%2520device.%2520Our%2520source%2520code%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/ljx1002/FE-TAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20Any%20Point%20with%20Frame-Event%20Fusion%20Network%20at%20High%20Frame%20Rate&entry.906535625=Jiaxiong%20Liu%20and%20Bo%20Wang%20and%20Zhen%20Tan%20and%20Jinpu%20Zhang%20and%20Hui%20Shen%20and%20Dewen%20Hu&entry.1292438233=%20%20Tracking%20any%20point%20based%20on%20image%20frames%20is%20constrained%20by%20frame%20rates%2C%0Aleading%20to%20instability%20in%20high-speed%20scenarios%20and%20limited%20generalization%20in%0Areal-world%20applications.%20To%20overcome%20these%20limitations%2C%20we%20propose%20an%0Aimage-event%20fusion%20point%20tracker%2C%20FE-TAP%2C%20which%20combines%20the%20contextual%0Ainformation%20from%20image%20frames%20with%20the%20high%20temporal%20resolution%20of%20events%2C%0Aachieving%20high%20frame%20rate%20and%20robust%20point%20tracking%20under%20various%20challenging%0Aconditions.%20Specifically%2C%20we%20designed%20an%20Evolution%20Fusion%20module%20%28EvoFusion%29%20to%0Amodel%20the%20image%20generation%20process%20guided%20by%20events.%20This%20module%20can%0Aeffectively%20integrate%20valuable%20information%20from%20both%20modalities%20operating%20at%0Adifferent%20frequencies.%20To%20achieve%20smoother%20point%20trajectories%2C%20we%20employed%20a%0Atransformer-based%20refinement%20strategy%20that%20updates%20the%20point%27s%20trajectories%20and%0Afeatures%20iteratively.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20approaches%2C%20particularly%20improving%20expected%0Afeature%20age%20by%2024%24%5C%25%24%20on%20EDS%20datasets.%20Finally%2C%20we%20qualitatively%20validated%20the%0Arobustness%20of%20our%20algorithm%20in%20real%20driving%20scenarios%20using%20our%20custom-designed%0Ahigh-resolution%20image-event%20synchronization%20device.%20Our%20source%20code%20will%20be%0Areleased%20at%20https%3A//github.com/ljx1002/FE-TAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11953v1&entry.124074799=Read"},
{"title": "Optimal Visual Search with Highly Heuristic Decision Rules", "author": "Anqi Zhang and Wilson S. Geisler", "abstract": "  Visual search is a fundamental natural task for humans and other animals. We\ninvestigated the decision processes humans use when searching briefly presented\ndisplays having well-separated potential target-object locations. Performance\nwas compared with the Bayesian-optimal decision process under the assumption\nthat the information from the different potential target locations is\nstatistically independent. Surprisingly, humans performed slightly better than\noptimal, despite humans' substantial loss of sensitivity in the fovea, and the\nimplausibility of the human brain replicating the optimal computations. We show\nthat three factors can quantitatively explain these seemingly paradoxical\nresults. Most importantly, simple and fixed heuristic decision rules reach near\noptimal search performance. Secondly, foveal neglect primarily affects only the\ncentral potential target location. Finally, spatially correlated neural noise\ncauses search performance to exceed that predicted for independent noise. These\nfindings have far-reaching implications for understanding visual search tasks\nand other identification tasks in humans and other animals.\n", "link": "http://arxiv.org/abs/2409.12124v1", "date": "2024-09-18", "relevancy": 2.5859, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5277}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules&body=Title%3A%20Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules%0AAuthor%3A%20Anqi%20Zhang%20and%20Wilson%20S.%20Geisler%0AAbstract%3A%20%20%20Visual%20search%20is%20a%20fundamental%20natural%20task%20for%20humans%20and%20other%20animals.%20We%0Ainvestigated%20the%20decision%20processes%20humans%20use%20when%20searching%20briefly%20presented%0Adisplays%20having%20well-separated%20potential%20target-object%20locations.%20Performance%0Awas%20compared%20with%20the%20Bayesian-optimal%20decision%20process%20under%20the%20assumption%0Athat%20the%20information%20from%20the%20different%20potential%20target%20locations%20is%0Astatistically%20independent.%20Surprisingly%2C%20humans%20performed%20slightly%20better%20than%0Aoptimal%2C%20despite%20humans%27%20substantial%20loss%20of%20sensitivity%20in%20the%20fovea%2C%20and%20the%0Aimplausibility%20of%20the%20human%20brain%20replicating%20the%20optimal%20computations.%20We%20show%0Athat%20three%20factors%20can%20quantitatively%20explain%20these%20seemingly%20paradoxical%0Aresults.%20Most%20importantly%2C%20simple%20and%20fixed%20heuristic%20decision%20rules%20reach%20near%0Aoptimal%20search%20performance.%20Secondly%2C%20foveal%20neglect%20primarily%20affects%20only%20the%0Acentral%20potential%20target%20location.%20Finally%2C%20spatially%20correlated%20neural%20noise%0Acauses%20search%20performance%20to%20exceed%20that%20predicted%20for%20independent%20noise.%20These%0Afindings%20have%20far-reaching%20implications%20for%20understanding%20visual%20search%20tasks%0Aand%20other%20identification%20tasks%20in%20humans%20and%20other%20animals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Visual%2520Search%2520with%2520Highly%2520Heuristic%2520Decision%2520Rules%26entry.906535625%3DAnqi%2520Zhang%2520and%2520Wilson%2520S.%2520Geisler%26entry.1292438233%3D%2520%2520Visual%2520search%2520is%2520a%2520fundamental%2520natural%2520task%2520for%2520humans%2520and%2520other%2520animals.%2520We%250Ainvestigated%2520the%2520decision%2520processes%2520humans%2520use%2520when%2520searching%2520briefly%2520presented%250Adisplays%2520having%2520well-separated%2520potential%2520target-object%2520locations.%2520Performance%250Awas%2520compared%2520with%2520the%2520Bayesian-optimal%2520decision%2520process%2520under%2520the%2520assumption%250Athat%2520the%2520information%2520from%2520the%2520different%2520potential%2520target%2520locations%2520is%250Astatistically%2520independent.%2520Surprisingly%252C%2520humans%2520performed%2520slightly%2520better%2520than%250Aoptimal%252C%2520despite%2520humans%2527%2520substantial%2520loss%2520of%2520sensitivity%2520in%2520the%2520fovea%252C%2520and%2520the%250Aimplausibility%2520of%2520the%2520human%2520brain%2520replicating%2520the%2520optimal%2520computations.%2520We%2520show%250Athat%2520three%2520factors%2520can%2520quantitatively%2520explain%2520these%2520seemingly%2520paradoxical%250Aresults.%2520Most%2520importantly%252C%2520simple%2520and%2520fixed%2520heuristic%2520decision%2520rules%2520reach%2520near%250Aoptimal%2520search%2520performance.%2520Secondly%252C%2520foveal%2520neglect%2520primarily%2520affects%2520only%2520the%250Acentral%2520potential%2520target%2520location.%2520Finally%252C%2520spatially%2520correlated%2520neural%2520noise%250Acauses%2520search%2520performance%2520to%2520exceed%2520that%2520predicted%2520for%2520independent%2520noise.%2520These%250Afindings%2520have%2520far-reaching%2520implications%2520for%2520understanding%2520visual%2520search%2520tasks%250Aand%2520other%2520identification%2520tasks%2520in%2520humans%2520and%2520other%2520animals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Visual%20Search%20with%20Highly%20Heuristic%20Decision%20Rules&entry.906535625=Anqi%20Zhang%20and%20Wilson%20S.%20Geisler&entry.1292438233=%20%20Visual%20search%20is%20a%20fundamental%20natural%20task%20for%20humans%20and%20other%20animals.%20We%0Ainvestigated%20the%20decision%20processes%20humans%20use%20when%20searching%20briefly%20presented%0Adisplays%20having%20well-separated%20potential%20target-object%20locations.%20Performance%0Awas%20compared%20with%20the%20Bayesian-optimal%20decision%20process%20under%20the%20assumption%0Athat%20the%20information%20from%20the%20different%20potential%20target%20locations%20is%0Astatistically%20independent.%20Surprisingly%2C%20humans%20performed%20slightly%20better%20than%0Aoptimal%2C%20despite%20humans%27%20substantial%20loss%20of%20sensitivity%20in%20the%20fovea%2C%20and%20the%0Aimplausibility%20of%20the%20human%20brain%20replicating%20the%20optimal%20computations.%20We%20show%0Athat%20three%20factors%20can%20quantitatively%20explain%20these%20seemingly%20paradoxical%0Aresults.%20Most%20importantly%2C%20simple%20and%20fixed%20heuristic%20decision%20rules%20reach%20near%0Aoptimal%20search%20performance.%20Secondly%2C%20foveal%20neglect%20primarily%20affects%20only%20the%0Acentral%20potential%20target%20location.%20Finally%2C%20spatially%20correlated%20neural%20noise%0Acauses%20search%20performance%20to%20exceed%20that%20predicted%20for%20independent%20noise.%20These%0Afindings%20have%20far-reaching%20implications%20for%20understanding%20visual%20search%20tasks%0Aand%20other%20identification%20tasks%20in%20humans%20and%20other%20animals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12124v1&entry.124074799=Read"},
{"title": "Unraveling the Hessian: A Key to Smooth Convergence in Loss Function\n  Landscapes", "author": "Nikita Kiselev and Andrey Grabovoy", "abstract": "  The loss landscape of neural networks is a critical aspect of their training,\nand understanding its properties is essential for improving their performance.\nIn this paper, we investigate how the loss surface changes when the sample size\nincreases, a previously unexplored issue. We theoretically analyze the\nconvergence of the loss landscape in a fully connected neural network and\nderive upper bounds for the difference in loss function values when adding a\nnew object to the sample. Our empirical study confirms these results on various\ndatasets, demonstrating the convergence of the loss function surface for image\nclassification tasks. Our findings provide insights into the local geometry of\nneural loss landscapes and have implications for the development of sample size\ndetermination techniques.\n", "link": "http://arxiv.org/abs/2409.11995v1", "date": "2024-09-18", "relevancy": 2.5639, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5132}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20Hessian%3A%20A%20Key%20to%20Smooth%20Convergence%20in%20Loss%20Function%0A%20%20Landscapes&body=Title%3A%20Unraveling%20the%20Hessian%3A%20A%20Key%20to%20Smooth%20Convergence%20in%20Loss%20Function%0A%20%20Landscapes%0AAuthor%3A%20Nikita%20Kiselev%20and%20Andrey%20Grabovoy%0AAbstract%3A%20%20%20The%20loss%20landscape%20of%20neural%20networks%20is%20a%20critical%20aspect%20of%20their%20training%2C%0Aand%20understanding%20its%20properties%20is%20essential%20for%20improving%20their%20performance.%0AIn%20this%20paper%2C%20we%20investigate%20how%20the%20loss%20surface%20changes%20when%20the%20sample%20size%0Aincreases%2C%20a%20previously%20unexplored%20issue.%20We%20theoretically%20analyze%20the%0Aconvergence%20of%20the%20loss%20landscape%20in%20a%20fully%20connected%20neural%20network%20and%0Aderive%20upper%20bounds%20for%20the%20difference%20in%20loss%20function%20values%20when%20adding%20a%0Anew%20object%20to%20the%20sample.%20Our%20empirical%20study%20confirms%20these%20results%20on%20various%0Adatasets%2C%20demonstrating%20the%20convergence%20of%20the%20loss%20function%20surface%20for%20image%0Aclassification%20tasks.%20Our%20findings%20provide%20insights%20into%20the%20local%20geometry%20of%0Aneural%20loss%20landscapes%20and%20have%20implications%20for%20the%20development%20of%20sample%20size%0Adetermination%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520Hessian%253A%2520A%2520Key%2520to%2520Smooth%2520Convergence%2520in%2520Loss%2520Function%250A%2520%2520Landscapes%26entry.906535625%3DNikita%2520Kiselev%2520and%2520Andrey%2520Grabovoy%26entry.1292438233%3D%2520%2520The%2520loss%2520landscape%2520of%2520neural%2520networks%2520is%2520a%2520critical%2520aspect%2520of%2520their%2520training%252C%250Aand%2520understanding%2520its%2520properties%2520is%2520essential%2520for%2520improving%2520their%2520performance.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520how%2520the%2520loss%2520surface%2520changes%2520when%2520the%2520sample%2520size%250Aincreases%252C%2520a%2520previously%2520unexplored%2520issue.%2520We%2520theoretically%2520analyze%2520the%250Aconvergence%2520of%2520the%2520loss%2520landscape%2520in%2520a%2520fully%2520connected%2520neural%2520network%2520and%250Aderive%2520upper%2520bounds%2520for%2520the%2520difference%2520in%2520loss%2520function%2520values%2520when%2520adding%2520a%250Anew%2520object%2520to%2520the%2520sample.%2520Our%2520empirical%2520study%2520confirms%2520these%2520results%2520on%2520various%250Adatasets%252C%2520demonstrating%2520the%2520convergence%2520of%2520the%2520loss%2520function%2520surface%2520for%2520image%250Aclassification%2520tasks.%2520Our%2520findings%2520provide%2520insights%2520into%2520the%2520local%2520geometry%2520of%250Aneural%2520loss%2520landscapes%2520and%2520have%2520implications%2520for%2520the%2520development%2520of%2520sample%2520size%250Adetermination%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20Hessian%3A%20A%20Key%20to%20Smooth%20Convergence%20in%20Loss%20Function%0A%20%20Landscapes&entry.906535625=Nikita%20Kiselev%20and%20Andrey%20Grabovoy&entry.1292438233=%20%20The%20loss%20landscape%20of%20neural%20networks%20is%20a%20critical%20aspect%20of%20their%20training%2C%0Aand%20understanding%20its%20properties%20is%20essential%20for%20improving%20their%20performance.%0AIn%20this%20paper%2C%20we%20investigate%20how%20the%20loss%20surface%20changes%20when%20the%20sample%20size%0Aincreases%2C%20a%20previously%20unexplored%20issue.%20We%20theoretically%20analyze%20the%0Aconvergence%20of%20the%20loss%20landscape%20in%20a%20fully%20connected%20neural%20network%20and%0Aderive%20upper%20bounds%20for%20the%20difference%20in%20loss%20function%20values%20when%20adding%20a%0Anew%20object%20to%20the%20sample.%20Our%20empirical%20study%20confirms%20these%20results%20on%20various%0Adatasets%2C%20demonstrating%20the%20convergence%20of%20the%20loss%20function%20surface%20for%20image%0Aclassification%20tasks.%20Our%20findings%20provide%20insights%20into%20the%20local%20geometry%20of%0Aneural%20loss%20landscapes%20and%20have%20implications%20for%20the%20development%20of%20sample%20size%0Adetermination%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11995v1&entry.124074799=Read"},
{"title": "Data Efficient Acoustic Scene Classification using Teacher-Informed\n  Confusing Class Instruction", "author": "Jin Jie Sean Yeo and Ee-Leng Tan and Jisheng Bai and Santi Peksi and Woon-Seng Gan", "abstract": "  In this technical report, we describe the SNTL-NTU team's submission for Task\n1 Data-Efficient Low-Complexity Acoustic Scene Classification of the detection\nand classification of acoustic scenes and events (DCASE) 2024 challenge. Three\nsystems are introduced to tackle training splits of different sizes. For small\ntraining splits, we explored reducing the complexity of the provided baseline\nmodel by reducing the number of base channels. We introduce data augmentation\nin the form of mixup to increase the diversity of training samples. For the\nlarger training splits, we use FocusNet to provide confusing class information\nto an ensemble of multiple Patchout faSt Spectrogram Transformer (PaSST) models\nand baseline models trained on the original sampling rate of 44.1 kHz. We use\nKnowledge Distillation to distill the ensemble model to the baseline student\nmodel. Training the systems on the TAU Urban Acoustic Scene 2022 Mobile\ndevelopment dataset yielded the highest average testing accuracy of (62.21,\n59.82, 56.81, 53.03, 47.97)% on split (100, 50, 25, 10, 5)% respectively over\nthe three systems.\n", "link": "http://arxiv.org/abs/2409.11964v1", "date": "2024-09-18", "relevancy": 2.5602, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Efficient%20Acoustic%20Scene%20Classification%20using%20Teacher-Informed%0A%20%20Confusing%20Class%20Instruction&body=Title%3A%20Data%20Efficient%20Acoustic%20Scene%20Classification%20using%20Teacher-Informed%0A%20%20Confusing%20Class%20Instruction%0AAuthor%3A%20Jin%20Jie%20Sean%20Yeo%20and%20Ee-Leng%20Tan%20and%20Jisheng%20Bai%20and%20Santi%20Peksi%20and%20Woon-Seng%20Gan%0AAbstract%3A%20%20%20In%20this%20technical%20report%2C%20we%20describe%20the%20SNTL-NTU%20team%27s%20submission%20for%20Task%0A1%20Data-Efficient%20Low-Complexity%20Acoustic%20Scene%20Classification%20of%20the%20detection%0Aand%20classification%20of%20acoustic%20scenes%20and%20events%20%28DCASE%29%202024%20challenge.%20Three%0Asystems%20are%20introduced%20to%20tackle%20training%20splits%20of%20different%20sizes.%20For%20small%0Atraining%20splits%2C%20we%20explored%20reducing%20the%20complexity%20of%20the%20provided%20baseline%0Amodel%20by%20reducing%20the%20number%20of%20base%20channels.%20We%20introduce%20data%20augmentation%0Ain%20the%20form%20of%20mixup%20to%20increase%20the%20diversity%20of%20training%20samples.%20For%20the%0Alarger%20training%20splits%2C%20we%20use%20FocusNet%20to%20provide%20confusing%20class%20information%0Ato%20an%20ensemble%20of%20multiple%20Patchout%20faSt%20Spectrogram%20Transformer%20%28PaSST%29%20models%0Aand%20baseline%20models%20trained%20on%20the%20original%20sampling%20rate%20of%2044.1%20kHz.%20We%20use%0AKnowledge%20Distillation%20to%20distill%20the%20ensemble%20model%20to%20the%20baseline%20student%0Amodel.%20Training%20the%20systems%20on%20the%20TAU%20Urban%20Acoustic%20Scene%202022%20Mobile%0Adevelopment%20dataset%20yielded%20the%20highest%20average%20testing%20accuracy%20of%20%2862.21%2C%0A59.82%2C%2056.81%2C%2053.03%2C%2047.97%29%25%20on%20split%20%28100%2C%2050%2C%2025%2C%2010%2C%205%29%25%20respectively%20over%0Athe%20three%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Efficient%2520Acoustic%2520Scene%2520Classification%2520using%2520Teacher-Informed%250A%2520%2520Confusing%2520Class%2520Instruction%26entry.906535625%3DJin%2520Jie%2520Sean%2520Yeo%2520and%2520Ee-Leng%2520Tan%2520and%2520Jisheng%2520Bai%2520and%2520Santi%2520Peksi%2520and%2520Woon-Seng%2520Gan%26entry.1292438233%3D%2520%2520In%2520this%2520technical%2520report%252C%2520we%2520describe%2520the%2520SNTL-NTU%2520team%2527s%2520submission%2520for%2520Task%250A1%2520Data-Efficient%2520Low-Complexity%2520Acoustic%2520Scene%2520Classification%2520of%2520the%2520detection%250Aand%2520classification%2520of%2520acoustic%2520scenes%2520and%2520events%2520%2528DCASE%2529%25202024%2520challenge.%2520Three%250Asystems%2520are%2520introduced%2520to%2520tackle%2520training%2520splits%2520of%2520different%2520sizes.%2520For%2520small%250Atraining%2520splits%252C%2520we%2520explored%2520reducing%2520the%2520complexity%2520of%2520the%2520provided%2520baseline%250Amodel%2520by%2520reducing%2520the%2520number%2520of%2520base%2520channels.%2520We%2520introduce%2520data%2520augmentation%250Ain%2520the%2520form%2520of%2520mixup%2520to%2520increase%2520the%2520diversity%2520of%2520training%2520samples.%2520For%2520the%250Alarger%2520training%2520splits%252C%2520we%2520use%2520FocusNet%2520to%2520provide%2520confusing%2520class%2520information%250Ato%2520an%2520ensemble%2520of%2520multiple%2520Patchout%2520faSt%2520Spectrogram%2520Transformer%2520%2528PaSST%2529%2520models%250Aand%2520baseline%2520models%2520trained%2520on%2520the%2520original%2520sampling%2520rate%2520of%252044.1%2520kHz.%2520We%2520use%250AKnowledge%2520Distillation%2520to%2520distill%2520the%2520ensemble%2520model%2520to%2520the%2520baseline%2520student%250Amodel.%2520Training%2520the%2520systems%2520on%2520the%2520TAU%2520Urban%2520Acoustic%2520Scene%25202022%2520Mobile%250Adevelopment%2520dataset%2520yielded%2520the%2520highest%2520average%2520testing%2520accuracy%2520of%2520%252862.21%252C%250A59.82%252C%252056.81%252C%252053.03%252C%252047.97%2529%2525%2520on%2520split%2520%2528100%252C%252050%252C%252025%252C%252010%252C%25205%2529%2525%2520respectively%2520over%250Athe%2520three%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Efficient%20Acoustic%20Scene%20Classification%20using%20Teacher-Informed%0A%20%20Confusing%20Class%20Instruction&entry.906535625=Jin%20Jie%20Sean%20Yeo%20and%20Ee-Leng%20Tan%20and%20Jisheng%20Bai%20and%20Santi%20Peksi%20and%20Woon-Seng%20Gan&entry.1292438233=%20%20In%20this%20technical%20report%2C%20we%20describe%20the%20SNTL-NTU%20team%27s%20submission%20for%20Task%0A1%20Data-Efficient%20Low-Complexity%20Acoustic%20Scene%20Classification%20of%20the%20detection%0Aand%20classification%20of%20acoustic%20scenes%20and%20events%20%28DCASE%29%202024%20challenge.%20Three%0Asystems%20are%20introduced%20to%20tackle%20training%20splits%20of%20different%20sizes.%20For%20small%0Atraining%20splits%2C%20we%20explored%20reducing%20the%20complexity%20of%20the%20provided%20baseline%0Amodel%20by%20reducing%20the%20number%20of%20base%20channels.%20We%20introduce%20data%20augmentation%0Ain%20the%20form%20of%20mixup%20to%20increase%20the%20diversity%20of%20training%20samples.%20For%20the%0Alarger%20training%20splits%2C%20we%20use%20FocusNet%20to%20provide%20confusing%20class%20information%0Ato%20an%20ensemble%20of%20multiple%20Patchout%20faSt%20Spectrogram%20Transformer%20%28PaSST%29%20models%0Aand%20baseline%20models%20trained%20on%20the%20original%20sampling%20rate%20of%2044.1%20kHz.%20We%20use%0AKnowledge%20Distillation%20to%20distill%20the%20ensemble%20model%20to%20the%20baseline%20student%0Amodel.%20Training%20the%20systems%20on%20the%20TAU%20Urban%20Acoustic%20Scene%202022%20Mobile%0Adevelopment%20dataset%20yielded%20the%20highest%20average%20testing%20accuracy%20of%20%2862.21%2C%0A59.82%2C%2056.81%2C%2053.03%2C%2047.97%29%25%20on%20split%20%28100%2C%2050%2C%2025%2C%2010%2C%205%29%25%20respectively%20over%0Athe%20three%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11964v1&entry.124074799=Read"},
{"title": "QNCD: Quantization Noise Correction for Diffusion Models", "author": "Huanpeng Chu and Wei Wu and Chengjie Zang and Kun Yuan", "abstract": "  Diffusion models have revolutionized image synthesis, setting new benchmarks\nin quality and creativity. However, their widespread adoption is hindered by\nthe intensive computation required during the iterative denoising process.\nPost-training quantization (PTQ) presents a solution to accelerate sampling,\naibeit at the expense of sample quality, extremely in low-bit settings.\nAddressing this, our study introduces a unified Quantization Noise Correction\nScheme (QNCD), aimed at minishing quantization noise throughout the sampling\nprocess. We identify two primary quantization challenges: intra and inter\nquantization noise. Intra quantization noise, mainly exacerbated by embeddings\nin the resblock module, extends activation quantization ranges, increasing\ndisturbances in each single denosing step. Besides, inter quantization noise\nstems from cumulative quantization deviations across the entire denoising\nprocess, altering data distributions step-by-step. QNCD combats these through\nembedding-derived feature smoothing for eliminating intra quantization noise\nand an effective runtime noise estimatiation module for dynamicly filtering\ninter quantization noise. Extensive experiments demonstrate that our method\noutperforms previous quantization methods for diffusion models, achieving\nlossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).\nCode is available at: https://github.com/huanpengchu/QNCD\n", "link": "http://arxiv.org/abs/2403.19140v2", "date": "2024-09-18", "relevancy": 2.5391, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6539}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6538}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QNCD%3A%20Quantization%20Noise%20Correction%20for%20Diffusion%20Models&body=Title%3A%20QNCD%3A%20Quantization%20Noise%20Correction%20for%20Diffusion%20Models%0AAuthor%3A%20Huanpeng%20Chu%20and%20Wei%20Wu%20and%20Chengjie%20Zang%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20Diffusion%20models%20have%20revolutionized%20image%20synthesis%2C%20setting%20new%20benchmarks%0Ain%20quality%20and%20creativity.%20However%2C%20their%20widespread%20adoption%20is%20hindered%20by%0Athe%20intensive%20computation%20required%20during%20the%20iterative%20denoising%20process.%0APost-training%20quantization%20%28PTQ%29%20presents%20a%20solution%20to%20accelerate%20sampling%2C%0Aaibeit%20at%20the%20expense%20of%20sample%20quality%2C%20extremely%20in%20low-bit%20settings.%0AAddressing%20this%2C%20our%20study%20introduces%20a%20unified%20Quantization%20Noise%20Correction%0AScheme%20%28QNCD%29%2C%20aimed%20at%20minishing%20quantization%20noise%20throughout%20the%20sampling%0Aprocess.%20We%20identify%20two%20primary%20quantization%20challenges%3A%20intra%20and%20inter%0Aquantization%20noise.%20Intra%20quantization%20noise%2C%20mainly%20exacerbated%20by%20embeddings%0Ain%20the%20resblock%20module%2C%20extends%20activation%20quantization%20ranges%2C%20increasing%0Adisturbances%20in%20each%20single%20denosing%20step.%20Besides%2C%20inter%20quantization%20noise%0Astems%20from%20cumulative%20quantization%20deviations%20across%20the%20entire%20denoising%0Aprocess%2C%20altering%20data%20distributions%20step-by-step.%20QNCD%20combats%20these%20through%0Aembedding-derived%20feature%20smoothing%20for%20eliminating%20intra%20quantization%20noise%0Aand%20an%20effective%20runtime%20noise%20estimatiation%20module%20for%20dynamicly%20filtering%0Ainter%20quantization%20noise.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20previous%20quantization%20methods%20for%20diffusion%20models%2C%20achieving%0Alossless%20results%20in%20W4A8%20and%20W8A8%20quantization%20settings%20on%20ImageNet%20%28LDM-4%29.%0ACode%20is%20available%20at%3A%20https%3A//github.com/huanpengchu/QNCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQNCD%253A%2520Quantization%2520Noise%2520Correction%2520for%2520Diffusion%2520Models%26entry.906535625%3DHuanpeng%2520Chu%2520and%2520Wei%2520Wu%2520and%2520Chengjie%2520Zang%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520revolutionized%2520image%2520synthesis%252C%2520setting%2520new%2520benchmarks%250Ain%2520quality%2520and%2520creativity.%2520However%252C%2520their%2520widespread%2520adoption%2520is%2520hindered%2520by%250Athe%2520intensive%2520computation%2520required%2520during%2520the%2520iterative%2520denoising%2520process.%250APost-training%2520quantization%2520%2528PTQ%2529%2520presents%2520a%2520solution%2520to%2520accelerate%2520sampling%252C%250Aaibeit%2520at%2520the%2520expense%2520of%2520sample%2520quality%252C%2520extremely%2520in%2520low-bit%2520settings.%250AAddressing%2520this%252C%2520our%2520study%2520introduces%2520a%2520unified%2520Quantization%2520Noise%2520Correction%250AScheme%2520%2528QNCD%2529%252C%2520aimed%2520at%2520minishing%2520quantization%2520noise%2520throughout%2520the%2520sampling%250Aprocess.%2520We%2520identify%2520two%2520primary%2520quantization%2520challenges%253A%2520intra%2520and%2520inter%250Aquantization%2520noise.%2520Intra%2520quantization%2520noise%252C%2520mainly%2520exacerbated%2520by%2520embeddings%250Ain%2520the%2520resblock%2520module%252C%2520extends%2520activation%2520quantization%2520ranges%252C%2520increasing%250Adisturbances%2520in%2520each%2520single%2520denosing%2520step.%2520Besides%252C%2520inter%2520quantization%2520noise%250Astems%2520from%2520cumulative%2520quantization%2520deviations%2520across%2520the%2520entire%2520denoising%250Aprocess%252C%2520altering%2520data%2520distributions%2520step-by-step.%2520QNCD%2520combats%2520these%2520through%250Aembedding-derived%2520feature%2520smoothing%2520for%2520eliminating%2520intra%2520quantization%2520noise%250Aand%2520an%2520effective%2520runtime%2520noise%2520estimatiation%2520module%2520for%2520dynamicly%2520filtering%250Ainter%2520quantization%2520noise.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520previous%2520quantization%2520methods%2520for%2520diffusion%2520models%252C%2520achieving%250Alossless%2520results%2520in%2520W4A8%2520and%2520W8A8%2520quantization%2520settings%2520on%2520ImageNet%2520%2528LDM-4%2529.%250ACode%2520is%2520available%2520at%253A%2520https%253A//github.com/huanpengchu/QNCD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QNCD%3A%20Quantization%20Noise%20Correction%20for%20Diffusion%20Models&entry.906535625=Huanpeng%20Chu%20and%20Wei%20Wu%20and%20Chengjie%20Zang%20and%20Kun%20Yuan&entry.1292438233=%20%20Diffusion%20models%20have%20revolutionized%20image%20synthesis%2C%20setting%20new%20benchmarks%0Ain%20quality%20and%20creativity.%20However%2C%20their%20widespread%20adoption%20is%20hindered%20by%0Athe%20intensive%20computation%20required%20during%20the%20iterative%20denoising%20process.%0APost-training%20quantization%20%28PTQ%29%20presents%20a%20solution%20to%20accelerate%20sampling%2C%0Aaibeit%20at%20the%20expense%20of%20sample%20quality%2C%20extremely%20in%20low-bit%20settings.%0AAddressing%20this%2C%20our%20study%20introduces%20a%20unified%20Quantization%20Noise%20Correction%0AScheme%20%28QNCD%29%2C%20aimed%20at%20minishing%20quantization%20noise%20throughout%20the%20sampling%0Aprocess.%20We%20identify%20two%20primary%20quantization%20challenges%3A%20intra%20and%20inter%0Aquantization%20noise.%20Intra%20quantization%20noise%2C%20mainly%20exacerbated%20by%20embeddings%0Ain%20the%20resblock%20module%2C%20extends%20activation%20quantization%20ranges%2C%20increasing%0Adisturbances%20in%20each%20single%20denosing%20step.%20Besides%2C%20inter%20quantization%20noise%0Astems%20from%20cumulative%20quantization%20deviations%20across%20the%20entire%20denoising%0Aprocess%2C%20altering%20data%20distributions%20step-by-step.%20QNCD%20combats%20these%20through%0Aembedding-derived%20feature%20smoothing%20for%20eliminating%20intra%20quantization%20noise%0Aand%20an%20effective%20runtime%20noise%20estimatiation%20module%20for%20dynamicly%20filtering%0Ainter%20quantization%20noise.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20previous%20quantization%20methods%20for%20diffusion%20models%2C%20achieving%0Alossless%20results%20in%20W4A8%20and%20W8A8%20quantization%20settings%20on%20ImageNet%20%28LDM-4%29.%0ACode%20is%20available%20at%3A%20https%3A//github.com/huanpengchu/QNCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19140v2&entry.124074799=Read"},
{"title": "High-Order Evolving Graphs for Enhanced Representation of Traffic\n  Dynamics", "author": "Aditya Humnabadkar and Arindam Sikdar and Benjamin Cave and Huaizhong Zhang and Paul Bakaki and Ardhendu Behera", "abstract": "  We present an innovative framework for traffic dynamics analysis using\nHigh-Order Evolving Graphs, designed to improve spatio-temporal representations\nin autonomous driving contexts. Our approach constructs temporal bidirectional\nbipartite graphs that effectively model the complex interactions within traffic\nscenes in real-time. By integrating Graph Neural Networks (GNNs) with\nhigh-order multi-aggregation strategies, we significantly enhance the modeling\nof traffic scene dynamics, providing a more accurate and detailed analysis of\nthese interactions. Additionally, we incorporate inductive learning techniques\ninspired by the GraphSAGE framework, enabling our model to adapt to new and\nunseen traffic scenarios without the need for retraining, thus ensuring robust\ngeneralization. Through extensive experiments on the ROAD and ROAD Waymo\ndatasets, we establish a comprehensive baseline for further developments,\ndemonstrating the potential of our method in accurately capturing traffic\nbehavior. Our results emphasize the value of high-order statistical moments and\nfeature-gated attention mechanisms in improving traffic behavior analysis,\nlaying the groundwork for advancing autonomous driving technologies. Our source\ncode is available at: https://github.com/Addy-1998/High_Order_Graphs\n", "link": "http://arxiv.org/abs/2409.11206v2", "date": "2024-09-18", "relevancy": 2.5201, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5044}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics&body=Title%3A%20High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics%0AAuthor%3A%20Aditya%20Humnabadkar%20and%20Arindam%20Sikdar%20and%20Benjamin%20Cave%20and%20Huaizhong%20Zhang%20and%20Paul%20Bakaki%20and%20Ardhendu%20Behera%0AAbstract%3A%20%20%20We%20present%20an%20innovative%20framework%20for%20traffic%20dynamics%20analysis%20using%0AHigh-Order%20Evolving%20Graphs%2C%20designed%20to%20improve%20spatio-temporal%20representations%0Ain%20autonomous%20driving%20contexts.%20Our%20approach%20constructs%20temporal%20bidirectional%0Abipartite%20graphs%20that%20effectively%20model%20the%20complex%20interactions%20within%20traffic%0Ascenes%20in%20real-time.%20By%20integrating%20Graph%20Neural%20Networks%20%28GNNs%29%20with%0Ahigh-order%20multi-aggregation%20strategies%2C%20we%20significantly%20enhance%20the%20modeling%0Aof%20traffic%20scene%20dynamics%2C%20providing%20a%20more%20accurate%20and%20detailed%20analysis%20of%0Athese%20interactions.%20Additionally%2C%20we%20incorporate%20inductive%20learning%20techniques%0Ainspired%20by%20the%20GraphSAGE%20framework%2C%20enabling%20our%20model%20to%20adapt%20to%20new%20and%0Aunseen%20traffic%20scenarios%20without%20the%20need%20for%20retraining%2C%20thus%20ensuring%20robust%0Ageneralization.%20Through%20extensive%20experiments%20on%20the%20ROAD%20and%20ROAD%20Waymo%0Adatasets%2C%20we%20establish%20a%20comprehensive%20baseline%20for%20further%20developments%2C%0Ademonstrating%20the%20potential%20of%20our%20method%20in%20accurately%20capturing%20traffic%0Abehavior.%20Our%20results%20emphasize%20the%20value%20of%20high-order%20statistical%20moments%20and%0Afeature-gated%20attention%20mechanisms%20in%20improving%20traffic%20behavior%20analysis%2C%0Alaying%20the%20groundwork%20for%20advancing%20autonomous%20driving%20technologies.%20Our%20source%0Acode%20is%20available%20at%3A%20https%3A//github.com/Addy-1998/High_Order_Graphs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11206v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Order%2520Evolving%2520Graphs%2520for%2520Enhanced%2520Representation%2520of%2520Traffic%250A%2520%2520Dynamics%26entry.906535625%3DAditya%2520Humnabadkar%2520and%2520Arindam%2520Sikdar%2520and%2520Benjamin%2520Cave%2520and%2520Huaizhong%2520Zhang%2520and%2520Paul%2520Bakaki%2520and%2520Ardhendu%2520Behera%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520innovative%2520framework%2520for%2520traffic%2520dynamics%2520analysis%2520using%250AHigh-Order%2520Evolving%2520Graphs%252C%2520designed%2520to%2520improve%2520spatio-temporal%2520representations%250Ain%2520autonomous%2520driving%2520contexts.%2520Our%2520approach%2520constructs%2520temporal%2520bidirectional%250Abipartite%2520graphs%2520that%2520effectively%2520model%2520the%2520complex%2520interactions%2520within%2520traffic%250Ascenes%2520in%2520real-time.%2520By%2520integrating%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520with%250Ahigh-order%2520multi-aggregation%2520strategies%252C%2520we%2520significantly%2520enhance%2520the%2520modeling%250Aof%2520traffic%2520scene%2520dynamics%252C%2520providing%2520a%2520more%2520accurate%2520and%2520detailed%2520analysis%2520of%250Athese%2520interactions.%2520Additionally%252C%2520we%2520incorporate%2520inductive%2520learning%2520techniques%250Ainspired%2520by%2520the%2520GraphSAGE%2520framework%252C%2520enabling%2520our%2520model%2520to%2520adapt%2520to%2520new%2520and%250Aunseen%2520traffic%2520scenarios%2520without%2520the%2520need%2520for%2520retraining%252C%2520thus%2520ensuring%2520robust%250Ageneralization.%2520Through%2520extensive%2520experiments%2520on%2520the%2520ROAD%2520and%2520ROAD%2520Waymo%250Adatasets%252C%2520we%2520establish%2520a%2520comprehensive%2520baseline%2520for%2520further%2520developments%252C%250Ademonstrating%2520the%2520potential%2520of%2520our%2520method%2520in%2520accurately%2520capturing%2520traffic%250Abehavior.%2520Our%2520results%2520emphasize%2520the%2520value%2520of%2520high-order%2520statistical%2520moments%2520and%250Afeature-gated%2520attention%2520mechanisms%2520in%2520improving%2520traffic%2520behavior%2520analysis%252C%250Alaying%2520the%2520groundwork%2520for%2520advancing%2520autonomous%2520driving%2520technologies.%2520Our%2520source%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/Addy-1998/High_Order_Graphs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11206v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Order%20Evolving%20Graphs%20for%20Enhanced%20Representation%20of%20Traffic%0A%20%20Dynamics&entry.906535625=Aditya%20Humnabadkar%20and%20Arindam%20Sikdar%20and%20Benjamin%20Cave%20and%20Huaizhong%20Zhang%20and%20Paul%20Bakaki%20and%20Ardhendu%20Behera&entry.1292438233=%20%20We%20present%20an%20innovative%20framework%20for%20traffic%20dynamics%20analysis%20using%0AHigh-Order%20Evolving%20Graphs%2C%20designed%20to%20improve%20spatio-temporal%20representations%0Ain%20autonomous%20driving%20contexts.%20Our%20approach%20constructs%20temporal%20bidirectional%0Abipartite%20graphs%20that%20effectively%20model%20the%20complex%20interactions%20within%20traffic%0Ascenes%20in%20real-time.%20By%20integrating%20Graph%20Neural%20Networks%20%28GNNs%29%20with%0Ahigh-order%20multi-aggregation%20strategies%2C%20we%20significantly%20enhance%20the%20modeling%0Aof%20traffic%20scene%20dynamics%2C%20providing%20a%20more%20accurate%20and%20detailed%20analysis%20of%0Athese%20interactions.%20Additionally%2C%20we%20incorporate%20inductive%20learning%20techniques%0Ainspired%20by%20the%20GraphSAGE%20framework%2C%20enabling%20our%20model%20to%20adapt%20to%20new%20and%0Aunseen%20traffic%20scenarios%20without%20the%20need%20for%20retraining%2C%20thus%20ensuring%20robust%0Ageneralization.%20Through%20extensive%20experiments%20on%20the%20ROAD%20and%20ROAD%20Waymo%0Adatasets%2C%20we%20establish%20a%20comprehensive%20baseline%20for%20further%20developments%2C%0Ademonstrating%20the%20potential%20of%20our%20method%20in%20accurately%20capturing%20traffic%0Abehavior.%20Our%20results%20emphasize%20the%20value%20of%20high-order%20statistical%20moments%20and%0Afeature-gated%20attention%20mechanisms%20in%20improving%20traffic%20behavior%20analysis%2C%0Alaying%20the%20groundwork%20for%20advancing%20autonomous%20driving%20technologies.%20Our%20source%0Acode%20is%20available%20at%3A%20https%3A//github.com/Addy-1998/High_Order_Graphs%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11206v2&entry.124074799=Read"},
{"title": "LOLA -- An Open-Source Massively Multilingual Large Language Model", "author": "Nikit Srivastava and Denis Kuchelev and Tatiana Moteu and Kshitij Shetty and Michael R\u00f6der and Diego Moussallem and Hamada Zahera and Axel-Cyrille Ngonga Ngomo", "abstract": "  This paper presents LOLA, a massively multilingual large language model\ntrained on more than 160 languages using a sparse Mixture-of-Experts\nTransformer architecture. Our architectural and implementation choices address\nthe challenge of harnessing linguistic diversity while maintaining efficiency\nand avoiding the common pitfalls of multilinguality. Our analysis of the\nevaluation results shows competitive performance in natural language generation\nand understanding tasks. Additionally, we demonstrate how the learned\nexpert-routing mechanism exploits implicit phylogenetic linguistic patterns to\npotentially alleviate the curse of multilinguality. We provide an in-depth look\nat the training process, an analysis of the datasets, and a balanced\nexploration of the model's strengths and limitations. As an open-source model,\nLOLA promotes reproducibility and serves as a robust foundation for future\nresearch. Our findings enable the development of compute-efficient multilingual\nmodels with strong, scalable performance across languages.\n", "link": "http://arxiv.org/abs/2409.11272v2", "date": "2024-09-18", "relevancy": 2.5153, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&body=Title%3A%20LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model%0AAuthor%3A%20Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20and%20Kshitij%20Shetty%20and%20Michael%20R%C3%B6der%20and%20Diego%20Moussallem%20and%20Hamada%20Zahera%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11272v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOLA%2520--%2520An%2520Open-Source%2520Massively%2520Multilingual%2520Large%2520Language%2520Model%26entry.906535625%3DNikit%2520Srivastava%2520and%2520Denis%2520Kuchelev%2520and%2520Tatiana%2520Moteu%2520and%2520Kshitij%2520Shetty%2520and%2520Michael%2520R%25C3%25B6der%2520and%2520Diego%2520Moussallem%2520and%2520Hamada%2520Zahera%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LOLA%252C%2520a%2520massively%2520multilingual%2520large%2520language%2520model%250Atrained%2520on%2520more%2520than%2520160%2520languages%2520using%2520a%2520sparse%2520Mixture-of-Experts%250ATransformer%2520architecture.%2520Our%2520architectural%2520and%2520implementation%2520choices%2520address%250Athe%2520challenge%2520of%2520harnessing%2520linguistic%2520diversity%2520while%2520maintaining%2520efficiency%250Aand%2520avoiding%2520the%2520common%2520pitfalls%2520of%2520multilinguality.%2520Our%2520analysis%2520of%2520the%250Aevaluation%2520results%2520shows%2520competitive%2520performance%2520in%2520natural%2520language%2520generation%250Aand%2520understanding%2520tasks.%2520Additionally%252C%2520we%2520demonstrate%2520how%2520the%2520learned%250Aexpert-routing%2520mechanism%2520exploits%2520implicit%2520phylogenetic%2520linguistic%2520patterns%2520to%250Apotentially%2520alleviate%2520the%2520curse%2520of%2520multilinguality.%2520We%2520provide%2520an%2520in-depth%2520look%250Aat%2520the%2520training%2520process%252C%2520an%2520analysis%2520of%2520the%2520datasets%252C%2520and%2520a%2520balanced%250Aexploration%2520of%2520the%2520model%2527s%2520strengths%2520and%2520limitations.%2520As%2520an%2520open-source%2520model%252C%250ALOLA%2520promotes%2520reproducibility%2520and%2520serves%2520as%2520a%2520robust%2520foundation%2520for%2520future%250Aresearch.%2520Our%2520findings%2520enable%2520the%2520development%2520of%2520compute-efficient%2520multilingual%250Amodels%2520with%2520strong%252C%2520scalable%2520performance%2520across%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11272v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOLA%20--%20An%20Open-Source%20Massively%20Multilingual%20Large%20Language%20Model&entry.906535625=Nikit%20Srivastava%20and%20Denis%20Kuchelev%20and%20Tatiana%20Moteu%20and%20Kshitij%20Shetty%20and%20Michael%20R%C3%B6der%20and%20Diego%20Moussallem%20and%20Hamada%20Zahera%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20This%20paper%20presents%20LOLA%2C%20a%20massively%20multilingual%20large%20language%20model%0Atrained%20on%20more%20than%20160%20languages%20using%20a%20sparse%20Mixture-of-Experts%0ATransformer%20architecture.%20Our%20architectural%20and%20implementation%20choices%20address%0Athe%20challenge%20of%20harnessing%20linguistic%20diversity%20while%20maintaining%20efficiency%0Aand%20avoiding%20the%20common%20pitfalls%20of%20multilinguality.%20Our%20analysis%20of%20the%0Aevaluation%20results%20shows%20competitive%20performance%20in%20natural%20language%20generation%0Aand%20understanding%20tasks.%20Additionally%2C%20we%20demonstrate%20how%20the%20learned%0Aexpert-routing%20mechanism%20exploits%20implicit%20phylogenetic%20linguistic%20patterns%20to%0Apotentially%20alleviate%20the%20curse%20of%20multilinguality.%20We%20provide%20an%20in-depth%20look%0Aat%20the%20training%20process%2C%20an%20analysis%20of%20the%20datasets%2C%20and%20a%20balanced%0Aexploration%20of%20the%20model%27s%20strengths%20and%20limitations.%20As%20an%20open-source%20model%2C%0ALOLA%20promotes%20reproducibility%20and%20serves%20as%20a%20robust%20foundation%20for%20future%0Aresearch.%20Our%20findings%20enable%20the%20development%20of%20compute-efficient%20multilingual%0Amodels%20with%20strong%2C%20scalable%20performance%20across%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11272v2&entry.124074799=Read"},
{"title": "PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification\n  and Fine-Tuning", "author": "Yukai Xu and Yujie Gu and Kouichi Sakurai", "abstract": "  Backdoor attacks pose a significant threat to deep neural networks,\nparticularly as recent advancements have led to increasingly subtle\nimplantation, making the defense more challenging. Existing defense mechanisms\ntypically rely on an additional clean dataset as a standard reference and\ninvolve retraining an auxiliary model or fine-tuning the entire victim model.\nHowever, these approaches are often computationally expensive and not always\nfeasible in practical applications. In this paper, we propose a novel and\nlightweight defense mechanism, termed PAD-FT, that does not require an\nadditional clean dataset and fine-tunes only a very small part of the model to\ndisinfect the victim model. To achieve this, our approach first introduces a\nsimple data purification process to identify and select the most-likely clean\ndata from the poisoned training dataset. The self-purified clean dataset is\nthen used for activation clipping and fine-tuning only the last classification\nlayer of the victim model. By integrating data purification, activation\nclipping, and classifier fine-tuning, our mechanism PAD-FT demonstrates\nsuperior effectiveness across multiple backdoor attack methods and datasets, as\nconfirmed through extensive experimental evaluation.\n", "link": "http://arxiv.org/abs/2409.12072v1", "date": "2024-09-18", "relevancy": 2.4808, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5133}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4939}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAD-FT%3A%20A%20Lightweight%20Defense%20for%20Backdoor%20Attacks%20via%20Data%20Purification%0A%20%20and%20Fine-Tuning&body=Title%3A%20PAD-FT%3A%20A%20Lightweight%20Defense%20for%20Backdoor%20Attacks%20via%20Data%20Purification%0A%20%20and%20Fine-Tuning%0AAuthor%3A%20Yukai%20Xu%20and%20Yujie%20Gu%20and%20Kouichi%20Sakurai%0AAbstract%3A%20%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20deep%20neural%20networks%2C%0Aparticularly%20as%20recent%20advancements%20have%20led%20to%20increasingly%20subtle%0Aimplantation%2C%20making%20the%20defense%20more%20challenging.%20Existing%20defense%20mechanisms%0Atypically%20rely%20on%20an%20additional%20clean%20dataset%20as%20a%20standard%20reference%20and%0Ainvolve%20retraining%20an%20auxiliary%20model%20or%20fine-tuning%20the%20entire%20victim%20model.%0AHowever%2C%20these%20approaches%20are%20often%20computationally%20expensive%20and%20not%20always%0Afeasible%20in%20practical%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%0Alightweight%20defense%20mechanism%2C%20termed%20PAD-FT%2C%20that%20does%20not%20require%20an%0Aadditional%20clean%20dataset%20and%20fine-tunes%20only%20a%20very%20small%20part%20of%20the%20model%20to%0Adisinfect%20the%20victim%20model.%20To%20achieve%20this%2C%20our%20approach%20first%20introduces%20a%0Asimple%20data%20purification%20process%20to%20identify%20and%20select%20the%20most-likely%20clean%0Adata%20from%20the%20poisoned%20training%20dataset.%20The%20self-purified%20clean%20dataset%20is%0Athen%20used%20for%20activation%20clipping%20and%20fine-tuning%20only%20the%20last%20classification%0Alayer%20of%20the%20victim%20model.%20By%20integrating%20data%20purification%2C%20activation%0Aclipping%2C%20and%20classifier%20fine-tuning%2C%20our%20mechanism%20PAD-FT%20demonstrates%0Asuperior%20effectiveness%20across%20multiple%20backdoor%20attack%20methods%20and%20datasets%2C%20as%0Aconfirmed%20through%20extensive%20experimental%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAD-FT%253A%2520A%2520Lightweight%2520Defense%2520for%2520Backdoor%2520Attacks%2520via%2520Data%2520Purification%250A%2520%2520and%2520Fine-Tuning%26entry.906535625%3DYukai%2520Xu%2520and%2520Yujie%2520Gu%2520and%2520Kouichi%2520Sakurai%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520deep%2520neural%2520networks%252C%250Aparticularly%2520as%2520recent%2520advancements%2520have%2520led%2520to%2520increasingly%2520subtle%250Aimplantation%252C%2520making%2520the%2520defense%2520more%2520challenging.%2520Existing%2520defense%2520mechanisms%250Atypically%2520rely%2520on%2520an%2520additional%2520clean%2520dataset%2520as%2520a%2520standard%2520reference%2520and%250Ainvolve%2520retraining%2520an%2520auxiliary%2520model%2520or%2520fine-tuning%2520the%2520entire%2520victim%2520model.%250AHowever%252C%2520these%2520approaches%2520are%2520often%2520computationally%2520expensive%2520and%2520not%2520always%250Afeasible%2520in%2520practical%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%250Alightweight%2520defense%2520mechanism%252C%2520termed%2520PAD-FT%252C%2520that%2520does%2520not%2520require%2520an%250Aadditional%2520clean%2520dataset%2520and%2520fine-tunes%2520only%2520a%2520very%2520small%2520part%2520of%2520the%2520model%2520to%250Adisinfect%2520the%2520victim%2520model.%2520To%2520achieve%2520this%252C%2520our%2520approach%2520first%2520introduces%2520a%250Asimple%2520data%2520purification%2520process%2520to%2520identify%2520and%2520select%2520the%2520most-likely%2520clean%250Adata%2520from%2520the%2520poisoned%2520training%2520dataset.%2520The%2520self-purified%2520clean%2520dataset%2520is%250Athen%2520used%2520for%2520activation%2520clipping%2520and%2520fine-tuning%2520only%2520the%2520last%2520classification%250Alayer%2520of%2520the%2520victim%2520model.%2520By%2520integrating%2520data%2520purification%252C%2520activation%250Aclipping%252C%2520and%2520classifier%2520fine-tuning%252C%2520our%2520mechanism%2520PAD-FT%2520demonstrates%250Asuperior%2520effectiveness%2520across%2520multiple%2520backdoor%2520attack%2520methods%2520and%2520datasets%252C%2520as%250Aconfirmed%2520through%2520extensive%2520experimental%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAD-FT%3A%20A%20Lightweight%20Defense%20for%20Backdoor%20Attacks%20via%20Data%20Purification%0A%20%20and%20Fine-Tuning&entry.906535625=Yukai%20Xu%20and%20Yujie%20Gu%20and%20Kouichi%20Sakurai&entry.1292438233=%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20deep%20neural%20networks%2C%0Aparticularly%20as%20recent%20advancements%20have%20led%20to%20increasingly%20subtle%0Aimplantation%2C%20making%20the%20defense%20more%20challenging.%20Existing%20defense%20mechanisms%0Atypically%20rely%20on%20an%20additional%20clean%20dataset%20as%20a%20standard%20reference%20and%0Ainvolve%20retraining%20an%20auxiliary%20model%20or%20fine-tuning%20the%20entire%20victim%20model.%0AHowever%2C%20these%20approaches%20are%20often%20computationally%20expensive%20and%20not%20always%0Afeasible%20in%20practical%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%0Alightweight%20defense%20mechanism%2C%20termed%20PAD-FT%2C%20that%20does%20not%20require%20an%0Aadditional%20clean%20dataset%20and%20fine-tunes%20only%20a%20very%20small%20part%20of%20the%20model%20to%0Adisinfect%20the%20victim%20model.%20To%20achieve%20this%2C%20our%20approach%20first%20introduces%20a%0Asimple%20data%20purification%20process%20to%20identify%20and%20select%20the%20most-likely%20clean%0Adata%20from%20the%20poisoned%20training%20dataset.%20The%20self-purified%20clean%20dataset%20is%0Athen%20used%20for%20activation%20clipping%20and%20fine-tuning%20only%20the%20last%20classification%0Alayer%20of%20the%20victim%20model.%20By%20integrating%20data%20purification%2C%20activation%0Aclipping%2C%20and%20classifier%20fine-tuning%2C%20our%20mechanism%20PAD-FT%20demonstrates%0Asuperior%20effectiveness%20across%20multiple%20backdoor%20attack%20methods%20and%20datasets%2C%20as%0Aconfirmed%20through%20extensive%20experimental%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12072v1&entry.124074799=Read"},
{"title": "High-Resolution Maps of Left Atrial Displacements and Strains Estimated\n  with 3D Cine MRI using Online Learning Neural Networks", "author": "Christoforos Galazis and Samuel Shepperd and Emma Brouwer and Sandro Queir\u00f3s and Ebraham Alskaf and Mustafa Anjari and Amedeo Chiribiri and Jack Lee and Anil A. Bharath and Marta Varela", "abstract": "  The functional analysis of the left atrium (LA) is important for evaluating\ncardiac health and understanding diseases like atrial fibrillation. Cine MRI is\nideally placed for the detailed 3D characterization of LA motion and\ndeformation but is lacking appropriate acquisition and analysis tools. Here, we\npropose tools for the Analysis for Left Atrial Displacements and DeformatIons\nusing online learning neural Networks (Aladdin) and present a technical\nfeasibility study on how Aladdin can characterize 3D LA function globally and\nregionally. Aladdin includes an online segmentation and image registration\nnetwork, and a strain calculation pipeline tailored to the LA. We create maps\nof LA Displacement Vector Field (DVF) magnitude and LA principal strain values\nfrom images of 10 healthy volunteers and 8 patients with cardiovascular disease\n(CVD), of which 2 had large left ventricular ejection fraction (LVEF)\nimpairment. We additionally create an atlas of these biomarkers using the data\nfrom the healthy volunteers. Results showed that Aladdin can accurately track\nthe LA wall across the cardiac cycle and characterize its motion and\ndeformation. Global LA function markers assessed with Aladdin agree well with\nestimates from 2D Cine MRI. A more marked active contraction phase was observed\nin the healthy cohort, while the CVD LVEF group showed overall reduced LA\nfunction. Aladdin is uniquely able to identify LA regions with abnormal\ndeformation metrics that may indicate focal pathology. We expect Aladdin to\nhave important clinical applications as it can non-invasively characterize\natrial pathophysiology. All source code and data are available at:\nhttps://github.com/cgalaz01/aladdin_cmr_la.\n", "link": "http://arxiv.org/abs/2312.09387v2", "date": "2024-09-18", "relevancy": 2.4749, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4974}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Resolution%20Maps%20of%20Left%20Atrial%20Displacements%20and%20Strains%20Estimated%0A%20%20with%203D%20Cine%20MRI%20using%20Online%20Learning%20Neural%20Networks&body=Title%3A%20High-Resolution%20Maps%20of%20Left%20Atrial%20Displacements%20and%20Strains%20Estimated%0A%20%20with%203D%20Cine%20MRI%20using%20Online%20Learning%20Neural%20Networks%0AAuthor%3A%20Christoforos%20Galazis%20and%20Samuel%20Shepperd%20and%20Emma%20Brouwer%20and%20Sandro%20Queir%C3%B3s%20and%20Ebraham%20Alskaf%20and%20Mustafa%20Anjari%20and%20Amedeo%20Chiribiri%20and%20Jack%20Lee%20and%20Anil%20A.%20Bharath%20and%20Marta%20Varela%0AAbstract%3A%20%20%20The%20functional%20analysis%20of%20the%20left%20atrium%20%28LA%29%20is%20important%20for%20evaluating%0Acardiac%20health%20and%20understanding%20diseases%20like%20atrial%20fibrillation.%20Cine%20MRI%20is%0Aideally%20placed%20for%20the%20detailed%203D%20characterization%20of%20LA%20motion%20and%0Adeformation%20but%20is%20lacking%20appropriate%20acquisition%20and%20analysis%20tools.%20Here%2C%20we%0Apropose%20tools%20for%20the%20Analysis%20for%20Left%20Atrial%20Displacements%20and%20DeformatIons%0Ausing%20online%20learning%20neural%20Networks%20%28Aladdin%29%20and%20present%20a%20technical%0Afeasibility%20study%20on%20how%20Aladdin%20can%20characterize%203D%20LA%20function%20globally%20and%0Aregionally.%20Aladdin%20includes%20an%20online%20segmentation%20and%20image%20registration%0Anetwork%2C%20and%20a%20strain%20calculation%20pipeline%20tailored%20to%20the%20LA.%20We%20create%20maps%0Aof%20LA%20Displacement%20Vector%20Field%20%28DVF%29%20magnitude%20and%20LA%20principal%20strain%20values%0Afrom%20images%20of%2010%20healthy%20volunteers%20and%208%20patients%20with%20cardiovascular%20disease%0A%28CVD%29%2C%20of%20which%202%20had%20large%20left%20ventricular%20ejection%20fraction%20%28LVEF%29%0Aimpairment.%20We%20additionally%20create%20an%20atlas%20of%20these%20biomarkers%20using%20the%20data%0Afrom%20the%20healthy%20volunteers.%20Results%20showed%20that%20Aladdin%20can%20accurately%20track%0Athe%20LA%20wall%20across%20the%20cardiac%20cycle%20and%20characterize%20its%20motion%20and%0Adeformation.%20Global%20LA%20function%20markers%20assessed%20with%20Aladdin%20agree%20well%20with%0Aestimates%20from%202D%20Cine%20MRI.%20A%20more%20marked%20active%20contraction%20phase%20was%20observed%0Ain%20the%20healthy%20cohort%2C%20while%20the%20CVD%20LVEF%20group%20showed%20overall%20reduced%20LA%0Afunction.%20Aladdin%20is%20uniquely%20able%20to%20identify%20LA%20regions%20with%20abnormal%0Adeformation%20metrics%20that%20may%20indicate%20focal%20pathology.%20We%20expect%20Aladdin%20to%0Ahave%20important%20clinical%20applications%20as%20it%20can%20non-invasively%20characterize%0Aatrial%20pathophysiology.%20All%20source%20code%20and%20data%20are%20available%20at%3A%0Ahttps%3A//github.com/cgalaz01/aladdin_cmr_la.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Resolution%2520Maps%2520of%2520Left%2520Atrial%2520Displacements%2520and%2520Strains%2520Estimated%250A%2520%2520with%25203D%2520Cine%2520MRI%2520using%2520Online%2520Learning%2520Neural%2520Networks%26entry.906535625%3DChristoforos%2520Galazis%2520and%2520Samuel%2520Shepperd%2520and%2520Emma%2520Brouwer%2520and%2520Sandro%2520Queir%25C3%25B3s%2520and%2520Ebraham%2520Alskaf%2520and%2520Mustafa%2520Anjari%2520and%2520Amedeo%2520Chiribiri%2520and%2520Jack%2520Lee%2520and%2520Anil%2520A.%2520Bharath%2520and%2520Marta%2520Varela%26entry.1292438233%3D%2520%2520The%2520functional%2520analysis%2520of%2520the%2520left%2520atrium%2520%2528LA%2529%2520is%2520important%2520for%2520evaluating%250Acardiac%2520health%2520and%2520understanding%2520diseases%2520like%2520atrial%2520fibrillation.%2520Cine%2520MRI%2520is%250Aideally%2520placed%2520for%2520the%2520detailed%25203D%2520characterization%2520of%2520LA%2520motion%2520and%250Adeformation%2520but%2520is%2520lacking%2520appropriate%2520acquisition%2520and%2520analysis%2520tools.%2520Here%252C%2520we%250Apropose%2520tools%2520for%2520the%2520Analysis%2520for%2520Left%2520Atrial%2520Displacements%2520and%2520DeformatIons%250Ausing%2520online%2520learning%2520neural%2520Networks%2520%2528Aladdin%2529%2520and%2520present%2520a%2520technical%250Afeasibility%2520study%2520on%2520how%2520Aladdin%2520can%2520characterize%25203D%2520LA%2520function%2520globally%2520and%250Aregionally.%2520Aladdin%2520includes%2520an%2520online%2520segmentation%2520and%2520image%2520registration%250Anetwork%252C%2520and%2520a%2520strain%2520calculation%2520pipeline%2520tailored%2520to%2520the%2520LA.%2520We%2520create%2520maps%250Aof%2520LA%2520Displacement%2520Vector%2520Field%2520%2528DVF%2529%2520magnitude%2520and%2520LA%2520principal%2520strain%2520values%250Afrom%2520images%2520of%252010%2520healthy%2520volunteers%2520and%25208%2520patients%2520with%2520cardiovascular%2520disease%250A%2528CVD%2529%252C%2520of%2520which%25202%2520had%2520large%2520left%2520ventricular%2520ejection%2520fraction%2520%2528LVEF%2529%250Aimpairment.%2520We%2520additionally%2520create%2520an%2520atlas%2520of%2520these%2520biomarkers%2520using%2520the%2520data%250Afrom%2520the%2520healthy%2520volunteers.%2520Results%2520showed%2520that%2520Aladdin%2520can%2520accurately%2520track%250Athe%2520LA%2520wall%2520across%2520the%2520cardiac%2520cycle%2520and%2520characterize%2520its%2520motion%2520and%250Adeformation.%2520Global%2520LA%2520function%2520markers%2520assessed%2520with%2520Aladdin%2520agree%2520well%2520with%250Aestimates%2520from%25202D%2520Cine%2520MRI.%2520A%2520more%2520marked%2520active%2520contraction%2520phase%2520was%2520observed%250Ain%2520the%2520healthy%2520cohort%252C%2520while%2520the%2520CVD%2520LVEF%2520group%2520showed%2520overall%2520reduced%2520LA%250Afunction.%2520Aladdin%2520is%2520uniquely%2520able%2520to%2520identify%2520LA%2520regions%2520with%2520abnormal%250Adeformation%2520metrics%2520that%2520may%2520indicate%2520focal%2520pathology.%2520We%2520expect%2520Aladdin%2520to%250Ahave%2520important%2520clinical%2520applications%2520as%2520it%2520can%2520non-invasively%2520characterize%250Aatrial%2520pathophysiology.%2520All%2520source%2520code%2520and%2520data%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/cgalaz01/aladdin_cmr_la.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Resolution%20Maps%20of%20Left%20Atrial%20Displacements%20and%20Strains%20Estimated%0A%20%20with%203D%20Cine%20MRI%20using%20Online%20Learning%20Neural%20Networks&entry.906535625=Christoforos%20Galazis%20and%20Samuel%20Shepperd%20and%20Emma%20Brouwer%20and%20Sandro%20Queir%C3%B3s%20and%20Ebraham%20Alskaf%20and%20Mustafa%20Anjari%20and%20Amedeo%20Chiribiri%20and%20Jack%20Lee%20and%20Anil%20A.%20Bharath%20and%20Marta%20Varela&entry.1292438233=%20%20The%20functional%20analysis%20of%20the%20left%20atrium%20%28LA%29%20is%20important%20for%20evaluating%0Acardiac%20health%20and%20understanding%20diseases%20like%20atrial%20fibrillation.%20Cine%20MRI%20is%0Aideally%20placed%20for%20the%20detailed%203D%20characterization%20of%20LA%20motion%20and%0Adeformation%20but%20is%20lacking%20appropriate%20acquisition%20and%20analysis%20tools.%20Here%2C%20we%0Apropose%20tools%20for%20the%20Analysis%20for%20Left%20Atrial%20Displacements%20and%20DeformatIons%0Ausing%20online%20learning%20neural%20Networks%20%28Aladdin%29%20and%20present%20a%20technical%0Afeasibility%20study%20on%20how%20Aladdin%20can%20characterize%203D%20LA%20function%20globally%20and%0Aregionally.%20Aladdin%20includes%20an%20online%20segmentation%20and%20image%20registration%0Anetwork%2C%20and%20a%20strain%20calculation%20pipeline%20tailored%20to%20the%20LA.%20We%20create%20maps%0Aof%20LA%20Displacement%20Vector%20Field%20%28DVF%29%20magnitude%20and%20LA%20principal%20strain%20values%0Afrom%20images%20of%2010%20healthy%20volunteers%20and%208%20patients%20with%20cardiovascular%20disease%0A%28CVD%29%2C%20of%20which%202%20had%20large%20left%20ventricular%20ejection%20fraction%20%28LVEF%29%0Aimpairment.%20We%20additionally%20create%20an%20atlas%20of%20these%20biomarkers%20using%20the%20data%0Afrom%20the%20healthy%20volunteers.%20Results%20showed%20that%20Aladdin%20can%20accurately%20track%0Athe%20LA%20wall%20across%20the%20cardiac%20cycle%20and%20characterize%20its%20motion%20and%0Adeformation.%20Global%20LA%20function%20markers%20assessed%20with%20Aladdin%20agree%20well%20with%0Aestimates%20from%202D%20Cine%20MRI.%20A%20more%20marked%20active%20contraction%20phase%20was%20observed%0Ain%20the%20healthy%20cohort%2C%20while%20the%20CVD%20LVEF%20group%20showed%20overall%20reduced%20LA%0Afunction.%20Aladdin%20is%20uniquely%20able%20to%20identify%20LA%20regions%20with%20abnormal%0Adeformation%20metrics%20that%20may%20indicate%20focal%20pathology.%20We%20expect%20Aladdin%20to%0Ahave%20important%20clinical%20applications%20as%20it%20can%20non-invasively%20characterize%0Aatrial%20pathophysiology.%20All%20source%20code%20and%20data%20are%20available%20at%3A%0Ahttps%3A//github.com/cgalaz01/aladdin_cmr_la.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09387v2&entry.124074799=Read"},
{"title": "Panoptic-Depth Forecasting", "author": "Juana Valeria Hurtado and Riya Mohan and Abhinav Valada", "abstract": "  Forecasting the semantics and 3D structure of scenes is essential for robots\nto navigate and plan actions safely. Recent methods have explored semantic and\npanoptic scene forecasting; however, they do not consider the geometry of the\nscene. In this work, we propose the panoptic-depth forecasting task for jointly\npredicting the panoptic segmentation and depth maps of unobserved future\nframes, from monocular camera images. To facilitate this work, we extend the\npopular KITTI-360 and Cityscapes benchmarks by computing depth maps from LiDAR\npoint clouds and leveraging sequential labeled data. We also introduce a\nsuitable evaluation metric that quantifies both the panoptic quality and depth\nestimation accuracy of forecasts in a coherent manner. Furthermore, we present\ntwo baselines and propose the novel PDcast architecture that learns rich\nspatio-temporal representations by incorporating a transformer-based encoder, a\nforecasting module, and task-specific decoders to predict future panoptic-depth\noutputs. Extensive evaluations demonstrate the effectiveness of PDcast across\ntwo datasets and three forecasting tasks, consistently addressing the primary\nchallenges. We make the code publicly available at\nhttps://pdcast.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2409.12008v1", "date": "2024-09-18", "relevancy": 2.4409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6314}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panoptic-Depth%20Forecasting&body=Title%3A%20Panoptic-Depth%20Forecasting%0AAuthor%3A%20Juana%20Valeria%20Hurtado%20and%20Riya%20Mohan%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Forecasting%20the%20semantics%20and%203D%20structure%20of%20scenes%20is%20essential%20for%20robots%0Ato%20navigate%20and%20plan%20actions%20safely.%20Recent%20methods%20have%20explored%20semantic%20and%0Apanoptic%20scene%20forecasting%3B%20however%2C%20they%20do%20not%20consider%20the%20geometry%20of%20the%0Ascene.%20In%20this%20work%2C%20we%20propose%20the%20panoptic-depth%20forecasting%20task%20for%20jointly%0Apredicting%20the%20panoptic%20segmentation%20and%20depth%20maps%20of%20unobserved%20future%0Aframes%2C%20from%20monocular%20camera%20images.%20To%20facilitate%20this%20work%2C%20we%20extend%20the%0Apopular%20KITTI-360%20and%20Cityscapes%20benchmarks%20by%20computing%20depth%20maps%20from%20LiDAR%0Apoint%20clouds%20and%20leveraging%20sequential%20labeled%20data.%20We%20also%20introduce%20a%0Asuitable%20evaluation%20metric%20that%20quantifies%20both%20the%20panoptic%20quality%20and%20depth%0Aestimation%20accuracy%20of%20forecasts%20in%20a%20coherent%20manner.%20Furthermore%2C%20we%20present%0Atwo%20baselines%20and%20propose%20the%20novel%20PDcast%20architecture%20that%20learns%20rich%0Aspatio-temporal%20representations%20by%20incorporating%20a%20transformer-based%20encoder%2C%20a%0Aforecasting%20module%2C%20and%20task-specific%20decoders%20to%20predict%20future%20panoptic-depth%0Aoutputs.%20Extensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20PDcast%20across%0Atwo%20datasets%20and%20three%20forecasting%20tasks%2C%20consistently%20addressing%20the%20primary%0Achallenges.%20We%20make%20the%20code%20publicly%20available%20at%0Ahttps%3A//pdcast.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoptic-Depth%2520Forecasting%26entry.906535625%3DJuana%2520Valeria%2520Hurtado%2520and%2520Riya%2520Mohan%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Forecasting%2520the%2520semantics%2520and%25203D%2520structure%2520of%2520scenes%2520is%2520essential%2520for%2520robots%250Ato%2520navigate%2520and%2520plan%2520actions%2520safely.%2520Recent%2520methods%2520have%2520explored%2520semantic%2520and%250Apanoptic%2520scene%2520forecasting%253B%2520however%252C%2520they%2520do%2520not%2520consider%2520the%2520geometry%2520of%2520the%250Ascene.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520panoptic-depth%2520forecasting%2520task%2520for%2520jointly%250Apredicting%2520the%2520panoptic%2520segmentation%2520and%2520depth%2520maps%2520of%2520unobserved%2520future%250Aframes%252C%2520from%2520monocular%2520camera%2520images.%2520To%2520facilitate%2520this%2520work%252C%2520we%2520extend%2520the%250Apopular%2520KITTI-360%2520and%2520Cityscapes%2520benchmarks%2520by%2520computing%2520depth%2520maps%2520from%2520LiDAR%250Apoint%2520clouds%2520and%2520leveraging%2520sequential%2520labeled%2520data.%2520We%2520also%2520introduce%2520a%250Asuitable%2520evaluation%2520metric%2520that%2520quantifies%2520both%2520the%2520panoptic%2520quality%2520and%2520depth%250Aestimation%2520accuracy%2520of%2520forecasts%2520in%2520a%2520coherent%2520manner.%2520Furthermore%252C%2520we%2520present%250Atwo%2520baselines%2520and%2520propose%2520the%2520novel%2520PDcast%2520architecture%2520that%2520learns%2520rich%250Aspatio-temporal%2520representations%2520by%2520incorporating%2520a%2520transformer-based%2520encoder%252C%2520a%250Aforecasting%2520module%252C%2520and%2520task-specific%2520decoders%2520to%2520predict%2520future%2520panoptic-depth%250Aoutputs.%2520Extensive%2520evaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520PDcast%2520across%250Atwo%2520datasets%2520and%2520three%2520forecasting%2520tasks%252C%2520consistently%2520addressing%2520the%2520primary%250Achallenges.%2520We%2520make%2520the%2520code%2520publicly%2520available%2520at%250Ahttps%253A//pdcast.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panoptic-Depth%20Forecasting&entry.906535625=Juana%20Valeria%20Hurtado%20and%20Riya%20Mohan%20and%20Abhinav%20Valada&entry.1292438233=%20%20Forecasting%20the%20semantics%20and%203D%20structure%20of%20scenes%20is%20essential%20for%20robots%0Ato%20navigate%20and%20plan%20actions%20safely.%20Recent%20methods%20have%20explored%20semantic%20and%0Apanoptic%20scene%20forecasting%3B%20however%2C%20they%20do%20not%20consider%20the%20geometry%20of%20the%0Ascene.%20In%20this%20work%2C%20we%20propose%20the%20panoptic-depth%20forecasting%20task%20for%20jointly%0Apredicting%20the%20panoptic%20segmentation%20and%20depth%20maps%20of%20unobserved%20future%0Aframes%2C%20from%20monocular%20camera%20images.%20To%20facilitate%20this%20work%2C%20we%20extend%20the%0Apopular%20KITTI-360%20and%20Cityscapes%20benchmarks%20by%20computing%20depth%20maps%20from%20LiDAR%0Apoint%20clouds%20and%20leveraging%20sequential%20labeled%20data.%20We%20also%20introduce%20a%0Asuitable%20evaluation%20metric%20that%20quantifies%20both%20the%20panoptic%20quality%20and%20depth%0Aestimation%20accuracy%20of%20forecasts%20in%20a%20coherent%20manner.%20Furthermore%2C%20we%20present%0Atwo%20baselines%20and%20propose%20the%20novel%20PDcast%20architecture%20that%20learns%20rich%0Aspatio-temporal%20representations%20by%20incorporating%20a%20transformer-based%20encoder%2C%20a%0Aforecasting%20module%2C%20and%20task-specific%20decoders%20to%20predict%20future%20panoptic-depth%0Aoutputs.%20Extensive%20evaluations%20demonstrate%20the%20effectiveness%20of%20PDcast%20across%0Atwo%20datasets%20and%20three%20forecasting%20tasks%2C%20consistently%20addressing%20the%20primary%0Achallenges.%20We%20make%20the%20code%20publicly%20available%20at%0Ahttps%3A//pdcast.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12008v1&entry.124074799=Read"},
{"title": "MitoSeg: Mitochondria Segmentation Tool", "author": "Faris Serdar Ta\u015fel and Efe \u00c7iftci", "abstract": "  Recent studies suggest a potential link between the physical structure of\nmitochondria and neurodegenerative diseases. With advances in Electron\nMicroscopy techniques, it has become possible to visualize the boundary and\ninternal membrane structures of mitochondria in detail. It is crucial to\nautomatically segment mitochondria from these images to investigate the\nrelationship between mitochondria and diseases. In this paper, we present a\nsoftware solution for mitochondrial segmentation, highlighting mitochondria\nboundaries in electron microscopy tomography images and generating\ncorresponding 3D meshes.\n", "link": "http://arxiv.org/abs/2409.11974v1", "date": "2024-09-18", "relevancy": 2.4103, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5037}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5037}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MitoSeg%3A%20Mitochondria%20Segmentation%20Tool&body=Title%3A%20MitoSeg%3A%20Mitochondria%20Segmentation%20Tool%0AAuthor%3A%20Faris%20Serdar%20Ta%C5%9Fel%20and%20Efe%20%C3%87iftci%0AAbstract%3A%20%20%20Recent%20studies%20suggest%20a%20potential%20link%20between%20the%20physical%20structure%20of%0Amitochondria%20and%20neurodegenerative%20diseases.%20With%20advances%20in%20Electron%0AMicroscopy%20techniques%2C%20it%20has%20become%20possible%20to%20visualize%20the%20boundary%20and%0Ainternal%20membrane%20structures%20of%20mitochondria%20in%20detail.%20It%20is%20crucial%20to%0Aautomatically%20segment%20mitochondria%20from%20these%20images%20to%20investigate%20the%0Arelationship%20between%20mitochondria%20and%20diseases.%20In%20this%20paper%2C%20we%20present%20a%0Asoftware%20solution%20for%20mitochondrial%20segmentation%2C%20highlighting%20mitochondria%0Aboundaries%20in%20electron%20microscopy%20tomography%20images%20and%20generating%0Acorresponding%203D%20meshes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitoSeg%253A%2520Mitochondria%2520Segmentation%2520Tool%26entry.906535625%3DFaris%2520Serdar%2520Ta%25C5%259Fel%2520and%2520Efe%2520%25C3%2587iftci%26entry.1292438233%3D%2520%2520Recent%2520studies%2520suggest%2520a%2520potential%2520link%2520between%2520the%2520physical%2520structure%2520of%250Amitochondria%2520and%2520neurodegenerative%2520diseases.%2520With%2520advances%2520in%2520Electron%250AMicroscopy%2520techniques%252C%2520it%2520has%2520become%2520possible%2520to%2520visualize%2520the%2520boundary%2520and%250Ainternal%2520membrane%2520structures%2520of%2520mitochondria%2520in%2520detail.%2520It%2520is%2520crucial%2520to%250Aautomatically%2520segment%2520mitochondria%2520from%2520these%2520images%2520to%2520investigate%2520the%250Arelationship%2520between%2520mitochondria%2520and%2520diseases.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Asoftware%2520solution%2520for%2520mitochondrial%2520segmentation%252C%2520highlighting%2520mitochondria%250Aboundaries%2520in%2520electron%2520microscopy%2520tomography%2520images%2520and%2520generating%250Acorresponding%25203D%2520meshes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MitoSeg%3A%20Mitochondria%20Segmentation%20Tool&entry.906535625=Faris%20Serdar%20Ta%C5%9Fel%20and%20Efe%20%C3%87iftci&entry.1292438233=%20%20Recent%20studies%20suggest%20a%20potential%20link%20between%20the%20physical%20structure%20of%0Amitochondria%20and%20neurodegenerative%20diseases.%20With%20advances%20in%20Electron%0AMicroscopy%20techniques%2C%20it%20has%20become%20possible%20to%20visualize%20the%20boundary%20and%0Ainternal%20membrane%20structures%20of%20mitochondria%20in%20detail.%20It%20is%20crucial%20to%0Aautomatically%20segment%20mitochondria%20from%20these%20images%20to%20investigate%20the%0Arelationship%20between%20mitochondria%20and%20diseases.%20In%20this%20paper%2C%20we%20present%20a%0Asoftware%20solution%20for%20mitochondrial%20segmentation%2C%20highlighting%20mitochondria%0Aboundaries%20in%20electron%20microscopy%20tomography%20images%20and%20generating%0Acorresponding%203D%20meshes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11974v1&entry.124074799=Read"},
{"title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via\n  Self-Improvement", "author": "An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang", "abstract": "  In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.\n", "link": "http://arxiv.org/abs/2409.12122v1", "date": "2024-09-18", "relevancy": 2.385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen2.5-Math%20Technical%20Report%3A%20Toward%20Mathematical%20Expert%20Model%20via%0A%20%20Self-Improvement&body=Title%3A%20Qwen2.5-Math%20Technical%20Report%3A%20Toward%20Mathematical%20Expert%20Model%20via%0A%20%20Self-Improvement%0AAuthor%3A%20An%20Yang%20and%20Beichen%20Zhang%20and%20Binyuan%20Hui%20and%20Bofei%20Gao%20and%20Bowen%20Yu%20and%20Chengpeng%20Li%20and%20Dayiheng%20Liu%20and%20Jianhong%20Tu%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%20and%20Keming%20Lu%20and%20Mingfeng%20Xue%20and%20Runji%20Lin%20and%20Tianyu%20Liu%20and%20Xingzhang%20Ren%20and%20Zhenru%20Zhang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20present%20a%20series%20of%20math-specific%20large%20language%20models%3A%0AQwen2.5-Math%20and%20Qwen2.5-Math-Instruct-1.5B/7B/72B.%20The%20core%20innovation%20of%20the%0AQwen2.5%20series%20lies%20in%20integrating%20the%20philosophy%20of%20self-improvement%0Athroughout%20the%20entire%20pipeline%2C%20from%20pre-training%20and%20post-training%20to%0Ainference%3A%20%281%29%20During%20the%20pre-training%20phase%2C%20Qwen2-Math-Instruct%20is%20utilized%0Ato%20generate%20large-scale%2C%20high-quality%20mathematical%20data.%20%282%29%20In%20the%0Apost-training%20phase%2C%20we%20develop%20a%20reward%20model%20%28RM%29%20by%20conducting%20massive%0Asampling%20from%20Qwen2-Math-Instruct.%20This%20RM%20is%20then%20applied%20to%20the%20iterative%0Aevolution%20of%20data%20in%20supervised%20fine-tuning%20%28SFT%29.%20With%20a%20stronger%20SFT%20model%2C%0Ait%27s%20possible%20to%20iteratively%20train%20and%20update%20the%20RM%2C%20which%20in%20turn%20guides%20the%0Anext%20round%20of%20SFT%20data%20iteration.%20On%20the%20final%20SFT%20model%2C%20we%20employ%20the%0Aultimate%20RM%20for%20reinforcement%20learning%2C%20resulting%20in%20the%20Qwen2.5-Math-Instruct.%0A%283%29%20Furthermore%2C%20during%20the%20inference%20stage%2C%20the%20RM%20is%20used%20to%20guide%20sampling%2C%0Aoptimizing%20the%20model%27s%20performance.%0A%20%20Qwen2.5-Math-Instruct%20supports%20both%20Chinese%20and%20English%2C%20and%20possess%20advanced%0Amathematical%20reasoning%20capabilities%2C%20including%20Chain-of-Thought%20%28CoT%29%20and%0ATool-Integrated%20Reasoning%20%28TIR%29.%20We%20evaluate%20our%20models%20on%2010%20mathematics%0Adatasets%20in%20both%20English%20and%20Chinese%2C%20such%20as%20GSM8K%2C%20MATH%2C%20GaoKao%2C%20AMC23%2C%20and%0AAIME24%2C%20covering%20a%20range%20of%20difficulties%20from%20grade%20school%20level%20to%20math%0Acompetition%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen2.5-Math%2520Technical%2520Report%253A%2520Toward%2520Mathematical%2520Expert%2520Model%2520via%250A%2520%2520Self-Improvement%26entry.906535625%3DAn%2520Yang%2520and%2520Beichen%2520Zhang%2520and%2520Binyuan%2520Hui%2520and%2520Bofei%2520Gao%2520and%2520Bowen%2520Yu%2520and%2520Chengpeng%2520Li%2520and%2520Dayiheng%2520Liu%2520and%2520Jianhong%2520Tu%2520and%2520Jingren%2520Zhou%2520and%2520Junyang%2520Lin%2520and%2520Keming%2520Lu%2520and%2520Mingfeng%2520Xue%2520and%2520Runji%2520Lin%2520and%2520Tianyu%2520Liu%2520and%2520Xingzhang%2520Ren%2520and%2520Zhenru%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520present%2520a%2520series%2520of%2520math-specific%2520large%2520language%2520models%253A%250AQwen2.5-Math%2520and%2520Qwen2.5-Math-Instruct-1.5B/7B/72B.%2520The%2520core%2520innovation%2520of%2520the%250AQwen2.5%2520series%2520lies%2520in%2520integrating%2520the%2520philosophy%2520of%2520self-improvement%250Athroughout%2520the%2520entire%2520pipeline%252C%2520from%2520pre-training%2520and%2520post-training%2520to%250Ainference%253A%2520%25281%2529%2520During%2520the%2520pre-training%2520phase%252C%2520Qwen2-Math-Instruct%2520is%2520utilized%250Ato%2520generate%2520large-scale%252C%2520high-quality%2520mathematical%2520data.%2520%25282%2529%2520In%2520the%250Apost-training%2520phase%252C%2520we%2520develop%2520a%2520reward%2520model%2520%2528RM%2529%2520by%2520conducting%2520massive%250Asampling%2520from%2520Qwen2-Math-Instruct.%2520This%2520RM%2520is%2520then%2520applied%2520to%2520the%2520iterative%250Aevolution%2520of%2520data%2520in%2520supervised%2520fine-tuning%2520%2528SFT%2529.%2520With%2520a%2520stronger%2520SFT%2520model%252C%250Ait%2527s%2520possible%2520to%2520iteratively%2520train%2520and%2520update%2520the%2520RM%252C%2520which%2520in%2520turn%2520guides%2520the%250Anext%2520round%2520of%2520SFT%2520data%2520iteration.%2520On%2520the%2520final%2520SFT%2520model%252C%2520we%2520employ%2520the%250Aultimate%2520RM%2520for%2520reinforcement%2520learning%252C%2520resulting%2520in%2520the%2520Qwen2.5-Math-Instruct.%250A%25283%2529%2520Furthermore%252C%2520during%2520the%2520inference%2520stage%252C%2520the%2520RM%2520is%2520used%2520to%2520guide%2520sampling%252C%250Aoptimizing%2520the%2520model%2527s%2520performance.%250A%2520%2520Qwen2.5-Math-Instruct%2520supports%2520both%2520Chinese%2520and%2520English%252C%2520and%2520possess%2520advanced%250Amathematical%2520reasoning%2520capabilities%252C%2520including%2520Chain-of-Thought%2520%2528CoT%2529%2520and%250ATool-Integrated%2520Reasoning%2520%2528TIR%2529.%2520We%2520evaluate%2520our%2520models%2520on%252010%2520mathematics%250Adatasets%2520in%2520both%2520English%2520and%2520Chinese%252C%2520such%2520as%2520GSM8K%252C%2520MATH%252C%2520GaoKao%252C%2520AMC23%252C%2520and%250AAIME24%252C%2520covering%2520a%2520range%2520of%2520difficulties%2520from%2520grade%2520school%2520level%2520to%2520math%250Acompetition%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen2.5-Math%20Technical%20Report%3A%20Toward%20Mathematical%20Expert%20Model%20via%0A%20%20Self-Improvement&entry.906535625=An%20Yang%20and%20Beichen%20Zhang%20and%20Binyuan%20Hui%20and%20Bofei%20Gao%20and%20Bowen%20Yu%20and%20Chengpeng%20Li%20and%20Dayiheng%20Liu%20and%20Jianhong%20Tu%20and%20Jingren%20Zhou%20and%20Junyang%20Lin%20and%20Keming%20Lu%20and%20Mingfeng%20Xue%20and%20Runji%20Lin%20and%20Tianyu%20Liu%20and%20Xingzhang%20Ren%20and%20Zhenru%20Zhang&entry.1292438233=%20%20In%20this%20report%2C%20we%20present%20a%20series%20of%20math-specific%20large%20language%20models%3A%0AQwen2.5-Math%20and%20Qwen2.5-Math-Instruct-1.5B/7B/72B.%20The%20core%20innovation%20of%20the%0AQwen2.5%20series%20lies%20in%20integrating%20the%20philosophy%20of%20self-improvement%0Athroughout%20the%20entire%20pipeline%2C%20from%20pre-training%20and%20post-training%20to%0Ainference%3A%20%281%29%20During%20the%20pre-training%20phase%2C%20Qwen2-Math-Instruct%20is%20utilized%0Ato%20generate%20large-scale%2C%20high-quality%20mathematical%20data.%20%282%29%20In%20the%0Apost-training%20phase%2C%20we%20develop%20a%20reward%20model%20%28RM%29%20by%20conducting%20massive%0Asampling%20from%20Qwen2-Math-Instruct.%20This%20RM%20is%20then%20applied%20to%20the%20iterative%0Aevolution%20of%20data%20in%20supervised%20fine-tuning%20%28SFT%29.%20With%20a%20stronger%20SFT%20model%2C%0Ait%27s%20possible%20to%20iteratively%20train%20and%20update%20the%20RM%2C%20which%20in%20turn%20guides%20the%0Anext%20round%20of%20SFT%20data%20iteration.%20On%20the%20final%20SFT%20model%2C%20we%20employ%20the%0Aultimate%20RM%20for%20reinforcement%20learning%2C%20resulting%20in%20the%20Qwen2.5-Math-Instruct.%0A%283%29%20Furthermore%2C%20during%20the%20inference%20stage%2C%20the%20RM%20is%20used%20to%20guide%20sampling%2C%0Aoptimizing%20the%20model%27s%20performance.%0A%20%20Qwen2.5-Math-Instruct%20supports%20both%20Chinese%20and%20English%2C%20and%20possess%20advanced%0Amathematical%20reasoning%20capabilities%2C%20including%20Chain-of-Thought%20%28CoT%29%20and%0ATool-Integrated%20Reasoning%20%28TIR%29.%20We%20evaluate%20our%20models%20on%2010%20mathematics%0Adatasets%20in%20both%20English%20and%20Chinese%2C%20such%20as%20GSM8K%2C%20MATH%2C%20GaoKao%2C%20AMC23%2C%20and%0AAIME24%2C%20covering%20a%20range%20of%20difficulties%20from%20grade%20school%20level%20to%20math%0Acompetition%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12122v1&entry.124074799=Read"},
{"title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference", "author": "Najmeh Forouzandehmehr and Nima Farrokhsiar and Ramin Giahi and Evren Korpeoglu and Kannan Achan", "abstract": "  Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.\n", "link": "http://arxiv.org/abs/2409.12150v1", "date": "2024-09-18", "relevancy": 2.3635, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6754}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5983}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Style%3A%20Efficient%20Fine-Tuning%20of%20LLMs%20for%20Image-Guided%20Outfit%0A%20%20Recommendation%20with%20Preference&body=Title%3A%20Decoding%20Style%3A%20Efficient%20Fine-Tuning%20of%20LLMs%20for%20Image-Guided%20Outfit%0A%20%20Recommendation%20with%20Preference%0AAuthor%3A%20Najmeh%20Forouzandehmehr%20and%20Nima%20Farrokhsiar%20and%20Ramin%20Giahi%20and%20Evren%20Korpeoglu%20and%20Kannan%20Achan%0AAbstract%3A%20%20%20Personalized%20outfit%20recommendation%20remains%20a%20complex%20challenge%2C%20demanding%0Aboth%20fashion%20compatibility%20understanding%20and%20trend%20awareness.%20This%20paper%0Apresents%20a%20novel%20framework%20that%20harnesses%20the%20expressive%20power%20of%20large%0Alanguage%20models%20%28LLMs%29%20for%20this%20task%2C%20mitigating%20their%20%22black%20box%22%20and%20static%0Anature%20through%20fine-tuning%20and%20direct%20feedback%20integration.%20We%20bridge%20the%20item%0Avisual-textual%20gap%20in%20items%20descriptions%20by%20employing%20image%20captioning%20with%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29.%20This%20enables%20the%20LLM%20to%20extract%20style%0Aand%20color%20characteristics%20from%20human-curated%20fashion%20images%2C%20forming%20the%20basis%0Afor%20personalized%20recommendations.%20The%20LLM%20is%20efficiently%20fine-tuned%20on%20the%0Aopen-source%20Polyvore%20dataset%20of%20curated%20fashion%20images%2C%20optimizing%20its%20ability%0Ato%20recommend%20stylish%20outfits.%20A%20direct%20preference%20mechanism%20using%20negative%0Aexamples%20is%20employed%20to%20enhance%20the%20LLM%27s%20decision-making%20process.%20This%20creates%0Aa%20self-enhancing%20AI%20feedback%20loop%20that%20continuously%20refines%20recommendations%20in%0Aline%20with%20seasonal%20fashion%20trends.%20Our%20framework%20is%20evaluated%20on%20the%20Polyvore%0Adataset%2C%20demonstrating%20its%20effectiveness%20in%20two%20key%20tasks%3A%20fill-in-the-blank%2C%0Aand%20complementary%20item%20retrieval.%20These%20evaluations%20underline%20the%20framework%27s%0Aability%20to%20generate%20stylish%2C%20trend-aligned%20outfit%20suggestions%2C%20continuously%0Aimproving%20through%20direct%20feedback.%20The%20evaluation%20results%20demonstrated%20that%20our%0Aproposed%20framework%20significantly%20outperforms%20the%20base%20LLM%2C%20creating%20more%0Acohesive%20outfits.%20The%20improved%20performance%20in%20these%20tasks%20underscores%20the%0Aproposed%20framework%27s%20potential%20to%20enhance%20the%20shopping%20experience%20with%20accurate%0Asuggestions%2C%20proving%20its%20effectiveness%20over%20the%20vanilla%20LLM%20based%20outfit%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Style%253A%2520Efficient%2520Fine-Tuning%2520of%2520LLMs%2520for%2520Image-Guided%2520Outfit%250A%2520%2520Recommendation%2520with%2520Preference%26entry.906535625%3DNajmeh%2520Forouzandehmehr%2520and%2520Nima%2520Farrokhsiar%2520and%2520Ramin%2520Giahi%2520and%2520Evren%2520Korpeoglu%2520and%2520Kannan%2520Achan%26entry.1292438233%3D%2520%2520Personalized%2520outfit%2520recommendation%2520remains%2520a%2520complex%2520challenge%252C%2520demanding%250Aboth%2520fashion%2520compatibility%2520understanding%2520and%2520trend%2520awareness.%2520This%2520paper%250Apresents%2520a%2520novel%2520framework%2520that%2520harnesses%2520the%2520expressive%2520power%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520for%2520this%2520task%252C%2520mitigating%2520their%2520%2522black%2520box%2522%2520and%2520static%250Anature%2520through%2520fine-tuning%2520and%2520direct%2520feedback%2520integration.%2520We%2520bridge%2520the%2520item%250Avisual-textual%2520gap%2520in%2520items%2520descriptions%2520by%2520employing%2520image%2520captioning%2520with%2520a%250AMultimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529.%2520This%2520enables%2520the%2520LLM%2520to%2520extract%2520style%250Aand%2520color%2520characteristics%2520from%2520human-curated%2520fashion%2520images%252C%2520forming%2520the%2520basis%250Afor%2520personalized%2520recommendations.%2520The%2520LLM%2520is%2520efficiently%2520fine-tuned%2520on%2520the%250Aopen-source%2520Polyvore%2520dataset%2520of%2520curated%2520fashion%2520images%252C%2520optimizing%2520its%2520ability%250Ato%2520recommend%2520stylish%2520outfits.%2520A%2520direct%2520preference%2520mechanism%2520using%2520negative%250Aexamples%2520is%2520employed%2520to%2520enhance%2520the%2520LLM%2527s%2520decision-making%2520process.%2520This%2520creates%250Aa%2520self-enhancing%2520AI%2520feedback%2520loop%2520that%2520continuously%2520refines%2520recommendations%2520in%250Aline%2520with%2520seasonal%2520fashion%2520trends.%2520Our%2520framework%2520is%2520evaluated%2520on%2520the%2520Polyvore%250Adataset%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520two%2520key%2520tasks%253A%2520fill-in-the-blank%252C%250Aand%2520complementary%2520item%2520retrieval.%2520These%2520evaluations%2520underline%2520the%2520framework%2527s%250Aability%2520to%2520generate%2520stylish%252C%2520trend-aligned%2520outfit%2520suggestions%252C%2520continuously%250Aimproving%2520through%2520direct%2520feedback.%2520The%2520evaluation%2520results%2520demonstrated%2520that%2520our%250Aproposed%2520framework%2520significantly%2520outperforms%2520the%2520base%2520LLM%252C%2520creating%2520more%250Acohesive%2520outfits.%2520The%2520improved%2520performance%2520in%2520these%2520tasks%2520underscores%2520the%250Aproposed%2520framework%2527s%2520potential%2520to%2520enhance%2520the%2520shopping%2520experience%2520with%2520accurate%250Asuggestions%252C%2520proving%2520its%2520effectiveness%2520over%2520the%2520vanilla%2520LLM%2520based%2520outfit%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Style%3A%20Efficient%20Fine-Tuning%20of%20LLMs%20for%20Image-Guided%20Outfit%0A%20%20Recommendation%20with%20Preference&entry.906535625=Najmeh%20Forouzandehmehr%20and%20Nima%20Farrokhsiar%20and%20Ramin%20Giahi%20and%20Evren%20Korpeoglu%20and%20Kannan%20Achan&entry.1292438233=%20%20Personalized%20outfit%20recommendation%20remains%20a%20complex%20challenge%2C%20demanding%0Aboth%20fashion%20compatibility%20understanding%20and%20trend%20awareness.%20This%20paper%0Apresents%20a%20novel%20framework%20that%20harnesses%20the%20expressive%20power%20of%20large%0Alanguage%20models%20%28LLMs%29%20for%20this%20task%2C%20mitigating%20their%20%22black%20box%22%20and%20static%0Anature%20through%20fine-tuning%20and%20direct%20feedback%20integration.%20We%20bridge%20the%20item%0Avisual-textual%20gap%20in%20items%20descriptions%20by%20employing%20image%20captioning%20with%20a%0AMultimodal%20Large%20Language%20Model%20%28MLLM%29.%20This%20enables%20the%20LLM%20to%20extract%20style%0Aand%20color%20characteristics%20from%20human-curated%20fashion%20images%2C%20forming%20the%20basis%0Afor%20personalized%20recommendations.%20The%20LLM%20is%20efficiently%20fine-tuned%20on%20the%0Aopen-source%20Polyvore%20dataset%20of%20curated%20fashion%20images%2C%20optimizing%20its%20ability%0Ato%20recommend%20stylish%20outfits.%20A%20direct%20preference%20mechanism%20using%20negative%0Aexamples%20is%20employed%20to%20enhance%20the%20LLM%27s%20decision-making%20process.%20This%20creates%0Aa%20self-enhancing%20AI%20feedback%20loop%20that%20continuously%20refines%20recommendations%20in%0Aline%20with%20seasonal%20fashion%20trends.%20Our%20framework%20is%20evaluated%20on%20the%20Polyvore%0Adataset%2C%20demonstrating%20its%20effectiveness%20in%20two%20key%20tasks%3A%20fill-in-the-blank%2C%0Aand%20complementary%20item%20retrieval.%20These%20evaluations%20underline%20the%20framework%27s%0Aability%20to%20generate%20stylish%2C%20trend-aligned%20outfit%20suggestions%2C%20continuously%0Aimproving%20through%20direct%20feedback.%20The%20evaluation%20results%20demonstrated%20that%20our%0Aproposed%20framework%20significantly%20outperforms%20the%20base%20LLM%2C%20creating%20more%0Acohesive%20outfits.%20The%20improved%20performance%20in%20these%20tasks%20underscores%20the%0Aproposed%20framework%27s%20potential%20to%20enhance%20the%20shopping%20experience%20with%20accurate%0Asuggestions%2C%20proving%20its%20effectiveness%20over%20the%20vanilla%20LLM%20based%20outfit%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12150v1&entry.124074799=Read"},
{"title": "Unveiling the Black Box: Independent Functional Module Evaluation for\n  Bird's-Eye-View Perception Model", "author": "Ludan Zhang and Xiaokang Ding and Yuqi Dai and Lei He and Keqiang Li", "abstract": "  End-to-end models are emerging as the mainstream in autonomous driving\nperception. However, the inability to meticulously deconstruct their internal\nmechanisms results in diminished development efficacy and impedes the\nestablishment of trust. Pioneering in the issue, we present the Independent\nFunctional Module Evaluation for Bird's-Eye-View Perception Model (BEV-IFME), a\nnovel framework that juxtaposes the module's feature maps against Ground Truth\nwithin a unified semantic Representation Space to quantify their similarity,\nthereby assessing the training maturity of individual functional modules. The\ncore of the framework lies in the process of feature map encoding and\nrepresentation aligning, facilitated by our proposed two-stage Alignment\nAutoEncoder, which ensures the preservation of salient information and the\nconsistency of feature structure. The metric for evaluating the training\nmaturity of functional modules, Similarity Score, demonstrates a robust\npositive correlation with BEV metrics, with an average correlation coefficient\nof 0.9387, attesting to the framework's reliability for assessment purposes.\n", "link": "http://arxiv.org/abs/2409.11969v1", "date": "2024-09-18", "relevancy": 2.272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Black%20Box%3A%20Independent%20Functional%20Module%20Evaluation%20for%0A%20%20Bird%27s-Eye-View%20Perception%20Model&body=Title%3A%20Unveiling%20the%20Black%20Box%3A%20Independent%20Functional%20Module%20Evaluation%20for%0A%20%20Bird%27s-Eye-View%20Perception%20Model%0AAuthor%3A%20Ludan%20Zhang%20and%20Xiaokang%20Ding%20and%20Yuqi%20Dai%20and%20Lei%20He%20and%20Keqiang%20Li%0AAbstract%3A%20%20%20End-to-end%20models%20are%20emerging%20as%20the%20mainstream%20in%20autonomous%20driving%0Aperception.%20However%2C%20the%20inability%20to%20meticulously%20deconstruct%20their%20internal%0Amechanisms%20results%20in%20diminished%20development%20efficacy%20and%20impedes%20the%0Aestablishment%20of%20trust.%20Pioneering%20in%20the%20issue%2C%20we%20present%20the%20Independent%0AFunctional%20Module%20Evaluation%20for%20Bird%27s-Eye-View%20Perception%20Model%20%28BEV-IFME%29%2C%20a%0Anovel%20framework%20that%20juxtaposes%20the%20module%27s%20feature%20maps%20against%20Ground%20Truth%0Awithin%20a%20unified%20semantic%20Representation%20Space%20to%20quantify%20their%20similarity%2C%0Athereby%20assessing%20the%20training%20maturity%20of%20individual%20functional%20modules.%20The%0Acore%20of%20the%20framework%20lies%20in%20the%20process%20of%20feature%20map%20encoding%20and%0Arepresentation%20aligning%2C%20facilitated%20by%20our%20proposed%20two-stage%20Alignment%0AAutoEncoder%2C%20which%20ensures%20the%20preservation%20of%20salient%20information%20and%20the%0Aconsistency%20of%20feature%20structure.%20The%20metric%20for%20evaluating%20the%20training%0Amaturity%20of%20functional%20modules%2C%20Similarity%20Score%2C%20demonstrates%20a%20robust%0Apositive%20correlation%20with%20BEV%20metrics%2C%20with%20an%20average%20correlation%20coefficient%0Aof%200.9387%2C%20attesting%20to%20the%20framework%27s%20reliability%20for%20assessment%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Black%2520Box%253A%2520Independent%2520Functional%2520Module%2520Evaluation%2520for%250A%2520%2520Bird%2527s-Eye-View%2520Perception%2520Model%26entry.906535625%3DLudan%2520Zhang%2520and%2520Xiaokang%2520Ding%2520and%2520Yuqi%2520Dai%2520and%2520Lei%2520He%2520and%2520Keqiang%2520Li%26entry.1292438233%3D%2520%2520End-to-end%2520models%2520are%2520emerging%2520as%2520the%2520mainstream%2520in%2520autonomous%2520driving%250Aperception.%2520However%252C%2520the%2520inability%2520to%2520meticulously%2520deconstruct%2520their%2520internal%250Amechanisms%2520results%2520in%2520diminished%2520development%2520efficacy%2520and%2520impedes%2520the%250Aestablishment%2520of%2520trust.%2520Pioneering%2520in%2520the%2520issue%252C%2520we%2520present%2520the%2520Independent%250AFunctional%2520Module%2520Evaluation%2520for%2520Bird%2527s-Eye-View%2520Perception%2520Model%2520%2528BEV-IFME%2529%252C%2520a%250Anovel%2520framework%2520that%2520juxtaposes%2520the%2520module%2527s%2520feature%2520maps%2520against%2520Ground%2520Truth%250Awithin%2520a%2520unified%2520semantic%2520Representation%2520Space%2520to%2520quantify%2520their%2520similarity%252C%250Athereby%2520assessing%2520the%2520training%2520maturity%2520of%2520individual%2520functional%2520modules.%2520The%250Acore%2520of%2520the%2520framework%2520lies%2520in%2520the%2520process%2520of%2520feature%2520map%2520encoding%2520and%250Arepresentation%2520aligning%252C%2520facilitated%2520by%2520our%2520proposed%2520two-stage%2520Alignment%250AAutoEncoder%252C%2520which%2520ensures%2520the%2520preservation%2520of%2520salient%2520information%2520and%2520the%250Aconsistency%2520of%2520feature%2520structure.%2520The%2520metric%2520for%2520evaluating%2520the%2520training%250Amaturity%2520of%2520functional%2520modules%252C%2520Similarity%2520Score%252C%2520demonstrates%2520a%2520robust%250Apositive%2520correlation%2520with%2520BEV%2520metrics%252C%2520with%2520an%2520average%2520correlation%2520coefficient%250Aof%25200.9387%252C%2520attesting%2520to%2520the%2520framework%2527s%2520reliability%2520for%2520assessment%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Black%20Box%3A%20Independent%20Functional%20Module%20Evaluation%20for%0A%20%20Bird%27s-Eye-View%20Perception%20Model&entry.906535625=Ludan%20Zhang%20and%20Xiaokang%20Ding%20and%20Yuqi%20Dai%20and%20Lei%20He%20and%20Keqiang%20Li&entry.1292438233=%20%20End-to-end%20models%20are%20emerging%20as%20the%20mainstream%20in%20autonomous%20driving%0Aperception.%20However%2C%20the%20inability%20to%20meticulously%20deconstruct%20their%20internal%0Amechanisms%20results%20in%20diminished%20development%20efficacy%20and%20impedes%20the%0Aestablishment%20of%20trust.%20Pioneering%20in%20the%20issue%2C%20we%20present%20the%20Independent%0AFunctional%20Module%20Evaluation%20for%20Bird%27s-Eye-View%20Perception%20Model%20%28BEV-IFME%29%2C%20a%0Anovel%20framework%20that%20juxtaposes%20the%20module%27s%20feature%20maps%20against%20Ground%20Truth%0Awithin%20a%20unified%20semantic%20Representation%20Space%20to%20quantify%20their%20similarity%2C%0Athereby%20assessing%20the%20training%20maturity%20of%20individual%20functional%20modules.%20The%0Acore%20of%20the%20framework%20lies%20in%20the%20process%20of%20feature%20map%20encoding%20and%0Arepresentation%20aligning%2C%20facilitated%20by%20our%20proposed%20two-stage%20Alignment%0AAutoEncoder%2C%20which%20ensures%20the%20preservation%20of%20salient%20information%20and%20the%0Aconsistency%20of%20feature%20structure.%20The%20metric%20for%20evaluating%20the%20training%0Amaturity%20of%20functional%20modules%2C%20Similarity%20Score%2C%20demonstrates%20a%20robust%0Apositive%20correlation%20with%20BEV%20metrics%2C%20with%20an%20average%20correlation%20coefficient%0Aof%200.9387%2C%20attesting%20to%20the%20framework%27s%20reliability%20for%20assessment%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11969v1&entry.124074799=Read"},
{"title": "Residual Descent Differential Dynamic Game (RD3G) -- A Fast Newton\n  Solver for Constrained General Sum Games", "author": "Zhiyuan Zhang and Panagiotis Tsiotras", "abstract": "  We present Residual Descent Differential Dynamic Game (RD3G), a Newton-based\nsolver for constrained multi-agent game-control problems. The proposed solver\nseeks a local Nash equilibrium for problems where agents are coupled through\ntheir rewards and state constraints. We compare the proposed method against\ncompeting state-of-the-art techniques and showcase the computational benefits\nof the RD3G algorithm on several example problems.\n", "link": "http://arxiv.org/abs/2409.12152v1", "date": "2024-09-18", "relevancy": 2.2619, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4694}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Descent%20Differential%20Dynamic%20Game%20%28RD3G%29%20--%20A%20Fast%20Newton%0A%20%20Solver%20for%20Constrained%20General%20Sum%20Games&body=Title%3A%20Residual%20Descent%20Differential%20Dynamic%20Game%20%28RD3G%29%20--%20A%20Fast%20Newton%0A%20%20Solver%20for%20Constrained%20General%20Sum%20Games%0AAuthor%3A%20Zhiyuan%20Zhang%20and%20Panagiotis%20Tsiotras%0AAbstract%3A%20%20%20We%20present%20Residual%20Descent%20Differential%20Dynamic%20Game%20%28RD3G%29%2C%20a%20Newton-based%0Asolver%20for%20constrained%20multi-agent%20game-control%20problems.%20The%20proposed%20solver%0Aseeks%20a%20local%20Nash%20equilibrium%20for%20problems%20where%20agents%20are%20coupled%20through%0Atheir%20rewards%20and%20state%20constraints.%20We%20compare%20the%20proposed%20method%20against%0Acompeting%20state-of-the-art%20techniques%20and%20showcase%20the%20computational%20benefits%0Aof%20the%20RD3G%20algorithm%20on%20several%20example%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Descent%2520Differential%2520Dynamic%2520Game%2520%2528RD3G%2529%2520--%2520A%2520Fast%2520Newton%250A%2520%2520Solver%2520for%2520Constrained%2520General%2520Sum%2520Games%26entry.906535625%3DZhiyuan%2520Zhang%2520and%2520Panagiotis%2520Tsiotras%26entry.1292438233%3D%2520%2520We%2520present%2520Residual%2520Descent%2520Differential%2520Dynamic%2520Game%2520%2528RD3G%2529%252C%2520a%2520Newton-based%250Asolver%2520for%2520constrained%2520multi-agent%2520game-control%2520problems.%2520The%2520proposed%2520solver%250Aseeks%2520a%2520local%2520Nash%2520equilibrium%2520for%2520problems%2520where%2520agents%2520are%2520coupled%2520through%250Atheir%2520rewards%2520and%2520state%2520constraints.%2520We%2520compare%2520the%2520proposed%2520method%2520against%250Acompeting%2520state-of-the-art%2520techniques%2520and%2520showcase%2520the%2520computational%2520benefits%250Aof%2520the%2520RD3G%2520algorithm%2520on%2520several%2520example%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Descent%20Differential%20Dynamic%20Game%20%28RD3G%29%20--%20A%20Fast%20Newton%0A%20%20Solver%20for%20Constrained%20General%20Sum%20Games&entry.906535625=Zhiyuan%20Zhang%20and%20Panagiotis%20Tsiotras&entry.1292438233=%20%20We%20present%20Residual%20Descent%20Differential%20Dynamic%20Game%20%28RD3G%29%2C%20a%20Newton-based%0Asolver%20for%20constrained%20multi-agent%20game-control%20problems.%20The%20proposed%20solver%0Aseeks%20a%20local%20Nash%20equilibrium%20for%20problems%20where%20agents%20are%20coupled%20through%0Atheir%20rewards%20and%20state%20constraints.%20We%20compare%20the%20proposed%20method%20against%0Acompeting%20state-of-the-art%20techniques%20and%20showcase%20the%20computational%20benefits%0Aof%20the%20RD3G%20algorithm%20on%20several%20example%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12152v1&entry.124074799=Read"},
{"title": "multiPI-TransBTS: A Multi-Path Learning Framework for Brain Tumor Image\n  Segmentation Based on Multi-Physical Information", "author": "Hongjun Zhu and Jiaohang Huang and Kuo Chen and Xuehui Ying and Ying Qian", "abstract": "  Brain Tumor Segmentation (BraTS) plays a critical role in clinical diagnosis,\ntreatment planning, and monitoring the progression of brain tumors. However,\ndue to the variability in tumor appearance, size, and intensity across\ndifferent MRI modalities, automated segmentation remains a challenging task. In\nthis study, we propose a novel Transformer-based framework, multiPI-TransBTS,\nwhich integrates multi-physical information to enhance segmentation accuracy.\nThe model leverages spatial information, semantic information, and multi-modal\nimaging data, addressing the inherent heterogeneity in brain tumor\ncharacteristics. The multiPI-TransBTS framework consists of an encoder, an\nAdaptive Feature Fusion (AFF) module, and a multi-source, multi-scale feature\ndecoder. The encoder incorporates a multi-branch architecture to separately\nextract modality-specific features from different MRI sequences. The AFF module\nfuses information from multiple sources using channel-wise and element-wise\nattention, ensuring effective feature recalibration. The decoder combines both\ncommon and task-specific features through a Task-Specific Feature Introduction\n(TSFI) strategy, producing accurate segmentation outputs for Whole Tumor (WT),\nTumor Core (TC), and Enhancing Tumor (ET) regions. Comprehensive evaluations on\nthe BraTS2019 and BraTS2020 datasets demonstrate the superiority of\nmultiPI-TransBTS over the state-of-the-art methods. The model consistently\nachieves better Dice coefficients, Hausdorff distances, and Sensitivity scores,\nhighlighting its effectiveness in addressing the BraTS challenges. Our results\nalso indicate the need for further exploration of the balance between precision\nand recall in the ET segmentation task. The proposed framework represents a\nsignificant advancement in BraTS, with potential implications for improving\nclinical outcomes for brain tumor patients.\n", "link": "http://arxiv.org/abs/2409.12167v1", "date": "2024-09-18", "relevancy": 2.24, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5595}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20multiPI-TransBTS%3A%20A%20Multi-Path%20Learning%20Framework%20for%20Brain%20Tumor%20Image%0A%20%20Segmentation%20Based%20on%20Multi-Physical%20Information&body=Title%3A%20multiPI-TransBTS%3A%20A%20Multi-Path%20Learning%20Framework%20for%20Brain%20Tumor%20Image%0A%20%20Segmentation%20Based%20on%20Multi-Physical%20Information%0AAuthor%3A%20Hongjun%20Zhu%20and%20Jiaohang%20Huang%20and%20Kuo%20Chen%20and%20Xuehui%20Ying%20and%20Ying%20Qian%0AAbstract%3A%20%20%20Brain%20Tumor%20Segmentation%20%28BraTS%29%20plays%20a%20critical%20role%20in%20clinical%20diagnosis%2C%0Atreatment%20planning%2C%20and%20monitoring%20the%20progression%20of%20brain%20tumors.%20However%2C%0Adue%20to%20the%20variability%20in%20tumor%20appearance%2C%20size%2C%20and%20intensity%20across%0Adifferent%20MRI%20modalities%2C%20automated%20segmentation%20remains%20a%20challenging%20task.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20Transformer-based%20framework%2C%20multiPI-TransBTS%2C%0Awhich%20integrates%20multi-physical%20information%20to%20enhance%20segmentation%20accuracy.%0AThe%20model%20leverages%20spatial%20information%2C%20semantic%20information%2C%20and%20multi-modal%0Aimaging%20data%2C%20addressing%20the%20inherent%20heterogeneity%20in%20brain%20tumor%0Acharacteristics.%20The%20multiPI-TransBTS%20framework%20consists%20of%20an%20encoder%2C%20an%0AAdaptive%20Feature%20Fusion%20%28AFF%29%20module%2C%20and%20a%20multi-source%2C%20multi-scale%20feature%0Adecoder.%20The%20encoder%20incorporates%20a%20multi-branch%20architecture%20to%20separately%0Aextract%20modality-specific%20features%20from%20different%20MRI%20sequences.%20The%20AFF%20module%0Afuses%20information%20from%20multiple%20sources%20using%20channel-wise%20and%20element-wise%0Aattention%2C%20ensuring%20effective%20feature%20recalibration.%20The%20decoder%20combines%20both%0Acommon%20and%20task-specific%20features%20through%20a%20Task-Specific%20Feature%20Introduction%0A%28TSFI%29%20strategy%2C%20producing%20accurate%20segmentation%20outputs%20for%20Whole%20Tumor%20%28WT%29%2C%0ATumor%20Core%20%28TC%29%2C%20and%20Enhancing%20Tumor%20%28ET%29%20regions.%20Comprehensive%20evaluations%20on%0Athe%20BraTS2019%20and%20BraTS2020%20datasets%20demonstrate%20the%20superiority%20of%0AmultiPI-TransBTS%20over%20the%20state-of-the-art%20methods.%20The%20model%20consistently%0Aachieves%20better%20Dice%20coefficients%2C%20Hausdorff%20distances%2C%20and%20Sensitivity%20scores%2C%0Ahighlighting%20its%20effectiveness%20in%20addressing%20the%20BraTS%20challenges.%20Our%20results%0Aalso%20indicate%20the%20need%20for%20further%20exploration%20of%20the%20balance%20between%20precision%0Aand%20recall%20in%20the%20ET%20segmentation%20task.%20The%20proposed%20framework%20represents%20a%0Asignificant%20advancement%20in%20BraTS%2C%20with%20potential%20implications%20for%20improving%0Aclinical%20outcomes%20for%20brain%20tumor%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmultiPI-TransBTS%253A%2520A%2520Multi-Path%2520Learning%2520Framework%2520for%2520Brain%2520Tumor%2520Image%250A%2520%2520Segmentation%2520Based%2520on%2520Multi-Physical%2520Information%26entry.906535625%3DHongjun%2520Zhu%2520and%2520Jiaohang%2520Huang%2520and%2520Kuo%2520Chen%2520and%2520Xuehui%2520Ying%2520and%2520Ying%2520Qian%26entry.1292438233%3D%2520%2520Brain%2520Tumor%2520Segmentation%2520%2528BraTS%2529%2520plays%2520a%2520critical%2520role%2520in%2520clinical%2520diagnosis%252C%250Atreatment%2520planning%252C%2520and%2520monitoring%2520the%2520progression%2520of%2520brain%2520tumors.%2520However%252C%250Adue%2520to%2520the%2520variability%2520in%2520tumor%2520appearance%252C%2520size%252C%2520and%2520intensity%2520across%250Adifferent%2520MRI%2520modalities%252C%2520automated%2520segmentation%2520remains%2520a%2520challenging%2520task.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520novel%2520Transformer-based%2520framework%252C%2520multiPI-TransBTS%252C%250Awhich%2520integrates%2520multi-physical%2520information%2520to%2520enhance%2520segmentation%2520accuracy.%250AThe%2520model%2520leverages%2520spatial%2520information%252C%2520semantic%2520information%252C%2520and%2520multi-modal%250Aimaging%2520data%252C%2520addressing%2520the%2520inherent%2520heterogeneity%2520in%2520brain%2520tumor%250Acharacteristics.%2520The%2520multiPI-TransBTS%2520framework%2520consists%2520of%2520an%2520encoder%252C%2520an%250AAdaptive%2520Feature%2520Fusion%2520%2528AFF%2529%2520module%252C%2520and%2520a%2520multi-source%252C%2520multi-scale%2520feature%250Adecoder.%2520The%2520encoder%2520incorporates%2520a%2520multi-branch%2520architecture%2520to%2520separately%250Aextract%2520modality-specific%2520features%2520from%2520different%2520MRI%2520sequences.%2520The%2520AFF%2520module%250Afuses%2520information%2520from%2520multiple%2520sources%2520using%2520channel-wise%2520and%2520element-wise%250Aattention%252C%2520ensuring%2520effective%2520feature%2520recalibration.%2520The%2520decoder%2520combines%2520both%250Acommon%2520and%2520task-specific%2520features%2520through%2520a%2520Task-Specific%2520Feature%2520Introduction%250A%2528TSFI%2529%2520strategy%252C%2520producing%2520accurate%2520segmentation%2520outputs%2520for%2520Whole%2520Tumor%2520%2528WT%2529%252C%250ATumor%2520Core%2520%2528TC%2529%252C%2520and%2520Enhancing%2520Tumor%2520%2528ET%2529%2520regions.%2520Comprehensive%2520evaluations%2520on%250Athe%2520BraTS2019%2520and%2520BraTS2020%2520datasets%2520demonstrate%2520the%2520superiority%2520of%250AmultiPI-TransBTS%2520over%2520the%2520state-of-the-art%2520methods.%2520The%2520model%2520consistently%250Aachieves%2520better%2520Dice%2520coefficients%252C%2520Hausdorff%2520distances%252C%2520and%2520Sensitivity%2520scores%252C%250Ahighlighting%2520its%2520effectiveness%2520in%2520addressing%2520the%2520BraTS%2520challenges.%2520Our%2520results%250Aalso%2520indicate%2520the%2520need%2520for%2520further%2520exploration%2520of%2520the%2520balance%2520between%2520precision%250Aand%2520recall%2520in%2520the%2520ET%2520segmentation%2520task.%2520The%2520proposed%2520framework%2520represents%2520a%250Asignificant%2520advancement%2520in%2520BraTS%252C%2520with%2520potential%2520implications%2520for%2520improving%250Aclinical%2520outcomes%2520for%2520brain%2520tumor%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=multiPI-TransBTS%3A%20A%20Multi-Path%20Learning%20Framework%20for%20Brain%20Tumor%20Image%0A%20%20Segmentation%20Based%20on%20Multi-Physical%20Information&entry.906535625=Hongjun%20Zhu%20and%20Jiaohang%20Huang%20and%20Kuo%20Chen%20and%20Xuehui%20Ying%20and%20Ying%20Qian&entry.1292438233=%20%20Brain%20Tumor%20Segmentation%20%28BraTS%29%20plays%20a%20critical%20role%20in%20clinical%20diagnosis%2C%0Atreatment%20planning%2C%20and%20monitoring%20the%20progression%20of%20brain%20tumors.%20However%2C%0Adue%20to%20the%20variability%20in%20tumor%20appearance%2C%20size%2C%20and%20intensity%20across%0Adifferent%20MRI%20modalities%2C%20automated%20segmentation%20remains%20a%20challenging%20task.%20In%0Athis%20study%2C%20we%20propose%20a%20novel%20Transformer-based%20framework%2C%20multiPI-TransBTS%2C%0Awhich%20integrates%20multi-physical%20information%20to%20enhance%20segmentation%20accuracy.%0AThe%20model%20leverages%20spatial%20information%2C%20semantic%20information%2C%20and%20multi-modal%0Aimaging%20data%2C%20addressing%20the%20inherent%20heterogeneity%20in%20brain%20tumor%0Acharacteristics.%20The%20multiPI-TransBTS%20framework%20consists%20of%20an%20encoder%2C%20an%0AAdaptive%20Feature%20Fusion%20%28AFF%29%20module%2C%20and%20a%20multi-source%2C%20multi-scale%20feature%0Adecoder.%20The%20encoder%20incorporates%20a%20multi-branch%20architecture%20to%20separately%0Aextract%20modality-specific%20features%20from%20different%20MRI%20sequences.%20The%20AFF%20module%0Afuses%20information%20from%20multiple%20sources%20using%20channel-wise%20and%20element-wise%0Aattention%2C%20ensuring%20effective%20feature%20recalibration.%20The%20decoder%20combines%20both%0Acommon%20and%20task-specific%20features%20through%20a%20Task-Specific%20Feature%20Introduction%0A%28TSFI%29%20strategy%2C%20producing%20accurate%20segmentation%20outputs%20for%20Whole%20Tumor%20%28WT%29%2C%0ATumor%20Core%20%28TC%29%2C%20and%20Enhancing%20Tumor%20%28ET%29%20regions.%20Comprehensive%20evaluations%20on%0Athe%20BraTS2019%20and%20BraTS2020%20datasets%20demonstrate%20the%20superiority%20of%0AmultiPI-TransBTS%20over%20the%20state-of-the-art%20methods.%20The%20model%20consistently%0Aachieves%20better%20Dice%20coefficients%2C%20Hausdorff%20distances%2C%20and%20Sensitivity%20scores%2C%0Ahighlighting%20its%20effectiveness%20in%20addressing%20the%20BraTS%20challenges.%20Our%20results%0Aalso%20indicate%20the%20need%20for%20further%20exploration%20of%20the%20balance%20between%20precision%0Aand%20recall%20in%20the%20ET%20segmentation%20task.%20The%20proposed%20framework%20represents%20a%0Asignificant%20advancement%20in%20BraTS%2C%20with%20potential%20implications%20for%20improving%0Aclinical%20outcomes%20for%20brain%20tumor%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12167v1&entry.124074799=Read"},
{"title": "Multi-Grid Graph Neural Networks with Self-Attention for Computational\n  Mechanics", "author": "Paul Garnier and Jonathan Viquerat and Elie Hachem", "abstract": "  Advancement in finite element methods have become essential in various\ndisciplines, and in particular for Computational Fluid Dynamics (CFD), driving\nresearch efforts for improved precision and efficiency. While Convolutional\nNeural Networks (CNNs) have found success in CFD by mapping meshes into images,\nrecent attention has turned to leveraging Graph Neural Networks (GNNs) for\ndirect mesh processing. This paper introduces a novel model merging\nSelf-Attention with Message Passing in GNNs, achieving a 15\\% reduction in RMSE\non the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh\npruning technique based on Self-Attention is proposed, that leads to a robust\nGNN-based multigrid approach, also reducing RMSE by 15\\%. Additionally, a new\nself-supervised training method based on BERT is presented, resulting in a 25\\%\nRMSE reduction. The paper includes an ablation study and outperforms\nstate-of-the-art models on several challenging datasets, promising advancements\nsimilar to those recently achieved in natural language and image processing.\nFinally, the paper introduces a dataset with meshes larger than existing ones\nby at least an order of magnitude. Code and Datasets will be released at\nhttps://github.com/DonsetPG/multigrid-gnn.\n", "link": "http://arxiv.org/abs/2409.11899v1", "date": "2024-09-18", "relevancy": 2.22, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5859}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5354}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Grid%20Graph%20Neural%20Networks%20with%20Self-Attention%20for%20Computational%0A%20%20Mechanics&body=Title%3A%20Multi-Grid%20Graph%20Neural%20Networks%20with%20Self-Attention%20for%20Computational%0A%20%20Mechanics%0AAuthor%3A%20Paul%20Garnier%20and%20Jonathan%20Viquerat%20and%20Elie%20Hachem%0AAbstract%3A%20%20%20Advancement%20in%20finite%20element%20methods%20have%20become%20essential%20in%20various%0Adisciplines%2C%20and%20in%20particular%20for%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20driving%0Aresearch%20efforts%20for%20improved%20precision%20and%20efficiency.%20While%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20have%20found%20success%20in%20CFD%20by%20mapping%20meshes%20into%20images%2C%0Arecent%20attention%20has%20turned%20to%20leveraging%20Graph%20Neural%20Networks%20%28GNNs%29%20for%0Adirect%20mesh%20processing.%20This%20paper%20introduces%20a%20novel%20model%20merging%0ASelf-Attention%20with%20Message%20Passing%20in%20GNNs%2C%20achieving%20a%2015%5C%25%20reduction%20in%20RMSE%0Aon%20the%20well%20known%20flow%20past%20a%20cylinder%20benchmark.%20Furthermore%2C%20a%20dynamic%20mesh%0Apruning%20technique%20based%20on%20Self-Attention%20is%20proposed%2C%20that%20leads%20to%20a%20robust%0AGNN-based%20multigrid%20approach%2C%20also%20reducing%20RMSE%20by%2015%5C%25.%20Additionally%2C%20a%20new%0Aself-supervised%20training%20method%20based%20on%20BERT%20is%20presented%2C%20resulting%20in%20a%2025%5C%25%0ARMSE%20reduction.%20The%20paper%20includes%20an%20ablation%20study%20and%20outperforms%0Astate-of-the-art%20models%20on%20several%20challenging%20datasets%2C%20promising%20advancements%0Asimilar%20to%20those%20recently%20achieved%20in%20natural%20language%20and%20image%20processing.%0AFinally%2C%20the%20paper%20introduces%20a%20dataset%20with%20meshes%20larger%20than%20existing%20ones%0Aby%20at%20least%20an%20order%20of%20magnitude.%20Code%20and%20Datasets%20will%20be%20released%20at%0Ahttps%3A//github.com/DonsetPG/multigrid-gnn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Grid%2520Graph%2520Neural%2520Networks%2520with%2520Self-Attention%2520for%2520Computational%250A%2520%2520Mechanics%26entry.906535625%3DPaul%2520Garnier%2520and%2520Jonathan%2520Viquerat%2520and%2520Elie%2520Hachem%26entry.1292438233%3D%2520%2520Advancement%2520in%2520finite%2520element%2520methods%2520have%2520become%2520essential%2520in%2520various%250Adisciplines%252C%2520and%2520in%2520particular%2520for%2520Computational%2520Fluid%2520Dynamics%2520%2528CFD%2529%252C%2520driving%250Aresearch%2520efforts%2520for%2520improved%2520precision%2520and%2520efficiency.%2520While%2520Convolutional%250ANeural%2520Networks%2520%2528CNNs%2529%2520have%2520found%2520success%2520in%2520CFD%2520by%2520mapping%2520meshes%2520into%2520images%252C%250Arecent%2520attention%2520has%2520turned%2520to%2520leveraging%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520for%250Adirect%2520mesh%2520processing.%2520This%2520paper%2520introduces%2520a%2520novel%2520model%2520merging%250ASelf-Attention%2520with%2520Message%2520Passing%2520in%2520GNNs%252C%2520achieving%2520a%252015%255C%2525%2520reduction%2520in%2520RMSE%250Aon%2520the%2520well%2520known%2520flow%2520past%2520a%2520cylinder%2520benchmark.%2520Furthermore%252C%2520a%2520dynamic%2520mesh%250Apruning%2520technique%2520based%2520on%2520Self-Attention%2520is%2520proposed%252C%2520that%2520leads%2520to%2520a%2520robust%250AGNN-based%2520multigrid%2520approach%252C%2520also%2520reducing%2520RMSE%2520by%252015%255C%2525.%2520Additionally%252C%2520a%2520new%250Aself-supervised%2520training%2520method%2520based%2520on%2520BERT%2520is%2520presented%252C%2520resulting%2520in%2520a%252025%255C%2525%250ARMSE%2520reduction.%2520The%2520paper%2520includes%2520an%2520ablation%2520study%2520and%2520outperforms%250Astate-of-the-art%2520models%2520on%2520several%2520challenging%2520datasets%252C%2520promising%2520advancements%250Asimilar%2520to%2520those%2520recently%2520achieved%2520in%2520natural%2520language%2520and%2520image%2520processing.%250AFinally%252C%2520the%2520paper%2520introduces%2520a%2520dataset%2520with%2520meshes%2520larger%2520than%2520existing%2520ones%250Aby%2520at%2520least%2520an%2520order%2520of%2520magnitude.%2520Code%2520and%2520Datasets%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/DonsetPG/multigrid-gnn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Grid%20Graph%20Neural%20Networks%20with%20Self-Attention%20for%20Computational%0A%20%20Mechanics&entry.906535625=Paul%20Garnier%20and%20Jonathan%20Viquerat%20and%20Elie%20Hachem&entry.1292438233=%20%20Advancement%20in%20finite%20element%20methods%20have%20become%20essential%20in%20various%0Adisciplines%2C%20and%20in%20particular%20for%20Computational%20Fluid%20Dynamics%20%28CFD%29%2C%20driving%0Aresearch%20efforts%20for%20improved%20precision%20and%20efficiency.%20While%20Convolutional%0ANeural%20Networks%20%28CNNs%29%20have%20found%20success%20in%20CFD%20by%20mapping%20meshes%20into%20images%2C%0Arecent%20attention%20has%20turned%20to%20leveraging%20Graph%20Neural%20Networks%20%28GNNs%29%20for%0Adirect%20mesh%20processing.%20This%20paper%20introduces%20a%20novel%20model%20merging%0ASelf-Attention%20with%20Message%20Passing%20in%20GNNs%2C%20achieving%20a%2015%5C%25%20reduction%20in%20RMSE%0Aon%20the%20well%20known%20flow%20past%20a%20cylinder%20benchmark.%20Furthermore%2C%20a%20dynamic%20mesh%0Apruning%20technique%20based%20on%20Self-Attention%20is%20proposed%2C%20that%20leads%20to%20a%20robust%0AGNN-based%20multigrid%20approach%2C%20also%20reducing%20RMSE%20by%2015%5C%25.%20Additionally%2C%20a%20new%0Aself-supervised%20training%20method%20based%20on%20BERT%20is%20presented%2C%20resulting%20in%20a%2025%5C%25%0ARMSE%20reduction.%20The%20paper%20includes%20an%20ablation%20study%20and%20outperforms%0Astate-of-the-art%20models%20on%20several%20challenging%20datasets%2C%20promising%20advancements%0Asimilar%20to%20those%20recently%20achieved%20in%20natural%20language%20and%20image%20processing.%0AFinally%2C%20the%20paper%20introduces%20a%20dataset%20with%20meshes%20larger%20than%20existing%20ones%0Aby%20at%20least%20an%20order%20of%20magnitude.%20Code%20and%20Datasets%20will%20be%20released%20at%0Ahttps%3A//github.com/DonsetPG/multigrid-gnn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11899v1&entry.124074799=Read"},
{"title": "Characterizing Dynamical Stability of Stochastic Gradient Descent in\n  Overparameterized Learning", "author": "Dennis Chemnitz and Maximilian Engel", "abstract": "  For overparameterized optimization tasks, such as the ones found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent which depends on the local\ndynamics around a global minimum and rigorously prove that the sign of this\nLyapunov exponent determines whether SGD can accumulate at the respective\nglobal minimum.\n", "link": "http://arxiv.org/abs/2407.20209v2", "date": "2024-09-18", "relevancy": 2.2138, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&body=Title%3A%20Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning%0AAuthor%3A%20Dennis%20Chemnitz%20and%20Maximilian%20Engel%0AAbstract%3A%20%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20the%20ones%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20which%20depends%20on%20the%20local%0Adynamics%20around%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%0ALyapunov%20exponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%0Aglobal%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Dynamical%2520Stability%2520of%2520Stochastic%2520Gradient%2520Descent%2520in%250A%2520%2520Overparameterized%2520Learning%26entry.906535625%3DDennis%2520Chemnitz%2520and%2520Maximilian%2520Engel%26entry.1292438233%3D%2520%2520For%2520overparameterized%2520optimization%2520tasks%252C%2520such%2520as%2520the%2520ones%2520found%2520in%2520modern%250Amachine%2520learning%252C%2520global%2520minima%2520are%2520generally%2520not%2520unique.%2520In%2520order%2520to%250Aunderstand%2520generalization%2520in%2520these%2520settings%252C%2520it%2520is%2520vital%2520to%2520study%2520to%2520which%250Aminimum%2520an%2520optimization%2520algorithm%2520converges.%2520The%2520possibility%2520of%2520having%2520minima%250Athat%2520are%2520unstable%2520under%2520the%2520dynamics%2520imposed%2520by%2520the%2520optimization%2520algorithm%250Alimits%2520the%2520potential%2520minima%2520that%2520the%2520algorithm%2520can%2520find.%2520In%2520this%2520paper%252C%2520we%250Acharacterize%2520the%2520global%2520minima%2520that%2520are%2520dynamically%2520stable/unstable%2520for%2520both%250Adeterministic%2520and%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529.%2520In%2520particular%252C%2520we%250Aintroduce%2520a%2520characteristic%2520Lyapunov%2520exponent%2520which%2520depends%2520on%2520the%2520local%250Adynamics%2520around%2520a%2520global%2520minimum%2520and%2520rigorously%2520prove%2520that%2520the%2520sign%2520of%2520this%250ALyapunov%2520exponent%2520determines%2520whether%2520SGD%2520can%2520accumulate%2520at%2520the%2520respective%250Aglobal%2520minimum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Dynamical%20Stability%20of%20Stochastic%20Gradient%20Descent%20in%0A%20%20Overparameterized%20Learning&entry.906535625=Dennis%20Chemnitz%20and%20Maximilian%20Engel&entry.1292438233=%20%20For%20overparameterized%20optimization%20tasks%2C%20such%20as%20the%20ones%20found%20in%20modern%0Amachine%20learning%2C%20global%20minima%20are%20generally%20not%20unique.%20In%20order%20to%0Aunderstand%20generalization%20in%20these%20settings%2C%20it%20is%20vital%20to%20study%20to%20which%0Aminimum%20an%20optimization%20algorithm%20converges.%20The%20possibility%20of%20having%20minima%0Athat%20are%20unstable%20under%20the%20dynamics%20imposed%20by%20the%20optimization%20algorithm%0Alimits%20the%20potential%20minima%20that%20the%20algorithm%20can%20find.%20In%20this%20paper%2C%20we%0Acharacterize%20the%20global%20minima%20that%20are%20dynamically%20stable/unstable%20for%20both%0Adeterministic%20and%20stochastic%20gradient%20descent%20%28SGD%29.%20In%20particular%2C%20we%0Aintroduce%20a%20characteristic%20Lyapunov%20exponent%20which%20depends%20on%20the%20local%0Adynamics%20around%20a%20global%20minimum%20and%20rigorously%20prove%20that%20the%20sign%20of%20this%0ALyapunov%20exponent%20determines%20whether%20SGD%20can%20accumulate%20at%20the%20respective%0Aglobal%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20209v2&entry.124074799=Read"},
{"title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control", "author": "Zichen Jeff Cui and Hengkai Pan and Aadhithya Iyer and Siddhant Haldar and Lerrel Pinto", "abstract": "  Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io\n", "link": "http://arxiv.org/abs/2409.12192v1", "date": "2024-09-18", "relevancy": 2.1805, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaMo%3A%20In-Domain%20Dynamics%20Pretraining%20for%20Visuo-Motor%20Control&body=Title%3A%20DynaMo%3A%20In-Domain%20Dynamics%20Pretraining%20for%20Visuo-Motor%20Control%0AAuthor%3A%20Zichen%20Jeff%20Cui%20and%20Hengkai%20Pan%20and%20Aadhithya%20Iyer%20and%20Siddhant%20Haldar%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20Imitation%20learning%20has%20proven%20to%20be%20a%20powerful%20tool%20for%20training%20complex%0Avisuomotor%20policies.%20However%2C%20current%20methods%20often%20require%20hundreds%20to%0Athousands%20of%20expert%20demonstrations%20to%20handle%20high-dimensional%20visual%0Aobservations.%20A%20key%20reason%20for%20this%20poor%20data%20efficiency%20is%20that%20visual%0Arepresentations%20are%20predominantly%20either%20pretrained%20on%20out-of-domain%20data%20or%0Atrained%20directly%20through%20a%20behavior%20cloning%20objective.%20In%20this%20work%2C%20we%20present%0ADynaMo%2C%20a%20new%20in-domain%2C%20self-supervised%20method%20for%20learning%20visual%0Arepresentations.%20Given%20a%20set%20of%20expert%20demonstrations%2C%20we%20jointly%20learn%20a%0Alatent%20inverse%20dynamics%20model%20and%20a%20forward%20dynamics%20model%20over%20a%20sequence%20of%0Aimage%20embeddings%2C%20predicting%20the%20next%20frame%20in%20latent%20space%2C%20without%0Aaugmentations%2C%20contrastive%20sampling%2C%20or%20access%20to%20ground%20truth%20actions.%0AImportantly%2C%20DynaMo%20does%20not%20require%20any%20out-of-domain%20data%20such%20as%20Internet%0Adatasets%20or%20cross-embodied%20datasets.%20On%20a%20suite%20of%20six%20simulated%20and%20real%0Aenvironments%2C%20we%20show%20that%20representations%20learned%20with%20DynaMo%20significantly%0Aimprove%20downstream%20imitation%20learning%20performance%20over%20prior%20self-supervised%0Alearning%20objectives%2C%20and%20pretrained%20representations.%20Gains%20from%20using%20DynaMo%0Ahold%20across%20policy%20classes%20such%20as%20Behavior%20Transformer%2C%20Diffusion%20Policy%2C%20MLP%2C%0Aand%20nearest%20neighbors.%20Finally%2C%20we%20ablate%20over%20key%20components%20of%20DynaMo%20and%0Ameasure%20its%20impact%20on%20downstream%20policy%20performance.%20Robot%20videos%20are%20best%0Aviewed%20at%20https%3A//dynamo-ssl.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaMo%253A%2520In-Domain%2520Dynamics%2520Pretraining%2520for%2520Visuo-Motor%2520Control%26entry.906535625%3DZichen%2520Jeff%2520Cui%2520and%2520Hengkai%2520Pan%2520and%2520Aadhithya%2520Iyer%2520and%2520Siddhant%2520Haldar%2520and%2520Lerrel%2520Pinto%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520has%2520proven%2520to%2520be%2520a%2520powerful%2520tool%2520for%2520training%2520complex%250Avisuomotor%2520policies.%2520However%252C%2520current%2520methods%2520often%2520require%2520hundreds%2520to%250Athousands%2520of%2520expert%2520demonstrations%2520to%2520handle%2520high-dimensional%2520visual%250Aobservations.%2520A%2520key%2520reason%2520for%2520this%2520poor%2520data%2520efficiency%2520is%2520that%2520visual%250Arepresentations%2520are%2520predominantly%2520either%2520pretrained%2520on%2520out-of-domain%2520data%2520or%250Atrained%2520directly%2520through%2520a%2520behavior%2520cloning%2520objective.%2520In%2520this%2520work%252C%2520we%2520present%250ADynaMo%252C%2520a%2520new%2520in-domain%252C%2520self-supervised%2520method%2520for%2520learning%2520visual%250Arepresentations.%2520Given%2520a%2520set%2520of%2520expert%2520demonstrations%252C%2520we%2520jointly%2520learn%2520a%250Alatent%2520inverse%2520dynamics%2520model%2520and%2520a%2520forward%2520dynamics%2520model%2520over%2520a%2520sequence%2520of%250Aimage%2520embeddings%252C%2520predicting%2520the%2520next%2520frame%2520in%2520latent%2520space%252C%2520without%250Aaugmentations%252C%2520contrastive%2520sampling%252C%2520or%2520access%2520to%2520ground%2520truth%2520actions.%250AImportantly%252C%2520DynaMo%2520does%2520not%2520require%2520any%2520out-of-domain%2520data%2520such%2520as%2520Internet%250Adatasets%2520or%2520cross-embodied%2520datasets.%2520On%2520a%2520suite%2520of%2520six%2520simulated%2520and%2520real%250Aenvironments%252C%2520we%2520show%2520that%2520representations%2520learned%2520with%2520DynaMo%2520significantly%250Aimprove%2520downstream%2520imitation%2520learning%2520performance%2520over%2520prior%2520self-supervised%250Alearning%2520objectives%252C%2520and%2520pretrained%2520representations.%2520Gains%2520from%2520using%2520DynaMo%250Ahold%2520across%2520policy%2520classes%2520such%2520as%2520Behavior%2520Transformer%252C%2520Diffusion%2520Policy%252C%2520MLP%252C%250Aand%2520nearest%2520neighbors.%2520Finally%252C%2520we%2520ablate%2520over%2520key%2520components%2520of%2520DynaMo%2520and%250Ameasure%2520its%2520impact%2520on%2520downstream%2520policy%2520performance.%2520Robot%2520videos%2520are%2520best%250Aviewed%2520at%2520https%253A//dynamo-ssl.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMo%3A%20In-Domain%20Dynamics%20Pretraining%20for%20Visuo-Motor%20Control&entry.906535625=Zichen%20Jeff%20Cui%20and%20Hengkai%20Pan%20and%20Aadhithya%20Iyer%20and%20Siddhant%20Haldar%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Imitation%20learning%20has%20proven%20to%20be%20a%20powerful%20tool%20for%20training%20complex%0Avisuomotor%20policies.%20However%2C%20current%20methods%20often%20require%20hundreds%20to%0Athousands%20of%20expert%20demonstrations%20to%20handle%20high-dimensional%20visual%0Aobservations.%20A%20key%20reason%20for%20this%20poor%20data%20efficiency%20is%20that%20visual%0Arepresentations%20are%20predominantly%20either%20pretrained%20on%20out-of-domain%20data%20or%0Atrained%20directly%20through%20a%20behavior%20cloning%20objective.%20In%20this%20work%2C%20we%20present%0ADynaMo%2C%20a%20new%20in-domain%2C%20self-supervised%20method%20for%20learning%20visual%0Arepresentations.%20Given%20a%20set%20of%20expert%20demonstrations%2C%20we%20jointly%20learn%20a%0Alatent%20inverse%20dynamics%20model%20and%20a%20forward%20dynamics%20model%20over%20a%20sequence%20of%0Aimage%20embeddings%2C%20predicting%20the%20next%20frame%20in%20latent%20space%2C%20without%0Aaugmentations%2C%20contrastive%20sampling%2C%20or%20access%20to%20ground%20truth%20actions.%0AImportantly%2C%20DynaMo%20does%20not%20require%20any%20out-of-domain%20data%20such%20as%20Internet%0Adatasets%20or%20cross-embodied%20datasets.%20On%20a%20suite%20of%20six%20simulated%20and%20real%0Aenvironments%2C%20we%20show%20that%20representations%20learned%20with%20DynaMo%20significantly%0Aimprove%20downstream%20imitation%20learning%20performance%20over%20prior%20self-supervised%0Alearning%20objectives%2C%20and%20pretrained%20representations.%20Gains%20from%20using%20DynaMo%0Ahold%20across%20policy%20classes%20such%20as%20Behavior%20Transformer%2C%20Diffusion%20Policy%2C%20MLP%2C%0Aand%20nearest%20neighbors.%20Finally%2C%20we%20ablate%20over%20key%20components%20of%20DynaMo%20and%0Ameasure%20its%20impact%20on%20downstream%20policy%20performance.%20Robot%20videos%20are%20best%0Aviewed%20at%20https%3A//dynamo-ssl.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12192v1&entry.124074799=Read"},
{"title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models", "author": " EverestAI and  : and Sijin Chen and Yuan Feng and Laipeng He and Tianwei He and Wendi He and Yanni Hu and Bin Lin and Yiting Lin and Pengfei Tan and Chengwei Tian and Chen Wang and Zhicheng Wang and Ruoye Xie and Jingjing Yin and Jianhao Ye and Jixun Yao and Quanlei Yan and Yuguang Yang", "abstract": "  With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to https://takinaudiollm.github.io.\n", "link": "http://arxiv.org/abs/2409.12139v1", "date": "2024-09-18", "relevancy": 2.1742, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5916}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5492}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models&body=Title%3A%20Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models%0AAuthor%3A%20%20EverestAI%20and%20%20%3A%20and%20Sijin%20Chen%20and%20Yuan%20Feng%20and%20Laipeng%20He%20and%20Tianwei%20He%20and%20Wendi%20He%20and%20Yanni%20Hu%20and%20Bin%20Lin%20and%20Yiting%20Lin%20and%20Pengfei%20Tan%20and%20Chengwei%20Tian%20and%20Chen%20Wang%20and%20Zhicheng%20Wang%20and%20Ruoye%20Xie%20and%20Jingjing%20Yin%20and%20Jianhao%20Ye%20and%20Jixun%20Yao%20and%20Quanlei%20Yan%20and%20Yuguang%20Yang%0AAbstract%3A%20%20%20With%20the%20advent%20of%20the%20big%20data%20and%20large%20language%20model%20era%2C%20zero-shot%0Apersonalized%20rapid%20customization%20has%20emerged%20as%20a%20significant%20trend.%20In%20this%0Areport%2C%20we%20introduce%20Takin%20AudioLLM%2C%20a%20series%20of%20techniques%20and%20models%2C%20mainly%0Aincluding%20Takin%20TTS%2C%20Takin%20VC%2C%20and%20Takin%20Morphing%2C%20specifically%20designed%20for%0Aaudiobook%20production.%20These%20models%20are%20capable%20of%20zero-shot%20speech%20production%2C%0Agenerating%20high-quality%20speech%20that%20is%20nearly%20indistinguishable%20from%20real%20human%0Aspeech%20and%20facilitating%20individuals%20to%20customize%20the%20speech%20content%20according%0Ato%20their%20own%20needs.%20Specifically%2C%20we%20first%20introduce%20Takin%20TTS%2C%20a%20neural%20codec%0Alanguage%20model%20that%20builds%20upon%20an%20enhanced%20neural%20speech%20codec%20and%20a%0Amulti-task%20training%20framework%2C%20capable%20of%20generating%20high-fidelity%20natural%0Aspeech%20in%20a%20zero-shot%20way.%20For%20Takin%20VC%2C%20we%20advocate%20an%20effective%20content%20and%0Atimbre%20joint%20modeling%20approach%20to%20improve%20the%20speaker%20similarity%2C%20while%0Aadvocating%20for%20a%20conditional%20flow%20matching%20based%20decoder%20to%20further%20enhance%20its%0Anaturalness%20and%20expressiveness.%20Last%2C%20we%20propose%20the%20Takin%20Morphing%20system%20with%0Ahighly%20decoupled%20and%20advanced%20timbre%20and%20prosody%20modeling%20approaches%2C%20which%0Aenables%20individuals%20to%20customize%20speech%20production%20with%20their%20preferred%20timbre%0Aand%20prosody%20in%20a%20precise%20and%20controllable%20manner.%20Extensive%20experiments%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20our%20Takin%20AudioLLM%20series%20models.%0AFor%20detailed%20demos%2C%20please%20refer%20to%20https%3A//takinaudiollm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTakin%253A%2520A%2520Cohort%2520of%2520Superior%2520Quality%2520Zero-shot%2520Speech%2520Generation%2520Models%26entry.906535625%3D%2520EverestAI%2520and%2520%2520%253A%2520and%2520Sijin%2520Chen%2520and%2520Yuan%2520Feng%2520and%2520Laipeng%2520He%2520and%2520Tianwei%2520He%2520and%2520Wendi%2520He%2520and%2520Yanni%2520Hu%2520and%2520Bin%2520Lin%2520and%2520Yiting%2520Lin%2520and%2520Pengfei%2520Tan%2520and%2520Chengwei%2520Tian%2520and%2520Chen%2520Wang%2520and%2520Zhicheng%2520Wang%2520and%2520Ruoye%2520Xie%2520and%2520Jingjing%2520Yin%2520and%2520Jianhao%2520Ye%2520and%2520Jixun%2520Yao%2520and%2520Quanlei%2520Yan%2520and%2520Yuguang%2520Yang%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520the%2520big%2520data%2520and%2520large%2520language%2520model%2520era%252C%2520zero-shot%250Apersonalized%2520rapid%2520customization%2520has%2520emerged%2520as%2520a%2520significant%2520trend.%2520In%2520this%250Areport%252C%2520we%2520introduce%2520Takin%2520AudioLLM%252C%2520a%2520series%2520of%2520techniques%2520and%2520models%252C%2520mainly%250Aincluding%2520Takin%2520TTS%252C%2520Takin%2520VC%252C%2520and%2520Takin%2520Morphing%252C%2520specifically%2520designed%2520for%250Aaudiobook%2520production.%2520These%2520models%2520are%2520capable%2520of%2520zero-shot%2520speech%2520production%252C%250Agenerating%2520high-quality%2520speech%2520that%2520is%2520nearly%2520indistinguishable%2520from%2520real%2520human%250Aspeech%2520and%2520facilitating%2520individuals%2520to%2520customize%2520the%2520speech%2520content%2520according%250Ato%2520their%2520own%2520needs.%2520Specifically%252C%2520we%2520first%2520introduce%2520Takin%2520TTS%252C%2520a%2520neural%2520codec%250Alanguage%2520model%2520that%2520builds%2520upon%2520an%2520enhanced%2520neural%2520speech%2520codec%2520and%2520a%250Amulti-task%2520training%2520framework%252C%2520capable%2520of%2520generating%2520high-fidelity%2520natural%250Aspeech%2520in%2520a%2520zero-shot%2520way.%2520For%2520Takin%2520VC%252C%2520we%2520advocate%2520an%2520effective%2520content%2520and%250Atimbre%2520joint%2520modeling%2520approach%2520to%2520improve%2520the%2520speaker%2520similarity%252C%2520while%250Aadvocating%2520for%2520a%2520conditional%2520flow%2520matching%2520based%2520decoder%2520to%2520further%2520enhance%2520its%250Anaturalness%2520and%2520expressiveness.%2520Last%252C%2520we%2520propose%2520the%2520Takin%2520Morphing%2520system%2520with%250Ahighly%2520decoupled%2520and%2520advanced%2520timbre%2520and%2520prosody%2520modeling%2520approaches%252C%2520which%250Aenables%2520individuals%2520to%2520customize%2520speech%2520production%2520with%2520their%2520preferred%2520timbre%250Aand%2520prosody%2520in%2520a%2520precise%2520and%2520controllable%2520manner.%2520Extensive%2520experiments%250Avalidate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520Takin%2520AudioLLM%2520series%2520models.%250AFor%2520detailed%2520demos%252C%2520please%2520refer%2520to%2520https%253A//takinaudiollm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Takin%3A%20A%20Cohort%20of%20Superior%20Quality%20Zero-shot%20Speech%20Generation%20Models&entry.906535625=%20EverestAI%20and%20%20%3A%20and%20Sijin%20Chen%20and%20Yuan%20Feng%20and%20Laipeng%20He%20and%20Tianwei%20He%20and%20Wendi%20He%20and%20Yanni%20Hu%20and%20Bin%20Lin%20and%20Yiting%20Lin%20and%20Pengfei%20Tan%20and%20Chengwei%20Tian%20and%20Chen%20Wang%20and%20Zhicheng%20Wang%20and%20Ruoye%20Xie%20and%20Jingjing%20Yin%20and%20Jianhao%20Ye%20and%20Jixun%20Yao%20and%20Quanlei%20Yan%20and%20Yuguang%20Yang&entry.1292438233=%20%20With%20the%20advent%20of%20the%20big%20data%20and%20large%20language%20model%20era%2C%20zero-shot%0Apersonalized%20rapid%20customization%20has%20emerged%20as%20a%20significant%20trend.%20In%20this%0Areport%2C%20we%20introduce%20Takin%20AudioLLM%2C%20a%20series%20of%20techniques%20and%20models%2C%20mainly%0Aincluding%20Takin%20TTS%2C%20Takin%20VC%2C%20and%20Takin%20Morphing%2C%20specifically%20designed%20for%0Aaudiobook%20production.%20These%20models%20are%20capable%20of%20zero-shot%20speech%20production%2C%0Agenerating%20high-quality%20speech%20that%20is%20nearly%20indistinguishable%20from%20real%20human%0Aspeech%20and%20facilitating%20individuals%20to%20customize%20the%20speech%20content%20according%0Ato%20their%20own%20needs.%20Specifically%2C%20we%20first%20introduce%20Takin%20TTS%2C%20a%20neural%20codec%0Alanguage%20model%20that%20builds%20upon%20an%20enhanced%20neural%20speech%20codec%20and%20a%0Amulti-task%20training%20framework%2C%20capable%20of%20generating%20high-fidelity%20natural%0Aspeech%20in%20a%20zero-shot%20way.%20For%20Takin%20VC%2C%20we%20advocate%20an%20effective%20content%20and%0Atimbre%20joint%20modeling%20approach%20to%20improve%20the%20speaker%20similarity%2C%20while%0Aadvocating%20for%20a%20conditional%20flow%20matching%20based%20decoder%20to%20further%20enhance%20its%0Anaturalness%20and%20expressiveness.%20Last%2C%20we%20propose%20the%20Takin%20Morphing%20system%20with%0Ahighly%20decoupled%20and%20advanced%20timbre%20and%20prosody%20modeling%20approaches%2C%20which%0Aenables%20individuals%20to%20customize%20speech%20production%20with%20their%20preferred%20timbre%0Aand%20prosody%20in%20a%20precise%20and%20controllable%20manner.%20Extensive%20experiments%0Avalidate%20the%20effectiveness%20and%20robustness%20of%20our%20Takin%20AudioLLM%20series%20models.%0AFor%20detailed%20demos%2C%20please%20refer%20to%20https%3A//takinaudiollm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12139v1&entry.124074799=Read"},
{"title": "ABHINAW: A method for Automatic Evaluation of Typography within\n  AI-Generated Images", "author": "Abhinaw Jagtap and Nachiket Tapas and R. G. Brajesh", "abstract": "  In the fast-evolving field of Generative AI, platforms like MidJourney,\nDALL-E, and Stable Diffusion have transformed Text-to-Image (T2I) Generation.\nHowever, despite their impressive ability to create high-quality images, they\noften struggle to generate accurate text within these images. Theoretically, if\nwe could achieve accurate text generation in AI images in a ``zero-shot''\nmanner, it would not only make AI-generated images more meaningful but also\ndemocratize the graphic design industry. The first step towards this goal is to\ncreate a robust scoring matrix for evaluating text accuracy in AI-generated\nimages. Although there are existing bench-marking methods like CLIP SCORE and\nT2I-CompBench++, there's still a gap in systematically evaluating text and\ntypography in AI-generated images, especially with diffusion-based methods. In\nthis paper, we introduce a novel evaluation matrix designed explicitly for\nquantifying the performance of text and typography generation within\nAI-generated images. We have used letter by letter matching strategy to compute\nthe exact matching scores from the reference text to the AI generated text. Our\nnovel approach to calculate the score takes care of multiple redundancies such\nas repetition of words, case sensitivity, mixing of words, irregular\nincorporation of letters etc. Moreover, we have developed a Novel method named\nas brevity adjustment to handle excess text. In addition we have also done a\nquantitative analysis of frequent errors arise due to frequently used words and\nless frequently used words. Project page is available at:\nhttps://github.com/Abhinaw3906/ABHINAW-MATRIX.\n", "link": "http://arxiv.org/abs/2409.11874v1", "date": "2024-09-18", "relevancy": 2.1734, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5626}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5367}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABHINAW%3A%20A%20method%20for%20Automatic%20Evaluation%20of%20Typography%20within%0A%20%20AI-Generated%20Images&body=Title%3A%20ABHINAW%3A%20A%20method%20for%20Automatic%20Evaluation%20of%20Typography%20within%0A%20%20AI-Generated%20Images%0AAuthor%3A%20Abhinaw%20Jagtap%20and%20Nachiket%20Tapas%20and%20R.%20G.%20Brajesh%0AAbstract%3A%20%20%20In%20the%20fast-evolving%20field%20of%20Generative%20AI%2C%20platforms%20like%20MidJourney%2C%0ADALL-E%2C%20and%20Stable%20Diffusion%20have%20transformed%20Text-to-Image%20%28T2I%29%20Generation.%0AHowever%2C%20despite%20their%20impressive%20ability%20to%20create%20high-quality%20images%2C%20they%0Aoften%20struggle%20to%20generate%20accurate%20text%20within%20these%20images.%20Theoretically%2C%20if%0Awe%20could%20achieve%20accurate%20text%20generation%20in%20AI%20images%20in%20a%20%60%60zero-shot%27%27%0Amanner%2C%20it%20would%20not%20only%20make%20AI-generated%20images%20more%20meaningful%20but%20also%0Ademocratize%20the%20graphic%20design%20industry.%20The%20first%20step%20towards%20this%20goal%20is%20to%0Acreate%20a%20robust%20scoring%20matrix%20for%20evaluating%20text%20accuracy%20in%20AI-generated%0Aimages.%20Although%20there%20are%20existing%20bench-marking%20methods%20like%20CLIP%20SCORE%20and%0AT2I-CompBench%2B%2B%2C%20there%27s%20still%20a%20gap%20in%20systematically%20evaluating%20text%20and%0Atypography%20in%20AI-generated%20images%2C%20especially%20with%20diffusion-based%20methods.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20evaluation%20matrix%20designed%20explicitly%20for%0Aquantifying%20the%20performance%20of%20text%20and%20typography%20generation%20within%0AAI-generated%20images.%20We%20have%20used%20letter%20by%20letter%20matching%20strategy%20to%20compute%0Athe%20exact%20matching%20scores%20from%20the%20reference%20text%20to%20the%20AI%20generated%20text.%20Our%0Anovel%20approach%20to%20calculate%20the%20score%20takes%20care%20of%20multiple%20redundancies%20such%0Aas%20repetition%20of%20words%2C%20case%20sensitivity%2C%20mixing%20of%20words%2C%20irregular%0Aincorporation%20of%20letters%20etc.%20Moreover%2C%20we%20have%20developed%20a%20Novel%20method%20named%0Aas%20brevity%20adjustment%20to%20handle%20excess%20text.%20In%20addition%20we%20have%20also%20done%20a%0Aquantitative%20analysis%20of%20frequent%20errors%20arise%20due%20to%20frequently%20used%20words%20and%0Aless%20frequently%20used%20words.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//github.com/Abhinaw3906/ABHINAW-MATRIX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABHINAW%253A%2520A%2520method%2520for%2520Automatic%2520Evaluation%2520of%2520Typography%2520within%250A%2520%2520AI-Generated%2520Images%26entry.906535625%3DAbhinaw%2520Jagtap%2520and%2520Nachiket%2520Tapas%2520and%2520R.%2520G.%2520Brajesh%26entry.1292438233%3D%2520%2520In%2520the%2520fast-evolving%2520field%2520of%2520Generative%2520AI%252C%2520platforms%2520like%2520MidJourney%252C%250ADALL-E%252C%2520and%2520Stable%2520Diffusion%2520have%2520transformed%2520Text-to-Image%2520%2528T2I%2529%2520Generation.%250AHowever%252C%2520despite%2520their%2520impressive%2520ability%2520to%2520create%2520high-quality%2520images%252C%2520they%250Aoften%2520struggle%2520to%2520generate%2520accurate%2520text%2520within%2520these%2520images.%2520Theoretically%252C%2520if%250Awe%2520could%2520achieve%2520accurate%2520text%2520generation%2520in%2520AI%2520images%2520in%2520a%2520%2560%2560zero-shot%2527%2527%250Amanner%252C%2520it%2520would%2520not%2520only%2520make%2520AI-generated%2520images%2520more%2520meaningful%2520but%2520also%250Ademocratize%2520the%2520graphic%2520design%2520industry.%2520The%2520first%2520step%2520towards%2520this%2520goal%2520is%2520to%250Acreate%2520a%2520robust%2520scoring%2520matrix%2520for%2520evaluating%2520text%2520accuracy%2520in%2520AI-generated%250Aimages.%2520Although%2520there%2520are%2520existing%2520bench-marking%2520methods%2520like%2520CLIP%2520SCORE%2520and%250AT2I-CompBench%252B%252B%252C%2520there%2527s%2520still%2520a%2520gap%2520in%2520systematically%2520evaluating%2520text%2520and%250Atypography%2520in%2520AI-generated%2520images%252C%2520especially%2520with%2520diffusion-based%2520methods.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520evaluation%2520matrix%2520designed%2520explicitly%2520for%250Aquantifying%2520the%2520performance%2520of%2520text%2520and%2520typography%2520generation%2520within%250AAI-generated%2520images.%2520We%2520have%2520used%2520letter%2520by%2520letter%2520matching%2520strategy%2520to%2520compute%250Athe%2520exact%2520matching%2520scores%2520from%2520the%2520reference%2520text%2520to%2520the%2520AI%2520generated%2520text.%2520Our%250Anovel%2520approach%2520to%2520calculate%2520the%2520score%2520takes%2520care%2520of%2520multiple%2520redundancies%2520such%250Aas%2520repetition%2520of%2520words%252C%2520case%2520sensitivity%252C%2520mixing%2520of%2520words%252C%2520irregular%250Aincorporation%2520of%2520letters%2520etc.%2520Moreover%252C%2520we%2520have%2520developed%2520a%2520Novel%2520method%2520named%250Aas%2520brevity%2520adjustment%2520to%2520handle%2520excess%2520text.%2520In%2520addition%2520we%2520have%2520also%2520done%2520a%250Aquantitative%2520analysis%2520of%2520frequent%2520errors%2520arise%2520due%2520to%2520frequently%2520used%2520words%2520and%250Aless%2520frequently%2520used%2520words.%2520Project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Abhinaw3906/ABHINAW-MATRIX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABHINAW%3A%20A%20method%20for%20Automatic%20Evaluation%20of%20Typography%20within%0A%20%20AI-Generated%20Images&entry.906535625=Abhinaw%20Jagtap%20and%20Nachiket%20Tapas%20and%20R.%20G.%20Brajesh&entry.1292438233=%20%20In%20the%20fast-evolving%20field%20of%20Generative%20AI%2C%20platforms%20like%20MidJourney%2C%0ADALL-E%2C%20and%20Stable%20Diffusion%20have%20transformed%20Text-to-Image%20%28T2I%29%20Generation.%0AHowever%2C%20despite%20their%20impressive%20ability%20to%20create%20high-quality%20images%2C%20they%0Aoften%20struggle%20to%20generate%20accurate%20text%20within%20these%20images.%20Theoretically%2C%20if%0Awe%20could%20achieve%20accurate%20text%20generation%20in%20AI%20images%20in%20a%20%60%60zero-shot%27%27%0Amanner%2C%20it%20would%20not%20only%20make%20AI-generated%20images%20more%20meaningful%20but%20also%0Ademocratize%20the%20graphic%20design%20industry.%20The%20first%20step%20towards%20this%20goal%20is%20to%0Acreate%20a%20robust%20scoring%20matrix%20for%20evaluating%20text%20accuracy%20in%20AI-generated%0Aimages.%20Although%20there%20are%20existing%20bench-marking%20methods%20like%20CLIP%20SCORE%20and%0AT2I-CompBench%2B%2B%2C%20there%27s%20still%20a%20gap%20in%20systematically%20evaluating%20text%20and%0Atypography%20in%20AI-generated%20images%2C%20especially%20with%20diffusion-based%20methods.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20evaluation%20matrix%20designed%20explicitly%20for%0Aquantifying%20the%20performance%20of%20text%20and%20typography%20generation%20within%0AAI-generated%20images.%20We%20have%20used%20letter%20by%20letter%20matching%20strategy%20to%20compute%0Athe%20exact%20matching%20scores%20from%20the%20reference%20text%20to%20the%20AI%20generated%20text.%20Our%0Anovel%20approach%20to%20calculate%20the%20score%20takes%20care%20of%20multiple%20redundancies%20such%0Aas%20repetition%20of%20words%2C%20case%20sensitivity%2C%20mixing%20of%20words%2C%20irregular%0Aincorporation%20of%20letters%20etc.%20Moreover%2C%20we%20have%20developed%20a%20Novel%20method%20named%0Aas%20brevity%20adjustment%20to%20handle%20excess%20text.%20In%20addition%20we%20have%20also%20done%20a%0Aquantitative%20analysis%20of%20frequent%20errors%20arise%20due%20to%20frequently%20used%20words%20and%0Aless%20frequently%20used%20words.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//github.com/Abhinaw3906/ABHINAW-MATRIX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11874v1&entry.124074799=Read"},
{"title": "Metric-Semantic Factor Graph Generation based on Graph Neural Networks", "author": "Jose Andres Millan-Romera and Hriday Bavle and Muhammad Shaheer and Holger Voos and Jose Luis Sanchez-Lopez", "abstract": "  Understanding the relationships between geometric structures and semantic\nconcepts is crucial for building accurate models of complex environments. In\nindoors, certain spatial constraints, such as the relative positioning of\nplanes, remain consistent despite variations in layout. This paper explores how\nthese invariant relationships can be captured in a graph SLAM framework by\nrepresenting high-level concepts like rooms and walls, linking them to\ngeometric elements like planes through an optimizable factor graph. Several\nefforts have tackled this issue with add-hoc solutions for each concept\ngeneration and with manually-defined factors.\n  This paper proposes a novel method for metric-semantic factor graph\ngeneration which includes defining a semantic scene graph, integrating\ngeometric information, and learning the interconnecting factors, all based on\nGraph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the\nedges between planes into same room, same wall or none types. The resulting\nrelations are clustered, generating a room or wall for each cluster. A second\nfamily of networks (F-GNN) infers the geometrical origin of the new nodes. The\ndefinition of the factors employs the same F-GNN used for the metric attribute\nof the generated nodes. Furthermore, share the new factor graph with the\nS-Graphs+ algorithm, extending its graph expressiveness and scene\nrepresentation with the ultimate goal of improving the SLAM performance. The\ncomplexity of the environments is increased to N-plane rooms by training the\nnetworks on L-shaped rooms. The framework is evaluated in synthetic and\nsimulated scenarios as no real datasets of the required complex layouts are\navailable.\n", "link": "http://arxiv.org/abs/2409.11972v1", "date": "2024-09-18", "relevancy": 2.1683, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5264}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric-Semantic%20Factor%20Graph%20Generation%20based%20on%20Graph%20Neural%20Networks&body=Title%3A%20Metric-Semantic%20Factor%20Graph%20Generation%20based%20on%20Graph%20Neural%20Networks%0AAuthor%3A%20Jose%20Andres%20Millan-Romera%20and%20Hriday%20Bavle%20and%20Muhammad%20Shaheer%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez%0AAbstract%3A%20%20%20Understanding%20the%20relationships%20between%20geometric%20structures%20and%20semantic%0Aconcepts%20is%20crucial%20for%20building%20accurate%20models%20of%20complex%20environments.%20In%0Aindoors%2C%20certain%20spatial%20constraints%2C%20such%20as%20the%20relative%20positioning%20of%0Aplanes%2C%20remain%20consistent%20despite%20variations%20in%20layout.%20This%20paper%20explores%20how%0Athese%20invariant%20relationships%20can%20be%20captured%20in%20a%20graph%20SLAM%20framework%20by%0Arepresenting%20high-level%20concepts%20like%20rooms%20and%20walls%2C%20linking%20them%20to%0Ageometric%20elements%20like%20planes%20through%20an%20optimizable%20factor%20graph.%20Several%0Aefforts%20have%20tackled%20this%20issue%20with%20add-hoc%20solutions%20for%20each%20concept%0Ageneration%20and%20with%20manually-defined%20factors.%0A%20%20This%20paper%20proposes%20a%20novel%20method%20for%20metric-semantic%20factor%20graph%0Ageneration%20which%20includes%20defining%20a%20semantic%20scene%20graph%2C%20integrating%0Ageometric%20information%2C%20and%20learning%20the%20interconnecting%20factors%2C%20all%20based%20on%0AGraph%20Neural%20Networks%20%28GNNs%29.%20An%20edge%20classification%20network%20%28G-GNN%29%20sorts%20the%0Aedges%20between%20planes%20into%20same%20room%2C%20same%20wall%20or%20none%20types.%20The%20resulting%0Arelations%20are%20clustered%2C%20generating%20a%20room%20or%20wall%20for%20each%20cluster.%20A%20second%0Afamily%20of%20networks%20%28F-GNN%29%20infers%20the%20geometrical%20origin%20of%20the%20new%20nodes.%20The%0Adefinition%20of%20the%20factors%20employs%20the%20same%20F-GNN%20used%20for%20the%20metric%20attribute%0Aof%20the%20generated%20nodes.%20Furthermore%2C%20share%20the%20new%20factor%20graph%20with%20the%0AS-Graphs%2B%20algorithm%2C%20extending%20its%20graph%20expressiveness%20and%20scene%0Arepresentation%20with%20the%20ultimate%20goal%20of%20improving%20the%20SLAM%20performance.%20The%0Acomplexity%20of%20the%20environments%20is%20increased%20to%20N-plane%20rooms%20by%20training%20the%0Anetworks%20on%20L-shaped%20rooms.%20The%20framework%20is%20evaluated%20in%20synthetic%20and%0Asimulated%20scenarios%20as%20no%20real%20datasets%20of%20the%20required%20complex%20layouts%20are%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric-Semantic%2520Factor%2520Graph%2520Generation%2520based%2520on%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJose%2520Andres%2520Millan-Romera%2520and%2520Hriday%2520Bavle%2520and%2520Muhammad%2520Shaheer%2520and%2520Holger%2520Voos%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%26entry.1292438233%3D%2520%2520Understanding%2520the%2520relationships%2520between%2520geometric%2520structures%2520and%2520semantic%250Aconcepts%2520is%2520crucial%2520for%2520building%2520accurate%2520models%2520of%2520complex%2520environments.%2520In%250Aindoors%252C%2520certain%2520spatial%2520constraints%252C%2520such%2520as%2520the%2520relative%2520positioning%2520of%250Aplanes%252C%2520remain%2520consistent%2520despite%2520variations%2520in%2520layout.%2520This%2520paper%2520explores%2520how%250Athese%2520invariant%2520relationships%2520can%2520be%2520captured%2520in%2520a%2520graph%2520SLAM%2520framework%2520by%250Arepresenting%2520high-level%2520concepts%2520like%2520rooms%2520and%2520walls%252C%2520linking%2520them%2520to%250Ageometric%2520elements%2520like%2520planes%2520through%2520an%2520optimizable%2520factor%2520graph.%2520Several%250Aefforts%2520have%2520tackled%2520this%2520issue%2520with%2520add-hoc%2520solutions%2520for%2520each%2520concept%250Ageneration%2520and%2520with%2520manually-defined%2520factors.%250A%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520method%2520for%2520metric-semantic%2520factor%2520graph%250Ageneration%2520which%2520includes%2520defining%2520a%2520semantic%2520scene%2520graph%252C%2520integrating%250Ageometric%2520information%252C%2520and%2520learning%2520the%2520interconnecting%2520factors%252C%2520all%2520based%2520on%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520An%2520edge%2520classification%2520network%2520%2528G-GNN%2529%2520sorts%2520the%250Aedges%2520between%2520planes%2520into%2520same%2520room%252C%2520same%2520wall%2520or%2520none%2520types.%2520The%2520resulting%250Arelations%2520are%2520clustered%252C%2520generating%2520a%2520room%2520or%2520wall%2520for%2520each%2520cluster.%2520A%2520second%250Afamily%2520of%2520networks%2520%2528F-GNN%2529%2520infers%2520the%2520geometrical%2520origin%2520of%2520the%2520new%2520nodes.%2520The%250Adefinition%2520of%2520the%2520factors%2520employs%2520the%2520same%2520F-GNN%2520used%2520for%2520the%2520metric%2520attribute%250Aof%2520the%2520generated%2520nodes.%2520Furthermore%252C%2520share%2520the%2520new%2520factor%2520graph%2520with%2520the%250AS-Graphs%252B%2520algorithm%252C%2520extending%2520its%2520graph%2520expressiveness%2520and%2520scene%250Arepresentation%2520with%2520the%2520ultimate%2520goal%2520of%2520improving%2520the%2520SLAM%2520performance.%2520The%250Acomplexity%2520of%2520the%2520environments%2520is%2520increased%2520to%2520N-plane%2520rooms%2520by%2520training%2520the%250Anetworks%2520on%2520L-shaped%2520rooms.%2520The%2520framework%2520is%2520evaluated%2520in%2520synthetic%2520and%250Asimulated%2520scenarios%2520as%2520no%2520real%2520datasets%2520of%2520the%2520required%2520complex%2520layouts%2520are%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric-Semantic%20Factor%20Graph%20Generation%20based%20on%20Graph%20Neural%20Networks&entry.906535625=Jose%20Andres%20Millan-Romera%20and%20Hriday%20Bavle%20and%20Muhammad%20Shaheer%20and%20Holger%20Voos%20and%20Jose%20Luis%20Sanchez-Lopez&entry.1292438233=%20%20Understanding%20the%20relationships%20between%20geometric%20structures%20and%20semantic%0Aconcepts%20is%20crucial%20for%20building%20accurate%20models%20of%20complex%20environments.%20In%0Aindoors%2C%20certain%20spatial%20constraints%2C%20such%20as%20the%20relative%20positioning%20of%0Aplanes%2C%20remain%20consistent%20despite%20variations%20in%20layout.%20This%20paper%20explores%20how%0Athese%20invariant%20relationships%20can%20be%20captured%20in%20a%20graph%20SLAM%20framework%20by%0Arepresenting%20high-level%20concepts%20like%20rooms%20and%20walls%2C%20linking%20them%20to%0Ageometric%20elements%20like%20planes%20through%20an%20optimizable%20factor%20graph.%20Several%0Aefforts%20have%20tackled%20this%20issue%20with%20add-hoc%20solutions%20for%20each%20concept%0Ageneration%20and%20with%20manually-defined%20factors.%0A%20%20This%20paper%20proposes%20a%20novel%20method%20for%20metric-semantic%20factor%20graph%0Ageneration%20which%20includes%20defining%20a%20semantic%20scene%20graph%2C%20integrating%0Ageometric%20information%2C%20and%20learning%20the%20interconnecting%20factors%2C%20all%20based%20on%0AGraph%20Neural%20Networks%20%28GNNs%29.%20An%20edge%20classification%20network%20%28G-GNN%29%20sorts%20the%0Aedges%20between%20planes%20into%20same%20room%2C%20same%20wall%20or%20none%20types.%20The%20resulting%0Arelations%20are%20clustered%2C%20generating%20a%20room%20or%20wall%20for%20each%20cluster.%20A%20second%0Afamily%20of%20networks%20%28F-GNN%29%20infers%20the%20geometrical%20origin%20of%20the%20new%20nodes.%20The%0Adefinition%20of%20the%20factors%20employs%20the%20same%20F-GNN%20used%20for%20the%20metric%20attribute%0Aof%20the%20generated%20nodes.%20Furthermore%2C%20share%20the%20new%20factor%20graph%20with%20the%0AS-Graphs%2B%20algorithm%2C%20extending%20its%20graph%20expressiveness%20and%20scene%0Arepresentation%20with%20the%20ultimate%20goal%20of%20improving%20the%20SLAM%20performance.%20The%0Acomplexity%20of%20the%20environments%20is%20increased%20to%20N-plane%20rooms%20by%20training%20the%0Anetworks%20on%20L-shaped%20rooms.%20The%20framework%20is%20evaluated%20in%20synthetic%20and%0Asimulated%20scenarios%20as%20no%20real%20datasets%20of%20the%20required%20complex%20layouts%20are%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11972v1&entry.124074799=Read"},
{"title": "Physically-Based Photometric Bundle Adjustment in Non-Lambertian\n  Environments", "author": "Lei Cheng and Junpeng Hu and Haodong Yan and Mariia Gladkova and Tianyu Huang and Yun-Hui Liu and Daniel Cremers and Haoang Li", "abstract": "  Photometric bundle adjustment (PBA) is widely used in estimating the camera\npose and 3D geometry by assuming a Lambertian world. However, the assumption of\nphotometric consistency is often violated since the non-diffuse reflection is\ncommon in real-world environments. The photometric inconsistency significantly\naffects the reliability of existing PBA methods. To solve this problem, we\npropose a novel physically-based PBA method. Specifically, we introduce the\nphysically-based weights regarding material, illumination, and light path.\nThese weights distinguish the pixel pairs with different levels of photometric\ninconsistency. We also design corresponding models for material estimation\nbased on sequential images and illumination estimation based on point clouds.\nIn addition, we establish the first SLAM-related dataset of non-Lambertian\nscenes with complete ground truth of illumination and material. Extensive\nexperiments demonstrated that our PBA method outperforms existing approaches in\naccuracy.\n", "link": "http://arxiv.org/abs/2409.11854v1", "date": "2024-09-18", "relevancy": 2.1633, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.544}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically-Based%20Photometric%20Bundle%20Adjustment%20in%20Non-Lambertian%0A%20%20Environments&body=Title%3A%20Physically-Based%20Photometric%20Bundle%20Adjustment%20in%20Non-Lambertian%0A%20%20Environments%0AAuthor%3A%20Lei%20Cheng%20and%20Junpeng%20Hu%20and%20Haodong%20Yan%20and%20Mariia%20Gladkova%20and%20Tianyu%20Huang%20and%20Yun-Hui%20Liu%20and%20Daniel%20Cremers%20and%20Haoang%20Li%0AAbstract%3A%20%20%20Photometric%20bundle%20adjustment%20%28PBA%29%20is%20widely%20used%20in%20estimating%20the%20camera%0Apose%20and%203D%20geometry%20by%20assuming%20a%20Lambertian%20world.%20However%2C%20the%20assumption%20of%0Aphotometric%20consistency%20is%20often%20violated%20since%20the%20non-diffuse%20reflection%20is%0Acommon%20in%20real-world%20environments.%20The%20photometric%20inconsistency%20significantly%0Aaffects%20the%20reliability%20of%20existing%20PBA%20methods.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20physically-based%20PBA%20method.%20Specifically%2C%20we%20introduce%20the%0Aphysically-based%20weights%20regarding%20material%2C%20illumination%2C%20and%20light%20path.%0AThese%20weights%20distinguish%20the%20pixel%20pairs%20with%20different%20levels%20of%20photometric%0Ainconsistency.%20We%20also%20design%20corresponding%20models%20for%20material%20estimation%0Abased%20on%20sequential%20images%20and%20illumination%20estimation%20based%20on%20point%20clouds.%0AIn%20addition%2C%20we%20establish%20the%20first%20SLAM-related%20dataset%20of%20non-Lambertian%0Ascenes%20with%20complete%20ground%20truth%20of%20illumination%20and%20material.%20Extensive%0Aexperiments%20demonstrated%20that%20our%20PBA%20method%20outperforms%20existing%20approaches%20in%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically-Based%2520Photometric%2520Bundle%2520Adjustment%2520in%2520Non-Lambertian%250A%2520%2520Environments%26entry.906535625%3DLei%2520Cheng%2520and%2520Junpeng%2520Hu%2520and%2520Haodong%2520Yan%2520and%2520Mariia%2520Gladkova%2520and%2520Tianyu%2520Huang%2520and%2520Yun-Hui%2520Liu%2520and%2520Daniel%2520Cremers%2520and%2520Haoang%2520Li%26entry.1292438233%3D%2520%2520Photometric%2520bundle%2520adjustment%2520%2528PBA%2529%2520is%2520widely%2520used%2520in%2520estimating%2520the%2520camera%250Apose%2520and%25203D%2520geometry%2520by%2520assuming%2520a%2520Lambertian%2520world.%2520However%252C%2520the%2520assumption%2520of%250Aphotometric%2520consistency%2520is%2520often%2520violated%2520since%2520the%2520non-diffuse%2520reflection%2520is%250Acommon%2520in%2520real-world%2520environments.%2520The%2520photometric%2520inconsistency%2520significantly%250Aaffects%2520the%2520reliability%2520of%2520existing%2520PBA%2520methods.%2520To%2520solve%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520physically-based%2520PBA%2520method.%2520Specifically%252C%2520we%2520introduce%2520the%250Aphysically-based%2520weights%2520regarding%2520material%252C%2520illumination%252C%2520and%2520light%2520path.%250AThese%2520weights%2520distinguish%2520the%2520pixel%2520pairs%2520with%2520different%2520levels%2520of%2520photometric%250Ainconsistency.%2520We%2520also%2520design%2520corresponding%2520models%2520for%2520material%2520estimation%250Abased%2520on%2520sequential%2520images%2520and%2520illumination%2520estimation%2520based%2520on%2520point%2520clouds.%250AIn%2520addition%252C%2520we%2520establish%2520the%2520first%2520SLAM-related%2520dataset%2520of%2520non-Lambertian%250Ascenes%2520with%2520complete%2520ground%2520truth%2520of%2520illumination%2520and%2520material.%2520Extensive%250Aexperiments%2520demonstrated%2520that%2520our%2520PBA%2520method%2520outperforms%2520existing%2520approaches%2520in%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically-Based%20Photometric%20Bundle%20Adjustment%20in%20Non-Lambertian%0A%20%20Environments&entry.906535625=Lei%20Cheng%20and%20Junpeng%20Hu%20and%20Haodong%20Yan%20and%20Mariia%20Gladkova%20and%20Tianyu%20Huang%20and%20Yun-Hui%20Liu%20and%20Daniel%20Cremers%20and%20Haoang%20Li&entry.1292438233=%20%20Photometric%20bundle%20adjustment%20%28PBA%29%20is%20widely%20used%20in%20estimating%20the%20camera%0Apose%20and%203D%20geometry%20by%20assuming%20a%20Lambertian%20world.%20However%2C%20the%20assumption%20of%0Aphotometric%20consistency%20is%20often%20violated%20since%20the%20non-diffuse%20reflection%20is%0Acommon%20in%20real-world%20environments.%20The%20photometric%20inconsistency%20significantly%0Aaffects%20the%20reliability%20of%20existing%20PBA%20methods.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20physically-based%20PBA%20method.%20Specifically%2C%20we%20introduce%20the%0Aphysically-based%20weights%20regarding%20material%2C%20illumination%2C%20and%20light%20path.%0AThese%20weights%20distinguish%20the%20pixel%20pairs%20with%20different%20levels%20of%20photometric%0Ainconsistency.%20We%20also%20design%20corresponding%20models%20for%20material%20estimation%0Abased%20on%20sequential%20images%20and%20illumination%20estimation%20based%20on%20point%20clouds.%0AIn%20addition%2C%20we%20establish%20the%20first%20SLAM-related%20dataset%20of%20non-Lambertian%0Ascenes%20with%20complete%20ground%20truth%20of%20illumination%20and%20material.%20Extensive%0Aexperiments%20demonstrated%20that%20our%20PBA%20method%20outperforms%20existing%20approaches%20in%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11854v1&entry.124074799=Read"},
{"title": "On Vision Transformers for Classification Tasks in Side-Scan Sonar\n  Imagery", "author": "BW Sheffield and Jeffrey Ellen and Ben Whitmore", "abstract": "  Side-scan sonar (SSS) imagery presents unique challenges in the\nclassification of man-made objects on the seafloor due to the complex and\nvaried underwater environments. Historically, experts have manually interpreted\nSSS images, relying on conventional machine learning techniques with\nhand-crafted features. While Convolutional Neural Networks (CNNs) significantly\nadvanced automated classification in this domain, they often fall short when\ndealing with diverse seafloor textures, such as rocky or ripple sand bottoms,\nwhere false positive rates may increase. Recently, Vision Transformers (ViTs)\nhave shown potential in addressing these limitations by utilizing a\nself-attention mechanism to capture global information in image patches,\noffering more flexibility in processing spatial hierarchies. This paper\nrigorously compares the performance of ViT models alongside commonly used CNN\narchitectures, such as ResNet and ConvNext, for binary classification tasks in\nSSS imagery. The dataset encompasses diverse geographical seafloor types and is\nbalanced between the presence and absence of man-made objects. ViT-based models\nexhibit superior classification performance across f1-score, precision, recall,\nand accuracy metrics, although at the cost of greater computational resources.\nCNNs, with their inductive biases, demonstrate better computational efficiency,\nmaking them suitable for deployment in resource-constrained environments like\nunderwater vehicles. Future research directions include exploring\nself-supervised learning for ViTs and multi-modal fusion to further enhance\nperformance in challenging underwater environments.\n", "link": "http://arxiv.org/abs/2409.12026v1", "date": "2024-09-18", "relevancy": 2.1624, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Vision%20Transformers%20for%20Classification%20Tasks%20in%20Side-Scan%20Sonar%0A%20%20Imagery&body=Title%3A%20On%20Vision%20Transformers%20for%20Classification%20Tasks%20in%20Side-Scan%20Sonar%0A%20%20Imagery%0AAuthor%3A%20BW%20Sheffield%20and%20Jeffrey%20Ellen%20and%20Ben%20Whitmore%0AAbstract%3A%20%20%20Side-scan%20sonar%20%28SSS%29%20imagery%20presents%20unique%20challenges%20in%20the%0Aclassification%20of%20man-made%20objects%20on%20the%20seafloor%20due%20to%20the%20complex%20and%0Avaried%20underwater%20environments.%20Historically%2C%20experts%20have%20manually%20interpreted%0ASSS%20images%2C%20relying%20on%20conventional%20machine%20learning%20techniques%20with%0Ahand-crafted%20features.%20While%20Convolutional%20Neural%20Networks%20%28CNNs%29%20significantly%0Aadvanced%20automated%20classification%20in%20this%20domain%2C%20they%20often%20fall%20short%20when%0Adealing%20with%20diverse%20seafloor%20textures%2C%20such%20as%20rocky%20or%20ripple%20sand%20bottoms%2C%0Awhere%20false%20positive%20rates%20may%20increase.%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%0Ahave%20shown%20potential%20in%20addressing%20these%20limitations%20by%20utilizing%20a%0Aself-attention%20mechanism%20to%20capture%20global%20information%20in%20image%20patches%2C%0Aoffering%20more%20flexibility%20in%20processing%20spatial%20hierarchies.%20This%20paper%0Arigorously%20compares%20the%20performance%20of%20ViT%20models%20alongside%20commonly%20used%20CNN%0Aarchitectures%2C%20such%20as%20ResNet%20and%20ConvNext%2C%20for%20binary%20classification%20tasks%20in%0ASSS%20imagery.%20The%20dataset%20encompasses%20diverse%20geographical%20seafloor%20types%20and%20is%0Abalanced%20between%20the%20presence%20and%20absence%20of%20man-made%20objects.%20ViT-based%20models%0Aexhibit%20superior%20classification%20performance%20across%20f1-score%2C%20precision%2C%20recall%2C%0Aand%20accuracy%20metrics%2C%20although%20at%20the%20cost%20of%20greater%20computational%20resources.%0ACNNs%2C%20with%20their%20inductive%20biases%2C%20demonstrate%20better%20computational%20efficiency%2C%0Amaking%20them%20suitable%20for%20deployment%20in%20resource-constrained%20environments%20like%0Aunderwater%20vehicles.%20Future%20research%20directions%20include%20exploring%0Aself-supervised%20learning%20for%20ViTs%20and%20multi-modal%20fusion%20to%20further%20enhance%0Aperformance%20in%20challenging%20underwater%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Vision%2520Transformers%2520for%2520Classification%2520Tasks%2520in%2520Side-Scan%2520Sonar%250A%2520%2520Imagery%26entry.906535625%3DBW%2520Sheffield%2520and%2520Jeffrey%2520Ellen%2520and%2520Ben%2520Whitmore%26entry.1292438233%3D%2520%2520Side-scan%2520sonar%2520%2528SSS%2529%2520imagery%2520presents%2520unique%2520challenges%2520in%2520the%250Aclassification%2520of%2520man-made%2520objects%2520on%2520the%2520seafloor%2520due%2520to%2520the%2520complex%2520and%250Avaried%2520underwater%2520environments.%2520Historically%252C%2520experts%2520have%2520manually%2520interpreted%250ASSS%2520images%252C%2520relying%2520on%2520conventional%2520machine%2520learning%2520techniques%2520with%250Ahand-crafted%2520features.%2520While%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520significantly%250Aadvanced%2520automated%2520classification%2520in%2520this%2520domain%252C%2520they%2520often%2520fall%2520short%2520when%250Adealing%2520with%2520diverse%2520seafloor%2520textures%252C%2520such%2520as%2520rocky%2520or%2520ripple%2520sand%2520bottoms%252C%250Awhere%2520false%2520positive%2520rates%2520may%2520increase.%2520Recently%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%250Ahave%2520shown%2520potential%2520in%2520addressing%2520these%2520limitations%2520by%2520utilizing%2520a%250Aself-attention%2520mechanism%2520to%2520capture%2520global%2520information%2520in%2520image%2520patches%252C%250Aoffering%2520more%2520flexibility%2520in%2520processing%2520spatial%2520hierarchies.%2520This%2520paper%250Arigorously%2520compares%2520the%2520performance%2520of%2520ViT%2520models%2520alongside%2520commonly%2520used%2520CNN%250Aarchitectures%252C%2520such%2520as%2520ResNet%2520and%2520ConvNext%252C%2520for%2520binary%2520classification%2520tasks%2520in%250ASSS%2520imagery.%2520The%2520dataset%2520encompasses%2520diverse%2520geographical%2520seafloor%2520types%2520and%2520is%250Abalanced%2520between%2520the%2520presence%2520and%2520absence%2520of%2520man-made%2520objects.%2520ViT-based%2520models%250Aexhibit%2520superior%2520classification%2520performance%2520across%2520f1-score%252C%2520precision%252C%2520recall%252C%250Aand%2520accuracy%2520metrics%252C%2520although%2520at%2520the%2520cost%2520of%2520greater%2520computational%2520resources.%250ACNNs%252C%2520with%2520their%2520inductive%2520biases%252C%2520demonstrate%2520better%2520computational%2520efficiency%252C%250Amaking%2520them%2520suitable%2520for%2520deployment%2520in%2520resource-constrained%2520environments%2520like%250Aunderwater%2520vehicles.%2520Future%2520research%2520directions%2520include%2520exploring%250Aself-supervised%2520learning%2520for%2520ViTs%2520and%2520multi-modal%2520fusion%2520to%2520further%2520enhance%250Aperformance%2520in%2520challenging%2520underwater%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Vision%20Transformers%20for%20Classification%20Tasks%20in%20Side-Scan%20Sonar%0A%20%20Imagery&entry.906535625=BW%20Sheffield%20and%20Jeffrey%20Ellen%20and%20Ben%20Whitmore&entry.1292438233=%20%20Side-scan%20sonar%20%28SSS%29%20imagery%20presents%20unique%20challenges%20in%20the%0Aclassification%20of%20man-made%20objects%20on%20the%20seafloor%20due%20to%20the%20complex%20and%0Avaried%20underwater%20environments.%20Historically%2C%20experts%20have%20manually%20interpreted%0ASSS%20images%2C%20relying%20on%20conventional%20machine%20learning%20techniques%20with%0Ahand-crafted%20features.%20While%20Convolutional%20Neural%20Networks%20%28CNNs%29%20significantly%0Aadvanced%20automated%20classification%20in%20this%20domain%2C%20they%20often%20fall%20short%20when%0Adealing%20with%20diverse%20seafloor%20textures%2C%20such%20as%20rocky%20or%20ripple%20sand%20bottoms%2C%0Awhere%20false%20positive%20rates%20may%20increase.%20Recently%2C%20Vision%20Transformers%20%28ViTs%29%0Ahave%20shown%20potential%20in%20addressing%20these%20limitations%20by%20utilizing%20a%0Aself-attention%20mechanism%20to%20capture%20global%20information%20in%20image%20patches%2C%0Aoffering%20more%20flexibility%20in%20processing%20spatial%20hierarchies.%20This%20paper%0Arigorously%20compares%20the%20performance%20of%20ViT%20models%20alongside%20commonly%20used%20CNN%0Aarchitectures%2C%20such%20as%20ResNet%20and%20ConvNext%2C%20for%20binary%20classification%20tasks%20in%0ASSS%20imagery.%20The%20dataset%20encompasses%20diverse%20geographical%20seafloor%20types%20and%20is%0Abalanced%20between%20the%20presence%20and%20absence%20of%20man-made%20objects.%20ViT-based%20models%0Aexhibit%20superior%20classification%20performance%20across%20f1-score%2C%20precision%2C%20recall%2C%0Aand%20accuracy%20metrics%2C%20although%20at%20the%20cost%20of%20greater%20computational%20resources.%0ACNNs%2C%20with%20their%20inductive%20biases%2C%20demonstrate%20better%20computational%20efficiency%2C%0Amaking%20them%20suitable%20for%20deployment%20in%20resource-constrained%20environments%20like%0Aunderwater%20vehicles.%20Future%20research%20directions%20include%20exploring%0Aself-supervised%20learning%20for%20ViTs%20and%20multi-modal%20fusion%20to%20further%20enhance%0Aperformance%20in%20challenging%20underwater%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12026v1&entry.124074799=Read"},
{"title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty", "author": "Arslan Chaudhry and Sridhar Thiagarajan and Dilan Gorur", "abstract": "  Large language models (LLMs) are increasingly employed in information-seeking\nand decision-making tasks. Despite their broad utility, LLMs tend to generate\ninformation that conflicts with real-world facts, and their persuasive style\ncan make these inaccuracies appear confident and convincing. As a result,\nend-users struggle to consistently align the confidence expressed by LLMs with\nthe accuracy of their predictions, often leading to either blind trust in all\noutputs or a complete disregard for their reliability. In this work, we explore\nsupervised finetuning on uncertainty-augmented predictions as a method to\ndevelop models that produce linguistic expressions of uncertainty.\nSpecifically, we measure the calibration of pre-trained models and then\nfine-tune language models to generate calibrated linguistic expressions of\nuncertainty. Through experiments on various question-answering datasets, we\ndemonstrate that LLMs are well-calibrated in assessing their predictions, and\nsupervised finetuning based on the model's own confidence leads to\nwell-calibrated expressions of uncertainty, particularly for single-claim\nanswers.\n", "link": "http://arxiv.org/abs/2409.12180v1", "date": "2024-09-18", "relevancy": 2.1469, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6097}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finetuning%20Language%20Models%20to%20Emit%20Linguistic%20Expressions%20of%20Uncertainty&body=Title%3A%20Finetuning%20Language%20Models%20to%20Emit%20Linguistic%20Expressions%20of%20Uncertainty%0AAuthor%3A%20Arslan%20Chaudhry%20and%20Sridhar%20Thiagarajan%20and%20Dilan%20Gorur%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20employed%20in%20information-seeking%0Aand%20decision-making%20tasks.%20Despite%20their%20broad%20utility%2C%20LLMs%20tend%20to%20generate%0Ainformation%20that%20conflicts%20with%20real-world%20facts%2C%20and%20their%20persuasive%20style%0Acan%20make%20these%20inaccuracies%20appear%20confident%20and%20convincing.%20As%20a%20result%2C%0Aend-users%20struggle%20to%20consistently%20align%20the%20confidence%20expressed%20by%20LLMs%20with%0Athe%20accuracy%20of%20their%20predictions%2C%20often%20leading%20to%20either%20blind%20trust%20in%20all%0Aoutputs%20or%20a%20complete%20disregard%20for%20their%20reliability.%20In%20this%20work%2C%20we%20explore%0Asupervised%20finetuning%20on%20uncertainty-augmented%20predictions%20as%20a%20method%20to%0Adevelop%20models%20that%20produce%20linguistic%20expressions%20of%20uncertainty.%0ASpecifically%2C%20we%20measure%20the%20calibration%20of%20pre-trained%20models%20and%20then%0Afine-tune%20language%20models%20to%20generate%20calibrated%20linguistic%20expressions%20of%0Auncertainty.%20Through%20experiments%20on%20various%20question-answering%20datasets%2C%20we%0Ademonstrate%20that%20LLMs%20are%20well-calibrated%20in%20assessing%20their%20predictions%2C%20and%0Asupervised%20finetuning%20based%20on%20the%20model%27s%20own%20confidence%20leads%20to%0Awell-calibrated%20expressions%20of%20uncertainty%2C%20particularly%20for%20single-claim%0Aanswers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinetuning%2520Language%2520Models%2520to%2520Emit%2520Linguistic%2520Expressions%2520of%2520Uncertainty%26entry.906535625%3DArslan%2520Chaudhry%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Dilan%2520Gorur%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520employed%2520in%2520information-seeking%250Aand%2520decision-making%2520tasks.%2520Despite%2520their%2520broad%2520utility%252C%2520LLMs%2520tend%2520to%2520generate%250Ainformation%2520that%2520conflicts%2520with%2520real-world%2520facts%252C%2520and%2520their%2520persuasive%2520style%250Acan%2520make%2520these%2520inaccuracies%2520appear%2520confident%2520and%2520convincing.%2520As%2520a%2520result%252C%250Aend-users%2520struggle%2520to%2520consistently%2520align%2520the%2520confidence%2520expressed%2520by%2520LLMs%2520with%250Athe%2520accuracy%2520of%2520their%2520predictions%252C%2520often%2520leading%2520to%2520either%2520blind%2520trust%2520in%2520all%250Aoutputs%2520or%2520a%2520complete%2520disregard%2520for%2520their%2520reliability.%2520In%2520this%2520work%252C%2520we%2520explore%250Asupervised%2520finetuning%2520on%2520uncertainty-augmented%2520predictions%2520as%2520a%2520method%2520to%250Adevelop%2520models%2520that%2520produce%2520linguistic%2520expressions%2520of%2520uncertainty.%250ASpecifically%252C%2520we%2520measure%2520the%2520calibration%2520of%2520pre-trained%2520models%2520and%2520then%250Afine-tune%2520language%2520models%2520to%2520generate%2520calibrated%2520linguistic%2520expressions%2520of%250Auncertainty.%2520Through%2520experiments%2520on%2520various%2520question-answering%2520datasets%252C%2520we%250Ademonstrate%2520that%2520LLMs%2520are%2520well-calibrated%2520in%2520assessing%2520their%2520predictions%252C%2520and%250Asupervised%2520finetuning%2520based%2520on%2520the%2520model%2527s%2520own%2520confidence%2520leads%2520to%250Awell-calibrated%2520expressions%2520of%2520uncertainty%252C%2520particularly%2520for%2520single-claim%250Aanswers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finetuning%20Language%20Models%20to%20Emit%20Linguistic%20Expressions%20of%20Uncertainty&entry.906535625=Arslan%20Chaudhry%20and%20Sridhar%20Thiagarajan%20and%20Dilan%20Gorur&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20employed%20in%20information-seeking%0Aand%20decision-making%20tasks.%20Despite%20their%20broad%20utility%2C%20LLMs%20tend%20to%20generate%0Ainformation%20that%20conflicts%20with%20real-world%20facts%2C%20and%20their%20persuasive%20style%0Acan%20make%20these%20inaccuracies%20appear%20confident%20and%20convincing.%20As%20a%20result%2C%0Aend-users%20struggle%20to%20consistently%20align%20the%20confidence%20expressed%20by%20LLMs%20with%0Athe%20accuracy%20of%20their%20predictions%2C%20often%20leading%20to%20either%20blind%20trust%20in%20all%0Aoutputs%20or%20a%20complete%20disregard%20for%20their%20reliability.%20In%20this%20work%2C%20we%20explore%0Asupervised%20finetuning%20on%20uncertainty-augmented%20predictions%20as%20a%20method%20to%0Adevelop%20models%20that%20produce%20linguistic%20expressions%20of%20uncertainty.%0ASpecifically%2C%20we%20measure%20the%20calibration%20of%20pre-trained%20models%20and%20then%0Afine-tune%20language%20models%20to%20generate%20calibrated%20linguistic%20expressions%20of%0Auncertainty.%20Through%20experiments%20on%20various%20question-answering%20datasets%2C%20we%0Ademonstrate%20that%20LLMs%20are%20well-calibrated%20in%20assessing%20their%20predictions%2C%20and%0Asupervised%20finetuning%20based%20on%20the%20model%27s%20own%20confidence%20leads%20to%0Awell-calibrated%20expressions%20of%20uncertainty%2C%20particularly%20for%20single-claim%0Aanswers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12180v1&entry.124074799=Read"},
{"title": "Mixture of Prompt Learning for Vision Language Models", "author": "Yu Du and Tong Niu and Rong Zhao", "abstract": "  As powerful pre-trained vision-language models (VLMs) like CLIP gain\nprominence, numerous studies have attempted to combine VLMs for downstream\ntasks. Among these, prompt learning has been validated as an effective method\nfor adapting to new tasks, which only requiring a small number of parameters.\nHowever, current prompt learning methods face two challenges: first, a single\nsoft prompt struggles to capture the diverse styles and patterns within a\ndataset; second, fine-tuning soft prompts is prone to overfitting. To address\nthese challenges, we propose a mixture of soft prompt learning method\nincorporating a routing module. This module is able to capture a dataset's\nvaried styles and dynamically selects the most suitable prompts for each\ninstance. Additionally, we introduce a novel gating mechanism to ensure the\nrouter selects prompts based on their similarity to hard prompt templates,\nwhich both retaining knowledge from hard prompts and improving selection\naccuracy. We also implement semantically grouped text-level supervision,\ninitializing each soft prompt with the token embeddings of manually designed\ntemplates from its group and applied a contrastive loss between the resulted\ntext feature and hard prompt encoded text feature. This supervision ensures\nthat the text features derived from soft prompts remain close to those from\ntheir corresponding hard prompts, preserving initial knowledge and mitigating\noverfitting. Our method has been validated on 11 datasets, demonstrating\nevident improvements in few-shot learning, domain generalization, and\nbase-to-new generalization scenarios compared to existing baselines. The code\nwill be available at \\url{https://anonymous.4open.science/r/mocoop-6387}\n", "link": "http://arxiv.org/abs/2409.12011v1", "date": "2024-09-18", "relevancy": 2.1455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Prompt%20Learning%20for%20Vision%20Language%20Models&body=Title%3A%20Mixture%20of%20Prompt%20Learning%20for%20Vision%20Language%20Models%0AAuthor%3A%20Yu%20Du%20and%20Tong%20Niu%20and%20Rong%20Zhao%0AAbstract%3A%20%20%20As%20powerful%20pre-trained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20gain%0Aprominence%2C%20numerous%20studies%20have%20attempted%20to%20combine%20VLMs%20for%20downstream%0Atasks.%20Among%20these%2C%20prompt%20learning%20has%20been%20validated%20as%20an%20effective%20method%0Afor%20adapting%20to%20new%20tasks%2C%20which%20only%20requiring%20a%20small%20number%20of%20parameters.%0AHowever%2C%20current%20prompt%20learning%20methods%20face%20two%20challenges%3A%20first%2C%20a%20single%0Asoft%20prompt%20struggles%20to%20capture%20the%20diverse%20styles%20and%20patterns%20within%20a%0Adataset%3B%20second%2C%20fine-tuning%20soft%20prompts%20is%20prone%20to%20overfitting.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20mixture%20of%20soft%20prompt%20learning%20method%0Aincorporating%20a%20routing%20module.%20This%20module%20is%20able%20to%20capture%20a%20dataset%27s%0Avaried%20styles%20and%20dynamically%20selects%20the%20most%20suitable%20prompts%20for%20each%0Ainstance.%20Additionally%2C%20we%20introduce%20a%20novel%20gating%20mechanism%20to%20ensure%20the%0Arouter%20selects%20prompts%20based%20on%20their%20similarity%20to%20hard%20prompt%20templates%2C%0Awhich%20both%20retaining%20knowledge%20from%20hard%20prompts%20and%20improving%20selection%0Aaccuracy.%20We%20also%20implement%20semantically%20grouped%20text-level%20supervision%2C%0Ainitializing%20each%20soft%20prompt%20with%20the%20token%20embeddings%20of%20manually%20designed%0Atemplates%20from%20its%20group%20and%20applied%20a%20contrastive%20loss%20between%20the%20resulted%0Atext%20feature%20and%20hard%20prompt%20encoded%20text%20feature.%20This%20supervision%20ensures%0Athat%20the%20text%20features%20derived%20from%20soft%20prompts%20remain%20close%20to%20those%20from%0Atheir%20corresponding%20hard%20prompts%2C%20preserving%20initial%20knowledge%20and%20mitigating%0Aoverfitting.%20Our%20method%20has%20been%20validated%20on%2011%20datasets%2C%20demonstrating%0Aevident%20improvements%20in%20few-shot%20learning%2C%20domain%20generalization%2C%20and%0Abase-to-new%20generalization%20scenarios%20compared%20to%20existing%20baselines.%20The%20code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//anonymous.4open.science/r/mocoop-6387%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Prompt%2520Learning%2520for%2520Vision%2520Language%2520Models%26entry.906535625%3DYu%2520Du%2520and%2520Tong%2520Niu%2520and%2520Rong%2520Zhao%26entry.1292438233%3D%2520%2520As%2520powerful%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%2520gain%250Aprominence%252C%2520numerous%2520studies%2520have%2520attempted%2520to%2520combine%2520VLMs%2520for%2520downstream%250Atasks.%2520Among%2520these%252C%2520prompt%2520learning%2520has%2520been%2520validated%2520as%2520an%2520effective%2520method%250Afor%2520adapting%2520to%2520new%2520tasks%252C%2520which%2520only%2520requiring%2520a%2520small%2520number%2520of%2520parameters.%250AHowever%252C%2520current%2520prompt%2520learning%2520methods%2520face%2520two%2520challenges%253A%2520first%252C%2520a%2520single%250Asoft%2520prompt%2520struggles%2520to%2520capture%2520the%2520diverse%2520styles%2520and%2520patterns%2520within%2520a%250Adataset%253B%2520second%252C%2520fine-tuning%2520soft%2520prompts%2520is%2520prone%2520to%2520overfitting.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520mixture%2520of%2520soft%2520prompt%2520learning%2520method%250Aincorporating%2520a%2520routing%2520module.%2520This%2520module%2520is%2520able%2520to%2520capture%2520a%2520dataset%2527s%250Avaried%2520styles%2520and%2520dynamically%2520selects%2520the%2520most%2520suitable%2520prompts%2520for%2520each%250Ainstance.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520gating%2520mechanism%2520to%2520ensure%2520the%250Arouter%2520selects%2520prompts%2520based%2520on%2520their%2520similarity%2520to%2520hard%2520prompt%2520templates%252C%250Awhich%2520both%2520retaining%2520knowledge%2520from%2520hard%2520prompts%2520and%2520improving%2520selection%250Aaccuracy.%2520We%2520also%2520implement%2520semantically%2520grouped%2520text-level%2520supervision%252C%250Ainitializing%2520each%2520soft%2520prompt%2520with%2520the%2520token%2520embeddings%2520of%2520manually%2520designed%250Atemplates%2520from%2520its%2520group%2520and%2520applied%2520a%2520contrastive%2520loss%2520between%2520the%2520resulted%250Atext%2520feature%2520and%2520hard%2520prompt%2520encoded%2520text%2520feature.%2520This%2520supervision%2520ensures%250Athat%2520the%2520text%2520features%2520derived%2520from%2520soft%2520prompts%2520remain%2520close%2520to%2520those%2520from%250Atheir%2520corresponding%2520hard%2520prompts%252C%2520preserving%2520initial%2520knowledge%2520and%2520mitigating%250Aoverfitting.%2520Our%2520method%2520has%2520been%2520validated%2520on%252011%2520datasets%252C%2520demonstrating%250Aevident%2520improvements%2520in%2520few-shot%2520learning%252C%2520domain%2520generalization%252C%2520and%250Abase-to-new%2520generalization%2520scenarios%2520compared%2520to%2520existing%2520baselines.%2520The%2520code%250Awill%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//anonymous.4open.science/r/mocoop-6387%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Prompt%20Learning%20for%20Vision%20Language%20Models&entry.906535625=Yu%20Du%20and%20Tong%20Niu%20and%20Rong%20Zhao&entry.1292438233=%20%20As%20powerful%20pre-trained%20vision-language%20models%20%28VLMs%29%20like%20CLIP%20gain%0Aprominence%2C%20numerous%20studies%20have%20attempted%20to%20combine%20VLMs%20for%20downstream%0Atasks.%20Among%20these%2C%20prompt%20learning%20has%20been%20validated%20as%20an%20effective%20method%0Afor%20adapting%20to%20new%20tasks%2C%20which%20only%20requiring%20a%20small%20number%20of%20parameters.%0AHowever%2C%20current%20prompt%20learning%20methods%20face%20two%20challenges%3A%20first%2C%20a%20single%0Asoft%20prompt%20struggles%20to%20capture%20the%20diverse%20styles%20and%20patterns%20within%20a%0Adataset%3B%20second%2C%20fine-tuning%20soft%20prompts%20is%20prone%20to%20overfitting.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20mixture%20of%20soft%20prompt%20learning%20method%0Aincorporating%20a%20routing%20module.%20This%20module%20is%20able%20to%20capture%20a%20dataset%27s%0Avaried%20styles%20and%20dynamically%20selects%20the%20most%20suitable%20prompts%20for%20each%0Ainstance.%20Additionally%2C%20we%20introduce%20a%20novel%20gating%20mechanism%20to%20ensure%20the%0Arouter%20selects%20prompts%20based%20on%20their%20similarity%20to%20hard%20prompt%20templates%2C%0Awhich%20both%20retaining%20knowledge%20from%20hard%20prompts%20and%20improving%20selection%0Aaccuracy.%20We%20also%20implement%20semantically%20grouped%20text-level%20supervision%2C%0Ainitializing%20each%20soft%20prompt%20with%20the%20token%20embeddings%20of%20manually%20designed%0Atemplates%20from%20its%20group%20and%20applied%20a%20contrastive%20loss%20between%20the%20resulted%0Atext%20feature%20and%20hard%20prompt%20encoded%20text%20feature.%20This%20supervision%20ensures%0Athat%20the%20text%20features%20derived%20from%20soft%20prompts%20remain%20close%20to%20those%20from%0Atheir%20corresponding%20hard%20prompts%2C%20preserving%20initial%20knowledge%20and%20mitigating%0Aoverfitting.%20Our%20method%20has%20been%20validated%20on%2011%20datasets%2C%20demonstrating%0Aevident%20improvements%20in%20few-shot%20learning%2C%20domain%20generalization%2C%20and%0Abase-to-new%20generalization%20scenarios%20compared%20to%20existing%20baselines.%20The%20code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//anonymous.4open.science/r/mocoop-6387%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12011v1&entry.124074799=Read"},
{"title": "BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF\n  Modelling", "author": "Lulin Zhang and Ewelina Rupnik and Tri Dung Nguyen and St\u00e9phane Jacquemoud and Yann Klinger", "abstract": "  Understanding the anisotropic reflectance of complex Earth surfaces from\nsatellite imagery is crucial for numerous applications. Neural radiance fields\n(NeRF) have become popular as a machine learning technique capable of deducing\nthe bidirectional reflectance distribution function (BRDF) of a scene from\nmultiple images. However, prior research has largely concentrated on applying\nNeRF to close-range imagery, estimating basic Microfacet BRDF models, which\nfall short for many Earth surfaces. Moreover, high-quality NeRFs generally\nrequire several images captured simultaneously, a rare occurrence in satellite\nimaging. To address these limitations, we propose BRDF-NeRF, developed to\nexplicitly estimate the Rahman-Pinty-Verstraete (RPV) model, a semi-empirical\nBRDF model commonly employed in remote sensing. We assess our approach using\ntwo datasets: (1) Djibouti, captured in a single epoch at varying viewing\nangles with a fixed Sun position, and (2) Lanzhou, captured over multiple\nepochs with different viewing angles and Sun positions. Our results, based on\nonly three to four satellite images for training, demonstrate that BRDF-NeRF\ncan effectively synthesize novel views from directions far removed from the\ntraining data and produce high-quality digital surface models (DSMs).\n", "link": "http://arxiv.org/abs/2409.12014v1", "date": "2024-09-18", "relevancy": 2.1416, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRDF-NeRF%3A%20Neural%20Radiance%20Fields%20with%20Optical%20Satellite%20Images%20and%20BRDF%0A%20%20Modelling&body=Title%3A%20BRDF-NeRF%3A%20Neural%20Radiance%20Fields%20with%20Optical%20Satellite%20Images%20and%20BRDF%0A%20%20Modelling%0AAuthor%3A%20Lulin%20Zhang%20and%20Ewelina%20Rupnik%20and%20Tri%20Dung%20Nguyen%20and%20St%C3%A9phane%20Jacquemoud%20and%20Yann%20Klinger%0AAbstract%3A%20%20%20Understanding%20the%20anisotropic%20reflectance%20of%20complex%20Earth%20surfaces%20from%0Asatellite%20imagery%20is%20crucial%20for%20numerous%20applications.%20Neural%20radiance%20fields%0A%28NeRF%29%20have%20become%20popular%20as%20a%20machine%20learning%20technique%20capable%20of%20deducing%0Athe%20bidirectional%20reflectance%20distribution%20function%20%28BRDF%29%20of%20a%20scene%20from%0Amultiple%20images.%20However%2C%20prior%20research%20has%20largely%20concentrated%20on%20applying%0ANeRF%20to%20close-range%20imagery%2C%20estimating%20basic%20Microfacet%20BRDF%20models%2C%20which%0Afall%20short%20for%20many%20Earth%20surfaces.%20Moreover%2C%20high-quality%20NeRFs%20generally%0Arequire%20several%20images%20captured%20simultaneously%2C%20a%20rare%20occurrence%20in%20satellite%0Aimaging.%20To%20address%20these%20limitations%2C%20we%20propose%20BRDF-NeRF%2C%20developed%20to%0Aexplicitly%20estimate%20the%20Rahman-Pinty-Verstraete%20%28RPV%29%20model%2C%20a%20semi-empirical%0ABRDF%20model%20commonly%20employed%20in%20remote%20sensing.%20We%20assess%20our%20approach%20using%0Atwo%20datasets%3A%20%281%29%20Djibouti%2C%20captured%20in%20a%20single%20epoch%20at%20varying%20viewing%0Aangles%20with%20a%20fixed%20Sun%20position%2C%20and%20%282%29%20Lanzhou%2C%20captured%20over%20multiple%0Aepochs%20with%20different%20viewing%20angles%20and%20Sun%20positions.%20Our%20results%2C%20based%20on%0Aonly%20three%20to%20four%20satellite%20images%20for%20training%2C%20demonstrate%20that%20BRDF-NeRF%0Acan%20effectively%20synthesize%20novel%20views%20from%20directions%20far%20removed%20from%20the%0Atraining%20data%20and%20produce%20high-quality%20digital%20surface%20models%20%28DSMs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12014v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRDF-NeRF%253A%2520Neural%2520Radiance%2520Fields%2520with%2520Optical%2520Satellite%2520Images%2520and%2520BRDF%250A%2520%2520Modelling%26entry.906535625%3DLulin%2520Zhang%2520and%2520Ewelina%2520Rupnik%2520and%2520Tri%2520Dung%2520Nguyen%2520and%2520St%25C3%25A9phane%2520Jacquemoud%2520and%2520Yann%2520Klinger%26entry.1292438233%3D%2520%2520Understanding%2520the%2520anisotropic%2520reflectance%2520of%2520complex%2520Earth%2520surfaces%2520from%250Asatellite%2520imagery%2520is%2520crucial%2520for%2520numerous%2520applications.%2520Neural%2520radiance%2520fields%250A%2528NeRF%2529%2520have%2520become%2520popular%2520as%2520a%2520machine%2520learning%2520technique%2520capable%2520of%2520deducing%250Athe%2520bidirectional%2520reflectance%2520distribution%2520function%2520%2528BRDF%2529%2520of%2520a%2520scene%2520from%250Amultiple%2520images.%2520However%252C%2520prior%2520research%2520has%2520largely%2520concentrated%2520on%2520applying%250ANeRF%2520to%2520close-range%2520imagery%252C%2520estimating%2520basic%2520Microfacet%2520BRDF%2520models%252C%2520which%250Afall%2520short%2520for%2520many%2520Earth%2520surfaces.%2520Moreover%252C%2520high-quality%2520NeRFs%2520generally%250Arequire%2520several%2520images%2520captured%2520simultaneously%252C%2520a%2520rare%2520occurrence%2520in%2520satellite%250Aimaging.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520BRDF-NeRF%252C%2520developed%2520to%250Aexplicitly%2520estimate%2520the%2520Rahman-Pinty-Verstraete%2520%2528RPV%2529%2520model%252C%2520a%2520semi-empirical%250ABRDF%2520model%2520commonly%2520employed%2520in%2520remote%2520sensing.%2520We%2520assess%2520our%2520approach%2520using%250Atwo%2520datasets%253A%2520%25281%2529%2520Djibouti%252C%2520captured%2520in%2520a%2520single%2520epoch%2520at%2520varying%2520viewing%250Aangles%2520with%2520a%2520fixed%2520Sun%2520position%252C%2520and%2520%25282%2529%2520Lanzhou%252C%2520captured%2520over%2520multiple%250Aepochs%2520with%2520different%2520viewing%2520angles%2520and%2520Sun%2520positions.%2520Our%2520results%252C%2520based%2520on%250Aonly%2520three%2520to%2520four%2520satellite%2520images%2520for%2520training%252C%2520demonstrate%2520that%2520BRDF-NeRF%250Acan%2520effectively%2520synthesize%2520novel%2520views%2520from%2520directions%2520far%2520removed%2520from%2520the%250Atraining%2520data%2520and%2520produce%2520high-quality%2520digital%2520surface%2520models%2520%2528DSMs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12014v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRDF-NeRF%3A%20Neural%20Radiance%20Fields%20with%20Optical%20Satellite%20Images%20and%20BRDF%0A%20%20Modelling&entry.906535625=Lulin%20Zhang%20and%20Ewelina%20Rupnik%20and%20Tri%20Dung%20Nguyen%20and%20St%C3%A9phane%20Jacquemoud%20and%20Yann%20Klinger&entry.1292438233=%20%20Understanding%20the%20anisotropic%20reflectance%20of%20complex%20Earth%20surfaces%20from%0Asatellite%20imagery%20is%20crucial%20for%20numerous%20applications.%20Neural%20radiance%20fields%0A%28NeRF%29%20have%20become%20popular%20as%20a%20machine%20learning%20technique%20capable%20of%20deducing%0Athe%20bidirectional%20reflectance%20distribution%20function%20%28BRDF%29%20of%20a%20scene%20from%0Amultiple%20images.%20However%2C%20prior%20research%20has%20largely%20concentrated%20on%20applying%0ANeRF%20to%20close-range%20imagery%2C%20estimating%20basic%20Microfacet%20BRDF%20models%2C%20which%0Afall%20short%20for%20many%20Earth%20surfaces.%20Moreover%2C%20high-quality%20NeRFs%20generally%0Arequire%20several%20images%20captured%20simultaneously%2C%20a%20rare%20occurrence%20in%20satellite%0Aimaging.%20To%20address%20these%20limitations%2C%20we%20propose%20BRDF-NeRF%2C%20developed%20to%0Aexplicitly%20estimate%20the%20Rahman-Pinty-Verstraete%20%28RPV%29%20model%2C%20a%20semi-empirical%0ABRDF%20model%20commonly%20employed%20in%20remote%20sensing.%20We%20assess%20our%20approach%20using%0Atwo%20datasets%3A%20%281%29%20Djibouti%2C%20captured%20in%20a%20single%20epoch%20at%20varying%20viewing%0Aangles%20with%20a%20fixed%20Sun%20position%2C%20and%20%282%29%20Lanzhou%2C%20captured%20over%20multiple%0Aepochs%20with%20different%20viewing%20angles%20and%20Sun%20positions.%20Our%20results%2C%20based%20on%0Aonly%20three%20to%20four%20satellite%20images%20for%20training%2C%20demonstrate%20that%20BRDF-NeRF%0Acan%20effectively%20synthesize%20novel%20views%20from%20directions%20far%20removed%20from%20the%0Atraining%20data%20and%20produce%20high-quality%20digital%20surface%20models%20%28DSMs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12014v1&entry.124074799=Read"},
{"title": "A machine learning framework for acoustic reflector mapping", "author": "Usama Saqib and Letizia Marchegiani and Jesper Rindom Jensen", "abstract": "  Sonar-based indoor mapping systems have been widely employed in robotics for\nseveral decades. While such systems are still the mainstream in underwater and\npipe inspection settings, the vulnerability to noise reduced, over time, their\ngeneral widespread usage in favour of other modalities(\\textit{e.g.}, cameras,\nlidars), whose technologies were encountering, instead, extraordinary\nadvancements. Nevertheless, mapping physical environments using acoustic\nsignals and echolocation can bring significant benefits to robot navigation in\nadverse scenarios, thanks to their complementary characteristics compared to\nother sensors. Cameras and lidars, indeed, struggle in harsh weather\nconditions, when dealing with lack of illumination, or with non-reflective\nwalls. Yet, for acoustic sensors to be able to generate accurate maps, noise\nhas to be properly and effectively handled. Traditional signal processing\ntechniques are not always a solution in those cases. In this paper, we propose\na framework where machine learning is exploited to aid more traditional signal\nprocessing methods to cope with background noise, by removing outliers and\nartefacts from the generated maps using acoustic sensors. Our goal is to\ndemonstrate that the performance of traditional echolocation mapping techniques\ncan be greatly enhanced, even in particularly noisy conditions, facilitating\nthe employment of acoustic sensors in state-of-the-art multi-modal robot\nnavigation systems. Our simulated evaluation demonstrates that the system can\nreliably operate at an SNR of $-10$dB. Moreover, we also show that the proposed\nmethod is capable of operating in different reverberate environments. In this\npaper, we also use the proposed method to map the outline of a simulated room\nusing a robotic platform.\n", "link": "http://arxiv.org/abs/2409.12094v1", "date": "2024-09-18", "relevancy": 2.1357, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5635}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5587}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20machine%20learning%20framework%20for%20acoustic%20reflector%20mapping&body=Title%3A%20A%20machine%20learning%20framework%20for%20acoustic%20reflector%20mapping%0AAuthor%3A%20Usama%20Saqib%20and%20Letizia%20Marchegiani%20and%20Jesper%20Rindom%20Jensen%0AAbstract%3A%20%20%20Sonar-based%20indoor%20mapping%20systems%20have%20been%20widely%20employed%20in%20robotics%20for%0Aseveral%20decades.%20While%20such%20systems%20are%20still%20the%20mainstream%20in%20underwater%20and%0Apipe%20inspection%20settings%2C%20the%20vulnerability%20to%20noise%20reduced%2C%20over%20time%2C%20their%0Ageneral%20widespread%20usage%20in%20favour%20of%20other%20modalities%28%5Ctextit%7Be.g.%7D%2C%20cameras%2C%0Alidars%29%2C%20whose%20technologies%20were%20encountering%2C%20instead%2C%20extraordinary%0Aadvancements.%20Nevertheless%2C%20mapping%20physical%20environments%20using%20acoustic%0Asignals%20and%20echolocation%20can%20bring%20significant%20benefits%20to%20robot%20navigation%20in%0Aadverse%20scenarios%2C%20thanks%20to%20their%20complementary%20characteristics%20compared%20to%0Aother%20sensors.%20Cameras%20and%20lidars%2C%20indeed%2C%20struggle%20in%20harsh%20weather%0Aconditions%2C%20when%20dealing%20with%20lack%20of%20illumination%2C%20or%20with%20non-reflective%0Awalls.%20Yet%2C%20for%20acoustic%20sensors%20to%20be%20able%20to%20generate%20accurate%20maps%2C%20noise%0Ahas%20to%20be%20properly%20and%20effectively%20handled.%20Traditional%20signal%20processing%0Atechniques%20are%20not%20always%20a%20solution%20in%20those%20cases.%20In%20this%20paper%2C%20we%20propose%0Aa%20framework%20where%20machine%20learning%20is%20exploited%20to%20aid%20more%20traditional%20signal%0Aprocessing%20methods%20to%20cope%20with%20background%20noise%2C%20by%20removing%20outliers%20and%0Aartefacts%20from%20the%20generated%20maps%20using%20acoustic%20sensors.%20Our%20goal%20is%20to%0Ademonstrate%20that%20the%20performance%20of%20traditional%20echolocation%20mapping%20techniques%0Acan%20be%20greatly%20enhanced%2C%20even%20in%20particularly%20noisy%20conditions%2C%20facilitating%0Athe%20employment%20of%20acoustic%20sensors%20in%20state-of-the-art%20multi-modal%20robot%0Anavigation%20systems.%20Our%20simulated%20evaluation%20demonstrates%20that%20the%20system%20can%0Areliably%20operate%20at%20an%20SNR%20of%20%24-10%24dB.%20Moreover%2C%20we%20also%20show%20that%20the%20proposed%0Amethod%20is%20capable%20of%20operating%20in%20different%20reverberate%20environments.%20In%20this%0Apaper%2C%20we%20also%20use%20the%20proposed%20method%20to%20map%20the%20outline%20of%20a%20simulated%20room%0Ausing%20a%20robotic%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520machine%2520learning%2520framework%2520for%2520acoustic%2520reflector%2520mapping%26entry.906535625%3DUsama%2520Saqib%2520and%2520Letizia%2520Marchegiani%2520and%2520Jesper%2520Rindom%2520Jensen%26entry.1292438233%3D%2520%2520Sonar-based%2520indoor%2520mapping%2520systems%2520have%2520been%2520widely%2520employed%2520in%2520robotics%2520for%250Aseveral%2520decades.%2520While%2520such%2520systems%2520are%2520still%2520the%2520mainstream%2520in%2520underwater%2520and%250Apipe%2520inspection%2520settings%252C%2520the%2520vulnerability%2520to%2520noise%2520reduced%252C%2520over%2520time%252C%2520their%250Ageneral%2520widespread%2520usage%2520in%2520favour%2520of%2520other%2520modalities%2528%255Ctextit%257Be.g.%257D%252C%2520cameras%252C%250Alidars%2529%252C%2520whose%2520technologies%2520were%2520encountering%252C%2520instead%252C%2520extraordinary%250Aadvancements.%2520Nevertheless%252C%2520mapping%2520physical%2520environments%2520using%2520acoustic%250Asignals%2520and%2520echolocation%2520can%2520bring%2520significant%2520benefits%2520to%2520robot%2520navigation%2520in%250Aadverse%2520scenarios%252C%2520thanks%2520to%2520their%2520complementary%2520characteristics%2520compared%2520to%250Aother%2520sensors.%2520Cameras%2520and%2520lidars%252C%2520indeed%252C%2520struggle%2520in%2520harsh%2520weather%250Aconditions%252C%2520when%2520dealing%2520with%2520lack%2520of%2520illumination%252C%2520or%2520with%2520non-reflective%250Awalls.%2520Yet%252C%2520for%2520acoustic%2520sensors%2520to%2520be%2520able%2520to%2520generate%2520accurate%2520maps%252C%2520noise%250Ahas%2520to%2520be%2520properly%2520and%2520effectively%2520handled.%2520Traditional%2520signal%2520processing%250Atechniques%2520are%2520not%2520always%2520a%2520solution%2520in%2520those%2520cases.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520framework%2520where%2520machine%2520learning%2520is%2520exploited%2520to%2520aid%2520more%2520traditional%2520signal%250Aprocessing%2520methods%2520to%2520cope%2520with%2520background%2520noise%252C%2520by%2520removing%2520outliers%2520and%250Aartefacts%2520from%2520the%2520generated%2520maps%2520using%2520acoustic%2520sensors.%2520Our%2520goal%2520is%2520to%250Ademonstrate%2520that%2520the%2520performance%2520of%2520traditional%2520echolocation%2520mapping%2520techniques%250Acan%2520be%2520greatly%2520enhanced%252C%2520even%2520in%2520particularly%2520noisy%2520conditions%252C%2520facilitating%250Athe%2520employment%2520of%2520acoustic%2520sensors%2520in%2520state-of-the-art%2520multi-modal%2520robot%250Anavigation%2520systems.%2520Our%2520simulated%2520evaluation%2520demonstrates%2520that%2520the%2520system%2520can%250Areliably%2520operate%2520at%2520an%2520SNR%2520of%2520%2524-10%2524dB.%2520Moreover%252C%2520we%2520also%2520show%2520that%2520the%2520proposed%250Amethod%2520is%2520capable%2520of%2520operating%2520in%2520different%2520reverberate%2520environments.%2520In%2520this%250Apaper%252C%2520we%2520also%2520use%2520the%2520proposed%2520method%2520to%2520map%2520the%2520outline%2520of%2520a%2520simulated%2520room%250Ausing%2520a%2520robotic%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20machine%20learning%20framework%20for%20acoustic%20reflector%20mapping&entry.906535625=Usama%20Saqib%20and%20Letizia%20Marchegiani%20and%20Jesper%20Rindom%20Jensen&entry.1292438233=%20%20Sonar-based%20indoor%20mapping%20systems%20have%20been%20widely%20employed%20in%20robotics%20for%0Aseveral%20decades.%20While%20such%20systems%20are%20still%20the%20mainstream%20in%20underwater%20and%0Apipe%20inspection%20settings%2C%20the%20vulnerability%20to%20noise%20reduced%2C%20over%20time%2C%20their%0Ageneral%20widespread%20usage%20in%20favour%20of%20other%20modalities%28%5Ctextit%7Be.g.%7D%2C%20cameras%2C%0Alidars%29%2C%20whose%20technologies%20were%20encountering%2C%20instead%2C%20extraordinary%0Aadvancements.%20Nevertheless%2C%20mapping%20physical%20environments%20using%20acoustic%0Asignals%20and%20echolocation%20can%20bring%20significant%20benefits%20to%20robot%20navigation%20in%0Aadverse%20scenarios%2C%20thanks%20to%20their%20complementary%20characteristics%20compared%20to%0Aother%20sensors.%20Cameras%20and%20lidars%2C%20indeed%2C%20struggle%20in%20harsh%20weather%0Aconditions%2C%20when%20dealing%20with%20lack%20of%20illumination%2C%20or%20with%20non-reflective%0Awalls.%20Yet%2C%20for%20acoustic%20sensors%20to%20be%20able%20to%20generate%20accurate%20maps%2C%20noise%0Ahas%20to%20be%20properly%20and%20effectively%20handled.%20Traditional%20signal%20processing%0Atechniques%20are%20not%20always%20a%20solution%20in%20those%20cases.%20In%20this%20paper%2C%20we%20propose%0Aa%20framework%20where%20machine%20learning%20is%20exploited%20to%20aid%20more%20traditional%20signal%0Aprocessing%20methods%20to%20cope%20with%20background%20noise%2C%20by%20removing%20outliers%20and%0Aartefacts%20from%20the%20generated%20maps%20using%20acoustic%20sensors.%20Our%20goal%20is%20to%0Ademonstrate%20that%20the%20performance%20of%20traditional%20echolocation%20mapping%20techniques%0Acan%20be%20greatly%20enhanced%2C%20even%20in%20particularly%20noisy%20conditions%2C%20facilitating%0Athe%20employment%20of%20acoustic%20sensors%20in%20state-of-the-art%20multi-modal%20robot%0Anavigation%20systems.%20Our%20simulated%20evaluation%20demonstrates%20that%20the%20system%20can%0Areliably%20operate%20at%20an%20SNR%20of%20%24-10%24dB.%20Moreover%2C%20we%20also%20show%20that%20the%20proposed%0Amethod%20is%20capable%20of%20operating%20in%20different%20reverberate%20environments.%20In%20this%0Apaper%2C%20we%20also%20use%20the%20proposed%20method%20to%20map%20the%20outline%20of%20a%20simulated%20room%0Ausing%20a%20robotic%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12094v1&entry.124074799=Read"},
{"title": "Autopet III challenge: Incorporating anatomical knowledge into nnUNet\n  for lesion segmentation in PET/CT", "author": "Hamza Kalisch and Fabian H\u00f6rst and Ken Herrmann and Jens Kleesiek and Constantin Seibold", "abstract": "  Lesion segmentation in PET/CT imaging is essential for precise tumor\ncharacterization, which supports personalized treatment planning and enhances\ndiagnostic precision in oncology. However, accurate manual segmentation of\nlesions is time-consuming and prone to inter-observer variability. Given the\nrising demand and clinical use of PET/CT, automated segmentation methods,\nparticularly deep-learning-based approaches, have become increasingly more\nrelevant. The autoPET III Challenge focuses on advancing automated segmentation\nof tumor lesions in PET/CT images in a multitracer multicenter setting,\naddressing the clinical need for quantitative, robust, and generalizable\nsolutions. Building on previous challenges, the third iteration of the autoPET\nchallenge introduces a more diverse dataset featuring two different tracers\n(FDG and PSMA) from two clinical centers. To this extent, we developed a\nclassifier that identifies the tracer of the given PET/CT based on the Maximum\nIntensity Projection of the PET scan. We trained two individual\nnnUNet-ensembles for each tracer where anatomical labels are included as a\nmulti-label task to enhance the model's performance. Our final submission\nachieves cross-validation Dice scores of 76.90% and 61.33% for the publicly\navailable FDG and PSMA datasets, respectively. The code is available at\nhttps://github.com/hakal104/autoPETIII/ .\n", "link": "http://arxiv.org/abs/2409.12155v1", "date": "2024-09-18", "relevancy": 2.1282, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autopet%20III%20challenge%3A%20Incorporating%20anatomical%20knowledge%20into%20nnUNet%0A%20%20for%20lesion%20segmentation%20in%20PET/CT&body=Title%3A%20Autopet%20III%20challenge%3A%20Incorporating%20anatomical%20knowledge%20into%20nnUNet%0A%20%20for%20lesion%20segmentation%20in%20PET/CT%0AAuthor%3A%20Hamza%20Kalisch%20and%20Fabian%20H%C3%B6rst%20and%20Ken%20Herrmann%20and%20Jens%20Kleesiek%20and%20Constantin%20Seibold%0AAbstract%3A%20%20%20Lesion%20segmentation%20in%20PET/CT%20imaging%20is%20essential%20for%20precise%20tumor%0Acharacterization%2C%20which%20supports%20personalized%20treatment%20planning%20and%20enhances%0Adiagnostic%20precision%20in%20oncology.%20However%2C%20accurate%20manual%20segmentation%20of%0Alesions%20is%20time-consuming%20and%20prone%20to%20inter-observer%20variability.%20Given%20the%0Arising%20demand%20and%20clinical%20use%20of%20PET/CT%2C%20automated%20segmentation%20methods%2C%0Aparticularly%20deep-learning-based%20approaches%2C%20have%20become%20increasingly%20more%0Arelevant.%20The%20autoPET%20III%20Challenge%20focuses%20on%20advancing%20automated%20segmentation%0Aof%20tumor%20lesions%20in%20PET/CT%20images%20in%20a%20multitracer%20multicenter%20setting%2C%0Aaddressing%20the%20clinical%20need%20for%20quantitative%2C%20robust%2C%20and%20generalizable%0Asolutions.%20Building%20on%20previous%20challenges%2C%20the%20third%20iteration%20of%20the%20autoPET%0Achallenge%20introduces%20a%20more%20diverse%20dataset%20featuring%20two%20different%20tracers%0A%28FDG%20and%20PSMA%29%20from%20two%20clinical%20centers.%20To%20this%20extent%2C%20we%20developed%20a%0Aclassifier%20that%20identifies%20the%20tracer%20of%20the%20given%20PET/CT%20based%20on%20the%20Maximum%0AIntensity%20Projection%20of%20the%20PET%20scan.%20We%20trained%20two%20individual%0AnnUNet-ensembles%20for%20each%20tracer%20where%20anatomical%20labels%20are%20included%20as%20a%0Amulti-label%20task%20to%20enhance%20the%20model%27s%20performance.%20Our%20final%20submission%0Aachieves%20cross-validation%20Dice%20scores%20of%2076.90%25%20and%2061.33%25%20for%20the%20publicly%0Aavailable%20FDG%20and%20PSMA%20datasets%2C%20respectively.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/hakal104/autoPETIII/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutopet%2520III%2520challenge%253A%2520Incorporating%2520anatomical%2520knowledge%2520into%2520nnUNet%250A%2520%2520for%2520lesion%2520segmentation%2520in%2520PET/CT%26entry.906535625%3DHamza%2520Kalisch%2520and%2520Fabian%2520H%25C3%25B6rst%2520and%2520Ken%2520Herrmann%2520and%2520Jens%2520Kleesiek%2520and%2520Constantin%2520Seibold%26entry.1292438233%3D%2520%2520Lesion%2520segmentation%2520in%2520PET/CT%2520imaging%2520is%2520essential%2520for%2520precise%2520tumor%250Acharacterization%252C%2520which%2520supports%2520personalized%2520treatment%2520planning%2520and%2520enhances%250Adiagnostic%2520precision%2520in%2520oncology.%2520However%252C%2520accurate%2520manual%2520segmentation%2520of%250Alesions%2520is%2520time-consuming%2520and%2520prone%2520to%2520inter-observer%2520variability.%2520Given%2520the%250Arising%2520demand%2520and%2520clinical%2520use%2520of%2520PET/CT%252C%2520automated%2520segmentation%2520methods%252C%250Aparticularly%2520deep-learning-based%2520approaches%252C%2520have%2520become%2520increasingly%2520more%250Arelevant.%2520The%2520autoPET%2520III%2520Challenge%2520focuses%2520on%2520advancing%2520automated%2520segmentation%250Aof%2520tumor%2520lesions%2520in%2520PET/CT%2520images%2520in%2520a%2520multitracer%2520multicenter%2520setting%252C%250Aaddressing%2520the%2520clinical%2520need%2520for%2520quantitative%252C%2520robust%252C%2520and%2520generalizable%250Asolutions.%2520Building%2520on%2520previous%2520challenges%252C%2520the%2520third%2520iteration%2520of%2520the%2520autoPET%250Achallenge%2520introduces%2520a%2520more%2520diverse%2520dataset%2520featuring%2520two%2520different%2520tracers%250A%2528FDG%2520and%2520PSMA%2529%2520from%2520two%2520clinical%2520centers.%2520To%2520this%2520extent%252C%2520we%2520developed%2520a%250Aclassifier%2520that%2520identifies%2520the%2520tracer%2520of%2520the%2520given%2520PET/CT%2520based%2520on%2520the%2520Maximum%250AIntensity%2520Projection%2520of%2520the%2520PET%2520scan.%2520We%2520trained%2520two%2520individual%250AnnUNet-ensembles%2520for%2520each%2520tracer%2520where%2520anatomical%2520labels%2520are%2520included%2520as%2520a%250Amulti-label%2520task%2520to%2520enhance%2520the%2520model%2527s%2520performance.%2520Our%2520final%2520submission%250Aachieves%2520cross-validation%2520Dice%2520scores%2520of%252076.90%2525%2520and%252061.33%2525%2520for%2520the%2520publicly%250Aavailable%2520FDG%2520and%2520PSMA%2520datasets%252C%2520respectively.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/hakal104/autoPETIII/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autopet%20III%20challenge%3A%20Incorporating%20anatomical%20knowledge%20into%20nnUNet%0A%20%20for%20lesion%20segmentation%20in%20PET/CT&entry.906535625=Hamza%20Kalisch%20and%20Fabian%20H%C3%B6rst%20and%20Ken%20Herrmann%20and%20Jens%20Kleesiek%20and%20Constantin%20Seibold&entry.1292438233=%20%20Lesion%20segmentation%20in%20PET/CT%20imaging%20is%20essential%20for%20precise%20tumor%0Acharacterization%2C%20which%20supports%20personalized%20treatment%20planning%20and%20enhances%0Adiagnostic%20precision%20in%20oncology.%20However%2C%20accurate%20manual%20segmentation%20of%0Alesions%20is%20time-consuming%20and%20prone%20to%20inter-observer%20variability.%20Given%20the%0Arising%20demand%20and%20clinical%20use%20of%20PET/CT%2C%20automated%20segmentation%20methods%2C%0Aparticularly%20deep-learning-based%20approaches%2C%20have%20become%20increasingly%20more%0Arelevant.%20The%20autoPET%20III%20Challenge%20focuses%20on%20advancing%20automated%20segmentation%0Aof%20tumor%20lesions%20in%20PET/CT%20images%20in%20a%20multitracer%20multicenter%20setting%2C%0Aaddressing%20the%20clinical%20need%20for%20quantitative%2C%20robust%2C%20and%20generalizable%0Asolutions.%20Building%20on%20previous%20challenges%2C%20the%20third%20iteration%20of%20the%20autoPET%0Achallenge%20introduces%20a%20more%20diverse%20dataset%20featuring%20two%20different%20tracers%0A%28FDG%20and%20PSMA%29%20from%20two%20clinical%20centers.%20To%20this%20extent%2C%20we%20developed%20a%0Aclassifier%20that%20identifies%20the%20tracer%20of%20the%20given%20PET/CT%20based%20on%20the%20Maximum%0AIntensity%20Projection%20of%20the%20PET%20scan.%20We%20trained%20two%20individual%0AnnUNet-ensembles%20for%20each%20tracer%20where%20anatomical%20labels%20are%20included%20as%20a%0Amulti-label%20task%20to%20enhance%20the%20model%27s%20performance.%20Our%20final%20submission%0Aachieves%20cross-validation%20Dice%20scores%20of%2076.90%25%20and%2061.33%25%20for%20the%20publicly%0Aavailable%20FDG%20and%20PSMA%20datasets%2C%20respectively.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/hakal104/autoPETIII/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12155v1&entry.124074799=Read"},
{"title": "FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated\n  Long-Tailed Learning", "author": "Xiuhua Lu and Peng Li and Xuefeng Jiang", "abstract": "  Federated learning offers a paradigm to the challenge of preserving privacy\nin distributed machine learning. However, datasets distributed across each\nclient in the real world are inevitably heterogeneous, and if the datasets can\nbe globally aggregated, they tend to be long-tailed distributed, which greatly\naffects the performance of the model. The traditional approach to federated\nlearning primarily addresses the heterogeneity of data among clients, yet it\nfails to address the phenomenon of class-wise bias in global long-tailed data.\nThis results in the trained model focusing on the head classes while neglecting\nthe equally important tail classes. Consequently, it is essential to develop a\nmethodology that considers classes holistically. To address the above problems,\nwe propose a new method FedLF, which introduces three modifications in the\nlocal training phase: adaptive logit adjustment, continuous class centred\noptimization, and feature decorrelation. We compare seven state-of-the-art\nmethods with varying degrees of data heterogeneity and long-tailed\ndistribution. Extensive experiments on benchmark datasets CIFAR-10-LT and\nCIFAR-100-LT demonstrate that our approach effectively mitigates the problem of\nmodel performance degradation due to data heterogeneity and long-tailed\ndistribution. our code is available at https://github.com/18sym/FedLF.\n", "link": "http://arxiv.org/abs/2409.12105v1", "date": "2024-09-18", "relevancy": 2.1259, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5927}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedLF%3A%20Adaptive%20Logit%20Adjustment%20and%20Feature%20Optimization%20in%20Federated%0A%20%20Long-Tailed%20Learning&body=Title%3A%20FedLF%3A%20Adaptive%20Logit%20Adjustment%20and%20Feature%20Optimization%20in%20Federated%0A%20%20Long-Tailed%20Learning%0AAuthor%3A%20Xiuhua%20Lu%20and%20Peng%20Li%20and%20Xuefeng%20Jiang%0AAbstract%3A%20%20%20Federated%20learning%20offers%20a%20paradigm%20to%20the%20challenge%20of%20preserving%20privacy%0Ain%20distributed%20machine%20learning.%20However%2C%20datasets%20distributed%20across%20each%0Aclient%20in%20the%20real%20world%20are%20inevitably%20heterogeneous%2C%20and%20if%20the%20datasets%20can%0Abe%20globally%20aggregated%2C%20they%20tend%20to%20be%20long-tailed%20distributed%2C%20which%20greatly%0Aaffects%20the%20performance%20of%20the%20model.%20The%20traditional%20approach%20to%20federated%0Alearning%20primarily%20addresses%20the%20heterogeneity%20of%20data%20among%20clients%2C%20yet%20it%0Afails%20to%20address%20the%20phenomenon%20of%20class-wise%20bias%20in%20global%20long-tailed%20data.%0AThis%20results%20in%20the%20trained%20model%20focusing%20on%20the%20head%20classes%20while%20neglecting%0Athe%20equally%20important%20tail%20classes.%20Consequently%2C%20it%20is%20essential%20to%20develop%20a%0Amethodology%20that%20considers%20classes%20holistically.%20To%20address%20the%20above%20problems%2C%0Awe%20propose%20a%20new%20method%20FedLF%2C%20which%20introduces%20three%20modifications%20in%20the%0Alocal%20training%20phase%3A%20adaptive%20logit%20adjustment%2C%20continuous%20class%20centred%0Aoptimization%2C%20and%20feature%20decorrelation.%20We%20compare%20seven%20state-of-the-art%0Amethods%20with%20varying%20degrees%20of%20data%20heterogeneity%20and%20long-tailed%0Adistribution.%20Extensive%20experiments%20on%20benchmark%20datasets%20CIFAR-10-LT%20and%0ACIFAR-100-LT%20demonstrate%20that%20our%20approach%20effectively%20mitigates%20the%20problem%20of%0Amodel%20performance%20degradation%20due%20to%20data%20heterogeneity%20and%20long-tailed%0Adistribution.%20our%20code%20is%20available%20at%20https%3A//github.com/18sym/FedLF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedLF%253A%2520Adaptive%2520Logit%2520Adjustment%2520and%2520Feature%2520Optimization%2520in%2520Federated%250A%2520%2520Long-Tailed%2520Learning%26entry.906535625%3DXiuhua%2520Lu%2520and%2520Peng%2520Li%2520and%2520Xuefeng%2520Jiang%26entry.1292438233%3D%2520%2520Federated%2520learning%2520offers%2520a%2520paradigm%2520to%2520the%2520challenge%2520of%2520preserving%2520privacy%250Ain%2520distributed%2520machine%2520learning.%2520However%252C%2520datasets%2520distributed%2520across%2520each%250Aclient%2520in%2520the%2520real%2520world%2520are%2520inevitably%2520heterogeneous%252C%2520and%2520if%2520the%2520datasets%2520can%250Abe%2520globally%2520aggregated%252C%2520they%2520tend%2520to%2520be%2520long-tailed%2520distributed%252C%2520which%2520greatly%250Aaffects%2520the%2520performance%2520of%2520the%2520model.%2520The%2520traditional%2520approach%2520to%2520federated%250Alearning%2520primarily%2520addresses%2520the%2520heterogeneity%2520of%2520data%2520among%2520clients%252C%2520yet%2520it%250Afails%2520to%2520address%2520the%2520phenomenon%2520of%2520class-wise%2520bias%2520in%2520global%2520long-tailed%2520data.%250AThis%2520results%2520in%2520the%2520trained%2520model%2520focusing%2520on%2520the%2520head%2520classes%2520while%2520neglecting%250Athe%2520equally%2520important%2520tail%2520classes.%2520Consequently%252C%2520it%2520is%2520essential%2520to%2520develop%2520a%250Amethodology%2520that%2520considers%2520classes%2520holistically.%2520To%2520address%2520the%2520above%2520problems%252C%250Awe%2520propose%2520a%2520new%2520method%2520FedLF%252C%2520which%2520introduces%2520three%2520modifications%2520in%2520the%250Alocal%2520training%2520phase%253A%2520adaptive%2520logit%2520adjustment%252C%2520continuous%2520class%2520centred%250Aoptimization%252C%2520and%2520feature%2520decorrelation.%2520We%2520compare%2520seven%2520state-of-the-art%250Amethods%2520with%2520varying%2520degrees%2520of%2520data%2520heterogeneity%2520and%2520long-tailed%250Adistribution.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520CIFAR-10-LT%2520and%250ACIFAR-100-LT%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520mitigates%2520the%2520problem%2520of%250Amodel%2520performance%2520degradation%2520due%2520to%2520data%2520heterogeneity%2520and%2520long-tailed%250Adistribution.%2520our%2520code%2520is%2520available%2520at%2520https%253A//github.com/18sym/FedLF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedLF%3A%20Adaptive%20Logit%20Adjustment%20and%20Feature%20Optimization%20in%20Federated%0A%20%20Long-Tailed%20Learning&entry.906535625=Xiuhua%20Lu%20and%20Peng%20Li%20and%20Xuefeng%20Jiang&entry.1292438233=%20%20Federated%20learning%20offers%20a%20paradigm%20to%20the%20challenge%20of%20preserving%20privacy%0Ain%20distributed%20machine%20learning.%20However%2C%20datasets%20distributed%20across%20each%0Aclient%20in%20the%20real%20world%20are%20inevitably%20heterogeneous%2C%20and%20if%20the%20datasets%20can%0Abe%20globally%20aggregated%2C%20they%20tend%20to%20be%20long-tailed%20distributed%2C%20which%20greatly%0Aaffects%20the%20performance%20of%20the%20model.%20The%20traditional%20approach%20to%20federated%0Alearning%20primarily%20addresses%20the%20heterogeneity%20of%20data%20among%20clients%2C%20yet%20it%0Afails%20to%20address%20the%20phenomenon%20of%20class-wise%20bias%20in%20global%20long-tailed%20data.%0AThis%20results%20in%20the%20trained%20model%20focusing%20on%20the%20head%20classes%20while%20neglecting%0Athe%20equally%20important%20tail%20classes.%20Consequently%2C%20it%20is%20essential%20to%20develop%20a%0Amethodology%20that%20considers%20classes%20holistically.%20To%20address%20the%20above%20problems%2C%0Awe%20propose%20a%20new%20method%20FedLF%2C%20which%20introduces%20three%20modifications%20in%20the%0Alocal%20training%20phase%3A%20adaptive%20logit%20adjustment%2C%20continuous%20class%20centred%0Aoptimization%2C%20and%20feature%20decorrelation.%20We%20compare%20seven%20state-of-the-art%0Amethods%20with%20varying%20degrees%20of%20data%20heterogeneity%20and%20long-tailed%0Adistribution.%20Extensive%20experiments%20on%20benchmark%20datasets%20CIFAR-10-LT%20and%0ACIFAR-100-LT%20demonstrate%20that%20our%20approach%20effectively%20mitigates%20the%20problem%20of%0Amodel%20performance%20degradation%20due%20to%20data%20heterogeneity%20and%20long-tailed%0Adistribution.%20our%20code%20is%20available%20at%20https%3A//github.com/18sym/FedLF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12105v1&entry.124074799=Read"},
{"title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering For Videos", "author": "Devesh Walawalkar and Pablo Garrido", "abstract": "  With the rise of digital media content production, the need for analyzing\nmovies and TV series episodes to locate the main cast of characters precisely\nis gaining importance.Specifically, Video Face Clustering aims to group\ntogether detected video face tracks with common facial identities. This problem\nis very challenging due to the large range of pose, expression, appearance, and\nlighting variations of a given face across video frames. Generic pre-trained\nFace Identification (ID) models fail to adapt well to the video production\ndomain, given its high dynamic range content and also unique cinematic style.\nFurthermore, traditional clustering algorithms depend on hyperparameters\nrequiring individual tuning across datasets. In this paper, we present a novel\nvideo face clustering approach that learns to adapt a generic face ID model to\nnew video face tracks in a fully self-supervised fashion. We also propose a\nparameter-free clustering algorithm that is capable of automatically adapting\nto the finetuned model's embedding space for any input video. Due to the lack\nof comprehensive movie face clustering benchmarks, we also present a\nfirst-of-kind movie dataset: MovieFaceCluster. Our dataset is handpicked by\nfilm industry professionals and contains extremely challenging face ID\nscenarios. Experiments show our method's effectiveness in handling difficult\nmainstream movie scenes on our benchmark dataset and state-of-the-art\nperformance on traditional TV series datasets.\n", "link": "http://arxiv.org/abs/2407.12214v2", "date": "2024-09-18", "relevancy": 2.1258, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5577}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoClusterNet%3A%20Self-Supervised%20and%20Adaptive%20Face%20Clustering%20For%20Videos&body=Title%3A%20VideoClusterNet%3A%20Self-Supervised%20and%20Adaptive%20Face%20Clustering%20For%20Videos%0AAuthor%3A%20Devesh%20Walawalkar%20and%20Pablo%20Garrido%0AAbstract%3A%20%20%20With%20the%20rise%20of%20digital%20media%20content%20production%2C%20the%20need%20for%20analyzing%0Amovies%20and%20TV%20series%20episodes%20to%20locate%20the%20main%20cast%20of%20characters%20precisely%0Ais%20gaining%20importance.Specifically%2C%20Video%20Face%20Clustering%20aims%20to%20group%0Atogether%20detected%20video%20face%20tracks%20with%20common%20facial%20identities.%20This%20problem%0Ais%20very%20challenging%20due%20to%20the%20large%20range%20of%20pose%2C%20expression%2C%20appearance%2C%20and%0Alighting%20variations%20of%20a%20given%20face%20across%20video%20frames.%20Generic%20pre-trained%0AFace%20Identification%20%28ID%29%20models%20fail%20to%20adapt%20well%20to%20the%20video%20production%0Adomain%2C%20given%20its%20high%20dynamic%20range%20content%20and%20also%20unique%20cinematic%20style.%0AFurthermore%2C%20traditional%20clustering%20algorithms%20depend%20on%20hyperparameters%0Arequiring%20individual%20tuning%20across%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Avideo%20face%20clustering%20approach%20that%20learns%20to%20adapt%20a%20generic%20face%20ID%20model%20to%0Anew%20video%20face%20tracks%20in%20a%20fully%20self-supervised%20fashion.%20We%20also%20propose%20a%0Aparameter-free%20clustering%20algorithm%20that%20is%20capable%20of%20automatically%20adapting%0Ato%20the%20finetuned%20model%27s%20embedding%20space%20for%20any%20input%20video.%20Due%20to%20the%20lack%0Aof%20comprehensive%20movie%20face%20clustering%20benchmarks%2C%20we%20also%20present%20a%0Afirst-of-kind%20movie%20dataset%3A%20MovieFaceCluster.%20Our%20dataset%20is%20handpicked%20by%0Afilm%20industry%20professionals%20and%20contains%20extremely%20challenging%20face%20ID%0Ascenarios.%20Experiments%20show%20our%20method%27s%20effectiveness%20in%20handling%20difficult%0Amainstream%20movie%20scenes%20on%20our%20benchmark%20dataset%20and%20state-of-the-art%0Aperformance%20on%20traditional%20TV%20series%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoClusterNet%253A%2520Self-Supervised%2520and%2520Adaptive%2520Face%2520Clustering%2520For%2520Videos%26entry.906535625%3DDevesh%2520Walawalkar%2520and%2520Pablo%2520Garrido%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520digital%2520media%2520content%2520production%252C%2520the%2520need%2520for%2520analyzing%250Amovies%2520and%2520TV%2520series%2520episodes%2520to%2520locate%2520the%2520main%2520cast%2520of%2520characters%2520precisely%250Ais%2520gaining%2520importance.Specifically%252C%2520Video%2520Face%2520Clustering%2520aims%2520to%2520group%250Atogether%2520detected%2520video%2520face%2520tracks%2520with%2520common%2520facial%2520identities.%2520This%2520problem%250Ais%2520very%2520challenging%2520due%2520to%2520the%2520large%2520range%2520of%2520pose%252C%2520expression%252C%2520appearance%252C%2520and%250Alighting%2520variations%2520of%2520a%2520given%2520face%2520across%2520video%2520frames.%2520Generic%2520pre-trained%250AFace%2520Identification%2520%2528ID%2529%2520models%2520fail%2520to%2520adapt%2520well%2520to%2520the%2520video%2520production%250Adomain%252C%2520given%2520its%2520high%2520dynamic%2520range%2520content%2520and%2520also%2520unique%2520cinematic%2520style.%250AFurthermore%252C%2520traditional%2520clustering%2520algorithms%2520depend%2520on%2520hyperparameters%250Arequiring%2520individual%2520tuning%2520across%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%250Avideo%2520face%2520clustering%2520approach%2520that%2520learns%2520to%2520adapt%2520a%2520generic%2520face%2520ID%2520model%2520to%250Anew%2520video%2520face%2520tracks%2520in%2520a%2520fully%2520self-supervised%2520fashion.%2520We%2520also%2520propose%2520a%250Aparameter-free%2520clustering%2520algorithm%2520that%2520is%2520capable%2520of%2520automatically%2520adapting%250Ato%2520the%2520finetuned%2520model%2527s%2520embedding%2520space%2520for%2520any%2520input%2520video.%2520Due%2520to%2520the%2520lack%250Aof%2520comprehensive%2520movie%2520face%2520clustering%2520benchmarks%252C%2520we%2520also%2520present%2520a%250Afirst-of-kind%2520movie%2520dataset%253A%2520MovieFaceCluster.%2520Our%2520dataset%2520is%2520handpicked%2520by%250Afilm%2520industry%2520professionals%2520and%2520contains%2520extremely%2520challenging%2520face%2520ID%250Ascenarios.%2520Experiments%2520show%2520our%2520method%2527s%2520effectiveness%2520in%2520handling%2520difficult%250Amainstream%2520movie%2520scenes%2520on%2520our%2520benchmark%2520dataset%2520and%2520state-of-the-art%250Aperformance%2520on%2520traditional%2520TV%2520series%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoClusterNet%3A%20Self-Supervised%20and%20Adaptive%20Face%20Clustering%20For%20Videos&entry.906535625=Devesh%20Walawalkar%20and%20Pablo%20Garrido&entry.1292438233=%20%20With%20the%20rise%20of%20digital%20media%20content%20production%2C%20the%20need%20for%20analyzing%0Amovies%20and%20TV%20series%20episodes%20to%20locate%20the%20main%20cast%20of%20characters%20precisely%0Ais%20gaining%20importance.Specifically%2C%20Video%20Face%20Clustering%20aims%20to%20group%0Atogether%20detected%20video%20face%20tracks%20with%20common%20facial%20identities.%20This%20problem%0Ais%20very%20challenging%20due%20to%20the%20large%20range%20of%20pose%2C%20expression%2C%20appearance%2C%20and%0Alighting%20variations%20of%20a%20given%20face%20across%20video%20frames.%20Generic%20pre-trained%0AFace%20Identification%20%28ID%29%20models%20fail%20to%20adapt%20well%20to%20the%20video%20production%0Adomain%2C%20given%20its%20high%20dynamic%20range%20content%20and%20also%20unique%20cinematic%20style.%0AFurthermore%2C%20traditional%20clustering%20algorithms%20depend%20on%20hyperparameters%0Arequiring%20individual%20tuning%20across%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Avideo%20face%20clustering%20approach%20that%20learns%20to%20adapt%20a%20generic%20face%20ID%20model%20to%0Anew%20video%20face%20tracks%20in%20a%20fully%20self-supervised%20fashion.%20We%20also%20propose%20a%0Aparameter-free%20clustering%20algorithm%20that%20is%20capable%20of%20automatically%20adapting%0Ato%20the%20finetuned%20model%27s%20embedding%20space%20for%20any%20input%20video.%20Due%20to%20the%20lack%0Aof%20comprehensive%20movie%20face%20clustering%20benchmarks%2C%20we%20also%20present%20a%0Afirst-of-kind%20movie%20dataset%3A%20MovieFaceCluster.%20Our%20dataset%20is%20handpicked%20by%0Afilm%20industry%20professionals%20and%20contains%20extremely%20challenging%20face%20ID%0Ascenarios.%20Experiments%20show%20our%20method%27s%20effectiveness%20in%20handling%20difficult%0Amainstream%20movie%20scenes%20on%20our%20benchmark%20dataset%20and%20state-of-the-art%0Aperformance%20on%20traditional%20TV%20series%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12214v2&entry.124074799=Read"},
{"title": "Secure Control Systems for Autonomous Quadrotors against Cyber-Attacks", "author": "Samuel Belkadi", "abstract": "  The problem of safety for robotic systems has been extensively studied.\nHowever, little attention has been given to security issues for\nthree-dimensional systems, such as quadrotors. Malicious adversaries can\ncompromise robot sensors and communication networks, causing incidents,\nachieving illegal objectives, or even injuring people. This study first designs\nan intelligent control system for autonomous quadrotors. Then, it investigates\nthe problems of optimal false data injection attack scheduling and\ncountermeasure design for unmanned aerial vehicles. Using a state-of-the-art\ndeep learning-based approach, an optimal false data injection attack scheme is\nproposed to deteriorate a quadrotor's tracking performance with limited attack\nenergy. Subsequently, an optimal tracking control strategy is learned to\nmitigate attacks and recover the quadrotor's tracking performance. We base our\nwork on Agilicious, a state-of-the-art quadrotor recently deployed for\nautonomous settings. This paper is the first in the United Kingdom to deploy\nthis quadrotor and implement reinforcement learning on its platform. Therefore,\nto promote easy reproducibility with minimal engineering overhead, we further\nprovide (1) a comprehensive breakdown of this quadrotor, including software\nstacks and hardware alternatives; (2) a detailed reinforcement-learning\nframework to train autonomous controllers on Agilicious agents; and (3) a new\nopen-source environment that builds upon PyFlyt for future reinforcement\nlearning research on Agilicious platforms. Both simulated and real-world\nexperiments are conducted to show the effectiveness of the proposed frameworks\nin section 5.2.\n", "link": "http://arxiv.org/abs/2409.11897v1", "date": "2024-09-18", "relevancy": 2.1214, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5389}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secure%20Control%20Systems%20for%20Autonomous%20Quadrotors%20against%20Cyber-Attacks&body=Title%3A%20Secure%20Control%20Systems%20for%20Autonomous%20Quadrotors%20against%20Cyber-Attacks%0AAuthor%3A%20Samuel%20Belkadi%0AAbstract%3A%20%20%20The%20problem%20of%20safety%20for%20robotic%20systems%20has%20been%20extensively%20studied.%0AHowever%2C%20little%20attention%20has%20been%20given%20to%20security%20issues%20for%0Athree-dimensional%20systems%2C%20such%20as%20quadrotors.%20Malicious%20adversaries%20can%0Acompromise%20robot%20sensors%20and%20communication%20networks%2C%20causing%20incidents%2C%0Aachieving%20illegal%20objectives%2C%20or%20even%20injuring%20people.%20This%20study%20first%20designs%0Aan%20intelligent%20control%20system%20for%20autonomous%20quadrotors.%20Then%2C%20it%20investigates%0Athe%20problems%20of%20optimal%20false%20data%20injection%20attack%20scheduling%20and%0Acountermeasure%20design%20for%20unmanned%20aerial%20vehicles.%20Using%20a%20state-of-the-art%0Adeep%20learning-based%20approach%2C%20an%20optimal%20false%20data%20injection%20attack%20scheme%20is%0Aproposed%20to%20deteriorate%20a%20quadrotor%27s%20tracking%20performance%20with%20limited%20attack%0Aenergy.%20Subsequently%2C%20an%20optimal%20tracking%20control%20strategy%20is%20learned%20to%0Amitigate%20attacks%20and%20recover%20the%20quadrotor%27s%20tracking%20performance.%20We%20base%20our%0Awork%20on%20Agilicious%2C%20a%20state-of-the-art%20quadrotor%20recently%20deployed%20for%0Aautonomous%20settings.%20This%20paper%20is%20the%20first%20in%20the%20United%20Kingdom%20to%20deploy%0Athis%20quadrotor%20and%20implement%20reinforcement%20learning%20on%20its%20platform.%20Therefore%2C%0Ato%20promote%20easy%20reproducibility%20with%20minimal%20engineering%20overhead%2C%20we%20further%0Aprovide%20%281%29%20a%20comprehensive%20breakdown%20of%20this%20quadrotor%2C%20including%20software%0Astacks%20and%20hardware%20alternatives%3B%20%282%29%20a%20detailed%20reinforcement-learning%0Aframework%20to%20train%20autonomous%20controllers%20on%20Agilicious%20agents%3B%20and%20%283%29%20a%20new%0Aopen-source%20environment%20that%20builds%20upon%20PyFlyt%20for%20future%20reinforcement%0Alearning%20research%20on%20Agilicious%20platforms.%20Both%20simulated%20and%20real-world%0Aexperiments%20are%20conducted%20to%20show%20the%20effectiveness%20of%20the%20proposed%20frameworks%0Ain%20section%205.2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecure%2520Control%2520Systems%2520for%2520Autonomous%2520Quadrotors%2520against%2520Cyber-Attacks%26entry.906535625%3DSamuel%2520Belkadi%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520safety%2520for%2520robotic%2520systems%2520has%2520been%2520extensively%2520studied.%250AHowever%252C%2520little%2520attention%2520has%2520been%2520given%2520to%2520security%2520issues%2520for%250Athree-dimensional%2520systems%252C%2520such%2520as%2520quadrotors.%2520Malicious%2520adversaries%2520can%250Acompromise%2520robot%2520sensors%2520and%2520communication%2520networks%252C%2520causing%2520incidents%252C%250Aachieving%2520illegal%2520objectives%252C%2520or%2520even%2520injuring%2520people.%2520This%2520study%2520first%2520designs%250Aan%2520intelligent%2520control%2520system%2520for%2520autonomous%2520quadrotors.%2520Then%252C%2520it%2520investigates%250Athe%2520problems%2520of%2520optimal%2520false%2520data%2520injection%2520attack%2520scheduling%2520and%250Acountermeasure%2520design%2520for%2520unmanned%2520aerial%2520vehicles.%2520Using%2520a%2520state-of-the-art%250Adeep%2520learning-based%2520approach%252C%2520an%2520optimal%2520false%2520data%2520injection%2520attack%2520scheme%2520is%250Aproposed%2520to%2520deteriorate%2520a%2520quadrotor%2527s%2520tracking%2520performance%2520with%2520limited%2520attack%250Aenergy.%2520Subsequently%252C%2520an%2520optimal%2520tracking%2520control%2520strategy%2520is%2520learned%2520to%250Amitigate%2520attacks%2520and%2520recover%2520the%2520quadrotor%2527s%2520tracking%2520performance.%2520We%2520base%2520our%250Awork%2520on%2520Agilicious%252C%2520a%2520state-of-the-art%2520quadrotor%2520recently%2520deployed%2520for%250Aautonomous%2520settings.%2520This%2520paper%2520is%2520the%2520first%2520in%2520the%2520United%2520Kingdom%2520to%2520deploy%250Athis%2520quadrotor%2520and%2520implement%2520reinforcement%2520learning%2520on%2520its%2520platform.%2520Therefore%252C%250Ato%2520promote%2520easy%2520reproducibility%2520with%2520minimal%2520engineering%2520overhead%252C%2520we%2520further%250Aprovide%2520%25281%2529%2520a%2520comprehensive%2520breakdown%2520of%2520this%2520quadrotor%252C%2520including%2520software%250Astacks%2520and%2520hardware%2520alternatives%253B%2520%25282%2529%2520a%2520detailed%2520reinforcement-learning%250Aframework%2520to%2520train%2520autonomous%2520controllers%2520on%2520Agilicious%2520agents%253B%2520and%2520%25283%2529%2520a%2520new%250Aopen-source%2520environment%2520that%2520builds%2520upon%2520PyFlyt%2520for%2520future%2520reinforcement%250Alearning%2520research%2520on%2520Agilicious%2520platforms.%2520Both%2520simulated%2520and%2520real-world%250Aexperiments%2520are%2520conducted%2520to%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520frameworks%250Ain%2520section%25205.2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secure%20Control%20Systems%20for%20Autonomous%20Quadrotors%20against%20Cyber-Attacks&entry.906535625=Samuel%20Belkadi&entry.1292438233=%20%20The%20problem%20of%20safety%20for%20robotic%20systems%20has%20been%20extensively%20studied.%0AHowever%2C%20little%20attention%20has%20been%20given%20to%20security%20issues%20for%0Athree-dimensional%20systems%2C%20such%20as%20quadrotors.%20Malicious%20adversaries%20can%0Acompromise%20robot%20sensors%20and%20communication%20networks%2C%20causing%20incidents%2C%0Aachieving%20illegal%20objectives%2C%20or%20even%20injuring%20people.%20This%20study%20first%20designs%0Aan%20intelligent%20control%20system%20for%20autonomous%20quadrotors.%20Then%2C%20it%20investigates%0Athe%20problems%20of%20optimal%20false%20data%20injection%20attack%20scheduling%20and%0Acountermeasure%20design%20for%20unmanned%20aerial%20vehicles.%20Using%20a%20state-of-the-art%0Adeep%20learning-based%20approach%2C%20an%20optimal%20false%20data%20injection%20attack%20scheme%20is%0Aproposed%20to%20deteriorate%20a%20quadrotor%27s%20tracking%20performance%20with%20limited%20attack%0Aenergy.%20Subsequently%2C%20an%20optimal%20tracking%20control%20strategy%20is%20learned%20to%0Amitigate%20attacks%20and%20recover%20the%20quadrotor%27s%20tracking%20performance.%20We%20base%20our%0Awork%20on%20Agilicious%2C%20a%20state-of-the-art%20quadrotor%20recently%20deployed%20for%0Aautonomous%20settings.%20This%20paper%20is%20the%20first%20in%20the%20United%20Kingdom%20to%20deploy%0Athis%20quadrotor%20and%20implement%20reinforcement%20learning%20on%20its%20platform.%20Therefore%2C%0Ato%20promote%20easy%20reproducibility%20with%20minimal%20engineering%20overhead%2C%20we%20further%0Aprovide%20%281%29%20a%20comprehensive%20breakdown%20of%20this%20quadrotor%2C%20including%20software%0Astacks%20and%20hardware%20alternatives%3B%20%282%29%20a%20detailed%20reinforcement-learning%0Aframework%20to%20train%20autonomous%20controllers%20on%20Agilicious%20agents%3B%20and%20%283%29%20a%20new%0Aopen-source%20environment%20that%20builds%20upon%20PyFlyt%20for%20future%20reinforcement%0Alearning%20research%20on%20Agilicious%20platforms.%20Both%20simulated%20and%20real-world%0Aexperiments%20are%20conducted%20to%20show%20the%20effectiveness%20of%20the%20proposed%20frameworks%0Ain%20section%205.2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11897v1&entry.124074799=Read"},
{"title": "Explore-Go: Leveraging Exploration for Generalisation in Deep\n  Reinforcement Learning", "author": "Max Weltevrede and Felix Kaubek and Matthijs T. J. Spaan and Wendelin B\u00f6hmer", "abstract": "  One of the remaining challenges in reinforcement learning is to develop\nagents that can generalise to novel scenarios they might encounter once\ndeployed. This challenge is often framed in a multi-task setting where agents\ntrain on a fixed set of tasks and have to generalise to new tasks. Recent work\nhas shown that in this setting increased exploration during training can be\nleveraged to increase the generalisation performance of the agent. This makes\nsense when the states encountered during testing can actually be explored\nduring training. In this paper, we provide intuition why exploration can also\nbenefit generalisation to states that cannot be explicitly encountered during\ntraining. Additionally, we propose a novel method Explore-Go that exploits this\nintuition by increasing the number of states on which the agent trains.\nExplore-Go effectively increases the starting state distribution of the agent\nand as a result can be used in conjunction with most existing on-policy or\noff-policy reinforcement learning algorithms. We show empirically that our\nmethod can increase generalisation performance in an illustrative environment\nand on the Procgen benchmark.\n", "link": "http://arxiv.org/abs/2406.08069v3", "date": "2024-09-18", "relevancy": 2.0995, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5622}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5528}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Max%20Weltevrede%20and%20Felix%20Kaubek%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer%0AAbstract%3A%20%20%20One%20of%20the%20remaining%20challenges%20in%20reinforcement%20learning%20is%20to%20develop%0Aagents%20that%20can%20generalise%20to%20novel%20scenarios%20they%20might%20encounter%20once%0Adeployed.%20This%20challenge%20is%20often%20framed%20in%20a%20multi-task%20setting%20where%20agents%0Atrain%20on%20a%20fixed%20set%20of%20tasks%20and%20have%20to%20generalise%20to%20new%20tasks.%20Recent%20work%0Ahas%20shown%20that%20in%20this%20setting%20increased%20exploration%20during%20training%20can%20be%0Aleveraged%20to%20increase%20the%20generalisation%20performance%20of%20the%20agent.%20This%20makes%0Asense%20when%20the%20states%20encountered%20during%20testing%20can%20actually%20be%20explored%0Aduring%20training.%20In%20this%20paper%2C%20we%20provide%20intuition%20why%20exploration%20can%20also%0Abenefit%20generalisation%20to%20states%20that%20cannot%20be%20explicitly%20encountered%20during%0Atraining.%20Additionally%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%20exploits%20this%0Aintuition%20by%20increasing%20the%20number%20of%20states%20on%20which%20the%20agent%20trains.%0AExplore-Go%20effectively%20increases%20the%20starting%20state%20distribution%20of%20the%20agent%0Aand%20as%20a%20result%20can%20be%20used%20in%20conjunction%20with%20most%20existing%20on-policy%20or%0Aoff-policy%20reinforcement%20learning%20algorithms.%20We%20show%20empirically%20that%20our%0Amethod%20can%20increase%20generalisation%20performance%20in%20an%20illustrative%20environment%0Aand%20on%20the%20Procgen%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08069v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplore-Go%253A%2520Leveraging%2520Exploration%2520for%2520Generalisation%2520in%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DMax%2520Weltevrede%2520and%2520Felix%2520Kaubek%2520and%2520Matthijs%2520T.%2520J.%2520Spaan%2520and%2520Wendelin%2520B%25C3%25B6hmer%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520remaining%2520challenges%2520in%2520reinforcement%2520learning%2520is%2520to%2520develop%250Aagents%2520that%2520can%2520generalise%2520to%2520novel%2520scenarios%2520they%2520might%2520encounter%2520once%250Adeployed.%2520This%2520challenge%2520is%2520often%2520framed%2520in%2520a%2520multi-task%2520setting%2520where%2520agents%250Atrain%2520on%2520a%2520fixed%2520set%2520of%2520tasks%2520and%2520have%2520to%2520generalise%2520to%2520new%2520tasks.%2520Recent%2520work%250Ahas%2520shown%2520that%2520in%2520this%2520setting%2520increased%2520exploration%2520during%2520training%2520can%2520be%250Aleveraged%2520to%2520increase%2520the%2520generalisation%2520performance%2520of%2520the%2520agent.%2520This%2520makes%250Asense%2520when%2520the%2520states%2520encountered%2520during%2520testing%2520can%2520actually%2520be%2520explored%250Aduring%2520training.%2520In%2520this%2520paper%252C%2520we%2520provide%2520intuition%2520why%2520exploration%2520can%2520also%250Abenefit%2520generalisation%2520to%2520states%2520that%2520cannot%2520be%2520explicitly%2520encountered%2520during%250Atraining.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520method%2520Explore-Go%2520that%2520exploits%2520this%250Aintuition%2520by%2520increasing%2520the%2520number%2520of%2520states%2520on%2520which%2520the%2520agent%2520trains.%250AExplore-Go%2520effectively%2520increases%2520the%2520starting%2520state%2520distribution%2520of%2520the%2520agent%250Aand%2520as%2520a%2520result%2520can%2520be%2520used%2520in%2520conjunction%2520with%2520most%2520existing%2520on-policy%2520or%250Aoff-policy%2520reinforcement%2520learning%2520algorithms.%2520We%2520show%2520empirically%2520that%2520our%250Amethod%2520can%2520increase%2520generalisation%2520performance%2520in%2520an%2520illustrative%2520environment%250Aand%2520on%2520the%2520Procgen%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08069v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Max%20Weltevrede%20and%20Felix%20Kaubek%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer&entry.1292438233=%20%20One%20of%20the%20remaining%20challenges%20in%20reinforcement%20learning%20is%20to%20develop%0Aagents%20that%20can%20generalise%20to%20novel%20scenarios%20they%20might%20encounter%20once%0Adeployed.%20This%20challenge%20is%20often%20framed%20in%20a%20multi-task%20setting%20where%20agents%0Atrain%20on%20a%20fixed%20set%20of%20tasks%20and%20have%20to%20generalise%20to%20new%20tasks.%20Recent%20work%0Ahas%20shown%20that%20in%20this%20setting%20increased%20exploration%20during%20training%20can%20be%0Aleveraged%20to%20increase%20the%20generalisation%20performance%20of%20the%20agent.%20This%20makes%0Asense%20when%20the%20states%20encountered%20during%20testing%20can%20actually%20be%20explored%0Aduring%20training.%20In%20this%20paper%2C%20we%20provide%20intuition%20why%20exploration%20can%20also%0Abenefit%20generalisation%20to%20states%20that%20cannot%20be%20explicitly%20encountered%20during%0Atraining.%20Additionally%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%20exploits%20this%0Aintuition%20by%20increasing%20the%20number%20of%20states%20on%20which%20the%20agent%20trains.%0AExplore-Go%20effectively%20increases%20the%20starting%20state%20distribution%20of%20the%20agent%0Aand%20as%20a%20result%20can%20be%20used%20in%20conjunction%20with%20most%20existing%20on-policy%20or%0Aoff-policy%20reinforcement%20learning%20algorithms.%20We%20show%20empirically%20that%20our%0Amethod%20can%20increase%20generalisation%20performance%20in%20an%20illustrative%20environment%0Aand%20on%20the%20Procgen%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08069v3&entry.124074799=Read"},
{"title": "Less Memory Means smaller GPUs: Backpropagation with Compressed\n  Activations", "author": "Daniel Barley and Holger Fr\u00f6ning", "abstract": "  The ever-growing scale of deep neural networks (DNNs) has lead to an equally\nrapid growth in computational resource requirements. Many recent architectures,\nmost prominently Large Language Models, have to be trained using supercomputers\nwith thousands of accelerators, such as GPUs or TPUs. Next to the vast number\nof floating point operations the memory footprint of DNNs is also exploding. In\ncontrast, GPU architectures are notoriously short on memory. Even comparatively\nsmall architectures like some EfficientNet variants cannot be trained on a\nsingle consumer-grade GPU at reasonable mini-batch sizes. During training,\nintermediate input activations have to be stored until backpropagation for\ngradient calculation. These make up the vast majority of the memory footprint.\nIn this work we therefore consider compressing activation maps for the backward\npass using pooling, which can reduce both the memory footprint and amount of\ndata movement. The forward computation remains uncompressed. We empirically\nshow convergence and study effects on feature detection at the example of the\ncommon vision architecture ResNet. With this approach we are able to reduce the\npeak memory consumption by 29% at the cost of a longer training schedule, while\nmaintaining prediction accuracy compared to an uncompressed baseline.\n", "link": "http://arxiv.org/abs/2409.11902v1", "date": "2024-09-18", "relevancy": 2.0992, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5484}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.544}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Less%20Memory%20Means%20smaller%20GPUs%3A%20Backpropagation%20with%20Compressed%0A%20%20Activations&body=Title%3A%20Less%20Memory%20Means%20smaller%20GPUs%3A%20Backpropagation%20with%20Compressed%0A%20%20Activations%0AAuthor%3A%20Daniel%20Barley%20and%20Holger%20Fr%C3%B6ning%0AAbstract%3A%20%20%20The%20ever-growing%20scale%20of%20deep%20neural%20networks%20%28DNNs%29%20has%20lead%20to%20an%20equally%0Arapid%20growth%20in%20computational%20resource%20requirements.%20Many%20recent%20architectures%2C%0Amost%20prominently%20Large%20Language%20Models%2C%20have%20to%20be%20trained%20using%20supercomputers%0Awith%20thousands%20of%20accelerators%2C%20such%20as%20GPUs%20or%20TPUs.%20Next%20to%20the%20vast%20number%0Aof%20floating%20point%20operations%20the%20memory%20footprint%20of%20DNNs%20is%20also%20exploding.%20In%0Acontrast%2C%20GPU%20architectures%20are%20notoriously%20short%20on%20memory.%20Even%20comparatively%0Asmall%20architectures%20like%20some%20EfficientNet%20variants%20cannot%20be%20trained%20on%20a%0Asingle%20consumer-grade%20GPU%20at%20reasonable%20mini-batch%20sizes.%20During%20training%2C%0Aintermediate%20input%20activations%20have%20to%20be%20stored%20until%20backpropagation%20for%0Agradient%20calculation.%20These%20make%20up%20the%20vast%20majority%20of%20the%20memory%20footprint.%0AIn%20this%20work%20we%20therefore%20consider%20compressing%20activation%20maps%20for%20the%20backward%0Apass%20using%20pooling%2C%20which%20can%20reduce%20both%20the%20memory%20footprint%20and%20amount%20of%0Adata%20movement.%20The%20forward%20computation%20remains%20uncompressed.%20We%20empirically%0Ashow%20convergence%20and%20study%20effects%20on%20feature%20detection%20at%20the%20example%20of%20the%0Acommon%20vision%20architecture%20ResNet.%20With%20this%20approach%20we%20are%20able%20to%20reduce%20the%0Apeak%20memory%20consumption%20by%2029%25%20at%20the%20cost%20of%20a%20longer%20training%20schedule%2C%20while%0Amaintaining%20prediction%20accuracy%20compared%20to%20an%20uncompressed%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLess%2520Memory%2520Means%2520smaller%2520GPUs%253A%2520Backpropagation%2520with%2520Compressed%250A%2520%2520Activations%26entry.906535625%3DDaniel%2520Barley%2520and%2520Holger%2520Fr%25C3%25B6ning%26entry.1292438233%3D%2520%2520The%2520ever-growing%2520scale%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520has%2520lead%2520to%2520an%2520equally%250Arapid%2520growth%2520in%2520computational%2520resource%2520requirements.%2520Many%2520recent%2520architectures%252C%250Amost%2520prominently%2520Large%2520Language%2520Models%252C%2520have%2520to%2520be%2520trained%2520using%2520supercomputers%250Awith%2520thousands%2520of%2520accelerators%252C%2520such%2520as%2520GPUs%2520or%2520TPUs.%2520Next%2520to%2520the%2520vast%2520number%250Aof%2520floating%2520point%2520operations%2520the%2520memory%2520footprint%2520of%2520DNNs%2520is%2520also%2520exploding.%2520In%250Acontrast%252C%2520GPU%2520architectures%2520are%2520notoriously%2520short%2520on%2520memory.%2520Even%2520comparatively%250Asmall%2520architectures%2520like%2520some%2520EfficientNet%2520variants%2520cannot%2520be%2520trained%2520on%2520a%250Asingle%2520consumer-grade%2520GPU%2520at%2520reasonable%2520mini-batch%2520sizes.%2520During%2520training%252C%250Aintermediate%2520input%2520activations%2520have%2520to%2520be%2520stored%2520until%2520backpropagation%2520for%250Agradient%2520calculation.%2520These%2520make%2520up%2520the%2520vast%2520majority%2520of%2520the%2520memory%2520footprint.%250AIn%2520this%2520work%2520we%2520therefore%2520consider%2520compressing%2520activation%2520maps%2520for%2520the%2520backward%250Apass%2520using%2520pooling%252C%2520which%2520can%2520reduce%2520both%2520the%2520memory%2520footprint%2520and%2520amount%2520of%250Adata%2520movement.%2520The%2520forward%2520computation%2520remains%2520uncompressed.%2520We%2520empirically%250Ashow%2520convergence%2520and%2520study%2520effects%2520on%2520feature%2520detection%2520at%2520the%2520example%2520of%2520the%250Acommon%2520vision%2520architecture%2520ResNet.%2520With%2520this%2520approach%2520we%2520are%2520able%2520to%2520reduce%2520the%250Apeak%2520memory%2520consumption%2520by%252029%2525%2520at%2520the%2520cost%2520of%2520a%2520longer%2520training%2520schedule%252C%2520while%250Amaintaining%2520prediction%2520accuracy%2520compared%2520to%2520an%2520uncompressed%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Less%20Memory%20Means%20smaller%20GPUs%3A%20Backpropagation%20with%20Compressed%0A%20%20Activations&entry.906535625=Daniel%20Barley%20and%20Holger%20Fr%C3%B6ning&entry.1292438233=%20%20The%20ever-growing%20scale%20of%20deep%20neural%20networks%20%28DNNs%29%20has%20lead%20to%20an%20equally%0Arapid%20growth%20in%20computational%20resource%20requirements.%20Many%20recent%20architectures%2C%0Amost%20prominently%20Large%20Language%20Models%2C%20have%20to%20be%20trained%20using%20supercomputers%0Awith%20thousands%20of%20accelerators%2C%20such%20as%20GPUs%20or%20TPUs.%20Next%20to%20the%20vast%20number%0Aof%20floating%20point%20operations%20the%20memory%20footprint%20of%20DNNs%20is%20also%20exploding.%20In%0Acontrast%2C%20GPU%20architectures%20are%20notoriously%20short%20on%20memory.%20Even%20comparatively%0Asmall%20architectures%20like%20some%20EfficientNet%20variants%20cannot%20be%20trained%20on%20a%0Asingle%20consumer-grade%20GPU%20at%20reasonable%20mini-batch%20sizes.%20During%20training%2C%0Aintermediate%20input%20activations%20have%20to%20be%20stored%20until%20backpropagation%20for%0Agradient%20calculation.%20These%20make%20up%20the%20vast%20majority%20of%20the%20memory%20footprint.%0AIn%20this%20work%20we%20therefore%20consider%20compressing%20activation%20maps%20for%20the%20backward%0Apass%20using%20pooling%2C%20which%20can%20reduce%20both%20the%20memory%20footprint%20and%20amount%20of%0Adata%20movement.%20The%20forward%20computation%20remains%20uncompressed.%20We%20empirically%0Ashow%20convergence%20and%20study%20effects%20on%20feature%20detection%20at%20the%20example%20of%20the%0Acommon%20vision%20architecture%20ResNet.%20With%20this%20approach%20we%20are%20able%20to%20reduce%20the%0Apeak%20memory%20consumption%20by%2029%25%20at%20the%20cost%20of%20a%20longer%20training%20schedule%2C%20while%0Amaintaining%20prediction%20accuracy%20compared%20to%20an%20uncompressed%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11902v1&entry.124074799=Read"},
{"title": "Dynamic PDB: A New Dataset and a SE(3) Model Extension by Integrating\n  Dynamic Behaviors and Physical Properties in Protein Structures", "author": "Ce Liu and Jun Wang and Zhiqiang Cai and Yingxu Wang and Huizhen Kuang and Kaihui Cheng and Liwei Zhang and Qingkun Su and Yining Tang and Fenglei Cao and Limei Han and Siyu Zhu and Yuan Qi", "abstract": "  Despite significant progress in static protein structure collection and\nprediction, the dynamic behavior of proteins, one of their most vital\ncharacteristics, has been largely overlooked in prior research. This oversight\ncan be attributed to the limited availability, diversity, and heterogeneity of\ndynamic protein datasets. To address this gap, we propose to enhance existing\nprestigious static 3D protein structural databases, such as the Protein Data\nBank (PDB), by integrating dynamic data and additional physical properties.\nSpecifically, we introduce a large-scale dataset, Dynamic PDB, encompassing\napproximately 12.6K proteins, each subjected to all-atom molecular dynamics\n(MD) simulations lasting 1 microsecond to capture conformational changes.\nFurthermore, we provide a comprehensive suite of physical properties, including\natomic velocities and forces, potential and kinetic energies of proteins, and\nthe temperature of the simulation environment, recorded at 1 picosecond\nintervals throughout the simulations. For benchmarking purposes, we evaluate\nstate-of-the-art methods on the proposed dataset for the task of trajectory\nprediction. To demonstrate the value of integrating richer physical properties\nin the study of protein dynamics and related model design, we base our approach\non the SE(3) diffusion model and incorporate these physical properties into the\ntrajectory prediction process. Preliminary results indicate that this\nstraightforward extension of the SE(3) model yields improved accuracy, as\nmeasured by MAE and RMSD, when the proposed physical properties are taken into\nconsideration. https://fudan-generative-vision.github.io/dynamicPDB/ .\n", "link": "http://arxiv.org/abs/2408.12413v3", "date": "2024-09-18", "relevancy": 2.096, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures&body=Title%3A%20Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures%0AAuthor%3A%20Ce%20Liu%20and%20Jun%20Wang%20and%20Zhiqiang%20Cai%20and%20Yingxu%20Wang%20and%20Huizhen%20Kuang%20and%20Kaihui%20Cheng%20and%20Liwei%20Zhang%20and%20Qingkun%20Su%20and%20Yining%20Tang%20and%20Fenglei%20Cao%20and%20Limei%20Han%20and%20Siyu%20Zhu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20static%20protein%20structure%20collection%20and%0Aprediction%2C%20the%20dynamic%20behavior%20of%20proteins%2C%20one%20of%20their%20most%20vital%0Acharacteristics%2C%20has%20been%20largely%20overlooked%20in%20prior%20research.%20This%20oversight%0Acan%20be%20attributed%20to%20the%20limited%20availability%2C%20diversity%2C%20and%20heterogeneity%20of%0Adynamic%20protein%20datasets.%20To%20address%20this%20gap%2C%20we%20propose%20to%20enhance%20existing%0Aprestigious%20static%203D%20protein%20structural%20databases%2C%20such%20as%20the%20Protein%20Data%0ABank%20%28PDB%29%2C%20by%20integrating%20dynamic%20data%20and%20additional%20physical%20properties.%0ASpecifically%2C%20we%20introduce%20a%20large-scale%20dataset%2C%20Dynamic%20PDB%2C%20encompassing%0Aapproximately%2012.6K%20proteins%2C%20each%20subjected%20to%20all-atom%20molecular%20dynamics%0A%28MD%29%20simulations%20lasting%201%20microsecond%20to%20capture%20conformational%20changes.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20suite%20of%20physical%20properties%2C%20including%0Aatomic%20velocities%20and%20forces%2C%20potential%20and%20kinetic%20energies%20of%20proteins%2C%20and%0Athe%20temperature%20of%20the%20simulation%20environment%2C%20recorded%20at%201%20picosecond%0Aintervals%20throughout%20the%20simulations.%20For%20benchmarking%20purposes%2C%20we%20evaluate%0Astate-of-the-art%20methods%20on%20the%20proposed%20dataset%20for%20the%20task%20of%20trajectory%0Aprediction.%20To%20demonstrate%20the%20value%20of%20integrating%20richer%20physical%20properties%0Ain%20the%20study%20of%20protein%20dynamics%20and%20related%20model%20design%2C%20we%20base%20our%20approach%0Aon%20the%20SE%283%29%20diffusion%20model%20and%20incorporate%20these%20physical%20properties%20into%20the%0Atrajectory%20prediction%20process.%20Preliminary%20results%20indicate%20that%20this%0Astraightforward%20extension%20of%20the%20SE%283%29%20model%20yields%20improved%20accuracy%2C%20as%0Ameasured%20by%20MAE%20and%20RMSD%2C%20when%20the%20proposed%20physical%20properties%20are%20taken%20into%0Aconsideration.%20https%3A//fudan-generative-vision.github.io/dynamicPDB/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12413v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520PDB%253A%2520A%2520New%2520Dataset%2520and%2520a%2520SE%25283%2529%2520Model%2520Extension%2520by%2520Integrating%250A%2520%2520Dynamic%2520Behaviors%2520and%2520Physical%2520Properties%2520in%2520Protein%2520Structures%26entry.906535625%3DCe%2520Liu%2520and%2520Jun%2520Wang%2520and%2520Zhiqiang%2520Cai%2520and%2520Yingxu%2520Wang%2520and%2520Huizhen%2520Kuang%2520and%2520Kaihui%2520Cheng%2520and%2520Liwei%2520Zhang%2520and%2520Qingkun%2520Su%2520and%2520Yining%2520Tang%2520and%2520Fenglei%2520Cao%2520and%2520Limei%2520Han%2520and%2520Siyu%2520Zhu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520static%2520protein%2520structure%2520collection%2520and%250Aprediction%252C%2520the%2520dynamic%2520behavior%2520of%2520proteins%252C%2520one%2520of%2520their%2520most%2520vital%250Acharacteristics%252C%2520has%2520been%2520largely%2520overlooked%2520in%2520prior%2520research.%2520This%2520oversight%250Acan%2520be%2520attributed%2520to%2520the%2520limited%2520availability%252C%2520diversity%252C%2520and%2520heterogeneity%2520of%250Adynamic%2520protein%2520datasets.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520to%2520enhance%2520existing%250Aprestigious%2520static%25203D%2520protein%2520structural%2520databases%252C%2520such%2520as%2520the%2520Protein%2520Data%250ABank%2520%2528PDB%2529%252C%2520by%2520integrating%2520dynamic%2520data%2520and%2520additional%2520physical%2520properties.%250ASpecifically%252C%2520we%2520introduce%2520a%2520large-scale%2520dataset%252C%2520Dynamic%2520PDB%252C%2520encompassing%250Aapproximately%252012.6K%2520proteins%252C%2520each%2520subjected%2520to%2520all-atom%2520molecular%2520dynamics%250A%2528MD%2529%2520simulations%2520lasting%25201%2520microsecond%2520to%2520capture%2520conformational%2520changes.%250AFurthermore%252C%2520we%2520provide%2520a%2520comprehensive%2520suite%2520of%2520physical%2520properties%252C%2520including%250Aatomic%2520velocities%2520and%2520forces%252C%2520potential%2520and%2520kinetic%2520energies%2520of%2520proteins%252C%2520and%250Athe%2520temperature%2520of%2520the%2520simulation%2520environment%252C%2520recorded%2520at%25201%2520picosecond%250Aintervals%2520throughout%2520the%2520simulations.%2520For%2520benchmarking%2520purposes%252C%2520we%2520evaluate%250Astate-of-the-art%2520methods%2520on%2520the%2520proposed%2520dataset%2520for%2520the%2520task%2520of%2520trajectory%250Aprediction.%2520To%2520demonstrate%2520the%2520value%2520of%2520integrating%2520richer%2520physical%2520properties%250Ain%2520the%2520study%2520of%2520protein%2520dynamics%2520and%2520related%2520model%2520design%252C%2520we%2520base%2520our%2520approach%250Aon%2520the%2520SE%25283%2529%2520diffusion%2520model%2520and%2520incorporate%2520these%2520physical%2520properties%2520into%2520the%250Atrajectory%2520prediction%2520process.%2520Preliminary%2520results%2520indicate%2520that%2520this%250Astraightforward%2520extension%2520of%2520the%2520SE%25283%2529%2520model%2520yields%2520improved%2520accuracy%252C%2520as%250Ameasured%2520by%2520MAE%2520and%2520RMSD%252C%2520when%2520the%2520proposed%2520physical%2520properties%2520are%2520taken%2520into%250Aconsideration.%2520https%253A//fudan-generative-vision.github.io/dynamicPDB/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12413v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures&entry.906535625=Ce%20Liu%20and%20Jun%20Wang%20and%20Zhiqiang%20Cai%20and%20Yingxu%20Wang%20and%20Huizhen%20Kuang%20and%20Kaihui%20Cheng%20and%20Liwei%20Zhang%20and%20Qingkun%20Su%20and%20Yining%20Tang%20and%20Fenglei%20Cao%20and%20Limei%20Han%20and%20Siyu%20Zhu%20and%20Yuan%20Qi&entry.1292438233=%20%20Despite%20significant%20progress%20in%20static%20protein%20structure%20collection%20and%0Aprediction%2C%20the%20dynamic%20behavior%20of%20proteins%2C%20one%20of%20their%20most%20vital%0Acharacteristics%2C%20has%20been%20largely%20overlooked%20in%20prior%20research.%20This%20oversight%0Acan%20be%20attributed%20to%20the%20limited%20availability%2C%20diversity%2C%20and%20heterogeneity%20of%0Adynamic%20protein%20datasets.%20To%20address%20this%20gap%2C%20we%20propose%20to%20enhance%20existing%0Aprestigious%20static%203D%20protein%20structural%20databases%2C%20such%20as%20the%20Protein%20Data%0ABank%20%28PDB%29%2C%20by%20integrating%20dynamic%20data%20and%20additional%20physical%20properties.%0ASpecifically%2C%20we%20introduce%20a%20large-scale%20dataset%2C%20Dynamic%20PDB%2C%20encompassing%0Aapproximately%2012.6K%20proteins%2C%20each%20subjected%20to%20all-atom%20molecular%20dynamics%0A%28MD%29%20simulations%20lasting%201%20microsecond%20to%20capture%20conformational%20changes.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20suite%20of%20physical%20properties%2C%20including%0Aatomic%20velocities%20and%20forces%2C%20potential%20and%20kinetic%20energies%20of%20proteins%2C%20and%0Athe%20temperature%20of%20the%20simulation%20environment%2C%20recorded%20at%201%20picosecond%0Aintervals%20throughout%20the%20simulations.%20For%20benchmarking%20purposes%2C%20we%20evaluate%0Astate-of-the-art%20methods%20on%20the%20proposed%20dataset%20for%20the%20task%20of%20trajectory%0Aprediction.%20To%20demonstrate%20the%20value%20of%20integrating%20richer%20physical%20properties%0Ain%20the%20study%20of%20protein%20dynamics%20and%20related%20model%20design%2C%20we%20base%20our%20approach%0Aon%20the%20SE%283%29%20diffusion%20model%20and%20incorporate%20these%20physical%20properties%20into%20the%0Atrajectory%20prediction%20process.%20Preliminary%20results%20indicate%20that%20this%0Astraightforward%20extension%20of%20the%20SE%283%29%20model%20yields%20improved%20accuracy%2C%20as%0Ameasured%20by%20MAE%20and%20RMSD%2C%20when%20the%20proposed%20physical%20properties%20are%20taken%20into%0Aconsideration.%20https%3A//fudan-generative-vision.github.io/dynamicPDB/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12413v3&entry.124074799=Read"},
{"title": "ChefFusion: Multimodal Foundation Model Integrating Recipe and Food\n  Image Generation", "author": "Peiyu Li and Xiaobao Huang and Yijun Tian and Nitesh V. Chawla", "abstract": "  Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub.\n", "link": "http://arxiv.org/abs/2409.12010v1", "date": "2024-09-18", "relevancy": 2.0909, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChefFusion%3A%20Multimodal%20Foundation%20Model%20Integrating%20Recipe%20and%20Food%0A%20%20Image%20Generation&body=Title%3A%20ChefFusion%3A%20Multimodal%20Foundation%20Model%20Integrating%20Recipe%20and%20Food%0A%20%20Image%20Generation%0AAuthor%3A%20Peiyu%20Li%20and%20Xiaobao%20Huang%20and%20Yijun%20Tian%20and%20Nitesh%20V.%20Chawla%0AAbstract%3A%20%20%20Significant%20work%20has%20been%20conducted%20in%20the%20domain%20of%20food%20computing%2C%20yet%0Athese%20studies%20typically%20focus%20on%20single%20tasks%20such%20as%20t2t%20%28instruction%0Ageneration%20from%20food%20titles%20and%20ingredients%29%2C%20i2t%20%28recipe%20generation%20from%20food%0Aimages%29%2C%20or%20t2i%20%28food%20image%20generation%20from%20recipes%29.%20None%20of%20these%20approaches%0Aintegrate%20all%20modalities%20simultaneously.%20To%20address%20this%20gap%2C%20we%20introduce%20a%0Anovel%20food%20computing%20foundation%20model%20that%20achieves%20true%20multimodality%2C%0Aencompassing%20tasks%20such%20as%20t2t%2C%20t2i%2C%20i2t%2C%20it2t%2C%20and%20t2ti.%20By%20leveraging%20large%0Alanguage%20models%20%28LLMs%29%20and%20pre-trained%20image%20encoder%20and%20decoder%20models%2C%20our%0Amodel%20can%20perform%20a%20diverse%20array%20of%20food%20computing-related%20tasks%2C%20including%0Afood%20understanding%2C%20food%20recognition%2C%20recipe%20generation%2C%20and%20food%20image%0Ageneration.%20Compared%20to%20previous%20models%2C%20our%20foundation%20model%20demonstrates%20a%0Asignificantly%20broader%20range%20of%20capabilities%20and%20exhibits%20superior%20performance%2C%0Aparticularly%20in%20food%20image%20generation%20and%20recipe%20generation%20tasks.%20We%0Aopen-sourced%20ChefFusion%20at%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChefFusion%253A%2520Multimodal%2520Foundation%2520Model%2520Integrating%2520Recipe%2520and%2520Food%250A%2520%2520Image%2520Generation%26entry.906535625%3DPeiyu%2520Li%2520and%2520Xiaobao%2520Huang%2520and%2520Yijun%2520Tian%2520and%2520Nitesh%2520V.%2520Chawla%26entry.1292438233%3D%2520%2520Significant%2520work%2520has%2520been%2520conducted%2520in%2520the%2520domain%2520of%2520food%2520computing%252C%2520yet%250Athese%2520studies%2520typically%2520focus%2520on%2520single%2520tasks%2520such%2520as%2520t2t%2520%2528instruction%250Ageneration%2520from%2520food%2520titles%2520and%2520ingredients%2529%252C%2520i2t%2520%2528recipe%2520generation%2520from%2520food%250Aimages%2529%252C%2520or%2520t2i%2520%2528food%2520image%2520generation%2520from%2520recipes%2529.%2520None%2520of%2520these%2520approaches%250Aintegrate%2520all%2520modalities%2520simultaneously.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%250Anovel%2520food%2520computing%2520foundation%2520model%2520that%2520achieves%2520true%2520multimodality%252C%250Aencompassing%2520tasks%2520such%2520as%2520t2t%252C%2520t2i%252C%2520i2t%252C%2520it2t%252C%2520and%2520t2ti.%2520By%2520leveraging%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520pre-trained%2520image%2520encoder%2520and%2520decoder%2520models%252C%2520our%250Amodel%2520can%2520perform%2520a%2520diverse%2520array%2520of%2520food%2520computing-related%2520tasks%252C%2520including%250Afood%2520understanding%252C%2520food%2520recognition%252C%2520recipe%2520generation%252C%2520and%2520food%2520image%250Ageneration.%2520Compared%2520to%2520previous%2520models%252C%2520our%2520foundation%2520model%2520demonstrates%2520a%250Asignificantly%2520broader%2520range%2520of%2520capabilities%2520and%2520exhibits%2520superior%2520performance%252C%250Aparticularly%2520in%2520food%2520image%2520generation%2520and%2520recipe%2520generation%2520tasks.%2520We%250Aopen-sourced%2520ChefFusion%2520at%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChefFusion%3A%20Multimodal%20Foundation%20Model%20Integrating%20Recipe%20and%20Food%0A%20%20Image%20Generation&entry.906535625=Peiyu%20Li%20and%20Xiaobao%20Huang%20and%20Yijun%20Tian%20and%20Nitesh%20V.%20Chawla&entry.1292438233=%20%20Significant%20work%20has%20been%20conducted%20in%20the%20domain%20of%20food%20computing%2C%20yet%0Athese%20studies%20typically%20focus%20on%20single%20tasks%20such%20as%20t2t%20%28instruction%0Ageneration%20from%20food%20titles%20and%20ingredients%29%2C%20i2t%20%28recipe%20generation%20from%20food%0Aimages%29%2C%20or%20t2i%20%28food%20image%20generation%20from%20recipes%29.%20None%20of%20these%20approaches%0Aintegrate%20all%20modalities%20simultaneously.%20To%20address%20this%20gap%2C%20we%20introduce%20a%0Anovel%20food%20computing%20foundation%20model%20that%20achieves%20true%20multimodality%2C%0Aencompassing%20tasks%20such%20as%20t2t%2C%20t2i%2C%20i2t%2C%20it2t%2C%20and%20t2ti.%20By%20leveraging%20large%0Alanguage%20models%20%28LLMs%29%20and%20pre-trained%20image%20encoder%20and%20decoder%20models%2C%20our%0Amodel%20can%20perform%20a%20diverse%20array%20of%20food%20computing-related%20tasks%2C%20including%0Afood%20understanding%2C%20food%20recognition%2C%20recipe%20generation%2C%20and%20food%20image%0Ageneration.%20Compared%20to%20previous%20models%2C%20our%20foundation%20model%20demonstrates%20a%0Asignificantly%20broader%20range%20of%20capabilities%20and%20exhibits%20superior%20performance%2C%0Aparticularly%20in%20food%20image%20generation%20and%20recipe%20generation%20tasks.%20We%0Aopen-sourced%20ChefFusion%20at%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12010v1&entry.124074799=Read"},
{"title": "Dynamic Gap: Safe Gap-based Navigation in Dynamic Environments", "author": "Max Asselmeier and Dhruv Ahuja and Abdel Zaro and Ahmad Abuaish and Ye Zhao and Patricio A. Vela", "abstract": "  This paper extends the family of gap-based local planners to unknown dynamic\nenvironments through generating provable collision-free properties for\nhierarchical navigation systems. Existing perception-informed local planners\nthat operate in dynamic environments rely on emergent or empirical robustness\nfor collision avoidance as opposed to performing formal analysis of dynamic\nobstacles. In addition to this, the obstacle tracking that is performed in\nthese existent planners is often achieved with respect to a global inertial\nframe, subjecting such tracking estimates to transformation errors from\nodometry drift. The proposed local planner, dynamic gap, shifts the tracking\nparadigm to modeling how the free space, represented as gaps, evolves over\ntime. Gap crossing and closing conditions are developed to aid in determining\nthe feasibility of passage through gaps, and a breadth of simulation\nbenchmarking is performed against other navigation planners in the literature\nwhere the proposed dynamic gap planner achieves the highest success rate out of\nall planners tested in all environments.\n", "link": "http://arxiv.org/abs/2210.05022v2", "date": "2024-09-18", "relevancy": 2.0828, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5207}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Gap%3A%20Safe%20Gap-based%20Navigation%20in%20Dynamic%20Environments&body=Title%3A%20Dynamic%20Gap%3A%20Safe%20Gap-based%20Navigation%20in%20Dynamic%20Environments%0AAuthor%3A%20Max%20Asselmeier%20and%20Dhruv%20Ahuja%20and%20Abdel%20Zaro%20and%20Ahmad%20Abuaish%20and%20Ye%20Zhao%20and%20Patricio%20A.%20Vela%0AAbstract%3A%20%20%20This%20paper%20extends%20the%20family%20of%20gap-based%20local%20planners%20to%20unknown%20dynamic%0Aenvironments%20through%20generating%20provable%20collision-free%20properties%20for%0Ahierarchical%20navigation%20systems.%20Existing%20perception-informed%20local%20planners%0Athat%20operate%20in%20dynamic%20environments%20rely%20on%20emergent%20or%20empirical%20robustness%0Afor%20collision%20avoidance%20as%20opposed%20to%20performing%20formal%20analysis%20of%20dynamic%0Aobstacles.%20In%20addition%20to%20this%2C%20the%20obstacle%20tracking%20that%20is%20performed%20in%0Athese%20existent%20planners%20is%20often%20achieved%20with%20respect%20to%20a%20global%20inertial%0Aframe%2C%20subjecting%20such%20tracking%20estimates%20to%20transformation%20errors%20from%0Aodometry%20drift.%20The%20proposed%20local%20planner%2C%20dynamic%20gap%2C%20shifts%20the%20tracking%0Aparadigm%20to%20modeling%20how%20the%20free%20space%2C%20represented%20as%20gaps%2C%20evolves%20over%0Atime.%20Gap%20crossing%20and%20closing%20conditions%20are%20developed%20to%20aid%20in%20determining%0Athe%20feasibility%20of%20passage%20through%20gaps%2C%20and%20a%20breadth%20of%20simulation%0Abenchmarking%20is%20performed%20against%20other%20navigation%20planners%20in%20the%20literature%0Awhere%20the%20proposed%20dynamic%20gap%20planner%20achieves%20the%20highest%20success%20rate%20out%20of%0Aall%20planners%20tested%20in%20all%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.05022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Gap%253A%2520Safe%2520Gap-based%2520Navigation%2520in%2520Dynamic%2520Environments%26entry.906535625%3DMax%2520Asselmeier%2520and%2520Dhruv%2520Ahuja%2520and%2520Abdel%2520Zaro%2520and%2520Ahmad%2520Abuaish%2520and%2520Ye%2520Zhao%2520and%2520Patricio%2520A.%2520Vela%26entry.1292438233%3D%2520%2520This%2520paper%2520extends%2520the%2520family%2520of%2520gap-based%2520local%2520planners%2520to%2520unknown%2520dynamic%250Aenvironments%2520through%2520generating%2520provable%2520collision-free%2520properties%2520for%250Ahierarchical%2520navigation%2520systems.%2520Existing%2520perception-informed%2520local%2520planners%250Athat%2520operate%2520in%2520dynamic%2520environments%2520rely%2520on%2520emergent%2520or%2520empirical%2520robustness%250Afor%2520collision%2520avoidance%2520as%2520opposed%2520to%2520performing%2520formal%2520analysis%2520of%2520dynamic%250Aobstacles.%2520In%2520addition%2520to%2520this%252C%2520the%2520obstacle%2520tracking%2520that%2520is%2520performed%2520in%250Athese%2520existent%2520planners%2520is%2520often%2520achieved%2520with%2520respect%2520to%2520a%2520global%2520inertial%250Aframe%252C%2520subjecting%2520such%2520tracking%2520estimates%2520to%2520transformation%2520errors%2520from%250Aodometry%2520drift.%2520The%2520proposed%2520local%2520planner%252C%2520dynamic%2520gap%252C%2520shifts%2520the%2520tracking%250Aparadigm%2520to%2520modeling%2520how%2520the%2520free%2520space%252C%2520represented%2520as%2520gaps%252C%2520evolves%2520over%250Atime.%2520Gap%2520crossing%2520and%2520closing%2520conditions%2520are%2520developed%2520to%2520aid%2520in%2520determining%250Athe%2520feasibility%2520of%2520passage%2520through%2520gaps%252C%2520and%2520a%2520breadth%2520of%2520simulation%250Abenchmarking%2520is%2520performed%2520against%2520other%2520navigation%2520planners%2520in%2520the%2520literature%250Awhere%2520the%2520proposed%2520dynamic%2520gap%2520planner%2520achieves%2520the%2520highest%2520success%2520rate%2520out%2520of%250Aall%2520planners%2520tested%2520in%2520all%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.05022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Gap%3A%20Safe%20Gap-based%20Navigation%20in%20Dynamic%20Environments&entry.906535625=Max%20Asselmeier%20and%20Dhruv%20Ahuja%20and%20Abdel%20Zaro%20and%20Ahmad%20Abuaish%20and%20Ye%20Zhao%20and%20Patricio%20A.%20Vela&entry.1292438233=%20%20This%20paper%20extends%20the%20family%20of%20gap-based%20local%20planners%20to%20unknown%20dynamic%0Aenvironments%20through%20generating%20provable%20collision-free%20properties%20for%0Ahierarchical%20navigation%20systems.%20Existing%20perception-informed%20local%20planners%0Athat%20operate%20in%20dynamic%20environments%20rely%20on%20emergent%20or%20empirical%20robustness%0Afor%20collision%20avoidance%20as%20opposed%20to%20performing%20formal%20analysis%20of%20dynamic%0Aobstacles.%20In%20addition%20to%20this%2C%20the%20obstacle%20tracking%20that%20is%20performed%20in%0Athese%20existent%20planners%20is%20often%20achieved%20with%20respect%20to%20a%20global%20inertial%0Aframe%2C%20subjecting%20such%20tracking%20estimates%20to%20transformation%20errors%20from%0Aodometry%20drift.%20The%20proposed%20local%20planner%2C%20dynamic%20gap%2C%20shifts%20the%20tracking%0Aparadigm%20to%20modeling%20how%20the%20free%20space%2C%20represented%20as%20gaps%2C%20evolves%20over%0Atime.%20Gap%20crossing%20and%20closing%20conditions%20are%20developed%20to%20aid%20in%20determining%0Athe%20feasibility%20of%20passage%20through%20gaps%2C%20and%20a%20breadth%20of%20simulation%0Abenchmarking%20is%20performed%20against%20other%20navigation%20planners%20in%20the%20literature%0Awhere%20the%20proposed%20dynamic%20gap%20planner%20achieves%20the%20highest%20success%20rate%20out%20of%0Aall%20planners%20tested%20in%20all%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.05022v2&entry.124074799=Read"},
{"title": "GRIN: GRadient-INformed MoE", "author": "Liyuan Liu and Young Jin Kim and Shuohang Wang and Chen Liang and Yelong Shen and Hao Cheng and Xiaodong Liu and Masahiro Tanaka and Xiaoxia Wu and Wenxiang Hu and Vishrav Chaudhary and Zeqi Lin and Chenruidong Zhang and Jilong Xue and Hany Awadalla and Jianfeng Gao and Weizhu Chen", "abstract": "  Mixture-of-Experts (MoE) models scale more effectively than dense models due\nto sparse computation through expert routing, selectively activating only a\nsmall subset of expert modules. However, sparse computation challenges\ntraditional training practices, as discrete expert routing hinders standard\nbackpropagation and thus gradient-based optimization, which are the cornerstone\nof deep learning. To better pursue the scaling power of MoE, we introduce GRIN\n(GRadient-INformed MoE training), which incorporates sparse gradient estimation\nfor expert routing and configures model parallelism to avoid token dropping.\nApplying GRIN to autoregressive language modeling, we develop a top-2\n16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters,\noutperforms a 7B dense model and matches the performance of a 14B dense model\ntrained on the same data. Extensive evaluations across diverse tasks\ndemonstrate the potential of GRIN to significantly enhance MoE efficacy,\nachieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.\n", "link": "http://arxiv.org/abs/2409.12136v1", "date": "2024-09-18", "relevancy": 2.0688, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5195}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRIN%3A%20GRadient-INformed%20MoE&body=Title%3A%20GRIN%3A%20GRadient-INformed%20MoE%0AAuthor%3A%20Liyuan%20Liu%20and%20Young%20Jin%20Kim%20and%20Shuohang%20Wang%20and%20Chen%20Liang%20and%20Yelong%20Shen%20and%20Hao%20Cheng%20and%20Xiaodong%20Liu%20and%20Masahiro%20Tanaka%20and%20Xiaoxia%20Wu%20and%20Wenxiang%20Hu%20and%20Vishrav%20Chaudhary%20and%20Zeqi%20Lin%20and%20Chenruidong%20Zhang%20and%20Jilong%20Xue%20and%20Hany%20Awadalla%20and%20Jianfeng%20Gao%20and%20Weizhu%20Chen%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20models%20scale%20more%20effectively%20than%20dense%20models%20due%0Ato%20sparse%20computation%20through%20expert%20routing%2C%20selectively%20activating%20only%20a%0Asmall%20subset%20of%20expert%20modules.%20However%2C%20sparse%20computation%20challenges%0Atraditional%20training%20practices%2C%20as%20discrete%20expert%20routing%20hinders%20standard%0Abackpropagation%20and%20thus%20gradient-based%20optimization%2C%20which%20are%20the%20cornerstone%0Aof%20deep%20learning.%20To%20better%20pursue%20the%20scaling%20power%20of%20MoE%2C%20we%20introduce%20GRIN%0A%28GRadient-INformed%20MoE%20training%29%2C%20which%20incorporates%20sparse%20gradient%20estimation%0Afor%20expert%20routing%20and%20configures%20model%20parallelism%20to%20avoid%20token%20dropping.%0AApplying%20GRIN%20to%20autoregressive%20language%20modeling%2C%20we%20develop%20a%20top-2%0A16%24%5Ctimes%243.8B%20MoE%20model.%20Our%20model%2C%20with%20only%206.6B%20activated%20parameters%2C%0Aoutperforms%20a%207B%20dense%20model%20and%20matches%20the%20performance%20of%20a%2014B%20dense%20model%0Atrained%20on%20the%20same%20data.%20Extensive%20evaluations%20across%20diverse%20tasks%0Ademonstrate%20the%20potential%20of%20GRIN%20to%20significantly%20enhance%20MoE%20efficacy%2C%0Aachieving%2079.4%20on%20MMLU%2C%2083.7%20on%20HellaSwag%2C%2074.4%20on%20HumanEval%2C%20and%2058.9%20on%20MATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRIN%253A%2520GRadient-INformed%2520MoE%26entry.906535625%3DLiyuan%2520Liu%2520and%2520Young%2520Jin%2520Kim%2520and%2520Shuohang%2520Wang%2520and%2520Chen%2520Liang%2520and%2520Yelong%2520Shen%2520and%2520Hao%2520Cheng%2520and%2520Xiaodong%2520Liu%2520and%2520Masahiro%2520Tanaka%2520and%2520Xiaoxia%2520Wu%2520and%2520Wenxiang%2520Hu%2520and%2520Vishrav%2520Chaudhary%2520and%2520Zeqi%2520Lin%2520and%2520Chenruidong%2520Zhang%2520and%2520Jilong%2520Xue%2520and%2520Hany%2520Awadalla%2520and%2520Jianfeng%2520Gao%2520and%2520Weizhu%2520Chen%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%2520scale%2520more%2520effectively%2520than%2520dense%2520models%2520due%250Ato%2520sparse%2520computation%2520through%2520expert%2520routing%252C%2520selectively%2520activating%2520only%2520a%250Asmall%2520subset%2520of%2520expert%2520modules.%2520However%252C%2520sparse%2520computation%2520challenges%250Atraditional%2520training%2520practices%252C%2520as%2520discrete%2520expert%2520routing%2520hinders%2520standard%250Abackpropagation%2520and%2520thus%2520gradient-based%2520optimization%252C%2520which%2520are%2520the%2520cornerstone%250Aof%2520deep%2520learning.%2520To%2520better%2520pursue%2520the%2520scaling%2520power%2520of%2520MoE%252C%2520we%2520introduce%2520GRIN%250A%2528GRadient-INformed%2520MoE%2520training%2529%252C%2520which%2520incorporates%2520sparse%2520gradient%2520estimation%250Afor%2520expert%2520routing%2520and%2520configures%2520model%2520parallelism%2520to%2520avoid%2520token%2520dropping.%250AApplying%2520GRIN%2520to%2520autoregressive%2520language%2520modeling%252C%2520we%2520develop%2520a%2520top-2%250A16%2524%255Ctimes%25243.8B%2520MoE%2520model.%2520Our%2520model%252C%2520with%2520only%25206.6B%2520activated%2520parameters%252C%250Aoutperforms%2520a%25207B%2520dense%2520model%2520and%2520matches%2520the%2520performance%2520of%2520a%252014B%2520dense%2520model%250Atrained%2520on%2520the%2520same%2520data.%2520Extensive%2520evaluations%2520across%2520diverse%2520tasks%250Ademonstrate%2520the%2520potential%2520of%2520GRIN%2520to%2520significantly%2520enhance%2520MoE%2520efficacy%252C%250Aachieving%252079.4%2520on%2520MMLU%252C%252083.7%2520on%2520HellaSwag%252C%252074.4%2520on%2520HumanEval%252C%2520and%252058.9%2520on%2520MATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRIN%3A%20GRadient-INformed%20MoE&entry.906535625=Liyuan%20Liu%20and%20Young%20Jin%20Kim%20and%20Shuohang%20Wang%20and%20Chen%20Liang%20and%20Yelong%20Shen%20and%20Hao%20Cheng%20and%20Xiaodong%20Liu%20and%20Masahiro%20Tanaka%20and%20Xiaoxia%20Wu%20and%20Wenxiang%20Hu%20and%20Vishrav%20Chaudhary%20and%20Zeqi%20Lin%20and%20Chenruidong%20Zhang%20and%20Jilong%20Xue%20and%20Hany%20Awadalla%20and%20Jianfeng%20Gao%20and%20Weizhu%20Chen&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20models%20scale%20more%20effectively%20than%20dense%20models%20due%0Ato%20sparse%20computation%20through%20expert%20routing%2C%20selectively%20activating%20only%20a%0Asmall%20subset%20of%20expert%20modules.%20However%2C%20sparse%20computation%20challenges%0Atraditional%20training%20practices%2C%20as%20discrete%20expert%20routing%20hinders%20standard%0Abackpropagation%20and%20thus%20gradient-based%20optimization%2C%20which%20are%20the%20cornerstone%0Aof%20deep%20learning.%20To%20better%20pursue%20the%20scaling%20power%20of%20MoE%2C%20we%20introduce%20GRIN%0A%28GRadient-INformed%20MoE%20training%29%2C%20which%20incorporates%20sparse%20gradient%20estimation%0Afor%20expert%20routing%20and%20configures%20model%20parallelism%20to%20avoid%20token%20dropping.%0AApplying%20GRIN%20to%20autoregressive%20language%20modeling%2C%20we%20develop%20a%20top-2%0A16%24%5Ctimes%243.8B%20MoE%20model.%20Our%20model%2C%20with%20only%206.6B%20activated%20parameters%2C%0Aoutperforms%20a%207B%20dense%20model%20and%20matches%20the%20performance%20of%20a%2014B%20dense%20model%0Atrained%20on%20the%20same%20data.%20Extensive%20evaluations%20across%20diverse%20tasks%0Ademonstrate%20the%20potential%20of%20GRIN%20to%20significantly%20enhance%20MoE%20efficacy%2C%0Aachieving%2079.4%20on%20MMLU%2C%2083.7%20on%20HellaSwag%2C%2074.4%20on%20HumanEval%2C%20and%2058.9%20on%20MATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12136v1&entry.124074799=Read"},
{"title": "Precise Forecasting of Sky Images Using Spatial Warping", "author": "Leron Julian and Aswin C. Sankaranarayanan", "abstract": "  The intermittency of solar power, due to occlusion from cloud cover, is one\nof the key factors inhibiting its widespread use in both commercial and\nresidential settings. Hence, real-time forecasting of solar irradiance for\ngrid-connected photovoltaic systems is necessary to schedule and allocate\nresources across the grid. Ground-based imagers that capture wide field-of-view\nimages of the sky are commonly used to monitor cloud movement around a\nparticular site in an effort to forecast solar irradiance. However, these wide\nFOV imagers capture a distorted image of sky image, where regions near the\nhorizon are heavily compressed. This hinders the ability to precisely predict\ncloud motion near the horizon which especially affects prediction over longer\ntime horizons. In this work, we combat the aforementioned constraint by\nintroducing a deep learning method to predict a future sky image frame with\nhigher resolution than previous methods. Our main contribution is to derive an\noptimal warping method to counter the adverse affects of clouds at the horizon,\nand learn a framework for future sky image prediction which better determines\ncloud evolution for longer time horizons.\n", "link": "http://arxiv.org/abs/2409.12162v1", "date": "2024-09-18", "relevancy": 2.0633, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.523}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5146}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precise%20Forecasting%20of%20Sky%20Images%20Using%20Spatial%20Warping&body=Title%3A%20Precise%20Forecasting%20of%20Sky%20Images%20Using%20Spatial%20Warping%0AAuthor%3A%20Leron%20Julian%20and%20Aswin%20C.%20Sankaranarayanan%0AAbstract%3A%20%20%20The%20intermittency%20of%20solar%20power%2C%20due%20to%20occlusion%20from%20cloud%20cover%2C%20is%20one%0Aof%20the%20key%20factors%20inhibiting%20its%20widespread%20use%20in%20both%20commercial%20and%0Aresidential%20settings.%20Hence%2C%20real-time%20forecasting%20of%20solar%20irradiance%20for%0Agrid-connected%20photovoltaic%20systems%20is%20necessary%20to%20schedule%20and%20allocate%0Aresources%20across%20the%20grid.%20Ground-based%20imagers%20that%20capture%20wide%20field-of-view%0Aimages%20of%20the%20sky%20are%20commonly%20used%20to%20monitor%20cloud%20movement%20around%20a%0Aparticular%20site%20in%20an%20effort%20to%20forecast%20solar%20irradiance.%20However%2C%20these%20wide%0AFOV%20imagers%20capture%20a%20distorted%20image%20of%20sky%20image%2C%20where%20regions%20near%20the%0Ahorizon%20are%20heavily%20compressed.%20This%20hinders%20the%20ability%20to%20precisely%20predict%0Acloud%20motion%20near%20the%20horizon%20which%20especially%20affects%20prediction%20over%20longer%0Atime%20horizons.%20In%20this%20work%2C%20we%20combat%20the%20aforementioned%20constraint%20by%0Aintroducing%20a%20deep%20learning%20method%20to%20predict%20a%20future%20sky%20image%20frame%20with%0Ahigher%20resolution%20than%20previous%20methods.%20Our%20main%20contribution%20is%20to%20derive%20an%0Aoptimal%20warping%20method%20to%20counter%20the%20adverse%20affects%20of%20clouds%20at%20the%20horizon%2C%0Aand%20learn%20a%20framework%20for%20future%20sky%20image%20prediction%20which%20better%20determines%0Acloud%20evolution%20for%20longer%20time%20horizons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecise%2520Forecasting%2520of%2520Sky%2520Images%2520Using%2520Spatial%2520Warping%26entry.906535625%3DLeron%2520Julian%2520and%2520Aswin%2520C.%2520Sankaranarayanan%26entry.1292438233%3D%2520%2520The%2520intermittency%2520of%2520solar%2520power%252C%2520due%2520to%2520occlusion%2520from%2520cloud%2520cover%252C%2520is%2520one%250Aof%2520the%2520key%2520factors%2520inhibiting%2520its%2520widespread%2520use%2520in%2520both%2520commercial%2520and%250Aresidential%2520settings.%2520Hence%252C%2520real-time%2520forecasting%2520of%2520solar%2520irradiance%2520for%250Agrid-connected%2520photovoltaic%2520systems%2520is%2520necessary%2520to%2520schedule%2520and%2520allocate%250Aresources%2520across%2520the%2520grid.%2520Ground-based%2520imagers%2520that%2520capture%2520wide%2520field-of-view%250Aimages%2520of%2520the%2520sky%2520are%2520commonly%2520used%2520to%2520monitor%2520cloud%2520movement%2520around%2520a%250Aparticular%2520site%2520in%2520an%2520effort%2520to%2520forecast%2520solar%2520irradiance.%2520However%252C%2520these%2520wide%250AFOV%2520imagers%2520capture%2520a%2520distorted%2520image%2520of%2520sky%2520image%252C%2520where%2520regions%2520near%2520the%250Ahorizon%2520are%2520heavily%2520compressed.%2520This%2520hinders%2520the%2520ability%2520to%2520precisely%2520predict%250Acloud%2520motion%2520near%2520the%2520horizon%2520which%2520especially%2520affects%2520prediction%2520over%2520longer%250Atime%2520horizons.%2520In%2520this%2520work%252C%2520we%2520combat%2520the%2520aforementioned%2520constraint%2520by%250Aintroducing%2520a%2520deep%2520learning%2520method%2520to%2520predict%2520a%2520future%2520sky%2520image%2520frame%2520with%250Ahigher%2520resolution%2520than%2520previous%2520methods.%2520Our%2520main%2520contribution%2520is%2520to%2520derive%2520an%250Aoptimal%2520warping%2520method%2520to%2520counter%2520the%2520adverse%2520affects%2520of%2520clouds%2520at%2520the%2520horizon%252C%250Aand%2520learn%2520a%2520framework%2520for%2520future%2520sky%2520image%2520prediction%2520which%2520better%2520determines%250Acloud%2520evolution%2520for%2520longer%2520time%2520horizons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precise%20Forecasting%20of%20Sky%20Images%20Using%20Spatial%20Warping&entry.906535625=Leron%20Julian%20and%20Aswin%20C.%20Sankaranarayanan&entry.1292438233=%20%20The%20intermittency%20of%20solar%20power%2C%20due%20to%20occlusion%20from%20cloud%20cover%2C%20is%20one%0Aof%20the%20key%20factors%20inhibiting%20its%20widespread%20use%20in%20both%20commercial%20and%0Aresidential%20settings.%20Hence%2C%20real-time%20forecasting%20of%20solar%20irradiance%20for%0Agrid-connected%20photovoltaic%20systems%20is%20necessary%20to%20schedule%20and%20allocate%0Aresources%20across%20the%20grid.%20Ground-based%20imagers%20that%20capture%20wide%20field-of-view%0Aimages%20of%20the%20sky%20are%20commonly%20used%20to%20monitor%20cloud%20movement%20around%20a%0Aparticular%20site%20in%20an%20effort%20to%20forecast%20solar%20irradiance.%20However%2C%20these%20wide%0AFOV%20imagers%20capture%20a%20distorted%20image%20of%20sky%20image%2C%20where%20regions%20near%20the%0Ahorizon%20are%20heavily%20compressed.%20This%20hinders%20the%20ability%20to%20precisely%20predict%0Acloud%20motion%20near%20the%20horizon%20which%20especially%20affects%20prediction%20over%20longer%0Atime%20horizons.%20In%20this%20work%2C%20we%20combat%20the%20aforementioned%20constraint%20by%0Aintroducing%20a%20deep%20learning%20method%20to%20predict%20a%20future%20sky%20image%20frame%20with%0Ahigher%20resolution%20than%20previous%20methods.%20Our%20main%20contribution%20is%20to%20derive%20an%0Aoptimal%20warping%20method%20to%20counter%20the%20adverse%20affects%20of%20clouds%20at%20the%20horizon%2C%0Aand%20learn%20a%20framework%20for%20future%20sky%20image%20prediction%20which%20better%20determines%0Acloud%20evolution%20for%20longer%20time%20horizons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12162v1&entry.124074799=Read"},
{"title": "Multi-Sensor Deep Learning for Glacier Mapping", "author": "Codru\u0163-Andrei Diaconu and Konrad Heidler and Jonathan L. Bamber and Harry Zekollari", "abstract": "  The more than 200,000 glaciers outside the ice sheets play a crucial role in\nour society by influencing sea-level rise, water resource management, natural\nhazards, biodiversity, and tourism. However, only a fraction of these glaciers\nbenefit from consistent and detailed in-situ observations that allow for\nassessing their status and changes over time. This limitation can, in part, be\novercome by relying on satellite-based Earth Observation techniques.\nSatellite-based glacier mapping applications have historically mainly relied on\nmanual and semi-automatic detection methods, while recently, a fast and notable\ntransition to deep learning techniques has started.\n  This chapter reviews how combining multi-sensor remote sensing data and deep\nlearning allows us to better delineate (i.e. map) glaciers and detect their\ntemporal changes. We explain how relying on deep learning multi-sensor\nframeworks to map glaciers benefits from the extensive availability of regional\nand global glacier inventories. We also analyse the rationale behind glacier\nmapping, the benefits of deep learning methodologies, and the inherent\nchallenges in integrating multi-sensor earth observation data with deep\nlearning algorithms.\n  While our review aims to provide a broad overview of glacier mapping efforts,\nwe highlight a few setups where deep learning multi-sensor remote sensing\napplications have a considerable potential added value. This includes\napplications for debris-covered and rock glaciers that are visually difficult\nto distinguish from surroundings and for calving glaciers that are in contact\nwith the ocean. These specific cases are illustrated through a series of visual\nimageries, highlighting some significant advantages and challenges when\ndetecting glacier changes, including dealing with seasonal snow cover, changing\ndebris coverage, and distinguishing glacier fronts from the surrounding sea\nice.\n", "link": "http://arxiv.org/abs/2409.12034v1", "date": "2024-09-18", "relevancy": 2.0569, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Sensor%20Deep%20Learning%20for%20Glacier%20Mapping&body=Title%3A%20Multi-Sensor%20Deep%20Learning%20for%20Glacier%20Mapping%0AAuthor%3A%20Codru%C5%A3-Andrei%20Diaconu%20and%20Konrad%20Heidler%20and%20Jonathan%20L.%20Bamber%20and%20Harry%20Zekollari%0AAbstract%3A%20%20%20The%20more%20than%20200%2C000%20glaciers%20outside%20the%20ice%20sheets%20play%20a%20crucial%20role%20in%0Aour%20society%20by%20influencing%20sea-level%20rise%2C%20water%20resource%20management%2C%20natural%0Ahazards%2C%20biodiversity%2C%20and%20tourism.%20However%2C%20only%20a%20fraction%20of%20these%20glaciers%0Abenefit%20from%20consistent%20and%20detailed%20in-situ%20observations%20that%20allow%20for%0Aassessing%20their%20status%20and%20changes%20over%20time.%20This%20limitation%20can%2C%20in%20part%2C%20be%0Aovercome%20by%20relying%20on%20satellite-based%20Earth%20Observation%20techniques.%0ASatellite-based%20glacier%20mapping%20applications%20have%20historically%20mainly%20relied%20on%0Amanual%20and%20semi-automatic%20detection%20methods%2C%20while%20recently%2C%20a%20fast%20and%20notable%0Atransition%20to%20deep%20learning%20techniques%20has%20started.%0A%20%20This%20chapter%20reviews%20how%20combining%20multi-sensor%20remote%20sensing%20data%20and%20deep%0Alearning%20allows%20us%20to%20better%20delineate%20%28i.e.%20map%29%20glaciers%20and%20detect%20their%0Atemporal%20changes.%20We%20explain%20how%20relying%20on%20deep%20learning%20multi-sensor%0Aframeworks%20to%20map%20glaciers%20benefits%20from%20the%20extensive%20availability%20of%20regional%0Aand%20global%20glacier%20inventories.%20We%20also%20analyse%20the%20rationale%20behind%20glacier%0Amapping%2C%20the%20benefits%20of%20deep%20learning%20methodologies%2C%20and%20the%20inherent%0Achallenges%20in%20integrating%20multi-sensor%20earth%20observation%20data%20with%20deep%0Alearning%20algorithms.%0A%20%20While%20our%20review%20aims%20to%20provide%20a%20broad%20overview%20of%20glacier%20mapping%20efforts%2C%0Awe%20highlight%20a%20few%20setups%20where%20deep%20learning%20multi-sensor%20remote%20sensing%0Aapplications%20have%20a%20considerable%20potential%20added%20value.%20This%20includes%0Aapplications%20for%20debris-covered%20and%20rock%20glaciers%20that%20are%20visually%20difficult%0Ato%20distinguish%20from%20surroundings%20and%20for%20calving%20glaciers%20that%20are%20in%20contact%0Awith%20the%20ocean.%20These%20specific%20cases%20are%20illustrated%20through%20a%20series%20of%20visual%0Aimageries%2C%20highlighting%20some%20significant%20advantages%20and%20challenges%20when%0Adetecting%20glacier%20changes%2C%20including%20dealing%20with%20seasonal%20snow%20cover%2C%20changing%0Adebris%20coverage%2C%20and%20distinguishing%20glacier%20fronts%20from%20the%20surrounding%20sea%0Aice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Sensor%2520Deep%2520Learning%2520for%2520Glacier%2520Mapping%26entry.906535625%3DCodru%25C5%25A3-Andrei%2520Diaconu%2520and%2520Konrad%2520Heidler%2520and%2520Jonathan%2520L.%2520Bamber%2520and%2520Harry%2520Zekollari%26entry.1292438233%3D%2520%2520The%2520more%2520than%2520200%252C000%2520glaciers%2520outside%2520the%2520ice%2520sheets%2520play%2520a%2520crucial%2520role%2520in%250Aour%2520society%2520by%2520influencing%2520sea-level%2520rise%252C%2520water%2520resource%2520management%252C%2520natural%250Ahazards%252C%2520biodiversity%252C%2520and%2520tourism.%2520However%252C%2520only%2520a%2520fraction%2520of%2520these%2520glaciers%250Abenefit%2520from%2520consistent%2520and%2520detailed%2520in-situ%2520observations%2520that%2520allow%2520for%250Aassessing%2520their%2520status%2520and%2520changes%2520over%2520time.%2520This%2520limitation%2520can%252C%2520in%2520part%252C%2520be%250Aovercome%2520by%2520relying%2520on%2520satellite-based%2520Earth%2520Observation%2520techniques.%250ASatellite-based%2520glacier%2520mapping%2520applications%2520have%2520historically%2520mainly%2520relied%2520on%250Amanual%2520and%2520semi-automatic%2520detection%2520methods%252C%2520while%2520recently%252C%2520a%2520fast%2520and%2520notable%250Atransition%2520to%2520deep%2520learning%2520techniques%2520has%2520started.%250A%2520%2520This%2520chapter%2520reviews%2520how%2520combining%2520multi-sensor%2520remote%2520sensing%2520data%2520and%2520deep%250Alearning%2520allows%2520us%2520to%2520better%2520delineate%2520%2528i.e.%2520map%2529%2520glaciers%2520and%2520detect%2520their%250Atemporal%2520changes.%2520We%2520explain%2520how%2520relying%2520on%2520deep%2520learning%2520multi-sensor%250Aframeworks%2520to%2520map%2520glaciers%2520benefits%2520from%2520the%2520extensive%2520availability%2520of%2520regional%250Aand%2520global%2520glacier%2520inventories.%2520We%2520also%2520analyse%2520the%2520rationale%2520behind%2520glacier%250Amapping%252C%2520the%2520benefits%2520of%2520deep%2520learning%2520methodologies%252C%2520and%2520the%2520inherent%250Achallenges%2520in%2520integrating%2520multi-sensor%2520earth%2520observation%2520data%2520with%2520deep%250Alearning%2520algorithms.%250A%2520%2520While%2520our%2520review%2520aims%2520to%2520provide%2520a%2520broad%2520overview%2520of%2520glacier%2520mapping%2520efforts%252C%250Awe%2520highlight%2520a%2520few%2520setups%2520where%2520deep%2520learning%2520multi-sensor%2520remote%2520sensing%250Aapplications%2520have%2520a%2520considerable%2520potential%2520added%2520value.%2520This%2520includes%250Aapplications%2520for%2520debris-covered%2520and%2520rock%2520glaciers%2520that%2520are%2520visually%2520difficult%250Ato%2520distinguish%2520from%2520surroundings%2520and%2520for%2520calving%2520glaciers%2520that%2520are%2520in%2520contact%250Awith%2520the%2520ocean.%2520These%2520specific%2520cases%2520are%2520illustrated%2520through%2520a%2520series%2520of%2520visual%250Aimageries%252C%2520highlighting%2520some%2520significant%2520advantages%2520and%2520challenges%2520when%250Adetecting%2520glacier%2520changes%252C%2520including%2520dealing%2520with%2520seasonal%2520snow%2520cover%252C%2520changing%250Adebris%2520coverage%252C%2520and%2520distinguishing%2520glacier%2520fronts%2520from%2520the%2520surrounding%2520sea%250Aice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Sensor%20Deep%20Learning%20for%20Glacier%20Mapping&entry.906535625=Codru%C5%A3-Andrei%20Diaconu%20and%20Konrad%20Heidler%20and%20Jonathan%20L.%20Bamber%20and%20Harry%20Zekollari&entry.1292438233=%20%20The%20more%20than%20200%2C000%20glaciers%20outside%20the%20ice%20sheets%20play%20a%20crucial%20role%20in%0Aour%20society%20by%20influencing%20sea-level%20rise%2C%20water%20resource%20management%2C%20natural%0Ahazards%2C%20biodiversity%2C%20and%20tourism.%20However%2C%20only%20a%20fraction%20of%20these%20glaciers%0Abenefit%20from%20consistent%20and%20detailed%20in-situ%20observations%20that%20allow%20for%0Aassessing%20their%20status%20and%20changes%20over%20time.%20This%20limitation%20can%2C%20in%20part%2C%20be%0Aovercome%20by%20relying%20on%20satellite-based%20Earth%20Observation%20techniques.%0ASatellite-based%20glacier%20mapping%20applications%20have%20historically%20mainly%20relied%20on%0Amanual%20and%20semi-automatic%20detection%20methods%2C%20while%20recently%2C%20a%20fast%20and%20notable%0Atransition%20to%20deep%20learning%20techniques%20has%20started.%0A%20%20This%20chapter%20reviews%20how%20combining%20multi-sensor%20remote%20sensing%20data%20and%20deep%0Alearning%20allows%20us%20to%20better%20delineate%20%28i.e.%20map%29%20glaciers%20and%20detect%20their%0Atemporal%20changes.%20We%20explain%20how%20relying%20on%20deep%20learning%20multi-sensor%0Aframeworks%20to%20map%20glaciers%20benefits%20from%20the%20extensive%20availability%20of%20regional%0Aand%20global%20glacier%20inventories.%20We%20also%20analyse%20the%20rationale%20behind%20glacier%0Amapping%2C%20the%20benefits%20of%20deep%20learning%20methodologies%2C%20and%20the%20inherent%0Achallenges%20in%20integrating%20multi-sensor%20earth%20observation%20data%20with%20deep%0Alearning%20algorithms.%0A%20%20While%20our%20review%20aims%20to%20provide%20a%20broad%20overview%20of%20glacier%20mapping%20efforts%2C%0Awe%20highlight%20a%20few%20setups%20where%20deep%20learning%20multi-sensor%20remote%20sensing%0Aapplications%20have%20a%20considerable%20potential%20added%20value.%20This%20includes%0Aapplications%20for%20debris-covered%20and%20rock%20glaciers%20that%20are%20visually%20difficult%0Ato%20distinguish%20from%20surroundings%20and%20for%20calving%20glaciers%20that%20are%20in%20contact%0Awith%20the%20ocean.%20These%20specific%20cases%20are%20illustrated%20through%20a%20series%20of%20visual%0Aimageries%2C%20highlighting%20some%20significant%20advantages%20and%20challenges%20when%0Adetecting%20glacier%20changes%2C%20including%20dealing%20with%20seasonal%20snow%20cover%2C%20changing%0Adebris%20coverage%2C%20and%20distinguishing%20glacier%20fronts%20from%20the%20surrounding%20sea%0Aice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12034v1&entry.124074799=Read"},
{"title": "Language Models and Retrieval Augmented Generation for Automated\n  Structured Data Extraction from Diagnostic Reports", "author": "Mohamed Sobhi Jabal and Pranav Warman and Jikai Zhang and Kartikeye Gupta and Ayush Jain and Maciej Mazurowski and Walter Wiggins and Kirti Magudia and Evan Calabrese", "abstract": "  Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction.\n", "link": "http://arxiv.org/abs/2409.10576v2", "date": "2024-09-18", "relevancy": 2.0555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5168}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Models%20and%20Retrieval%20Augmented%20Generation%20for%20Automated%0A%20%20Structured%20Data%20Extraction%20from%20Diagnostic%20Reports&body=Title%3A%20Language%20Models%20and%20Retrieval%20Augmented%20Generation%20for%20Automated%0A%20%20Structured%20Data%20Extraction%20from%20Diagnostic%20Reports%0AAuthor%3A%20Mohamed%20Sobhi%20Jabal%20and%20Pranav%20Warman%20and%20Jikai%20Zhang%20and%20Kartikeye%20Gupta%20and%20Ayush%20Jain%20and%20Maciej%20Mazurowski%20and%20Walter%20Wiggins%20and%20Kirti%20Magudia%20and%20Evan%20Calabrese%0AAbstract%3A%20%20%20Purpose%3A%20To%20develop%20and%20evaluate%20an%20automated%20system%20for%20extracting%0Astructured%20clinical%20information%20from%20unstructured%20radiology%20and%20pathology%0Areports%20using%20open-weights%20large%20language%20models%20%28LMs%29%20and%20retrieval%20augmented%0Ageneration%20%28RAG%29%2C%20and%20to%20assess%20the%20effects%20of%20model%20configuration%20variables%20on%0Aextraction%20performance.%20Methods%20and%20Materials%3A%20The%20study%20utilized%20two%20datasets%3A%0A7%2C294%20radiology%20reports%20annotated%20for%20Brain%20Tumor%20Reporting%20and%20Data%20System%0A%28BT-RADS%29%20scores%20and%202%2C154%20pathology%20reports%20annotated%20for%20isocitrate%0Adehydrogenase%20%28IDH%29%20mutation%20status.%20An%20automated%20pipeline%20was%20developed%20to%0Abenchmark%20the%20performance%20of%20various%20LMs%20and%20RAG%20configurations.%20The%20impact%20of%0Amodel%20size%2C%20quantization%2C%20prompting%20strategies%2C%20output%20formatting%2C%20and%0Ainference%20parameters%20was%20systematically%20evaluated.%20Results%3A%20The%20best%20performing%0Amodels%20achieved%20over%2098%25%20accuracy%20in%20extracting%20BT-RADS%20scores%20from%20radiology%0Areports%20and%20over%2090%25%20for%20IDH%20mutation%20status%20extraction%20from%20pathology%20reports.%0AThe%20top%20model%20being%20medical%20fine-tuned%20llama3.%20Larger%2C%20newer%2C%20and%20domain%0Afine-tuned%20models%20consistently%20outperformed%20older%20and%20smaller%20models.%20Model%0Aquantization%20had%20minimal%20impact%20on%20performance.%20Few-shot%20prompting%0Asignificantly%20improved%20accuracy.%20RAG%20improved%20performance%20for%20complex%20pathology%0Areports%20but%20not%20for%20shorter%20radiology%20reports.%20Conclusions%3A%20Open%20LMs%0Ademonstrate%20significant%20potential%20for%20automated%20extraction%20of%20structured%0Aclinical%20data%20from%20unstructured%20clinical%20reports%20with%20local%20privacy-preserving%0Aapplication.%20Careful%20model%20selection%2C%20prompt%20engineering%2C%20and%20semi-automated%0Aoptimization%20using%20annotated%20data%20are%20critical%20for%20optimal%20performance.%20These%0Aapproaches%20could%20be%20reliable%20enough%20for%20practical%20use%20in%20research%20workflows%2C%0Ahighlighting%20the%20potential%20for%20human-machine%20collaboration%20in%20healthcare%20data%0Aextraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Models%2520and%2520Retrieval%2520Augmented%2520Generation%2520for%2520Automated%250A%2520%2520Structured%2520Data%2520Extraction%2520from%2520Diagnostic%2520Reports%26entry.906535625%3DMohamed%2520Sobhi%2520Jabal%2520and%2520Pranav%2520Warman%2520and%2520Jikai%2520Zhang%2520and%2520Kartikeye%2520Gupta%2520and%2520Ayush%2520Jain%2520and%2520Maciej%2520Mazurowski%2520and%2520Walter%2520Wiggins%2520and%2520Kirti%2520Magudia%2520and%2520Evan%2520Calabrese%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520develop%2520and%2520evaluate%2520an%2520automated%2520system%2520for%2520extracting%250Astructured%2520clinical%2520information%2520from%2520unstructured%2520radiology%2520and%2520pathology%250Areports%2520using%2520open-weights%2520large%2520language%2520models%2520%2528LMs%2529%2520and%2520retrieval%2520augmented%250Ageneration%2520%2528RAG%2529%252C%2520and%2520to%2520assess%2520the%2520effects%2520of%2520model%2520configuration%2520variables%2520on%250Aextraction%2520performance.%2520Methods%2520and%2520Materials%253A%2520The%2520study%2520utilized%2520two%2520datasets%253A%250A7%252C294%2520radiology%2520reports%2520annotated%2520for%2520Brain%2520Tumor%2520Reporting%2520and%2520Data%2520System%250A%2528BT-RADS%2529%2520scores%2520and%25202%252C154%2520pathology%2520reports%2520annotated%2520for%2520isocitrate%250Adehydrogenase%2520%2528IDH%2529%2520mutation%2520status.%2520An%2520automated%2520pipeline%2520was%2520developed%2520to%250Abenchmark%2520the%2520performance%2520of%2520various%2520LMs%2520and%2520RAG%2520configurations.%2520The%2520impact%2520of%250Amodel%2520size%252C%2520quantization%252C%2520prompting%2520strategies%252C%2520output%2520formatting%252C%2520and%250Ainference%2520parameters%2520was%2520systematically%2520evaluated.%2520Results%253A%2520The%2520best%2520performing%250Amodels%2520achieved%2520over%252098%2525%2520accuracy%2520in%2520extracting%2520BT-RADS%2520scores%2520from%2520radiology%250Areports%2520and%2520over%252090%2525%2520for%2520IDH%2520mutation%2520status%2520extraction%2520from%2520pathology%2520reports.%250AThe%2520top%2520model%2520being%2520medical%2520fine-tuned%2520llama3.%2520Larger%252C%2520newer%252C%2520and%2520domain%250Afine-tuned%2520models%2520consistently%2520outperformed%2520older%2520and%2520smaller%2520models.%2520Model%250Aquantization%2520had%2520minimal%2520impact%2520on%2520performance.%2520Few-shot%2520prompting%250Asignificantly%2520improved%2520accuracy.%2520RAG%2520improved%2520performance%2520for%2520complex%2520pathology%250Areports%2520but%2520not%2520for%2520shorter%2520radiology%2520reports.%2520Conclusions%253A%2520Open%2520LMs%250Ademonstrate%2520significant%2520potential%2520for%2520automated%2520extraction%2520of%2520structured%250Aclinical%2520data%2520from%2520unstructured%2520clinical%2520reports%2520with%2520local%2520privacy-preserving%250Aapplication.%2520Careful%2520model%2520selection%252C%2520prompt%2520engineering%252C%2520and%2520semi-automated%250Aoptimization%2520using%2520annotated%2520data%2520are%2520critical%2520for%2520optimal%2520performance.%2520These%250Aapproaches%2520could%2520be%2520reliable%2520enough%2520for%2520practical%2520use%2520in%2520research%2520workflows%252C%250Ahighlighting%2520the%2520potential%2520for%2520human-machine%2520collaboration%2520in%2520healthcare%2520data%250Aextraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Models%20and%20Retrieval%20Augmented%20Generation%20for%20Automated%0A%20%20Structured%20Data%20Extraction%20from%20Diagnostic%20Reports&entry.906535625=Mohamed%20Sobhi%20Jabal%20and%20Pranav%20Warman%20and%20Jikai%20Zhang%20and%20Kartikeye%20Gupta%20and%20Ayush%20Jain%20and%20Maciej%20Mazurowski%20and%20Walter%20Wiggins%20and%20Kirti%20Magudia%20and%20Evan%20Calabrese&entry.1292438233=%20%20Purpose%3A%20To%20develop%20and%20evaluate%20an%20automated%20system%20for%20extracting%0Astructured%20clinical%20information%20from%20unstructured%20radiology%20and%20pathology%0Areports%20using%20open-weights%20large%20language%20models%20%28LMs%29%20and%20retrieval%20augmented%0Ageneration%20%28RAG%29%2C%20and%20to%20assess%20the%20effects%20of%20model%20configuration%20variables%20on%0Aextraction%20performance.%20Methods%20and%20Materials%3A%20The%20study%20utilized%20two%20datasets%3A%0A7%2C294%20radiology%20reports%20annotated%20for%20Brain%20Tumor%20Reporting%20and%20Data%20System%0A%28BT-RADS%29%20scores%20and%202%2C154%20pathology%20reports%20annotated%20for%20isocitrate%0Adehydrogenase%20%28IDH%29%20mutation%20status.%20An%20automated%20pipeline%20was%20developed%20to%0Abenchmark%20the%20performance%20of%20various%20LMs%20and%20RAG%20configurations.%20The%20impact%20of%0Amodel%20size%2C%20quantization%2C%20prompting%20strategies%2C%20output%20formatting%2C%20and%0Ainference%20parameters%20was%20systematically%20evaluated.%20Results%3A%20The%20best%20performing%0Amodels%20achieved%20over%2098%25%20accuracy%20in%20extracting%20BT-RADS%20scores%20from%20radiology%0Areports%20and%20over%2090%25%20for%20IDH%20mutation%20status%20extraction%20from%20pathology%20reports.%0AThe%20top%20model%20being%20medical%20fine-tuned%20llama3.%20Larger%2C%20newer%2C%20and%20domain%0Afine-tuned%20models%20consistently%20outperformed%20older%20and%20smaller%20models.%20Model%0Aquantization%20had%20minimal%20impact%20on%20performance.%20Few-shot%20prompting%0Asignificantly%20improved%20accuracy.%20RAG%20improved%20performance%20for%20complex%20pathology%0Areports%20but%20not%20for%20shorter%20radiology%20reports.%20Conclusions%3A%20Open%20LMs%0Ademonstrate%20significant%20potential%20for%20automated%20extraction%20of%20structured%0Aclinical%20data%20from%20unstructured%20clinical%20reports%20with%20local%20privacy-preserving%0Aapplication.%20Careful%20model%20selection%2C%20prompt%20engineering%2C%20and%20semi-automated%0Aoptimization%20using%20annotated%20data%20are%20critical%20for%20optimal%20performance.%20These%0Aapproaches%20could%20be%20reliable%20enough%20for%20practical%20use%20in%20research%20workflows%2C%0Ahighlighting%20the%20potential%20for%20human-machine%20collaboration%20in%20healthcare%20data%0Aextraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10576v2&entry.124074799=Read"},
{"title": "SFDA-rPPG: Source-Free Domain Adaptive Remote Physiological Measurement\n  with Spatio-Temporal Consistency", "author": "Yiping Xie and Zitong Yu and Bingjie Wu and Weicheng Xie and Linlin Shen", "abstract": "  Remote Photoplethysmography (rPPG) is a non-contact method that uses facial\nvideo to predict changes in blood volume, enabling physiological metrics\nmeasurement. Traditional rPPG models often struggle with poor generalization\ncapacity in unseen domains. Current solutions to this problem is to improve its\ngeneralization in the target domain through Domain Generalization (DG) or\nDomain Adaptation (DA). However, both traditional methods require access to\nboth source domain data and target domain data, which cannot be implemented in\nscenarios with limited access to source data, and another issue is the privacy\nof accessing source domain data. In this paper, we propose the first\nSource-free Domain Adaptation benchmark for rPPG measurement (SFDA-rPPG), which\novercomes these limitations by enabling effective domain adaptation without\naccess to source domain data. Our framework incorporates a Three-Branch\nSpatio-Temporal Consistency Network (TSTC-Net) to enhance feature consistency\nacross domains. Furthermore, we propose a new rPPG distribution alignment loss\nbased on the Frequency-domain Wasserstein Distance (FWD), which leverages\noptimal transport to align power spectrum distributions across domains\neffectively and further enforces the alignment of the three branches. Extensive\ncross-domain experiments and ablation studies demonstrate the effectiveness of\nour proposed method in source-free domain adaptation settings. Our findings\nhighlight the significant contribution of the proposed FWD loss for\ndistributional alignment, providing a valuable reference for future research\nand applications. The source code is available at\nhttps://github.com/XieYiping66/SFDA-rPPG\n", "link": "http://arxiv.org/abs/2409.12040v1", "date": "2024-09-18", "relevancy": 2.0539, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5201}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5139}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFDA-rPPG%3A%20Source-Free%20Domain%20Adaptive%20Remote%20Physiological%20Measurement%0A%20%20with%20Spatio-Temporal%20Consistency&body=Title%3A%20SFDA-rPPG%3A%20Source-Free%20Domain%20Adaptive%20Remote%20Physiological%20Measurement%0A%20%20with%20Spatio-Temporal%20Consistency%0AAuthor%3A%20Yiping%20Xie%20and%20Zitong%20Yu%20and%20Bingjie%20Wu%20and%20Weicheng%20Xie%20and%20Linlin%20Shen%0AAbstract%3A%20%20%20Remote%20Photoplethysmography%20%28rPPG%29%20is%20a%20non-contact%20method%20that%20uses%20facial%0Avideo%20to%20predict%20changes%20in%20blood%20volume%2C%20enabling%20physiological%20metrics%0Ameasurement.%20Traditional%20rPPG%20models%20often%20struggle%20with%20poor%20generalization%0Acapacity%20in%20unseen%20domains.%20Current%20solutions%20to%20this%20problem%20is%20to%20improve%20its%0Ageneralization%20in%20the%20target%20domain%20through%20Domain%20Generalization%20%28DG%29%20or%0ADomain%20Adaptation%20%28DA%29.%20However%2C%20both%20traditional%20methods%20require%20access%20to%0Aboth%20source%20domain%20data%20and%20target%20domain%20data%2C%20which%20cannot%20be%20implemented%20in%0Ascenarios%20with%20limited%20access%20to%20source%20data%2C%20and%20another%20issue%20is%20the%20privacy%0Aof%20accessing%20source%20domain%20data.%20In%20this%20paper%2C%20we%20propose%20the%20first%0ASource-free%20Domain%20Adaptation%20benchmark%20for%20rPPG%20measurement%20%28SFDA-rPPG%29%2C%20which%0Aovercomes%20these%20limitations%20by%20enabling%20effective%20domain%20adaptation%20without%0Aaccess%20to%20source%20domain%20data.%20Our%20framework%20incorporates%20a%20Three-Branch%0ASpatio-Temporal%20Consistency%20Network%20%28TSTC-Net%29%20to%20enhance%20feature%20consistency%0Aacross%20domains.%20Furthermore%2C%20we%20propose%20a%20new%20rPPG%20distribution%20alignment%20loss%0Abased%20on%20the%20Frequency-domain%20Wasserstein%20Distance%20%28FWD%29%2C%20which%20leverages%0Aoptimal%20transport%20to%20align%20power%20spectrum%20distributions%20across%20domains%0Aeffectively%20and%20further%20enforces%20the%20alignment%20of%20the%20three%20branches.%20Extensive%0Across-domain%20experiments%20and%20ablation%20studies%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20method%20in%20source-free%20domain%20adaptation%20settings.%20Our%20findings%0Ahighlight%20the%20significant%20contribution%20of%20the%20proposed%20FWD%20loss%20for%0Adistributional%20alignment%2C%20providing%20a%20valuable%20reference%20for%20future%20research%0Aand%20applications.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/XieYiping66/SFDA-rPPG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFDA-rPPG%253A%2520Source-Free%2520Domain%2520Adaptive%2520Remote%2520Physiological%2520Measurement%250A%2520%2520with%2520Spatio-Temporal%2520Consistency%26entry.906535625%3DYiping%2520Xie%2520and%2520Zitong%2520Yu%2520and%2520Bingjie%2520Wu%2520and%2520Weicheng%2520Xie%2520and%2520Linlin%2520Shen%26entry.1292438233%3D%2520%2520Remote%2520Photoplethysmography%2520%2528rPPG%2529%2520is%2520a%2520non-contact%2520method%2520that%2520uses%2520facial%250Avideo%2520to%2520predict%2520changes%2520in%2520blood%2520volume%252C%2520enabling%2520physiological%2520metrics%250Ameasurement.%2520Traditional%2520rPPG%2520models%2520often%2520struggle%2520with%2520poor%2520generalization%250Acapacity%2520in%2520unseen%2520domains.%2520Current%2520solutions%2520to%2520this%2520problem%2520is%2520to%2520improve%2520its%250Ageneralization%2520in%2520the%2520target%2520domain%2520through%2520Domain%2520Generalization%2520%2528DG%2529%2520or%250ADomain%2520Adaptation%2520%2528DA%2529.%2520However%252C%2520both%2520traditional%2520methods%2520require%2520access%2520to%250Aboth%2520source%2520domain%2520data%2520and%2520target%2520domain%2520data%252C%2520which%2520cannot%2520be%2520implemented%2520in%250Ascenarios%2520with%2520limited%2520access%2520to%2520source%2520data%252C%2520and%2520another%2520issue%2520is%2520the%2520privacy%250Aof%2520accessing%2520source%2520domain%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%250ASource-free%2520Domain%2520Adaptation%2520benchmark%2520for%2520rPPG%2520measurement%2520%2528SFDA-rPPG%2529%252C%2520which%250Aovercomes%2520these%2520limitations%2520by%2520enabling%2520effective%2520domain%2520adaptation%2520without%250Aaccess%2520to%2520source%2520domain%2520data.%2520Our%2520framework%2520incorporates%2520a%2520Three-Branch%250ASpatio-Temporal%2520Consistency%2520Network%2520%2528TSTC-Net%2529%2520to%2520enhance%2520feature%2520consistency%250Aacross%2520domains.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%2520rPPG%2520distribution%2520alignment%2520loss%250Abased%2520on%2520the%2520Frequency-domain%2520Wasserstein%2520Distance%2520%2528FWD%2529%252C%2520which%2520leverages%250Aoptimal%2520transport%2520to%2520align%2520power%2520spectrum%2520distributions%2520across%2520domains%250Aeffectively%2520and%2520further%2520enforces%2520the%2520alignment%2520of%2520the%2520three%2520branches.%2520Extensive%250Across-domain%2520experiments%2520and%2520ablation%2520studies%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520proposed%2520method%2520in%2520source-free%2520domain%2520adaptation%2520settings.%2520Our%2520findings%250Ahighlight%2520the%2520significant%2520contribution%2520of%2520the%2520proposed%2520FWD%2520loss%2520for%250Adistributional%2520alignment%252C%2520providing%2520a%2520valuable%2520reference%2520for%2520future%2520research%250Aand%2520applications.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/XieYiping66/SFDA-rPPG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFDA-rPPG%3A%20Source-Free%20Domain%20Adaptive%20Remote%20Physiological%20Measurement%0A%20%20with%20Spatio-Temporal%20Consistency&entry.906535625=Yiping%20Xie%20and%20Zitong%20Yu%20and%20Bingjie%20Wu%20and%20Weicheng%20Xie%20and%20Linlin%20Shen&entry.1292438233=%20%20Remote%20Photoplethysmography%20%28rPPG%29%20is%20a%20non-contact%20method%20that%20uses%20facial%0Avideo%20to%20predict%20changes%20in%20blood%20volume%2C%20enabling%20physiological%20metrics%0Ameasurement.%20Traditional%20rPPG%20models%20often%20struggle%20with%20poor%20generalization%0Acapacity%20in%20unseen%20domains.%20Current%20solutions%20to%20this%20problem%20is%20to%20improve%20its%0Ageneralization%20in%20the%20target%20domain%20through%20Domain%20Generalization%20%28DG%29%20or%0ADomain%20Adaptation%20%28DA%29.%20However%2C%20both%20traditional%20methods%20require%20access%20to%0Aboth%20source%20domain%20data%20and%20target%20domain%20data%2C%20which%20cannot%20be%20implemented%20in%0Ascenarios%20with%20limited%20access%20to%20source%20data%2C%20and%20another%20issue%20is%20the%20privacy%0Aof%20accessing%20source%20domain%20data.%20In%20this%20paper%2C%20we%20propose%20the%20first%0ASource-free%20Domain%20Adaptation%20benchmark%20for%20rPPG%20measurement%20%28SFDA-rPPG%29%2C%20which%0Aovercomes%20these%20limitations%20by%20enabling%20effective%20domain%20adaptation%20without%0Aaccess%20to%20source%20domain%20data.%20Our%20framework%20incorporates%20a%20Three-Branch%0ASpatio-Temporal%20Consistency%20Network%20%28TSTC-Net%29%20to%20enhance%20feature%20consistency%0Aacross%20domains.%20Furthermore%2C%20we%20propose%20a%20new%20rPPG%20distribution%20alignment%20loss%0Abased%20on%20the%20Frequency-domain%20Wasserstein%20Distance%20%28FWD%29%2C%20which%20leverages%0Aoptimal%20transport%20to%20align%20power%20spectrum%20distributions%20across%20domains%0Aeffectively%20and%20further%20enforces%20the%20alignment%20of%20the%20three%20branches.%20Extensive%0Across-domain%20experiments%20and%20ablation%20studies%20demonstrate%20the%20effectiveness%20of%0Aour%20proposed%20method%20in%20source-free%20domain%20adaptation%20settings.%20Our%20findings%0Ahighlight%20the%20significant%20contribution%20of%20the%20proposed%20FWD%20loss%20for%0Adistributional%20alignment%2C%20providing%20a%20valuable%20reference%20for%20future%20research%0Aand%20applications.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/XieYiping66/SFDA-rPPG%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12040v1&entry.124074799=Read"},
{"title": "Tumor aware recurrent inter-patient deformable image registration of\n  computed tomography scans with lung cancer", "author": "Jue Jiang and Chloe Min Seo Choi and Maria Thor and Joseph O. Deasy and Harini Veeraraghavan", "abstract": "  Background: Voxel-based analysis (VBA) for population level radiotherapy (RT)\noutcomes modeling requires topology preserving inter-patient deformable image\nregistration (DIR) that preserves tumors on moving images while avoiding\nunrealistic deformations due to tumors occurring on fixed images. Purpose: We\ndeveloped a tumor-aware recurrent registration (TRACER) deep learning (DL)\nmethod and evaluated its suitability for VBA. Methods: TRACER consists of\nencoder layers implemented with stacked 3D convolutional long short term memory\nnetwork (3D-CLSTM) followed by decoder and spatial transform layers to compute\ndense deformation vector field (DVF). Multiple CLSTM steps are used to compute\na progressive sequence of deformations. Input conditioning was applied by\nincluding tumor segmentations with 3D image pairs as input channels.\nBidirectional tumor rigidity, image similarity, and deformation smoothness\nlosses were used to optimize the network in an unsupervised manner. TRACER and\nmultiple DL methods were trained with 204 3D CT image pairs from patients with\nlung cancers (LC) and evaluated using (a) Dataset I (N = 308 pairs) with DL\nsegmented LCs, (b) Dataset II (N = 765 pairs) with manually delineated LCs, and\n(c) Dataset III with 42 LC patients treated with RT. Results: TRACER accurately\naligned normal tissues. It best preserved tumors, blackindicated by the\nsmallest tumor volume difference of 0.24\\%, 0.40\\%, and 0.13 \\% and mean square\nerror in CT intensities of 0.005, 0.005, 0.004, computed between original and\nresampled moving image tumors, for Datasets I, II, and III, respectively. It\nresulted in the smallest planned RT tumor dose difference computed between\noriginal and resampled moving images of 0.01 Gy and 0.013 Gy when using a\nfemale and a male reference.\n", "link": "http://arxiv.org/abs/2409.11910v1", "date": "2024-09-18", "relevancy": 2.0482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4934}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tumor%20aware%20recurrent%20inter-patient%20deformable%20image%20registration%20of%0A%20%20computed%20tomography%20scans%20with%20lung%20cancer&body=Title%3A%20Tumor%20aware%20recurrent%20inter-patient%20deformable%20image%20registration%20of%0A%20%20computed%20tomography%20scans%20with%20lung%20cancer%0AAuthor%3A%20Jue%20Jiang%20and%20Chloe%20Min%20Seo%20Choi%20and%20Maria%20Thor%20and%20Joseph%20O.%20Deasy%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Background%3A%20Voxel-based%20analysis%20%28VBA%29%20for%20population%20level%20radiotherapy%20%28RT%29%0Aoutcomes%20modeling%20requires%20topology%20preserving%20inter-patient%20deformable%20image%0Aregistration%20%28DIR%29%20that%20preserves%20tumors%20on%20moving%20images%20while%20avoiding%0Aunrealistic%20deformations%20due%20to%20tumors%20occurring%20on%20fixed%20images.%20Purpose%3A%20We%0Adeveloped%20a%20tumor-aware%20recurrent%20registration%20%28TRACER%29%20deep%20learning%20%28DL%29%0Amethod%20and%20evaluated%20its%20suitability%20for%20VBA.%20Methods%3A%20TRACER%20consists%20of%0Aencoder%20layers%20implemented%20with%20stacked%203D%20convolutional%20long%20short%20term%20memory%0Anetwork%20%283D-CLSTM%29%20followed%20by%20decoder%20and%20spatial%20transform%20layers%20to%20compute%0Adense%20deformation%20vector%20field%20%28DVF%29.%20Multiple%20CLSTM%20steps%20are%20used%20to%20compute%0Aa%20progressive%20sequence%20of%20deformations.%20Input%20conditioning%20was%20applied%20by%0Aincluding%20tumor%20segmentations%20with%203D%20image%20pairs%20as%20input%20channels.%0ABidirectional%20tumor%20rigidity%2C%20image%20similarity%2C%20and%20deformation%20smoothness%0Alosses%20were%20used%20to%20optimize%20the%20network%20in%20an%20unsupervised%20manner.%20TRACER%20and%0Amultiple%20DL%20methods%20were%20trained%20with%20204%203D%20CT%20image%20pairs%20from%20patients%20with%0Alung%20cancers%20%28LC%29%20and%20evaluated%20using%20%28a%29%20Dataset%20I%20%28N%20%3D%20308%20pairs%29%20with%20DL%0Asegmented%20LCs%2C%20%28b%29%20Dataset%20II%20%28N%20%3D%20765%20pairs%29%20with%20manually%20delineated%20LCs%2C%20and%0A%28c%29%20Dataset%20III%20with%2042%20LC%20patients%20treated%20with%20RT.%20Results%3A%20TRACER%20accurately%0Aaligned%20normal%20tissues.%20It%20best%20preserved%20tumors%2C%20blackindicated%20by%20the%0Asmallest%20tumor%20volume%20difference%20of%200.24%5C%25%2C%200.40%5C%25%2C%20and%200.13%20%5C%25%20and%20mean%20square%0Aerror%20in%20CT%20intensities%20of%200.005%2C%200.005%2C%200.004%2C%20computed%20between%20original%20and%0Aresampled%20moving%20image%20tumors%2C%20for%20Datasets%20I%2C%20II%2C%20and%20III%2C%20respectively.%20It%0Aresulted%20in%20the%20smallest%20planned%20RT%20tumor%20dose%20difference%20computed%20between%0Aoriginal%20and%20resampled%20moving%20images%20of%200.01%20Gy%20and%200.013%20Gy%20when%20using%20a%0Afemale%20and%20a%20male%20reference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTumor%2520aware%2520recurrent%2520inter-patient%2520deformable%2520image%2520registration%2520of%250A%2520%2520computed%2520tomography%2520scans%2520with%2520lung%2520cancer%26entry.906535625%3DJue%2520Jiang%2520and%2520Chloe%2520Min%2520Seo%2520Choi%2520and%2520Maria%2520Thor%2520and%2520Joseph%2520O.%2520Deasy%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Background%253A%2520Voxel-based%2520analysis%2520%2528VBA%2529%2520for%2520population%2520level%2520radiotherapy%2520%2528RT%2529%250Aoutcomes%2520modeling%2520requires%2520topology%2520preserving%2520inter-patient%2520deformable%2520image%250Aregistration%2520%2528DIR%2529%2520that%2520preserves%2520tumors%2520on%2520moving%2520images%2520while%2520avoiding%250Aunrealistic%2520deformations%2520due%2520to%2520tumors%2520occurring%2520on%2520fixed%2520images.%2520Purpose%253A%2520We%250Adeveloped%2520a%2520tumor-aware%2520recurrent%2520registration%2520%2528TRACER%2529%2520deep%2520learning%2520%2528DL%2529%250Amethod%2520and%2520evaluated%2520its%2520suitability%2520for%2520VBA.%2520Methods%253A%2520TRACER%2520consists%2520of%250Aencoder%2520layers%2520implemented%2520with%2520stacked%25203D%2520convolutional%2520long%2520short%2520term%2520memory%250Anetwork%2520%25283D-CLSTM%2529%2520followed%2520by%2520decoder%2520and%2520spatial%2520transform%2520layers%2520to%2520compute%250Adense%2520deformation%2520vector%2520field%2520%2528DVF%2529.%2520Multiple%2520CLSTM%2520steps%2520are%2520used%2520to%2520compute%250Aa%2520progressive%2520sequence%2520of%2520deformations.%2520Input%2520conditioning%2520was%2520applied%2520by%250Aincluding%2520tumor%2520segmentations%2520with%25203D%2520image%2520pairs%2520as%2520input%2520channels.%250ABidirectional%2520tumor%2520rigidity%252C%2520image%2520similarity%252C%2520and%2520deformation%2520smoothness%250Alosses%2520were%2520used%2520to%2520optimize%2520the%2520network%2520in%2520an%2520unsupervised%2520manner.%2520TRACER%2520and%250Amultiple%2520DL%2520methods%2520were%2520trained%2520with%2520204%25203D%2520CT%2520image%2520pairs%2520from%2520patients%2520with%250Alung%2520cancers%2520%2528LC%2529%2520and%2520evaluated%2520using%2520%2528a%2529%2520Dataset%2520I%2520%2528N%2520%253D%2520308%2520pairs%2529%2520with%2520DL%250Asegmented%2520LCs%252C%2520%2528b%2529%2520Dataset%2520II%2520%2528N%2520%253D%2520765%2520pairs%2529%2520with%2520manually%2520delineated%2520LCs%252C%2520and%250A%2528c%2529%2520Dataset%2520III%2520with%252042%2520LC%2520patients%2520treated%2520with%2520RT.%2520Results%253A%2520TRACER%2520accurately%250Aaligned%2520normal%2520tissues.%2520It%2520best%2520preserved%2520tumors%252C%2520blackindicated%2520by%2520the%250Asmallest%2520tumor%2520volume%2520difference%2520of%25200.24%255C%2525%252C%25200.40%255C%2525%252C%2520and%25200.13%2520%255C%2525%2520and%2520mean%2520square%250Aerror%2520in%2520CT%2520intensities%2520of%25200.005%252C%25200.005%252C%25200.004%252C%2520computed%2520between%2520original%2520and%250Aresampled%2520moving%2520image%2520tumors%252C%2520for%2520Datasets%2520I%252C%2520II%252C%2520and%2520III%252C%2520respectively.%2520It%250Aresulted%2520in%2520the%2520smallest%2520planned%2520RT%2520tumor%2520dose%2520difference%2520computed%2520between%250Aoriginal%2520and%2520resampled%2520moving%2520images%2520of%25200.01%2520Gy%2520and%25200.013%2520Gy%2520when%2520using%2520a%250Afemale%2520and%2520a%2520male%2520reference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tumor%20aware%20recurrent%20inter-patient%20deformable%20image%20registration%20of%0A%20%20computed%20tomography%20scans%20with%20lung%20cancer&entry.906535625=Jue%20Jiang%20and%20Chloe%20Min%20Seo%20Choi%20and%20Maria%20Thor%20and%20Joseph%20O.%20Deasy%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Background%3A%20Voxel-based%20analysis%20%28VBA%29%20for%20population%20level%20radiotherapy%20%28RT%29%0Aoutcomes%20modeling%20requires%20topology%20preserving%20inter-patient%20deformable%20image%0Aregistration%20%28DIR%29%20that%20preserves%20tumors%20on%20moving%20images%20while%20avoiding%0Aunrealistic%20deformations%20due%20to%20tumors%20occurring%20on%20fixed%20images.%20Purpose%3A%20We%0Adeveloped%20a%20tumor-aware%20recurrent%20registration%20%28TRACER%29%20deep%20learning%20%28DL%29%0Amethod%20and%20evaluated%20its%20suitability%20for%20VBA.%20Methods%3A%20TRACER%20consists%20of%0Aencoder%20layers%20implemented%20with%20stacked%203D%20convolutional%20long%20short%20term%20memory%0Anetwork%20%283D-CLSTM%29%20followed%20by%20decoder%20and%20spatial%20transform%20layers%20to%20compute%0Adense%20deformation%20vector%20field%20%28DVF%29.%20Multiple%20CLSTM%20steps%20are%20used%20to%20compute%0Aa%20progressive%20sequence%20of%20deformations.%20Input%20conditioning%20was%20applied%20by%0Aincluding%20tumor%20segmentations%20with%203D%20image%20pairs%20as%20input%20channels.%0ABidirectional%20tumor%20rigidity%2C%20image%20similarity%2C%20and%20deformation%20smoothness%0Alosses%20were%20used%20to%20optimize%20the%20network%20in%20an%20unsupervised%20manner.%20TRACER%20and%0Amultiple%20DL%20methods%20were%20trained%20with%20204%203D%20CT%20image%20pairs%20from%20patients%20with%0Alung%20cancers%20%28LC%29%20and%20evaluated%20using%20%28a%29%20Dataset%20I%20%28N%20%3D%20308%20pairs%29%20with%20DL%0Asegmented%20LCs%2C%20%28b%29%20Dataset%20II%20%28N%20%3D%20765%20pairs%29%20with%20manually%20delineated%20LCs%2C%20and%0A%28c%29%20Dataset%20III%20with%2042%20LC%20patients%20treated%20with%20RT.%20Results%3A%20TRACER%20accurately%0Aaligned%20normal%20tissues.%20It%20best%20preserved%20tumors%2C%20blackindicated%20by%20the%0Asmallest%20tumor%20volume%20difference%20of%200.24%5C%25%2C%200.40%5C%25%2C%20and%200.13%20%5C%25%20and%20mean%20square%0Aerror%20in%20CT%20intensities%20of%200.005%2C%200.005%2C%200.004%2C%20computed%20between%20original%20and%0Aresampled%20moving%20image%20tumors%2C%20for%20Datasets%20I%2C%20II%2C%20and%20III%2C%20respectively.%20It%0Aresulted%20in%20the%20smallest%20planned%20RT%20tumor%20dose%20difference%20computed%20between%0Aoriginal%20and%20resampled%20moving%20images%20of%200.01%20Gy%20and%200.013%20Gy%20when%20using%20a%0Afemale%20and%20a%20male%20reference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11910v1&entry.124074799=Read"},
{"title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models", "author": "Hanjun Luo and Yingbin Jin and Xuecheng Liu and Tong Shang and Ruizhe Chen and Zuozhu Liu", "abstract": "  Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.\n", "link": "http://arxiv.org/abs/2409.11022v2", "date": "2024-09-18", "relevancy": 2.0436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Hanjun%20Luo%20and%20Yingbin%20Jin%20and%20Xuecheng%20Liu%20and%20Tong%20Shang%20and%20Ruizhe%20Chen%20and%20Zuozhu%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20supplanted%20traditional%20methods%20in%20numerous%0Anatural%20language%20processing%20tasks.%20Nonetheless%2C%20in%20Named%20Entity%20Recognition%0A%28NER%29%2C%20existing%20LLM-based%20methods%20underperform%20compared%20to%20baselines%20and%0Arequire%20significantly%20more%20computational%20resources%2C%20limiting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20the%20task%20of%20generation-based%20extraction%20and%0Ain-context%20classification%20%28GEIC%29%2C%20designed%20to%20leverage%20LLMs%27%20prior%20knowledge%0Aand%20self-attention%20mechanisms%20for%20NER%20tasks.%20We%20then%20propose%20CascadeNER%2C%20a%0Auniversal%20and%20multilingual%20GEIC%20framework%20for%20few-shot%20and%20zero-shot%20NER.%0ACascadeNER%20employs%20model%20cascading%20to%20utilize%20two%20small-parameter%20LLMs%20to%0Aextract%20and%20classify%20independently%2C%20reducing%20resource%20consumption%20while%0Aenhancing%20accuracy.%20We%20also%20introduce%20AnythingNER%2C%20the%20first%20NER%20dataset%0Aspecifically%20designed%20for%20LLMs%2C%20including%208%20languages%2C%20155%20entity%20types%20and%20a%0Anovel%20dynamic%20categorization%20system.%20Experiments%20show%20that%20CascadeNER%20achieves%0Astate-of-the-art%20performance%20on%20low-resource%20and%20fine-grained%20scenarios%2C%0Aincluding%20CrossNER%20and%20FewNERD.%20Our%20work%20is%20openly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEIC%253A%2520Universal%2520and%2520Multilingual%2520Named%2520Entity%2520Recognition%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHanjun%2520Luo%2520and%2520Yingbin%2520Jin%2520and%2520Xuecheng%2520Liu%2520and%2520Tong%2520Shang%2520and%2520Ruizhe%2520Chen%2520and%2520Zuozhu%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520supplanted%2520traditional%2520methods%2520in%2520numerous%250Anatural%2520language%2520processing%2520tasks.%2520Nonetheless%252C%2520in%2520Named%2520Entity%2520Recognition%250A%2528NER%2529%252C%2520existing%2520LLM-based%2520methods%2520underperform%2520compared%2520to%2520baselines%2520and%250Arequire%2520significantly%2520more%2520computational%2520resources%252C%2520limiting%2520their%2520application.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520task%2520of%2520generation-based%2520extraction%2520and%250Ain-context%2520classification%2520%2528GEIC%2529%252C%2520designed%2520to%2520leverage%2520LLMs%2527%2520prior%2520knowledge%250Aand%2520self-attention%2520mechanisms%2520for%2520NER%2520tasks.%2520We%2520then%2520propose%2520CascadeNER%252C%2520a%250Auniversal%2520and%2520multilingual%2520GEIC%2520framework%2520for%2520few-shot%2520and%2520zero-shot%2520NER.%250ACascadeNER%2520employs%2520model%2520cascading%2520to%2520utilize%2520two%2520small-parameter%2520LLMs%2520to%250Aextract%2520and%2520classify%2520independently%252C%2520reducing%2520resource%2520consumption%2520while%250Aenhancing%2520accuracy.%2520We%2520also%2520introduce%2520AnythingNER%252C%2520the%2520first%2520NER%2520dataset%250Aspecifically%2520designed%2520for%2520LLMs%252C%2520including%25208%2520languages%252C%2520155%2520entity%2520types%2520and%2520a%250Anovel%2520dynamic%2520categorization%2520system.%2520Experiments%2520show%2520that%2520CascadeNER%2520achieves%250Astate-of-the-art%2520performance%2520on%2520low-resource%2520and%2520fine-grained%2520scenarios%252C%250Aincluding%2520CrossNER%2520and%2520FewNERD.%2520Our%2520work%2520is%2520openly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models&entry.906535625=Hanjun%20Luo%20and%20Yingbin%20Jin%20and%20Xuecheng%20Liu%20and%20Tong%20Shang%20and%20Ruizhe%20Chen%20and%20Zuozhu%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20supplanted%20traditional%20methods%20in%20numerous%0Anatural%20language%20processing%20tasks.%20Nonetheless%2C%20in%20Named%20Entity%20Recognition%0A%28NER%29%2C%20existing%20LLM-based%20methods%20underperform%20compared%20to%20baselines%20and%0Arequire%20significantly%20more%20computational%20resources%2C%20limiting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20the%20task%20of%20generation-based%20extraction%20and%0Ain-context%20classification%20%28GEIC%29%2C%20designed%20to%20leverage%20LLMs%27%20prior%20knowledge%0Aand%20self-attention%20mechanisms%20for%20NER%20tasks.%20We%20then%20propose%20CascadeNER%2C%20a%0Auniversal%20and%20multilingual%20GEIC%20framework%20for%20few-shot%20and%20zero-shot%20NER.%0ACascadeNER%20employs%20model%20cascading%20to%20utilize%20two%20small-parameter%20LLMs%20to%0Aextract%20and%20classify%20independently%2C%20reducing%20resource%20consumption%20while%0Aenhancing%20accuracy.%20We%20also%20introduce%20AnythingNER%2C%20the%20first%20NER%20dataset%0Aspecifically%20designed%20for%20LLMs%2C%20including%208%20languages%2C%20155%20entity%20types%20and%20a%0Anovel%20dynamic%20categorization%20system.%20Experiments%20show%20that%20CascadeNER%20achieves%0Astate-of-the-art%20performance%20on%20low-resource%20and%20fine-grained%20scenarios%2C%0Aincluding%20CrossNER%20and%20FewNERD.%20Our%20work%20is%20openly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11022v2&entry.124074799=Read"},
{"title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation", "author": "Giorgio Franceschelli and Mirco Musolesi", "abstract": "  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n", "link": "http://arxiv.org/abs/2405.00099v3", "date": "2024-09-18", "relevancy": 2.0398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5113}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation&body=Title%3A%20Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation%0AAuthor%3A%20Giorgio%20Franceschelli%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Large%20language%20models%20are%20revolutionizing%20several%20areas%2C%20including%20artificial%0Acreativity.%20However%2C%20the%20process%20of%20generation%20in%20machines%20profoundly%20diverges%0Afrom%20that%20observed%20in%20humans.%20In%20particular%2C%20machine%20generation%20is%0Acharacterized%20by%20a%20lack%20of%20intentionality%20and%20an%20underlying%20creative%20process.%0AWe%20propose%20a%20method%20called%20Creative%20Beam%20Search%20that%20uses%20Diverse%20Beam%20Search%0Aand%20LLM-as-a-Judge%20to%20perform%20response%20generation%20and%20response%20validation.%20The%0Aresults%20of%20a%20qualitative%20experiment%20show%20how%20our%20approach%20can%20provide%20better%0Aoutput%20than%20standard%20sampling%20techniques.%20We%20also%20show%20that%20the%20response%0Avalidation%20step%20is%20a%20necessary%20complement%20to%20the%20response%20generation%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00099v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreative%2520Beam%2520Search%253A%2520LLM-as-a-Judge%2520For%2520Improving%2520Response%2520Generation%26entry.906535625%3DGiorgio%2520Franceschelli%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520revolutionizing%2520several%2520areas%252C%2520including%2520artificial%250Acreativity.%2520However%252C%2520the%2520process%2520of%2520generation%2520in%2520machines%2520profoundly%2520diverges%250Afrom%2520that%2520observed%2520in%2520humans.%2520In%2520particular%252C%2520machine%2520generation%2520is%250Acharacterized%2520by%2520a%2520lack%2520of%2520intentionality%2520and%2520an%2520underlying%2520creative%2520process.%250AWe%2520propose%2520a%2520method%2520called%2520Creative%2520Beam%2520Search%2520that%2520uses%2520Diverse%2520Beam%2520Search%250Aand%2520LLM-as-a-Judge%2520to%2520perform%2520response%2520generation%2520and%2520response%2520validation.%2520The%250Aresults%2520of%2520a%2520qualitative%2520experiment%2520show%2520how%2520our%2520approach%2520can%2520provide%2520better%250Aoutput%2520than%2520standard%2520sampling%2520techniques.%2520We%2520also%2520show%2520that%2520the%2520response%250Avalidation%2520step%2520is%2520a%2520necessary%2520complement%2520to%2520the%2520response%2520generation%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00099v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation&entry.906535625=Giorgio%20Franceschelli%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Large%20language%20models%20are%20revolutionizing%20several%20areas%2C%20including%20artificial%0Acreativity.%20However%2C%20the%20process%20of%20generation%20in%20machines%20profoundly%20diverges%0Afrom%20that%20observed%20in%20humans.%20In%20particular%2C%20machine%20generation%20is%0Acharacterized%20by%20a%20lack%20of%20intentionality%20and%20an%20underlying%20creative%20process.%0AWe%20propose%20a%20method%20called%20Creative%20Beam%20Search%20that%20uses%20Diverse%20Beam%20Search%0Aand%20LLM-as-a-Judge%20to%20perform%20response%20generation%20and%20response%20validation.%20The%0Aresults%20of%20a%20qualitative%20experiment%20show%20how%20our%20approach%20can%20provide%20better%0Aoutput%20than%20standard%20sampling%20techniques.%20We%20also%20show%20that%20the%20response%0Avalidation%20step%20is%20a%20necessary%20complement%20to%20the%20response%20generation%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00099v3&entry.124074799=Read"},
{"title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning", "author": "Faeze Brahman and Chandra Bhagavatula and Valentina Pyatkin and Jena D. Hwang and Xiang Lorraine Li and Hirona J. Arai and Soumya Sanyal and Keisuke Sakaguchi and Xiang Ren and Yejin Choi", "abstract": "  Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome.\n", "link": "http://arxiv.org/abs/2305.19472v3", "date": "2024-09-18", "relevancy": 2.0395, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5245}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlaSma%3A%20Making%20Small%20Language%20Models%20Better%20Procedural%20Knowledge%20Models%0A%20%20for%20%28Counterfactual%29%20Planning&body=Title%3A%20PlaSma%3A%20Making%20Small%20Language%20Models%20Better%20Procedural%20Knowledge%20Models%0A%20%20for%20%28Counterfactual%29%20Planning%0AAuthor%3A%20Faeze%20Brahman%20and%20Chandra%20Bhagavatula%20and%20Valentina%20Pyatkin%20and%20Jena%20D.%20Hwang%20and%20Xiang%20Lorraine%20Li%20and%20Hirona%20J.%20Arai%20and%20Soumya%20Sanyal%20and%20Keisuke%20Sakaguchi%20and%20Xiang%20Ren%20and%20Yejin%20Choi%0AAbstract%3A%20%20%20Procedural%20planning%2C%20which%20entails%20decomposing%20a%20high-level%20goal%20into%20a%0Asequence%20of%20temporally%20ordered%20steps%2C%20is%20an%20important%20yet%20intricate%20task%20for%0Amachines.%20It%20involves%20integrating%20common-sense%20knowledge%20to%20reason%20about%0Acomplex%20and%20often%20contextualized%20situations%2C%20e.g.%20%60%60scheduling%20a%20doctor%27s%0Aappointment%20without%20a%20phone%27%27.%20While%20current%20approaches%20show%20encouraging%0Aresults%20using%20large%20language%20models%20%28LLMs%29%2C%20they%20are%20hindered%20by%20drawbacks%20such%0Aas%20costly%20API%20calls%20and%20reproducibility%20issues.%20In%20this%20paper%2C%20we%20advocate%0Aplanning%20using%20smaller%20language%20models.%20We%20present%20PlaSma%2C%20a%20novel%20two-pronged%0Aapproach%20to%20endow%20small%20language%20models%20with%20procedural%20knowledge%20and%0A%28constrained%29%20language%20planning%20capabilities.%20More%20concretely%2C%20we%20develop%0Asymbolic%20procedural%20knowledge%20distillation%20to%20enhance%20the%20commonsense%20knowledge%0Ain%20small%20language%20models%20and%20an%20inference-time%20algorithm%20to%20facilitate%20more%0Astructured%20and%20accurate%20reasoning.%20In%20addition%2C%20we%20introduce%20a%20new%20related%0Atask%2C%20Replanning%2C%20that%20requires%20a%20revision%20of%20a%20plan%20to%20cope%20with%20a%20constrained%0Asituation.%20In%20both%20the%20planning%20and%20replanning%20settings%2C%20we%20show%20that%0Aorders-of-magnitude%20smaller%20models%20%28770M-11B%20parameters%29%20can%20compete%20and%20often%0Asurpass%20their%20larger%20teacher%20models%27%20capabilities.%20Finally%2C%20we%20showcase%0Asuccessful%20application%20of%20PlaSma%20in%20an%20embodied%20environment%2C%20VirtualHome.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19472v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlaSma%253A%2520Making%2520Small%2520Language%2520Models%2520Better%2520Procedural%2520Knowledge%2520Models%250A%2520%2520for%2520%2528Counterfactual%2529%2520Planning%26entry.906535625%3DFaeze%2520Brahman%2520and%2520Chandra%2520Bhagavatula%2520and%2520Valentina%2520Pyatkin%2520and%2520Jena%2520D.%2520Hwang%2520and%2520Xiang%2520Lorraine%2520Li%2520and%2520Hirona%2520J.%2520Arai%2520and%2520Soumya%2520Sanyal%2520and%2520Keisuke%2520Sakaguchi%2520and%2520Xiang%2520Ren%2520and%2520Yejin%2520Choi%26entry.1292438233%3D%2520%2520Procedural%2520planning%252C%2520which%2520entails%2520decomposing%2520a%2520high-level%2520goal%2520into%2520a%250Asequence%2520of%2520temporally%2520ordered%2520steps%252C%2520is%2520an%2520important%2520yet%2520intricate%2520task%2520for%250Amachines.%2520It%2520involves%2520integrating%2520common-sense%2520knowledge%2520to%2520reason%2520about%250Acomplex%2520and%2520often%2520contextualized%2520situations%252C%2520e.g.%2520%2560%2560scheduling%2520a%2520doctor%2527s%250Aappointment%2520without%2520a%2520phone%2527%2527.%2520While%2520current%2520approaches%2520show%2520encouraging%250Aresults%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520they%2520are%2520hindered%2520by%2520drawbacks%2520such%250Aas%2520costly%2520API%2520calls%2520and%2520reproducibility%2520issues.%2520In%2520this%2520paper%252C%2520we%2520advocate%250Aplanning%2520using%2520smaller%2520language%2520models.%2520We%2520present%2520PlaSma%252C%2520a%2520novel%2520two-pronged%250Aapproach%2520to%2520endow%2520small%2520language%2520models%2520with%2520procedural%2520knowledge%2520and%250A%2528constrained%2529%2520language%2520planning%2520capabilities.%2520More%2520concretely%252C%2520we%2520develop%250Asymbolic%2520procedural%2520knowledge%2520distillation%2520to%2520enhance%2520the%2520commonsense%2520knowledge%250Ain%2520small%2520language%2520models%2520and%2520an%2520inference-time%2520algorithm%2520to%2520facilitate%2520more%250Astructured%2520and%2520accurate%2520reasoning.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520new%2520related%250Atask%252C%2520Replanning%252C%2520that%2520requires%2520a%2520revision%2520of%2520a%2520plan%2520to%2520cope%2520with%2520a%2520constrained%250Asituation.%2520In%2520both%2520the%2520planning%2520and%2520replanning%2520settings%252C%2520we%2520show%2520that%250Aorders-of-magnitude%2520smaller%2520models%2520%2528770M-11B%2520parameters%2529%2520can%2520compete%2520and%2520often%250Asurpass%2520their%2520larger%2520teacher%2520models%2527%2520capabilities.%2520Finally%252C%2520we%2520showcase%250Asuccessful%2520application%2520of%2520PlaSma%2520in%2520an%2520embodied%2520environment%252C%2520VirtualHome.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.19472v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlaSma%3A%20Making%20Small%20Language%20Models%20Better%20Procedural%20Knowledge%20Models%0A%20%20for%20%28Counterfactual%29%20Planning&entry.906535625=Faeze%20Brahman%20and%20Chandra%20Bhagavatula%20and%20Valentina%20Pyatkin%20and%20Jena%20D.%20Hwang%20and%20Xiang%20Lorraine%20Li%20and%20Hirona%20J.%20Arai%20and%20Soumya%20Sanyal%20and%20Keisuke%20Sakaguchi%20and%20Xiang%20Ren%20and%20Yejin%20Choi&entry.1292438233=%20%20Procedural%20planning%2C%20which%20entails%20decomposing%20a%20high-level%20goal%20into%20a%0Asequence%20of%20temporally%20ordered%20steps%2C%20is%20an%20important%20yet%20intricate%20task%20for%0Amachines.%20It%20involves%20integrating%20common-sense%20knowledge%20to%20reason%20about%0Acomplex%20and%20often%20contextualized%20situations%2C%20e.g.%20%60%60scheduling%20a%20doctor%27s%0Aappointment%20without%20a%20phone%27%27.%20While%20current%20approaches%20show%20encouraging%0Aresults%20using%20large%20language%20models%20%28LLMs%29%2C%20they%20are%20hindered%20by%20drawbacks%20such%0Aas%20costly%20API%20calls%20and%20reproducibility%20issues.%20In%20this%20paper%2C%20we%20advocate%0Aplanning%20using%20smaller%20language%20models.%20We%20present%20PlaSma%2C%20a%20novel%20two-pronged%0Aapproach%20to%20endow%20small%20language%20models%20with%20procedural%20knowledge%20and%0A%28constrained%29%20language%20planning%20capabilities.%20More%20concretely%2C%20we%20develop%0Asymbolic%20procedural%20knowledge%20distillation%20to%20enhance%20the%20commonsense%20knowledge%0Ain%20small%20language%20models%20and%20an%20inference-time%20algorithm%20to%20facilitate%20more%0Astructured%20and%20accurate%20reasoning.%20In%20addition%2C%20we%20introduce%20a%20new%20related%0Atask%2C%20Replanning%2C%20that%20requires%20a%20revision%20of%20a%20plan%20to%20cope%20with%20a%20constrained%0Asituation.%20In%20both%20the%20planning%20and%20replanning%20settings%2C%20we%20show%20that%0Aorders-of-magnitude%20smaller%20models%20%28770M-11B%20parameters%29%20can%20compete%20and%20often%0Asurpass%20their%20larger%20teacher%20models%27%20capabilities.%20Finally%2C%20we%20showcase%0Asuccessful%20application%20of%20PlaSma%20in%20an%20embodied%20environment%2C%20VirtualHome.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19472v3&entry.124074799=Read"},
{"title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions", "author": "Ninghan Zhong and Alessandro Potenza and Stephen L. Smith", "abstract": "  Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.\n", "link": "http://arxiv.org/abs/2409.11326v2", "date": "2024-09-18", "relevancy": 2.0351, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions&body=Title%3A%20Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions%0AAuthor%3A%20Ninghan%20Zhong%20and%20Alessandro%20Potenza%20and%20Stephen%20L.%20Smith%0AAbstract%3A%20%20%20Autonomous%20navigation%20in%20ice-covered%20waters%20poses%20significant%20challenges%20due%0Ato%20the%20frequent%20lack%20of%20viable%20collision-free%20trajectories.%20When%20complete%0Aobstacle%20avoidance%20is%20infeasible%2C%20it%20becomes%20imperative%20for%20the%20navigation%0Astrategy%20to%20minimize%20collisions.%20Additionally%2C%20the%20dynamic%20nature%20of%20ice%2C%20which%0Amoves%20in%20response%20to%20ship%20maneuvers%2C%20complicates%20the%20path%20planning%20process.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20deep%20learning%20model%20to%20estimate%0Athe%20coarse%20dynamics%20of%20ice%20movements%20triggered%20by%20ship%20actions%20through%0Aoccupancy%20estimation.%20To%20ensure%20real-time%20applicability%2C%20we%20propose%20a%20novel%0Aapproach%20that%20caches%20intermediate%20prediction%20results%20and%20seamlessly%20integrates%0Athe%20predictive%20model%20into%20a%20graph%20search%20planner.%20We%20evaluate%20the%20proposed%0Aplanner%20both%20in%20simulation%20and%20in%20a%20physical%20testbed%20against%20existing%0Aapproaches%20and%20show%20that%20our%20planner%20significantly%20reduces%20collisions%20with%20ice%0Awhen%20compared%20to%20the%20state-of-the-art.%20Codes%20and%20demos%20of%20this%20work%20are%0Aavailable%20at%20https%3A//github.com/IvanIZ/predictive-asv-planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Navigation%2520in%2520Ice-Covered%2520Waters%2520with%2520Learned%2520Predictions%2520on%250A%2520%2520Ship-Ice%2520Interactions%26entry.906535625%3DNinghan%2520Zhong%2520and%2520Alessandro%2520Potenza%2520and%2520Stephen%2520L.%2520Smith%26entry.1292438233%3D%2520%2520Autonomous%2520navigation%2520in%2520ice-covered%2520waters%2520poses%2520significant%2520challenges%2520due%250Ato%2520the%2520frequent%2520lack%2520of%2520viable%2520collision-free%2520trajectories.%2520When%2520complete%250Aobstacle%2520avoidance%2520is%2520infeasible%252C%2520it%2520becomes%2520imperative%2520for%2520the%2520navigation%250Astrategy%2520to%2520minimize%2520collisions.%2520Additionally%252C%2520the%2520dynamic%2520nature%2520of%2520ice%252C%2520which%250Amoves%2520in%2520response%2520to%2520ship%2520maneuvers%252C%2520complicates%2520the%2520path%2520planning%2520process.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520deep%2520learning%2520model%2520to%2520estimate%250Athe%2520coarse%2520dynamics%2520of%2520ice%2520movements%2520triggered%2520by%2520ship%2520actions%2520through%250Aoccupancy%2520estimation.%2520To%2520ensure%2520real-time%2520applicability%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520that%2520caches%2520intermediate%2520prediction%2520results%2520and%2520seamlessly%2520integrates%250Athe%2520predictive%2520model%2520into%2520a%2520graph%2520search%2520planner.%2520We%2520evaluate%2520the%2520proposed%250Aplanner%2520both%2520in%2520simulation%2520and%2520in%2520a%2520physical%2520testbed%2520against%2520existing%250Aapproaches%2520and%2520show%2520that%2520our%2520planner%2520significantly%2520reduces%2520collisions%2520with%2520ice%250Awhen%2520compared%2520to%2520the%2520state-of-the-art.%2520Codes%2520and%2520demos%2520of%2520this%2520work%2520are%250Aavailable%2520at%2520https%253A//github.com/IvanIZ/predictive-asv-planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Navigation%20in%20Ice-Covered%20Waters%20with%20Learned%20Predictions%20on%0A%20%20Ship-Ice%20Interactions&entry.906535625=Ninghan%20Zhong%20and%20Alessandro%20Potenza%20and%20Stephen%20L.%20Smith&entry.1292438233=%20%20Autonomous%20navigation%20in%20ice-covered%20waters%20poses%20significant%20challenges%20due%0Ato%20the%20frequent%20lack%20of%20viable%20collision-free%20trajectories.%20When%20complete%0Aobstacle%20avoidance%20is%20infeasible%2C%20it%20becomes%20imperative%20for%20the%20navigation%0Astrategy%20to%20minimize%20collisions.%20Additionally%2C%20the%20dynamic%20nature%20of%20ice%2C%20which%0Amoves%20in%20response%20to%20ship%20maneuvers%2C%20complicates%20the%20path%20planning%20process.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20novel%20deep%20learning%20model%20to%20estimate%0Athe%20coarse%20dynamics%20of%20ice%20movements%20triggered%20by%20ship%20actions%20through%0Aoccupancy%20estimation.%20To%20ensure%20real-time%20applicability%2C%20we%20propose%20a%20novel%0Aapproach%20that%20caches%20intermediate%20prediction%20results%20and%20seamlessly%20integrates%0Athe%20predictive%20model%20into%20a%20graph%20search%20planner.%20We%20evaluate%20the%20proposed%0Aplanner%20both%20in%20simulation%20and%20in%20a%20physical%20testbed%20against%20existing%0Aapproaches%20and%20show%20that%20our%20planner%20significantly%20reduces%20collisions%20with%20ice%0Awhen%20compared%20to%20the%20state-of-the-art.%20Codes%20and%20demos%20of%20this%20work%20are%0Aavailable%20at%20https%3A//github.com/IvanIZ/predictive-asv-planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11326v2&entry.124074799=Read"},
{"title": "Intraoperative Registration by Cross-Modal Inverse Neural Rendering", "author": "Maximilian Fehrentz and Mohammad Farid Azampour and Reuben Dorent and Hassan Rasheed and Colin Galvin and Alexandra Golby and William M. Wells and Sarah Frisken and Nassir Navab and Nazim Haouchine", "abstract": "  We present in this paper a novel approach for 3D/2D intraoperative\nregistration during neurosurgery via cross-modal inverse neural rendering. Our\napproach separates implicit neural representation into two components, handling\nanatomical structure preoperatively and appearance intraoperatively. This\ndisentanglement is achieved by controlling a Neural Radiance Field's appearance\nwith a multi-style hypernetwork. Once trained, the implicit neural\nrepresentation serves as a differentiable rendering engine, which can be used\nto estimate the surgical camera pose by minimizing the dissimilarity between\nits rendered images and the target intraoperative image. We tested our method\non retrospective patients' data from clinical cases, showing that our method\noutperforms state-of-the-art while meeting current clinical standards for\nregistration. Code and additional resources can be found at\nhttps://maxfehrentz.github.io/style-ngp/.\n", "link": "http://arxiv.org/abs/2409.11983v1", "date": "2024-09-18", "relevancy": 2.0347, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4987}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intraoperative%20Registration%20by%20Cross-Modal%20Inverse%20Neural%20Rendering&body=Title%3A%20Intraoperative%20Registration%20by%20Cross-Modal%20Inverse%20Neural%20Rendering%0AAuthor%3A%20Maximilian%20Fehrentz%20and%20Mohammad%20Farid%20Azampour%20and%20Reuben%20Dorent%20and%20Hassan%20Rasheed%20and%20Colin%20Galvin%20and%20Alexandra%20Golby%20and%20William%20M.%20Wells%20and%20Sarah%20Frisken%20and%20Nassir%20Navab%20and%20Nazim%20Haouchine%0AAbstract%3A%20%20%20We%20present%20in%20this%20paper%20a%20novel%20approach%20for%203D/2D%20intraoperative%0Aregistration%20during%20neurosurgery%20via%20cross-modal%20inverse%20neural%20rendering.%20Our%0Aapproach%20separates%20implicit%20neural%20representation%20into%20two%20components%2C%20handling%0Aanatomical%20structure%20preoperatively%20and%20appearance%20intraoperatively.%20This%0Adisentanglement%20is%20achieved%20by%20controlling%20a%20Neural%20Radiance%20Field%27s%20appearance%0Awith%20a%20multi-style%20hypernetwork.%20Once%20trained%2C%20the%20implicit%20neural%0Arepresentation%20serves%20as%20a%20differentiable%20rendering%20engine%2C%20which%20can%20be%20used%0Ato%20estimate%20the%20surgical%20camera%20pose%20by%20minimizing%20the%20dissimilarity%20between%0Aits%20rendered%20images%20and%20the%20target%20intraoperative%20image.%20We%20tested%20our%20method%0Aon%20retrospective%20patients%27%20data%20from%20clinical%20cases%2C%20showing%20that%20our%20method%0Aoutperforms%20state-of-the-art%20while%20meeting%20current%20clinical%20standards%20for%0Aregistration.%20Code%20and%20additional%20resources%20can%20be%20found%20at%0Ahttps%3A//maxfehrentz.github.io/style-ngp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntraoperative%2520Registration%2520by%2520Cross-Modal%2520Inverse%2520Neural%2520Rendering%26entry.906535625%3DMaximilian%2520Fehrentz%2520and%2520Mohammad%2520Farid%2520Azampour%2520and%2520Reuben%2520Dorent%2520and%2520Hassan%2520Rasheed%2520and%2520Colin%2520Galvin%2520and%2520Alexandra%2520Golby%2520and%2520William%2520M.%2520Wells%2520and%2520Sarah%2520Frisken%2520and%2520Nassir%2520Navab%2520and%2520Nazim%2520Haouchine%26entry.1292438233%3D%2520%2520We%2520present%2520in%2520this%2520paper%2520a%2520novel%2520approach%2520for%25203D/2D%2520intraoperative%250Aregistration%2520during%2520neurosurgery%2520via%2520cross-modal%2520inverse%2520neural%2520rendering.%2520Our%250Aapproach%2520separates%2520implicit%2520neural%2520representation%2520into%2520two%2520components%252C%2520handling%250Aanatomical%2520structure%2520preoperatively%2520and%2520appearance%2520intraoperatively.%2520This%250Adisentanglement%2520is%2520achieved%2520by%2520controlling%2520a%2520Neural%2520Radiance%2520Field%2527s%2520appearance%250Awith%2520a%2520multi-style%2520hypernetwork.%2520Once%2520trained%252C%2520the%2520implicit%2520neural%250Arepresentation%2520serves%2520as%2520a%2520differentiable%2520rendering%2520engine%252C%2520which%2520can%2520be%2520used%250Ato%2520estimate%2520the%2520surgical%2520camera%2520pose%2520by%2520minimizing%2520the%2520dissimilarity%2520between%250Aits%2520rendered%2520images%2520and%2520the%2520target%2520intraoperative%2520image.%2520We%2520tested%2520our%2520method%250Aon%2520retrospective%2520patients%2527%2520data%2520from%2520clinical%2520cases%252C%2520showing%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520while%2520meeting%2520current%2520clinical%2520standards%2520for%250Aregistration.%2520Code%2520and%2520additional%2520resources%2520can%2520be%2520found%2520at%250Ahttps%253A//maxfehrentz.github.io/style-ngp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intraoperative%20Registration%20by%20Cross-Modal%20Inverse%20Neural%20Rendering&entry.906535625=Maximilian%20Fehrentz%20and%20Mohammad%20Farid%20Azampour%20and%20Reuben%20Dorent%20and%20Hassan%20Rasheed%20and%20Colin%20Galvin%20and%20Alexandra%20Golby%20and%20William%20M.%20Wells%20and%20Sarah%20Frisken%20and%20Nassir%20Navab%20and%20Nazim%20Haouchine&entry.1292438233=%20%20We%20present%20in%20this%20paper%20a%20novel%20approach%20for%203D/2D%20intraoperative%0Aregistration%20during%20neurosurgery%20via%20cross-modal%20inverse%20neural%20rendering.%20Our%0Aapproach%20separates%20implicit%20neural%20representation%20into%20two%20components%2C%20handling%0Aanatomical%20structure%20preoperatively%20and%20appearance%20intraoperatively.%20This%0Adisentanglement%20is%20achieved%20by%20controlling%20a%20Neural%20Radiance%20Field%27s%20appearance%0Awith%20a%20multi-style%20hypernetwork.%20Once%20trained%2C%20the%20implicit%20neural%0Arepresentation%20serves%20as%20a%20differentiable%20rendering%20engine%2C%20which%20can%20be%20used%0Ato%20estimate%20the%20surgical%20camera%20pose%20by%20minimizing%20the%20dissimilarity%20between%0Aits%20rendered%20images%20and%20the%20target%20intraoperative%20image.%20We%20tested%20our%20method%0Aon%20retrospective%20patients%27%20data%20from%20clinical%20cases%2C%20showing%20that%20our%20method%0Aoutperforms%20state-of-the-art%20while%20meeting%20current%20clinical%20standards%20for%0Aregistration.%20Code%20and%20additional%20resources%20can%20be%20found%20at%0Ahttps%3A//maxfehrentz.github.io/style-ngp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11983v1&entry.124074799=Read"},
{"title": "Agglomerative Token Clustering", "author": "Joakim Bruslund Haurum and Sergio Escalera and Graham W. Taylor and Thomas B. Moeslund", "abstract": "  We present Agglomerative Token Clustering (ATC), a novel token merging method\nthat consistently outperforms previous token merging and pruning methods across\nimage classification, image synthesis, and object detection & segmentation\ntasks. ATC merges clusters through bottom-up hierarchical clustering, without\nthe introduction of extra learnable parameters. We find that ATC achieves\nstate-of-the-art performance across all tasks, and can even perform on par with\nprior state-of-the-art when applied off-the-shelf, i.e. without fine-tuning.\nATC is particularly effective when applied with low keep rates, where only a\nsmall fraction of tokens are kept and retaining task performance is especially\ndifficult.\n", "link": "http://arxiv.org/abs/2409.11923v1", "date": "2024-09-18", "relevancy": 2.0327, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agglomerative%20Token%20Clustering&body=Title%3A%20Agglomerative%20Token%20Clustering%0AAuthor%3A%20Joakim%20Bruslund%20Haurum%20and%20Sergio%20Escalera%20and%20Graham%20W.%20Taylor%20and%20Thomas%20B.%20Moeslund%0AAbstract%3A%20%20%20We%20present%20Agglomerative%20Token%20Clustering%20%28ATC%29%2C%20a%20novel%20token%20merging%20method%0Athat%20consistently%20outperforms%20previous%20token%20merging%20and%20pruning%20methods%20across%0Aimage%20classification%2C%20image%20synthesis%2C%20and%20object%20detection%20%26%20segmentation%0Atasks.%20ATC%20merges%20clusters%20through%20bottom-up%20hierarchical%20clustering%2C%20without%0Athe%20introduction%20of%20extra%20learnable%20parameters.%20We%20find%20that%20ATC%20achieves%0Astate-of-the-art%20performance%20across%20all%20tasks%2C%20and%20can%20even%20perform%20on%20par%20with%0Aprior%20state-of-the-art%20when%20applied%20off-the-shelf%2C%20i.e.%20without%20fine-tuning.%0AATC%20is%20particularly%20effective%20when%20applied%20with%20low%20keep%20rates%2C%20where%20only%20a%0Asmall%20fraction%20of%20tokens%20are%20kept%20and%20retaining%20task%20performance%20is%20especially%0Adifficult.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgglomerative%2520Token%2520Clustering%26entry.906535625%3DJoakim%2520Bruslund%2520Haurum%2520and%2520Sergio%2520Escalera%2520and%2520Graham%2520W.%2520Taylor%2520and%2520Thomas%2520B.%2520Moeslund%26entry.1292438233%3D%2520%2520We%2520present%2520Agglomerative%2520Token%2520Clustering%2520%2528ATC%2529%252C%2520a%2520novel%2520token%2520merging%2520method%250Athat%2520consistently%2520outperforms%2520previous%2520token%2520merging%2520and%2520pruning%2520methods%2520across%250Aimage%2520classification%252C%2520image%2520synthesis%252C%2520and%2520object%2520detection%2520%2526%2520segmentation%250Atasks.%2520ATC%2520merges%2520clusters%2520through%2520bottom-up%2520hierarchical%2520clustering%252C%2520without%250Athe%2520introduction%2520of%2520extra%2520learnable%2520parameters.%2520We%2520find%2520that%2520ATC%2520achieves%250Astate-of-the-art%2520performance%2520across%2520all%2520tasks%252C%2520and%2520can%2520even%2520perform%2520on%2520par%2520with%250Aprior%2520state-of-the-art%2520when%2520applied%2520off-the-shelf%252C%2520i.e.%2520without%2520fine-tuning.%250AATC%2520is%2520particularly%2520effective%2520when%2520applied%2520with%2520low%2520keep%2520rates%252C%2520where%2520only%2520a%250Asmall%2520fraction%2520of%2520tokens%2520are%2520kept%2520and%2520retaining%2520task%2520performance%2520is%2520especially%250Adifficult.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agglomerative%20Token%20Clustering&entry.906535625=Joakim%20Bruslund%20Haurum%20and%20Sergio%20Escalera%20and%20Graham%20W.%20Taylor%20and%20Thomas%20B.%20Moeslund&entry.1292438233=%20%20We%20present%20Agglomerative%20Token%20Clustering%20%28ATC%29%2C%20a%20novel%20token%20merging%20method%0Athat%20consistently%20outperforms%20previous%20token%20merging%20and%20pruning%20methods%20across%0Aimage%20classification%2C%20image%20synthesis%2C%20and%20object%20detection%20%26%20segmentation%0Atasks.%20ATC%20merges%20clusters%20through%20bottom-up%20hierarchical%20clustering%2C%20without%0Athe%20introduction%20of%20extra%20learnable%20parameters.%20We%20find%20that%20ATC%20achieves%0Astate-of-the-art%20performance%20across%20all%20tasks%2C%20and%20can%20even%20perform%20on%20par%20with%0Aprior%20state-of-the-art%20when%20applied%20off-the-shelf%2C%20i.e.%20without%20fine-tuning.%0AATC%20is%20particularly%20effective%20when%20applied%20with%20low%20keep%20rates%2C%20where%20only%20a%0Asmall%20fraction%20of%20tokens%20are%20kept%20and%20retaining%20task%20performance%20is%20especially%0Adifficult.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11923v1&entry.124074799=Read"},
{"title": "XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning\n  to Address Non-Stationarity", "author": "Jianye Xu and Omar Sobhy and Bassam Alrifaee", "abstract": "  Non-stationarity poses a fundamental challenge in Multi-Agent Reinforcement\nLearning (MARL), arising from agents simultaneously learning and altering their\npolicies. This creates a non-stationary environment from the perspective of\neach individual agent, often leading to suboptimal or even unconverged learning\noutcomes. We propose an open-source framework named XP-MARL, which augments\nMARL with auxiliary prioritization to address this challenge in cooperative\nsettings. XP-MARL is 1) founded upon our hypothesis that prioritizing agents\nand letting higher-priority agents establish their actions first would\nstabilize the learning process and thus mitigate non-stationarity and 2)\nenabled by our proposed mechanism called action propagation, where\nhigher-priority agents act first and communicate their actions, providing a\nmore stationary environment for others. Moreover, instead of using a predefined\nor heuristic priority assignment, XP-MARL learns priority-assignment policies\nwith an auxiliary MARL problem, leading to a joint learning scheme. Experiments\nin a motion-planning scenario involving Connected and Automated Vehicles (CAVs)\ndemonstrate that XP-MARL improves the safety of a baseline model by 84.4% and\noutperforms a state-of-the-art approach, which improves the baseline by only\n12.8%. Code: github.com/cas-lab-munich/sigmarl\n", "link": "http://arxiv.org/abs/2409.11852v1", "date": "2024-09-18", "relevancy": 2.0229, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5318}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XP-MARL%3A%20Auxiliary%20Prioritization%20in%20Multi-Agent%20Reinforcement%20Learning%0A%20%20to%20Address%20Non-Stationarity&body=Title%3A%20XP-MARL%3A%20Auxiliary%20Prioritization%20in%20Multi-Agent%20Reinforcement%20Learning%0A%20%20to%20Address%20Non-Stationarity%0AAuthor%3A%20Jianye%20Xu%20and%20Omar%20Sobhy%20and%20Bassam%20Alrifaee%0AAbstract%3A%20%20%20Non-stationarity%20poses%20a%20fundamental%20challenge%20in%20Multi-Agent%20Reinforcement%0ALearning%20%28MARL%29%2C%20arising%20from%20agents%20simultaneously%20learning%20and%20altering%20their%0Apolicies.%20This%20creates%20a%20non-stationary%20environment%20from%20the%20perspective%20of%0Aeach%20individual%20agent%2C%20often%20leading%20to%20suboptimal%20or%20even%20unconverged%20learning%0Aoutcomes.%20We%20propose%20an%20open-source%20framework%20named%20XP-MARL%2C%20which%20augments%0AMARL%20with%20auxiliary%20prioritization%20to%20address%20this%20challenge%20in%20cooperative%0Asettings.%20XP-MARL%20is%201%29%20founded%20upon%20our%20hypothesis%20that%20prioritizing%20agents%0Aand%20letting%20higher-priority%20agents%20establish%20their%20actions%20first%20would%0Astabilize%20the%20learning%20process%20and%20thus%20mitigate%20non-stationarity%20and%202%29%0Aenabled%20by%20our%20proposed%20mechanism%20called%20action%20propagation%2C%20where%0Ahigher-priority%20agents%20act%20first%20and%20communicate%20their%20actions%2C%20providing%20a%0Amore%20stationary%20environment%20for%20others.%20Moreover%2C%20instead%20of%20using%20a%20predefined%0Aor%20heuristic%20priority%20assignment%2C%20XP-MARL%20learns%20priority-assignment%20policies%0Awith%20an%20auxiliary%20MARL%20problem%2C%20leading%20to%20a%20joint%20learning%20scheme.%20Experiments%0Ain%20a%20motion-planning%20scenario%20involving%20Connected%20and%20Automated%20Vehicles%20%28CAVs%29%0Ademonstrate%20that%20XP-MARL%20improves%20the%20safety%20of%20a%20baseline%20model%20by%2084.4%25%20and%0Aoutperforms%20a%20state-of-the-art%20approach%2C%20which%20improves%20the%20baseline%20by%20only%0A12.8%25.%20Code%3A%20github.com/cas-lab-munich/sigmarl%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXP-MARL%253A%2520Auxiliary%2520Prioritization%2520in%2520Multi-Agent%2520Reinforcement%2520Learning%250A%2520%2520to%2520Address%2520Non-Stationarity%26entry.906535625%3DJianye%2520Xu%2520and%2520Omar%2520Sobhy%2520and%2520Bassam%2520Alrifaee%26entry.1292438233%3D%2520%2520Non-stationarity%2520poses%2520a%2520fundamental%2520challenge%2520in%2520Multi-Agent%2520Reinforcement%250ALearning%2520%2528MARL%2529%252C%2520arising%2520from%2520agents%2520simultaneously%2520learning%2520and%2520altering%2520their%250Apolicies.%2520This%2520creates%2520a%2520non-stationary%2520environment%2520from%2520the%2520perspective%2520of%250Aeach%2520individual%2520agent%252C%2520often%2520leading%2520to%2520suboptimal%2520or%2520even%2520unconverged%2520learning%250Aoutcomes.%2520We%2520propose%2520an%2520open-source%2520framework%2520named%2520XP-MARL%252C%2520which%2520augments%250AMARL%2520with%2520auxiliary%2520prioritization%2520to%2520address%2520this%2520challenge%2520in%2520cooperative%250Asettings.%2520XP-MARL%2520is%25201%2529%2520founded%2520upon%2520our%2520hypothesis%2520that%2520prioritizing%2520agents%250Aand%2520letting%2520higher-priority%2520agents%2520establish%2520their%2520actions%2520first%2520would%250Astabilize%2520the%2520learning%2520process%2520and%2520thus%2520mitigate%2520non-stationarity%2520and%25202%2529%250Aenabled%2520by%2520our%2520proposed%2520mechanism%2520called%2520action%2520propagation%252C%2520where%250Ahigher-priority%2520agents%2520act%2520first%2520and%2520communicate%2520their%2520actions%252C%2520providing%2520a%250Amore%2520stationary%2520environment%2520for%2520others.%2520Moreover%252C%2520instead%2520of%2520using%2520a%2520predefined%250Aor%2520heuristic%2520priority%2520assignment%252C%2520XP-MARL%2520learns%2520priority-assignment%2520policies%250Awith%2520an%2520auxiliary%2520MARL%2520problem%252C%2520leading%2520to%2520a%2520joint%2520learning%2520scheme.%2520Experiments%250Ain%2520a%2520motion-planning%2520scenario%2520involving%2520Connected%2520and%2520Automated%2520Vehicles%2520%2528CAVs%2529%250Ademonstrate%2520that%2520XP-MARL%2520improves%2520the%2520safety%2520of%2520a%2520baseline%2520model%2520by%252084.4%2525%2520and%250Aoutperforms%2520a%2520state-of-the-art%2520approach%252C%2520which%2520improves%2520the%2520baseline%2520by%2520only%250A12.8%2525.%2520Code%253A%2520github.com/cas-lab-munich/sigmarl%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XP-MARL%3A%20Auxiliary%20Prioritization%20in%20Multi-Agent%20Reinforcement%20Learning%0A%20%20to%20Address%20Non-Stationarity&entry.906535625=Jianye%20Xu%20and%20Omar%20Sobhy%20and%20Bassam%20Alrifaee&entry.1292438233=%20%20Non-stationarity%20poses%20a%20fundamental%20challenge%20in%20Multi-Agent%20Reinforcement%0ALearning%20%28MARL%29%2C%20arising%20from%20agents%20simultaneously%20learning%20and%20altering%20their%0Apolicies.%20This%20creates%20a%20non-stationary%20environment%20from%20the%20perspective%20of%0Aeach%20individual%20agent%2C%20often%20leading%20to%20suboptimal%20or%20even%20unconverged%20learning%0Aoutcomes.%20We%20propose%20an%20open-source%20framework%20named%20XP-MARL%2C%20which%20augments%0AMARL%20with%20auxiliary%20prioritization%20to%20address%20this%20challenge%20in%20cooperative%0Asettings.%20XP-MARL%20is%201%29%20founded%20upon%20our%20hypothesis%20that%20prioritizing%20agents%0Aand%20letting%20higher-priority%20agents%20establish%20their%20actions%20first%20would%0Astabilize%20the%20learning%20process%20and%20thus%20mitigate%20non-stationarity%20and%202%29%0Aenabled%20by%20our%20proposed%20mechanism%20called%20action%20propagation%2C%20where%0Ahigher-priority%20agents%20act%20first%20and%20communicate%20their%20actions%2C%20providing%20a%0Amore%20stationary%20environment%20for%20others.%20Moreover%2C%20instead%20of%20using%20a%20predefined%0Aor%20heuristic%20priority%20assignment%2C%20XP-MARL%20learns%20priority-assignment%20policies%0Awith%20an%20auxiliary%20MARL%20problem%2C%20leading%20to%20a%20joint%20learning%20scheme.%20Experiments%0Ain%20a%20motion-planning%20scenario%20involving%20Connected%20and%20Automated%20Vehicles%20%28CAVs%29%0Ademonstrate%20that%20XP-MARL%20improves%20the%20safety%20of%20a%20baseline%20model%20by%2084.4%25%20and%0Aoutperforms%20a%20state-of-the-art%20approach%2C%20which%20improves%20the%20baseline%20by%20only%0A12.8%25.%20Code%3A%20github.com/cas-lab-munich/sigmarl%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11852v1&entry.124074799=Read"},
{"title": "LMMCoDrive: Cooperative Driving with Large Multimodal Model", "author": "Haichao Liu and Ruoyu Yao and Zhenmin Huang and Shaojie Shen and Jun Ma", "abstract": "  To address the intricate challenges of decentralized cooperative scheduling\nand motion planning in Autonomous Mobility-on-Demand (AMoD) systems, this paper\nintroduces LMMCoDrive, a novel cooperative driving framework that leverages a\nLarge Multimodal Model (LMM) to enhance traffic efficiency in dynamic urban\nenvironments. This framework seamlessly integrates scheduling and motion\nplanning processes to ensure the effective operation of Cooperative Autonomous\nVehicles (CAVs). The spatial relationship between CAVs and passenger requests\nis abstracted into a Bird's-Eye View (BEV) to fully exploit the potential of\nthe LMM. Besides, trajectories are cautiously refined for each CAV while\nensuring collision avoidance through safety constraints. A decentralized\noptimization strategy, facilitated by the Alternating Direction Method of\nMultipliers (ADMM) within the LMM framework, is proposed to drive the graph\nevolution of CAVs. Simulation results demonstrate the pivotal role and\nsignificant impact of LMM in optimizing CAV scheduling and enhancing\ndecentralized cooperative optimization process for each vehicle. This marks a\nsubstantial stride towards achieving practical, efficient, and safe AMoD\nsystems that are poised to revolutionize urban transportation. The code is\navailable at https://github.com/henryhcliu/LMMCoDrive.\n", "link": "http://arxiv.org/abs/2409.11981v1", "date": "2024-09-18", "relevancy": 2.0198, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5012}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMMCoDrive%3A%20Cooperative%20Driving%20with%20Large%20Multimodal%20Model&body=Title%3A%20LMMCoDrive%3A%20Cooperative%20Driving%20with%20Large%20Multimodal%20Model%0AAuthor%3A%20Haichao%20Liu%20and%20Ruoyu%20Yao%20and%20Zhenmin%20Huang%20and%20Shaojie%20Shen%20and%20Jun%20Ma%0AAbstract%3A%20%20%20To%20address%20the%20intricate%20challenges%20of%20decentralized%20cooperative%20scheduling%0Aand%20motion%20planning%20in%20Autonomous%20Mobility-on-Demand%20%28AMoD%29%20systems%2C%20this%20paper%0Aintroduces%20LMMCoDrive%2C%20a%20novel%20cooperative%20driving%20framework%20that%20leverages%20a%0ALarge%20Multimodal%20Model%20%28LMM%29%20to%20enhance%20traffic%20efficiency%20in%20dynamic%20urban%0Aenvironments.%20This%20framework%20seamlessly%20integrates%20scheduling%20and%20motion%0Aplanning%20processes%20to%20ensure%20the%20effective%20operation%20of%20Cooperative%20Autonomous%0AVehicles%20%28CAVs%29.%20The%20spatial%20relationship%20between%20CAVs%20and%20passenger%20requests%0Ais%20abstracted%20into%20a%20Bird%27s-Eye%20View%20%28BEV%29%20to%20fully%20exploit%20the%20potential%20of%0Athe%20LMM.%20Besides%2C%20trajectories%20are%20cautiously%20refined%20for%20each%20CAV%20while%0Aensuring%20collision%20avoidance%20through%20safety%20constraints.%20A%20decentralized%0Aoptimization%20strategy%2C%20facilitated%20by%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20within%20the%20LMM%20framework%2C%20is%20proposed%20to%20drive%20the%20graph%0Aevolution%20of%20CAVs.%20Simulation%20results%20demonstrate%20the%20pivotal%20role%20and%0Asignificant%20impact%20of%20LMM%20in%20optimizing%20CAV%20scheduling%20and%20enhancing%0Adecentralized%20cooperative%20optimization%20process%20for%20each%20vehicle.%20This%20marks%20a%0Asubstantial%20stride%20towards%20achieving%20practical%2C%20efficient%2C%20and%20safe%20AMoD%0Asystems%20that%20are%20poised%20to%20revolutionize%20urban%20transportation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/henryhcliu/LMMCoDrive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMMCoDrive%253A%2520Cooperative%2520Driving%2520with%2520Large%2520Multimodal%2520Model%26entry.906535625%3DHaichao%2520Liu%2520and%2520Ruoyu%2520Yao%2520and%2520Zhenmin%2520Huang%2520and%2520Shaojie%2520Shen%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520intricate%2520challenges%2520of%2520decentralized%2520cooperative%2520scheduling%250Aand%2520motion%2520planning%2520in%2520Autonomous%2520Mobility-on-Demand%2520%2528AMoD%2529%2520systems%252C%2520this%2520paper%250Aintroduces%2520LMMCoDrive%252C%2520a%2520novel%2520cooperative%2520driving%2520framework%2520that%2520leverages%2520a%250ALarge%2520Multimodal%2520Model%2520%2528LMM%2529%2520to%2520enhance%2520traffic%2520efficiency%2520in%2520dynamic%2520urban%250Aenvironments.%2520This%2520framework%2520seamlessly%2520integrates%2520scheduling%2520and%2520motion%250Aplanning%2520processes%2520to%2520ensure%2520the%2520effective%2520operation%2520of%2520Cooperative%2520Autonomous%250AVehicles%2520%2528CAVs%2529.%2520The%2520spatial%2520relationship%2520between%2520CAVs%2520and%2520passenger%2520requests%250Ais%2520abstracted%2520into%2520a%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520to%2520fully%2520exploit%2520the%2520potential%2520of%250Athe%2520LMM.%2520Besides%252C%2520trajectories%2520are%2520cautiously%2520refined%2520for%2520each%2520CAV%2520while%250Aensuring%2520collision%2520avoidance%2520through%2520safety%2520constraints.%2520A%2520decentralized%250Aoptimization%2520strategy%252C%2520facilitated%2520by%2520the%2520Alternating%2520Direction%2520Method%2520of%250AMultipliers%2520%2528ADMM%2529%2520within%2520the%2520LMM%2520framework%252C%2520is%2520proposed%2520to%2520drive%2520the%2520graph%250Aevolution%2520of%2520CAVs.%2520Simulation%2520results%2520demonstrate%2520the%2520pivotal%2520role%2520and%250Asignificant%2520impact%2520of%2520LMM%2520in%2520optimizing%2520CAV%2520scheduling%2520and%2520enhancing%250Adecentralized%2520cooperative%2520optimization%2520process%2520for%2520each%2520vehicle.%2520This%2520marks%2520a%250Asubstantial%2520stride%2520towards%2520achieving%2520practical%252C%2520efficient%252C%2520and%2520safe%2520AMoD%250Asystems%2520that%2520are%2520poised%2520to%2520revolutionize%2520urban%2520transportation.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/henryhcliu/LMMCoDrive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMMCoDrive%3A%20Cooperative%20Driving%20with%20Large%20Multimodal%20Model&entry.906535625=Haichao%20Liu%20and%20Ruoyu%20Yao%20and%20Zhenmin%20Huang%20and%20Shaojie%20Shen%20and%20Jun%20Ma&entry.1292438233=%20%20To%20address%20the%20intricate%20challenges%20of%20decentralized%20cooperative%20scheduling%0Aand%20motion%20planning%20in%20Autonomous%20Mobility-on-Demand%20%28AMoD%29%20systems%2C%20this%20paper%0Aintroduces%20LMMCoDrive%2C%20a%20novel%20cooperative%20driving%20framework%20that%20leverages%20a%0ALarge%20Multimodal%20Model%20%28LMM%29%20to%20enhance%20traffic%20efficiency%20in%20dynamic%20urban%0Aenvironments.%20This%20framework%20seamlessly%20integrates%20scheduling%20and%20motion%0Aplanning%20processes%20to%20ensure%20the%20effective%20operation%20of%20Cooperative%20Autonomous%0AVehicles%20%28CAVs%29.%20The%20spatial%20relationship%20between%20CAVs%20and%20passenger%20requests%0Ais%20abstracted%20into%20a%20Bird%27s-Eye%20View%20%28BEV%29%20to%20fully%20exploit%20the%20potential%20of%0Athe%20LMM.%20Besides%2C%20trajectories%20are%20cautiously%20refined%20for%20each%20CAV%20while%0Aensuring%20collision%20avoidance%20through%20safety%20constraints.%20A%20decentralized%0Aoptimization%20strategy%2C%20facilitated%20by%20the%20Alternating%20Direction%20Method%20of%0AMultipliers%20%28ADMM%29%20within%20the%20LMM%20framework%2C%20is%20proposed%20to%20drive%20the%20graph%0Aevolution%20of%20CAVs.%20Simulation%20results%20demonstrate%20the%20pivotal%20role%20and%0Asignificant%20impact%20of%20LMM%20in%20optimizing%20CAV%20scheduling%20and%20enhancing%0Adecentralized%20cooperative%20optimization%20process%20for%20each%20vehicle.%20This%20marks%20a%0Asubstantial%20stride%20towards%20achieving%20practical%2C%20efficient%2C%20and%20safe%20AMoD%0Asystems%20that%20are%20poised%20to%20revolutionize%20urban%20transportation.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/henryhcliu/LMMCoDrive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11981v1&entry.124074799=Read"},
{"title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation", "author": "Kasra Hosseini and Thomas Kober and Josip Krapac and Roland Vollgraf and Weiwei Cheng and Ana Peleteiro Ramallo", "abstract": "  Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.\n", "link": "http://arxiv.org/abs/2409.11860v1", "date": "2024-09-18", "relevancy": 2.0185, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieve%2C%20Annotate%2C%20Evaluate%2C%20Repeat%3A%20Leveraging%20Multimodal%20LLMs%20for%0A%20%20Large-Scale%20Product%20Retrieval%20Evaluation&body=Title%3A%20Retrieve%2C%20Annotate%2C%20Evaluate%2C%20Repeat%3A%20Leveraging%20Multimodal%20LLMs%20for%0A%20%20Large-Scale%20Product%20Retrieval%20Evaluation%0AAuthor%3A%20Kasra%20Hosseini%20and%20Thomas%20Kober%20and%20Josip%20Krapac%20and%20Roland%20Vollgraf%20and%20Weiwei%20Cheng%20and%20Ana%20Peleteiro%20Ramallo%0AAbstract%3A%20%20%20Evaluating%20production-level%20retrieval%20systems%20at%20scale%20is%20a%20crucial%20yet%0Achallenging%20task%20due%20to%20the%20limited%20availability%20of%20a%20large%20pool%20of%0Awell-trained%20human%20annotators.%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20potential%0Ato%20address%20this%20scaling%20issue%20and%20offer%20a%20viable%20alternative%20to%20humans%20for%20the%0Abulk%20of%20annotation%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20assessing%0Athe%20product%20search%20engines%20in%20a%20large-scale%20e-commerce%20setting%2C%20leveraging%0AMultimodal%20LLMs%20for%20%28i%29%20generating%20tailored%20annotation%20guidelines%20for%0Aindividual%20queries%2C%20and%20%28ii%29%20conducting%20the%20subsequent%20annotation%20task.%20Our%0Amethod%2C%20validated%20through%20deployment%20on%20a%20large%20e-commerce%20platform%2C%0Ademonstrates%20comparable%20quality%20to%20human%20annotations%2C%20significantly%20reduces%0Atime%20and%20cost%2C%20facilitates%20rapid%20problem%20discovery%2C%20and%20provides%20an%20effective%0Asolution%20for%20production-level%20quality%20control%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieve%252C%2520Annotate%252C%2520Evaluate%252C%2520Repeat%253A%2520Leveraging%2520Multimodal%2520LLMs%2520for%250A%2520%2520Large-Scale%2520Product%2520Retrieval%2520Evaluation%26entry.906535625%3DKasra%2520Hosseini%2520and%2520Thomas%2520Kober%2520and%2520Josip%2520Krapac%2520and%2520Roland%2520Vollgraf%2520and%2520Weiwei%2520Cheng%2520and%2520Ana%2520Peleteiro%2520Ramallo%26entry.1292438233%3D%2520%2520Evaluating%2520production-level%2520retrieval%2520systems%2520at%2520scale%2520is%2520a%2520crucial%2520yet%250Achallenging%2520task%2520due%2520to%2520the%2520limited%2520availability%2520of%2520a%2520large%2520pool%2520of%250Awell-trained%2520human%2520annotators.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520the%2520potential%250Ato%2520address%2520this%2520scaling%2520issue%2520and%2520offer%2520a%2520viable%2520alternative%2520to%2520humans%2520for%2520the%250Abulk%2520of%2520annotation%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520framework%2520for%2520assessing%250Athe%2520product%2520search%2520engines%2520in%2520a%2520large-scale%2520e-commerce%2520setting%252C%2520leveraging%250AMultimodal%2520LLMs%2520for%2520%2528i%2529%2520generating%2520tailored%2520annotation%2520guidelines%2520for%250Aindividual%2520queries%252C%2520and%2520%2528ii%2529%2520conducting%2520the%2520subsequent%2520annotation%2520task.%2520Our%250Amethod%252C%2520validated%2520through%2520deployment%2520on%2520a%2520large%2520e-commerce%2520platform%252C%250Ademonstrates%2520comparable%2520quality%2520to%2520human%2520annotations%252C%2520significantly%2520reduces%250Atime%2520and%2520cost%252C%2520facilitates%2520rapid%2520problem%2520discovery%252C%2520and%2520provides%2520an%2520effective%250Asolution%2520for%2520production-level%2520quality%2520control%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieve%2C%20Annotate%2C%20Evaluate%2C%20Repeat%3A%20Leveraging%20Multimodal%20LLMs%20for%0A%20%20Large-Scale%20Product%20Retrieval%20Evaluation&entry.906535625=Kasra%20Hosseini%20and%20Thomas%20Kober%20and%20Josip%20Krapac%20and%20Roland%20Vollgraf%20and%20Weiwei%20Cheng%20and%20Ana%20Peleteiro%20Ramallo&entry.1292438233=%20%20Evaluating%20production-level%20retrieval%20systems%20at%20scale%20is%20a%20crucial%20yet%0Achallenging%20task%20due%20to%20the%20limited%20availability%20of%20a%20large%20pool%20of%0Awell-trained%20human%20annotators.%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20potential%0Ato%20address%20this%20scaling%20issue%20and%20offer%20a%20viable%20alternative%20to%20humans%20for%20the%0Abulk%20of%20annotation%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20framework%20for%20assessing%0Athe%20product%20search%20engines%20in%20a%20large-scale%20e-commerce%20setting%2C%20leveraging%0AMultimodal%20LLMs%20for%20%28i%29%20generating%20tailored%20annotation%20guidelines%20for%0Aindividual%20queries%2C%20and%20%28ii%29%20conducting%20the%20subsequent%20annotation%20task.%20Our%0Amethod%2C%20validated%20through%20deployment%20on%20a%20large%20e-commerce%20platform%2C%0Ademonstrates%20comparable%20quality%20to%20human%20annotations%2C%20significantly%20reduces%0Atime%20and%20cost%2C%20facilitates%20rapid%20problem%20discovery%2C%20and%20provides%20an%20effective%0Asolution%20for%20production-level%20quality%20control%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11860v1&entry.124074799=Read"},
{"title": "Bundle Adjustment in the Eager Mode", "author": "Zitong Zhan and Huan Xu and Zihang Fang and Xinpeng Wei and Yaoyu Hu and Chen Wang", "abstract": "  Bundle adjustment (BA) is a critical technique in various robotic\napplications, such as simultaneous localization and mapping (SLAM), augmented\nreality (AR), and photogrammetry. BA optimizes parameters such as camera poses\nand 3D landmarks to align them with observations. With the growing importance\nof deep learning in perception systems, there is an increasing need to\nintegrate BA with deep learning frameworks for enhanced reliability and\nperformance. However, widely-used C++-based BA frameworks, such as GTSAM,\ng$^2$o, and Ceres, lack native integration with modern deep learning libraries\nlike PyTorch. This limitation affects their flexibility, adaptability, ease of\ndebugging, and overall implementation efficiency. To address this gap, we\nintroduce an eager-mode BA framework seamlessly integrated with PyPose,\nproviding PyTorch-compatible interfaces with high efficiency. Our approach\nincludes GPU-accelerated, differentiable, and sparse operations designed for\n2nd-order optimization, Lie group and Lie algebra operations, and linear\nsolvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiency,\nachieving an average speedup of 18.5$\\times$, 22$\\times$, and 23$\\times$\ncompared to GTSAM, g$^2$o, and Ceres, respectively.\n", "link": "http://arxiv.org/abs/2409.12190v1", "date": "2024-09-18", "relevancy": 2.0157, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5122}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bundle%20Adjustment%20in%20the%20Eager%20Mode&body=Title%3A%20Bundle%20Adjustment%20in%20the%20Eager%20Mode%0AAuthor%3A%20Zitong%20Zhan%20and%20Huan%20Xu%20and%20Zihang%20Fang%20and%20Xinpeng%20Wei%20and%20Yaoyu%20Hu%20and%20Chen%20Wang%0AAbstract%3A%20%20%20Bundle%20adjustment%20%28BA%29%20is%20a%20critical%20technique%20in%20various%20robotic%0Aapplications%2C%20such%20as%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%2C%20augmented%0Areality%20%28AR%29%2C%20and%20photogrammetry.%20BA%20optimizes%20parameters%20such%20as%20camera%20poses%0Aand%203D%20landmarks%20to%20align%20them%20with%20observations.%20With%20the%20growing%20importance%0Aof%20deep%20learning%20in%20perception%20systems%2C%20there%20is%20an%20increasing%20need%20to%0Aintegrate%20BA%20with%20deep%20learning%20frameworks%20for%20enhanced%20reliability%20and%0Aperformance.%20However%2C%20widely-used%20C%2B%2B-based%20BA%20frameworks%2C%20such%20as%20GTSAM%2C%0Ag%24%5E2%24o%2C%20and%20Ceres%2C%20lack%20native%20integration%20with%20modern%20deep%20learning%20libraries%0Alike%20PyTorch.%20This%20limitation%20affects%20their%20flexibility%2C%20adaptability%2C%20ease%20of%0Adebugging%2C%20and%20overall%20implementation%20efficiency.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20an%20eager-mode%20BA%20framework%20seamlessly%20integrated%20with%20PyPose%2C%0Aproviding%20PyTorch-compatible%20interfaces%20with%20high%20efficiency.%20Our%20approach%0Aincludes%20GPU-accelerated%2C%20differentiable%2C%20and%20sparse%20operations%20designed%20for%0A2nd-order%20optimization%2C%20Lie%20group%20and%20Lie%20algebra%20operations%2C%20and%20linear%0Asolvers.%20Our%20eager-mode%20BA%20on%20GPU%20demonstrates%20substantial%20runtime%20efficiency%2C%0Aachieving%20an%20average%20speedup%20of%2018.5%24%5Ctimes%24%2C%2022%24%5Ctimes%24%2C%20and%2023%24%5Ctimes%24%0Acompared%20to%20GTSAM%2C%20g%24%5E2%24o%2C%20and%20Ceres%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12190v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBundle%2520Adjustment%2520in%2520the%2520Eager%2520Mode%26entry.906535625%3DZitong%2520Zhan%2520and%2520Huan%2520Xu%2520and%2520Zihang%2520Fang%2520and%2520Xinpeng%2520Wei%2520and%2520Yaoyu%2520Hu%2520and%2520Chen%2520Wang%26entry.1292438233%3D%2520%2520Bundle%2520adjustment%2520%2528BA%2529%2520is%2520a%2520critical%2520technique%2520in%2520various%2520robotic%250Aapplications%252C%2520such%2520as%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%252C%2520augmented%250Areality%2520%2528AR%2529%252C%2520and%2520photogrammetry.%2520BA%2520optimizes%2520parameters%2520such%2520as%2520camera%2520poses%250Aand%25203D%2520landmarks%2520to%2520align%2520them%2520with%2520observations.%2520With%2520the%2520growing%2520importance%250Aof%2520deep%2520learning%2520in%2520perception%2520systems%252C%2520there%2520is%2520an%2520increasing%2520need%2520to%250Aintegrate%2520BA%2520with%2520deep%2520learning%2520frameworks%2520for%2520enhanced%2520reliability%2520and%250Aperformance.%2520However%252C%2520widely-used%2520C%252B%252B-based%2520BA%2520frameworks%252C%2520such%2520as%2520GTSAM%252C%250Ag%2524%255E2%2524o%252C%2520and%2520Ceres%252C%2520lack%2520native%2520integration%2520with%2520modern%2520deep%2520learning%2520libraries%250Alike%2520PyTorch.%2520This%2520limitation%2520affects%2520their%2520flexibility%252C%2520adaptability%252C%2520ease%2520of%250Adebugging%252C%2520and%2520overall%2520implementation%2520efficiency.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520an%2520eager-mode%2520BA%2520framework%2520seamlessly%2520integrated%2520with%2520PyPose%252C%250Aproviding%2520PyTorch-compatible%2520interfaces%2520with%2520high%2520efficiency.%2520Our%2520approach%250Aincludes%2520GPU-accelerated%252C%2520differentiable%252C%2520and%2520sparse%2520operations%2520designed%2520for%250A2nd-order%2520optimization%252C%2520Lie%2520group%2520and%2520Lie%2520algebra%2520operations%252C%2520and%2520linear%250Asolvers.%2520Our%2520eager-mode%2520BA%2520on%2520GPU%2520demonstrates%2520substantial%2520runtime%2520efficiency%252C%250Aachieving%2520an%2520average%2520speedup%2520of%252018.5%2524%255Ctimes%2524%252C%252022%2524%255Ctimes%2524%252C%2520and%252023%2524%255Ctimes%2524%250Acompared%2520to%2520GTSAM%252C%2520g%2524%255E2%2524o%252C%2520and%2520Ceres%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12190v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bundle%20Adjustment%20in%20the%20Eager%20Mode&entry.906535625=Zitong%20Zhan%20and%20Huan%20Xu%20and%20Zihang%20Fang%20and%20Xinpeng%20Wei%20and%20Yaoyu%20Hu%20and%20Chen%20Wang&entry.1292438233=%20%20Bundle%20adjustment%20%28BA%29%20is%20a%20critical%20technique%20in%20various%20robotic%0Aapplications%2C%20such%20as%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%2C%20augmented%0Areality%20%28AR%29%2C%20and%20photogrammetry.%20BA%20optimizes%20parameters%20such%20as%20camera%20poses%0Aand%203D%20landmarks%20to%20align%20them%20with%20observations.%20With%20the%20growing%20importance%0Aof%20deep%20learning%20in%20perception%20systems%2C%20there%20is%20an%20increasing%20need%20to%0Aintegrate%20BA%20with%20deep%20learning%20frameworks%20for%20enhanced%20reliability%20and%0Aperformance.%20However%2C%20widely-used%20C%2B%2B-based%20BA%20frameworks%2C%20such%20as%20GTSAM%2C%0Ag%24%5E2%24o%2C%20and%20Ceres%2C%20lack%20native%20integration%20with%20modern%20deep%20learning%20libraries%0Alike%20PyTorch.%20This%20limitation%20affects%20their%20flexibility%2C%20adaptability%2C%20ease%20of%0Adebugging%2C%20and%20overall%20implementation%20efficiency.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20an%20eager-mode%20BA%20framework%20seamlessly%20integrated%20with%20PyPose%2C%0Aproviding%20PyTorch-compatible%20interfaces%20with%20high%20efficiency.%20Our%20approach%0Aincludes%20GPU-accelerated%2C%20differentiable%2C%20and%20sparse%20operations%20designed%20for%0A2nd-order%20optimization%2C%20Lie%20group%20and%20Lie%20algebra%20operations%2C%20and%20linear%0Asolvers.%20Our%20eager-mode%20BA%20on%20GPU%20demonstrates%20substantial%20runtime%20efficiency%2C%0Aachieving%20an%20average%20speedup%20of%2018.5%24%5Ctimes%24%2C%2022%24%5Ctimes%24%2C%20and%2023%24%5Ctimes%24%0Acompared%20to%20GTSAM%2C%20g%24%5E2%24o%2C%20and%20Ceres%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12190v1&entry.124074799=Read"},
{"title": "UKAN: Unbound Kolmogorov-Arnold Network Accompanied with Accelerated\n  Library", "author": "Alireza Moradzadeh and Lukasz Wawrzyniak and Miles Macklin and Saee G. Paliwal", "abstract": "  In this work, we present a GPU-accelerated library for the underlying\ncomponents of Kolmogorov-Arnold Networks (KANs), along with an algorithm to\neliminate bounded grids in KANs. The GPU-accelerated library reduces the\ncomputational complexity of Basis Spline (B-spline) evaluation by a factor of\n$\\mathcal{O}$(grid size) compared to existing codes, enabling batch computation\nfor large-scale learning. To overcome the limitations of traditional KANs, we\nintroduce Unbounded KANs (UKANs), which eliminate the need for a bounded grid\nand a fixed number of B-spline coefficients. To do so, we replace the KAN\nparameters (B-spline coefficients) with a coefficient generator (CG) model. The\ninputs to the CG model are designed based on the idea of an infinite symmetric\ngrid extending from negative infinity to positive infinity. The positional\nencoding of grid group, a sequential collection of B-spline grid indexes, is\nfed into the CG model, and coefficients are consumed by the efficient\nimplementation (matrix representations) of B-spline functions to generate\noutputs. We perform several experiments on regression, classification, and\ngenerative tasks, which are promising. In particular, UKAN does not require\ndata normalization or a bounded domain for evaluation. Additionally, our\nbenchmarking results indicate the superior memory and computational efficiency\nof our library compared to existing codes.\n", "link": "http://arxiv.org/abs/2408.11200v2", "date": "2024-09-18", "relevancy": 2.0009, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5116}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.498}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UKAN%3A%20Unbound%20Kolmogorov-Arnold%20Network%20Accompanied%20with%20Accelerated%0A%20%20Library&body=Title%3A%20UKAN%3A%20Unbound%20Kolmogorov-Arnold%20Network%20Accompanied%20with%20Accelerated%0A%20%20Library%0AAuthor%3A%20Alireza%20Moradzadeh%20and%20Lukasz%20Wawrzyniak%20and%20Miles%20Macklin%20and%20Saee%20G.%20Paliwal%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20GPU-accelerated%20library%20for%20the%20underlying%0Acomponents%20of%20Kolmogorov-Arnold%20Networks%20%28KANs%29%2C%20along%20with%20an%20algorithm%20to%0Aeliminate%20bounded%20grids%20in%20KANs.%20The%20GPU-accelerated%20library%20reduces%20the%0Acomputational%20complexity%20of%20Basis%20Spline%20%28B-spline%29%20evaluation%20by%20a%20factor%20of%0A%24%5Cmathcal%7BO%7D%24%28grid%20size%29%20compared%20to%20existing%20codes%2C%20enabling%20batch%20computation%0Afor%20large-scale%20learning.%20To%20overcome%20the%20limitations%20of%20traditional%20KANs%2C%20we%0Aintroduce%20Unbounded%20KANs%20%28UKANs%29%2C%20which%20eliminate%20the%20need%20for%20a%20bounded%20grid%0Aand%20a%20fixed%20number%20of%20B-spline%20coefficients.%20To%20do%20so%2C%20we%20replace%20the%20KAN%0Aparameters%20%28B-spline%20coefficients%29%20with%20a%20coefficient%20generator%20%28CG%29%20model.%20The%0Ainputs%20to%20the%20CG%20model%20are%20designed%20based%20on%20the%20idea%20of%20an%20infinite%20symmetric%0Agrid%20extending%20from%20negative%20infinity%20to%20positive%20infinity.%20The%20positional%0Aencoding%20of%20grid%20group%2C%20a%20sequential%20collection%20of%20B-spline%20grid%20indexes%2C%20is%0Afed%20into%20the%20CG%20model%2C%20and%20coefficients%20are%20consumed%20by%20the%20efficient%0Aimplementation%20%28matrix%20representations%29%20of%20B-spline%20functions%20to%20generate%0Aoutputs.%20We%20perform%20several%20experiments%20on%20regression%2C%20classification%2C%20and%0Agenerative%20tasks%2C%20which%20are%20promising.%20In%20particular%2C%20UKAN%20does%20not%20require%0Adata%20normalization%20or%20a%20bounded%20domain%20for%20evaluation.%20Additionally%2C%20our%0Abenchmarking%20results%20indicate%20the%20superior%20memory%20and%20computational%20efficiency%0Aof%20our%20library%20compared%20to%20existing%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUKAN%253A%2520Unbound%2520Kolmogorov-Arnold%2520Network%2520Accompanied%2520with%2520Accelerated%250A%2520%2520Library%26entry.906535625%3DAlireza%2520Moradzadeh%2520and%2520Lukasz%2520Wawrzyniak%2520and%2520Miles%2520Macklin%2520and%2520Saee%2520G.%2520Paliwal%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520GPU-accelerated%2520library%2520for%2520the%2520underlying%250Acomponents%2520of%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%252C%2520along%2520with%2520an%2520algorithm%2520to%250Aeliminate%2520bounded%2520grids%2520in%2520KANs.%2520The%2520GPU-accelerated%2520library%2520reduces%2520the%250Acomputational%2520complexity%2520of%2520Basis%2520Spline%2520%2528B-spline%2529%2520evaluation%2520by%2520a%2520factor%2520of%250A%2524%255Cmathcal%257BO%257D%2524%2528grid%2520size%2529%2520compared%2520to%2520existing%2520codes%252C%2520enabling%2520batch%2520computation%250Afor%2520large-scale%2520learning.%2520To%2520overcome%2520the%2520limitations%2520of%2520traditional%2520KANs%252C%2520we%250Aintroduce%2520Unbounded%2520KANs%2520%2528UKANs%2529%252C%2520which%2520eliminate%2520the%2520need%2520for%2520a%2520bounded%2520grid%250Aand%2520a%2520fixed%2520number%2520of%2520B-spline%2520coefficients.%2520To%2520do%2520so%252C%2520we%2520replace%2520the%2520KAN%250Aparameters%2520%2528B-spline%2520coefficients%2529%2520with%2520a%2520coefficient%2520generator%2520%2528CG%2529%2520model.%2520The%250Ainputs%2520to%2520the%2520CG%2520model%2520are%2520designed%2520based%2520on%2520the%2520idea%2520of%2520an%2520infinite%2520symmetric%250Agrid%2520extending%2520from%2520negative%2520infinity%2520to%2520positive%2520infinity.%2520The%2520positional%250Aencoding%2520of%2520grid%2520group%252C%2520a%2520sequential%2520collection%2520of%2520B-spline%2520grid%2520indexes%252C%2520is%250Afed%2520into%2520the%2520CG%2520model%252C%2520and%2520coefficients%2520are%2520consumed%2520by%2520the%2520efficient%250Aimplementation%2520%2528matrix%2520representations%2529%2520of%2520B-spline%2520functions%2520to%2520generate%250Aoutputs.%2520We%2520perform%2520several%2520experiments%2520on%2520regression%252C%2520classification%252C%2520and%250Agenerative%2520tasks%252C%2520which%2520are%2520promising.%2520In%2520particular%252C%2520UKAN%2520does%2520not%2520require%250Adata%2520normalization%2520or%2520a%2520bounded%2520domain%2520for%2520evaluation.%2520Additionally%252C%2520our%250Abenchmarking%2520results%2520indicate%2520the%2520superior%2520memory%2520and%2520computational%2520efficiency%250Aof%2520our%2520library%2520compared%2520to%2520existing%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UKAN%3A%20Unbound%20Kolmogorov-Arnold%20Network%20Accompanied%20with%20Accelerated%0A%20%20Library&entry.906535625=Alireza%20Moradzadeh%20and%20Lukasz%20Wawrzyniak%20and%20Miles%20Macklin%20and%20Saee%20G.%20Paliwal&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20GPU-accelerated%20library%20for%20the%20underlying%0Acomponents%20of%20Kolmogorov-Arnold%20Networks%20%28KANs%29%2C%20along%20with%20an%20algorithm%20to%0Aeliminate%20bounded%20grids%20in%20KANs.%20The%20GPU-accelerated%20library%20reduces%20the%0Acomputational%20complexity%20of%20Basis%20Spline%20%28B-spline%29%20evaluation%20by%20a%20factor%20of%0A%24%5Cmathcal%7BO%7D%24%28grid%20size%29%20compared%20to%20existing%20codes%2C%20enabling%20batch%20computation%0Afor%20large-scale%20learning.%20To%20overcome%20the%20limitations%20of%20traditional%20KANs%2C%20we%0Aintroduce%20Unbounded%20KANs%20%28UKANs%29%2C%20which%20eliminate%20the%20need%20for%20a%20bounded%20grid%0Aand%20a%20fixed%20number%20of%20B-spline%20coefficients.%20To%20do%20so%2C%20we%20replace%20the%20KAN%0Aparameters%20%28B-spline%20coefficients%29%20with%20a%20coefficient%20generator%20%28CG%29%20model.%20The%0Ainputs%20to%20the%20CG%20model%20are%20designed%20based%20on%20the%20idea%20of%20an%20infinite%20symmetric%0Agrid%20extending%20from%20negative%20infinity%20to%20positive%20infinity.%20The%20positional%0Aencoding%20of%20grid%20group%2C%20a%20sequential%20collection%20of%20B-spline%20grid%20indexes%2C%20is%0Afed%20into%20the%20CG%20model%2C%20and%20coefficients%20are%20consumed%20by%20the%20efficient%0Aimplementation%20%28matrix%20representations%29%20of%20B-spline%20functions%20to%20generate%0Aoutputs.%20We%20perform%20several%20experiments%20on%20regression%2C%20classification%2C%20and%0Agenerative%20tasks%2C%20which%20are%20promising.%20In%20particular%2C%20UKAN%20does%20not%20require%0Adata%20normalization%20or%20a%20bounded%20domain%20for%20evaluation.%20Additionally%2C%20our%0Abenchmarking%20results%20indicate%20the%20superior%20memory%20and%20computational%20efficiency%0Aof%20our%20library%20compared%20to%20existing%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11200v2&entry.124074799=Read"},
{"title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "author": "Tianle Gu and Kexin Huang and Ruilin Luo and Yuanqi Yao and Yujiu Yang and Yan Teng and Yingchun Wang", "abstract": "  Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.\n", "link": "http://arxiv.org/abs/2409.11844v1", "date": "2024-09-18", "relevancy": 1.9931, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEOW%3A%20MEMOry%20Supervised%20LLM%20Unlearning%20Via%20Inverted%20Facts&body=Title%3A%20MEOW%3A%20MEMOry%20Supervised%20LLM%20Unlearning%20Via%20Inverted%20Facts%0AAuthor%3A%20Tianle%20Gu%20and%20Kexin%20Huang%20and%20Ruilin%20Luo%20and%20Yuanqi%20Yao%20and%20Yujiu%20Yang%20and%20Yan%20Teng%20and%20Yingchun%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20memorize%20sensitive%20information%2C%20raising%0Aconcerns%20about%20potential%20misuse.%20LLM%20Unlearning%2C%20a%20post-hoc%20approach%20to%20remove%0Athis%20information%20from%20trained%20LLMs%2C%20offers%20a%20promising%20solution%20to%20mitigate%0Athese%20risks.%20However%2C%20previous%20practices%20face%20three%20key%20challenges%3A%201.%20Utility%3A%0Asuccessful%20unlearning%20often%20causes%20catastrophic%20collapse%20on%20unrelated%20tasks.%202.%0AEfficiency%3A%20many%20methods%20either%20involve%20adding%20similarly%20sized%20models%2C%20which%0Aslows%20down%20unlearning%20or%20inference%2C%20or%20require%20retain%20data%20that%20are%20difficult%0Ato%20obtain.%203.%20Robustness%3A%20even%20effective%20methods%20may%20still%20leak%20data%20via%0Aextraction%20techniques.%20To%20address%20these%20challenges%2C%20we%20propose%20MEOW%2C%20a%20simple%0Ayet%20effective%20gradient%20descent-based%20unlearning%20method.%20Specifically%2C%20we%20use%20an%0Aoffline%20LLM%20to%20generate%20a%20set%20of%20inverted%20facts.%20Then%2C%20we%20design%20a%20new%20metric%2C%0AMEMO%2C%20to%20quantify%20memorization%20in%20LLMs.%20Finally%2C%20based%20on%20the%20signals%20provided%0Aby%20MEMO%2C%20we%20select%20the%20most%20appropriate%20set%20of%20inverted%20facts%20and%20finetune%20the%0Amodel%20based%20on%20them.%20We%20evaluate%20MEOW%20on%20the%20commonly%20used%20unlearn%20benchmark%2C%0AToFU%2C%20with%20Llama2-7B-Chat%20and%20Phi-1.5B%2C%20and%20test%20it%20on%20both%20NLU%20and%20NLG%20tasks.%0AResults%20demonstrate%20significant%20improvement%20of%20MEOW%20in%20forget%20quality%20without%0Asubstantial%20loss%20in%20model%20utility.%20Meanwhile%2C%20MEOW%20does%20not%20exhibit%20significant%0Adegradation%20in%20NLU%20or%20NLG%20capabilities%2C%20and%20there%20is%20even%20a%20slight%20improvement%0Ain%20NLU%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEOW%253A%2520MEMOry%2520Supervised%2520LLM%2520Unlearning%2520Via%2520Inverted%2520Facts%26entry.906535625%3DTianle%2520Gu%2520and%2520Kexin%2520Huang%2520and%2520Ruilin%2520Luo%2520and%2520Yuanqi%2520Yao%2520and%2520Yujiu%2520Yang%2520and%2520Yan%2520Teng%2520and%2520Yingchun%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520memorize%2520sensitive%2520information%252C%2520raising%250Aconcerns%2520about%2520potential%2520misuse.%2520LLM%2520Unlearning%252C%2520a%2520post-hoc%2520approach%2520to%2520remove%250Athis%2520information%2520from%2520trained%2520LLMs%252C%2520offers%2520a%2520promising%2520solution%2520to%2520mitigate%250Athese%2520risks.%2520However%252C%2520previous%2520practices%2520face%2520three%2520key%2520challenges%253A%25201.%2520Utility%253A%250Asuccessful%2520unlearning%2520often%2520causes%2520catastrophic%2520collapse%2520on%2520unrelated%2520tasks.%25202.%250AEfficiency%253A%2520many%2520methods%2520either%2520involve%2520adding%2520similarly%2520sized%2520models%252C%2520which%250Aslows%2520down%2520unlearning%2520or%2520inference%252C%2520or%2520require%2520retain%2520data%2520that%2520are%2520difficult%250Ato%2520obtain.%25203.%2520Robustness%253A%2520even%2520effective%2520methods%2520may%2520still%2520leak%2520data%2520via%250Aextraction%2520techniques.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MEOW%252C%2520a%2520simple%250Ayet%2520effective%2520gradient%2520descent-based%2520unlearning%2520method.%2520Specifically%252C%2520we%2520use%2520an%250Aoffline%2520LLM%2520to%2520generate%2520a%2520set%2520of%2520inverted%2520facts.%2520Then%252C%2520we%2520design%2520a%2520new%2520metric%252C%250AMEMO%252C%2520to%2520quantify%2520memorization%2520in%2520LLMs.%2520Finally%252C%2520based%2520on%2520the%2520signals%2520provided%250Aby%2520MEMO%252C%2520we%2520select%2520the%2520most%2520appropriate%2520set%2520of%2520inverted%2520facts%2520and%2520finetune%2520the%250Amodel%2520based%2520on%2520them.%2520We%2520evaluate%2520MEOW%2520on%2520the%2520commonly%2520used%2520unlearn%2520benchmark%252C%250AToFU%252C%2520with%2520Llama2-7B-Chat%2520and%2520Phi-1.5B%252C%2520and%2520test%2520it%2520on%2520both%2520NLU%2520and%2520NLG%2520tasks.%250AResults%2520demonstrate%2520significant%2520improvement%2520of%2520MEOW%2520in%2520forget%2520quality%2520without%250Asubstantial%2520loss%2520in%2520model%2520utility.%2520Meanwhile%252C%2520MEOW%2520does%2520not%2520exhibit%2520significant%250Adegradation%2520in%2520NLU%2520or%2520NLG%2520capabilities%252C%2520and%2520there%2520is%2520even%2520a%2520slight%2520improvement%250Ain%2520NLU%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEOW%3A%20MEMOry%20Supervised%20LLM%20Unlearning%20Via%20Inverted%20Facts&entry.906535625=Tianle%20Gu%20and%20Kexin%20Huang%20and%20Ruilin%20Luo%20and%20Yuanqi%20Yao%20and%20Yujiu%20Yang%20and%20Yan%20Teng%20and%20Yingchun%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20memorize%20sensitive%20information%2C%20raising%0Aconcerns%20about%20potential%20misuse.%20LLM%20Unlearning%2C%20a%20post-hoc%20approach%20to%20remove%0Athis%20information%20from%20trained%20LLMs%2C%20offers%20a%20promising%20solution%20to%20mitigate%0Athese%20risks.%20However%2C%20previous%20practices%20face%20three%20key%20challenges%3A%201.%20Utility%3A%0Asuccessful%20unlearning%20often%20causes%20catastrophic%20collapse%20on%20unrelated%20tasks.%202.%0AEfficiency%3A%20many%20methods%20either%20involve%20adding%20similarly%20sized%20models%2C%20which%0Aslows%20down%20unlearning%20or%20inference%2C%20or%20require%20retain%20data%20that%20are%20difficult%0Ato%20obtain.%203.%20Robustness%3A%20even%20effective%20methods%20may%20still%20leak%20data%20via%0Aextraction%20techniques.%20To%20address%20these%20challenges%2C%20we%20propose%20MEOW%2C%20a%20simple%0Ayet%20effective%20gradient%20descent-based%20unlearning%20method.%20Specifically%2C%20we%20use%20an%0Aoffline%20LLM%20to%20generate%20a%20set%20of%20inverted%20facts.%20Then%2C%20we%20design%20a%20new%20metric%2C%0AMEMO%2C%20to%20quantify%20memorization%20in%20LLMs.%20Finally%2C%20based%20on%20the%20signals%20provided%0Aby%20MEMO%2C%20we%20select%20the%20most%20appropriate%20set%20of%20inverted%20facts%20and%20finetune%20the%0Amodel%20based%20on%20them.%20We%20evaluate%20MEOW%20on%20the%20commonly%20used%20unlearn%20benchmark%2C%0AToFU%2C%20with%20Llama2-7B-Chat%20and%20Phi-1.5B%2C%20and%20test%20it%20on%20both%20NLU%20and%20NLG%20tasks.%0AResults%20demonstrate%20significant%20improvement%20of%20MEOW%20in%20forget%20quality%20without%0Asubstantial%20loss%20in%20model%20utility.%20Meanwhile%2C%20MEOW%20does%20not%20exhibit%20significant%0Adegradation%20in%20NLU%20or%20NLG%20capabilities%2C%20and%20there%20is%20even%20a%20slight%20improvement%0Ain%20NLU%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11844v1&entry.124074799=Read"},
{"title": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase\n  Detection Models", "author": "Andrianos Michail and Simon Clematide and Juri Opitz", "abstract": "  The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe release paraphrasus, a benchmark designed for multi-dimensional assessment\nof paraphrase detection models and finer model selection. We find that\nparaphrase detection models under a fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\n", "link": "http://arxiv.org/abs/2409.12060v1", "date": "2024-09-18", "relevancy": 1.9878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARAPHRASUS%20%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Paraphrase%0A%20%20Detection%20Models&body=Title%3A%20PARAPHRASUS%20%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Paraphrase%0A%20%20Detection%20Models%0AAuthor%3A%20Andrianos%20Michail%20and%20Simon%20Clematide%20and%20Juri%20Opitz%0AAbstract%3A%20%20%20The%20task%20of%20determining%20whether%20two%20texts%20are%20paraphrases%20has%20long%20been%20a%0Achallenge%20in%20NLP.%20However%2C%20the%20prevailing%20notion%20of%20paraphrase%20is%20often%20quite%0Asimplistic%2C%20offering%20only%20a%20limited%20view%20of%20the%20vast%20spectrum%20of%20paraphrase%0Aphenomena.%20Indeed%2C%20we%20find%20that%20evaluating%20models%20in%20a%20paraphrase%20dataset%20can%0Aleave%20uncertainty%20about%20their%20true%20semantic%20understanding.%20To%20alleviate%20this%2C%0Awe%20release%20paraphrasus%2C%20a%20benchmark%20designed%20for%20multi-dimensional%20assessment%0Aof%20paraphrase%20detection%20models%20and%20finer%20model%20selection.%20We%20find%20that%0Aparaphrase%20detection%20models%20under%20a%20fine-grained%20evaluation%20lens%20exhibit%0Atrade-offs%20that%20cannot%20be%20captured%20through%20a%20single%20classification%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARAPHRASUS%2520%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520Paraphrase%250A%2520%2520Detection%2520Models%26entry.906535625%3DAndrianos%2520Michail%2520and%2520Simon%2520Clematide%2520and%2520Juri%2520Opitz%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520determining%2520whether%2520two%2520texts%2520are%2520paraphrases%2520has%2520long%2520been%2520a%250Achallenge%2520in%2520NLP.%2520However%252C%2520the%2520prevailing%2520notion%2520of%2520paraphrase%2520is%2520often%2520quite%250Asimplistic%252C%2520offering%2520only%2520a%2520limited%2520view%2520of%2520the%2520vast%2520spectrum%2520of%2520paraphrase%250Aphenomena.%2520Indeed%252C%2520we%2520find%2520that%2520evaluating%2520models%2520in%2520a%2520paraphrase%2520dataset%2520can%250Aleave%2520uncertainty%2520about%2520their%2520true%2520semantic%2520understanding.%2520To%2520alleviate%2520this%252C%250Awe%2520release%2520paraphrasus%252C%2520a%2520benchmark%2520designed%2520for%2520multi-dimensional%2520assessment%250Aof%2520paraphrase%2520detection%2520models%2520and%2520finer%2520model%2520selection.%2520We%2520find%2520that%250Aparaphrase%2520detection%2520models%2520under%2520a%2520fine-grained%2520evaluation%2520lens%2520exhibit%250Atrade-offs%2520that%2520cannot%2520be%2520captured%2520through%2520a%2520single%2520classification%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARAPHRASUS%20%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Paraphrase%0A%20%20Detection%20Models&entry.906535625=Andrianos%20Michail%20and%20Simon%20Clematide%20and%20Juri%20Opitz&entry.1292438233=%20%20The%20task%20of%20determining%20whether%20two%20texts%20are%20paraphrases%20has%20long%20been%20a%0Achallenge%20in%20NLP.%20However%2C%20the%20prevailing%20notion%20of%20paraphrase%20is%20often%20quite%0Asimplistic%2C%20offering%20only%20a%20limited%20view%20of%20the%20vast%20spectrum%20of%20paraphrase%0Aphenomena.%20Indeed%2C%20we%20find%20that%20evaluating%20models%20in%20a%20paraphrase%20dataset%20can%0Aleave%20uncertainty%20about%20their%20true%20semantic%20understanding.%20To%20alleviate%20this%2C%0Awe%20release%20paraphrasus%2C%20a%20benchmark%20designed%20for%20multi-dimensional%20assessment%0Aof%20paraphrase%20detection%20models%20and%20finer%20model%20selection.%20We%20find%20that%0Aparaphrase%20detection%20models%20under%20a%20fine-grained%20evaluation%20lens%20exhibit%0Atrade-offs%20that%20cannot%20be%20captured%20through%20a%20single%20classification%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12060v1&entry.124074799=Read"},
{"title": "A Robust Autoencoder Ensemble-Based Approach for Anomaly Detection in\n  Text", "author": "Jeremie Pantin and Christophe Marsala", "abstract": "  Anomaly detection (AD) is a fast growing and popular domain among established\napplications like vision and time series. We observe a rich literature for\nthese applications, but anomaly detection in text is only starting to blossom.\nRecently, self-supervised methods with self-attention mechanism have been the\nmost popular choice. While recent works have proposed a working ground for\nbuilding and benchmarking state of the art approaches, we propose two principal\ncontributions in this paper: contextual anomaly contamination and a novel\nensemble-based approach. Our method, Textual Anomaly Contamination (TAC),\nallows to contaminate inlier classes with either independent or contextual\nanomalies. In the literature, it appears that this distinction is not\nperformed. For finding contextual anomalies, we propose RoSAE, a Robust\nSubspace Local Recovery Autoencoder Ensemble. All autoencoders of the ensemble\npresent a different latent representation through local manifold learning.\nBenchmark shows that our approach outperforms recent works on both independent\nand contextual anomalies, while being more robust. We also provide 8 dataset\ncomparison instead of only relying to Reuters and 20 Newsgroups corpora.\n", "link": "http://arxiv.org/abs/2405.13031v2", "date": "2024-09-18", "relevancy": 1.9823, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5084}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4876}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20Autoencoder%20Ensemble-Based%20Approach%20for%20Anomaly%20Detection%20in%0A%20%20Text&body=Title%3A%20A%20Robust%20Autoencoder%20Ensemble-Based%20Approach%20for%20Anomaly%20Detection%20in%0A%20%20Text%0AAuthor%3A%20Jeremie%20Pantin%20and%20Christophe%20Marsala%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20is%20a%20fast%20growing%20and%20popular%20domain%20among%20established%0Aapplications%20like%20vision%20and%20time%20series.%20We%20observe%20a%20rich%20literature%20for%0Athese%20applications%2C%20but%20anomaly%20detection%20in%20text%20is%20only%20starting%20to%20blossom.%0ARecently%2C%20self-supervised%20methods%20with%20self-attention%20mechanism%20have%20been%20the%0Amost%20popular%20choice.%20While%20recent%20works%20have%20proposed%20a%20working%20ground%20for%0Abuilding%20and%20benchmarking%20state%20of%20the%20art%20approaches%2C%20we%20propose%20two%20principal%0Acontributions%20in%20this%20paper%3A%20contextual%20anomaly%20contamination%20and%20a%20novel%0Aensemble-based%20approach.%20Our%20method%2C%20Textual%20Anomaly%20Contamination%20%28TAC%29%2C%0Aallows%20to%20contaminate%20inlier%20classes%20with%20either%20independent%20or%20contextual%0Aanomalies.%20In%20the%20literature%2C%20it%20appears%20that%20this%20distinction%20is%20not%0Aperformed.%20For%20finding%20contextual%20anomalies%2C%20we%20propose%20RoSAE%2C%20a%20Robust%0ASubspace%20Local%20Recovery%20Autoencoder%20Ensemble.%20All%20autoencoders%20of%20the%20ensemble%0Apresent%20a%20different%20latent%20representation%20through%20local%20manifold%20learning.%0ABenchmark%20shows%20that%20our%20approach%20outperforms%20recent%20works%20on%20both%20independent%0Aand%20contextual%20anomalies%2C%20while%20being%20more%20robust.%20We%20also%20provide%208%20dataset%0Acomparison%20instead%20of%20only%20relying%20to%20Reuters%20and%2020%20Newsgroups%20corpora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520Autoencoder%2520Ensemble-Based%2520Approach%2520for%2520Anomaly%2520Detection%2520in%250A%2520%2520Text%26entry.906535625%3DJeremie%2520Pantin%2520and%2520Christophe%2520Marsala%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520%2528AD%2529%2520is%2520a%2520fast%2520growing%2520and%2520popular%2520domain%2520among%2520established%250Aapplications%2520like%2520vision%2520and%2520time%2520series.%2520We%2520observe%2520a%2520rich%2520literature%2520for%250Athese%2520applications%252C%2520but%2520anomaly%2520detection%2520in%2520text%2520is%2520only%2520starting%2520to%2520blossom.%250ARecently%252C%2520self-supervised%2520methods%2520with%2520self-attention%2520mechanism%2520have%2520been%2520the%250Amost%2520popular%2520choice.%2520While%2520recent%2520works%2520have%2520proposed%2520a%2520working%2520ground%2520for%250Abuilding%2520and%2520benchmarking%2520state%2520of%2520the%2520art%2520approaches%252C%2520we%2520propose%2520two%2520principal%250Acontributions%2520in%2520this%2520paper%253A%2520contextual%2520anomaly%2520contamination%2520and%2520a%2520novel%250Aensemble-based%2520approach.%2520Our%2520method%252C%2520Textual%2520Anomaly%2520Contamination%2520%2528TAC%2529%252C%250Aallows%2520to%2520contaminate%2520inlier%2520classes%2520with%2520either%2520independent%2520or%2520contextual%250Aanomalies.%2520In%2520the%2520literature%252C%2520it%2520appears%2520that%2520this%2520distinction%2520is%2520not%250Aperformed.%2520For%2520finding%2520contextual%2520anomalies%252C%2520we%2520propose%2520RoSAE%252C%2520a%2520Robust%250ASubspace%2520Local%2520Recovery%2520Autoencoder%2520Ensemble.%2520All%2520autoencoders%2520of%2520the%2520ensemble%250Apresent%2520a%2520different%2520latent%2520representation%2520through%2520local%2520manifold%2520learning.%250ABenchmark%2520shows%2520that%2520our%2520approach%2520outperforms%2520recent%2520works%2520on%2520both%2520independent%250Aand%2520contextual%2520anomalies%252C%2520while%2520being%2520more%2520robust.%2520We%2520also%2520provide%25208%2520dataset%250Acomparison%2520instead%2520of%2520only%2520relying%2520to%2520Reuters%2520and%252020%2520Newsgroups%2520corpora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20Autoencoder%20Ensemble-Based%20Approach%20for%20Anomaly%20Detection%20in%0A%20%20Text&entry.906535625=Jeremie%20Pantin%20and%20Christophe%20Marsala&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20is%20a%20fast%20growing%20and%20popular%20domain%20among%20established%0Aapplications%20like%20vision%20and%20time%20series.%20We%20observe%20a%20rich%20literature%20for%0Athese%20applications%2C%20but%20anomaly%20detection%20in%20text%20is%20only%20starting%20to%20blossom.%0ARecently%2C%20self-supervised%20methods%20with%20self-attention%20mechanism%20have%20been%20the%0Amost%20popular%20choice.%20While%20recent%20works%20have%20proposed%20a%20working%20ground%20for%0Abuilding%20and%20benchmarking%20state%20of%20the%20art%20approaches%2C%20we%20propose%20two%20principal%0Acontributions%20in%20this%20paper%3A%20contextual%20anomaly%20contamination%20and%20a%20novel%0Aensemble-based%20approach.%20Our%20method%2C%20Textual%20Anomaly%20Contamination%20%28TAC%29%2C%0Aallows%20to%20contaminate%20inlier%20classes%20with%20either%20independent%20or%20contextual%0Aanomalies.%20In%20the%20literature%2C%20it%20appears%20that%20this%20distinction%20is%20not%0Aperformed.%20For%20finding%20contextual%20anomalies%2C%20we%20propose%20RoSAE%2C%20a%20Robust%0ASubspace%20Local%20Recovery%20Autoencoder%20Ensemble.%20All%20autoencoders%20of%20the%20ensemble%0Apresent%20a%20different%20latent%20representation%20through%20local%20manifold%20learning.%0ABenchmark%20shows%20that%20our%20approach%20outperforms%20recent%20works%20on%20both%20independent%0Aand%20contextual%20anomalies%2C%20while%20being%20more%20robust.%20We%20also%20provide%208%20dataset%0Acomparison%20instead%20of%20only%20relying%20to%20Reuters%20and%2020%20Newsgroups%20corpora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13031v2&entry.124074799=Read"},
{"title": "High-Resolution Building and Road Detection from Sentinel-2", "author": "Wojciech Sirko and Emmanuel Asiedu Brempong and Juliana T. C. Marcos and Abigail Annkah and Abel Korme and Mohammed Alewi Hassen and Krishna Sapkota and Tomer Shekel and Abdoulaye Diack and Sella Nevo and Jason Hickey and John Quinn", "abstract": "  Mapping buildings and roads automatically with remote sensing typically\nrequires high-resolution imagery, which is expensive to obtain and often\nsparsely available. In this work we demonstrate how multiple 10 m resolution\nSentinel-2 images can be used to generate 50 cm resolution building and road\nsegmentation masks. This is done by training a `student' model with access to\nSentinel-2 images to reproduce the predictions of a `teacher' model which has\naccess to corresponding high-resolution imagery. While the predictions do not\nhave all the fine detail of the teacher model, we find that we are able to\nretain much of the performance: for building segmentation we achieve 79.0\\%\nmIoU, compared to the high-resolution teacher model accuracy of 85.5\\% mIoU. We\nalso describe two related methods that work on Sentinel-2 imagery: one for\ncounting individual buildings which achieves $R^2 = 0.91$ against true counts\nand one for predicting building height with 1.5 meter mean absolute error. This\nwork opens up new possibilities for using freely available Sentinel-2 imagery\nfor a range of tasks that previously could only be done with high-resolution\nsatellite imagery.\n", "link": "http://arxiv.org/abs/2310.11622v3", "date": "2024-09-18", "relevancy": 1.9796, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4996}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2&body=Title%3A%20High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2%0AAuthor%3A%20Wojciech%20Sirko%20and%20Emmanuel%20Asiedu%20Brempong%20and%20Juliana%20T.%20C.%20Marcos%20and%20Abigail%20Annkah%20and%20Abel%20Korme%20and%20Mohammed%20Alewi%20Hassen%20and%20Krishna%20Sapkota%20and%20Tomer%20Shekel%20and%20Abdoulaye%20Diack%20and%20Sella%20Nevo%20and%20Jason%20Hickey%20and%20John%20Quinn%0AAbstract%3A%20%20%20Mapping%20buildings%20and%20roads%20automatically%20with%20remote%20sensing%20typically%0Arequires%20high-resolution%20imagery%2C%20which%20is%20expensive%20to%20obtain%20and%20often%0Asparsely%20available.%20In%20this%20work%20we%20demonstrate%20how%20multiple%2010%20m%20resolution%0ASentinel-2%20images%20can%20be%20used%20to%20generate%2050%20cm%20resolution%20building%20and%20road%0Asegmentation%20masks.%20This%20is%20done%20by%20training%20a%20%60student%27%20model%20with%20access%20to%0ASentinel-2%20images%20to%20reproduce%20the%20predictions%20of%20a%20%60teacher%27%20model%20which%20has%0Aaccess%20to%20corresponding%20high-resolution%20imagery.%20While%20the%20predictions%20do%20not%0Ahave%20all%20the%20fine%20detail%20of%20the%20teacher%20model%2C%20we%20find%20that%20we%20are%20able%20to%0Aretain%20much%20of%20the%20performance%3A%20for%20building%20segmentation%20we%20achieve%2079.0%5C%25%0AmIoU%2C%20compared%20to%20the%20high-resolution%20teacher%20model%20accuracy%20of%2085.5%5C%25%20mIoU.%20We%0Aalso%20describe%20two%20related%20methods%20that%20work%20on%20Sentinel-2%20imagery%3A%20one%20for%0Acounting%20individual%20buildings%20which%20achieves%20%24R%5E2%20%3D%200.91%24%20against%20true%20counts%0Aand%20one%20for%20predicting%20building%20height%20with%201.5%20meter%20mean%20absolute%20error.%20This%0Awork%20opens%20up%20new%20possibilities%20for%20using%20freely%20available%20Sentinel-2%20imagery%0Afor%20a%20range%20of%20tasks%20that%20previously%20could%20only%20be%20done%20with%20high-resolution%0Asatellite%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Resolution%2520Building%2520and%2520Road%2520Detection%2520from%2520Sentinel-2%26entry.906535625%3DWojciech%2520Sirko%2520and%2520Emmanuel%2520Asiedu%2520Brempong%2520and%2520Juliana%2520T.%2520C.%2520Marcos%2520and%2520Abigail%2520Annkah%2520and%2520Abel%2520Korme%2520and%2520Mohammed%2520Alewi%2520Hassen%2520and%2520Krishna%2520Sapkota%2520and%2520Tomer%2520Shekel%2520and%2520Abdoulaye%2520Diack%2520and%2520Sella%2520Nevo%2520and%2520Jason%2520Hickey%2520and%2520John%2520Quinn%26entry.1292438233%3D%2520%2520Mapping%2520buildings%2520and%2520roads%2520automatically%2520with%2520remote%2520sensing%2520typically%250Arequires%2520high-resolution%2520imagery%252C%2520which%2520is%2520expensive%2520to%2520obtain%2520and%2520often%250Asparsely%2520available.%2520In%2520this%2520work%2520we%2520demonstrate%2520how%2520multiple%252010%2520m%2520resolution%250ASentinel-2%2520images%2520can%2520be%2520used%2520to%2520generate%252050%2520cm%2520resolution%2520building%2520and%2520road%250Asegmentation%2520masks.%2520This%2520is%2520done%2520by%2520training%2520a%2520%2560student%2527%2520model%2520with%2520access%2520to%250ASentinel-2%2520images%2520to%2520reproduce%2520the%2520predictions%2520of%2520a%2520%2560teacher%2527%2520model%2520which%2520has%250Aaccess%2520to%2520corresponding%2520high-resolution%2520imagery.%2520While%2520the%2520predictions%2520do%2520not%250Ahave%2520all%2520the%2520fine%2520detail%2520of%2520the%2520teacher%2520model%252C%2520we%2520find%2520that%2520we%2520are%2520able%2520to%250Aretain%2520much%2520of%2520the%2520performance%253A%2520for%2520building%2520segmentation%2520we%2520achieve%252079.0%255C%2525%250AmIoU%252C%2520compared%2520to%2520the%2520high-resolution%2520teacher%2520model%2520accuracy%2520of%252085.5%255C%2525%2520mIoU.%2520We%250Aalso%2520describe%2520two%2520related%2520methods%2520that%2520work%2520on%2520Sentinel-2%2520imagery%253A%2520one%2520for%250Acounting%2520individual%2520buildings%2520which%2520achieves%2520%2524R%255E2%2520%253D%25200.91%2524%2520against%2520true%2520counts%250Aand%2520one%2520for%2520predicting%2520building%2520height%2520with%25201.5%2520meter%2520mean%2520absolute%2520error.%2520This%250Awork%2520opens%2520up%2520new%2520possibilities%2520for%2520using%2520freely%2520available%2520Sentinel-2%2520imagery%250Afor%2520a%2520range%2520of%2520tasks%2520that%2520previously%2520could%2520only%2520be%2520done%2520with%2520high-resolution%250Asatellite%2520imagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Resolution%20Building%20and%20Road%20Detection%20from%20Sentinel-2&entry.906535625=Wojciech%20Sirko%20and%20Emmanuel%20Asiedu%20Brempong%20and%20Juliana%20T.%20C.%20Marcos%20and%20Abigail%20Annkah%20and%20Abel%20Korme%20and%20Mohammed%20Alewi%20Hassen%20and%20Krishna%20Sapkota%20and%20Tomer%20Shekel%20and%20Abdoulaye%20Diack%20and%20Sella%20Nevo%20and%20Jason%20Hickey%20and%20John%20Quinn&entry.1292438233=%20%20Mapping%20buildings%20and%20roads%20automatically%20with%20remote%20sensing%20typically%0Arequires%20high-resolution%20imagery%2C%20which%20is%20expensive%20to%20obtain%20and%20often%0Asparsely%20available.%20In%20this%20work%20we%20demonstrate%20how%20multiple%2010%20m%20resolution%0ASentinel-2%20images%20can%20be%20used%20to%20generate%2050%20cm%20resolution%20building%20and%20road%0Asegmentation%20masks.%20This%20is%20done%20by%20training%20a%20%60student%27%20model%20with%20access%20to%0ASentinel-2%20images%20to%20reproduce%20the%20predictions%20of%20a%20%60teacher%27%20model%20which%20has%0Aaccess%20to%20corresponding%20high-resolution%20imagery.%20While%20the%20predictions%20do%20not%0Ahave%20all%20the%20fine%20detail%20of%20the%20teacher%20model%2C%20we%20find%20that%20we%20are%20able%20to%0Aretain%20much%20of%20the%20performance%3A%20for%20building%20segmentation%20we%20achieve%2079.0%5C%25%0AmIoU%2C%20compared%20to%20the%20high-resolution%20teacher%20model%20accuracy%20of%2085.5%5C%25%20mIoU.%20We%0Aalso%20describe%20two%20related%20methods%20that%20work%20on%20Sentinel-2%20imagery%3A%20one%20for%0Acounting%20individual%20buildings%20which%20achieves%20%24R%5E2%20%3D%200.91%24%20against%20true%20counts%0Aand%20one%20for%20predicting%20building%20height%20with%201.5%20meter%20mean%20absolute%20error.%20This%0Awork%20opens%20up%20new%20possibilities%20for%20using%20freely%20available%20Sentinel-2%20imagery%0Afor%20a%20range%20of%20tasks%20that%20previously%20could%20only%20be%20done%20with%20high-resolution%0Asatellite%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11622v3&entry.124074799=Read"},
{"title": "Comparison of Two Augmentation Methods in Improving Detection Accuracy\n  of Hemarthrosis", "author": "Qianyu Fan", "abstract": "  With the increase of computing power, machine learning models in medical\nimaging have been introduced to help in rending medical diagnosis and\ninspection, like hemophilia, a rare disorder in which blood cannot clot\nnormally. Often, one of the bottlenecks of detecting hemophilia is the lack of\ndata available to train the algorithm to increase the accuracy. As a possible\nsolution, this research investigated whether introducing augmented data by data\nsynthesis or traditional augmentation techniques can improve model accuracy,\nhelping to diagnose the diseases. To tackle this research, features of\nultrasound images were extracted by the pre-trained VGG-16, and similarities\nwere compared by cosine similarity measure based on extracted features in\ndifferent distributions among real images, synthetic images, and augmentation\nimages (Real vs. Real, Syn vs. Syn, Real vs. Different Batches of Syn, Real vs.\nAugmentation Techniques). Model testing performance was investigated using\nEffientNet-B4 to recognize \"blood\" images with two augmentation methods. In\naddition, a gradient-weighted class activation mapping (Grad-CAM) visualization\nwas used to interpret the unexpected results like loss of accuracy. Synthetic\nand real images do not show high similarity, with a mean similarity score of\n0.4737. Synthetic batch 1 dataset and images by horizontal flip are more\nsimilar to the original images. Classic augmentation techniques and data\nsynthesis can improve model accuracy, and data by traditional augmentation\ntechniques have a better performance than synthetic data. In addition, the\nGrad-CAM heatmap figured out the loss of accuracy is due to a shift in the\ndomain. Overall, this research found that two augmentation methods, data\nsynthesis and traditional augmentation techniques, both can improve accuracy to\na certain extent to help to diagnose rare diseases.\n", "link": "http://arxiv.org/abs/2409.05225v2", "date": "2024-09-18", "relevancy": 1.9721, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4989}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4951}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparison%20of%20Two%20Augmentation%20Methods%20in%20Improving%20Detection%20Accuracy%0A%20%20of%20Hemarthrosis&body=Title%3A%20Comparison%20of%20Two%20Augmentation%20Methods%20in%20Improving%20Detection%20Accuracy%0A%20%20of%20Hemarthrosis%0AAuthor%3A%20Qianyu%20Fan%0AAbstract%3A%20%20%20With%20the%20increase%20of%20computing%20power%2C%20machine%20learning%20models%20in%20medical%0Aimaging%20have%20been%20introduced%20to%20help%20in%20rending%20medical%20diagnosis%20and%0Ainspection%2C%20like%20hemophilia%2C%20a%20rare%20disorder%20in%20which%20blood%20cannot%20clot%0Anormally.%20Often%2C%20one%20of%20the%20bottlenecks%20of%20detecting%20hemophilia%20is%20the%20lack%20of%0Adata%20available%20to%20train%20the%20algorithm%20to%20increase%20the%20accuracy.%20As%20a%20possible%0Asolution%2C%20this%20research%20investigated%20whether%20introducing%20augmented%20data%20by%20data%0Asynthesis%20or%20traditional%20augmentation%20techniques%20can%20improve%20model%20accuracy%2C%0Ahelping%20to%20diagnose%20the%20diseases.%20To%20tackle%20this%20research%2C%20features%20of%0Aultrasound%20images%20were%20extracted%20by%20the%20pre-trained%20VGG-16%2C%20and%20similarities%0Awere%20compared%20by%20cosine%20similarity%20measure%20based%20on%20extracted%20features%20in%0Adifferent%20distributions%20among%20real%20images%2C%20synthetic%20images%2C%20and%20augmentation%0Aimages%20%28Real%20vs.%20Real%2C%20Syn%20vs.%20Syn%2C%20Real%20vs.%20Different%20Batches%20of%20Syn%2C%20Real%20vs.%0AAugmentation%20Techniques%29.%20Model%20testing%20performance%20was%20investigated%20using%0AEffientNet-B4%20to%20recognize%20%22blood%22%20images%20with%20two%20augmentation%20methods.%20In%0Aaddition%2C%20a%20gradient-weighted%20class%20activation%20mapping%20%28Grad-CAM%29%20visualization%0Awas%20used%20to%20interpret%20the%20unexpected%20results%20like%20loss%20of%20accuracy.%20Synthetic%0Aand%20real%20images%20do%20not%20show%20high%20similarity%2C%20with%20a%20mean%20similarity%20score%20of%0A0.4737.%20Synthetic%20batch%201%20dataset%20and%20images%20by%20horizontal%20flip%20are%20more%0Asimilar%20to%20the%20original%20images.%20Classic%20augmentation%20techniques%20and%20data%0Asynthesis%20can%20improve%20model%20accuracy%2C%20and%20data%20by%20traditional%20augmentation%0Atechniques%20have%20a%20better%20performance%20than%20synthetic%20data.%20In%20addition%2C%20the%0AGrad-CAM%20heatmap%20figured%20out%20the%20loss%20of%20accuracy%20is%20due%20to%20a%20shift%20in%20the%0Adomain.%20Overall%2C%20this%20research%20found%20that%20two%20augmentation%20methods%2C%20data%0Asynthesis%20and%20traditional%20augmentation%20techniques%2C%20both%20can%20improve%20accuracy%20to%0Aa%20certain%20extent%20to%20help%20to%20diagnose%20rare%20diseases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparison%2520of%2520Two%2520Augmentation%2520Methods%2520in%2520Improving%2520Detection%2520Accuracy%250A%2520%2520of%2520Hemarthrosis%26entry.906535625%3DQianyu%2520Fan%26entry.1292438233%3D%2520%2520With%2520the%2520increase%2520of%2520computing%2520power%252C%2520machine%2520learning%2520models%2520in%2520medical%250Aimaging%2520have%2520been%2520introduced%2520to%2520help%2520in%2520rending%2520medical%2520diagnosis%2520and%250Ainspection%252C%2520like%2520hemophilia%252C%2520a%2520rare%2520disorder%2520in%2520which%2520blood%2520cannot%2520clot%250Anormally.%2520Often%252C%2520one%2520of%2520the%2520bottlenecks%2520of%2520detecting%2520hemophilia%2520is%2520the%2520lack%2520of%250Adata%2520available%2520to%2520train%2520the%2520algorithm%2520to%2520increase%2520the%2520accuracy.%2520As%2520a%2520possible%250Asolution%252C%2520this%2520research%2520investigated%2520whether%2520introducing%2520augmented%2520data%2520by%2520data%250Asynthesis%2520or%2520traditional%2520augmentation%2520techniques%2520can%2520improve%2520model%2520accuracy%252C%250Ahelping%2520to%2520diagnose%2520the%2520diseases.%2520To%2520tackle%2520this%2520research%252C%2520features%2520of%250Aultrasound%2520images%2520were%2520extracted%2520by%2520the%2520pre-trained%2520VGG-16%252C%2520and%2520similarities%250Awere%2520compared%2520by%2520cosine%2520similarity%2520measure%2520based%2520on%2520extracted%2520features%2520in%250Adifferent%2520distributions%2520among%2520real%2520images%252C%2520synthetic%2520images%252C%2520and%2520augmentation%250Aimages%2520%2528Real%2520vs.%2520Real%252C%2520Syn%2520vs.%2520Syn%252C%2520Real%2520vs.%2520Different%2520Batches%2520of%2520Syn%252C%2520Real%2520vs.%250AAugmentation%2520Techniques%2529.%2520Model%2520testing%2520performance%2520was%2520investigated%2520using%250AEffientNet-B4%2520to%2520recognize%2520%2522blood%2522%2520images%2520with%2520two%2520augmentation%2520methods.%2520In%250Aaddition%252C%2520a%2520gradient-weighted%2520class%2520activation%2520mapping%2520%2528Grad-CAM%2529%2520visualization%250Awas%2520used%2520to%2520interpret%2520the%2520unexpected%2520results%2520like%2520loss%2520of%2520accuracy.%2520Synthetic%250Aand%2520real%2520images%2520do%2520not%2520show%2520high%2520similarity%252C%2520with%2520a%2520mean%2520similarity%2520score%2520of%250A0.4737.%2520Synthetic%2520batch%25201%2520dataset%2520and%2520images%2520by%2520horizontal%2520flip%2520are%2520more%250Asimilar%2520to%2520the%2520original%2520images.%2520Classic%2520augmentation%2520techniques%2520and%2520data%250Asynthesis%2520can%2520improve%2520model%2520accuracy%252C%2520and%2520data%2520by%2520traditional%2520augmentation%250Atechniques%2520have%2520a%2520better%2520performance%2520than%2520synthetic%2520data.%2520In%2520addition%252C%2520the%250AGrad-CAM%2520heatmap%2520figured%2520out%2520the%2520loss%2520of%2520accuracy%2520is%2520due%2520to%2520a%2520shift%2520in%2520the%250Adomain.%2520Overall%252C%2520this%2520research%2520found%2520that%2520two%2520augmentation%2520methods%252C%2520data%250Asynthesis%2520and%2520traditional%2520augmentation%2520techniques%252C%2520both%2520can%2520improve%2520accuracy%2520to%250Aa%2520certain%2520extent%2520to%2520help%2520to%2520diagnose%2520rare%2520diseases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparison%20of%20Two%20Augmentation%20Methods%20in%20Improving%20Detection%20Accuracy%0A%20%20of%20Hemarthrosis&entry.906535625=Qianyu%20Fan&entry.1292438233=%20%20With%20the%20increase%20of%20computing%20power%2C%20machine%20learning%20models%20in%20medical%0Aimaging%20have%20been%20introduced%20to%20help%20in%20rending%20medical%20diagnosis%20and%0Ainspection%2C%20like%20hemophilia%2C%20a%20rare%20disorder%20in%20which%20blood%20cannot%20clot%0Anormally.%20Often%2C%20one%20of%20the%20bottlenecks%20of%20detecting%20hemophilia%20is%20the%20lack%20of%0Adata%20available%20to%20train%20the%20algorithm%20to%20increase%20the%20accuracy.%20As%20a%20possible%0Asolution%2C%20this%20research%20investigated%20whether%20introducing%20augmented%20data%20by%20data%0Asynthesis%20or%20traditional%20augmentation%20techniques%20can%20improve%20model%20accuracy%2C%0Ahelping%20to%20diagnose%20the%20diseases.%20To%20tackle%20this%20research%2C%20features%20of%0Aultrasound%20images%20were%20extracted%20by%20the%20pre-trained%20VGG-16%2C%20and%20similarities%0Awere%20compared%20by%20cosine%20similarity%20measure%20based%20on%20extracted%20features%20in%0Adifferent%20distributions%20among%20real%20images%2C%20synthetic%20images%2C%20and%20augmentation%0Aimages%20%28Real%20vs.%20Real%2C%20Syn%20vs.%20Syn%2C%20Real%20vs.%20Different%20Batches%20of%20Syn%2C%20Real%20vs.%0AAugmentation%20Techniques%29.%20Model%20testing%20performance%20was%20investigated%20using%0AEffientNet-B4%20to%20recognize%20%22blood%22%20images%20with%20two%20augmentation%20methods.%20In%0Aaddition%2C%20a%20gradient-weighted%20class%20activation%20mapping%20%28Grad-CAM%29%20visualization%0Awas%20used%20to%20interpret%20the%20unexpected%20results%20like%20loss%20of%20accuracy.%20Synthetic%0Aand%20real%20images%20do%20not%20show%20high%20similarity%2C%20with%20a%20mean%20similarity%20score%20of%0A0.4737.%20Synthetic%20batch%201%20dataset%20and%20images%20by%20horizontal%20flip%20are%20more%0Asimilar%20to%20the%20original%20images.%20Classic%20augmentation%20techniques%20and%20data%0Asynthesis%20can%20improve%20model%20accuracy%2C%20and%20data%20by%20traditional%20augmentation%0Atechniques%20have%20a%20better%20performance%20than%20synthetic%20data.%20In%20addition%2C%20the%0AGrad-CAM%20heatmap%20figured%20out%20the%20loss%20of%20accuracy%20is%20due%20to%20a%20shift%20in%20the%0Adomain.%20Overall%2C%20this%20research%20found%20that%20two%20augmentation%20methods%2C%20data%0Asynthesis%20and%20traditional%20augmentation%20techniques%2C%20both%20can%20improve%20accuracy%20to%0Aa%20certain%20extent%20to%20help%20to%20diagnose%20rare%20diseases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05225v2&entry.124074799=Read"},
{"title": "Skill matching at scale: freelancer-project alignment for efficient\n  multilingual candidate retrieval", "author": "Warren Jouanneau and Marc Palyart and Emma Jouffroy", "abstract": "  Finding the perfect match between a job proposal and a set of freelancers is\nnot an easy task to perform at scale, especially in multiple languages. In this\npaper, we propose a novel neural retriever architecture that tackles this\nproblem in a multilingual setting. Our method encodes project descriptions and\nfreelancer profiles by leveraging pre-trained multilingual language models. The\nlatter are used as backbone for a custom transformer architecture that aims to\nkeep the structure of the profiles and project. This model is trained with a\ncontrastive loss on historical data. Thanks to several experiments, we show\nthat this approach effectively captures skill matching similarity and\nfacilitates efficient matching, outperforming traditional methods.\n", "link": "http://arxiv.org/abs/2409.12097v1", "date": "2024-09-18", "relevancy": 1.9599, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5205}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skill%20matching%20at%20scale%3A%20freelancer-project%20alignment%20for%20efficient%0A%20%20multilingual%20candidate%20retrieval&body=Title%3A%20Skill%20matching%20at%20scale%3A%20freelancer-project%20alignment%20for%20efficient%0A%20%20multilingual%20candidate%20retrieval%0AAuthor%3A%20Warren%20Jouanneau%20and%20Marc%20Palyart%20and%20Emma%20Jouffroy%0AAbstract%3A%20%20%20Finding%20the%20perfect%20match%20between%20a%20job%20proposal%20and%20a%20set%20of%20freelancers%20is%0Anot%20an%20easy%20task%20to%20perform%20at%20scale%2C%20especially%20in%20multiple%20languages.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20neural%20retriever%20architecture%20that%20tackles%20this%0Aproblem%20in%20a%20multilingual%20setting.%20Our%20method%20encodes%20project%20descriptions%20and%0Afreelancer%20profiles%20by%20leveraging%20pre-trained%20multilingual%20language%20models.%20The%0Alatter%20are%20used%20as%20backbone%20for%20a%20custom%20transformer%20architecture%20that%20aims%20to%0Akeep%20the%20structure%20of%20the%20profiles%20and%20project.%20This%20model%20is%20trained%20with%20a%0Acontrastive%20loss%20on%20historical%20data.%20Thanks%20to%20several%20experiments%2C%20we%20show%0Athat%20this%20approach%20effectively%20captures%20skill%20matching%20similarity%20and%0Afacilitates%20efficient%20matching%2C%20outperforming%20traditional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkill%2520matching%2520at%2520scale%253A%2520freelancer-project%2520alignment%2520for%2520efficient%250A%2520%2520multilingual%2520candidate%2520retrieval%26entry.906535625%3DWarren%2520Jouanneau%2520and%2520Marc%2520Palyart%2520and%2520Emma%2520Jouffroy%26entry.1292438233%3D%2520%2520Finding%2520the%2520perfect%2520match%2520between%2520a%2520job%2520proposal%2520and%2520a%2520set%2520of%2520freelancers%2520is%250Anot%2520an%2520easy%2520task%2520to%2520perform%2520at%2520scale%252C%2520especially%2520in%2520multiple%2520languages.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520neural%2520retriever%2520architecture%2520that%2520tackles%2520this%250Aproblem%2520in%2520a%2520multilingual%2520setting.%2520Our%2520method%2520encodes%2520project%2520descriptions%2520and%250Afreelancer%2520profiles%2520by%2520leveraging%2520pre-trained%2520multilingual%2520language%2520models.%2520The%250Alatter%2520are%2520used%2520as%2520backbone%2520for%2520a%2520custom%2520transformer%2520architecture%2520that%2520aims%2520to%250Akeep%2520the%2520structure%2520of%2520the%2520profiles%2520and%2520project.%2520This%2520model%2520is%2520trained%2520with%2520a%250Acontrastive%2520loss%2520on%2520historical%2520data.%2520Thanks%2520to%2520several%2520experiments%252C%2520we%2520show%250Athat%2520this%2520approach%2520effectively%2520captures%2520skill%2520matching%2520similarity%2520and%250Afacilitates%2520efficient%2520matching%252C%2520outperforming%2520traditional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skill%20matching%20at%20scale%3A%20freelancer-project%20alignment%20for%20efficient%0A%20%20multilingual%20candidate%20retrieval&entry.906535625=Warren%20Jouanneau%20and%20Marc%20Palyart%20and%20Emma%20Jouffroy&entry.1292438233=%20%20Finding%20the%20perfect%20match%20between%20a%20job%20proposal%20and%20a%20set%20of%20freelancers%20is%0Anot%20an%20easy%20task%20to%20perform%20at%20scale%2C%20especially%20in%20multiple%20languages.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20neural%20retriever%20architecture%20that%20tackles%20this%0Aproblem%20in%20a%20multilingual%20setting.%20Our%20method%20encodes%20project%20descriptions%20and%0Afreelancer%20profiles%20by%20leveraging%20pre-trained%20multilingual%20language%20models.%20The%0Alatter%20are%20used%20as%20backbone%20for%20a%20custom%20transformer%20architecture%20that%20aims%20to%0Akeep%20the%20structure%20of%20the%20profiles%20and%20project.%20This%20model%20is%20trained%20with%20a%0Acontrastive%20loss%20on%20historical%20data.%20Thanks%20to%20several%20experiments%2C%20we%20show%0Athat%20this%20approach%20effectively%20captures%20skill%20matching%20similarity%20and%0Afacilitates%20efficient%20matching%2C%20outperforming%20traditional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12097v1&entry.124074799=Read"},
{"title": "The Impact of Element Ordering on LM Agent Performance", "author": "Wayne Chi and Ameet Talwalkar and Chris Donahue", "abstract": "  There has been a surge of interest in language model agents that can navigate\nvirtual environments such as the web or desktop. To navigate such environments,\nagents benefit from information on the various elements (e.g., buttons, text,\nor images) present. It remains unclear which element attributes have the\ngreatest impact on agent performance, especially in environments that only\nprovide a graphical representation (i.e., pixels). Here we find that the\nordering in which elements are presented to the language model is surprisingly\nimpactful--randomizing element ordering in a webpage degrades agent performance\ncomparably to removing all visible text from an agent's state representation.\nWhile a webpage provides a hierarchical ordering of elements, there is no such\nordering when parsing elements directly from pixels. Moreover, as tasks become\nmore challenging and models more sophisticated, our experiments suggest that\nthe impact of ordering increases. Finding an effective ordering is non-trivial.\nWe investigate the impact of various element ordering methods in web and\ndesktop environments. We find that dimensionality reduction provides a viable\nordering for pixel-only environments. We train a UI element detection model to\nderive elements from pixels and apply our findings to an agent\nbenchmark--OmniACT--where we only have access to pixels. Our method completes\nmore than two times as many tasks on average relative to the previous\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2409.12089v1", "date": "2024-09-18", "relevancy": 1.9597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Element%20Ordering%20on%20LM%20Agent%20Performance&body=Title%3A%20The%20Impact%20of%20Element%20Ordering%20on%20LM%20Agent%20Performance%0AAuthor%3A%20Wayne%20Chi%20and%20Ameet%20Talwalkar%20and%20Chris%20Donahue%0AAbstract%3A%20%20%20There%20has%20been%20a%20surge%20of%20interest%20in%20language%20model%20agents%20that%20can%20navigate%0Avirtual%20environments%20such%20as%20the%20web%20or%20desktop.%20To%20navigate%20such%20environments%2C%0Aagents%20benefit%20from%20information%20on%20the%20various%20elements%20%28e.g.%2C%20buttons%2C%20text%2C%0Aor%20images%29%20present.%20It%20remains%20unclear%20which%20element%20attributes%20have%20the%0Agreatest%20impact%20on%20agent%20performance%2C%20especially%20in%20environments%20that%20only%0Aprovide%20a%20graphical%20representation%20%28i.e.%2C%20pixels%29.%20Here%20we%20find%20that%20the%0Aordering%20in%20which%20elements%20are%20presented%20to%20the%20language%20model%20is%20surprisingly%0Aimpactful--randomizing%20element%20ordering%20in%20a%20webpage%20degrades%20agent%20performance%0Acomparably%20to%20removing%20all%20visible%20text%20from%20an%20agent%27s%20state%20representation.%0AWhile%20a%20webpage%20provides%20a%20hierarchical%20ordering%20of%20elements%2C%20there%20is%20no%20such%0Aordering%20when%20parsing%20elements%20directly%20from%20pixels.%20Moreover%2C%20as%20tasks%20become%0Amore%20challenging%20and%20models%20more%20sophisticated%2C%20our%20experiments%20suggest%20that%0Athe%20impact%20of%20ordering%20increases.%20Finding%20an%20effective%20ordering%20is%20non-trivial.%0AWe%20investigate%20the%20impact%20of%20various%20element%20ordering%20methods%20in%20web%20and%0Adesktop%20environments.%20We%20find%20that%20dimensionality%20reduction%20provides%20a%20viable%0Aordering%20for%20pixel-only%20environments.%20We%20train%20a%20UI%20element%20detection%20model%20to%0Aderive%20elements%20from%20pixels%20and%20apply%20our%20findings%20to%20an%20agent%0Abenchmark--OmniACT--where%20we%20only%20have%20access%20to%20pixels.%20Our%20method%20completes%0Amore%20than%20two%20times%20as%20many%20tasks%20on%20average%20relative%20to%20the%20previous%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Element%2520Ordering%2520on%2520LM%2520Agent%2520Performance%26entry.906535625%3DWayne%2520Chi%2520and%2520Ameet%2520Talwalkar%2520and%2520Chris%2520Donahue%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520surge%2520of%2520interest%2520in%2520language%2520model%2520agents%2520that%2520can%2520navigate%250Avirtual%2520environments%2520such%2520as%2520the%2520web%2520or%2520desktop.%2520To%2520navigate%2520such%2520environments%252C%250Aagents%2520benefit%2520from%2520information%2520on%2520the%2520various%2520elements%2520%2528e.g.%252C%2520buttons%252C%2520text%252C%250Aor%2520images%2529%2520present.%2520It%2520remains%2520unclear%2520which%2520element%2520attributes%2520have%2520the%250Agreatest%2520impact%2520on%2520agent%2520performance%252C%2520especially%2520in%2520environments%2520that%2520only%250Aprovide%2520a%2520graphical%2520representation%2520%2528i.e.%252C%2520pixels%2529.%2520Here%2520we%2520find%2520that%2520the%250Aordering%2520in%2520which%2520elements%2520are%2520presented%2520to%2520the%2520language%2520model%2520is%2520surprisingly%250Aimpactful--randomizing%2520element%2520ordering%2520in%2520a%2520webpage%2520degrades%2520agent%2520performance%250Acomparably%2520to%2520removing%2520all%2520visible%2520text%2520from%2520an%2520agent%2527s%2520state%2520representation.%250AWhile%2520a%2520webpage%2520provides%2520a%2520hierarchical%2520ordering%2520of%2520elements%252C%2520there%2520is%2520no%2520such%250Aordering%2520when%2520parsing%2520elements%2520directly%2520from%2520pixels.%2520Moreover%252C%2520as%2520tasks%2520become%250Amore%2520challenging%2520and%2520models%2520more%2520sophisticated%252C%2520our%2520experiments%2520suggest%2520that%250Athe%2520impact%2520of%2520ordering%2520increases.%2520Finding%2520an%2520effective%2520ordering%2520is%2520non-trivial.%250AWe%2520investigate%2520the%2520impact%2520of%2520various%2520element%2520ordering%2520methods%2520in%2520web%2520and%250Adesktop%2520environments.%2520We%2520find%2520that%2520dimensionality%2520reduction%2520provides%2520a%2520viable%250Aordering%2520for%2520pixel-only%2520environments.%2520We%2520train%2520a%2520UI%2520element%2520detection%2520model%2520to%250Aderive%2520elements%2520from%2520pixels%2520and%2520apply%2520our%2520findings%2520to%2520an%2520agent%250Abenchmark--OmniACT--where%2520we%2520only%2520have%2520access%2520to%2520pixels.%2520Our%2520method%2520completes%250Amore%2520than%2520two%2520times%2520as%2520many%2520tasks%2520on%2520average%2520relative%2520to%2520the%2520previous%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Element%20Ordering%20on%20LM%20Agent%20Performance&entry.906535625=Wayne%20Chi%20and%20Ameet%20Talwalkar%20and%20Chris%20Donahue&entry.1292438233=%20%20There%20has%20been%20a%20surge%20of%20interest%20in%20language%20model%20agents%20that%20can%20navigate%0Avirtual%20environments%20such%20as%20the%20web%20or%20desktop.%20To%20navigate%20such%20environments%2C%0Aagents%20benefit%20from%20information%20on%20the%20various%20elements%20%28e.g.%2C%20buttons%2C%20text%2C%0Aor%20images%29%20present.%20It%20remains%20unclear%20which%20element%20attributes%20have%20the%0Agreatest%20impact%20on%20agent%20performance%2C%20especially%20in%20environments%20that%20only%0Aprovide%20a%20graphical%20representation%20%28i.e.%2C%20pixels%29.%20Here%20we%20find%20that%20the%0Aordering%20in%20which%20elements%20are%20presented%20to%20the%20language%20model%20is%20surprisingly%0Aimpactful--randomizing%20element%20ordering%20in%20a%20webpage%20degrades%20agent%20performance%0Acomparably%20to%20removing%20all%20visible%20text%20from%20an%20agent%27s%20state%20representation.%0AWhile%20a%20webpage%20provides%20a%20hierarchical%20ordering%20of%20elements%2C%20there%20is%20no%20such%0Aordering%20when%20parsing%20elements%20directly%20from%20pixels.%20Moreover%2C%20as%20tasks%20become%0Amore%20challenging%20and%20models%20more%20sophisticated%2C%20our%20experiments%20suggest%20that%0Athe%20impact%20of%20ordering%20increases.%20Finding%20an%20effective%20ordering%20is%20non-trivial.%0AWe%20investigate%20the%20impact%20of%20various%20element%20ordering%20methods%20in%20web%20and%0Adesktop%20environments.%20We%20find%20that%20dimensionality%20reduction%20provides%20a%20viable%0Aordering%20for%20pixel-only%20environments.%20We%20train%20a%20UI%20element%20detection%20model%20to%0Aderive%20elements%20from%20pixels%20and%20apply%20our%20findings%20to%20an%20agent%0Abenchmark--OmniACT--where%20we%20only%20have%20access%20to%20pixels.%20Our%20method%20completes%0Amore%20than%20two%20times%20as%20many%20tasks%20on%20average%20relative%20to%20the%20previous%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12089v1&entry.124074799=Read"},
{"title": "Unsupervised Domain Adaptation Via Data Pruning", "author": "Andrea Napoli and Paul White", "abstract": "  The removal of carefully-selected examples from training data has recently\nemerged as an effective way of improving the robustness of machine learning\nmodels. However, the best way to select these examples remains an open\nquestion. In this paper, we consider the problem from the perspective of\nunsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDA\nwhereby training examples are removed to attempt to align the training\ndistribution to that of the target data. By adopting the maximum mean\ndiscrepancy (MMD) as the criterion for alignment, the problem can be neatly\nformulated and solved as an integer quadratic program. We evaluate our approach\non a real-world domain shift task of bioacoustic event detection. As a method\nfor UDA, we show that AdaPrune outperforms related techniques, and is\ncomplementary to other UDA algorithms such as CORAL. Our analysis of the\nrelationship between the MMD and model accuracy, along with t-SNE plots,\nvalidate the proposed method as a principled and well-founded way of performing\ndata pruning.\n", "link": "http://arxiv.org/abs/2409.12076v1", "date": "2024-09-18", "relevancy": 1.9512, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5052}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4983}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Domain%20Adaptation%20Via%20Data%20Pruning&body=Title%3A%20Unsupervised%20Domain%20Adaptation%20Via%20Data%20Pruning%0AAuthor%3A%20Andrea%20Napoli%20and%20Paul%20White%0AAbstract%3A%20%20%20The%20removal%20of%20carefully-selected%20examples%20from%20training%20data%20has%20recently%0Aemerged%20as%20an%20effective%20way%20of%20improving%20the%20robustness%20of%20machine%20learning%0Amodels.%20However%2C%20the%20best%20way%20to%20select%20these%20examples%20remains%20an%20open%0Aquestion.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20from%20the%20perspective%20of%0Aunsupervised%20domain%20adaptation%20%28UDA%29.%20We%20propose%20AdaPrune%2C%20a%20method%20for%20UDA%0Awhereby%20training%20examples%20are%20removed%20to%20attempt%20to%20align%20the%20training%0Adistribution%20to%20that%20of%20the%20target%20data.%20By%20adopting%20the%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20as%20the%20criterion%20for%20alignment%2C%20the%20problem%20can%20be%20neatly%0Aformulated%20and%20solved%20as%20an%20integer%20quadratic%20program.%20We%20evaluate%20our%20approach%0Aon%20a%20real-world%20domain%20shift%20task%20of%20bioacoustic%20event%20detection.%20As%20a%20method%0Afor%20UDA%2C%20we%20show%20that%20AdaPrune%20outperforms%20related%20techniques%2C%20and%20is%0Acomplementary%20to%20other%20UDA%20algorithms%20such%20as%20CORAL.%20Our%20analysis%20of%20the%0Arelationship%20between%20the%20MMD%20and%20model%20accuracy%2C%20along%20with%20t-SNE%20plots%2C%0Avalidate%20the%20proposed%20method%20as%20a%20principled%20and%20well-founded%20way%20of%20performing%0Adata%20pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Domain%2520Adaptation%2520Via%2520Data%2520Pruning%26entry.906535625%3DAndrea%2520Napoli%2520and%2520Paul%2520White%26entry.1292438233%3D%2520%2520The%2520removal%2520of%2520carefully-selected%2520examples%2520from%2520training%2520data%2520has%2520recently%250Aemerged%2520as%2520an%2520effective%2520way%2520of%2520improving%2520the%2520robustness%2520of%2520machine%2520learning%250Amodels.%2520However%252C%2520the%2520best%2520way%2520to%2520select%2520these%2520examples%2520remains%2520an%2520open%250Aquestion.%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520problem%2520from%2520the%2520perspective%2520of%250Aunsupervised%2520domain%2520adaptation%2520%2528UDA%2529.%2520We%2520propose%2520AdaPrune%252C%2520a%2520method%2520for%2520UDA%250Awhereby%2520training%2520examples%2520are%2520removed%2520to%2520attempt%2520to%2520align%2520the%2520training%250Adistribution%2520to%2520that%2520of%2520the%2520target%2520data.%2520By%2520adopting%2520the%2520maximum%2520mean%250Adiscrepancy%2520%2528MMD%2529%2520as%2520the%2520criterion%2520for%2520alignment%252C%2520the%2520problem%2520can%2520be%2520neatly%250Aformulated%2520and%2520solved%2520as%2520an%2520integer%2520quadratic%2520program.%2520We%2520evaluate%2520our%2520approach%250Aon%2520a%2520real-world%2520domain%2520shift%2520task%2520of%2520bioacoustic%2520event%2520detection.%2520As%2520a%2520method%250Afor%2520UDA%252C%2520we%2520show%2520that%2520AdaPrune%2520outperforms%2520related%2520techniques%252C%2520and%2520is%250Acomplementary%2520to%2520other%2520UDA%2520algorithms%2520such%2520as%2520CORAL.%2520Our%2520analysis%2520of%2520the%250Arelationship%2520between%2520the%2520MMD%2520and%2520model%2520accuracy%252C%2520along%2520with%2520t-SNE%2520plots%252C%250Avalidate%2520the%2520proposed%2520method%2520as%2520a%2520principled%2520and%2520well-founded%2520way%2520of%2520performing%250Adata%2520pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Domain%20Adaptation%20Via%20Data%20Pruning&entry.906535625=Andrea%20Napoli%20and%20Paul%20White&entry.1292438233=%20%20The%20removal%20of%20carefully-selected%20examples%20from%20training%20data%20has%20recently%0Aemerged%20as%20an%20effective%20way%20of%20improving%20the%20robustness%20of%20machine%20learning%0Amodels.%20However%2C%20the%20best%20way%20to%20select%20these%20examples%20remains%20an%20open%0Aquestion.%20In%20this%20paper%2C%20we%20consider%20the%20problem%20from%20the%20perspective%20of%0Aunsupervised%20domain%20adaptation%20%28UDA%29.%20We%20propose%20AdaPrune%2C%20a%20method%20for%20UDA%0Awhereby%20training%20examples%20are%20removed%20to%20attempt%20to%20align%20the%20training%0Adistribution%20to%20that%20of%20the%20target%20data.%20By%20adopting%20the%20maximum%20mean%0Adiscrepancy%20%28MMD%29%20as%20the%20criterion%20for%20alignment%2C%20the%20problem%20can%20be%20neatly%0Aformulated%20and%20solved%20as%20an%20integer%20quadratic%20program.%20We%20evaluate%20our%20approach%0Aon%20a%20real-world%20domain%20shift%20task%20of%20bioacoustic%20event%20detection.%20As%20a%20method%0Afor%20UDA%2C%20we%20show%20that%20AdaPrune%20outperforms%20related%20techniques%2C%20and%20is%0Acomplementary%20to%20other%20UDA%20algorithms%20such%20as%20CORAL.%20Our%20analysis%20of%20the%0Arelationship%20between%20the%20MMD%20and%20model%20accuracy%2C%20along%20with%20t-SNE%20plots%2C%0Avalidate%20the%20proposed%20method%20as%20a%20principled%20and%20well-founded%20way%20of%20performing%0Adata%20pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12076v1&entry.124074799=Read"},
{"title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework", "author": "Tuan-Cuong Vuong and Trang Mai Xuan and Thien Van Luong", "abstract": "  In tackling the challenge of Multi-Document Summarization (MDS), numerous\nmethods have been proposed, spanning both extractive and abstractive\nsummarization techniques. However, each approach has its own limitations,\nmaking it less effective to rely solely on either one. An emerging and\npromising strategy involves a synergistic fusion of extractive and abstractive\nsummarization methods. Despite the plethora of studies in this domain, research\non the combined methodology remains scarce, particularly in the context of\nVietnamese language processing. This paper presents a novel Vietnamese MDS\nframework leveraging a two-component pipeline architecture that integrates\nextractive and abstractive techniques. The first component employs an\nextractive approach to identify key sentences within each document. This is\nachieved by a modification of the pre-trained BERT network, which derives\nsemantically meaningful phrase embeddings using siamese and triplet network\nstructures. The second component utilizes the VBD-LLaMA2-7B-50b model for\nabstractive summarization, ultimately generating the final summary document.\nOur proposed framework demonstrates a positive performance, attaining ROUGE-2\nscores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art\nbaselines.\n", "link": "http://arxiv.org/abs/2409.12134v1", "date": "2024-09-18", "relevancy": 1.9499, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BERT-VBD%3A%20Vietnamese%20Multi-Document%20Summarization%20Framework&body=Title%3A%20BERT-VBD%3A%20Vietnamese%20Multi-Document%20Summarization%20Framework%0AAuthor%3A%20Tuan-Cuong%20Vuong%20and%20Trang%20Mai%20Xuan%20and%20Thien%20Van%20Luong%0AAbstract%3A%20%20%20In%20tackling%20the%20challenge%20of%20Multi-Document%20Summarization%20%28MDS%29%2C%20numerous%0Amethods%20have%20been%20proposed%2C%20spanning%20both%20extractive%20and%20abstractive%0Asummarization%20techniques.%20However%2C%20each%20approach%20has%20its%20own%20limitations%2C%0Amaking%20it%20less%20effective%20to%20rely%20solely%20on%20either%20one.%20An%20emerging%20and%0Apromising%20strategy%20involves%20a%20synergistic%20fusion%20of%20extractive%20and%20abstractive%0Asummarization%20methods.%20Despite%20the%20plethora%20of%20studies%20in%20this%20domain%2C%20research%0Aon%20the%20combined%20methodology%20remains%20scarce%2C%20particularly%20in%20the%20context%20of%0AVietnamese%20language%20processing.%20This%20paper%20presents%20a%20novel%20Vietnamese%20MDS%0Aframework%20leveraging%20a%20two-component%20pipeline%20architecture%20that%20integrates%0Aextractive%20and%20abstractive%20techniques.%20The%20first%20component%20employs%20an%0Aextractive%20approach%20to%20identify%20key%20sentences%20within%20each%20document.%20This%20is%0Aachieved%20by%20a%20modification%20of%20the%20pre-trained%20BERT%20network%2C%20which%20derives%0Asemantically%20meaningful%20phrase%20embeddings%20using%20siamese%20and%20triplet%20network%0Astructures.%20The%20second%20component%20utilizes%20the%20VBD-LLaMA2-7B-50b%20model%20for%0Aabstractive%20summarization%2C%20ultimately%20generating%20the%20final%20summary%20document.%0AOur%20proposed%20framework%20demonstrates%20a%20positive%20performance%2C%20attaining%20ROUGE-2%0Ascores%20of%2039.6%25%20on%20the%20VN-MDS%20dataset%20and%20outperforming%20the%20state-of-the-art%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBERT-VBD%253A%2520Vietnamese%2520Multi-Document%2520Summarization%2520Framework%26entry.906535625%3DTuan-Cuong%2520Vuong%2520and%2520Trang%2520Mai%2520Xuan%2520and%2520Thien%2520Van%2520Luong%26entry.1292438233%3D%2520%2520In%2520tackling%2520the%2520challenge%2520of%2520Multi-Document%2520Summarization%2520%2528MDS%2529%252C%2520numerous%250Amethods%2520have%2520been%2520proposed%252C%2520spanning%2520both%2520extractive%2520and%2520abstractive%250Asummarization%2520techniques.%2520However%252C%2520each%2520approach%2520has%2520its%2520own%2520limitations%252C%250Amaking%2520it%2520less%2520effective%2520to%2520rely%2520solely%2520on%2520either%2520one.%2520An%2520emerging%2520and%250Apromising%2520strategy%2520involves%2520a%2520synergistic%2520fusion%2520of%2520extractive%2520and%2520abstractive%250Asummarization%2520methods.%2520Despite%2520the%2520plethora%2520of%2520studies%2520in%2520this%2520domain%252C%2520research%250Aon%2520the%2520combined%2520methodology%2520remains%2520scarce%252C%2520particularly%2520in%2520the%2520context%2520of%250AVietnamese%2520language%2520processing.%2520This%2520paper%2520presents%2520a%2520novel%2520Vietnamese%2520MDS%250Aframework%2520leveraging%2520a%2520two-component%2520pipeline%2520architecture%2520that%2520integrates%250Aextractive%2520and%2520abstractive%2520techniques.%2520The%2520first%2520component%2520employs%2520an%250Aextractive%2520approach%2520to%2520identify%2520key%2520sentences%2520within%2520each%2520document.%2520This%2520is%250Aachieved%2520by%2520a%2520modification%2520of%2520the%2520pre-trained%2520BERT%2520network%252C%2520which%2520derives%250Asemantically%2520meaningful%2520phrase%2520embeddings%2520using%2520siamese%2520and%2520triplet%2520network%250Astructures.%2520The%2520second%2520component%2520utilizes%2520the%2520VBD-LLaMA2-7B-50b%2520model%2520for%250Aabstractive%2520summarization%252C%2520ultimately%2520generating%2520the%2520final%2520summary%2520document.%250AOur%2520proposed%2520framework%2520demonstrates%2520a%2520positive%2520performance%252C%2520attaining%2520ROUGE-2%250Ascores%2520of%252039.6%2525%2520on%2520the%2520VN-MDS%2520dataset%2520and%2520outperforming%2520the%2520state-of-the-art%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BERT-VBD%3A%20Vietnamese%20Multi-Document%20Summarization%20Framework&entry.906535625=Tuan-Cuong%20Vuong%20and%20Trang%20Mai%20Xuan%20and%20Thien%20Van%20Luong&entry.1292438233=%20%20In%20tackling%20the%20challenge%20of%20Multi-Document%20Summarization%20%28MDS%29%2C%20numerous%0Amethods%20have%20been%20proposed%2C%20spanning%20both%20extractive%20and%20abstractive%0Asummarization%20techniques.%20However%2C%20each%20approach%20has%20its%20own%20limitations%2C%0Amaking%20it%20less%20effective%20to%20rely%20solely%20on%20either%20one.%20An%20emerging%20and%0Apromising%20strategy%20involves%20a%20synergistic%20fusion%20of%20extractive%20and%20abstractive%0Asummarization%20methods.%20Despite%20the%20plethora%20of%20studies%20in%20this%20domain%2C%20research%0Aon%20the%20combined%20methodology%20remains%20scarce%2C%20particularly%20in%20the%20context%20of%0AVietnamese%20language%20processing.%20This%20paper%20presents%20a%20novel%20Vietnamese%20MDS%0Aframework%20leveraging%20a%20two-component%20pipeline%20architecture%20that%20integrates%0Aextractive%20and%20abstractive%20techniques.%20The%20first%20component%20employs%20an%0Aextractive%20approach%20to%20identify%20key%20sentences%20within%20each%20document.%20This%20is%0Aachieved%20by%20a%20modification%20of%20the%20pre-trained%20BERT%20network%2C%20which%20derives%0Asemantically%20meaningful%20phrase%20embeddings%20using%20siamese%20and%20triplet%20network%0Astructures.%20The%20second%20component%20utilizes%20the%20VBD-LLaMA2-7B-50b%20model%20for%0Aabstractive%20summarization%2C%20ultimately%20generating%20the%20final%20summary%20document.%0AOur%20proposed%20framework%20demonstrates%20a%20positive%20performance%2C%20attaining%20ROUGE-2%0Ascores%20of%2039.6%25%20on%20the%20VN-MDS%20dataset%20and%20outperforming%20the%20state-of-the-art%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12134v1&entry.124074799=Read"},
{"title": "Topological Deep Learning with State-Space Models: A Mamba Approach for\n  Simplicial Complexes", "author": "Marco Montagna and Simone Scardapane and Lev Telyatnikov", "abstract": "  Graph Neural Networks based on the message-passing (MP) mechanism are a\ndominant approach for handling graph-structured data. However, they are\ninherently limited to modeling only pairwise interactions, making it difficult\nto explicitly capture the complexity of systems with $n$-body relations. To\naddress this, topological deep learning has emerged as a promising field for\nstudying and modeling higher-order interactions using various topological\ndomains, such as simplicial and cellular complexes. While these new domains\nprovide powerful representations, they introduce new challenges, such as\neffectively modeling the interactions among higher-order structures through\nhigher-order MP. Meanwhile, structured state-space sequence models have proven\nto be effective for sequence modeling and have recently been adapted for graph\ndata by encoding the neighborhood of a node as a sequence, thereby avoiding the\nMP mechanism. In this work, we propose a novel architecture designed to operate\nwith simplicial complexes, utilizing the Mamba state-space model as its\nbackbone. Our approach generates sequences for the nodes based on the\nneighboring cells, enabling direct communication between all higher-order\nstructures, regardless of their rank. We extensively validate our model,\ndemonstrating that it achieves competitive performance compared to\nstate-of-the-art models developed for simplicial complexes.\n", "link": "http://arxiv.org/abs/2409.12033v1", "date": "2024-09-18", "relevancy": 1.9499, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5001}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Deep%20Learning%20with%20State-Space%20Models%3A%20A%20Mamba%20Approach%20for%0A%20%20Simplicial%20Complexes&body=Title%3A%20Topological%20Deep%20Learning%20with%20State-Space%20Models%3A%20A%20Mamba%20Approach%20for%0A%20%20Simplicial%20Complexes%0AAuthor%3A%20Marco%20Montagna%20and%20Simone%20Scardapane%20and%20Lev%20Telyatnikov%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20based%20on%20the%20message-passing%20%28MP%29%20mechanism%20are%20a%0Adominant%20approach%20for%20handling%20graph-structured%20data.%20However%2C%20they%20are%0Ainherently%20limited%20to%20modeling%20only%20pairwise%20interactions%2C%20making%20it%20difficult%0Ato%20explicitly%20capture%20the%20complexity%20of%20systems%20with%20%24n%24-body%20relations.%20To%0Aaddress%20this%2C%20topological%20deep%20learning%20has%20emerged%20as%20a%20promising%20field%20for%0Astudying%20and%20modeling%20higher-order%20interactions%20using%20various%20topological%0Adomains%2C%20such%20as%20simplicial%20and%20cellular%20complexes.%20While%20these%20new%20domains%0Aprovide%20powerful%20representations%2C%20they%20introduce%20new%20challenges%2C%20such%20as%0Aeffectively%20modeling%20the%20interactions%20among%20higher-order%20structures%20through%0Ahigher-order%20MP.%20Meanwhile%2C%20structured%20state-space%20sequence%20models%20have%20proven%0Ato%20be%20effective%20for%20sequence%20modeling%20and%20have%20recently%20been%20adapted%20for%20graph%0Adata%20by%20encoding%20the%20neighborhood%20of%20a%20node%20as%20a%20sequence%2C%20thereby%20avoiding%20the%0AMP%20mechanism.%20In%20this%20work%2C%20we%20propose%20a%20novel%20architecture%20designed%20to%20operate%0Awith%20simplicial%20complexes%2C%20utilizing%20the%20Mamba%20state-space%20model%20as%20its%0Abackbone.%20Our%20approach%20generates%20sequences%20for%20the%20nodes%20based%20on%20the%0Aneighboring%20cells%2C%20enabling%20direct%20communication%20between%20all%20higher-order%0Astructures%2C%20regardless%20of%20their%20rank.%20We%20extensively%20validate%20our%20model%2C%0Ademonstrating%20that%20it%20achieves%20competitive%20performance%20compared%20to%0Astate-of-the-art%20models%20developed%20for%20simplicial%20complexes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Deep%2520Learning%2520with%2520State-Space%2520Models%253A%2520A%2520Mamba%2520Approach%2520for%250A%2520%2520Simplicial%2520Complexes%26entry.906535625%3DMarco%2520Montagna%2520and%2520Simone%2520Scardapane%2520and%2520Lev%2520Telyatnikov%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520based%2520on%2520the%2520message-passing%2520%2528MP%2529%2520mechanism%2520are%2520a%250Adominant%2520approach%2520for%2520handling%2520graph-structured%2520data.%2520However%252C%2520they%2520are%250Ainherently%2520limited%2520to%2520modeling%2520only%2520pairwise%2520interactions%252C%2520making%2520it%2520difficult%250Ato%2520explicitly%2520capture%2520the%2520complexity%2520of%2520systems%2520with%2520%2524n%2524-body%2520relations.%2520To%250Aaddress%2520this%252C%2520topological%2520deep%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520field%2520for%250Astudying%2520and%2520modeling%2520higher-order%2520interactions%2520using%2520various%2520topological%250Adomains%252C%2520such%2520as%2520simplicial%2520and%2520cellular%2520complexes.%2520While%2520these%2520new%2520domains%250Aprovide%2520powerful%2520representations%252C%2520they%2520introduce%2520new%2520challenges%252C%2520such%2520as%250Aeffectively%2520modeling%2520the%2520interactions%2520among%2520higher-order%2520structures%2520through%250Ahigher-order%2520MP.%2520Meanwhile%252C%2520structured%2520state-space%2520sequence%2520models%2520have%2520proven%250Ato%2520be%2520effective%2520for%2520sequence%2520modeling%2520and%2520have%2520recently%2520been%2520adapted%2520for%2520graph%250Adata%2520by%2520encoding%2520the%2520neighborhood%2520of%2520a%2520node%2520as%2520a%2520sequence%252C%2520thereby%2520avoiding%2520the%250AMP%2520mechanism.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520architecture%2520designed%2520to%2520operate%250Awith%2520simplicial%2520complexes%252C%2520utilizing%2520the%2520Mamba%2520state-space%2520model%2520as%2520its%250Abackbone.%2520Our%2520approach%2520generates%2520sequences%2520for%2520the%2520nodes%2520based%2520on%2520the%250Aneighboring%2520cells%252C%2520enabling%2520direct%2520communication%2520between%2520all%2520higher-order%250Astructures%252C%2520regardless%2520of%2520their%2520rank.%2520We%2520extensively%2520validate%2520our%2520model%252C%250Ademonstrating%2520that%2520it%2520achieves%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520models%2520developed%2520for%2520simplicial%2520complexes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Deep%20Learning%20with%20State-Space%20Models%3A%20A%20Mamba%20Approach%20for%0A%20%20Simplicial%20Complexes&entry.906535625=Marco%20Montagna%20and%20Simone%20Scardapane%20and%20Lev%20Telyatnikov&entry.1292438233=%20%20Graph%20Neural%20Networks%20based%20on%20the%20message-passing%20%28MP%29%20mechanism%20are%20a%0Adominant%20approach%20for%20handling%20graph-structured%20data.%20However%2C%20they%20are%0Ainherently%20limited%20to%20modeling%20only%20pairwise%20interactions%2C%20making%20it%20difficult%0Ato%20explicitly%20capture%20the%20complexity%20of%20systems%20with%20%24n%24-body%20relations.%20To%0Aaddress%20this%2C%20topological%20deep%20learning%20has%20emerged%20as%20a%20promising%20field%20for%0Astudying%20and%20modeling%20higher-order%20interactions%20using%20various%20topological%0Adomains%2C%20such%20as%20simplicial%20and%20cellular%20complexes.%20While%20these%20new%20domains%0Aprovide%20powerful%20representations%2C%20they%20introduce%20new%20challenges%2C%20such%20as%0Aeffectively%20modeling%20the%20interactions%20among%20higher-order%20structures%20through%0Ahigher-order%20MP.%20Meanwhile%2C%20structured%20state-space%20sequence%20models%20have%20proven%0Ato%20be%20effective%20for%20sequence%20modeling%20and%20have%20recently%20been%20adapted%20for%20graph%0Adata%20by%20encoding%20the%20neighborhood%20of%20a%20node%20as%20a%20sequence%2C%20thereby%20avoiding%20the%0AMP%20mechanism.%20In%20this%20work%2C%20we%20propose%20a%20novel%20architecture%20designed%20to%20operate%0Awith%20simplicial%20complexes%2C%20utilizing%20the%20Mamba%20state-space%20model%20as%20its%0Abackbone.%20Our%20approach%20generates%20sequences%20for%20the%20nodes%20based%20on%20the%0Aneighboring%20cells%2C%20enabling%20direct%20communication%20between%20all%20higher-order%0Astructures%2C%20regardless%20of%20their%20rank.%20We%20extensively%20validate%20our%20model%2C%0Ademonstrating%20that%20it%20achieves%20competitive%20performance%20compared%20to%0Astate-of-the-art%20models%20developed%20for%20simplicial%20complexes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12033v1&entry.124074799=Read"},
{"title": "MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion", "author": "Kalakonda Sai Shashank and Shubh Maheshwari and Ravi Kiran Sarvadevabhatla", "abstract": "  We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/\n", "link": "http://arxiv.org/abs/2409.12140v1", "date": "2024-09-18", "relevancy": 1.6963, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6049}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoRAG%20--%20Multi-Fusion%20Retrieval%20Augmented%20Generation%20for%20Human%20Motion&body=Title%3A%20MoRAG%20--%20Multi-Fusion%20Retrieval%20Augmented%20Generation%20for%20Human%20Motion%0AAuthor%3A%20Kalakonda%20Sai%20Shashank%20and%20Shubh%20Maheshwari%20and%20Ravi%20Kiran%20Sarvadevabhatla%0AAbstract%3A%20%20%20We%20introduce%20MoRAG%2C%20a%20novel%20multi-part%20fusion%20based%20retrieval-augmented%0Ageneration%20strategy%20for%20text-based%20human%20motion%20generation.%20The%20method%20enhances%0Amotion%20diffusion%20models%20by%20leveraging%20additional%20knowledge%20obtained%20through%20an%0Aimproved%20motion%20retrieval%20process.%20By%20effectively%20prompting%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20address%20spelling%20errors%20and%20rephrasing%20issues%20in%20motion%0Aretrieval.%20Our%20approach%20utilizes%20a%20multi-part%20retrieval%20strategy%20to%20improve%20the%0Ageneralizability%20of%20motion%20retrieval%20across%20the%20language%20space.%20We%20create%0Adiverse%20samples%20through%20the%20spatial%20composition%20of%20the%20retrieved%20motions.%0AFurthermore%2C%20by%20utilizing%20low-level%2C%20part-specific%20motion%20information%2C%20we%20can%0Aconstruct%20motion%20samples%20for%20unseen%20text%20descriptions.%20Our%20experiments%0Ademonstrate%20that%20our%20framework%20can%20serve%20as%20a%20plug-and-play%20module%2C%20improving%0Athe%20performance%20of%20motion%20diffusion%20models.%20Code%2C%20pretrained%20models%20and%20sample%0Avideos%20will%20be%20made%20available%20at%3A%20https%3A//motion-rag.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoRAG%2520--%2520Multi-Fusion%2520Retrieval%2520Augmented%2520Generation%2520for%2520Human%2520Motion%26entry.906535625%3DKalakonda%2520Sai%2520Shashank%2520and%2520Shubh%2520Maheshwari%2520and%2520Ravi%2520Kiran%2520Sarvadevabhatla%26entry.1292438233%3D%2520%2520We%2520introduce%2520MoRAG%252C%2520a%2520novel%2520multi-part%2520fusion%2520based%2520retrieval-augmented%250Ageneration%2520strategy%2520for%2520text-based%2520human%2520motion%2520generation.%2520The%2520method%2520enhances%250Amotion%2520diffusion%2520models%2520by%2520leveraging%2520additional%2520knowledge%2520obtained%2520through%2520an%250Aimproved%2520motion%2520retrieval%2520process.%2520By%2520effectively%2520prompting%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520we%2520address%2520spelling%2520errors%2520and%2520rephrasing%2520issues%2520in%2520motion%250Aretrieval.%2520Our%2520approach%2520utilizes%2520a%2520multi-part%2520retrieval%2520strategy%2520to%2520improve%2520the%250Ageneralizability%2520of%2520motion%2520retrieval%2520across%2520the%2520language%2520space.%2520We%2520create%250Adiverse%2520samples%2520through%2520the%2520spatial%2520composition%2520of%2520the%2520retrieved%2520motions.%250AFurthermore%252C%2520by%2520utilizing%2520low-level%252C%2520part-specific%2520motion%2520information%252C%2520we%2520can%250Aconstruct%2520motion%2520samples%2520for%2520unseen%2520text%2520descriptions.%2520Our%2520experiments%250Ademonstrate%2520that%2520our%2520framework%2520can%2520serve%2520as%2520a%2520plug-and-play%2520module%252C%2520improving%250Athe%2520performance%2520of%2520motion%2520diffusion%2520models.%2520Code%252C%2520pretrained%2520models%2520and%2520sample%250Avideos%2520will%2520be%2520made%2520available%2520at%253A%2520https%253A//motion-rag.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoRAG%20--%20Multi-Fusion%20Retrieval%20Augmented%20Generation%20for%20Human%20Motion&entry.906535625=Kalakonda%20Sai%20Shashank%20and%20Shubh%20Maheshwari%20and%20Ravi%20Kiran%20Sarvadevabhatla&entry.1292438233=%20%20We%20introduce%20MoRAG%2C%20a%20novel%20multi-part%20fusion%20based%20retrieval-augmented%0Ageneration%20strategy%20for%20text-based%20human%20motion%20generation.%20The%20method%20enhances%0Amotion%20diffusion%20models%20by%20leveraging%20additional%20knowledge%20obtained%20through%20an%0Aimproved%20motion%20retrieval%20process.%20By%20effectively%20prompting%20large%20language%0Amodels%20%28LLMs%29%2C%20we%20address%20spelling%20errors%20and%20rephrasing%20issues%20in%20motion%0Aretrieval.%20Our%20approach%20utilizes%20a%20multi-part%20retrieval%20strategy%20to%20improve%20the%0Ageneralizability%20of%20motion%20retrieval%20across%20the%20language%20space.%20We%20create%0Adiverse%20samples%20through%20the%20spatial%20composition%20of%20the%20retrieved%20motions.%0AFurthermore%2C%20by%20utilizing%20low-level%2C%20part-specific%20motion%20information%2C%20we%20can%0Aconstruct%20motion%20samples%20for%20unseen%20text%20descriptions.%20Our%20experiments%0Ademonstrate%20that%20our%20framework%20can%20serve%20as%20a%20plug-and-play%20module%2C%20improving%0Athe%20performance%20of%20motion%20diffusion%20models.%20Code%2C%20pretrained%20models%20and%20sample%0Avideos%20will%20be%20made%20available%20at%3A%20https%3A//motion-rag.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12140v1&entry.124074799=Read"},
{"title": "Tight and Efficient Upper Bound on Spectral Norm of Convolutional Layers", "author": "Ekaterina Grishina and Mikhail Gorbunov and Maxim Rakhuba", "abstract": "  Controlling the spectral norm of the Jacobian matrix, which is related to the\nconvolution operation, has been shown to improve generalization, training\nstability and robustness in CNNs. Existing methods for computing the norm\neither tend to overestimate it or their performance may deteriorate quickly\nwith increasing the input and kernel sizes. In this paper, we demonstrate that\nthe tensor version of the spectral norm of a four-dimensional convolution\nkernel, up to a constant factor, serves as an upper bound for the spectral norm\nof the Jacobian matrix associated with the convolution operation. This new\nupper bound is independent of the input image resolution, differentiable and\ncan be efficiently calculated during training. Through experiments, we\ndemonstrate how this new bound can be used to improve the performance of\nconvolutional architectures.\n", "link": "http://arxiv.org/abs/2409.11859v1", "date": "2024-09-18", "relevancy": 1.8587, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4719}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4617}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20and%20Efficient%20Upper%20Bound%20on%20Spectral%20Norm%20of%20Convolutional%20Layers&body=Title%3A%20Tight%20and%20Efficient%20Upper%20Bound%20on%20Spectral%20Norm%20of%20Convolutional%20Layers%0AAuthor%3A%20Ekaterina%20Grishina%20and%20Mikhail%20Gorbunov%20and%20Maxim%20Rakhuba%0AAbstract%3A%20%20%20Controlling%20the%20spectral%20norm%20of%20the%20Jacobian%20matrix%2C%20which%20is%20related%20to%20the%0Aconvolution%20operation%2C%20has%20been%20shown%20to%20improve%20generalization%2C%20training%0Astability%20and%20robustness%20in%20CNNs.%20Existing%20methods%20for%20computing%20the%20norm%0Aeither%20tend%20to%20overestimate%20it%20or%20their%20performance%20may%20deteriorate%20quickly%0Awith%20increasing%20the%20input%20and%20kernel%20sizes.%20In%20this%20paper%2C%20we%20demonstrate%20that%0Athe%20tensor%20version%20of%20the%20spectral%20norm%20of%20a%20four-dimensional%20convolution%0Akernel%2C%20up%20to%20a%20constant%20factor%2C%20serves%20as%20an%20upper%20bound%20for%20the%20spectral%20norm%0Aof%20the%20Jacobian%20matrix%20associated%20with%20the%20convolution%20operation.%20This%20new%0Aupper%20bound%20is%20independent%20of%20the%20input%20image%20resolution%2C%20differentiable%20and%0Acan%20be%20efficiently%20calculated%20during%20training.%20Through%20experiments%2C%20we%0Ademonstrate%20how%20this%20new%20bound%20can%20be%20used%20to%20improve%20the%20performance%20of%0Aconvolutional%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520and%2520Efficient%2520Upper%2520Bound%2520on%2520Spectral%2520Norm%2520of%2520Convolutional%2520Layers%26entry.906535625%3DEkaterina%2520Grishina%2520and%2520Mikhail%2520Gorbunov%2520and%2520Maxim%2520Rakhuba%26entry.1292438233%3D%2520%2520Controlling%2520the%2520spectral%2520norm%2520of%2520the%2520Jacobian%2520matrix%252C%2520which%2520is%2520related%2520to%2520the%250Aconvolution%2520operation%252C%2520has%2520been%2520shown%2520to%2520improve%2520generalization%252C%2520training%250Astability%2520and%2520robustness%2520in%2520CNNs.%2520Existing%2520methods%2520for%2520computing%2520the%2520norm%250Aeither%2520tend%2520to%2520overestimate%2520it%2520or%2520their%2520performance%2520may%2520deteriorate%2520quickly%250Awith%2520increasing%2520the%2520input%2520and%2520kernel%2520sizes.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%250Athe%2520tensor%2520version%2520of%2520the%2520spectral%2520norm%2520of%2520a%2520four-dimensional%2520convolution%250Akernel%252C%2520up%2520to%2520a%2520constant%2520factor%252C%2520serves%2520as%2520an%2520upper%2520bound%2520for%2520the%2520spectral%2520norm%250Aof%2520the%2520Jacobian%2520matrix%2520associated%2520with%2520the%2520convolution%2520operation.%2520This%2520new%250Aupper%2520bound%2520is%2520independent%2520of%2520the%2520input%2520image%2520resolution%252C%2520differentiable%2520and%250Acan%2520be%2520efficiently%2520calculated%2520during%2520training.%2520Through%2520experiments%252C%2520we%250Ademonstrate%2520how%2520this%2520new%2520bound%2520can%2520be%2520used%2520to%2520improve%2520the%2520performance%2520of%250Aconvolutional%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20and%20Efficient%20Upper%20Bound%20on%20Spectral%20Norm%20of%20Convolutional%20Layers&entry.906535625=Ekaterina%20Grishina%20and%20Mikhail%20Gorbunov%20and%20Maxim%20Rakhuba&entry.1292438233=%20%20Controlling%20the%20spectral%20norm%20of%20the%20Jacobian%20matrix%2C%20which%20is%20related%20to%20the%0Aconvolution%20operation%2C%20has%20been%20shown%20to%20improve%20generalization%2C%20training%0Astability%20and%20robustness%20in%20CNNs.%20Existing%20methods%20for%20computing%20the%20norm%0Aeither%20tend%20to%20overestimate%20it%20or%20their%20performance%20may%20deteriorate%20quickly%0Awith%20increasing%20the%20input%20and%20kernel%20sizes.%20In%20this%20paper%2C%20we%20demonstrate%20that%0Athe%20tensor%20version%20of%20the%20spectral%20norm%20of%20a%20four-dimensional%20convolution%0Akernel%2C%20up%20to%20a%20constant%20factor%2C%20serves%20as%20an%20upper%20bound%20for%20the%20spectral%20norm%0Aof%20the%20Jacobian%20matrix%20associated%20with%20the%20convolution%20operation.%20This%20new%0Aupper%20bound%20is%20independent%20of%20the%20input%20image%20resolution%2C%20differentiable%20and%0Acan%20be%20efficiently%20calculated%20during%20training.%20Through%20experiments%2C%20we%0Ademonstrate%20how%20this%20new%20bound%20can%20be%20used%20to%20improve%20the%20performance%20of%0Aconvolutional%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11859v1&entry.124074799=Read"},
{"title": "WeHelp: A Shared Autonomy System for Wheelchair Users", "author": "Abulikemu Abuduweili and Alice Wu and Tianhao Wei and Weiye Zhao", "abstract": "  There is a large population of wheelchair users. Most of the wheelchair users\nneed help with daily tasks. However, according to recent reports, their needs\nare not properly satisfied due to the lack of caregivers. Therefore, in this\nproject, we develop WeHelp, a shared autonomy system aimed for wheelchair\nusers. A robot with a WeHelp system has three modes, following mode, remote\ncontrol mode and tele-operation mode. In the following mode, the robot follows\nthe wheelchair user automatically via visual tracking. The wheelchair user can\nask the robot to follow them from behind, by the left or by the right. When the\nwheelchair user asks for help, the robot will recognize the command via speech\nrecognition, and then switch to the teleoperation mode or remote control mode.\nIn the teleoperation mode, the wheelchair user takes over the robot with a joy\nstick and controls the robot to complete some complex tasks for their needs,\nsuch as opening doors, moving obstacles on the way, reaching objects on a high\nshelf or on the low ground, etc. In the remote control mode, a remote assistant\ntakes over the robot and helps the wheelchair user complete some complex tasks\nfor their needs. Our evaluation shows that the pipeline is useful and practical\nfor wheelchair users. Source code and demo of the paper are available at\n\\url{https://github.com/Walleclipse/WeHelp}.\n", "link": "http://arxiv.org/abs/2409.12159v1", "date": "2024-09-18", "relevancy": 1.8889, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.7235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.513}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeHelp%3A%20A%20Shared%20Autonomy%20System%20for%20Wheelchair%20Users&body=Title%3A%20WeHelp%3A%20A%20Shared%20Autonomy%20System%20for%20Wheelchair%20Users%0AAuthor%3A%20Abulikemu%20Abuduweili%20and%20Alice%20Wu%20and%20Tianhao%20Wei%20and%20Weiye%20Zhao%0AAbstract%3A%20%20%20There%20is%20a%20large%20population%20of%20wheelchair%20users.%20Most%20of%20the%20wheelchair%20users%0Aneed%20help%20with%20daily%20tasks.%20However%2C%20according%20to%20recent%20reports%2C%20their%20needs%0Aare%20not%20properly%20satisfied%20due%20to%20the%20lack%20of%20caregivers.%20Therefore%2C%20in%20this%0Aproject%2C%20we%20develop%20WeHelp%2C%20a%20shared%20autonomy%20system%20aimed%20for%20wheelchair%0Ausers.%20A%20robot%20with%20a%20WeHelp%20system%20has%20three%20modes%2C%20following%20mode%2C%20remote%0Acontrol%20mode%20and%20tele-operation%20mode.%20In%20the%20following%20mode%2C%20the%20robot%20follows%0Athe%20wheelchair%20user%20automatically%20via%20visual%20tracking.%20The%20wheelchair%20user%20can%0Aask%20the%20robot%20to%20follow%20them%20from%20behind%2C%20by%20the%20left%20or%20by%20the%20right.%20When%20the%0Awheelchair%20user%20asks%20for%20help%2C%20the%20robot%20will%20recognize%20the%20command%20via%20speech%0Arecognition%2C%20and%20then%20switch%20to%20the%20teleoperation%20mode%20or%20remote%20control%20mode.%0AIn%20the%20teleoperation%20mode%2C%20the%20wheelchair%20user%20takes%20over%20the%20robot%20with%20a%20joy%0Astick%20and%20controls%20the%20robot%20to%20complete%20some%20complex%20tasks%20for%20their%20needs%2C%0Asuch%20as%20opening%20doors%2C%20moving%20obstacles%20on%20the%20way%2C%20reaching%20objects%20on%20a%20high%0Ashelf%20or%20on%20the%20low%20ground%2C%20etc.%20In%20the%20remote%20control%20mode%2C%20a%20remote%20assistant%0Atakes%20over%20the%20robot%20and%20helps%20the%20wheelchair%20user%20complete%20some%20complex%20tasks%0Afor%20their%20needs.%20Our%20evaluation%20shows%20that%20the%20pipeline%20is%20useful%20and%20practical%0Afor%20wheelchair%20users.%20Source%20code%20and%20demo%20of%20the%20paper%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Walleclipse/WeHelp%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeHelp%253A%2520A%2520Shared%2520Autonomy%2520System%2520for%2520Wheelchair%2520Users%26entry.906535625%3DAbulikemu%2520Abuduweili%2520and%2520Alice%2520Wu%2520and%2520Tianhao%2520Wei%2520and%2520Weiye%2520Zhao%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520large%2520population%2520of%2520wheelchair%2520users.%2520Most%2520of%2520the%2520wheelchair%2520users%250Aneed%2520help%2520with%2520daily%2520tasks.%2520However%252C%2520according%2520to%2520recent%2520reports%252C%2520their%2520needs%250Aare%2520not%2520properly%2520satisfied%2520due%2520to%2520the%2520lack%2520of%2520caregivers.%2520Therefore%252C%2520in%2520this%250Aproject%252C%2520we%2520develop%2520WeHelp%252C%2520a%2520shared%2520autonomy%2520system%2520aimed%2520for%2520wheelchair%250Ausers.%2520A%2520robot%2520with%2520a%2520WeHelp%2520system%2520has%2520three%2520modes%252C%2520following%2520mode%252C%2520remote%250Acontrol%2520mode%2520and%2520tele-operation%2520mode.%2520In%2520the%2520following%2520mode%252C%2520the%2520robot%2520follows%250Athe%2520wheelchair%2520user%2520automatically%2520via%2520visual%2520tracking.%2520The%2520wheelchair%2520user%2520can%250Aask%2520the%2520robot%2520to%2520follow%2520them%2520from%2520behind%252C%2520by%2520the%2520left%2520or%2520by%2520the%2520right.%2520When%2520the%250Awheelchair%2520user%2520asks%2520for%2520help%252C%2520the%2520robot%2520will%2520recognize%2520the%2520command%2520via%2520speech%250Arecognition%252C%2520and%2520then%2520switch%2520to%2520the%2520teleoperation%2520mode%2520or%2520remote%2520control%2520mode.%250AIn%2520the%2520teleoperation%2520mode%252C%2520the%2520wheelchair%2520user%2520takes%2520over%2520the%2520robot%2520with%2520a%2520joy%250Astick%2520and%2520controls%2520the%2520robot%2520to%2520complete%2520some%2520complex%2520tasks%2520for%2520their%2520needs%252C%250Asuch%2520as%2520opening%2520doors%252C%2520moving%2520obstacles%2520on%2520the%2520way%252C%2520reaching%2520objects%2520on%2520a%2520high%250Ashelf%2520or%2520on%2520the%2520low%2520ground%252C%2520etc.%2520In%2520the%2520remote%2520control%2520mode%252C%2520a%2520remote%2520assistant%250Atakes%2520over%2520the%2520robot%2520and%2520helps%2520the%2520wheelchair%2520user%2520complete%2520some%2520complex%2520tasks%250Afor%2520their%2520needs.%2520Our%2520evaluation%2520shows%2520that%2520the%2520pipeline%2520is%2520useful%2520and%2520practical%250Afor%2520wheelchair%2520users.%2520Source%2520code%2520and%2520demo%2520of%2520the%2520paper%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Walleclipse/WeHelp%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeHelp%3A%20A%20Shared%20Autonomy%20System%20for%20Wheelchair%20Users&entry.906535625=Abulikemu%20Abuduweili%20and%20Alice%20Wu%20and%20Tianhao%20Wei%20and%20Weiye%20Zhao&entry.1292438233=%20%20There%20is%20a%20large%20population%20of%20wheelchair%20users.%20Most%20of%20the%20wheelchair%20users%0Aneed%20help%20with%20daily%20tasks.%20However%2C%20according%20to%20recent%20reports%2C%20their%20needs%0Aare%20not%20properly%20satisfied%20due%20to%20the%20lack%20of%20caregivers.%20Therefore%2C%20in%20this%0Aproject%2C%20we%20develop%20WeHelp%2C%20a%20shared%20autonomy%20system%20aimed%20for%20wheelchair%0Ausers.%20A%20robot%20with%20a%20WeHelp%20system%20has%20three%20modes%2C%20following%20mode%2C%20remote%0Acontrol%20mode%20and%20tele-operation%20mode.%20In%20the%20following%20mode%2C%20the%20robot%20follows%0Athe%20wheelchair%20user%20automatically%20via%20visual%20tracking.%20The%20wheelchair%20user%20can%0Aask%20the%20robot%20to%20follow%20them%20from%20behind%2C%20by%20the%20left%20or%20by%20the%20right.%20When%20the%0Awheelchair%20user%20asks%20for%20help%2C%20the%20robot%20will%20recognize%20the%20command%20via%20speech%0Arecognition%2C%20and%20then%20switch%20to%20the%20teleoperation%20mode%20or%20remote%20control%20mode.%0AIn%20the%20teleoperation%20mode%2C%20the%20wheelchair%20user%20takes%20over%20the%20robot%20with%20a%20joy%0Astick%20and%20controls%20the%20robot%20to%20complete%20some%20complex%20tasks%20for%20their%20needs%2C%0Asuch%20as%20opening%20doors%2C%20moving%20obstacles%20on%20the%20way%2C%20reaching%20objects%20on%20a%20high%0Ashelf%20or%20on%20the%20low%20ground%2C%20etc.%20In%20the%20remote%20control%20mode%2C%20a%20remote%20assistant%0Atakes%20over%20the%20robot%20and%20helps%20the%20wheelchair%20user%20complete%20some%20complex%20tasks%0Afor%20their%20needs.%20Our%20evaluation%20shows%20that%20the%20pipeline%20is%20useful%20and%20practical%0Afor%20wheelchair%20users.%20Source%20code%20and%20demo%20of%20the%20paper%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Walleclipse/WeHelp%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12159v1&entry.124074799=Read"},
{"title": "Recent Advances in OOD Detection: Problems and Approaches", "author": "Shuo Lu and YingSheng Wang and LuJun Sheng and AiHua Zheng and LinXiao He and Jian Liang", "abstract": "  Out-of-distribution (OOD) detection aims to detect test samples outside the\ntraining category space, which is an essential component in building reliable\nmachine learning systems. Existing reviews on OOD detection primarily focus on\nmethod taxonomy, surveying the field by categorizing various approaches.\nHowever, many recent works concentrate on non-traditional OOD detection\nscenarios, such as test-time adaptation, multi-modal data sources and other\nnovel contexts. In this survey, we uniquely review recent advances in OOD\ndetection from the problem scenario perspective for the first time. According\nto whether the training process is completely controlled, we divide OOD\ndetection methods into training-driven and training-agnostic. Besides,\nconsidering the rapid development of pre-trained models, large pre-trained\nmodel-based OOD detection is also regarded as an important category and\ndiscussed separately. Furthermore, we provide a discussion of the evaluation\nscenarios, a variety of applications, and several future research directions.\nWe believe this survey with new taxonomy will benefit the proposal of new\nmethods and the expansion of more practical scenarios. A curated list of\nrelated papers is provided in the Github repository:\n\\url{https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection}\n", "link": "http://arxiv.org/abs/2409.11884v1", "date": "2024-09-18", "relevancy": 1.456, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4906}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4842}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20OOD%20Detection%3A%20Problems%20and%20Approaches&body=Title%3A%20Recent%20Advances%20in%20OOD%20Detection%3A%20Problems%20and%20Approaches%0AAuthor%3A%20Shuo%20Lu%20and%20YingSheng%20Wang%20and%20LuJun%20Sheng%20and%20AiHua%20Zheng%20and%20LinXiao%20He%20and%20Jian%20Liang%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20aims%20to%20detect%20test%20samples%20outside%20the%0Atraining%20category%20space%2C%20which%20is%20an%20essential%20component%20in%20building%20reliable%0Amachine%20learning%20systems.%20Existing%20reviews%20on%20OOD%20detection%20primarily%20focus%20on%0Amethod%20taxonomy%2C%20surveying%20the%20field%20by%20categorizing%20various%20approaches.%0AHowever%2C%20many%20recent%20works%20concentrate%20on%20non-traditional%20OOD%20detection%0Ascenarios%2C%20such%20as%20test-time%20adaptation%2C%20multi-modal%20data%20sources%20and%20other%0Anovel%20contexts.%20In%20this%20survey%2C%20we%20uniquely%20review%20recent%20advances%20in%20OOD%0Adetection%20from%20the%20problem%20scenario%20perspective%20for%20the%20first%20time.%20According%0Ato%20whether%20the%20training%20process%20is%20completely%20controlled%2C%20we%20divide%20OOD%0Adetection%20methods%20into%20training-driven%20and%20training-agnostic.%20Besides%2C%0Aconsidering%20the%20rapid%20development%20of%20pre-trained%20models%2C%20large%20pre-trained%0Amodel-based%20OOD%20detection%20is%20also%20regarded%20as%20an%20important%20category%20and%0Adiscussed%20separately.%20Furthermore%2C%20we%20provide%20a%20discussion%20of%20the%20evaluation%0Ascenarios%2C%20a%20variety%20of%20applications%2C%20and%20several%20future%20research%20directions.%0AWe%20believe%20this%20survey%20with%20new%20taxonomy%20will%20benefit%20the%20proposal%20of%20new%0Amethods%20and%20the%20expansion%20of%20more%20practical%20scenarios.%20A%20curated%20list%20of%0Arelated%20papers%20is%20provided%20in%20the%20Github%20repository%3A%0A%5Curl%7Bhttps%3A//github.com/shuolucs/Awesome-Out-Of-Distribution-Detection%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520OOD%2520Detection%253A%2520Problems%2520and%2520Approaches%26entry.906535625%3DShuo%2520Lu%2520and%2520YingSheng%2520Wang%2520and%2520LuJun%2520Sheng%2520and%2520AiHua%2520Zheng%2520and%2520LinXiao%2520He%2520and%2520Jian%2520Liang%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520aims%2520to%2520detect%2520test%2520samples%2520outside%2520the%250Atraining%2520category%2520space%252C%2520which%2520is%2520an%2520essential%2520component%2520in%2520building%2520reliable%250Amachine%2520learning%2520systems.%2520Existing%2520reviews%2520on%2520OOD%2520detection%2520primarily%2520focus%2520on%250Amethod%2520taxonomy%252C%2520surveying%2520the%2520field%2520by%2520categorizing%2520various%2520approaches.%250AHowever%252C%2520many%2520recent%2520works%2520concentrate%2520on%2520non-traditional%2520OOD%2520detection%250Ascenarios%252C%2520such%2520as%2520test-time%2520adaptation%252C%2520multi-modal%2520data%2520sources%2520and%2520other%250Anovel%2520contexts.%2520In%2520this%2520survey%252C%2520we%2520uniquely%2520review%2520recent%2520advances%2520in%2520OOD%250Adetection%2520from%2520the%2520problem%2520scenario%2520perspective%2520for%2520the%2520first%2520time.%2520According%250Ato%2520whether%2520the%2520training%2520process%2520is%2520completely%2520controlled%252C%2520we%2520divide%2520OOD%250Adetection%2520methods%2520into%2520training-driven%2520and%2520training-agnostic.%2520Besides%252C%250Aconsidering%2520the%2520rapid%2520development%2520of%2520pre-trained%2520models%252C%2520large%2520pre-trained%250Amodel-based%2520OOD%2520detection%2520is%2520also%2520regarded%2520as%2520an%2520important%2520category%2520and%250Adiscussed%2520separately.%2520Furthermore%252C%2520we%2520provide%2520a%2520discussion%2520of%2520the%2520evaluation%250Ascenarios%252C%2520a%2520variety%2520of%2520applications%252C%2520and%2520several%2520future%2520research%2520directions.%250AWe%2520believe%2520this%2520survey%2520with%2520new%2520taxonomy%2520will%2520benefit%2520the%2520proposal%2520of%2520new%250Amethods%2520and%2520the%2520expansion%2520of%2520more%2520practical%2520scenarios.%2520A%2520curated%2520list%2520of%250Arelated%2520papers%2520is%2520provided%2520in%2520the%2520Github%2520repository%253A%250A%255Curl%257Bhttps%253A//github.com/shuolucs/Awesome-Out-Of-Distribution-Detection%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20OOD%20Detection%3A%20Problems%20and%20Approaches&entry.906535625=Shuo%20Lu%20and%20YingSheng%20Wang%20and%20LuJun%20Sheng%20and%20AiHua%20Zheng%20and%20LinXiao%20He%20and%20Jian%20Liang&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20aims%20to%20detect%20test%20samples%20outside%20the%0Atraining%20category%20space%2C%20which%20is%20an%20essential%20component%20in%20building%20reliable%0Amachine%20learning%20systems.%20Existing%20reviews%20on%20OOD%20detection%20primarily%20focus%20on%0Amethod%20taxonomy%2C%20surveying%20the%20field%20by%20categorizing%20various%20approaches.%0AHowever%2C%20many%20recent%20works%20concentrate%20on%20non-traditional%20OOD%20detection%0Ascenarios%2C%20such%20as%20test-time%20adaptation%2C%20multi-modal%20data%20sources%20and%20other%0Anovel%20contexts.%20In%20this%20survey%2C%20we%20uniquely%20review%20recent%20advances%20in%20OOD%0Adetection%20from%20the%20problem%20scenario%20perspective%20for%20the%20first%20time.%20According%0Ato%20whether%20the%20training%20process%20is%20completely%20controlled%2C%20we%20divide%20OOD%0Adetection%20methods%20into%20training-driven%20and%20training-agnostic.%20Besides%2C%0Aconsidering%20the%20rapid%20development%20of%20pre-trained%20models%2C%20large%20pre-trained%0Amodel-based%20OOD%20detection%20is%20also%20regarded%20as%20an%20important%20category%20and%0Adiscussed%20separately.%20Furthermore%2C%20we%20provide%20a%20discussion%20of%20the%20evaluation%0Ascenarios%2C%20a%20variety%20of%20applications%2C%20and%20several%20future%20research%20directions.%0AWe%20believe%20this%20survey%20with%20new%20taxonomy%20will%20benefit%20the%20proposal%20of%20new%0Amethods%20and%20the%20expansion%20of%20more%20practical%20scenarios.%20A%20curated%20list%20of%0Arelated%20papers%20is%20provided%20in%20the%20Github%20repository%3A%0A%5Curl%7Bhttps%3A//github.com/shuolucs/Awesome-Out-Of-Distribution-Detection%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11884v1&entry.124074799=Read"},
{"title": "A Fisher-Rao gradient flow for entropic mean-field min-max games", "author": "Razvan-Andrei Lascu and Mateusz B. Majka and \u0141ukasz Szpruch", "abstract": "  Gradient flows play a substantial role in addressing many machine learning\nproblems. We examine the convergence in continuous-time of a\n\\textit{Fisher-Rao} (Mean-Field Birth-Death) gradient flow in the context of\nsolving convex-concave min-max games with entropy regularization. We propose\nappropriate Lyapunov functions to demonstrate convergence with explicit rates\nto the unique mixed Nash equilibrium.\n", "link": "http://arxiv.org/abs/2405.15834v2", "date": "2024-09-18", "relevancy": 1.2346, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4422}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4043}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Fisher-Rao%20gradient%20flow%20for%20entropic%20mean-field%20min-max%20games&body=Title%3A%20A%20Fisher-Rao%20gradient%20flow%20for%20entropic%20mean-field%20min-max%20games%0AAuthor%3A%20Razvan-Andrei%20Lascu%20and%20Mateusz%20B.%20Majka%20and%20%C5%81ukasz%20Szpruch%0AAbstract%3A%20%20%20Gradient%20flows%20play%20a%20substantial%20role%20in%20addressing%20many%20machine%20learning%0Aproblems.%20We%20examine%20the%20convergence%20in%20continuous-time%20of%20a%0A%5Ctextit%7BFisher-Rao%7D%20%28Mean-Field%20Birth-Death%29%20gradient%20flow%20in%20the%20context%20of%0Asolving%20convex-concave%20min-max%20games%20with%20entropy%20regularization.%20We%20propose%0Aappropriate%20Lyapunov%20functions%20to%20demonstrate%20convergence%20with%20explicit%20rates%0Ato%20the%20unique%20mixed%20Nash%20equilibrium.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Fisher-Rao%2520gradient%2520flow%2520for%2520entropic%2520mean-field%2520min-max%2520games%26entry.906535625%3DRazvan-Andrei%2520Lascu%2520and%2520Mateusz%2520B.%2520Majka%2520and%2520%25C5%2581ukasz%2520Szpruch%26entry.1292438233%3D%2520%2520Gradient%2520flows%2520play%2520a%2520substantial%2520role%2520in%2520addressing%2520many%2520machine%2520learning%250Aproblems.%2520We%2520examine%2520the%2520convergence%2520in%2520continuous-time%2520of%2520a%250A%255Ctextit%257BFisher-Rao%257D%2520%2528Mean-Field%2520Birth-Death%2529%2520gradient%2520flow%2520in%2520the%2520context%2520of%250Asolving%2520convex-concave%2520min-max%2520games%2520with%2520entropy%2520regularization.%2520We%2520propose%250Aappropriate%2520Lyapunov%2520functions%2520to%2520demonstrate%2520convergence%2520with%2520explicit%2520rates%250Ato%2520the%2520unique%2520mixed%2520Nash%2520equilibrium.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fisher-Rao%20gradient%20flow%20for%20entropic%20mean-field%20min-max%20games&entry.906535625=Razvan-Andrei%20Lascu%20and%20Mateusz%20B.%20Majka%20and%20%C5%81ukasz%20Szpruch&entry.1292438233=%20%20Gradient%20flows%20play%20a%20substantial%20role%20in%20addressing%20many%20machine%20learning%0Aproblems.%20We%20examine%20the%20convergence%20in%20continuous-time%20of%20a%0A%5Ctextit%7BFisher-Rao%7D%20%28Mean-Field%20Birth-Death%29%20gradient%20flow%20in%20the%20context%20of%0Asolving%20convex-concave%20min-max%20games%20with%20entropy%20regularization.%20We%20propose%0Aappropriate%20Lyapunov%20functions%20to%20demonstrate%20convergence%20with%20explicit%20rates%0Ato%20the%20unique%20mixed%20Nash%20equilibrium.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15834v2&entry.124074799=Read"},
{"title": "Almost Sure Convergence of Linear Temporal Difference Learning with\n  Arbitrary Features", "author": "Jiuqi Wang and Shangtong Zhang", "abstract": "  Temporal difference (TD) learning with linear function approximation,\nabbreviated as linear TD, is a classic and powerful prediction algorithm in\nreinforcement learning. While it is well understood that linear TD converges\nalmost surely to a unique point, this convergence traditionally requires the\nassumption that the features used by the approximator are linearly independent.\nHowever, this linear independence assumption does not hold in many practical\nscenarios. This work is the first to establish the almost sure convergence of\nlinear TD without requiring linearly independent features. In fact, we do not\nmake any assumptions on the features. We prove that the approximated value\nfunction converges to a unique point and the weight iterates converge to a set.\nWe also establish a notion of local stability of the weight iterates.\nImportantly, we do not need to introduce any other additional assumptions and\ndo not need to make any modification to the linear TD algorithm. Key to our\nanalysis is a novel characterization of bounded invariant sets of the mean ODE\nof linear TD.\n", "link": "http://arxiv.org/abs/2409.12135v1", "date": "2024-09-18", "relevancy": 1.3793, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4655}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Almost%20Sure%20Convergence%20of%20Linear%20Temporal%20Difference%20Learning%20with%0A%20%20Arbitrary%20Features&body=Title%3A%20Almost%20Sure%20Convergence%20of%20Linear%20Temporal%20Difference%20Learning%20with%0A%20%20Arbitrary%20Features%0AAuthor%3A%20Jiuqi%20Wang%20and%20Shangtong%20Zhang%0AAbstract%3A%20%20%20Temporal%20difference%20%28TD%29%20learning%20with%20linear%20function%20approximation%2C%0Aabbreviated%20as%20linear%20TD%2C%20is%20a%20classic%20and%20powerful%20prediction%20algorithm%20in%0Areinforcement%20learning.%20While%20it%20is%20well%20understood%20that%20linear%20TD%20converges%0Aalmost%20surely%20to%20a%20unique%20point%2C%20this%20convergence%20traditionally%20requires%20the%0Aassumption%20that%20the%20features%20used%20by%20the%20approximator%20are%20linearly%20independent.%0AHowever%2C%20this%20linear%20independence%20assumption%20does%20not%20hold%20in%20many%20practical%0Ascenarios.%20This%20work%20is%20the%20first%20to%20establish%20the%20almost%20sure%20convergence%20of%0Alinear%20TD%20without%20requiring%20linearly%20independent%20features.%20In%20fact%2C%20we%20do%20not%0Amake%20any%20assumptions%20on%20the%20features.%20We%20prove%20that%20the%20approximated%20value%0Afunction%20converges%20to%20a%20unique%20point%20and%20the%20weight%20iterates%20converge%20to%20a%20set.%0AWe%20also%20establish%20a%20notion%20of%20local%20stability%20of%20the%20weight%20iterates.%0AImportantly%2C%20we%20do%20not%20need%20to%20introduce%20any%20other%20additional%20assumptions%20and%0Ado%20not%20need%20to%20make%20any%20modification%20to%20the%20linear%20TD%20algorithm.%20Key%20to%20our%0Aanalysis%20is%20a%20novel%20characterization%20of%20bounded%20invariant%20sets%20of%20the%20mean%20ODE%0Aof%20linear%20TD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlmost%2520Sure%2520Convergence%2520of%2520Linear%2520Temporal%2520Difference%2520Learning%2520with%250A%2520%2520Arbitrary%2520Features%26entry.906535625%3DJiuqi%2520Wang%2520and%2520Shangtong%2520Zhang%26entry.1292438233%3D%2520%2520Temporal%2520difference%2520%2528TD%2529%2520learning%2520with%2520linear%2520function%2520approximation%252C%250Aabbreviated%2520as%2520linear%2520TD%252C%2520is%2520a%2520classic%2520and%2520powerful%2520prediction%2520algorithm%2520in%250Areinforcement%2520learning.%2520While%2520it%2520is%2520well%2520understood%2520that%2520linear%2520TD%2520converges%250Aalmost%2520surely%2520to%2520a%2520unique%2520point%252C%2520this%2520convergence%2520traditionally%2520requires%2520the%250Aassumption%2520that%2520the%2520features%2520used%2520by%2520the%2520approximator%2520are%2520linearly%2520independent.%250AHowever%252C%2520this%2520linear%2520independence%2520assumption%2520does%2520not%2520hold%2520in%2520many%2520practical%250Ascenarios.%2520This%2520work%2520is%2520the%2520first%2520to%2520establish%2520the%2520almost%2520sure%2520convergence%2520of%250Alinear%2520TD%2520without%2520requiring%2520linearly%2520independent%2520features.%2520In%2520fact%252C%2520we%2520do%2520not%250Amake%2520any%2520assumptions%2520on%2520the%2520features.%2520We%2520prove%2520that%2520the%2520approximated%2520value%250Afunction%2520converges%2520to%2520a%2520unique%2520point%2520and%2520the%2520weight%2520iterates%2520converge%2520to%2520a%2520set.%250AWe%2520also%2520establish%2520a%2520notion%2520of%2520local%2520stability%2520of%2520the%2520weight%2520iterates.%250AImportantly%252C%2520we%2520do%2520not%2520need%2520to%2520introduce%2520any%2520other%2520additional%2520assumptions%2520and%250Ado%2520not%2520need%2520to%2520make%2520any%2520modification%2520to%2520the%2520linear%2520TD%2520algorithm.%2520Key%2520to%2520our%250Aanalysis%2520is%2520a%2520novel%2520characterization%2520of%2520bounded%2520invariant%2520sets%2520of%2520the%2520mean%2520ODE%250Aof%2520linear%2520TD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Almost%20Sure%20Convergence%20of%20Linear%20Temporal%20Difference%20Learning%20with%0A%20%20Arbitrary%20Features&entry.906535625=Jiuqi%20Wang%20and%20Shangtong%20Zhang&entry.1292438233=%20%20Temporal%20difference%20%28TD%29%20learning%20with%20linear%20function%20approximation%2C%0Aabbreviated%20as%20linear%20TD%2C%20is%20a%20classic%20and%20powerful%20prediction%20algorithm%20in%0Areinforcement%20learning.%20While%20it%20is%20well%20understood%20that%20linear%20TD%20converges%0Aalmost%20surely%20to%20a%20unique%20point%2C%20this%20convergence%20traditionally%20requires%20the%0Aassumption%20that%20the%20features%20used%20by%20the%20approximator%20are%20linearly%20independent.%0AHowever%2C%20this%20linear%20independence%20assumption%20does%20not%20hold%20in%20many%20practical%0Ascenarios.%20This%20work%20is%20the%20first%20to%20establish%20the%20almost%20sure%20convergence%20of%0Alinear%20TD%20without%20requiring%20linearly%20independent%20features.%20In%20fact%2C%20we%20do%20not%0Amake%20any%20assumptions%20on%20the%20features.%20We%20prove%20that%20the%20approximated%20value%0Afunction%20converges%20to%20a%20unique%20point%20and%20the%20weight%20iterates%20converge%20to%20a%20set.%0AWe%20also%20establish%20a%20notion%20of%20local%20stability%20of%20the%20weight%20iterates.%0AImportantly%2C%20we%20do%20not%20need%20to%20introduce%20any%20other%20additional%20assumptions%20and%0Ado%20not%20need%20to%20make%20any%20modification%20to%20the%20linear%20TD%20algorithm.%20Key%20to%20our%0Aanalysis%20is%20a%20novel%20characterization%20of%20bounded%20invariant%20sets%20of%20the%20mean%20ODE%0Aof%20linear%20TD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12135v1&entry.124074799=Read"},
{"title": "Calibration Error for Decision Making", "author": "Lunjia Hu and Yifan Wu", "abstract": "  Calibration allows predictions to be reliably interpreted as probabilities by\ndecision makers. We propose a decision-theoretic calibration error, the\nCalibration Decision Loss (CDL), defined as the maximum improvement in decision\npayoff obtained by calibrating the predictions, where the maximum is over all\npayoff-bounded decision tasks. Vanishing CDL guarantees the payoff loss from\nmiscalibration vanishes simultaneously for all downstream decision tasks. We\nshow separations between CDL and existing calibration error metrics, including\nthe most well-studied metric Expected Calibration Error (ECE). Our main\ntechnical contribution is a new efficient algorithm for online calibration that\nachieves near-optimal $O(\\frac{\\log T}{\\sqrt{T}})$ expected CDL, bypassing the\n$\\Omega(T^{-0.472})$ lower bound for ECE by Qiao and Valiant (2021).\n", "link": "http://arxiv.org/abs/2404.13503v3", "date": "2024-09-18", "relevancy": 1.832, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4805}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibration%20Error%20for%20Decision%20Making&body=Title%3A%20Calibration%20Error%20for%20Decision%20Making%0AAuthor%3A%20Lunjia%20Hu%20and%20Yifan%20Wu%0AAbstract%3A%20%20%20Calibration%20allows%20predictions%20to%20be%20reliably%20interpreted%20as%20probabilities%20by%0Adecision%20makers.%20We%20propose%20a%20decision-theoretic%20calibration%20error%2C%20the%0ACalibration%20Decision%20Loss%20%28CDL%29%2C%20defined%20as%20the%20maximum%20improvement%20in%20decision%0Apayoff%20obtained%20by%20calibrating%20the%20predictions%2C%20where%20the%20maximum%20is%20over%20all%0Apayoff-bounded%20decision%20tasks.%20Vanishing%20CDL%20guarantees%20the%20payoff%20loss%20from%0Amiscalibration%20vanishes%20simultaneously%20for%20all%20downstream%20decision%20tasks.%20We%0Ashow%20separations%20between%20CDL%20and%20existing%20calibration%20error%20metrics%2C%20including%0Athe%20most%20well-studied%20metric%20Expected%20Calibration%20Error%20%28ECE%29.%20Our%20main%0Atechnical%20contribution%20is%20a%20new%20efficient%20algorithm%20for%20online%20calibration%20that%0Aachieves%20near-optimal%20%24O%28%5Cfrac%7B%5Clog%20T%7D%7B%5Csqrt%7BT%7D%7D%29%24%20expected%20CDL%2C%20bypassing%20the%0A%24%5COmega%28T%5E%7B-0.472%7D%29%24%20lower%20bound%20for%20ECE%20by%20Qiao%20and%20Valiant%20%282021%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibration%2520Error%2520for%2520Decision%2520Making%26entry.906535625%3DLunjia%2520Hu%2520and%2520Yifan%2520Wu%26entry.1292438233%3D%2520%2520Calibration%2520allows%2520predictions%2520to%2520be%2520reliably%2520interpreted%2520as%2520probabilities%2520by%250Adecision%2520makers.%2520We%2520propose%2520a%2520decision-theoretic%2520calibration%2520error%252C%2520the%250ACalibration%2520Decision%2520Loss%2520%2528CDL%2529%252C%2520defined%2520as%2520the%2520maximum%2520improvement%2520in%2520decision%250Apayoff%2520obtained%2520by%2520calibrating%2520the%2520predictions%252C%2520where%2520the%2520maximum%2520is%2520over%2520all%250Apayoff-bounded%2520decision%2520tasks.%2520Vanishing%2520CDL%2520guarantees%2520the%2520payoff%2520loss%2520from%250Amiscalibration%2520vanishes%2520simultaneously%2520for%2520all%2520downstream%2520decision%2520tasks.%2520We%250Ashow%2520separations%2520between%2520CDL%2520and%2520existing%2520calibration%2520error%2520metrics%252C%2520including%250Athe%2520most%2520well-studied%2520metric%2520Expected%2520Calibration%2520Error%2520%2528ECE%2529.%2520Our%2520main%250Atechnical%2520contribution%2520is%2520a%2520new%2520efficient%2520algorithm%2520for%2520online%2520calibration%2520that%250Aachieves%2520near-optimal%2520%2524O%2528%255Cfrac%257B%255Clog%2520T%257D%257B%255Csqrt%257BT%257D%257D%2529%2524%2520expected%2520CDL%252C%2520bypassing%2520the%250A%2524%255COmega%2528T%255E%257B-0.472%257D%2529%2524%2520lower%2520bound%2520for%2520ECE%2520by%2520Qiao%2520and%2520Valiant%2520%25282021%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibration%20Error%20for%20Decision%20Making&entry.906535625=Lunjia%20Hu%20and%20Yifan%20Wu&entry.1292438233=%20%20Calibration%20allows%20predictions%20to%20be%20reliably%20interpreted%20as%20probabilities%20by%0Adecision%20makers.%20We%20propose%20a%20decision-theoretic%20calibration%20error%2C%20the%0ACalibration%20Decision%20Loss%20%28CDL%29%2C%20defined%20as%20the%20maximum%20improvement%20in%20decision%0Apayoff%20obtained%20by%20calibrating%20the%20predictions%2C%20where%20the%20maximum%20is%20over%20all%0Apayoff-bounded%20decision%20tasks.%20Vanishing%20CDL%20guarantees%20the%20payoff%20loss%20from%0Amiscalibration%20vanishes%20simultaneously%20for%20all%20downstream%20decision%20tasks.%20We%0Ashow%20separations%20between%20CDL%20and%20existing%20calibration%20error%20metrics%2C%20including%0Athe%20most%20well-studied%20metric%20Expected%20Calibration%20Error%20%28ECE%29.%20Our%20main%0Atechnical%20contribution%20is%20a%20new%20efficient%20algorithm%20for%20online%20calibration%20that%0Aachieves%20near-optimal%20%24O%28%5Cfrac%7B%5Clog%20T%7D%7B%5Csqrt%7BT%7D%7D%29%24%20expected%20CDL%2C%20bypassing%20the%0A%24%5COmega%28T%5E%7B-0.472%7D%29%24%20lower%20bound%20for%20ECE%20by%20Qiao%20and%20Valiant%20%282021%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13503v3&entry.124074799=Read"},
{"title": "An efficient wavelet-based physics-informed neural networks for\n  singularly perturbed problems", "author": "Himanshu Pandey and Anshima Singh and Ratikanta Behera", "abstract": "  Physics-informed neural networks (PINNs) are a class of deep learning models\nthat utilize physics as differential equations to address complex problems,\nincluding ones that may involve limited data availability. However, tackling\nsolutions of differential equations with oscillations or singular perturbations\nand shock-like structures becomes challenging for PINNs. Considering these\nchallenges, we designed an efficient wavelet-based PINNs (W-PINNs) model to\nsolve singularly perturbed differential equations. Here, we represent the\nsolution in wavelet space using a family of smooth-compactly supported\nwavelets. This framework represents the solution of a differential equation\nwith significantly fewer degrees of freedom while still retaining in capturing,\nidentifying, and analyzing the local structure of complex physical phenomena.\nThe architecture allows the training process to search for a solution within\nwavelet space, making the process faster and more accurate. The proposed model\ndoes not rely on automatic differentiations for derivatives involved in\ndifferential equations and does not require any prior information regarding the\nbehavior of the solution, such as the location of abrupt features. Thus,\nthrough a strategic fusion of wavelets with PINNs, W-PINNs excel at capturing\nlocalized nonlinear information, making them well-suited for problems showing\nabrupt behavior in certain regions, such as singularly perturbed problems. The\nefficiency and accuracy of the proposed neural network model are demonstrated\nin various test problems, i.e., highly singularly perturbed nonlinear\ndifferential equations, the FitzHugh-Nagumo (FHN), and Predator-prey\ninteraction models. The proposed design model exhibits impressive comparisons\nwith traditional PINNs and the recently developed wavelet-based PINNs, which\nuse wavelets as an activation function for solving nonlinear differential\nequations.\n", "link": "http://arxiv.org/abs/2409.11847v1", "date": "2024-09-18", "relevancy": 1.3915, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4599}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20efficient%20wavelet-based%20physics-informed%20neural%20networks%20for%0A%20%20singularly%20perturbed%20problems&body=Title%3A%20An%20efficient%20wavelet-based%20physics-informed%20neural%20networks%20for%0A%20%20singularly%20perturbed%20problems%0AAuthor%3A%20Himanshu%20Pandey%20and%20Anshima%20Singh%20and%20Ratikanta%20Behera%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20a%20class%20of%20deep%20learning%20models%0Athat%20utilize%20physics%20as%20differential%20equations%20to%20address%20complex%20problems%2C%0Aincluding%20ones%20that%20may%20involve%20limited%20data%20availability.%20However%2C%20tackling%0Asolutions%20of%20differential%20equations%20with%20oscillations%20or%20singular%20perturbations%0Aand%20shock-like%20structures%20becomes%20challenging%20for%20PINNs.%20Considering%20these%0Achallenges%2C%20we%20designed%20an%20efficient%20wavelet-based%20PINNs%20%28W-PINNs%29%20model%20to%0Asolve%20singularly%20perturbed%20differential%20equations.%20Here%2C%20we%20represent%20the%0Asolution%20in%20wavelet%20space%20using%20a%20family%20of%20smooth-compactly%20supported%0Awavelets.%20This%20framework%20represents%20the%20solution%20of%20a%20differential%20equation%0Awith%20significantly%20fewer%20degrees%20of%20freedom%20while%20still%20retaining%20in%20capturing%2C%0Aidentifying%2C%20and%20analyzing%20the%20local%20structure%20of%20complex%20physical%20phenomena.%0AThe%20architecture%20allows%20the%20training%20process%20to%20search%20for%20a%20solution%20within%0Awavelet%20space%2C%20making%20the%20process%20faster%20and%20more%20accurate.%20The%20proposed%20model%0Adoes%20not%20rely%20on%20automatic%20differentiations%20for%20derivatives%20involved%20in%0Adifferential%20equations%20and%20does%20not%20require%20any%20prior%20information%20regarding%20the%0Abehavior%20of%20the%20solution%2C%20such%20as%20the%20location%20of%20abrupt%20features.%20Thus%2C%0Athrough%20a%20strategic%20fusion%20of%20wavelets%20with%20PINNs%2C%20W-PINNs%20excel%20at%20capturing%0Alocalized%20nonlinear%20information%2C%20making%20them%20well-suited%20for%20problems%20showing%0Aabrupt%20behavior%20in%20certain%20regions%2C%20such%20as%20singularly%20perturbed%20problems.%20The%0Aefficiency%20and%20accuracy%20of%20the%20proposed%20neural%20network%20model%20are%20demonstrated%0Ain%20various%20test%20problems%2C%20i.e.%2C%20highly%20singularly%20perturbed%20nonlinear%0Adifferential%20equations%2C%20the%20FitzHugh-Nagumo%20%28FHN%29%2C%20and%20Predator-prey%0Ainteraction%20models.%20The%20proposed%20design%20model%20exhibits%20impressive%20comparisons%0Awith%20traditional%20PINNs%20and%20the%20recently%20developed%20wavelet-based%20PINNs%2C%20which%0Ause%20wavelets%20as%20an%20activation%20function%20for%20solving%20nonlinear%20differential%0Aequations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520efficient%2520wavelet-based%2520physics-informed%2520neural%2520networks%2520for%250A%2520%2520singularly%2520perturbed%2520problems%26entry.906535625%3DHimanshu%2520Pandey%2520and%2520Anshima%2520Singh%2520and%2520Ratikanta%2520Behera%26entry.1292438233%3D%2520%2520Physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520are%2520a%2520class%2520of%2520deep%2520learning%2520models%250Athat%2520utilize%2520physics%2520as%2520differential%2520equations%2520to%2520address%2520complex%2520problems%252C%250Aincluding%2520ones%2520that%2520may%2520involve%2520limited%2520data%2520availability.%2520However%252C%2520tackling%250Asolutions%2520of%2520differential%2520equations%2520with%2520oscillations%2520or%2520singular%2520perturbations%250Aand%2520shock-like%2520structures%2520becomes%2520challenging%2520for%2520PINNs.%2520Considering%2520these%250Achallenges%252C%2520we%2520designed%2520an%2520efficient%2520wavelet-based%2520PINNs%2520%2528W-PINNs%2529%2520model%2520to%250Asolve%2520singularly%2520perturbed%2520differential%2520equations.%2520Here%252C%2520we%2520represent%2520the%250Asolution%2520in%2520wavelet%2520space%2520using%2520a%2520family%2520of%2520smooth-compactly%2520supported%250Awavelets.%2520This%2520framework%2520represents%2520the%2520solution%2520of%2520a%2520differential%2520equation%250Awith%2520significantly%2520fewer%2520degrees%2520of%2520freedom%2520while%2520still%2520retaining%2520in%2520capturing%252C%250Aidentifying%252C%2520and%2520analyzing%2520the%2520local%2520structure%2520of%2520complex%2520physical%2520phenomena.%250AThe%2520architecture%2520allows%2520the%2520training%2520process%2520to%2520search%2520for%2520a%2520solution%2520within%250Awavelet%2520space%252C%2520making%2520the%2520process%2520faster%2520and%2520more%2520accurate.%2520The%2520proposed%2520model%250Adoes%2520not%2520rely%2520on%2520automatic%2520differentiations%2520for%2520derivatives%2520involved%2520in%250Adifferential%2520equations%2520and%2520does%2520not%2520require%2520any%2520prior%2520information%2520regarding%2520the%250Abehavior%2520of%2520the%2520solution%252C%2520such%2520as%2520the%2520location%2520of%2520abrupt%2520features.%2520Thus%252C%250Athrough%2520a%2520strategic%2520fusion%2520of%2520wavelets%2520with%2520PINNs%252C%2520W-PINNs%2520excel%2520at%2520capturing%250Alocalized%2520nonlinear%2520information%252C%2520making%2520them%2520well-suited%2520for%2520problems%2520showing%250Aabrupt%2520behavior%2520in%2520certain%2520regions%252C%2520such%2520as%2520singularly%2520perturbed%2520problems.%2520The%250Aefficiency%2520and%2520accuracy%2520of%2520the%2520proposed%2520neural%2520network%2520model%2520are%2520demonstrated%250Ain%2520various%2520test%2520problems%252C%2520i.e.%252C%2520highly%2520singularly%2520perturbed%2520nonlinear%250Adifferential%2520equations%252C%2520the%2520FitzHugh-Nagumo%2520%2528FHN%2529%252C%2520and%2520Predator-prey%250Ainteraction%2520models.%2520The%2520proposed%2520design%2520model%2520exhibits%2520impressive%2520comparisons%250Awith%2520traditional%2520PINNs%2520and%2520the%2520recently%2520developed%2520wavelet-based%2520PINNs%252C%2520which%250Ause%2520wavelets%2520as%2520an%2520activation%2520function%2520for%2520solving%2520nonlinear%2520differential%250Aequations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20efficient%20wavelet-based%20physics-informed%20neural%20networks%20for%0A%20%20singularly%20perturbed%20problems&entry.906535625=Himanshu%20Pandey%20and%20Anshima%20Singh%20and%20Ratikanta%20Behera&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20are%20a%20class%20of%20deep%20learning%20models%0Athat%20utilize%20physics%20as%20differential%20equations%20to%20address%20complex%20problems%2C%0Aincluding%20ones%20that%20may%20involve%20limited%20data%20availability.%20However%2C%20tackling%0Asolutions%20of%20differential%20equations%20with%20oscillations%20or%20singular%20perturbations%0Aand%20shock-like%20structures%20becomes%20challenging%20for%20PINNs.%20Considering%20these%0Achallenges%2C%20we%20designed%20an%20efficient%20wavelet-based%20PINNs%20%28W-PINNs%29%20model%20to%0Asolve%20singularly%20perturbed%20differential%20equations.%20Here%2C%20we%20represent%20the%0Asolution%20in%20wavelet%20space%20using%20a%20family%20of%20smooth-compactly%20supported%0Awavelets.%20This%20framework%20represents%20the%20solution%20of%20a%20differential%20equation%0Awith%20significantly%20fewer%20degrees%20of%20freedom%20while%20still%20retaining%20in%20capturing%2C%0Aidentifying%2C%20and%20analyzing%20the%20local%20structure%20of%20complex%20physical%20phenomena.%0AThe%20architecture%20allows%20the%20training%20process%20to%20search%20for%20a%20solution%20within%0Awavelet%20space%2C%20making%20the%20process%20faster%20and%20more%20accurate.%20The%20proposed%20model%0Adoes%20not%20rely%20on%20automatic%20differentiations%20for%20derivatives%20involved%20in%0Adifferential%20equations%20and%20does%20not%20require%20any%20prior%20information%20regarding%20the%0Abehavior%20of%20the%20solution%2C%20such%20as%20the%20location%20of%20abrupt%20features.%20Thus%2C%0Athrough%20a%20strategic%20fusion%20of%20wavelets%20with%20PINNs%2C%20W-PINNs%20excel%20at%20capturing%0Alocalized%20nonlinear%20information%2C%20making%20them%20well-suited%20for%20problems%20showing%0Aabrupt%20behavior%20in%20certain%20regions%2C%20such%20as%20singularly%20perturbed%20problems.%20The%0Aefficiency%20and%20accuracy%20of%20the%20proposed%20neural%20network%20model%20are%20demonstrated%0Ain%20various%20test%20problems%2C%20i.e.%2C%20highly%20singularly%20perturbed%20nonlinear%0Adifferential%20equations%2C%20the%20FitzHugh-Nagumo%20%28FHN%29%2C%20and%20Predator-prey%0Ainteraction%20models.%20The%20proposed%20design%20model%20exhibits%20impressive%20comparisons%0Awith%20traditional%20PINNs%20and%20the%20recently%20developed%20wavelet-based%20PINNs%2C%20which%0Ause%20wavelets%20as%20an%20activation%20function%20for%20solving%20nonlinear%20differential%0Aequations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11847v1&entry.124074799=Read"},
{"title": "Adaptive Step Sizes for Preconditioned Stochastic Gradient Descent", "author": "Frederik K\u00f6hne and Leonie Kreis and Anton Schiela and Roland Herzog", "abstract": "  This paper proposes a novel approach to adaptive step sizes in stochastic\ngradient descent (SGD) by utilizing quantities that we have identified as\nnumerically traceable -- the Lipschitz constant for gradients and a concept of\nthe local variance in search directions. Our findings yield a nearly\nhyperparameter-free algorithm for stochastic optimization, which has provable\nconvergence properties and exhibits truly problem adaptive behavior on\nclassical image classification tasks. Our framework is set in a general Hilbert\nspace and thus enables the potential inclusion of a preconditioner through the\nchoice of the inner product.\n", "link": "http://arxiv.org/abs/2311.16956v2", "date": "2024-09-18", "relevancy": 1.5735, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5376}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Step%20Sizes%20for%20Preconditioned%20Stochastic%20Gradient%20Descent&body=Title%3A%20Adaptive%20Step%20Sizes%20for%20Preconditioned%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Frederik%20K%C3%B6hne%20and%20Leonie%20Kreis%20and%20Anton%20Schiela%20and%20Roland%20Herzog%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20adaptive%20step%20sizes%20in%20stochastic%0Agradient%20descent%20%28SGD%29%20by%20utilizing%20quantities%20that%20we%20have%20identified%20as%0Anumerically%20traceable%20--%20the%20Lipschitz%20constant%20for%20gradients%20and%20a%20concept%20of%0Athe%20local%20variance%20in%20search%20directions.%20Our%20findings%20yield%20a%20nearly%0Ahyperparameter-free%20algorithm%20for%20stochastic%20optimization%2C%20which%20has%20provable%0Aconvergence%20properties%20and%20exhibits%20truly%20problem%20adaptive%20behavior%20on%0Aclassical%20image%20classification%20tasks.%20Our%20framework%20is%20set%20in%20a%20general%20Hilbert%0Aspace%20and%20thus%20enables%20the%20potential%20inclusion%20of%20a%20preconditioner%20through%20the%0Achoice%20of%20the%20inner%20product.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Step%2520Sizes%2520for%2520Preconditioned%2520Stochastic%2520Gradient%2520Descent%26entry.906535625%3DFrederik%2520K%25C3%25B6hne%2520and%2520Leonie%2520Kreis%2520and%2520Anton%2520Schiela%2520and%2520Roland%2520Herzog%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520to%2520adaptive%2520step%2520sizes%2520in%2520stochastic%250Agradient%2520descent%2520%2528SGD%2529%2520by%2520utilizing%2520quantities%2520that%2520we%2520have%2520identified%2520as%250Anumerically%2520traceable%2520--%2520the%2520Lipschitz%2520constant%2520for%2520gradients%2520and%2520a%2520concept%2520of%250Athe%2520local%2520variance%2520in%2520search%2520directions.%2520Our%2520findings%2520yield%2520a%2520nearly%250Ahyperparameter-free%2520algorithm%2520for%2520stochastic%2520optimization%252C%2520which%2520has%2520provable%250Aconvergence%2520properties%2520and%2520exhibits%2520truly%2520problem%2520adaptive%2520behavior%2520on%250Aclassical%2520image%2520classification%2520tasks.%2520Our%2520framework%2520is%2520set%2520in%2520a%2520general%2520Hilbert%250Aspace%2520and%2520thus%2520enables%2520the%2520potential%2520inclusion%2520of%2520a%2520preconditioner%2520through%2520the%250Achoice%2520of%2520the%2520inner%2520product.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Step%20Sizes%20for%20Preconditioned%20Stochastic%20Gradient%20Descent&entry.906535625=Frederik%20K%C3%B6hne%20and%20Leonie%20Kreis%20and%20Anton%20Schiela%20and%20Roland%20Herzog&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20approach%20to%20adaptive%20step%20sizes%20in%20stochastic%0Agradient%20descent%20%28SGD%29%20by%20utilizing%20quantities%20that%20we%20have%20identified%20as%0Anumerically%20traceable%20--%20the%20Lipschitz%20constant%20for%20gradients%20and%20a%20concept%20of%0Athe%20local%20variance%20in%20search%20directions.%20Our%20findings%20yield%20a%20nearly%0Ahyperparameter-free%20algorithm%20for%20stochastic%20optimization%2C%20which%20has%20provable%0Aconvergence%20properties%20and%20exhibits%20truly%20problem%20adaptive%20behavior%20on%0Aclassical%20image%20classification%20tasks.%20Our%20framework%20is%20set%20in%20a%20general%20Hilbert%0Aspace%20and%20thus%20enables%20the%20potential%20inclusion%20of%20a%20preconditioner%20through%20the%0Achoice%20of%20the%20inner%20product.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16956v2&entry.124074799=Read"},
{"title": "Abductive explanations of classifiers under constraints: Complexity and\n  properties", "author": "Martin Cooper and Leila Amgoud", "abstract": "  Abductive explanations (AXp's) are widely used for understanding decisions of\nclassifiers. Existing definitions are suitable when features are independent.\nHowever, we show that ignoring constraints when they exist between features may\nlead to an explosion in the number of redundant or superfluous AXp's. We\npropose three new types of explanations that take into account constraints and\nthat can be generated from the whole feature space or from a sample (such as a\ndataset). They are based on a key notion of coverage of an explanation, the set\nof instances it explains. We show that coverage is powerful enough to discard\nredundant and superfluous AXp's. For each type, we analyse the complexity of\nfinding an explanation and investigate its formal properties. The final result\nis a catalogue of different forms of AXp's with different complexities and\ndifferent formal guarantees.\n", "link": "http://arxiv.org/abs/2409.12154v1", "date": "2024-09-18", "relevancy": 1.7515, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4396}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Abductive%20explanations%20of%20classifiers%20under%20constraints%3A%20Complexity%20and%0A%20%20properties&body=Title%3A%20Abductive%20explanations%20of%20classifiers%20under%20constraints%3A%20Complexity%20and%0A%20%20properties%0AAuthor%3A%20Martin%20Cooper%20and%20Leila%20Amgoud%0AAbstract%3A%20%20%20Abductive%20explanations%20%28AXp%27s%29%20are%20widely%20used%20for%20understanding%20decisions%20of%0Aclassifiers.%20Existing%20definitions%20are%20suitable%20when%20features%20are%20independent.%0AHowever%2C%20we%20show%20that%20ignoring%20constraints%20when%20they%20exist%20between%20features%20may%0Alead%20to%20an%20explosion%20in%20the%20number%20of%20redundant%20or%20superfluous%20AXp%27s.%20We%0Apropose%20three%20new%20types%20of%20explanations%20that%20take%20into%20account%20constraints%20and%0Athat%20can%20be%20generated%20from%20the%20whole%20feature%20space%20or%20from%20a%20sample%20%28such%20as%20a%0Adataset%29.%20They%20are%20based%20on%20a%20key%20notion%20of%20coverage%20of%20an%20explanation%2C%20the%20set%0Aof%20instances%20it%20explains.%20We%20show%20that%20coverage%20is%20powerful%20enough%20to%20discard%0Aredundant%20and%20superfluous%20AXp%27s.%20For%20each%20type%2C%20we%20analyse%20the%20complexity%20of%0Afinding%20an%20explanation%20and%20investigate%20its%20formal%20properties.%20The%20final%20result%0Ais%20a%20catalogue%20of%20different%20forms%20of%20AXp%27s%20with%20different%20complexities%20and%0Adifferent%20formal%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbductive%2520explanations%2520of%2520classifiers%2520under%2520constraints%253A%2520Complexity%2520and%250A%2520%2520properties%26entry.906535625%3DMartin%2520Cooper%2520and%2520Leila%2520Amgoud%26entry.1292438233%3D%2520%2520Abductive%2520explanations%2520%2528AXp%2527s%2529%2520are%2520widely%2520used%2520for%2520understanding%2520decisions%2520of%250Aclassifiers.%2520Existing%2520definitions%2520are%2520suitable%2520when%2520features%2520are%2520independent.%250AHowever%252C%2520we%2520show%2520that%2520ignoring%2520constraints%2520when%2520they%2520exist%2520between%2520features%2520may%250Alead%2520to%2520an%2520explosion%2520in%2520the%2520number%2520of%2520redundant%2520or%2520superfluous%2520AXp%2527s.%2520We%250Apropose%2520three%2520new%2520types%2520of%2520explanations%2520that%2520take%2520into%2520account%2520constraints%2520and%250Athat%2520can%2520be%2520generated%2520from%2520the%2520whole%2520feature%2520space%2520or%2520from%2520a%2520sample%2520%2528such%2520as%2520a%250Adataset%2529.%2520They%2520are%2520based%2520on%2520a%2520key%2520notion%2520of%2520coverage%2520of%2520an%2520explanation%252C%2520the%2520set%250Aof%2520instances%2520it%2520explains.%2520We%2520show%2520that%2520coverage%2520is%2520powerful%2520enough%2520to%2520discard%250Aredundant%2520and%2520superfluous%2520AXp%2527s.%2520For%2520each%2520type%252C%2520we%2520analyse%2520the%2520complexity%2520of%250Afinding%2520an%2520explanation%2520and%2520investigate%2520its%2520formal%2520properties.%2520The%2520final%2520result%250Ais%2520a%2520catalogue%2520of%2520different%2520forms%2520of%2520AXp%2527s%2520with%2520different%2520complexities%2520and%250Adifferent%2520formal%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Abductive%20explanations%20of%20classifiers%20under%20constraints%3A%20Complexity%20and%0A%20%20properties&entry.906535625=Martin%20Cooper%20and%20Leila%20Amgoud&entry.1292438233=%20%20Abductive%20explanations%20%28AXp%27s%29%20are%20widely%20used%20for%20understanding%20decisions%20of%0Aclassifiers.%20Existing%20definitions%20are%20suitable%20when%20features%20are%20independent.%0AHowever%2C%20we%20show%20that%20ignoring%20constraints%20when%20they%20exist%20between%20features%20may%0Alead%20to%20an%20explosion%20in%20the%20number%20of%20redundant%20or%20superfluous%20AXp%27s.%20We%0Apropose%20three%20new%20types%20of%20explanations%20that%20take%20into%20account%20constraints%20and%0Athat%20can%20be%20generated%20from%20the%20whole%20feature%20space%20or%20from%20a%20sample%20%28such%20as%20a%0Adataset%29.%20They%20are%20based%20on%20a%20key%20notion%20of%20coverage%20of%20an%20explanation%2C%20the%20set%0Aof%20instances%20it%20explains.%20We%20show%20that%20coverage%20is%20powerful%20enough%20to%20discard%0Aredundant%20and%20superfluous%20AXp%27s.%20For%20each%20type%2C%20we%20analyse%20the%20complexity%20of%0Afinding%20an%20explanation%20and%20investigate%20its%20formal%20properties.%20The%20final%20result%0Ais%20a%20catalogue%20of%20different%20forms%20of%20AXp%27s%20with%20different%20complexities%20and%0Adifferent%20formal%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12154v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


