<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240512.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via\n  Generative Prior", "author": "Gihoon Kim and Kwanggyoon Seo and Sihun Cha and Junyong Noh", "abstract": "  Audio-driven talking head generation is advancing from 2D to 3D content.\nNotably, Neural Radiance Field (NeRF) is in the spotlight as a means to\nsynthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based\napproach typically requires a large number of paired audio-visual data for each\nidentity, thereby limiting the scalability of the method. Although there have\nbeen attempts to generate audio-driven 3D talking head animations with a single\nimage, the results are often unsatisfactory due to insufficient information on\nobscured regions in the image. In this paper, we mainly focus on addressing the\noverlooked aspect of 3D consistency in the one-shot, audio-driven domain, where\nfacial animations are synthesized primarily in front-facing perspectives. We\npropose a novel method, NeRFFaceSpeech, which enables to produce high-quality\n3D-aware talking head. Using prior knowledge of generative models combined with\nNeRF, our method can craft a 3D-consistent facial feature space corresponding\nto a single image. Our spatial synchronization method employs audio-correlated\nvertex dynamics of a parametric face model to transform static image features\ninto dynamic visuals through ray deformation, ensuring realistic 3D facial\nmotion. Moreover, we introduce LipaintNet that can replenish the lacking\ninformation in the inner-mouth area, which can not be obtained from a given\nsingle image. The network is trained in a self-supervised manner by utilizing\nthe generative capabilities without additional data. The comprehensive\nexperiments demonstrate the superiority of our method in generating\naudio-driven talking heads from a single image with enhanced 3D consistency\ncompared to previous approaches. In addition, we introduce a quantitative way\nof measuring the robustness of a model against pose changes for the first time,\nwhich has been possible only qualitatively.\n", "link": "http://arxiv.org/abs/2405.05749v2", "date": "2024-05-10", "relevancy": 3.0138, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6164}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6164}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRFFaceSpeech%3A%20One-shot%20Audio-driven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior&body=Title%3A%20NeRFFaceSpeech%3A%20One-shot%20Audio-driven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior%0AAuthor%3A%20Gihoon%20Kim%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Junyong%20Noh%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20is%20advancing%20from%202D%20to%203D%20content.%0ANotably%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20in%20the%20spotlight%20as%20a%20means%20to%0Asynthesize%20high-quality%203D%20talking%20head%20outputs.%20Unfortunately%2C%20this%20NeRF-based%0Aapproach%20typically%20requires%20a%20large%20number%20of%20paired%20audio-visual%20data%20for%20each%0Aidentity%2C%20thereby%20limiting%20the%20scalability%20of%20the%20method.%20Although%20there%20have%0Abeen%20attempts%20to%20generate%20audio-driven%203D%20talking%20head%20animations%20with%20a%20single%0Aimage%2C%20the%20results%20are%20often%20unsatisfactory%20due%20to%20insufficient%20information%20on%0Aobscured%20regions%20in%20the%20image.%20In%20this%20paper%2C%20we%20mainly%20focus%20on%20addressing%20the%0Aoverlooked%20aspect%20of%203D%20consistency%20in%20the%20one-shot%2C%20audio-driven%20domain%2C%20where%0Afacial%20animations%20are%20synthesized%20primarily%20in%20front-facing%20perspectives.%20We%0Apropose%20a%20novel%20method%2C%20NeRFFaceSpeech%2C%20which%20enables%20to%20produce%20high-quality%0A3D-aware%20talking%20head.%20Using%20prior%20knowledge%20of%20generative%20models%20combined%20with%0ANeRF%2C%20our%20method%20can%20craft%20a%203D-consistent%20facial%20feature%20space%20corresponding%0Ato%20a%20single%20image.%20Our%20spatial%20synchronization%20method%20employs%20audio-correlated%0Avertex%20dynamics%20of%20a%20parametric%20face%20model%20to%20transform%20static%20image%20features%0Ainto%20dynamic%20visuals%20through%20ray%20deformation%2C%20ensuring%20realistic%203D%20facial%0Amotion.%20Moreover%2C%20we%20introduce%20LipaintNet%20that%20can%20replenish%20the%20lacking%0Ainformation%20in%20the%20inner-mouth%20area%2C%20which%20can%20not%20be%20obtained%20from%20a%20given%0Asingle%20image.%20The%20network%20is%20trained%20in%20a%20self-supervised%20manner%20by%20utilizing%0Athe%20generative%20capabilities%20without%20additional%20data.%20The%20comprehensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20in%20generating%0Aaudio-driven%20talking%20heads%20from%20a%20single%20image%20with%20enhanced%203D%20consistency%0Acompared%20to%20previous%20approaches.%20In%20addition%2C%20we%20introduce%20a%20quantitative%20way%0Aof%20measuring%20the%20robustness%20of%20a%20model%20against%20pose%20changes%20for%20the%20first%20time%2C%0Awhich%20has%20been%20possible%20only%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05749v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRFFaceSpeech%253A%2520One-shot%2520Audio-driven%25203D%2520Talking%2520Head%2520Synthesis%2520via%250A%2520%2520Generative%2520Prior%26entry.906535625%3DGihoon%2520Kim%2520and%2520Kwanggyoon%2520Seo%2520and%2520Sihun%2520Cha%2520and%2520Junyong%2520Noh%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520is%2520advancing%2520from%25202D%2520to%25203D%2520content.%250ANotably%252C%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520is%2520in%2520the%2520spotlight%2520as%2520a%2520means%2520to%250Asynthesize%2520high-quality%25203D%2520talking%2520head%2520outputs.%2520Unfortunately%252C%2520this%2520NeRF-based%250Aapproach%2520typically%2520requires%2520a%2520large%2520number%2520of%2520paired%2520audio-visual%2520data%2520for%2520each%250Aidentity%252C%2520thereby%2520limiting%2520the%2520scalability%2520of%2520the%2520method.%2520Although%2520there%2520have%250Abeen%2520attempts%2520to%2520generate%2520audio-driven%25203D%2520talking%2520head%2520animations%2520with%2520a%2520single%250Aimage%252C%2520the%2520results%2520are%2520often%2520unsatisfactory%2520due%2520to%2520insufficient%2520information%2520on%250Aobscured%2520regions%2520in%2520the%2520image.%2520In%2520this%2520paper%252C%2520we%2520mainly%2520focus%2520on%2520addressing%2520the%250Aoverlooked%2520aspect%2520of%25203D%2520consistency%2520in%2520the%2520one-shot%252C%2520audio-driven%2520domain%252C%2520where%250Afacial%2520animations%2520are%2520synthesized%2520primarily%2520in%2520front-facing%2520perspectives.%2520We%250Apropose%2520a%2520novel%2520method%252C%2520NeRFFaceSpeech%252C%2520which%2520enables%2520to%2520produce%2520high-quality%250A3D-aware%2520talking%2520head.%2520Using%2520prior%2520knowledge%2520of%2520generative%2520models%2520combined%2520with%250ANeRF%252C%2520our%2520method%2520can%2520craft%2520a%25203D-consistent%2520facial%2520feature%2520space%2520corresponding%250Ato%2520a%2520single%2520image.%2520Our%2520spatial%2520synchronization%2520method%2520employs%2520audio-correlated%250Avertex%2520dynamics%2520of%2520a%2520parametric%2520face%2520model%2520to%2520transform%2520static%2520image%2520features%250Ainto%2520dynamic%2520visuals%2520through%2520ray%2520deformation%252C%2520ensuring%2520realistic%25203D%2520facial%250Amotion.%2520Moreover%252C%2520we%2520introduce%2520LipaintNet%2520that%2520can%2520replenish%2520the%2520lacking%250Ainformation%2520in%2520the%2520inner-mouth%2520area%252C%2520which%2520can%2520not%2520be%2520obtained%2520from%2520a%2520given%250Asingle%2520image.%2520The%2520network%2520is%2520trained%2520in%2520a%2520self-supervised%2520manner%2520by%2520utilizing%250Athe%2520generative%2520capabilities%2520without%2520additional%2520data.%2520The%2520comprehensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520generating%250Aaudio-driven%2520talking%2520heads%2520from%2520a%2520single%2520image%2520with%2520enhanced%25203D%2520consistency%250Acompared%2520to%2520previous%2520approaches.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520quantitative%2520way%250Aof%2520measuring%2520the%2520robustness%2520of%2520a%2520model%2520against%2520pose%2520changes%2520for%2520the%2520first%2520time%252C%250Awhich%2520has%2520been%2520possible%2520only%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05749v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRFFaceSpeech%3A%20One-shot%20Audio-driven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior&entry.906535625=Gihoon%20Kim%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Junyong%20Noh&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20is%20advancing%20from%202D%20to%203D%20content.%0ANotably%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20in%20the%20spotlight%20as%20a%20means%20to%0Asynthesize%20high-quality%203D%20talking%20head%20outputs.%20Unfortunately%2C%20this%20NeRF-based%0Aapproach%20typically%20requires%20a%20large%20number%20of%20paired%20audio-visual%20data%20for%20each%0Aidentity%2C%20thereby%20limiting%20the%20scalability%20of%20the%20method.%20Although%20there%20have%0Abeen%20attempts%20to%20generate%20audio-driven%203D%20talking%20head%20animations%20with%20a%20single%0Aimage%2C%20the%20results%20are%20often%20unsatisfactory%20due%20to%20insufficient%20information%20on%0Aobscured%20regions%20in%20the%20image.%20In%20this%20paper%2C%20we%20mainly%20focus%20on%20addressing%20the%0Aoverlooked%20aspect%20of%203D%20consistency%20in%20the%20one-shot%2C%20audio-driven%20domain%2C%20where%0Afacial%20animations%20are%20synthesized%20primarily%20in%20front-facing%20perspectives.%20We%0Apropose%20a%20novel%20method%2C%20NeRFFaceSpeech%2C%20which%20enables%20to%20produce%20high-quality%0A3D-aware%20talking%20head.%20Using%20prior%20knowledge%20of%20generative%20models%20combined%20with%0ANeRF%2C%20our%20method%20can%20craft%20a%203D-consistent%20facial%20feature%20space%20corresponding%0Ato%20a%20single%20image.%20Our%20spatial%20synchronization%20method%20employs%20audio-correlated%0Avertex%20dynamics%20of%20a%20parametric%20face%20model%20to%20transform%20static%20image%20features%0Ainto%20dynamic%20visuals%20through%20ray%20deformation%2C%20ensuring%20realistic%203D%20facial%0Amotion.%20Moreover%2C%20we%20introduce%20LipaintNet%20that%20can%20replenish%20the%20lacking%0Ainformation%20in%20the%20inner-mouth%20area%2C%20which%20can%20not%20be%20obtained%20from%20a%20given%0Asingle%20image.%20The%20network%20is%20trained%20in%20a%20self-supervised%20manner%20by%20utilizing%0Athe%20generative%20capabilities%20without%20additional%20data.%20The%20comprehensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20in%20generating%0Aaudio-driven%20talking%20heads%20from%20a%20single%20image%20with%20enhanced%203D%20consistency%0Acompared%20to%20previous%20approaches.%20In%20addition%2C%20we%20introduce%20a%20quantitative%20way%0Aof%20measuring%20the%20robustness%20of%20a%20model%20against%20pose%20changes%20for%20the%20first%20time%2C%0Awhich%20has%20been%20possible%20only%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05749v2&entry.124074799=Read"},
{"title": "Free-Moving Object Reconstruction and Pose Estimation with Virtual\n  Camera", "author": "Haixin Shi and Yinlin Hu and Daniel Koguciuk and Juan-Ting Lin and Mathieu Salzmann and David Ferstl", "abstract": "  We propose an approach for reconstructing free-moving object from a monocular\nRGB video. Most existing methods either assume scene prior, hand pose prior,\nobject category pose prior, or rely on local optimization with multiple\nsequence segments. We propose a method that allows free interaction with the\nobject in front of a moving camera without relying on any prior, and optimizes\nthe sequence globally without any segments. We progressively optimize the\nobject shape and pose simultaneously based on an implicit neural\nrepresentation. A key aspect of our method is a virtual camera system that\nreduces the search space of the optimization significantly. We evaluate our\nmethod on the standard HO3D dataset and a collection of egocentric RGB\nsequences captured with a head-mounted device. We demonstrate that our approach\noutperforms most methods significantly, and is on par with recent techniques\nthat assume prior information.\n", "link": "http://arxiv.org/abs/2405.05858v2", "date": "2024-05-10", "relevancy": 2.9428, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6033}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera&body=Title%3A%20Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera%0AAuthor%3A%20Haixin%20Shi%20and%20Yinlin%20Hu%20and%20Daniel%20Koguciuk%20and%20Juan-Ting%20Lin%20and%20Mathieu%20Salzmann%20and%20David%20Ferstl%0AAbstract%3A%20%20%20We%20propose%20an%20approach%20for%20reconstructing%20free-moving%20object%20from%20a%20monocular%0ARGB%20video.%20Most%20existing%20methods%20either%20assume%20scene%20prior%2C%20hand%20pose%20prior%2C%0Aobject%20category%20pose%20prior%2C%20or%20rely%20on%20local%20optimization%20with%20multiple%0Asequence%20segments.%20We%20propose%20a%20method%20that%20allows%20free%20interaction%20with%20the%0Aobject%20in%20front%20of%20a%20moving%20camera%20without%20relying%20on%20any%20prior%2C%20and%20optimizes%0Athe%20sequence%20globally%20without%20any%20segments.%20We%20progressively%20optimize%20the%0Aobject%20shape%20and%20pose%20simultaneously%20based%20on%20an%20implicit%20neural%0Arepresentation.%20A%20key%20aspect%20of%20our%20method%20is%20a%20virtual%20camera%20system%20that%0Areduces%20the%20search%20space%20of%20the%20optimization%20significantly.%20We%20evaluate%20our%0Amethod%20on%20the%20standard%20HO3D%20dataset%20and%20a%20collection%20of%20egocentric%20RGB%0Asequences%20captured%20with%20a%20head-mounted%20device.%20We%20demonstrate%20that%20our%20approach%0Aoutperforms%20most%20methods%20significantly%2C%20and%20is%20on%20par%20with%20recent%20techniques%0Athat%20assume%20prior%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Moving%2520Object%2520Reconstruction%2520and%2520Pose%2520Estimation%2520with%2520Virtual%250A%2520%2520Camera%26entry.906535625%3DHaixin%2520Shi%2520and%2520Yinlin%2520Hu%2520and%2520Daniel%2520Koguciuk%2520and%2520Juan-Ting%2520Lin%2520and%2520Mathieu%2520Salzmann%2520and%2520David%2520Ferstl%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520approach%2520for%2520reconstructing%2520free-moving%2520object%2520from%2520a%2520monocular%250ARGB%2520video.%2520Most%2520existing%2520methods%2520either%2520assume%2520scene%2520prior%252C%2520hand%2520pose%2520prior%252C%250Aobject%2520category%2520pose%2520prior%252C%2520or%2520rely%2520on%2520local%2520optimization%2520with%2520multiple%250Asequence%2520segments.%2520We%2520propose%2520a%2520method%2520that%2520allows%2520free%2520interaction%2520with%2520the%250Aobject%2520in%2520front%2520of%2520a%2520moving%2520camera%2520without%2520relying%2520on%2520any%2520prior%252C%2520and%2520optimizes%250Athe%2520sequence%2520globally%2520without%2520any%2520segments.%2520We%2520progressively%2520optimize%2520the%250Aobject%2520shape%2520and%2520pose%2520simultaneously%2520based%2520on%2520an%2520implicit%2520neural%250Arepresentation.%2520A%2520key%2520aspect%2520of%2520our%2520method%2520is%2520a%2520virtual%2520camera%2520system%2520that%250Areduces%2520the%2520search%2520space%2520of%2520the%2520optimization%2520significantly.%2520We%2520evaluate%2520our%250Amethod%2520on%2520the%2520standard%2520HO3D%2520dataset%2520and%2520a%2520collection%2520of%2520egocentric%2520RGB%250Asequences%2520captured%2520with%2520a%2520head-mounted%2520device.%2520We%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520most%2520methods%2520significantly%252C%2520and%2520is%2520on%2520par%2520with%2520recent%2520techniques%250Athat%2520assume%2520prior%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera&entry.906535625=Haixin%20Shi%20and%20Yinlin%20Hu%20and%20Daniel%20Koguciuk%20and%20Juan-Ting%20Lin%20and%20Mathieu%20Salzmann%20and%20David%20Ferstl&entry.1292438233=%20%20We%20propose%20an%20approach%20for%20reconstructing%20free-moving%20object%20from%20a%20monocular%0ARGB%20video.%20Most%20existing%20methods%20either%20assume%20scene%20prior%2C%20hand%20pose%20prior%2C%0Aobject%20category%20pose%20prior%2C%20or%20rely%20on%20local%20optimization%20with%20multiple%0Asequence%20segments.%20We%20propose%20a%20method%20that%20allows%20free%20interaction%20with%20the%0Aobject%20in%20front%20of%20a%20moving%20camera%20without%20relying%20on%20any%20prior%2C%20and%20optimizes%0Athe%20sequence%20globally%20without%20any%20segments.%20We%20progressively%20optimize%20the%0Aobject%20shape%20and%20pose%20simultaneously%20based%20on%20an%20implicit%20neural%0Arepresentation.%20A%20key%20aspect%20of%20our%20method%20is%20a%20virtual%20camera%20system%20that%0Areduces%20the%20search%20space%20of%20the%20optimization%20significantly.%20We%20evaluate%20our%0Amethod%20on%20the%20standard%20HO3D%20dataset%20and%20a%20collection%20of%20egocentric%20RGB%0Asequences%20captured%20with%20a%20head-mounted%20device.%20We%20demonstrate%20that%20our%20approach%0Aoutperforms%20most%20methods%20significantly%2C%20and%20is%20on%20par%20with%20recent%20techniques%0Athat%20assume%20prior%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05858v2&entry.124074799=Read"},
{"title": "Multi-Object Tracking in the Dark", "author": "Xinzhe Wang and Kang Ma and Qiankun Liu and Yunhao Zou and Ying Fu", "abstract": "  Low-light scenes are prevalent in real-world applications (e.g. autonomous\ndriving and surveillance at night). Recently, multi-object tracking in various\npractical use cases have received much attention, but multi-object tracking in\ndark scenes is rarely considered. In this paper, we focus on multi-object\ntracking in dark scenes. To address the lack of datasets, we first build a\nLow-light Multi-Object Tracking (LMOT) dataset. LMOT provides well-aligned\nlow-light video pairs captured by our dual-camera system, and high-quality\nmulti-object tracking annotations for all videos. Then, we propose a low-light\nmulti-object tracking method, termed as LTrack. We introduce the adaptive\nlow-pass downsample module to enhance low-frequency components of images\noutside the sensor noises. The degradation suppression learning strategy\nenables the model to learn invariant information under noise disturbance and\nimage quality degradation. These components improve the robustness of\nmulti-object tracking in dark scenes. We conducted a comprehensive analysis of\nour LMOT dataset and proposed LTrack. Experimental results demonstrate the\nsuperiority of the proposed method and its competitiveness in real night\nlow-light scenes. Dataset and Code: https: //github.com/ying-fu/LMOT\n", "link": "http://arxiv.org/abs/2405.06600v1", "date": "2024-05-10", "relevancy": 2.6572, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Object%20Tracking%20in%20the%20Dark&body=Title%3A%20Multi-Object%20Tracking%20in%20the%20Dark%0AAuthor%3A%20Xinzhe%20Wang%20and%20Kang%20Ma%20and%20Qiankun%20Liu%20and%20Yunhao%20Zou%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Low-light%20scenes%20are%20prevalent%20in%20real-world%20applications%20%28e.g.%20autonomous%0Adriving%20and%20surveillance%20at%20night%29.%20Recently%2C%20multi-object%20tracking%20in%20various%0Apractical%20use%20cases%20have%20received%20much%20attention%2C%20but%20multi-object%20tracking%20in%0Adark%20scenes%20is%20rarely%20considered.%20In%20this%20paper%2C%20we%20focus%20on%20multi-object%0Atracking%20in%20dark%20scenes.%20To%20address%20the%20lack%20of%20datasets%2C%20we%20first%20build%20a%0ALow-light%20Multi-Object%20Tracking%20%28LMOT%29%20dataset.%20LMOT%20provides%20well-aligned%0Alow-light%20video%20pairs%20captured%20by%20our%20dual-camera%20system%2C%20and%20high-quality%0Amulti-object%20tracking%20annotations%20for%20all%20videos.%20Then%2C%20we%20propose%20a%20low-light%0Amulti-object%20tracking%20method%2C%20termed%20as%20LTrack.%20We%20introduce%20the%20adaptive%0Alow-pass%20downsample%20module%20to%20enhance%20low-frequency%20components%20of%20images%0Aoutside%20the%20sensor%20noises.%20The%20degradation%20suppression%20learning%20strategy%0Aenables%20the%20model%20to%20learn%20invariant%20information%20under%20noise%20disturbance%20and%0Aimage%20quality%20degradation.%20These%20components%20improve%20the%20robustness%20of%0Amulti-object%20tracking%20in%20dark%20scenes.%20We%20conducted%20a%20comprehensive%20analysis%20of%0Aour%20LMOT%20dataset%20and%20proposed%20LTrack.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20and%20its%20competitiveness%20in%20real%20night%0Alow-light%20scenes.%20Dataset%20and%20Code%3A%20https%3A%20//github.com/ying-fu/LMOT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Object%2520Tracking%2520in%2520the%2520Dark%26entry.906535625%3DXinzhe%2520Wang%2520and%2520Kang%2520Ma%2520and%2520Qiankun%2520Liu%2520and%2520Yunhao%2520Zou%2520and%2520Ying%2520Fu%26entry.1292438233%3D%2520%2520Low-light%2520scenes%2520are%2520prevalent%2520in%2520real-world%2520applications%2520%2528e.g.%2520autonomous%250Adriving%2520and%2520surveillance%2520at%2520night%2529.%2520Recently%252C%2520multi-object%2520tracking%2520in%2520various%250Apractical%2520use%2520cases%2520have%2520received%2520much%2520attention%252C%2520but%2520multi-object%2520tracking%2520in%250Adark%2520scenes%2520is%2520rarely%2520considered.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520multi-object%250Atracking%2520in%2520dark%2520scenes.%2520To%2520address%2520the%2520lack%2520of%2520datasets%252C%2520we%2520first%2520build%2520a%250ALow-light%2520Multi-Object%2520Tracking%2520%2528LMOT%2529%2520dataset.%2520LMOT%2520provides%2520well-aligned%250Alow-light%2520video%2520pairs%2520captured%2520by%2520our%2520dual-camera%2520system%252C%2520and%2520high-quality%250Amulti-object%2520tracking%2520annotations%2520for%2520all%2520videos.%2520Then%252C%2520we%2520propose%2520a%2520low-light%250Amulti-object%2520tracking%2520method%252C%2520termed%2520as%2520LTrack.%2520We%2520introduce%2520the%2520adaptive%250Alow-pass%2520downsample%2520module%2520to%2520enhance%2520low-frequency%2520components%2520of%2520images%250Aoutside%2520the%2520sensor%2520noises.%2520The%2520degradation%2520suppression%2520learning%2520strategy%250Aenables%2520the%2520model%2520to%2520learn%2520invariant%2520information%2520under%2520noise%2520disturbance%2520and%250Aimage%2520quality%2520degradation.%2520These%2520components%2520improve%2520the%2520robustness%2520of%250Amulti-object%2520tracking%2520in%2520dark%2520scenes.%2520We%2520conducted%2520a%2520comprehensive%2520analysis%2520of%250Aour%2520LMOT%2520dataset%2520and%2520proposed%2520LTrack.%2520Experimental%2520results%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method%2520and%2520its%2520competitiveness%2520in%2520real%2520night%250Alow-light%2520scenes.%2520Dataset%2520and%2520Code%253A%2520https%253A%2520//github.com/ying-fu/LMOT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Object%20Tracking%20in%20the%20Dark&entry.906535625=Xinzhe%20Wang%20and%20Kang%20Ma%20and%20Qiankun%20Liu%20and%20Yunhao%20Zou%20and%20Ying%20Fu&entry.1292438233=%20%20Low-light%20scenes%20are%20prevalent%20in%20real-world%20applications%20%28e.g.%20autonomous%0Adriving%20and%20surveillance%20at%20night%29.%20Recently%2C%20multi-object%20tracking%20in%20various%0Apractical%20use%20cases%20have%20received%20much%20attention%2C%20but%20multi-object%20tracking%20in%0Adark%20scenes%20is%20rarely%20considered.%20In%20this%20paper%2C%20we%20focus%20on%20multi-object%0Atracking%20in%20dark%20scenes.%20To%20address%20the%20lack%20of%20datasets%2C%20we%20first%20build%20a%0ALow-light%20Multi-Object%20Tracking%20%28LMOT%29%20dataset.%20LMOT%20provides%20well-aligned%0Alow-light%20video%20pairs%20captured%20by%20our%20dual-camera%20system%2C%20and%20high-quality%0Amulti-object%20tracking%20annotations%20for%20all%20videos.%20Then%2C%20we%20propose%20a%20low-light%0Amulti-object%20tracking%20method%2C%20termed%20as%20LTrack.%20We%20introduce%20the%20adaptive%0Alow-pass%20downsample%20module%20to%20enhance%20low-frequency%20components%20of%20images%0Aoutside%20the%20sensor%20noises.%20The%20degradation%20suppression%20learning%20strategy%0Aenables%20the%20model%20to%20learn%20invariant%20information%20under%20noise%20disturbance%20and%0Aimage%20quality%20degradation.%20These%20components%20improve%20the%20robustness%20of%0Amulti-object%20tracking%20in%20dark%20scenes.%20We%20conducted%20a%20comprehensive%20analysis%20of%0Aour%20LMOT%20dataset%20and%20proposed%20LTrack.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method%20and%20its%20competitiveness%20in%20real%20night%0Alow-light%20scenes.%20Dataset%20and%20Code%3A%20https%3A%20//github.com/ying-fu/LMOT%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06600v1&entry.124074799=Read"},
{"title": "Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum\n  Learning", "author": "Yili Wang", "abstract": "  In recent years, heterogeneous graph neural networks (HGNNs) have achieved\nexcellent performance in handling heterogeneous information networks (HINs).\nCurriculum learning is a machine learning strategy where training examples are\npresented to a model in a structured order, starting with easy examples and\ngradually increasing difficulty, aiming to improve learning efficiency and\ngeneralization. To better exploit the rich information in HINs, previous\nmethods have started to explore the use of curriculum learning strategy to\ntrain HGNNs. Specifically, these works utilize the absolute value of the loss\nat each training epoch to evaluate the learning difficulty of each training\nsample. However, the relative loss, rather than the absolute value of loss,\nreveals the learning difficulty. Therefore, we propose a novel\nloss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss\ndecrease between each training epoch to better evaluating the difficulty of\ntraining samples, thereby enhancing the curriculum learning of HGNNs for\ndownstream tasks. Additionally, we propose a sampling strategy to alleviate\ntraining imbalance issues. Our method further demonstrate the efficacy of\ncurriculum learning in enhancing HGNNs capabilities. We call our method\nLoss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is\npublic at https://github.com/wangyili00/LDHGNN.\n", "link": "http://arxiv.org/abs/2405.06522v1", "date": "2024-05-10", "relevancy": 2.6185, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5305}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Graph%20Neural%20Networks%20with%20Loss-decrease-aware%20Curriculum%0A%20%20Learning&body=Title%3A%20Heterogeneous%20Graph%20Neural%20Networks%20with%20Loss-decrease-aware%20Curriculum%0A%20%20Learning%0AAuthor%3A%20Yili%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20achieved%0Aexcellent%20performance%20in%20handling%20heterogeneous%20information%20networks%20%28HINs%29.%0ACurriculum%20learning%20is%20a%20machine%20learning%20strategy%20where%20training%20examples%20are%0Apresented%20to%20a%20model%20in%20a%20structured%20order%2C%20starting%20with%20easy%20examples%20and%0Agradually%20increasing%20difficulty%2C%20aiming%20to%20improve%20learning%20efficiency%20and%0Ageneralization.%20To%20better%20exploit%20the%20rich%20information%20in%20HINs%2C%20previous%0Amethods%20have%20started%20to%20explore%20the%20use%20of%20curriculum%20learning%20strategy%20to%0Atrain%20HGNNs.%20Specifically%2C%20these%20works%20utilize%20the%20absolute%20value%20of%20the%20loss%0Aat%20each%20training%20epoch%20to%20evaluate%20the%20learning%20difficulty%20of%20each%20training%0Asample.%20However%2C%20the%20relative%20loss%2C%20rather%20than%20the%20absolute%20value%20of%20loss%2C%0Areveals%20the%20learning%20difficulty.%20Therefore%2C%20we%20propose%20a%20novel%0Aloss-decrease-aware%20training%20schedule%20%28LDTS%29.%20LDTS%20uses%20the%20trend%20of%20loss%0Adecrease%20between%20each%20training%20epoch%20to%20better%20evaluating%20the%20difficulty%20of%0Atraining%20samples%2C%20thereby%20enhancing%20the%20curriculum%20learning%20of%20HGNNs%20for%0Adownstream%20tasks.%20Additionally%2C%20we%20propose%20a%20sampling%20strategy%20to%20alleviate%0Atraining%20imbalance%20issues.%20Our%20method%20further%20demonstrate%20the%20efficacy%20of%0Acurriculum%20learning%20in%20enhancing%20HGNNs%20capabilities.%20We%20call%20our%20method%0ALoss-decrease-aware%20Heterogeneous%20Graph%20Neural%20Networks%20%28LDHGNN%29.%20The%20code%20is%0Apublic%20at%20https%3A//github.com/wangyili00/LDHGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Graph%2520Neural%2520Networks%2520with%2520Loss-decrease-aware%2520Curriculum%250A%2520%2520Learning%26entry.906535625%3DYili%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520heterogeneous%2520graph%2520neural%2520networks%2520%2528HGNNs%2529%2520have%2520achieved%250Aexcellent%2520performance%2520in%2520handling%2520heterogeneous%2520information%2520networks%2520%2528HINs%2529.%250ACurriculum%2520learning%2520is%2520a%2520machine%2520learning%2520strategy%2520where%2520training%2520examples%2520are%250Apresented%2520to%2520a%2520model%2520in%2520a%2520structured%2520order%252C%2520starting%2520with%2520easy%2520examples%2520and%250Agradually%2520increasing%2520difficulty%252C%2520aiming%2520to%2520improve%2520learning%2520efficiency%2520and%250Ageneralization.%2520To%2520better%2520exploit%2520the%2520rich%2520information%2520in%2520HINs%252C%2520previous%250Amethods%2520have%2520started%2520to%2520explore%2520the%2520use%2520of%2520curriculum%2520learning%2520strategy%2520to%250Atrain%2520HGNNs.%2520Specifically%252C%2520these%2520works%2520utilize%2520the%2520absolute%2520value%2520of%2520the%2520loss%250Aat%2520each%2520training%2520epoch%2520to%2520evaluate%2520the%2520learning%2520difficulty%2520of%2520each%2520training%250Asample.%2520However%252C%2520the%2520relative%2520loss%252C%2520rather%2520than%2520the%2520absolute%2520value%2520of%2520loss%252C%250Areveals%2520the%2520learning%2520difficulty.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%250Aloss-decrease-aware%2520training%2520schedule%2520%2528LDTS%2529.%2520LDTS%2520uses%2520the%2520trend%2520of%2520loss%250Adecrease%2520between%2520each%2520training%2520epoch%2520to%2520better%2520evaluating%2520the%2520difficulty%2520of%250Atraining%2520samples%252C%2520thereby%2520enhancing%2520the%2520curriculum%2520learning%2520of%2520HGNNs%2520for%250Adownstream%2520tasks.%2520Additionally%252C%2520we%2520propose%2520a%2520sampling%2520strategy%2520to%2520alleviate%250Atraining%2520imbalance%2520issues.%2520Our%2520method%2520further%2520demonstrate%2520the%2520efficacy%2520of%250Acurriculum%2520learning%2520in%2520enhancing%2520HGNNs%2520capabilities.%2520We%2520call%2520our%2520method%250ALoss-decrease-aware%2520Heterogeneous%2520Graph%2520Neural%2520Networks%2520%2528LDHGNN%2529.%2520The%2520code%2520is%250Apublic%2520at%2520https%253A//github.com/wangyili00/LDHGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Graph%20Neural%20Networks%20with%20Loss-decrease-aware%20Curriculum%0A%20%20Learning&entry.906535625=Yili%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%20have%20achieved%0Aexcellent%20performance%20in%20handling%20heterogeneous%20information%20networks%20%28HINs%29.%0ACurriculum%20learning%20is%20a%20machine%20learning%20strategy%20where%20training%20examples%20are%0Apresented%20to%20a%20model%20in%20a%20structured%20order%2C%20starting%20with%20easy%20examples%20and%0Agradually%20increasing%20difficulty%2C%20aiming%20to%20improve%20learning%20efficiency%20and%0Ageneralization.%20To%20better%20exploit%20the%20rich%20information%20in%20HINs%2C%20previous%0Amethods%20have%20started%20to%20explore%20the%20use%20of%20curriculum%20learning%20strategy%20to%0Atrain%20HGNNs.%20Specifically%2C%20these%20works%20utilize%20the%20absolute%20value%20of%20the%20loss%0Aat%20each%20training%20epoch%20to%20evaluate%20the%20learning%20difficulty%20of%20each%20training%0Asample.%20However%2C%20the%20relative%20loss%2C%20rather%20than%20the%20absolute%20value%20of%20loss%2C%0Areveals%20the%20learning%20difficulty.%20Therefore%2C%20we%20propose%20a%20novel%0Aloss-decrease-aware%20training%20schedule%20%28LDTS%29.%20LDTS%20uses%20the%20trend%20of%20loss%0Adecrease%20between%20each%20training%20epoch%20to%20better%20evaluating%20the%20difficulty%20of%0Atraining%20samples%2C%20thereby%20enhancing%20the%20curriculum%20learning%20of%20HGNNs%20for%0Adownstream%20tasks.%20Additionally%2C%20we%20propose%20a%20sampling%20strategy%20to%20alleviate%0Atraining%20imbalance%20issues.%20Our%20method%20further%20demonstrate%20the%20efficacy%20of%0Acurriculum%20learning%20in%20enhancing%20HGNNs%20capabilities.%20We%20call%20our%20method%0ALoss-decrease-aware%20Heterogeneous%20Graph%20Neural%20Networks%20%28LDHGNN%29.%20The%20code%20is%0Apublic%20at%20https%3A//github.com/wangyili00/LDHGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06522v1&entry.124074799=Read"},
{"title": "Continual Novel Class Discovery via Feature Enhancement and Adaptation", "author": "Yifan Yu and Shaokun Wang and Yuhang He and Junzhe Chen and Yihong Gong", "abstract": "  Continual Novel Class Discovery (CNCD) aims to continually discover novel\nclasses without labels while maintaining the recognition capability for\npreviously learned classes. The main challenges faced by CNCD include the\nfeature-discrepancy problem, the inter-session confusion problem, etc. In this\npaper, we propose a novel Feature Enhancement and Adaptation method for the\nCNCD to tackle the above challenges, which consists of a guide-to-novel\nframework, a centroid-to-samples similarity constraint (CSS), and a\nboundary-aware prototype constraint (BAP). More specifically, the\nguide-to-novel framework is established to continually discover novel classes\nunder the guidance of prior distribution. Afterward, the CSS is designed to\nconstrain the relationship between centroid-to-samples similarities of\ndifferent classes, thereby enhancing the distinctiveness of features among\nnovel classes. Finally, the BAP is proposed to keep novel class features aware\nof the positions of other class prototypes during incremental sessions, and\nbetter adapt novel class features to the shared feature space. Experimental\nresults on three benchmark datasets demonstrate the superiority of our method,\nespecially in more challenging protocols with more incremental sessions.\n", "link": "http://arxiv.org/abs/2405.06389v1", "date": "2024-05-10", "relevancy": 2.5804, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5569}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Novel%20Class%20Discovery%20via%20Feature%20Enhancement%20and%20Adaptation&body=Title%3A%20Continual%20Novel%20Class%20Discovery%20via%20Feature%20Enhancement%20and%20Adaptation%0AAuthor%3A%20Yifan%20Yu%20and%20Shaokun%20Wang%20and%20Yuhang%20He%20and%20Junzhe%20Chen%20and%20Yihong%20Gong%0AAbstract%3A%20%20%20Continual%20Novel%20Class%20Discovery%20%28CNCD%29%20aims%20to%20continually%20discover%20novel%0Aclasses%20without%20labels%20while%20maintaining%20the%20recognition%20capability%20for%0Apreviously%20learned%20classes.%20The%20main%20challenges%20faced%20by%20CNCD%20include%20the%0Afeature-discrepancy%20problem%2C%20the%20inter-session%20confusion%20problem%2C%20etc.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Feature%20Enhancement%20and%20Adaptation%20method%20for%20the%0ACNCD%20to%20tackle%20the%20above%20challenges%2C%20which%20consists%20of%20a%20guide-to-novel%0Aframework%2C%20a%20centroid-to-samples%20similarity%20constraint%20%28CSS%29%2C%20and%20a%0Aboundary-aware%20prototype%20constraint%20%28BAP%29.%20More%20specifically%2C%20the%0Aguide-to-novel%20framework%20is%20established%20to%20continually%20discover%20novel%20classes%0Aunder%20the%20guidance%20of%20prior%20distribution.%20Afterward%2C%20the%20CSS%20is%20designed%20to%0Aconstrain%20the%20relationship%20between%20centroid-to-samples%20similarities%20of%0Adifferent%20classes%2C%20thereby%20enhancing%20the%20distinctiveness%20of%20features%20among%0Anovel%20classes.%20Finally%2C%20the%20BAP%20is%20proposed%20to%20keep%20novel%20class%20features%20aware%0Aof%20the%20positions%20of%20other%20class%20prototypes%20during%20incremental%20sessions%2C%20and%0Abetter%20adapt%20novel%20class%20features%20to%20the%20shared%20feature%20space.%20Experimental%0Aresults%20on%20three%20benchmark%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%2C%0Aespecially%20in%20more%20challenging%20protocols%20with%20more%20incremental%20sessions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Novel%2520Class%2520Discovery%2520via%2520Feature%2520Enhancement%2520and%2520Adaptation%26entry.906535625%3DYifan%2520Yu%2520and%2520Shaokun%2520Wang%2520and%2520Yuhang%2520He%2520and%2520Junzhe%2520Chen%2520and%2520Yihong%2520Gong%26entry.1292438233%3D%2520%2520Continual%2520Novel%2520Class%2520Discovery%2520%2528CNCD%2529%2520aims%2520to%2520continually%2520discover%2520novel%250Aclasses%2520without%2520labels%2520while%2520maintaining%2520the%2520recognition%2520capability%2520for%250Apreviously%2520learned%2520classes.%2520The%2520main%2520challenges%2520faced%2520by%2520CNCD%2520include%2520the%250Afeature-discrepancy%2520problem%252C%2520the%2520inter-session%2520confusion%2520problem%252C%2520etc.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520Feature%2520Enhancement%2520and%2520Adaptation%2520method%2520for%2520the%250ACNCD%2520to%2520tackle%2520the%2520above%2520challenges%252C%2520which%2520consists%2520of%2520a%2520guide-to-novel%250Aframework%252C%2520a%2520centroid-to-samples%2520similarity%2520constraint%2520%2528CSS%2529%252C%2520and%2520a%250Aboundary-aware%2520prototype%2520constraint%2520%2528BAP%2529.%2520More%2520specifically%252C%2520the%250Aguide-to-novel%2520framework%2520is%2520established%2520to%2520continually%2520discover%2520novel%2520classes%250Aunder%2520the%2520guidance%2520of%2520prior%2520distribution.%2520Afterward%252C%2520the%2520CSS%2520is%2520designed%2520to%250Aconstrain%2520the%2520relationship%2520between%2520centroid-to-samples%2520similarities%2520of%250Adifferent%2520classes%252C%2520thereby%2520enhancing%2520the%2520distinctiveness%2520of%2520features%2520among%250Anovel%2520classes.%2520Finally%252C%2520the%2520BAP%2520is%2520proposed%2520to%2520keep%2520novel%2520class%2520features%2520aware%250Aof%2520the%2520positions%2520of%2520other%2520class%2520prototypes%2520during%2520incremental%2520sessions%252C%2520and%250Abetter%2520adapt%2520novel%2520class%2520features%2520to%2520the%2520shared%2520feature%2520space.%2520Experimental%250Aresults%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%252C%250Aespecially%2520in%2520more%2520challenging%2520protocols%2520with%2520more%2520incremental%2520sessions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Novel%20Class%20Discovery%20via%20Feature%20Enhancement%20and%20Adaptation&entry.906535625=Yifan%20Yu%20and%20Shaokun%20Wang%20and%20Yuhang%20He%20and%20Junzhe%20Chen%20and%20Yihong%20Gong&entry.1292438233=%20%20Continual%20Novel%20Class%20Discovery%20%28CNCD%29%20aims%20to%20continually%20discover%20novel%0Aclasses%20without%20labels%20while%20maintaining%20the%20recognition%20capability%20for%0Apreviously%20learned%20classes.%20The%20main%20challenges%20faced%20by%20CNCD%20include%20the%0Afeature-discrepancy%20problem%2C%20the%20inter-session%20confusion%20problem%2C%20etc.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20Feature%20Enhancement%20and%20Adaptation%20method%20for%20the%0ACNCD%20to%20tackle%20the%20above%20challenges%2C%20which%20consists%20of%20a%20guide-to-novel%0Aframework%2C%20a%20centroid-to-samples%20similarity%20constraint%20%28CSS%29%2C%20and%20a%0Aboundary-aware%20prototype%20constraint%20%28BAP%29.%20More%20specifically%2C%20the%0Aguide-to-novel%20framework%20is%20established%20to%20continually%20discover%20novel%20classes%0Aunder%20the%20guidance%20of%20prior%20distribution.%20Afterward%2C%20the%20CSS%20is%20designed%20to%0Aconstrain%20the%20relationship%20between%20centroid-to-samples%20similarities%20of%0Adifferent%20classes%2C%20thereby%20enhancing%20the%20distinctiveness%20of%20features%20among%0Anovel%20classes.%20Finally%2C%20the%20BAP%20is%20proposed%20to%20keep%20novel%20class%20features%20aware%0Aof%20the%20positions%20of%20other%20class%20prototypes%20during%20incremental%20sessions%2C%20and%0Abetter%20adapt%20novel%20class%20features%20to%20the%20shared%20feature%20space.%20Experimental%0Aresults%20on%20three%20benchmark%20datasets%20demonstrate%20the%20superiority%20of%20our%20method%2C%0Aespecially%20in%20more%20challenging%20protocols%20with%20more%20incremental%20sessions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06389v1&entry.124074799=Read"},
{"title": "PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation", "author": "Soubarna Banik and Edvard Avagyan and Sayantan Auddy and Alejandro Mendoza Gracia and Alois Knoll", "abstract": "  Existing skeleton-based 3D human pose estimation methods only predict joint\npositions. Although the yaw and pitch of bone rotations can be derived from\njoint positions, the roll around the bone axis remains unresolved. We present\nPoseGraphNet++ (PGN++), a novel 2D-to-3D lifting Graph Convolution Network that\npredicts the complete human pose in 3D including joint positions and bone\norientations. We employ both node and edge convolutions to utilize the joint\nand bone features. Our model is evaluated on multiple datasets using both\nposition and rotation metrics. PGN++ performs on par with the state-of-the-art\n(SoA) on the Human3.6M benchmark. In generalization experiments, it achieves\nthe best results in position and matches the SoA in orientation, showcasing a\nmore balanced performance than the current SoA. PGN++ exploits the mutual\nrelationship of joints and bones resulting in significantly \\SB{improved}\nposition predictions, as shown by our ablation results.\n", "link": "http://arxiv.org/abs/2308.11440v2", "date": "2024-05-10", "relevancy": 2.5713, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5222}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseGraphNet%2B%2B%3A%20Enriching%203D%20Human%20Pose%20with%20Orientation%20Estimation&body=Title%3A%20PoseGraphNet%2B%2B%3A%20Enriching%203D%20Human%20Pose%20with%20Orientation%20Estimation%0AAuthor%3A%20Soubarna%20Banik%20and%20Edvard%20Avagyan%20and%20Sayantan%20Auddy%20and%20Alejandro%20Mendoza%20Gracia%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Existing%20skeleton-based%203D%20human%20pose%20estimation%20methods%20only%20predict%20joint%0Apositions.%20Although%20the%20yaw%20and%20pitch%20of%20bone%20rotations%20can%20be%20derived%20from%0Ajoint%20positions%2C%20the%20roll%20around%20the%20bone%20axis%20remains%20unresolved.%20We%20present%0APoseGraphNet%2B%2B%20%28PGN%2B%2B%29%2C%20a%20novel%202D-to-3D%20lifting%20Graph%20Convolution%20Network%20that%0Apredicts%20the%20complete%20human%20pose%20in%203D%20including%20joint%20positions%20and%20bone%0Aorientations.%20We%20employ%20both%20node%20and%20edge%20convolutions%20to%20utilize%20the%20joint%0Aand%20bone%20features.%20Our%20model%20is%20evaluated%20on%20multiple%20datasets%20using%20both%0Aposition%20and%20rotation%20metrics.%20PGN%2B%2B%20performs%20on%20par%20with%20the%20state-of-the-art%0A%28SoA%29%20on%20the%20Human3.6M%20benchmark.%20In%20generalization%20experiments%2C%20it%20achieves%0Athe%20best%20results%20in%20position%20and%20matches%20the%20SoA%20in%20orientation%2C%20showcasing%20a%0Amore%20balanced%20performance%20than%20the%20current%20SoA.%20PGN%2B%2B%20exploits%20the%20mutual%0Arelationship%20of%20joints%20and%20bones%20resulting%20in%20significantly%20%5CSB%7Bimproved%7D%0Aposition%20predictions%2C%20as%20shown%20by%20our%20ablation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseGraphNet%252B%252B%253A%2520Enriching%25203D%2520Human%2520Pose%2520with%2520Orientation%2520Estimation%26entry.906535625%3DSoubarna%2520Banik%2520and%2520Edvard%2520Avagyan%2520and%2520Sayantan%2520Auddy%2520and%2520Alejandro%2520Mendoza%2520Gracia%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Existing%2520skeleton-based%25203D%2520human%2520pose%2520estimation%2520methods%2520only%2520predict%2520joint%250Apositions.%2520Although%2520the%2520yaw%2520and%2520pitch%2520of%2520bone%2520rotations%2520can%2520be%2520derived%2520from%250Ajoint%2520positions%252C%2520the%2520roll%2520around%2520the%2520bone%2520axis%2520remains%2520unresolved.%2520We%2520present%250APoseGraphNet%252B%252B%2520%2528PGN%252B%252B%2529%252C%2520a%2520novel%25202D-to-3D%2520lifting%2520Graph%2520Convolution%2520Network%2520that%250Apredicts%2520the%2520complete%2520human%2520pose%2520in%25203D%2520including%2520joint%2520positions%2520and%2520bone%250Aorientations.%2520We%2520employ%2520both%2520node%2520and%2520edge%2520convolutions%2520to%2520utilize%2520the%2520joint%250Aand%2520bone%2520features.%2520Our%2520model%2520is%2520evaluated%2520on%2520multiple%2520datasets%2520using%2520both%250Aposition%2520and%2520rotation%2520metrics.%2520PGN%252B%252B%2520performs%2520on%2520par%2520with%2520the%2520state-of-the-art%250A%2528SoA%2529%2520on%2520the%2520Human3.6M%2520benchmark.%2520In%2520generalization%2520experiments%252C%2520it%2520achieves%250Athe%2520best%2520results%2520in%2520position%2520and%2520matches%2520the%2520SoA%2520in%2520orientation%252C%2520showcasing%2520a%250Amore%2520balanced%2520performance%2520than%2520the%2520current%2520SoA.%2520PGN%252B%252B%2520exploits%2520the%2520mutual%250Arelationship%2520of%2520joints%2520and%2520bones%2520resulting%2520in%2520significantly%2520%255CSB%257Bimproved%257D%250Aposition%2520predictions%252C%2520as%2520shown%2520by%2520our%2520ablation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.11440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseGraphNet%2B%2B%3A%20Enriching%203D%20Human%20Pose%20with%20Orientation%20Estimation&entry.906535625=Soubarna%20Banik%20and%20Edvard%20Avagyan%20and%20Sayantan%20Auddy%20and%20Alejandro%20Mendoza%20Gracia%20and%20Alois%20Knoll&entry.1292438233=%20%20Existing%20skeleton-based%203D%20human%20pose%20estimation%20methods%20only%20predict%20joint%0Apositions.%20Although%20the%20yaw%20and%20pitch%20of%20bone%20rotations%20can%20be%20derived%20from%0Ajoint%20positions%2C%20the%20roll%20around%20the%20bone%20axis%20remains%20unresolved.%20We%20present%0APoseGraphNet%2B%2B%20%28PGN%2B%2B%29%2C%20a%20novel%202D-to-3D%20lifting%20Graph%20Convolution%20Network%20that%0Apredicts%20the%20complete%20human%20pose%20in%203D%20including%20joint%20positions%20and%20bone%0Aorientations.%20We%20employ%20both%20node%20and%20edge%20convolutions%20to%20utilize%20the%20joint%0Aand%20bone%20features.%20Our%20model%20is%20evaluated%20on%20multiple%20datasets%20using%20both%0Aposition%20and%20rotation%20metrics.%20PGN%2B%2B%20performs%20on%20par%20with%20the%20state-of-the-art%0A%28SoA%29%20on%20the%20Human3.6M%20benchmark.%20In%20generalization%20experiments%2C%20it%20achieves%0Athe%20best%20results%20in%20position%20and%20matches%20the%20SoA%20in%20orientation%2C%20showcasing%20a%0Amore%20balanced%20performance%20than%20the%20current%20SoA.%20PGN%2B%2B%20exploits%20the%20mutual%0Arelationship%20of%20joints%20and%20bones%20resulting%20in%20significantly%20%5CSB%7Bimproved%7D%0Aposition%20predictions%2C%20as%20shown%20by%20our%20ablation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11440v2&entry.124074799=Read"},
{"title": "OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation", "author": "Jinwei Lin", "abstract": "  One image to editable dynamic 3D model and video generation is novel\ndirection and change in the research area of single image to 3D representation\nor 3D reconstruction of image. Gaussian Splatting has demonstrated its\nadvantages in implicit 3D reconstruction, compared with the original Neural\nRadiance Fields. As the rapid development of technologies and principles,\npeople tried to used the Stable Diffusion models to generate targeted models\nwith text instructions. However, using the normal implicit machine learning\nmethods is hard to gain the precise motions and actions control, further more,\nit is difficult to generate a long content and semantic continuous 3D video. To\naddress this issue, we propose the OneTo3D, a method and theory to used one\nsingle image to generate the editable 3D model and generate the targeted\nsemantic continuous time-unlimited 3D video. We used a normal basic Gaussian\nSplatting model to generate the 3D model from a single image, which requires\nless volume of video memory and computer calculation ability. Subsequently, we\ndesigned an automatic generation and self-adaptive binding mechanism for the\nobject armature. Combined with the re-editable motions and actions analyzing\nand controlling algorithm we proposed, we can achieve a better performance than\nthe SOTA projects in the area of building the 3D model precise motions and\nactions control, and generating a stable semantic continuous time-unlimited 3D\nvideo with the input text instructions. Here we will analyze the detailed\nimplementation methods and theories analyses. Relative comparisons and\nconclusions will be presented. The project code is open source.\n", "link": "http://arxiv.org/abs/2405.06547v1", "date": "2024-05-10", "relevancy": 2.5597, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6514}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.636}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.63}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneTo3D%3A%20One%20Image%20to%20Re-editable%20Dynamic%203D%20Model%20and%20Video%20Generation&body=Title%3A%20OneTo3D%3A%20One%20Image%20to%20Re-editable%20Dynamic%203D%20Model%20and%20Video%20Generation%0AAuthor%3A%20Jinwei%20Lin%0AAbstract%3A%20%20%20One%20image%20to%20editable%20dynamic%203D%20model%20and%20video%20generation%20is%20novel%0Adirection%20and%20change%20in%20the%20research%20area%20of%20single%20image%20to%203D%20representation%0Aor%203D%20reconstruction%20of%20image.%20Gaussian%20Splatting%20has%20demonstrated%20its%0Aadvantages%20in%20implicit%203D%20reconstruction%2C%20compared%20with%20the%20original%20Neural%0ARadiance%20Fields.%20As%20the%20rapid%20development%20of%20technologies%20and%20principles%2C%0Apeople%20tried%20to%20used%20the%20Stable%20Diffusion%20models%20to%20generate%20targeted%20models%0Awith%20text%20instructions.%20However%2C%20using%20the%20normal%20implicit%20machine%20learning%0Amethods%20is%20hard%20to%20gain%20the%20precise%20motions%20and%20actions%20control%2C%20further%20more%2C%0Ait%20is%20difficult%20to%20generate%20a%20long%20content%20and%20semantic%20continuous%203D%20video.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20the%20OneTo3D%2C%20a%20method%20and%20theory%20to%20used%20one%0Asingle%20image%20to%20generate%20the%20editable%203D%20model%20and%20generate%20the%20targeted%0Asemantic%20continuous%20time-unlimited%203D%20video.%20We%20used%20a%20normal%20basic%20Gaussian%0ASplatting%20model%20to%20generate%20the%203D%20model%20from%20a%20single%20image%2C%20which%20requires%0Aless%20volume%20of%20video%20memory%20and%20computer%20calculation%20ability.%20Subsequently%2C%20we%0Adesigned%20an%20automatic%20generation%20and%20self-adaptive%20binding%20mechanism%20for%20the%0Aobject%20armature.%20Combined%20with%20the%20re-editable%20motions%20and%20actions%20analyzing%0Aand%20controlling%20algorithm%20we%20proposed%2C%20we%20can%20achieve%20a%20better%20performance%20than%0Athe%20SOTA%20projects%20in%20the%20area%20of%20building%20the%203D%20model%20precise%20motions%20and%0Aactions%20control%2C%20and%20generating%20a%20stable%20semantic%20continuous%20time-unlimited%203D%0Avideo%20with%20the%20input%20text%20instructions.%20Here%20we%20will%20analyze%20the%20detailed%0Aimplementation%20methods%20and%20theories%20analyses.%20Relative%20comparisons%20and%0Aconclusions%20will%20be%20presented.%20The%20project%20code%20is%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneTo3D%253A%2520One%2520Image%2520to%2520Re-editable%2520Dynamic%25203D%2520Model%2520and%2520Video%2520Generation%26entry.906535625%3DJinwei%2520Lin%26entry.1292438233%3D%2520%2520One%2520image%2520to%2520editable%2520dynamic%25203D%2520model%2520and%2520video%2520generation%2520is%2520novel%250Adirection%2520and%2520change%2520in%2520the%2520research%2520area%2520of%2520single%2520image%2520to%25203D%2520representation%250Aor%25203D%2520reconstruction%2520of%2520image.%2520Gaussian%2520Splatting%2520has%2520demonstrated%2520its%250Aadvantages%2520in%2520implicit%25203D%2520reconstruction%252C%2520compared%2520with%2520the%2520original%2520Neural%250ARadiance%2520Fields.%2520As%2520the%2520rapid%2520development%2520of%2520technologies%2520and%2520principles%252C%250Apeople%2520tried%2520to%2520used%2520the%2520Stable%2520Diffusion%2520models%2520to%2520generate%2520targeted%2520models%250Awith%2520text%2520instructions.%2520However%252C%2520using%2520the%2520normal%2520implicit%2520machine%2520learning%250Amethods%2520is%2520hard%2520to%2520gain%2520the%2520precise%2520motions%2520and%2520actions%2520control%252C%2520further%2520more%252C%250Ait%2520is%2520difficult%2520to%2520generate%2520a%2520long%2520content%2520and%2520semantic%2520continuous%25203D%2520video.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520the%2520OneTo3D%252C%2520a%2520method%2520and%2520theory%2520to%2520used%2520one%250Asingle%2520image%2520to%2520generate%2520the%2520editable%25203D%2520model%2520and%2520generate%2520the%2520targeted%250Asemantic%2520continuous%2520time-unlimited%25203D%2520video.%2520We%2520used%2520a%2520normal%2520basic%2520Gaussian%250ASplatting%2520model%2520to%2520generate%2520the%25203D%2520model%2520from%2520a%2520single%2520image%252C%2520which%2520requires%250Aless%2520volume%2520of%2520video%2520memory%2520and%2520computer%2520calculation%2520ability.%2520Subsequently%252C%2520we%250Adesigned%2520an%2520automatic%2520generation%2520and%2520self-adaptive%2520binding%2520mechanism%2520for%2520the%250Aobject%2520armature.%2520Combined%2520with%2520the%2520re-editable%2520motions%2520and%2520actions%2520analyzing%250Aand%2520controlling%2520algorithm%2520we%2520proposed%252C%2520we%2520can%2520achieve%2520a%2520better%2520performance%2520than%250Athe%2520SOTA%2520projects%2520in%2520the%2520area%2520of%2520building%2520the%25203D%2520model%2520precise%2520motions%2520and%250Aactions%2520control%252C%2520and%2520generating%2520a%2520stable%2520semantic%2520continuous%2520time-unlimited%25203D%250Avideo%2520with%2520the%2520input%2520text%2520instructions.%2520Here%2520we%2520will%2520analyze%2520the%2520detailed%250Aimplementation%2520methods%2520and%2520theories%2520analyses.%2520Relative%2520comparisons%2520and%250Aconclusions%2520will%2520be%2520presented.%2520The%2520project%2520code%2520is%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneTo3D%3A%20One%20Image%20to%20Re-editable%20Dynamic%203D%20Model%20and%20Video%20Generation&entry.906535625=Jinwei%20Lin&entry.1292438233=%20%20One%20image%20to%20editable%20dynamic%203D%20model%20and%20video%20generation%20is%20novel%0Adirection%20and%20change%20in%20the%20research%20area%20of%20single%20image%20to%203D%20representation%0Aor%203D%20reconstruction%20of%20image.%20Gaussian%20Splatting%20has%20demonstrated%20its%0Aadvantages%20in%20implicit%203D%20reconstruction%2C%20compared%20with%20the%20original%20Neural%0ARadiance%20Fields.%20As%20the%20rapid%20development%20of%20technologies%20and%20principles%2C%0Apeople%20tried%20to%20used%20the%20Stable%20Diffusion%20models%20to%20generate%20targeted%20models%0Awith%20text%20instructions.%20However%2C%20using%20the%20normal%20implicit%20machine%20learning%0Amethods%20is%20hard%20to%20gain%20the%20precise%20motions%20and%20actions%20control%2C%20further%20more%2C%0Ait%20is%20difficult%20to%20generate%20a%20long%20content%20and%20semantic%20continuous%203D%20video.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20the%20OneTo3D%2C%20a%20method%20and%20theory%20to%20used%20one%0Asingle%20image%20to%20generate%20the%20editable%203D%20model%20and%20generate%20the%20targeted%0Asemantic%20continuous%20time-unlimited%203D%20video.%20We%20used%20a%20normal%20basic%20Gaussian%0ASplatting%20model%20to%20generate%20the%203D%20model%20from%20a%20single%20image%2C%20which%20requires%0Aless%20volume%20of%20video%20memory%20and%20computer%20calculation%20ability.%20Subsequently%2C%20we%0Adesigned%20an%20automatic%20generation%20and%20self-adaptive%20binding%20mechanism%20for%20the%0Aobject%20armature.%20Combined%20with%20the%20re-editable%20motions%20and%20actions%20analyzing%0Aand%20controlling%20algorithm%20we%20proposed%2C%20we%20can%20achieve%20a%20better%20performance%20than%0Athe%20SOTA%20projects%20in%20the%20area%20of%20building%20the%203D%20model%20precise%20motions%20and%0Aactions%20control%2C%20and%20generating%20a%20stable%20semantic%20continuous%20time-unlimited%203D%0Avideo%20with%20the%20input%20text%20instructions.%20Here%20we%20will%20analyze%20the%20detailed%0Aimplementation%20methods%20and%20theories%20analyses.%20Relative%20comparisons%20and%0Aconclusions%20will%20be%20presented.%20The%20project%20code%20is%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06547v1&entry.124074799=Read"},
{"title": "Visualizing Neural Network Imagination", "author": "Nevan Wichers and Victor Tao and Riccardo Volpato and Fazl Barez", "abstract": "  In certain situations, neural networks will represent environment states in\ntheir hidden activations. Our goal is to visualize what environment states the\nnetworks are representing. We experiment with a recurrent neural network (RNN)\narchitecture with a decoder network at the end. After training, we apply the\ndecoder to the intermediate representations of the network to visualize what\nthey represent. We define a quantitative interpretability metric and use it to\ndemonstrate that hidden states can be highly interpretable on a simple task. We\nalso develop autoencoder and adversarial techniques and show that benefit\ninterpretability.\n", "link": "http://arxiv.org/abs/2405.06409v1", "date": "2024-05-10", "relevancy": 2.5559, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.519}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5172}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20Neural%20Network%20Imagination&body=Title%3A%20Visualizing%20Neural%20Network%20Imagination%0AAuthor%3A%20Nevan%20Wichers%20and%20Victor%20Tao%20and%20Riccardo%20Volpato%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20In%20certain%20situations%2C%20neural%20networks%20will%20represent%20environment%20states%20in%0Atheir%20hidden%20activations.%20Our%20goal%20is%20to%20visualize%20what%20environment%20states%20the%0Anetworks%20are%20representing.%20We%20experiment%20with%20a%20recurrent%20neural%20network%20%28RNN%29%0Aarchitecture%20with%20a%20decoder%20network%20at%20the%20end.%20After%20training%2C%20we%20apply%20the%0Adecoder%20to%20the%20intermediate%20representations%20of%20the%20network%20to%20visualize%20what%0Athey%20represent.%20We%20define%20a%20quantitative%20interpretability%20metric%20and%20use%20it%20to%0Ademonstrate%20that%20hidden%20states%20can%20be%20highly%20interpretable%20on%20a%20simple%20task.%20We%0Aalso%20develop%20autoencoder%20and%20adversarial%20techniques%20and%20show%20that%20benefit%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520Neural%2520Network%2520Imagination%26entry.906535625%3DNevan%2520Wichers%2520and%2520Victor%2520Tao%2520and%2520Riccardo%2520Volpato%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520In%2520certain%2520situations%252C%2520neural%2520networks%2520will%2520represent%2520environment%2520states%2520in%250Atheir%2520hidden%2520activations.%2520Our%2520goal%2520is%2520to%2520visualize%2520what%2520environment%2520states%2520the%250Anetworks%2520are%2520representing.%2520We%2520experiment%2520with%2520a%2520recurrent%2520neural%2520network%2520%2528RNN%2529%250Aarchitecture%2520with%2520a%2520decoder%2520network%2520at%2520the%2520end.%2520After%2520training%252C%2520we%2520apply%2520the%250Adecoder%2520to%2520the%2520intermediate%2520representations%2520of%2520the%2520network%2520to%2520visualize%2520what%250Athey%2520represent.%2520We%2520define%2520a%2520quantitative%2520interpretability%2520metric%2520and%2520use%2520it%2520to%250Ademonstrate%2520that%2520hidden%2520states%2520can%2520be%2520highly%2520interpretable%2520on%2520a%2520simple%2520task.%2520We%250Aalso%2520develop%2520autoencoder%2520and%2520adversarial%2520techniques%2520and%2520show%2520that%2520benefit%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20Neural%20Network%20Imagination&entry.906535625=Nevan%20Wichers%20and%20Victor%20Tao%20and%20Riccardo%20Volpato%20and%20Fazl%20Barez&entry.1292438233=%20%20In%20certain%20situations%2C%20neural%20networks%20will%20represent%20environment%20states%20in%0Atheir%20hidden%20activations.%20Our%20goal%20is%20to%20visualize%20what%20environment%20states%20the%0Anetworks%20are%20representing.%20We%20experiment%20with%20a%20recurrent%20neural%20network%20%28RNN%29%0Aarchitecture%20with%20a%20decoder%20network%20at%20the%20end.%20After%20training%2C%20we%20apply%20the%0Adecoder%20to%20the%20intermediate%20representations%20of%20the%20network%20to%20visualize%20what%0Athey%20represent.%20We%20define%20a%20quantitative%20interpretability%20metric%20and%20use%20it%20to%0Ademonstrate%20that%20hidden%20states%20can%20be%20highly%20interpretable%20on%20a%20simple%20task.%20We%0Aalso%20develop%20autoencoder%20and%20adversarial%20techniques%20and%20show%20that%20benefit%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06409v1&entry.124074799=Read"},
{"title": "Attend, Distill, Detect: Attention-aware Entropy Distillation for\n  Anomaly Detection", "author": "Sushovan Jena and Vishwas Saini and Ujjwal Shaw and Pavitra Jain and Abhay Singh Raihal and Anoushka Banerjee and Sharad Joshi and Ananth Ganesh and Arnav Bhavsar", "abstract": "  Unsupervised anomaly detection encompasses diverse applications in industrial\nsettings where a high-throughput and precision is imperative. Early works were\ncentered around one-class-one-model paradigm, which poses significant\nchallenges in large-scale production environments. Knowledge-distillation based\nmulti-class anomaly detection promises a low latency with a reasonably good\nperformance but with a significant drop as compared to one-class version. We\npropose a DCAM (Distributed Convolutional Attention Module) which improves the\ndistillation process between teacher and student networks when there is a high\nvariance among multiple classes or objects. Integrated multi-scale feature\nmatching strategy to utilise a mixture of multi-level knowledge from the\nfeature pyramid of the two networks, intuitively helping in detecting anomalies\nof varying sizes which is also an inherent problem in the multi-class scenario.\nBriefly, our DCAM module consists of Convolutional Attention blocks distributed\nacross the feature maps of the student network, which essentially learns to\nmasks the irrelevant information during student learning alleviating the\n\"cross-class interference\" problem. This process is accompanied by minimizing\nthe relative entropy using KL-Divergence in Spatial dimension and a\nChannel-wise Cosine Similarity between the same feature maps of teacher and\nstudent. The losses enables to achieve scale-invariance and capture non-linear\nrelationships. We also highlight that the DCAM module would only be used during\ntraining and not during inference as we only need the learned feature maps and\nlosses for anomaly scoring and hence, gaining a performance gain of 3.92% than\nthe multi-class baseline with a preserved latency.\n", "link": "http://arxiv.org/abs/2405.06467v1", "date": "2024-05-10", "relevancy": 2.5051, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5128}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.502}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attend%2C%20Distill%2C%20Detect%3A%20Attention-aware%20Entropy%20Distillation%20for%0A%20%20Anomaly%20Detection&body=Title%3A%20Attend%2C%20Distill%2C%20Detect%3A%20Attention-aware%20Entropy%20Distillation%20for%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Sushovan%20Jena%20and%20Vishwas%20Saini%20and%20Ujjwal%20Shaw%20and%20Pavitra%20Jain%20and%20Abhay%20Singh%20Raihal%20and%20Anoushka%20Banerjee%20and%20Sharad%20Joshi%20and%20Ananth%20Ganesh%20and%20Arnav%20Bhavsar%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20encompasses%20diverse%20applications%20in%20industrial%0Asettings%20where%20a%20high-throughput%20and%20precision%20is%20imperative.%20Early%20works%20were%0Acentered%20around%20one-class-one-model%20paradigm%2C%20which%20poses%20significant%0Achallenges%20in%20large-scale%20production%20environments.%20Knowledge-distillation%20based%0Amulti-class%20anomaly%20detection%20promises%20a%20low%20latency%20with%20a%20reasonably%20good%0Aperformance%20but%20with%20a%20significant%20drop%20as%20compared%20to%20one-class%20version.%20We%0Apropose%20a%20DCAM%20%28Distributed%20Convolutional%20Attention%20Module%29%20which%20improves%20the%0Adistillation%20process%20between%20teacher%20and%20student%20networks%20when%20there%20is%20a%20high%0Avariance%20among%20multiple%20classes%20or%20objects.%20Integrated%20multi-scale%20feature%0Amatching%20strategy%20to%20utilise%20a%20mixture%20of%20multi-level%20knowledge%20from%20the%0Afeature%20pyramid%20of%20the%20two%20networks%2C%20intuitively%20helping%20in%20detecting%20anomalies%0Aof%20varying%20sizes%20which%20is%20also%20an%20inherent%20problem%20in%20the%20multi-class%20scenario.%0ABriefly%2C%20our%20DCAM%20module%20consists%20of%20Convolutional%20Attention%20blocks%20distributed%0Aacross%20the%20feature%20maps%20of%20the%20student%20network%2C%20which%20essentially%20learns%20to%0Amasks%20the%20irrelevant%20information%20during%20student%20learning%20alleviating%20the%0A%22cross-class%20interference%22%20problem.%20This%20process%20is%20accompanied%20by%20minimizing%0Athe%20relative%20entropy%20using%20KL-Divergence%20in%20Spatial%20dimension%20and%20a%0AChannel-wise%20Cosine%20Similarity%20between%20the%20same%20feature%20maps%20of%20teacher%20and%0Astudent.%20The%20losses%20enables%20to%20achieve%20scale-invariance%20and%20capture%20non-linear%0Arelationships.%20We%20also%20highlight%20that%20the%20DCAM%20module%20would%20only%20be%20used%20during%0Atraining%20and%20not%20during%20inference%20as%20we%20only%20need%20the%20learned%20feature%20maps%20and%0Alosses%20for%20anomaly%20scoring%20and%20hence%2C%20gaining%20a%20performance%20gain%20of%203.92%25%20than%0Athe%20multi-class%20baseline%20with%20a%20preserved%20latency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttend%252C%2520Distill%252C%2520Detect%253A%2520Attention-aware%2520Entropy%2520Distillation%2520for%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DSushovan%2520Jena%2520and%2520Vishwas%2520Saini%2520and%2520Ujjwal%2520Shaw%2520and%2520Pavitra%2520Jain%2520and%2520Abhay%2520Singh%2520Raihal%2520and%2520Anoushka%2520Banerjee%2520and%2520Sharad%2520Joshi%2520and%2520Ananth%2520Ganesh%2520and%2520Arnav%2520Bhavsar%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520encompasses%2520diverse%2520applications%2520in%2520industrial%250Asettings%2520where%2520a%2520high-throughput%2520and%2520precision%2520is%2520imperative.%2520Early%2520works%2520were%250Acentered%2520around%2520one-class-one-model%2520paradigm%252C%2520which%2520poses%2520significant%250Achallenges%2520in%2520large-scale%2520production%2520environments.%2520Knowledge-distillation%2520based%250Amulti-class%2520anomaly%2520detection%2520promises%2520a%2520low%2520latency%2520with%2520a%2520reasonably%2520good%250Aperformance%2520but%2520with%2520a%2520significant%2520drop%2520as%2520compared%2520to%2520one-class%2520version.%2520We%250Apropose%2520a%2520DCAM%2520%2528Distributed%2520Convolutional%2520Attention%2520Module%2529%2520which%2520improves%2520the%250Adistillation%2520process%2520between%2520teacher%2520and%2520student%2520networks%2520when%2520there%2520is%2520a%2520high%250Avariance%2520among%2520multiple%2520classes%2520or%2520objects.%2520Integrated%2520multi-scale%2520feature%250Amatching%2520strategy%2520to%2520utilise%2520a%2520mixture%2520of%2520multi-level%2520knowledge%2520from%2520the%250Afeature%2520pyramid%2520of%2520the%2520two%2520networks%252C%2520intuitively%2520helping%2520in%2520detecting%2520anomalies%250Aof%2520varying%2520sizes%2520which%2520is%2520also%2520an%2520inherent%2520problem%2520in%2520the%2520multi-class%2520scenario.%250ABriefly%252C%2520our%2520DCAM%2520module%2520consists%2520of%2520Convolutional%2520Attention%2520blocks%2520distributed%250Aacross%2520the%2520feature%2520maps%2520of%2520the%2520student%2520network%252C%2520which%2520essentially%2520learns%2520to%250Amasks%2520the%2520irrelevant%2520information%2520during%2520student%2520learning%2520alleviating%2520the%250A%2522cross-class%2520interference%2522%2520problem.%2520This%2520process%2520is%2520accompanied%2520by%2520minimizing%250Athe%2520relative%2520entropy%2520using%2520KL-Divergence%2520in%2520Spatial%2520dimension%2520and%2520a%250AChannel-wise%2520Cosine%2520Similarity%2520between%2520the%2520same%2520feature%2520maps%2520of%2520teacher%2520and%250Astudent.%2520The%2520losses%2520enables%2520to%2520achieve%2520scale-invariance%2520and%2520capture%2520non-linear%250Arelationships.%2520We%2520also%2520highlight%2520that%2520the%2520DCAM%2520module%2520would%2520only%2520be%2520used%2520during%250Atraining%2520and%2520not%2520during%2520inference%2520as%2520we%2520only%2520need%2520the%2520learned%2520feature%2520maps%2520and%250Alosses%2520for%2520anomaly%2520scoring%2520and%2520hence%252C%2520gaining%2520a%2520performance%2520gain%2520of%25203.92%2525%2520than%250Athe%2520multi-class%2520baseline%2520with%2520a%2520preserved%2520latency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attend%2C%20Distill%2C%20Detect%3A%20Attention-aware%20Entropy%20Distillation%20for%0A%20%20Anomaly%20Detection&entry.906535625=Sushovan%20Jena%20and%20Vishwas%20Saini%20and%20Ujjwal%20Shaw%20and%20Pavitra%20Jain%20and%20Abhay%20Singh%20Raihal%20and%20Anoushka%20Banerjee%20and%20Sharad%20Joshi%20and%20Ananth%20Ganesh%20and%20Arnav%20Bhavsar&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20encompasses%20diverse%20applications%20in%20industrial%0Asettings%20where%20a%20high-throughput%20and%20precision%20is%20imperative.%20Early%20works%20were%0Acentered%20around%20one-class-one-model%20paradigm%2C%20which%20poses%20significant%0Achallenges%20in%20large-scale%20production%20environments.%20Knowledge-distillation%20based%0Amulti-class%20anomaly%20detection%20promises%20a%20low%20latency%20with%20a%20reasonably%20good%0Aperformance%20but%20with%20a%20significant%20drop%20as%20compared%20to%20one-class%20version.%20We%0Apropose%20a%20DCAM%20%28Distributed%20Convolutional%20Attention%20Module%29%20which%20improves%20the%0Adistillation%20process%20between%20teacher%20and%20student%20networks%20when%20there%20is%20a%20high%0Avariance%20among%20multiple%20classes%20or%20objects.%20Integrated%20multi-scale%20feature%0Amatching%20strategy%20to%20utilise%20a%20mixture%20of%20multi-level%20knowledge%20from%20the%0Afeature%20pyramid%20of%20the%20two%20networks%2C%20intuitively%20helping%20in%20detecting%20anomalies%0Aof%20varying%20sizes%20which%20is%20also%20an%20inherent%20problem%20in%20the%20multi-class%20scenario.%0ABriefly%2C%20our%20DCAM%20module%20consists%20of%20Convolutional%20Attention%20blocks%20distributed%0Aacross%20the%20feature%20maps%20of%20the%20student%20network%2C%20which%20essentially%20learns%20to%0Amasks%20the%20irrelevant%20information%20during%20student%20learning%20alleviating%20the%0A%22cross-class%20interference%22%20problem.%20This%20process%20is%20accompanied%20by%20minimizing%0Athe%20relative%20entropy%20using%20KL-Divergence%20in%20Spatial%20dimension%20and%20a%0AChannel-wise%20Cosine%20Similarity%20between%20the%20same%20feature%20maps%20of%20teacher%20and%0Astudent.%20The%20losses%20enables%20to%20achieve%20scale-invariance%20and%20capture%20non-linear%0Arelationships.%20We%20also%20highlight%20that%20the%20DCAM%20module%20would%20only%20be%20used%20during%0Atraining%20and%20not%20during%20inference%20as%20we%20only%20need%20the%20learned%20feature%20maps%20and%0Alosses%20for%20anomaly%20scoring%20and%20hence%2C%20gaining%20a%20performance%20gain%20of%203.92%25%20than%0Athe%20multi-class%20baseline%20with%20a%20preserved%20latency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06467v1&entry.124074799=Read"},
{"title": "I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions", "author": "Jinwei Lin", "abstract": "  3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain\nan implicit neural learning rendering result than the traditional neural\nrendering technology but keep the more high-definition fast rendering speed.\nBut it is still difficult to achieve a fast enough efficiency on 3D Gaussian\nSplatting for the practical applications. To Address this issue, we propose the\nI3DS, a synthetic model performance improvement evaluation solution and\nexperiments test. From multiple and important levels or dimensions of the\noriginal 3D Gaussian Splatting, we made more than two thousand various kinds of\nexperiments to test how the selected different items and components can make an\nimpact on the training efficiency of the 3D Gaussian Splatting model. In this\npaper, we will share abundant and meaningful experiences and methods about how\nto improve the training, performance and the impacts caused by different items\nof the model. A special but normal Integer compression in base 95 and a\nfloating-point compression in base 94 with ASCII encoding and decoding\nmechanism is presented. Many real and effective experiments and test results or\nphenomena will be recorded. After a series of reasonable fine-tuning, I3DS can\ngain excellent performance improvements than the previous one. The project code\nis available as open source.\n", "link": "http://arxiv.org/abs/2405.06408v1", "date": "2024-05-10", "relevancy": 2.4741, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6617}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5992}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I3DGS%3A%20Improve%203D%20Gaussian%20Splatting%20from%20Multiple%20Dimensions&body=Title%3A%20I3DGS%3A%20Improve%203D%20Gaussian%20Splatting%20from%20Multiple%20Dimensions%0AAuthor%3A%20Jinwei%20Lin%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20is%20a%20novel%20method%20for%203D%20view%20synthesis%2C%20which%20can%20gain%0Aan%20implicit%20neural%20learning%20rendering%20result%20than%20the%20traditional%20neural%0Arendering%20technology%20but%20keep%20the%20more%20high-definition%20fast%20rendering%20speed.%0ABut%20it%20is%20still%20difficult%20to%20achieve%20a%20fast%20enough%20efficiency%20on%203D%20Gaussian%0ASplatting%20for%20the%20practical%20applications.%20To%20Address%20this%20issue%2C%20we%20propose%20the%0AI3DS%2C%20a%20synthetic%20model%20performance%20improvement%20evaluation%20solution%20and%0Aexperiments%20test.%20From%20multiple%20and%20important%20levels%20or%20dimensions%20of%20the%0Aoriginal%203D%20Gaussian%20Splatting%2C%20we%20made%20more%20than%20two%20thousand%20various%20kinds%20of%0Aexperiments%20to%20test%20how%20the%20selected%20different%20items%20and%20components%20can%20make%20an%0Aimpact%20on%20the%20training%20efficiency%20of%20the%203D%20Gaussian%20Splatting%20model.%20In%20this%0Apaper%2C%20we%20will%20share%20abundant%20and%20meaningful%20experiences%20and%20methods%20about%20how%0Ato%20improve%20the%20training%2C%20performance%20and%20the%20impacts%20caused%20by%20different%20items%0Aof%20the%20model.%20A%20special%20but%20normal%20Integer%20compression%20in%20base%2095%20and%20a%0Afloating-point%20compression%20in%20base%2094%20with%20ASCII%20encoding%20and%20decoding%0Amechanism%20is%20presented.%20Many%20real%20and%20effective%20experiments%20and%20test%20results%20or%0Aphenomena%20will%20be%20recorded.%20After%20a%20series%20of%20reasonable%20fine-tuning%2C%20I3DS%20can%0Again%20excellent%20performance%20improvements%20than%20the%20previous%20one.%20The%20project%20code%0Ais%20available%20as%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI3DGS%253A%2520Improve%25203D%2520Gaussian%2520Splatting%2520from%2520Multiple%2520Dimensions%26entry.906535625%3DJinwei%2520Lin%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520is%2520a%2520novel%2520method%2520for%25203D%2520view%2520synthesis%252C%2520which%2520can%2520gain%250Aan%2520implicit%2520neural%2520learning%2520rendering%2520result%2520than%2520the%2520traditional%2520neural%250Arendering%2520technology%2520but%2520keep%2520the%2520more%2520high-definition%2520fast%2520rendering%2520speed.%250ABut%2520it%2520is%2520still%2520difficult%2520to%2520achieve%2520a%2520fast%2520enough%2520efficiency%2520on%25203D%2520Gaussian%250ASplatting%2520for%2520the%2520practical%2520applications.%2520To%2520Address%2520this%2520issue%252C%2520we%2520propose%2520the%250AI3DS%252C%2520a%2520synthetic%2520model%2520performance%2520improvement%2520evaluation%2520solution%2520and%250Aexperiments%2520test.%2520From%2520multiple%2520and%2520important%2520levels%2520or%2520dimensions%2520of%2520the%250Aoriginal%25203D%2520Gaussian%2520Splatting%252C%2520we%2520made%2520more%2520than%2520two%2520thousand%2520various%2520kinds%2520of%250Aexperiments%2520to%2520test%2520how%2520the%2520selected%2520different%2520items%2520and%2520components%2520can%2520make%2520an%250Aimpact%2520on%2520the%2520training%2520efficiency%2520of%2520the%25203D%2520Gaussian%2520Splatting%2520model.%2520In%2520this%250Apaper%252C%2520we%2520will%2520share%2520abundant%2520and%2520meaningful%2520experiences%2520and%2520methods%2520about%2520how%250Ato%2520improve%2520the%2520training%252C%2520performance%2520and%2520the%2520impacts%2520caused%2520by%2520different%2520items%250Aof%2520the%2520model.%2520A%2520special%2520but%2520normal%2520Integer%2520compression%2520in%2520base%252095%2520and%2520a%250Afloating-point%2520compression%2520in%2520base%252094%2520with%2520ASCII%2520encoding%2520and%2520decoding%250Amechanism%2520is%2520presented.%2520Many%2520real%2520and%2520effective%2520experiments%2520and%2520test%2520results%2520or%250Aphenomena%2520will%2520be%2520recorded.%2520After%2520a%2520series%2520of%2520reasonable%2520fine-tuning%252C%2520I3DS%2520can%250Again%2520excellent%2520performance%2520improvements%2520than%2520the%2520previous%2520one.%2520The%2520project%2520code%250Ais%2520available%2520as%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I3DGS%3A%20Improve%203D%20Gaussian%20Splatting%20from%20Multiple%20Dimensions&entry.906535625=Jinwei%20Lin&entry.1292438233=%20%203D%20Gaussian%20Splatting%20is%20a%20novel%20method%20for%203D%20view%20synthesis%2C%20which%20can%20gain%0Aan%20implicit%20neural%20learning%20rendering%20result%20than%20the%20traditional%20neural%0Arendering%20technology%20but%20keep%20the%20more%20high-definition%20fast%20rendering%20speed.%0ABut%20it%20is%20still%20difficult%20to%20achieve%20a%20fast%20enough%20efficiency%20on%203D%20Gaussian%0ASplatting%20for%20the%20practical%20applications.%20To%20Address%20this%20issue%2C%20we%20propose%20the%0AI3DS%2C%20a%20synthetic%20model%20performance%20improvement%20evaluation%20solution%20and%0Aexperiments%20test.%20From%20multiple%20and%20important%20levels%20or%20dimensions%20of%20the%0Aoriginal%203D%20Gaussian%20Splatting%2C%20we%20made%20more%20than%20two%20thousand%20various%20kinds%20of%0Aexperiments%20to%20test%20how%20the%20selected%20different%20items%20and%20components%20can%20make%20an%0Aimpact%20on%20the%20training%20efficiency%20of%20the%203D%20Gaussian%20Splatting%20model.%20In%20this%0Apaper%2C%20we%20will%20share%20abundant%20and%20meaningful%20experiences%20and%20methods%20about%20how%0Ato%20improve%20the%20training%2C%20performance%20and%20the%20impacts%20caused%20by%20different%20items%0Aof%20the%20model.%20A%20special%20but%20normal%20Integer%20compression%20in%20base%2095%20and%20a%0Afloating-point%20compression%20in%20base%2094%20with%20ASCII%20encoding%20and%20decoding%0Amechanism%20is%20presented.%20Many%20real%20and%20effective%20experiments%20and%20test%20results%20or%0Aphenomena%20will%20be%20recorded.%20After%20a%20series%20of%20reasonable%20fine-tuning%2C%20I3DS%20can%0Again%20excellent%20performance%20improvements%20than%20the%20previous%20one.%20The%20project%20code%0Ais%20available%20as%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06408v1&entry.124074799=Read"},
{"title": "Controllable Image Generation With Composed Parallel Token Prediction", "author": "Jamie Stirling and Noura Al-Moubayed", "abstract": "  Compositional image generation requires models to generalise well in\nsituations where two or more input concepts do not necessarily appear together\nin training (compositional generalisation). Despite recent progress in\ncompositional image generation via composing continuous sampling processes such\nas diffusion and energy-based models, composing discrete generative processes\nhas remained an open challenge, with the promise of providing improvements in\nefficiency, interpretability and simplicity. To this end, we propose a\nformulation for controllable conditional generation of images via composing the\nlog-probability outputs of discrete generative models of the latent space. Our\napproach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art\ngeneration accuracy in three distinct settings (FFHQ, Positional CLEVR and\nRelational CLEVR) while attaining competitive Fr\\'echet Inception Distance\n(FID) scores. Our method attains an average generation accuracy of $80.71\\%$\nacross the studied settings. Our method also outperforms the next-best approach\n(ranked by accuracy) in terms of FID in seven out of nine experiments, with an\naverage FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our\nmethod offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous\ncompositional methods on our hardware. We find that our method can generalise\nto combinations of input conditions that lie outside the training data (e.g.\nmore objects per image) in addition to offering an interpretable dimension of\ncontrollability via concept weighting. We further demonstrate that our approach\ncan be readily applied to an open pre-trained discrete text-to-image model\nwithout any fine-tuning, allowing for fine-grained control of text-to-image\ngeneration.\n", "link": "http://arxiv.org/abs/2405.06535v1", "date": "2024-05-10", "relevancy": 2.4187, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6082}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6072}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Image%20Generation%20With%20Composed%20Parallel%20Token%20Prediction&body=Title%3A%20Controllable%20Image%20Generation%20With%20Composed%20Parallel%20Token%20Prediction%0AAuthor%3A%20Jamie%20Stirling%20and%20Noura%20Al-Moubayed%0AAbstract%3A%20%20%20Compositional%20image%20generation%20requires%20models%20to%20generalise%20well%20in%0Asituations%20where%20two%20or%20more%20input%20concepts%20do%20not%20necessarily%20appear%20together%0Ain%20training%20%28compositional%20generalisation%29.%20Despite%20recent%20progress%20in%0Acompositional%20image%20generation%20via%20composing%20continuous%20sampling%20processes%20such%0Aas%20diffusion%20and%20energy-based%20models%2C%20composing%20discrete%20generative%20processes%0Ahas%20remained%20an%20open%20challenge%2C%20with%20the%20promise%20of%20providing%20improvements%20in%0Aefficiency%2C%20interpretability%20and%20simplicity.%20To%20this%20end%2C%20we%20propose%20a%0Aformulation%20for%20controllable%20conditional%20generation%20of%20images%20via%20composing%20the%0Alog-probability%20outputs%20of%20discrete%20generative%20models%20of%20the%20latent%20space.%20Our%0Aapproach%2C%20when%20applied%20alongside%20VQ-VAE%20and%20VQ-GAN%2C%20achieves%20state-of-the-art%0Ageneration%20accuracy%20in%20three%20distinct%20settings%20%28FFHQ%2C%20Positional%20CLEVR%20and%0ARelational%20CLEVR%29%20while%20attaining%20competitive%20Fr%5C%27echet%20Inception%20Distance%0A%28FID%29%20scores.%20Our%20method%20attains%20an%20average%20generation%20accuracy%20of%20%2480.71%5C%25%24%0Aacross%20the%20studied%20settings.%20Our%20method%20also%20outperforms%20the%20next-best%20approach%0A%28ranked%20by%20accuracy%29%20in%20terms%20of%20FID%20in%20seven%20out%20of%20nine%20experiments%2C%20with%20an%0Aaverage%20FID%20of%20%2424.23%24%20%28an%20average%20improvement%20of%20%24-9.58%24%29.%20Furthermore%2C%20our%0Amethod%20offers%20a%20%242.3%5Ctimes%24%20to%20%2412%5Ctimes%24%20speedup%20over%20comparable%20continuous%0Acompositional%20methods%20on%20our%20hardware.%20We%20find%20that%20our%20method%20can%20generalise%0Ato%20combinations%20of%20input%20conditions%20that%20lie%20outside%20the%20training%20data%20%28e.g.%0Amore%20objects%20per%20image%29%20in%20addition%20to%20offering%20an%20interpretable%20dimension%20of%0Acontrollability%20via%20concept%20weighting.%20We%20further%20demonstrate%20that%20our%20approach%0Acan%20be%20readily%20applied%20to%20an%20open%20pre-trained%20discrete%20text-to-image%20model%0Awithout%20any%20fine-tuning%2C%20allowing%20for%20fine-grained%20control%20of%20text-to-image%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Image%2520Generation%2520With%2520Composed%2520Parallel%2520Token%2520Prediction%26entry.906535625%3DJamie%2520Stirling%2520and%2520Noura%2520Al-Moubayed%26entry.1292438233%3D%2520%2520Compositional%2520image%2520generation%2520requires%2520models%2520to%2520generalise%2520well%2520in%250Asituations%2520where%2520two%2520or%2520more%2520input%2520concepts%2520do%2520not%2520necessarily%2520appear%2520together%250Ain%2520training%2520%2528compositional%2520generalisation%2529.%2520Despite%2520recent%2520progress%2520in%250Acompositional%2520image%2520generation%2520via%2520composing%2520continuous%2520sampling%2520processes%2520such%250Aas%2520diffusion%2520and%2520energy-based%2520models%252C%2520composing%2520discrete%2520generative%2520processes%250Ahas%2520remained%2520an%2520open%2520challenge%252C%2520with%2520the%2520promise%2520of%2520providing%2520improvements%2520in%250Aefficiency%252C%2520interpretability%2520and%2520simplicity.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Aformulation%2520for%2520controllable%2520conditional%2520generation%2520of%2520images%2520via%2520composing%2520the%250Alog-probability%2520outputs%2520of%2520discrete%2520generative%2520models%2520of%2520the%2520latent%2520space.%2520Our%250Aapproach%252C%2520when%2520applied%2520alongside%2520VQ-VAE%2520and%2520VQ-GAN%252C%2520achieves%2520state-of-the-art%250Ageneration%2520accuracy%2520in%2520three%2520distinct%2520settings%2520%2528FFHQ%252C%2520Positional%2520CLEVR%2520and%250ARelational%2520CLEVR%2529%2520while%2520attaining%2520competitive%2520Fr%255C%2527echet%2520Inception%2520Distance%250A%2528FID%2529%2520scores.%2520Our%2520method%2520attains%2520an%2520average%2520generation%2520accuracy%2520of%2520%252480.71%255C%2525%2524%250Aacross%2520the%2520studied%2520settings.%2520Our%2520method%2520also%2520outperforms%2520the%2520next-best%2520approach%250A%2528ranked%2520by%2520accuracy%2529%2520in%2520terms%2520of%2520FID%2520in%2520seven%2520out%2520of%2520nine%2520experiments%252C%2520with%2520an%250Aaverage%2520FID%2520of%2520%252424.23%2524%2520%2528an%2520average%2520improvement%2520of%2520%2524-9.58%2524%2529.%2520Furthermore%252C%2520our%250Amethod%2520offers%2520a%2520%25242.3%255Ctimes%2524%2520to%2520%252412%255Ctimes%2524%2520speedup%2520over%2520comparable%2520continuous%250Acompositional%2520methods%2520on%2520our%2520hardware.%2520We%2520find%2520that%2520our%2520method%2520can%2520generalise%250Ato%2520combinations%2520of%2520input%2520conditions%2520that%2520lie%2520outside%2520the%2520training%2520data%2520%2528e.g.%250Amore%2520objects%2520per%2520image%2529%2520in%2520addition%2520to%2520offering%2520an%2520interpretable%2520dimension%2520of%250Acontrollability%2520via%2520concept%2520weighting.%2520We%2520further%2520demonstrate%2520that%2520our%2520approach%250Acan%2520be%2520readily%2520applied%2520to%2520an%2520open%2520pre-trained%2520discrete%2520text-to-image%2520model%250Awithout%2520any%2520fine-tuning%252C%2520allowing%2520for%2520fine-grained%2520control%2520of%2520text-to-image%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Image%20Generation%20With%20Composed%20Parallel%20Token%20Prediction&entry.906535625=Jamie%20Stirling%20and%20Noura%20Al-Moubayed&entry.1292438233=%20%20Compositional%20image%20generation%20requires%20models%20to%20generalise%20well%20in%0Asituations%20where%20two%20or%20more%20input%20concepts%20do%20not%20necessarily%20appear%20together%0Ain%20training%20%28compositional%20generalisation%29.%20Despite%20recent%20progress%20in%0Acompositional%20image%20generation%20via%20composing%20continuous%20sampling%20processes%20such%0Aas%20diffusion%20and%20energy-based%20models%2C%20composing%20discrete%20generative%20processes%0Ahas%20remained%20an%20open%20challenge%2C%20with%20the%20promise%20of%20providing%20improvements%20in%0Aefficiency%2C%20interpretability%20and%20simplicity.%20To%20this%20end%2C%20we%20propose%20a%0Aformulation%20for%20controllable%20conditional%20generation%20of%20images%20via%20composing%20the%0Alog-probability%20outputs%20of%20discrete%20generative%20models%20of%20the%20latent%20space.%20Our%0Aapproach%2C%20when%20applied%20alongside%20VQ-VAE%20and%20VQ-GAN%2C%20achieves%20state-of-the-art%0Ageneration%20accuracy%20in%20three%20distinct%20settings%20%28FFHQ%2C%20Positional%20CLEVR%20and%0ARelational%20CLEVR%29%20while%20attaining%20competitive%20Fr%5C%27echet%20Inception%20Distance%0A%28FID%29%20scores.%20Our%20method%20attains%20an%20average%20generation%20accuracy%20of%20%2480.71%5C%25%24%0Aacross%20the%20studied%20settings.%20Our%20method%20also%20outperforms%20the%20next-best%20approach%0A%28ranked%20by%20accuracy%29%20in%20terms%20of%20FID%20in%20seven%20out%20of%20nine%20experiments%2C%20with%20an%0Aaverage%20FID%20of%20%2424.23%24%20%28an%20average%20improvement%20of%20%24-9.58%24%29.%20Furthermore%2C%20our%0Amethod%20offers%20a%20%242.3%5Ctimes%24%20to%20%2412%5Ctimes%24%20speedup%20over%20comparable%20continuous%0Acompositional%20methods%20on%20our%20hardware.%20We%20find%20that%20our%20method%20can%20generalise%0Ato%20combinations%20of%20input%20conditions%20that%20lie%20outside%20the%20training%20data%20%28e.g.%0Amore%20objects%20per%20image%29%20in%20addition%20to%20offering%20an%20interpretable%20dimension%20of%0Acontrollability%20via%20concept%20weighting.%20We%20further%20demonstrate%20that%20our%20approach%0Acan%20be%20readily%20applied%20to%20an%20open%20pre-trained%20discrete%20text-to-image%20model%0Awithout%20any%20fine-tuning%2C%20allowing%20for%20fine-grained%20control%20of%20text-to-image%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06535v1&entry.124074799=Read"},
{"title": "360DVD: Controllable Panorama Video Generation with 360-Degree Video\n  Diffusion Model", "author": "Qian Wang and Weiqi Li and Chong Mou and Xinhua Cheng and Jian Zhang", "abstract": "  Panorama video recently attracts more interest in both study and application,\ncourtesy of its immersive experience. Due to the expensive cost of capturing\n360-degree panoramic videos, generating desirable panorama videos by prompts is\nurgently required. Lately, the emerging text-to-video (T2V) diffusion methods\ndemonstrate notable effectiveness in standard video generation. However, due to\nthe significant gap in content and motion patterns between panoramic and\nstandard videos, these methods encounter challenges in yielding satisfactory\n360-degree panoramic videos. In this paper, we propose a pipeline named\n360-Degree Video Diffusion model (360DVD) for generating 360-degree panoramic\nvideos based on the given prompts and motion conditions. Specifically, we\nintroduce a lightweight 360-Adapter accompanied by 360 Enhancement Techniques\nto transform pre-trained T2V models for panorama video generation. We further\npropose a new panorama dataset named WEB360 consisting of panoramic video-text\npairs for training 360DVD, addressing the absence of captioned panoramic video\ndatasets. Extensive experiments demonstrate the superiority and effectiveness\nof 360DVD for panorama video generation. Our project page is at\nhttps://akaneqwq.github.io/360DVD/.\n", "link": "http://arxiv.org/abs/2401.06578v2", "date": "2024-05-10", "relevancy": 2.3374, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6401}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6038}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360DVD%3A%20Controllable%20Panorama%20Video%20Generation%20with%20360-Degree%20Video%0A%20%20Diffusion%20Model&body=Title%3A%20360DVD%3A%20Controllable%20Panorama%20Video%20Generation%20with%20360-Degree%20Video%0A%20%20Diffusion%20Model%0AAuthor%3A%20Qian%20Wang%20and%20Weiqi%20Li%20and%20Chong%20Mou%20and%20Xinhua%20Cheng%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Panorama%20video%20recently%20attracts%20more%20interest%20in%20both%20study%20and%20application%2C%0Acourtesy%20of%20its%20immersive%20experience.%20Due%20to%20the%20expensive%20cost%20of%20capturing%0A360-degree%20panoramic%20videos%2C%20generating%20desirable%20panorama%20videos%20by%20prompts%20is%0Aurgently%20required.%20Lately%2C%20the%20emerging%20text-to-video%20%28T2V%29%20diffusion%20methods%0Ademonstrate%20notable%20effectiveness%20in%20standard%20video%20generation.%20However%2C%20due%20to%0Athe%20significant%20gap%20in%20content%20and%20motion%20patterns%20between%20panoramic%20and%0Astandard%20videos%2C%20these%20methods%20encounter%20challenges%20in%20yielding%20satisfactory%0A360-degree%20panoramic%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20pipeline%20named%0A360-Degree%20Video%20Diffusion%20model%20%28360DVD%29%20for%20generating%20360-degree%20panoramic%0Avideos%20based%20on%20the%20given%20prompts%20and%20motion%20conditions.%20Specifically%2C%20we%0Aintroduce%20a%20lightweight%20360-Adapter%20accompanied%20by%20360%20Enhancement%20Techniques%0Ato%20transform%20pre-trained%20T2V%20models%20for%20panorama%20video%20generation.%20We%20further%0Apropose%20a%20new%20panorama%20dataset%20named%20WEB360%20consisting%20of%20panoramic%20video-text%0Apairs%20for%20training%20360DVD%2C%20addressing%20the%20absence%20of%20captioned%20panoramic%20video%0Adatasets.%20Extensive%20experiments%20demonstrate%20the%20superiority%20and%20effectiveness%0Aof%20360DVD%20for%20panorama%20video%20generation.%20Our%20project%20page%20is%20at%0Ahttps%3A//akaneqwq.github.io/360DVD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360DVD%253A%2520Controllable%2520Panorama%2520Video%2520Generation%2520with%2520360-Degree%2520Video%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DQian%2520Wang%2520and%2520Weiqi%2520Li%2520and%2520Chong%2520Mou%2520and%2520Xinhua%2520Cheng%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Panorama%2520video%2520recently%2520attracts%2520more%2520interest%2520in%2520both%2520study%2520and%2520application%252C%250Acourtesy%2520of%2520its%2520immersive%2520experience.%2520Due%2520to%2520the%2520expensive%2520cost%2520of%2520capturing%250A360-degree%2520panoramic%2520videos%252C%2520generating%2520desirable%2520panorama%2520videos%2520by%2520prompts%2520is%250Aurgently%2520required.%2520Lately%252C%2520the%2520emerging%2520text-to-video%2520%2528T2V%2529%2520diffusion%2520methods%250Ademonstrate%2520notable%2520effectiveness%2520in%2520standard%2520video%2520generation.%2520However%252C%2520due%2520to%250Athe%2520significant%2520gap%2520in%2520content%2520and%2520motion%2520patterns%2520between%2520panoramic%2520and%250Astandard%2520videos%252C%2520these%2520methods%2520encounter%2520challenges%2520in%2520yielding%2520satisfactory%250A360-degree%2520panoramic%2520videos.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520pipeline%2520named%250A360-Degree%2520Video%2520Diffusion%2520model%2520%2528360DVD%2529%2520for%2520generating%2520360-degree%2520panoramic%250Avideos%2520based%2520on%2520the%2520given%2520prompts%2520and%2520motion%2520conditions.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520lightweight%2520360-Adapter%2520accompanied%2520by%2520360%2520Enhancement%2520Techniques%250Ato%2520transform%2520pre-trained%2520T2V%2520models%2520for%2520panorama%2520video%2520generation.%2520We%2520further%250Apropose%2520a%2520new%2520panorama%2520dataset%2520named%2520WEB360%2520consisting%2520of%2520panoramic%2520video-text%250Apairs%2520for%2520training%2520360DVD%252C%2520addressing%2520the%2520absence%2520of%2520captioned%2520panoramic%2520video%250Adatasets.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%2520and%2520effectiveness%250Aof%2520360DVD%2520for%2520panorama%2520video%2520generation.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//akaneqwq.github.io/360DVD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360DVD%3A%20Controllable%20Panorama%20Video%20Generation%20with%20360-Degree%20Video%0A%20%20Diffusion%20Model&entry.906535625=Qian%20Wang%20and%20Weiqi%20Li%20and%20Chong%20Mou%20and%20Xinhua%20Cheng%20and%20Jian%20Zhang&entry.1292438233=%20%20Panorama%20video%20recently%20attracts%20more%20interest%20in%20both%20study%20and%20application%2C%0Acourtesy%20of%20its%20immersive%20experience.%20Due%20to%20the%20expensive%20cost%20of%20capturing%0A360-degree%20panoramic%20videos%2C%20generating%20desirable%20panorama%20videos%20by%20prompts%20is%0Aurgently%20required.%20Lately%2C%20the%20emerging%20text-to-video%20%28T2V%29%20diffusion%20methods%0Ademonstrate%20notable%20effectiveness%20in%20standard%20video%20generation.%20However%2C%20due%20to%0Athe%20significant%20gap%20in%20content%20and%20motion%20patterns%20between%20panoramic%20and%0Astandard%20videos%2C%20these%20methods%20encounter%20challenges%20in%20yielding%20satisfactory%0A360-degree%20panoramic%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20pipeline%20named%0A360-Degree%20Video%20Diffusion%20model%20%28360DVD%29%20for%20generating%20360-degree%20panoramic%0Avideos%20based%20on%20the%20given%20prompts%20and%20motion%20conditions.%20Specifically%2C%20we%0Aintroduce%20a%20lightweight%20360-Adapter%20accompanied%20by%20360%20Enhancement%20Techniques%0Ato%20transform%20pre-trained%20T2V%20models%20for%20panorama%20video%20generation.%20We%20further%0Apropose%20a%20new%20panorama%20dataset%20named%20WEB360%20consisting%20of%20panoramic%20video-text%0Apairs%20for%20training%20360DVD%2C%20addressing%20the%20absence%20of%20captioned%20panoramic%20video%0Adatasets.%20Extensive%20experiments%20demonstrate%20the%20superiority%20and%20effectiveness%0Aof%20360DVD%20for%20panorama%20video%20generation.%20Our%20project%20page%20is%20at%0Ahttps%3A//akaneqwq.github.io/360DVD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06578v2&entry.124074799=Read"},
{"title": "Learning from SAM: Harnessing a Foundation Model for Sim2Real Adaptation\n  by Regularization", "author": "Mayara E. Bonani and Max Schwarz and Sven Behnke", "abstract": "  Domain adaptation is especially important for robotics applications, where\ntarget domain training data is usually scarce and annotations are costly to\nobtain. We present a method for self-supervised domain adaptation for the\nscenario where annotated source domain data (e.g. from synthetic generation) is\navailable, but the target domain data is completely unannotated. Our method\ntargets the semantic segmentation task and leverages a segmentation foundation\nmodel (Segment Anything Model) to obtain segment information on unannotated\ndata. We take inspiration from recent advances in unsupervised local feature\nlearning and propose an invariance-variance loss over the detected segments for\nregularizing feature representations in the target domain. Crucially, this loss\nstructure and network architecture can handle overlapping segments and\noversegmentation as produced by Segment Anything. We demonstrate the advantage\nof our method on the challenging YCB-Video and HomebrewedDB datasets and show\nthat it outperforms prior work and, on YCB-Video, even a network trained with\nreal annotations. Additionally, we provide insight through model ablations and\nshow applicability to a custom robotic application.\n", "link": "http://arxiv.org/abs/2309.15562v3", "date": "2024-05-10", "relevancy": 2.2948, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20SAM%3A%20Harnessing%20a%20Foundation%20Model%20for%20Sim2Real%20Adaptation%0A%20%20by%20Regularization&body=Title%3A%20Learning%20from%20SAM%3A%20Harnessing%20a%20Foundation%20Model%20for%20Sim2Real%20Adaptation%0A%20%20by%20Regularization%0AAuthor%3A%20Mayara%20E.%20Bonani%20and%20Max%20Schwarz%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Domain%20adaptation%20is%20especially%20important%20for%20robotics%20applications%2C%20where%0Atarget%20domain%20training%20data%20is%20usually%20scarce%20and%20annotations%20are%20costly%20to%0Aobtain.%20We%20present%20a%20method%20for%20self-supervised%20domain%20adaptation%20for%20the%0Ascenario%20where%20annotated%20source%20domain%20data%20%28e.g.%20from%20synthetic%20generation%29%20is%0Aavailable%2C%20but%20the%20target%20domain%20data%20is%20completely%20unannotated.%20Our%20method%0Atargets%20the%20semantic%20segmentation%20task%20and%20leverages%20a%20segmentation%20foundation%0Amodel%20%28Segment%20Anything%20Model%29%20to%20obtain%20segment%20information%20on%20unannotated%0Adata.%20We%20take%20inspiration%20from%20recent%20advances%20in%20unsupervised%20local%20feature%0Alearning%20and%20propose%20an%20invariance-variance%20loss%20over%20the%20detected%20segments%20for%0Aregularizing%20feature%20representations%20in%20the%20target%20domain.%20Crucially%2C%20this%20loss%0Astructure%20and%20network%20architecture%20can%20handle%20overlapping%20segments%20and%0Aoversegmentation%20as%20produced%20by%20Segment%20Anything.%20We%20demonstrate%20the%20advantage%0Aof%20our%20method%20on%20the%20challenging%20YCB-Video%20and%20HomebrewedDB%20datasets%20and%20show%0Athat%20it%20outperforms%20prior%20work%20and%2C%20on%20YCB-Video%2C%20even%20a%20network%20trained%20with%0Areal%20annotations.%20Additionally%2C%20we%20provide%20insight%20through%20model%20ablations%20and%0Ashow%20applicability%20to%20a%20custom%20robotic%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.15562v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520SAM%253A%2520Harnessing%2520a%2520Foundation%2520Model%2520for%2520Sim2Real%2520Adaptation%250A%2520%2520by%2520Regularization%26entry.906535625%3DMayara%2520E.%2520Bonani%2520and%2520Max%2520Schwarz%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520is%2520especially%2520important%2520for%2520robotics%2520applications%252C%2520where%250Atarget%2520domain%2520training%2520data%2520is%2520usually%2520scarce%2520and%2520annotations%2520are%2520costly%2520to%250Aobtain.%2520We%2520present%2520a%2520method%2520for%2520self-supervised%2520domain%2520adaptation%2520for%2520the%250Ascenario%2520where%2520annotated%2520source%2520domain%2520data%2520%2528e.g.%2520from%2520synthetic%2520generation%2529%2520is%250Aavailable%252C%2520but%2520the%2520target%2520domain%2520data%2520is%2520completely%2520unannotated.%2520Our%2520method%250Atargets%2520the%2520semantic%2520segmentation%2520task%2520and%2520leverages%2520a%2520segmentation%2520foundation%250Amodel%2520%2528Segment%2520Anything%2520Model%2529%2520to%2520obtain%2520segment%2520information%2520on%2520unannotated%250Adata.%2520We%2520take%2520inspiration%2520from%2520recent%2520advances%2520in%2520unsupervised%2520local%2520feature%250Alearning%2520and%2520propose%2520an%2520invariance-variance%2520loss%2520over%2520the%2520detected%2520segments%2520for%250Aregularizing%2520feature%2520representations%2520in%2520the%2520target%2520domain.%2520Crucially%252C%2520this%2520loss%250Astructure%2520and%2520network%2520architecture%2520can%2520handle%2520overlapping%2520segments%2520and%250Aoversegmentation%2520as%2520produced%2520by%2520Segment%2520Anything.%2520We%2520demonstrate%2520the%2520advantage%250Aof%2520our%2520method%2520on%2520the%2520challenging%2520YCB-Video%2520and%2520HomebrewedDB%2520datasets%2520and%2520show%250Athat%2520it%2520outperforms%2520prior%2520work%2520and%252C%2520on%2520YCB-Video%252C%2520even%2520a%2520network%2520trained%2520with%250Areal%2520annotations.%2520Additionally%252C%2520we%2520provide%2520insight%2520through%2520model%2520ablations%2520and%250Ashow%2520applicability%2520to%2520a%2520custom%2520robotic%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.15562v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20SAM%3A%20Harnessing%20a%20Foundation%20Model%20for%20Sim2Real%20Adaptation%0A%20%20by%20Regularization&entry.906535625=Mayara%20E.%20Bonani%20and%20Max%20Schwarz%20and%20Sven%20Behnke&entry.1292438233=%20%20Domain%20adaptation%20is%20especially%20important%20for%20robotics%20applications%2C%20where%0Atarget%20domain%20training%20data%20is%20usually%20scarce%20and%20annotations%20are%20costly%20to%0Aobtain.%20We%20present%20a%20method%20for%20self-supervised%20domain%20adaptation%20for%20the%0Ascenario%20where%20annotated%20source%20domain%20data%20%28e.g.%20from%20synthetic%20generation%29%20is%0Aavailable%2C%20but%20the%20target%20domain%20data%20is%20completely%20unannotated.%20Our%20method%0Atargets%20the%20semantic%20segmentation%20task%20and%20leverages%20a%20segmentation%20foundation%0Amodel%20%28Segment%20Anything%20Model%29%20to%20obtain%20segment%20information%20on%20unannotated%0Adata.%20We%20take%20inspiration%20from%20recent%20advances%20in%20unsupervised%20local%20feature%0Alearning%20and%20propose%20an%20invariance-variance%20loss%20over%20the%20detected%20segments%20for%0Aregularizing%20feature%20representations%20in%20the%20target%20domain.%20Crucially%2C%20this%20loss%0Astructure%20and%20network%20architecture%20can%20handle%20overlapping%20segments%20and%0Aoversegmentation%20as%20produced%20by%20Segment%20Anything.%20We%20demonstrate%20the%20advantage%0Aof%20our%20method%20on%20the%20challenging%20YCB-Video%20and%20HomebrewedDB%20datasets%20and%20show%0Athat%20it%20outperforms%20prior%20work%20and%2C%20on%20YCB-Video%2C%20even%20a%20network%20trained%20with%0Areal%20annotations.%20Additionally%2C%20we%20provide%20insight%20through%20model%20ablations%20and%0Ashow%20applicability%20to%20a%20custom%20robotic%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.15562v3&entry.124074799=Read"},
{"title": "Hierarchical Learned Risk-Aware Planning Framework for Human Driving\n  Modeling", "author": "Nathan Ludlow and Yiwei Lyu and John Dolan", "abstract": "  This paper presents a novel approach to modeling human driving behavior,\ndesigned for use in evaluating autonomous vehicle control systems in a\nsimulation environments. Our methodology leverages a hierarchical\nforward-looking, risk-aware estimation framework with learned parameters to\ngenerate human-like driving trajectories, accommodating multiple driver levels\ndetermined by model parameters. This approach is grounded in multimodal\ntrajectory prediction, using a deep neural network with LSTM-based social\npooling to predict the trajectories of surrounding vehicles. These trajectories\nare used to compute forward-looking risk assessments along the ego vehicle's\npath, guiding its navigation. Our method aims to replicate human driving\nbehaviors by learning parameters that emulate human decision-making during\ndriving. We ensure that our model exhibits robust generalization capabilities\nby conducting simulations, employing real-world driving data to validate the\naccuracy of our approach in modeling human behavior. The results reveal that\nour model effectively captures human behavior, showcasing its versatility in\nmodeling human drivers in diverse highway scenarios.\n", "link": "http://arxiv.org/abs/2405.06578v1", "date": "2024-05-10", "relevancy": 2.2672, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.595}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5696}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Learned%20Risk-Aware%20Planning%20Framework%20for%20Human%20Driving%0A%20%20Modeling&body=Title%3A%20Hierarchical%20Learned%20Risk-Aware%20Planning%20Framework%20for%20Human%20Driving%0A%20%20Modeling%0AAuthor%3A%20Nathan%20Ludlow%20and%20Yiwei%20Lyu%20and%20John%20Dolan%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20modeling%20human%20driving%20behavior%2C%0Adesigned%20for%20use%20in%20evaluating%20autonomous%20vehicle%20control%20systems%20in%20a%0Asimulation%20environments.%20Our%20methodology%20leverages%20a%20hierarchical%0Aforward-looking%2C%20risk-aware%20estimation%20framework%20with%20learned%20parameters%20to%0Agenerate%20human-like%20driving%20trajectories%2C%20accommodating%20multiple%20driver%20levels%0Adetermined%20by%20model%20parameters.%20This%20approach%20is%20grounded%20in%20multimodal%0Atrajectory%20prediction%2C%20using%20a%20deep%20neural%20network%20with%20LSTM-based%20social%0Apooling%20to%20predict%20the%20trajectories%20of%20surrounding%20vehicles.%20These%20trajectories%0Aare%20used%20to%20compute%20forward-looking%20risk%20assessments%20along%20the%20ego%20vehicle%27s%0Apath%2C%20guiding%20its%20navigation.%20Our%20method%20aims%20to%20replicate%20human%20driving%0Abehaviors%20by%20learning%20parameters%20that%20emulate%20human%20decision-making%20during%0Adriving.%20We%20ensure%20that%20our%20model%20exhibits%20robust%20generalization%20capabilities%0Aby%20conducting%20simulations%2C%20employing%20real-world%20driving%20data%20to%20validate%20the%0Aaccuracy%20of%20our%20approach%20in%20modeling%20human%20behavior.%20The%20results%20reveal%20that%0Aour%20model%20effectively%20captures%20human%20behavior%2C%20showcasing%20its%20versatility%20in%0Amodeling%20human%20drivers%20in%20diverse%20highway%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Learned%2520Risk-Aware%2520Planning%2520Framework%2520for%2520Human%2520Driving%250A%2520%2520Modeling%26entry.906535625%3DNathan%2520Ludlow%2520and%2520Yiwei%2520Lyu%2520and%2520John%2520Dolan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520modeling%2520human%2520driving%2520behavior%252C%250Adesigned%2520for%2520use%2520in%2520evaluating%2520autonomous%2520vehicle%2520control%2520systems%2520in%2520a%250Asimulation%2520environments.%2520Our%2520methodology%2520leverages%2520a%2520hierarchical%250Aforward-looking%252C%2520risk-aware%2520estimation%2520framework%2520with%2520learned%2520parameters%2520to%250Agenerate%2520human-like%2520driving%2520trajectories%252C%2520accommodating%2520multiple%2520driver%2520levels%250Adetermined%2520by%2520model%2520parameters.%2520This%2520approach%2520is%2520grounded%2520in%2520multimodal%250Atrajectory%2520prediction%252C%2520using%2520a%2520deep%2520neural%2520network%2520with%2520LSTM-based%2520social%250Apooling%2520to%2520predict%2520the%2520trajectories%2520of%2520surrounding%2520vehicles.%2520These%2520trajectories%250Aare%2520used%2520to%2520compute%2520forward-looking%2520risk%2520assessments%2520along%2520the%2520ego%2520vehicle%2527s%250Apath%252C%2520guiding%2520its%2520navigation.%2520Our%2520method%2520aims%2520to%2520replicate%2520human%2520driving%250Abehaviors%2520by%2520learning%2520parameters%2520that%2520emulate%2520human%2520decision-making%2520during%250Adriving.%2520We%2520ensure%2520that%2520our%2520model%2520exhibits%2520robust%2520generalization%2520capabilities%250Aby%2520conducting%2520simulations%252C%2520employing%2520real-world%2520driving%2520data%2520to%2520validate%2520the%250Aaccuracy%2520of%2520our%2520approach%2520in%2520modeling%2520human%2520behavior.%2520The%2520results%2520reveal%2520that%250Aour%2520model%2520effectively%2520captures%2520human%2520behavior%252C%2520showcasing%2520its%2520versatility%2520in%250Amodeling%2520human%2520drivers%2520in%2520diverse%2520highway%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Learned%20Risk-Aware%20Planning%20Framework%20for%20Human%20Driving%0A%20%20Modeling&entry.906535625=Nathan%20Ludlow%20and%20Yiwei%20Lyu%20and%20John%20Dolan&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20modeling%20human%20driving%20behavior%2C%0Adesigned%20for%20use%20in%20evaluating%20autonomous%20vehicle%20control%20systems%20in%20a%0Asimulation%20environments.%20Our%20methodology%20leverages%20a%20hierarchical%0Aforward-looking%2C%20risk-aware%20estimation%20framework%20with%20learned%20parameters%20to%0Agenerate%20human-like%20driving%20trajectories%2C%20accommodating%20multiple%20driver%20levels%0Adetermined%20by%20model%20parameters.%20This%20approach%20is%20grounded%20in%20multimodal%0Atrajectory%20prediction%2C%20using%20a%20deep%20neural%20network%20with%20LSTM-based%20social%0Apooling%20to%20predict%20the%20trajectories%20of%20surrounding%20vehicles.%20These%20trajectories%0Aare%20used%20to%20compute%20forward-looking%20risk%20assessments%20along%20the%20ego%20vehicle%27s%0Apath%2C%20guiding%20its%20navigation.%20Our%20method%20aims%20to%20replicate%20human%20driving%0Abehaviors%20by%20learning%20parameters%20that%20emulate%20human%20decision-making%20during%0Adriving.%20We%20ensure%20that%20our%20model%20exhibits%20robust%20generalization%20capabilities%0Aby%20conducting%20simulations%2C%20employing%20real-world%20driving%20data%20to%20validate%20the%0Aaccuracy%20of%20our%20approach%20in%20modeling%20human%20behavior.%20The%20results%20reveal%20that%0Aour%20model%20effectively%20captures%20human%20behavior%2C%20showcasing%20its%20versatility%20in%0Amodeling%20human%20drivers%20in%20diverse%20highway%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06578v1&entry.124074799=Read"},
{"title": "Multi-Target Unsupervised Domain Adaptation for Semantic Segmentation\n  without External Data", "author": "Yonghao Xu and Pedram Ghamisi and Yannis Avrithis", "abstract": "  Multi-target unsupervised domain adaptation (UDA) aims to learn a unified\nmodel to address the domain shift between multiple target domains. Due to the\ndifficulty of obtaining annotations for dense predictions, it has recently been\nintroduced into cross-domain semantic segmentation. However, most existing\nsolutions require labeled data from the source domain and unlabeled data from\nmultiple target domains concurrently during training. Collectively, we refer to\nthis data as \"external\". When faced with new unlabeled data from an unseen\ntarget domain, these solutions either do not generalize well or require\nretraining from scratch on all data. To address these challenges, we introduce\na new strategy called \"multi-target UDA without external data\" for semantic\nsegmentation. Specifically, the segmentation model is initially trained on the\nexternal data. Then, it is adapted to a new unseen target domain without\naccessing any external data. This approach is thus more scalable than existing\nsolutions and remains applicable when external data is inaccessible. We\ndemonstrate this strategy using a simple method that incorporates\nself-distillation and adversarial learning, where knowledge acquired from the\nexternal data is preserved during adaptation through \"one-way\" adversarial\nlearning. Extensive experiments in several synthetic-to-real and real-to-real\nadaptation settings on four benchmark urban driving datasets show that our\nmethod significantly outperforms current state-of-the-art solutions, even in\nthe absence of external data. Our source code is available online\n(https://github.com/YonghaoXu/UT-KD).\n", "link": "http://arxiv.org/abs/2405.06502v1", "date": "2024-05-10", "relevancy": 2.2643, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5735}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Target%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%20%20without%20External%20Data&body=Title%3A%20Multi-Target%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%20%20without%20External%20Data%0AAuthor%3A%20Yonghao%20Xu%20and%20Pedram%20Ghamisi%20and%20Yannis%20Avrithis%0AAbstract%3A%20%20%20Multi-target%20unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20learn%20a%20unified%0Amodel%20to%20address%20the%20domain%20shift%20between%20multiple%20target%20domains.%20Due%20to%20the%0Adifficulty%20of%20obtaining%20annotations%20for%20dense%20predictions%2C%20it%20has%20recently%20been%0Aintroduced%20into%20cross-domain%20semantic%20segmentation.%20However%2C%20most%20existing%0Asolutions%20require%20labeled%20data%20from%20the%20source%20domain%20and%20unlabeled%20data%20from%0Amultiple%20target%20domains%20concurrently%20during%20training.%20Collectively%2C%20we%20refer%20to%0Athis%20data%20as%20%22external%22.%20When%20faced%20with%20new%20unlabeled%20data%20from%20an%20unseen%0Atarget%20domain%2C%20these%20solutions%20either%20do%20not%20generalize%20well%20or%20require%0Aretraining%20from%20scratch%20on%20all%20data.%20To%20address%20these%20challenges%2C%20we%20introduce%0Aa%20new%20strategy%20called%20%22multi-target%20UDA%20without%20external%20data%22%20for%20semantic%0Asegmentation.%20Specifically%2C%20the%20segmentation%20model%20is%20initially%20trained%20on%20the%0Aexternal%20data.%20Then%2C%20it%20is%20adapted%20to%20a%20new%20unseen%20target%20domain%20without%0Aaccessing%20any%20external%20data.%20This%20approach%20is%20thus%20more%20scalable%20than%20existing%0Asolutions%20and%20remains%20applicable%20when%20external%20data%20is%20inaccessible.%20We%0Ademonstrate%20this%20strategy%20using%20a%20simple%20method%20that%20incorporates%0Aself-distillation%20and%20adversarial%20learning%2C%20where%20knowledge%20acquired%20from%20the%0Aexternal%20data%20is%20preserved%20during%20adaptation%20through%20%22one-way%22%20adversarial%0Alearning.%20Extensive%20experiments%20in%20several%20synthetic-to-real%20and%20real-to-real%0Aadaptation%20settings%20on%20four%20benchmark%20urban%20driving%20datasets%20show%20that%20our%0Amethod%20significantly%20outperforms%20current%20state-of-the-art%20solutions%2C%20even%20in%0Athe%20absence%20of%20external%20data.%20Our%20source%20code%20is%20available%20online%0A%28https%3A//github.com/YonghaoXu/UT-KD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Target%2520Unsupervised%2520Domain%2520Adaptation%2520for%2520Semantic%2520Segmentation%250A%2520%2520without%2520External%2520Data%26entry.906535625%3DYonghao%2520Xu%2520and%2520Pedram%2520Ghamisi%2520and%2520Yannis%2520Avrithis%26entry.1292438233%3D%2520%2520Multi-target%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520aims%2520to%2520learn%2520a%2520unified%250Amodel%2520to%2520address%2520the%2520domain%2520shift%2520between%2520multiple%2520target%2520domains.%2520Due%2520to%2520the%250Adifficulty%2520of%2520obtaining%2520annotations%2520for%2520dense%2520predictions%252C%2520it%2520has%2520recently%2520been%250Aintroduced%2520into%2520cross-domain%2520semantic%2520segmentation.%2520However%252C%2520most%2520existing%250Asolutions%2520require%2520labeled%2520data%2520from%2520the%2520source%2520domain%2520and%2520unlabeled%2520data%2520from%250Amultiple%2520target%2520domains%2520concurrently%2520during%2520training.%2520Collectively%252C%2520we%2520refer%2520to%250Athis%2520data%2520as%2520%2522external%2522.%2520When%2520faced%2520with%2520new%2520unlabeled%2520data%2520from%2520an%2520unseen%250Atarget%2520domain%252C%2520these%2520solutions%2520either%2520do%2520not%2520generalize%2520well%2520or%2520require%250Aretraining%2520from%2520scratch%2520on%2520all%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250Aa%2520new%2520strategy%2520called%2520%2522multi-target%2520UDA%2520without%2520external%2520data%2522%2520for%2520semantic%250Asegmentation.%2520Specifically%252C%2520the%2520segmentation%2520model%2520is%2520initially%2520trained%2520on%2520the%250Aexternal%2520data.%2520Then%252C%2520it%2520is%2520adapted%2520to%2520a%2520new%2520unseen%2520target%2520domain%2520without%250Aaccessing%2520any%2520external%2520data.%2520This%2520approach%2520is%2520thus%2520more%2520scalable%2520than%2520existing%250Asolutions%2520and%2520remains%2520applicable%2520when%2520external%2520data%2520is%2520inaccessible.%2520We%250Ademonstrate%2520this%2520strategy%2520using%2520a%2520simple%2520method%2520that%2520incorporates%250Aself-distillation%2520and%2520adversarial%2520learning%252C%2520where%2520knowledge%2520acquired%2520from%2520the%250Aexternal%2520data%2520is%2520preserved%2520during%2520adaptation%2520through%2520%2522one-way%2522%2520adversarial%250Alearning.%2520Extensive%2520experiments%2520in%2520several%2520synthetic-to-real%2520and%2520real-to-real%250Aadaptation%2520settings%2520on%2520four%2520benchmark%2520urban%2520driving%2520datasets%2520show%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520current%2520state-of-the-art%2520solutions%252C%2520even%2520in%250Athe%2520absence%2520of%2520external%2520data.%2520Our%2520source%2520code%2520is%2520available%2520online%250A%2528https%253A//github.com/YonghaoXu/UT-KD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Target%20Unsupervised%20Domain%20Adaptation%20for%20Semantic%20Segmentation%0A%20%20without%20External%20Data&entry.906535625=Yonghao%20Xu%20and%20Pedram%20Ghamisi%20and%20Yannis%20Avrithis&entry.1292438233=%20%20Multi-target%20unsupervised%20domain%20adaptation%20%28UDA%29%20aims%20to%20learn%20a%20unified%0Amodel%20to%20address%20the%20domain%20shift%20between%20multiple%20target%20domains.%20Due%20to%20the%0Adifficulty%20of%20obtaining%20annotations%20for%20dense%20predictions%2C%20it%20has%20recently%20been%0Aintroduced%20into%20cross-domain%20semantic%20segmentation.%20However%2C%20most%20existing%0Asolutions%20require%20labeled%20data%20from%20the%20source%20domain%20and%20unlabeled%20data%20from%0Amultiple%20target%20domains%20concurrently%20during%20training.%20Collectively%2C%20we%20refer%20to%0Athis%20data%20as%20%22external%22.%20When%20faced%20with%20new%20unlabeled%20data%20from%20an%20unseen%0Atarget%20domain%2C%20these%20solutions%20either%20do%20not%20generalize%20well%20or%20require%0Aretraining%20from%20scratch%20on%20all%20data.%20To%20address%20these%20challenges%2C%20we%20introduce%0Aa%20new%20strategy%20called%20%22multi-target%20UDA%20without%20external%20data%22%20for%20semantic%0Asegmentation.%20Specifically%2C%20the%20segmentation%20model%20is%20initially%20trained%20on%20the%0Aexternal%20data.%20Then%2C%20it%20is%20adapted%20to%20a%20new%20unseen%20target%20domain%20without%0Aaccessing%20any%20external%20data.%20This%20approach%20is%20thus%20more%20scalable%20than%20existing%0Asolutions%20and%20remains%20applicable%20when%20external%20data%20is%20inaccessible.%20We%0Ademonstrate%20this%20strategy%20using%20a%20simple%20method%20that%20incorporates%0Aself-distillation%20and%20adversarial%20learning%2C%20where%20knowledge%20acquired%20from%20the%0Aexternal%20data%20is%20preserved%20during%20adaptation%20through%20%22one-way%22%20adversarial%0Alearning.%20Extensive%20experiments%20in%20several%20synthetic-to-real%20and%20real-to-real%0Aadaptation%20settings%20on%20four%20benchmark%20urban%20driving%20datasets%20show%20that%20our%0Amethod%20significantly%20outperforms%20current%20state-of-the-art%20solutions%2C%20even%20in%0Athe%20absence%20of%20external%20data.%20Our%20source%20code%20is%20available%20online%0A%28https%3A//github.com/YonghaoXu/UT-KD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06502v1&entry.124074799=Read"},
{"title": "Semantic and Spatial Adaptive Pixel-level Classifier for Semantic\n  Segmentation", "author": "Xiaowen Ma and Zhenliang Ni and Xinghao Chen", "abstract": "  Vanilla pixel-level classifiers for semantic segmentation are based on a\ncertain paradigm, involving the inner product of fixed prototypes obtained from\nthe training set and pixel features in the test image. This approach, however,\nencounters significant limitations, i.e., feature deviation in the semantic\ndomain and information loss in the spatial domain. The former struggles with\nlarge intra-class variance among pixel features from different images, while\nthe latter fails to utilize the structured information of semantic objects\neffectively. This leads to blurred mask boundaries as well as a deficiency of\nfine-grained recognition capability. In this paper, we propose a novel Semantic\nand Spatial Adaptive (SSA) classifier to address the above challenges.\nSpecifically, we employ the coarse masks obtained from the fixed prototypes as\na guide to adjust the fixed prototype towards the center of the semantic and\nspatial domains in the test image. The adapted prototypes in semantic and\nspatial domains are then simultaneously considered to accomplish classification\ndecisions. In addition, we propose an online multi-domain distillation learning\nstrategy to improve the adaption process. Experimental results on three\npublicly available benchmarks show that the proposed SSA significantly improves\nthe segmentation performance of the baseline models with only a minimal\nincrease in computational cost. Code is available at\nhttps://github.com/xwmaxwma/SSA.\n", "link": "http://arxiv.org/abs/2405.06525v1", "date": "2024-05-10", "relevancy": 2.2545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20and%20Spatial%20Adaptive%20Pixel-level%20Classifier%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20Semantic%20and%20Spatial%20Adaptive%20Pixel-level%20Classifier%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Xiaowen%20Ma%20and%20Zhenliang%20Ni%20and%20Xinghao%20Chen%0AAbstract%3A%20%20%20Vanilla%20pixel-level%20classifiers%20for%20semantic%20segmentation%20are%20based%20on%20a%0Acertain%20paradigm%2C%20involving%20the%20inner%20product%20of%20fixed%20prototypes%20obtained%20from%0Athe%20training%20set%20and%20pixel%20features%20in%20the%20test%20image.%20This%20approach%2C%20however%2C%0Aencounters%20significant%20limitations%2C%20i.e.%2C%20feature%20deviation%20in%20the%20semantic%0Adomain%20and%20information%20loss%20in%20the%20spatial%20domain.%20The%20former%20struggles%20with%0Alarge%20intra-class%20variance%20among%20pixel%20features%20from%20different%20images%2C%20while%0Athe%20latter%20fails%20to%20utilize%20the%20structured%20information%20of%20semantic%20objects%0Aeffectively.%20This%20leads%20to%20blurred%20mask%20boundaries%20as%20well%20as%20a%20deficiency%20of%0Afine-grained%20recognition%20capability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Semantic%0Aand%20Spatial%20Adaptive%20%28SSA%29%20classifier%20to%20address%20the%20above%20challenges.%0ASpecifically%2C%20we%20employ%20the%20coarse%20masks%20obtained%20from%20the%20fixed%20prototypes%20as%0Aa%20guide%20to%20adjust%20the%20fixed%20prototype%20towards%20the%20center%20of%20the%20semantic%20and%0Aspatial%20domains%20in%20the%20test%20image.%20The%20adapted%20prototypes%20in%20semantic%20and%0Aspatial%20domains%20are%20then%20simultaneously%20considered%20to%20accomplish%20classification%0Adecisions.%20In%20addition%2C%20we%20propose%20an%20online%20multi-domain%20distillation%20learning%0Astrategy%20to%20improve%20the%20adaption%20process.%20Experimental%20results%20on%20three%0Apublicly%20available%20benchmarks%20show%20that%20the%20proposed%20SSA%20significantly%20improves%0Athe%20segmentation%20performance%20of%20the%20baseline%20models%20with%20only%20a%20minimal%0Aincrease%20in%20computational%20cost.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xwmaxwma/SSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520and%2520Spatial%2520Adaptive%2520Pixel-level%2520Classifier%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Zhenliang%2520Ni%2520and%2520Xinghao%2520Chen%26entry.1292438233%3D%2520%2520Vanilla%2520pixel-level%2520classifiers%2520for%2520semantic%2520segmentation%2520are%2520based%2520on%2520a%250Acertain%2520paradigm%252C%2520involving%2520the%2520inner%2520product%2520of%2520fixed%2520prototypes%2520obtained%2520from%250Athe%2520training%2520set%2520and%2520pixel%2520features%2520in%2520the%2520test%2520image.%2520This%2520approach%252C%2520however%252C%250Aencounters%2520significant%2520limitations%252C%2520i.e.%252C%2520feature%2520deviation%2520in%2520the%2520semantic%250Adomain%2520and%2520information%2520loss%2520in%2520the%2520spatial%2520domain.%2520The%2520former%2520struggles%2520with%250Alarge%2520intra-class%2520variance%2520among%2520pixel%2520features%2520from%2520different%2520images%252C%2520while%250Athe%2520latter%2520fails%2520to%2520utilize%2520the%2520structured%2520information%2520of%2520semantic%2520objects%250Aeffectively.%2520This%2520leads%2520to%2520blurred%2520mask%2520boundaries%2520as%2520well%2520as%2520a%2520deficiency%2520of%250Afine-grained%2520recognition%2520capability.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Semantic%250Aand%2520Spatial%2520Adaptive%2520%2528SSA%2529%2520classifier%2520to%2520address%2520the%2520above%2520challenges.%250ASpecifically%252C%2520we%2520employ%2520the%2520coarse%2520masks%2520obtained%2520from%2520the%2520fixed%2520prototypes%2520as%250Aa%2520guide%2520to%2520adjust%2520the%2520fixed%2520prototype%2520towards%2520the%2520center%2520of%2520the%2520semantic%2520and%250Aspatial%2520domains%2520in%2520the%2520test%2520image.%2520The%2520adapted%2520prototypes%2520in%2520semantic%2520and%250Aspatial%2520domains%2520are%2520then%2520simultaneously%2520considered%2520to%2520accomplish%2520classification%250Adecisions.%2520In%2520addition%252C%2520we%2520propose%2520an%2520online%2520multi-domain%2520distillation%2520learning%250Astrategy%2520to%2520improve%2520the%2520adaption%2520process.%2520Experimental%2520results%2520on%2520three%250Apublicly%2520available%2520benchmarks%2520show%2520that%2520the%2520proposed%2520SSA%2520significantly%2520improves%250Athe%2520segmentation%2520performance%2520of%2520the%2520baseline%2520models%2520with%2520only%2520a%2520minimal%250Aincrease%2520in%2520computational%2520cost.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/xwmaxwma/SSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20and%20Spatial%20Adaptive%20Pixel-level%20Classifier%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Xiaowen%20Ma%20and%20Zhenliang%20Ni%20and%20Xinghao%20Chen&entry.1292438233=%20%20Vanilla%20pixel-level%20classifiers%20for%20semantic%20segmentation%20are%20based%20on%20a%0Acertain%20paradigm%2C%20involving%20the%20inner%20product%20of%20fixed%20prototypes%20obtained%20from%0Athe%20training%20set%20and%20pixel%20features%20in%20the%20test%20image.%20This%20approach%2C%20however%2C%0Aencounters%20significant%20limitations%2C%20i.e.%2C%20feature%20deviation%20in%20the%20semantic%0Adomain%20and%20information%20loss%20in%20the%20spatial%20domain.%20The%20former%20struggles%20with%0Alarge%20intra-class%20variance%20among%20pixel%20features%20from%20different%20images%2C%20while%0Athe%20latter%20fails%20to%20utilize%20the%20structured%20information%20of%20semantic%20objects%0Aeffectively.%20This%20leads%20to%20blurred%20mask%20boundaries%20as%20well%20as%20a%20deficiency%20of%0Afine-grained%20recognition%20capability.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Semantic%0Aand%20Spatial%20Adaptive%20%28SSA%29%20classifier%20to%20address%20the%20above%20challenges.%0ASpecifically%2C%20we%20employ%20the%20coarse%20masks%20obtained%20from%20the%20fixed%20prototypes%20as%0Aa%20guide%20to%20adjust%20the%20fixed%20prototype%20towards%20the%20center%20of%20the%20semantic%20and%0Aspatial%20domains%20in%20the%20test%20image.%20The%20adapted%20prototypes%20in%20semantic%20and%0Aspatial%20domains%20are%20then%20simultaneously%20considered%20to%20accomplish%20classification%0Adecisions.%20In%20addition%2C%20we%20propose%20an%20online%20multi-domain%20distillation%20learning%0Astrategy%20to%20improve%20the%20adaption%20process.%20Experimental%20results%20on%20three%0Apublicly%20available%20benchmarks%20show%20that%20the%20proposed%20SSA%20significantly%20improves%0Athe%20segmentation%20performance%20of%20the%20baseline%20models%20with%20only%20a%20minimal%0Aincrease%20in%20computational%20cost.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xwmaxwma/SSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06525v1&entry.124074799=Read"},
{"title": "Efficient Federated Low Rank Matrix Completion", "author": "Ahmed Ali Abbasi and Namrata Vaswani", "abstract": "  In this work, we develop and analyze a Gradient Descent (GD) based solution,\ncalled Alternating GD and Minimization (AltGDmin), for efficiently solving the\nlow rank matrix completion (LRMC) in a federated setting. LRMC involves\nrecovering an $n \\times q$ rank-$r$ matrix $\\Xstar$ from a subset of its\nentries when $r \\ll \\min(n,q)$. Our theoretical guarantees (iteration and\nsample complexity bounds) imply that AltGDmin is the most\ncommunication-efficient solution in a federated setting, is one of the fastest,\nand has the second best sample complexity among all iterative solutions to\nLRMC. In addition, we also prove two important corollaries. (a) We provide a\nguarantee for AltGDmin for solving the noisy LRMC problem. (b) We show how our\nlemmas can be used to provide an improved sample complexity guarantee for\nAltMin, which is the fastest centralized solution.\n", "link": "http://arxiv.org/abs/2405.06569v1", "date": "2024-05-10", "relevancy": 2.2503, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Federated%20Low%20Rank%20Matrix%20Completion&body=Title%3A%20Efficient%20Federated%20Low%20Rank%20Matrix%20Completion%0AAuthor%3A%20Ahmed%20Ali%20Abbasi%20and%20Namrata%20Vaswani%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20develop%20and%20analyze%20a%20Gradient%20Descent%20%28GD%29%20based%20solution%2C%0Acalled%20Alternating%20GD%20and%20Minimization%20%28AltGDmin%29%2C%20for%20efficiently%20solving%20the%0Alow%20rank%20matrix%20completion%20%28LRMC%29%20in%20a%20federated%20setting.%20LRMC%20involves%0Arecovering%20an%20%24n%20%5Ctimes%20q%24%20rank-%24r%24%20matrix%20%24%5CXstar%24%20from%20a%20subset%20of%20its%0Aentries%20when%20%24r%20%5Cll%20%5Cmin%28n%2Cq%29%24.%20Our%20theoretical%20guarantees%20%28iteration%20and%0Asample%20complexity%20bounds%29%20imply%20that%20AltGDmin%20is%20the%20most%0Acommunication-efficient%20solution%20in%20a%20federated%20setting%2C%20is%20one%20of%20the%20fastest%2C%0Aand%20has%20the%20second%20best%20sample%20complexity%20among%20all%20iterative%20solutions%20to%0ALRMC.%20In%20addition%2C%20we%20also%20prove%20two%20important%20corollaries.%20%28a%29%20We%20provide%20a%0Aguarantee%20for%20AltGDmin%20for%20solving%20the%20noisy%20LRMC%20problem.%20%28b%29%20We%20show%20how%20our%0Alemmas%20can%20be%20used%20to%20provide%20an%20improved%20sample%20complexity%20guarantee%20for%0AAltMin%2C%20which%20is%20the%20fastest%20centralized%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Federated%2520Low%2520Rank%2520Matrix%2520Completion%26entry.906535625%3DAhmed%2520Ali%2520Abbasi%2520and%2520Namrata%2520Vaswani%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520develop%2520and%2520analyze%2520a%2520Gradient%2520Descent%2520%2528GD%2529%2520based%2520solution%252C%250Acalled%2520Alternating%2520GD%2520and%2520Minimization%2520%2528AltGDmin%2529%252C%2520for%2520efficiently%2520solving%2520the%250Alow%2520rank%2520matrix%2520completion%2520%2528LRMC%2529%2520in%2520a%2520federated%2520setting.%2520LRMC%2520involves%250Arecovering%2520an%2520%2524n%2520%255Ctimes%2520q%2524%2520rank-%2524r%2524%2520matrix%2520%2524%255CXstar%2524%2520from%2520a%2520subset%2520of%2520its%250Aentries%2520when%2520%2524r%2520%255Cll%2520%255Cmin%2528n%252Cq%2529%2524.%2520Our%2520theoretical%2520guarantees%2520%2528iteration%2520and%250Asample%2520complexity%2520bounds%2529%2520imply%2520that%2520AltGDmin%2520is%2520the%2520most%250Acommunication-efficient%2520solution%2520in%2520a%2520federated%2520setting%252C%2520is%2520one%2520of%2520the%2520fastest%252C%250Aand%2520has%2520the%2520second%2520best%2520sample%2520complexity%2520among%2520all%2520iterative%2520solutions%2520to%250ALRMC.%2520In%2520addition%252C%2520we%2520also%2520prove%2520two%2520important%2520corollaries.%2520%2528a%2529%2520We%2520provide%2520a%250Aguarantee%2520for%2520AltGDmin%2520for%2520solving%2520the%2520noisy%2520LRMC%2520problem.%2520%2528b%2529%2520We%2520show%2520how%2520our%250Alemmas%2520can%2520be%2520used%2520to%2520provide%2520an%2520improved%2520sample%2520complexity%2520guarantee%2520for%250AAltMin%252C%2520which%2520is%2520the%2520fastest%2520centralized%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Federated%20Low%20Rank%20Matrix%20Completion&entry.906535625=Ahmed%20Ali%20Abbasi%20and%20Namrata%20Vaswani&entry.1292438233=%20%20In%20this%20work%2C%20we%20develop%20and%20analyze%20a%20Gradient%20Descent%20%28GD%29%20based%20solution%2C%0Acalled%20Alternating%20GD%20and%20Minimization%20%28AltGDmin%29%2C%20for%20efficiently%20solving%20the%0Alow%20rank%20matrix%20completion%20%28LRMC%29%20in%20a%20federated%20setting.%20LRMC%20involves%0Arecovering%20an%20%24n%20%5Ctimes%20q%24%20rank-%24r%24%20matrix%20%24%5CXstar%24%20from%20a%20subset%20of%20its%0Aentries%20when%20%24r%20%5Cll%20%5Cmin%28n%2Cq%29%24.%20Our%20theoretical%20guarantees%20%28iteration%20and%0Asample%20complexity%20bounds%29%20imply%20that%20AltGDmin%20is%20the%20most%0Acommunication-efficient%20solution%20in%20a%20federated%20setting%2C%20is%20one%20of%20the%20fastest%2C%0Aand%20has%20the%20second%20best%20sample%20complexity%20among%20all%20iterative%20solutions%20to%0ALRMC.%20In%20addition%2C%20we%20also%20prove%20two%20important%20corollaries.%20%28a%29%20We%20provide%20a%0Aguarantee%20for%20AltGDmin%20for%20solving%20the%20noisy%20LRMC%20problem.%20%28b%29%20We%20show%20how%20our%0Alemmas%20can%20be%20used%20to%20provide%20an%20improved%20sample%20complexity%20guarantee%20for%0AAltMin%2C%20which%20is%20the%20fastest%20centralized%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06569v1&entry.124074799=Read"},
{"title": "Enhancing Weakly Supervised Semantic Segmentation with Multi-modal\n  Foundation Models: An End-to-End Approach", "author": "Elham Ravanbakhsh and Cheng Niu and Yongqing Liang and J. Ramanujam and Xin Li", "abstract": "  Semantic segmentation is a core computer vision problem, but the high costs\nof data annotation have hindered its wide application. Weakly-Supervised\nSemantic Segmentation (WSSS) offers a cost-efficient workaround to extensive\nlabeling in comparison to fully-supervised methods by using partial or\nincomplete labels. Existing WSSS methods have difficulties in learning the\nboundaries of objects leading to poor segmentation results. We propose a novel\nand effective framework that addresses these issues by leveraging visual\nfoundation models inside the bounding box. Adopting a two-stage WSSS framework,\nour proposed network consists of a pseudo-label generation module and a\nsegmentation module. The first stage leverages Segment Anything Model (SAM) to\ngenerate high-quality pseudo-labels. To alleviate the problem of delineating\nprecise boundaries, we adopt SAM inside the bounding box with the help of\nanother pre-trained foundation model (e.g., Grounding-DINO). Furthermore, we\neliminate the necessity of using the supervision of image labels, by employing\nCLIP in classification. Then in the second stage, the generated high-quality\npseudo-labels are used to train an off-the-shelf segmenter that achieves the\nstate-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014.\n", "link": "http://arxiv.org/abs/2405.06586v1", "date": "2024-05-10", "relevancy": 2.243, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5639}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation%20with%20Multi-modal%0A%20%20Foundation%20Models%3A%20An%20End-to-End%20Approach&body=Title%3A%20Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation%20with%20Multi-modal%0A%20%20Foundation%20Models%3A%20An%20End-to-End%20Approach%0AAuthor%3A%20Elham%20Ravanbakhsh%20and%20Cheng%20Niu%20and%20Yongqing%20Liang%20and%20J.%20Ramanujam%20and%20Xin%20Li%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20a%20core%20computer%20vision%20problem%2C%20but%20the%20high%20costs%0Aof%20data%20annotation%20have%20hindered%20its%20wide%20application.%20Weakly-Supervised%0ASemantic%20Segmentation%20%28WSSS%29%20offers%20a%20cost-efficient%20workaround%20to%20extensive%0Alabeling%20in%20comparison%20to%20fully-supervised%20methods%20by%20using%20partial%20or%0Aincomplete%20labels.%20Existing%20WSSS%20methods%20have%20difficulties%20in%20learning%20the%0Aboundaries%20of%20objects%20leading%20to%20poor%20segmentation%20results.%20We%20propose%20a%20novel%0Aand%20effective%20framework%20that%20addresses%20these%20issues%20by%20leveraging%20visual%0Afoundation%20models%20inside%20the%20bounding%20box.%20Adopting%20a%20two-stage%20WSSS%20framework%2C%0Aour%20proposed%20network%20consists%20of%20a%20pseudo-label%20generation%20module%20and%20a%0Asegmentation%20module.%20The%20first%20stage%20leverages%20Segment%20Anything%20Model%20%28SAM%29%20to%0Agenerate%20high-quality%20pseudo-labels.%20To%20alleviate%20the%20problem%20of%20delineating%0Aprecise%20boundaries%2C%20we%20adopt%20SAM%20inside%20the%20bounding%20box%20with%20the%20help%20of%0Aanother%20pre-trained%20foundation%20model%20%28e.g.%2C%20Grounding-DINO%29.%20Furthermore%2C%20we%0Aeliminate%20the%20necessity%20of%20using%20the%20supervision%20of%20image%20labels%2C%20by%20employing%0ACLIP%20in%20classification.%20Then%20in%20the%20second%20stage%2C%20the%20generated%20high-quality%0Apseudo-labels%20are%20used%20to%20train%20an%20off-the-shelf%20segmenter%20that%20achieves%20the%0Astate-of-the-art%20performance%20on%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%2520with%2520Multi-modal%250A%2520%2520Foundation%2520Models%253A%2520An%2520End-to-End%2520Approach%26entry.906535625%3DElham%2520Ravanbakhsh%2520and%2520Cheng%2520Niu%2520and%2520Yongqing%2520Liang%2520and%2520J.%2520Ramanujam%2520and%2520Xin%2520Li%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520a%2520core%2520computer%2520vision%2520problem%252C%2520but%2520the%2520high%2520costs%250Aof%2520data%2520annotation%2520have%2520hindered%2520its%2520wide%2520application.%2520Weakly-Supervised%250ASemantic%2520Segmentation%2520%2528WSSS%2529%2520offers%2520a%2520cost-efficient%2520workaround%2520to%2520extensive%250Alabeling%2520in%2520comparison%2520to%2520fully-supervised%2520methods%2520by%2520using%2520partial%2520or%250Aincomplete%2520labels.%2520Existing%2520WSSS%2520methods%2520have%2520difficulties%2520in%2520learning%2520the%250Aboundaries%2520of%2520objects%2520leading%2520to%2520poor%2520segmentation%2520results.%2520We%2520propose%2520a%2520novel%250Aand%2520effective%2520framework%2520that%2520addresses%2520these%2520issues%2520by%2520leveraging%2520visual%250Afoundation%2520models%2520inside%2520the%2520bounding%2520box.%2520Adopting%2520a%2520two-stage%2520WSSS%2520framework%252C%250Aour%2520proposed%2520network%2520consists%2520of%2520a%2520pseudo-label%2520generation%2520module%2520and%2520a%250Asegmentation%2520module.%2520The%2520first%2520stage%2520leverages%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%250Agenerate%2520high-quality%2520pseudo-labels.%2520To%2520alleviate%2520the%2520problem%2520of%2520delineating%250Aprecise%2520boundaries%252C%2520we%2520adopt%2520SAM%2520inside%2520the%2520bounding%2520box%2520with%2520the%2520help%2520of%250Aanother%2520pre-trained%2520foundation%2520model%2520%2528e.g.%252C%2520Grounding-DINO%2529.%2520Furthermore%252C%2520we%250Aeliminate%2520the%2520necessity%2520of%2520using%2520the%2520supervision%2520of%2520image%2520labels%252C%2520by%2520employing%250ACLIP%2520in%2520classification.%2520Then%2520in%2520the%2520second%2520stage%252C%2520the%2520generated%2520high-quality%250Apseudo-labels%2520are%2520used%2520to%2520train%2520an%2520off-the-shelf%2520segmenter%2520that%2520achieves%2520the%250Astate-of-the-art%2520performance%2520on%2520PASCAL%2520VOC%25202012%2520and%2520MS%2520COCO%25202014.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Weakly%20Supervised%20Semantic%20Segmentation%20with%20Multi-modal%0A%20%20Foundation%20Models%3A%20An%20End-to-End%20Approach&entry.906535625=Elham%20Ravanbakhsh%20and%20Cheng%20Niu%20and%20Yongqing%20Liang%20and%20J.%20Ramanujam%20and%20Xin%20Li&entry.1292438233=%20%20Semantic%20segmentation%20is%20a%20core%20computer%20vision%20problem%2C%20but%20the%20high%20costs%0Aof%20data%20annotation%20have%20hindered%20its%20wide%20application.%20Weakly-Supervised%0ASemantic%20Segmentation%20%28WSSS%29%20offers%20a%20cost-efficient%20workaround%20to%20extensive%0Alabeling%20in%20comparison%20to%20fully-supervised%20methods%20by%20using%20partial%20or%0Aincomplete%20labels.%20Existing%20WSSS%20methods%20have%20difficulties%20in%20learning%20the%0Aboundaries%20of%20objects%20leading%20to%20poor%20segmentation%20results.%20We%20propose%20a%20novel%0Aand%20effective%20framework%20that%20addresses%20these%20issues%20by%20leveraging%20visual%0Afoundation%20models%20inside%20the%20bounding%20box.%20Adopting%20a%20two-stage%20WSSS%20framework%2C%0Aour%20proposed%20network%20consists%20of%20a%20pseudo-label%20generation%20module%20and%20a%0Asegmentation%20module.%20The%20first%20stage%20leverages%20Segment%20Anything%20Model%20%28SAM%29%20to%0Agenerate%20high-quality%20pseudo-labels.%20To%20alleviate%20the%20problem%20of%20delineating%0Aprecise%20boundaries%2C%20we%20adopt%20SAM%20inside%20the%20bounding%20box%20with%20the%20help%20of%0Aanother%20pre-trained%20foundation%20model%20%28e.g.%2C%20Grounding-DINO%29.%20Furthermore%2C%20we%0Aeliminate%20the%20necessity%20of%20using%20the%20supervision%20of%20image%20labels%2C%20by%20employing%0ACLIP%20in%20classification.%20Then%20in%20the%20second%20stage%2C%20the%20generated%20high-quality%0Apseudo-labels%20are%20used%20to%20train%20an%20off-the-shelf%20segmenter%20that%20achieves%20the%0Astate-of-the-art%20performance%20on%20PASCAL%20VOC%202012%20and%20MS%20COCO%202014.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06586v1&entry.124074799=Read"},
{"title": "Multimodal Active Measurement for Human Mesh Recovery in Close Proximity", "author": "Takahiro Maeda and Keisuke Takeshita and Kazuhito Tanaka", "abstract": "  For physical human-robot interactions (pHRI), a robot needs to estimate the\naccurate body pose of a target person. However, in these pHRI scenarios, the\nrobot cannot fully observe the target person's body with equipped cameras\nbecause the target person must be close to the robot for physical interaction.\nThis closeness leads to severe truncation and occlusions and thus results in\npoor accuracy of human pose estimation. For better accuracy in this challenging\nenvironment, we propose an active measurement and sensor fusion framework of\nthe equipped cameras with touch and ranging sensors such as 2D LiDAR. Touch and\nranging sensor measurements are sparse, but reliable and informative cues for\nlocalizing human body parts. In our active measurement process, camera\nviewpoints and sensor placements are dynamically optimized to measure body\nparts with higher estimation uncertainty, which is closely related to\ntruncation or occlusion. In our sensor fusion process, assuming that the\nmeasurements of touch and ranging sensors are more reliable than the\ncamera-based estimations, we fuse the sensor measurements to the camera-based\nestimated pose by aligning the estimated pose towards the measured points. Our\nproposed method outperformed previous methods on the standard occlusion\nbenchmark with simulated active measurement. Furthermore, our method reliably\nestimated human poses using a real robot even with practical constraints such\nas occlusion by blankets.\n", "link": "http://arxiv.org/abs/2310.08116v2", "date": "2024-05-10", "relevancy": 2.2381, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6186}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.551}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Active%20Measurement%20for%20Human%20Mesh%20Recovery%20in%20Close%20Proximity&body=Title%3A%20Multimodal%20Active%20Measurement%20for%20Human%20Mesh%20Recovery%20in%20Close%20Proximity%0AAuthor%3A%20Takahiro%20Maeda%20and%20Keisuke%20Takeshita%20and%20Kazuhito%20Tanaka%0AAbstract%3A%20%20%20For%20physical%20human-robot%20interactions%20%28pHRI%29%2C%20a%20robot%20needs%20to%20estimate%20the%0Aaccurate%20body%20pose%20of%20a%20target%20person.%20However%2C%20in%20these%20pHRI%20scenarios%2C%20the%0Arobot%20cannot%20fully%20observe%20the%20target%20person%27s%20body%20with%20equipped%20cameras%0Abecause%20the%20target%20person%20must%20be%20close%20to%20the%20robot%20for%20physical%20interaction.%0AThis%20closeness%20leads%20to%20severe%20truncation%20and%20occlusions%20and%20thus%20results%20in%0Apoor%20accuracy%20of%20human%20pose%20estimation.%20For%20better%20accuracy%20in%20this%20challenging%0Aenvironment%2C%20we%20propose%20an%20active%20measurement%20and%20sensor%20fusion%20framework%20of%0Athe%20equipped%20cameras%20with%20touch%20and%20ranging%20sensors%20such%20as%202D%20LiDAR.%20Touch%20and%0Aranging%20sensor%20measurements%20are%20sparse%2C%20but%20reliable%20and%20informative%20cues%20for%0Alocalizing%20human%20body%20parts.%20In%20our%20active%20measurement%20process%2C%20camera%0Aviewpoints%20and%20sensor%20placements%20are%20dynamically%20optimized%20to%20measure%20body%0Aparts%20with%20higher%20estimation%20uncertainty%2C%20which%20is%20closely%20related%20to%0Atruncation%20or%20occlusion.%20In%20our%20sensor%20fusion%20process%2C%20assuming%20that%20the%0Ameasurements%20of%20touch%20and%20ranging%20sensors%20are%20more%20reliable%20than%20the%0Acamera-based%20estimations%2C%20we%20fuse%20the%20sensor%20measurements%20to%20the%20camera-based%0Aestimated%20pose%20by%20aligning%20the%20estimated%20pose%20towards%20the%20measured%20points.%20Our%0Aproposed%20method%20outperformed%20previous%20methods%20on%20the%20standard%20occlusion%0Abenchmark%20with%20simulated%20active%20measurement.%20Furthermore%2C%20our%20method%20reliably%0Aestimated%20human%20poses%20using%20a%20real%20robot%20even%20with%20practical%20constraints%20such%0Aas%20occlusion%20by%20blankets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Active%2520Measurement%2520for%2520Human%2520Mesh%2520Recovery%2520in%2520Close%2520Proximity%26entry.906535625%3DTakahiro%2520Maeda%2520and%2520Keisuke%2520Takeshita%2520and%2520Kazuhito%2520Tanaka%26entry.1292438233%3D%2520%2520For%2520physical%2520human-robot%2520interactions%2520%2528pHRI%2529%252C%2520a%2520robot%2520needs%2520to%2520estimate%2520the%250Aaccurate%2520body%2520pose%2520of%2520a%2520target%2520person.%2520However%252C%2520in%2520these%2520pHRI%2520scenarios%252C%2520the%250Arobot%2520cannot%2520fully%2520observe%2520the%2520target%2520person%2527s%2520body%2520with%2520equipped%2520cameras%250Abecause%2520the%2520target%2520person%2520must%2520be%2520close%2520to%2520the%2520robot%2520for%2520physical%2520interaction.%250AThis%2520closeness%2520leads%2520to%2520severe%2520truncation%2520and%2520occlusions%2520and%2520thus%2520results%2520in%250Apoor%2520accuracy%2520of%2520human%2520pose%2520estimation.%2520For%2520better%2520accuracy%2520in%2520this%2520challenging%250Aenvironment%252C%2520we%2520propose%2520an%2520active%2520measurement%2520and%2520sensor%2520fusion%2520framework%2520of%250Athe%2520equipped%2520cameras%2520with%2520touch%2520and%2520ranging%2520sensors%2520such%2520as%25202D%2520LiDAR.%2520Touch%2520and%250Aranging%2520sensor%2520measurements%2520are%2520sparse%252C%2520but%2520reliable%2520and%2520informative%2520cues%2520for%250Alocalizing%2520human%2520body%2520parts.%2520In%2520our%2520active%2520measurement%2520process%252C%2520camera%250Aviewpoints%2520and%2520sensor%2520placements%2520are%2520dynamically%2520optimized%2520to%2520measure%2520body%250Aparts%2520with%2520higher%2520estimation%2520uncertainty%252C%2520which%2520is%2520closely%2520related%2520to%250Atruncation%2520or%2520occlusion.%2520In%2520our%2520sensor%2520fusion%2520process%252C%2520assuming%2520that%2520the%250Ameasurements%2520of%2520touch%2520and%2520ranging%2520sensors%2520are%2520more%2520reliable%2520than%2520the%250Acamera-based%2520estimations%252C%2520we%2520fuse%2520the%2520sensor%2520measurements%2520to%2520the%2520camera-based%250Aestimated%2520pose%2520by%2520aligning%2520the%2520estimated%2520pose%2520towards%2520the%2520measured%2520points.%2520Our%250Aproposed%2520method%2520outperformed%2520previous%2520methods%2520on%2520the%2520standard%2520occlusion%250Abenchmark%2520with%2520simulated%2520active%2520measurement.%2520Furthermore%252C%2520our%2520method%2520reliably%250Aestimated%2520human%2520poses%2520using%2520a%2520real%2520robot%2520even%2520with%2520practical%2520constraints%2520such%250Aas%2520occlusion%2520by%2520blankets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Active%20Measurement%20for%20Human%20Mesh%20Recovery%20in%20Close%20Proximity&entry.906535625=Takahiro%20Maeda%20and%20Keisuke%20Takeshita%20and%20Kazuhito%20Tanaka&entry.1292438233=%20%20For%20physical%20human-robot%20interactions%20%28pHRI%29%2C%20a%20robot%20needs%20to%20estimate%20the%0Aaccurate%20body%20pose%20of%20a%20target%20person.%20However%2C%20in%20these%20pHRI%20scenarios%2C%20the%0Arobot%20cannot%20fully%20observe%20the%20target%20person%27s%20body%20with%20equipped%20cameras%0Abecause%20the%20target%20person%20must%20be%20close%20to%20the%20robot%20for%20physical%20interaction.%0AThis%20closeness%20leads%20to%20severe%20truncation%20and%20occlusions%20and%20thus%20results%20in%0Apoor%20accuracy%20of%20human%20pose%20estimation.%20For%20better%20accuracy%20in%20this%20challenging%0Aenvironment%2C%20we%20propose%20an%20active%20measurement%20and%20sensor%20fusion%20framework%20of%0Athe%20equipped%20cameras%20with%20touch%20and%20ranging%20sensors%20such%20as%202D%20LiDAR.%20Touch%20and%0Aranging%20sensor%20measurements%20are%20sparse%2C%20but%20reliable%20and%20informative%20cues%20for%0Alocalizing%20human%20body%20parts.%20In%20our%20active%20measurement%20process%2C%20camera%0Aviewpoints%20and%20sensor%20placements%20are%20dynamically%20optimized%20to%20measure%20body%0Aparts%20with%20higher%20estimation%20uncertainty%2C%20which%20is%20closely%20related%20to%0Atruncation%20or%20occlusion.%20In%20our%20sensor%20fusion%20process%2C%20assuming%20that%20the%0Ameasurements%20of%20touch%20and%20ranging%20sensors%20are%20more%20reliable%20than%20the%0Acamera-based%20estimations%2C%20we%20fuse%20the%20sensor%20measurements%20to%20the%20camera-based%0Aestimated%20pose%20by%20aligning%20the%20estimated%20pose%20towards%20the%20measured%20points.%20Our%0Aproposed%20method%20outperformed%20previous%20methods%20on%20the%20standard%20occlusion%0Abenchmark%20with%20simulated%20active%20measurement.%20Furthermore%2C%20our%20method%20reliably%0Aestimated%20human%20poses%20using%20a%20real%20robot%20even%20with%20practical%20constraints%20such%0Aas%20occlusion%20by%20blankets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08116v2&entry.124074799=Read"},
{"title": "Mesh Denoising Transformer", "author": "Wenbo Zhao and Xianming Liu and Deming Zhai and Junjun Jiang and Xiangyang Ji", "abstract": "  Mesh denoising, aimed at removing noise from input meshes while preserving\ntheir feature structures, is a practical yet challenging task. Despite the\nremarkable progress in learning-based mesh denoising methodologies in recent\nyears, their network designs often encounter two principal drawbacks: a\ndependence on single-modal geometric representations, which fall short in\ncapturing the multifaceted attributes of meshes, and a lack of effective global\nfeature aggregation, hindering their ability to fully understand the mesh's\ncomprehensive structure. To tackle these issues, we propose SurfaceFormer, a\npioneering Transformer-based mesh denoising framework. Our first contribution\nis the development of a new representation known as Local Surface Descriptor,\nwhich is crafted by establishing polar systems on each mesh face, followed by\nsampling points from adjacent surfaces using geodesics. The normals of these\npoints are organized into 2D patches, mimicking images to capture local\ngeometric intricacies, whereas the poles and vertex coordinates are\nconsolidated into a point cloud to embody spatial information. This advancement\nsurmounts the hurdles posed by the irregular and non-Euclidean characteristics\nof mesh data, facilitating a smooth integration with Transformer architecture.\nNext, we propose a dual-stream structure consisting of a Geometric Encoder\nbranch and a Spatial Encoder branch, which jointly encode local geometry\ndetails and spatial information to fully explore multimodal information for\nmesh denoising. A subsequent Denoising Transformer module receives the\nmultimodal information and achieves efficient global feature aggregation\nthrough self-attention operators. Our experimental evaluations demonstrate that\nthis novel approach outperforms existing state-of-the-art methods in both\nobjective and subjective assessments, marking a significant leap forward in\nmesh denoising.\n", "link": "http://arxiv.org/abs/2405.06536v1", "date": "2024-05-10", "relevancy": 2.2145, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5781}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5672}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh%20Denoising%20Transformer&body=Title%3A%20Mesh%20Denoising%20Transformer%0AAuthor%3A%20Wenbo%20Zhao%20and%20Xianming%20Liu%20and%20Deming%20Zhai%20and%20Junjun%20Jiang%20and%20Xiangyang%20Ji%0AAbstract%3A%20%20%20Mesh%20denoising%2C%20aimed%20at%20removing%20noise%20from%20input%20meshes%20while%20preserving%0Atheir%20feature%20structures%2C%20is%20a%20practical%20yet%20challenging%20task.%20Despite%20the%0Aremarkable%20progress%20in%20learning-based%20mesh%20denoising%20methodologies%20in%20recent%0Ayears%2C%20their%20network%20designs%20often%20encounter%20two%20principal%20drawbacks%3A%20a%0Adependence%20on%20single-modal%20geometric%20representations%2C%20which%20fall%20short%20in%0Acapturing%20the%20multifaceted%20attributes%20of%20meshes%2C%20and%20a%20lack%20of%20effective%20global%0Afeature%20aggregation%2C%20hindering%20their%20ability%20to%20fully%20understand%20the%20mesh%27s%0Acomprehensive%20structure.%20To%20tackle%20these%20issues%2C%20we%20propose%20SurfaceFormer%2C%20a%0Apioneering%20Transformer-based%20mesh%20denoising%20framework.%20Our%20first%20contribution%0Ais%20the%20development%20of%20a%20new%20representation%20known%20as%20Local%20Surface%20Descriptor%2C%0Awhich%20is%20crafted%20by%20establishing%20polar%20systems%20on%20each%20mesh%20face%2C%20followed%20by%0Asampling%20points%20from%20adjacent%20surfaces%20using%20geodesics.%20The%20normals%20of%20these%0Apoints%20are%20organized%20into%202D%20patches%2C%20mimicking%20images%20to%20capture%20local%0Ageometric%20intricacies%2C%20whereas%20the%20poles%20and%20vertex%20coordinates%20are%0Aconsolidated%20into%20a%20point%20cloud%20to%20embody%20spatial%20information.%20This%20advancement%0Asurmounts%20the%20hurdles%20posed%20by%20the%20irregular%20and%20non-Euclidean%20characteristics%0Aof%20mesh%20data%2C%20facilitating%20a%20smooth%20integration%20with%20Transformer%20architecture.%0ANext%2C%20we%20propose%20a%20dual-stream%20structure%20consisting%20of%20a%20Geometric%20Encoder%0Abranch%20and%20a%20Spatial%20Encoder%20branch%2C%20which%20jointly%20encode%20local%20geometry%0Adetails%20and%20spatial%20information%20to%20fully%20explore%20multimodal%20information%20for%0Amesh%20denoising.%20A%20subsequent%20Denoising%20Transformer%20module%20receives%20the%0Amultimodal%20information%20and%20achieves%20efficient%20global%20feature%20aggregation%0Athrough%20self-attention%20operators.%20Our%20experimental%20evaluations%20demonstrate%20that%0Athis%20novel%20approach%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%0Aobjective%20and%20subjective%20assessments%2C%20marking%20a%20significant%20leap%20forward%20in%0Amesh%20denoising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh%2520Denoising%2520Transformer%26entry.906535625%3DWenbo%2520Zhao%2520and%2520Xianming%2520Liu%2520and%2520Deming%2520Zhai%2520and%2520Junjun%2520Jiang%2520and%2520Xiangyang%2520Ji%26entry.1292438233%3D%2520%2520Mesh%2520denoising%252C%2520aimed%2520at%2520removing%2520noise%2520from%2520input%2520meshes%2520while%2520preserving%250Atheir%2520feature%2520structures%252C%2520is%2520a%2520practical%2520yet%2520challenging%2520task.%2520Despite%2520the%250Aremarkable%2520progress%2520in%2520learning-based%2520mesh%2520denoising%2520methodologies%2520in%2520recent%250Ayears%252C%2520their%2520network%2520designs%2520often%2520encounter%2520two%2520principal%2520drawbacks%253A%2520a%250Adependence%2520on%2520single-modal%2520geometric%2520representations%252C%2520which%2520fall%2520short%2520in%250Acapturing%2520the%2520multifaceted%2520attributes%2520of%2520meshes%252C%2520and%2520a%2520lack%2520of%2520effective%2520global%250Afeature%2520aggregation%252C%2520hindering%2520their%2520ability%2520to%2520fully%2520understand%2520the%2520mesh%2527s%250Acomprehensive%2520structure.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520SurfaceFormer%252C%2520a%250Apioneering%2520Transformer-based%2520mesh%2520denoising%2520framework.%2520Our%2520first%2520contribution%250Ais%2520the%2520development%2520of%2520a%2520new%2520representation%2520known%2520as%2520Local%2520Surface%2520Descriptor%252C%250Awhich%2520is%2520crafted%2520by%2520establishing%2520polar%2520systems%2520on%2520each%2520mesh%2520face%252C%2520followed%2520by%250Asampling%2520points%2520from%2520adjacent%2520surfaces%2520using%2520geodesics.%2520The%2520normals%2520of%2520these%250Apoints%2520are%2520organized%2520into%25202D%2520patches%252C%2520mimicking%2520images%2520to%2520capture%2520local%250Ageometric%2520intricacies%252C%2520whereas%2520the%2520poles%2520and%2520vertex%2520coordinates%2520are%250Aconsolidated%2520into%2520a%2520point%2520cloud%2520to%2520embody%2520spatial%2520information.%2520This%2520advancement%250Asurmounts%2520the%2520hurdles%2520posed%2520by%2520the%2520irregular%2520and%2520non-Euclidean%2520characteristics%250Aof%2520mesh%2520data%252C%2520facilitating%2520a%2520smooth%2520integration%2520with%2520Transformer%2520architecture.%250ANext%252C%2520we%2520propose%2520a%2520dual-stream%2520structure%2520consisting%2520of%2520a%2520Geometric%2520Encoder%250Abranch%2520and%2520a%2520Spatial%2520Encoder%2520branch%252C%2520which%2520jointly%2520encode%2520local%2520geometry%250Adetails%2520and%2520spatial%2520information%2520to%2520fully%2520explore%2520multimodal%2520information%2520for%250Amesh%2520denoising.%2520A%2520subsequent%2520Denoising%2520Transformer%2520module%2520receives%2520the%250Amultimodal%2520information%2520and%2520achieves%2520efficient%2520global%2520feature%2520aggregation%250Athrough%2520self-attention%2520operators.%2520Our%2520experimental%2520evaluations%2520demonstrate%2520that%250Athis%2520novel%2520approach%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520both%250Aobjective%2520and%2520subjective%2520assessments%252C%2520marking%2520a%2520significant%2520leap%2520forward%2520in%250Amesh%2520denoising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh%20Denoising%20Transformer&entry.906535625=Wenbo%20Zhao%20and%20Xianming%20Liu%20and%20Deming%20Zhai%20and%20Junjun%20Jiang%20and%20Xiangyang%20Ji&entry.1292438233=%20%20Mesh%20denoising%2C%20aimed%20at%20removing%20noise%20from%20input%20meshes%20while%20preserving%0Atheir%20feature%20structures%2C%20is%20a%20practical%20yet%20challenging%20task.%20Despite%20the%0Aremarkable%20progress%20in%20learning-based%20mesh%20denoising%20methodologies%20in%20recent%0Ayears%2C%20their%20network%20designs%20often%20encounter%20two%20principal%20drawbacks%3A%20a%0Adependence%20on%20single-modal%20geometric%20representations%2C%20which%20fall%20short%20in%0Acapturing%20the%20multifaceted%20attributes%20of%20meshes%2C%20and%20a%20lack%20of%20effective%20global%0Afeature%20aggregation%2C%20hindering%20their%20ability%20to%20fully%20understand%20the%20mesh%27s%0Acomprehensive%20structure.%20To%20tackle%20these%20issues%2C%20we%20propose%20SurfaceFormer%2C%20a%0Apioneering%20Transformer-based%20mesh%20denoising%20framework.%20Our%20first%20contribution%0Ais%20the%20development%20of%20a%20new%20representation%20known%20as%20Local%20Surface%20Descriptor%2C%0Awhich%20is%20crafted%20by%20establishing%20polar%20systems%20on%20each%20mesh%20face%2C%20followed%20by%0Asampling%20points%20from%20adjacent%20surfaces%20using%20geodesics.%20The%20normals%20of%20these%0Apoints%20are%20organized%20into%202D%20patches%2C%20mimicking%20images%20to%20capture%20local%0Ageometric%20intricacies%2C%20whereas%20the%20poles%20and%20vertex%20coordinates%20are%0Aconsolidated%20into%20a%20point%20cloud%20to%20embody%20spatial%20information.%20This%20advancement%0Asurmounts%20the%20hurdles%20posed%20by%20the%20irregular%20and%20non-Euclidean%20characteristics%0Aof%20mesh%20data%2C%20facilitating%20a%20smooth%20integration%20with%20Transformer%20architecture.%0ANext%2C%20we%20propose%20a%20dual-stream%20structure%20consisting%20of%20a%20Geometric%20Encoder%0Abranch%20and%20a%20Spatial%20Encoder%20branch%2C%20which%20jointly%20encode%20local%20geometry%0Adetails%20and%20spatial%20information%20to%20fully%20explore%20multimodal%20information%20for%0Amesh%20denoising.%20A%20subsequent%20Denoising%20Transformer%20module%20receives%20the%0Amultimodal%20information%20and%20achieves%20efficient%20global%20feature%20aggregation%0Athrough%20self-attention%20operators.%20Our%20experimental%20evaluations%20demonstrate%20that%0Athis%20novel%20approach%20outperforms%20existing%20state-of-the-art%20methods%20in%20both%0Aobjective%20and%20subjective%20assessments%2C%20marking%20a%20significant%20leap%20forward%20in%0Amesh%20denoising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06536v1&entry.124074799=Read"},
{"title": "Multi-level Personalized Federated Learning on Heterogeneous and\n  Long-Tailed Data", "author": "Rongyu Zhang and Yun Chen and Chenrui Wu and Fangxin Wang and Bo Li", "abstract": "  Federated learning (FL) offers a privacy-centric distributed learning\nframework, enabling model training on individual clients and central\naggregation without necessitating data exchange. Nonetheless, FL\nimplementations often suffer from non-i.i.d. and long-tailed class\ndistributions across mobile applications, e.g., autonomous vehicles, which\nleads models to overfitting as local training may converge to sub-optimal. In\nour study, we explore the impact of data heterogeneity on model bias and\nintroduce an innovative personalized FL framework, Multi-level Personalized\nFederated Learning (MuPFL), which leverages the hierarchical architecture of FL\nto fully harness computational resources at various levels. This framework\nintegrates three pivotal modules: Biased Activation Value Dropout (BAVD) to\nmitigate overfitting and accelerate training; Adaptive Cluster-based Model\nUpdate (ACMU) to refine local models ensuring coherent global aggregation; and\nPrior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster\nclassification and personalize models in accord with skewed local data with\nshared knowledge. Extensive experiments on diverse real-world datasets for\nimage classification and semantic segmentation validate that MuPFL consistently\noutperforms state-of-the-art baselines, even under extreme non-i.i.d. and\nlong-tail conditions, which enhances accuracy by as much as 7.39% and\naccelerates training by up to 80% at most, marking significant advancements in\nboth efficiency and effectiveness.\n", "link": "http://arxiv.org/abs/2405.06413v1", "date": "2024-05-10", "relevancy": 2.2054, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-level%20Personalized%20Federated%20Learning%20on%20Heterogeneous%20and%0A%20%20Long-Tailed%20Data&body=Title%3A%20Multi-level%20Personalized%20Federated%20Learning%20on%20Heterogeneous%20and%0A%20%20Long-Tailed%20Data%0AAuthor%3A%20Rongyu%20Zhang%20and%20Yun%20Chen%20and%20Chenrui%20Wu%20and%20Fangxin%20Wang%20and%20Bo%20Li%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20offers%20a%20privacy-centric%20distributed%20learning%0Aframework%2C%20enabling%20model%20training%20on%20individual%20clients%20and%20central%0Aaggregation%20without%20necessitating%20data%20exchange.%20Nonetheless%2C%20FL%0Aimplementations%20often%20suffer%20from%20non-i.i.d.%20and%20long-tailed%20class%0Adistributions%20across%20mobile%20applications%2C%20e.g.%2C%20autonomous%20vehicles%2C%20which%0Aleads%20models%20to%20overfitting%20as%20local%20training%20may%20converge%20to%20sub-optimal.%20In%0Aour%20study%2C%20we%20explore%20the%20impact%20of%20data%20heterogeneity%20on%20model%20bias%20and%0Aintroduce%20an%20innovative%20personalized%20FL%20framework%2C%20Multi-level%20Personalized%0AFederated%20Learning%20%28MuPFL%29%2C%20which%20leverages%20the%20hierarchical%20architecture%20of%20FL%0Ato%20fully%20harness%20computational%20resources%20at%20various%20levels.%20This%20framework%0Aintegrates%20three%20pivotal%20modules%3A%20Biased%20Activation%20Value%20Dropout%20%28BAVD%29%20to%0Amitigate%20overfitting%20and%20accelerate%20training%3B%20Adaptive%20Cluster-based%20Model%0AUpdate%20%28ACMU%29%20to%20refine%20local%20models%20ensuring%20coherent%20global%20aggregation%3B%20and%0APrior%20Knowledge-assisted%20Classifier%20Fine-tuning%20%28PKCF%29%20to%20bolster%0Aclassification%20and%20personalize%20models%20in%20accord%20with%20skewed%20local%20data%20with%0Ashared%20knowledge.%20Extensive%20experiments%20on%20diverse%20real-world%20datasets%20for%0Aimage%20classification%20and%20semantic%20segmentation%20validate%20that%20MuPFL%20consistently%0Aoutperforms%20state-of-the-art%20baselines%2C%20even%20under%20extreme%20non-i.i.d.%20and%0Along-tail%20conditions%2C%20which%20enhances%20accuracy%20by%20as%20much%20as%207.39%25%20and%0Aaccelerates%20training%20by%20up%20to%2080%25%20at%20most%2C%20marking%20significant%20advancements%20in%0Aboth%20efficiency%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-level%2520Personalized%2520Federated%2520Learning%2520on%2520Heterogeneous%2520and%250A%2520%2520Long-Tailed%2520Data%26entry.906535625%3DRongyu%2520Zhang%2520and%2520Yun%2520Chen%2520and%2520Chenrui%2520Wu%2520and%2520Fangxin%2520Wang%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520offers%2520a%2520privacy-centric%2520distributed%2520learning%250Aframework%252C%2520enabling%2520model%2520training%2520on%2520individual%2520clients%2520and%2520central%250Aaggregation%2520without%2520necessitating%2520data%2520exchange.%2520Nonetheless%252C%2520FL%250Aimplementations%2520often%2520suffer%2520from%2520non-i.i.d.%2520and%2520long-tailed%2520class%250Adistributions%2520across%2520mobile%2520applications%252C%2520e.g.%252C%2520autonomous%2520vehicles%252C%2520which%250Aleads%2520models%2520to%2520overfitting%2520as%2520local%2520training%2520may%2520converge%2520to%2520sub-optimal.%2520In%250Aour%2520study%252C%2520we%2520explore%2520the%2520impact%2520of%2520data%2520heterogeneity%2520on%2520model%2520bias%2520and%250Aintroduce%2520an%2520innovative%2520personalized%2520FL%2520framework%252C%2520Multi-level%2520Personalized%250AFederated%2520Learning%2520%2528MuPFL%2529%252C%2520which%2520leverages%2520the%2520hierarchical%2520architecture%2520of%2520FL%250Ato%2520fully%2520harness%2520computational%2520resources%2520at%2520various%2520levels.%2520This%2520framework%250Aintegrates%2520three%2520pivotal%2520modules%253A%2520Biased%2520Activation%2520Value%2520Dropout%2520%2528BAVD%2529%2520to%250Amitigate%2520overfitting%2520and%2520accelerate%2520training%253B%2520Adaptive%2520Cluster-based%2520Model%250AUpdate%2520%2528ACMU%2529%2520to%2520refine%2520local%2520models%2520ensuring%2520coherent%2520global%2520aggregation%253B%2520and%250APrior%2520Knowledge-assisted%2520Classifier%2520Fine-tuning%2520%2528PKCF%2529%2520to%2520bolster%250Aclassification%2520and%2520personalize%2520models%2520in%2520accord%2520with%2520skewed%2520local%2520data%2520with%250Ashared%2520knowledge.%2520Extensive%2520experiments%2520on%2520diverse%2520real-world%2520datasets%2520for%250Aimage%2520classification%2520and%2520semantic%2520segmentation%2520validate%2520that%2520MuPFL%2520consistently%250Aoutperforms%2520state-of-the-art%2520baselines%252C%2520even%2520under%2520extreme%2520non-i.i.d.%2520and%250Along-tail%2520conditions%252C%2520which%2520enhances%2520accuracy%2520by%2520as%2520much%2520as%25207.39%2525%2520and%250Aaccelerates%2520training%2520by%2520up%2520to%252080%2525%2520at%2520most%252C%2520marking%2520significant%2520advancements%2520in%250Aboth%2520efficiency%2520and%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20Personalized%20Federated%20Learning%20on%20Heterogeneous%20and%0A%20%20Long-Tailed%20Data&entry.906535625=Rongyu%20Zhang%20and%20Yun%20Chen%20and%20Chenrui%20Wu%20and%20Fangxin%20Wang%20and%20Bo%20Li&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20offers%20a%20privacy-centric%20distributed%20learning%0Aframework%2C%20enabling%20model%20training%20on%20individual%20clients%20and%20central%0Aaggregation%20without%20necessitating%20data%20exchange.%20Nonetheless%2C%20FL%0Aimplementations%20often%20suffer%20from%20non-i.i.d.%20and%20long-tailed%20class%0Adistributions%20across%20mobile%20applications%2C%20e.g.%2C%20autonomous%20vehicles%2C%20which%0Aleads%20models%20to%20overfitting%20as%20local%20training%20may%20converge%20to%20sub-optimal.%20In%0Aour%20study%2C%20we%20explore%20the%20impact%20of%20data%20heterogeneity%20on%20model%20bias%20and%0Aintroduce%20an%20innovative%20personalized%20FL%20framework%2C%20Multi-level%20Personalized%0AFederated%20Learning%20%28MuPFL%29%2C%20which%20leverages%20the%20hierarchical%20architecture%20of%20FL%0Ato%20fully%20harness%20computational%20resources%20at%20various%20levels.%20This%20framework%0Aintegrates%20three%20pivotal%20modules%3A%20Biased%20Activation%20Value%20Dropout%20%28BAVD%29%20to%0Amitigate%20overfitting%20and%20accelerate%20training%3B%20Adaptive%20Cluster-based%20Model%0AUpdate%20%28ACMU%29%20to%20refine%20local%20models%20ensuring%20coherent%20global%20aggregation%3B%20and%0APrior%20Knowledge-assisted%20Classifier%20Fine-tuning%20%28PKCF%29%20to%20bolster%0Aclassification%20and%20personalize%20models%20in%20accord%20with%20skewed%20local%20data%20with%0Ashared%20knowledge.%20Extensive%20experiments%20on%20diverse%20real-world%20datasets%20for%0Aimage%20classification%20and%20semantic%20segmentation%20validate%20that%20MuPFL%20consistently%0Aoutperforms%20state-of-the-art%20baselines%2C%20even%20under%20extreme%20non-i.i.d.%20and%0Along-tail%20conditions%2C%20which%20enhances%20accuracy%20by%20as%20much%20as%207.39%25%20and%0Aaccelerates%20training%20by%20up%20to%2080%25%20at%20most%2C%20marking%20significant%20advancements%20in%0Aboth%20efficiency%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06413v1&entry.124074799=Read"},
{"title": "Rasterized Edge Gradients: Handling Discontinuities Differentiably", "author": "Stanislav Pidhorskyi and Tomas Simon and Gabriel Schwartz and He Wen and Yaser Sheikh and Jason Saragih", "abstract": "  Computing the gradients of a rendering process is paramount for diverse\napplications in computer vision and graphics. However, accurate computation of\nthese gradients is challenging due to discontinuities and rendering\napproximations, particularly for surface-based representations and\nrasterization-based rendering. We present a novel method for computing\ngradients at visibility discontinuities for rasterization-based differentiable\nrenderers. Our method elegantly simplifies the traditionally complex problem\nthrough a carefully designed approximation strategy, allowing for a\nstraightforward, effective, and performant solution. We introduce a novel\nconcept of micro-edges, which allows us to treat the rasterized images as\noutcomes of a differentiable, continuous process aligned with the inherently\nnon-differentiable, discrete-pixel rasterization. This technique eliminates the\nnecessity for rendering approximations or other modifications to the forward\npass, preserving the integrity of the rendered image, which makes it applicable\nto rasterized masks, depth, and normals images where filtering is prohibitive.\nUtilizing micro-edges simplifies gradient interpretation at discontinuities and\nenables handling of geometry intersections, offering an advantage over the\nprior art. We showcase our method in dynamic human head scene reconstruction,\ndemonstrating effective handling of camera images and segmentation masks.\n", "link": "http://arxiv.org/abs/2405.02508v2", "date": "2024-05-10", "relevancy": 2.1967, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5535}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5497}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rasterized%20Edge%20Gradients%3A%20Handling%20Discontinuities%20Differentiably&body=Title%3A%20Rasterized%20Edge%20Gradients%3A%20Handling%20Discontinuities%20Differentiably%0AAuthor%3A%20Stanislav%20Pidhorskyi%20and%20Tomas%20Simon%20and%20Gabriel%20Schwartz%20and%20He%20Wen%20and%20Yaser%20Sheikh%20and%20Jason%20Saragih%0AAbstract%3A%20%20%20Computing%20the%20gradients%20of%20a%20rendering%20process%20is%20paramount%20for%20diverse%0Aapplications%20in%20computer%20vision%20and%20graphics.%20However%2C%20accurate%20computation%20of%0Athese%20gradients%20is%20challenging%20due%20to%20discontinuities%20and%20rendering%0Aapproximations%2C%20particularly%20for%20surface-based%20representations%20and%0Arasterization-based%20rendering.%20We%20present%20a%20novel%20method%20for%20computing%0Agradients%20at%20visibility%20discontinuities%20for%20rasterization-based%20differentiable%0Arenderers.%20Our%20method%20elegantly%20simplifies%20the%20traditionally%20complex%20problem%0Athrough%20a%20carefully%20designed%20approximation%20strategy%2C%20allowing%20for%20a%0Astraightforward%2C%20effective%2C%20and%20performant%20solution.%20We%20introduce%20a%20novel%0Aconcept%20of%20micro-edges%2C%20which%20allows%20us%20to%20treat%20the%20rasterized%20images%20as%0Aoutcomes%20of%20a%20differentiable%2C%20continuous%20process%20aligned%20with%20the%20inherently%0Anon-differentiable%2C%20discrete-pixel%20rasterization.%20This%20technique%20eliminates%20the%0Anecessity%20for%20rendering%20approximations%20or%20other%20modifications%20to%20the%20forward%0Apass%2C%20preserving%20the%20integrity%20of%20the%20rendered%20image%2C%20which%20makes%20it%20applicable%0Ato%20rasterized%20masks%2C%20depth%2C%20and%20normals%20images%20where%20filtering%20is%20prohibitive.%0AUtilizing%20micro-edges%20simplifies%20gradient%20interpretation%20at%20discontinuities%20and%0Aenables%20handling%20of%20geometry%20intersections%2C%20offering%20an%20advantage%20over%20the%0Aprior%20art.%20We%20showcase%20our%20method%20in%20dynamic%20human%20head%20scene%20reconstruction%2C%0Ademonstrating%20effective%20handling%20of%20camera%20images%20and%20segmentation%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRasterized%2520Edge%2520Gradients%253A%2520Handling%2520Discontinuities%2520Differentiably%26entry.906535625%3DStanislav%2520Pidhorskyi%2520and%2520Tomas%2520Simon%2520and%2520Gabriel%2520Schwartz%2520and%2520He%2520Wen%2520and%2520Yaser%2520Sheikh%2520and%2520Jason%2520Saragih%26entry.1292438233%3D%2520%2520Computing%2520the%2520gradients%2520of%2520a%2520rendering%2520process%2520is%2520paramount%2520for%2520diverse%250Aapplications%2520in%2520computer%2520vision%2520and%2520graphics.%2520However%252C%2520accurate%2520computation%2520of%250Athese%2520gradients%2520is%2520challenging%2520due%2520to%2520discontinuities%2520and%2520rendering%250Aapproximations%252C%2520particularly%2520for%2520surface-based%2520representations%2520and%250Arasterization-based%2520rendering.%2520We%2520present%2520a%2520novel%2520method%2520for%2520computing%250Agradients%2520at%2520visibility%2520discontinuities%2520for%2520rasterization-based%2520differentiable%250Arenderers.%2520Our%2520method%2520elegantly%2520simplifies%2520the%2520traditionally%2520complex%2520problem%250Athrough%2520a%2520carefully%2520designed%2520approximation%2520strategy%252C%2520allowing%2520for%2520a%250Astraightforward%252C%2520effective%252C%2520and%2520performant%2520solution.%2520We%2520introduce%2520a%2520novel%250Aconcept%2520of%2520micro-edges%252C%2520which%2520allows%2520us%2520to%2520treat%2520the%2520rasterized%2520images%2520as%250Aoutcomes%2520of%2520a%2520differentiable%252C%2520continuous%2520process%2520aligned%2520with%2520the%2520inherently%250Anon-differentiable%252C%2520discrete-pixel%2520rasterization.%2520This%2520technique%2520eliminates%2520the%250Anecessity%2520for%2520rendering%2520approximations%2520or%2520other%2520modifications%2520to%2520the%2520forward%250Apass%252C%2520preserving%2520the%2520integrity%2520of%2520the%2520rendered%2520image%252C%2520which%2520makes%2520it%2520applicable%250Ato%2520rasterized%2520masks%252C%2520depth%252C%2520and%2520normals%2520images%2520where%2520filtering%2520is%2520prohibitive.%250AUtilizing%2520micro-edges%2520simplifies%2520gradient%2520interpretation%2520at%2520discontinuities%2520and%250Aenables%2520handling%2520of%2520geometry%2520intersections%252C%2520offering%2520an%2520advantage%2520over%2520the%250Aprior%2520art.%2520We%2520showcase%2520our%2520method%2520in%2520dynamic%2520human%2520head%2520scene%2520reconstruction%252C%250Ademonstrating%2520effective%2520handling%2520of%2520camera%2520images%2520and%2520segmentation%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rasterized%20Edge%20Gradients%3A%20Handling%20Discontinuities%20Differentiably&entry.906535625=Stanislav%20Pidhorskyi%20and%20Tomas%20Simon%20and%20Gabriel%20Schwartz%20and%20He%20Wen%20and%20Yaser%20Sheikh%20and%20Jason%20Saragih&entry.1292438233=%20%20Computing%20the%20gradients%20of%20a%20rendering%20process%20is%20paramount%20for%20diverse%0Aapplications%20in%20computer%20vision%20and%20graphics.%20However%2C%20accurate%20computation%20of%0Athese%20gradients%20is%20challenging%20due%20to%20discontinuities%20and%20rendering%0Aapproximations%2C%20particularly%20for%20surface-based%20representations%20and%0Arasterization-based%20rendering.%20We%20present%20a%20novel%20method%20for%20computing%0Agradients%20at%20visibility%20discontinuities%20for%20rasterization-based%20differentiable%0Arenderers.%20Our%20method%20elegantly%20simplifies%20the%20traditionally%20complex%20problem%0Athrough%20a%20carefully%20designed%20approximation%20strategy%2C%20allowing%20for%20a%0Astraightforward%2C%20effective%2C%20and%20performant%20solution.%20We%20introduce%20a%20novel%0Aconcept%20of%20micro-edges%2C%20which%20allows%20us%20to%20treat%20the%20rasterized%20images%20as%0Aoutcomes%20of%20a%20differentiable%2C%20continuous%20process%20aligned%20with%20the%20inherently%0Anon-differentiable%2C%20discrete-pixel%20rasterization.%20This%20technique%20eliminates%20the%0Anecessity%20for%20rendering%20approximations%20or%20other%20modifications%20to%20the%20forward%0Apass%2C%20preserving%20the%20integrity%20of%20the%20rendered%20image%2C%20which%20makes%20it%20applicable%0Ato%20rasterized%20masks%2C%20depth%2C%20and%20normals%20images%20where%20filtering%20is%20prohibitive.%0AUtilizing%20micro-edges%20simplifies%20gradient%20interpretation%20at%20discontinuities%20and%0Aenables%20handling%20of%20geometry%20intersections%2C%20offering%20an%20advantage%20over%20the%0Aprior%20art.%20We%20showcase%20our%20method%20in%20dynamic%20human%20head%20scene%20reconstruction%2C%0Ademonstrating%20effective%20handling%20of%20camera%20images%20and%20segmentation%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02508v2&entry.124074799=Read"},
{"title": "Sharp analysis of out-of-distribution error for \"importance-weighted\"\n  estimators in the overparameterized regime", "author": "Kuo-Wei Lai and Vidya Muthukumar", "abstract": "  Overparameterized models that achieve zero training error are observed to\ngeneralize well on average, but degrade in performance when faced with data\nthat is under-represented in the training sample. In this work, we study an\noverparameterized Gaussian mixture model imbued with a spurious feature, and\nsharply analyze the in-distribution and out-of-distribution test error of a\ncost-sensitive interpolating solution that incorporates \"importance weights\".\nCompared to recent work Wang et al. (2021), Behnia et al. (2022), our analysis\nis sharp with matching upper and lower bounds, and significantly weakens\nrequired assumptions on data dimensionality. Our error characterizations also\napply to any choice of importance weights and unveil a novel tradeoff between\nworst-case robustness to distribution shift and average accuracy as a function\nof the importance weight magnitude.\n", "link": "http://arxiv.org/abs/2405.06546v1", "date": "2024-05-10", "relevancy": 2.1825, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4394}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4376}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharp%20analysis%20of%20out-of-distribution%20error%20for%20%22importance-weighted%22%0A%20%20estimators%20in%20the%20overparameterized%20regime&body=Title%3A%20Sharp%20analysis%20of%20out-of-distribution%20error%20for%20%22importance-weighted%22%0A%20%20estimators%20in%20the%20overparameterized%20regime%0AAuthor%3A%20Kuo-Wei%20Lai%20and%20Vidya%20Muthukumar%0AAbstract%3A%20%20%20Overparameterized%20models%20that%20achieve%20zero%20training%20error%20are%20observed%20to%0Ageneralize%20well%20on%20average%2C%20but%20degrade%20in%20performance%20when%20faced%20with%20data%0Athat%20is%20under-represented%20in%20the%20training%20sample.%20In%20this%20work%2C%20we%20study%20an%0Aoverparameterized%20Gaussian%20mixture%20model%20imbued%20with%20a%20spurious%20feature%2C%20and%0Asharply%20analyze%20the%20in-distribution%20and%20out-of-distribution%20test%20error%20of%20a%0Acost-sensitive%20interpolating%20solution%20that%20incorporates%20%22importance%20weights%22.%0ACompared%20to%20recent%20work%20Wang%20et%20al.%20%282021%29%2C%20Behnia%20et%20al.%20%282022%29%2C%20our%20analysis%0Ais%20sharp%20with%20matching%20upper%20and%20lower%20bounds%2C%20and%20significantly%20weakens%0Arequired%20assumptions%20on%20data%20dimensionality.%20Our%20error%20characterizations%20also%0Aapply%20to%20any%20choice%20of%20importance%20weights%20and%20unveil%20a%20novel%20tradeoff%20between%0Aworst-case%20robustness%20to%20distribution%20shift%20and%20average%20accuracy%20as%20a%20function%0Aof%20the%20importance%20weight%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharp%2520analysis%2520of%2520out-of-distribution%2520error%2520for%2520%2522importance-weighted%2522%250A%2520%2520estimators%2520in%2520the%2520overparameterized%2520regime%26entry.906535625%3DKuo-Wei%2520Lai%2520and%2520Vidya%2520Muthukumar%26entry.1292438233%3D%2520%2520Overparameterized%2520models%2520that%2520achieve%2520zero%2520training%2520error%2520are%2520observed%2520to%250Ageneralize%2520well%2520on%2520average%252C%2520but%2520degrade%2520in%2520performance%2520when%2520faced%2520with%2520data%250Athat%2520is%2520under-represented%2520in%2520the%2520training%2520sample.%2520In%2520this%2520work%252C%2520we%2520study%2520an%250Aoverparameterized%2520Gaussian%2520mixture%2520model%2520imbued%2520with%2520a%2520spurious%2520feature%252C%2520and%250Asharply%2520analyze%2520the%2520in-distribution%2520and%2520out-of-distribution%2520test%2520error%2520of%2520a%250Acost-sensitive%2520interpolating%2520solution%2520that%2520incorporates%2520%2522importance%2520weights%2522.%250ACompared%2520to%2520recent%2520work%2520Wang%2520et%2520al.%2520%25282021%2529%252C%2520Behnia%2520et%2520al.%2520%25282022%2529%252C%2520our%2520analysis%250Ais%2520sharp%2520with%2520matching%2520upper%2520and%2520lower%2520bounds%252C%2520and%2520significantly%2520weakens%250Arequired%2520assumptions%2520on%2520data%2520dimensionality.%2520Our%2520error%2520characterizations%2520also%250Aapply%2520to%2520any%2520choice%2520of%2520importance%2520weights%2520and%2520unveil%2520a%2520novel%2520tradeoff%2520between%250Aworst-case%2520robustness%2520to%2520distribution%2520shift%2520and%2520average%2520accuracy%2520as%2520a%2520function%250Aof%2520the%2520importance%2520weight%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20analysis%20of%20out-of-distribution%20error%20for%20%22importance-weighted%22%0A%20%20estimators%20in%20the%20overparameterized%20regime&entry.906535625=Kuo-Wei%20Lai%20and%20Vidya%20Muthukumar&entry.1292438233=%20%20Overparameterized%20models%20that%20achieve%20zero%20training%20error%20are%20observed%20to%0Ageneralize%20well%20on%20average%2C%20but%20degrade%20in%20performance%20when%20faced%20with%20data%0Athat%20is%20under-represented%20in%20the%20training%20sample.%20In%20this%20work%2C%20we%20study%20an%0Aoverparameterized%20Gaussian%20mixture%20model%20imbued%20with%20a%20spurious%20feature%2C%20and%0Asharply%20analyze%20the%20in-distribution%20and%20out-of-distribution%20test%20error%20of%20a%0Acost-sensitive%20interpolating%20solution%20that%20incorporates%20%22importance%20weights%22.%0ACompared%20to%20recent%20work%20Wang%20et%20al.%20%282021%29%2C%20Behnia%20et%20al.%20%282022%29%2C%20our%20analysis%0Ais%20sharp%20with%20matching%20upper%20and%20lower%20bounds%2C%20and%20significantly%20weakens%0Arequired%20assumptions%20on%20data%20dimensionality.%20Our%20error%20characterizations%20also%0Aapply%20to%20any%20choice%20of%20importance%20weights%20and%20unveil%20a%20novel%20tradeoff%20between%0Aworst-case%20robustness%20to%20distribution%20shift%20and%20average%20accuracy%20as%20a%20function%0Aof%20the%20importance%20weight%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06546v1&entry.124074799=Read"},
{"title": "Deep learning enhanced mixed integer optimization: Learning to reduce\n  model dimensionality", "author": "Niki Triantafyllou and Maria M. Papathanasiou", "abstract": "  This work introduces a framework to address the computational complexity\ninherent in Mixed-Integer Programming (MIP) models by harnessing the potential\nof deep learning. By employing deep learning, we construct problem-specific\nheuristics that identify and exploit common structures across MIP instances. We\ntrain deep learning models to estimate complicating binary variables for target\nMIP problem instances. The resulting reduced MIP models are solved using\nstandard off-the-shelf solvers. We present an algorithm for generating\nsynthetic data enhancing the robustness and generalizability of our models\nacross diverse MIP instances. We compare the effectiveness of (a) feed-forward\nneural networks (ANN) and (b) convolutional neural networks (CNN). To enhance\nthe framework's performance, we employ Bayesian optimization for hyperparameter\ntuning, aiming to maximize the occurrence of global optimum solutions. We apply\nthis framework to a flow-based facility location allocation MIP formulation\nthat describes long-term investment planning and medium-term tactical\nscheduling in a personalized medicine supply chain.\n", "link": "http://arxiv.org/abs/2401.09556v2", "date": "2024-05-10", "relevancy": 2.1585, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5712}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5291}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20enhanced%20mixed%20integer%20optimization%3A%20Learning%20to%20reduce%0A%20%20model%20dimensionality&body=Title%3A%20Deep%20learning%20enhanced%20mixed%20integer%20optimization%3A%20Learning%20to%20reduce%0A%20%20model%20dimensionality%0AAuthor%3A%20Niki%20Triantafyllou%20and%20Maria%20M.%20Papathanasiou%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20framework%20to%20address%20the%20computational%20complexity%0Ainherent%20in%20Mixed-Integer%20Programming%20%28MIP%29%20models%20by%20harnessing%20the%20potential%0Aof%20deep%20learning.%20By%20employing%20deep%20learning%2C%20we%20construct%20problem-specific%0Aheuristics%20that%20identify%20and%20exploit%20common%20structures%20across%20MIP%20instances.%20We%0Atrain%20deep%20learning%20models%20to%20estimate%20complicating%20binary%20variables%20for%20target%0AMIP%20problem%20instances.%20The%20resulting%20reduced%20MIP%20models%20are%20solved%20using%0Astandard%20off-the-shelf%20solvers.%20We%20present%20an%20algorithm%20for%20generating%0Asynthetic%20data%20enhancing%20the%20robustness%20and%20generalizability%20of%20our%20models%0Aacross%20diverse%20MIP%20instances.%20We%20compare%20the%20effectiveness%20of%20%28a%29%20feed-forward%0Aneural%20networks%20%28ANN%29%20and%20%28b%29%20convolutional%20neural%20networks%20%28CNN%29.%20To%20enhance%0Athe%20framework%27s%20performance%2C%20we%20employ%20Bayesian%20optimization%20for%20hyperparameter%0Atuning%2C%20aiming%20to%20maximize%20the%20occurrence%20of%20global%20optimum%20solutions.%20We%20apply%0Athis%20framework%20to%20a%20flow-based%20facility%20location%20allocation%20MIP%20formulation%0Athat%20describes%20long-term%20investment%20planning%20and%20medium-term%20tactical%0Ascheduling%20in%20a%20personalized%20medicine%20supply%20chain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520enhanced%2520mixed%2520integer%2520optimization%253A%2520Learning%2520to%2520reduce%250A%2520%2520model%2520dimensionality%26entry.906535625%3DNiki%2520Triantafyllou%2520and%2520Maria%2520M.%2520Papathanasiou%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520framework%2520to%2520address%2520the%2520computational%2520complexity%250Ainherent%2520in%2520Mixed-Integer%2520Programming%2520%2528MIP%2529%2520models%2520by%2520harnessing%2520the%2520potential%250Aof%2520deep%2520learning.%2520By%2520employing%2520deep%2520learning%252C%2520we%2520construct%2520problem-specific%250Aheuristics%2520that%2520identify%2520and%2520exploit%2520common%2520structures%2520across%2520MIP%2520instances.%2520We%250Atrain%2520deep%2520learning%2520models%2520to%2520estimate%2520complicating%2520binary%2520variables%2520for%2520target%250AMIP%2520problem%2520instances.%2520The%2520resulting%2520reduced%2520MIP%2520models%2520are%2520solved%2520using%250Astandard%2520off-the-shelf%2520solvers.%2520We%2520present%2520an%2520algorithm%2520for%2520generating%250Asynthetic%2520data%2520enhancing%2520the%2520robustness%2520and%2520generalizability%2520of%2520our%2520models%250Aacross%2520diverse%2520MIP%2520instances.%2520We%2520compare%2520the%2520effectiveness%2520of%2520%2528a%2529%2520feed-forward%250Aneural%2520networks%2520%2528ANN%2529%2520and%2520%2528b%2529%2520convolutional%2520neural%2520networks%2520%2528CNN%2529.%2520To%2520enhance%250Athe%2520framework%2527s%2520performance%252C%2520we%2520employ%2520Bayesian%2520optimization%2520for%2520hyperparameter%250Atuning%252C%2520aiming%2520to%2520maximize%2520the%2520occurrence%2520of%2520global%2520optimum%2520solutions.%2520We%2520apply%250Athis%2520framework%2520to%2520a%2520flow-based%2520facility%2520location%2520allocation%2520MIP%2520formulation%250Athat%2520describes%2520long-term%2520investment%2520planning%2520and%2520medium-term%2520tactical%250Ascheduling%2520in%2520a%2520personalized%2520medicine%2520supply%2520chain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20enhanced%20mixed%20integer%20optimization%3A%20Learning%20to%20reduce%0A%20%20model%20dimensionality&entry.906535625=Niki%20Triantafyllou%20and%20Maria%20M.%20Papathanasiou&entry.1292438233=%20%20This%20work%20introduces%20a%20framework%20to%20address%20the%20computational%20complexity%0Ainherent%20in%20Mixed-Integer%20Programming%20%28MIP%29%20models%20by%20harnessing%20the%20potential%0Aof%20deep%20learning.%20By%20employing%20deep%20learning%2C%20we%20construct%20problem-specific%0Aheuristics%20that%20identify%20and%20exploit%20common%20structures%20across%20MIP%20instances.%20We%0Atrain%20deep%20learning%20models%20to%20estimate%20complicating%20binary%20variables%20for%20target%0AMIP%20problem%20instances.%20The%20resulting%20reduced%20MIP%20models%20are%20solved%20using%0Astandard%20off-the-shelf%20solvers.%20We%20present%20an%20algorithm%20for%20generating%0Asynthetic%20data%20enhancing%20the%20robustness%20and%20generalizability%20of%20our%20models%0Aacross%20diverse%20MIP%20instances.%20We%20compare%20the%20effectiveness%20of%20%28a%29%20feed-forward%0Aneural%20networks%20%28ANN%29%20and%20%28b%29%20convolutional%20neural%20networks%20%28CNN%29.%20To%20enhance%0Athe%20framework%27s%20performance%2C%20we%20employ%20Bayesian%20optimization%20for%20hyperparameter%0Atuning%2C%20aiming%20to%20maximize%20the%20occurrence%20of%20global%20optimum%20solutions.%20We%20apply%0Athis%20framework%20to%20a%20flow-based%20facility%20location%20allocation%20MIP%20formulation%0Athat%20describes%20long-term%20investment%20planning%20and%20medium-term%20tactical%0Ascheduling%20in%20a%20personalized%20medicine%20supply%20chain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09556v2&entry.124074799=Read"},
{"title": "Taylor Videos for Action Recognition", "author": "Lei Wang and Xiuyuan Yuan and Tom Gedeon and Liang Zheng", "abstract": "  Effectively extracting motions from video is a critical and long-standing\nproblem for action recognition. This problem is very challenging because\nmotions (i) do not have an explicit form, (ii) have various concepts such as\ndisplacement, velocity, and acceleration, and (iii) often contain noise caused\nby unstable pixels. Addressing these challenges, we propose the Taylor video, a\nnew video format that highlights the dominate motions (e.g., a waving hand) in\neach of its frames named the Taylor frame. Taylor video is named after Taylor\nseries, which approximates a function at a given point using important terms.\nIn the scenario of videos, we define an implicit motion-extraction function\nwhich aims to extract motions from video temporal block. In this block, using\nthe frames, the difference frames, and higher-order difference frames, we\nperform Taylor expansion to approximate this function at the starting frame. We\nshow the summation of the higher-order terms in the Taylor series gives us\ndominant motion patterns, where static objects, small and unstable motions are\nremoved. Experimentally we show that Taylor videos are effective inputs to\npopular architectures including 2D CNNs, 3D CNNs, and transformers. When used\nindividually, Taylor videos yield competitive action recognition accuracy\ncompared to RGB videos and optical flow. When fused with RGB or optical flow\nvideos, further accuracy improvement is achieved. Additionally, we apply Taylor\nvideo computation to human skeleton sequences, resulting in Taylor skeleton\nsequences that outperform the use of original skeletons for skeleton-based\naction recognition.\n", "link": "http://arxiv.org/abs/2402.03019v4", "date": "2024-05-10", "relevancy": 2.1192, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.609}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5236}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taylor%20Videos%20for%20Action%20Recognition&body=Title%3A%20Taylor%20Videos%20for%20Action%20Recognition%0AAuthor%3A%20Lei%20Wang%20and%20Xiuyuan%20Yuan%20and%20Tom%20Gedeon%20and%20Liang%20Zheng%0AAbstract%3A%20%20%20Effectively%20extracting%20motions%20from%20video%20is%20a%20critical%20and%20long-standing%0Aproblem%20for%20action%20recognition.%20This%20problem%20is%20very%20challenging%20because%0Amotions%20%28i%29%20do%20not%20have%20an%20explicit%20form%2C%20%28ii%29%20have%20various%20concepts%20such%20as%0Adisplacement%2C%20velocity%2C%20and%20acceleration%2C%20and%20%28iii%29%20often%20contain%20noise%20caused%0Aby%20unstable%20pixels.%20Addressing%20these%20challenges%2C%20we%20propose%20the%20Taylor%20video%2C%20a%0Anew%20video%20format%20that%20highlights%20the%20dominate%20motions%20%28e.g.%2C%20a%20waving%20hand%29%20in%0Aeach%20of%20its%20frames%20named%20the%20Taylor%20frame.%20Taylor%20video%20is%20named%20after%20Taylor%0Aseries%2C%20which%20approximates%20a%20function%20at%20a%20given%20point%20using%20important%20terms.%0AIn%20the%20scenario%20of%20videos%2C%20we%20define%20an%20implicit%20motion-extraction%20function%0Awhich%20aims%20to%20extract%20motions%20from%20video%20temporal%20block.%20In%20this%20block%2C%20using%0Athe%20frames%2C%20the%20difference%20frames%2C%20and%20higher-order%20difference%20frames%2C%20we%0Aperform%20Taylor%20expansion%20to%20approximate%20this%20function%20at%20the%20starting%20frame.%20We%0Ashow%20the%20summation%20of%20the%20higher-order%20terms%20in%20the%20Taylor%20series%20gives%20us%0Adominant%20motion%20patterns%2C%20where%20static%20objects%2C%20small%20and%20unstable%20motions%20are%0Aremoved.%20Experimentally%20we%20show%20that%20Taylor%20videos%20are%20effective%20inputs%20to%0Apopular%20architectures%20including%202D%20CNNs%2C%203D%20CNNs%2C%20and%20transformers.%20When%20used%0Aindividually%2C%20Taylor%20videos%20yield%20competitive%20action%20recognition%20accuracy%0Acompared%20to%20RGB%20videos%20and%20optical%20flow.%20When%20fused%20with%20RGB%20or%20optical%20flow%0Avideos%2C%20further%20accuracy%20improvement%20is%20achieved.%20Additionally%2C%20we%20apply%20Taylor%0Avideo%20computation%20to%20human%20skeleton%20sequences%2C%20resulting%20in%20Taylor%20skeleton%0Asequences%20that%20outperform%20the%20use%20of%20original%20skeletons%20for%20skeleton-based%0Aaction%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03019v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaylor%2520Videos%2520for%2520Action%2520Recognition%26entry.906535625%3DLei%2520Wang%2520and%2520Xiuyuan%2520Yuan%2520and%2520Tom%2520Gedeon%2520and%2520Liang%2520Zheng%26entry.1292438233%3D%2520%2520Effectively%2520extracting%2520motions%2520from%2520video%2520is%2520a%2520critical%2520and%2520long-standing%250Aproblem%2520for%2520action%2520recognition.%2520This%2520problem%2520is%2520very%2520challenging%2520because%250Amotions%2520%2528i%2529%2520do%2520not%2520have%2520an%2520explicit%2520form%252C%2520%2528ii%2529%2520have%2520various%2520concepts%2520such%2520as%250Adisplacement%252C%2520velocity%252C%2520and%2520acceleration%252C%2520and%2520%2528iii%2529%2520often%2520contain%2520noise%2520caused%250Aby%2520unstable%2520pixels.%2520Addressing%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Taylor%2520video%252C%2520a%250Anew%2520video%2520format%2520that%2520highlights%2520the%2520dominate%2520motions%2520%2528e.g.%252C%2520a%2520waving%2520hand%2529%2520in%250Aeach%2520of%2520its%2520frames%2520named%2520the%2520Taylor%2520frame.%2520Taylor%2520video%2520is%2520named%2520after%2520Taylor%250Aseries%252C%2520which%2520approximates%2520a%2520function%2520at%2520a%2520given%2520point%2520using%2520important%2520terms.%250AIn%2520the%2520scenario%2520of%2520videos%252C%2520we%2520define%2520an%2520implicit%2520motion-extraction%2520function%250Awhich%2520aims%2520to%2520extract%2520motions%2520from%2520video%2520temporal%2520block.%2520In%2520this%2520block%252C%2520using%250Athe%2520frames%252C%2520the%2520difference%2520frames%252C%2520and%2520higher-order%2520difference%2520frames%252C%2520we%250Aperform%2520Taylor%2520expansion%2520to%2520approximate%2520this%2520function%2520at%2520the%2520starting%2520frame.%2520We%250Ashow%2520the%2520summation%2520of%2520the%2520higher-order%2520terms%2520in%2520the%2520Taylor%2520series%2520gives%2520us%250Adominant%2520motion%2520patterns%252C%2520where%2520static%2520objects%252C%2520small%2520and%2520unstable%2520motions%2520are%250Aremoved.%2520Experimentally%2520we%2520show%2520that%2520Taylor%2520videos%2520are%2520effective%2520inputs%2520to%250Apopular%2520architectures%2520including%25202D%2520CNNs%252C%25203D%2520CNNs%252C%2520and%2520transformers.%2520When%2520used%250Aindividually%252C%2520Taylor%2520videos%2520yield%2520competitive%2520action%2520recognition%2520accuracy%250Acompared%2520to%2520RGB%2520videos%2520and%2520optical%2520flow.%2520When%2520fused%2520with%2520RGB%2520or%2520optical%2520flow%250Avideos%252C%2520further%2520accuracy%2520improvement%2520is%2520achieved.%2520Additionally%252C%2520we%2520apply%2520Taylor%250Avideo%2520computation%2520to%2520human%2520skeleton%2520sequences%252C%2520resulting%2520in%2520Taylor%2520skeleton%250Asequences%2520that%2520outperform%2520the%2520use%2520of%2520original%2520skeletons%2520for%2520skeleton-based%250Aaction%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03019v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taylor%20Videos%20for%20Action%20Recognition&entry.906535625=Lei%20Wang%20and%20Xiuyuan%20Yuan%20and%20Tom%20Gedeon%20and%20Liang%20Zheng&entry.1292438233=%20%20Effectively%20extracting%20motions%20from%20video%20is%20a%20critical%20and%20long-standing%0Aproblem%20for%20action%20recognition.%20This%20problem%20is%20very%20challenging%20because%0Amotions%20%28i%29%20do%20not%20have%20an%20explicit%20form%2C%20%28ii%29%20have%20various%20concepts%20such%20as%0Adisplacement%2C%20velocity%2C%20and%20acceleration%2C%20and%20%28iii%29%20often%20contain%20noise%20caused%0Aby%20unstable%20pixels.%20Addressing%20these%20challenges%2C%20we%20propose%20the%20Taylor%20video%2C%20a%0Anew%20video%20format%20that%20highlights%20the%20dominate%20motions%20%28e.g.%2C%20a%20waving%20hand%29%20in%0Aeach%20of%20its%20frames%20named%20the%20Taylor%20frame.%20Taylor%20video%20is%20named%20after%20Taylor%0Aseries%2C%20which%20approximates%20a%20function%20at%20a%20given%20point%20using%20important%20terms.%0AIn%20the%20scenario%20of%20videos%2C%20we%20define%20an%20implicit%20motion-extraction%20function%0Awhich%20aims%20to%20extract%20motions%20from%20video%20temporal%20block.%20In%20this%20block%2C%20using%0Athe%20frames%2C%20the%20difference%20frames%2C%20and%20higher-order%20difference%20frames%2C%20we%0Aperform%20Taylor%20expansion%20to%20approximate%20this%20function%20at%20the%20starting%20frame.%20We%0Ashow%20the%20summation%20of%20the%20higher-order%20terms%20in%20the%20Taylor%20series%20gives%20us%0Adominant%20motion%20patterns%2C%20where%20static%20objects%2C%20small%20and%20unstable%20motions%20are%0Aremoved.%20Experimentally%20we%20show%20that%20Taylor%20videos%20are%20effective%20inputs%20to%0Apopular%20architectures%20including%202D%20CNNs%2C%203D%20CNNs%2C%20and%20transformers.%20When%20used%0Aindividually%2C%20Taylor%20videos%20yield%20competitive%20action%20recognition%20accuracy%0Acompared%20to%20RGB%20videos%20and%20optical%20flow.%20When%20fused%20with%20RGB%20or%20optical%20flow%0Avideos%2C%20further%20accuracy%20improvement%20is%20achieved.%20Additionally%2C%20we%20apply%20Taylor%0Avideo%20computation%20to%20human%20skeleton%20sequences%2C%20resulting%20in%20Taylor%20skeleton%0Asequences%20that%20outperform%20the%20use%20of%20original%20skeletons%20for%20skeleton-based%0Aaction%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03019v4&entry.124074799=Read"},
{"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large\n  Language Models", "author": "Jingyang Zhang and Jingwei Sun and Eric Yeats and Yang Ouyang and Martin Kuo and Jianyi Zhang and Hao Frank Yang and Hai Li", "abstract": "  The problem of pre-training data detection for large language models (LLMs)\nhas received growing attention due to its implications in critical issues like\ncopyright violation and test data contamination. A common intuition for this\nproblem is to identify training data by checking if the input comes from a mode\nof the LLM's distribution. However, existing approaches, including the\nstate-of-the-art Min-K%, often use zeroth-order signals for detection, which\nare less robust in determining local maxima than second-order statistics. In\nthis work, we propose a novel methodology Min-K%++ for pre-training data\ndetection that measures how sharply peaked the likelihood is around the input,\na measurement analogous to the curvature of continuous distribution. Our method\nis theoretically motivated by the observation that maximum likelihood training\nimplicitly optimizes the trace of the Hessian matrix of likelihood through\nscore matching. Empirically, the proposed method achieves new SOTA performance\nacross multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the\nrunner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the\nmore challenging MIMIR benchmark, it consistently improves upon reference-free\nmethods while performing on par with reference-based method that requires an\nextra reference model.\n", "link": "http://arxiv.org/abs/2404.02936v2", "date": "2024-05-10", "relevancy": 2.0861, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5284}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Min-K%25%2B%2B%3A%20Improved%20Baseline%20for%20Detecting%20Pre-Training%20Data%20from%20Large%0A%20%20Language%20Models&body=Title%3A%20Min-K%25%2B%2B%3A%20Improved%20Baseline%20for%20Detecting%20Pre-Training%20Data%20from%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Jingyang%20Zhang%20and%20Jingwei%20Sun%20and%20Eric%20Yeats%20and%20Yang%20Ouyang%20and%20Martin%20Kuo%20and%20Jianyi%20Zhang%20and%20Hao%20Frank%20Yang%20and%20Hai%20Li%0AAbstract%3A%20%20%20The%20problem%20of%20pre-training%20data%20detection%20for%20large%20language%20models%20%28LLMs%29%0Ahas%20received%20growing%20attention%20due%20to%20its%20implications%20in%20critical%20issues%20like%0Acopyright%20violation%20and%20test%20data%20contamination.%20A%20common%20intuition%20for%20this%0Aproblem%20is%20to%20identify%20training%20data%20by%20checking%20if%20the%20input%20comes%20from%20a%20mode%0Aof%20the%20LLM%27s%20distribution.%20However%2C%20existing%20approaches%2C%20including%20the%0Astate-of-the-art%20Min-K%25%2C%20often%20use%20zeroth-order%20signals%20for%20detection%2C%20which%0Aare%20less%20robust%20in%20determining%20local%20maxima%20than%20second-order%20statistics.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20methodology%20Min-K%25%2B%2B%20for%20pre-training%20data%0Adetection%20that%20measures%20how%20sharply%20peaked%20the%20likelihood%20is%20around%20the%20input%2C%0Aa%20measurement%20analogous%20to%20the%20curvature%20of%20continuous%20distribution.%20Our%20method%0Ais%20theoretically%20motivated%20by%20the%20observation%20that%20maximum%20likelihood%20training%0Aimplicitly%20optimizes%20the%20trace%20of%20the%20Hessian%20matrix%20of%20likelihood%20through%0Ascore%20matching.%20Empirically%2C%20the%20proposed%20method%20achieves%20new%20SOTA%20performance%0Aacross%20multiple%20settings.%20On%20the%20WikiMIA%20benchmark%2C%20Min-K%25%2B%2B%20outperforms%20the%0Arunner-up%20by%206.2%25%20to%2010.5%25%20in%20detection%20AUROC%20averaged%20over%20five%20models.%20On%20the%0Amore%20challenging%20MIMIR%20benchmark%2C%20it%20consistently%20improves%20upon%20reference-free%0Amethods%20while%20performing%20on%20par%20with%20reference-based%20method%20that%20requires%20an%0Aextra%20reference%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMin-K%2525%252B%252B%253A%2520Improved%2520Baseline%2520for%2520Detecting%2520Pre-Training%2520Data%2520from%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DJingyang%2520Zhang%2520and%2520Jingwei%2520Sun%2520and%2520Eric%2520Yeats%2520and%2520Yang%2520Ouyang%2520and%2520Martin%2520Kuo%2520and%2520Jianyi%2520Zhang%2520and%2520Hao%2520Frank%2520Yang%2520and%2520Hai%2520Li%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520pre-training%2520data%2520detection%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%250Ahas%2520received%2520growing%2520attention%2520due%2520to%2520its%2520implications%2520in%2520critical%2520issues%2520like%250Acopyright%2520violation%2520and%2520test%2520data%2520contamination.%2520A%2520common%2520intuition%2520for%2520this%250Aproblem%2520is%2520to%2520identify%2520training%2520data%2520by%2520checking%2520if%2520the%2520input%2520comes%2520from%2520a%2520mode%250Aof%2520the%2520LLM%2527s%2520distribution.%2520However%252C%2520existing%2520approaches%252C%2520including%2520the%250Astate-of-the-art%2520Min-K%2525%252C%2520often%2520use%2520zeroth-order%2520signals%2520for%2520detection%252C%2520which%250Aare%2520less%2520robust%2520in%2520determining%2520local%2520maxima%2520than%2520second-order%2520statistics.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520methodology%2520Min-K%2525%252B%252B%2520for%2520pre-training%2520data%250Adetection%2520that%2520measures%2520how%2520sharply%2520peaked%2520the%2520likelihood%2520is%2520around%2520the%2520input%252C%250Aa%2520measurement%2520analogous%2520to%2520the%2520curvature%2520of%2520continuous%2520distribution.%2520Our%2520method%250Ais%2520theoretically%2520motivated%2520by%2520the%2520observation%2520that%2520maximum%2520likelihood%2520training%250Aimplicitly%2520optimizes%2520the%2520trace%2520of%2520the%2520Hessian%2520matrix%2520of%2520likelihood%2520through%250Ascore%2520matching.%2520Empirically%252C%2520the%2520proposed%2520method%2520achieves%2520new%2520SOTA%2520performance%250Aacross%2520multiple%2520settings.%2520On%2520the%2520WikiMIA%2520benchmark%252C%2520Min-K%2525%252B%252B%2520outperforms%2520the%250Arunner-up%2520by%25206.2%2525%2520to%252010.5%2525%2520in%2520detection%2520AUROC%2520averaged%2520over%2520five%2520models.%2520On%2520the%250Amore%2520challenging%2520MIMIR%2520benchmark%252C%2520it%2520consistently%2520improves%2520upon%2520reference-free%250Amethods%2520while%2520performing%2520on%2520par%2520with%2520reference-based%2520method%2520that%2520requires%2520an%250Aextra%2520reference%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Min-K%25%2B%2B%3A%20Improved%20Baseline%20for%20Detecting%20Pre-Training%20Data%20from%20Large%0A%20%20Language%20Models&entry.906535625=Jingyang%20Zhang%20and%20Jingwei%20Sun%20and%20Eric%20Yeats%20and%20Yang%20Ouyang%20and%20Martin%20Kuo%20and%20Jianyi%20Zhang%20and%20Hao%20Frank%20Yang%20and%20Hai%20Li&entry.1292438233=%20%20The%20problem%20of%20pre-training%20data%20detection%20for%20large%20language%20models%20%28LLMs%29%0Ahas%20received%20growing%20attention%20due%20to%20its%20implications%20in%20critical%20issues%20like%0Acopyright%20violation%20and%20test%20data%20contamination.%20A%20common%20intuition%20for%20this%0Aproblem%20is%20to%20identify%20training%20data%20by%20checking%20if%20the%20input%20comes%20from%20a%20mode%0Aof%20the%20LLM%27s%20distribution.%20However%2C%20existing%20approaches%2C%20including%20the%0Astate-of-the-art%20Min-K%25%2C%20often%20use%20zeroth-order%20signals%20for%20detection%2C%20which%0Aare%20less%20robust%20in%20determining%20local%20maxima%20than%20second-order%20statistics.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20methodology%20Min-K%25%2B%2B%20for%20pre-training%20data%0Adetection%20that%20measures%20how%20sharply%20peaked%20the%20likelihood%20is%20around%20the%20input%2C%0Aa%20measurement%20analogous%20to%20the%20curvature%20of%20continuous%20distribution.%20Our%20method%0Ais%20theoretically%20motivated%20by%20the%20observation%20that%20maximum%20likelihood%20training%0Aimplicitly%20optimizes%20the%20trace%20of%20the%20Hessian%20matrix%20of%20likelihood%20through%0Ascore%20matching.%20Empirically%2C%20the%20proposed%20method%20achieves%20new%20SOTA%20performance%0Aacross%20multiple%20settings.%20On%20the%20WikiMIA%20benchmark%2C%20Min-K%25%2B%2B%20outperforms%20the%0Arunner-up%20by%206.2%25%20to%2010.5%25%20in%20detection%20AUROC%20averaged%20over%20five%20models.%20On%20the%0Amore%20challenging%20MIMIR%20benchmark%2C%20it%20consistently%20improves%20upon%20reference-free%0Amethods%20while%20performing%20on%20par%20with%20reference-based%20method%20that%20requires%20an%0Aextra%20reference%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02936v2&entry.124074799=Read"},
{"title": "Pseudo-Prompt Generating in Pre-trained Vision-Language Models for\n  Multi-Label Medical Image Classification", "author": "Yaoqin Ye and Junjie Zhang and Hongwei Shi", "abstract": "  The task of medical image recognition is notably complicated by the presence\nof varied and multiple pathological indications, presenting a unique challenge\nin multi-label classification with unseen labels. This complexity underlines\nthe need for computer-aided diagnosis methods employing multi-label zero-shot\nlearning. Recent advancements in pre-trained vision-language models (VLMs) have\nshowcased notable zero-shot classification abilities on medical images.\nHowever, these methods have limitations on leveraging extensive pre-trained\nknowledge from broader image datasets, and often depend on manual prompt\nconstruction by expert radiologists. By automating the process of prompt\ntuning, prompt learning techniques have emerged as an efficient way to adapt\nVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in\nperforming class-specific prompts on unseen categories, limiting\ngeneralizability in fine-grained scenarios. To overcome these constraints, we\nintroduce a novel prompt generation approach inspirited by text generation in\nnatural language processing (NLP). Our method, named Pseudo-Prompt Generating\n(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring\na RNN-based decoder, PsPG autoregressively generates class-tailored embedding\nvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label\nchest radiograph datasets affirm the superiority of our approach against\nleading medical vision-language and multi-label prompt learning methods. The\nsource code is available at https://github.com/fallingnight/PsPG\n", "link": "http://arxiv.org/abs/2405.06468v1", "date": "2024-05-10", "relevancy": 2.0724, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5234}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5171}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&body=Title%3A%20Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification%0AAuthor%3A%20Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi%0AAbstract%3A%20%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Prompt%2520Generating%2520in%2520Pre-trained%2520Vision-Language%2520Models%2520for%250A%2520%2520Multi-Label%2520Medical%2520Image%2520Classification%26entry.906535625%3DYaoqin%2520Ye%2520and%2520Junjie%2520Zhang%2520and%2520Hongwei%2520Shi%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520medical%2520image%2520recognition%2520is%2520notably%2520complicated%2520by%2520the%2520presence%250Aof%2520varied%2520and%2520multiple%2520pathological%2520indications%252C%2520presenting%2520a%2520unique%2520challenge%250Ain%2520multi-label%2520classification%2520with%2520unseen%2520labels.%2520This%2520complexity%2520underlines%250Athe%2520need%2520for%2520computer-aided%2520diagnosis%2520methods%2520employing%2520multi-label%2520zero-shot%250Alearning.%2520Recent%2520advancements%2520in%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%250Ashowcased%2520notable%2520zero-shot%2520classification%2520abilities%2520on%2520medical%2520images.%250AHowever%252C%2520these%2520methods%2520have%2520limitations%2520on%2520leveraging%2520extensive%2520pre-trained%250Aknowledge%2520from%2520broader%2520image%2520datasets%252C%2520and%2520often%2520depend%2520on%2520manual%2520prompt%250Aconstruction%2520by%2520expert%2520radiologists.%2520By%2520automating%2520the%2520process%2520of%2520prompt%250Atuning%252C%2520prompt%2520learning%2520techniques%2520have%2520emerged%2520as%2520an%2520efficient%2520way%2520to%2520adapt%250AVLMs%2520to%2520downstream%2520tasks.%2520Yet%252C%2520existing%2520CoOp-based%2520strategies%2520fall%2520short%2520in%250Aperforming%2520class-specific%2520prompts%2520on%2520unseen%2520categories%252C%2520limiting%250Ageneralizability%2520in%2520fine-grained%2520scenarios.%2520To%2520overcome%2520these%2520constraints%252C%2520we%250Aintroduce%2520a%2520novel%2520prompt%2520generation%2520approach%2520inspirited%2520by%2520text%2520generation%2520in%250Anatural%2520language%2520processing%2520%2528NLP%2529.%2520Our%2520method%252C%2520named%2520Pseudo-Prompt%2520Generating%250A%2528PsPG%2529%252C%2520capitalizes%2520on%2520the%2520priori%2520knowledge%2520of%2520multi-modal%2520features.%2520Featuring%250Aa%2520RNN-based%2520decoder%252C%2520PsPG%2520autoregressively%2520generates%2520class-tailored%2520embedding%250Avectors%252C%2520i.e.%252C%2520pseudo-prompts.%2520Comparative%2520evaluations%2520on%2520various%2520multi-label%250Achest%2520radiograph%2520datasets%2520affirm%2520the%2520superiority%2520of%2520our%2520approach%2520against%250Aleading%2520medical%2520vision-language%2520and%2520multi-label%2520prompt%2520learning%2520methods.%2520The%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/fallingnight/PsPG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Prompt%20Generating%20in%20Pre-trained%20Vision-Language%20Models%20for%0A%20%20Multi-Label%20Medical%20Image%20Classification&entry.906535625=Yaoqin%20Ye%20and%20Junjie%20Zhang%20and%20Hongwei%20Shi&entry.1292438233=%20%20The%20task%20of%20medical%20image%20recognition%20is%20notably%20complicated%20by%20the%20presence%0Aof%20varied%20and%20multiple%20pathological%20indications%2C%20presenting%20a%20unique%20challenge%0Ain%20multi-label%20classification%20with%20unseen%20labels.%20This%20complexity%20underlines%0Athe%20need%20for%20computer-aided%20diagnosis%20methods%20employing%20multi-label%20zero-shot%0Alearning.%20Recent%20advancements%20in%20pre-trained%20vision-language%20models%20%28VLMs%29%20have%0Ashowcased%20notable%20zero-shot%20classification%20abilities%20on%20medical%20images.%0AHowever%2C%20these%20methods%20have%20limitations%20on%20leveraging%20extensive%20pre-trained%0Aknowledge%20from%20broader%20image%20datasets%2C%20and%20often%20depend%20on%20manual%20prompt%0Aconstruction%20by%20expert%20radiologists.%20By%20automating%20the%20process%20of%20prompt%0Atuning%2C%20prompt%20learning%20techniques%20have%20emerged%20as%20an%20efficient%20way%20to%20adapt%0AVLMs%20to%20downstream%20tasks.%20Yet%2C%20existing%20CoOp-based%20strategies%20fall%20short%20in%0Aperforming%20class-specific%20prompts%20on%20unseen%20categories%2C%20limiting%0Ageneralizability%20in%20fine-grained%20scenarios.%20To%20overcome%20these%20constraints%2C%20we%0Aintroduce%20a%20novel%20prompt%20generation%20approach%20inspirited%20by%20text%20generation%20in%0Anatural%20language%20processing%20%28NLP%29.%20Our%20method%2C%20named%20Pseudo-Prompt%20Generating%0A%28PsPG%29%2C%20capitalizes%20on%20the%20priori%20knowledge%20of%20multi-modal%20features.%20Featuring%0Aa%20RNN-based%20decoder%2C%20PsPG%20autoregressively%20generates%20class-tailored%20embedding%0Avectors%2C%20i.e.%2C%20pseudo-prompts.%20Comparative%20evaluations%20on%20various%20multi-label%0Achest%20radiograph%20datasets%20affirm%20the%20superiority%20of%20our%20approach%20against%0Aleading%20medical%20vision-language%20and%20multi-label%20prompt%20learning%20methods.%20The%0Asource%20code%20is%20available%20at%20https%3A//github.com/fallingnight/PsPG%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06468v1&entry.124074799=Read"},
{"title": "Deep video representation learning: a survey", "author": "Elham Ravanbakhsh and Yongqing Liang and J. Ramanujam and Xin Li", "abstract": "  This paper provides a review on representation learning for videos. We\nclassify recent spatiotemporal feature learning methods for sequential visual\ndata and compare their pros and cons for general video analysis. Building\neffective features for videos is a fundamental problem in computer vision tasks\ninvolving video analysis and understanding. Existing features can be generally\ncategorized into spatial and temporal features. Their effectiveness under\nvariations of illumination, occlusion, view and background are discussed.\nFinally, we discuss the remaining challenges in existing deep video\nrepresentation learning studies.\n", "link": "http://arxiv.org/abs/2405.06574v1", "date": "2024-05-10", "relevancy": 2.0624, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20video%20representation%20learning%3A%20a%20survey&body=Title%3A%20Deep%20video%20representation%20learning%3A%20a%20survey%0AAuthor%3A%20Elham%20Ravanbakhsh%20and%20Yongqing%20Liang%20and%20J.%20Ramanujam%20and%20Xin%20Li%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20review%20on%20representation%20learning%20for%20videos.%20We%0Aclassify%20recent%20spatiotemporal%20feature%20learning%20methods%20for%20sequential%20visual%0Adata%20and%20compare%20their%20pros%20and%20cons%20for%20general%20video%20analysis.%20Building%0Aeffective%20features%20for%20videos%20is%20a%20fundamental%20problem%20in%20computer%20vision%20tasks%0Ainvolving%20video%20analysis%20and%20understanding.%20Existing%20features%20can%20be%20generally%0Acategorized%20into%20spatial%20and%20temporal%20features.%20Their%20effectiveness%20under%0Avariations%20of%20illumination%2C%20occlusion%2C%20view%20and%20background%20are%20discussed.%0AFinally%2C%20we%20discuss%20the%20remaining%20challenges%20in%20existing%20deep%20video%0Arepresentation%20learning%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520video%2520representation%2520learning%253A%2520a%2520survey%26entry.906535625%3DElham%2520Ravanbakhsh%2520and%2520Yongqing%2520Liang%2520and%2520J.%2520Ramanujam%2520and%2520Xin%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520review%2520on%2520representation%2520learning%2520for%2520videos.%2520We%250Aclassify%2520recent%2520spatiotemporal%2520feature%2520learning%2520methods%2520for%2520sequential%2520visual%250Adata%2520and%2520compare%2520their%2520pros%2520and%2520cons%2520for%2520general%2520video%2520analysis.%2520Building%250Aeffective%2520features%2520for%2520videos%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%2520tasks%250Ainvolving%2520video%2520analysis%2520and%2520understanding.%2520Existing%2520features%2520can%2520be%2520generally%250Acategorized%2520into%2520spatial%2520and%2520temporal%2520features.%2520Their%2520effectiveness%2520under%250Avariations%2520of%2520illumination%252C%2520occlusion%252C%2520view%2520and%2520background%2520are%2520discussed.%250AFinally%252C%2520we%2520discuss%2520the%2520remaining%2520challenges%2520in%2520existing%2520deep%2520video%250Arepresentation%2520learning%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20video%20representation%20learning%3A%20a%20survey&entry.906535625=Elham%20Ravanbakhsh%20and%20Yongqing%20Liang%20and%20J.%20Ramanujam%20and%20Xin%20Li&entry.1292438233=%20%20This%20paper%20provides%20a%20review%20on%20representation%20learning%20for%20videos.%20We%0Aclassify%20recent%20spatiotemporal%20feature%20learning%20methods%20for%20sequential%20visual%0Adata%20and%20compare%20their%20pros%20and%20cons%20for%20general%20video%20analysis.%20Building%0Aeffective%20features%20for%20videos%20is%20a%20fundamental%20problem%20in%20computer%20vision%20tasks%0Ainvolving%20video%20analysis%20and%20understanding.%20Existing%20features%20can%20be%20generally%0Acategorized%20into%20spatial%20and%20temporal%20features.%20Their%20effectiveness%20under%0Avariations%20of%20illumination%2C%20occlusion%2C%20view%20and%20background%20are%20discussed.%0AFinally%2C%20we%20discuss%20the%20remaining%20challenges%20in%20existing%20deep%20video%0Arepresentation%20learning%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06574v1&entry.124074799=Read"},
{"title": "Video ReCap: Recursive Captioning of Hour-Long Videos", "author": "Md Mohaiminul Islam and Ngan Ho and Xitong Yang and Tushar Nagarajan and Lorenzo Torresani and Gedas Bertasius", "abstract": "  Most video captioning models are designed to process short video clips of few\nseconds and output text describing low-level visual concepts (e.g., objects,\nscenes, atomic actions). However, most real-world videos last for minutes or\nhours and have a complex hierarchical structure spanning different temporal\ngranularities. We propose Video ReCap, a recursive video captioning model that\ncan process video inputs of dramatically different lengths (from 1 second to 2\nhours) and output video captions at multiple hierarchy levels. The recursive\nvideo-language architecture exploits the synergy between different video\nhierarchies and can process hour-long videos efficiently. We utilize a\ncurriculum learning training scheme to learn the hierarchical structure of\nvideos, starting from clip-level captions describing atomic actions, then\nfocusing on segment-level descriptions, and concluding with generating\nsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by\naugmenting Ego4D with 8,267 manually collected long-range video summaries. Our\nrecursive model can flexibly generate captions at different hierarchy levels\nwhile also being useful for other complex video understanding tasks, such as\nVideoQA on EgoSchema. Data, code, and models are available at:\nhttps://sites.google.com/view/vidrecap\n", "link": "http://arxiv.org/abs/2402.13250v5", "date": "2024-05-10", "relevancy": 2.0615, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.527}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5206}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20ReCap%3A%20Recursive%20Captioning%20of%20Hour-Long%20Videos&body=Title%3A%20Video%20ReCap%3A%20Recursive%20Captioning%20of%20Hour-Long%20Videos%0AAuthor%3A%20Md%20Mohaiminul%20Islam%20and%20Ngan%20Ho%20and%20Xitong%20Yang%20and%20Tushar%20Nagarajan%20and%20Lorenzo%20Torresani%20and%20Gedas%20Bertasius%0AAbstract%3A%20%20%20Most%20video%20captioning%20models%20are%20designed%20to%20process%20short%20video%20clips%20of%20few%0Aseconds%20and%20output%20text%20describing%20low-level%20visual%20concepts%20%28e.g.%2C%20objects%2C%0Ascenes%2C%20atomic%20actions%29.%20However%2C%20most%20real-world%20videos%20last%20for%20minutes%20or%0Ahours%20and%20have%20a%20complex%20hierarchical%20structure%20spanning%20different%20temporal%0Agranularities.%20We%20propose%20Video%20ReCap%2C%20a%20recursive%20video%20captioning%20model%20that%0Acan%20process%20video%20inputs%20of%20dramatically%20different%20lengths%20%28from%201%20second%20to%202%0Ahours%29%20and%20output%20video%20captions%20at%20multiple%20hierarchy%20levels.%20The%20recursive%0Avideo-language%20architecture%20exploits%20the%20synergy%20between%20different%20video%0Ahierarchies%20and%20can%20process%20hour-long%20videos%20efficiently.%20We%20utilize%20a%0Acurriculum%20learning%20training%20scheme%20to%20learn%20the%20hierarchical%20structure%20of%0Avideos%2C%20starting%20from%20clip-level%20captions%20describing%20atomic%20actions%2C%20then%0Afocusing%20on%20segment-level%20descriptions%2C%20and%20concluding%20with%20generating%0Asummaries%20for%20hour-long%20videos.%20Furthermore%2C%20we%20introduce%20Ego4D-HCap%20dataset%20by%0Aaugmenting%20Ego4D%20with%208%2C267%20manually%20collected%20long-range%20video%20summaries.%20Our%0Arecursive%20model%20can%20flexibly%20generate%20captions%20at%20different%20hierarchy%20levels%0Awhile%20also%20being%20useful%20for%20other%20complex%20video%20understanding%20tasks%2C%20such%20as%0AVideoQA%20on%20EgoSchema.%20Data%2C%20code%2C%20and%20models%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/vidrecap%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13250v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520ReCap%253A%2520Recursive%2520Captioning%2520of%2520Hour-Long%2520Videos%26entry.906535625%3DMd%2520Mohaiminul%2520Islam%2520and%2520Ngan%2520Ho%2520and%2520Xitong%2520Yang%2520and%2520Tushar%2520Nagarajan%2520and%2520Lorenzo%2520Torresani%2520and%2520Gedas%2520Bertasius%26entry.1292438233%3D%2520%2520Most%2520video%2520captioning%2520models%2520are%2520designed%2520to%2520process%2520short%2520video%2520clips%2520of%2520few%250Aseconds%2520and%2520output%2520text%2520describing%2520low-level%2520visual%2520concepts%2520%2528e.g.%252C%2520objects%252C%250Ascenes%252C%2520atomic%2520actions%2529.%2520However%252C%2520most%2520real-world%2520videos%2520last%2520for%2520minutes%2520or%250Ahours%2520and%2520have%2520a%2520complex%2520hierarchical%2520structure%2520spanning%2520different%2520temporal%250Agranularities.%2520We%2520propose%2520Video%2520ReCap%252C%2520a%2520recursive%2520video%2520captioning%2520model%2520that%250Acan%2520process%2520video%2520inputs%2520of%2520dramatically%2520different%2520lengths%2520%2528from%25201%2520second%2520to%25202%250Ahours%2529%2520and%2520output%2520video%2520captions%2520at%2520multiple%2520hierarchy%2520levels.%2520The%2520recursive%250Avideo-language%2520architecture%2520exploits%2520the%2520synergy%2520between%2520different%2520video%250Ahierarchies%2520and%2520can%2520process%2520hour-long%2520videos%2520efficiently.%2520We%2520utilize%2520a%250Acurriculum%2520learning%2520training%2520scheme%2520to%2520learn%2520the%2520hierarchical%2520structure%2520of%250Avideos%252C%2520starting%2520from%2520clip-level%2520captions%2520describing%2520atomic%2520actions%252C%2520then%250Afocusing%2520on%2520segment-level%2520descriptions%252C%2520and%2520concluding%2520with%2520generating%250Asummaries%2520for%2520hour-long%2520videos.%2520Furthermore%252C%2520we%2520introduce%2520Ego4D-HCap%2520dataset%2520by%250Aaugmenting%2520Ego4D%2520with%25208%252C267%2520manually%2520collected%2520long-range%2520video%2520summaries.%2520Our%250Arecursive%2520model%2520can%2520flexibly%2520generate%2520captions%2520at%2520different%2520hierarchy%2520levels%250Awhile%2520also%2520being%2520useful%2520for%2520other%2520complex%2520video%2520understanding%2520tasks%252C%2520such%2520as%250AVideoQA%2520on%2520EgoSchema.%2520Data%252C%2520code%252C%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//sites.google.com/view/vidrecap%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13250v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20ReCap%3A%20Recursive%20Captioning%20of%20Hour-Long%20Videos&entry.906535625=Md%20Mohaiminul%20Islam%20and%20Ngan%20Ho%20and%20Xitong%20Yang%20and%20Tushar%20Nagarajan%20and%20Lorenzo%20Torresani%20and%20Gedas%20Bertasius&entry.1292438233=%20%20Most%20video%20captioning%20models%20are%20designed%20to%20process%20short%20video%20clips%20of%20few%0Aseconds%20and%20output%20text%20describing%20low-level%20visual%20concepts%20%28e.g.%2C%20objects%2C%0Ascenes%2C%20atomic%20actions%29.%20However%2C%20most%20real-world%20videos%20last%20for%20minutes%20or%0Ahours%20and%20have%20a%20complex%20hierarchical%20structure%20spanning%20different%20temporal%0Agranularities.%20We%20propose%20Video%20ReCap%2C%20a%20recursive%20video%20captioning%20model%20that%0Acan%20process%20video%20inputs%20of%20dramatically%20different%20lengths%20%28from%201%20second%20to%202%0Ahours%29%20and%20output%20video%20captions%20at%20multiple%20hierarchy%20levels.%20The%20recursive%0Avideo-language%20architecture%20exploits%20the%20synergy%20between%20different%20video%0Ahierarchies%20and%20can%20process%20hour-long%20videos%20efficiently.%20We%20utilize%20a%0Acurriculum%20learning%20training%20scheme%20to%20learn%20the%20hierarchical%20structure%20of%0Avideos%2C%20starting%20from%20clip-level%20captions%20describing%20atomic%20actions%2C%20then%0Afocusing%20on%20segment-level%20descriptions%2C%20and%20concluding%20with%20generating%0Asummaries%20for%20hour-long%20videos.%20Furthermore%2C%20we%20introduce%20Ego4D-HCap%20dataset%20by%0Aaugmenting%20Ego4D%20with%208%2C267%20manually%20collected%20long-range%20video%20summaries.%20Our%0Arecursive%20model%20can%20flexibly%20generate%20captions%20at%20different%20hierarchy%20levels%0Awhile%20also%20being%20useful%20for%20other%20complex%20video%20understanding%20tasks%2C%20such%20as%0AVideoQA%20on%20EgoSchema.%20Data%2C%20code%2C%20and%20models%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/vidrecap%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13250v5&entry.124074799=Read"},
{"title": "Value Augmented Sampling for Language Model Alignment and\n  Personalization", "author": "Seungwook Han and Idan Shenfeld and Akash Srivastava and Yoon Kim and Pulkit Agrawal", "abstract": "  Aligning Large Language Models (LLMs) to cater to different human\npreferences, learning new skills, and unlearning harmful behavior is an\nimportant problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree\nSearch, are performant, but impractical for LLM adaptation due to their high\ninference cost. On the other hand, using Reinforcement Learning (RL) for\nadaptation is computationally efficient, but performs worse due to the\noptimization challenges in co-training the value function and the policy. We\npresent a new framework for reward optimization, Value Augmented Sampling\n(VAS), that can maximize different reward functions using data sampled from\nonly the initial, frozen LLM. VAS solves for the optimal reward-maximizing\npolicy without co-training the policy and the value function, making the\noptimization stable, outperforming established baselines, such as PPO and DPO,\non standard benchmarks, and achieving comparable results to Best-of-128 with\nlower inference cost. Unlike existing RL methods that require changing the\nweights of the LLM, VAS does not require access to the weights of the\npre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are\navailable only as APIs. In addition, our algorithm unlocks the new capability\nof composing several rewards and controlling the extent of each one during\ndeployment time, paving the road ahead for the future of aligned, personalized\nLLMs.\n", "link": "http://arxiv.org/abs/2405.06639v1", "date": "2024-05-10", "relevancy": 2.0548, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Value%20Augmented%20Sampling%20for%20Language%20Model%20Alignment%20and%0A%20%20Personalization&body=Title%3A%20Value%20Augmented%20Sampling%20for%20Language%20Model%20Alignment%20and%0A%20%20Personalization%0AAuthor%3A%20Seungwook%20Han%20and%20Idan%20Shenfeld%20and%20Akash%20Srivastava%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20to%20cater%20to%20different%20human%0Apreferences%2C%20learning%20new%20skills%2C%20and%20unlearning%20harmful%20behavior%20is%20an%0Aimportant%20problem.%20Search-based%20methods%2C%20such%20as%20Best-of-N%20or%20Monte-Carlo%20Tree%0ASearch%2C%20are%20performant%2C%20but%20impractical%20for%20LLM%20adaptation%20due%20to%20their%20high%0Ainference%20cost.%20On%20the%20other%20hand%2C%20using%20Reinforcement%20Learning%20%28RL%29%20for%0Aadaptation%20is%20computationally%20efficient%2C%20but%20performs%20worse%20due%20to%20the%0Aoptimization%20challenges%20in%20co-training%20the%20value%20function%20and%20the%20policy.%20We%0Apresent%20a%20new%20framework%20for%20reward%20optimization%2C%20Value%20Augmented%20Sampling%0A%28VAS%29%2C%20that%20can%20maximize%20different%20reward%20functions%20using%20data%20sampled%20from%0Aonly%20the%20initial%2C%20frozen%20LLM.%20VAS%20solves%20for%20the%20optimal%20reward-maximizing%0Apolicy%20without%20co-training%20the%20policy%20and%20the%20value%20function%2C%20making%20the%0Aoptimization%20stable%2C%20outperforming%20established%20baselines%2C%20such%20as%20PPO%20and%20DPO%2C%0Aon%20standard%20benchmarks%2C%20and%20achieving%20comparable%20results%20to%20Best-of-128%20with%0Alower%20inference%20cost.%20Unlike%20existing%20RL%20methods%20that%20require%20changing%20the%0Aweights%20of%20the%20LLM%2C%20VAS%20does%20not%20require%20access%20to%20the%20weights%20of%20the%0Apre-trained%20LLM.%20Thus%2C%20it%20can%20even%20adapt%20LLMs%20%28e.g.%2C%20ChatGPT%29%2C%20which%20are%0Aavailable%20only%20as%20APIs.%20In%20addition%2C%20our%20algorithm%20unlocks%20the%20new%20capability%0Aof%20composing%20several%20rewards%20and%20controlling%20the%20extent%20of%20each%20one%20during%0Adeployment%20time%2C%20paving%20the%20road%20ahead%20for%20the%20future%20of%20aligned%2C%20personalized%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValue%2520Augmented%2520Sampling%2520for%2520Language%2520Model%2520Alignment%2520and%250A%2520%2520Personalization%26entry.906535625%3DSeungwook%2520Han%2520and%2520Idan%2520Shenfeld%2520and%2520Akash%2520Srivastava%2520and%2520Yoon%2520Kim%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Aligning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520cater%2520to%2520different%2520human%250Apreferences%252C%2520learning%2520new%2520skills%252C%2520and%2520unlearning%2520harmful%2520behavior%2520is%2520an%250Aimportant%2520problem.%2520Search-based%2520methods%252C%2520such%2520as%2520Best-of-N%2520or%2520Monte-Carlo%2520Tree%250ASearch%252C%2520are%2520performant%252C%2520but%2520impractical%2520for%2520LLM%2520adaptation%2520due%2520to%2520their%2520high%250Ainference%2520cost.%2520On%2520the%2520other%2520hand%252C%2520using%2520Reinforcement%2520Learning%2520%2528RL%2529%2520for%250Aadaptation%2520is%2520computationally%2520efficient%252C%2520but%2520performs%2520worse%2520due%2520to%2520the%250Aoptimization%2520challenges%2520in%2520co-training%2520the%2520value%2520function%2520and%2520the%2520policy.%2520We%250Apresent%2520a%2520new%2520framework%2520for%2520reward%2520optimization%252C%2520Value%2520Augmented%2520Sampling%250A%2528VAS%2529%252C%2520that%2520can%2520maximize%2520different%2520reward%2520functions%2520using%2520data%2520sampled%2520from%250Aonly%2520the%2520initial%252C%2520frozen%2520LLM.%2520VAS%2520solves%2520for%2520the%2520optimal%2520reward-maximizing%250Apolicy%2520without%2520co-training%2520the%2520policy%2520and%2520the%2520value%2520function%252C%2520making%2520the%250Aoptimization%2520stable%252C%2520outperforming%2520established%2520baselines%252C%2520such%2520as%2520PPO%2520and%2520DPO%252C%250Aon%2520standard%2520benchmarks%252C%2520and%2520achieving%2520comparable%2520results%2520to%2520Best-of-128%2520with%250Alower%2520inference%2520cost.%2520Unlike%2520existing%2520RL%2520methods%2520that%2520require%2520changing%2520the%250Aweights%2520of%2520the%2520LLM%252C%2520VAS%2520does%2520not%2520require%2520access%2520to%2520the%2520weights%2520of%2520the%250Apre-trained%2520LLM.%2520Thus%252C%2520it%2520can%2520even%2520adapt%2520LLMs%2520%2528e.g.%252C%2520ChatGPT%2529%252C%2520which%2520are%250Aavailable%2520only%2520as%2520APIs.%2520In%2520addition%252C%2520our%2520algorithm%2520unlocks%2520the%2520new%2520capability%250Aof%2520composing%2520several%2520rewards%2520and%2520controlling%2520the%2520extent%2520of%2520each%2520one%2520during%250Adeployment%2520time%252C%2520paving%2520the%2520road%2520ahead%2520for%2520the%2520future%2520of%2520aligned%252C%2520personalized%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Value%20Augmented%20Sampling%20for%20Language%20Model%20Alignment%20and%0A%20%20Personalization&entry.906535625=Seungwook%20Han%20and%20Idan%20Shenfeld%20and%20Akash%20Srivastava%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20to%20cater%20to%20different%20human%0Apreferences%2C%20learning%20new%20skills%2C%20and%20unlearning%20harmful%20behavior%20is%20an%0Aimportant%20problem.%20Search-based%20methods%2C%20such%20as%20Best-of-N%20or%20Monte-Carlo%20Tree%0ASearch%2C%20are%20performant%2C%20but%20impractical%20for%20LLM%20adaptation%20due%20to%20their%20high%0Ainference%20cost.%20On%20the%20other%20hand%2C%20using%20Reinforcement%20Learning%20%28RL%29%20for%0Aadaptation%20is%20computationally%20efficient%2C%20but%20performs%20worse%20due%20to%20the%0Aoptimization%20challenges%20in%20co-training%20the%20value%20function%20and%20the%20policy.%20We%0Apresent%20a%20new%20framework%20for%20reward%20optimization%2C%20Value%20Augmented%20Sampling%0A%28VAS%29%2C%20that%20can%20maximize%20different%20reward%20functions%20using%20data%20sampled%20from%0Aonly%20the%20initial%2C%20frozen%20LLM.%20VAS%20solves%20for%20the%20optimal%20reward-maximizing%0Apolicy%20without%20co-training%20the%20policy%20and%20the%20value%20function%2C%20making%20the%0Aoptimization%20stable%2C%20outperforming%20established%20baselines%2C%20such%20as%20PPO%20and%20DPO%2C%0Aon%20standard%20benchmarks%2C%20and%20achieving%20comparable%20results%20to%20Best-of-128%20with%0Alower%20inference%20cost.%20Unlike%20existing%20RL%20methods%20that%20require%20changing%20the%0Aweights%20of%20the%20LLM%2C%20VAS%20does%20not%20require%20access%20to%20the%20weights%20of%20the%0Apre-trained%20LLM.%20Thus%2C%20it%20can%20even%20adapt%20LLMs%20%28e.g.%2C%20ChatGPT%29%2C%20which%20are%0Aavailable%20only%20as%20APIs.%20In%20addition%2C%20our%20algorithm%20unlocks%20the%20new%20capability%0Aof%20composing%20several%20rewards%20and%20controlling%20the%20extent%20of%20each%20one%20during%0Adeployment%20time%2C%20paving%20the%20road%20ahead%20for%20the%20future%20of%20aligned%2C%20personalized%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06639v1&entry.124074799=Read"},
{"title": "Lightweight Inference for Forward-Forward Algorithm", "author": "Amin Aminifar and Baichuan Huang and Azra Abtahi and Amir Aminifar", "abstract": "  The human brain performs tasks with an outstanding energy-efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance.\n", "link": "http://arxiv.org/abs/2404.05241v3", "date": "2024-05-10", "relevancy": 2.0483, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5086}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&body=Title%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm%0AAuthor%3A%20Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar%0AAbstract%3A%20%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy-efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05241v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Inference%2520for%2520Forward-Forward%2520Algorithm%26entry.906535625%3DAmin%2520Aminifar%2520and%2520Baichuan%2520Huang%2520and%2520Azra%2520Abtahi%2520and%2520Amir%2520Aminifar%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520performs%2520tasks%2520with%2520an%2520outstanding%2520energy-efficiency%252C%2520i.e.%252C%250Awith%2520approximately%252020%2520Watts.%2520The%2520state-of-the-art%2520Artificial/Deep%2520Neural%250ANetworks%2520%2528ANN/DNN%2529%252C%2520on%2520the%2520other%2520hand%252C%2520have%2520recently%2520been%2520shown%2520to%2520consume%250Amassive%2520amounts%2520of%2520energy.%2520The%2520training%2520of%2520these%2520ANNs/DNNs%2520is%2520done%2520almost%250Aexclusively%2520based%2520on%2520the%2520back-propagation%2520algorithm%252C%2520which%2520is%2520known%2520to%2520be%250Abiologically%2520implausible.%2520This%2520has%2520led%2520to%2520a%2520new%2520generation%2520of%2520forward-only%250Atechniques%252C%2520including%2520the%2520Forward-Forward%2520algorithm.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520lightweight%2520inference%2520scheme%2520specifically%2520designed%2520for%2520DNNs%2520trained%2520using%2520the%250AForward-Forward%2520algorithm.%2520We%2520have%2520evaluated%2520our%2520proposed%2520lightweight%2520inference%250Ascheme%2520in%2520the%2520case%2520of%2520the%2520MNIST%2520and%2520CIFAR%2520datasets%252C%2520as%2520well%2520as%2520two%2520real-world%250Aapplications%252C%2520namely%252C%2520epileptic%2520seizure%2520detection%2520and%2520cardiac%2520arrhythmia%250Aclassification%2520using%2520wearable%2520technologies%252C%2520where%2520complexity%2520overheads/energy%250Aconsumption%2520is%2520a%2520major%2520constraint%252C%2520and%2520demonstrate%2520its%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05241v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&entry.906535625=Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar&entry.1292438233=%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy-efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05241v3&entry.124074799=Read"},
{"title": "Analyzing the Roles of Language and Vision in Learning from Limited Data", "author": "Allison Chen and Ilia Sucholutsky and Olga Russakovsky and Thomas L. Griffiths", "abstract": "  Does language help make sense of the visual world? How important is it to\nactually see the world rather than having it described with words? These basic\nquestions about the nature of intelligence have been difficult to answer\nbecause we only had one example of an intelligent system -- humans -- and\nlimited access to cases that isolated language or vision. However, the\ndevelopment of sophisticated Vision-Language Models (VLMs) by artificial\nintelligence researchers offers us new opportunities to explore the\ncontributions that language and vision make to learning about the world. We\nablate components from the cognitive architecture of these models to identify\ntheir contributions to learning new tasks from limited data. We find that a\nlanguage model leveraging all components recovers a majority of a VLM's\nperformance, despite its lack of visual input, and that language seems to allow\nthis by providing access to prior knowledge and reasoning.\n", "link": "http://arxiv.org/abs/2403.19669v2", "date": "2024-05-10", "relevancy": 2.0442, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20the%20Roles%20of%20Language%20and%20Vision%20in%20Learning%20from%20Limited%20Data&body=Title%3A%20Analyzing%20the%20Roles%20of%20Language%20and%20Vision%20in%20Learning%20from%20Limited%20Data%0AAuthor%3A%20Allison%20Chen%20and%20Ilia%20Sucholutsky%20and%20Olga%20Russakovsky%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Does%20language%20help%20make%20sense%20of%20the%20visual%20world%3F%20How%20important%20is%20it%20to%0Aactually%20see%20the%20world%20rather%20than%20having%20it%20described%20with%20words%3F%20These%20basic%0Aquestions%20about%20the%20nature%20of%20intelligence%20have%20been%20difficult%20to%20answer%0Abecause%20we%20only%20had%20one%20example%20of%20an%20intelligent%20system%20--%20humans%20--%20and%0Alimited%20access%20to%20cases%20that%20isolated%20language%20or%20vision.%20However%2C%20the%0Adevelopment%20of%20sophisticated%20Vision-Language%20Models%20%28VLMs%29%20by%20artificial%0Aintelligence%20researchers%20offers%20us%20new%20opportunities%20to%20explore%20the%0Acontributions%20that%20language%20and%20vision%20make%20to%20learning%20about%20the%20world.%20We%0Aablate%20components%20from%20the%20cognitive%20architecture%20of%20these%20models%20to%20identify%0Atheir%20contributions%20to%20learning%20new%20tasks%20from%20limited%20data.%20We%20find%20that%20a%0Alanguage%20model%20leveraging%20all%20components%20recovers%20a%20majority%20of%20a%20VLM%27s%0Aperformance%2C%20despite%20its%20lack%20of%20visual%20input%2C%20and%20that%20language%20seems%20to%20allow%0Athis%20by%20providing%20access%20to%20prior%20knowledge%20and%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520the%2520Roles%2520of%2520Language%2520and%2520Vision%2520in%2520Learning%2520from%2520Limited%2520Data%26entry.906535625%3DAllison%2520Chen%2520and%2520Ilia%2520Sucholutsky%2520and%2520Olga%2520Russakovsky%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Does%2520language%2520help%2520make%2520sense%2520of%2520the%2520visual%2520world%253F%2520How%2520important%2520is%2520it%2520to%250Aactually%2520see%2520the%2520world%2520rather%2520than%2520having%2520it%2520described%2520with%2520words%253F%2520These%2520basic%250Aquestions%2520about%2520the%2520nature%2520of%2520intelligence%2520have%2520been%2520difficult%2520to%2520answer%250Abecause%2520we%2520only%2520had%2520one%2520example%2520of%2520an%2520intelligent%2520system%2520--%2520humans%2520--%2520and%250Alimited%2520access%2520to%2520cases%2520that%2520isolated%2520language%2520or%2520vision.%2520However%252C%2520the%250Adevelopment%2520of%2520sophisticated%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520by%2520artificial%250Aintelligence%2520researchers%2520offers%2520us%2520new%2520opportunities%2520to%2520explore%2520the%250Acontributions%2520that%2520language%2520and%2520vision%2520make%2520to%2520learning%2520about%2520the%2520world.%2520We%250Aablate%2520components%2520from%2520the%2520cognitive%2520architecture%2520of%2520these%2520models%2520to%2520identify%250Atheir%2520contributions%2520to%2520learning%2520new%2520tasks%2520from%2520limited%2520data.%2520We%2520find%2520that%2520a%250Alanguage%2520model%2520leveraging%2520all%2520components%2520recovers%2520a%2520majority%2520of%2520a%2520VLM%2527s%250Aperformance%252C%2520despite%2520its%2520lack%2520of%2520visual%2520input%252C%2520and%2520that%2520language%2520seems%2520to%2520allow%250Athis%2520by%2520providing%2520access%2520to%2520prior%2520knowledge%2520and%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20the%20Roles%20of%20Language%20and%20Vision%20in%20Learning%20from%20Limited%20Data&entry.906535625=Allison%20Chen%20and%20Ilia%20Sucholutsky%20and%20Olga%20Russakovsky%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Does%20language%20help%20make%20sense%20of%20the%20visual%20world%3F%20How%20important%20is%20it%20to%0Aactually%20see%20the%20world%20rather%20than%20having%20it%20described%20with%20words%3F%20These%20basic%0Aquestions%20about%20the%20nature%20of%20intelligence%20have%20been%20difficult%20to%20answer%0Abecause%20we%20only%20had%20one%20example%20of%20an%20intelligent%20system%20--%20humans%20--%20and%0Alimited%20access%20to%20cases%20that%20isolated%20language%20or%20vision.%20However%2C%20the%0Adevelopment%20of%20sophisticated%20Vision-Language%20Models%20%28VLMs%29%20by%20artificial%0Aintelligence%20researchers%20offers%20us%20new%20opportunities%20to%20explore%20the%0Acontributions%20that%20language%20and%20vision%20make%20to%20learning%20about%20the%20world.%20We%0Aablate%20components%20from%20the%20cognitive%20architecture%20of%20these%20models%20to%20identify%0Atheir%20contributions%20to%20learning%20new%20tasks%20from%20limited%20data.%20We%20find%20that%20a%0Alanguage%20model%20leveraging%20all%20components%20recovers%20a%20majority%20of%20a%20VLM%27s%0Aperformance%2C%20despite%20its%20lack%20of%20visual%20input%2C%20and%20that%20language%20seems%20to%20allow%0Athis%20by%20providing%20access%20to%20prior%20knowledge%20and%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19669v2&entry.124074799=Read"},
{"title": "Autonomous Driving with a Deep Dual-Model Solution for Steering and\n  Braking Control", "author": "Ana Petra Juki\u0107 and Ana \u0160elek and Marija Seder and Ivana Podnar \u017darko", "abstract": "  The technology of autonomous driving is currently attracting a great deal of\ninterest in both research and industry. In this paper, we present a deep\nlearning dual-model solution that uses two deep neural networks for combined\nbraking and steering in autonomous vehicles. Steering control is achieved by\napplying the NVIDIA's PilotNet model to predict the steering wheel angle, while\nbraking control relies on the use of MobileNet SSD. Both models rely on a\nsingle front-facing camera for image input. The MobileNet SSD model is suitable\nfor devices with constrained resources, whereas PilotNet struggles to operate\nefficiently on smaller devices with limited resources. To make it suitable for\nsuch devices, we modified the PilotNet model using our own original network\ndesign and reduced the number of model parameters and its memory footprint by\napproximately 60%. The inference latency has also been reduced, making the\nmodel more suitable to operate on resource-constrained devices. The modified\nPilotNet model achieves similar loss and accuracy compared to the original\nPilotNet model. When evaluated in a simulated environment, both autonomous\ndriving systems, one using the modified PilotNet model and the other using the\noriginal PilotNet model for steering, show similar levels of autonomous driving\nperformance.\n", "link": "http://arxiv.org/abs/2405.06473v1", "date": "2024-05-10", "relevancy": 2.042, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5187}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5122}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Driving%20with%20a%20Deep%20Dual-Model%20Solution%20for%20Steering%20and%0A%20%20Braking%20Control&body=Title%3A%20Autonomous%20Driving%20with%20a%20Deep%20Dual-Model%20Solution%20for%20Steering%20and%0A%20%20Braking%20Control%0AAuthor%3A%20Ana%20Petra%20Juki%C4%87%20and%20Ana%20%C5%A0elek%20and%20Marija%20Seder%20and%20Ivana%20Podnar%20%C5%BDarko%0AAbstract%3A%20%20%20The%20technology%20of%20autonomous%20driving%20is%20currently%20attracting%20a%20great%20deal%20of%0Ainterest%20in%20both%20research%20and%20industry.%20In%20this%20paper%2C%20we%20present%20a%20deep%0Alearning%20dual-model%20solution%20that%20uses%20two%20deep%20neural%20networks%20for%20combined%0Abraking%20and%20steering%20in%20autonomous%20vehicles.%20Steering%20control%20is%20achieved%20by%0Aapplying%20the%20NVIDIA%27s%20PilotNet%20model%20to%20predict%20the%20steering%20wheel%20angle%2C%20while%0Abraking%20control%20relies%20on%20the%20use%20of%20MobileNet%20SSD.%20Both%20models%20rely%20on%20a%0Asingle%20front-facing%20camera%20for%20image%20input.%20The%20MobileNet%20SSD%20model%20is%20suitable%0Afor%20devices%20with%20constrained%20resources%2C%20whereas%20PilotNet%20struggles%20to%20operate%0Aefficiently%20on%20smaller%20devices%20with%20limited%20resources.%20To%20make%20it%20suitable%20for%0Asuch%20devices%2C%20we%20modified%20the%20PilotNet%20model%20using%20our%20own%20original%20network%0Adesign%20and%20reduced%20the%20number%20of%20model%20parameters%20and%20its%20memory%20footprint%20by%0Aapproximately%2060%25.%20The%20inference%20latency%20has%20also%20been%20reduced%2C%20making%20the%0Amodel%20more%20suitable%20to%20operate%20on%20resource-constrained%20devices.%20The%20modified%0APilotNet%20model%20achieves%20similar%20loss%20and%20accuracy%20compared%20to%20the%20original%0APilotNet%20model.%20When%20evaluated%20in%20a%20simulated%20environment%2C%20both%20autonomous%0Adriving%20systems%2C%20one%20using%20the%20modified%20PilotNet%20model%20and%20the%20other%20using%20the%0Aoriginal%20PilotNet%20model%20for%20steering%2C%20show%20similar%20levels%20of%20autonomous%20driving%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Driving%2520with%2520a%2520Deep%2520Dual-Model%2520Solution%2520for%2520Steering%2520and%250A%2520%2520Braking%2520Control%26entry.906535625%3DAna%2520Petra%2520Juki%25C4%2587%2520and%2520Ana%2520%25C5%25A0elek%2520and%2520Marija%2520Seder%2520and%2520Ivana%2520Podnar%2520%25C5%25BDarko%26entry.1292438233%3D%2520%2520The%2520technology%2520of%2520autonomous%2520driving%2520is%2520currently%2520attracting%2520a%2520great%2520deal%2520of%250Ainterest%2520in%2520both%2520research%2520and%2520industry.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520deep%250Alearning%2520dual-model%2520solution%2520that%2520uses%2520two%2520deep%2520neural%2520networks%2520for%2520combined%250Abraking%2520and%2520steering%2520in%2520autonomous%2520vehicles.%2520Steering%2520control%2520is%2520achieved%2520by%250Aapplying%2520the%2520NVIDIA%2527s%2520PilotNet%2520model%2520to%2520predict%2520the%2520steering%2520wheel%2520angle%252C%2520while%250Abraking%2520control%2520relies%2520on%2520the%2520use%2520of%2520MobileNet%2520SSD.%2520Both%2520models%2520rely%2520on%2520a%250Asingle%2520front-facing%2520camera%2520for%2520image%2520input.%2520The%2520MobileNet%2520SSD%2520model%2520is%2520suitable%250Afor%2520devices%2520with%2520constrained%2520resources%252C%2520whereas%2520PilotNet%2520struggles%2520to%2520operate%250Aefficiently%2520on%2520smaller%2520devices%2520with%2520limited%2520resources.%2520To%2520make%2520it%2520suitable%2520for%250Asuch%2520devices%252C%2520we%2520modified%2520the%2520PilotNet%2520model%2520using%2520our%2520own%2520original%2520network%250Adesign%2520and%2520reduced%2520the%2520number%2520of%2520model%2520parameters%2520and%2520its%2520memory%2520footprint%2520by%250Aapproximately%252060%2525.%2520The%2520inference%2520latency%2520has%2520also%2520been%2520reduced%252C%2520making%2520the%250Amodel%2520more%2520suitable%2520to%2520operate%2520on%2520resource-constrained%2520devices.%2520The%2520modified%250APilotNet%2520model%2520achieves%2520similar%2520loss%2520and%2520accuracy%2520compared%2520to%2520the%2520original%250APilotNet%2520model.%2520When%2520evaluated%2520in%2520a%2520simulated%2520environment%252C%2520both%2520autonomous%250Adriving%2520systems%252C%2520one%2520using%2520the%2520modified%2520PilotNet%2520model%2520and%2520the%2520other%2520using%2520the%250Aoriginal%2520PilotNet%2520model%2520for%2520steering%252C%2520show%2520similar%2520levels%2520of%2520autonomous%2520driving%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Driving%20with%20a%20Deep%20Dual-Model%20Solution%20for%20Steering%20and%0A%20%20Braking%20Control&entry.906535625=Ana%20Petra%20Juki%C4%87%20and%20Ana%20%C5%A0elek%20and%20Marija%20Seder%20and%20Ivana%20Podnar%20%C5%BDarko&entry.1292438233=%20%20The%20technology%20of%20autonomous%20driving%20is%20currently%20attracting%20a%20great%20deal%20of%0Ainterest%20in%20both%20research%20and%20industry.%20In%20this%20paper%2C%20we%20present%20a%20deep%0Alearning%20dual-model%20solution%20that%20uses%20two%20deep%20neural%20networks%20for%20combined%0Abraking%20and%20steering%20in%20autonomous%20vehicles.%20Steering%20control%20is%20achieved%20by%0Aapplying%20the%20NVIDIA%27s%20PilotNet%20model%20to%20predict%20the%20steering%20wheel%20angle%2C%20while%0Abraking%20control%20relies%20on%20the%20use%20of%20MobileNet%20SSD.%20Both%20models%20rely%20on%20a%0Asingle%20front-facing%20camera%20for%20image%20input.%20The%20MobileNet%20SSD%20model%20is%20suitable%0Afor%20devices%20with%20constrained%20resources%2C%20whereas%20PilotNet%20struggles%20to%20operate%0Aefficiently%20on%20smaller%20devices%20with%20limited%20resources.%20To%20make%20it%20suitable%20for%0Asuch%20devices%2C%20we%20modified%20the%20PilotNet%20model%20using%20our%20own%20original%20network%0Adesign%20and%20reduced%20the%20number%20of%20model%20parameters%20and%20its%20memory%20footprint%20by%0Aapproximately%2060%25.%20The%20inference%20latency%20has%20also%20been%20reduced%2C%20making%20the%0Amodel%20more%20suitable%20to%20operate%20on%20resource-constrained%20devices.%20The%20modified%0APilotNet%20model%20achieves%20similar%20loss%20and%20accuracy%20compared%20to%20the%20original%0APilotNet%20model.%20When%20evaluated%20in%20a%20simulated%20environment%2C%20both%20autonomous%0Adriving%20systems%2C%20one%20using%20the%20modified%20PilotNet%20model%20and%20the%20other%20using%20the%0Aoriginal%20PilotNet%20model%20for%20steering%2C%20show%20similar%20levels%20of%20autonomous%20driving%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06473v1&entry.124074799=Read"},
{"title": "Synthetic Data Generation for Bridging Sim2Real Gap in a Production\n  Environment", "author": "Parth Rawal and Mrunal Sompura and Wolfgang Hintze", "abstract": "  Synthetic data is being used lately for training deep neural networks in\ncomputer vision applications such as object detection, object segmentation and\n6D object pose estimation. Domain randomization hereby plays an important role\nin reducing the simulation to reality gap. However, this generalization might\nnot be effective in specialized domains like a production environment involving\ncomplex assemblies. Either the individual parts, trained with synthetic images,\nare integrated in much larger assemblies making them indistinguishable from\ntheir counterparts and result in false positives or are partially occluded just\nenough to give rise to false negatives. Domain knowledge is vital in these\ncases and if conceived effectively while generating synthetic data, can show a\nconsiderable improvement in bridging the simulation to reality gap. This paper\nfocuses on synthetic data generation procedures for parts and assemblies used\nin a production environment. The basic procedures for synthetic data generation\nand their various combinations are evaluated and compared on images captured in\na production environment, where results show up to 15% improvement using\ncombinations of basic procedures. Reducing the simulation to reality gap in\nthis way can aid to utilize the true potential of robot assisted production\nusing artificial intelligence.\n", "link": "http://arxiv.org/abs/2311.11039v2", "date": "2024-05-10", "relevancy": 2.036, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5106}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5098}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20Generation%20for%20Bridging%20Sim2Real%20Gap%20in%20a%20Production%0A%20%20Environment&body=Title%3A%20Synthetic%20Data%20Generation%20for%20Bridging%20Sim2Real%20Gap%20in%20a%20Production%0A%20%20Environment%0AAuthor%3A%20Parth%20Rawal%20and%20Mrunal%20Sompura%20and%20Wolfgang%20Hintze%0AAbstract%3A%20%20%20Synthetic%20data%20is%20being%20used%20lately%20for%20training%20deep%20neural%20networks%20in%0Acomputer%20vision%20applications%20such%20as%20object%20detection%2C%20object%20segmentation%20and%0A6D%20object%20pose%20estimation.%20Domain%20randomization%20hereby%20plays%20an%20important%20role%0Ain%20reducing%20the%20simulation%20to%20reality%20gap.%20However%2C%20this%20generalization%20might%0Anot%20be%20effective%20in%20specialized%20domains%20like%20a%20production%20environment%20involving%0Acomplex%20assemblies.%20Either%20the%20individual%20parts%2C%20trained%20with%20synthetic%20images%2C%0Aare%20integrated%20in%20much%20larger%20assemblies%20making%20them%20indistinguishable%20from%0Atheir%20counterparts%20and%20result%20in%20false%20positives%20or%20are%20partially%20occluded%20just%0Aenough%20to%20give%20rise%20to%20false%20negatives.%20Domain%20knowledge%20is%20vital%20in%20these%0Acases%20and%20if%20conceived%20effectively%20while%20generating%20synthetic%20data%2C%20can%20show%20a%0Aconsiderable%20improvement%20in%20bridging%20the%20simulation%20to%20reality%20gap.%20This%20paper%0Afocuses%20on%20synthetic%20data%20generation%20procedures%20for%20parts%20and%20assemblies%20used%0Ain%20a%20production%20environment.%20The%20basic%20procedures%20for%20synthetic%20data%20generation%0Aand%20their%20various%20combinations%20are%20evaluated%20and%20compared%20on%20images%20captured%20in%0Aa%20production%20environment%2C%20where%20results%20show%20up%20to%2015%25%20improvement%20using%0Acombinations%20of%20basic%20procedures.%20Reducing%20the%20simulation%20to%20reality%20gap%20in%0Athis%20way%20can%20aid%20to%20utilize%20the%20true%20potential%20of%20robot%20assisted%20production%0Ausing%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520Generation%2520for%2520Bridging%2520Sim2Real%2520Gap%2520in%2520a%2520Production%250A%2520%2520Environment%26entry.906535625%3DParth%2520Rawal%2520and%2520Mrunal%2520Sompura%2520and%2520Wolfgang%2520Hintze%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520is%2520being%2520used%2520lately%2520for%2520training%2520deep%2520neural%2520networks%2520in%250Acomputer%2520vision%2520applications%2520such%2520as%2520object%2520detection%252C%2520object%2520segmentation%2520and%250A6D%2520object%2520pose%2520estimation.%2520Domain%2520randomization%2520hereby%2520plays%2520an%2520important%2520role%250Ain%2520reducing%2520the%2520simulation%2520to%2520reality%2520gap.%2520However%252C%2520this%2520generalization%2520might%250Anot%2520be%2520effective%2520in%2520specialized%2520domains%2520like%2520a%2520production%2520environment%2520involving%250Acomplex%2520assemblies.%2520Either%2520the%2520individual%2520parts%252C%2520trained%2520with%2520synthetic%2520images%252C%250Aare%2520integrated%2520in%2520much%2520larger%2520assemblies%2520making%2520them%2520indistinguishable%2520from%250Atheir%2520counterparts%2520and%2520result%2520in%2520false%2520positives%2520or%2520are%2520partially%2520occluded%2520just%250Aenough%2520to%2520give%2520rise%2520to%2520false%2520negatives.%2520Domain%2520knowledge%2520is%2520vital%2520in%2520these%250Acases%2520and%2520if%2520conceived%2520effectively%2520while%2520generating%2520synthetic%2520data%252C%2520can%2520show%2520a%250Aconsiderable%2520improvement%2520in%2520bridging%2520the%2520simulation%2520to%2520reality%2520gap.%2520This%2520paper%250Afocuses%2520on%2520synthetic%2520data%2520generation%2520procedures%2520for%2520parts%2520and%2520assemblies%2520used%250Ain%2520a%2520production%2520environment.%2520The%2520basic%2520procedures%2520for%2520synthetic%2520data%2520generation%250Aand%2520their%2520various%2520combinations%2520are%2520evaluated%2520and%2520compared%2520on%2520images%2520captured%2520in%250Aa%2520production%2520environment%252C%2520where%2520results%2520show%2520up%2520to%252015%2525%2520improvement%2520using%250Acombinations%2520of%2520basic%2520procedures.%2520Reducing%2520the%2520simulation%2520to%2520reality%2520gap%2520in%250Athis%2520way%2520can%2520aid%2520to%2520utilize%2520the%2520true%2520potential%2520of%2520robot%2520assisted%2520production%250Ausing%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20Generation%20for%20Bridging%20Sim2Real%20Gap%20in%20a%20Production%0A%20%20Environment&entry.906535625=Parth%20Rawal%20and%20Mrunal%20Sompura%20and%20Wolfgang%20Hintze&entry.1292438233=%20%20Synthetic%20data%20is%20being%20used%20lately%20for%20training%20deep%20neural%20networks%20in%0Acomputer%20vision%20applications%20such%20as%20object%20detection%2C%20object%20segmentation%20and%0A6D%20object%20pose%20estimation.%20Domain%20randomization%20hereby%20plays%20an%20important%20role%0Ain%20reducing%20the%20simulation%20to%20reality%20gap.%20However%2C%20this%20generalization%20might%0Anot%20be%20effective%20in%20specialized%20domains%20like%20a%20production%20environment%20involving%0Acomplex%20assemblies.%20Either%20the%20individual%20parts%2C%20trained%20with%20synthetic%20images%2C%0Aare%20integrated%20in%20much%20larger%20assemblies%20making%20them%20indistinguishable%20from%0Atheir%20counterparts%20and%20result%20in%20false%20positives%20or%20are%20partially%20occluded%20just%0Aenough%20to%20give%20rise%20to%20false%20negatives.%20Domain%20knowledge%20is%20vital%20in%20these%0Acases%20and%20if%20conceived%20effectively%20while%20generating%20synthetic%20data%2C%20can%20show%20a%0Aconsiderable%20improvement%20in%20bridging%20the%20simulation%20to%20reality%20gap.%20This%20paper%0Afocuses%20on%20synthetic%20data%20generation%20procedures%20for%20parts%20and%20assemblies%20used%0Ain%20a%20production%20environment.%20The%20basic%20procedures%20for%20synthetic%20data%20generation%0Aand%20their%20various%20combinations%20are%20evaluated%20and%20compared%20on%20images%20captured%20in%0Aa%20production%20environment%2C%20where%20results%20show%20up%20to%2015%25%20improvement%20using%0Acombinations%20of%20basic%20procedures.%20Reducing%20the%20simulation%20to%20reality%20gap%20in%0Athis%20way%20can%20aid%20to%20utilize%20the%20true%20potential%20of%20robot%20assisted%20production%0Ausing%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11039v2&entry.124074799=Read"},
{"title": "Certified $\\ell_2$ Attribution Robustness via Uniformly Smoothed\n  Attributions", "author": "Fan Wang and Adams Wai-Kin Kong", "abstract": "  Model attribution is a popular tool to explain the rationales behind model\npredictions. However, recent work suggests that the attributions are vulnerable\nto minute perturbations, which can be added to input samples to fool the\nattributions while maintaining the prediction outputs. Although empirical\nstudies have shown positive performance via adversarial training, an effective\ncertified defense method is eminently needed to understand the robustness of\nattributions. In this work, we propose to use uniform smoothing technique that\naugments the vanilla attributions by noises uniformly sampled from a certain\nspace. It is proved that, for all perturbations within the attack region, the\ncosine similarity between uniformly smoothed attribution of perturbed sample\nand the unperturbed sample is guaranteed to be lower bounded. We also derive\nalternative formulations of the certification that is equivalent to the\noriginal one and provides the maximum size of perturbation or the minimum\nsmoothing radius such that the attribution can not be perturbed. We evaluate\nthe proposed method on three datasets and show that the proposed method can\neffectively protect the attributions from attacks, regardless of the\narchitecture of networks, training schemes and the size of the datasets.\n", "link": "http://arxiv.org/abs/2405.06361v1", "date": "2024-05-10", "relevancy": 2.0314, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5138}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20%24%5Cell_2%24%20Attribution%20Robustness%20via%20Uniformly%20Smoothed%0A%20%20Attributions&body=Title%3A%20Certified%20%24%5Cell_2%24%20Attribution%20Robustness%20via%20Uniformly%20Smoothed%0A%20%20Attributions%0AAuthor%3A%20Fan%20Wang%20and%20Adams%20Wai-Kin%20Kong%0AAbstract%3A%20%20%20Model%20attribution%20is%20a%20popular%20tool%20to%20explain%20the%20rationales%20behind%20model%0Apredictions.%20However%2C%20recent%20work%20suggests%20that%20the%20attributions%20are%20vulnerable%0Ato%20minute%20perturbations%2C%20which%20can%20be%20added%20to%20input%20samples%20to%20fool%20the%0Aattributions%20while%20maintaining%20the%20prediction%20outputs.%20Although%20empirical%0Astudies%20have%20shown%20positive%20performance%20via%20adversarial%20training%2C%20an%20effective%0Acertified%20defense%20method%20is%20eminently%20needed%20to%20understand%20the%20robustness%20of%0Aattributions.%20In%20this%20work%2C%20we%20propose%20to%20use%20uniform%20smoothing%20technique%20that%0Aaugments%20the%20vanilla%20attributions%20by%20noises%20uniformly%20sampled%20from%20a%20certain%0Aspace.%20It%20is%20proved%20that%2C%20for%20all%20perturbations%20within%20the%20attack%20region%2C%20the%0Acosine%20similarity%20between%20uniformly%20smoothed%20attribution%20of%20perturbed%20sample%0Aand%20the%20unperturbed%20sample%20is%20guaranteed%20to%20be%20lower%20bounded.%20We%20also%20derive%0Aalternative%20formulations%20of%20the%20certification%20that%20is%20equivalent%20to%20the%0Aoriginal%20one%20and%20provides%20the%20maximum%20size%20of%20perturbation%20or%20the%20minimum%0Asmoothing%20radius%20such%20that%20the%20attribution%20can%20not%20be%20perturbed.%20We%20evaluate%0Athe%20proposed%20method%20on%20three%20datasets%20and%20show%20that%20the%20proposed%20method%20can%0Aeffectively%20protect%20the%20attributions%20from%20attacks%2C%20regardless%20of%20the%0Aarchitecture%20of%20networks%2C%20training%20schemes%20and%20the%20size%20of%20the%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520%2524%255Cell_2%2524%2520Attribution%2520Robustness%2520via%2520Uniformly%2520Smoothed%250A%2520%2520Attributions%26entry.906535625%3DFan%2520Wang%2520and%2520Adams%2520Wai-Kin%2520Kong%26entry.1292438233%3D%2520%2520Model%2520attribution%2520is%2520a%2520popular%2520tool%2520to%2520explain%2520the%2520rationales%2520behind%2520model%250Apredictions.%2520However%252C%2520recent%2520work%2520suggests%2520that%2520the%2520attributions%2520are%2520vulnerable%250Ato%2520minute%2520perturbations%252C%2520which%2520can%2520be%2520added%2520to%2520input%2520samples%2520to%2520fool%2520the%250Aattributions%2520while%2520maintaining%2520the%2520prediction%2520outputs.%2520Although%2520empirical%250Astudies%2520have%2520shown%2520positive%2520performance%2520via%2520adversarial%2520training%252C%2520an%2520effective%250Acertified%2520defense%2520method%2520is%2520eminently%2520needed%2520to%2520understand%2520the%2520robustness%2520of%250Aattributions.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520use%2520uniform%2520smoothing%2520technique%2520that%250Aaugments%2520the%2520vanilla%2520attributions%2520by%2520noises%2520uniformly%2520sampled%2520from%2520a%2520certain%250Aspace.%2520It%2520is%2520proved%2520that%252C%2520for%2520all%2520perturbations%2520within%2520the%2520attack%2520region%252C%2520the%250Acosine%2520similarity%2520between%2520uniformly%2520smoothed%2520attribution%2520of%2520perturbed%2520sample%250Aand%2520the%2520unperturbed%2520sample%2520is%2520guaranteed%2520to%2520be%2520lower%2520bounded.%2520We%2520also%2520derive%250Aalternative%2520formulations%2520of%2520the%2520certification%2520that%2520is%2520equivalent%2520to%2520the%250Aoriginal%2520one%2520and%2520provides%2520the%2520maximum%2520size%2520of%2520perturbation%2520or%2520the%2520minimum%250Asmoothing%2520radius%2520such%2520that%2520the%2520attribution%2520can%2520not%2520be%2520perturbed.%2520We%2520evaluate%250Athe%2520proposed%2520method%2520on%2520three%2520datasets%2520and%2520show%2520that%2520the%2520proposed%2520method%2520can%250Aeffectively%2520protect%2520the%2520attributions%2520from%2520attacks%252C%2520regardless%2520of%2520the%250Aarchitecture%2520of%2520networks%252C%2520training%2520schemes%2520and%2520the%2520size%2520of%2520the%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20%24%5Cell_2%24%20Attribution%20Robustness%20via%20Uniformly%20Smoothed%0A%20%20Attributions&entry.906535625=Fan%20Wang%20and%20Adams%20Wai-Kin%20Kong&entry.1292438233=%20%20Model%20attribution%20is%20a%20popular%20tool%20to%20explain%20the%20rationales%20behind%20model%0Apredictions.%20However%2C%20recent%20work%20suggests%20that%20the%20attributions%20are%20vulnerable%0Ato%20minute%20perturbations%2C%20which%20can%20be%20added%20to%20input%20samples%20to%20fool%20the%0Aattributions%20while%20maintaining%20the%20prediction%20outputs.%20Although%20empirical%0Astudies%20have%20shown%20positive%20performance%20via%20adversarial%20training%2C%20an%20effective%0Acertified%20defense%20method%20is%20eminently%20needed%20to%20understand%20the%20robustness%20of%0Aattributions.%20In%20this%20work%2C%20we%20propose%20to%20use%20uniform%20smoothing%20technique%20that%0Aaugments%20the%20vanilla%20attributions%20by%20noises%20uniformly%20sampled%20from%20a%20certain%0Aspace.%20It%20is%20proved%20that%2C%20for%20all%20perturbations%20within%20the%20attack%20region%2C%20the%0Acosine%20similarity%20between%20uniformly%20smoothed%20attribution%20of%20perturbed%20sample%0Aand%20the%20unperturbed%20sample%20is%20guaranteed%20to%20be%20lower%20bounded.%20We%20also%20derive%0Aalternative%20formulations%20of%20the%20certification%20that%20is%20equivalent%20to%20the%0Aoriginal%20one%20and%20provides%20the%20maximum%20size%20of%20perturbation%20or%20the%20minimum%0Asmoothing%20radius%20such%20that%20the%20attribution%20can%20not%20be%20perturbed.%20We%20evaluate%0Athe%20proposed%20method%20on%20three%20datasets%20and%20show%20that%20the%20proposed%20method%20can%0Aeffectively%20protect%20the%20attributions%20from%20attacks%2C%20regardless%20of%20the%0Aarchitecture%20of%20networks%2C%20training%20schemes%20and%20the%20size%20of%20the%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06361v1&entry.124074799=Read"},
{"title": "Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA\n  Benchmark", "author": "Evan M. Williams and Kathleen M. Carley", "abstract": "  We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.\n", "link": "http://arxiv.org/abs/2405.06634v1", "date": "2024-05-10", "relevancy": 2.0083, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark&body=Title%3A%20Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark%0AAuthor%3A%20Evan%20M.%20Williams%20and%20Kathleen%20M.%20Carley%0AAbstract%3A%20%20%20We%20evaluate%20the%20zero-shot%20ability%20of%20GPT-4%20and%20LLaVa%20to%20perform%20simple%20Visual%0ANetwork%20Analysis%20%28VNA%29%20tasks%20on%20small-scale%20graphs.%20We%20evaluate%20the%20Vision%0ALanguage%20Models%20%28VLMs%29%20on%205%20tasks%20related%20to%20three%20foundational%20network%20science%0Aconcepts%3A%20identifying%20nodes%20of%20maximal%20degree%20on%20a%20rendered%20graph%2C%20identifying%0Awhether%20signed%20triads%20are%20balanced%20or%20unbalanced%2C%20and%20counting%20components.%20The%0Atasks%20are%20structured%20to%20be%20easy%20for%20a%20human%20who%20understands%20the%20underlying%0Agraph%20theoretic%20concepts%2C%20and%20can%20all%20be%20solved%20by%20counting%20the%20appropriate%0Aelements%20in%20graphs.%20We%20find%20that%20while%20GPT-4%20consistently%20outperforms%20LLaVa%2C%0Aboth%20models%20struggle%20with%20every%20visual%20network%20analysis%20task%20we%20propose.%20We%0Apublicly%20release%20the%20first%20benchmark%20for%20the%20evaluation%20of%20VLMs%20on%20foundational%0AVNA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520LLMs%2520Struggle%2520with%2520Basic%2520Visual%2520Network%2520Analysis%253A%2520a%2520VNA%250A%2520%2520Benchmark%26entry.906535625%3DEvan%2520M.%2520Williams%2520and%2520Kathleen%2520M.%2520Carley%26entry.1292438233%3D%2520%2520We%2520evaluate%2520the%2520zero-shot%2520ability%2520of%2520GPT-4%2520and%2520LLaVa%2520to%2520perform%2520simple%2520Visual%250ANetwork%2520Analysis%2520%2528VNA%2529%2520tasks%2520on%2520small-scale%2520graphs.%2520We%2520evaluate%2520the%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520on%25205%2520tasks%2520related%2520to%2520three%2520foundational%2520network%2520science%250Aconcepts%253A%2520identifying%2520nodes%2520of%2520maximal%2520degree%2520on%2520a%2520rendered%2520graph%252C%2520identifying%250Awhether%2520signed%2520triads%2520are%2520balanced%2520or%2520unbalanced%252C%2520and%2520counting%2520components.%2520The%250Atasks%2520are%2520structured%2520to%2520be%2520easy%2520for%2520a%2520human%2520who%2520understands%2520the%2520underlying%250Agraph%2520theoretic%2520concepts%252C%2520and%2520can%2520all%2520be%2520solved%2520by%2520counting%2520the%2520appropriate%250Aelements%2520in%2520graphs.%2520We%2520find%2520that%2520while%2520GPT-4%2520consistently%2520outperforms%2520LLaVa%252C%250Aboth%2520models%2520struggle%2520with%2520every%2520visual%2520network%2520analysis%2520task%2520we%2520propose.%2520We%250Apublicly%2520release%2520the%2520first%2520benchmark%2520for%2520the%2520evaluation%2520of%2520VLMs%2520on%2520foundational%250AVNA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20LLMs%20Struggle%20with%20Basic%20Visual%20Network%20Analysis%3A%20a%20VNA%0A%20%20Benchmark&entry.906535625=Evan%20M.%20Williams%20and%20Kathleen%20M.%20Carley&entry.1292438233=%20%20We%20evaluate%20the%20zero-shot%20ability%20of%20GPT-4%20and%20LLaVa%20to%20perform%20simple%20Visual%0ANetwork%20Analysis%20%28VNA%29%20tasks%20on%20small-scale%20graphs.%20We%20evaluate%20the%20Vision%0ALanguage%20Models%20%28VLMs%29%20on%205%20tasks%20related%20to%20three%20foundational%20network%20science%0Aconcepts%3A%20identifying%20nodes%20of%20maximal%20degree%20on%20a%20rendered%20graph%2C%20identifying%0Awhether%20signed%20triads%20are%20balanced%20or%20unbalanced%2C%20and%20counting%20components.%20The%0Atasks%20are%20structured%20to%20be%20easy%20for%20a%20human%20who%20understands%20the%20underlying%0Agraph%20theoretic%20concepts%2C%20and%20can%20all%20be%20solved%20by%20counting%20the%20appropriate%0Aelements%20in%20graphs.%20We%20find%20that%20while%20GPT-4%20consistently%20outperforms%20LLaVa%2C%0Aboth%20models%20struggle%20with%20every%20visual%20network%20analysis%20task%20we%20propose.%20We%0Apublicly%20release%20the%20first%20benchmark%20for%20the%20evaluation%20of%20VLMs%20on%20foundational%0AVNA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06634v1&entry.124074799=Read"},
{"title": "From Interpolation to Extrapolation: Complete Length Generalization for\n  Arithmetic Transformers", "author": "Shaoxiong Duan and Yining Shi and Wei Xu", "abstract": "  In this paper, we investigate the inherent capabilities of transformer models\nin learning arithmetic algorithms, such as addition and parity. Through\nexperiments and attention analysis, we identify a number of crucial factors for\nachieving optimal length generalization. We show that transformer models are\nable to generalize to long lengths with the help of targeted attention biasing.\nIn particular, our solution solves the Parity task, a well-known and\ntheoretically proven failure mode for Transformers. We then introduce Attention\nBias Calibration (ABC), a calibration stage that enables the model to\nautomatically learn the proper attention biases, which we show to be connected\nto mechanisms in relative position encoding. We demonstrate that using ABC, the\ntransformer model can achieve unprecedented near-perfect length generalization\non certain arithmetic tasks. In addition, we show that ABC bears remarkable\nsimilarities to RPE and LoRA, which may indicate the potential for applications\nto more complex tasks.\n", "link": "http://arxiv.org/abs/2310.11984v3", "date": "2024-05-10", "relevancy": 2.0021, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4997}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Interpolation%20to%20Extrapolation%3A%20Complete%20Length%20Generalization%20for%0A%20%20Arithmetic%20Transformers&body=Title%3A%20From%20Interpolation%20to%20Extrapolation%3A%20Complete%20Length%20Generalization%20for%0A%20%20Arithmetic%20Transformers%0AAuthor%3A%20Shaoxiong%20Duan%20and%20Yining%20Shi%20and%20Wei%20Xu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20inherent%20capabilities%20of%20transformer%20models%0Ain%20learning%20arithmetic%20algorithms%2C%20such%20as%20addition%20and%20parity.%20Through%0Aexperiments%20and%20attention%20analysis%2C%20we%20identify%20a%20number%20of%20crucial%20factors%20for%0Aachieving%20optimal%20length%20generalization.%20We%20show%20that%20transformer%20models%20are%0Aable%20to%20generalize%20to%20long%20lengths%20with%20the%20help%20of%20targeted%20attention%20biasing.%0AIn%20particular%2C%20our%20solution%20solves%20the%20Parity%20task%2C%20a%20well-known%20and%0Atheoretically%20proven%20failure%20mode%20for%20Transformers.%20We%20then%20introduce%20Attention%0ABias%20Calibration%20%28ABC%29%2C%20a%20calibration%20stage%20that%20enables%20the%20model%20to%0Aautomatically%20learn%20the%20proper%20attention%20biases%2C%20which%20we%20show%20to%20be%20connected%0Ato%20mechanisms%20in%20relative%20position%20encoding.%20We%20demonstrate%20that%20using%20ABC%2C%20the%0Atransformer%20model%20can%20achieve%20unprecedented%20near-perfect%20length%20generalization%0Aon%20certain%20arithmetic%20tasks.%20In%20addition%2C%20we%20show%20that%20ABC%20bears%20remarkable%0Asimilarities%20to%20RPE%20and%20LoRA%2C%20which%20may%20indicate%20the%20potential%20for%20applications%0Ato%20more%20complex%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Interpolation%2520to%2520Extrapolation%253A%2520Complete%2520Length%2520Generalization%2520for%250A%2520%2520Arithmetic%2520Transformers%26entry.906535625%3DShaoxiong%2520Duan%2520and%2520Yining%2520Shi%2520and%2520Wei%2520Xu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520inherent%2520capabilities%2520of%2520transformer%2520models%250Ain%2520learning%2520arithmetic%2520algorithms%252C%2520such%2520as%2520addition%2520and%2520parity.%2520Through%250Aexperiments%2520and%2520attention%2520analysis%252C%2520we%2520identify%2520a%2520number%2520of%2520crucial%2520factors%2520for%250Aachieving%2520optimal%2520length%2520generalization.%2520We%2520show%2520that%2520transformer%2520models%2520are%250Aable%2520to%2520generalize%2520to%2520long%2520lengths%2520with%2520the%2520help%2520of%2520targeted%2520attention%2520biasing.%250AIn%2520particular%252C%2520our%2520solution%2520solves%2520the%2520Parity%2520task%252C%2520a%2520well-known%2520and%250Atheoretically%2520proven%2520failure%2520mode%2520for%2520Transformers.%2520We%2520then%2520introduce%2520Attention%250ABias%2520Calibration%2520%2528ABC%2529%252C%2520a%2520calibration%2520stage%2520that%2520enables%2520the%2520model%2520to%250Aautomatically%2520learn%2520the%2520proper%2520attention%2520biases%252C%2520which%2520we%2520show%2520to%2520be%2520connected%250Ato%2520mechanisms%2520in%2520relative%2520position%2520encoding.%2520We%2520demonstrate%2520that%2520using%2520ABC%252C%2520the%250Atransformer%2520model%2520can%2520achieve%2520unprecedented%2520near-perfect%2520length%2520generalization%250Aon%2520certain%2520arithmetic%2520tasks.%2520In%2520addition%252C%2520we%2520show%2520that%2520ABC%2520bears%2520remarkable%250Asimilarities%2520to%2520RPE%2520and%2520LoRA%252C%2520which%2520may%2520indicate%2520the%2520potential%2520for%2520applications%250Ato%2520more%2520complex%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Interpolation%20to%20Extrapolation%3A%20Complete%20Length%20Generalization%20for%0A%20%20Arithmetic%20Transformers&entry.906535625=Shaoxiong%20Duan%20and%20Yining%20Shi%20and%20Wei%20Xu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20inherent%20capabilities%20of%20transformer%20models%0Ain%20learning%20arithmetic%20algorithms%2C%20such%20as%20addition%20and%20parity.%20Through%0Aexperiments%20and%20attention%20analysis%2C%20we%20identify%20a%20number%20of%20crucial%20factors%20for%0Aachieving%20optimal%20length%20generalization.%20We%20show%20that%20transformer%20models%20are%0Aable%20to%20generalize%20to%20long%20lengths%20with%20the%20help%20of%20targeted%20attention%20biasing.%0AIn%20particular%2C%20our%20solution%20solves%20the%20Parity%20task%2C%20a%20well-known%20and%0Atheoretically%20proven%20failure%20mode%20for%20Transformers.%20We%20then%20introduce%20Attention%0ABias%20Calibration%20%28ABC%29%2C%20a%20calibration%20stage%20that%20enables%20the%20model%20to%0Aautomatically%20learn%20the%20proper%20attention%20biases%2C%20which%20we%20show%20to%20be%20connected%0Ato%20mechanisms%20in%20relative%20position%20encoding.%20We%20demonstrate%20that%20using%20ABC%2C%20the%0Atransformer%20model%20can%20achieve%20unprecedented%20near-perfect%20length%20generalization%0Aon%20certain%20arithmetic%20tasks.%20In%20addition%2C%20we%20show%20that%20ABC%20bears%20remarkable%0Asimilarities%20to%20RPE%20and%20LoRA%2C%20which%20may%20indicate%20the%20potential%20for%20applications%0Ato%20more%20complex%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11984v3&entry.124074799=Read"},
{"title": "Progressive Evolution from Single-Point to Polygon for Scene Text", "author": "Linger Deng and Mingxin Huang and Xudong Xie and Yuliang Liu and Lianwen Jin and Xiang Bai", "abstract": "  The advancement of text shape representations towards compactness has\nenhanced text detection and spotting performance, but at a high annotation\ncost. Current models use single-point annotations to reduce costs, yet they\nlack sufficient localization information for downstream applications. To\novercome this limitation, we introduce Point2Polygon, which can efficiently\ntransform single-points into compact polygons. Our method uses a coarse-to-fine\nprocess, starting with creating and selecting anchor points based on\nrecognition confidence, then vertically and horizontally refining the polygon\nusing recognition information to optimize its shape. We demonstrate the\naccuracy of the generated polygons through extensive experiments: 1) By\ncreating polygons from ground truth points, we achieved an accuracy of 82.0% on\nICDAR 2015; 2) In training detectors with polygons generated by our method, we\nattained 86% of the accuracy relative to training with ground truth (GT); 3)\nAdditionally, the proposed Point2Polygon can be seamlessly integrated to\nempower single-point spotters to generate polygons. This integration led to an\nimpressive 82.5% accuracy for the generated polygons. It is worth mentioning\nthat our method relies solely on synthetic recognition information, eliminating\nthe need for any manual annotation beyond single points.\n", "link": "http://arxiv.org/abs/2312.13778v3", "date": "2024-05-10", "relevancy": 2.0014, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5112}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4927}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Evolution%20from%20Single-Point%20to%20Polygon%20for%20Scene%20Text&body=Title%3A%20Progressive%20Evolution%20from%20Single-Point%20to%20Polygon%20for%20Scene%20Text%0AAuthor%3A%20Linger%20Deng%20and%20Mingxin%20Huang%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Lianwen%20Jin%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20advancement%20of%20text%20shape%20representations%20towards%20compactness%20has%0Aenhanced%20text%20detection%20and%20spotting%20performance%2C%20but%20at%20a%20high%20annotation%0Acost.%20Current%20models%20use%20single-point%20annotations%20to%20reduce%20costs%2C%20yet%20they%0Alack%20sufficient%20localization%20information%20for%20downstream%20applications.%20To%0Aovercome%20this%20limitation%2C%20we%20introduce%20Point2Polygon%2C%20which%20can%20efficiently%0Atransform%20single-points%20into%20compact%20polygons.%20Our%20method%20uses%20a%20coarse-to-fine%0Aprocess%2C%20starting%20with%20creating%20and%20selecting%20anchor%20points%20based%20on%0Arecognition%20confidence%2C%20then%20vertically%20and%20horizontally%20refining%20the%20polygon%0Ausing%20recognition%20information%20to%20optimize%20its%20shape.%20We%20demonstrate%20the%0Aaccuracy%20of%20the%20generated%20polygons%20through%20extensive%20experiments%3A%201%29%20By%0Acreating%20polygons%20from%20ground%20truth%20points%2C%20we%20achieved%20an%20accuracy%20of%2082.0%25%20on%0AICDAR%202015%3B%202%29%20In%20training%20detectors%20with%20polygons%20generated%20by%20our%20method%2C%20we%0Aattained%2086%25%20of%20the%20accuracy%20relative%20to%20training%20with%20ground%20truth%20%28GT%29%3B%203%29%0AAdditionally%2C%20the%20proposed%20Point2Polygon%20can%20be%20seamlessly%20integrated%20to%0Aempower%20single-point%20spotters%20to%20generate%20polygons.%20This%20integration%20led%20to%20an%0Aimpressive%2082.5%25%20accuracy%20for%20the%20generated%20polygons.%20It%20is%20worth%20mentioning%0Athat%20our%20method%20relies%20solely%20on%20synthetic%20recognition%20information%2C%20eliminating%0Athe%20need%20for%20any%20manual%20annotation%20beyond%20single%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13778v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Evolution%2520from%2520Single-Point%2520to%2520Polygon%2520for%2520Scene%2520Text%26entry.906535625%3DLinger%2520Deng%2520and%2520Mingxin%2520Huang%2520and%2520Xudong%2520Xie%2520and%2520Yuliang%2520Liu%2520and%2520Lianwen%2520Jin%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520text%2520shape%2520representations%2520towards%2520compactness%2520has%250Aenhanced%2520text%2520detection%2520and%2520spotting%2520performance%252C%2520but%2520at%2520a%2520high%2520annotation%250Acost.%2520Current%2520models%2520use%2520single-point%2520annotations%2520to%2520reduce%2520costs%252C%2520yet%2520they%250Alack%2520sufficient%2520localization%2520information%2520for%2520downstream%2520applications.%2520To%250Aovercome%2520this%2520limitation%252C%2520we%2520introduce%2520Point2Polygon%252C%2520which%2520can%2520efficiently%250Atransform%2520single-points%2520into%2520compact%2520polygons.%2520Our%2520method%2520uses%2520a%2520coarse-to-fine%250Aprocess%252C%2520starting%2520with%2520creating%2520and%2520selecting%2520anchor%2520points%2520based%2520on%250Arecognition%2520confidence%252C%2520then%2520vertically%2520and%2520horizontally%2520refining%2520the%2520polygon%250Ausing%2520recognition%2520information%2520to%2520optimize%2520its%2520shape.%2520We%2520demonstrate%2520the%250Aaccuracy%2520of%2520the%2520generated%2520polygons%2520through%2520extensive%2520experiments%253A%25201%2529%2520By%250Acreating%2520polygons%2520from%2520ground%2520truth%2520points%252C%2520we%2520achieved%2520an%2520accuracy%2520of%252082.0%2525%2520on%250AICDAR%25202015%253B%25202%2529%2520In%2520training%2520detectors%2520with%2520polygons%2520generated%2520by%2520our%2520method%252C%2520we%250Aattained%252086%2525%2520of%2520the%2520accuracy%2520relative%2520to%2520training%2520with%2520ground%2520truth%2520%2528GT%2529%253B%25203%2529%250AAdditionally%252C%2520the%2520proposed%2520Point2Polygon%2520can%2520be%2520seamlessly%2520integrated%2520to%250Aempower%2520single-point%2520spotters%2520to%2520generate%2520polygons.%2520This%2520integration%2520led%2520to%2520an%250Aimpressive%252082.5%2525%2520accuracy%2520for%2520the%2520generated%2520polygons.%2520It%2520is%2520worth%2520mentioning%250Athat%2520our%2520method%2520relies%2520solely%2520on%2520synthetic%2520recognition%2520information%252C%2520eliminating%250Athe%2520need%2520for%2520any%2520manual%2520annotation%2520beyond%2520single%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13778v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Evolution%20from%20Single-Point%20to%20Polygon%20for%20Scene%20Text&entry.906535625=Linger%20Deng%20and%20Mingxin%20Huang%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Lianwen%20Jin%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20advancement%20of%20text%20shape%20representations%20towards%20compactness%20has%0Aenhanced%20text%20detection%20and%20spotting%20performance%2C%20but%20at%20a%20high%20annotation%0Acost.%20Current%20models%20use%20single-point%20annotations%20to%20reduce%20costs%2C%20yet%20they%0Alack%20sufficient%20localization%20information%20for%20downstream%20applications.%20To%0Aovercome%20this%20limitation%2C%20we%20introduce%20Point2Polygon%2C%20which%20can%20efficiently%0Atransform%20single-points%20into%20compact%20polygons.%20Our%20method%20uses%20a%20coarse-to-fine%0Aprocess%2C%20starting%20with%20creating%20and%20selecting%20anchor%20points%20based%20on%0Arecognition%20confidence%2C%20then%20vertically%20and%20horizontally%20refining%20the%20polygon%0Ausing%20recognition%20information%20to%20optimize%20its%20shape.%20We%20demonstrate%20the%0Aaccuracy%20of%20the%20generated%20polygons%20through%20extensive%20experiments%3A%201%29%20By%0Acreating%20polygons%20from%20ground%20truth%20points%2C%20we%20achieved%20an%20accuracy%20of%2082.0%25%20on%0AICDAR%202015%3B%202%29%20In%20training%20detectors%20with%20polygons%20generated%20by%20our%20method%2C%20we%0Aattained%2086%25%20of%20the%20accuracy%20relative%20to%20training%20with%20ground%20truth%20%28GT%29%3B%203%29%0AAdditionally%2C%20the%20proposed%20Point2Polygon%20can%20be%20seamlessly%20integrated%20to%0Aempower%20single-point%20spotters%20to%20generate%20polygons.%20This%20integration%20led%20to%20an%0Aimpressive%2082.5%25%20accuracy%20for%20the%20generated%20polygons.%20It%20is%20worth%20mentioning%0Athat%20our%20method%20relies%20solely%20on%20synthetic%20recognition%20information%2C%20eliminating%0Athe%20need%20for%20any%20manual%20annotation%20beyond%20single%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13778v3&entry.124074799=Read"},
{"title": "Time-aware Heterogeneous Graph Transformer with Adaptive Attention\n  Merging for Health Event Prediction", "author": "Shibo Li and Hengliang Cheng and Weihua Li", "abstract": "  The widespread application of Electronic Health Records (EHR) data in the\nmedical field has led to early successes in disease risk prediction using deep\nlearning methods. These methods typically require extensive data for training\ndue to their large parameter sets. However, existing works do not exploit the\nfull potential of EHR data. A significant challenge arises from the infrequent\noccurrence of many medical codes within EHR data, limiting their clinical\napplicability. Current research often lacks in critical areas: 1) incorporating\ndisease domain knowledge; 2) heterogeneously learning disease representations\nwith rich meanings; 3) capturing the temporal dynamics of disease progression.\nTo overcome these limitations, we introduce a novel heterogeneous graph\nlearning model designed to assimilate disease domain knowledge and elucidate\nthe intricate relationships between drugs and diseases. This model innovatively\nincorporates temporal data into visit-level embeddings and leverages a\ntime-aware transformer alongside an adaptive attention mechanism to produce\npatient representations. When evaluated on two healthcare datasets, our\napproach demonstrated notable enhancements in both prediction accuracy and\ninterpretability over existing methodologies, signifying a substantial\nadvancement towards personalized and proactive healthcare management.\n", "link": "http://arxiv.org/abs/2404.14815v2", "date": "2024-05-10", "relevancy": 1.9922, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5051}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.503}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-aware%20Heterogeneous%20Graph%20Transformer%20with%20Adaptive%20Attention%0A%20%20Merging%20for%20Health%20Event%20Prediction&body=Title%3A%20Time-aware%20Heterogeneous%20Graph%20Transformer%20with%20Adaptive%20Attention%0A%20%20Merging%20for%20Health%20Event%20Prediction%0AAuthor%3A%20Shibo%20Li%20and%20Hengliang%20Cheng%20and%20Weihua%20Li%0AAbstract%3A%20%20%20The%20widespread%20application%20of%20Electronic%20Health%20Records%20%28EHR%29%20data%20in%20the%0Amedical%20field%20has%20led%20to%20early%20successes%20in%20disease%20risk%20prediction%20using%20deep%0Alearning%20methods.%20These%20methods%20typically%20require%20extensive%20data%20for%20training%0Adue%20to%20their%20large%20parameter%20sets.%20However%2C%20existing%20works%20do%20not%20exploit%20the%0Afull%20potential%20of%20EHR%20data.%20A%20significant%20challenge%20arises%20from%20the%20infrequent%0Aoccurrence%20of%20many%20medical%20codes%20within%20EHR%20data%2C%20limiting%20their%20clinical%0Aapplicability.%20Current%20research%20often%20lacks%20in%20critical%20areas%3A%201%29%20incorporating%0Adisease%20domain%20knowledge%3B%202%29%20heterogeneously%20learning%20disease%20representations%0Awith%20rich%20meanings%3B%203%29%20capturing%20the%20temporal%20dynamics%20of%20disease%20progression.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20a%20novel%20heterogeneous%20graph%0Alearning%20model%20designed%20to%20assimilate%20disease%20domain%20knowledge%20and%20elucidate%0Athe%20intricate%20relationships%20between%20drugs%20and%20diseases.%20This%20model%20innovatively%0Aincorporates%20temporal%20data%20into%20visit-level%20embeddings%20and%20leverages%20a%0Atime-aware%20transformer%20alongside%20an%20adaptive%20attention%20mechanism%20to%20produce%0Apatient%20representations.%20When%20evaluated%20on%20two%20healthcare%20datasets%2C%20our%0Aapproach%20demonstrated%20notable%20enhancements%20in%20both%20prediction%20accuracy%20and%0Ainterpretability%20over%20existing%20methodologies%2C%20signifying%20a%20substantial%0Aadvancement%20towards%20personalized%20and%20proactive%20healthcare%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-aware%2520Heterogeneous%2520Graph%2520Transformer%2520with%2520Adaptive%2520Attention%250A%2520%2520Merging%2520for%2520Health%2520Event%2520Prediction%26entry.906535625%3DShibo%2520Li%2520and%2520Hengliang%2520Cheng%2520and%2520Weihua%2520Li%26entry.1292438233%3D%2520%2520The%2520widespread%2520application%2520of%2520Electronic%2520Health%2520Records%2520%2528EHR%2529%2520data%2520in%2520the%250Amedical%2520field%2520has%2520led%2520to%2520early%2520successes%2520in%2520disease%2520risk%2520prediction%2520using%2520deep%250Alearning%2520methods.%2520These%2520methods%2520typically%2520require%2520extensive%2520data%2520for%2520training%250Adue%2520to%2520their%2520large%2520parameter%2520sets.%2520However%252C%2520existing%2520works%2520do%2520not%2520exploit%2520the%250Afull%2520potential%2520of%2520EHR%2520data.%2520A%2520significant%2520challenge%2520arises%2520from%2520the%2520infrequent%250Aoccurrence%2520of%2520many%2520medical%2520codes%2520within%2520EHR%2520data%252C%2520limiting%2520their%2520clinical%250Aapplicability.%2520Current%2520research%2520often%2520lacks%2520in%2520critical%2520areas%253A%25201%2529%2520incorporating%250Adisease%2520domain%2520knowledge%253B%25202%2529%2520heterogeneously%2520learning%2520disease%2520representations%250Awith%2520rich%2520meanings%253B%25203%2529%2520capturing%2520the%2520temporal%2520dynamics%2520of%2520disease%2520progression.%250ATo%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520novel%2520heterogeneous%2520graph%250Alearning%2520model%2520designed%2520to%2520assimilate%2520disease%2520domain%2520knowledge%2520and%2520elucidate%250Athe%2520intricate%2520relationships%2520between%2520drugs%2520and%2520diseases.%2520This%2520model%2520innovatively%250Aincorporates%2520temporal%2520data%2520into%2520visit-level%2520embeddings%2520and%2520leverages%2520a%250Atime-aware%2520transformer%2520alongside%2520an%2520adaptive%2520attention%2520mechanism%2520to%2520produce%250Apatient%2520representations.%2520When%2520evaluated%2520on%2520two%2520healthcare%2520datasets%252C%2520our%250Aapproach%2520demonstrated%2520notable%2520enhancements%2520in%2520both%2520prediction%2520accuracy%2520and%250Ainterpretability%2520over%2520existing%2520methodologies%252C%2520signifying%2520a%2520substantial%250Aadvancement%2520towards%2520personalized%2520and%2520proactive%2520healthcare%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-aware%20Heterogeneous%20Graph%20Transformer%20with%20Adaptive%20Attention%0A%20%20Merging%20for%20Health%20Event%20Prediction&entry.906535625=Shibo%20Li%20and%20Hengliang%20Cheng%20and%20Weihua%20Li&entry.1292438233=%20%20The%20widespread%20application%20of%20Electronic%20Health%20Records%20%28EHR%29%20data%20in%20the%0Amedical%20field%20has%20led%20to%20early%20successes%20in%20disease%20risk%20prediction%20using%20deep%0Alearning%20methods.%20These%20methods%20typically%20require%20extensive%20data%20for%20training%0Adue%20to%20their%20large%20parameter%20sets.%20However%2C%20existing%20works%20do%20not%20exploit%20the%0Afull%20potential%20of%20EHR%20data.%20A%20significant%20challenge%20arises%20from%20the%20infrequent%0Aoccurrence%20of%20many%20medical%20codes%20within%20EHR%20data%2C%20limiting%20their%20clinical%0Aapplicability.%20Current%20research%20often%20lacks%20in%20critical%20areas%3A%201%29%20incorporating%0Adisease%20domain%20knowledge%3B%202%29%20heterogeneously%20learning%20disease%20representations%0Awith%20rich%20meanings%3B%203%29%20capturing%20the%20temporal%20dynamics%20of%20disease%20progression.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20a%20novel%20heterogeneous%20graph%0Alearning%20model%20designed%20to%20assimilate%20disease%20domain%20knowledge%20and%20elucidate%0Athe%20intricate%20relationships%20between%20drugs%20and%20diseases.%20This%20model%20innovatively%0Aincorporates%20temporal%20data%20into%20visit-level%20embeddings%20and%20leverages%20a%0Atime-aware%20transformer%20alongside%20an%20adaptive%20attention%20mechanism%20to%20produce%0Apatient%20representations.%20When%20evaluated%20on%20two%20healthcare%20datasets%2C%20our%0Aapproach%20demonstrated%20notable%20enhancements%20in%20both%20prediction%20accuracy%20and%0Ainterpretability%20over%20existing%20methodologies%2C%20signifying%20a%20substantial%0Aadvancement%20towards%20personalized%20and%20proactive%20healthcare%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14815v2&entry.124074799=Read"},
{"title": "Invariant Learning via Probability of Sufficient and Necessary Causes", "author": "Mengyue Yang and Zhen Fang and Yonggang Zhang and Yali Du and Furui Liu and Jean-Francois Ton and Jianhong Wang and Jun Wang", "abstract": "  Out-of-distribution (OOD) generalization is indispensable for learning models\nin the wild, where testing distribution typically unknown and different from\nthe training. Recent methods derived from causality have shown great potential\nin achieving OOD generalization. However, existing methods mainly focus on the\ninvariance property of causes, while largely overlooking the property of\n\\textit{sufficiency} and \\textit{necessity} conditions. Namely, a necessary but\ninsufficient cause (feature) is invariant to distribution shift, yet it may not\nhave required accuracy. By contrast, a sufficient yet unnecessary cause\n(feature) tends to fit specific data well but may have a risk of adapting to a\nnew domain. To capture the information of sufficient and necessary causes, we\nemploy a classical concept, the probability of sufficiency and necessary causes\n(PNS), which indicates the probability of whether one is the necessary and\nsufficient cause. To associate PNS with OOD generalization, we propose PNS risk\nand formulate an algorithm to learn representation with a high PNS value. We\ntheoretically analyze and prove the generalizability of the PNS risk.\nExperiments on both synthetic and real-world benchmarks demonstrate the\neffectiveness of the proposed method. The details of the implementation can be\nfound at the GitHub repository: https://github.com/ymy4323460/CaSN.\n", "link": "http://arxiv.org/abs/2309.12559v5", "date": "2024-05-10", "relevancy": 1.9855, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5092}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Learning%20via%20Probability%20of%20Sufficient%20and%20Necessary%20Causes&body=Title%3A%20Invariant%20Learning%20via%20Probability%20of%20Sufficient%20and%20Necessary%20Causes%0AAuthor%3A%20Mengyue%20Yang%20and%20Zhen%20Fang%20and%20Yonggang%20Zhang%20and%20Yali%20Du%20and%20Furui%20Liu%20and%20Jean-Francois%20Ton%20and%20Jianhong%20Wang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20generalization%20is%20indispensable%20for%20learning%20models%0Ain%20the%20wild%2C%20where%20testing%20distribution%20typically%20unknown%20and%20different%20from%0Athe%20training.%20Recent%20methods%20derived%20from%20causality%20have%20shown%20great%20potential%0Ain%20achieving%20OOD%20generalization.%20However%2C%20existing%20methods%20mainly%20focus%20on%20the%0Ainvariance%20property%20of%20causes%2C%20while%20largely%20overlooking%20the%20property%20of%0A%5Ctextit%7Bsufficiency%7D%20and%20%5Ctextit%7Bnecessity%7D%20conditions.%20Namely%2C%20a%20necessary%20but%0Ainsufficient%20cause%20%28feature%29%20is%20invariant%20to%20distribution%20shift%2C%20yet%20it%20may%20not%0Ahave%20required%20accuracy.%20By%20contrast%2C%20a%20sufficient%20yet%20unnecessary%20cause%0A%28feature%29%20tends%20to%20fit%20specific%20data%20well%20but%20may%20have%20a%20risk%20of%20adapting%20to%20a%0Anew%20domain.%20To%20capture%20the%20information%20of%20sufficient%20and%20necessary%20causes%2C%20we%0Aemploy%20a%20classical%20concept%2C%20the%20probability%20of%20sufficiency%20and%20necessary%20causes%0A%28PNS%29%2C%20which%20indicates%20the%20probability%20of%20whether%20one%20is%20the%20necessary%20and%0Asufficient%20cause.%20To%20associate%20PNS%20with%20OOD%20generalization%2C%20we%20propose%20PNS%20risk%0Aand%20formulate%20an%20algorithm%20to%20learn%20representation%20with%20a%20high%20PNS%20value.%20We%0Atheoretically%20analyze%20and%20prove%20the%20generalizability%20of%20the%20PNS%20risk.%0AExperiments%20on%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%20The%20details%20of%20the%20implementation%20can%20be%0Afound%20at%20the%20GitHub%20repository%3A%20https%3A//github.com/ymy4323460/CaSN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12559v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Learning%2520via%2520Probability%2520of%2520Sufficient%2520and%2520Necessary%2520Causes%26entry.906535625%3DMengyue%2520Yang%2520and%2520Zhen%2520Fang%2520and%2520Yonggang%2520Zhang%2520and%2520Yali%2520Du%2520and%2520Furui%2520Liu%2520and%2520Jean-Francois%2520Ton%2520and%2520Jianhong%2520Wang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520generalization%2520is%2520indispensable%2520for%2520learning%2520models%250Ain%2520the%2520wild%252C%2520where%2520testing%2520distribution%2520typically%2520unknown%2520and%2520different%2520from%250Athe%2520training.%2520Recent%2520methods%2520derived%2520from%2520causality%2520have%2520shown%2520great%2520potential%250Ain%2520achieving%2520OOD%2520generalization.%2520However%252C%2520existing%2520methods%2520mainly%2520focus%2520on%2520the%250Ainvariance%2520property%2520of%2520causes%252C%2520while%2520largely%2520overlooking%2520the%2520property%2520of%250A%255Ctextit%257Bsufficiency%257D%2520and%2520%255Ctextit%257Bnecessity%257D%2520conditions.%2520Namely%252C%2520a%2520necessary%2520but%250Ainsufficient%2520cause%2520%2528feature%2529%2520is%2520invariant%2520to%2520distribution%2520shift%252C%2520yet%2520it%2520may%2520not%250Ahave%2520required%2520accuracy.%2520By%2520contrast%252C%2520a%2520sufficient%2520yet%2520unnecessary%2520cause%250A%2528feature%2529%2520tends%2520to%2520fit%2520specific%2520data%2520well%2520but%2520may%2520have%2520a%2520risk%2520of%2520adapting%2520to%2520a%250Anew%2520domain.%2520To%2520capture%2520the%2520information%2520of%2520sufficient%2520and%2520necessary%2520causes%252C%2520we%250Aemploy%2520a%2520classical%2520concept%252C%2520the%2520probability%2520of%2520sufficiency%2520and%2520necessary%2520causes%250A%2528PNS%2529%252C%2520which%2520indicates%2520the%2520probability%2520of%2520whether%2520one%2520is%2520the%2520necessary%2520and%250Asufficient%2520cause.%2520To%2520associate%2520PNS%2520with%2520OOD%2520generalization%252C%2520we%2520propose%2520PNS%2520risk%250Aand%2520formulate%2520an%2520algorithm%2520to%2520learn%2520representation%2520with%2520a%2520high%2520PNS%2520value.%2520We%250Atheoretically%2520analyze%2520and%2520prove%2520the%2520generalizability%2520of%2520the%2520PNS%2520risk.%250AExperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method.%2520The%2520details%2520of%2520the%2520implementation%2520can%2520be%250Afound%2520at%2520the%2520GitHub%2520repository%253A%2520https%253A//github.com/ymy4323460/CaSN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.12559v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Learning%20via%20Probability%20of%20Sufficient%20and%20Necessary%20Causes&entry.906535625=Mengyue%20Yang%20and%20Zhen%20Fang%20and%20Yonggang%20Zhang%20and%20Yali%20Du%20and%20Furui%20Liu%20and%20Jean-Francois%20Ton%20and%20Jianhong%20Wang%20and%20Jun%20Wang&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20generalization%20is%20indispensable%20for%20learning%20models%0Ain%20the%20wild%2C%20where%20testing%20distribution%20typically%20unknown%20and%20different%20from%0Athe%20training.%20Recent%20methods%20derived%20from%20causality%20have%20shown%20great%20potential%0Ain%20achieving%20OOD%20generalization.%20However%2C%20existing%20methods%20mainly%20focus%20on%20the%0Ainvariance%20property%20of%20causes%2C%20while%20largely%20overlooking%20the%20property%20of%0A%5Ctextit%7Bsufficiency%7D%20and%20%5Ctextit%7Bnecessity%7D%20conditions.%20Namely%2C%20a%20necessary%20but%0Ainsufficient%20cause%20%28feature%29%20is%20invariant%20to%20distribution%20shift%2C%20yet%20it%20may%20not%0Ahave%20required%20accuracy.%20By%20contrast%2C%20a%20sufficient%20yet%20unnecessary%20cause%0A%28feature%29%20tends%20to%20fit%20specific%20data%20well%20but%20may%20have%20a%20risk%20of%20adapting%20to%20a%0Anew%20domain.%20To%20capture%20the%20information%20of%20sufficient%20and%20necessary%20causes%2C%20we%0Aemploy%20a%20classical%20concept%2C%20the%20probability%20of%20sufficiency%20and%20necessary%20causes%0A%28PNS%29%2C%20which%20indicates%20the%20probability%20of%20whether%20one%20is%20the%20necessary%20and%0Asufficient%20cause.%20To%20associate%20PNS%20with%20OOD%20generalization%2C%20we%20propose%20PNS%20risk%0Aand%20formulate%20an%20algorithm%20to%20learn%20representation%20with%20a%20high%20PNS%20value.%20We%0Atheoretically%20analyze%20and%20prove%20the%20generalizability%20of%20the%20PNS%20risk.%0AExperiments%20on%20both%20synthetic%20and%20real-world%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method.%20The%20details%20of%20the%20implementation%20can%20be%0Afound%20at%20the%20GitHub%20repository%3A%20https%3A//github.com/ymy4323460/CaSN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12559v5&entry.124074799=Read"},
{"title": "Characterizing the Accuracy - Efficiency Trade-off of Low-rank\n  Decomposition in Language Models", "author": "Chakshu Moar and Michael Pellauer and Hyoukjun Kwon", "abstract": "  Large language models (LLMs) have emerged and presented their general\nproblem-solving capabilities with one model. However, the model size has\nincreased dramatically with billions of parameters to enable such broad\nproblem-solving capabilities. In addition, due to the dominance of\nmatrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model\nsize ratio is significantly lower than that of CNNs. This shift pushes LLMs\nfrom a computation-bound regime to a memory-bound regime. Therefore, optimizing\nthe memory footprint and traffic is an important optimization direction for\nLLMs today.\n  Model compression methods such as quantization and parameter pruning have\nbeen actively explored for achieving the memory footprint and traffic\noptimization. However, the accuracy-efficiency trade-off of rank pruning for\nLLMs is not well-understood yet. Therefore, we characterize the\naccuracy-efficiency trade-off of a low-rank decomposition method, specifically\nTucker decomposition, on recent language models, including an open-source LLM,\nLlama 2.\n  We formalize the low-rank decomposition design space and show that the\ndecomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To\nnavigate such a vast design space, we formulate the design space and perform\nthorough case studies of accuracy-efficiency trade-offs using six widely used\nLLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve\na 9\\% model size reduction with minimal accuracy drops, which range from 4\\%p\nto 10\\%p, depending on the difficulty of the benchmark, without any retraining\nto recover accuracy after decomposition. The results show that low-rank\ndecomposition can be a promising direction for LLM-based applications that\nrequire real-time service in scale (e.g., AI agent assist and real-time coding\nassistant), where the latency is as important as the model accuracy.\n", "link": "http://arxiv.org/abs/2405.06626v1", "date": "2024-05-10", "relevancy": 1.9676, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4999}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20the%20Accuracy%20-%20Efficiency%20Trade-off%20of%20Low-rank%0A%20%20Decomposition%20in%20Language%20Models&body=Title%3A%20Characterizing%20the%20Accuracy%20-%20Efficiency%20Trade-off%20of%20Low-rank%0A%20%20Decomposition%20in%20Language%20Models%0AAuthor%3A%20Chakshu%20Moar%20and%20Michael%20Pellauer%20and%20Hyoukjun%20Kwon%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20and%20presented%20their%20general%0Aproblem-solving%20capabilities%20with%20one%20model.%20However%2C%20the%20model%20size%20has%0Aincreased%20dramatically%20with%20billions%20of%20parameters%20to%20enable%20such%20broad%0Aproblem-solving%20capabilities.%20In%20addition%2C%20due%20to%20the%20dominance%20of%0Amatrix-matrix%20and%20matrix-vector%20multiplications%20in%20LLMs%2C%20the%20compute-to-model%0Asize%20ratio%20is%20significantly%20lower%20than%20that%20of%20CNNs.%20This%20shift%20pushes%20LLMs%0Afrom%20a%20computation-bound%20regime%20to%20a%20memory-bound%20regime.%20Therefore%2C%20optimizing%0Athe%20memory%20footprint%20and%20traffic%20is%20an%20important%20optimization%20direction%20for%0ALLMs%20today.%0A%20%20Model%20compression%20methods%20such%20as%20quantization%20and%20parameter%20pruning%20have%0Abeen%20actively%20explored%20for%20achieving%20the%20memory%20footprint%20and%20traffic%0Aoptimization.%20However%2C%20the%20accuracy-efficiency%20trade-off%20of%20rank%20pruning%20for%0ALLMs%20is%20not%20well-understood%20yet.%20Therefore%2C%20we%20characterize%20the%0Aaccuracy-efficiency%20trade-off%20of%20a%20low-rank%20decomposition%20method%2C%20specifically%0ATucker%20decomposition%2C%20on%20recent%20language%20models%2C%20including%20an%20open-source%20LLM%2C%0ALlama%202.%0A%20%20We%20formalize%20the%20low-rank%20decomposition%20design%20space%20and%20show%20that%20the%0Adecomposition%20design%20space%20is%20enormous%20%28e.g.%2C%20O%28%242%5E%7B37%7D%24%29%20for%20Llama2-7B%29.%20To%0Anavigate%20such%20a%20vast%20design%20space%2C%20we%20formulate%20the%20design%20space%20and%20perform%0Athorough%20case%20studies%20of%20accuracy-efficiency%20trade-offs%20using%20six%20widely%20used%0ALLM%20benchmarks%20on%20BERT%20and%20Llama%202%20models.%20Our%20results%20show%20that%20we%20can%20achieve%0Aa%209%5C%25%20model%20size%20reduction%20with%20minimal%20accuracy%20drops%2C%20which%20range%20from%204%5C%25p%0Ato%2010%5C%25p%2C%20depending%20on%20the%20difficulty%20of%20the%20benchmark%2C%20without%20any%20retraining%0Ato%20recover%20accuracy%20after%20decomposition.%20The%20results%20show%20that%20low-rank%0Adecomposition%20can%20be%20a%20promising%20direction%20for%20LLM-based%20applications%20that%0Arequire%20real-time%20service%20in%20scale%20%28e.g.%2C%20AI%20agent%20assist%20and%20real-time%20coding%0Aassistant%29%2C%20where%20the%20latency%20is%20as%20important%20as%20the%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520the%2520Accuracy%2520-%2520Efficiency%2520Trade-off%2520of%2520Low-rank%250A%2520%2520Decomposition%2520in%2520Language%2520Models%26entry.906535625%3DChakshu%2520Moar%2520and%2520Michael%2520Pellauer%2520and%2520Hyoukjun%2520Kwon%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520emerged%2520and%2520presented%2520their%2520general%250Aproblem-solving%2520capabilities%2520with%2520one%2520model.%2520However%252C%2520the%2520model%2520size%2520has%250Aincreased%2520dramatically%2520with%2520billions%2520of%2520parameters%2520to%2520enable%2520such%2520broad%250Aproblem-solving%2520capabilities.%2520In%2520addition%252C%2520due%2520to%2520the%2520dominance%2520of%250Amatrix-matrix%2520and%2520matrix-vector%2520multiplications%2520in%2520LLMs%252C%2520the%2520compute-to-model%250Asize%2520ratio%2520is%2520significantly%2520lower%2520than%2520that%2520of%2520CNNs.%2520This%2520shift%2520pushes%2520LLMs%250Afrom%2520a%2520computation-bound%2520regime%2520to%2520a%2520memory-bound%2520regime.%2520Therefore%252C%2520optimizing%250Athe%2520memory%2520footprint%2520and%2520traffic%2520is%2520an%2520important%2520optimization%2520direction%2520for%250ALLMs%2520today.%250A%2520%2520Model%2520compression%2520methods%2520such%2520as%2520quantization%2520and%2520parameter%2520pruning%2520have%250Abeen%2520actively%2520explored%2520for%2520achieving%2520the%2520memory%2520footprint%2520and%2520traffic%250Aoptimization.%2520However%252C%2520the%2520accuracy-efficiency%2520trade-off%2520of%2520rank%2520pruning%2520for%250ALLMs%2520is%2520not%2520well-understood%2520yet.%2520Therefore%252C%2520we%2520characterize%2520the%250Aaccuracy-efficiency%2520trade-off%2520of%2520a%2520low-rank%2520decomposition%2520method%252C%2520specifically%250ATucker%2520decomposition%252C%2520on%2520recent%2520language%2520models%252C%2520including%2520an%2520open-source%2520LLM%252C%250ALlama%25202.%250A%2520%2520We%2520formalize%2520the%2520low-rank%2520decomposition%2520design%2520space%2520and%2520show%2520that%2520the%250Adecomposition%2520design%2520space%2520is%2520enormous%2520%2528e.g.%252C%2520O%2528%25242%255E%257B37%257D%2524%2529%2520for%2520Llama2-7B%2529.%2520To%250Anavigate%2520such%2520a%2520vast%2520design%2520space%252C%2520we%2520formulate%2520the%2520design%2520space%2520and%2520perform%250Athorough%2520case%2520studies%2520of%2520accuracy-efficiency%2520trade-offs%2520using%2520six%2520widely%2520used%250ALLM%2520benchmarks%2520on%2520BERT%2520and%2520Llama%25202%2520models.%2520Our%2520results%2520show%2520that%2520we%2520can%2520achieve%250Aa%25209%255C%2525%2520model%2520size%2520reduction%2520with%2520minimal%2520accuracy%2520drops%252C%2520which%2520range%2520from%25204%255C%2525p%250Ato%252010%255C%2525p%252C%2520depending%2520on%2520the%2520difficulty%2520of%2520the%2520benchmark%252C%2520without%2520any%2520retraining%250Ato%2520recover%2520accuracy%2520after%2520decomposition.%2520The%2520results%2520show%2520that%2520low-rank%250Adecomposition%2520can%2520be%2520a%2520promising%2520direction%2520for%2520LLM-based%2520applications%2520that%250Arequire%2520real-time%2520service%2520in%2520scale%2520%2528e.g.%252C%2520AI%2520agent%2520assist%2520and%2520real-time%2520coding%250Aassistant%2529%252C%2520where%2520the%2520latency%2520is%2520as%2520important%2520as%2520the%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20the%20Accuracy%20-%20Efficiency%20Trade-off%20of%20Low-rank%0A%20%20Decomposition%20in%20Language%20Models&entry.906535625=Chakshu%20Moar%20and%20Michael%20Pellauer%20and%20Hyoukjun%20Kwon&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20emerged%20and%20presented%20their%20general%0Aproblem-solving%20capabilities%20with%20one%20model.%20However%2C%20the%20model%20size%20has%0Aincreased%20dramatically%20with%20billions%20of%20parameters%20to%20enable%20such%20broad%0Aproblem-solving%20capabilities.%20In%20addition%2C%20due%20to%20the%20dominance%20of%0Amatrix-matrix%20and%20matrix-vector%20multiplications%20in%20LLMs%2C%20the%20compute-to-model%0Asize%20ratio%20is%20significantly%20lower%20than%20that%20of%20CNNs.%20This%20shift%20pushes%20LLMs%0Afrom%20a%20computation-bound%20regime%20to%20a%20memory-bound%20regime.%20Therefore%2C%20optimizing%0Athe%20memory%20footprint%20and%20traffic%20is%20an%20important%20optimization%20direction%20for%0ALLMs%20today.%0A%20%20Model%20compression%20methods%20such%20as%20quantization%20and%20parameter%20pruning%20have%0Abeen%20actively%20explored%20for%20achieving%20the%20memory%20footprint%20and%20traffic%0Aoptimization.%20However%2C%20the%20accuracy-efficiency%20trade-off%20of%20rank%20pruning%20for%0ALLMs%20is%20not%20well-understood%20yet.%20Therefore%2C%20we%20characterize%20the%0Aaccuracy-efficiency%20trade-off%20of%20a%20low-rank%20decomposition%20method%2C%20specifically%0ATucker%20decomposition%2C%20on%20recent%20language%20models%2C%20including%20an%20open-source%20LLM%2C%0ALlama%202.%0A%20%20We%20formalize%20the%20low-rank%20decomposition%20design%20space%20and%20show%20that%20the%0Adecomposition%20design%20space%20is%20enormous%20%28e.g.%2C%20O%28%242%5E%7B37%7D%24%29%20for%20Llama2-7B%29.%20To%0Anavigate%20such%20a%20vast%20design%20space%2C%20we%20formulate%20the%20design%20space%20and%20perform%0Athorough%20case%20studies%20of%20accuracy-efficiency%20trade-offs%20using%20six%20widely%20used%0ALLM%20benchmarks%20on%20BERT%20and%20Llama%202%20models.%20Our%20results%20show%20that%20we%20can%20achieve%0Aa%209%5C%25%20model%20size%20reduction%20with%20minimal%20accuracy%20drops%2C%20which%20range%20from%204%5C%25p%0Ato%2010%5C%25p%2C%20depending%20on%20the%20difficulty%20of%20the%20benchmark%2C%20without%20any%20retraining%0Ato%20recover%20accuracy%20after%20decomposition.%20The%20results%20show%20that%20low-rank%0Adecomposition%20can%20be%20a%20promising%20direction%20for%20LLM-based%20applications%20that%0Arequire%20real-time%20service%20in%20scale%20%28e.g.%2C%20AI%20agent%20assist%20and%20real-time%20coding%0Aassistant%29%2C%20where%20the%20latency%20is%20as%20important%20as%20the%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06626v1&entry.124074799=Read"},
{"title": "UniDM: A Unified Framework for Data Manipulation with Large Language\n  Models", "author": "Yichen Qian and Yongyi He and Rong Zhu and Jintao Huang and Zhijian Ma and Haibin Wang and Yaohua Wang and Xiuyu Sun and Defu Lian and Bolin Ding and Jingren Zhou", "abstract": "  Designing effective data manipulation methods is a long standing problem in\ndata lakes. Traditional methods, which rely on rules or machine learning\nmodels, require extensive human efforts on training data collection and tuning\nmodels. Recent methods apply Large Language Models (LLMs) to resolve multiple\ndata manipulation tasks. They exhibit bright benefits in terms of performance\nbut still require customized designs to fit each specific task. This is very\ncostly and can not catch up with the requirements of big data lake platforms.\nIn this paper, inspired by the cross-task generality of LLMs on NLP tasks, we\npave the first step to design an automatic and general solution to tackle with\ndata manipulation tasks. We propose UniDM, a unified framework which\nestablishes a new paradigm to process data manipulation tasks using LLMs. UniDM\nformalizes a number of data manipulation tasks in a unified form and abstracts\nthree main general steps to solve each task. We develop an automatic context\nretrieval to allow the LLMs to retrieve data from data lakes, potentially\ncontaining evidence and factual information. For each step, we design effective\nprompts to guide LLMs to produce high quality results. By our comprehensive\nevaluation on a variety of benchmarks, our UniDM exhibits great generality and\nstate-of-the-art performance on a wide variety of data manipulation tasks.\n", "link": "http://arxiv.org/abs/2405.06510v1", "date": "2024-05-10", "relevancy": 1.9619, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniDM%3A%20A%20Unified%20Framework%20for%20Data%20Manipulation%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20UniDM%3A%20A%20Unified%20Framework%20for%20Data%20Manipulation%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yichen%20Qian%20and%20Yongyi%20He%20and%20Rong%20Zhu%20and%20Jintao%20Huang%20and%20Zhijian%20Ma%20and%20Haibin%20Wang%20and%20Yaohua%20Wang%20and%20Xiuyu%20Sun%20and%20Defu%20Lian%20and%20Bolin%20Ding%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Designing%20effective%20data%20manipulation%20methods%20is%20a%20long%20standing%20problem%20in%0Adata%20lakes.%20Traditional%20methods%2C%20which%20rely%20on%20rules%20or%20machine%20learning%0Amodels%2C%20require%20extensive%20human%20efforts%20on%20training%20data%20collection%20and%20tuning%0Amodels.%20Recent%20methods%20apply%20Large%20Language%20Models%20%28LLMs%29%20to%20resolve%20multiple%0Adata%20manipulation%20tasks.%20They%20exhibit%20bright%20benefits%20in%20terms%20of%20performance%0Abut%20still%20require%20customized%20designs%20to%20fit%20each%20specific%20task.%20This%20is%20very%0Acostly%20and%20can%20not%20catch%20up%20with%20the%20requirements%20of%20big%20data%20lake%20platforms.%0AIn%20this%20paper%2C%20inspired%20by%20the%20cross-task%20generality%20of%20LLMs%20on%20NLP%20tasks%2C%20we%0Apave%20the%20first%20step%20to%20design%20an%20automatic%20and%20general%20solution%20to%20tackle%20with%0Adata%20manipulation%20tasks.%20We%20propose%20UniDM%2C%20a%20unified%20framework%20which%0Aestablishes%20a%20new%20paradigm%20to%20process%20data%20manipulation%20tasks%20using%20LLMs.%20UniDM%0Aformalizes%20a%20number%20of%20data%20manipulation%20tasks%20in%20a%20unified%20form%20and%20abstracts%0Athree%20main%20general%20steps%20to%20solve%20each%20task.%20We%20develop%20an%20automatic%20context%0Aretrieval%20to%20allow%20the%20LLMs%20to%20retrieve%20data%20from%20data%20lakes%2C%20potentially%0Acontaining%20evidence%20and%20factual%20information.%20For%20each%20step%2C%20we%20design%20effective%0Aprompts%20to%20guide%20LLMs%20to%20produce%20high%20quality%20results.%20By%20our%20comprehensive%0Aevaluation%20on%20a%20variety%20of%20benchmarks%2C%20our%20UniDM%20exhibits%20great%20generality%20and%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20data%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06510v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniDM%253A%2520A%2520Unified%2520Framework%2520for%2520Data%2520Manipulation%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYichen%2520Qian%2520and%2520Yongyi%2520He%2520and%2520Rong%2520Zhu%2520and%2520Jintao%2520Huang%2520and%2520Zhijian%2520Ma%2520and%2520Haibin%2520Wang%2520and%2520Yaohua%2520Wang%2520and%2520Xiuyu%2520Sun%2520and%2520Defu%2520Lian%2520and%2520Bolin%2520Ding%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Designing%2520effective%2520data%2520manipulation%2520methods%2520is%2520a%2520long%2520standing%2520problem%2520in%250Adata%2520lakes.%2520Traditional%2520methods%252C%2520which%2520rely%2520on%2520rules%2520or%2520machine%2520learning%250Amodels%252C%2520require%2520extensive%2520human%2520efforts%2520on%2520training%2520data%2520collection%2520and%2520tuning%250Amodels.%2520Recent%2520methods%2520apply%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520resolve%2520multiple%250Adata%2520manipulation%2520tasks.%2520They%2520exhibit%2520bright%2520benefits%2520in%2520terms%2520of%2520performance%250Abut%2520still%2520require%2520customized%2520designs%2520to%2520fit%2520each%2520specific%2520task.%2520This%2520is%2520very%250Acostly%2520and%2520can%2520not%2520catch%2520up%2520with%2520the%2520requirements%2520of%2520big%2520data%2520lake%2520platforms.%250AIn%2520this%2520paper%252C%2520inspired%2520by%2520the%2520cross-task%2520generality%2520of%2520LLMs%2520on%2520NLP%2520tasks%252C%2520we%250Apave%2520the%2520first%2520step%2520to%2520design%2520an%2520automatic%2520and%2520general%2520solution%2520to%2520tackle%2520with%250Adata%2520manipulation%2520tasks.%2520We%2520propose%2520UniDM%252C%2520a%2520unified%2520framework%2520which%250Aestablishes%2520a%2520new%2520paradigm%2520to%2520process%2520data%2520manipulation%2520tasks%2520using%2520LLMs.%2520UniDM%250Aformalizes%2520a%2520number%2520of%2520data%2520manipulation%2520tasks%2520in%2520a%2520unified%2520form%2520and%2520abstracts%250Athree%2520main%2520general%2520steps%2520to%2520solve%2520each%2520task.%2520We%2520develop%2520an%2520automatic%2520context%250Aretrieval%2520to%2520allow%2520the%2520LLMs%2520to%2520retrieve%2520data%2520from%2520data%2520lakes%252C%2520potentially%250Acontaining%2520evidence%2520and%2520factual%2520information.%2520For%2520each%2520step%252C%2520we%2520design%2520effective%250Aprompts%2520to%2520guide%2520LLMs%2520to%2520produce%2520high%2520quality%2520results.%2520By%2520our%2520comprehensive%250Aevaluation%2520on%2520a%2520variety%2520of%2520benchmarks%252C%2520our%2520UniDM%2520exhibits%2520great%2520generality%2520and%250Astate-of-the-art%2520performance%2520on%2520a%2520wide%2520variety%2520of%2520data%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06510v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniDM%3A%20A%20Unified%20Framework%20for%20Data%20Manipulation%20with%20Large%20Language%0A%20%20Models&entry.906535625=Yichen%20Qian%20and%20Yongyi%20He%20and%20Rong%20Zhu%20and%20Jintao%20Huang%20and%20Zhijian%20Ma%20and%20Haibin%20Wang%20and%20Yaohua%20Wang%20and%20Xiuyu%20Sun%20and%20Defu%20Lian%20and%20Bolin%20Ding%20and%20Jingren%20Zhou&entry.1292438233=%20%20Designing%20effective%20data%20manipulation%20methods%20is%20a%20long%20standing%20problem%20in%0Adata%20lakes.%20Traditional%20methods%2C%20which%20rely%20on%20rules%20or%20machine%20learning%0Amodels%2C%20require%20extensive%20human%20efforts%20on%20training%20data%20collection%20and%20tuning%0Amodels.%20Recent%20methods%20apply%20Large%20Language%20Models%20%28LLMs%29%20to%20resolve%20multiple%0Adata%20manipulation%20tasks.%20They%20exhibit%20bright%20benefits%20in%20terms%20of%20performance%0Abut%20still%20require%20customized%20designs%20to%20fit%20each%20specific%20task.%20This%20is%20very%0Acostly%20and%20can%20not%20catch%20up%20with%20the%20requirements%20of%20big%20data%20lake%20platforms.%0AIn%20this%20paper%2C%20inspired%20by%20the%20cross-task%20generality%20of%20LLMs%20on%20NLP%20tasks%2C%20we%0Apave%20the%20first%20step%20to%20design%20an%20automatic%20and%20general%20solution%20to%20tackle%20with%0Adata%20manipulation%20tasks.%20We%20propose%20UniDM%2C%20a%20unified%20framework%20which%0Aestablishes%20a%20new%20paradigm%20to%20process%20data%20manipulation%20tasks%20using%20LLMs.%20UniDM%0Aformalizes%20a%20number%20of%20data%20manipulation%20tasks%20in%20a%20unified%20form%20and%20abstracts%0Athree%20main%20general%20steps%20to%20solve%20each%20task.%20We%20develop%20an%20automatic%20context%0Aretrieval%20to%20allow%20the%20LLMs%20to%20retrieve%20data%20from%20data%20lakes%2C%20potentially%0Acontaining%20evidence%20and%20factual%20information.%20For%20each%20step%2C%20we%20design%20effective%0Aprompts%20to%20guide%20LLMs%20to%20produce%20high%20quality%20results.%20By%20our%20comprehensive%0Aevaluation%20on%20a%20variety%20of%20benchmarks%2C%20our%20UniDM%20exhibits%20great%20generality%20and%0Astate-of-the-art%20performance%20on%20a%20wide%20variety%20of%20data%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06510v1&entry.124074799=Read"},
{"title": "PAC-Bayesian Generalization Bounds for Knowledge Graph Representation\n  Learning", "author": "Jaejun Lee and Minsung Hwang and Joyce Jiyoung Whang", "abstract": "  While a number of knowledge graph representation learning (KGRL) methods have\nbeen proposed over the past decade, very few theoretical analyses have been\nconducted on them. In this paper, we present the first PAC-Bayesian\ngeneralization bounds for KGRL methods. To analyze a broad class of KGRL\nmodels, we propose a generic framework named ReED (Relation-aware\nEncoder-Decoder), which consists of a relation-aware message passing encoder\nand a triplet classification decoder. Our ReED framework can express at least\n15 different existing KGRL models, including not only graph neural\nnetwork-based models such as R-GCN and CompGCN but also shallow-architecture\nmodels such as RotatE and ANALOGY. Our generalization bounds for the ReED\nframework provide theoretical grounds for the commonly used tricks in KGRL,\ne.g., parameter-sharing and weight normalization schemes, and guide desirable\ndesign choices for practical KGRL methods. We empirically show that the\ncritical factors in our generalization bounds can explain actual generalization\nerrors on three real-world knowledge graphs.\n", "link": "http://arxiv.org/abs/2405.06418v1", "date": "2024-05-10", "relevancy": 1.9605, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4932}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning&body=Title%3A%20PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning%0AAuthor%3A%20Jaejun%20Lee%20and%20Minsung%20Hwang%20and%20Joyce%20Jiyoung%20Whang%0AAbstract%3A%20%20%20While%20a%20number%20of%20knowledge%20graph%20representation%20learning%20%28KGRL%29%20methods%20have%0Abeen%20proposed%20over%20the%20past%20decade%2C%20very%20few%20theoretical%20analyses%20have%20been%0Aconducted%20on%20them.%20In%20this%20paper%2C%20we%20present%20the%20first%20PAC-Bayesian%0Ageneralization%20bounds%20for%20KGRL%20methods.%20To%20analyze%20a%20broad%20class%20of%20KGRL%0Amodels%2C%20we%20propose%20a%20generic%20framework%20named%20ReED%20%28Relation-aware%0AEncoder-Decoder%29%2C%20which%20consists%20of%20a%20relation-aware%20message%20passing%20encoder%0Aand%20a%20triplet%20classification%20decoder.%20Our%20ReED%20framework%20can%20express%20at%20least%0A15%20different%20existing%20KGRL%20models%2C%20including%20not%20only%20graph%20neural%0Anetwork-based%20models%20such%20as%20R-GCN%20and%20CompGCN%20but%20also%20shallow-architecture%0Amodels%20such%20as%20RotatE%20and%20ANALOGY.%20Our%20generalization%20bounds%20for%20the%20ReED%0Aframework%20provide%20theoretical%20grounds%20for%20the%20commonly%20used%20tricks%20in%20KGRL%2C%0Ae.g.%2C%20parameter-sharing%20and%20weight%20normalization%20schemes%2C%20and%20guide%20desirable%0Adesign%20choices%20for%20practical%20KGRL%20methods.%20We%20empirically%20show%20that%20the%0Acritical%20factors%20in%20our%20generalization%20bounds%20can%20explain%20actual%20generalization%0Aerrors%20on%20three%20real-world%20knowledge%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAC-Bayesian%2520Generalization%2520Bounds%2520for%2520Knowledge%2520Graph%2520Representation%250A%2520%2520Learning%26entry.906535625%3DJaejun%2520Lee%2520and%2520Minsung%2520Hwang%2520and%2520Joyce%2520Jiyoung%2520Whang%26entry.1292438233%3D%2520%2520While%2520a%2520number%2520of%2520knowledge%2520graph%2520representation%2520learning%2520%2528KGRL%2529%2520methods%2520have%250Abeen%2520proposed%2520over%2520the%2520past%2520decade%252C%2520very%2520few%2520theoretical%2520analyses%2520have%2520been%250Aconducted%2520on%2520them.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520PAC-Bayesian%250Ageneralization%2520bounds%2520for%2520KGRL%2520methods.%2520To%2520analyze%2520a%2520broad%2520class%2520of%2520KGRL%250Amodels%252C%2520we%2520propose%2520a%2520generic%2520framework%2520named%2520ReED%2520%2528Relation-aware%250AEncoder-Decoder%2529%252C%2520which%2520consists%2520of%2520a%2520relation-aware%2520message%2520passing%2520encoder%250Aand%2520a%2520triplet%2520classification%2520decoder.%2520Our%2520ReED%2520framework%2520can%2520express%2520at%2520least%250A15%2520different%2520existing%2520KGRL%2520models%252C%2520including%2520not%2520only%2520graph%2520neural%250Anetwork-based%2520models%2520such%2520as%2520R-GCN%2520and%2520CompGCN%2520but%2520also%2520shallow-architecture%250Amodels%2520such%2520as%2520RotatE%2520and%2520ANALOGY.%2520Our%2520generalization%2520bounds%2520for%2520the%2520ReED%250Aframework%2520provide%2520theoretical%2520grounds%2520for%2520the%2520commonly%2520used%2520tricks%2520in%2520KGRL%252C%250Ae.g.%252C%2520parameter-sharing%2520and%2520weight%2520normalization%2520schemes%252C%2520and%2520guide%2520desirable%250Adesign%2520choices%2520for%2520practical%2520KGRL%2520methods.%2520We%2520empirically%2520show%2520that%2520the%250Acritical%2520factors%2520in%2520our%2520generalization%2520bounds%2520can%2520explain%2520actual%2520generalization%250Aerrors%2520on%2520three%2520real-world%2520knowledge%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-Bayesian%20Generalization%20Bounds%20for%20Knowledge%20Graph%20Representation%0A%20%20Learning&entry.906535625=Jaejun%20Lee%20and%20Minsung%20Hwang%20and%20Joyce%20Jiyoung%20Whang&entry.1292438233=%20%20While%20a%20number%20of%20knowledge%20graph%20representation%20learning%20%28KGRL%29%20methods%20have%0Abeen%20proposed%20over%20the%20past%20decade%2C%20very%20few%20theoretical%20analyses%20have%20been%0Aconducted%20on%20them.%20In%20this%20paper%2C%20we%20present%20the%20first%20PAC-Bayesian%0Ageneralization%20bounds%20for%20KGRL%20methods.%20To%20analyze%20a%20broad%20class%20of%20KGRL%0Amodels%2C%20we%20propose%20a%20generic%20framework%20named%20ReED%20%28Relation-aware%0AEncoder-Decoder%29%2C%20which%20consists%20of%20a%20relation-aware%20message%20passing%20encoder%0Aand%20a%20triplet%20classification%20decoder.%20Our%20ReED%20framework%20can%20express%20at%20least%0A15%20different%20existing%20KGRL%20models%2C%20including%20not%20only%20graph%20neural%0Anetwork-based%20models%20such%20as%20R-GCN%20and%20CompGCN%20but%20also%20shallow-architecture%0Amodels%20such%20as%20RotatE%20and%20ANALOGY.%20Our%20generalization%20bounds%20for%20the%20ReED%0Aframework%20provide%20theoretical%20grounds%20for%20the%20commonly%20used%20tricks%20in%20KGRL%2C%0Ae.g.%2C%20parameter-sharing%20and%20weight%20normalization%20schemes%2C%20and%20guide%20desirable%0Adesign%20choices%20for%20practical%20KGRL%20methods.%20We%20empirically%20show%20that%20the%0Acritical%20factors%20in%20our%20generalization%20bounds%20can%20explain%20actual%20generalization%0Aerrors%20on%20three%20real-world%20knowledge%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06418v1&entry.124074799=Read"},
{"title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving", "author": "Yujun Lin and Haotian Tang and Shang Yang and Zhekai Zhang and Guangxuan Xiao and Chuang Gan and Song Han", "abstract": "  Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.\n", "link": "http://arxiv.org/abs/2405.04532v2", "date": "2024-05-10", "relevancy": 1.9524, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5051}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving&body=Title%3A%20QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving%0AAuthor%3A%20Yujun%20Lin%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhekai%20Zhang%20and%20Guangxuan%20Xiao%20and%20Chuang%20Gan%20and%20Song%20Han%0AAbstract%3A%20%20%20Quantization%20can%20accelerate%20large%20language%20model%20%28LLM%29%20inference.%20Going%0Abeyond%20INT8%20quantization%2C%20the%20research%20community%20is%20actively%20exploring%20even%0Alower%20precision%2C%20such%20as%20INT4.%20Nonetheless%2C%20state-of-the-art%20INT4%20quantization%0Atechniques%20only%20accelerate%20low-batch%2C%20edge%20LLM%20inference%2C%20failing%20to%20deliver%0Aperformance%20gains%20in%20large-batch%2C%20cloud-based%20LLM%20serving.%20We%20uncover%20a%0Acritical%20issue%3A%20existing%20INT4%20quantization%20methods%20suffer%20from%20significant%0Aruntime%20overhead%20%2820-90%25%29%20when%20dequantizing%20either%20weights%20or%20partial%20sums%20on%0AGPUs.%20To%20address%20this%20challenge%2C%20we%20introduce%20QoQ%2C%20a%20W4A8KV4%20quantization%0Aalgorithm%20with%204-bit%20weight%2C%208-bit%20activation%2C%20and%204-bit%20KV%20cache.%20QoQ%20stands%0Afor%20quattuor-octo-quattuor%2C%20which%20represents%204-8-4%20in%20Latin.%20QoQ%20is%20implemented%0Aby%20the%20QServe%20inference%20library%20that%20achieves%20measured%20speedup.%20The%20key%20insight%0Adriving%20QServe%20is%20that%20the%20efficiency%20of%20LLM%20serving%20on%20GPUs%20is%20critically%0Ainfluenced%20by%20operations%20on%20low-throughput%20CUDA%20cores.%20Building%20upon%20this%0Ainsight%2C%20in%20QoQ%20algorithm%2C%20we%20introduce%20progressive%20quantization%20that%20can%20allow%0Alow%20dequantization%20overhead%20in%20W4A8%20GEMM.%20Additionally%2C%20we%20develop%0ASmoothAttention%20to%20effectively%20mitigate%20the%20accuracy%20degradation%20incurred%20by%0A4-bit%20KV%20quantization.%20In%20the%20QServe%20system%2C%20we%20perform%20compute-aware%20weight%0Areordering%20and%20take%20advantage%20of%20register-level%20parallelism%20to%20reduce%0Adequantization%20latency.%20We%20also%20make%20fused%20attention%20memory-bound%2C%20harnessing%0Athe%20performance%20gain%20brought%20by%20KV4%20quantization.%20As%20a%20result%2C%20QServe%20improves%0Athe%20maximum%20achievable%20serving%20throughput%20of%20Llama-3-8B%20by%201.2x%20on%20A100%2C%201.4x%0Aon%20L40S%3B%20and%20Qwen1.5-72B%20by%202.4x%20on%20A100%2C%203.5x%20on%20L40S%2C%20compared%20to%0ATensorRT-LLM.%20Remarkably%2C%20QServe%20on%20L40S%20GPU%20can%20achieve%20even%20higher%20throughput%0Athan%20TensorRT-LLM%20on%20A100.%20Thus%2C%20QServe%20effectively%20reduces%20the%20dollar%20cost%20of%0ALLM%20serving%20by%203x.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/qserve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04532v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQServe%253A%2520W4A8KV4%2520Quantization%2520and%2520System%2520Co-design%2520for%2520Efficient%2520LLM%250A%2520%2520Serving%26entry.906535625%3DYujun%2520Lin%2520and%2520Haotian%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Zhekai%2520Zhang%2520and%2520Guangxuan%2520Xiao%2520and%2520Chuang%2520Gan%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Quantization%2520can%2520accelerate%2520large%2520language%2520model%2520%2528LLM%2529%2520inference.%2520Going%250Abeyond%2520INT8%2520quantization%252C%2520the%2520research%2520community%2520is%2520actively%2520exploring%2520even%250Alower%2520precision%252C%2520such%2520as%2520INT4.%2520Nonetheless%252C%2520state-of-the-art%2520INT4%2520quantization%250Atechniques%2520only%2520accelerate%2520low-batch%252C%2520edge%2520LLM%2520inference%252C%2520failing%2520to%2520deliver%250Aperformance%2520gains%2520in%2520large-batch%252C%2520cloud-based%2520LLM%2520serving.%2520We%2520uncover%2520a%250Acritical%2520issue%253A%2520existing%2520INT4%2520quantization%2520methods%2520suffer%2520from%2520significant%250Aruntime%2520overhead%2520%252820-90%2525%2529%2520when%2520dequantizing%2520either%2520weights%2520or%2520partial%2520sums%2520on%250AGPUs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520QoQ%252C%2520a%2520W4A8KV4%2520quantization%250Aalgorithm%2520with%25204-bit%2520weight%252C%25208-bit%2520activation%252C%2520and%25204-bit%2520KV%2520cache.%2520QoQ%2520stands%250Afor%2520quattuor-octo-quattuor%252C%2520which%2520represents%25204-8-4%2520in%2520Latin.%2520QoQ%2520is%2520implemented%250Aby%2520the%2520QServe%2520inference%2520library%2520that%2520achieves%2520measured%2520speedup.%2520The%2520key%2520insight%250Adriving%2520QServe%2520is%2520that%2520the%2520efficiency%2520of%2520LLM%2520serving%2520on%2520GPUs%2520is%2520critically%250Ainfluenced%2520by%2520operations%2520on%2520low-throughput%2520CUDA%2520cores.%2520Building%2520upon%2520this%250Ainsight%252C%2520in%2520QoQ%2520algorithm%252C%2520we%2520introduce%2520progressive%2520quantization%2520that%2520can%2520allow%250Alow%2520dequantization%2520overhead%2520in%2520W4A8%2520GEMM.%2520Additionally%252C%2520we%2520develop%250ASmoothAttention%2520to%2520effectively%2520mitigate%2520the%2520accuracy%2520degradation%2520incurred%2520by%250A4-bit%2520KV%2520quantization.%2520In%2520the%2520QServe%2520system%252C%2520we%2520perform%2520compute-aware%2520weight%250Areordering%2520and%2520take%2520advantage%2520of%2520register-level%2520parallelism%2520to%2520reduce%250Adequantization%2520latency.%2520We%2520also%2520make%2520fused%2520attention%2520memory-bound%252C%2520harnessing%250Athe%2520performance%2520gain%2520brought%2520by%2520KV4%2520quantization.%2520As%2520a%2520result%252C%2520QServe%2520improves%250Athe%2520maximum%2520achievable%2520serving%2520throughput%2520of%2520Llama-3-8B%2520by%25201.2x%2520on%2520A100%252C%25201.4x%250Aon%2520L40S%253B%2520and%2520Qwen1.5-72B%2520by%25202.4x%2520on%2520A100%252C%25203.5x%2520on%2520L40S%252C%2520compared%2520to%250ATensorRT-LLM.%2520Remarkably%252C%2520QServe%2520on%2520L40S%2520GPU%2520can%2520achieve%2520even%2520higher%2520throughput%250Athan%2520TensorRT-LLM%2520on%2520A100.%2520Thus%252C%2520QServe%2520effectively%2520reduces%2520the%2520dollar%2520cost%2520of%250ALLM%2520serving%2520by%25203x.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mit-han-lab/qserve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04532v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QServe%3A%20W4A8KV4%20Quantization%20and%20System%20Co-design%20for%20Efficient%20LLM%0A%20%20Serving&entry.906535625=Yujun%20Lin%20and%20Haotian%20Tang%20and%20Shang%20Yang%20and%20Zhekai%20Zhang%20and%20Guangxuan%20Xiao%20and%20Chuang%20Gan%20and%20Song%20Han&entry.1292438233=%20%20Quantization%20can%20accelerate%20large%20language%20model%20%28LLM%29%20inference.%20Going%0Abeyond%20INT8%20quantization%2C%20the%20research%20community%20is%20actively%20exploring%20even%0Alower%20precision%2C%20such%20as%20INT4.%20Nonetheless%2C%20state-of-the-art%20INT4%20quantization%0Atechniques%20only%20accelerate%20low-batch%2C%20edge%20LLM%20inference%2C%20failing%20to%20deliver%0Aperformance%20gains%20in%20large-batch%2C%20cloud-based%20LLM%20serving.%20We%20uncover%20a%0Acritical%20issue%3A%20existing%20INT4%20quantization%20methods%20suffer%20from%20significant%0Aruntime%20overhead%20%2820-90%25%29%20when%20dequantizing%20either%20weights%20or%20partial%20sums%20on%0AGPUs.%20To%20address%20this%20challenge%2C%20we%20introduce%20QoQ%2C%20a%20W4A8KV4%20quantization%0Aalgorithm%20with%204-bit%20weight%2C%208-bit%20activation%2C%20and%204-bit%20KV%20cache.%20QoQ%20stands%0Afor%20quattuor-octo-quattuor%2C%20which%20represents%204-8-4%20in%20Latin.%20QoQ%20is%20implemented%0Aby%20the%20QServe%20inference%20library%20that%20achieves%20measured%20speedup.%20The%20key%20insight%0Adriving%20QServe%20is%20that%20the%20efficiency%20of%20LLM%20serving%20on%20GPUs%20is%20critically%0Ainfluenced%20by%20operations%20on%20low-throughput%20CUDA%20cores.%20Building%20upon%20this%0Ainsight%2C%20in%20QoQ%20algorithm%2C%20we%20introduce%20progressive%20quantization%20that%20can%20allow%0Alow%20dequantization%20overhead%20in%20W4A8%20GEMM.%20Additionally%2C%20we%20develop%0ASmoothAttention%20to%20effectively%20mitigate%20the%20accuracy%20degradation%20incurred%20by%0A4-bit%20KV%20quantization.%20In%20the%20QServe%20system%2C%20we%20perform%20compute-aware%20weight%0Areordering%20and%20take%20advantage%20of%20register-level%20parallelism%20to%20reduce%0Adequantization%20latency.%20We%20also%20make%20fused%20attention%20memory-bound%2C%20harnessing%0Athe%20performance%20gain%20brought%20by%20KV4%20quantization.%20As%20a%20result%2C%20QServe%20improves%0Athe%20maximum%20achievable%20serving%20throughput%20of%20Llama-3-8B%20by%201.2x%20on%20A100%2C%201.4x%0Aon%20L40S%3B%20and%20Qwen1.5-72B%20by%202.4x%20on%20A100%2C%203.5x%20on%20L40S%2C%20compared%20to%0ATensorRT-LLM.%20Remarkably%2C%20QServe%20on%20L40S%20GPU%20can%20achieve%20even%20higher%20throughput%0Athan%20TensorRT-LLM%20on%20A100.%20Thus%2C%20QServe%20effectively%20reduces%20the%20dollar%20cost%20of%0ALLM%20serving%20by%203x.%20Code%20is%20available%20at%20https%3A//github.com/mit-han-lab/qserve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04532v2&entry.124074799=Read"},
{"title": "MasterWeaver: Taming Editability and Identity for Personalized\n  Text-to-Image Generation", "author": "Yuxiang Wei and Zhilong Ji and Jinfeng Bai and Hongzhi Zhang and Lei Zhang and Wangmeng Zuo", "abstract": "  Text-to-image (T2I) diffusion models have shown significant success in\npersonalized text-to-image generation, which aims to generate novel images with\nhuman identities indicated by the reference images. Despite promising identity\nfidelity has been achieved by several tuning-free methods, they usually suffer\nfrom overfitting issues. The learned identity tends to entangle with irrelevant\ninformation, resulting in unsatisfied text controllability, especially on\nfaces. In this work, we present MasterWeaver, a test-time tuning-free method\ndesigned to generate personalized images with both faithful identity fidelity\nand flexible editability. Specifically, MasterWeaver adopts an encoder to\nextract identity features and steers the image generation through additional\nintroduced cross attention. To improve editability while maintaining identity\nfidelity, we propose an editing direction loss for training, which aligns the\nediting directions of our MasterWeaver with those of the original T2I model.\nAdditionally, a face-augmented dataset is constructed to facilitate\ndisentangled identity learning, and further improve the editability. Extensive\nexperiments demonstrate that our MasterWeaver can not only generate\npersonalized images with faithful identity, but also exhibit superiority in\ntext controllability. Our code will be publicly available at\nhttps://github.com/csyxwei/MasterWeaver.\n", "link": "http://arxiv.org/abs/2405.05806v2", "date": "2024-05-10", "relevancy": 1.9506, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6986}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6292}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation&body=Title%3A%20MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Yuxiang%20Wei%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Hongzhi%20Zhang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20significant%20success%20in%0Apersonalized%20text-to-image%20generation%2C%20which%20aims%20to%20generate%20novel%20images%20with%0Ahuman%20identities%20indicated%20by%20the%20reference%20images.%20Despite%20promising%20identity%0Afidelity%20has%20been%20achieved%20by%20several%20tuning-free%20methods%2C%20they%20usually%20suffer%0Afrom%20overfitting%20issues.%20The%20learned%20identity%20tends%20to%20entangle%20with%20irrelevant%0Ainformation%2C%20resulting%20in%20unsatisfied%20text%20controllability%2C%20especially%20on%0Afaces.%20In%20this%20work%2C%20we%20present%20MasterWeaver%2C%20a%20test-time%20tuning-free%20method%0Adesigned%20to%20generate%20personalized%20images%20with%20both%20faithful%20identity%20fidelity%0Aand%20flexible%20editability.%20Specifically%2C%20MasterWeaver%20adopts%20an%20encoder%20to%0Aextract%20identity%20features%20and%20steers%20the%20image%20generation%20through%20additional%0Aintroduced%20cross%20attention.%20To%20improve%20editability%20while%20maintaining%20identity%0Afidelity%2C%20we%20propose%20an%20editing%20direction%20loss%20for%20training%2C%20which%20aligns%20the%0Aediting%20directions%20of%20our%20MasterWeaver%20with%20those%20of%20the%20original%20T2I%20model.%0AAdditionally%2C%20a%20face-augmented%20dataset%20is%20constructed%20to%20facilitate%0Adisentangled%20identity%20learning%2C%20and%20further%20improve%20the%20editability.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MasterWeaver%20can%20not%20only%20generate%0Apersonalized%20images%20with%20faithful%20identity%2C%20but%20also%20exhibit%20superiority%20in%0Atext%20controllability.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/csyxwei/MasterWeaver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasterWeaver%253A%2520Taming%2520Editability%2520and%2520Identity%2520for%2520Personalized%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Zhilong%2520Ji%2520and%2520Jinfeng%2520Bai%2520and%2520Hongzhi%2520Zhang%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520shown%2520significant%2520success%2520in%250Apersonalized%2520text-to-image%2520generation%252C%2520which%2520aims%2520to%2520generate%2520novel%2520images%2520with%250Ahuman%2520identities%2520indicated%2520by%2520the%2520reference%2520images.%2520Despite%2520promising%2520identity%250Afidelity%2520has%2520been%2520achieved%2520by%2520several%2520tuning-free%2520methods%252C%2520they%2520usually%2520suffer%250Afrom%2520overfitting%2520issues.%2520The%2520learned%2520identity%2520tends%2520to%2520entangle%2520with%2520irrelevant%250Ainformation%252C%2520resulting%2520in%2520unsatisfied%2520text%2520controllability%252C%2520especially%2520on%250Afaces.%2520In%2520this%2520work%252C%2520we%2520present%2520MasterWeaver%252C%2520a%2520test-time%2520tuning-free%2520method%250Adesigned%2520to%2520generate%2520personalized%2520images%2520with%2520both%2520faithful%2520identity%2520fidelity%250Aand%2520flexible%2520editability.%2520Specifically%252C%2520MasterWeaver%2520adopts%2520an%2520encoder%2520to%250Aextract%2520identity%2520features%2520and%2520steers%2520the%2520image%2520generation%2520through%2520additional%250Aintroduced%2520cross%2520attention.%2520To%2520improve%2520editability%2520while%2520maintaining%2520identity%250Afidelity%252C%2520we%2520propose%2520an%2520editing%2520direction%2520loss%2520for%2520training%252C%2520which%2520aligns%2520the%250Aediting%2520directions%2520of%2520our%2520MasterWeaver%2520with%2520those%2520of%2520the%2520original%2520T2I%2520model.%250AAdditionally%252C%2520a%2520face-augmented%2520dataset%2520is%2520constructed%2520to%2520facilitate%250Adisentangled%2520identity%2520learning%252C%2520and%2520further%2520improve%2520the%2520editability.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520MasterWeaver%2520can%2520not%2520only%2520generate%250Apersonalized%2520images%2520with%2520faithful%2520identity%252C%2520but%2520also%2520exhibit%2520superiority%2520in%250Atext%2520controllability.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/csyxwei/MasterWeaver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation&entry.906535625=Yuxiang%20Wei%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Hongzhi%20Zhang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20significant%20success%20in%0Apersonalized%20text-to-image%20generation%2C%20which%20aims%20to%20generate%20novel%20images%20with%0Ahuman%20identities%20indicated%20by%20the%20reference%20images.%20Despite%20promising%20identity%0Afidelity%20has%20been%20achieved%20by%20several%20tuning-free%20methods%2C%20they%20usually%20suffer%0Afrom%20overfitting%20issues.%20The%20learned%20identity%20tends%20to%20entangle%20with%20irrelevant%0Ainformation%2C%20resulting%20in%20unsatisfied%20text%20controllability%2C%20especially%20on%0Afaces.%20In%20this%20work%2C%20we%20present%20MasterWeaver%2C%20a%20test-time%20tuning-free%20method%0Adesigned%20to%20generate%20personalized%20images%20with%20both%20faithful%20identity%20fidelity%0Aand%20flexible%20editability.%20Specifically%2C%20MasterWeaver%20adopts%20an%20encoder%20to%0Aextract%20identity%20features%20and%20steers%20the%20image%20generation%20through%20additional%0Aintroduced%20cross%20attention.%20To%20improve%20editability%20while%20maintaining%20identity%0Afidelity%2C%20we%20propose%20an%20editing%20direction%20loss%20for%20training%2C%20which%20aligns%20the%0Aediting%20directions%20of%20our%20MasterWeaver%20with%20those%20of%20the%20original%20T2I%20model.%0AAdditionally%2C%20a%20face-augmented%20dataset%20is%20constructed%20to%20facilitate%0Adisentangled%20identity%20learning%2C%20and%20further%20improve%20the%20editability.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MasterWeaver%20can%20not%20only%20generate%0Apersonalized%20images%20with%20faithful%20identity%2C%20but%20also%20exhibit%20superiority%20in%0Atext%20controllability.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/csyxwei/MasterWeaver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05806v2&entry.124074799=Read"},
{"title": "Non-Uniform Spatial Alignment Errors in sUAS Imagery From Wide-Area\n  Disasters", "author": "Thomas Manzini and Priyankari Perali and Raisa Karnik and Mihir Godbole and Hasnat Abdullah and Robin Murphy", "abstract": "  This work presents the first quantitative study of alignment errors between\nsmall uncrewed aerial systems (sUAS) geospatial imagery and a priori building\npolygons and finds that alignment errors are non-uniform and irregular. The\nwork also introduces a publicly available dataset of imagery, building\npolygons, and human-generated and curated adjustments that can be used to\nevaluate existing strategies for aligning building polygons with sUAS imagery.\nThere are no efforts that have aligned pre-existing spatial data with sUAS\nimagery, and thus, there is no clear state of practice. However, this effort\nand analysis show that the translational alignment errors present in this type\nof data, averaging 82px and an intersection over the union of 0.65, which would\ninduce further errors and biases in downstream machine learning systems unless\naddressed. This study identifies and analyzes the translational alignment\nerrors of 21,619 building polygons in fifty-one orthomosaic images, covering\n16787.2 Acres (26.23 square miles), constructed from sUAS raw imagery from nine\nwide-area disasters (Hurricane Ian, Hurricane Harvey, Hurricane Michael,\nHurricane Ida, Hurricane Idalia, Hurricane Laura, the Mayfield Tornado, the\nMusset Bayou Fire, and the Kilauea Eruption). The analysis finds no uniformity\namong the angle and distance metrics of the building polygon alignments as they\npresent an average degree variance of 0.4 and an average pixel distance\nvariance of 0.45. This work alerts the sUAS community to the problem of spatial\nalignment and that a simple linear transform, often used to align satellite\nimagery, will not be sufficient to align spatial data in sUAS orthomosaic\nimagery.\n", "link": "http://arxiv.org/abs/2405.06593v1", "date": "2024-05-10", "relevancy": 1.9496, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4547}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Uniform%20Spatial%20Alignment%20Errors%20in%20sUAS%20Imagery%20From%20Wide-Area%0A%20%20Disasters&body=Title%3A%20Non-Uniform%20Spatial%20Alignment%20Errors%20in%20sUAS%20Imagery%20From%20Wide-Area%0A%20%20Disasters%0AAuthor%3A%20Thomas%20Manzini%20and%20Priyankari%20Perali%20and%20Raisa%20Karnik%20and%20Mihir%20Godbole%20and%20Hasnat%20Abdullah%20and%20Robin%20Murphy%0AAbstract%3A%20%20%20This%20work%20presents%20the%20first%20quantitative%20study%20of%20alignment%20errors%20between%0Asmall%20uncrewed%20aerial%20systems%20%28sUAS%29%20geospatial%20imagery%20and%20a%20priori%20building%0Apolygons%20and%20finds%20that%20alignment%20errors%20are%20non-uniform%20and%20irregular.%20The%0Awork%20also%20introduces%20a%20publicly%20available%20dataset%20of%20imagery%2C%20building%0Apolygons%2C%20and%20human-generated%20and%20curated%20adjustments%20that%20can%20be%20used%20to%0Aevaluate%20existing%20strategies%20for%20aligning%20building%20polygons%20with%20sUAS%20imagery.%0AThere%20are%20no%20efforts%20that%20have%20aligned%20pre-existing%20spatial%20data%20with%20sUAS%0Aimagery%2C%20and%20thus%2C%20there%20is%20no%20clear%20state%20of%20practice.%20However%2C%20this%20effort%0Aand%20analysis%20show%20that%20the%20translational%20alignment%20errors%20present%20in%20this%20type%0Aof%20data%2C%20averaging%2082px%20and%20an%20intersection%20over%20the%20union%20of%200.65%2C%20which%20would%0Ainduce%20further%20errors%20and%20biases%20in%20downstream%20machine%20learning%20systems%20unless%0Aaddressed.%20This%20study%20identifies%20and%20analyzes%20the%20translational%20alignment%0Aerrors%20of%2021%2C619%20building%20polygons%20in%20fifty-one%20orthomosaic%20images%2C%20covering%0A16787.2%20Acres%20%2826.23%20square%20miles%29%2C%20constructed%20from%20sUAS%20raw%20imagery%20from%20nine%0Awide-area%20disasters%20%28Hurricane%20Ian%2C%20Hurricane%20Harvey%2C%20Hurricane%20Michael%2C%0AHurricane%20Ida%2C%20Hurricane%20Idalia%2C%20Hurricane%20Laura%2C%20the%20Mayfield%20Tornado%2C%20the%0AMusset%20Bayou%20Fire%2C%20and%20the%20Kilauea%20Eruption%29.%20The%20analysis%20finds%20no%20uniformity%0Aamong%20the%20angle%20and%20distance%20metrics%20of%20the%20building%20polygon%20alignments%20as%20they%0Apresent%20an%20average%20degree%20variance%20of%200.4%20and%20an%20average%20pixel%20distance%0Avariance%20of%200.45.%20This%20work%20alerts%20the%20sUAS%20community%20to%20the%20problem%20of%20spatial%0Aalignment%20and%20that%20a%20simple%20linear%20transform%2C%20often%20used%20to%20align%20satellite%0Aimagery%2C%20will%20not%20be%20sufficient%20to%20align%20spatial%20data%20in%20sUAS%20orthomosaic%0Aimagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Uniform%2520Spatial%2520Alignment%2520Errors%2520in%2520sUAS%2520Imagery%2520From%2520Wide-Area%250A%2520%2520Disasters%26entry.906535625%3DThomas%2520Manzini%2520and%2520Priyankari%2520Perali%2520and%2520Raisa%2520Karnik%2520and%2520Mihir%2520Godbole%2520and%2520Hasnat%2520Abdullah%2520and%2520Robin%2520Murphy%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520the%2520first%2520quantitative%2520study%2520of%2520alignment%2520errors%2520between%250Asmall%2520uncrewed%2520aerial%2520systems%2520%2528sUAS%2529%2520geospatial%2520imagery%2520and%2520a%2520priori%2520building%250Apolygons%2520and%2520finds%2520that%2520alignment%2520errors%2520are%2520non-uniform%2520and%2520irregular.%2520The%250Awork%2520also%2520introduces%2520a%2520publicly%2520available%2520dataset%2520of%2520imagery%252C%2520building%250Apolygons%252C%2520and%2520human-generated%2520and%2520curated%2520adjustments%2520that%2520can%2520be%2520used%2520to%250Aevaluate%2520existing%2520strategies%2520for%2520aligning%2520building%2520polygons%2520with%2520sUAS%2520imagery.%250AThere%2520are%2520no%2520efforts%2520that%2520have%2520aligned%2520pre-existing%2520spatial%2520data%2520with%2520sUAS%250Aimagery%252C%2520and%2520thus%252C%2520there%2520is%2520no%2520clear%2520state%2520of%2520practice.%2520However%252C%2520this%2520effort%250Aand%2520analysis%2520show%2520that%2520the%2520translational%2520alignment%2520errors%2520present%2520in%2520this%2520type%250Aof%2520data%252C%2520averaging%252082px%2520and%2520an%2520intersection%2520over%2520the%2520union%2520of%25200.65%252C%2520which%2520would%250Ainduce%2520further%2520errors%2520and%2520biases%2520in%2520downstream%2520machine%2520learning%2520systems%2520unless%250Aaddressed.%2520This%2520study%2520identifies%2520and%2520analyzes%2520the%2520translational%2520alignment%250Aerrors%2520of%252021%252C619%2520building%2520polygons%2520in%2520fifty-one%2520orthomosaic%2520images%252C%2520covering%250A16787.2%2520Acres%2520%252826.23%2520square%2520miles%2529%252C%2520constructed%2520from%2520sUAS%2520raw%2520imagery%2520from%2520nine%250Awide-area%2520disasters%2520%2528Hurricane%2520Ian%252C%2520Hurricane%2520Harvey%252C%2520Hurricane%2520Michael%252C%250AHurricane%2520Ida%252C%2520Hurricane%2520Idalia%252C%2520Hurricane%2520Laura%252C%2520the%2520Mayfield%2520Tornado%252C%2520the%250AMusset%2520Bayou%2520Fire%252C%2520and%2520the%2520Kilauea%2520Eruption%2529.%2520The%2520analysis%2520finds%2520no%2520uniformity%250Aamong%2520the%2520angle%2520and%2520distance%2520metrics%2520of%2520the%2520building%2520polygon%2520alignments%2520as%2520they%250Apresent%2520an%2520average%2520degree%2520variance%2520of%25200.4%2520and%2520an%2520average%2520pixel%2520distance%250Avariance%2520of%25200.45.%2520This%2520work%2520alerts%2520the%2520sUAS%2520community%2520to%2520the%2520problem%2520of%2520spatial%250Aalignment%2520and%2520that%2520a%2520simple%2520linear%2520transform%252C%2520often%2520used%2520to%2520align%2520satellite%250Aimagery%252C%2520will%2520not%2520be%2520sufficient%2520to%2520align%2520spatial%2520data%2520in%2520sUAS%2520orthomosaic%250Aimagery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Uniform%20Spatial%20Alignment%20Errors%20in%20sUAS%20Imagery%20From%20Wide-Area%0A%20%20Disasters&entry.906535625=Thomas%20Manzini%20and%20Priyankari%20Perali%20and%20Raisa%20Karnik%20and%20Mihir%20Godbole%20and%20Hasnat%20Abdullah%20and%20Robin%20Murphy&entry.1292438233=%20%20This%20work%20presents%20the%20first%20quantitative%20study%20of%20alignment%20errors%20between%0Asmall%20uncrewed%20aerial%20systems%20%28sUAS%29%20geospatial%20imagery%20and%20a%20priori%20building%0Apolygons%20and%20finds%20that%20alignment%20errors%20are%20non-uniform%20and%20irregular.%20The%0Awork%20also%20introduces%20a%20publicly%20available%20dataset%20of%20imagery%2C%20building%0Apolygons%2C%20and%20human-generated%20and%20curated%20adjustments%20that%20can%20be%20used%20to%0Aevaluate%20existing%20strategies%20for%20aligning%20building%20polygons%20with%20sUAS%20imagery.%0AThere%20are%20no%20efforts%20that%20have%20aligned%20pre-existing%20spatial%20data%20with%20sUAS%0Aimagery%2C%20and%20thus%2C%20there%20is%20no%20clear%20state%20of%20practice.%20However%2C%20this%20effort%0Aand%20analysis%20show%20that%20the%20translational%20alignment%20errors%20present%20in%20this%20type%0Aof%20data%2C%20averaging%2082px%20and%20an%20intersection%20over%20the%20union%20of%200.65%2C%20which%20would%0Ainduce%20further%20errors%20and%20biases%20in%20downstream%20machine%20learning%20systems%20unless%0Aaddressed.%20This%20study%20identifies%20and%20analyzes%20the%20translational%20alignment%0Aerrors%20of%2021%2C619%20building%20polygons%20in%20fifty-one%20orthomosaic%20images%2C%20covering%0A16787.2%20Acres%20%2826.23%20square%20miles%29%2C%20constructed%20from%20sUAS%20raw%20imagery%20from%20nine%0Awide-area%20disasters%20%28Hurricane%20Ian%2C%20Hurricane%20Harvey%2C%20Hurricane%20Michael%2C%0AHurricane%20Ida%2C%20Hurricane%20Idalia%2C%20Hurricane%20Laura%2C%20the%20Mayfield%20Tornado%2C%20the%0AMusset%20Bayou%20Fire%2C%20and%20the%20Kilauea%20Eruption%29.%20The%20analysis%20finds%20no%20uniformity%0Aamong%20the%20angle%20and%20distance%20metrics%20of%20the%20building%20polygon%20alignments%20as%20they%0Apresent%20an%20average%20degree%20variance%20of%200.4%20and%20an%20average%20pixel%20distance%0Avariance%20of%200.45.%20This%20work%20alerts%20the%20sUAS%20community%20to%20the%20problem%20of%20spatial%0Aalignment%20and%20that%20a%20simple%20linear%20transform%2C%20often%20used%20to%20align%20satellite%0Aimagery%2C%20will%20not%20be%20sufficient%20to%20align%20spatial%20data%20in%20sUAS%20orthomosaic%0Aimagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06593v1&entry.124074799=Read"},
{"title": "Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter\n  Simulation", "author": "Qibin Liu and Chase Shimmin and Xiulong Liu and Eli Shlizerman and Shu Li and Shih-Chieh Hsu", "abstract": "  We introduce a novel machine learning method developed for the fast\nsimulation of calorimeter detector response, adapting vector-quantized\nvariational autoencoder (VQ-VAE). Our model adopts a two-stage generation\nstrategy: initially compressing geometry-aware calorimeter data into a discrete\nlatent space, followed by the application of a sequence model to learn and\ngenerate the latent tokens. Extensive experimentation on the Calo-challenge\ndataset underscores the efficiency of our approach, showcasing a remarkable\nimprovement in the generation speed compared with conventional method by a\nfactor of 2000. Remarkably, our model achieves the generation of calorimeter\nshowers within milliseconds. Furthermore, comprehensive quantitative\nevaluations across various metrics are performed to validate physics\nperformance of generation.\n", "link": "http://arxiv.org/abs/2405.06605v1", "date": "2024-05-10", "relevancy": 1.9277, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4961}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4877}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation&body=Title%3A%20Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation%0AAuthor%3A%20Qibin%20Liu%20and%20Chase%20Shimmin%20and%20Xiulong%20Liu%20and%20Eli%20Shlizerman%20and%20Shu%20Li%20and%20Shih-Chieh%20Hsu%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20machine%20learning%20method%20developed%20for%20the%20fast%0Asimulation%20of%20calorimeter%20detector%20response%2C%20adapting%20vector-quantized%0Avariational%20autoencoder%20%28VQ-VAE%29.%20Our%20model%20adopts%20a%20two-stage%20generation%0Astrategy%3A%20initially%20compressing%20geometry-aware%20calorimeter%20data%20into%20a%20discrete%0Alatent%20space%2C%20followed%20by%20the%20application%20of%20a%20sequence%20model%20to%20learn%20and%0Agenerate%20the%20latent%20tokens.%20Extensive%20experimentation%20on%20the%20Calo-challenge%0Adataset%20underscores%20the%20efficiency%20of%20our%20approach%2C%20showcasing%20a%20remarkable%0Aimprovement%20in%20the%20generation%20speed%20compared%20with%20conventional%20method%20by%20a%0Afactor%20of%202000.%20Remarkably%2C%20our%20model%20achieves%20the%20generation%20of%20calorimeter%0Ashowers%20within%20milliseconds.%20Furthermore%2C%20comprehensive%20quantitative%0Aevaluations%20across%20various%20metrics%20are%20performed%20to%20validate%20physics%0Aperformance%20of%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalo-VQ%253A%2520Vector-Quantized%2520Two-Stage%2520Generative%2520Model%2520in%2520Calorimeter%250A%2520%2520Simulation%26entry.906535625%3DQibin%2520Liu%2520and%2520Chase%2520Shimmin%2520and%2520Xiulong%2520Liu%2520and%2520Eli%2520Shlizerman%2520and%2520Shu%2520Li%2520and%2520Shih-Chieh%2520Hsu%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520machine%2520learning%2520method%2520developed%2520for%2520the%2520fast%250Asimulation%2520of%2520calorimeter%2520detector%2520response%252C%2520adapting%2520vector-quantized%250Avariational%2520autoencoder%2520%2528VQ-VAE%2529.%2520Our%2520model%2520adopts%2520a%2520two-stage%2520generation%250Astrategy%253A%2520initially%2520compressing%2520geometry-aware%2520calorimeter%2520data%2520into%2520a%2520discrete%250Alatent%2520space%252C%2520followed%2520by%2520the%2520application%2520of%2520a%2520sequence%2520model%2520to%2520learn%2520and%250Agenerate%2520the%2520latent%2520tokens.%2520Extensive%2520experimentation%2520on%2520the%2520Calo-challenge%250Adataset%2520underscores%2520the%2520efficiency%2520of%2520our%2520approach%252C%2520showcasing%2520a%2520remarkable%250Aimprovement%2520in%2520the%2520generation%2520speed%2520compared%2520with%2520conventional%2520method%2520by%2520a%250Afactor%2520of%25202000.%2520Remarkably%252C%2520our%2520model%2520achieves%2520the%2520generation%2520of%2520calorimeter%250Ashowers%2520within%2520milliseconds.%2520Furthermore%252C%2520comprehensive%2520quantitative%250Aevaluations%2520across%2520various%2520metrics%2520are%2520performed%2520to%2520validate%2520physics%250Aperformance%2520of%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calo-VQ%3A%20Vector-Quantized%20Two-Stage%20Generative%20Model%20in%20Calorimeter%0A%20%20Simulation&entry.906535625=Qibin%20Liu%20and%20Chase%20Shimmin%20and%20Xiulong%20Liu%20and%20Eli%20Shlizerman%20and%20Shu%20Li%20and%20Shih-Chieh%20Hsu&entry.1292438233=%20%20We%20introduce%20a%20novel%20machine%20learning%20method%20developed%20for%20the%20fast%0Asimulation%20of%20calorimeter%20detector%20response%2C%20adapting%20vector-quantized%0Avariational%20autoencoder%20%28VQ-VAE%29.%20Our%20model%20adopts%20a%20two-stage%20generation%0Astrategy%3A%20initially%20compressing%20geometry-aware%20calorimeter%20data%20into%20a%20discrete%0Alatent%20space%2C%20followed%20by%20the%20application%20of%20a%20sequence%20model%20to%20learn%20and%0Agenerate%20the%20latent%20tokens.%20Extensive%20experimentation%20on%20the%20Calo-challenge%0Adataset%20underscores%20the%20efficiency%20of%20our%20approach%2C%20showcasing%20a%20remarkable%0Aimprovement%20in%20the%20generation%20speed%20compared%20with%20conventional%20method%20by%20a%0Afactor%20of%202000.%20Remarkably%2C%20our%20model%20achieves%20the%20generation%20of%20calorimeter%0Ashowers%20within%20milliseconds.%20Furthermore%2C%20comprehensive%20quantitative%0Aevaluations%20across%20various%20metrics%20are%20performed%20to%20validate%20physics%0Aperformance%20of%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06605v1&entry.124074799=Read"},
{"title": "Federated Document Visual Question Answering: A Pilot Study", "author": "Khanh Nguyen and Dimosthenis Karatzas", "abstract": "  An important handicap of document analysis research is that documents tend to\nbe copyrighted or contain private information, which prohibits their open\npublication and the creation of centralised, large-scale document datasets.\nInstead, documents are scattered in private data silos, making extensive\ntraining over heterogeneous data a tedious task. In this work, we explore the\nuse of a federated learning (FL) scheme as a way to train a shared model on\ndecentralised private document data. We focus on the problem of Document VQA, a\ntask particularly suited to this approach, as the type of reasoning\ncapabilities required from the model can be quite different in diverse domains.\nEnabling training over heterogeneous document datasets can thus substantially\nenrich DocVQA models. We assemble existing DocVQA datasets from diverse domains\nto reflect the data heterogeneity in real-world applications. We explore the\nself-pretraining technique in this multi-modal setting, where the same data is\nused for both pretraining and finetuning, making it relevant for privacy\npreservation. We further propose combining self-pretraining with a Federated\nDocVQA training method using centralized adaptive optimization that outperforms\nthe FedAvg baseline. With extensive experiments, we also present a\nmulti-faceted analysis on training DocVQA models with FL, which provides\ninsights for future research on this task. We show that our pretraining\nstrategies can effectively learn and scale up under federated training with\ndiverse DocVQA datasets and tuning hyperparameters is essential for practical\ndocument tasks under federation.\n", "link": "http://arxiv.org/abs/2405.06636v1", "date": "2024-05-10", "relevancy": 1.9246, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4846}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Document%20Visual%20Question%20Answering%3A%20A%20Pilot%20Study&body=Title%3A%20Federated%20Document%20Visual%20Question%20Answering%3A%20A%20Pilot%20Study%0AAuthor%3A%20Khanh%20Nguyen%20and%20Dimosthenis%20Karatzas%0AAbstract%3A%20%20%20An%20important%20handicap%20of%20document%20analysis%20research%20is%20that%20documents%20tend%20to%0Abe%20copyrighted%20or%20contain%20private%20information%2C%20which%20prohibits%20their%20open%0Apublication%20and%20the%20creation%20of%20centralised%2C%20large-scale%20document%20datasets.%0AInstead%2C%20documents%20are%20scattered%20in%20private%20data%20silos%2C%20making%20extensive%0Atraining%20over%20heterogeneous%20data%20a%20tedious%20task.%20In%20this%20work%2C%20we%20explore%20the%0Ause%20of%20a%20federated%20learning%20%28FL%29%20scheme%20as%20a%20way%20to%20train%20a%20shared%20model%20on%0Adecentralised%20private%20document%20data.%20We%20focus%20on%20the%20problem%20of%20Document%20VQA%2C%20a%0Atask%20particularly%20suited%20to%20this%20approach%2C%20as%20the%20type%20of%20reasoning%0Acapabilities%20required%20from%20the%20model%20can%20be%20quite%20different%20in%20diverse%20domains.%0AEnabling%20training%20over%20heterogeneous%20document%20datasets%20can%20thus%20substantially%0Aenrich%20DocVQA%20models.%20We%20assemble%20existing%20DocVQA%20datasets%20from%20diverse%20domains%0Ato%20reflect%20the%20data%20heterogeneity%20in%20real-world%20applications.%20We%20explore%20the%0Aself-pretraining%20technique%20in%20this%20multi-modal%20setting%2C%20where%20the%20same%20data%20is%0Aused%20for%20both%20pretraining%20and%20finetuning%2C%20making%20it%20relevant%20for%20privacy%0Apreservation.%20We%20further%20propose%20combining%20self-pretraining%20with%20a%20Federated%0ADocVQA%20training%20method%20using%20centralized%20adaptive%20optimization%20that%20outperforms%0Athe%20FedAvg%20baseline.%20With%20extensive%20experiments%2C%20we%20also%20present%20a%0Amulti-faceted%20analysis%20on%20training%20DocVQA%20models%20with%20FL%2C%20which%20provides%0Ainsights%20for%20future%20research%20on%20this%20task.%20We%20show%20that%20our%20pretraining%0Astrategies%20can%20effectively%20learn%20and%20scale%20up%20under%20federated%20training%20with%0Adiverse%20DocVQA%20datasets%20and%20tuning%20hyperparameters%20is%20essential%20for%20practical%0Adocument%20tasks%20under%20federation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Document%2520Visual%2520Question%2520Answering%253A%2520A%2520Pilot%2520Study%26entry.906535625%3DKhanh%2520Nguyen%2520and%2520Dimosthenis%2520Karatzas%26entry.1292438233%3D%2520%2520An%2520important%2520handicap%2520of%2520document%2520analysis%2520research%2520is%2520that%2520documents%2520tend%2520to%250Abe%2520copyrighted%2520or%2520contain%2520private%2520information%252C%2520which%2520prohibits%2520their%2520open%250Apublication%2520and%2520the%2520creation%2520of%2520centralised%252C%2520large-scale%2520document%2520datasets.%250AInstead%252C%2520documents%2520are%2520scattered%2520in%2520private%2520data%2520silos%252C%2520making%2520extensive%250Atraining%2520over%2520heterogeneous%2520data%2520a%2520tedious%2520task.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Ause%2520of%2520a%2520federated%2520learning%2520%2528FL%2529%2520scheme%2520as%2520a%2520way%2520to%2520train%2520a%2520shared%2520model%2520on%250Adecentralised%2520private%2520document%2520data.%2520We%2520focus%2520on%2520the%2520problem%2520of%2520Document%2520VQA%252C%2520a%250Atask%2520particularly%2520suited%2520to%2520this%2520approach%252C%2520as%2520the%2520type%2520of%2520reasoning%250Acapabilities%2520required%2520from%2520the%2520model%2520can%2520be%2520quite%2520different%2520in%2520diverse%2520domains.%250AEnabling%2520training%2520over%2520heterogeneous%2520document%2520datasets%2520can%2520thus%2520substantially%250Aenrich%2520DocVQA%2520models.%2520We%2520assemble%2520existing%2520DocVQA%2520datasets%2520from%2520diverse%2520domains%250Ato%2520reflect%2520the%2520data%2520heterogeneity%2520in%2520real-world%2520applications.%2520We%2520explore%2520the%250Aself-pretraining%2520technique%2520in%2520this%2520multi-modal%2520setting%252C%2520where%2520the%2520same%2520data%2520is%250Aused%2520for%2520both%2520pretraining%2520and%2520finetuning%252C%2520making%2520it%2520relevant%2520for%2520privacy%250Apreservation.%2520We%2520further%2520propose%2520combining%2520self-pretraining%2520with%2520a%2520Federated%250ADocVQA%2520training%2520method%2520using%2520centralized%2520adaptive%2520optimization%2520that%2520outperforms%250Athe%2520FedAvg%2520baseline.%2520With%2520extensive%2520experiments%252C%2520we%2520also%2520present%2520a%250Amulti-faceted%2520analysis%2520on%2520training%2520DocVQA%2520models%2520with%2520FL%252C%2520which%2520provides%250Ainsights%2520for%2520future%2520research%2520on%2520this%2520task.%2520We%2520show%2520that%2520our%2520pretraining%250Astrategies%2520can%2520effectively%2520learn%2520and%2520scale%2520up%2520under%2520federated%2520training%2520with%250Adiverse%2520DocVQA%2520datasets%2520and%2520tuning%2520hyperparameters%2520is%2520essential%2520for%2520practical%250Adocument%2520tasks%2520under%2520federation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Document%20Visual%20Question%20Answering%3A%20A%20Pilot%20Study&entry.906535625=Khanh%20Nguyen%20and%20Dimosthenis%20Karatzas&entry.1292438233=%20%20An%20important%20handicap%20of%20document%20analysis%20research%20is%20that%20documents%20tend%20to%0Abe%20copyrighted%20or%20contain%20private%20information%2C%20which%20prohibits%20their%20open%0Apublication%20and%20the%20creation%20of%20centralised%2C%20large-scale%20document%20datasets.%0AInstead%2C%20documents%20are%20scattered%20in%20private%20data%20silos%2C%20making%20extensive%0Atraining%20over%20heterogeneous%20data%20a%20tedious%20task.%20In%20this%20work%2C%20we%20explore%20the%0Ause%20of%20a%20federated%20learning%20%28FL%29%20scheme%20as%20a%20way%20to%20train%20a%20shared%20model%20on%0Adecentralised%20private%20document%20data.%20We%20focus%20on%20the%20problem%20of%20Document%20VQA%2C%20a%0Atask%20particularly%20suited%20to%20this%20approach%2C%20as%20the%20type%20of%20reasoning%0Acapabilities%20required%20from%20the%20model%20can%20be%20quite%20different%20in%20diverse%20domains.%0AEnabling%20training%20over%20heterogeneous%20document%20datasets%20can%20thus%20substantially%0Aenrich%20DocVQA%20models.%20We%20assemble%20existing%20DocVQA%20datasets%20from%20diverse%20domains%0Ato%20reflect%20the%20data%20heterogeneity%20in%20real-world%20applications.%20We%20explore%20the%0Aself-pretraining%20technique%20in%20this%20multi-modal%20setting%2C%20where%20the%20same%20data%20is%0Aused%20for%20both%20pretraining%20and%20finetuning%2C%20making%20it%20relevant%20for%20privacy%0Apreservation.%20We%20further%20propose%20combining%20self-pretraining%20with%20a%20Federated%0ADocVQA%20training%20method%20using%20centralized%20adaptive%20optimization%20that%20outperforms%0Athe%20FedAvg%20baseline.%20With%20extensive%20experiments%2C%20we%20also%20present%20a%0Amulti-faceted%20analysis%20on%20training%20DocVQA%20models%20with%20FL%2C%20which%20provides%0Ainsights%20for%20future%20research%20on%20this%20task.%20We%20show%20that%20our%20pretraining%0Astrategies%20can%20effectively%20learn%20and%20scale%20up%20under%20federated%20training%20with%0Adiverse%20DocVQA%20datasets%20and%20tuning%20hyperparameters%20is%20essential%20for%20practical%0Adocument%20tasks%20under%20federation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06636v1&entry.124074799=Read"},
{"title": "Projection by Convolution: Optimal Sample Complexity for Reinforcement\n  Learning in Continuous-Space MDPs", "author": "Davide Maran and Alberto Maria Metelli and Matteo Papini and Marcello Restelli", "abstract": "  We consider the problem of learning an $\\varepsilon$-optimal policy in a\ngeneral class of continuous-space Markov decision processes (MDPs) having\nsmooth Bellman operators. Given access to a generative model, we achieve\nrate-optimal sample complexity by performing a simple, \\emph{perturbed} version\nof least-squares value iteration with orthogonal trigonometric polynomials as\nfeatures. Key to our solution is a novel projection technique based on ideas\nfrom harmonic analysis. Our~$\\widetilde{\\mathcal{O}}(\\epsilon^{-2-d/(\\nu+1)})$\nsample complexity, where $d$ is the dimension of the state-action space and\n$\\nu$ the order of smoothness, recovers the state-of-the-art result of\ndiscretization approaches for the special case of Lipschitz MDPs $(\\nu=0)$. At\nthe same time, for $\\nu\\to\\infty$, it recovers and greatly generalizes the\n$\\mathcal{O}(\\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to\nregression approaches. In this sense, our result bridges the gap between two\npopular but conflicting perspectives on continuous-space MDPs.\n", "link": "http://arxiv.org/abs/2405.06363v1", "date": "2024-05-10", "relevancy": 1.8715, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4681}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projection%20by%20Convolution%3A%20Optimal%20Sample%20Complexity%20for%20Reinforcement%0A%20%20Learning%20in%20Continuous-Space%20MDPs&body=Title%3A%20Projection%20by%20Convolution%3A%20Optimal%20Sample%20Complexity%20for%20Reinforcement%0A%20%20Learning%20in%20Continuous-Space%20MDPs%0AAuthor%3A%20Davide%20Maran%20and%20Alberto%20Maria%20Metelli%20and%20Matteo%20Papini%20and%20Marcello%20Restelli%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20an%20%24%5Cvarepsilon%24-optimal%20policy%20in%20a%0Ageneral%20class%20of%20continuous-space%20Markov%20decision%20processes%20%28MDPs%29%20having%0Asmooth%20Bellman%20operators.%20Given%20access%20to%20a%20generative%20model%2C%20we%20achieve%0Arate-optimal%20sample%20complexity%20by%20performing%20a%20simple%2C%20%5Cemph%7Bperturbed%7D%20version%0Aof%20least-squares%20value%20iteration%20with%20orthogonal%20trigonometric%20polynomials%20as%0Afeatures.%20Key%20to%20our%20solution%20is%20a%20novel%20projection%20technique%20based%20on%20ideas%0Afrom%20harmonic%20analysis.%20Our~%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2-d/%28%5Cnu%2B1%29%7D%29%24%0Asample%20complexity%2C%20where%20%24d%24%20is%20the%20dimension%20of%20the%20state-action%20space%20and%0A%24%5Cnu%24%20the%20order%20of%20smoothness%2C%20recovers%20the%20state-of-the-art%20result%20of%0Adiscretization%20approaches%20for%20the%20special%20case%20of%20Lipschitz%20MDPs%20%24%28%5Cnu%3D0%29%24.%20At%0Athe%20same%20time%2C%20for%20%24%5Cnu%5Cto%5Cinfty%24%2C%20it%20recovers%20and%20greatly%20generalizes%20the%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20rate%20of%20low-rank%20MDPs%2C%20which%20are%20more%20amenable%20to%0Aregression%20approaches.%20In%20this%20sense%2C%20our%20result%20bridges%20the%20gap%20between%20two%0Apopular%20but%20conflicting%20perspectives%20on%20continuous-space%20MDPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjection%2520by%2520Convolution%253A%2520Optimal%2520Sample%2520Complexity%2520for%2520Reinforcement%250A%2520%2520Learning%2520in%2520Continuous-Space%2520MDPs%26entry.906535625%3DDavide%2520Maran%2520and%2520Alberto%2520Maria%2520Metelli%2520and%2520Matteo%2520Papini%2520and%2520Marcello%2520Restelli%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520an%2520%2524%255Cvarepsilon%2524-optimal%2520policy%2520in%2520a%250Ageneral%2520class%2520of%2520continuous-space%2520Markov%2520decision%2520processes%2520%2528MDPs%2529%2520having%250Asmooth%2520Bellman%2520operators.%2520Given%2520access%2520to%2520a%2520generative%2520model%252C%2520we%2520achieve%250Arate-optimal%2520sample%2520complexity%2520by%2520performing%2520a%2520simple%252C%2520%255Cemph%257Bperturbed%257D%2520version%250Aof%2520least-squares%2520value%2520iteration%2520with%2520orthogonal%2520trigonometric%2520polynomials%2520as%250Afeatures.%2520Key%2520to%2520our%2520solution%2520is%2520a%2520novel%2520projection%2520technique%2520based%2520on%2520ideas%250Afrom%2520harmonic%2520analysis.%2520Our~%2524%255Cwidetilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cepsilon%255E%257B-2-d/%2528%255Cnu%252B1%2529%257D%2529%2524%250Asample%2520complexity%252C%2520where%2520%2524d%2524%2520is%2520the%2520dimension%2520of%2520the%2520state-action%2520space%2520and%250A%2524%255Cnu%2524%2520the%2520order%2520of%2520smoothness%252C%2520recovers%2520the%2520state-of-the-art%2520result%2520of%250Adiscretization%2520approaches%2520for%2520the%2520special%2520case%2520of%2520Lipschitz%2520MDPs%2520%2524%2528%255Cnu%253D0%2529%2524.%2520At%250Athe%2520same%2520time%252C%2520for%2520%2524%255Cnu%255Cto%255Cinfty%2524%252C%2520it%2520recovers%2520and%2520greatly%2520generalizes%2520the%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520rate%2520of%2520low-rank%2520MDPs%252C%2520which%2520are%2520more%2520amenable%2520to%250Aregression%2520approaches.%2520In%2520this%2520sense%252C%2520our%2520result%2520bridges%2520the%2520gap%2520between%2520two%250Apopular%2520but%2520conflicting%2520perspectives%2520on%2520continuous-space%2520MDPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projection%20by%20Convolution%3A%20Optimal%20Sample%20Complexity%20for%20Reinforcement%0A%20%20Learning%20in%20Continuous-Space%20MDPs&entry.906535625=Davide%20Maran%20and%20Alberto%20Maria%20Metelli%20and%20Matteo%20Papini%20and%20Marcello%20Restelli&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20an%20%24%5Cvarepsilon%24-optimal%20policy%20in%20a%0Ageneral%20class%20of%20continuous-space%20Markov%20decision%20processes%20%28MDPs%29%20having%0Asmooth%20Bellman%20operators.%20Given%20access%20to%20a%20generative%20model%2C%20we%20achieve%0Arate-optimal%20sample%20complexity%20by%20performing%20a%20simple%2C%20%5Cemph%7Bperturbed%7D%20version%0Aof%20least-squares%20value%20iteration%20with%20orthogonal%20trigonometric%20polynomials%20as%0Afeatures.%20Key%20to%20our%20solution%20is%20a%20novel%20projection%20technique%20based%20on%20ideas%0Afrom%20harmonic%20analysis.%20Our~%24%5Cwidetilde%7B%5Cmathcal%7BO%7D%7D%28%5Cepsilon%5E%7B-2-d/%28%5Cnu%2B1%29%7D%29%24%0Asample%20complexity%2C%20where%20%24d%24%20is%20the%20dimension%20of%20the%20state-action%20space%20and%0A%24%5Cnu%24%20the%20order%20of%20smoothness%2C%20recovers%20the%20state-of-the-art%20result%20of%0Adiscretization%20approaches%20for%20the%20special%20case%20of%20Lipschitz%20MDPs%20%24%28%5Cnu%3D0%29%24.%20At%0Athe%20same%20time%2C%20for%20%24%5Cnu%5Cto%5Cinfty%24%2C%20it%20recovers%20and%20greatly%20generalizes%20the%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20rate%20of%20low-rank%20MDPs%2C%20which%20are%20more%20amenable%20to%0Aregression%20approaches.%20In%20this%20sense%2C%20our%20result%20bridges%20the%20gap%20between%20two%0Apopular%20but%20conflicting%20perspectives%20on%20continuous-space%20MDPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06363v1&entry.124074799=Read"},
{"title": "Mitigating Hallucinations in Large Language Models via\n  Self-Refinement-Enhanced Knowledge Retrieval", "author": "Mengjia Niu and Hao Li and Jie Shi and Hamed Haddadi and Fan Mo", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, although their susceptibility to hallucination poses\nsignificant challenges for their deployment in critical areas such as\nhealthcare. To address this issue, retrieving relevant facts from knowledge\ngraphs (KGs) is considered a promising method. Existing KG-augmented approaches\ntend to be resource-intensive, requiring multiple rounds of retrieval and\nverification for each factoid, which impedes their application in real-world\nscenarios.\n  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval\n(Re-KGR) to augment the factuality of LLMs' responses with less retrieval\nefforts in the medical field. Our approach leverages the attribution of\nnext-token predictive probability distributions across different tokens, and\nvarious model layers to primarily identify tokens with a high potential for\nhallucination, reducing verification rounds by refining knowledge triples\nassociated with these tokens. Moreover, we rectify inaccurate content using\nretrieved knowledge in the post-processing stage, which improves the\ntruthfulness of generated responses. Experimental results on a medical dataset\ndemonstrate that our approach can enhance the factual capability of LLMs across\nvarious foundational models as evidenced by the highest scores on truthfulness.\n", "link": "http://arxiv.org/abs/2405.06545v1", "date": "2024-05-10", "relevancy": 1.8537, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5284}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4512}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Self-Refinement-Enhanced%20Knowledge%20Retrieval&body=Title%3A%20Mitigating%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Self-Refinement-Enhanced%20Knowledge%20Retrieval%0AAuthor%3A%20Mengjia%20Niu%20and%20Hao%20Li%20and%20Jie%20Shi%20and%20Hamed%20Haddadi%20and%20Fan%20Mo%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20although%20their%20susceptibility%20to%20hallucination%20poses%0Asignificant%20challenges%20for%20their%20deployment%20in%20critical%20areas%20such%20as%0Ahealthcare.%20To%20address%20this%20issue%2C%20retrieving%20relevant%20facts%20from%20knowledge%0Agraphs%20%28KGs%29%20is%20considered%20a%20promising%20method.%20Existing%20KG-augmented%20approaches%0Atend%20to%20be%20resource-intensive%2C%20requiring%20multiple%20rounds%20of%20retrieval%20and%0Averification%20for%20each%20factoid%2C%20which%20impedes%20their%20application%20in%20real-world%0Ascenarios.%0A%20%20In%20this%20study%2C%20we%20propose%20Self-Refinement-Enhanced%20Knowledge%20Graph%20Retrieval%0A%28Re-KGR%29%20to%20augment%20the%20factuality%20of%20LLMs%27%20responses%20with%20less%20retrieval%0Aefforts%20in%20the%20medical%20field.%20Our%20approach%20leverages%20the%20attribution%20of%0Anext-token%20predictive%20probability%20distributions%20across%20different%20tokens%2C%20and%0Avarious%20model%20layers%20to%20primarily%20identify%20tokens%20with%20a%20high%20potential%20for%0Ahallucination%2C%20reducing%20verification%20rounds%20by%20refining%20knowledge%20triples%0Aassociated%20with%20these%20tokens.%20Moreover%2C%20we%20rectify%20inaccurate%20content%20using%0Aretrieved%20knowledge%20in%20the%20post-processing%20stage%2C%20which%20improves%20the%0Atruthfulness%20of%20generated%20responses.%20Experimental%20results%20on%20a%20medical%20dataset%0Ademonstrate%20that%20our%20approach%20can%20enhance%20the%20factual%20capability%20of%20LLMs%20across%0Avarious%20foundational%20models%20as%20evidenced%20by%20the%20highest%20scores%20on%20truthfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucinations%2520in%2520Large%2520Language%2520Models%2520via%250A%2520%2520Self-Refinement-Enhanced%2520Knowledge%2520Retrieval%26entry.906535625%3DMengjia%2520Niu%2520and%2520Hao%2520Li%2520and%2520Jie%2520Shi%2520and%2520Hamed%2520Haddadi%2520and%2520Fan%2520Mo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Avarious%2520domains%252C%2520although%2520their%2520susceptibility%2520to%2520hallucination%2520poses%250Asignificant%2520challenges%2520for%2520their%2520deployment%2520in%2520critical%2520areas%2520such%2520as%250Ahealthcare.%2520To%2520address%2520this%2520issue%252C%2520retrieving%2520relevant%2520facts%2520from%2520knowledge%250Agraphs%2520%2528KGs%2529%2520is%2520considered%2520a%2520promising%2520method.%2520Existing%2520KG-augmented%2520approaches%250Atend%2520to%2520be%2520resource-intensive%252C%2520requiring%2520multiple%2520rounds%2520of%2520retrieval%2520and%250Averification%2520for%2520each%2520factoid%252C%2520which%2520impedes%2520their%2520application%2520in%2520real-world%250Ascenarios.%250A%2520%2520In%2520this%2520study%252C%2520we%2520propose%2520Self-Refinement-Enhanced%2520Knowledge%2520Graph%2520Retrieval%250A%2528Re-KGR%2529%2520to%2520augment%2520the%2520factuality%2520of%2520LLMs%2527%2520responses%2520with%2520less%2520retrieval%250Aefforts%2520in%2520the%2520medical%2520field.%2520Our%2520approach%2520leverages%2520the%2520attribution%2520of%250Anext-token%2520predictive%2520probability%2520distributions%2520across%2520different%2520tokens%252C%2520and%250Avarious%2520model%2520layers%2520to%2520primarily%2520identify%2520tokens%2520with%2520a%2520high%2520potential%2520for%250Ahallucination%252C%2520reducing%2520verification%2520rounds%2520by%2520refining%2520knowledge%2520triples%250Aassociated%2520with%2520these%2520tokens.%2520Moreover%252C%2520we%2520rectify%2520inaccurate%2520content%2520using%250Aretrieved%2520knowledge%2520in%2520the%2520post-processing%2520stage%252C%2520which%2520improves%2520the%250Atruthfulness%2520of%2520generated%2520responses.%2520Experimental%2520results%2520on%2520a%2520medical%2520dataset%250Ademonstrate%2520that%2520our%2520approach%2520can%2520enhance%2520the%2520factual%2520capability%2520of%2520LLMs%2520across%250Avarious%2520foundational%2520models%2520as%2520evidenced%2520by%2520the%2520highest%2520scores%2520on%2520truthfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Self-Refinement-Enhanced%20Knowledge%20Retrieval&entry.906535625=Mengjia%20Niu%20and%20Hao%20Li%20and%20Jie%20Shi%20and%20Hamed%20Haddadi%20and%20Fan%20Mo&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20domains%2C%20although%20their%20susceptibility%20to%20hallucination%20poses%0Asignificant%20challenges%20for%20their%20deployment%20in%20critical%20areas%20such%20as%0Ahealthcare.%20To%20address%20this%20issue%2C%20retrieving%20relevant%20facts%20from%20knowledge%0Agraphs%20%28KGs%29%20is%20considered%20a%20promising%20method.%20Existing%20KG-augmented%20approaches%0Atend%20to%20be%20resource-intensive%2C%20requiring%20multiple%20rounds%20of%20retrieval%20and%0Averification%20for%20each%20factoid%2C%20which%20impedes%20their%20application%20in%20real-world%0Ascenarios.%0A%20%20In%20this%20study%2C%20we%20propose%20Self-Refinement-Enhanced%20Knowledge%20Graph%20Retrieval%0A%28Re-KGR%29%20to%20augment%20the%20factuality%20of%20LLMs%27%20responses%20with%20less%20retrieval%0Aefforts%20in%20the%20medical%20field.%20Our%20approach%20leverages%20the%20attribution%20of%0Anext-token%20predictive%20probability%20distributions%20across%20different%20tokens%2C%20and%0Avarious%20model%20layers%20to%20primarily%20identify%20tokens%20with%20a%20high%20potential%20for%0Ahallucination%2C%20reducing%20verification%20rounds%20by%20refining%20knowledge%20triples%0Aassociated%20with%20these%20tokens.%20Moreover%2C%20we%20rectify%20inaccurate%20content%20using%0Aretrieved%20knowledge%20in%20the%20post-processing%20stage%2C%20which%20improves%20the%0Atruthfulness%20of%20generated%20responses.%20Experimental%20results%20on%20a%20medical%20dataset%0Ademonstrate%20that%20our%20approach%20can%20enhance%20the%20factual%20capability%20of%20LLMs%20across%0Avarious%20foundational%20models%20as%20evidenced%20by%20the%20highest%20scores%20on%20truthfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06545v1&entry.124074799=Read"},
{"title": "Fair Mixed Effects Support Vector Machine", "author": "Jo\u00e3o Vitor Pamplona and Jan Pablo Burgard", "abstract": "  To ensure unbiased and ethical automated predictions, fairness must be a core\nprinciple in machine learning applications. Fairness in machine learning aims\nto mitigate biases present in the training data and model imperfections that\ncould lead to discriminatory outcomes. This is achieved by preventing the model\nfrom making decisions based on sensitive characteristics like ethnicity or\nsexual orientation. A fundamental assumption in machine learning is the\nindependence of observations. However, this assumption often does not hold true\nfor data describing social phenomena, where data points are often clustered\nbased. Hence, if the machine learning models do not account for the cluster\ncorrelations, the results may be biased. Especially high is the bias in cases\nwhere the cluster assignment is correlated to the variable of interest. We\npresent a fair mixed effects support vector machine algorithm that can handle\nboth problems simultaneously. With a reproducible simulation study we\ndemonstrate the impact of clustered data on the quality of fair machine\nlearning predictions.\n", "link": "http://arxiv.org/abs/2405.06433v1", "date": "2024-05-10", "relevancy": 1.8303, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Mixed%20Effects%20Support%20Vector%20Machine&body=Title%3A%20Fair%20Mixed%20Effects%20Support%20Vector%20Machine%0AAuthor%3A%20Jo%C3%A3o%20Vitor%20Pamplona%20and%20Jan%20Pablo%20Burgard%0AAbstract%3A%20%20%20To%20ensure%20unbiased%20and%20ethical%20automated%20predictions%2C%20fairness%20must%20be%20a%20core%0Aprinciple%20in%20machine%20learning%20applications.%20Fairness%20in%20machine%20learning%20aims%0Ato%20mitigate%20biases%20present%20in%20the%20training%20data%20and%20model%20imperfections%20that%0Acould%20lead%20to%20discriminatory%20outcomes.%20This%20is%20achieved%20by%20preventing%20the%20model%0Afrom%20making%20decisions%20based%20on%20sensitive%20characteristics%20like%20ethnicity%20or%0Asexual%20orientation.%20A%20fundamental%20assumption%20in%20machine%20learning%20is%20the%0Aindependence%20of%20observations.%20However%2C%20this%20assumption%20often%20does%20not%20hold%20true%0Afor%20data%20describing%20social%20phenomena%2C%20where%20data%20points%20are%20often%20clustered%0Abased.%20Hence%2C%20if%20the%20machine%20learning%20models%20do%20not%20account%20for%20the%20cluster%0Acorrelations%2C%20the%20results%20may%20be%20biased.%20Especially%20high%20is%20the%20bias%20in%20cases%0Awhere%20the%20cluster%20assignment%20is%20correlated%20to%20the%20variable%20of%20interest.%20We%0Apresent%20a%20fair%20mixed%20effects%20support%20vector%20machine%20algorithm%20that%20can%20handle%0Aboth%20problems%20simultaneously.%20With%20a%20reproducible%20simulation%20study%20we%0Ademonstrate%20the%20impact%20of%20clustered%20data%20on%20the%20quality%20of%20fair%20machine%0Alearning%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Mixed%2520Effects%2520Support%2520Vector%2520Machine%26entry.906535625%3DJo%25C3%25A3o%2520Vitor%2520Pamplona%2520and%2520Jan%2520Pablo%2520Burgard%26entry.1292438233%3D%2520%2520To%2520ensure%2520unbiased%2520and%2520ethical%2520automated%2520predictions%252C%2520fairness%2520must%2520be%2520a%2520core%250Aprinciple%2520in%2520machine%2520learning%2520applications.%2520Fairness%2520in%2520machine%2520learning%2520aims%250Ato%2520mitigate%2520biases%2520present%2520in%2520the%2520training%2520data%2520and%2520model%2520imperfections%2520that%250Acould%2520lead%2520to%2520discriminatory%2520outcomes.%2520This%2520is%2520achieved%2520by%2520preventing%2520the%2520model%250Afrom%2520making%2520decisions%2520based%2520on%2520sensitive%2520characteristics%2520like%2520ethnicity%2520or%250Asexual%2520orientation.%2520A%2520fundamental%2520assumption%2520in%2520machine%2520learning%2520is%2520the%250Aindependence%2520of%2520observations.%2520However%252C%2520this%2520assumption%2520often%2520does%2520not%2520hold%2520true%250Afor%2520data%2520describing%2520social%2520phenomena%252C%2520where%2520data%2520points%2520are%2520often%2520clustered%250Abased.%2520Hence%252C%2520if%2520the%2520machine%2520learning%2520models%2520do%2520not%2520account%2520for%2520the%2520cluster%250Acorrelations%252C%2520the%2520results%2520may%2520be%2520biased.%2520Especially%2520high%2520is%2520the%2520bias%2520in%2520cases%250Awhere%2520the%2520cluster%2520assignment%2520is%2520correlated%2520to%2520the%2520variable%2520of%2520interest.%2520We%250Apresent%2520a%2520fair%2520mixed%2520effects%2520support%2520vector%2520machine%2520algorithm%2520that%2520can%2520handle%250Aboth%2520problems%2520simultaneously.%2520With%2520a%2520reproducible%2520simulation%2520study%2520we%250Ademonstrate%2520the%2520impact%2520of%2520clustered%2520data%2520on%2520the%2520quality%2520of%2520fair%2520machine%250Alearning%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Mixed%20Effects%20Support%20Vector%20Machine&entry.906535625=Jo%C3%A3o%20Vitor%20Pamplona%20and%20Jan%20Pablo%20Burgard&entry.1292438233=%20%20To%20ensure%20unbiased%20and%20ethical%20automated%20predictions%2C%20fairness%20must%20be%20a%20core%0Aprinciple%20in%20machine%20learning%20applications.%20Fairness%20in%20machine%20learning%20aims%0Ato%20mitigate%20biases%20present%20in%20the%20training%20data%20and%20model%20imperfections%20that%0Acould%20lead%20to%20discriminatory%20outcomes.%20This%20is%20achieved%20by%20preventing%20the%20model%0Afrom%20making%20decisions%20based%20on%20sensitive%20characteristics%20like%20ethnicity%20or%0Asexual%20orientation.%20A%20fundamental%20assumption%20in%20machine%20learning%20is%20the%0Aindependence%20of%20observations.%20However%2C%20this%20assumption%20often%20does%20not%20hold%20true%0Afor%20data%20describing%20social%20phenomena%2C%20where%20data%20points%20are%20often%20clustered%0Abased.%20Hence%2C%20if%20the%20machine%20learning%20models%20do%20not%20account%20for%20the%20cluster%0Acorrelations%2C%20the%20results%20may%20be%20biased.%20Especially%20high%20is%20the%20bias%20in%20cases%0Awhere%20the%20cluster%20assignment%20is%20correlated%20to%20the%20variable%20of%20interest.%20We%0Apresent%20a%20fair%20mixed%20effects%20support%20vector%20machine%20algorithm%20that%20can%20handle%0Aboth%20problems%20simultaneously.%20With%20a%20reproducible%20simulation%20study%20we%0Ademonstrate%20the%20impact%20of%20clustered%20data%20on%20the%20quality%20of%20fair%20machine%0Alearning%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06433v1&entry.124074799=Read"},
{"title": "Contextual Affordances for Safe Exploration in Robotic Scenarios", "author": "William Z. Ye and Eduardo B. Sandoval and Pamela Carreno-Medrano and Francisco Cru", "abstract": "  Robotics has been a popular field of research in the past few decades, with\nmuch success in industrial applications such as manufacturing and logistics.\nThis success is led by clearly defined use cases and controlled operating\nenvironments. However, robotics has yet to make a large impact in domestic\nsettings. This is due in part to the difficulty and complexity of designing\nmass-manufactured robots that can succeed in the variety of homes and\nenvironments that humans live in and that can operate safely in close proximity\nto humans. This paper explores the use of contextual affordances to enable safe\nexploration and learning in robotic scenarios targeted in the home. In\nparticular, we propose a simple state representation that allows us to extend\ncontextual affordances to larger state spaces and showcase how affordances can\nimprove the success and convergence rate of a reinforcement learning algorithm\nin simulation. Our results suggest that after further iterations, it is\npossible to consider the implementation of this approach in a real robot\nmanipulator. Furthermore, in the long term, this work could be the foundation\nfor future explorations of human-robot interactions in complex domestic\nenvironments. This could be possible once state-of-the-art robot manipulators\nachieve the required level of dexterity for the described affordances in this\npaper.\n", "link": "http://arxiv.org/abs/2405.06422v1", "date": "2024-05-10", "relevancy": 1.8226, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6058}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Affordances%20for%20Safe%20Exploration%20in%20Robotic%20Scenarios&body=Title%3A%20Contextual%20Affordances%20for%20Safe%20Exploration%20in%20Robotic%20Scenarios%0AAuthor%3A%20William%20Z.%20Ye%20and%20Eduardo%20B.%20Sandoval%20and%20Pamela%20Carreno-Medrano%20and%20Francisco%20Cru%0AAbstract%3A%20%20%20Robotics%20has%20been%20a%20popular%20field%20of%20research%20in%20the%20past%20few%20decades%2C%20with%0Amuch%20success%20in%20industrial%20applications%20such%20as%20manufacturing%20and%20logistics.%0AThis%20success%20is%20led%20by%20clearly%20defined%20use%20cases%20and%20controlled%20operating%0Aenvironments.%20However%2C%20robotics%20has%20yet%20to%20make%20a%20large%20impact%20in%20domestic%0Asettings.%20This%20is%20due%20in%20part%20to%20the%20difficulty%20and%20complexity%20of%20designing%0Amass-manufactured%20robots%20that%20can%20succeed%20in%20the%20variety%20of%20homes%20and%0Aenvironments%20that%20humans%20live%20in%20and%20that%20can%20operate%20safely%20in%20close%20proximity%0Ato%20humans.%20This%20paper%20explores%20the%20use%20of%20contextual%20affordances%20to%20enable%20safe%0Aexploration%20and%20learning%20in%20robotic%20scenarios%20targeted%20in%20the%20home.%20In%0Aparticular%2C%20we%20propose%20a%20simple%20state%20representation%20that%20allows%20us%20to%20extend%0Acontextual%20affordances%20to%20larger%20state%20spaces%20and%20showcase%20how%20affordances%20can%0Aimprove%20the%20success%20and%20convergence%20rate%20of%20a%20reinforcement%20learning%20algorithm%0Ain%20simulation.%20Our%20results%20suggest%20that%20after%20further%20iterations%2C%20it%20is%0Apossible%20to%20consider%20the%20implementation%20of%20this%20approach%20in%20a%20real%20robot%0Amanipulator.%20Furthermore%2C%20in%20the%20long%20term%2C%20this%20work%20could%20be%20the%20foundation%0Afor%20future%20explorations%20of%20human-robot%20interactions%20in%20complex%20domestic%0Aenvironments.%20This%20could%20be%20possible%20once%20state-of-the-art%20robot%20manipulators%0Aachieve%20the%20required%20level%20of%20dexterity%20for%20the%20described%20affordances%20in%20this%0Apaper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Affordances%2520for%2520Safe%2520Exploration%2520in%2520Robotic%2520Scenarios%26entry.906535625%3DWilliam%2520Z.%2520Ye%2520and%2520Eduardo%2520B.%2520Sandoval%2520and%2520Pamela%2520Carreno-Medrano%2520and%2520Francisco%2520Cru%26entry.1292438233%3D%2520%2520Robotics%2520has%2520been%2520a%2520popular%2520field%2520of%2520research%2520in%2520the%2520past%2520few%2520decades%252C%2520with%250Amuch%2520success%2520in%2520industrial%2520applications%2520such%2520as%2520manufacturing%2520and%2520logistics.%250AThis%2520success%2520is%2520led%2520by%2520clearly%2520defined%2520use%2520cases%2520and%2520controlled%2520operating%250Aenvironments.%2520However%252C%2520robotics%2520has%2520yet%2520to%2520make%2520a%2520large%2520impact%2520in%2520domestic%250Asettings.%2520This%2520is%2520due%2520in%2520part%2520to%2520the%2520difficulty%2520and%2520complexity%2520of%2520designing%250Amass-manufactured%2520robots%2520that%2520can%2520succeed%2520in%2520the%2520variety%2520of%2520homes%2520and%250Aenvironments%2520that%2520humans%2520live%2520in%2520and%2520that%2520can%2520operate%2520safely%2520in%2520close%2520proximity%250Ato%2520humans.%2520This%2520paper%2520explores%2520the%2520use%2520of%2520contextual%2520affordances%2520to%2520enable%2520safe%250Aexploration%2520and%2520learning%2520in%2520robotic%2520scenarios%2520targeted%2520in%2520the%2520home.%2520In%250Aparticular%252C%2520we%2520propose%2520a%2520simple%2520state%2520representation%2520that%2520allows%2520us%2520to%2520extend%250Acontextual%2520affordances%2520to%2520larger%2520state%2520spaces%2520and%2520showcase%2520how%2520affordances%2520can%250Aimprove%2520the%2520success%2520and%2520convergence%2520rate%2520of%2520a%2520reinforcement%2520learning%2520algorithm%250Ain%2520simulation.%2520Our%2520results%2520suggest%2520that%2520after%2520further%2520iterations%252C%2520it%2520is%250Apossible%2520to%2520consider%2520the%2520implementation%2520of%2520this%2520approach%2520in%2520a%2520real%2520robot%250Amanipulator.%2520Furthermore%252C%2520in%2520the%2520long%2520term%252C%2520this%2520work%2520could%2520be%2520the%2520foundation%250Afor%2520future%2520explorations%2520of%2520human-robot%2520interactions%2520in%2520complex%2520domestic%250Aenvironments.%2520This%2520could%2520be%2520possible%2520once%2520state-of-the-art%2520robot%2520manipulators%250Aachieve%2520the%2520required%2520level%2520of%2520dexterity%2520for%2520the%2520described%2520affordances%2520in%2520this%250Apaper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Affordances%20for%20Safe%20Exploration%20in%20Robotic%20Scenarios&entry.906535625=William%20Z.%20Ye%20and%20Eduardo%20B.%20Sandoval%20and%20Pamela%20Carreno-Medrano%20and%20Francisco%20Cru&entry.1292438233=%20%20Robotics%20has%20been%20a%20popular%20field%20of%20research%20in%20the%20past%20few%20decades%2C%20with%0Amuch%20success%20in%20industrial%20applications%20such%20as%20manufacturing%20and%20logistics.%0AThis%20success%20is%20led%20by%20clearly%20defined%20use%20cases%20and%20controlled%20operating%0Aenvironments.%20However%2C%20robotics%20has%20yet%20to%20make%20a%20large%20impact%20in%20domestic%0Asettings.%20This%20is%20due%20in%20part%20to%20the%20difficulty%20and%20complexity%20of%20designing%0Amass-manufactured%20robots%20that%20can%20succeed%20in%20the%20variety%20of%20homes%20and%0Aenvironments%20that%20humans%20live%20in%20and%20that%20can%20operate%20safely%20in%20close%20proximity%0Ato%20humans.%20This%20paper%20explores%20the%20use%20of%20contextual%20affordances%20to%20enable%20safe%0Aexploration%20and%20learning%20in%20robotic%20scenarios%20targeted%20in%20the%20home.%20In%0Aparticular%2C%20we%20propose%20a%20simple%20state%20representation%20that%20allows%20us%20to%20extend%0Acontextual%20affordances%20to%20larger%20state%20spaces%20and%20showcase%20how%20affordances%20can%0Aimprove%20the%20success%20and%20convergence%20rate%20of%20a%20reinforcement%20learning%20algorithm%0Ain%20simulation.%20Our%20results%20suggest%20that%20after%20further%20iterations%2C%20it%20is%0Apossible%20to%20consider%20the%20implementation%20of%20this%20approach%20in%20a%20real%20robot%0Amanipulator.%20Furthermore%2C%20in%20the%20long%20term%2C%20this%20work%20could%20be%20the%20foundation%0Afor%20future%20explorations%20of%20human-robot%20interactions%20in%20complex%20domestic%0Aenvironments.%20This%20could%20be%20possible%20once%20state-of-the-art%20robot%20manipulators%0Aachieve%20the%20required%20level%20of%20dexterity%20for%20the%20described%20affordances%20in%20this%0Apaper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06422v1&entry.124074799=Read"},
{"title": "No-Regret is not enough! Bandits with General Constraints through\n  Adaptive Regret Minimization", "author": "Martino Bernasconi and Matteo Castiglioni and Andrea Celli", "abstract": "  In the bandits with knapsacks framework (BwK) the learner has $m$\nresource-consumption (packing) constraints. We focus on the generalization of\nBwK in which the learner has a set of general long-term constraints. The goal\nof the learner is to maximize their cumulative reward, while at the same time\nachieving small cumulative constraints violations. In this scenario, there\nexist simple instances where conventional methods for BwK fail to yield\nsublinear violations of constraints. We show that it is possible to circumvent\nthis issue by requiring the primal and dual algorithm to be weakly adaptive.\nIndeed, even in absence on any information on the Slater's parameter $\\rho$\ncharacterizing the problem, the interplay between weakly adaptive primal and\ndual regret minimizers yields a \"self-bounding\" property of dual variables. In\nparticular, their norm remains suitably upper bounded across the entire time\nhorizon even without explicit projection steps. By exploiting this property, we\nprovide best-of-both-worlds guarantees for stochastic and adversarial inputs.\nIn the first case, we show that the algorithm guarantees sublinear regret. In\nthe latter case, we establish a tight competitive ratio of $\\rho/(1+\\rho)$. In\nboth settings, constraints violations are guaranteed to be sublinear in time.\nFinally, this results allow us to obtain new result for the problem of\ncontextual bandits with linear constraints, providing the first\nno-$\\alpha$-regret guarantees for adversarial contexts.\n", "link": "http://arxiv.org/abs/2405.06575v1", "date": "2024-05-10", "relevancy": 1.8048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4683}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4606}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No-Regret%20is%20not%20enough%21%20Bandits%20with%20General%20Constraints%20through%0A%20%20Adaptive%20Regret%20Minimization&body=Title%3A%20No-Regret%20is%20not%20enough%21%20Bandits%20with%20General%20Constraints%20through%0A%20%20Adaptive%20Regret%20Minimization%0AAuthor%3A%20Martino%20Bernasconi%20and%20Matteo%20Castiglioni%20and%20Andrea%20Celli%0AAbstract%3A%20%20%20In%20the%20bandits%20with%20knapsacks%20framework%20%28BwK%29%20the%20learner%20has%20%24m%24%0Aresource-consumption%20%28packing%29%20constraints.%20We%20focus%20on%20the%20generalization%20of%0ABwK%20in%20which%20the%20learner%20has%20a%20set%20of%20general%20long-term%20constraints.%20The%20goal%0Aof%20the%20learner%20is%20to%20maximize%20their%20cumulative%20reward%2C%20while%20at%20the%20same%20time%0Aachieving%20small%20cumulative%20constraints%20violations.%20In%20this%20scenario%2C%20there%0Aexist%20simple%20instances%20where%20conventional%20methods%20for%20BwK%20fail%20to%20yield%0Asublinear%20violations%20of%20constraints.%20We%20show%20that%20it%20is%20possible%20to%20circumvent%0Athis%20issue%20by%20requiring%20the%20primal%20and%20dual%20algorithm%20to%20be%20weakly%20adaptive.%0AIndeed%2C%20even%20in%20absence%20on%20any%20information%20on%20the%20Slater%27s%20parameter%20%24%5Crho%24%0Acharacterizing%20the%20problem%2C%20the%20interplay%20between%20weakly%20adaptive%20primal%20and%0Adual%20regret%20minimizers%20yields%20a%20%22self-bounding%22%20property%20of%20dual%20variables.%20In%0Aparticular%2C%20their%20norm%20remains%20suitably%20upper%20bounded%20across%20the%20entire%20time%0Ahorizon%20even%20without%20explicit%20projection%20steps.%20By%20exploiting%20this%20property%2C%20we%0Aprovide%20best-of-both-worlds%20guarantees%20for%20stochastic%20and%20adversarial%20inputs.%0AIn%20the%20first%20case%2C%20we%20show%20that%20the%20algorithm%20guarantees%20sublinear%20regret.%20In%0Athe%20latter%20case%2C%20we%20establish%20a%20tight%20competitive%20ratio%20of%20%24%5Crho/%281%2B%5Crho%29%24.%20In%0Aboth%20settings%2C%20constraints%20violations%20are%20guaranteed%20to%20be%20sublinear%20in%20time.%0AFinally%2C%20this%20results%20allow%20us%20to%20obtain%20new%20result%20for%20the%20problem%20of%0Acontextual%20bandits%20with%20linear%20constraints%2C%20providing%20the%20first%0Ano-%24%5Calpha%24-regret%20guarantees%20for%20adversarial%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo-Regret%2520is%2520not%2520enough%2521%2520Bandits%2520with%2520General%2520Constraints%2520through%250A%2520%2520Adaptive%2520Regret%2520Minimization%26entry.906535625%3DMartino%2520Bernasconi%2520and%2520Matteo%2520Castiglioni%2520and%2520Andrea%2520Celli%26entry.1292438233%3D%2520%2520In%2520the%2520bandits%2520with%2520knapsacks%2520framework%2520%2528BwK%2529%2520the%2520learner%2520has%2520%2524m%2524%250Aresource-consumption%2520%2528packing%2529%2520constraints.%2520We%2520focus%2520on%2520the%2520generalization%2520of%250ABwK%2520in%2520which%2520the%2520learner%2520has%2520a%2520set%2520of%2520general%2520long-term%2520constraints.%2520The%2520goal%250Aof%2520the%2520learner%2520is%2520to%2520maximize%2520their%2520cumulative%2520reward%252C%2520while%2520at%2520the%2520same%2520time%250Aachieving%2520small%2520cumulative%2520constraints%2520violations.%2520In%2520this%2520scenario%252C%2520there%250Aexist%2520simple%2520instances%2520where%2520conventional%2520methods%2520for%2520BwK%2520fail%2520to%2520yield%250Asublinear%2520violations%2520of%2520constraints.%2520We%2520show%2520that%2520it%2520is%2520possible%2520to%2520circumvent%250Athis%2520issue%2520by%2520requiring%2520the%2520primal%2520and%2520dual%2520algorithm%2520to%2520be%2520weakly%2520adaptive.%250AIndeed%252C%2520even%2520in%2520absence%2520on%2520any%2520information%2520on%2520the%2520Slater%2527s%2520parameter%2520%2524%255Crho%2524%250Acharacterizing%2520the%2520problem%252C%2520the%2520interplay%2520between%2520weakly%2520adaptive%2520primal%2520and%250Adual%2520regret%2520minimizers%2520yields%2520a%2520%2522self-bounding%2522%2520property%2520of%2520dual%2520variables.%2520In%250Aparticular%252C%2520their%2520norm%2520remains%2520suitably%2520upper%2520bounded%2520across%2520the%2520entire%2520time%250Ahorizon%2520even%2520without%2520explicit%2520projection%2520steps.%2520By%2520exploiting%2520this%2520property%252C%2520we%250Aprovide%2520best-of-both-worlds%2520guarantees%2520for%2520stochastic%2520and%2520adversarial%2520inputs.%250AIn%2520the%2520first%2520case%252C%2520we%2520show%2520that%2520the%2520algorithm%2520guarantees%2520sublinear%2520regret.%2520In%250Athe%2520latter%2520case%252C%2520we%2520establish%2520a%2520tight%2520competitive%2520ratio%2520of%2520%2524%255Crho/%25281%252B%255Crho%2529%2524.%2520In%250Aboth%2520settings%252C%2520constraints%2520violations%2520are%2520guaranteed%2520to%2520be%2520sublinear%2520in%2520time.%250AFinally%252C%2520this%2520results%2520allow%2520us%2520to%2520obtain%2520new%2520result%2520for%2520the%2520problem%2520of%250Acontextual%2520bandits%2520with%2520linear%2520constraints%252C%2520providing%2520the%2520first%250Ano-%2524%255Calpha%2524-regret%2520guarantees%2520for%2520adversarial%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No-Regret%20is%20not%20enough%21%20Bandits%20with%20General%20Constraints%20through%0A%20%20Adaptive%20Regret%20Minimization&entry.906535625=Martino%20Bernasconi%20and%20Matteo%20Castiglioni%20and%20Andrea%20Celli&entry.1292438233=%20%20In%20the%20bandits%20with%20knapsacks%20framework%20%28BwK%29%20the%20learner%20has%20%24m%24%0Aresource-consumption%20%28packing%29%20constraints.%20We%20focus%20on%20the%20generalization%20of%0ABwK%20in%20which%20the%20learner%20has%20a%20set%20of%20general%20long-term%20constraints.%20The%20goal%0Aof%20the%20learner%20is%20to%20maximize%20their%20cumulative%20reward%2C%20while%20at%20the%20same%20time%0Aachieving%20small%20cumulative%20constraints%20violations.%20In%20this%20scenario%2C%20there%0Aexist%20simple%20instances%20where%20conventional%20methods%20for%20BwK%20fail%20to%20yield%0Asublinear%20violations%20of%20constraints.%20We%20show%20that%20it%20is%20possible%20to%20circumvent%0Athis%20issue%20by%20requiring%20the%20primal%20and%20dual%20algorithm%20to%20be%20weakly%20adaptive.%0AIndeed%2C%20even%20in%20absence%20on%20any%20information%20on%20the%20Slater%27s%20parameter%20%24%5Crho%24%0Acharacterizing%20the%20problem%2C%20the%20interplay%20between%20weakly%20adaptive%20primal%20and%0Adual%20regret%20minimizers%20yields%20a%20%22self-bounding%22%20property%20of%20dual%20variables.%20In%0Aparticular%2C%20their%20norm%20remains%20suitably%20upper%20bounded%20across%20the%20entire%20time%0Ahorizon%20even%20without%20explicit%20projection%20steps.%20By%20exploiting%20this%20property%2C%20we%0Aprovide%20best-of-both-worlds%20guarantees%20for%20stochastic%20and%20adversarial%20inputs.%0AIn%20the%20first%20case%2C%20we%20show%20that%20the%20algorithm%20guarantees%20sublinear%20regret.%20In%0Athe%20latter%20case%2C%20we%20establish%20a%20tight%20competitive%20ratio%20of%20%24%5Crho/%281%2B%5Crho%29%24.%20In%0Aboth%20settings%2C%20constraints%20violations%20are%20guaranteed%20to%20be%20sublinear%20in%20time.%0AFinally%2C%20this%20results%20allow%20us%20to%20obtain%20new%20result%20for%20the%20problem%20of%0Acontextual%20bandits%20with%20linear%20constraints%2C%20providing%20the%20first%0Ano-%24%5Calpha%24-regret%20guarantees%20for%20adversarial%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06575v1&entry.124074799=Read"},
{"title": "Towards Comprehensive Multimodal Perception: Introducing the\n  Touch-Language-Vision Dataset", "author": "Ning Cheng and You Li and Jing Gao and Bin Fang and Jinan Xu and Wenjuan Han", "abstract": "  Tactility provides crucial support and enhancement for the perception and\ninteraction capabilities of both humans and robots. Nevertheless, the\nmultimodal research related to touch primarily focuses on visual and tactile\nmodalities, with limited exploration in the domain of language. Beyond\nvocabulary, sentence-level descriptions contain richer semantics. Based on\nthis, we construct a touch-language-vision dataset named TLV\n(Touch-Language-Vision) by human-machine cascade collaboration, featuring\nsentence-level descriptions for multimode alignment. The new dataset is used to\nfine-tune our proposed lightweight training framework, TLV-Link (Linking Touch,\nLanguage, and Vision through Alignment), achieving effective semantic alignment\nwith minimal parameter adjustments (1%). Project Page:\nhttps://xiaoen0.github.io/touch.page/.\n", "link": "http://arxiv.org/abs/2403.09813v2", "date": "2024-05-10", "relevancy": 1.7953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Comprehensive%20Multimodal%20Perception%3A%20Introducing%20the%0A%20%20Touch-Language-Vision%20Dataset&body=Title%3A%20Towards%20Comprehensive%20Multimodal%20Perception%3A%20Introducing%20the%0A%20%20Touch-Language-Vision%20Dataset%0AAuthor%3A%20Ning%20Cheng%20and%20You%20Li%20and%20Jing%20Gao%20and%20Bin%20Fang%20and%20Jinan%20Xu%20and%20Wenjuan%20Han%0AAbstract%3A%20%20%20Tactility%20provides%20crucial%20support%20and%20enhancement%20for%20the%20perception%20and%0Ainteraction%20capabilities%20of%20both%20humans%20and%20robots.%20Nevertheless%2C%20the%0Amultimodal%20research%20related%20to%20touch%20primarily%20focuses%20on%20visual%20and%20tactile%0Amodalities%2C%20with%20limited%20exploration%20in%20the%20domain%20of%20language.%20Beyond%0Avocabulary%2C%20sentence-level%20descriptions%20contain%20richer%20semantics.%20Based%20on%0Athis%2C%20we%20construct%20a%20touch-language-vision%20dataset%20named%20TLV%0A%28Touch-Language-Vision%29%20by%20human-machine%20cascade%20collaboration%2C%20featuring%0Asentence-level%20descriptions%20for%20multimode%20alignment.%20The%20new%20dataset%20is%20used%20to%0Afine-tune%20our%20proposed%20lightweight%20training%20framework%2C%20TLV-Link%20%28Linking%20Touch%2C%0ALanguage%2C%20and%20Vision%20through%20Alignment%29%2C%20achieving%20effective%20semantic%20alignment%0Awith%20minimal%20parameter%20adjustments%20%281%25%29.%20Project%20Page%3A%0Ahttps%3A//xiaoen0.github.io/touch.page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Comprehensive%2520Multimodal%2520Perception%253A%2520Introducing%2520the%250A%2520%2520Touch-Language-Vision%2520Dataset%26entry.906535625%3DNing%2520Cheng%2520and%2520You%2520Li%2520and%2520Jing%2520Gao%2520and%2520Bin%2520Fang%2520and%2520Jinan%2520Xu%2520and%2520Wenjuan%2520Han%26entry.1292438233%3D%2520%2520Tactility%2520provides%2520crucial%2520support%2520and%2520enhancement%2520for%2520the%2520perception%2520and%250Ainteraction%2520capabilities%2520of%2520both%2520humans%2520and%2520robots.%2520Nevertheless%252C%2520the%250Amultimodal%2520research%2520related%2520to%2520touch%2520primarily%2520focuses%2520on%2520visual%2520and%2520tactile%250Amodalities%252C%2520with%2520limited%2520exploration%2520in%2520the%2520domain%2520of%2520language.%2520Beyond%250Avocabulary%252C%2520sentence-level%2520descriptions%2520contain%2520richer%2520semantics.%2520Based%2520on%250Athis%252C%2520we%2520construct%2520a%2520touch-language-vision%2520dataset%2520named%2520TLV%250A%2528Touch-Language-Vision%2529%2520by%2520human-machine%2520cascade%2520collaboration%252C%2520featuring%250Asentence-level%2520descriptions%2520for%2520multimode%2520alignment.%2520The%2520new%2520dataset%2520is%2520used%2520to%250Afine-tune%2520our%2520proposed%2520lightweight%2520training%2520framework%252C%2520TLV-Link%2520%2528Linking%2520Touch%252C%250ALanguage%252C%2520and%2520Vision%2520through%2520Alignment%2529%252C%2520achieving%2520effective%2520semantic%2520alignment%250Awith%2520minimal%2520parameter%2520adjustments%2520%25281%2525%2529.%2520Project%2520Page%253A%250Ahttps%253A//xiaoen0.github.io/touch.page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Comprehensive%20Multimodal%20Perception%3A%20Introducing%20the%0A%20%20Touch-Language-Vision%20Dataset&entry.906535625=Ning%20Cheng%20and%20You%20Li%20and%20Jing%20Gao%20and%20Bin%20Fang%20and%20Jinan%20Xu%20and%20Wenjuan%20Han&entry.1292438233=%20%20Tactility%20provides%20crucial%20support%20and%20enhancement%20for%20the%20perception%20and%0Ainteraction%20capabilities%20of%20both%20humans%20and%20robots.%20Nevertheless%2C%20the%0Amultimodal%20research%20related%20to%20touch%20primarily%20focuses%20on%20visual%20and%20tactile%0Amodalities%2C%20with%20limited%20exploration%20in%20the%20domain%20of%20language.%20Beyond%0Avocabulary%2C%20sentence-level%20descriptions%20contain%20richer%20semantics.%20Based%20on%0Athis%2C%20we%20construct%20a%20touch-language-vision%20dataset%20named%20TLV%0A%28Touch-Language-Vision%29%20by%20human-machine%20cascade%20collaboration%2C%20featuring%0Asentence-level%20descriptions%20for%20multimode%20alignment.%20The%20new%20dataset%20is%20used%20to%0Afine-tune%20our%20proposed%20lightweight%20training%20framework%2C%20TLV-Link%20%28Linking%20Touch%2C%0ALanguage%2C%20and%20Vision%20through%20Alignment%29%2C%20achieving%20effective%20semantic%20alignment%0Awith%20minimal%20parameter%20adjustments%20%281%25%29.%20Project%20Page%3A%0Ahttps%3A//xiaoen0.github.io/touch.page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09813v2&entry.124074799=Read"},
{"title": "An Investigation of Incorporating Mamba for Speech Enhancement", "author": "Rong Chao and Wen-Huang Cheng and Moreno La Quatra and Sabato Marco Siniscalchi and Chao-Han Huck Yang and Szu-Wei Fu and Yu Tsao", "abstract": "  This work aims to study a scalable state-space model (SSM), Mamba, for the\nspeech enhancement (SE) task. We exploit a Mamba-based regression model to\ncharacterize speech signals and build an SE system upon Mamba, termed SEMamba.\nWe explore the properties of Mamba by integrating it as the core model in both\nbasic and advanced SE systems, along with utilizing signal-level distances as\nwell as metric-oriented loss functions. SEMamba demonstrates promising results\nand attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset. When combined\nwith the perceptual contrast stretching technique, the proposed SEMamba yields\na new state-of-the-art PESQ score of 3.69.\n", "link": "http://arxiv.org/abs/2405.06573v1", "date": "2024-05-10", "relevancy": 1.7883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4569}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Investigation%20of%20Incorporating%20Mamba%20for%20Speech%20Enhancement&body=Title%3A%20An%20Investigation%20of%20Incorporating%20Mamba%20for%20Speech%20Enhancement%0AAuthor%3A%20Rong%20Chao%20and%20Wen-Huang%20Cheng%20and%20Moreno%20La%20Quatra%20and%20Sabato%20Marco%20Siniscalchi%20and%20Chao-Han%20Huck%20Yang%20and%20Szu-Wei%20Fu%20and%20Yu%20Tsao%0AAbstract%3A%20%20%20This%20work%20aims%20to%20study%20a%20scalable%20state-space%20model%20%28SSM%29%2C%20Mamba%2C%20for%20the%0Aspeech%20enhancement%20%28SE%29%20task.%20We%20exploit%20a%20Mamba-based%20regression%20model%20to%0Acharacterize%20speech%20signals%20and%20build%20an%20SE%20system%20upon%20Mamba%2C%20termed%20SEMamba.%0AWe%20explore%20the%20properties%20of%20Mamba%20by%20integrating%20it%20as%20the%20core%20model%20in%20both%0Abasic%20and%20advanced%20SE%20systems%2C%20along%20with%20utilizing%20signal-level%20distances%20as%0Awell%20as%20metric-oriented%20loss%20functions.%20SEMamba%20demonstrates%20promising%20results%0Aand%20attains%20a%20PESQ%20score%20of%203.55%20on%20the%20VoiceBank-DEMAND%20dataset.%20When%20combined%0Awith%20the%20perceptual%20contrast%20stretching%20technique%2C%20the%20proposed%20SEMamba%20yields%0Aa%20new%20state-of-the-art%20PESQ%20score%20of%203.69.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Investigation%2520of%2520Incorporating%2520Mamba%2520for%2520Speech%2520Enhancement%26entry.906535625%3DRong%2520Chao%2520and%2520Wen-Huang%2520Cheng%2520and%2520Moreno%2520La%2520Quatra%2520and%2520Sabato%2520Marco%2520Siniscalchi%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Szu-Wei%2520Fu%2520and%2520Yu%2520Tsao%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520study%2520a%2520scalable%2520state-space%2520model%2520%2528SSM%2529%252C%2520Mamba%252C%2520for%2520the%250Aspeech%2520enhancement%2520%2528SE%2529%2520task.%2520We%2520exploit%2520a%2520Mamba-based%2520regression%2520model%2520to%250Acharacterize%2520speech%2520signals%2520and%2520build%2520an%2520SE%2520system%2520upon%2520Mamba%252C%2520termed%2520SEMamba.%250AWe%2520explore%2520the%2520properties%2520of%2520Mamba%2520by%2520integrating%2520it%2520as%2520the%2520core%2520model%2520in%2520both%250Abasic%2520and%2520advanced%2520SE%2520systems%252C%2520along%2520with%2520utilizing%2520signal-level%2520distances%2520as%250Awell%2520as%2520metric-oriented%2520loss%2520functions.%2520SEMamba%2520demonstrates%2520promising%2520results%250Aand%2520attains%2520a%2520PESQ%2520score%2520of%25203.55%2520on%2520the%2520VoiceBank-DEMAND%2520dataset.%2520When%2520combined%250Awith%2520the%2520perceptual%2520contrast%2520stretching%2520technique%252C%2520the%2520proposed%2520SEMamba%2520yields%250Aa%2520new%2520state-of-the-art%2520PESQ%2520score%2520of%25203.69.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Investigation%20of%20Incorporating%20Mamba%20for%20Speech%20Enhancement&entry.906535625=Rong%20Chao%20and%20Wen-Huang%20Cheng%20and%20Moreno%20La%20Quatra%20and%20Sabato%20Marco%20Siniscalchi%20and%20Chao-Han%20Huck%20Yang%20and%20Szu-Wei%20Fu%20and%20Yu%20Tsao&entry.1292438233=%20%20This%20work%20aims%20to%20study%20a%20scalable%20state-space%20model%20%28SSM%29%2C%20Mamba%2C%20for%20the%0Aspeech%20enhancement%20%28SE%29%20task.%20We%20exploit%20a%20Mamba-based%20regression%20model%20to%0Acharacterize%20speech%20signals%20and%20build%20an%20SE%20system%20upon%20Mamba%2C%20termed%20SEMamba.%0AWe%20explore%20the%20properties%20of%20Mamba%20by%20integrating%20it%20as%20the%20core%20model%20in%20both%0Abasic%20and%20advanced%20SE%20systems%2C%20along%20with%20utilizing%20signal-level%20distances%20as%0Awell%20as%20metric-oriented%20loss%20functions.%20SEMamba%20demonstrates%20promising%20results%0Aand%20attains%20a%20PESQ%20score%20of%203.55%20on%20the%20VoiceBank-DEMAND%20dataset.%20When%20combined%0Awith%20the%20perceptual%20contrast%20stretching%20technique%2C%20the%20proposed%20SEMamba%20yields%0Aa%20new%20state-of-the-art%20PESQ%20score%20of%203.69.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06573v1&entry.124074799=Read"},
{"title": "Intrinsic Bayesian Cram\u00e9r-Rao Bound with an Application to Covariance\n  Matrix Estimation", "author": "Florent Bouchard and Alexandre Renaux and Guillaume Ginolhac and Arnaud Breloy", "abstract": "  This paper presents a new performance bound for estimation problems where the\nparameter to estimate lies in a Riemannian manifold (a smooth manifold endowed\nwith a Riemannian metric) and follows a given prior distribution. In this\nsetup, the chosen Riemannian metric induces a geometry for the parameter\nmanifold, as well as an intrinsic notion of the estimation error measure.\nPerformance bound for such error measure were previously obtained in the\nnon-Bayesian case (when the unknown parameter is assumed to deterministic), and\nreferred to as \\textit{intrinsic} Cram\\'er-Rao bound. The presented result then\nappears either as: \\textit{a}) an extension of the intrinsic Cram\\'er-Rao bound\nto the Bayesian estimation framework; \\textit{b}) a generalization of the\nVan-Trees inequality (Bayesian Cram\\'er-Rao bound) that accounts for the\naforementioned geometric structures. In a second part, we leverage this\nformalism to study the problem of covariance matrix estimation when the data\nfollow a Gaussian distribution, and whose covariance matrix is drawn from an\ninverse Wishart distribution. Performance bounds for this problem are obtained\nfor both the mean squared error (Euclidean metric) and the natural Riemannian\ndistance for Hermitian positive definite matrices (affine invariant metric).\nNumerical simulation illustrate that assessing the error with the affine\ninvariant metric is revealing of interesting properties of the maximum a\nposteriori and minimum mean square error estimator, which are not observed when\nusing the Euclidean metric.\n", "link": "http://arxiv.org/abs/2311.04748v2", "date": "2024-05-10", "relevancy": 1.7778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4367}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Bayesian%20Cram%C3%A9r-Rao%20Bound%20with%20an%20Application%20to%20Covariance%0A%20%20Matrix%20Estimation&body=Title%3A%20Intrinsic%20Bayesian%20Cram%C3%A9r-Rao%20Bound%20with%20an%20Application%20to%20Covariance%0A%20%20Matrix%20Estimation%0AAuthor%3A%20Florent%20Bouchard%20and%20Alexandre%20Renaux%20and%20Guillaume%20Ginolhac%20and%20Arnaud%20Breloy%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20performance%20bound%20for%20estimation%20problems%20where%20the%0Aparameter%20to%20estimate%20lies%20in%20a%20Riemannian%20manifold%20%28a%20smooth%20manifold%20endowed%0Awith%20a%20Riemannian%20metric%29%20and%20follows%20a%20given%20prior%20distribution.%20In%20this%0Asetup%2C%20the%20chosen%20Riemannian%20metric%20induces%20a%20geometry%20for%20the%20parameter%0Amanifold%2C%20as%20well%20as%20an%20intrinsic%20notion%20of%20the%20estimation%20error%20measure.%0APerformance%20bound%20for%20such%20error%20measure%20were%20previously%20obtained%20in%20the%0Anon-Bayesian%20case%20%28when%20the%20unknown%20parameter%20is%20assumed%20to%20deterministic%29%2C%20and%0Areferred%20to%20as%20%5Ctextit%7Bintrinsic%7D%20Cram%5C%27er-Rao%20bound.%20The%20presented%20result%20then%0Aappears%20either%20as%3A%20%5Ctextit%7Ba%7D%29%20an%20extension%20of%20the%20intrinsic%20Cram%5C%27er-Rao%20bound%0Ato%20the%20Bayesian%20estimation%20framework%3B%20%5Ctextit%7Bb%7D%29%20a%20generalization%20of%20the%0AVan-Trees%20inequality%20%28Bayesian%20Cram%5C%27er-Rao%20bound%29%20that%20accounts%20for%20the%0Aaforementioned%20geometric%20structures.%20In%20a%20second%20part%2C%20we%20leverage%20this%0Aformalism%20to%20study%20the%20problem%20of%20covariance%20matrix%20estimation%20when%20the%20data%0Afollow%20a%20Gaussian%20distribution%2C%20and%20whose%20covariance%20matrix%20is%20drawn%20from%20an%0Ainverse%20Wishart%20distribution.%20Performance%20bounds%20for%20this%20problem%20are%20obtained%0Afor%20both%20the%20mean%20squared%20error%20%28Euclidean%20metric%29%20and%20the%20natural%20Riemannian%0Adistance%20for%20Hermitian%20positive%20definite%20matrices%20%28affine%20invariant%20metric%29.%0ANumerical%20simulation%20illustrate%20that%20assessing%20the%20error%20with%20the%20affine%0Ainvariant%20metric%20is%20revealing%20of%20interesting%20properties%20of%20the%20maximum%20a%0Aposteriori%20and%20minimum%20mean%20square%20error%20estimator%2C%20which%20are%20not%20observed%20when%0Ausing%20the%20Euclidean%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520Bayesian%2520Cram%25C3%25A9r-Rao%2520Bound%2520with%2520an%2520Application%2520to%2520Covariance%250A%2520%2520Matrix%2520Estimation%26entry.906535625%3DFlorent%2520Bouchard%2520and%2520Alexandre%2520Renaux%2520and%2520Guillaume%2520Ginolhac%2520and%2520Arnaud%2520Breloy%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520performance%2520bound%2520for%2520estimation%2520problems%2520where%2520the%250Aparameter%2520to%2520estimate%2520lies%2520in%2520a%2520Riemannian%2520manifold%2520%2528a%2520smooth%2520manifold%2520endowed%250Awith%2520a%2520Riemannian%2520metric%2529%2520and%2520follows%2520a%2520given%2520prior%2520distribution.%2520In%2520this%250Asetup%252C%2520the%2520chosen%2520Riemannian%2520metric%2520induces%2520a%2520geometry%2520for%2520the%2520parameter%250Amanifold%252C%2520as%2520well%2520as%2520an%2520intrinsic%2520notion%2520of%2520the%2520estimation%2520error%2520measure.%250APerformance%2520bound%2520for%2520such%2520error%2520measure%2520were%2520previously%2520obtained%2520in%2520the%250Anon-Bayesian%2520case%2520%2528when%2520the%2520unknown%2520parameter%2520is%2520assumed%2520to%2520deterministic%2529%252C%2520and%250Areferred%2520to%2520as%2520%255Ctextit%257Bintrinsic%257D%2520Cram%255C%2527er-Rao%2520bound.%2520The%2520presented%2520result%2520then%250Aappears%2520either%2520as%253A%2520%255Ctextit%257Ba%257D%2529%2520an%2520extension%2520of%2520the%2520intrinsic%2520Cram%255C%2527er-Rao%2520bound%250Ato%2520the%2520Bayesian%2520estimation%2520framework%253B%2520%255Ctextit%257Bb%257D%2529%2520a%2520generalization%2520of%2520the%250AVan-Trees%2520inequality%2520%2528Bayesian%2520Cram%255C%2527er-Rao%2520bound%2529%2520that%2520accounts%2520for%2520the%250Aaforementioned%2520geometric%2520structures.%2520In%2520a%2520second%2520part%252C%2520we%2520leverage%2520this%250Aformalism%2520to%2520study%2520the%2520problem%2520of%2520covariance%2520matrix%2520estimation%2520when%2520the%2520data%250Afollow%2520a%2520Gaussian%2520distribution%252C%2520and%2520whose%2520covariance%2520matrix%2520is%2520drawn%2520from%2520an%250Ainverse%2520Wishart%2520distribution.%2520Performance%2520bounds%2520for%2520this%2520problem%2520are%2520obtained%250Afor%2520both%2520the%2520mean%2520squared%2520error%2520%2528Euclidean%2520metric%2529%2520and%2520the%2520natural%2520Riemannian%250Adistance%2520for%2520Hermitian%2520positive%2520definite%2520matrices%2520%2528affine%2520invariant%2520metric%2529.%250ANumerical%2520simulation%2520illustrate%2520that%2520assessing%2520the%2520error%2520with%2520the%2520affine%250Ainvariant%2520metric%2520is%2520revealing%2520of%2520interesting%2520properties%2520of%2520the%2520maximum%2520a%250Aposteriori%2520and%2520minimum%2520mean%2520square%2520error%2520estimator%252C%2520which%2520are%2520not%2520observed%2520when%250Ausing%2520the%2520Euclidean%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Bayesian%20Cram%C3%A9r-Rao%20Bound%20with%20an%20Application%20to%20Covariance%0A%20%20Matrix%20Estimation&entry.906535625=Florent%20Bouchard%20and%20Alexandre%20Renaux%20and%20Guillaume%20Ginolhac%20and%20Arnaud%20Breloy&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20performance%20bound%20for%20estimation%20problems%20where%20the%0Aparameter%20to%20estimate%20lies%20in%20a%20Riemannian%20manifold%20%28a%20smooth%20manifold%20endowed%0Awith%20a%20Riemannian%20metric%29%20and%20follows%20a%20given%20prior%20distribution.%20In%20this%0Asetup%2C%20the%20chosen%20Riemannian%20metric%20induces%20a%20geometry%20for%20the%20parameter%0Amanifold%2C%20as%20well%20as%20an%20intrinsic%20notion%20of%20the%20estimation%20error%20measure.%0APerformance%20bound%20for%20such%20error%20measure%20were%20previously%20obtained%20in%20the%0Anon-Bayesian%20case%20%28when%20the%20unknown%20parameter%20is%20assumed%20to%20deterministic%29%2C%20and%0Areferred%20to%20as%20%5Ctextit%7Bintrinsic%7D%20Cram%5C%27er-Rao%20bound.%20The%20presented%20result%20then%0Aappears%20either%20as%3A%20%5Ctextit%7Ba%7D%29%20an%20extension%20of%20the%20intrinsic%20Cram%5C%27er-Rao%20bound%0Ato%20the%20Bayesian%20estimation%20framework%3B%20%5Ctextit%7Bb%7D%29%20a%20generalization%20of%20the%0AVan-Trees%20inequality%20%28Bayesian%20Cram%5C%27er-Rao%20bound%29%20that%20accounts%20for%20the%0Aaforementioned%20geometric%20structures.%20In%20a%20second%20part%2C%20we%20leverage%20this%0Aformalism%20to%20study%20the%20problem%20of%20covariance%20matrix%20estimation%20when%20the%20data%0Afollow%20a%20Gaussian%20distribution%2C%20and%20whose%20covariance%20matrix%20is%20drawn%20from%20an%0Ainverse%20Wishart%20distribution.%20Performance%20bounds%20for%20this%20problem%20are%20obtained%0Afor%20both%20the%20mean%20squared%20error%20%28Euclidean%20metric%29%20and%20the%20natural%20Riemannian%0Adistance%20for%20Hermitian%20positive%20definite%20matrices%20%28affine%20invariant%20metric%29.%0ANumerical%20simulation%20illustrate%20that%20assessing%20the%20error%20with%20the%20affine%0Ainvariant%20metric%20is%20revealing%20of%20interesting%20properties%20of%20the%20maximum%20a%0Aposteriori%20and%20minimum%20mean%20square%20error%20estimator%2C%20which%20are%20not%20observed%20when%0Ausing%20the%20Euclidean%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04748v2&entry.124074799=Read"},
{"title": "Incentive-compatible Bandits: Importance Weighting No More", "author": "Julian Zimmert and Teodor V. Marinov", "abstract": "  We study the problem of incentive-compatible online learning with bandit\nfeedback. In this class of problems, the experts are self-interested agents who\nmight misrepresent their preferences with the goal of being selected most\noften. The goal is to devise algorithms which are simultaneously\nincentive-compatible, that is the experts are incentivised to report their true\npreferences, and have no regret with respect to the preferences of the best\nfixed expert in hindsight. \\citet{freeman2020no} propose an algorithm in the\nfull information setting with optimal $O(\\sqrt{T \\log(K)})$ regret and\n$O(T^{2/3}(K\\log(K))^{1/3})$ regret in the bandit setting.\n  In this work we propose the first incentive-compatible algorithms that enjoy\n$O(\\sqrt{KT})$ regret bounds. We further demonstrate how simple loss-biasing\nallows the algorithm proposed in Freeman et al. 2020 to enjoy $\\tilde\nO(\\sqrt{KT})$ regret. As a byproduct of our approach we obtain the first bandit\nalgorithm with nearly optimal regret bounds in the adversarial setting which\nworks entirely on the observed loss sequence without the need for\nimportance-weighted estimators. Finally, we provide an incentive-compatible\nalgorithm that enjoys asymptotically optimal best-of-both-worlds regret\nguarantees, i.e., logarithmic regret in the stochastic regime as well as\nworst-case $O(\\sqrt{KT})$ regret.\n", "link": "http://arxiv.org/abs/2405.06480v1", "date": "2024-05-10", "relevancy": 1.7734, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incentive-compatible%20Bandits%3A%20Importance%20Weighting%20No%20More&body=Title%3A%20Incentive-compatible%20Bandits%3A%20Importance%20Weighting%20No%20More%0AAuthor%3A%20Julian%20Zimmert%20and%20Teodor%20V.%20Marinov%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20incentive-compatible%20online%20learning%20with%20bandit%0Afeedback.%20In%20this%20class%20of%20problems%2C%20the%20experts%20are%20self-interested%20agents%20who%0Amight%20misrepresent%20their%20preferences%20with%20the%20goal%20of%20being%20selected%20most%0Aoften.%20The%20goal%20is%20to%20devise%20algorithms%20which%20are%20simultaneously%0Aincentive-compatible%2C%20that%20is%20the%20experts%20are%20incentivised%20to%20report%20their%20true%0Apreferences%2C%20and%20have%20no%20regret%20with%20respect%20to%20the%20preferences%20of%20the%20best%0Afixed%20expert%20in%20hindsight.%20%5Ccitet%7Bfreeman2020no%7D%20propose%20an%20algorithm%20in%20the%0Afull%20information%20setting%20with%20optimal%20%24O%28%5Csqrt%7BT%20%5Clog%28K%29%7D%29%24%20regret%20and%0A%24O%28T%5E%7B2/3%7D%28K%5Clog%28K%29%29%5E%7B1/3%7D%29%24%20regret%20in%20the%20bandit%20setting.%0A%20%20In%20this%20work%20we%20propose%20the%20first%20incentive-compatible%20algorithms%20that%20enjoy%0A%24O%28%5Csqrt%7BKT%7D%29%24%20regret%20bounds.%20We%20further%20demonstrate%20how%20simple%20loss-biasing%0Aallows%20the%20algorithm%20proposed%20in%20Freeman%20et%20al.%202020%20to%20enjoy%20%24%5Ctilde%0AO%28%5Csqrt%7BKT%7D%29%24%20regret.%20As%20a%20byproduct%20of%20our%20approach%20we%20obtain%20the%20first%20bandit%0Aalgorithm%20with%20nearly%20optimal%20regret%20bounds%20in%20the%20adversarial%20setting%20which%0Aworks%20entirely%20on%20the%20observed%20loss%20sequence%20without%20the%20need%20for%0Aimportance-weighted%20estimators.%20Finally%2C%20we%20provide%20an%20incentive-compatible%0Aalgorithm%20that%20enjoys%20asymptotically%20optimal%20best-of-both-worlds%20regret%0Aguarantees%2C%20i.e.%2C%20logarithmic%20regret%20in%20the%20stochastic%20regime%20as%20well%20as%0Aworst-case%20%24O%28%5Csqrt%7BKT%7D%29%24%20regret.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncentive-compatible%2520Bandits%253A%2520Importance%2520Weighting%2520No%2520More%26entry.906535625%3DJulian%2520Zimmert%2520and%2520Teodor%2520V.%2520Marinov%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520incentive-compatible%2520online%2520learning%2520with%2520bandit%250Afeedback.%2520In%2520this%2520class%2520of%2520problems%252C%2520the%2520experts%2520are%2520self-interested%2520agents%2520who%250Amight%2520misrepresent%2520their%2520preferences%2520with%2520the%2520goal%2520of%2520being%2520selected%2520most%250Aoften.%2520The%2520goal%2520is%2520to%2520devise%2520algorithms%2520which%2520are%2520simultaneously%250Aincentive-compatible%252C%2520that%2520is%2520the%2520experts%2520are%2520incentivised%2520to%2520report%2520their%2520true%250Apreferences%252C%2520and%2520have%2520no%2520regret%2520with%2520respect%2520to%2520the%2520preferences%2520of%2520the%2520best%250Afixed%2520expert%2520in%2520hindsight.%2520%255Ccitet%257Bfreeman2020no%257D%2520propose%2520an%2520algorithm%2520in%2520the%250Afull%2520information%2520setting%2520with%2520optimal%2520%2524O%2528%255Csqrt%257BT%2520%255Clog%2528K%2529%257D%2529%2524%2520regret%2520and%250A%2524O%2528T%255E%257B2/3%257D%2528K%255Clog%2528K%2529%2529%255E%257B1/3%257D%2529%2524%2520regret%2520in%2520the%2520bandit%2520setting.%250A%2520%2520In%2520this%2520work%2520we%2520propose%2520the%2520first%2520incentive-compatible%2520algorithms%2520that%2520enjoy%250A%2524O%2528%255Csqrt%257BKT%257D%2529%2524%2520regret%2520bounds.%2520We%2520further%2520demonstrate%2520how%2520simple%2520loss-biasing%250Aallows%2520the%2520algorithm%2520proposed%2520in%2520Freeman%2520et%2520al.%25202020%2520to%2520enjoy%2520%2524%255Ctilde%250AO%2528%255Csqrt%257BKT%257D%2529%2524%2520regret.%2520As%2520a%2520byproduct%2520of%2520our%2520approach%2520we%2520obtain%2520the%2520first%2520bandit%250Aalgorithm%2520with%2520nearly%2520optimal%2520regret%2520bounds%2520in%2520the%2520adversarial%2520setting%2520which%250Aworks%2520entirely%2520on%2520the%2520observed%2520loss%2520sequence%2520without%2520the%2520need%2520for%250Aimportance-weighted%2520estimators.%2520Finally%252C%2520we%2520provide%2520an%2520incentive-compatible%250Aalgorithm%2520that%2520enjoys%2520asymptotically%2520optimal%2520best-of-both-worlds%2520regret%250Aguarantees%252C%2520i.e.%252C%2520logarithmic%2520regret%2520in%2520the%2520stochastic%2520regime%2520as%2520well%2520as%250Aworst-case%2520%2524O%2528%255Csqrt%257BKT%257D%2529%2524%2520regret.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incentive-compatible%20Bandits%3A%20Importance%20Weighting%20No%20More&entry.906535625=Julian%20Zimmert%20and%20Teodor%20V.%20Marinov&entry.1292438233=%20%20We%20study%20the%20problem%20of%20incentive-compatible%20online%20learning%20with%20bandit%0Afeedback.%20In%20this%20class%20of%20problems%2C%20the%20experts%20are%20self-interested%20agents%20who%0Amight%20misrepresent%20their%20preferences%20with%20the%20goal%20of%20being%20selected%20most%0Aoften.%20The%20goal%20is%20to%20devise%20algorithms%20which%20are%20simultaneously%0Aincentive-compatible%2C%20that%20is%20the%20experts%20are%20incentivised%20to%20report%20their%20true%0Apreferences%2C%20and%20have%20no%20regret%20with%20respect%20to%20the%20preferences%20of%20the%20best%0Afixed%20expert%20in%20hindsight.%20%5Ccitet%7Bfreeman2020no%7D%20propose%20an%20algorithm%20in%20the%0Afull%20information%20setting%20with%20optimal%20%24O%28%5Csqrt%7BT%20%5Clog%28K%29%7D%29%24%20regret%20and%0A%24O%28T%5E%7B2/3%7D%28K%5Clog%28K%29%29%5E%7B1/3%7D%29%24%20regret%20in%20the%20bandit%20setting.%0A%20%20In%20this%20work%20we%20propose%20the%20first%20incentive-compatible%20algorithms%20that%20enjoy%0A%24O%28%5Csqrt%7BKT%7D%29%24%20regret%20bounds.%20We%20further%20demonstrate%20how%20simple%20loss-biasing%0Aallows%20the%20algorithm%20proposed%20in%20Freeman%20et%20al.%202020%20to%20enjoy%20%24%5Ctilde%0AO%28%5Csqrt%7BKT%7D%29%24%20regret.%20As%20a%20byproduct%20of%20our%20approach%20we%20obtain%20the%20first%20bandit%0Aalgorithm%20with%20nearly%20optimal%20regret%20bounds%20in%20the%20adversarial%20setting%20which%0Aworks%20entirely%20on%20the%20observed%20loss%20sequence%20without%20the%20need%20for%0Aimportance-weighted%20estimators.%20Finally%2C%20we%20provide%20an%20incentive-compatible%0Aalgorithm%20that%20enjoys%20asymptotically%20optimal%20best-of-both-worlds%20regret%0Aguarantees%2C%20i.e.%2C%20logarithmic%20regret%20in%20the%20stochastic%20regime%20as%20well%20as%0Aworst-case%20%24O%28%5Csqrt%7BKT%7D%29%24%20regret.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06480v1&entry.124074799=Read"},
{"title": "Correct and Optimal: the Regular Expression Inference Challenge", "author": "Mojtaba Valizadeh and Philip John Gorinski and Ignacio Iacobacci and Martin Berger", "abstract": "  We propose regular expression inference (REI) as a challenge for\ncode/language modelling, and the wider machine learning community. REI is a\nsupervised machine learning (ML) and program optimisation task, and poses the\nproblem of finding minimal regular expressions from examples: Given two finite\nsets of strings $P$ and $N$ and a cost function $cost(\\cdot)$, the task is to\ngenerate an expression $r$ that accepts all strings in $P$ and rejects all\nstrings in $N$, while no other such expression $r'$ exists with\n$cost(r')<cost(r)$. REI has advantages as a challenge problem: (i) regular\nexpressions are well-known, widely used, and a natural idealisation of code;\n(ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a\nsmall number of easy to understand parameters (e.g. $P$ or $N$ cardinality,\nstring lengths of examples, or the cost function); this lets us easily finetune\nREI-hardness; (iv) REI, with its emphasis on optimisation, is an unsolved\nproblem for deep learning based ML. Recently, an REI solver was implemented on\nGPUs, using program synthesis techniques. This enabled, for the first time,\nfast generation of minimal regular expressions for complex REI instances.\nBuilding on this advance, we generate and publish the first large-scale\ndatasets for REI, and devise and evaluate several initial heuristic and machine\nlearning baselines. We invite the community to participate and explore ML\nmethods that learn to solve REI problems. We believe that progress in REI\ndirectly translates to progress in code/language modelling.\n", "link": "http://arxiv.org/abs/2308.07899v2", "date": "2024-05-10", "relevancy": 1.7619, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4448}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4383}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correct%20and%20Optimal%3A%20the%20Regular%20Expression%20Inference%20Challenge&body=Title%3A%20Correct%20and%20Optimal%3A%20the%20Regular%20Expression%20Inference%20Challenge%0AAuthor%3A%20Mojtaba%20Valizadeh%20and%20Philip%20John%20Gorinski%20and%20Ignacio%20Iacobacci%20and%20Martin%20Berger%0AAbstract%3A%20%20%20We%20propose%20regular%20expression%20inference%20%28REI%29%20as%20a%20challenge%20for%0Acode/language%20modelling%2C%20and%20the%20wider%20machine%20learning%20community.%20REI%20is%20a%0Asupervised%20machine%20learning%20%28ML%29%20and%20program%20optimisation%20task%2C%20and%20poses%20the%0Aproblem%20of%20finding%20minimal%20regular%20expressions%20from%20examples%3A%20Given%20two%20finite%0Asets%20of%20strings%20%24P%24%20and%20%24N%24%20and%20a%20cost%20function%20%24cost%28%5Ccdot%29%24%2C%20the%20task%20is%20to%0Agenerate%20an%20expression%20%24r%24%20that%20accepts%20all%20strings%20in%20%24P%24%20and%20rejects%20all%0Astrings%20in%20%24N%24%2C%20while%20no%20other%20such%20expression%20%24r%27%24%20exists%20with%0A%24cost%28r%27%29%3Ccost%28r%29%24.%20REI%20has%20advantages%20as%20a%20challenge%20problem%3A%20%28i%29%20regular%0Aexpressions%20are%20well-known%2C%20widely%20used%2C%20and%20a%20natural%20idealisation%20of%20code%3B%0A%28ii%29%20REI%27s%20asymptotic%20worst-case%20complexity%20is%20well%20understood%3B%20%28iii%29%20REI%20has%20a%0Asmall%20number%20of%20easy%20to%20understand%20parameters%20%28e.g.%20%24P%24%20or%20%24N%24%20cardinality%2C%0Astring%20lengths%20of%20examples%2C%20or%20the%20cost%20function%29%3B%20this%20lets%20us%20easily%20finetune%0AREI-hardness%3B%20%28iv%29%20REI%2C%20with%20its%20emphasis%20on%20optimisation%2C%20is%20an%20unsolved%0Aproblem%20for%20deep%20learning%20based%20ML.%20Recently%2C%20an%20REI%20solver%20was%20implemented%20on%0AGPUs%2C%20using%20program%20synthesis%20techniques.%20This%20enabled%2C%20for%20the%20first%20time%2C%0Afast%20generation%20of%20minimal%20regular%20expressions%20for%20complex%20REI%20instances.%0ABuilding%20on%20this%20advance%2C%20we%20generate%20and%20publish%20the%20first%20large-scale%0Adatasets%20for%20REI%2C%20and%20devise%20and%20evaluate%20several%20initial%20heuristic%20and%20machine%0Alearning%20baselines.%20We%20invite%20the%20community%20to%20participate%20and%20explore%20ML%0Amethods%20that%20learn%20to%20solve%20REI%20problems.%20We%20believe%20that%20progress%20in%20REI%0Adirectly%20translates%20to%20progress%20in%20code/language%20modelling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrect%2520and%2520Optimal%253A%2520the%2520Regular%2520Expression%2520Inference%2520Challenge%26entry.906535625%3DMojtaba%2520Valizadeh%2520and%2520Philip%2520John%2520Gorinski%2520and%2520Ignacio%2520Iacobacci%2520and%2520Martin%2520Berger%26entry.1292438233%3D%2520%2520We%2520propose%2520regular%2520expression%2520inference%2520%2528REI%2529%2520as%2520a%2520challenge%2520for%250Acode/language%2520modelling%252C%2520and%2520the%2520wider%2520machine%2520learning%2520community.%2520REI%2520is%2520a%250Asupervised%2520machine%2520learning%2520%2528ML%2529%2520and%2520program%2520optimisation%2520task%252C%2520and%2520poses%2520the%250Aproblem%2520of%2520finding%2520minimal%2520regular%2520expressions%2520from%2520examples%253A%2520Given%2520two%2520finite%250Asets%2520of%2520strings%2520%2524P%2524%2520and%2520%2524N%2524%2520and%2520a%2520cost%2520function%2520%2524cost%2528%255Ccdot%2529%2524%252C%2520the%2520task%2520is%2520to%250Agenerate%2520an%2520expression%2520%2524r%2524%2520that%2520accepts%2520all%2520strings%2520in%2520%2524P%2524%2520and%2520rejects%2520all%250Astrings%2520in%2520%2524N%2524%252C%2520while%2520no%2520other%2520such%2520expression%2520%2524r%2527%2524%2520exists%2520with%250A%2524cost%2528r%2527%2529%253Ccost%2528r%2529%2524.%2520REI%2520has%2520advantages%2520as%2520a%2520challenge%2520problem%253A%2520%2528i%2529%2520regular%250Aexpressions%2520are%2520well-known%252C%2520widely%2520used%252C%2520and%2520a%2520natural%2520idealisation%2520of%2520code%253B%250A%2528ii%2529%2520REI%2527s%2520asymptotic%2520worst-case%2520complexity%2520is%2520well%2520understood%253B%2520%2528iii%2529%2520REI%2520has%2520a%250Asmall%2520number%2520of%2520easy%2520to%2520understand%2520parameters%2520%2528e.g.%2520%2524P%2524%2520or%2520%2524N%2524%2520cardinality%252C%250Astring%2520lengths%2520of%2520examples%252C%2520or%2520the%2520cost%2520function%2529%253B%2520this%2520lets%2520us%2520easily%2520finetune%250AREI-hardness%253B%2520%2528iv%2529%2520REI%252C%2520with%2520its%2520emphasis%2520on%2520optimisation%252C%2520is%2520an%2520unsolved%250Aproblem%2520for%2520deep%2520learning%2520based%2520ML.%2520Recently%252C%2520an%2520REI%2520solver%2520was%2520implemented%2520on%250AGPUs%252C%2520using%2520program%2520synthesis%2520techniques.%2520This%2520enabled%252C%2520for%2520the%2520first%2520time%252C%250Afast%2520generation%2520of%2520minimal%2520regular%2520expressions%2520for%2520complex%2520REI%2520instances.%250ABuilding%2520on%2520this%2520advance%252C%2520we%2520generate%2520and%2520publish%2520the%2520first%2520large-scale%250Adatasets%2520for%2520REI%252C%2520and%2520devise%2520and%2520evaluate%2520several%2520initial%2520heuristic%2520and%2520machine%250Alearning%2520baselines.%2520We%2520invite%2520the%2520community%2520to%2520participate%2520and%2520explore%2520ML%250Amethods%2520that%2520learn%2520to%2520solve%2520REI%2520problems.%2520We%2520believe%2520that%2520progress%2520in%2520REI%250Adirectly%2520translates%2520to%2520progress%2520in%2520code/language%2520modelling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.07899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correct%20and%20Optimal%3A%20the%20Regular%20Expression%20Inference%20Challenge&entry.906535625=Mojtaba%20Valizadeh%20and%20Philip%20John%20Gorinski%20and%20Ignacio%20Iacobacci%20and%20Martin%20Berger&entry.1292438233=%20%20We%20propose%20regular%20expression%20inference%20%28REI%29%20as%20a%20challenge%20for%0Acode/language%20modelling%2C%20and%20the%20wider%20machine%20learning%20community.%20REI%20is%20a%0Asupervised%20machine%20learning%20%28ML%29%20and%20program%20optimisation%20task%2C%20and%20poses%20the%0Aproblem%20of%20finding%20minimal%20regular%20expressions%20from%20examples%3A%20Given%20two%20finite%0Asets%20of%20strings%20%24P%24%20and%20%24N%24%20and%20a%20cost%20function%20%24cost%28%5Ccdot%29%24%2C%20the%20task%20is%20to%0Agenerate%20an%20expression%20%24r%24%20that%20accepts%20all%20strings%20in%20%24P%24%20and%20rejects%20all%0Astrings%20in%20%24N%24%2C%20while%20no%20other%20such%20expression%20%24r%27%24%20exists%20with%0A%24cost%28r%27%29%3Ccost%28r%29%24.%20REI%20has%20advantages%20as%20a%20challenge%20problem%3A%20%28i%29%20regular%0Aexpressions%20are%20well-known%2C%20widely%20used%2C%20and%20a%20natural%20idealisation%20of%20code%3B%0A%28ii%29%20REI%27s%20asymptotic%20worst-case%20complexity%20is%20well%20understood%3B%20%28iii%29%20REI%20has%20a%0Asmall%20number%20of%20easy%20to%20understand%20parameters%20%28e.g.%20%24P%24%20or%20%24N%24%20cardinality%2C%0Astring%20lengths%20of%20examples%2C%20or%20the%20cost%20function%29%3B%20this%20lets%20us%20easily%20finetune%0AREI-hardness%3B%20%28iv%29%20REI%2C%20with%20its%20emphasis%20on%20optimisation%2C%20is%20an%20unsolved%0Aproblem%20for%20deep%20learning%20based%20ML.%20Recently%2C%20an%20REI%20solver%20was%20implemented%20on%0AGPUs%2C%20using%20program%20synthesis%20techniques.%20This%20enabled%2C%20for%20the%20first%20time%2C%0Afast%20generation%20of%20minimal%20regular%20expressions%20for%20complex%20REI%20instances.%0ABuilding%20on%20this%20advance%2C%20we%20generate%20and%20publish%20the%20first%20large-scale%0Adatasets%20for%20REI%2C%20and%20devise%20and%20evaluate%20several%20initial%20heuristic%20and%20machine%0Alearning%20baselines.%20We%20invite%20the%20community%20to%20participate%20and%20explore%20ML%0Amethods%20that%20learn%20to%20solve%20REI%20problems.%20We%20believe%20that%20progress%20in%20REI%0Adirectly%20translates%20to%20progress%20in%20code/language%20modelling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07899v2&entry.124074799=Read"},
{"title": "A View on Out-of-Distribution Identification from a Statistical Testing\n  Theory Perspective", "author": "Alberto Caron and Chris Hicks and Vasilios Mavroudis", "abstract": "  We study the problem of efficiently detecting Out-of-Distribution (OOD)\nsamples at test time in supervised and unsupervised learning contexts. While ML\nmodels are typically trained under the assumption that training and test data\nstem from the same distribution, this is often not the case in realistic\nsettings, thus reliably detecting distribution shifts is crucial at deployment.\nWe re-formulate the OOD problem under the lenses of statistical testing and\nthen discuss conditions that render the OOD problem identifiable in statistical\nterms. Building on this framework, we study convergence guarantees of an OOD\ntest based on the Wasserstein distance, and provide a simple empirical\nevaluation.\n", "link": "http://arxiv.org/abs/2405.03052v3", "date": "2024-05-10", "relevancy": 1.7207, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4451}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4294}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective&body=Title%3A%20A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective%0AAuthor%3A%20Alberto%20Caron%20and%20Chris%20Hicks%20and%20Vasilios%20Mavroudis%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20efficiently%20detecting%20Out-of-Distribution%20%28OOD%29%0Asamples%20at%20test%20time%20in%20supervised%20and%20unsupervised%20learning%20contexts.%20While%20ML%0Amodels%20are%20typically%20trained%20under%20the%20assumption%20that%20training%20and%20test%20data%0Astem%20from%20the%20same%20distribution%2C%20this%20is%20often%20not%20the%20case%20in%20realistic%0Asettings%2C%20thus%20reliably%20detecting%20distribution%20shifts%20is%20crucial%20at%20deployment.%0AWe%20re-formulate%20the%20OOD%20problem%20under%20the%20lenses%20of%20statistical%20testing%20and%0Athen%20discuss%20conditions%20that%20render%20the%20OOD%20problem%20identifiable%20in%20statistical%0Aterms.%20Building%20on%20this%20framework%2C%20we%20study%20convergence%20guarantees%20of%20an%20OOD%0Atest%20based%20on%20the%20Wasserstein%20distance%2C%20and%20provide%20a%20simple%20empirical%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03052v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520View%2520on%2520Out-of-Distribution%2520Identification%2520from%2520a%2520Statistical%2520Testing%250A%2520%2520Theory%2520Perspective%26entry.906535625%3DAlberto%2520Caron%2520and%2520Chris%2520Hicks%2520and%2520Vasilios%2520Mavroudis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520efficiently%2520detecting%2520Out-of-Distribution%2520%2528OOD%2529%250Asamples%2520at%2520test%2520time%2520in%2520supervised%2520and%2520unsupervised%2520learning%2520contexts.%2520While%2520ML%250Amodels%2520are%2520typically%2520trained%2520under%2520the%2520assumption%2520that%2520training%2520and%2520test%2520data%250Astem%2520from%2520the%2520same%2520distribution%252C%2520this%2520is%2520often%2520not%2520the%2520case%2520in%2520realistic%250Asettings%252C%2520thus%2520reliably%2520detecting%2520distribution%2520shifts%2520is%2520crucial%2520at%2520deployment.%250AWe%2520re-formulate%2520the%2520OOD%2520problem%2520under%2520the%2520lenses%2520of%2520statistical%2520testing%2520and%250Athen%2520discuss%2520conditions%2520that%2520render%2520the%2520OOD%2520problem%2520identifiable%2520in%2520statistical%250Aterms.%2520Building%2520on%2520this%2520framework%252C%2520we%2520study%2520convergence%2520guarantees%2520of%2520an%2520OOD%250Atest%2520based%2520on%2520the%2520Wasserstein%2520distance%252C%2520and%2520provide%2520a%2520simple%2520empirical%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03052v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20View%20on%20Out-of-Distribution%20Identification%20from%20a%20Statistical%20Testing%0A%20%20Theory%20Perspective&entry.906535625=Alberto%20Caron%20and%20Chris%20Hicks%20and%20Vasilios%20Mavroudis&entry.1292438233=%20%20We%20study%20the%20problem%20of%20efficiently%20detecting%20Out-of-Distribution%20%28OOD%29%0Asamples%20at%20test%20time%20in%20supervised%20and%20unsupervised%20learning%20contexts.%20While%20ML%0Amodels%20are%20typically%20trained%20under%20the%20assumption%20that%20training%20and%20test%20data%0Astem%20from%20the%20same%20distribution%2C%20this%20is%20often%20not%20the%20case%20in%20realistic%0Asettings%2C%20thus%20reliably%20detecting%20distribution%20shifts%20is%20crucial%20at%20deployment.%0AWe%20re-formulate%20the%20OOD%20problem%20under%20the%20lenses%20of%20statistical%20testing%20and%0Athen%20discuss%20conditions%20that%20render%20the%20OOD%20problem%20identifiable%20in%20statistical%0Aterms.%20Building%20on%20this%20framework%2C%20we%20study%20convergence%20guarantees%20of%20an%20OOD%0Atest%20based%20on%20the%20Wasserstein%20distance%2C%20and%20provide%20a%20simple%20empirical%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03052v3&entry.124074799=Read"},
{"title": "\"We are at the mercy of others' opinion\": Supporting Blind People in\n  Recreational Window Shopping with AI-infused Technology", "author": "Rie Kamikubo and Hernisa Kacorri and Chieko Asakawa", "abstract": "  Engaging in recreational activities in public spaces poses challenges for\nblind people, often involving dependency on sighted help. Window shopping is a\nkey recreational activity that remains inaccessible. In this paper, we\ninvestigate the information needs, challenges, and current approaches blind\npeople have to recreational window shopping to inform the design of existing\nwayfinding and navigation technology for supporting blind shoppers in\nexploration and serendipitous discovery. We conduct a formative study with a\ntotal of 18 blind participants that include both focus groups (N=8) and\ninterviews for requirements analysis (N=10). We find that there is a desire for\npush notifications of promotional information and pull notifications about\nshops of interest such as the targeted audience of a brand. Information about\nobstacles and points-of-interest required customization depending on one's\nmobility aid as well as presence of a crowd, children, and wheelchair users. We\ntranslate these findings into specific information modalities and rendering in\nthe context of two existing AI-infused assistive applications: NavCog (a\nturn-by-turn navigation app) and Cabot (a navigation robot).\n", "link": "http://arxiv.org/abs/2405.06611v1", "date": "2024-05-10", "relevancy": 1.7197, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6254}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22We%20are%20at%20the%20mercy%20of%20others%27%20opinion%22%3A%20Supporting%20Blind%20People%20in%0A%20%20Recreational%20Window%20Shopping%20with%20AI-infused%20Technology&body=Title%3A%20%22We%20are%20at%20the%20mercy%20of%20others%27%20opinion%22%3A%20Supporting%20Blind%20People%20in%0A%20%20Recreational%20Window%20Shopping%20with%20AI-infused%20Technology%0AAuthor%3A%20Rie%20Kamikubo%20and%20Hernisa%20Kacorri%20and%20Chieko%20Asakawa%0AAbstract%3A%20%20%20Engaging%20in%20recreational%20activities%20in%20public%20spaces%20poses%20challenges%20for%0Ablind%20people%2C%20often%20involving%20dependency%20on%20sighted%20help.%20Window%20shopping%20is%20a%0Akey%20recreational%20activity%20that%20remains%20inaccessible.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20information%20needs%2C%20challenges%2C%20and%20current%20approaches%20blind%0Apeople%20have%20to%20recreational%20window%20shopping%20to%20inform%20the%20design%20of%20existing%0Awayfinding%20and%20navigation%20technology%20for%20supporting%20blind%20shoppers%20in%0Aexploration%20and%20serendipitous%20discovery.%20We%20conduct%20a%20formative%20study%20with%20a%0Atotal%20of%2018%20blind%20participants%20that%20include%20both%20focus%20groups%20%28N%3D8%29%20and%0Ainterviews%20for%20requirements%20analysis%20%28N%3D10%29.%20We%20find%20that%20there%20is%20a%20desire%20for%0Apush%20notifications%20of%20promotional%20information%20and%20pull%20notifications%20about%0Ashops%20of%20interest%20such%20as%20the%20targeted%20audience%20of%20a%20brand.%20Information%20about%0Aobstacles%20and%20points-of-interest%20required%20customization%20depending%20on%20one%27s%0Amobility%20aid%20as%20well%20as%20presence%20of%20a%20crowd%2C%20children%2C%20and%20wheelchair%20users.%20We%0Atranslate%20these%20findings%20into%20specific%20information%20modalities%20and%20rendering%20in%0Athe%20context%20of%20two%20existing%20AI-infused%20assistive%20applications%3A%20NavCog%20%28a%0Aturn-by-turn%20navigation%20app%29%20and%20Cabot%20%28a%20navigation%20robot%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522We%2520are%2520at%2520the%2520mercy%2520of%2520others%2527%2520opinion%2522%253A%2520Supporting%2520Blind%2520People%2520in%250A%2520%2520Recreational%2520Window%2520Shopping%2520with%2520AI-infused%2520Technology%26entry.906535625%3DRie%2520Kamikubo%2520and%2520Hernisa%2520Kacorri%2520and%2520Chieko%2520Asakawa%26entry.1292438233%3D%2520%2520Engaging%2520in%2520recreational%2520activities%2520in%2520public%2520spaces%2520poses%2520challenges%2520for%250Ablind%2520people%252C%2520often%2520involving%2520dependency%2520on%2520sighted%2520help.%2520Window%2520shopping%2520is%2520a%250Akey%2520recreational%2520activity%2520that%2520remains%2520inaccessible.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520the%2520information%2520needs%252C%2520challenges%252C%2520and%2520current%2520approaches%2520blind%250Apeople%2520have%2520to%2520recreational%2520window%2520shopping%2520to%2520inform%2520the%2520design%2520of%2520existing%250Awayfinding%2520and%2520navigation%2520technology%2520for%2520supporting%2520blind%2520shoppers%2520in%250Aexploration%2520and%2520serendipitous%2520discovery.%2520We%2520conduct%2520a%2520formative%2520study%2520with%2520a%250Atotal%2520of%252018%2520blind%2520participants%2520that%2520include%2520both%2520focus%2520groups%2520%2528N%253D8%2529%2520and%250Ainterviews%2520for%2520requirements%2520analysis%2520%2528N%253D10%2529.%2520We%2520find%2520that%2520there%2520is%2520a%2520desire%2520for%250Apush%2520notifications%2520of%2520promotional%2520information%2520and%2520pull%2520notifications%2520about%250Ashops%2520of%2520interest%2520such%2520as%2520the%2520targeted%2520audience%2520of%2520a%2520brand.%2520Information%2520about%250Aobstacles%2520and%2520points-of-interest%2520required%2520customization%2520depending%2520on%2520one%2527s%250Amobility%2520aid%2520as%2520well%2520as%2520presence%2520of%2520a%2520crowd%252C%2520children%252C%2520and%2520wheelchair%2520users.%2520We%250Atranslate%2520these%2520findings%2520into%2520specific%2520information%2520modalities%2520and%2520rendering%2520in%250Athe%2520context%2520of%2520two%2520existing%2520AI-infused%2520assistive%2520applications%253A%2520NavCog%2520%2528a%250Aturn-by-turn%2520navigation%2520app%2529%2520and%2520Cabot%2520%2528a%2520navigation%2520robot%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22We%20are%20at%20the%20mercy%20of%20others%27%20opinion%22%3A%20Supporting%20Blind%20People%20in%0A%20%20Recreational%20Window%20Shopping%20with%20AI-infused%20Technology&entry.906535625=Rie%20Kamikubo%20and%20Hernisa%20Kacorri%20and%20Chieko%20Asakawa&entry.1292438233=%20%20Engaging%20in%20recreational%20activities%20in%20public%20spaces%20poses%20challenges%20for%0Ablind%20people%2C%20often%20involving%20dependency%20on%20sighted%20help.%20Window%20shopping%20is%20a%0Akey%20recreational%20activity%20that%20remains%20inaccessible.%20In%20this%20paper%2C%20we%0Ainvestigate%20the%20information%20needs%2C%20challenges%2C%20and%20current%20approaches%20blind%0Apeople%20have%20to%20recreational%20window%20shopping%20to%20inform%20the%20design%20of%20existing%0Awayfinding%20and%20navigation%20technology%20for%20supporting%20blind%20shoppers%20in%0Aexploration%20and%20serendipitous%20discovery.%20We%20conduct%20a%20formative%20study%20with%20a%0Atotal%20of%2018%20blind%20participants%20that%20include%20both%20focus%20groups%20%28N%3D8%29%20and%0Ainterviews%20for%20requirements%20analysis%20%28N%3D10%29.%20We%20find%20that%20there%20is%20a%20desire%20for%0Apush%20notifications%20of%20promotional%20information%20and%20pull%20notifications%20about%0Ashops%20of%20interest%20such%20as%20the%20targeted%20audience%20of%20a%20brand.%20Information%20about%0Aobstacles%20and%20points-of-interest%20required%20customization%20depending%20on%20one%27s%0Amobility%20aid%20as%20well%20as%20presence%20of%20a%20crowd%2C%20children%2C%20and%20wheelchair%20users.%20We%0Atranslate%20these%20findings%20into%20specific%20information%20modalities%20and%20rendering%20in%0Athe%20context%20of%20two%20existing%20AI-infused%20assistive%20applications%3A%20NavCog%20%28a%0Aturn-by-turn%20navigation%20app%29%20and%20Cabot%20%28a%20navigation%20robot%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06611v1&entry.124074799=Read"},
{"title": "Improving Deep Learning Model Calibration for Cardiac Applications using\n  Deterministic Uncertainty Networks and Uncertainty-aware Training", "author": "Tareen Dawood and Bram Ruijsink and Reza Razavi and Andrew P. King and Esther Puyol-Ant\u00f3n", "abstract": "  Improving calibration performance in deep learning (DL) classification models\nis important when planning the use of DL in a decision-support setting. In such\na scenario, a confident wrong prediction could lead to a lack of trust and/or\nharm in a high-risk application. We evaluate the impact on accuracy and\ncalibration of two types of approach that aim to improve DL classification\nmodel calibration: deterministic uncertainty methods (DUM) and\nuncertainty-aware training. Specifically, we test the performance of three DUMs\nand two uncertainty-aware training approaches as well as their combinations. To\nevaluate their utility, we use two realistic clinical applications from the\nfield of cardiac imaging: artefact detection from phase contrast cardiac\nmagnetic resonance (CMR) and disease diagnosis from the public ACDC CMR\ndataset. Our results indicate that both DUMs and uncertainty-aware training can\nimprove both accuracy and calibration in both of our applications, with DUMs\ngenerally offering the best improvements. We also investigate the combination\nof the two approaches, resulting in a novel deterministic uncertainty-aware\ntraining approach. This provides further improvements for some combinations of\nDUMs and uncertainty-aware training approaches.\n", "link": "http://arxiv.org/abs/2405.06487v1", "date": "2024-05-10", "relevancy": 1.7195, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5808}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Deep%20Learning%20Model%20Calibration%20for%20Cardiac%20Applications%20using%0A%20%20Deterministic%20Uncertainty%20Networks%20and%20Uncertainty-aware%20Training&body=Title%3A%20Improving%20Deep%20Learning%20Model%20Calibration%20for%20Cardiac%20Applications%20using%0A%20%20Deterministic%20Uncertainty%20Networks%20and%20Uncertainty-aware%20Training%0AAuthor%3A%20Tareen%20Dawood%20and%20Bram%20Ruijsink%20and%20Reza%20Razavi%20and%20Andrew%20P.%20King%20and%20Esther%20Puyol-Ant%C3%B3n%0AAbstract%3A%20%20%20Improving%20calibration%20performance%20in%20deep%20learning%20%28DL%29%20classification%20models%0Ais%20important%20when%20planning%20the%20use%20of%20DL%20in%20a%20decision-support%20setting.%20In%20such%0Aa%20scenario%2C%20a%20confident%20wrong%20prediction%20could%20lead%20to%20a%20lack%20of%20trust%20and/or%0Aharm%20in%20a%20high-risk%20application.%20We%20evaluate%20the%20impact%20on%20accuracy%20and%0Acalibration%20of%20two%20types%20of%20approach%20that%20aim%20to%20improve%20DL%20classification%0Amodel%20calibration%3A%20deterministic%20uncertainty%20methods%20%28DUM%29%20and%0Auncertainty-aware%20training.%20Specifically%2C%20we%20test%20the%20performance%20of%20three%20DUMs%0Aand%20two%20uncertainty-aware%20training%20approaches%20as%20well%20as%20their%20combinations.%20To%0Aevaluate%20their%20utility%2C%20we%20use%20two%20realistic%20clinical%20applications%20from%20the%0Afield%20of%20cardiac%20imaging%3A%20artefact%20detection%20from%20phase%20contrast%20cardiac%0Amagnetic%20resonance%20%28CMR%29%20and%20disease%20diagnosis%20from%20the%20public%20ACDC%20CMR%0Adataset.%20Our%20results%20indicate%20that%20both%20DUMs%20and%20uncertainty-aware%20training%20can%0Aimprove%20both%20accuracy%20and%20calibration%20in%20both%20of%20our%20applications%2C%20with%20DUMs%0Agenerally%20offering%20the%20best%20improvements.%20We%20also%20investigate%20the%20combination%0Aof%20the%20two%20approaches%2C%20resulting%20in%20a%20novel%20deterministic%20uncertainty-aware%0Atraining%20approach.%20This%20provides%20further%20improvements%20for%20some%20combinations%20of%0ADUMs%20and%20uncertainty-aware%20training%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Deep%2520Learning%2520Model%2520Calibration%2520for%2520Cardiac%2520Applications%2520using%250A%2520%2520Deterministic%2520Uncertainty%2520Networks%2520and%2520Uncertainty-aware%2520Training%26entry.906535625%3DTareen%2520Dawood%2520and%2520Bram%2520Ruijsink%2520and%2520Reza%2520Razavi%2520and%2520Andrew%2520P.%2520King%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%26entry.1292438233%3D%2520%2520Improving%2520calibration%2520performance%2520in%2520deep%2520learning%2520%2528DL%2529%2520classification%2520models%250Ais%2520important%2520when%2520planning%2520the%2520use%2520of%2520DL%2520in%2520a%2520decision-support%2520setting.%2520In%2520such%250Aa%2520scenario%252C%2520a%2520confident%2520wrong%2520prediction%2520could%2520lead%2520to%2520a%2520lack%2520of%2520trust%2520and/or%250Aharm%2520in%2520a%2520high-risk%2520application.%2520We%2520evaluate%2520the%2520impact%2520on%2520accuracy%2520and%250Acalibration%2520of%2520two%2520types%2520of%2520approach%2520that%2520aim%2520to%2520improve%2520DL%2520classification%250Amodel%2520calibration%253A%2520deterministic%2520uncertainty%2520methods%2520%2528DUM%2529%2520and%250Auncertainty-aware%2520training.%2520Specifically%252C%2520we%2520test%2520the%2520performance%2520of%2520three%2520DUMs%250Aand%2520two%2520uncertainty-aware%2520training%2520approaches%2520as%2520well%2520as%2520their%2520combinations.%2520To%250Aevaluate%2520their%2520utility%252C%2520we%2520use%2520two%2520realistic%2520clinical%2520applications%2520from%2520the%250Afield%2520of%2520cardiac%2520imaging%253A%2520artefact%2520detection%2520from%2520phase%2520contrast%2520cardiac%250Amagnetic%2520resonance%2520%2528CMR%2529%2520and%2520disease%2520diagnosis%2520from%2520the%2520public%2520ACDC%2520CMR%250Adataset.%2520Our%2520results%2520indicate%2520that%2520both%2520DUMs%2520and%2520uncertainty-aware%2520training%2520can%250Aimprove%2520both%2520accuracy%2520and%2520calibration%2520in%2520both%2520of%2520our%2520applications%252C%2520with%2520DUMs%250Agenerally%2520offering%2520the%2520best%2520improvements.%2520We%2520also%2520investigate%2520the%2520combination%250Aof%2520the%2520two%2520approaches%252C%2520resulting%2520in%2520a%2520novel%2520deterministic%2520uncertainty-aware%250Atraining%2520approach.%2520This%2520provides%2520further%2520improvements%2520for%2520some%2520combinations%2520of%250ADUMs%2520and%2520uncertainty-aware%2520training%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Deep%20Learning%20Model%20Calibration%20for%20Cardiac%20Applications%20using%0A%20%20Deterministic%20Uncertainty%20Networks%20and%20Uncertainty-aware%20Training&entry.906535625=Tareen%20Dawood%20and%20Bram%20Ruijsink%20and%20Reza%20Razavi%20and%20Andrew%20P.%20King%20and%20Esther%20Puyol-Ant%C3%B3n&entry.1292438233=%20%20Improving%20calibration%20performance%20in%20deep%20learning%20%28DL%29%20classification%20models%0Ais%20important%20when%20planning%20the%20use%20of%20DL%20in%20a%20decision-support%20setting.%20In%20such%0Aa%20scenario%2C%20a%20confident%20wrong%20prediction%20could%20lead%20to%20a%20lack%20of%20trust%20and/or%0Aharm%20in%20a%20high-risk%20application.%20We%20evaluate%20the%20impact%20on%20accuracy%20and%0Acalibration%20of%20two%20types%20of%20approach%20that%20aim%20to%20improve%20DL%20classification%0Amodel%20calibration%3A%20deterministic%20uncertainty%20methods%20%28DUM%29%20and%0Auncertainty-aware%20training.%20Specifically%2C%20we%20test%20the%20performance%20of%20three%20DUMs%0Aand%20two%20uncertainty-aware%20training%20approaches%20as%20well%20as%20their%20combinations.%20To%0Aevaluate%20their%20utility%2C%20we%20use%20two%20realistic%20clinical%20applications%20from%20the%0Afield%20of%20cardiac%20imaging%3A%20artefact%20detection%20from%20phase%20contrast%20cardiac%0Amagnetic%20resonance%20%28CMR%29%20and%20disease%20diagnosis%20from%20the%20public%20ACDC%20CMR%0Adataset.%20Our%20results%20indicate%20that%20both%20DUMs%20and%20uncertainty-aware%20training%20can%0Aimprove%20both%20accuracy%20and%20calibration%20in%20both%20of%20our%20applications%2C%20with%20DUMs%0Agenerally%20offering%20the%20best%20improvements.%20We%20also%20investigate%20the%20combination%0Aof%20the%20two%20approaches%2C%20resulting%20in%20a%20novel%20deterministic%20uncertainty-aware%0Atraining%20approach.%20This%20provides%20further%20improvements%20for%20some%20combinations%20of%0ADUMs%20and%20uncertainty-aware%20training%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06487v1&entry.124074799=Read"},
{"title": "Intelligent Duty Cycling Management and Wake-up for Energy Harvesting\n  IoT Networks with Correlated Activity", "author": "David E. Ru\u00edz-Guirola and Onel L. A. L\u00f3pez and Samuel Montejo-S\u00e1nchez and Israel Leyva Mayorga and Zhu Han and Petar Popovski", "abstract": "  This paper presents an approach for energy-neutral Internet of Things (IoT)\nscenarios where the IoT devices (IoTDs) rely entirely on their energy\nharvesting capabilities to sustain operation. We use a Markov chain to\nrepresent the operation and transmission states of the IoTDs, a modulated\nPoisson process to model their energy harvesting process, and a discrete-time\nMarkov chain to model their battery state. The aim is to efficiently manage the\nduty cycling of the IoTDs, so as to prolong their battery life and reduce\ninstances of low-energy availability. We propose a duty-cycling management\nbased on K- nearest neighbors, aiming to strike a trade-off between energy\nefficiency and detection accuracy. This is done by incorporating spatial and\ntemporal correlations among IoTDs' activity, as well as their energy harvesting\ncapabilities. We also allow the base station to wake up specific IoTDs if more\ninformation about an event is needed upon initial detection. Our proposed\nscheme shows significant improvements in energy savings and performance, with\nup to 11 times lower misdetection probability and 50\\% lower energy consumption\nfor high-density scenarios compared to a random duty cycling benchmark.\n", "link": "http://arxiv.org/abs/2405.06372v1", "date": "2024-05-10", "relevancy": 1.7067, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4288}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Duty%20Cycling%20Management%20and%20Wake-up%20for%20Energy%20Harvesting%0A%20%20IoT%20Networks%20with%20Correlated%20Activity&body=Title%3A%20Intelligent%20Duty%20Cycling%20Management%20and%20Wake-up%20for%20Energy%20Harvesting%0A%20%20IoT%20Networks%20with%20Correlated%20Activity%0AAuthor%3A%20David%20E.%20Ru%C3%ADz-Guirola%20and%20Onel%20L.%20A.%20L%C3%B3pez%20and%20Samuel%20Montejo-S%C3%A1nchez%20and%20Israel%20Leyva%20Mayorga%20and%20Zhu%20Han%20and%20Petar%20Popovski%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20approach%20for%20energy-neutral%20Internet%20of%20Things%20%28IoT%29%0Ascenarios%20where%20the%20IoT%20devices%20%28IoTDs%29%20rely%20entirely%20on%20their%20energy%0Aharvesting%20capabilities%20to%20sustain%20operation.%20We%20use%20a%20Markov%20chain%20to%0Arepresent%20the%20operation%20and%20transmission%20states%20of%20the%20IoTDs%2C%20a%20modulated%0APoisson%20process%20to%20model%20their%20energy%20harvesting%20process%2C%20and%20a%20discrete-time%0AMarkov%20chain%20to%20model%20their%20battery%20state.%20The%20aim%20is%20to%20efficiently%20manage%20the%0Aduty%20cycling%20of%20the%20IoTDs%2C%20so%20as%20to%20prolong%20their%20battery%20life%20and%20reduce%0Ainstances%20of%20low-energy%20availability.%20We%20propose%20a%20duty-cycling%20management%0Abased%20on%20K-%20nearest%20neighbors%2C%20aiming%20to%20strike%20a%20trade-off%20between%20energy%0Aefficiency%20and%20detection%20accuracy.%20This%20is%20done%20by%20incorporating%20spatial%20and%0Atemporal%20correlations%20among%20IoTDs%27%20activity%2C%20as%20well%20as%20their%20energy%20harvesting%0Acapabilities.%20We%20also%20allow%20the%20base%20station%20to%20wake%20up%20specific%20IoTDs%20if%20more%0Ainformation%20about%20an%20event%20is%20needed%20upon%20initial%20detection.%20Our%20proposed%0Ascheme%20shows%20significant%20improvements%20in%20energy%20savings%20and%20performance%2C%20with%0Aup%20to%2011%20times%20lower%20misdetection%20probability%20and%2050%5C%25%20lower%20energy%20consumption%0Afor%20high-density%20scenarios%20compared%20to%20a%20random%20duty%20cycling%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Duty%2520Cycling%2520Management%2520and%2520Wake-up%2520for%2520Energy%2520Harvesting%250A%2520%2520IoT%2520Networks%2520with%2520Correlated%2520Activity%26entry.906535625%3DDavid%2520E.%2520Ru%25C3%25ADz-Guirola%2520and%2520Onel%2520L.%2520A.%2520L%25C3%25B3pez%2520and%2520Samuel%2520Montejo-S%25C3%25A1nchez%2520and%2520Israel%2520Leyva%2520Mayorga%2520and%2520Zhu%2520Han%2520and%2520Petar%2520Popovski%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520approach%2520for%2520energy-neutral%2520Internet%2520of%2520Things%2520%2528IoT%2529%250Ascenarios%2520where%2520the%2520IoT%2520devices%2520%2528IoTDs%2529%2520rely%2520entirely%2520on%2520their%2520energy%250Aharvesting%2520capabilities%2520to%2520sustain%2520operation.%2520We%2520use%2520a%2520Markov%2520chain%2520to%250Arepresent%2520the%2520operation%2520and%2520transmission%2520states%2520of%2520the%2520IoTDs%252C%2520a%2520modulated%250APoisson%2520process%2520to%2520model%2520their%2520energy%2520harvesting%2520process%252C%2520and%2520a%2520discrete-time%250AMarkov%2520chain%2520to%2520model%2520their%2520battery%2520state.%2520The%2520aim%2520is%2520to%2520efficiently%2520manage%2520the%250Aduty%2520cycling%2520of%2520the%2520IoTDs%252C%2520so%2520as%2520to%2520prolong%2520their%2520battery%2520life%2520and%2520reduce%250Ainstances%2520of%2520low-energy%2520availability.%2520We%2520propose%2520a%2520duty-cycling%2520management%250Abased%2520on%2520K-%2520nearest%2520neighbors%252C%2520aiming%2520to%2520strike%2520a%2520trade-off%2520between%2520energy%250Aefficiency%2520and%2520detection%2520accuracy.%2520This%2520is%2520done%2520by%2520incorporating%2520spatial%2520and%250Atemporal%2520correlations%2520among%2520IoTDs%2527%2520activity%252C%2520as%2520well%2520as%2520their%2520energy%2520harvesting%250Acapabilities.%2520We%2520also%2520allow%2520the%2520base%2520station%2520to%2520wake%2520up%2520specific%2520IoTDs%2520if%2520more%250Ainformation%2520about%2520an%2520event%2520is%2520needed%2520upon%2520initial%2520detection.%2520Our%2520proposed%250Ascheme%2520shows%2520significant%2520improvements%2520in%2520energy%2520savings%2520and%2520performance%252C%2520with%250Aup%2520to%252011%2520times%2520lower%2520misdetection%2520probability%2520and%252050%255C%2525%2520lower%2520energy%2520consumption%250Afor%2520high-density%2520scenarios%2520compared%2520to%2520a%2520random%2520duty%2520cycling%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Duty%20Cycling%20Management%20and%20Wake-up%20for%20Energy%20Harvesting%0A%20%20IoT%20Networks%20with%20Correlated%20Activity&entry.906535625=David%20E.%20Ru%C3%ADz-Guirola%20and%20Onel%20L.%20A.%20L%C3%B3pez%20and%20Samuel%20Montejo-S%C3%A1nchez%20and%20Israel%20Leyva%20Mayorga%20and%20Zhu%20Han%20and%20Petar%20Popovski&entry.1292438233=%20%20This%20paper%20presents%20an%20approach%20for%20energy-neutral%20Internet%20of%20Things%20%28IoT%29%0Ascenarios%20where%20the%20IoT%20devices%20%28IoTDs%29%20rely%20entirely%20on%20their%20energy%0Aharvesting%20capabilities%20to%20sustain%20operation.%20We%20use%20a%20Markov%20chain%20to%0Arepresent%20the%20operation%20and%20transmission%20states%20of%20the%20IoTDs%2C%20a%20modulated%0APoisson%20process%20to%20model%20their%20energy%20harvesting%20process%2C%20and%20a%20discrete-time%0AMarkov%20chain%20to%20model%20their%20battery%20state.%20The%20aim%20is%20to%20efficiently%20manage%20the%0Aduty%20cycling%20of%20the%20IoTDs%2C%20so%20as%20to%20prolong%20their%20battery%20life%20and%20reduce%0Ainstances%20of%20low-energy%20availability.%20We%20propose%20a%20duty-cycling%20management%0Abased%20on%20K-%20nearest%20neighbors%2C%20aiming%20to%20strike%20a%20trade-off%20between%20energy%0Aefficiency%20and%20detection%20accuracy.%20This%20is%20done%20by%20incorporating%20spatial%20and%0Atemporal%20correlations%20among%20IoTDs%27%20activity%2C%20as%20well%20as%20their%20energy%20harvesting%0Acapabilities.%20We%20also%20allow%20the%20base%20station%20to%20wake%20up%20specific%20IoTDs%20if%20more%0Ainformation%20about%20an%20event%20is%20needed%20upon%20initial%20detection.%20Our%20proposed%0Ascheme%20shows%20significant%20improvements%20in%20energy%20savings%20and%20performance%2C%20with%0Aup%20to%2011%20times%20lower%20misdetection%20probability%20and%2050%5C%25%20lower%20energy%20consumption%0Afor%20high-density%20scenarios%20compared%20to%20a%20random%20duty%20cycling%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06372v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification Metrics for Deep Regression", "author": "Ziliang Xiong and Simon Kristoffersson Lind and Per-Erik Forss\u00e9n and Volker Kr\u00fcger", "abstract": "  When deploying deep neural networks on robots or other physical systems, the\nlearned model should reliably quantify predictive uncertainty. A reliable\nuncertainty allows downstream modules to reason about the safety of its\nactions. In this work, we address metrics for evaluating such an uncertainty.\nSpecifically, we focus on regression tasks, and investigate Area Under\nSparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation,\nand Negative Log-Likelihood (NLL). Using synthetic regression datasets, we look\ninto how those metrics behave under four typical types of uncertainty, their\nstability regarding the size of the test set, and reveal their strengths and\nweaknesses. Our results indicate that Calibration Error is the most stable and\ninterpretable metric, but AUSE and NLL also have their respective use cases. We\ndiscourage the usage of Spearman's Rank Correlation for evaluating\nuncertainties and recommend replacing it with AUSE.\n", "link": "http://arxiv.org/abs/2405.04278v2", "date": "2024-05-10", "relevancy": 1.6963, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5727}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20Metrics%20for%20Deep%20Regression&body=Title%3A%20Uncertainty%20Quantification%20Metrics%20for%20Deep%20Regression%0AAuthor%3A%20Ziliang%20Xiong%20and%20Simon%20Kristoffersson%20Lind%20and%20Per-Erik%20Forss%C3%A9n%20and%20Volker%20Kr%C3%BCger%0AAbstract%3A%20%20%20When%20deploying%20deep%20neural%20networks%20on%20robots%20or%20other%20physical%20systems%2C%20the%0Alearned%20model%20should%20reliably%20quantify%20predictive%20uncertainty.%20A%20reliable%0Auncertainty%20allows%20downstream%20modules%20to%20reason%20about%20the%20safety%20of%20its%0Aactions.%20In%20this%20work%2C%20we%20address%20metrics%20for%20evaluating%20such%20an%20uncertainty.%0ASpecifically%2C%20we%20focus%20on%20regression%20tasks%2C%20and%20investigate%20Area%20Under%0ASparsification%20Error%20%28AUSE%29%2C%20Calibration%20Error%2C%20Spearman%27s%20Rank%20Correlation%2C%0Aand%20Negative%20Log-Likelihood%20%28NLL%29.%20Using%20synthetic%20regression%20datasets%2C%20we%20look%0Ainto%20how%20those%20metrics%20behave%20under%20four%20typical%20types%20of%20uncertainty%2C%20their%0Astability%20regarding%20the%20size%20of%20the%20test%20set%2C%20and%20reveal%20their%20strengths%20and%0Aweaknesses.%20Our%20results%20indicate%20that%20Calibration%20Error%20is%20the%20most%20stable%20and%0Ainterpretable%20metric%2C%20but%20AUSE%20and%20NLL%20also%20have%20their%20respective%20use%20cases.%20We%0Adiscourage%20the%20usage%20of%20Spearman%27s%20Rank%20Correlation%20for%20evaluating%0Auncertainties%20and%20recommend%20replacing%20it%20with%20AUSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520Metrics%2520for%2520Deep%2520Regression%26entry.906535625%3DZiliang%2520Xiong%2520and%2520Simon%2520Kristoffersson%2520Lind%2520and%2520Per-Erik%2520Forss%25C3%25A9n%2520and%2520Volker%2520Kr%25C3%25BCger%26entry.1292438233%3D%2520%2520When%2520deploying%2520deep%2520neural%2520networks%2520on%2520robots%2520or%2520other%2520physical%2520systems%252C%2520the%250Alearned%2520model%2520should%2520reliably%2520quantify%2520predictive%2520uncertainty.%2520A%2520reliable%250Auncertainty%2520allows%2520downstream%2520modules%2520to%2520reason%2520about%2520the%2520safety%2520of%2520its%250Aactions.%2520In%2520this%2520work%252C%2520we%2520address%2520metrics%2520for%2520evaluating%2520such%2520an%2520uncertainty.%250ASpecifically%252C%2520we%2520focus%2520on%2520regression%2520tasks%252C%2520and%2520investigate%2520Area%2520Under%250ASparsification%2520Error%2520%2528AUSE%2529%252C%2520Calibration%2520Error%252C%2520Spearman%2527s%2520Rank%2520Correlation%252C%250Aand%2520Negative%2520Log-Likelihood%2520%2528NLL%2529.%2520Using%2520synthetic%2520regression%2520datasets%252C%2520we%2520look%250Ainto%2520how%2520those%2520metrics%2520behave%2520under%2520four%2520typical%2520types%2520of%2520uncertainty%252C%2520their%250Astability%2520regarding%2520the%2520size%2520of%2520the%2520test%2520set%252C%2520and%2520reveal%2520their%2520strengths%2520and%250Aweaknesses.%2520Our%2520results%2520indicate%2520that%2520Calibration%2520Error%2520is%2520the%2520most%2520stable%2520and%250Ainterpretable%2520metric%252C%2520but%2520AUSE%2520and%2520NLL%2520also%2520have%2520their%2520respective%2520use%2520cases.%2520We%250Adiscourage%2520the%2520usage%2520of%2520Spearman%2527s%2520Rank%2520Correlation%2520for%2520evaluating%250Auncertainties%2520and%2520recommend%2520replacing%2520it%2520with%2520AUSE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20Metrics%20for%20Deep%20Regression&entry.906535625=Ziliang%20Xiong%20and%20Simon%20Kristoffersson%20Lind%20and%20Per-Erik%20Forss%C3%A9n%20and%20Volker%20Kr%C3%BCger&entry.1292438233=%20%20When%20deploying%20deep%20neural%20networks%20on%20robots%20or%20other%20physical%20systems%2C%20the%0Alearned%20model%20should%20reliably%20quantify%20predictive%20uncertainty.%20A%20reliable%0Auncertainty%20allows%20downstream%20modules%20to%20reason%20about%20the%20safety%20of%20its%0Aactions.%20In%20this%20work%2C%20we%20address%20metrics%20for%20evaluating%20such%20an%20uncertainty.%0ASpecifically%2C%20we%20focus%20on%20regression%20tasks%2C%20and%20investigate%20Area%20Under%0ASparsification%20Error%20%28AUSE%29%2C%20Calibration%20Error%2C%20Spearman%27s%20Rank%20Correlation%2C%0Aand%20Negative%20Log-Likelihood%20%28NLL%29.%20Using%20synthetic%20regression%20datasets%2C%20we%20look%0Ainto%20how%20those%20metrics%20behave%20under%20four%20typical%20types%20of%20uncertainty%2C%20their%0Astability%20regarding%20the%20size%20of%20the%20test%20set%2C%20and%20reveal%20their%20strengths%20and%0Aweaknesses.%20Our%20results%20indicate%20that%20Calibration%20Error%20is%20the%20most%20stable%20and%0Ainterpretable%20metric%2C%20but%20AUSE%20and%20NLL%20also%20have%20their%20respective%20use%20cases.%20We%0Adiscourage%20the%20usage%20of%20Spearman%27s%20Rank%20Correlation%20for%20evaluating%0Auncertainties%20and%20recommend%20replacing%20it%20with%20AUSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04278v2&entry.124074799=Read"},
{"title": "Improving Instruction Following in Language Models through Proxy-Based\n  Uncertainty Estimation", "author": "JoonHo Lee and Jae Oh Woo and Juree Seok and Parisa Hassanzadeh and Wooseok Jang and JuYoun Son and Sima Didari and Baruch Gutow and Heng Hao and Hankyu Moon and Wenjun Hu and Yeong-Dae Kwon and Taehee Lee and Seungjai Min", "abstract": "  Assessing response quality to instructions in language models is vital but\nchallenging due to the complexity of human language across different contexts.\nThis complexity often results in ambiguous or inconsistent interpretations,\nmaking accurate assessment difficult. To address this issue, we propose a novel\nUncertainty-aware Reward Model (URM) that introduces a robust uncertainty\nestimation for the quality of paired responses based on Bayesian approximation.\nTrained with preference datasets, our uncertainty-enabled proxy not only scores\nrewards for responses but also evaluates their inherent uncertainty. Empirical\nresults demonstrate significant benefits of incorporating the proposed proxy\ninto language model training. Our method boosts the instruction following\ncapability of language models by refining data curation for training and\nimproving policy optimization objectives, thereby surpassing existing methods\nby a large margin on benchmarks such as Vicuna and MT-bench. These findings\nhighlight that our proposed approach substantially advances language model\ntraining and paves a new way of harnessing uncertainty within language models.\n", "link": "http://arxiv.org/abs/2405.06424v1", "date": "2024-05-10", "relevancy": 1.6756, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5984}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Instruction%20Following%20in%20Language%20Models%20through%20Proxy-Based%0A%20%20Uncertainty%20Estimation&body=Title%3A%20Improving%20Instruction%20Following%20in%20Language%20Models%20through%20Proxy-Based%0A%20%20Uncertainty%20Estimation%0AAuthor%3A%20JoonHo%20Lee%20and%20Jae%20Oh%20Woo%20and%20Juree%20Seok%20and%20Parisa%20Hassanzadeh%20and%20Wooseok%20Jang%20and%20JuYoun%20Son%20and%20Sima%20Didari%20and%20Baruch%20Gutow%20and%20Heng%20Hao%20and%20Hankyu%20Moon%20and%20Wenjun%20Hu%20and%20Yeong-Dae%20Kwon%20and%20Taehee%20Lee%20and%20Seungjai%20Min%0AAbstract%3A%20%20%20Assessing%20response%20quality%20to%20instructions%20in%20language%20models%20is%20vital%20but%0Achallenging%20due%20to%20the%20complexity%20of%20human%20language%20across%20different%20contexts.%0AThis%20complexity%20often%20results%20in%20ambiguous%20or%20inconsistent%20interpretations%2C%0Amaking%20accurate%20assessment%20difficult.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0AUncertainty-aware%20Reward%20Model%20%28URM%29%20that%20introduces%20a%20robust%20uncertainty%0Aestimation%20for%20the%20quality%20of%20paired%20responses%20based%20on%20Bayesian%20approximation.%0ATrained%20with%20preference%20datasets%2C%20our%20uncertainty-enabled%20proxy%20not%20only%20scores%0Arewards%20for%20responses%20but%20also%20evaluates%20their%20inherent%20uncertainty.%20Empirical%0Aresults%20demonstrate%20significant%20benefits%20of%20incorporating%20the%20proposed%20proxy%0Ainto%20language%20model%20training.%20Our%20method%20boosts%20the%20instruction%20following%0Acapability%20of%20language%20models%20by%20refining%20data%20curation%20for%20training%20and%0Aimproving%20policy%20optimization%20objectives%2C%20thereby%20surpassing%20existing%20methods%0Aby%20a%20large%20margin%20on%20benchmarks%20such%20as%20Vicuna%20and%20MT-bench.%20These%20findings%0Ahighlight%20that%20our%20proposed%20approach%20substantially%20advances%20language%20model%0Atraining%20and%20paves%20a%20new%20way%20of%20harnessing%20uncertainty%20within%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Instruction%2520Following%2520in%2520Language%2520Models%2520through%2520Proxy-Based%250A%2520%2520Uncertainty%2520Estimation%26entry.906535625%3DJoonHo%2520Lee%2520and%2520Jae%2520Oh%2520Woo%2520and%2520Juree%2520Seok%2520and%2520Parisa%2520Hassanzadeh%2520and%2520Wooseok%2520Jang%2520and%2520JuYoun%2520Son%2520and%2520Sima%2520Didari%2520and%2520Baruch%2520Gutow%2520and%2520Heng%2520Hao%2520and%2520Hankyu%2520Moon%2520and%2520Wenjun%2520Hu%2520and%2520Yeong-Dae%2520Kwon%2520and%2520Taehee%2520Lee%2520and%2520Seungjai%2520Min%26entry.1292438233%3D%2520%2520Assessing%2520response%2520quality%2520to%2520instructions%2520in%2520language%2520models%2520is%2520vital%2520but%250Achallenging%2520due%2520to%2520the%2520complexity%2520of%2520human%2520language%2520across%2520different%2520contexts.%250AThis%2520complexity%2520often%2520results%2520in%2520ambiguous%2520or%2520inconsistent%2520interpretations%252C%250Amaking%2520accurate%2520assessment%2520difficult.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250AUncertainty-aware%2520Reward%2520Model%2520%2528URM%2529%2520that%2520introduces%2520a%2520robust%2520uncertainty%250Aestimation%2520for%2520the%2520quality%2520of%2520paired%2520responses%2520based%2520on%2520Bayesian%2520approximation.%250ATrained%2520with%2520preference%2520datasets%252C%2520our%2520uncertainty-enabled%2520proxy%2520not%2520only%2520scores%250Arewards%2520for%2520responses%2520but%2520also%2520evaluates%2520their%2520inherent%2520uncertainty.%2520Empirical%250Aresults%2520demonstrate%2520significant%2520benefits%2520of%2520incorporating%2520the%2520proposed%2520proxy%250Ainto%2520language%2520model%2520training.%2520Our%2520method%2520boosts%2520the%2520instruction%2520following%250Acapability%2520of%2520language%2520models%2520by%2520refining%2520data%2520curation%2520for%2520training%2520and%250Aimproving%2520policy%2520optimization%2520objectives%252C%2520thereby%2520surpassing%2520existing%2520methods%250Aby%2520a%2520large%2520margin%2520on%2520benchmarks%2520such%2520as%2520Vicuna%2520and%2520MT-bench.%2520These%2520findings%250Ahighlight%2520that%2520our%2520proposed%2520approach%2520substantially%2520advances%2520language%2520model%250Atraining%2520and%2520paves%2520a%2520new%2520way%2520of%2520harnessing%2520uncertainty%2520within%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Instruction%20Following%20in%20Language%20Models%20through%20Proxy-Based%0A%20%20Uncertainty%20Estimation&entry.906535625=JoonHo%20Lee%20and%20Jae%20Oh%20Woo%20and%20Juree%20Seok%20and%20Parisa%20Hassanzadeh%20and%20Wooseok%20Jang%20and%20JuYoun%20Son%20and%20Sima%20Didari%20and%20Baruch%20Gutow%20and%20Heng%20Hao%20and%20Hankyu%20Moon%20and%20Wenjun%20Hu%20and%20Yeong-Dae%20Kwon%20and%20Taehee%20Lee%20and%20Seungjai%20Min&entry.1292438233=%20%20Assessing%20response%20quality%20to%20instructions%20in%20language%20models%20is%20vital%20but%0Achallenging%20due%20to%20the%20complexity%20of%20human%20language%20across%20different%20contexts.%0AThis%20complexity%20often%20results%20in%20ambiguous%20or%20inconsistent%20interpretations%2C%0Amaking%20accurate%20assessment%20difficult.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0AUncertainty-aware%20Reward%20Model%20%28URM%29%20that%20introduces%20a%20robust%20uncertainty%0Aestimation%20for%20the%20quality%20of%20paired%20responses%20based%20on%20Bayesian%20approximation.%0ATrained%20with%20preference%20datasets%2C%20our%20uncertainty-enabled%20proxy%20not%20only%20scores%0Arewards%20for%20responses%20but%20also%20evaluates%20their%20inherent%20uncertainty.%20Empirical%0Aresults%20demonstrate%20significant%20benefits%20of%20incorporating%20the%20proposed%20proxy%0Ainto%20language%20model%20training.%20Our%20method%20boosts%20the%20instruction%20following%0Acapability%20of%20language%20models%20by%20refining%20data%20curation%20for%20training%20and%0Aimproving%20policy%20optimization%20objectives%2C%20thereby%20surpassing%20existing%20methods%0Aby%20a%20large%20margin%20on%20benchmarks%20such%20as%20Vicuna%20and%20MT-bench.%20These%20findings%0Ahighlight%20that%20our%20proposed%20approach%20substantially%20advances%20language%20model%0Atraining%20and%20paves%20a%20new%20way%20of%20harnessing%20uncertainty%20within%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06424v1&entry.124074799=Read"},
{"title": "CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models", "author": "Sehmimul Hoque and Hao Jia and Abhishek Abhishek and Mojde Fadaie and J. Quetzalcoatl Toledo-Mar\u00edn and Tiago Vale and Roger G. Melko and Maximilian Swiatlowski and Wojciech T. Fedorko", "abstract": "  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n", "link": "http://arxiv.org/abs/2312.03179v3", "date": "2024-05-10", "relevancy": 1.6629, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4152}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models&body=Title%3A%20CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models%0AAuthor%3A%20Sehmimul%20Hoque%20and%20Hao%20Jia%20and%20Abhishek%20Abhishek%20and%20Mojde%20Fadaie%20and%20J.%20Quetzalcoatl%20Toledo-Mar%C3%ADn%20and%20Tiago%20Vale%20and%20Roger%20G.%20Melko%20and%20Maximilian%20Swiatlowski%20and%20Wojciech%20T.%20Fedorko%0AAbstract%3A%20%20%20The%20Large%20Hadron%20Collider%27s%20high%20luminosity%20era%20presents%20major%20computational%0Achallenges%20in%20the%20analysis%20of%20collision%20events.%20Large%20amounts%20of%20Monte%20Carlo%0A%28MC%29%20simulation%20will%20be%20required%20to%20constrain%20the%20statistical%20uncertainties%20of%0Athe%20simulated%20datasets%20below%20these%20of%20the%20experimental%20data.%20Modelling%20of%0Ahigh-energy%20particles%20propagating%20through%20the%20calorimeter%20section%20of%20the%0Adetector%20is%20the%20most%20computationally%20intensive%20MC%20simulation%20task.%20We%20introduce%0Aa%20technique%20combining%20recent%20advancements%20in%20generative%20models%20and%20quantum%0Aannealing%20for%20fast%20and%20efficient%20simulation%20of%20high-energy%20particle-calorimeter%0Ainteractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03179v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaloQVAE%2520%253A%2520Simulating%2520high-energy%2520particle-calorimeter%2520interactions%250A%2520%2520using%2520hybrid%2520quantum-classical%2520generative%2520models%26entry.906535625%3DSehmimul%2520Hoque%2520and%2520Hao%2520Jia%2520and%2520Abhishek%2520Abhishek%2520and%2520Mojde%2520Fadaie%2520and%2520J.%2520Quetzalcoatl%2520Toledo-Mar%25C3%25ADn%2520and%2520Tiago%2520Vale%2520and%2520Roger%2520G.%2520Melko%2520and%2520Maximilian%2520Swiatlowski%2520and%2520Wojciech%2520T.%2520Fedorko%26entry.1292438233%3D%2520%2520The%2520Large%2520Hadron%2520Collider%2527s%2520high%2520luminosity%2520era%2520presents%2520major%2520computational%250Achallenges%2520in%2520the%2520analysis%2520of%2520collision%2520events.%2520Large%2520amounts%2520of%2520Monte%2520Carlo%250A%2528MC%2529%2520simulation%2520will%2520be%2520required%2520to%2520constrain%2520the%2520statistical%2520uncertainties%2520of%250Athe%2520simulated%2520datasets%2520below%2520these%2520of%2520the%2520experimental%2520data.%2520Modelling%2520of%250Ahigh-energy%2520particles%2520propagating%2520through%2520the%2520calorimeter%2520section%2520of%2520the%250Adetector%2520is%2520the%2520most%2520computationally%2520intensive%2520MC%2520simulation%2520task.%2520We%2520introduce%250Aa%2520technique%2520combining%2520recent%2520advancements%2520in%2520generative%2520models%2520and%2520quantum%250Aannealing%2520for%2520fast%2520and%2520efficient%2520simulation%2520of%2520high-energy%2520particle-calorimeter%250Ainteractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03179v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaloQVAE%20%3A%20Simulating%20high-energy%20particle-calorimeter%20interactions%0A%20%20using%20hybrid%20quantum-classical%20generative%20models&entry.906535625=Sehmimul%20Hoque%20and%20Hao%20Jia%20and%20Abhishek%20Abhishek%20and%20Mojde%20Fadaie%20and%20J.%20Quetzalcoatl%20Toledo-Mar%C3%ADn%20and%20Tiago%20Vale%20and%20Roger%20G.%20Melko%20and%20Maximilian%20Swiatlowski%20and%20Wojciech%20T.%20Fedorko&entry.1292438233=%20%20The%20Large%20Hadron%20Collider%27s%20high%20luminosity%20era%20presents%20major%20computational%0Achallenges%20in%20the%20analysis%20of%20collision%20events.%20Large%20amounts%20of%20Monte%20Carlo%0A%28MC%29%20simulation%20will%20be%20required%20to%20constrain%20the%20statistical%20uncertainties%20of%0Athe%20simulated%20datasets%20below%20these%20of%20the%20experimental%20data.%20Modelling%20of%0Ahigh-energy%20particles%20propagating%20through%20the%20calorimeter%20section%20of%20the%0Adetector%20is%20the%20most%20computationally%20intensive%20MC%20simulation%20task.%20We%20introduce%0Aa%20technique%20combining%20recent%20advancements%20in%20generative%20models%20and%20quantum%0Aannealing%20for%20fast%20and%20efficient%20simulation%20of%20high-energy%20particle-calorimeter%0Ainteractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03179v3&entry.124074799=Read"},
{"title": "Efficient Reinforcement Learning via Decoupling Exploration and\n  Utilization", "author": "Jingpu Yang and Helin Wang and Qirui Zhao and Zhecheng Shi and Zirui Song and Miao Fang", "abstract": "  Reinforcement Learning (RL), recognized as an efficient learning approach,\nhas achieved remarkable success across multiple fields and applications,\nincluding gaming, robotics, and autonomous vehicles. Classical single-agent\nreinforcement learning grapples with the imbalance of exploration and\nexploitation as well as limited generalization abilities. This methodology\nfrequently leads to algorithms settling for suboptimal solutions that are\ntailored only to specific datasets. In this work, our aim is to train agent\nwith efficient learning by decoupling exploration and utilization, so that\nagent can escaping the conundrum of suboptimal Solutions. In reinforcement\nlearning, the previously imposed pessimistic punitive measures have deprived\nthe model of its exploratory potential, resulting in diminished exploration\ncapabilities. To address this, we have introduced an additional optimistic\nActor to enhance the model's exploration ability, while employing a more\nconstrained pessimistic Actor for performance evaluation. The above idea is\nimplemented in the proposed OPARL (Optimistic and Pessimistic Actor\nReinforcement Learning) algorithm. This unique amalgamation within the\nreinforcement learning paradigm fosters a more balanced and efficient approach.\nIt facilitates the optimization of policies that concentrate on high-reward\nactions via pessimistic exploitation strategies while concurrently ensuring\nextensive state coverage through optimistic exploration. Empirical and\ntheoretical investigations demonstrate that OPARL enhances agent capabilities\nin both utilization and exploration. In the most tasks of DMControl benchmark\nand Mujoco environment, OPARL performed better than state-of-the-art methods.\nOur code has released on https://github.com/yydsok/OPARL\n", "link": "http://arxiv.org/abs/2312.15965v4", "date": "2024-05-10", "relevancy": 1.6628, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5767}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.559}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Reinforcement%20Learning%20via%20Decoupling%20Exploration%20and%0A%20%20Utilization&body=Title%3A%20Efficient%20Reinforcement%20Learning%20via%20Decoupling%20Exploration%20and%0A%20%20Utilization%0AAuthor%3A%20Jingpu%20Yang%20and%20Helin%20Wang%20and%20Qirui%20Zhao%20and%20Zhecheng%20Shi%20and%20Zirui%20Song%20and%20Miao%20Fang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%2C%20recognized%20as%20an%20efficient%20learning%20approach%2C%0Ahas%20achieved%20remarkable%20success%20across%20multiple%20fields%20and%20applications%2C%0Aincluding%20gaming%2C%20robotics%2C%20and%20autonomous%20vehicles.%20Classical%20single-agent%0Areinforcement%20learning%20grapples%20with%20the%20imbalance%20of%20exploration%20and%0Aexploitation%20as%20well%20as%20limited%20generalization%20abilities.%20This%20methodology%0Afrequently%20leads%20to%20algorithms%20settling%20for%20suboptimal%20solutions%20that%20are%0Atailored%20only%20to%20specific%20datasets.%20In%20this%20work%2C%20our%20aim%20is%20to%20train%20agent%0Awith%20efficient%20learning%20by%20decoupling%20exploration%20and%20utilization%2C%20so%20that%0Aagent%20can%20escaping%20the%20conundrum%20of%20suboptimal%20Solutions.%20In%20reinforcement%0Alearning%2C%20the%20previously%20imposed%20pessimistic%20punitive%20measures%20have%20deprived%0Athe%20model%20of%20its%20exploratory%20potential%2C%20resulting%20in%20diminished%20exploration%0Acapabilities.%20To%20address%20this%2C%20we%20have%20introduced%20an%20additional%20optimistic%0AActor%20to%20enhance%20the%20model%27s%20exploration%20ability%2C%20while%20employing%20a%20more%0Aconstrained%20pessimistic%20Actor%20for%20performance%20evaluation.%20The%20above%20idea%20is%0Aimplemented%20in%20the%20proposed%20OPARL%20%28Optimistic%20and%20Pessimistic%20Actor%0AReinforcement%20Learning%29%20algorithm.%20This%20unique%20amalgamation%20within%20the%0Areinforcement%20learning%20paradigm%20fosters%20a%20more%20balanced%20and%20efficient%20approach.%0AIt%20facilitates%20the%20optimization%20of%20policies%20that%20concentrate%20on%20high-reward%0Aactions%20via%20pessimistic%20exploitation%20strategies%20while%20concurrently%20ensuring%0Aextensive%20state%20coverage%20through%20optimistic%20exploration.%20Empirical%20and%0Atheoretical%20investigations%20demonstrate%20that%20OPARL%20enhances%20agent%20capabilities%0Ain%20both%20utilization%20and%20exploration.%20In%20the%20most%20tasks%20of%20DMControl%20benchmark%0Aand%20Mujoco%20environment%2C%20OPARL%20performed%20better%20than%20state-of-the-art%20methods.%0AOur%20code%20has%20released%20on%20https%3A//github.com/yydsok/OPARL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15965v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Reinforcement%2520Learning%2520via%2520Decoupling%2520Exploration%2520and%250A%2520%2520Utilization%26entry.906535625%3DJingpu%2520Yang%2520and%2520Helin%2520Wang%2520and%2520Qirui%2520Zhao%2520and%2520Zhecheng%2520Shi%2520and%2520Zirui%2520Song%2520and%2520Miao%2520Fang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520recognized%2520as%2520an%2520efficient%2520learning%2520approach%252C%250Ahas%2520achieved%2520remarkable%2520success%2520across%2520multiple%2520fields%2520and%2520applications%252C%250Aincluding%2520gaming%252C%2520robotics%252C%2520and%2520autonomous%2520vehicles.%2520Classical%2520single-agent%250Areinforcement%2520learning%2520grapples%2520with%2520the%2520imbalance%2520of%2520exploration%2520and%250Aexploitation%2520as%2520well%2520as%2520limited%2520generalization%2520abilities.%2520This%2520methodology%250Afrequently%2520leads%2520to%2520algorithms%2520settling%2520for%2520suboptimal%2520solutions%2520that%2520are%250Atailored%2520only%2520to%2520specific%2520datasets.%2520In%2520this%2520work%252C%2520our%2520aim%2520is%2520to%2520train%2520agent%250Awith%2520efficient%2520learning%2520by%2520decoupling%2520exploration%2520and%2520utilization%252C%2520so%2520that%250Aagent%2520can%2520escaping%2520the%2520conundrum%2520of%2520suboptimal%2520Solutions.%2520In%2520reinforcement%250Alearning%252C%2520the%2520previously%2520imposed%2520pessimistic%2520punitive%2520measures%2520have%2520deprived%250Athe%2520model%2520of%2520its%2520exploratory%2520potential%252C%2520resulting%2520in%2520diminished%2520exploration%250Acapabilities.%2520To%2520address%2520this%252C%2520we%2520have%2520introduced%2520an%2520additional%2520optimistic%250AActor%2520to%2520enhance%2520the%2520model%2527s%2520exploration%2520ability%252C%2520while%2520employing%2520a%2520more%250Aconstrained%2520pessimistic%2520Actor%2520for%2520performance%2520evaluation.%2520The%2520above%2520idea%2520is%250Aimplemented%2520in%2520the%2520proposed%2520OPARL%2520%2528Optimistic%2520and%2520Pessimistic%2520Actor%250AReinforcement%2520Learning%2529%2520algorithm.%2520This%2520unique%2520amalgamation%2520within%2520the%250Areinforcement%2520learning%2520paradigm%2520fosters%2520a%2520more%2520balanced%2520and%2520efficient%2520approach.%250AIt%2520facilitates%2520the%2520optimization%2520of%2520policies%2520that%2520concentrate%2520on%2520high-reward%250Aactions%2520via%2520pessimistic%2520exploitation%2520strategies%2520while%2520concurrently%2520ensuring%250Aextensive%2520state%2520coverage%2520through%2520optimistic%2520exploration.%2520Empirical%2520and%250Atheoretical%2520investigations%2520demonstrate%2520that%2520OPARL%2520enhances%2520agent%2520capabilities%250Ain%2520both%2520utilization%2520and%2520exploration.%2520In%2520the%2520most%2520tasks%2520of%2520DMControl%2520benchmark%250Aand%2520Mujoco%2520environment%252C%2520OPARL%2520performed%2520better%2520than%2520state-of-the-art%2520methods.%250AOur%2520code%2520has%2520released%2520on%2520https%253A//github.com/yydsok/OPARL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15965v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Reinforcement%20Learning%20via%20Decoupling%20Exploration%20and%0A%20%20Utilization&entry.906535625=Jingpu%20Yang%20and%20Helin%20Wang%20and%20Qirui%20Zhao%20and%20Zhecheng%20Shi%20and%20Zirui%20Song%20and%20Miao%20Fang&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%2C%20recognized%20as%20an%20efficient%20learning%20approach%2C%0Ahas%20achieved%20remarkable%20success%20across%20multiple%20fields%20and%20applications%2C%0Aincluding%20gaming%2C%20robotics%2C%20and%20autonomous%20vehicles.%20Classical%20single-agent%0Areinforcement%20learning%20grapples%20with%20the%20imbalance%20of%20exploration%20and%0Aexploitation%20as%20well%20as%20limited%20generalization%20abilities.%20This%20methodology%0Afrequently%20leads%20to%20algorithms%20settling%20for%20suboptimal%20solutions%20that%20are%0Atailored%20only%20to%20specific%20datasets.%20In%20this%20work%2C%20our%20aim%20is%20to%20train%20agent%0Awith%20efficient%20learning%20by%20decoupling%20exploration%20and%20utilization%2C%20so%20that%0Aagent%20can%20escaping%20the%20conundrum%20of%20suboptimal%20Solutions.%20In%20reinforcement%0Alearning%2C%20the%20previously%20imposed%20pessimistic%20punitive%20measures%20have%20deprived%0Athe%20model%20of%20its%20exploratory%20potential%2C%20resulting%20in%20diminished%20exploration%0Acapabilities.%20To%20address%20this%2C%20we%20have%20introduced%20an%20additional%20optimistic%0AActor%20to%20enhance%20the%20model%27s%20exploration%20ability%2C%20while%20employing%20a%20more%0Aconstrained%20pessimistic%20Actor%20for%20performance%20evaluation.%20The%20above%20idea%20is%0Aimplemented%20in%20the%20proposed%20OPARL%20%28Optimistic%20and%20Pessimistic%20Actor%0AReinforcement%20Learning%29%20algorithm.%20This%20unique%20amalgamation%20within%20the%0Areinforcement%20learning%20paradigm%20fosters%20a%20more%20balanced%20and%20efficient%20approach.%0AIt%20facilitates%20the%20optimization%20of%20policies%20that%20concentrate%20on%20high-reward%0Aactions%20via%20pessimistic%20exploitation%20strategies%20while%20concurrently%20ensuring%0Aextensive%20state%20coverage%20through%20optimistic%20exploration.%20Empirical%20and%0Atheoretical%20investigations%20demonstrate%20that%20OPARL%20enhances%20agent%20capabilities%0Ain%20both%20utilization%20and%20exploration.%20In%20the%20most%20tasks%20of%20DMControl%20benchmark%0Aand%20Mujoco%20environment%2C%20OPARL%20performed%20better%20than%20state-of-the-art%20methods.%0AOur%20code%20has%20released%20on%20https%3A//github.com/yydsok/OPARL%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15965v4&entry.124074799=Read"},
{"title": "Partially Stochastic Infinitely Deep Bayesian Neural Networks", "author": "Sergio Calvo-Ordonez and Matthieu Meunier and Francesco Piatti and Yuantao Shi", "abstract": "  In this paper, we present Partially Stochastic Infinitely Deep Bayesian\nNeural Networks, a novel family of architectures that integrates partial\nstochasticity into the framework of infinitely deep neural networks. Our new\nclass of architectures is designed to improve the limitations of existing\narchitectures around computational efficiency at training and inference time.\nTo do this, we leverage the advantages of partial stochasticity in the\ninfinite-depth limit which include the benefits of full stochasticity e.g.\nrobustness, uncertainty quantification, and memory efficiency, whilst improving\ntheir limitations around computational complexity. We present a variety of\narchitectural configurations, offering flexibility in network design including\ndifferent methods for weight partition. We also provide mathematical guarantees\non the expressivity of our models by establishing that our network family\nqualifies as Universal Conditional Distribution Approximators. Lastly,\nempirical evaluations across multiple tasks show that our proposed\narchitectures achieve better downstream task performance and uncertainty\nquantification than their counterparts while being significantly more\nefficient.\n", "link": "http://arxiv.org/abs/2402.03495v2", "date": "2024-05-10", "relevancy": 1.6499, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5694}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5404}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partially%20Stochastic%20Infinitely%20Deep%20Bayesian%20Neural%20Networks&body=Title%3A%20Partially%20Stochastic%20Infinitely%20Deep%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Sergio%20Calvo-Ordonez%20and%20Matthieu%20Meunier%20and%20Francesco%20Piatti%20and%20Yuantao%20Shi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20Partially%20Stochastic%20Infinitely%20Deep%20Bayesian%0ANeural%20Networks%2C%20a%20novel%20family%20of%20architectures%20that%20integrates%20partial%0Astochasticity%20into%20the%20framework%20of%20infinitely%20deep%20neural%20networks.%20Our%20new%0Aclass%20of%20architectures%20is%20designed%20to%20improve%20the%20limitations%20of%20existing%0Aarchitectures%20around%20computational%20efficiency%20at%20training%20and%20inference%20time.%0ATo%20do%20this%2C%20we%20leverage%20the%20advantages%20of%20partial%20stochasticity%20in%20the%0Ainfinite-depth%20limit%20which%20include%20the%20benefits%20of%20full%20stochasticity%20e.g.%0Arobustness%2C%20uncertainty%20quantification%2C%20and%20memory%20efficiency%2C%20whilst%20improving%0Atheir%20limitations%20around%20computational%20complexity.%20We%20present%20a%20variety%20of%0Aarchitectural%20configurations%2C%20offering%20flexibility%20in%20network%20design%20including%0Adifferent%20methods%20for%20weight%20partition.%20We%20also%20provide%20mathematical%20guarantees%0Aon%20the%20expressivity%20of%20our%20models%20by%20establishing%20that%20our%20network%20family%0Aqualifies%20as%20Universal%20Conditional%20Distribution%20Approximators.%20Lastly%2C%0Aempirical%20evaluations%20across%20multiple%20tasks%20show%20that%20our%20proposed%0Aarchitectures%20achieve%20better%20downstream%20task%20performance%20and%20uncertainty%0Aquantification%20than%20their%20counterparts%20while%20being%20significantly%20more%0Aefficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03495v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartially%2520Stochastic%2520Infinitely%2520Deep%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DSergio%2520Calvo-Ordonez%2520and%2520Matthieu%2520Meunier%2520and%2520Francesco%2520Piatti%2520and%2520Yuantao%2520Shi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520Partially%2520Stochastic%2520Infinitely%2520Deep%2520Bayesian%250ANeural%2520Networks%252C%2520a%2520novel%2520family%2520of%2520architectures%2520that%2520integrates%2520partial%250Astochasticity%2520into%2520the%2520framework%2520of%2520infinitely%2520deep%2520neural%2520networks.%2520Our%2520new%250Aclass%2520of%2520architectures%2520is%2520designed%2520to%2520improve%2520the%2520limitations%2520of%2520existing%250Aarchitectures%2520around%2520computational%2520efficiency%2520at%2520training%2520and%2520inference%2520time.%250ATo%2520do%2520this%252C%2520we%2520leverage%2520the%2520advantages%2520of%2520partial%2520stochasticity%2520in%2520the%250Ainfinite-depth%2520limit%2520which%2520include%2520the%2520benefits%2520of%2520full%2520stochasticity%2520e.g.%250Arobustness%252C%2520uncertainty%2520quantification%252C%2520and%2520memory%2520efficiency%252C%2520whilst%2520improving%250Atheir%2520limitations%2520around%2520computational%2520complexity.%2520We%2520present%2520a%2520variety%2520of%250Aarchitectural%2520configurations%252C%2520offering%2520flexibility%2520in%2520network%2520design%2520including%250Adifferent%2520methods%2520for%2520weight%2520partition.%2520We%2520also%2520provide%2520mathematical%2520guarantees%250Aon%2520the%2520expressivity%2520of%2520our%2520models%2520by%2520establishing%2520that%2520our%2520network%2520family%250Aqualifies%2520as%2520Universal%2520Conditional%2520Distribution%2520Approximators.%2520Lastly%252C%250Aempirical%2520evaluations%2520across%2520multiple%2520tasks%2520show%2520that%2520our%2520proposed%250Aarchitectures%2520achieve%2520better%2520downstream%2520task%2520performance%2520and%2520uncertainty%250Aquantification%2520than%2520their%2520counterparts%2520while%2520being%2520significantly%2520more%250Aefficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03495v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partially%20Stochastic%20Infinitely%20Deep%20Bayesian%20Neural%20Networks&entry.906535625=Sergio%20Calvo-Ordonez%20and%20Matthieu%20Meunier%20and%20Francesco%20Piatti%20and%20Yuantao%20Shi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20Partially%20Stochastic%20Infinitely%20Deep%20Bayesian%0ANeural%20Networks%2C%20a%20novel%20family%20of%20architectures%20that%20integrates%20partial%0Astochasticity%20into%20the%20framework%20of%20infinitely%20deep%20neural%20networks.%20Our%20new%0Aclass%20of%20architectures%20is%20designed%20to%20improve%20the%20limitations%20of%20existing%0Aarchitectures%20around%20computational%20efficiency%20at%20training%20and%20inference%20time.%0ATo%20do%20this%2C%20we%20leverage%20the%20advantages%20of%20partial%20stochasticity%20in%20the%0Ainfinite-depth%20limit%20which%20include%20the%20benefits%20of%20full%20stochasticity%20e.g.%0Arobustness%2C%20uncertainty%20quantification%2C%20and%20memory%20efficiency%2C%20whilst%20improving%0Atheir%20limitations%20around%20computational%20complexity.%20We%20present%20a%20variety%20of%0Aarchitectural%20configurations%2C%20offering%20flexibility%20in%20network%20design%20including%0Adifferent%20methods%20for%20weight%20partition.%20We%20also%20provide%20mathematical%20guarantees%0Aon%20the%20expressivity%20of%20our%20models%20by%20establishing%20that%20our%20network%20family%0Aqualifies%20as%20Universal%20Conditional%20Distribution%20Approximators.%20Lastly%2C%0Aempirical%20evaluations%20across%20multiple%20tasks%20show%20that%20our%20proposed%0Aarchitectures%20achieve%20better%20downstream%20task%20performance%20and%20uncertainty%0Aquantification%20than%20their%20counterparts%20while%20being%20significantly%20more%0Aefficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03495v2&entry.124074799=Read"},
{"title": "CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous\n  Underwater Cave Exploration", "author": "A. Abdullah and T. Barua and R. Tibbetts and Z. Chen and M. J. Islam and I. Rekleitis", "abstract": "  In this paper, we present CaveSeg - the first visual learning pipeline for\nsemantic segmentation and scene parsing for AUV navigation inside underwater\ncaves. We address the problem of scarce annotated training data by preparing a\ncomprehensive dataset for semantic segmentation of underwater cave scenes. It\ncontains pixel annotations for important navigation markers (e.g. caveline,\narrows), obstacles (e.g. ground plane and overhead layers), scuba divers, and\nopen areas for servoing. Through comprehensive benchmark analyses on cave\nsystems in USA, Mexico, and Spain locations, we demonstrate that robust deep\nvisual models can be developed based on CaveSeg for fast semantic scene parsing\nof underwater cave environments. In particular, we formulate a novel\ntransformer-based model that is computationally light and offers near real-time\nexecution in addition to achieving state-of-the-art performance. Finally, we\nexplore the design choices and implications of semantic segmentation for visual\nservoing by AUVs inside underwater caves. The proposed model and benchmark\ndataset open up promising opportunities for future research in autonomous\nunderwater cave exploration and mapping.\n", "link": "http://arxiv.org/abs/2309.11038v6", "date": "2024-05-10", "relevancy": 1.6106, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5415}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5363}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaveSeg%3A%20Deep%20Semantic%20Segmentation%20and%20Scene%20Parsing%20for%20Autonomous%0A%20%20Underwater%20Cave%20Exploration&body=Title%3A%20CaveSeg%3A%20Deep%20Semantic%20Segmentation%20and%20Scene%20Parsing%20for%20Autonomous%0A%20%20Underwater%20Cave%20Exploration%0AAuthor%3A%20A.%20Abdullah%20and%20T.%20Barua%20and%20R.%20Tibbetts%20and%20Z.%20Chen%20and%20M.%20J.%20Islam%20and%20I.%20Rekleitis%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20CaveSeg%20-%20the%20first%20visual%20learning%20pipeline%20for%0Asemantic%20segmentation%20and%20scene%20parsing%20for%20AUV%20navigation%20inside%20underwater%0Acaves.%20We%20address%20the%20problem%20of%20scarce%20annotated%20training%20data%20by%20preparing%20a%0Acomprehensive%20dataset%20for%20semantic%20segmentation%20of%20underwater%20cave%20scenes.%20It%0Acontains%20pixel%20annotations%20for%20important%20navigation%20markers%20%28e.g.%20caveline%2C%0Aarrows%29%2C%20obstacles%20%28e.g.%20ground%20plane%20and%20overhead%20layers%29%2C%20scuba%20divers%2C%20and%0Aopen%20areas%20for%20servoing.%20Through%20comprehensive%20benchmark%20analyses%20on%20cave%0Asystems%20in%20USA%2C%20Mexico%2C%20and%20Spain%20locations%2C%20we%20demonstrate%20that%20robust%20deep%0Avisual%20models%20can%20be%20developed%20based%20on%20CaveSeg%20for%20fast%20semantic%20scene%20parsing%0Aof%20underwater%20cave%20environments.%20In%20particular%2C%20we%20formulate%20a%20novel%0Atransformer-based%20model%20that%20is%20computationally%20light%20and%20offers%20near%20real-time%0Aexecution%20in%20addition%20to%20achieving%20state-of-the-art%20performance.%20Finally%2C%20we%0Aexplore%20the%20design%20choices%20and%20implications%20of%20semantic%20segmentation%20for%20visual%0Aservoing%20by%20AUVs%20inside%20underwater%20caves.%20The%20proposed%20model%20and%20benchmark%0Adataset%20open%20up%20promising%20opportunities%20for%20future%20research%20in%20autonomous%0Aunderwater%20cave%20exploration%20and%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11038v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaveSeg%253A%2520Deep%2520Semantic%2520Segmentation%2520and%2520Scene%2520Parsing%2520for%2520Autonomous%250A%2520%2520Underwater%2520Cave%2520Exploration%26entry.906535625%3DA.%2520Abdullah%2520and%2520T.%2520Barua%2520and%2520R.%2520Tibbetts%2520and%2520Z.%2520Chen%2520and%2520M.%2520J.%2520Islam%2520and%2520I.%2520Rekleitis%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520CaveSeg%2520-%2520the%2520first%2520visual%2520learning%2520pipeline%2520for%250Asemantic%2520segmentation%2520and%2520scene%2520parsing%2520for%2520AUV%2520navigation%2520inside%2520underwater%250Acaves.%2520We%2520address%2520the%2520problem%2520of%2520scarce%2520annotated%2520training%2520data%2520by%2520preparing%2520a%250Acomprehensive%2520dataset%2520for%2520semantic%2520segmentation%2520of%2520underwater%2520cave%2520scenes.%2520It%250Acontains%2520pixel%2520annotations%2520for%2520important%2520navigation%2520markers%2520%2528e.g.%2520caveline%252C%250Aarrows%2529%252C%2520obstacles%2520%2528e.g.%2520ground%2520plane%2520and%2520overhead%2520layers%2529%252C%2520scuba%2520divers%252C%2520and%250Aopen%2520areas%2520for%2520servoing.%2520Through%2520comprehensive%2520benchmark%2520analyses%2520on%2520cave%250Asystems%2520in%2520USA%252C%2520Mexico%252C%2520and%2520Spain%2520locations%252C%2520we%2520demonstrate%2520that%2520robust%2520deep%250Avisual%2520models%2520can%2520be%2520developed%2520based%2520on%2520CaveSeg%2520for%2520fast%2520semantic%2520scene%2520parsing%250Aof%2520underwater%2520cave%2520environments.%2520In%2520particular%252C%2520we%2520formulate%2520a%2520novel%250Atransformer-based%2520model%2520that%2520is%2520computationally%2520light%2520and%2520offers%2520near%2520real-time%250Aexecution%2520in%2520addition%2520to%2520achieving%2520state-of-the-art%2520performance.%2520Finally%252C%2520we%250Aexplore%2520the%2520design%2520choices%2520and%2520implications%2520of%2520semantic%2520segmentation%2520for%2520visual%250Aservoing%2520by%2520AUVs%2520inside%2520underwater%2520caves.%2520The%2520proposed%2520model%2520and%2520benchmark%250Adataset%2520open%2520up%2520promising%2520opportunities%2520for%2520future%2520research%2520in%2520autonomous%250Aunderwater%2520cave%2520exploration%2520and%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11038v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaveSeg%3A%20Deep%20Semantic%20Segmentation%20and%20Scene%20Parsing%20for%20Autonomous%0A%20%20Underwater%20Cave%20Exploration&entry.906535625=A.%20Abdullah%20and%20T.%20Barua%20and%20R.%20Tibbetts%20and%20Z.%20Chen%20and%20M.%20J.%20Islam%20and%20I.%20Rekleitis&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20CaveSeg%20-%20the%20first%20visual%20learning%20pipeline%20for%0Asemantic%20segmentation%20and%20scene%20parsing%20for%20AUV%20navigation%20inside%20underwater%0Acaves.%20We%20address%20the%20problem%20of%20scarce%20annotated%20training%20data%20by%20preparing%20a%0Acomprehensive%20dataset%20for%20semantic%20segmentation%20of%20underwater%20cave%20scenes.%20It%0Acontains%20pixel%20annotations%20for%20important%20navigation%20markers%20%28e.g.%20caveline%2C%0Aarrows%29%2C%20obstacles%20%28e.g.%20ground%20plane%20and%20overhead%20layers%29%2C%20scuba%20divers%2C%20and%0Aopen%20areas%20for%20servoing.%20Through%20comprehensive%20benchmark%20analyses%20on%20cave%0Asystems%20in%20USA%2C%20Mexico%2C%20and%20Spain%20locations%2C%20we%20demonstrate%20that%20robust%20deep%0Avisual%20models%20can%20be%20developed%20based%20on%20CaveSeg%20for%20fast%20semantic%20scene%20parsing%0Aof%20underwater%20cave%20environments.%20In%20particular%2C%20we%20formulate%20a%20novel%0Atransformer-based%20model%20that%20is%20computationally%20light%20and%20offers%20near%20real-time%0Aexecution%20in%20addition%20to%20achieving%20state-of-the-art%20performance.%20Finally%2C%20we%0Aexplore%20the%20design%20choices%20and%20implications%20of%20semantic%20segmentation%20for%20visual%0Aservoing%20by%20AUVs%20inside%20underwater%20caves.%20The%20proposed%20model%20and%20benchmark%0Adataset%20open%20up%20promising%20opportunities%20for%20future%20research%20in%20autonomous%0Aunderwater%20cave%20exploration%20and%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11038v6&entry.124074799=Read"},
{"title": "MRSegmentator: Robust Multi-Modality Segmentation of 40 Classes in MRI\n  and CT Sequences", "author": "Hartmut H\u00e4ntze and Lina Xu and Felix J. Dorfner and Leonhard Donle and Daniel Truhn and Hugo Aerts and Mathias Prokop and Bram van Ginneken and Alessa Hering and Lisa C. Adams and Keno K. Bressem", "abstract": "  Purpose: To introduce a deep learning model capable of multi-organ\nsegmentation in MRI scans, offering a solution to the current limitations in\nMRI analysis due to challenges in resolution, standardized intensity values,\nand variability in sequences.\n  Materials and Methods: he model was trained on 1,200 manually annotated MRI\nscans from the UK Biobank, 221 in-house MRI scans and 1228 CT scans, leveraging\ncross-modality transfer learning from CT segmentation models. A\nhuman-in-the-loop annotation workflow was employed to efficiently create\nhigh-quality segmentations. The model's performance was evaluated on NAKO and\nthe AMOS22 dataset containing 600 and 60 MRI examinations. Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD) was used to assess segmentation\naccuracy. The model will be open sourced.\n  Results: The model showcased high accuracy in segmenting well-defined organs,\nachieving Dice Similarity Coefficient (DSC) scores of 0.97 for the right and\nleft lungs, and 0.95 for the heart. It also demonstrated robustness in organs\nlike the liver (DSC: 0.96) and kidneys (DSC: 0.95 left, 0.95 right), which\npresent more variability. However, segmentation of smaller and complex\nstructures such as the portal and splenic veins (DSC: 0.54) and adrenal glands\n(DSC: 0.65 left, 0.61 right) revealed the need for further model optimization.\n  Conclusion: The proposed model is a robust, tool for accurate segmentation of\n40 anatomical structures in MRI and CT images. By leveraging cross-modality\nlearning and interactive annotation, the model achieves strong performance and\ngeneralizability across diverse datasets, making it a valuable resource for\nresearchers and clinicians. It is open source and can be downloaded from\nhttps://github.com/hhaentze/MRSegmentator.\n", "link": "http://arxiv.org/abs/2405.06463v1", "date": "2024-05-10", "relevancy": 1.6088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5586}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRSegmentator%3A%20Robust%20Multi-Modality%20Segmentation%20of%2040%20Classes%20in%20MRI%0A%20%20and%20CT%20Sequences&body=Title%3A%20MRSegmentator%3A%20Robust%20Multi-Modality%20Segmentation%20of%2040%20Classes%20in%20MRI%0A%20%20and%20CT%20Sequences%0AAuthor%3A%20Hartmut%20H%C3%A4ntze%20and%20Lina%20Xu%20and%20Felix%20J.%20Dorfner%20and%20Leonhard%20Donle%20and%20Daniel%20Truhn%20and%20Hugo%20Aerts%20and%20Mathias%20Prokop%20and%20Bram%20van%20Ginneken%20and%20Alessa%20Hering%20and%20Lisa%20C.%20Adams%20and%20Keno%20K.%20Bressem%0AAbstract%3A%20%20%20Purpose%3A%20To%20introduce%20a%20deep%20learning%20model%20capable%20of%20multi-organ%0Asegmentation%20in%20MRI%20scans%2C%20offering%20a%20solution%20to%20the%20current%20limitations%20in%0AMRI%20analysis%20due%20to%20challenges%20in%20resolution%2C%20standardized%20intensity%20values%2C%0Aand%20variability%20in%20sequences.%0A%20%20Materials%20and%20Methods%3A%20he%20model%20was%20trained%20on%201%2C200%20manually%20annotated%20MRI%0Ascans%20from%20the%20UK%20Biobank%2C%20221%20in-house%20MRI%20scans%20and%201228%20CT%20scans%2C%20leveraging%0Across-modality%20transfer%20learning%20from%20CT%20segmentation%20models.%20A%0Ahuman-in-the-loop%20annotation%20workflow%20was%20employed%20to%20efficiently%20create%0Ahigh-quality%20segmentations.%20The%20model%27s%20performance%20was%20evaluated%20on%20NAKO%20and%0Athe%20AMOS22%20dataset%20containing%20600%20and%2060%20MRI%20examinations.%20Dice%20Similarity%0ACoefficient%20%28DSC%29%20and%20Hausdorff%20Distance%20%28HD%29%20was%20used%20to%20assess%20segmentation%0Aaccuracy.%20The%20model%20will%20be%20open%20sourced.%0A%20%20Results%3A%20The%20model%20showcased%20high%20accuracy%20in%20segmenting%20well-defined%20organs%2C%0Aachieving%20Dice%20Similarity%20Coefficient%20%28DSC%29%20scores%20of%200.97%20for%20the%20right%20and%0Aleft%20lungs%2C%20and%200.95%20for%20the%20heart.%20It%20also%20demonstrated%20robustness%20in%20organs%0Alike%20the%20liver%20%28DSC%3A%200.96%29%20and%20kidneys%20%28DSC%3A%200.95%20left%2C%200.95%20right%29%2C%20which%0Apresent%20more%20variability.%20However%2C%20segmentation%20of%20smaller%20and%20complex%0Astructures%20such%20as%20the%20portal%20and%20splenic%20veins%20%28DSC%3A%200.54%29%20and%20adrenal%20glands%0A%28DSC%3A%200.65%20left%2C%200.61%20right%29%20revealed%20the%20need%20for%20further%20model%20optimization.%0A%20%20Conclusion%3A%20The%20proposed%20model%20is%20a%20robust%2C%20tool%20for%20accurate%20segmentation%20of%0A40%20anatomical%20structures%20in%20MRI%20and%20CT%20images.%20By%20leveraging%20cross-modality%0Alearning%20and%20interactive%20annotation%2C%20the%20model%20achieves%20strong%20performance%20and%0Ageneralizability%20across%20diverse%20datasets%2C%20making%20it%20a%20valuable%20resource%20for%0Aresearchers%20and%20clinicians.%20It%20is%20open%20source%20and%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/hhaentze/MRSegmentator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRSegmentator%253A%2520Robust%2520Multi-Modality%2520Segmentation%2520of%252040%2520Classes%2520in%2520MRI%250A%2520%2520and%2520CT%2520Sequences%26entry.906535625%3DHartmut%2520H%25C3%25A4ntze%2520and%2520Lina%2520Xu%2520and%2520Felix%2520J.%2520Dorfner%2520and%2520Leonhard%2520Donle%2520and%2520Daniel%2520Truhn%2520and%2520Hugo%2520Aerts%2520and%2520Mathias%2520Prokop%2520and%2520Bram%2520van%2520Ginneken%2520and%2520Alessa%2520Hering%2520and%2520Lisa%2520C.%2520Adams%2520and%2520Keno%2520K.%2520Bressem%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520introduce%2520a%2520deep%2520learning%2520model%2520capable%2520of%2520multi-organ%250Asegmentation%2520in%2520MRI%2520scans%252C%2520offering%2520a%2520solution%2520to%2520the%2520current%2520limitations%2520in%250AMRI%2520analysis%2520due%2520to%2520challenges%2520in%2520resolution%252C%2520standardized%2520intensity%2520values%252C%250Aand%2520variability%2520in%2520sequences.%250A%2520%2520Materials%2520and%2520Methods%253A%2520he%2520model%2520was%2520trained%2520on%25201%252C200%2520manually%2520annotated%2520MRI%250Ascans%2520from%2520the%2520UK%2520Biobank%252C%2520221%2520in-house%2520MRI%2520scans%2520and%25201228%2520CT%2520scans%252C%2520leveraging%250Across-modality%2520transfer%2520learning%2520from%2520CT%2520segmentation%2520models.%2520A%250Ahuman-in-the-loop%2520annotation%2520workflow%2520was%2520employed%2520to%2520efficiently%2520create%250Ahigh-quality%2520segmentations.%2520The%2520model%2527s%2520performance%2520was%2520evaluated%2520on%2520NAKO%2520and%250Athe%2520AMOS22%2520dataset%2520containing%2520600%2520and%252060%2520MRI%2520examinations.%2520Dice%2520Similarity%250ACoefficient%2520%2528DSC%2529%2520and%2520Hausdorff%2520Distance%2520%2528HD%2529%2520was%2520used%2520to%2520assess%2520segmentation%250Aaccuracy.%2520The%2520model%2520will%2520be%2520open%2520sourced.%250A%2520%2520Results%253A%2520The%2520model%2520showcased%2520high%2520accuracy%2520in%2520segmenting%2520well-defined%2520organs%252C%250Aachieving%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520scores%2520of%25200.97%2520for%2520the%2520right%2520and%250Aleft%2520lungs%252C%2520and%25200.95%2520for%2520the%2520heart.%2520It%2520also%2520demonstrated%2520robustness%2520in%2520organs%250Alike%2520the%2520liver%2520%2528DSC%253A%25200.96%2529%2520and%2520kidneys%2520%2528DSC%253A%25200.95%2520left%252C%25200.95%2520right%2529%252C%2520which%250Apresent%2520more%2520variability.%2520However%252C%2520segmentation%2520of%2520smaller%2520and%2520complex%250Astructures%2520such%2520as%2520the%2520portal%2520and%2520splenic%2520veins%2520%2528DSC%253A%25200.54%2529%2520and%2520adrenal%2520glands%250A%2528DSC%253A%25200.65%2520left%252C%25200.61%2520right%2529%2520revealed%2520the%2520need%2520for%2520further%2520model%2520optimization.%250A%2520%2520Conclusion%253A%2520The%2520proposed%2520model%2520is%2520a%2520robust%252C%2520tool%2520for%2520accurate%2520segmentation%2520of%250A40%2520anatomical%2520structures%2520in%2520MRI%2520and%2520CT%2520images.%2520By%2520leveraging%2520cross-modality%250Alearning%2520and%2520interactive%2520annotation%252C%2520the%2520model%2520achieves%2520strong%2520performance%2520and%250Ageneralizability%2520across%2520diverse%2520datasets%252C%2520making%2520it%2520a%2520valuable%2520resource%2520for%250Aresearchers%2520and%2520clinicians.%2520It%2520is%2520open%2520source%2520and%2520can%2520be%2520downloaded%2520from%250Ahttps%253A//github.com/hhaentze/MRSegmentator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRSegmentator%3A%20Robust%20Multi-Modality%20Segmentation%20of%2040%20Classes%20in%20MRI%0A%20%20and%20CT%20Sequences&entry.906535625=Hartmut%20H%C3%A4ntze%20and%20Lina%20Xu%20and%20Felix%20J.%20Dorfner%20and%20Leonhard%20Donle%20and%20Daniel%20Truhn%20and%20Hugo%20Aerts%20and%20Mathias%20Prokop%20and%20Bram%20van%20Ginneken%20and%20Alessa%20Hering%20and%20Lisa%20C.%20Adams%20and%20Keno%20K.%20Bressem&entry.1292438233=%20%20Purpose%3A%20To%20introduce%20a%20deep%20learning%20model%20capable%20of%20multi-organ%0Asegmentation%20in%20MRI%20scans%2C%20offering%20a%20solution%20to%20the%20current%20limitations%20in%0AMRI%20analysis%20due%20to%20challenges%20in%20resolution%2C%20standardized%20intensity%20values%2C%0Aand%20variability%20in%20sequences.%0A%20%20Materials%20and%20Methods%3A%20he%20model%20was%20trained%20on%201%2C200%20manually%20annotated%20MRI%0Ascans%20from%20the%20UK%20Biobank%2C%20221%20in-house%20MRI%20scans%20and%201228%20CT%20scans%2C%20leveraging%0Across-modality%20transfer%20learning%20from%20CT%20segmentation%20models.%20A%0Ahuman-in-the-loop%20annotation%20workflow%20was%20employed%20to%20efficiently%20create%0Ahigh-quality%20segmentations.%20The%20model%27s%20performance%20was%20evaluated%20on%20NAKO%20and%0Athe%20AMOS22%20dataset%20containing%20600%20and%2060%20MRI%20examinations.%20Dice%20Similarity%0ACoefficient%20%28DSC%29%20and%20Hausdorff%20Distance%20%28HD%29%20was%20used%20to%20assess%20segmentation%0Aaccuracy.%20The%20model%20will%20be%20open%20sourced.%0A%20%20Results%3A%20The%20model%20showcased%20high%20accuracy%20in%20segmenting%20well-defined%20organs%2C%0Aachieving%20Dice%20Similarity%20Coefficient%20%28DSC%29%20scores%20of%200.97%20for%20the%20right%20and%0Aleft%20lungs%2C%20and%200.95%20for%20the%20heart.%20It%20also%20demonstrated%20robustness%20in%20organs%0Alike%20the%20liver%20%28DSC%3A%200.96%29%20and%20kidneys%20%28DSC%3A%200.95%20left%2C%200.95%20right%29%2C%20which%0Apresent%20more%20variability.%20However%2C%20segmentation%20of%20smaller%20and%20complex%0Astructures%20such%20as%20the%20portal%20and%20splenic%20veins%20%28DSC%3A%200.54%29%20and%20adrenal%20glands%0A%28DSC%3A%200.65%20left%2C%200.61%20right%29%20revealed%20the%20need%20for%20further%20model%20optimization.%0A%20%20Conclusion%3A%20The%20proposed%20model%20is%20a%20robust%2C%20tool%20for%20accurate%20segmentation%20of%0A40%20anatomical%20structures%20in%20MRI%20and%20CT%20images.%20By%20leveraging%20cross-modality%0Alearning%20and%20interactive%20annotation%2C%20the%20model%20achieves%20strong%20performance%20and%0Ageneralizability%20across%20diverse%20datasets%2C%20making%20it%20a%20valuable%20resource%20for%0Aresearchers%20and%20clinicians.%20It%20is%20open%20source%20and%20can%20be%20downloaded%20from%0Ahttps%3A//github.com/hhaentze/MRSegmentator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06463v1&entry.124074799=Read"},
{"title": "Phylo2Vec: a vector representation for binary trees", "author": "Matthew J Penn and Neil Scheidwasser and Mark P Khurana and David A Duch\u00eane and Christl A Donnelly and Samir Bhatt", "abstract": "  Binary phylogenetic trees inferred from biological data are central to\nunderstanding the shared history among evolutionary units. However, inferring\nthe placement of latent nodes in a tree is NP-hard and thus computationally\nexpensive. State-of-the-art methods rely on carefully designed heuristics for\ntree search. These methods use different data structures for easy manipulation\n(e.g., classes in object-oriented programming languages) and readable\nrepresentation of trees (e.g., Newick-format strings). Here, we present\nPhylo2Vec, a parsimonious encoding for phylogenetic trees that serves as a\nunified approach for both manipulating and representing phylogenetic trees.\nPhylo2Vec maps any binary tree with $n$ leaves to a unique integer vector of\nlength $n-1$. The advantages of Phylo2Vec are fourfold: i) fast tree sampling,\n(ii) compressed tree representation compared to a Newick string, iii) quick and\nunambiguous verification if two binary trees are identical topologically, and\niv) systematic ability to traverse tree space in very large or small jumps. As\na proof of concept, we use Phylo2Vec for maximum likelihood inference on five\nreal-world datasets and show that a simple hill-climbing-based optimisation\nscheme can efficiently traverse the vastness of tree space from a random to an\noptimal tree.\n", "link": "http://arxiv.org/abs/2304.12693v3", "date": "2024-05-10", "relevancy": 1.5794, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4163}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phylo2Vec%3A%20a%20vector%20representation%20for%20binary%20trees&body=Title%3A%20Phylo2Vec%3A%20a%20vector%20representation%20for%20binary%20trees%0AAuthor%3A%20Matthew%20J%20Penn%20and%20Neil%20Scheidwasser%20and%20Mark%20P%20Khurana%20and%20David%20A%20Duch%C3%AAne%20and%20Christl%20A%20Donnelly%20and%20Samir%20Bhatt%0AAbstract%3A%20%20%20Binary%20phylogenetic%20trees%20inferred%20from%20biological%20data%20are%20central%20to%0Aunderstanding%20the%20shared%20history%20among%20evolutionary%20units.%20However%2C%20inferring%0Athe%20placement%20of%20latent%20nodes%20in%20a%20tree%20is%20NP-hard%20and%20thus%20computationally%0Aexpensive.%20State-of-the-art%20methods%20rely%20on%20carefully%20designed%20heuristics%20for%0Atree%20search.%20These%20methods%20use%20different%20data%20structures%20for%20easy%20manipulation%0A%28e.g.%2C%20classes%20in%20object-oriented%20programming%20languages%29%20and%20readable%0Arepresentation%20of%20trees%20%28e.g.%2C%20Newick-format%20strings%29.%20Here%2C%20we%20present%0APhylo2Vec%2C%20a%20parsimonious%20encoding%20for%20phylogenetic%20trees%20that%20serves%20as%20a%0Aunified%20approach%20for%20both%20manipulating%20and%20representing%20phylogenetic%20trees.%0APhylo2Vec%20maps%20any%20binary%20tree%20with%20%24n%24%20leaves%20to%20a%20unique%20integer%20vector%20of%0Alength%20%24n-1%24.%20The%20advantages%20of%20Phylo2Vec%20are%20fourfold%3A%20i%29%20fast%20tree%20sampling%2C%0A%28ii%29%20compressed%20tree%20representation%20compared%20to%20a%20Newick%20string%2C%20iii%29%20quick%20and%0Aunambiguous%20verification%20if%20two%20binary%20trees%20are%20identical%20topologically%2C%20and%0Aiv%29%20systematic%20ability%20to%20traverse%20tree%20space%20in%20very%20large%20or%20small%20jumps.%20As%0Aa%20proof%20of%20concept%2C%20we%20use%20Phylo2Vec%20for%20maximum%20likelihood%20inference%20on%20five%0Areal-world%20datasets%20and%20show%20that%20a%20simple%20hill-climbing-based%20optimisation%0Ascheme%20can%20efficiently%20traverse%20the%20vastness%20of%20tree%20space%20from%20a%20random%20to%20an%0Aoptimal%20tree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.12693v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhylo2Vec%253A%2520a%2520vector%2520representation%2520for%2520binary%2520trees%26entry.906535625%3DMatthew%2520J%2520Penn%2520and%2520Neil%2520Scheidwasser%2520and%2520Mark%2520P%2520Khurana%2520and%2520David%2520A%2520Duch%25C3%25AAne%2520and%2520Christl%2520A%2520Donnelly%2520and%2520Samir%2520Bhatt%26entry.1292438233%3D%2520%2520Binary%2520phylogenetic%2520trees%2520inferred%2520from%2520biological%2520data%2520are%2520central%2520to%250Aunderstanding%2520the%2520shared%2520history%2520among%2520evolutionary%2520units.%2520However%252C%2520inferring%250Athe%2520placement%2520of%2520latent%2520nodes%2520in%2520a%2520tree%2520is%2520NP-hard%2520and%2520thus%2520computationally%250Aexpensive.%2520State-of-the-art%2520methods%2520rely%2520on%2520carefully%2520designed%2520heuristics%2520for%250Atree%2520search.%2520These%2520methods%2520use%2520different%2520data%2520structures%2520for%2520easy%2520manipulation%250A%2528e.g.%252C%2520classes%2520in%2520object-oriented%2520programming%2520languages%2529%2520and%2520readable%250Arepresentation%2520of%2520trees%2520%2528e.g.%252C%2520Newick-format%2520strings%2529.%2520Here%252C%2520we%2520present%250APhylo2Vec%252C%2520a%2520parsimonious%2520encoding%2520for%2520phylogenetic%2520trees%2520that%2520serves%2520as%2520a%250Aunified%2520approach%2520for%2520both%2520manipulating%2520and%2520representing%2520phylogenetic%2520trees.%250APhylo2Vec%2520maps%2520any%2520binary%2520tree%2520with%2520%2524n%2524%2520leaves%2520to%2520a%2520unique%2520integer%2520vector%2520of%250Alength%2520%2524n-1%2524.%2520The%2520advantages%2520of%2520Phylo2Vec%2520are%2520fourfold%253A%2520i%2529%2520fast%2520tree%2520sampling%252C%250A%2528ii%2529%2520compressed%2520tree%2520representation%2520compared%2520to%2520a%2520Newick%2520string%252C%2520iii%2529%2520quick%2520and%250Aunambiguous%2520verification%2520if%2520two%2520binary%2520trees%2520are%2520identical%2520topologically%252C%2520and%250Aiv%2529%2520systematic%2520ability%2520to%2520traverse%2520tree%2520space%2520in%2520very%2520large%2520or%2520small%2520jumps.%2520As%250Aa%2520proof%2520of%2520concept%252C%2520we%2520use%2520Phylo2Vec%2520for%2520maximum%2520likelihood%2520inference%2520on%2520five%250Areal-world%2520datasets%2520and%2520show%2520that%2520a%2520simple%2520hill-climbing-based%2520optimisation%250Ascheme%2520can%2520efficiently%2520traverse%2520the%2520vastness%2520of%2520tree%2520space%2520from%2520a%2520random%2520to%2520an%250Aoptimal%2520tree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.12693v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phylo2Vec%3A%20a%20vector%20representation%20for%20binary%20trees&entry.906535625=Matthew%20J%20Penn%20and%20Neil%20Scheidwasser%20and%20Mark%20P%20Khurana%20and%20David%20A%20Duch%C3%AAne%20and%20Christl%20A%20Donnelly%20and%20Samir%20Bhatt&entry.1292438233=%20%20Binary%20phylogenetic%20trees%20inferred%20from%20biological%20data%20are%20central%20to%0Aunderstanding%20the%20shared%20history%20among%20evolutionary%20units.%20However%2C%20inferring%0Athe%20placement%20of%20latent%20nodes%20in%20a%20tree%20is%20NP-hard%20and%20thus%20computationally%0Aexpensive.%20State-of-the-art%20methods%20rely%20on%20carefully%20designed%20heuristics%20for%0Atree%20search.%20These%20methods%20use%20different%20data%20structures%20for%20easy%20manipulation%0A%28e.g.%2C%20classes%20in%20object-oriented%20programming%20languages%29%20and%20readable%0Arepresentation%20of%20trees%20%28e.g.%2C%20Newick-format%20strings%29.%20Here%2C%20we%20present%0APhylo2Vec%2C%20a%20parsimonious%20encoding%20for%20phylogenetic%20trees%20that%20serves%20as%20a%0Aunified%20approach%20for%20both%20manipulating%20and%20representing%20phylogenetic%20trees.%0APhylo2Vec%20maps%20any%20binary%20tree%20with%20%24n%24%20leaves%20to%20a%20unique%20integer%20vector%20of%0Alength%20%24n-1%24.%20The%20advantages%20of%20Phylo2Vec%20are%20fourfold%3A%20i%29%20fast%20tree%20sampling%2C%0A%28ii%29%20compressed%20tree%20representation%20compared%20to%20a%20Newick%20string%2C%20iii%29%20quick%20and%0Aunambiguous%20verification%20if%20two%20binary%20trees%20are%20identical%20topologically%2C%20and%0Aiv%29%20systematic%20ability%20to%20traverse%20tree%20space%20in%20very%20large%20or%20small%20jumps.%20As%0Aa%20proof%20of%20concept%2C%20we%20use%20Phylo2Vec%20for%20maximum%20likelihood%20inference%20on%20five%0Areal-world%20datasets%20and%20show%20that%20a%20simple%20hill-climbing-based%20optimisation%0Ascheme%20can%20efficiently%20traverse%20the%20vastness%20of%20tree%20space%20from%20a%20random%20to%20an%0Aoptimal%20tree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.12693v3&entry.124074799=Read"},
{"title": "A Lightweight Transformer for Remote Sensing Image Change Captioning", "author": "Dongwei Sun and Yajie Bao and Xiangyong Cao", "abstract": "  Remote sensing image change captioning (RSICC) aims to automatically generate\nsentences that describe content differences in remote sensing bitemporal\nimages. Recently, attention-based transformers have become a prevalent idea for\ncapturing the features of global change. However, existing transformer-based\nRSICC methods face challenges, e.g., high parameters and high computational\ncomplexity caused by the self-attention operation in the transformer encoder\ncomponent. To alleviate these issues, this paper proposes a Sparse Focus\nTransformer (SFT) for the RSICC task. Specifically, the SFT network consists of\nthree main components, i.e. a high-level features extractor based on a\nconvolutional neural network (CNN), a sparse focus attention mechanism-based\ntransformer encoder network designed to locate and capture changing regions in\ndual-temporal images, and a description decoder that embeds images and words to\ngenerate sentences for captioning differences. The proposed SFT network can\nreduce the parameter number and computational complexity by incorporating a\nsparse attention mechanism within the transformer encoder network. Experimental\nresults on various datasets demonstrate that even with a reduction of over 90\\%\nin parameters and computational complexity for the transformer encoder, our\nproposed network can still obtain competitive performance compared to other\nstate-of-the-art RSICC methods. The code can be available at\n", "link": "http://arxiv.org/abs/2405.06598v1", "date": "2024-05-10", "relevancy": 1.5545, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5593}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5077}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Transformer%20for%20Remote%20Sensing%20Image%20Change%20Captioning&body=Title%3A%20A%20Lightweight%20Transformer%20for%20Remote%20Sensing%20Image%20Change%20Captioning%0AAuthor%3A%20Dongwei%20Sun%20and%20Yajie%20Bao%20and%20Xiangyong%20Cao%0AAbstract%3A%20%20%20Remote%20sensing%20image%20change%20captioning%20%28RSICC%29%20aims%20to%20automatically%20generate%0Asentences%20that%20describe%20content%20differences%20in%20remote%20sensing%20bitemporal%0Aimages.%20Recently%2C%20attention-based%20transformers%20have%20become%20a%20prevalent%20idea%20for%0Acapturing%20the%20features%20of%20global%20change.%20However%2C%20existing%20transformer-based%0ARSICC%20methods%20face%20challenges%2C%20e.g.%2C%20high%20parameters%20and%20high%20computational%0Acomplexity%20caused%20by%20the%20self-attention%20operation%20in%20the%20transformer%20encoder%0Acomponent.%20To%20alleviate%20these%20issues%2C%20this%20paper%20proposes%20a%20Sparse%20Focus%0ATransformer%20%28SFT%29%20for%20the%20RSICC%20task.%20Specifically%2C%20the%20SFT%20network%20consists%20of%0Athree%20main%20components%2C%20i.e.%20a%20high-level%20features%20extractor%20based%20on%20a%0Aconvolutional%20neural%20network%20%28CNN%29%2C%20a%20sparse%20focus%20attention%20mechanism-based%0Atransformer%20encoder%20network%20designed%20to%20locate%20and%20capture%20changing%20regions%20in%0Adual-temporal%20images%2C%20and%20a%20description%20decoder%20that%20embeds%20images%20and%20words%20to%0Agenerate%20sentences%20for%20captioning%20differences.%20The%20proposed%20SFT%20network%20can%0Areduce%20the%20parameter%20number%20and%20computational%20complexity%20by%20incorporating%20a%0Asparse%20attention%20mechanism%20within%20the%20transformer%20encoder%20network.%20Experimental%0Aresults%20on%20various%20datasets%20demonstrate%20that%20even%20with%20a%20reduction%20of%20over%2090%5C%25%0Ain%20parameters%20and%20computational%20complexity%20for%20the%20transformer%20encoder%2C%20our%0Aproposed%20network%20can%20still%20obtain%20competitive%20performance%20compared%20to%20other%0Astate-of-the-art%20RSICC%20methods.%20The%20code%20can%20be%20available%20at%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Transformer%2520for%2520Remote%2520Sensing%2520Image%2520Change%2520Captioning%26entry.906535625%3DDongwei%2520Sun%2520and%2520Yajie%2520Bao%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520change%2520captioning%2520%2528RSICC%2529%2520aims%2520to%2520automatically%2520generate%250Asentences%2520that%2520describe%2520content%2520differences%2520in%2520remote%2520sensing%2520bitemporal%250Aimages.%2520Recently%252C%2520attention-based%2520transformers%2520have%2520become%2520a%2520prevalent%2520idea%2520for%250Acapturing%2520the%2520features%2520of%2520global%2520change.%2520However%252C%2520existing%2520transformer-based%250ARSICC%2520methods%2520face%2520challenges%252C%2520e.g.%252C%2520high%2520parameters%2520and%2520high%2520computational%250Acomplexity%2520caused%2520by%2520the%2520self-attention%2520operation%2520in%2520the%2520transformer%2520encoder%250Acomponent.%2520To%2520alleviate%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520Sparse%2520Focus%250ATransformer%2520%2528SFT%2529%2520for%2520the%2520RSICC%2520task.%2520Specifically%252C%2520the%2520SFT%2520network%2520consists%2520of%250Athree%2520main%2520components%252C%2520i.e.%2520a%2520high-level%2520features%2520extractor%2520based%2520on%2520a%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%252C%2520a%2520sparse%2520focus%2520attention%2520mechanism-based%250Atransformer%2520encoder%2520network%2520designed%2520to%2520locate%2520and%2520capture%2520changing%2520regions%2520in%250Adual-temporal%2520images%252C%2520and%2520a%2520description%2520decoder%2520that%2520embeds%2520images%2520and%2520words%2520to%250Agenerate%2520sentences%2520for%2520captioning%2520differences.%2520The%2520proposed%2520SFT%2520network%2520can%250Areduce%2520the%2520parameter%2520number%2520and%2520computational%2520complexity%2520by%2520incorporating%2520a%250Asparse%2520attention%2520mechanism%2520within%2520the%2520transformer%2520encoder%2520network.%2520Experimental%250Aresults%2520on%2520various%2520datasets%2520demonstrate%2520that%2520even%2520with%2520a%2520reduction%2520of%2520over%252090%255C%2525%250Ain%2520parameters%2520and%2520computational%2520complexity%2520for%2520the%2520transformer%2520encoder%252C%2520our%250Aproposed%2520network%2520can%2520still%2520obtain%2520competitive%2520performance%2520compared%2520to%2520other%250Astate-of-the-art%2520RSICC%2520methods.%2520The%2520code%2520can%2520be%2520available%2520at%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Transformer%20for%20Remote%20Sensing%20Image%20Change%20Captioning&entry.906535625=Dongwei%20Sun%20and%20Yajie%20Bao%20and%20Xiangyong%20Cao&entry.1292438233=%20%20Remote%20sensing%20image%20change%20captioning%20%28RSICC%29%20aims%20to%20automatically%20generate%0Asentences%20that%20describe%20content%20differences%20in%20remote%20sensing%20bitemporal%0Aimages.%20Recently%2C%20attention-based%20transformers%20have%20become%20a%20prevalent%20idea%20for%0Acapturing%20the%20features%20of%20global%20change.%20However%2C%20existing%20transformer-based%0ARSICC%20methods%20face%20challenges%2C%20e.g.%2C%20high%20parameters%20and%20high%20computational%0Acomplexity%20caused%20by%20the%20self-attention%20operation%20in%20the%20transformer%20encoder%0Acomponent.%20To%20alleviate%20these%20issues%2C%20this%20paper%20proposes%20a%20Sparse%20Focus%0ATransformer%20%28SFT%29%20for%20the%20RSICC%20task.%20Specifically%2C%20the%20SFT%20network%20consists%20of%0Athree%20main%20components%2C%20i.e.%20a%20high-level%20features%20extractor%20based%20on%20a%0Aconvolutional%20neural%20network%20%28CNN%29%2C%20a%20sparse%20focus%20attention%20mechanism-based%0Atransformer%20encoder%20network%20designed%20to%20locate%20and%20capture%20changing%20regions%20in%0Adual-temporal%20images%2C%20and%20a%20description%20decoder%20that%20embeds%20images%20and%20words%20to%0Agenerate%20sentences%20for%20captioning%20differences.%20The%20proposed%20SFT%20network%20can%0Areduce%20the%20parameter%20number%20and%20computational%20complexity%20by%20incorporating%20a%0Asparse%20attention%20mechanism%20within%20the%20transformer%20encoder%20network.%20Experimental%0Aresults%20on%20various%20datasets%20demonstrate%20that%20even%20with%20a%20reduction%20of%20over%2090%5C%25%0Ain%20parameters%20and%20computational%20complexity%20for%20the%20transformer%20encoder%2C%20our%0Aproposed%20network%20can%20still%20obtain%20competitive%20performance%20compared%20to%20other%0Astate-of-the-art%20RSICC%20methods.%20The%20code%20can%20be%20available%20at%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06598v1&entry.124074799=Read"},
{"title": "CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs\n  for Reduced hERG Liability", "author": "Gregory W. Kyro and Matthew T. Martin and Eric D. Watt and Victor S. Batista", "abstract": "  The link between in vitro hERG ion channel inhibition and subsequent in vivo\nQT interval prolongation, a critical risk factor for the development of\narrythmias such as Torsade de Pointes, is so well established that in vitro\nhERG activity alone is often sufficient to end the development of an otherwise\npromising drug candidate. It is therefore of tremendous interest to develop\nadvanced methods for identifying hERG-active compounds in the early stages of\ndrug development, as well as for proposing redesigned compounds with reduced\nhERG liability and preserved on-target potency. In this work, we present\nCardioGenAI, a machine learning-based framework for re-engineering both\ndevelopmental and commercially available drugs for reduced hERG activity while\npreserving their pharmacological activity. The framework incorporates novel\nstate-of-the-art discriminative models for predicting hERG channel activity, as\nwell as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to\ntheir potential implications in modulating the arrhythmogenic potential induced\nby hERG channel blockade. We applied the complete framework to pimozide, an\nFDA-approved antipsychotic agent that demonstrates high affinity to the hERG\nchannel, and generated 100 refined candidates. Remarkably, among the candidates\nis fluspirilene, a compound which is of the same class of drugs\n(diphenylmethanes) as pimozide and therefore has similar pharmacological\nactivity, yet exhibits over 700-fold weaker binding to hERG. We envision that\nthis method can effectively be applied to developmental compounds exhibiting\nhERG liabilities to provide a means of rescuing drug development programs that\nhave stalled due to hERG-related safety concerns. Additionally, the\ndiscriminative models can also serve independently as effective components of a\nvirtual screening pipeline. We have made all of our software open-source.\n", "link": "http://arxiv.org/abs/2403.07632v2", "date": "2024-05-10", "relevancy": 1.548, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3933}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3859}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardioGenAI%3A%20A%20Machine%20Learning-Based%20Framework%20for%20Re-Engineering%20Drugs%0A%20%20for%20Reduced%20hERG%20Liability&body=Title%3A%20CardioGenAI%3A%20A%20Machine%20Learning-Based%20Framework%20for%20Re-Engineering%20Drugs%0A%20%20for%20Reduced%20hERG%20Liability%0AAuthor%3A%20Gregory%20W.%20Kyro%20and%20Matthew%20T.%20Martin%20and%20Eric%20D.%20Watt%20and%20Victor%20S.%20Batista%0AAbstract%3A%20%20%20The%20link%20between%20in%20vitro%20hERG%20ion%20channel%20inhibition%20and%20subsequent%20in%20vivo%0AQT%20interval%20prolongation%2C%20a%20critical%20risk%20factor%20for%20the%20development%20of%0Aarrythmias%20such%20as%20Torsade%20de%20Pointes%2C%20is%20so%20well%20established%20that%20in%20vitro%0AhERG%20activity%20alone%20is%20often%20sufficient%20to%20end%20the%20development%20of%20an%20otherwise%0Apromising%20drug%20candidate.%20It%20is%20therefore%20of%20tremendous%20interest%20to%20develop%0Aadvanced%20methods%20for%20identifying%20hERG-active%20compounds%20in%20the%20early%20stages%20of%0Adrug%20development%2C%20as%20well%20as%20for%20proposing%20redesigned%20compounds%20with%20reduced%0AhERG%20liability%20and%20preserved%20on-target%20potency.%20In%20this%20work%2C%20we%20present%0ACardioGenAI%2C%20a%20machine%20learning-based%20framework%20for%20re-engineering%20both%0Adevelopmental%20and%20commercially%20available%20drugs%20for%20reduced%20hERG%20activity%20while%0Apreserving%20their%20pharmacological%20activity.%20The%20framework%20incorporates%20novel%0Astate-of-the-art%20discriminative%20models%20for%20predicting%20hERG%20channel%20activity%2C%20as%0Awell%20as%20activity%20against%20the%20voltage-gated%20NaV1.5%20and%20CaV1.2%20channels%20due%20to%0Atheir%20potential%20implications%20in%20modulating%20the%20arrhythmogenic%20potential%20induced%0Aby%20hERG%20channel%20blockade.%20We%20applied%20the%20complete%20framework%20to%20pimozide%2C%20an%0AFDA-approved%20antipsychotic%20agent%20that%20demonstrates%20high%20affinity%20to%20the%20hERG%0Achannel%2C%20and%20generated%20100%20refined%20candidates.%20Remarkably%2C%20among%20the%20candidates%0Ais%20fluspirilene%2C%20a%20compound%20which%20is%20of%20the%20same%20class%20of%20drugs%0A%28diphenylmethanes%29%20as%20pimozide%20and%20therefore%20has%20similar%20pharmacological%0Aactivity%2C%20yet%20exhibits%20over%20700-fold%20weaker%20binding%20to%20hERG.%20We%20envision%20that%0Athis%20method%20can%20effectively%20be%20applied%20to%20developmental%20compounds%20exhibiting%0AhERG%20liabilities%20to%20provide%20a%20means%20of%20rescuing%20drug%20development%20programs%20that%0Ahave%20stalled%20due%20to%20hERG-related%20safety%20concerns.%20Additionally%2C%20the%0Adiscriminative%20models%20can%20also%20serve%20independently%20as%20effective%20components%20of%20a%0Avirtual%20screening%20pipeline.%20We%20have%20made%20all%20of%20our%20software%20open-source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardioGenAI%253A%2520A%2520Machine%2520Learning-Based%2520Framework%2520for%2520Re-Engineering%2520Drugs%250A%2520%2520for%2520Reduced%2520hERG%2520Liability%26entry.906535625%3DGregory%2520W.%2520Kyro%2520and%2520Matthew%2520T.%2520Martin%2520and%2520Eric%2520D.%2520Watt%2520and%2520Victor%2520S.%2520Batista%26entry.1292438233%3D%2520%2520The%2520link%2520between%2520in%2520vitro%2520hERG%2520ion%2520channel%2520inhibition%2520and%2520subsequent%2520in%2520vivo%250AQT%2520interval%2520prolongation%252C%2520a%2520critical%2520risk%2520factor%2520for%2520the%2520development%2520of%250Aarrythmias%2520such%2520as%2520Torsade%2520de%2520Pointes%252C%2520is%2520so%2520well%2520established%2520that%2520in%2520vitro%250AhERG%2520activity%2520alone%2520is%2520often%2520sufficient%2520to%2520end%2520the%2520development%2520of%2520an%2520otherwise%250Apromising%2520drug%2520candidate.%2520It%2520is%2520therefore%2520of%2520tremendous%2520interest%2520to%2520develop%250Aadvanced%2520methods%2520for%2520identifying%2520hERG-active%2520compounds%2520in%2520the%2520early%2520stages%2520of%250Adrug%2520development%252C%2520as%2520well%2520as%2520for%2520proposing%2520redesigned%2520compounds%2520with%2520reduced%250AhERG%2520liability%2520and%2520preserved%2520on-target%2520potency.%2520In%2520this%2520work%252C%2520we%2520present%250ACardioGenAI%252C%2520a%2520machine%2520learning-based%2520framework%2520for%2520re-engineering%2520both%250Adevelopmental%2520and%2520commercially%2520available%2520drugs%2520for%2520reduced%2520hERG%2520activity%2520while%250Apreserving%2520their%2520pharmacological%2520activity.%2520The%2520framework%2520incorporates%2520novel%250Astate-of-the-art%2520discriminative%2520models%2520for%2520predicting%2520hERG%2520channel%2520activity%252C%2520as%250Awell%2520as%2520activity%2520against%2520the%2520voltage-gated%2520NaV1.5%2520and%2520CaV1.2%2520channels%2520due%2520to%250Atheir%2520potential%2520implications%2520in%2520modulating%2520the%2520arrhythmogenic%2520potential%2520induced%250Aby%2520hERG%2520channel%2520blockade.%2520We%2520applied%2520the%2520complete%2520framework%2520to%2520pimozide%252C%2520an%250AFDA-approved%2520antipsychotic%2520agent%2520that%2520demonstrates%2520high%2520affinity%2520to%2520the%2520hERG%250Achannel%252C%2520and%2520generated%2520100%2520refined%2520candidates.%2520Remarkably%252C%2520among%2520the%2520candidates%250Ais%2520fluspirilene%252C%2520a%2520compound%2520which%2520is%2520of%2520the%2520same%2520class%2520of%2520drugs%250A%2528diphenylmethanes%2529%2520as%2520pimozide%2520and%2520therefore%2520has%2520similar%2520pharmacological%250Aactivity%252C%2520yet%2520exhibits%2520over%2520700-fold%2520weaker%2520binding%2520to%2520hERG.%2520We%2520envision%2520that%250Athis%2520method%2520can%2520effectively%2520be%2520applied%2520to%2520developmental%2520compounds%2520exhibiting%250AhERG%2520liabilities%2520to%2520provide%2520a%2520means%2520of%2520rescuing%2520drug%2520development%2520programs%2520that%250Ahave%2520stalled%2520due%2520to%2520hERG-related%2520safety%2520concerns.%2520Additionally%252C%2520the%250Adiscriminative%2520models%2520can%2520also%2520serve%2520independently%2520as%2520effective%2520components%2520of%2520a%250Avirtual%2520screening%2520pipeline.%2520We%2520have%2520made%2520all%2520of%2520our%2520software%2520open-source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioGenAI%3A%20A%20Machine%20Learning-Based%20Framework%20for%20Re-Engineering%20Drugs%0A%20%20for%20Reduced%20hERG%20Liability&entry.906535625=Gregory%20W.%20Kyro%20and%20Matthew%20T.%20Martin%20and%20Eric%20D.%20Watt%20and%20Victor%20S.%20Batista&entry.1292438233=%20%20The%20link%20between%20in%20vitro%20hERG%20ion%20channel%20inhibition%20and%20subsequent%20in%20vivo%0AQT%20interval%20prolongation%2C%20a%20critical%20risk%20factor%20for%20the%20development%20of%0Aarrythmias%20such%20as%20Torsade%20de%20Pointes%2C%20is%20so%20well%20established%20that%20in%20vitro%0AhERG%20activity%20alone%20is%20often%20sufficient%20to%20end%20the%20development%20of%20an%20otherwise%0Apromising%20drug%20candidate.%20It%20is%20therefore%20of%20tremendous%20interest%20to%20develop%0Aadvanced%20methods%20for%20identifying%20hERG-active%20compounds%20in%20the%20early%20stages%20of%0Adrug%20development%2C%20as%20well%20as%20for%20proposing%20redesigned%20compounds%20with%20reduced%0AhERG%20liability%20and%20preserved%20on-target%20potency.%20In%20this%20work%2C%20we%20present%0ACardioGenAI%2C%20a%20machine%20learning-based%20framework%20for%20re-engineering%20both%0Adevelopmental%20and%20commercially%20available%20drugs%20for%20reduced%20hERG%20activity%20while%0Apreserving%20their%20pharmacological%20activity.%20The%20framework%20incorporates%20novel%0Astate-of-the-art%20discriminative%20models%20for%20predicting%20hERG%20channel%20activity%2C%20as%0Awell%20as%20activity%20against%20the%20voltage-gated%20NaV1.5%20and%20CaV1.2%20channels%20due%20to%0Atheir%20potential%20implications%20in%20modulating%20the%20arrhythmogenic%20potential%20induced%0Aby%20hERG%20channel%20blockade.%20We%20applied%20the%20complete%20framework%20to%20pimozide%2C%20an%0AFDA-approved%20antipsychotic%20agent%20that%20demonstrates%20high%20affinity%20to%20the%20hERG%0Achannel%2C%20and%20generated%20100%20refined%20candidates.%20Remarkably%2C%20among%20the%20candidates%0Ais%20fluspirilene%2C%20a%20compound%20which%20is%20of%20the%20same%20class%20of%20drugs%0A%28diphenylmethanes%29%20as%20pimozide%20and%20therefore%20has%20similar%20pharmacological%0Aactivity%2C%20yet%20exhibits%20over%20700-fold%20weaker%20binding%20to%20hERG.%20We%20envision%20that%0Athis%20method%20can%20effectively%20be%20applied%20to%20developmental%20compounds%20exhibiting%0AhERG%20liabilities%20to%20provide%20a%20means%20of%20rescuing%20drug%20development%20programs%20that%0Ahave%20stalled%20due%20to%20hERG-related%20safety%20concerns.%20Additionally%2C%20the%0Adiscriminative%20models%20can%20also%20serve%20independently%20as%20effective%20components%20of%20a%0Avirtual%20screening%20pipeline.%20We%20have%20made%20all%20of%20our%20software%20open-source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07632v2&entry.124074799=Read"},
{"title": "Neurosymbolic Grounding for Compositional World Models", "author": "Atharva Sehgal and Arya Grayeli and Jennifer J. Sun and Swarat Chaudhuri", "abstract": "  We introduce Cosmos, a framework for object-centric world modeling that is\ndesigned for compositional generalization (CompGen), i.e., high performance on\nunseen input scenes obtained through the composition of known visual \"atoms.\"\nThe central insight behind Cosmos is the use of a novel form of neurosymbolic\ngrounding. Specifically, the framework introduces two new tools: (i)\nneurosymbolic scene encodings, which represent each entity in a scene using a\nreal vector computed using a neural encoder, as well as a vector of composable\nsymbols describing attributes of the entity, and (ii) a neurosymbolic attention\nmechanism that binds these entities to learned rules of interaction. Cosmos is\nend-to-end differentiable; also, unlike traditional neurosymbolic methods that\nrequire representations to be manually mapped to symbols, it computes an\nentity's symbolic attributes using vision-language foundation models. Through\nan evaluation that considers two different forms of CompGen on an established\nblocks-pushing domain, we show that the framework establishes a new\nstate-of-the-art for CompGen in world modeling. Artifacts are available at:\nhttps://trishullab.github.io/cosmos-web/\n", "link": "http://arxiv.org/abs/2310.12690v2", "date": "2024-05-10", "relevancy": 1.5473, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5331}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neurosymbolic%20Grounding%20for%20Compositional%20World%20Models&body=Title%3A%20Neurosymbolic%20Grounding%20for%20Compositional%20World%20Models%0AAuthor%3A%20Atharva%20Sehgal%20and%20Arya%20Grayeli%20and%20Jennifer%20J.%20Sun%20and%20Swarat%20Chaudhuri%0AAbstract%3A%20%20%20We%20introduce%20Cosmos%2C%20a%20framework%20for%20object-centric%20world%20modeling%20that%20is%0Adesigned%20for%20compositional%20generalization%20%28CompGen%29%2C%20i.e.%2C%20high%20performance%20on%0Aunseen%20input%20scenes%20obtained%20through%20the%20composition%20of%20known%20visual%20%22atoms.%22%0AThe%20central%20insight%20behind%20Cosmos%20is%20the%20use%20of%20a%20novel%20form%20of%20neurosymbolic%0Agrounding.%20Specifically%2C%20the%20framework%20introduces%20two%20new%20tools%3A%20%28i%29%0Aneurosymbolic%20scene%20encodings%2C%20which%20represent%20each%20entity%20in%20a%20scene%20using%20a%0Areal%20vector%20computed%20using%20a%20neural%20encoder%2C%20as%20well%20as%20a%20vector%20of%20composable%0Asymbols%20describing%20attributes%20of%20the%20entity%2C%20and%20%28ii%29%20a%20neurosymbolic%20attention%0Amechanism%20that%20binds%20these%20entities%20to%20learned%20rules%20of%20interaction.%20Cosmos%20is%0Aend-to-end%20differentiable%3B%20also%2C%20unlike%20traditional%20neurosymbolic%20methods%20that%0Arequire%20representations%20to%20be%20manually%20mapped%20to%20symbols%2C%20it%20computes%20an%0Aentity%27s%20symbolic%20attributes%20using%20vision-language%20foundation%20models.%20Through%0Aan%20evaluation%20that%20considers%20two%20different%20forms%20of%20CompGen%20on%20an%20established%0Ablocks-pushing%20domain%2C%20we%20show%20that%20the%20framework%20establishes%20a%20new%0Astate-of-the-art%20for%20CompGen%20in%20world%20modeling.%20Artifacts%20are%20available%20at%3A%0Ahttps%3A//trishullab.github.io/cosmos-web/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurosymbolic%2520Grounding%2520for%2520Compositional%2520World%2520Models%26entry.906535625%3DAtharva%2520Sehgal%2520and%2520Arya%2520Grayeli%2520and%2520Jennifer%2520J.%2520Sun%2520and%2520Swarat%2520Chaudhuri%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cosmos%252C%2520a%2520framework%2520for%2520object-centric%2520world%2520modeling%2520that%2520is%250Adesigned%2520for%2520compositional%2520generalization%2520%2528CompGen%2529%252C%2520i.e.%252C%2520high%2520performance%2520on%250Aunseen%2520input%2520scenes%2520obtained%2520through%2520the%2520composition%2520of%2520known%2520visual%2520%2522atoms.%2522%250AThe%2520central%2520insight%2520behind%2520Cosmos%2520is%2520the%2520use%2520of%2520a%2520novel%2520form%2520of%2520neurosymbolic%250Agrounding.%2520Specifically%252C%2520the%2520framework%2520introduces%2520two%2520new%2520tools%253A%2520%2528i%2529%250Aneurosymbolic%2520scene%2520encodings%252C%2520which%2520represent%2520each%2520entity%2520in%2520a%2520scene%2520using%2520a%250Areal%2520vector%2520computed%2520using%2520a%2520neural%2520encoder%252C%2520as%2520well%2520as%2520a%2520vector%2520of%2520composable%250Asymbols%2520describing%2520attributes%2520of%2520the%2520entity%252C%2520and%2520%2528ii%2529%2520a%2520neurosymbolic%2520attention%250Amechanism%2520that%2520binds%2520these%2520entities%2520to%2520learned%2520rules%2520of%2520interaction.%2520Cosmos%2520is%250Aend-to-end%2520differentiable%253B%2520also%252C%2520unlike%2520traditional%2520neurosymbolic%2520methods%2520that%250Arequire%2520representations%2520to%2520be%2520manually%2520mapped%2520to%2520symbols%252C%2520it%2520computes%2520an%250Aentity%2527s%2520symbolic%2520attributes%2520using%2520vision-language%2520foundation%2520models.%2520Through%250Aan%2520evaluation%2520that%2520considers%2520two%2520different%2520forms%2520of%2520CompGen%2520on%2520an%2520established%250Ablocks-pushing%2520domain%252C%2520we%2520show%2520that%2520the%2520framework%2520establishes%2520a%2520new%250Astate-of-the-art%2520for%2520CompGen%2520in%2520world%2520modeling.%2520Artifacts%2520are%2520available%2520at%253A%250Ahttps%253A//trishullab.github.io/cosmos-web/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neurosymbolic%20Grounding%20for%20Compositional%20World%20Models&entry.906535625=Atharva%20Sehgal%20and%20Arya%20Grayeli%20and%20Jennifer%20J.%20Sun%20and%20Swarat%20Chaudhuri&entry.1292438233=%20%20We%20introduce%20Cosmos%2C%20a%20framework%20for%20object-centric%20world%20modeling%20that%20is%0Adesigned%20for%20compositional%20generalization%20%28CompGen%29%2C%20i.e.%2C%20high%20performance%20on%0Aunseen%20input%20scenes%20obtained%20through%20the%20composition%20of%20known%20visual%20%22atoms.%22%0AThe%20central%20insight%20behind%20Cosmos%20is%20the%20use%20of%20a%20novel%20form%20of%20neurosymbolic%0Agrounding.%20Specifically%2C%20the%20framework%20introduces%20two%20new%20tools%3A%20%28i%29%0Aneurosymbolic%20scene%20encodings%2C%20which%20represent%20each%20entity%20in%20a%20scene%20using%20a%0Areal%20vector%20computed%20using%20a%20neural%20encoder%2C%20as%20well%20as%20a%20vector%20of%20composable%0Asymbols%20describing%20attributes%20of%20the%20entity%2C%20and%20%28ii%29%20a%20neurosymbolic%20attention%0Amechanism%20that%20binds%20these%20entities%20to%20learned%20rules%20of%20interaction.%20Cosmos%20is%0Aend-to-end%20differentiable%3B%20also%2C%20unlike%20traditional%20neurosymbolic%20methods%20that%0Arequire%20representations%20to%20be%20manually%20mapped%20to%20symbols%2C%20it%20computes%20an%0Aentity%27s%20symbolic%20attributes%20using%20vision-language%20foundation%20models.%20Through%0Aan%20evaluation%20that%20considers%20two%20different%20forms%20of%20CompGen%20on%20an%20established%0Ablocks-pushing%20domain%2C%20we%20show%20that%20the%20framework%20establishes%20a%20new%0Astate-of-the-art%20for%20CompGen%20in%20world%20modeling.%20Artifacts%20are%20available%20at%3A%0Ahttps%3A//trishullab.github.io/cosmos-web/%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12690v2&entry.124074799=Read"},
{"title": "Good Things Come in Trees: Emotion and Context Aware Behaviour Trees for\n  Ethical Robotic Decision-Making", "author": "Paige Tutt\u00f6s\u00ed and Zhitian Zhang and Emma Hughson and Angelica Lim", "abstract": "  Emotions guide our decision making process and yet have been little explored\nin practical ethical decision making scenarios. In this challenge, we explore\nemotions and how they can influence ethical decision making in a home robot\ncontext: which fetch requests should a robot execute, and why or why not? We\ndiscuss, in particular, two aspects of emotion: (1) somatic markers: objects to\nbe retrieved are tagged as negative (dangerous, e.g. knives or mind-altering,\ne.g. medicine with overdose potential), providing a quick heuristic for where\nto focus attention to avoid the classic Frame Problem of artificial\nintelligence, (2) emotion inference: users' valence and arousal levels are\ntaken into account in defining how and when a robot should respond to a human's\nrequests, e.g. to carefully consider giving dangerous items to users\nexperiencing intense emotions. Our emotion-based approach builds a foundation\nfor the primary consideration of Safety, and is complemented by policies that\nsupport overriding based on Context (e.g. age of user, allergies) and Privacy\n(e.g. administrator settings). Transparency is another key aspect of our\nsolution. Our solution is defined using behaviour trees, towards an\nimplementable design that can provide reasoning information in real-time.\n", "link": "http://arxiv.org/abs/2405.06543v1", "date": "2024-05-10", "relevancy": 1.5376, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.594}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5353}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Good%20Things%20Come%20in%20Trees%3A%20Emotion%20and%20Context%20Aware%20Behaviour%20Trees%20for%0A%20%20Ethical%20Robotic%20Decision-Making&body=Title%3A%20Good%20Things%20Come%20in%20Trees%3A%20Emotion%20and%20Context%20Aware%20Behaviour%20Trees%20for%0A%20%20Ethical%20Robotic%20Decision-Making%0AAuthor%3A%20Paige%20Tutt%C3%B6s%C3%AD%20and%20Zhitian%20Zhang%20and%20Emma%20Hughson%20and%20Angelica%20Lim%0AAbstract%3A%20%20%20Emotions%20guide%20our%20decision%20making%20process%20and%20yet%20have%20been%20little%20explored%0Ain%20practical%20ethical%20decision%20making%20scenarios.%20In%20this%20challenge%2C%20we%20explore%0Aemotions%20and%20how%20they%20can%20influence%20ethical%20decision%20making%20in%20a%20home%20robot%0Acontext%3A%20which%20fetch%20requests%20should%20a%20robot%20execute%2C%20and%20why%20or%20why%20not%3F%20We%0Adiscuss%2C%20in%20particular%2C%20two%20aspects%20of%20emotion%3A%20%281%29%20somatic%20markers%3A%20objects%20to%0Abe%20retrieved%20are%20tagged%20as%20negative%20%28dangerous%2C%20e.g.%20knives%20or%20mind-altering%2C%0Ae.g.%20medicine%20with%20overdose%20potential%29%2C%20providing%20a%20quick%20heuristic%20for%20where%0Ato%20focus%20attention%20to%20avoid%20the%20classic%20Frame%20Problem%20of%20artificial%0Aintelligence%2C%20%282%29%20emotion%20inference%3A%20users%27%20valence%20and%20arousal%20levels%20are%0Ataken%20into%20account%20in%20defining%20how%20and%20when%20a%20robot%20should%20respond%20to%20a%20human%27s%0Arequests%2C%20e.g.%20to%20carefully%20consider%20giving%20dangerous%20items%20to%20users%0Aexperiencing%20intense%20emotions.%20Our%20emotion-based%20approach%20builds%20a%20foundation%0Afor%20the%20primary%20consideration%20of%20Safety%2C%20and%20is%20complemented%20by%20policies%20that%0Asupport%20overriding%20based%20on%20Context%20%28e.g.%20age%20of%20user%2C%20allergies%29%20and%20Privacy%0A%28e.g.%20administrator%20settings%29.%20Transparency%20is%20another%20key%20aspect%20of%20our%0Asolution.%20Our%20solution%20is%20defined%20using%20behaviour%20trees%2C%20towards%20an%0Aimplementable%20design%20that%20can%20provide%20reasoning%20information%20in%20real-time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGood%2520Things%2520Come%2520in%2520Trees%253A%2520Emotion%2520and%2520Context%2520Aware%2520Behaviour%2520Trees%2520for%250A%2520%2520Ethical%2520Robotic%2520Decision-Making%26entry.906535625%3DPaige%2520Tutt%25C3%25B6s%25C3%25AD%2520and%2520Zhitian%2520Zhang%2520and%2520Emma%2520Hughson%2520and%2520Angelica%2520Lim%26entry.1292438233%3D%2520%2520Emotions%2520guide%2520our%2520decision%2520making%2520process%2520and%2520yet%2520have%2520been%2520little%2520explored%250Ain%2520practical%2520ethical%2520decision%2520making%2520scenarios.%2520In%2520this%2520challenge%252C%2520we%2520explore%250Aemotions%2520and%2520how%2520they%2520can%2520influence%2520ethical%2520decision%2520making%2520in%2520a%2520home%2520robot%250Acontext%253A%2520which%2520fetch%2520requests%2520should%2520a%2520robot%2520execute%252C%2520and%2520why%2520or%2520why%2520not%253F%2520We%250Adiscuss%252C%2520in%2520particular%252C%2520two%2520aspects%2520of%2520emotion%253A%2520%25281%2529%2520somatic%2520markers%253A%2520objects%2520to%250Abe%2520retrieved%2520are%2520tagged%2520as%2520negative%2520%2528dangerous%252C%2520e.g.%2520knives%2520or%2520mind-altering%252C%250Ae.g.%2520medicine%2520with%2520overdose%2520potential%2529%252C%2520providing%2520a%2520quick%2520heuristic%2520for%2520where%250Ato%2520focus%2520attention%2520to%2520avoid%2520the%2520classic%2520Frame%2520Problem%2520of%2520artificial%250Aintelligence%252C%2520%25282%2529%2520emotion%2520inference%253A%2520users%2527%2520valence%2520and%2520arousal%2520levels%2520are%250Ataken%2520into%2520account%2520in%2520defining%2520how%2520and%2520when%2520a%2520robot%2520should%2520respond%2520to%2520a%2520human%2527s%250Arequests%252C%2520e.g.%2520to%2520carefully%2520consider%2520giving%2520dangerous%2520items%2520to%2520users%250Aexperiencing%2520intense%2520emotions.%2520Our%2520emotion-based%2520approach%2520builds%2520a%2520foundation%250Afor%2520the%2520primary%2520consideration%2520of%2520Safety%252C%2520and%2520is%2520complemented%2520by%2520policies%2520that%250Asupport%2520overriding%2520based%2520on%2520Context%2520%2528e.g.%2520age%2520of%2520user%252C%2520allergies%2529%2520and%2520Privacy%250A%2528e.g.%2520administrator%2520settings%2529.%2520Transparency%2520is%2520another%2520key%2520aspect%2520of%2520our%250Asolution.%2520Our%2520solution%2520is%2520defined%2520using%2520behaviour%2520trees%252C%2520towards%2520an%250Aimplementable%2520design%2520that%2520can%2520provide%2520reasoning%2520information%2520in%2520real-time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Good%20Things%20Come%20in%20Trees%3A%20Emotion%20and%20Context%20Aware%20Behaviour%20Trees%20for%0A%20%20Ethical%20Robotic%20Decision-Making&entry.906535625=Paige%20Tutt%C3%B6s%C3%AD%20and%20Zhitian%20Zhang%20and%20Emma%20Hughson%20and%20Angelica%20Lim&entry.1292438233=%20%20Emotions%20guide%20our%20decision%20making%20process%20and%20yet%20have%20been%20little%20explored%0Ain%20practical%20ethical%20decision%20making%20scenarios.%20In%20this%20challenge%2C%20we%20explore%0Aemotions%20and%20how%20they%20can%20influence%20ethical%20decision%20making%20in%20a%20home%20robot%0Acontext%3A%20which%20fetch%20requests%20should%20a%20robot%20execute%2C%20and%20why%20or%20why%20not%3F%20We%0Adiscuss%2C%20in%20particular%2C%20two%20aspects%20of%20emotion%3A%20%281%29%20somatic%20markers%3A%20objects%20to%0Abe%20retrieved%20are%20tagged%20as%20negative%20%28dangerous%2C%20e.g.%20knives%20or%20mind-altering%2C%0Ae.g.%20medicine%20with%20overdose%20potential%29%2C%20providing%20a%20quick%20heuristic%20for%20where%0Ato%20focus%20attention%20to%20avoid%20the%20classic%20Frame%20Problem%20of%20artificial%0Aintelligence%2C%20%282%29%20emotion%20inference%3A%20users%27%20valence%20and%20arousal%20levels%20are%0Ataken%20into%20account%20in%20defining%20how%20and%20when%20a%20robot%20should%20respond%20to%20a%20human%27s%0Arequests%2C%20e.g.%20to%20carefully%20consider%20giving%20dangerous%20items%20to%20users%0Aexperiencing%20intense%20emotions.%20Our%20emotion-based%20approach%20builds%20a%20foundation%0Afor%20the%20primary%20consideration%20of%20Safety%2C%20and%20is%20complemented%20by%20policies%20that%0Asupport%20overriding%20based%20on%20Context%20%28e.g.%20age%20of%20user%2C%20allergies%29%20and%20Privacy%0A%28e.g.%20administrator%20settings%29.%20Transparency%20is%20another%20key%20aspect%20of%20our%0Asolution.%20Our%20solution%20is%20defined%20using%20behaviour%20trees%2C%20towards%20an%0Aimplementable%20design%20that%20can%20provide%20reasoning%20information%20in%20real-time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06543v1&entry.124074799=Read"},
{"title": "Scalable Property Valuation Models via Graph-based Deep Learning", "author": "Enrique Riveros and Carla Vairetti and Christian Wegmann and Santiago Truffa and Sebasti\u00e1n Maldonado", "abstract": "  This paper aims to enrich the capabilities of existing deep learning-based\nautomated valuation models through an efficient graph representation of peer\ndependencies, thus capturing intricate spatial relationships. In particular, we\ndevelop two novel graph neural network models that effectively identify\nsequences of neighboring houses with similar features, employing different\nmessage passing algorithms. The first strategy consider standard spatial graph\nconvolutions, while the second one utilizes transformer graph convolutions.\nThis approach confers scalability to the modeling process. The experimental\nevaluation is conducted using a proprietary dataset comprising approximately\n200,000 houses located in Santiago, Chile. We show that employing tailored\ngraph neural networks significantly improves the accuracy of house price\nprediction, especially when utilizing transformer convolutional message passing\nlayers.\n", "link": "http://arxiv.org/abs/2405.06553v1", "date": "2024-05-10", "relevancy": 1.5365, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5175}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Property%20Valuation%20Models%20via%20Graph-based%20Deep%20Learning&body=Title%3A%20Scalable%20Property%20Valuation%20Models%20via%20Graph-based%20Deep%20Learning%0AAuthor%3A%20Enrique%20Riveros%20and%20Carla%20Vairetti%20and%20Christian%20Wegmann%20and%20Santiago%20Truffa%20and%20Sebasti%C3%A1n%20Maldonado%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20enrich%20the%20capabilities%20of%20existing%20deep%20learning-based%0Aautomated%20valuation%20models%20through%20an%20efficient%20graph%20representation%20of%20peer%0Adependencies%2C%20thus%20capturing%20intricate%20spatial%20relationships.%20In%20particular%2C%20we%0Adevelop%20two%20novel%20graph%20neural%20network%20models%20that%20effectively%20identify%0Asequences%20of%20neighboring%20houses%20with%20similar%20features%2C%20employing%20different%0Amessage%20passing%20algorithms.%20The%20first%20strategy%20consider%20standard%20spatial%20graph%0Aconvolutions%2C%20while%20the%20second%20one%20utilizes%20transformer%20graph%20convolutions.%0AThis%20approach%20confers%20scalability%20to%20the%20modeling%20process.%20The%20experimental%0Aevaluation%20is%20conducted%20using%20a%20proprietary%20dataset%20comprising%20approximately%0A200%2C000%20houses%20located%20in%20Santiago%2C%20Chile.%20We%20show%20that%20employing%20tailored%0Agraph%20neural%20networks%20significantly%20improves%20the%20accuracy%20of%20house%20price%0Aprediction%2C%20especially%20when%20utilizing%20transformer%20convolutional%20message%20passing%0Alayers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Property%2520Valuation%2520Models%2520via%2520Graph-based%2520Deep%2520Learning%26entry.906535625%3DEnrique%2520Riveros%2520and%2520Carla%2520Vairetti%2520and%2520Christian%2520Wegmann%2520and%2520Santiago%2520Truffa%2520and%2520Sebasti%25C3%25A1n%2520Maldonado%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520enrich%2520the%2520capabilities%2520of%2520existing%2520deep%2520learning-based%250Aautomated%2520valuation%2520models%2520through%2520an%2520efficient%2520graph%2520representation%2520of%2520peer%250Adependencies%252C%2520thus%2520capturing%2520intricate%2520spatial%2520relationships.%2520In%2520particular%252C%2520we%250Adevelop%2520two%2520novel%2520graph%2520neural%2520network%2520models%2520that%2520effectively%2520identify%250Asequences%2520of%2520neighboring%2520houses%2520with%2520similar%2520features%252C%2520employing%2520different%250Amessage%2520passing%2520algorithms.%2520The%2520first%2520strategy%2520consider%2520standard%2520spatial%2520graph%250Aconvolutions%252C%2520while%2520the%2520second%2520one%2520utilizes%2520transformer%2520graph%2520convolutions.%250AThis%2520approach%2520confers%2520scalability%2520to%2520the%2520modeling%2520process.%2520The%2520experimental%250Aevaluation%2520is%2520conducted%2520using%2520a%2520proprietary%2520dataset%2520comprising%2520approximately%250A200%252C000%2520houses%2520located%2520in%2520Santiago%252C%2520Chile.%2520We%2520show%2520that%2520employing%2520tailored%250Agraph%2520neural%2520networks%2520significantly%2520improves%2520the%2520accuracy%2520of%2520house%2520price%250Aprediction%252C%2520especially%2520when%2520utilizing%2520transformer%2520convolutional%2520message%2520passing%250Alayers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Property%20Valuation%20Models%20via%20Graph-based%20Deep%20Learning&entry.906535625=Enrique%20Riveros%20and%20Carla%20Vairetti%20and%20Christian%20Wegmann%20and%20Santiago%20Truffa%20and%20Sebasti%C3%A1n%20Maldonado&entry.1292438233=%20%20This%20paper%20aims%20to%20enrich%20the%20capabilities%20of%20existing%20deep%20learning-based%0Aautomated%20valuation%20models%20through%20an%20efficient%20graph%20representation%20of%20peer%0Adependencies%2C%20thus%20capturing%20intricate%20spatial%20relationships.%20In%20particular%2C%20we%0Adevelop%20two%20novel%20graph%20neural%20network%20models%20that%20effectively%20identify%0Asequences%20of%20neighboring%20houses%20with%20similar%20features%2C%20employing%20different%0Amessage%20passing%20algorithms.%20The%20first%20strategy%20consider%20standard%20spatial%20graph%0Aconvolutions%2C%20while%20the%20second%20one%20utilizes%20transformer%20graph%20convolutions.%0AThis%20approach%20confers%20scalability%20to%20the%20modeling%20process.%20The%20experimental%0Aevaluation%20is%20conducted%20using%20a%20proprietary%20dataset%20comprising%20approximately%0A200%2C000%20houses%20located%20in%20Santiago%2C%20Chile.%20We%20show%20that%20employing%20tailored%0Agraph%20neural%20networks%20significantly%20improves%20the%20accuracy%20of%20house%20price%0Aprediction%2C%20especially%20when%20utilizing%20transformer%20convolutional%20message%20passing%0Alayers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06553v1&entry.124074799=Read"},
{"title": "DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under\n  Differentially Private Federated Learning using Dynamic Low-Rank Adaptation", "author": "Jie Xu and Karthikeyan Saravanan and Rogier van Dalen and Haaris Mehmood and David Tuckey and Mete Ozay", "abstract": "  Federated learning (FL) allows clients in an Internet of Things (IoT) system\nto collaboratively train a global model without sharing their local data with a\nserver. However, clients' contributions to the server can still leak sensitive\ninformation. Differential privacy (DP) addresses such leakage by providing\nformal privacy guarantees, with mechanisms that add randomness to the clients'\ncontributions. The randomness makes it infeasible to train large\ntransformer-based models, common in modern IoT systems. In this work, we\nempirically evaluate the practicality of fine-tuning large scale on-device\ntransformer-based models with differential privacy in a federated learning\nsystem. We conduct comprehensive experiments on various system properties for\ntasks spanning a multitude of domains: speech recognition, computer vision (CV)\nand natural language understanding (NLU). Our results show that full\nfine-tuning under differentially private federated learning (DP-FL) generally\nleads to huge performance degradation which can be alleviated by reducing the\ndimensionality of contributions through parameter-efficient fine-tuning (PEFT).\nOur benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation\n(DP-LoRA) consistently outperforms other methods. An even more promising\napproach, DyLoRA, which makes the low rank variable, when naively combined with\nFL would straightforwardly break differential privacy. We therefore propose an\nadaptation method that can be combined with differential privacy and call it\nDP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word\nerror rate (WER) increase due to DP to less than 2% and 7% respectively with 1\nmillion clients and a stringent privacy budget of {\\epsilon}=2.\n", "link": "http://arxiv.org/abs/2405.06368v1", "date": "2024-05-10", "relevancy": 1.5126, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5163}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-DyLoRA%3A%20Fine-Tuning%20Transformer-Based%20Models%20On-Device%20under%0A%20%20Differentially%20Private%20Federated%20Learning%20using%20Dynamic%20Low-Rank%20Adaptation&body=Title%3A%20DP-DyLoRA%3A%20Fine-Tuning%20Transformer-Based%20Models%20On-Device%20under%0A%20%20Differentially%20Private%20Federated%20Learning%20using%20Dynamic%20Low-Rank%20Adaptation%0AAuthor%3A%20Jie%20Xu%20and%20Karthikeyan%20Saravanan%20and%20Rogier%20van%20Dalen%20and%20Haaris%20Mehmood%20and%20David%20Tuckey%20and%20Mete%20Ozay%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20allows%20clients%20in%20an%20Internet%20of%20Things%20%28IoT%29%20system%0Ato%20collaboratively%20train%20a%20global%20model%20without%20sharing%20their%20local%20data%20with%20a%0Aserver.%20However%2C%20clients%27%20contributions%20to%20the%20server%20can%20still%20leak%20sensitive%0Ainformation.%20Differential%20privacy%20%28DP%29%20addresses%20such%20leakage%20by%20providing%0Aformal%20privacy%20guarantees%2C%20with%20mechanisms%20that%20add%20randomness%20to%20the%20clients%27%0Acontributions.%20The%20randomness%20makes%20it%20infeasible%20to%20train%20large%0Atransformer-based%20models%2C%20common%20in%20modern%20IoT%20systems.%20In%20this%20work%2C%20we%0Aempirically%20evaluate%20the%20practicality%20of%20fine-tuning%20large%20scale%20on-device%0Atransformer-based%20models%20with%20differential%20privacy%20in%20a%20federated%20learning%0Asystem.%20We%20conduct%20comprehensive%20experiments%20on%20various%20system%20properties%20for%0Atasks%20spanning%20a%20multitude%20of%20domains%3A%20speech%20recognition%2C%20computer%20vision%20%28CV%29%0Aand%20natural%20language%20understanding%20%28NLU%29.%20Our%20results%20show%20that%20full%0Afine-tuning%20under%20differentially%20private%20federated%20learning%20%28DP-FL%29%20generally%0Aleads%20to%20huge%20performance%20degradation%20which%20can%20be%20alleviated%20by%20reducing%20the%0Adimensionality%20of%20contributions%20through%20parameter-efficient%20fine-tuning%20%28PEFT%29.%0AOur%20benchmarks%20of%20existing%20DP-PEFT%20methods%20show%20that%20DP-Low-Rank%20Adaptation%0A%28DP-LoRA%29%20consistently%20outperforms%20other%20methods.%20An%20even%20more%20promising%0Aapproach%2C%20DyLoRA%2C%20which%20makes%20the%20low%20rank%20variable%2C%20when%20naively%20combined%20with%0AFL%20would%20straightforwardly%20break%20differential%20privacy.%20We%20therefore%20propose%20an%0Aadaptation%20method%20that%20can%20be%20combined%20with%20differential%20privacy%20and%20call%20it%0ADP-DyLoRA.%20Finally%2C%20we%20are%20able%20to%20reduce%20the%20accuracy%20degradation%20and%20word%0Aerror%20rate%20%28WER%29%20increase%20due%20to%20DP%20to%20less%20than%202%25%20and%207%25%20respectively%20with%201%0Amillion%20clients%20and%20a%20stringent%20privacy%20budget%20of%20%7B%5Cepsilon%7D%3D2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-DyLoRA%253A%2520Fine-Tuning%2520Transformer-Based%2520Models%2520On-Device%2520under%250A%2520%2520Differentially%2520Private%2520Federated%2520Learning%2520using%2520Dynamic%2520Low-Rank%2520Adaptation%26entry.906535625%3DJie%2520Xu%2520and%2520Karthikeyan%2520Saravanan%2520and%2520Rogier%2520van%2520Dalen%2520and%2520Haaris%2520Mehmood%2520and%2520David%2520Tuckey%2520and%2520Mete%2520Ozay%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520allows%2520clients%2520in%2520an%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520system%250Ato%2520collaboratively%2520train%2520a%2520global%2520model%2520without%2520sharing%2520their%2520local%2520data%2520with%2520a%250Aserver.%2520However%252C%2520clients%2527%2520contributions%2520to%2520the%2520server%2520can%2520still%2520leak%2520sensitive%250Ainformation.%2520Differential%2520privacy%2520%2528DP%2529%2520addresses%2520such%2520leakage%2520by%2520providing%250Aformal%2520privacy%2520guarantees%252C%2520with%2520mechanisms%2520that%2520add%2520randomness%2520to%2520the%2520clients%2527%250Acontributions.%2520The%2520randomness%2520makes%2520it%2520infeasible%2520to%2520train%2520large%250Atransformer-based%2520models%252C%2520common%2520in%2520modern%2520IoT%2520systems.%2520In%2520this%2520work%252C%2520we%250Aempirically%2520evaluate%2520the%2520practicality%2520of%2520fine-tuning%2520large%2520scale%2520on-device%250Atransformer-based%2520models%2520with%2520differential%2520privacy%2520in%2520a%2520federated%2520learning%250Asystem.%2520We%2520conduct%2520comprehensive%2520experiments%2520on%2520various%2520system%2520properties%2520for%250Atasks%2520spanning%2520a%2520multitude%2520of%2520domains%253A%2520speech%2520recognition%252C%2520computer%2520vision%2520%2528CV%2529%250Aand%2520natural%2520language%2520understanding%2520%2528NLU%2529.%2520Our%2520results%2520show%2520that%2520full%250Afine-tuning%2520under%2520differentially%2520private%2520federated%2520learning%2520%2528DP-FL%2529%2520generally%250Aleads%2520to%2520huge%2520performance%2520degradation%2520which%2520can%2520be%2520alleviated%2520by%2520reducing%2520the%250Adimensionality%2520of%2520contributions%2520through%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529.%250AOur%2520benchmarks%2520of%2520existing%2520DP-PEFT%2520methods%2520show%2520that%2520DP-Low-Rank%2520Adaptation%250A%2528DP-LoRA%2529%2520consistently%2520outperforms%2520other%2520methods.%2520An%2520even%2520more%2520promising%250Aapproach%252C%2520DyLoRA%252C%2520which%2520makes%2520the%2520low%2520rank%2520variable%252C%2520when%2520naively%2520combined%2520with%250AFL%2520would%2520straightforwardly%2520break%2520differential%2520privacy.%2520We%2520therefore%2520propose%2520an%250Aadaptation%2520method%2520that%2520can%2520be%2520combined%2520with%2520differential%2520privacy%2520and%2520call%2520it%250ADP-DyLoRA.%2520Finally%252C%2520we%2520are%2520able%2520to%2520reduce%2520the%2520accuracy%2520degradation%2520and%2520word%250Aerror%2520rate%2520%2528WER%2529%2520increase%2520due%2520to%2520DP%2520to%2520less%2520than%25202%2525%2520and%25207%2525%2520respectively%2520with%25201%250Amillion%2520clients%2520and%2520a%2520stringent%2520privacy%2520budget%2520of%2520%257B%255Cepsilon%257D%253D2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-DyLoRA%3A%20Fine-Tuning%20Transformer-Based%20Models%20On-Device%20under%0A%20%20Differentially%20Private%20Federated%20Learning%20using%20Dynamic%20Low-Rank%20Adaptation&entry.906535625=Jie%20Xu%20and%20Karthikeyan%20Saravanan%20and%20Rogier%20van%20Dalen%20and%20Haaris%20Mehmood%20and%20David%20Tuckey%20and%20Mete%20Ozay&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20allows%20clients%20in%20an%20Internet%20of%20Things%20%28IoT%29%20system%0Ato%20collaboratively%20train%20a%20global%20model%20without%20sharing%20their%20local%20data%20with%20a%0Aserver.%20However%2C%20clients%27%20contributions%20to%20the%20server%20can%20still%20leak%20sensitive%0Ainformation.%20Differential%20privacy%20%28DP%29%20addresses%20such%20leakage%20by%20providing%0Aformal%20privacy%20guarantees%2C%20with%20mechanisms%20that%20add%20randomness%20to%20the%20clients%27%0Acontributions.%20The%20randomness%20makes%20it%20infeasible%20to%20train%20large%0Atransformer-based%20models%2C%20common%20in%20modern%20IoT%20systems.%20In%20this%20work%2C%20we%0Aempirically%20evaluate%20the%20practicality%20of%20fine-tuning%20large%20scale%20on-device%0Atransformer-based%20models%20with%20differential%20privacy%20in%20a%20federated%20learning%0Asystem.%20We%20conduct%20comprehensive%20experiments%20on%20various%20system%20properties%20for%0Atasks%20spanning%20a%20multitude%20of%20domains%3A%20speech%20recognition%2C%20computer%20vision%20%28CV%29%0Aand%20natural%20language%20understanding%20%28NLU%29.%20Our%20results%20show%20that%20full%0Afine-tuning%20under%20differentially%20private%20federated%20learning%20%28DP-FL%29%20generally%0Aleads%20to%20huge%20performance%20degradation%20which%20can%20be%20alleviated%20by%20reducing%20the%0Adimensionality%20of%20contributions%20through%20parameter-efficient%20fine-tuning%20%28PEFT%29.%0AOur%20benchmarks%20of%20existing%20DP-PEFT%20methods%20show%20that%20DP-Low-Rank%20Adaptation%0A%28DP-LoRA%29%20consistently%20outperforms%20other%20methods.%20An%20even%20more%20promising%0Aapproach%2C%20DyLoRA%2C%20which%20makes%20the%20low%20rank%20variable%2C%20when%20naively%20combined%20with%0AFL%20would%20straightforwardly%20break%20differential%20privacy.%20We%20therefore%20propose%20an%0Aadaptation%20method%20that%20can%20be%20combined%20with%20differential%20privacy%20and%20call%20it%0ADP-DyLoRA.%20Finally%2C%20we%20are%20able%20to%20reduce%20the%20accuracy%20degradation%20and%20word%0Aerror%20rate%20%28WER%29%20increase%20due%20to%20DP%20to%20less%20than%202%25%20and%207%25%20respectively%20with%201%0Amillion%20clients%20and%20a%20stringent%20privacy%20budget%20of%20%7B%5Cepsilon%7D%3D2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06368v1&entry.124074799=Read"},
{"title": "Information-Theoretic Safe Bayesian Optimization", "author": "Alessandro G. Bottero and Carlos E. Luis and Julia Vinogradska and Felix Berkenkamp and Jan Peters", "abstract": "  We consider a sequential decision making task, where the goal is to optimize\nan unknown function without evaluating parameters that violate an a~priori\nunknown (safety) constraint. A common approach is to place a Gaussian process\nprior on the unknown functions and allow evaluations only in regions that are\nsafe with high probability. Most current methods rely on a discretization of\nthe domain and cannot be directly extended to the continuous case. Moreover,\nthe way in which they exploit regularity assumptions about the constraint\nintroduces an additional critical hyperparameter. In this paper, we propose an\ninformation-theoretic safe exploration criterion that directly exploits the GP\nposterior to identify the most informative safe parameters to evaluate. The\ncombination of this exploration criterion with a well known Bayesian\noptimization acquisition function yields a novel safe Bayesian optimization\nselection criterion. Our approach is naturally applicable to continuous domains\nand does not require additional explicit hyperparameters. We theoretically\nanalyze the method and show that we do not violate the safety constraint with\nhigh probability and that we learn about the value of the safe optimum up to\narbitrary precision. Empirical evaluations demonstrate improved data-efficiency\nand scalability.\n", "link": "http://arxiv.org/abs/2402.15347v2", "date": "2024-05-10", "relevancy": 1.5033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4929}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Theoretic%20Safe%20Bayesian%20Optimization&body=Title%3A%20Information-Theoretic%20Safe%20Bayesian%20Optimization%0AAuthor%3A%20Alessandro%20G.%20Bottero%20and%20Carlos%20E.%20Luis%20and%20Julia%20Vinogradska%20and%20Felix%20Berkenkamp%20and%20Jan%20Peters%0AAbstract%3A%20%20%20We%20consider%20a%20sequential%20decision%20making%20task%2C%20where%20the%20goal%20is%20to%20optimize%0Aan%20unknown%20function%20without%20evaluating%20parameters%20that%20violate%20an%20a~priori%0Aunknown%20%28safety%29%20constraint.%20A%20common%20approach%20is%20to%20place%20a%20Gaussian%20process%0Aprior%20on%20the%20unknown%20functions%20and%20allow%20evaluations%20only%20in%20regions%20that%20are%0Asafe%20with%20high%20probability.%20Most%20current%20methods%20rely%20on%20a%20discretization%20of%0Athe%20domain%20and%20cannot%20be%20directly%20extended%20to%20the%20continuous%20case.%20Moreover%2C%0Athe%20way%20in%20which%20they%20exploit%20regularity%20assumptions%20about%20the%20constraint%0Aintroduces%20an%20additional%20critical%20hyperparameter.%20In%20this%20paper%2C%20we%20propose%20an%0Ainformation-theoretic%20safe%20exploration%20criterion%20that%20directly%20exploits%20the%20GP%0Aposterior%20to%20identify%20the%20most%20informative%20safe%20parameters%20to%20evaluate.%20The%0Acombination%20of%20this%20exploration%20criterion%20with%20a%20well%20known%20Bayesian%0Aoptimization%20acquisition%20function%20yields%20a%20novel%20safe%20Bayesian%20optimization%0Aselection%20criterion.%20Our%20approach%20is%20naturally%20applicable%20to%20continuous%20domains%0Aand%20does%20not%20require%20additional%20explicit%20hyperparameters.%20We%20theoretically%0Aanalyze%20the%20method%20and%20show%20that%20we%20do%20not%20violate%20the%20safety%20constraint%20with%0Ahigh%20probability%20and%20that%20we%20learn%20about%20the%20value%20of%20the%20safe%20optimum%20up%20to%0Aarbitrary%20precision.%20Empirical%20evaluations%20demonstrate%20improved%20data-efficiency%0Aand%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Theoretic%2520Safe%2520Bayesian%2520Optimization%26entry.906535625%3DAlessandro%2520G.%2520Bottero%2520and%2520Carlos%2520E.%2520Luis%2520and%2520Julia%2520Vinogradska%2520and%2520Felix%2520Berkenkamp%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520sequential%2520decision%2520making%2520task%252C%2520where%2520the%2520goal%2520is%2520to%2520optimize%250Aan%2520unknown%2520function%2520without%2520evaluating%2520parameters%2520that%2520violate%2520an%2520a~priori%250Aunknown%2520%2528safety%2529%2520constraint.%2520A%2520common%2520approach%2520is%2520to%2520place%2520a%2520Gaussian%2520process%250Aprior%2520on%2520the%2520unknown%2520functions%2520and%2520allow%2520evaluations%2520only%2520in%2520regions%2520that%2520are%250Asafe%2520with%2520high%2520probability.%2520Most%2520current%2520methods%2520rely%2520on%2520a%2520discretization%2520of%250Athe%2520domain%2520and%2520cannot%2520be%2520directly%2520extended%2520to%2520the%2520continuous%2520case.%2520Moreover%252C%250Athe%2520way%2520in%2520which%2520they%2520exploit%2520regularity%2520assumptions%2520about%2520the%2520constraint%250Aintroduces%2520an%2520additional%2520critical%2520hyperparameter.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Ainformation-theoretic%2520safe%2520exploration%2520criterion%2520that%2520directly%2520exploits%2520the%2520GP%250Aposterior%2520to%2520identify%2520the%2520most%2520informative%2520safe%2520parameters%2520to%2520evaluate.%2520The%250Acombination%2520of%2520this%2520exploration%2520criterion%2520with%2520a%2520well%2520known%2520Bayesian%250Aoptimization%2520acquisition%2520function%2520yields%2520a%2520novel%2520safe%2520Bayesian%2520optimization%250Aselection%2520criterion.%2520Our%2520approach%2520is%2520naturally%2520applicable%2520to%2520continuous%2520domains%250Aand%2520does%2520not%2520require%2520additional%2520explicit%2520hyperparameters.%2520We%2520theoretically%250Aanalyze%2520the%2520method%2520and%2520show%2520that%2520we%2520do%2520not%2520violate%2520the%2520safety%2520constraint%2520with%250Ahigh%2520probability%2520and%2520that%2520we%2520learn%2520about%2520the%2520value%2520of%2520the%2520safe%2520optimum%2520up%2520to%250Aarbitrary%2520precision.%2520Empirical%2520evaluations%2520demonstrate%2520improved%2520data-efficiency%250Aand%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Theoretic%20Safe%20Bayesian%20Optimization&entry.906535625=Alessandro%20G.%20Bottero%20and%20Carlos%20E.%20Luis%20and%20Julia%20Vinogradska%20and%20Felix%20Berkenkamp%20and%20Jan%20Peters&entry.1292438233=%20%20We%20consider%20a%20sequential%20decision%20making%20task%2C%20where%20the%20goal%20is%20to%20optimize%0Aan%20unknown%20function%20without%20evaluating%20parameters%20that%20violate%20an%20a~priori%0Aunknown%20%28safety%29%20constraint.%20A%20common%20approach%20is%20to%20place%20a%20Gaussian%20process%0Aprior%20on%20the%20unknown%20functions%20and%20allow%20evaluations%20only%20in%20regions%20that%20are%0Asafe%20with%20high%20probability.%20Most%20current%20methods%20rely%20on%20a%20discretization%20of%0Athe%20domain%20and%20cannot%20be%20directly%20extended%20to%20the%20continuous%20case.%20Moreover%2C%0Athe%20way%20in%20which%20they%20exploit%20regularity%20assumptions%20about%20the%20constraint%0Aintroduces%20an%20additional%20critical%20hyperparameter.%20In%20this%20paper%2C%20we%20propose%20an%0Ainformation-theoretic%20safe%20exploration%20criterion%20that%20directly%20exploits%20the%20GP%0Aposterior%20to%20identify%20the%20most%20informative%20safe%20parameters%20to%20evaluate.%20The%0Acombination%20of%20this%20exploration%20criterion%20with%20a%20well%20known%20Bayesian%0Aoptimization%20acquisition%20function%20yields%20a%20novel%20safe%20Bayesian%20optimization%0Aselection%20criterion.%20Our%20approach%20is%20naturally%20applicable%20to%20continuous%20domains%0Aand%20does%20not%20require%20additional%20explicit%20hyperparameters.%20We%20theoretically%0Aanalyze%20the%20method%20and%20show%20that%20we%20do%20not%20violate%20the%20safety%20constraint%20with%0Ahigh%20probability%20and%20that%20we%20learn%20about%20the%20value%20of%20the%20safe%20optimum%20up%20to%0Aarbitrary%20precision.%20Empirical%20evaluations%20demonstrate%20improved%20data-efficiency%0Aand%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15347v2&entry.124074799=Read"},
{"title": "Koopman-Based Surrogate Modelling of Turbulent Rayleigh-B\u00e9nard\n  Convection", "author": "Thorben Markmann and Michiel Straat and Barbara Hammer", "abstract": "  Several related works have introduced Koopman-based Machine Learning\narchitectures as a surrogate model for dynamical systems. These architectures\naim to learn non-linear measurements (also known as observables) of the\nsystem's state that evolve by a linear operator and are, therefore, amenable to\nmodel-based linear control techniques. So far, mainly simple systems have been\ntargeted, and Koopman architectures as reduced-order models for more complex\ndynamics have not been fully explored. Hence, we use a Koopman-inspired\narchitecture called the Linear Recurrent Autoencoder Network (LRAN) for\nlearning reduced-order dynamics in convection flows of a Rayleigh B\\'enard\nConvection (RBC) system at different amounts of turbulence. The data is\nobtained from direct numerical simulations of the RBC system. A traditional\nfluid dynamics method, the Kernel Dynamic Mode Decomposition (KDMD), is used to\ncompare the LRAN. For both methods, we performed hyperparameter sweeps to\nidentify optimal settings. We used a Normalized Sum of Square Error measure for\nthe quantitative evaluation of the models, and we also studied the model\npredictions qualitatively. We obtained more accurate predictions with the LRAN\nthan with KDMD in the most turbulent setting. We conjecture that this is due to\nthe LRAN's flexibility in learning complicated observables from data, thereby\nserving as a viable surrogate model for the main structure of fluid dynamics in\nturbulent convection settings. In contrast, KDMD was more effective in lower\nturbulence settings due to the repetitiveness of the convection flow. The\nfeasibility of Koopman-based surrogate models for turbulent fluid flows opens\npossibilities for efficient model-based control techniques useful in a variety\nof industrial settings.\n", "link": "http://arxiv.org/abs/2405.06425v1", "date": "2024-05-10", "relevancy": 1.4995, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5324}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5118}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Koopman-Based%20Surrogate%20Modelling%20of%20Turbulent%20Rayleigh-B%C3%A9nard%0A%20%20Convection&body=Title%3A%20Koopman-Based%20Surrogate%20Modelling%20of%20Turbulent%20Rayleigh-B%C3%A9nard%0A%20%20Convection%0AAuthor%3A%20Thorben%20Markmann%20and%20Michiel%20Straat%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Several%20related%20works%20have%20introduced%20Koopman-based%20Machine%20Learning%0Aarchitectures%20as%20a%20surrogate%20model%20for%20dynamical%20systems.%20These%20architectures%0Aaim%20to%20learn%20non-linear%20measurements%20%28also%20known%20as%20observables%29%20of%20the%0Asystem%27s%20state%20that%20evolve%20by%20a%20linear%20operator%20and%20are%2C%20therefore%2C%20amenable%20to%0Amodel-based%20linear%20control%20techniques.%20So%20far%2C%20mainly%20simple%20systems%20have%20been%0Atargeted%2C%20and%20Koopman%20architectures%20as%20reduced-order%20models%20for%20more%20complex%0Adynamics%20have%20not%20been%20fully%20explored.%20Hence%2C%20we%20use%20a%20Koopman-inspired%0Aarchitecture%20called%20the%20Linear%20Recurrent%20Autoencoder%20Network%20%28LRAN%29%20for%0Alearning%20reduced-order%20dynamics%20in%20convection%20flows%20of%20a%20Rayleigh%20B%5C%27enard%0AConvection%20%28RBC%29%20system%20at%20different%20amounts%20of%20turbulence.%20The%20data%20is%0Aobtained%20from%20direct%20numerical%20simulations%20of%20the%20RBC%20system.%20A%20traditional%0Afluid%20dynamics%20method%2C%20the%20Kernel%20Dynamic%20Mode%20Decomposition%20%28KDMD%29%2C%20is%20used%20to%0Acompare%20the%20LRAN.%20For%20both%20methods%2C%20we%20performed%20hyperparameter%20sweeps%20to%0Aidentify%20optimal%20settings.%20We%20used%20a%20Normalized%20Sum%20of%20Square%20Error%20measure%20for%0Athe%20quantitative%20evaluation%20of%20the%20models%2C%20and%20we%20also%20studied%20the%20model%0Apredictions%20qualitatively.%20We%20obtained%20more%20accurate%20predictions%20with%20the%20LRAN%0Athan%20with%20KDMD%20in%20the%20most%20turbulent%20setting.%20We%20conjecture%20that%20this%20is%20due%20to%0Athe%20LRAN%27s%20flexibility%20in%20learning%20complicated%20observables%20from%20data%2C%20thereby%0Aserving%20as%20a%20viable%20surrogate%20model%20for%20the%20main%20structure%20of%20fluid%20dynamics%20in%0Aturbulent%20convection%20settings.%20In%20contrast%2C%20KDMD%20was%20more%20effective%20in%20lower%0Aturbulence%20settings%20due%20to%20the%20repetitiveness%20of%20the%20convection%20flow.%20The%0Afeasibility%20of%20Koopman-based%20surrogate%20models%20for%20turbulent%20fluid%20flows%20opens%0Apossibilities%20for%20efficient%20model-based%20control%20techniques%20useful%20in%20a%20variety%0Aof%20industrial%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKoopman-Based%2520Surrogate%2520Modelling%2520of%2520Turbulent%2520Rayleigh-B%25C3%25A9nard%250A%2520%2520Convection%26entry.906535625%3DThorben%2520Markmann%2520and%2520Michiel%2520Straat%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Several%2520related%2520works%2520have%2520introduced%2520Koopman-based%2520Machine%2520Learning%250Aarchitectures%2520as%2520a%2520surrogate%2520model%2520for%2520dynamical%2520systems.%2520These%2520architectures%250Aaim%2520to%2520learn%2520non-linear%2520measurements%2520%2528also%2520known%2520as%2520observables%2529%2520of%2520the%250Asystem%2527s%2520state%2520that%2520evolve%2520by%2520a%2520linear%2520operator%2520and%2520are%252C%2520therefore%252C%2520amenable%2520to%250Amodel-based%2520linear%2520control%2520techniques.%2520So%2520far%252C%2520mainly%2520simple%2520systems%2520have%2520been%250Atargeted%252C%2520and%2520Koopman%2520architectures%2520as%2520reduced-order%2520models%2520for%2520more%2520complex%250Adynamics%2520have%2520not%2520been%2520fully%2520explored.%2520Hence%252C%2520we%2520use%2520a%2520Koopman-inspired%250Aarchitecture%2520called%2520the%2520Linear%2520Recurrent%2520Autoencoder%2520Network%2520%2528LRAN%2529%2520for%250Alearning%2520reduced-order%2520dynamics%2520in%2520convection%2520flows%2520of%2520a%2520Rayleigh%2520B%255C%2527enard%250AConvection%2520%2528RBC%2529%2520system%2520at%2520different%2520amounts%2520of%2520turbulence.%2520The%2520data%2520is%250Aobtained%2520from%2520direct%2520numerical%2520simulations%2520of%2520the%2520RBC%2520system.%2520A%2520traditional%250Afluid%2520dynamics%2520method%252C%2520the%2520Kernel%2520Dynamic%2520Mode%2520Decomposition%2520%2528KDMD%2529%252C%2520is%2520used%2520to%250Acompare%2520the%2520LRAN.%2520For%2520both%2520methods%252C%2520we%2520performed%2520hyperparameter%2520sweeps%2520to%250Aidentify%2520optimal%2520settings.%2520We%2520used%2520a%2520Normalized%2520Sum%2520of%2520Square%2520Error%2520measure%2520for%250Athe%2520quantitative%2520evaluation%2520of%2520the%2520models%252C%2520and%2520we%2520also%2520studied%2520the%2520model%250Apredictions%2520qualitatively.%2520We%2520obtained%2520more%2520accurate%2520predictions%2520with%2520the%2520LRAN%250Athan%2520with%2520KDMD%2520in%2520the%2520most%2520turbulent%2520setting.%2520We%2520conjecture%2520that%2520this%2520is%2520due%2520to%250Athe%2520LRAN%2527s%2520flexibility%2520in%2520learning%2520complicated%2520observables%2520from%2520data%252C%2520thereby%250Aserving%2520as%2520a%2520viable%2520surrogate%2520model%2520for%2520the%2520main%2520structure%2520of%2520fluid%2520dynamics%2520in%250Aturbulent%2520convection%2520settings.%2520In%2520contrast%252C%2520KDMD%2520was%2520more%2520effective%2520in%2520lower%250Aturbulence%2520settings%2520due%2520to%2520the%2520repetitiveness%2520of%2520the%2520convection%2520flow.%2520The%250Afeasibility%2520of%2520Koopman-based%2520surrogate%2520models%2520for%2520turbulent%2520fluid%2520flows%2520opens%250Apossibilities%2520for%2520efficient%2520model-based%2520control%2520techniques%2520useful%2520in%2520a%2520variety%250Aof%2520industrial%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Koopman-Based%20Surrogate%20Modelling%20of%20Turbulent%20Rayleigh-B%C3%A9nard%0A%20%20Convection&entry.906535625=Thorben%20Markmann%20and%20Michiel%20Straat%20and%20Barbara%20Hammer&entry.1292438233=%20%20Several%20related%20works%20have%20introduced%20Koopman-based%20Machine%20Learning%0Aarchitectures%20as%20a%20surrogate%20model%20for%20dynamical%20systems.%20These%20architectures%0Aaim%20to%20learn%20non-linear%20measurements%20%28also%20known%20as%20observables%29%20of%20the%0Asystem%27s%20state%20that%20evolve%20by%20a%20linear%20operator%20and%20are%2C%20therefore%2C%20amenable%20to%0Amodel-based%20linear%20control%20techniques.%20So%20far%2C%20mainly%20simple%20systems%20have%20been%0Atargeted%2C%20and%20Koopman%20architectures%20as%20reduced-order%20models%20for%20more%20complex%0Adynamics%20have%20not%20been%20fully%20explored.%20Hence%2C%20we%20use%20a%20Koopman-inspired%0Aarchitecture%20called%20the%20Linear%20Recurrent%20Autoencoder%20Network%20%28LRAN%29%20for%0Alearning%20reduced-order%20dynamics%20in%20convection%20flows%20of%20a%20Rayleigh%20B%5C%27enard%0AConvection%20%28RBC%29%20system%20at%20different%20amounts%20of%20turbulence.%20The%20data%20is%0Aobtained%20from%20direct%20numerical%20simulations%20of%20the%20RBC%20system.%20A%20traditional%0Afluid%20dynamics%20method%2C%20the%20Kernel%20Dynamic%20Mode%20Decomposition%20%28KDMD%29%2C%20is%20used%20to%0Acompare%20the%20LRAN.%20For%20both%20methods%2C%20we%20performed%20hyperparameter%20sweeps%20to%0Aidentify%20optimal%20settings.%20We%20used%20a%20Normalized%20Sum%20of%20Square%20Error%20measure%20for%0Athe%20quantitative%20evaluation%20of%20the%20models%2C%20and%20we%20also%20studied%20the%20model%0Apredictions%20qualitatively.%20We%20obtained%20more%20accurate%20predictions%20with%20the%20LRAN%0Athan%20with%20KDMD%20in%20the%20most%20turbulent%20setting.%20We%20conjecture%20that%20this%20is%20due%20to%0Athe%20LRAN%27s%20flexibility%20in%20learning%20complicated%20observables%20from%20data%2C%20thereby%0Aserving%20as%20a%20viable%20surrogate%20model%20for%20the%20main%20structure%20of%20fluid%20dynamics%20in%0Aturbulent%20convection%20settings.%20In%20contrast%2C%20KDMD%20was%20more%20effective%20in%20lower%0Aturbulence%20settings%20due%20to%20the%20repetitiveness%20of%20the%20convection%20flow.%20The%0Afeasibility%20of%20Koopman-based%20surrogate%20models%20for%20turbulent%20fluid%20flows%20opens%0Apossibilities%20for%20efficient%20model-based%20control%20techniques%20useful%20in%20a%20variety%0Aof%20industrial%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06425v1&entry.124074799=Read"},
{"title": "Decomposing weather forecasting into advection and convection with\n  neural networks", "author": "Mengxuan Chen and Ziqi Yuan and Jinxiao Zhang and Runmin Dong and Haohuan Fu", "abstract": "  Operational weather forecasting models have advanced for decades on both the\nexplicit numerical solvers and the empirical physical parameterization schemes.\nHowever, the involved high computational costs and uncertainties in these\nexisting schemes are requiring potential improvements through alternative\nmachine learning methods. Previous works use a unified model to learn the\ndynamics and physics of the atmospheric model. Contrarily, we propose a simple\nyet effective machine learning model that learns the horizontal movement in the\ndynamical core and vertical movement in the physical parameterization\nseparately. By replacing the advection with a graph attention network and the\nconvection with a multi-layer perceptron, our model provides a new and\nefficient perspective to simulate the transition of variables in atmospheric\nmodels. We also assess the model's performance over a 5-day iterative\nforecasting. Under the same input variables and training methods, our model\noutperforms existing data-driven methods with a significantly-reduced number of\nparameters with a resolution of 5.625 deg. Overall, this work aims to\ncontribute to the ongoing efforts that leverage machine learning techniques for\nimproving both the accuracy and efficiency of global weather forecasting.\n", "link": "http://arxiv.org/abs/2405.06590v1", "date": "2024-05-10", "relevancy": 1.4931, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4994}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20weather%20forecasting%20into%20advection%20and%20convection%20with%0A%20%20neural%20networks&body=Title%3A%20Decomposing%20weather%20forecasting%20into%20advection%20and%20convection%20with%0A%20%20neural%20networks%0AAuthor%3A%20Mengxuan%20Chen%20and%20Ziqi%20Yuan%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu%0AAbstract%3A%20%20%20Operational%20weather%20forecasting%20models%20have%20advanced%20for%20decades%20on%20both%20the%0Aexplicit%20numerical%20solvers%20and%20the%20empirical%20physical%20parameterization%20schemes.%0AHowever%2C%20the%20involved%20high%20computational%20costs%20and%20uncertainties%20in%20these%0Aexisting%20schemes%20are%20requiring%20potential%20improvements%20through%20alternative%0Amachine%20learning%20methods.%20Previous%20works%20use%20a%20unified%20model%20to%20learn%20the%0Adynamics%20and%20physics%20of%20the%20atmospheric%20model.%20Contrarily%2C%20we%20propose%20a%20simple%0Ayet%20effective%20machine%20learning%20model%20that%20learns%20the%20horizontal%20movement%20in%20the%0Adynamical%20core%20and%20vertical%20movement%20in%20the%20physical%20parameterization%0Aseparately.%20By%20replacing%20the%20advection%20with%20a%20graph%20attention%20network%20and%20the%0Aconvection%20with%20a%20multi-layer%20perceptron%2C%20our%20model%20provides%20a%20new%20and%0Aefficient%20perspective%20to%20simulate%20the%20transition%20of%20variables%20in%20atmospheric%0Amodels.%20We%20also%20assess%20the%20model%27s%20performance%20over%20a%205-day%20iterative%0Aforecasting.%20Under%20the%20same%20input%20variables%20and%20training%20methods%2C%20our%20model%0Aoutperforms%20existing%20data-driven%20methods%20with%20a%20significantly-reduced%20number%20of%0Aparameters%20with%20a%20resolution%20of%205.625%20deg.%20Overall%2C%20this%20work%20aims%20to%0Acontribute%20to%20the%20ongoing%20efforts%20that%20leverage%20machine%20learning%20techniques%20for%0Aimproving%20both%20the%20accuracy%20and%20efficiency%20of%20global%20weather%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520weather%2520forecasting%2520into%2520advection%2520and%2520convection%2520with%250A%2520%2520neural%2520networks%26entry.906535625%3DMengxuan%2520Chen%2520and%2520Ziqi%2520Yuan%2520and%2520Jinxiao%2520Zhang%2520and%2520Runmin%2520Dong%2520and%2520Haohuan%2520Fu%26entry.1292438233%3D%2520%2520Operational%2520weather%2520forecasting%2520models%2520have%2520advanced%2520for%2520decades%2520on%2520both%2520the%250Aexplicit%2520numerical%2520solvers%2520and%2520the%2520empirical%2520physical%2520parameterization%2520schemes.%250AHowever%252C%2520the%2520involved%2520high%2520computational%2520costs%2520and%2520uncertainties%2520in%2520these%250Aexisting%2520schemes%2520are%2520requiring%2520potential%2520improvements%2520through%2520alternative%250Amachine%2520learning%2520methods.%2520Previous%2520works%2520use%2520a%2520unified%2520model%2520to%2520learn%2520the%250Adynamics%2520and%2520physics%2520of%2520the%2520atmospheric%2520model.%2520Contrarily%252C%2520we%2520propose%2520a%2520simple%250Ayet%2520effective%2520machine%2520learning%2520model%2520that%2520learns%2520the%2520horizontal%2520movement%2520in%2520the%250Adynamical%2520core%2520and%2520vertical%2520movement%2520in%2520the%2520physical%2520parameterization%250Aseparately.%2520By%2520replacing%2520the%2520advection%2520with%2520a%2520graph%2520attention%2520network%2520and%2520the%250Aconvection%2520with%2520a%2520multi-layer%2520perceptron%252C%2520our%2520model%2520provides%2520a%2520new%2520and%250Aefficient%2520perspective%2520to%2520simulate%2520the%2520transition%2520of%2520variables%2520in%2520atmospheric%250Amodels.%2520We%2520also%2520assess%2520the%2520model%2527s%2520performance%2520over%2520a%25205-day%2520iterative%250Aforecasting.%2520Under%2520the%2520same%2520input%2520variables%2520and%2520training%2520methods%252C%2520our%2520model%250Aoutperforms%2520existing%2520data-driven%2520methods%2520with%2520a%2520significantly-reduced%2520number%2520of%250Aparameters%2520with%2520a%2520resolution%2520of%25205.625%2520deg.%2520Overall%252C%2520this%2520work%2520aims%2520to%250Acontribute%2520to%2520the%2520ongoing%2520efforts%2520that%2520leverage%2520machine%2520learning%2520techniques%2520for%250Aimproving%2520both%2520the%2520accuracy%2520and%2520efficiency%2520of%2520global%2520weather%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20weather%20forecasting%20into%20advection%20and%20convection%20with%0A%20%20neural%20networks&entry.906535625=Mengxuan%20Chen%20and%20Ziqi%20Yuan%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu&entry.1292438233=%20%20Operational%20weather%20forecasting%20models%20have%20advanced%20for%20decades%20on%20both%20the%0Aexplicit%20numerical%20solvers%20and%20the%20empirical%20physical%20parameterization%20schemes.%0AHowever%2C%20the%20involved%20high%20computational%20costs%20and%20uncertainties%20in%20these%0Aexisting%20schemes%20are%20requiring%20potential%20improvements%20through%20alternative%0Amachine%20learning%20methods.%20Previous%20works%20use%20a%20unified%20model%20to%20learn%20the%0Adynamics%20and%20physics%20of%20the%20atmospheric%20model.%20Contrarily%2C%20we%20propose%20a%20simple%0Ayet%20effective%20machine%20learning%20model%20that%20learns%20the%20horizontal%20movement%20in%20the%0Adynamical%20core%20and%20vertical%20movement%20in%20the%20physical%20parameterization%0Aseparately.%20By%20replacing%20the%20advection%20with%20a%20graph%20attention%20network%20and%20the%0Aconvection%20with%20a%20multi-layer%20perceptron%2C%20our%20model%20provides%20a%20new%20and%0Aefficient%20perspective%20to%20simulate%20the%20transition%20of%20variables%20in%20atmospheric%0Amodels.%20We%20also%20assess%20the%20model%27s%20performance%20over%20a%205-day%20iterative%0Aforecasting.%20Under%20the%20same%20input%20variables%20and%20training%20methods%2C%20our%20model%0Aoutperforms%20existing%20data-driven%20methods%20with%20a%20significantly-reduced%20number%20of%0Aparameters%20with%20a%20resolution%20of%205.625%20deg.%20Overall%2C%20this%20work%20aims%20to%0Acontribute%20to%20the%20ongoing%20efforts%20that%20leverage%20machine%20learning%20techniques%20for%0Aimproving%20both%20the%20accuracy%20and%20efficiency%20of%20global%20weather%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06590v1&entry.124074799=Read"},
{"title": "Memory Mosaics", "author": "Jianyu Zhang and Niklas Nolte and Ranajoy Sadhukhan and Beidi Chen and L\u00e9on Bottou", "abstract": "  Memory Mosaics are networks of associative memories working in concert to\nachieve a prediction task of interest. Like transformers, memory mosaics\npossess compositional capabilities and in-context learning capabilities. Unlike\ntransformers, memory mosaics achieve these capabilities in comparatively\ntransparent ways. We demonstrate these capabilities on toy examples and we also\nshow that memory mosaics perform as well or better than transformers on\nmedium-scale language modeling tasks.\n", "link": "http://arxiv.org/abs/2405.06394v1", "date": "2024-05-10", "relevancy": 1.4898, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4854}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Mosaics&body=Title%3A%20Memory%20Mosaics%0AAuthor%3A%20Jianyu%20Zhang%20and%20Niklas%20Nolte%20and%20Ranajoy%20Sadhukhan%20and%20Beidi%20Chen%20and%20L%C3%A9on%20Bottou%0AAbstract%3A%20%20%20Memory%20Mosaics%20are%20networks%20of%20associative%20memories%20working%20in%20concert%20to%0Aachieve%20a%20prediction%20task%20of%20interest.%20Like%20transformers%2C%20memory%20mosaics%0Apossess%20compositional%20capabilities%20and%20in-context%20learning%20capabilities.%20Unlike%0Atransformers%2C%20memory%20mosaics%20achieve%20these%20capabilities%20in%20comparatively%0Atransparent%20ways.%20We%20demonstrate%20these%20capabilities%20on%20toy%20examples%20and%20we%20also%0Ashow%20that%20memory%20mosaics%20perform%20as%20well%20or%20better%20than%20transformers%20on%0Amedium-scale%20language%20modeling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Mosaics%26entry.906535625%3DJianyu%2520Zhang%2520and%2520Niklas%2520Nolte%2520and%2520Ranajoy%2520Sadhukhan%2520and%2520Beidi%2520Chen%2520and%2520L%25C3%25A9on%2520Bottou%26entry.1292438233%3D%2520%2520Memory%2520Mosaics%2520are%2520networks%2520of%2520associative%2520memories%2520working%2520in%2520concert%2520to%250Aachieve%2520a%2520prediction%2520task%2520of%2520interest.%2520Like%2520transformers%252C%2520memory%2520mosaics%250Apossess%2520compositional%2520capabilities%2520and%2520in-context%2520learning%2520capabilities.%2520Unlike%250Atransformers%252C%2520memory%2520mosaics%2520achieve%2520these%2520capabilities%2520in%2520comparatively%250Atransparent%2520ways.%2520We%2520demonstrate%2520these%2520capabilities%2520on%2520toy%2520examples%2520and%2520we%2520also%250Ashow%2520that%2520memory%2520mosaics%2520perform%2520as%2520well%2520or%2520better%2520than%2520transformers%2520on%250Amedium-scale%2520language%2520modeling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Mosaics&entry.906535625=Jianyu%20Zhang%20and%20Niklas%20Nolte%20and%20Ranajoy%20Sadhukhan%20and%20Beidi%20Chen%20and%20L%C3%A9on%20Bottou&entry.1292438233=%20%20Memory%20Mosaics%20are%20networks%20of%20associative%20memories%20working%20in%20concert%20to%0Aachieve%20a%20prediction%20task%20of%20interest.%20Like%20transformers%2C%20memory%20mosaics%0Apossess%20compositional%20capabilities%20and%20in-context%20learning%20capabilities.%20Unlike%0Atransformers%2C%20memory%20mosaics%20achieve%20these%20capabilities%20in%20comparatively%0Atransparent%20ways.%20We%20demonstrate%20these%20capabilities%20on%20toy%20examples%20and%20we%20also%0Ashow%20that%20memory%20mosaics%20perform%20as%20well%20or%20better%20than%20transformers%20on%0Amedium-scale%20language%20modeling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06394v1&entry.124074799=Read"},
{"title": "Conformal Validity Guarantees Exist for Any Data Distribution", "author": "Drew Prinster and Samuel Stanton and Anqi Liu and Suchi Saria", "abstract": "  As machine learning (ML) gains widespread adoption, practitioners are\nincreasingly seeking means to quantify and control the risk these systems\nincur. This challenge is especially salient when ML systems have autonomy to\ncollect their own data, such as in black-box optimization and active learning,\nwhere their actions induce sequential feedback-loop shifts in the data\ndistribution. Conformal prediction has emerged as a promising approach to\nuncertainty and risk quantification, but existing variants either fail to\naccommodate sequences of data-dependent shifts, or do not fully exploit the\nfact that agent-induced shift is under our control. In this work we prove that\nconformal prediction can theoretically be extended to \\textit{any} joint data\ndistribution, not just exchangeable or quasi-exchangeable ones, although it is\nexceedingly impractical to compute in the most general case. For practical\napplications, we outline a procedure for deriving specific conformal algorithms\nfor any data distribution, and we use this procedure to derive tractable\nalgorithms for a series of agent-induced covariate shifts. We evaluate the\nproposed algorithms empirically on synthetic black-box optimization and active\nlearning tasks.\n", "link": "http://arxiv.org/abs/2405.06627v1", "date": "2024-05-10", "relevancy": 1.46, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4944}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Validity%20Guarantees%20Exist%20for%20Any%20Data%20Distribution&body=Title%3A%20Conformal%20Validity%20Guarantees%20Exist%20for%20Any%20Data%20Distribution%0AAuthor%3A%20Drew%20Prinster%20and%20Samuel%20Stanton%20and%20Anqi%20Liu%20and%20Suchi%20Saria%0AAbstract%3A%20%20%20As%20machine%20learning%20%28ML%29%20gains%20widespread%20adoption%2C%20practitioners%20are%0Aincreasingly%20seeking%20means%20to%20quantify%20and%20control%20the%20risk%20these%20systems%0Aincur.%20This%20challenge%20is%20especially%20salient%20when%20ML%20systems%20have%20autonomy%20to%0Acollect%20their%20own%20data%2C%20such%20as%20in%20black-box%20optimization%20and%20active%20learning%2C%0Awhere%20their%20actions%20induce%20sequential%20feedback-loop%20shifts%20in%20the%20data%0Adistribution.%20Conformal%20prediction%20has%20emerged%20as%20a%20promising%20approach%20to%0Auncertainty%20and%20risk%20quantification%2C%20but%20existing%20variants%20either%20fail%20to%0Aaccommodate%20sequences%20of%20data-dependent%20shifts%2C%20or%20do%20not%20fully%20exploit%20the%0Afact%20that%20agent-induced%20shift%20is%20under%20our%20control.%20In%20this%20work%20we%20prove%20that%0Aconformal%20prediction%20can%20theoretically%20be%20extended%20to%20%5Ctextit%7Bany%7D%20joint%20data%0Adistribution%2C%20not%20just%20exchangeable%20or%20quasi-exchangeable%20ones%2C%20although%20it%20is%0Aexceedingly%20impractical%20to%20compute%20in%20the%20most%20general%20case.%20For%20practical%0Aapplications%2C%20we%20outline%20a%20procedure%20for%20deriving%20specific%20conformal%20algorithms%0Afor%20any%20data%20distribution%2C%20and%20we%20use%20this%20procedure%20to%20derive%20tractable%0Aalgorithms%20for%20a%20series%20of%20agent-induced%20covariate%20shifts.%20We%20evaluate%20the%0Aproposed%20algorithms%20empirically%20on%20synthetic%20black-box%20optimization%20and%20active%0Alearning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Validity%2520Guarantees%2520Exist%2520for%2520Any%2520Data%2520Distribution%26entry.906535625%3DDrew%2520Prinster%2520and%2520Samuel%2520Stanton%2520and%2520Anqi%2520Liu%2520and%2520Suchi%2520Saria%26entry.1292438233%3D%2520%2520As%2520machine%2520learning%2520%2528ML%2529%2520gains%2520widespread%2520adoption%252C%2520practitioners%2520are%250Aincreasingly%2520seeking%2520means%2520to%2520quantify%2520and%2520control%2520the%2520risk%2520these%2520systems%250Aincur.%2520This%2520challenge%2520is%2520especially%2520salient%2520when%2520ML%2520systems%2520have%2520autonomy%2520to%250Acollect%2520their%2520own%2520data%252C%2520such%2520as%2520in%2520black-box%2520optimization%2520and%2520active%2520learning%252C%250Awhere%2520their%2520actions%2520induce%2520sequential%2520feedback-loop%2520shifts%2520in%2520the%2520data%250Adistribution.%2520Conformal%2520prediction%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%250Auncertainty%2520and%2520risk%2520quantification%252C%2520but%2520existing%2520variants%2520either%2520fail%2520to%250Aaccommodate%2520sequences%2520of%2520data-dependent%2520shifts%252C%2520or%2520do%2520not%2520fully%2520exploit%2520the%250Afact%2520that%2520agent-induced%2520shift%2520is%2520under%2520our%2520control.%2520In%2520this%2520work%2520we%2520prove%2520that%250Aconformal%2520prediction%2520can%2520theoretically%2520be%2520extended%2520to%2520%255Ctextit%257Bany%257D%2520joint%2520data%250Adistribution%252C%2520not%2520just%2520exchangeable%2520or%2520quasi-exchangeable%2520ones%252C%2520although%2520it%2520is%250Aexceedingly%2520impractical%2520to%2520compute%2520in%2520the%2520most%2520general%2520case.%2520For%2520practical%250Aapplications%252C%2520we%2520outline%2520a%2520procedure%2520for%2520deriving%2520specific%2520conformal%2520algorithms%250Afor%2520any%2520data%2520distribution%252C%2520and%2520we%2520use%2520this%2520procedure%2520to%2520derive%2520tractable%250Aalgorithms%2520for%2520a%2520series%2520of%2520agent-induced%2520covariate%2520shifts.%2520We%2520evaluate%2520the%250Aproposed%2520algorithms%2520empirically%2520on%2520synthetic%2520black-box%2520optimization%2520and%2520active%250Alearning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Validity%20Guarantees%20Exist%20for%20Any%20Data%20Distribution&entry.906535625=Drew%20Prinster%20and%20Samuel%20Stanton%20and%20Anqi%20Liu%20and%20Suchi%20Saria&entry.1292438233=%20%20As%20machine%20learning%20%28ML%29%20gains%20widespread%20adoption%2C%20practitioners%20are%0Aincreasingly%20seeking%20means%20to%20quantify%20and%20control%20the%20risk%20these%20systems%0Aincur.%20This%20challenge%20is%20especially%20salient%20when%20ML%20systems%20have%20autonomy%20to%0Acollect%20their%20own%20data%2C%20such%20as%20in%20black-box%20optimization%20and%20active%20learning%2C%0Awhere%20their%20actions%20induce%20sequential%20feedback-loop%20shifts%20in%20the%20data%0Adistribution.%20Conformal%20prediction%20has%20emerged%20as%20a%20promising%20approach%20to%0Auncertainty%20and%20risk%20quantification%2C%20but%20existing%20variants%20either%20fail%20to%0Aaccommodate%20sequences%20of%20data-dependent%20shifts%2C%20or%20do%20not%20fully%20exploit%20the%0Afact%20that%20agent-induced%20shift%20is%20under%20our%20control.%20In%20this%20work%20we%20prove%20that%0Aconformal%20prediction%20can%20theoretically%20be%20extended%20to%20%5Ctextit%7Bany%7D%20joint%20data%0Adistribution%2C%20not%20just%20exchangeable%20or%20quasi-exchangeable%20ones%2C%20although%20it%20is%0Aexceedingly%20impractical%20to%20compute%20in%20the%20most%20general%20case.%20For%20practical%0Aapplications%2C%20we%20outline%20a%20procedure%20for%20deriving%20specific%20conformal%20algorithms%0Afor%20any%20data%20distribution%2C%20and%20we%20use%20this%20procedure%20to%20derive%20tractable%0Aalgorithms%20for%20a%20series%20of%20agent-induced%20covariate%20shifts.%20We%20evaluate%20the%0Aproposed%20algorithms%20empirically%20on%20synthetic%20black-box%20optimization%20and%20active%0Alearning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06627v1&entry.124074799=Read"},
{"title": "Dynamically Scaled Temperature in Self-Supervised Contrastive Learning", "author": "Siladittya Manna and Soumitri Chattopadhyay and Rakesh Dey and Saumik Bhattacharya and Umapada Pal", "abstract": "  In contemporary self-supervised contrastive algorithms like SimCLR, MoCo,\netc., the task of balancing attraction between two semantically similar samples\nand repulsion between two samples of different classes is primarily affected by\nthe presence of hard negative samples. While the InfoNCE loss has been shown to\nimpose penalties based on hardness, the temperature hyper-parameter is the key\nto regulating the penalties and the trade-off between uniformity and tolerance.\nIn this work, we focus our attention on improving the performance of InfoNCE\nloss in self-supervised learning by proposing a novel cosine similarity\ndependent temperature scaling function to effectively optimize the distribution\nof the samples in the feature space. We also provide mathematical analyses to\nsupport the construction of such a dynamically scaled temperature function.\nExperimental evidence shows that the proposed framework outperforms the\ncontrastive loss-based SSL algorithms.\n", "link": "http://arxiv.org/abs/2308.01140v2", "date": "2024-05-10", "relevancy": 1.4427, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5092}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamically%20Scaled%20Temperature%20in%20Self-Supervised%20Contrastive%20Learning&body=Title%3A%20Dynamically%20Scaled%20Temperature%20in%20Self-Supervised%20Contrastive%20Learning%0AAuthor%3A%20Siladittya%20Manna%20and%20Soumitri%20Chattopadhyay%20and%20Rakesh%20Dey%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal%0AAbstract%3A%20%20%20In%20contemporary%20self-supervised%20contrastive%20algorithms%20like%20SimCLR%2C%20MoCo%2C%0Aetc.%2C%20the%20task%20of%20balancing%20attraction%20between%20two%20semantically%20similar%20samples%0Aand%20repulsion%20between%20two%20samples%20of%20different%20classes%20is%20primarily%20affected%20by%0Athe%20presence%20of%20hard%20negative%20samples.%20While%20the%20InfoNCE%20loss%20has%20been%20shown%20to%0Aimpose%20penalties%20based%20on%20hardness%2C%20the%20temperature%20hyper-parameter%20is%20the%20key%0Ato%20regulating%20the%20penalties%20and%20the%20trade-off%20between%20uniformity%20and%20tolerance.%0AIn%20this%20work%2C%20we%20focus%20our%20attention%20on%20improving%20the%20performance%20of%20InfoNCE%0Aloss%20in%20self-supervised%20learning%20by%20proposing%20a%20novel%20cosine%20similarity%0Adependent%20temperature%20scaling%20function%20to%20effectively%20optimize%20the%20distribution%0Aof%20the%20samples%20in%20the%20feature%20space.%20We%20also%20provide%20mathematical%20analyses%20to%0Asupport%20the%20construction%20of%20such%20a%20dynamically%20scaled%20temperature%20function.%0AExperimental%20evidence%20shows%20that%20the%20proposed%20framework%20outperforms%20the%0Acontrastive%20loss-based%20SSL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamically%2520Scaled%2520Temperature%2520in%2520Self-Supervised%2520Contrastive%2520Learning%26entry.906535625%3DSiladittya%2520Manna%2520and%2520Soumitri%2520Chattopadhyay%2520and%2520Rakesh%2520Dey%2520and%2520Saumik%2520Bhattacharya%2520and%2520Umapada%2520Pal%26entry.1292438233%3D%2520%2520In%2520contemporary%2520self-supervised%2520contrastive%2520algorithms%2520like%2520SimCLR%252C%2520MoCo%252C%250Aetc.%252C%2520the%2520task%2520of%2520balancing%2520attraction%2520between%2520two%2520semantically%2520similar%2520samples%250Aand%2520repulsion%2520between%2520two%2520samples%2520of%2520different%2520classes%2520is%2520primarily%2520affected%2520by%250Athe%2520presence%2520of%2520hard%2520negative%2520samples.%2520While%2520the%2520InfoNCE%2520loss%2520has%2520been%2520shown%2520to%250Aimpose%2520penalties%2520based%2520on%2520hardness%252C%2520the%2520temperature%2520hyper-parameter%2520is%2520the%2520key%250Ato%2520regulating%2520the%2520penalties%2520and%2520the%2520trade-off%2520between%2520uniformity%2520and%2520tolerance.%250AIn%2520this%2520work%252C%2520we%2520focus%2520our%2520attention%2520on%2520improving%2520the%2520performance%2520of%2520InfoNCE%250Aloss%2520in%2520self-supervised%2520learning%2520by%2520proposing%2520a%2520novel%2520cosine%2520similarity%250Adependent%2520temperature%2520scaling%2520function%2520to%2520effectively%2520optimize%2520the%2520distribution%250Aof%2520the%2520samples%2520in%2520the%2520feature%2520space.%2520We%2520also%2520provide%2520mathematical%2520analyses%2520to%250Asupport%2520the%2520construction%2520of%2520such%2520a%2520dynamically%2520scaled%2520temperature%2520function.%250AExperimental%2520evidence%2520shows%2520that%2520the%2520proposed%2520framework%2520outperforms%2520the%250Acontrastive%2520loss-based%2520SSL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.01140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamically%20Scaled%20Temperature%20in%20Self-Supervised%20Contrastive%20Learning&entry.906535625=Siladittya%20Manna%20and%20Soumitri%20Chattopadhyay%20and%20Rakesh%20Dey%20and%20Saumik%20Bhattacharya%20and%20Umapada%20Pal&entry.1292438233=%20%20In%20contemporary%20self-supervised%20contrastive%20algorithms%20like%20SimCLR%2C%20MoCo%2C%0Aetc.%2C%20the%20task%20of%20balancing%20attraction%20between%20two%20semantically%20similar%20samples%0Aand%20repulsion%20between%20two%20samples%20of%20different%20classes%20is%20primarily%20affected%20by%0Athe%20presence%20of%20hard%20negative%20samples.%20While%20the%20InfoNCE%20loss%20has%20been%20shown%20to%0Aimpose%20penalties%20based%20on%20hardness%2C%20the%20temperature%20hyper-parameter%20is%20the%20key%0Ato%20regulating%20the%20penalties%20and%20the%20trade-off%20between%20uniformity%20and%20tolerance.%0AIn%20this%20work%2C%20we%20focus%20our%20attention%20on%20improving%20the%20performance%20of%20InfoNCE%0Aloss%20in%20self-supervised%20learning%20by%20proposing%20a%20novel%20cosine%20similarity%0Adependent%20temperature%20scaling%20function%20to%20effectively%20optimize%20the%20distribution%0Aof%20the%20samples%20in%20the%20feature%20space.%20We%20also%20provide%20mathematical%20analyses%20to%0Asupport%20the%20construction%20of%20such%20a%20dynamically%20scaled%20temperature%20function.%0AExperimental%20evidence%20shows%20that%20the%20proposed%20framework%20outperforms%20the%0Acontrastive%20loss-based%20SSL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01140v2&entry.124074799=Read"},
{"title": "Open-Source Drift Detection Tools in Action: Insights from Two Use Cases", "author": "Rieke M\u00fcller and Mohamed Abdelaal and Davor Stjelja", "abstract": "  Data drifts pose a critical challenge in the lifecycle of machine learning\n(ML) models, affecting their performance and reliability. In response to this\nchallenge, we present a microbenchmark study, called D3Bench, which evaluates\nthe efficacy of open-source drift detection tools. D3Bench examines the\ncapabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world\ndata from two smart building use cases.We prioritize assessing the functional\nsuitability of these tools to identify and analyze data drifts. Furthermore, we\nconsider a comprehensive set of non-functional criteria, such as the\nintegrability with ML pipelines, the adaptability to diverse data types,\nuser-friendliness, computational efficiency, and resource demands. Our findings\nreveal that Evidently AI stands out for its general data drift detection,\nwhereas NannyML excels at pinpointing the precise timing of shifts and\nevaluating their consequent effects on predictive accuracy.\n", "link": "http://arxiv.org/abs/2404.18673v2", "date": "2024-05-10", "relevancy": 1.4366, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases&body=Title%3A%20Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases%0AAuthor%3A%20Rieke%20M%C3%BCller%20and%20Mohamed%20Abdelaal%20and%20Davor%20Stjelja%0AAbstract%3A%20%20%20Data%20drifts%20pose%20a%20critical%20challenge%20in%20the%20lifecycle%20of%20machine%20learning%0A%28ML%29%20models%2C%20affecting%20their%20performance%20and%20reliability.%20In%20response%20to%20this%0Achallenge%2C%20we%20present%20a%20microbenchmark%20study%2C%20called%20D3Bench%2C%20which%20evaluates%0Athe%20efficacy%20of%20open-source%20drift%20detection%20tools.%20D3Bench%20examines%20the%0Acapabilities%20of%20Evidently%20AI%2C%20NannyML%2C%20and%20Alibi-Detect%2C%20leveraging%20real-world%0Adata%20from%20two%20smart%20building%20use%20cases.We%20prioritize%20assessing%20the%20functional%0Asuitability%20of%20these%20tools%20to%20identify%20and%20analyze%20data%20drifts.%20Furthermore%2C%20we%0Aconsider%20a%20comprehensive%20set%20of%20non-functional%20criteria%2C%20such%20as%20the%0Aintegrability%20with%20ML%20pipelines%2C%20the%20adaptability%20to%20diverse%20data%20types%2C%0Auser-friendliness%2C%20computational%20efficiency%2C%20and%20resource%20demands.%20Our%20findings%0Areveal%20that%20Evidently%20AI%20stands%20out%20for%20its%20general%20data%20drift%20detection%2C%0Awhereas%20NannyML%20excels%20at%20pinpointing%20the%20precise%20timing%20of%20shifts%20and%0Aevaluating%20their%20consequent%20effects%20on%20predictive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18673v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Source%2520Drift%2520Detection%2520Tools%2520in%2520Action%253A%2520Insights%2520from%2520Two%2520Use%2520Cases%26entry.906535625%3DRieke%2520M%25C3%25BCller%2520and%2520Mohamed%2520Abdelaal%2520and%2520Davor%2520Stjelja%26entry.1292438233%3D%2520%2520Data%2520drifts%2520pose%2520a%2520critical%2520challenge%2520in%2520the%2520lifecycle%2520of%2520machine%2520learning%250A%2528ML%2529%2520models%252C%2520affecting%2520their%2520performance%2520and%2520reliability.%2520In%2520response%2520to%2520this%250Achallenge%252C%2520we%2520present%2520a%2520microbenchmark%2520study%252C%2520called%2520D3Bench%252C%2520which%2520evaluates%250Athe%2520efficacy%2520of%2520open-source%2520drift%2520detection%2520tools.%2520D3Bench%2520examines%2520the%250Acapabilities%2520of%2520Evidently%2520AI%252C%2520NannyML%252C%2520and%2520Alibi-Detect%252C%2520leveraging%2520real-world%250Adata%2520from%2520two%2520smart%2520building%2520use%2520cases.We%2520prioritize%2520assessing%2520the%2520functional%250Asuitability%2520of%2520these%2520tools%2520to%2520identify%2520and%2520analyze%2520data%2520drifts.%2520Furthermore%252C%2520we%250Aconsider%2520a%2520comprehensive%2520set%2520of%2520non-functional%2520criteria%252C%2520such%2520as%2520the%250Aintegrability%2520with%2520ML%2520pipelines%252C%2520the%2520adaptability%2520to%2520diverse%2520data%2520types%252C%250Auser-friendliness%252C%2520computational%2520efficiency%252C%2520and%2520resource%2520demands.%2520Our%2520findings%250Areveal%2520that%2520Evidently%2520AI%2520stands%2520out%2520for%2520its%2520general%2520data%2520drift%2520detection%252C%250Awhereas%2520NannyML%2520excels%2520at%2520pinpointing%2520the%2520precise%2520timing%2520of%2520shifts%2520and%250Aevaluating%2520their%2520consequent%2520effects%2520on%2520predictive%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18673v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases&entry.906535625=Rieke%20M%C3%BCller%20and%20Mohamed%20Abdelaal%20and%20Davor%20Stjelja&entry.1292438233=%20%20Data%20drifts%20pose%20a%20critical%20challenge%20in%20the%20lifecycle%20of%20machine%20learning%0A%28ML%29%20models%2C%20affecting%20their%20performance%20and%20reliability.%20In%20response%20to%20this%0Achallenge%2C%20we%20present%20a%20microbenchmark%20study%2C%20called%20D3Bench%2C%20which%20evaluates%0Athe%20efficacy%20of%20open-source%20drift%20detection%20tools.%20D3Bench%20examines%20the%0Acapabilities%20of%20Evidently%20AI%2C%20NannyML%2C%20and%20Alibi-Detect%2C%20leveraging%20real-world%0Adata%20from%20two%20smart%20building%20use%20cases.We%20prioritize%20assessing%20the%20functional%0Asuitability%20of%20these%20tools%20to%20identify%20and%20analyze%20data%20drifts.%20Furthermore%2C%20we%0Aconsider%20a%20comprehensive%20set%20of%20non-functional%20criteria%2C%20such%20as%20the%0Aintegrability%20with%20ML%20pipelines%2C%20the%20adaptability%20to%20diverse%20data%20types%2C%0Auser-friendliness%2C%20computational%20efficiency%2C%20and%20resource%20demands.%20Our%20findings%0Areveal%20that%20Evidently%20AI%20stands%20out%20for%20its%20general%20data%20drift%20detection%2C%0Awhereas%20NannyML%20excels%20at%20pinpointing%20the%20precise%20timing%20of%20shifts%20and%0Aevaluating%20their%20consequent%20effects%20on%20predictive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18673v2&entry.124074799=Read"},
{"title": "Explaining Text Similarity in Transformer Models", "author": "Alexandros Vasileiou and Oliver Eberle", "abstract": "  As Transformers have become state-of-the-art models for natural language\nprocessing (NLP) tasks, the need to understand and explain their predictions is\nincreasingly apparent. Especially in unsupervised applications, such as\ninformation retrieval tasks, similarity models built on top of foundation model\nrepresentations have been widely applied. However, their inner prediction\nmechanisms have mostly remained opaque. Recent advances in explainable AI have\nmade it possible to mitigate these limitations by leveraging improved\nexplanations for Transformers through layer-wise relevance propagation (LRP).\nUsing BiLRP, an extension developed for computing second-order explanations in\nbilinear similarity models, we investigate which feature interactions drive\nsimilarity in NLP models. We validate the resulting explanations and\ndemonstrate their utility in three corpus-level use cases, analyzing\ngrammatical interactions, multilingual semantics, and biomedical text\nretrieval. Our findings contribute to a deeper understanding of different\nsemantic similarity tasks and models, highlighting how novel explainable AI\nmethods enable in-depth analyses and corpus-level insights.\n", "link": "http://arxiv.org/abs/2405.06604v1", "date": "2024-05-10", "relevancy": 1.4299, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Text%20Similarity%20in%20Transformer%20Models&body=Title%3A%20Explaining%20Text%20Similarity%20in%20Transformer%20Models%0AAuthor%3A%20Alexandros%20Vasileiou%20and%20Oliver%20Eberle%0AAbstract%3A%20%20%20As%20Transformers%20have%20become%20state-of-the-art%20models%20for%20natural%20language%0Aprocessing%20%28NLP%29%20tasks%2C%20the%20need%20to%20understand%20and%20explain%20their%20predictions%20is%0Aincreasingly%20apparent.%20Especially%20in%20unsupervised%20applications%2C%20such%20as%0Ainformation%20retrieval%20tasks%2C%20similarity%20models%20built%20on%20top%20of%20foundation%20model%0Arepresentations%20have%20been%20widely%20applied.%20However%2C%20their%20inner%20prediction%0Amechanisms%20have%20mostly%20remained%20opaque.%20Recent%20advances%20in%20explainable%20AI%20have%0Amade%20it%20possible%20to%20mitigate%20these%20limitations%20by%20leveraging%20improved%0Aexplanations%20for%20Transformers%20through%20layer-wise%20relevance%20propagation%20%28LRP%29.%0AUsing%20BiLRP%2C%20an%20extension%20developed%20for%20computing%20second-order%20explanations%20in%0Abilinear%20similarity%20models%2C%20we%20investigate%20which%20feature%20interactions%20drive%0Asimilarity%20in%20NLP%20models.%20We%20validate%20the%20resulting%20explanations%20and%0Ademonstrate%20their%20utility%20in%20three%20corpus-level%20use%20cases%2C%20analyzing%0Agrammatical%20interactions%2C%20multilingual%20semantics%2C%20and%20biomedical%20text%0Aretrieval.%20Our%20findings%20contribute%20to%20a%20deeper%20understanding%20of%20different%0Asemantic%20similarity%20tasks%20and%20models%2C%20highlighting%20how%20novel%20explainable%20AI%0Amethods%20enable%20in-depth%20analyses%20and%20corpus-level%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Text%2520Similarity%2520in%2520Transformer%2520Models%26entry.906535625%3DAlexandros%2520Vasileiou%2520and%2520Oliver%2520Eberle%26entry.1292438233%3D%2520%2520As%2520Transformers%2520have%2520become%2520state-of-the-art%2520models%2520for%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520tasks%252C%2520the%2520need%2520to%2520understand%2520and%2520explain%2520their%2520predictions%2520is%250Aincreasingly%2520apparent.%2520Especially%2520in%2520unsupervised%2520applications%252C%2520such%2520as%250Ainformation%2520retrieval%2520tasks%252C%2520similarity%2520models%2520built%2520on%2520top%2520of%2520foundation%2520model%250Arepresentations%2520have%2520been%2520widely%2520applied.%2520However%252C%2520their%2520inner%2520prediction%250Amechanisms%2520have%2520mostly%2520remained%2520opaque.%2520Recent%2520advances%2520in%2520explainable%2520AI%2520have%250Amade%2520it%2520possible%2520to%2520mitigate%2520these%2520limitations%2520by%2520leveraging%2520improved%250Aexplanations%2520for%2520Transformers%2520through%2520layer-wise%2520relevance%2520propagation%2520%2528LRP%2529.%250AUsing%2520BiLRP%252C%2520an%2520extension%2520developed%2520for%2520computing%2520second-order%2520explanations%2520in%250Abilinear%2520similarity%2520models%252C%2520we%2520investigate%2520which%2520feature%2520interactions%2520drive%250Asimilarity%2520in%2520NLP%2520models.%2520We%2520validate%2520the%2520resulting%2520explanations%2520and%250Ademonstrate%2520their%2520utility%2520in%2520three%2520corpus-level%2520use%2520cases%252C%2520analyzing%250Agrammatical%2520interactions%252C%2520multilingual%2520semantics%252C%2520and%2520biomedical%2520text%250Aretrieval.%2520Our%2520findings%2520contribute%2520to%2520a%2520deeper%2520understanding%2520of%2520different%250Asemantic%2520similarity%2520tasks%2520and%2520models%252C%2520highlighting%2520how%2520novel%2520explainable%2520AI%250Amethods%2520enable%2520in-depth%2520analyses%2520and%2520corpus-level%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Text%20Similarity%20in%20Transformer%20Models&entry.906535625=Alexandros%20Vasileiou%20and%20Oliver%20Eberle&entry.1292438233=%20%20As%20Transformers%20have%20become%20state-of-the-art%20models%20for%20natural%20language%0Aprocessing%20%28NLP%29%20tasks%2C%20the%20need%20to%20understand%20and%20explain%20their%20predictions%20is%0Aincreasingly%20apparent.%20Especially%20in%20unsupervised%20applications%2C%20such%20as%0Ainformation%20retrieval%20tasks%2C%20similarity%20models%20built%20on%20top%20of%20foundation%20model%0Arepresentations%20have%20been%20widely%20applied.%20However%2C%20their%20inner%20prediction%0Amechanisms%20have%20mostly%20remained%20opaque.%20Recent%20advances%20in%20explainable%20AI%20have%0Amade%20it%20possible%20to%20mitigate%20these%20limitations%20by%20leveraging%20improved%0Aexplanations%20for%20Transformers%20through%20layer-wise%20relevance%20propagation%20%28LRP%29.%0AUsing%20BiLRP%2C%20an%20extension%20developed%20for%20computing%20second-order%20explanations%20in%0Abilinear%20similarity%20models%2C%20we%20investigate%20which%20feature%20interactions%20drive%0Asimilarity%20in%20NLP%20models.%20We%20validate%20the%20resulting%20explanations%20and%0Ademonstrate%20their%20utility%20in%20three%20corpus-level%20use%20cases%2C%20analyzing%0Agrammatical%20interactions%2C%20multilingual%20semantics%2C%20and%20biomedical%20text%0Aretrieval.%20Our%20findings%20contribute%20to%20a%20deeper%20understanding%20of%20different%0Asemantic%20similarity%20tasks%20and%20models%2C%20highlighting%20how%20novel%20explainable%20AI%0Amethods%20enable%20in-depth%20analyses%20and%20corpus-level%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06604v1&entry.124074799=Read"},
{"title": "Compressing Sign Information in DCT-based Image Coding via Deep Sign\n  Retrieval", "author": "Kei Suzuki and Chihiro Tsutake and Keita Takahashi and Toshiaki Fujii", "abstract": "  Compressing the sign information of discrete cosine transform (DCT)\ncoefficients is an intractable problem in image coding schemes due to the\nequiprobable characteristics of the signs. To overcome this difficulty, we\npropose an efficient compression method for the sign information called \"sign\nretrieval.\" This method is inspired by phase retrieval, which is a classical\nsignal restoration problem of finding the phase information of discrete Fourier\ntransform coefficients from their magnitudes. The sign information of all DCT\ncoefficients is excluded from a bitstream at the encoder and is complemented at\nthe decoder through our sign retrieval method. We show through experiments that\nour method outperforms previous ones in terms of the bit amount for the signs\nand computation cost. Our method, implemented in Python language, is available\nfrom https://github.com/ctsutake/dsr.\n", "link": "http://arxiv.org/abs/2209.10712v2", "date": "2024-05-10", "relevancy": 1.4209, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4696}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressing%20Sign%20Information%20in%20DCT-based%20Image%20Coding%20via%20Deep%20Sign%0A%20%20Retrieval&body=Title%3A%20Compressing%20Sign%20Information%20in%20DCT-based%20Image%20Coding%20via%20Deep%20Sign%0A%20%20Retrieval%0AAuthor%3A%20Kei%20Suzuki%20and%20Chihiro%20Tsutake%20and%20Keita%20Takahashi%20and%20Toshiaki%20Fujii%0AAbstract%3A%20%20%20Compressing%20the%20sign%20information%20of%20discrete%20cosine%20transform%20%28DCT%29%0Acoefficients%20is%20an%20intractable%20problem%20in%20image%20coding%20schemes%20due%20to%20the%0Aequiprobable%20characteristics%20of%20the%20signs.%20To%20overcome%20this%20difficulty%2C%20we%0Apropose%20an%20efficient%20compression%20method%20for%20the%20sign%20information%20called%20%22sign%0Aretrieval.%22%20This%20method%20is%20inspired%20by%20phase%20retrieval%2C%20which%20is%20a%20classical%0Asignal%20restoration%20problem%20of%20finding%20the%20phase%20information%20of%20discrete%20Fourier%0Atransform%20coefficients%20from%20their%20magnitudes.%20The%20sign%20information%20of%20all%20DCT%0Acoefficients%20is%20excluded%20from%20a%20bitstream%20at%20the%20encoder%20and%20is%20complemented%20at%0Athe%20decoder%20through%20our%20sign%20retrieval%20method.%20We%20show%20through%20experiments%20that%0Aour%20method%20outperforms%20previous%20ones%20in%20terms%20of%20the%20bit%20amount%20for%20the%20signs%0Aand%20computation%20cost.%20Our%20method%2C%20implemented%20in%20Python%20language%2C%20is%20available%0Afrom%20https%3A//github.com/ctsutake/dsr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressing%2520Sign%2520Information%2520in%2520DCT-based%2520Image%2520Coding%2520via%2520Deep%2520Sign%250A%2520%2520Retrieval%26entry.906535625%3DKei%2520Suzuki%2520and%2520Chihiro%2520Tsutake%2520and%2520Keita%2520Takahashi%2520and%2520Toshiaki%2520Fujii%26entry.1292438233%3D%2520%2520Compressing%2520the%2520sign%2520information%2520of%2520discrete%2520cosine%2520transform%2520%2528DCT%2529%250Acoefficients%2520is%2520an%2520intractable%2520problem%2520in%2520image%2520coding%2520schemes%2520due%2520to%2520the%250Aequiprobable%2520characteristics%2520of%2520the%2520signs.%2520To%2520overcome%2520this%2520difficulty%252C%2520we%250Apropose%2520an%2520efficient%2520compression%2520method%2520for%2520the%2520sign%2520information%2520called%2520%2522sign%250Aretrieval.%2522%2520This%2520method%2520is%2520inspired%2520by%2520phase%2520retrieval%252C%2520which%2520is%2520a%2520classical%250Asignal%2520restoration%2520problem%2520of%2520finding%2520the%2520phase%2520information%2520of%2520discrete%2520Fourier%250Atransform%2520coefficients%2520from%2520their%2520magnitudes.%2520The%2520sign%2520information%2520of%2520all%2520DCT%250Acoefficients%2520is%2520excluded%2520from%2520a%2520bitstream%2520at%2520the%2520encoder%2520and%2520is%2520complemented%2520at%250Athe%2520decoder%2520through%2520our%2520sign%2520retrieval%2520method.%2520We%2520show%2520through%2520experiments%2520that%250Aour%2520method%2520outperforms%2520previous%2520ones%2520in%2520terms%2520of%2520the%2520bit%2520amount%2520for%2520the%2520signs%250Aand%2520computation%2520cost.%2520Our%2520method%252C%2520implemented%2520in%2520Python%2520language%252C%2520is%2520available%250Afrom%2520https%253A//github.com/ctsutake/dsr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.10712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressing%20Sign%20Information%20in%20DCT-based%20Image%20Coding%20via%20Deep%20Sign%0A%20%20Retrieval&entry.906535625=Kei%20Suzuki%20and%20Chihiro%20Tsutake%20and%20Keita%20Takahashi%20and%20Toshiaki%20Fujii&entry.1292438233=%20%20Compressing%20the%20sign%20information%20of%20discrete%20cosine%20transform%20%28DCT%29%0Acoefficients%20is%20an%20intractable%20problem%20in%20image%20coding%20schemes%20due%20to%20the%0Aequiprobable%20characteristics%20of%20the%20signs.%20To%20overcome%20this%20difficulty%2C%20we%0Apropose%20an%20efficient%20compression%20method%20for%20the%20sign%20information%20called%20%22sign%0Aretrieval.%22%20This%20method%20is%20inspired%20by%20phase%20retrieval%2C%20which%20is%20a%20classical%0Asignal%20restoration%20problem%20of%20finding%20the%20phase%20information%20of%20discrete%20Fourier%0Atransform%20coefficients%20from%20their%20magnitudes.%20The%20sign%20information%20of%20all%20DCT%0Acoefficients%20is%20excluded%20from%20a%20bitstream%20at%20the%20encoder%20and%20is%20complemented%20at%0Athe%20decoder%20through%20our%20sign%20retrieval%20method.%20We%20show%20through%20experiments%20that%0Aour%20method%20outperforms%20previous%20ones%20in%20terms%20of%20the%20bit%20amount%20for%20the%20signs%0Aand%20computation%20cost.%20Our%20method%2C%20implemented%20in%20Python%20language%2C%20is%20available%0Afrom%20https%3A//github.com/ctsutake/dsr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10712v2&entry.124074799=Read"},
{"title": "Residual-based Attention Physics-informed Neural Networks for Efficient\n  Spatio-Temporal Lifetime Assessment of Transformers Operated in Renewable\n  Power Plants", "author": "Ibai Ramirez and Joel Pino and David Pardo and Mikel Sanz and Luis del Rio and Alvaro Ortiz and Kateryna Morozovska and Jose I. Aizpurua", "abstract": "  Transformers are vital assets for the reliable and efficient operation of\npower and energy systems. They support the integration of renewables to the\ngrid through improved grid stability and operation efficiency. Monitoring the\nhealth of transformers is essential to ensure grid reliability and efficiency.\nThermal insulation ageing is a key transformer failure mode, which is generally\ntracked by monitoring the hotspot temperature (HST). However, HST measurement\nis complex and expensive and often estimated from indirect measurements.\nExisting computationally-efficient HST models focus on space-agnostic thermal\nmodels, providing worst-case HST estimates. This article introduces an\nefficient spatio-temporal model for transformer winding temperature and ageing\nestimation, which leverages physics-based partial differential equations (PDEs)\nwith data-driven Neural Networks (NN) in a Physics Informed Neural Networks\n(PINNs) configuration to improve prediction accuracy and acquire\nspatio-temporal resolution. The computational efficiency of the PINN model is\nimproved through the implementation of the Residual-Based Attention scheme that\naccelerates the PINN model convergence. PINN based oil temperature predictions\nare used to estimate spatio-temporal transformer winding temperature values,\nwhich are validated through PDE resolution models and fiber optic sensor\nmeasurements, respectively. Furthermore, the spatio-temporal transformer ageing\nmodel is inferred, aiding transformer health management decision-making and\nproviding insights into localized thermal ageing phenomena in the transformer\ninsulation. Results are validated with a distribution transformer operated on a\nfloating photovoltaic power plant.\n", "link": "http://arxiv.org/abs/2405.06443v1", "date": "2024-05-10", "relevancy": 1.4189, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5298}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual-based%20Attention%20Physics-informed%20Neural%20Networks%20for%20Efficient%0A%20%20Spatio-Temporal%20Lifetime%20Assessment%20of%20Transformers%20Operated%20in%20Renewable%0A%20%20Power%20Plants&body=Title%3A%20Residual-based%20Attention%20Physics-informed%20Neural%20Networks%20for%20Efficient%0A%20%20Spatio-Temporal%20Lifetime%20Assessment%20of%20Transformers%20Operated%20in%20Renewable%0A%20%20Power%20Plants%0AAuthor%3A%20Ibai%20Ramirez%20and%20Joel%20Pino%20and%20David%20Pardo%20and%20Mikel%20Sanz%20and%20Luis%20del%20Rio%20and%20Alvaro%20Ortiz%20and%20Kateryna%20Morozovska%20and%20Jose%20I.%20Aizpurua%0AAbstract%3A%20%20%20Transformers%20are%20vital%20assets%20for%20the%20reliable%20and%20efficient%20operation%20of%0Apower%20and%20energy%20systems.%20They%20support%20the%20integration%20of%20renewables%20to%20the%0Agrid%20through%20improved%20grid%20stability%20and%20operation%20efficiency.%20Monitoring%20the%0Ahealth%20of%20transformers%20is%20essential%20to%20ensure%20grid%20reliability%20and%20efficiency.%0AThermal%20insulation%20ageing%20is%20a%20key%20transformer%20failure%20mode%2C%20which%20is%20generally%0Atracked%20by%20monitoring%20the%20hotspot%20temperature%20%28HST%29.%20However%2C%20HST%20measurement%0Ais%20complex%20and%20expensive%20and%20often%20estimated%20from%20indirect%20measurements.%0AExisting%20computationally-efficient%20HST%20models%20focus%20on%20space-agnostic%20thermal%0Amodels%2C%20providing%20worst-case%20HST%20estimates.%20This%20article%20introduces%20an%0Aefficient%20spatio-temporal%20model%20for%20transformer%20winding%20temperature%20and%20ageing%0Aestimation%2C%20which%20leverages%20physics-based%20partial%20differential%20equations%20%28PDEs%29%0Awith%20data-driven%20Neural%20Networks%20%28NN%29%20in%20a%20Physics%20Informed%20Neural%20Networks%0A%28PINNs%29%20configuration%20to%20improve%20prediction%20accuracy%20and%20acquire%0Aspatio-temporal%20resolution.%20The%20computational%20efficiency%20of%20the%20PINN%20model%20is%0Aimproved%20through%20the%20implementation%20of%20the%20Residual-Based%20Attention%20scheme%20that%0Aaccelerates%20the%20PINN%20model%20convergence.%20PINN%20based%20oil%20temperature%20predictions%0Aare%20used%20to%20estimate%20spatio-temporal%20transformer%20winding%20temperature%20values%2C%0Awhich%20are%20validated%20through%20PDE%20resolution%20models%20and%20fiber%20optic%20sensor%0Ameasurements%2C%20respectively.%20Furthermore%2C%20the%20spatio-temporal%20transformer%20ageing%0Amodel%20is%20inferred%2C%20aiding%20transformer%20health%20management%20decision-making%20and%0Aproviding%20insights%20into%20localized%20thermal%20ageing%20phenomena%20in%20the%20transformer%0Ainsulation.%20Results%20are%20validated%20with%20a%20distribution%20transformer%20operated%20on%20a%0Afloating%20photovoltaic%20power%20plant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual-based%2520Attention%2520Physics-informed%2520Neural%2520Networks%2520for%2520Efficient%250A%2520%2520Spatio-Temporal%2520Lifetime%2520Assessment%2520of%2520Transformers%2520Operated%2520in%2520Renewable%250A%2520%2520Power%2520Plants%26entry.906535625%3DIbai%2520Ramirez%2520and%2520Joel%2520Pino%2520and%2520David%2520Pardo%2520and%2520Mikel%2520Sanz%2520and%2520Luis%2520del%2520Rio%2520and%2520Alvaro%2520Ortiz%2520and%2520Kateryna%2520Morozovska%2520and%2520Jose%2520I.%2520Aizpurua%26entry.1292438233%3D%2520%2520Transformers%2520are%2520vital%2520assets%2520for%2520the%2520reliable%2520and%2520efficient%2520operation%2520of%250Apower%2520and%2520energy%2520systems.%2520They%2520support%2520the%2520integration%2520of%2520renewables%2520to%2520the%250Agrid%2520through%2520improved%2520grid%2520stability%2520and%2520operation%2520efficiency.%2520Monitoring%2520the%250Ahealth%2520of%2520transformers%2520is%2520essential%2520to%2520ensure%2520grid%2520reliability%2520and%2520efficiency.%250AThermal%2520insulation%2520ageing%2520is%2520a%2520key%2520transformer%2520failure%2520mode%252C%2520which%2520is%2520generally%250Atracked%2520by%2520monitoring%2520the%2520hotspot%2520temperature%2520%2528HST%2529.%2520However%252C%2520HST%2520measurement%250Ais%2520complex%2520and%2520expensive%2520and%2520often%2520estimated%2520from%2520indirect%2520measurements.%250AExisting%2520computationally-efficient%2520HST%2520models%2520focus%2520on%2520space-agnostic%2520thermal%250Amodels%252C%2520providing%2520worst-case%2520HST%2520estimates.%2520This%2520article%2520introduces%2520an%250Aefficient%2520spatio-temporal%2520model%2520for%2520transformer%2520winding%2520temperature%2520and%2520ageing%250Aestimation%252C%2520which%2520leverages%2520physics-based%2520partial%2520differential%2520equations%2520%2528PDEs%2529%250Awith%2520data-driven%2520Neural%2520Networks%2520%2528NN%2529%2520in%2520a%2520Physics%2520Informed%2520Neural%2520Networks%250A%2528PINNs%2529%2520configuration%2520to%2520improve%2520prediction%2520accuracy%2520and%2520acquire%250Aspatio-temporal%2520resolution.%2520The%2520computational%2520efficiency%2520of%2520the%2520PINN%2520model%2520is%250Aimproved%2520through%2520the%2520implementation%2520of%2520the%2520Residual-Based%2520Attention%2520scheme%2520that%250Aaccelerates%2520the%2520PINN%2520model%2520convergence.%2520PINN%2520based%2520oil%2520temperature%2520predictions%250Aare%2520used%2520to%2520estimate%2520spatio-temporal%2520transformer%2520winding%2520temperature%2520values%252C%250Awhich%2520are%2520validated%2520through%2520PDE%2520resolution%2520models%2520and%2520fiber%2520optic%2520sensor%250Ameasurements%252C%2520respectively.%2520Furthermore%252C%2520the%2520spatio-temporal%2520transformer%2520ageing%250Amodel%2520is%2520inferred%252C%2520aiding%2520transformer%2520health%2520management%2520decision-making%2520and%250Aproviding%2520insights%2520into%2520localized%2520thermal%2520ageing%2520phenomena%2520in%2520the%2520transformer%250Ainsulation.%2520Results%2520are%2520validated%2520with%2520a%2520distribution%2520transformer%2520operated%2520on%2520a%250Afloating%2520photovoltaic%2520power%2520plant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual-based%20Attention%20Physics-informed%20Neural%20Networks%20for%20Efficient%0A%20%20Spatio-Temporal%20Lifetime%20Assessment%20of%20Transformers%20Operated%20in%20Renewable%0A%20%20Power%20Plants&entry.906535625=Ibai%20Ramirez%20and%20Joel%20Pino%20and%20David%20Pardo%20and%20Mikel%20Sanz%20and%20Luis%20del%20Rio%20and%20Alvaro%20Ortiz%20and%20Kateryna%20Morozovska%20and%20Jose%20I.%20Aizpurua&entry.1292438233=%20%20Transformers%20are%20vital%20assets%20for%20the%20reliable%20and%20efficient%20operation%20of%0Apower%20and%20energy%20systems.%20They%20support%20the%20integration%20of%20renewables%20to%20the%0Agrid%20through%20improved%20grid%20stability%20and%20operation%20efficiency.%20Monitoring%20the%0Ahealth%20of%20transformers%20is%20essential%20to%20ensure%20grid%20reliability%20and%20efficiency.%0AThermal%20insulation%20ageing%20is%20a%20key%20transformer%20failure%20mode%2C%20which%20is%20generally%0Atracked%20by%20monitoring%20the%20hotspot%20temperature%20%28HST%29.%20However%2C%20HST%20measurement%0Ais%20complex%20and%20expensive%20and%20often%20estimated%20from%20indirect%20measurements.%0AExisting%20computationally-efficient%20HST%20models%20focus%20on%20space-agnostic%20thermal%0Amodels%2C%20providing%20worst-case%20HST%20estimates.%20This%20article%20introduces%20an%0Aefficient%20spatio-temporal%20model%20for%20transformer%20winding%20temperature%20and%20ageing%0Aestimation%2C%20which%20leverages%20physics-based%20partial%20differential%20equations%20%28PDEs%29%0Awith%20data-driven%20Neural%20Networks%20%28NN%29%20in%20a%20Physics%20Informed%20Neural%20Networks%0A%28PINNs%29%20configuration%20to%20improve%20prediction%20accuracy%20and%20acquire%0Aspatio-temporal%20resolution.%20The%20computational%20efficiency%20of%20the%20PINN%20model%20is%0Aimproved%20through%20the%20implementation%20of%20the%20Residual-Based%20Attention%20scheme%20that%0Aaccelerates%20the%20PINN%20model%20convergence.%20PINN%20based%20oil%20temperature%20predictions%0Aare%20used%20to%20estimate%20spatio-temporal%20transformer%20winding%20temperature%20values%2C%0Awhich%20are%20validated%20through%20PDE%20resolution%20models%20and%20fiber%20optic%20sensor%0Ameasurements%2C%20respectively.%20Furthermore%2C%20the%20spatio-temporal%20transformer%20ageing%0Amodel%20is%20inferred%2C%20aiding%20transformer%20health%20management%20decision-making%20and%0Aproviding%20insights%20into%20localized%20thermal%20ageing%20phenomena%20in%20the%20transformer%0Ainsulation.%20Results%20are%20validated%20with%20a%20distribution%20transformer%20operated%20on%20a%0Afloating%20photovoltaic%20power%20plant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06443v1&entry.124074799=Read"},
{"title": "Are EEG-to-Text Models Working?", "author": "Hyejeong Jo and Yiqian Yang and Juhyeok Han and Yiqun Duan and Hui Xiong and Won Hee Lee", "abstract": "  This work critically analyzes existing models for open-vocabulary EEG-to-Text\ntranslation. We identify a crucial limitation: previous studies often employed\nimplicit teacher-forcing during evaluation, artificially inflating performance\nmetrics. Additionally, they lacked a critical benchmark - comparing model\nperformance on pure noise inputs. We propose a methodology to differentiate\nbetween models that truly learn from EEG signals and those that simply memorize\ntraining data. Our analysis reveals that model performance on noise data can be\ncomparable to that on EEG data. These findings highlight the need for stricter\nevaluation practices in EEG-to-Text research, emphasizing transparent reporting\nand rigorous benchmarking with noise inputs. This approach will lead to more\nreliable assessments of model capabilities and pave the way for robust\nEEG-to-Text communication systems.\n", "link": "http://arxiv.org/abs/2405.06459v1", "date": "2024-05-10", "relevancy": 1.4162, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5463}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4522}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20EEG-to-Text%20Models%20Working%3F&body=Title%3A%20Are%20EEG-to-Text%20Models%20Working%3F%0AAuthor%3A%20Hyejeong%20Jo%20and%20Yiqian%20Yang%20and%20Juhyeok%20Han%20and%20Yiqun%20Duan%20and%20Hui%20Xiong%20and%20Won%20Hee%20Lee%0AAbstract%3A%20%20%20This%20work%20critically%20analyzes%20existing%20models%20for%20open-vocabulary%20EEG-to-Text%0Atranslation.%20We%20identify%20a%20crucial%20limitation%3A%20previous%20studies%20often%20employed%0Aimplicit%20teacher-forcing%20during%20evaluation%2C%20artificially%20inflating%20performance%0Ametrics.%20Additionally%2C%20they%20lacked%20a%20critical%20benchmark%20-%20comparing%20model%0Aperformance%20on%20pure%20noise%20inputs.%20We%20propose%20a%20methodology%20to%20differentiate%0Abetween%20models%20that%20truly%20learn%20from%20EEG%20signals%20and%20those%20that%20simply%20memorize%0Atraining%20data.%20Our%20analysis%20reveals%20that%20model%20performance%20on%20noise%20data%20can%20be%0Acomparable%20to%20that%20on%20EEG%20data.%20These%20findings%20highlight%20the%20need%20for%20stricter%0Aevaluation%20practices%20in%20EEG-to-Text%20research%2C%20emphasizing%20transparent%20reporting%0Aand%20rigorous%20benchmarking%20with%20noise%20inputs.%20This%20approach%20will%20lead%20to%20more%0Areliable%20assessments%20of%20model%20capabilities%20and%20pave%20the%20way%20for%20robust%0AEEG-to-Text%20communication%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520EEG-to-Text%2520Models%2520Working%253F%26entry.906535625%3DHyejeong%2520Jo%2520and%2520Yiqian%2520Yang%2520and%2520Juhyeok%2520Han%2520and%2520Yiqun%2520Duan%2520and%2520Hui%2520Xiong%2520and%2520Won%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520This%2520work%2520critically%2520analyzes%2520existing%2520models%2520for%2520open-vocabulary%2520EEG-to-Text%250Atranslation.%2520We%2520identify%2520a%2520crucial%2520limitation%253A%2520previous%2520studies%2520often%2520employed%250Aimplicit%2520teacher-forcing%2520during%2520evaluation%252C%2520artificially%2520inflating%2520performance%250Ametrics.%2520Additionally%252C%2520they%2520lacked%2520a%2520critical%2520benchmark%2520-%2520comparing%2520model%250Aperformance%2520on%2520pure%2520noise%2520inputs.%2520We%2520propose%2520a%2520methodology%2520to%2520differentiate%250Abetween%2520models%2520that%2520truly%2520learn%2520from%2520EEG%2520signals%2520and%2520those%2520that%2520simply%2520memorize%250Atraining%2520data.%2520Our%2520analysis%2520reveals%2520that%2520model%2520performance%2520on%2520noise%2520data%2520can%2520be%250Acomparable%2520to%2520that%2520on%2520EEG%2520data.%2520These%2520findings%2520highlight%2520the%2520need%2520for%2520stricter%250Aevaluation%2520practices%2520in%2520EEG-to-Text%2520research%252C%2520emphasizing%2520transparent%2520reporting%250Aand%2520rigorous%2520benchmarking%2520with%2520noise%2520inputs.%2520This%2520approach%2520will%2520lead%2520to%2520more%250Areliable%2520assessments%2520of%2520model%2520capabilities%2520and%2520pave%2520the%2520way%2520for%2520robust%250AEEG-to-Text%2520communication%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20EEG-to-Text%20Models%20Working%3F&entry.906535625=Hyejeong%20Jo%20and%20Yiqian%20Yang%20and%20Juhyeok%20Han%20and%20Yiqun%20Duan%20and%20Hui%20Xiong%20and%20Won%20Hee%20Lee&entry.1292438233=%20%20This%20work%20critically%20analyzes%20existing%20models%20for%20open-vocabulary%20EEG-to-Text%0Atranslation.%20We%20identify%20a%20crucial%20limitation%3A%20previous%20studies%20often%20employed%0Aimplicit%20teacher-forcing%20during%20evaluation%2C%20artificially%20inflating%20performance%0Ametrics.%20Additionally%2C%20they%20lacked%20a%20critical%20benchmark%20-%20comparing%20model%0Aperformance%20on%20pure%20noise%20inputs.%20We%20propose%20a%20methodology%20to%20differentiate%0Abetween%20models%20that%20truly%20learn%20from%20EEG%20signals%20and%20those%20that%20simply%20memorize%0Atraining%20data.%20Our%20analysis%20reveals%20that%20model%20performance%20on%20noise%20data%20can%20be%0Acomparable%20to%20that%20on%20EEG%20data.%20These%20findings%20highlight%20the%20need%20for%20stricter%0Aevaluation%20practices%20in%20EEG-to-Text%20research%2C%20emphasizing%20transparent%20reporting%0Aand%20rigorous%20benchmarking%20with%20noise%20inputs.%20This%20approach%20will%20lead%20to%20more%0Areliable%20assessments%20of%20model%20capabilities%20and%20pave%20the%20way%20for%20robust%0AEEG-to-Text%20communication%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06459v1&entry.124074799=Read"},
{"title": "Program Synthesis using Inductive Logic Programming for the Abstraction\n  and Reasoning Corpus", "author": "Filipe Marinho Rocha and In\u00eas Dutra and V\u00edtor Santos Costa", "abstract": "  The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.\n", "link": "http://arxiv.org/abs/2405.06399v1", "date": "2024-05-10", "relevancy": 1.405, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5455}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Program%20Synthesis%20using%20Inductive%20Logic%20Programming%20for%20the%20Abstraction%0A%20%20and%20Reasoning%20Corpus&body=Title%3A%20Program%20Synthesis%20using%20Inductive%20Logic%20Programming%20for%20the%20Abstraction%0A%20%20and%20Reasoning%20Corpus%0AAuthor%3A%20Filipe%20Marinho%20Rocha%20and%20In%C3%AAs%20Dutra%20and%20V%C3%ADtor%20Santos%20Costa%0AAbstract%3A%20%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20is%20a%20general%20artificial%0Aintelligence%20benchmark%20that%20is%20currently%20unsolvable%20by%20any%20Machine%20Learning%0Amethod%2C%20including%20Large%20Language%20Models%20%28LLMs%29.%20It%20demands%20strong%0Ageneralization%20and%20reasoning%20capabilities%20which%20are%20known%20to%20be%20weaknesses%20of%0ANeural%20Network%20based%20systems.%20In%20this%20work%2C%20we%20propose%20a%20Program%20Synthesis%0Asystem%20that%20uses%20Inductive%20Logic%20Programming%20%28ILP%29%2C%20a%20branch%20of%20Symbolic%20AI%2C%20to%0Asolve%20ARC.%20We%20have%20manually%20defined%20a%20simple%20Domain%20Specific%20Language%20%28DSL%29%0Athat%20corresponds%20to%20a%20small%20set%20of%20object-centric%20abstractions%20relevant%20to%20ARC.%0AThis%20is%20the%20Background%20Knowledge%20used%20by%20ILP%20to%20create%20Logic%20Programs%20that%0Aprovide%20reasoning%20capabilities%20to%20our%20system.%20The%20full%20system%20is%20capable%20of%0Ageneralize%20to%20unseen%20tasks%2C%20since%20ILP%20can%20create%20Logic%20Program%28s%29%20from%20few%0Aexamples%2C%20in%20the%20case%20of%20ARC%3A%20pairs%20of%20Input-Output%20grids%20examples%20for%20each%0Atask.%20These%20Logic%20Programs%20are%20able%20to%20generate%20Objects%20present%20in%20the%20Output%0Agrid%20and%20the%20combination%20of%20these%20can%20form%20a%20complete%20program%20that%20transforms%0Aan%20Input%20grid%20into%20an%20Output%20grid.%20We%20randomly%20chose%20some%20tasks%20from%20ARC%20that%0Adont%20require%20more%20than%20the%20small%20number%20of%20the%20Object%20primitives%20we%20implemented%0Aand%20show%20that%20given%20only%20these%2C%20our%20system%20can%20solve%20tasks%20that%20require%20each%2C%0Asuch%20different%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgram%2520Synthesis%2520using%2520Inductive%2520Logic%2520Programming%2520for%2520the%2520Abstraction%250A%2520%2520and%2520Reasoning%2520Corpus%26entry.906535625%3DFilipe%2520Marinho%2520Rocha%2520and%2520In%25C3%25AAs%2520Dutra%2520and%2520V%25C3%25ADtor%2520Santos%2520Costa%26entry.1292438233%3D%2520%2520The%2520Abstraction%2520and%2520Reasoning%2520Corpus%2520%2528ARC%2529%2520is%2520a%2520general%2520artificial%250Aintelligence%2520benchmark%2520that%2520is%2520currently%2520unsolvable%2520by%2520any%2520Machine%2520Learning%250Amethod%252C%2520including%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520It%2520demands%2520strong%250Ageneralization%2520and%2520reasoning%2520capabilities%2520which%2520are%2520known%2520to%2520be%2520weaknesses%2520of%250ANeural%2520Network%2520based%2520systems.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Program%2520Synthesis%250Asystem%2520that%2520uses%2520Inductive%2520Logic%2520Programming%2520%2528ILP%2529%252C%2520a%2520branch%2520of%2520Symbolic%2520AI%252C%2520to%250Asolve%2520ARC.%2520We%2520have%2520manually%2520defined%2520a%2520simple%2520Domain%2520Specific%2520Language%2520%2528DSL%2529%250Athat%2520corresponds%2520to%2520a%2520small%2520set%2520of%2520object-centric%2520abstractions%2520relevant%2520to%2520ARC.%250AThis%2520is%2520the%2520Background%2520Knowledge%2520used%2520by%2520ILP%2520to%2520create%2520Logic%2520Programs%2520that%250Aprovide%2520reasoning%2520capabilities%2520to%2520our%2520system.%2520The%2520full%2520system%2520is%2520capable%2520of%250Ageneralize%2520to%2520unseen%2520tasks%252C%2520since%2520ILP%2520can%2520create%2520Logic%2520Program%2528s%2529%2520from%2520few%250Aexamples%252C%2520in%2520the%2520case%2520of%2520ARC%253A%2520pairs%2520of%2520Input-Output%2520grids%2520examples%2520for%2520each%250Atask.%2520These%2520Logic%2520Programs%2520are%2520able%2520to%2520generate%2520Objects%2520present%2520in%2520the%2520Output%250Agrid%2520and%2520the%2520combination%2520of%2520these%2520can%2520form%2520a%2520complete%2520program%2520that%2520transforms%250Aan%2520Input%2520grid%2520into%2520an%2520Output%2520grid.%2520We%2520randomly%2520chose%2520some%2520tasks%2520from%2520ARC%2520that%250Adont%2520require%2520more%2520than%2520the%2520small%2520number%2520of%2520the%2520Object%2520primitives%2520we%2520implemented%250Aand%2520show%2520that%2520given%2520only%2520these%252C%2520our%2520system%2520can%2520solve%2520tasks%2520that%2520require%2520each%252C%250Asuch%2520different%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Program%20Synthesis%20using%20Inductive%20Logic%20Programming%20for%20the%20Abstraction%0A%20%20and%20Reasoning%20Corpus&entry.906535625=Filipe%20Marinho%20Rocha%20and%20In%C3%AAs%20Dutra%20and%20V%C3%ADtor%20Santos%20Costa&entry.1292438233=%20%20The%20Abstraction%20and%20Reasoning%20Corpus%20%28ARC%29%20is%20a%20general%20artificial%0Aintelligence%20benchmark%20that%20is%20currently%20unsolvable%20by%20any%20Machine%20Learning%0Amethod%2C%20including%20Large%20Language%20Models%20%28LLMs%29.%20It%20demands%20strong%0Ageneralization%20and%20reasoning%20capabilities%20which%20are%20known%20to%20be%20weaknesses%20of%0ANeural%20Network%20based%20systems.%20In%20this%20work%2C%20we%20propose%20a%20Program%20Synthesis%0Asystem%20that%20uses%20Inductive%20Logic%20Programming%20%28ILP%29%2C%20a%20branch%20of%20Symbolic%20AI%2C%20to%0Asolve%20ARC.%20We%20have%20manually%20defined%20a%20simple%20Domain%20Specific%20Language%20%28DSL%29%0Athat%20corresponds%20to%20a%20small%20set%20of%20object-centric%20abstractions%20relevant%20to%20ARC.%0AThis%20is%20the%20Background%20Knowledge%20used%20by%20ILP%20to%20create%20Logic%20Programs%20that%0Aprovide%20reasoning%20capabilities%20to%20our%20system.%20The%20full%20system%20is%20capable%20of%0Ageneralize%20to%20unseen%20tasks%2C%20since%20ILP%20can%20create%20Logic%20Program%28s%29%20from%20few%0Aexamples%2C%20in%20the%20case%20of%20ARC%3A%20pairs%20of%20Input-Output%20grids%20examples%20for%20each%0Atask.%20These%20Logic%20Programs%20are%20able%20to%20generate%20Objects%20present%20in%20the%20Output%0Agrid%20and%20the%20combination%20of%20these%20can%20form%20a%20complete%20program%20that%20transforms%0Aan%20Input%20grid%20into%20an%20Output%20grid.%20We%20randomly%20chose%20some%20tasks%20from%20ARC%20that%0Adont%20require%20more%20than%20the%20small%20number%20of%20the%20Object%20primitives%20we%20implemented%0Aand%20show%20that%20given%20only%20these%2C%20our%20system%20can%20solve%20tasks%20that%20require%20each%2C%0Asuch%20different%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06399v1&entry.124074799=Read"},
{"title": "Robotic Stroke Motion Following the Shape of the Human Back: Motion\n  Generation and Psychological Effects", "author": "Akishige Yuguchi and Tomoki Ishikura and Sung-Gwi Cho and Jun Takamatsu and Tsukasa Ogasawara", "abstract": "  In this study, to perform the robotic stroke motions following the shape of\nthe human back similar to the stroke motions by humans, in contrast to the\nconventional robotic stroke motion with a linear trajectory, we propose a\ntrajectory generation method for a robotic stroke motion following the shape of\nthe human back. We confirmed that the accuracy of the method's trajectory was\nclose to that of the actual stroking motion by a human. Furthermore, we\nconducted a subjective experiment to evaluate the psychological effects of the\nproposed stroke motion in contrast to those of the conventional stroke motion\nwith a linear trajectory. The experimental results showed that the actual\nstroke motion following the shape of the human back tended to evoke more\npleasant and active feelings than the conventional stroke motion.\n", "link": "http://arxiv.org/abs/2405.06588v1", "date": "2024-05-10", "relevancy": 1.403, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4754}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4583}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Stroke%20Motion%20Following%20the%20Shape%20of%20the%20Human%20Back%3A%20Motion%0A%20%20Generation%20and%20Psychological%20Effects&body=Title%3A%20Robotic%20Stroke%20Motion%20Following%20the%20Shape%20of%20the%20Human%20Back%3A%20Motion%0A%20%20Generation%20and%20Psychological%20Effects%0AAuthor%3A%20Akishige%20Yuguchi%20and%20Tomoki%20Ishikura%20and%20Sung-Gwi%20Cho%20and%20Jun%20Takamatsu%20and%20Tsukasa%20Ogasawara%0AAbstract%3A%20%20%20In%20this%20study%2C%20to%20perform%20the%20robotic%20stroke%20motions%20following%20the%20shape%20of%0Athe%20human%20back%20similar%20to%20the%20stroke%20motions%20by%20humans%2C%20in%20contrast%20to%20the%0Aconventional%20robotic%20stroke%20motion%20with%20a%20linear%20trajectory%2C%20we%20propose%20a%0Atrajectory%20generation%20method%20for%20a%20robotic%20stroke%20motion%20following%20the%20shape%20of%0Athe%20human%20back.%20We%20confirmed%20that%20the%20accuracy%20of%20the%20method%27s%20trajectory%20was%0Aclose%20to%20that%20of%20the%20actual%20stroking%20motion%20by%20a%20human.%20Furthermore%2C%20we%0Aconducted%20a%20subjective%20experiment%20to%20evaluate%20the%20psychological%20effects%20of%20the%0Aproposed%20stroke%20motion%20in%20contrast%20to%20those%20of%20the%20conventional%20stroke%20motion%0Awith%20a%20linear%20trajectory.%20The%20experimental%20results%20showed%20that%20the%20actual%0Astroke%20motion%20following%20the%20shape%20of%20the%20human%20back%20tended%20to%20evoke%20more%0Apleasant%20and%20active%20feelings%20than%20the%20conventional%20stroke%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Stroke%2520Motion%2520Following%2520the%2520Shape%2520of%2520the%2520Human%2520Back%253A%2520Motion%250A%2520%2520Generation%2520and%2520Psychological%2520Effects%26entry.906535625%3DAkishige%2520Yuguchi%2520and%2520Tomoki%2520Ishikura%2520and%2520Sung-Gwi%2520Cho%2520and%2520Jun%2520Takamatsu%2520and%2520Tsukasa%2520Ogasawara%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520to%2520perform%2520the%2520robotic%2520stroke%2520motions%2520following%2520the%2520shape%2520of%250Athe%2520human%2520back%2520similar%2520to%2520the%2520stroke%2520motions%2520by%2520humans%252C%2520in%2520contrast%2520to%2520the%250Aconventional%2520robotic%2520stroke%2520motion%2520with%2520a%2520linear%2520trajectory%252C%2520we%2520propose%2520a%250Atrajectory%2520generation%2520method%2520for%2520a%2520robotic%2520stroke%2520motion%2520following%2520the%2520shape%2520of%250Athe%2520human%2520back.%2520We%2520confirmed%2520that%2520the%2520accuracy%2520of%2520the%2520method%2527s%2520trajectory%2520was%250Aclose%2520to%2520that%2520of%2520the%2520actual%2520stroking%2520motion%2520by%2520a%2520human.%2520Furthermore%252C%2520we%250Aconducted%2520a%2520subjective%2520experiment%2520to%2520evaluate%2520the%2520psychological%2520effects%2520of%2520the%250Aproposed%2520stroke%2520motion%2520in%2520contrast%2520to%2520those%2520of%2520the%2520conventional%2520stroke%2520motion%250Awith%2520a%2520linear%2520trajectory.%2520The%2520experimental%2520results%2520showed%2520that%2520the%2520actual%250Astroke%2520motion%2520following%2520the%2520shape%2520of%2520the%2520human%2520back%2520tended%2520to%2520evoke%2520more%250Apleasant%2520and%2520active%2520feelings%2520than%2520the%2520conventional%2520stroke%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Stroke%20Motion%20Following%20the%20Shape%20of%20the%20Human%20Back%3A%20Motion%0A%20%20Generation%20and%20Psychological%20Effects&entry.906535625=Akishige%20Yuguchi%20and%20Tomoki%20Ishikura%20and%20Sung-Gwi%20Cho%20and%20Jun%20Takamatsu%20and%20Tsukasa%20Ogasawara&entry.1292438233=%20%20In%20this%20study%2C%20to%20perform%20the%20robotic%20stroke%20motions%20following%20the%20shape%20of%0Athe%20human%20back%20similar%20to%20the%20stroke%20motions%20by%20humans%2C%20in%20contrast%20to%20the%0Aconventional%20robotic%20stroke%20motion%20with%20a%20linear%20trajectory%2C%20we%20propose%20a%0Atrajectory%20generation%20method%20for%20a%20robotic%20stroke%20motion%20following%20the%20shape%20of%0Athe%20human%20back.%20We%20confirmed%20that%20the%20accuracy%20of%20the%20method%27s%20trajectory%20was%0Aclose%20to%20that%20of%20the%20actual%20stroking%20motion%20by%20a%20human.%20Furthermore%2C%20we%0Aconducted%20a%20subjective%20experiment%20to%20evaluate%20the%20psychological%20effects%20of%20the%0Aproposed%20stroke%20motion%20in%20contrast%20to%20those%20of%20the%20conventional%20stroke%20motion%0Awith%20a%20linear%20trajectory.%20The%20experimental%20results%20showed%20that%20the%20actual%0Astroke%20motion%20following%20the%20shape%20of%20the%20human%20back%20tended%20to%20evoke%20more%0Apleasant%20and%20active%20feelings%20than%20the%20conventional%20stroke%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06588v1&entry.124074799=Read"},
{"title": "Single-seed generation of Brownian paths and integrals for adaptive and\n  high order SDE solvers", "author": "Andra\u017e Jelin\u010di\u010d and James Foster and Patrick Kidger", "abstract": "  Despite the success of adaptive time-stepping in ODE simulation, it has so\nfar seen few applications for Stochastic Differential Equations (SDEs). To\nsimulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have\nbeen developed, which can generate Brownian motion (BM) non-chronologically.\nHowever, in most applications, knowing only the values of Brownian motion is\nnot enough to achieve a high order of convergence; for that, we must compute\ntime-integrals of BM such as $\\int_s^t W_r \\, dr$. With the aim of using high\norder SDE solvers adaptively, we extend the VBT to generate these integrals of\nBM in addition to the Brownian increments. A JAX-based implementation of our\nconstruction is included in the popular Diffrax library\n(https://github.com/patrick-kidger/diffrax).\n  Since the entire Brownian path produced by VBT is uniquely determined by a\nsingle PRNG seed, previously generated samples need not be stored, which\nresults in a constant memory footprint and enables experiment repeatability and\nstrong error estimation. Based on binary search, the VBT's time complexity is\nlogarithmic in the tolerance parameter $\\varepsilon$. Unlike the original VBT\nalgorithm, which was only precise at some dyadic times, we prove that our\nconstruction exactly matches the joint distribution of the Brownian motion and\nits time integrals at any query times, provided they are at least $\\varepsilon$\napart.\n  We present two applications of adaptive high order solvers enabled by our new\nVBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve\nmore than twice the convergence order of constant stepping. We apply an\nadaptive third order underdamped or kinetic Langevin solver to an MCMC problem,\nwhere our approach outperforms the No U-Turn Sampler, while using only a tenth\nof its function evaluations.\n", "link": "http://arxiv.org/abs/2405.06464v1", "date": "2024-05-10", "relevancy": 1.4023, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4797}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-seed%20generation%20of%20Brownian%20paths%20and%20integrals%20for%20adaptive%20and%0A%20%20high%20order%20SDE%20solvers&body=Title%3A%20Single-seed%20generation%20of%20Brownian%20paths%20and%20integrals%20for%20adaptive%20and%0A%20%20high%20order%20SDE%20solvers%0AAuthor%3A%20Andra%C5%BE%20Jelin%C4%8Di%C4%8D%20and%20James%20Foster%20and%20Patrick%20Kidger%0AAbstract%3A%20%20%20Despite%20the%20success%20of%20adaptive%20time-stepping%20in%20ODE%20simulation%2C%20it%20has%20so%0Afar%20seen%20few%20applications%20for%20Stochastic%20Differential%20Equations%20%28SDEs%29.%20To%0Asimulate%20SDEs%20adaptively%2C%20methods%20such%20as%20the%20Virtual%20Brownian%20Tree%20%28VBT%29%20have%0Abeen%20developed%2C%20which%20can%20generate%20Brownian%20motion%20%28BM%29%20non-chronologically.%0AHowever%2C%20in%20most%20applications%2C%20knowing%20only%20the%20values%20of%20Brownian%20motion%20is%0Anot%20enough%20to%20achieve%20a%20high%20order%20of%20convergence%3B%20for%20that%2C%20we%20must%20compute%0Atime-integrals%20of%20BM%20such%20as%20%24%5Cint_s%5Et%20W_r%20%5C%2C%20dr%24.%20With%20the%20aim%20of%20using%20high%0Aorder%20SDE%20solvers%20adaptively%2C%20we%20extend%20the%20VBT%20to%20generate%20these%20integrals%20of%0ABM%20in%20addition%20to%20the%20Brownian%20increments.%20A%20JAX-based%20implementation%20of%20our%0Aconstruction%20is%20included%20in%20the%20popular%20Diffrax%20library%0A%28https%3A//github.com/patrick-kidger/diffrax%29.%0A%20%20Since%20the%20entire%20Brownian%20path%20produced%20by%20VBT%20is%20uniquely%20determined%20by%20a%0Asingle%20PRNG%20seed%2C%20previously%20generated%20samples%20need%20not%20be%20stored%2C%20which%0Aresults%20in%20a%20constant%20memory%20footprint%20and%20enables%20experiment%20repeatability%20and%0Astrong%20error%20estimation.%20Based%20on%20binary%20search%2C%20the%20VBT%27s%20time%20complexity%20is%0Alogarithmic%20in%20the%20tolerance%20parameter%20%24%5Cvarepsilon%24.%20Unlike%20the%20original%20VBT%0Aalgorithm%2C%20which%20was%20only%20precise%20at%20some%20dyadic%20times%2C%20we%20prove%20that%20our%0Aconstruction%20exactly%20matches%20the%20joint%20distribution%20of%20the%20Brownian%20motion%20and%0Aits%20time%20integrals%20at%20any%20query%20times%2C%20provided%20they%20are%20at%20least%20%24%5Cvarepsilon%24%0Aapart.%0A%20%20We%20present%20two%20applications%20of%20adaptive%20high%20order%20solvers%20enabled%20by%20our%20new%0AVBT.%20Using%20adaptive%20solvers%20to%20simulate%20a%20high-volatility%20CIR%20model%2C%20we%20achieve%0Amore%20than%20twice%20the%20convergence%20order%20of%20constant%20stepping.%20We%20apply%20an%0Aadaptive%20third%20order%20underdamped%20or%20kinetic%20Langevin%20solver%20to%20an%20MCMC%20problem%2C%0Awhere%20our%20approach%20outperforms%20the%20No%20U-Turn%20Sampler%2C%20while%20using%20only%20a%20tenth%0Aof%20its%20function%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-seed%2520generation%2520of%2520Brownian%2520paths%2520and%2520integrals%2520for%2520adaptive%2520and%250A%2520%2520high%2520order%2520SDE%2520solvers%26entry.906535625%3DAndra%25C5%25BE%2520Jelin%25C4%258Di%25C4%258D%2520and%2520James%2520Foster%2520and%2520Patrick%2520Kidger%26entry.1292438233%3D%2520%2520Despite%2520the%2520success%2520of%2520adaptive%2520time-stepping%2520in%2520ODE%2520simulation%252C%2520it%2520has%2520so%250Afar%2520seen%2520few%2520applications%2520for%2520Stochastic%2520Differential%2520Equations%2520%2528SDEs%2529.%2520To%250Asimulate%2520SDEs%2520adaptively%252C%2520methods%2520such%2520as%2520the%2520Virtual%2520Brownian%2520Tree%2520%2528VBT%2529%2520have%250Abeen%2520developed%252C%2520which%2520can%2520generate%2520Brownian%2520motion%2520%2528BM%2529%2520non-chronologically.%250AHowever%252C%2520in%2520most%2520applications%252C%2520knowing%2520only%2520the%2520values%2520of%2520Brownian%2520motion%2520is%250Anot%2520enough%2520to%2520achieve%2520a%2520high%2520order%2520of%2520convergence%253B%2520for%2520that%252C%2520we%2520must%2520compute%250Atime-integrals%2520of%2520BM%2520such%2520as%2520%2524%255Cint_s%255Et%2520W_r%2520%255C%252C%2520dr%2524.%2520With%2520the%2520aim%2520of%2520using%2520high%250Aorder%2520SDE%2520solvers%2520adaptively%252C%2520we%2520extend%2520the%2520VBT%2520to%2520generate%2520these%2520integrals%2520of%250ABM%2520in%2520addition%2520to%2520the%2520Brownian%2520increments.%2520A%2520JAX-based%2520implementation%2520of%2520our%250Aconstruction%2520is%2520included%2520in%2520the%2520popular%2520Diffrax%2520library%250A%2528https%253A//github.com/patrick-kidger/diffrax%2529.%250A%2520%2520Since%2520the%2520entire%2520Brownian%2520path%2520produced%2520by%2520VBT%2520is%2520uniquely%2520determined%2520by%2520a%250Asingle%2520PRNG%2520seed%252C%2520previously%2520generated%2520samples%2520need%2520not%2520be%2520stored%252C%2520which%250Aresults%2520in%2520a%2520constant%2520memory%2520footprint%2520and%2520enables%2520experiment%2520repeatability%2520and%250Astrong%2520error%2520estimation.%2520Based%2520on%2520binary%2520search%252C%2520the%2520VBT%2527s%2520time%2520complexity%2520is%250Alogarithmic%2520in%2520the%2520tolerance%2520parameter%2520%2524%255Cvarepsilon%2524.%2520Unlike%2520the%2520original%2520VBT%250Aalgorithm%252C%2520which%2520was%2520only%2520precise%2520at%2520some%2520dyadic%2520times%252C%2520we%2520prove%2520that%2520our%250Aconstruction%2520exactly%2520matches%2520the%2520joint%2520distribution%2520of%2520the%2520Brownian%2520motion%2520and%250Aits%2520time%2520integrals%2520at%2520any%2520query%2520times%252C%2520provided%2520they%2520are%2520at%2520least%2520%2524%255Cvarepsilon%2524%250Aapart.%250A%2520%2520We%2520present%2520two%2520applications%2520of%2520adaptive%2520high%2520order%2520solvers%2520enabled%2520by%2520our%2520new%250AVBT.%2520Using%2520adaptive%2520solvers%2520to%2520simulate%2520a%2520high-volatility%2520CIR%2520model%252C%2520we%2520achieve%250Amore%2520than%2520twice%2520the%2520convergence%2520order%2520of%2520constant%2520stepping.%2520We%2520apply%2520an%250Aadaptive%2520third%2520order%2520underdamped%2520or%2520kinetic%2520Langevin%2520solver%2520to%2520an%2520MCMC%2520problem%252C%250Awhere%2520our%2520approach%2520outperforms%2520the%2520No%2520U-Turn%2520Sampler%252C%2520while%2520using%2520only%2520a%2520tenth%250Aof%2520its%2520function%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-seed%20generation%20of%20Brownian%20paths%20and%20integrals%20for%20adaptive%20and%0A%20%20high%20order%20SDE%20solvers&entry.906535625=Andra%C5%BE%20Jelin%C4%8Di%C4%8D%20and%20James%20Foster%20and%20Patrick%20Kidger&entry.1292438233=%20%20Despite%20the%20success%20of%20adaptive%20time-stepping%20in%20ODE%20simulation%2C%20it%20has%20so%0Afar%20seen%20few%20applications%20for%20Stochastic%20Differential%20Equations%20%28SDEs%29.%20To%0Asimulate%20SDEs%20adaptively%2C%20methods%20such%20as%20the%20Virtual%20Brownian%20Tree%20%28VBT%29%20have%0Abeen%20developed%2C%20which%20can%20generate%20Brownian%20motion%20%28BM%29%20non-chronologically.%0AHowever%2C%20in%20most%20applications%2C%20knowing%20only%20the%20values%20of%20Brownian%20motion%20is%0Anot%20enough%20to%20achieve%20a%20high%20order%20of%20convergence%3B%20for%20that%2C%20we%20must%20compute%0Atime-integrals%20of%20BM%20such%20as%20%24%5Cint_s%5Et%20W_r%20%5C%2C%20dr%24.%20With%20the%20aim%20of%20using%20high%0Aorder%20SDE%20solvers%20adaptively%2C%20we%20extend%20the%20VBT%20to%20generate%20these%20integrals%20of%0ABM%20in%20addition%20to%20the%20Brownian%20increments.%20A%20JAX-based%20implementation%20of%20our%0Aconstruction%20is%20included%20in%20the%20popular%20Diffrax%20library%0A%28https%3A//github.com/patrick-kidger/diffrax%29.%0A%20%20Since%20the%20entire%20Brownian%20path%20produced%20by%20VBT%20is%20uniquely%20determined%20by%20a%0Asingle%20PRNG%20seed%2C%20previously%20generated%20samples%20need%20not%20be%20stored%2C%20which%0Aresults%20in%20a%20constant%20memory%20footprint%20and%20enables%20experiment%20repeatability%20and%0Astrong%20error%20estimation.%20Based%20on%20binary%20search%2C%20the%20VBT%27s%20time%20complexity%20is%0Alogarithmic%20in%20the%20tolerance%20parameter%20%24%5Cvarepsilon%24.%20Unlike%20the%20original%20VBT%0Aalgorithm%2C%20which%20was%20only%20precise%20at%20some%20dyadic%20times%2C%20we%20prove%20that%20our%0Aconstruction%20exactly%20matches%20the%20joint%20distribution%20of%20the%20Brownian%20motion%20and%0Aits%20time%20integrals%20at%20any%20query%20times%2C%20provided%20they%20are%20at%20least%20%24%5Cvarepsilon%24%0Aapart.%0A%20%20We%20present%20two%20applications%20of%20adaptive%20high%20order%20solvers%20enabled%20by%20our%20new%0AVBT.%20Using%20adaptive%20solvers%20to%20simulate%20a%20high-volatility%20CIR%20model%2C%20we%20achieve%0Amore%20than%20twice%20the%20convergence%20order%20of%20constant%20stepping.%20We%20apply%20an%0Aadaptive%20third%20order%20underdamped%20or%20kinetic%20Langevin%20solver%20to%20an%20MCMC%20problem%2C%0Awhere%20our%20approach%20outperforms%20the%20No%20U-Turn%20Sampler%2C%20while%20using%20only%20a%20tenth%0Aof%20its%20function%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06464v1&entry.124074799=Read"},
{"title": "Time Evidence Fusion Network: Multi-source View in Long-Term Time Series\n  Forecasting", "author": "Tianxiang Zhan and Yuanpeng He and Zhen Li and Yong Deng", "abstract": "  In real-world scenarios, time series forecasting often demands timeliness,\nmaking research on model backbones a perennially hot topic. To meet these\nperformance demands, we propose a novel backbone from the perspective of\ninformation fusion. Introducing the Basic Probability Assignment (BPA) Module\nand the Time Evidence Fusion Network (TEFN), based on evidence theory, allows\nus to achieve superior performance. On the other hand, the perspective of\nmulti-source information fusion effectively improves the accuracy of\nforecasting. Due to the fact that BPA is generated by fuzzy theory, TEFN also\nhas considerable interpretability. In real data experiments, the TEFN partially\nachieved state-of-the-art, with low errors comparable to PatchTST, and\noperating efficiency surpass performance models such as Dlinear. Meanwhile,\nTEFN has high robustness and small error fluctuations in the random\nhyperparameter selection. TEFN is not a model that achieves the ultimate in\nsingle aspect, but a model that balances performance, accuracy, stability, and\ninterpretability.\n", "link": "http://arxiv.org/abs/2405.06419v1", "date": "2024-05-10", "relevancy": 1.3644, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4596}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20Evidence%20Fusion%20Network%3A%20Multi-source%20View%20in%20Long-Term%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Time%20Evidence%20Fusion%20Network%3A%20Multi-source%20View%20in%20Long-Term%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Tianxiang%20Zhan%20and%20Yuanpeng%20He%20and%20Zhen%20Li%20and%20Yong%20Deng%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20time%20series%20forecasting%20often%20demands%20timeliness%2C%0Amaking%20research%20on%20model%20backbones%20a%20perennially%20hot%20topic.%20To%20meet%20these%0Aperformance%20demands%2C%20we%20propose%20a%20novel%20backbone%20from%20the%20perspective%20of%0Ainformation%20fusion.%20Introducing%20the%20Basic%20Probability%20Assignment%20%28BPA%29%20Module%0Aand%20the%20Time%20Evidence%20Fusion%20Network%20%28TEFN%29%2C%20based%20on%20evidence%20theory%2C%20allows%0Aus%20to%20achieve%20superior%20performance.%20On%20the%20other%20hand%2C%20the%20perspective%20of%0Amulti-source%20information%20fusion%20effectively%20improves%20the%20accuracy%20of%0Aforecasting.%20Due%20to%20the%20fact%20that%20BPA%20is%20generated%20by%20fuzzy%20theory%2C%20TEFN%20also%0Ahas%20considerable%20interpretability.%20In%20real%20data%20experiments%2C%20the%20TEFN%20partially%0Aachieved%20state-of-the-art%2C%20with%20low%20errors%20comparable%20to%20PatchTST%2C%20and%0Aoperating%20efficiency%20surpass%20performance%20models%20such%20as%20Dlinear.%20Meanwhile%2C%0ATEFN%20has%20high%20robustness%20and%20small%20error%20fluctuations%20in%20the%20random%0Ahyperparameter%20selection.%20TEFN%20is%20not%20a%20model%20that%20achieves%20the%20ultimate%20in%0Asingle%20aspect%2C%20but%20a%20model%20that%20balances%20performance%2C%20accuracy%2C%20stability%2C%20and%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520Evidence%2520Fusion%2520Network%253A%2520Multi-source%2520View%2520in%2520Long-Term%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DTianxiang%2520Zhan%2520and%2520Yuanpeng%2520He%2520and%2520Zhen%2520Li%2520and%2520Yong%2520Deng%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520time%2520series%2520forecasting%2520often%2520demands%2520timeliness%252C%250Amaking%2520research%2520on%2520model%2520backbones%2520a%2520perennially%2520hot%2520topic.%2520To%2520meet%2520these%250Aperformance%2520demands%252C%2520we%2520propose%2520a%2520novel%2520backbone%2520from%2520the%2520perspective%2520of%250Ainformation%2520fusion.%2520Introducing%2520the%2520Basic%2520Probability%2520Assignment%2520%2528BPA%2529%2520Module%250Aand%2520the%2520Time%2520Evidence%2520Fusion%2520Network%2520%2528TEFN%2529%252C%2520based%2520on%2520evidence%2520theory%252C%2520allows%250Aus%2520to%2520achieve%2520superior%2520performance.%2520On%2520the%2520other%2520hand%252C%2520the%2520perspective%2520of%250Amulti-source%2520information%2520fusion%2520effectively%2520improves%2520the%2520accuracy%2520of%250Aforecasting.%2520Due%2520to%2520the%2520fact%2520that%2520BPA%2520is%2520generated%2520by%2520fuzzy%2520theory%252C%2520TEFN%2520also%250Ahas%2520considerable%2520interpretability.%2520In%2520real%2520data%2520experiments%252C%2520the%2520TEFN%2520partially%250Aachieved%2520state-of-the-art%252C%2520with%2520low%2520errors%2520comparable%2520to%2520PatchTST%252C%2520and%250Aoperating%2520efficiency%2520surpass%2520performance%2520models%2520such%2520as%2520Dlinear.%2520Meanwhile%252C%250ATEFN%2520has%2520high%2520robustness%2520and%2520small%2520error%2520fluctuations%2520in%2520the%2520random%250Ahyperparameter%2520selection.%2520TEFN%2520is%2520not%2520a%2520model%2520that%2520achieves%2520the%2520ultimate%2520in%250Asingle%2520aspect%252C%2520but%2520a%2520model%2520that%2520balances%2520performance%252C%2520accuracy%252C%2520stability%252C%2520and%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Evidence%20Fusion%20Network%3A%20Multi-source%20View%20in%20Long-Term%20Time%20Series%0A%20%20Forecasting&entry.906535625=Tianxiang%20Zhan%20and%20Yuanpeng%20He%20and%20Zhen%20Li%20and%20Yong%20Deng&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20time%20series%20forecasting%20often%20demands%20timeliness%2C%0Amaking%20research%20on%20model%20backbones%20a%20perennially%20hot%20topic.%20To%20meet%20these%0Aperformance%20demands%2C%20we%20propose%20a%20novel%20backbone%20from%20the%20perspective%20of%0Ainformation%20fusion.%20Introducing%20the%20Basic%20Probability%20Assignment%20%28BPA%29%20Module%0Aand%20the%20Time%20Evidence%20Fusion%20Network%20%28TEFN%29%2C%20based%20on%20evidence%20theory%2C%20allows%0Aus%20to%20achieve%20superior%20performance.%20On%20the%20other%20hand%2C%20the%20perspective%20of%0Amulti-source%20information%20fusion%20effectively%20improves%20the%20accuracy%20of%0Aforecasting.%20Due%20to%20the%20fact%20that%20BPA%20is%20generated%20by%20fuzzy%20theory%2C%20TEFN%20also%0Ahas%20considerable%20interpretability.%20In%20real%20data%20experiments%2C%20the%20TEFN%20partially%0Aachieved%20state-of-the-art%2C%20with%20low%20errors%20comparable%20to%20PatchTST%2C%20and%0Aoperating%20efficiency%20surpass%20performance%20models%20such%20as%20Dlinear.%20Meanwhile%2C%0ATEFN%20has%20high%20robustness%20and%20small%20error%20fluctuations%20in%20the%20random%0Ahyperparameter%20selection.%20TEFN%20is%20not%20a%20model%20that%20achieves%20the%20ultimate%20in%0Asingle%20aspect%2C%20but%20a%20model%20that%20balances%20performance%2C%20accuracy%2C%20stability%2C%20and%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06419v1&entry.124074799=Read"},
{"title": "Random matrix theory improved Fr\u00e9chet mean of symmetric positive\n  definite matrices", "author": "Florent Bouchard and Ammar Mian and Malik Tiomoko and Guillaume Ginolhac and Fr\u00e9d\u00e9ric Pascal", "abstract": "  In this study, we consider the realm of covariance matrices in machine\nlearning, particularly focusing on computing Fr\\'echet means on the manifold of\nsymmetric positive definite matrices, commonly referred to as Karcher or\ngeometric means. Such means are leveraged in numerous machine-learning tasks.\nRelying on advanced statistical tools, we introduce a random matrix\ntheory-based method that estimates Fr\\'echet means, which is particularly\nbeneficial when dealing with low sample support and a high number of matrices\nto average. Our experimental evaluation, involving both synthetic and\nreal-world EEG and hyperspectral datasets, shows that we largely outperform\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.06558v1", "date": "2024-05-10", "relevancy": 1.3273, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4649}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.44}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20matrix%20theory%20improved%20Fr%C3%A9chet%20mean%20of%20symmetric%20positive%0A%20%20definite%20matrices&body=Title%3A%20Random%20matrix%20theory%20improved%20Fr%C3%A9chet%20mean%20of%20symmetric%20positive%0A%20%20definite%20matrices%0AAuthor%3A%20Florent%20Bouchard%20and%20Ammar%20Mian%20and%20Malik%20Tiomoko%20and%20Guillaume%20Ginolhac%20and%20Fr%C3%A9d%C3%A9ric%20Pascal%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20consider%20the%20realm%20of%20covariance%20matrices%20in%20machine%0Alearning%2C%20particularly%20focusing%20on%20computing%20Fr%5C%27echet%20means%20on%20the%20manifold%20of%0Asymmetric%20positive%20definite%20matrices%2C%20commonly%20referred%20to%20as%20Karcher%20or%0Ageometric%20means.%20Such%20means%20are%20leveraged%20in%20numerous%20machine-learning%20tasks.%0ARelying%20on%20advanced%20statistical%20tools%2C%20we%20introduce%20a%20random%20matrix%0Atheory-based%20method%20that%20estimates%20Fr%5C%27echet%20means%2C%20which%20is%20particularly%0Abeneficial%20when%20dealing%20with%20low%20sample%20support%20and%20a%20high%20number%20of%20matrices%0Ato%20average.%20Our%20experimental%20evaluation%2C%20involving%20both%20synthetic%20and%0Areal-world%20EEG%20and%20hyperspectral%20datasets%2C%20shows%20that%20we%20largely%20outperform%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520matrix%2520theory%2520improved%2520Fr%25C3%25A9chet%2520mean%2520of%2520symmetric%2520positive%250A%2520%2520definite%2520matrices%26entry.906535625%3DFlorent%2520Bouchard%2520and%2520Ammar%2520Mian%2520and%2520Malik%2520Tiomoko%2520and%2520Guillaume%2520Ginolhac%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Pascal%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520consider%2520the%2520realm%2520of%2520covariance%2520matrices%2520in%2520machine%250Alearning%252C%2520particularly%2520focusing%2520on%2520computing%2520Fr%255C%2527echet%2520means%2520on%2520the%2520manifold%2520of%250Asymmetric%2520positive%2520definite%2520matrices%252C%2520commonly%2520referred%2520to%2520as%2520Karcher%2520or%250Ageometric%2520means.%2520Such%2520means%2520are%2520leveraged%2520in%2520numerous%2520machine-learning%2520tasks.%250ARelying%2520on%2520advanced%2520statistical%2520tools%252C%2520we%2520introduce%2520a%2520random%2520matrix%250Atheory-based%2520method%2520that%2520estimates%2520Fr%255C%2527echet%2520means%252C%2520which%2520is%2520particularly%250Abeneficial%2520when%2520dealing%2520with%2520low%2520sample%2520support%2520and%2520a%2520high%2520number%2520of%2520matrices%250Ato%2520average.%2520Our%2520experimental%2520evaluation%252C%2520involving%2520both%2520synthetic%2520and%250Areal-world%2520EEG%2520and%2520hyperspectral%2520datasets%252C%2520shows%2520that%2520we%2520largely%2520outperform%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20matrix%20theory%20improved%20Fr%C3%A9chet%20mean%20of%20symmetric%20positive%0A%20%20definite%20matrices&entry.906535625=Florent%20Bouchard%20and%20Ammar%20Mian%20and%20Malik%20Tiomoko%20and%20Guillaume%20Ginolhac%20and%20Fr%C3%A9d%C3%A9ric%20Pascal&entry.1292438233=%20%20In%20this%20study%2C%20we%20consider%20the%20realm%20of%20covariance%20matrices%20in%20machine%0Alearning%2C%20particularly%20focusing%20on%20computing%20Fr%5C%27echet%20means%20on%20the%20manifold%20of%0Asymmetric%20positive%20definite%20matrices%2C%20commonly%20referred%20to%20as%20Karcher%20or%0Ageometric%20means.%20Such%20means%20are%20leveraged%20in%20numerous%20machine-learning%20tasks.%0ARelying%20on%20advanced%20statistical%20tools%2C%20we%20introduce%20a%20random%20matrix%0Atheory-based%20method%20that%20estimates%20Fr%5C%27echet%20means%2C%20which%20is%20particularly%0Abeneficial%20when%20dealing%20with%20low%20sample%20support%20and%20a%20high%20number%20of%20matrices%0Ato%20average.%20Our%20experimental%20evaluation%2C%20involving%20both%20synthetic%20and%0Areal-world%20EEG%20and%20hyperspectral%20datasets%2C%20shows%20that%20we%20largely%20outperform%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06558v1&entry.124074799=Read"},
{"title": "Reservoir Computing Benchmarks: a review, a taxonomy, some best\n  practices", "author": "Chester Wringe and Martin Trefzer and Susan Stepney", "abstract": "  Reservoir Computing is an Unconventional Computation model to perform\ncomputation on various different substrates, such as RNNs or physical\nmaterials. The method takes a \"black-box\" approach, training only the outputs\nof the system it is built on. As such, evaluating the computational capacity of\nthese systems can be challenging. We review and critique the evaluation methods\nused in the field of Reservoir Computing. We introduce a categorisation of\nbenchmark tasks. We review multiple examples of benchmarks from the literature\nas applied to reservoir computing, and note their strengths and shortcomings.\nWe suggest ways in which benchmarks and their uses may be improved to the\nbenefit of the reservoir computing community\n", "link": "http://arxiv.org/abs/2405.06561v1", "date": "2024-05-10", "relevancy": 1.2479, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.407}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reservoir%20Computing%20Benchmarks%3A%20a%20review%2C%20a%20taxonomy%2C%20some%20best%0A%20%20practices&body=Title%3A%20Reservoir%20Computing%20Benchmarks%3A%20a%20review%2C%20a%20taxonomy%2C%20some%20best%0A%20%20practices%0AAuthor%3A%20Chester%20Wringe%20and%20Martin%20Trefzer%20and%20Susan%20Stepney%0AAbstract%3A%20%20%20Reservoir%20Computing%20is%20an%20Unconventional%20Computation%20model%20to%20perform%0Acomputation%20on%20various%20different%20substrates%2C%20such%20as%20RNNs%20or%20physical%0Amaterials.%20The%20method%20takes%20a%20%22black-box%22%20approach%2C%20training%20only%20the%20outputs%0Aof%20the%20system%20it%20is%20built%20on.%20As%20such%2C%20evaluating%20the%20computational%20capacity%20of%0Athese%20systems%20can%20be%20challenging.%20We%20review%20and%20critique%20the%20evaluation%20methods%0Aused%20in%20the%20field%20of%20Reservoir%20Computing.%20We%20introduce%20a%20categorisation%20of%0Abenchmark%20tasks.%20We%20review%20multiple%20examples%20of%20benchmarks%20from%20the%20literature%0Aas%20applied%20to%20reservoir%20computing%2C%20and%20note%20their%20strengths%20and%20shortcomings.%0AWe%20suggest%20ways%20in%20which%20benchmarks%20and%20their%20uses%20may%20be%20improved%20to%20the%0Abenefit%20of%20the%20reservoir%20computing%20community%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReservoir%2520Computing%2520Benchmarks%253A%2520a%2520review%252C%2520a%2520taxonomy%252C%2520some%2520best%250A%2520%2520practices%26entry.906535625%3DChester%2520Wringe%2520and%2520Martin%2520Trefzer%2520and%2520Susan%2520Stepney%26entry.1292438233%3D%2520%2520Reservoir%2520Computing%2520is%2520an%2520Unconventional%2520Computation%2520model%2520to%2520perform%250Acomputation%2520on%2520various%2520different%2520substrates%252C%2520such%2520as%2520RNNs%2520or%2520physical%250Amaterials.%2520The%2520method%2520takes%2520a%2520%2522black-box%2522%2520approach%252C%2520training%2520only%2520the%2520outputs%250Aof%2520the%2520system%2520it%2520is%2520built%2520on.%2520As%2520such%252C%2520evaluating%2520the%2520computational%2520capacity%2520of%250Athese%2520systems%2520can%2520be%2520challenging.%2520We%2520review%2520and%2520critique%2520the%2520evaluation%2520methods%250Aused%2520in%2520the%2520field%2520of%2520Reservoir%2520Computing.%2520We%2520introduce%2520a%2520categorisation%2520of%250Abenchmark%2520tasks.%2520We%2520review%2520multiple%2520examples%2520of%2520benchmarks%2520from%2520the%2520literature%250Aas%2520applied%2520to%2520reservoir%2520computing%252C%2520and%2520note%2520their%2520strengths%2520and%2520shortcomings.%250AWe%2520suggest%2520ways%2520in%2520which%2520benchmarks%2520and%2520their%2520uses%2520may%2520be%2520improved%2520to%2520the%250Abenefit%2520of%2520the%2520reservoir%2520computing%2520community%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reservoir%20Computing%20Benchmarks%3A%20a%20review%2C%20a%20taxonomy%2C%20some%20best%0A%20%20practices&entry.906535625=Chester%20Wringe%20and%20Martin%20Trefzer%20and%20Susan%20Stepney&entry.1292438233=%20%20Reservoir%20Computing%20is%20an%20Unconventional%20Computation%20model%20to%20perform%0Acomputation%20on%20various%20different%20substrates%2C%20such%20as%20RNNs%20or%20physical%0Amaterials.%20The%20method%20takes%20a%20%22black-box%22%20approach%2C%20training%20only%20the%20outputs%0Aof%20the%20system%20it%20is%20built%20on.%20As%20such%2C%20evaluating%20the%20computational%20capacity%20of%0Athese%20systems%20can%20be%20challenging.%20We%20review%20and%20critique%20the%20evaluation%20methods%0Aused%20in%20the%20field%20of%20Reservoir%20Computing.%20We%20introduce%20a%20categorisation%20of%0Abenchmark%20tasks.%20We%20review%20multiple%20examples%20of%20benchmarks%20from%20the%20literature%0Aas%20applied%20to%20reservoir%20computing%2C%20and%20note%20their%20strengths%20and%20shortcomings.%0AWe%20suggest%20ways%20in%20which%20benchmarks%20and%20their%20uses%20may%20be%20improved%20to%20the%0Abenefit%20of%20the%20reservoir%20computing%20community%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06561v1&entry.124074799=Read"},
{"title": "The Role of Learning Algorithms in Collective Action", "author": "Omri Ben-Dov and Jake Fawkes and Samira Samadi and Amartya Sanyal", "abstract": "  Collective action in Machine Learning is the study of the control that a\ncoordinated group can have over machine learning algorithms. While previous\nresearch has concentrated on assessing the impact of collectives against Bayes\noptimal classifiers, this perspective is limited, given that in reality,\nclassifiers seldom achieve Bayes optimality and are influenced by the choice of\nlearning algorithms along with their inherent inductive biases. In this work,\nwe initiate the study of how the choice of the learning algorithm plays a role\nin the success of a collective in practical settings. Specifically, we focus on\ndistributionally robust algorithms (DRO), popular for improving a worst group\nerror, and on the popular stochastic gradient descent (SGD), due to its\ninductive bias for \"simpler\" functions. Our empirical results, supported by a\ntheoretical foundation, show that the effective size and success of the\ncollective are highly dependent on properties of the learning algorithm. This\nhighlights the necessity of taking the learning algorithm into account when\nstudying the impact of collective action in Machine learning.\n", "link": "http://arxiv.org/abs/2405.06582v1", "date": "2024-05-10", "relevancy": 1.2898, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4259}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Learning%20Algorithms%20in%20Collective%20Action&body=Title%3A%20The%20Role%20of%20Learning%20Algorithms%20in%20Collective%20Action%0AAuthor%3A%20Omri%20Ben-Dov%20and%20Jake%20Fawkes%20and%20Samira%20Samadi%20and%20Amartya%20Sanyal%0AAbstract%3A%20%20%20Collective%20action%20in%20Machine%20Learning%20is%20the%20study%20of%20the%20control%20that%20a%0Acoordinated%20group%20can%20have%20over%20machine%20learning%20algorithms.%20While%20previous%0Aresearch%20has%20concentrated%20on%20assessing%20the%20impact%20of%20collectives%20against%20Bayes%0Aoptimal%20classifiers%2C%20this%20perspective%20is%20limited%2C%20given%20that%20in%20reality%2C%0Aclassifiers%20seldom%20achieve%20Bayes%20optimality%20and%20are%20influenced%20by%20the%20choice%20of%0Alearning%20algorithms%20along%20with%20their%20inherent%20inductive%20biases.%20In%20this%20work%2C%0Awe%20initiate%20the%20study%20of%20how%20the%20choice%20of%20the%20learning%20algorithm%20plays%20a%20role%0Ain%20the%20success%20of%20a%20collective%20in%20practical%20settings.%20Specifically%2C%20we%20focus%20on%0Adistributionally%20robust%20algorithms%20%28DRO%29%2C%20popular%20for%20improving%20a%20worst%20group%0Aerror%2C%20and%20on%20the%20popular%20stochastic%20gradient%20descent%20%28SGD%29%2C%20due%20to%20its%0Ainductive%20bias%20for%20%22simpler%22%20functions.%20Our%20empirical%20results%2C%20supported%20by%20a%0Atheoretical%20foundation%2C%20show%20that%20the%20effective%20size%20and%20success%20of%20the%0Acollective%20are%20highly%20dependent%20on%20properties%20of%20the%20learning%20algorithm.%20This%0Ahighlights%20the%20necessity%20of%20taking%20the%20learning%20algorithm%20into%20account%20when%0Astudying%20the%20impact%20of%20collective%20action%20in%20Machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Learning%2520Algorithms%2520in%2520Collective%2520Action%26entry.906535625%3DOmri%2520Ben-Dov%2520and%2520Jake%2520Fawkes%2520and%2520Samira%2520Samadi%2520and%2520Amartya%2520Sanyal%26entry.1292438233%3D%2520%2520Collective%2520action%2520in%2520Machine%2520Learning%2520is%2520the%2520study%2520of%2520the%2520control%2520that%2520a%250Acoordinated%2520group%2520can%2520have%2520over%2520machine%2520learning%2520algorithms.%2520While%2520previous%250Aresearch%2520has%2520concentrated%2520on%2520assessing%2520the%2520impact%2520of%2520collectives%2520against%2520Bayes%250Aoptimal%2520classifiers%252C%2520this%2520perspective%2520is%2520limited%252C%2520given%2520that%2520in%2520reality%252C%250Aclassifiers%2520seldom%2520achieve%2520Bayes%2520optimality%2520and%2520are%2520influenced%2520by%2520the%2520choice%2520of%250Alearning%2520algorithms%2520along%2520with%2520their%2520inherent%2520inductive%2520biases.%2520In%2520this%2520work%252C%250Awe%2520initiate%2520the%2520study%2520of%2520how%2520the%2520choice%2520of%2520the%2520learning%2520algorithm%2520plays%2520a%2520role%250Ain%2520the%2520success%2520of%2520a%2520collective%2520in%2520practical%2520settings.%2520Specifically%252C%2520we%2520focus%2520on%250Adistributionally%2520robust%2520algorithms%2520%2528DRO%2529%252C%2520popular%2520for%2520improving%2520a%2520worst%2520group%250Aerror%252C%2520and%2520on%2520the%2520popular%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520due%2520to%2520its%250Ainductive%2520bias%2520for%2520%2522simpler%2522%2520functions.%2520Our%2520empirical%2520results%252C%2520supported%2520by%2520a%250Atheoretical%2520foundation%252C%2520show%2520that%2520the%2520effective%2520size%2520and%2520success%2520of%2520the%250Acollective%2520are%2520highly%2520dependent%2520on%2520properties%2520of%2520the%2520learning%2520algorithm.%2520This%250Ahighlights%2520the%2520necessity%2520of%2520taking%2520the%2520learning%2520algorithm%2520into%2520account%2520when%250Astudying%2520the%2520impact%2520of%2520collective%2520action%2520in%2520Machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Learning%20Algorithms%20in%20Collective%20Action&entry.906535625=Omri%20Ben-Dov%20and%20Jake%20Fawkes%20and%20Samira%20Samadi%20and%20Amartya%20Sanyal&entry.1292438233=%20%20Collective%20action%20in%20Machine%20Learning%20is%20the%20study%20of%20the%20control%20that%20a%0Acoordinated%20group%20can%20have%20over%20machine%20learning%20algorithms.%20While%20previous%0Aresearch%20has%20concentrated%20on%20assessing%20the%20impact%20of%20collectives%20against%20Bayes%0Aoptimal%20classifiers%2C%20this%20perspective%20is%20limited%2C%20given%20that%20in%20reality%2C%0Aclassifiers%20seldom%20achieve%20Bayes%20optimality%20and%20are%20influenced%20by%20the%20choice%20of%0Alearning%20algorithms%20along%20with%20their%20inherent%20inductive%20biases.%20In%20this%20work%2C%0Awe%20initiate%20the%20study%20of%20how%20the%20choice%20of%20the%20learning%20algorithm%20plays%20a%20role%0Ain%20the%20success%20of%20a%20collective%20in%20practical%20settings.%20Specifically%2C%20we%20focus%20on%0Adistributionally%20robust%20algorithms%20%28DRO%29%2C%20popular%20for%20improving%20a%20worst%20group%0Aerror%2C%20and%20on%20the%20popular%20stochastic%20gradient%20descent%20%28SGD%29%2C%20due%20to%20its%0Ainductive%20bias%20for%20%22simpler%22%20functions.%20Our%20empirical%20results%2C%20supported%20by%20a%0Atheoretical%20foundation%2C%20show%20that%20the%20effective%20size%20and%20success%20of%20the%0Acollective%20are%20highly%20dependent%20on%20properties%20of%20the%20learning%20algorithm.%20This%0Ahighlights%20the%20necessity%20of%20taking%20the%20learning%20algorithm%20into%20account%20when%0Astudying%20the%20impact%20of%20collective%20action%20in%20Machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06582v1&entry.124074799=Read"},
{"title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in\n  Large Language Models", "author": "Nishant Vishwamitra and Keyan Guo and Farhan Tajwar Romit and Isabelle Ondracek and Long Cheng and Ziming Zhao and Hongxin Hu", "abstract": "  Online hate is an escalating problem that negatively impacts the lives of\nInternet users, and is also subject to rapid changes due to evolving events,\nresulting in new waves of online hate that pose a critical threat. Detecting\nand mitigating these new waves present two key challenges: it demands\nreasoning-based complex decision-making to determine the presence of hateful\ncontent, and the limited availability of training samples hinders updating the\ndetection model. To address this critical issue, we present a novel framework\ncalled HATEGUARD for effectively moderating new waves of online hate. HATEGUARD\nemploys a reasoning-based approach that leverages the recently introduced\nchain-of-thought (CoT) prompting technique, harnessing the capabilities of\nlarge language models (LLMs). HATEGUARD further achieves prompt-based zero-shot\ndetection by automatically generating and updating detection prompts with new\nderogatory terms and targets in new wave samples to effectively address new\nwaves of online hate. To demonstrate the effectiveness of our approach, we\ncompile a new dataset consisting of tweets related to three recently witnessed\nnew waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the\nUS Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal\npatterns in these new waves concerning the evolution of events and the pressing\nneed for techniques to rapidly update existing moderation tools to counteract\nthem. Comparative evaluations against state-of-the-art tools illustrate the\nsuperiority of our framework, showcasing a substantial 22.22% to 83.33%\nimprovement in detecting the three new waves of online hate. Our work\nhighlights the severe threat posed by the emergence of new waves of online hate\nand represents a paradigm shift in addressing this threat practically.\n", "link": "http://arxiv.org/abs/2312.15099v2", "date": "2024-05-10", "relevancy": 1.2932, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4304}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moderating%20New%20Waves%20of%20Online%20Hate%20with%20Chain-of-Thought%20Reasoning%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Moderating%20New%20Waves%20of%20Online%20Hate%20with%20Chain-of-Thought%20Reasoning%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Nishant%20Vishwamitra%20and%20Keyan%20Guo%20and%20Farhan%20Tajwar%20Romit%20and%20Isabelle%20Ondracek%20and%20Long%20Cheng%20and%20Ziming%20Zhao%20and%20Hongxin%20Hu%0AAbstract%3A%20%20%20Online%20hate%20is%20an%20escalating%20problem%20that%20negatively%20impacts%20the%20lives%20of%0AInternet%20users%2C%20and%20is%20also%20subject%20to%20rapid%20changes%20due%20to%20evolving%20events%2C%0Aresulting%20in%20new%20waves%20of%20online%20hate%20that%20pose%20a%20critical%20threat.%20Detecting%0Aand%20mitigating%20these%20new%20waves%20present%20two%20key%20challenges%3A%20it%20demands%0Areasoning-based%20complex%20decision-making%20to%20determine%20the%20presence%20of%20hateful%0Acontent%2C%20and%20the%20limited%20availability%20of%20training%20samples%20hinders%20updating%20the%0Adetection%20model.%20To%20address%20this%20critical%20issue%2C%20we%20present%20a%20novel%20framework%0Acalled%20HATEGUARD%20for%20effectively%20moderating%20new%20waves%20of%20online%20hate.%20HATEGUARD%0Aemploys%20a%20reasoning-based%20approach%20that%20leverages%20the%20recently%20introduced%0Achain-of-thought%20%28CoT%29%20prompting%20technique%2C%20harnessing%20the%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20HATEGUARD%20further%20achieves%20prompt-based%20zero-shot%0Adetection%20by%20automatically%20generating%20and%20updating%20detection%20prompts%20with%20new%0Aderogatory%20terms%20and%20targets%20in%20new%20wave%20samples%20to%20effectively%20address%20new%0Awaves%20of%20online%20hate.%20To%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20we%0Acompile%20a%20new%20dataset%20consisting%20of%20tweets%20related%20to%20three%20recently%20witnessed%0Anew%20waves%3A%20the%202022%20Russian%20invasion%20of%20Ukraine%2C%20the%202021%20insurrection%20of%20the%0AUS%20Capitol%2C%20and%20the%20COVID-19%20pandemic.%20Our%20studies%20reveal%20crucial%20longitudinal%0Apatterns%20in%20these%20new%20waves%20concerning%20the%20evolution%20of%20events%20and%20the%20pressing%0Aneed%20for%20techniques%20to%20rapidly%20update%20existing%20moderation%20tools%20to%20counteract%0Athem.%20Comparative%20evaluations%20against%20state-of-the-art%20tools%20illustrate%20the%0Asuperiority%20of%20our%20framework%2C%20showcasing%20a%20substantial%2022.22%25%20to%2083.33%25%0Aimprovement%20in%20detecting%20the%20three%20new%20waves%20of%20online%20hate.%20Our%20work%0Ahighlights%20the%20severe%20threat%20posed%20by%20the%20emergence%20of%20new%20waves%20of%20online%20hate%0Aand%20represents%20a%20paradigm%20shift%20in%20addressing%20this%20threat%20practically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModerating%2520New%2520Waves%2520of%2520Online%2520Hate%2520with%2520Chain-of-Thought%2520Reasoning%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DNishant%2520Vishwamitra%2520and%2520Keyan%2520Guo%2520and%2520Farhan%2520Tajwar%2520Romit%2520and%2520Isabelle%2520Ondracek%2520and%2520Long%2520Cheng%2520and%2520Ziming%2520Zhao%2520and%2520Hongxin%2520Hu%26entry.1292438233%3D%2520%2520Online%2520hate%2520is%2520an%2520escalating%2520problem%2520that%2520negatively%2520impacts%2520the%2520lives%2520of%250AInternet%2520users%252C%2520and%2520is%2520also%2520subject%2520to%2520rapid%2520changes%2520due%2520to%2520evolving%2520events%252C%250Aresulting%2520in%2520new%2520waves%2520of%2520online%2520hate%2520that%2520pose%2520a%2520critical%2520threat.%2520Detecting%250Aand%2520mitigating%2520these%2520new%2520waves%2520present%2520two%2520key%2520challenges%253A%2520it%2520demands%250Areasoning-based%2520complex%2520decision-making%2520to%2520determine%2520the%2520presence%2520of%2520hateful%250Acontent%252C%2520and%2520the%2520limited%2520availability%2520of%2520training%2520samples%2520hinders%2520updating%2520the%250Adetection%2520model.%2520To%2520address%2520this%2520critical%2520issue%252C%2520we%2520present%2520a%2520novel%2520framework%250Acalled%2520HATEGUARD%2520for%2520effectively%2520moderating%2520new%2520waves%2520of%2520online%2520hate.%2520HATEGUARD%250Aemploys%2520a%2520reasoning-based%2520approach%2520that%2520leverages%2520the%2520recently%2520introduced%250Achain-of-thought%2520%2528CoT%2529%2520prompting%2520technique%252C%2520harnessing%2520the%2520capabilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520HATEGUARD%2520further%2520achieves%2520prompt-based%2520zero-shot%250Adetection%2520by%2520automatically%2520generating%2520and%2520updating%2520detection%2520prompts%2520with%2520new%250Aderogatory%2520terms%2520and%2520targets%2520in%2520new%2520wave%2520samples%2520to%2520effectively%2520address%2520new%250Awaves%2520of%2520online%2520hate.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520we%250Acompile%2520a%2520new%2520dataset%2520consisting%2520of%2520tweets%2520related%2520to%2520three%2520recently%2520witnessed%250Anew%2520waves%253A%2520the%25202022%2520Russian%2520invasion%2520of%2520Ukraine%252C%2520the%25202021%2520insurrection%2520of%2520the%250AUS%2520Capitol%252C%2520and%2520the%2520COVID-19%2520pandemic.%2520Our%2520studies%2520reveal%2520crucial%2520longitudinal%250Apatterns%2520in%2520these%2520new%2520waves%2520concerning%2520the%2520evolution%2520of%2520events%2520and%2520the%2520pressing%250Aneed%2520for%2520techniques%2520to%2520rapidly%2520update%2520existing%2520moderation%2520tools%2520to%2520counteract%250Athem.%2520Comparative%2520evaluations%2520against%2520state-of-the-art%2520tools%2520illustrate%2520the%250Asuperiority%2520of%2520our%2520framework%252C%2520showcasing%2520a%2520substantial%252022.22%2525%2520to%252083.33%2525%250Aimprovement%2520in%2520detecting%2520the%2520three%2520new%2520waves%2520of%2520online%2520hate.%2520Our%2520work%250Ahighlights%2520the%2520severe%2520threat%2520posed%2520by%2520the%2520emergence%2520of%2520new%2520waves%2520of%2520online%2520hate%250Aand%2520represents%2520a%2520paradigm%2520shift%2520in%2520addressing%2520this%2520threat%2520practically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moderating%20New%20Waves%20of%20Online%20Hate%20with%20Chain-of-Thought%20Reasoning%20in%0A%20%20Large%20Language%20Models&entry.906535625=Nishant%20Vishwamitra%20and%20Keyan%20Guo%20and%20Farhan%20Tajwar%20Romit%20and%20Isabelle%20Ondracek%20and%20Long%20Cheng%20and%20Ziming%20Zhao%20and%20Hongxin%20Hu&entry.1292438233=%20%20Online%20hate%20is%20an%20escalating%20problem%20that%20negatively%20impacts%20the%20lives%20of%0AInternet%20users%2C%20and%20is%20also%20subject%20to%20rapid%20changes%20due%20to%20evolving%20events%2C%0Aresulting%20in%20new%20waves%20of%20online%20hate%20that%20pose%20a%20critical%20threat.%20Detecting%0Aand%20mitigating%20these%20new%20waves%20present%20two%20key%20challenges%3A%20it%20demands%0Areasoning-based%20complex%20decision-making%20to%20determine%20the%20presence%20of%20hateful%0Acontent%2C%20and%20the%20limited%20availability%20of%20training%20samples%20hinders%20updating%20the%0Adetection%20model.%20To%20address%20this%20critical%20issue%2C%20we%20present%20a%20novel%20framework%0Acalled%20HATEGUARD%20for%20effectively%20moderating%20new%20waves%20of%20online%20hate.%20HATEGUARD%0Aemploys%20a%20reasoning-based%20approach%20that%20leverages%20the%20recently%20introduced%0Achain-of-thought%20%28CoT%29%20prompting%20technique%2C%20harnessing%20the%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20HATEGUARD%20further%20achieves%20prompt-based%20zero-shot%0Adetection%20by%20automatically%20generating%20and%20updating%20detection%20prompts%20with%20new%0Aderogatory%20terms%20and%20targets%20in%20new%20wave%20samples%20to%20effectively%20address%20new%0Awaves%20of%20online%20hate.%20To%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20we%0Acompile%20a%20new%20dataset%20consisting%20of%20tweets%20related%20to%20three%20recently%20witnessed%0Anew%20waves%3A%20the%202022%20Russian%20invasion%20of%20Ukraine%2C%20the%202021%20insurrection%20of%20the%0AUS%20Capitol%2C%20and%20the%20COVID-19%20pandemic.%20Our%20studies%20reveal%20crucial%20longitudinal%0Apatterns%20in%20these%20new%20waves%20concerning%20the%20evolution%20of%20events%20and%20the%20pressing%0Aneed%20for%20techniques%20to%20rapidly%20update%20existing%20moderation%20tools%20to%20counteract%0Athem.%20Comparative%20evaluations%20against%20state-of-the-art%20tools%20illustrate%20the%0Asuperiority%20of%20our%20framework%2C%20showcasing%20a%20substantial%2022.22%25%20to%2083.33%25%0Aimprovement%20in%20detecting%20the%20three%20new%20waves%20of%20online%20hate.%20Our%20work%0Ahighlights%20the%20severe%20threat%20posed%20by%20the%20emergence%20of%20new%20waves%20of%20online%20hate%0Aand%20represents%20a%20paradigm%20shift%20in%20addressing%20this%20threat%20practically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15099v2&entry.124074799=Read"},
{"title": "Solving Quantified Boolean Formulas with Few Existential Variables", "author": "Leif Eriksson and Victor Lagerkvist and George Osipov and Sebastian Ordyniak and Fahad Panolan and Mateusz Rychlicki", "abstract": "  The quantified Boolean formula (QBF) problem is an important decision problem\ngenerally viewed as the archetype for PSPACE-completeness. Many problems of\ncentral interest in AI are in general not included in NP, e.g., planning, model\nchecking, and non-monotonic reasoning, and for such problems QBF has\nsuccessfully been used as a modelling tool. However, solvers for QBF are not as\nadvanced as state of the art SAT solvers, which has prevented QBF from becoming\na universal modelling language for PSPACE-complete problems. A theoretical\nexplanation is that QBF (as well as many other PSPACE-complete problems) lacks\nnatural parameters} guaranteeing fixed-parameter tractability (FPT).\n  In this paper we tackle this problem and consider a simple but overlooked\nparameter: the number of existentially quantified variables. This natural\nparameter is virtually unexplored in the literature which one might find\nsurprising given the general scarcity of FPT algorithms for QBF. Via this\nparameterization we then develop a novel FPT algorithm applicable to QBF\ninstances in conjunctive normal form (CNF) of bounded clause length. We\ncomplement this by a W[1]-hardness result for QBF in CNF of unbounded clause\nlength as well as sharper lower bounds for the bounded arity case under the\n(strong) exponential-time hypothesis.\n", "link": "http://arxiv.org/abs/2405.06485v1", "date": "2024-05-10", "relevancy": 1.1301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3963}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3728}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Quantified%20Boolean%20Formulas%20with%20Few%20Existential%20Variables&body=Title%3A%20Solving%20Quantified%20Boolean%20Formulas%20with%20Few%20Existential%20Variables%0AAuthor%3A%20Leif%20Eriksson%20and%20Victor%20Lagerkvist%20and%20George%20Osipov%20and%20Sebastian%20Ordyniak%20and%20Fahad%20Panolan%20and%20Mateusz%20Rychlicki%0AAbstract%3A%20%20%20The%20quantified%20Boolean%20formula%20%28QBF%29%20problem%20is%20an%20important%20decision%20problem%0Agenerally%20viewed%20as%20the%20archetype%20for%20PSPACE-completeness.%20Many%20problems%20of%0Acentral%20interest%20in%20AI%20are%20in%20general%20not%20included%20in%20NP%2C%20e.g.%2C%20planning%2C%20model%0Achecking%2C%20and%20non-monotonic%20reasoning%2C%20and%20for%20such%20problems%20QBF%20has%0Asuccessfully%20been%20used%20as%20a%20modelling%20tool.%20However%2C%20solvers%20for%20QBF%20are%20not%20as%0Aadvanced%20as%20state%20of%20the%20art%20SAT%20solvers%2C%20which%20has%20prevented%20QBF%20from%20becoming%0Aa%20universal%20modelling%20language%20for%20PSPACE-complete%20problems.%20A%20theoretical%0Aexplanation%20is%20that%20QBF%20%28as%20well%20as%20many%20other%20PSPACE-complete%20problems%29%20lacks%0Anatural%20parameters%7D%20guaranteeing%20fixed-parameter%20tractability%20%28FPT%29.%0A%20%20In%20this%20paper%20we%20tackle%20this%20problem%20and%20consider%20a%20simple%20but%20overlooked%0Aparameter%3A%20the%20number%20of%20existentially%20quantified%20variables.%20This%20natural%0Aparameter%20is%20virtually%20unexplored%20in%20the%20literature%20which%20one%20might%20find%0Asurprising%20given%20the%20general%20scarcity%20of%20FPT%20algorithms%20for%20QBF.%20Via%20this%0Aparameterization%20we%20then%20develop%20a%20novel%20FPT%20algorithm%20applicable%20to%20QBF%0Ainstances%20in%20conjunctive%20normal%20form%20%28CNF%29%20of%20bounded%20clause%20length.%20We%0Acomplement%20this%20by%20a%20W%5B1%5D-hardness%20result%20for%20QBF%20in%20CNF%20of%20unbounded%20clause%0Alength%20as%20well%20as%20sharper%20lower%20bounds%20for%20the%20bounded%20arity%20case%20under%20the%0A%28strong%29%20exponential-time%20hypothesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Quantified%2520Boolean%2520Formulas%2520with%2520Few%2520Existential%2520Variables%26entry.906535625%3DLeif%2520Eriksson%2520and%2520Victor%2520Lagerkvist%2520and%2520George%2520Osipov%2520and%2520Sebastian%2520Ordyniak%2520and%2520Fahad%2520Panolan%2520and%2520Mateusz%2520Rychlicki%26entry.1292438233%3D%2520%2520The%2520quantified%2520Boolean%2520formula%2520%2528QBF%2529%2520problem%2520is%2520an%2520important%2520decision%2520problem%250Agenerally%2520viewed%2520as%2520the%2520archetype%2520for%2520PSPACE-completeness.%2520Many%2520problems%2520of%250Acentral%2520interest%2520in%2520AI%2520are%2520in%2520general%2520not%2520included%2520in%2520NP%252C%2520e.g.%252C%2520planning%252C%2520model%250Achecking%252C%2520and%2520non-monotonic%2520reasoning%252C%2520and%2520for%2520such%2520problems%2520QBF%2520has%250Asuccessfully%2520been%2520used%2520as%2520a%2520modelling%2520tool.%2520However%252C%2520solvers%2520for%2520QBF%2520are%2520not%2520as%250Aadvanced%2520as%2520state%2520of%2520the%2520art%2520SAT%2520solvers%252C%2520which%2520has%2520prevented%2520QBF%2520from%2520becoming%250Aa%2520universal%2520modelling%2520language%2520for%2520PSPACE-complete%2520problems.%2520A%2520theoretical%250Aexplanation%2520is%2520that%2520QBF%2520%2528as%2520well%2520as%2520many%2520other%2520PSPACE-complete%2520problems%2529%2520lacks%250Anatural%2520parameters%257D%2520guaranteeing%2520fixed-parameter%2520tractability%2520%2528FPT%2529.%250A%2520%2520In%2520this%2520paper%2520we%2520tackle%2520this%2520problem%2520and%2520consider%2520a%2520simple%2520but%2520overlooked%250Aparameter%253A%2520the%2520number%2520of%2520existentially%2520quantified%2520variables.%2520This%2520natural%250Aparameter%2520is%2520virtually%2520unexplored%2520in%2520the%2520literature%2520which%2520one%2520might%2520find%250Asurprising%2520given%2520the%2520general%2520scarcity%2520of%2520FPT%2520algorithms%2520for%2520QBF.%2520Via%2520this%250Aparameterization%2520we%2520then%2520develop%2520a%2520novel%2520FPT%2520algorithm%2520applicable%2520to%2520QBF%250Ainstances%2520in%2520conjunctive%2520normal%2520form%2520%2528CNF%2529%2520of%2520bounded%2520clause%2520length.%2520We%250Acomplement%2520this%2520by%2520a%2520W%255B1%255D-hardness%2520result%2520for%2520QBF%2520in%2520CNF%2520of%2520unbounded%2520clause%250Alength%2520as%2520well%2520as%2520sharper%2520lower%2520bounds%2520for%2520the%2520bounded%2520arity%2520case%2520under%2520the%250A%2528strong%2529%2520exponential-time%2520hypothesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Quantified%20Boolean%20Formulas%20with%20Few%20Existential%20Variables&entry.906535625=Leif%20Eriksson%20and%20Victor%20Lagerkvist%20and%20George%20Osipov%20and%20Sebastian%20Ordyniak%20and%20Fahad%20Panolan%20and%20Mateusz%20Rychlicki&entry.1292438233=%20%20The%20quantified%20Boolean%20formula%20%28QBF%29%20problem%20is%20an%20important%20decision%20problem%0Agenerally%20viewed%20as%20the%20archetype%20for%20PSPACE-completeness.%20Many%20problems%20of%0Acentral%20interest%20in%20AI%20are%20in%20general%20not%20included%20in%20NP%2C%20e.g.%2C%20planning%2C%20model%0Achecking%2C%20and%20non-monotonic%20reasoning%2C%20and%20for%20such%20problems%20QBF%20has%0Asuccessfully%20been%20used%20as%20a%20modelling%20tool.%20However%2C%20solvers%20for%20QBF%20are%20not%20as%0Aadvanced%20as%20state%20of%20the%20art%20SAT%20solvers%2C%20which%20has%20prevented%20QBF%20from%20becoming%0Aa%20universal%20modelling%20language%20for%20PSPACE-complete%20problems.%20A%20theoretical%0Aexplanation%20is%20that%20QBF%20%28as%20well%20as%20many%20other%20PSPACE-complete%20problems%29%20lacks%0Anatural%20parameters%7D%20guaranteeing%20fixed-parameter%20tractability%20%28FPT%29.%0A%20%20In%20this%20paper%20we%20tackle%20this%20problem%20and%20consider%20a%20simple%20but%20overlooked%0Aparameter%3A%20the%20number%20of%20existentially%20quantified%20variables.%20This%20natural%0Aparameter%20is%20virtually%20unexplored%20in%20the%20literature%20which%20one%20might%20find%0Asurprising%20given%20the%20general%20scarcity%20of%20FPT%20algorithms%20for%20QBF.%20Via%20this%0Aparameterization%20we%20then%20develop%20a%20novel%20FPT%20algorithm%20applicable%20to%20QBF%0Ainstances%20in%20conjunctive%20normal%20form%20%28CNF%29%20of%20bounded%20clause%20length.%20We%0Acomplement%20this%20by%20a%20W%5B1%5D-hardness%20result%20for%20QBF%20in%20CNF%20of%20unbounded%20clause%0Alength%20as%20well%20as%20sharper%20lower%20bounds%20for%20the%20bounded%20arity%20case%20under%20the%0A%28strong%29%20exponential-time%20hypothesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06485v1&entry.124074799=Read"},
{"title": "Attention is all they need: Cognitive science and the (techno)political\n  economy of attention in humans and machines", "author": "Pablo Gonz\u00e1lez de la Torre and Marta P\u00e9rez-Verdugo and Xabier E. Barandiaran", "abstract": "  This paper critically analyses the \"attention economy\" within the framework\nof cognitive science and techno-political economics, as applied to both human\nand machine interactions. We explore how current business models, particularly\nin digital platform capitalism, harness user engagement by strategically\nshaping attentional patterns. These platforms utilize advanced AI and massive\ndata analytics to enhance user engagement, creating a cycle of attention\ncapture and data extraction. We review contemporary (neuro)cognitive theories\nof attention and platform engagement design techniques and criticize classical\ncognitivist and behaviourist theories for their inadequacies in addressing the\npotential harms of such engagement on user autonomy and wellbeing. 4E\napproaches to cognitive science, instead, emphasizing the embodied, extended,\nenactive, and ecological aspects of cognition, offer us an intrinsic normative\nstandpoint and a more integrated understanding of how attentional patterns are\nactively constituted by adaptive digital environments. By examining the\nprecarious nature of habit formation in digital contexts, we reveal the\ntechno-economic underpinnings that threaten personal autonomy by disaggregating\nhabits away from the individual, into an AI managed collection of behavioural\npatterns. Our current predicament suggests the necessity of a paradigm shift\ntowards an ecology of attention. This shift aims to foster environments that\nrespect and preserve human cognitive and social capacities, countering the\nexploitative tendencies of cognitive capitalism.\n", "link": "http://arxiv.org/abs/2405.06478v1", "date": "2024-05-10", "relevancy": 1.1903, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4047}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3962}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20is%20all%20they%20need%3A%20Cognitive%20science%20and%20the%20%28techno%29political%0A%20%20economy%20of%20attention%20in%20humans%20and%20machines&body=Title%3A%20Attention%20is%20all%20they%20need%3A%20Cognitive%20science%20and%20the%20%28techno%29political%0A%20%20economy%20of%20attention%20in%20humans%20and%20machines%0AAuthor%3A%20Pablo%20Gonz%C3%A1lez%20de%20la%20Torre%20and%20Marta%20P%C3%A9rez-Verdugo%20and%20Xabier%20E.%20Barandiaran%0AAbstract%3A%20%20%20This%20paper%20critically%20analyses%20the%20%22attention%20economy%22%20within%20the%20framework%0Aof%20cognitive%20science%20and%20techno-political%20economics%2C%20as%20applied%20to%20both%20human%0Aand%20machine%20interactions.%20We%20explore%20how%20current%20business%20models%2C%20particularly%0Ain%20digital%20platform%20capitalism%2C%20harness%20user%20engagement%20by%20strategically%0Ashaping%20attentional%20patterns.%20These%20platforms%20utilize%20advanced%20AI%20and%20massive%0Adata%20analytics%20to%20enhance%20user%20engagement%2C%20creating%20a%20cycle%20of%20attention%0Acapture%20and%20data%20extraction.%20We%20review%20contemporary%20%28neuro%29cognitive%20theories%0Aof%20attention%20and%20platform%20engagement%20design%20techniques%20and%20criticize%20classical%0Acognitivist%20and%20behaviourist%20theories%20for%20their%20inadequacies%20in%20addressing%20the%0Apotential%20harms%20of%20such%20engagement%20on%20user%20autonomy%20and%20wellbeing.%204E%0Aapproaches%20to%20cognitive%20science%2C%20instead%2C%20emphasizing%20the%20embodied%2C%20extended%2C%0Aenactive%2C%20and%20ecological%20aspects%20of%20cognition%2C%20offer%20us%20an%20intrinsic%20normative%0Astandpoint%20and%20a%20more%20integrated%20understanding%20of%20how%20attentional%20patterns%20are%0Aactively%20constituted%20by%20adaptive%20digital%20environments.%20By%20examining%20the%0Aprecarious%20nature%20of%20habit%20formation%20in%20digital%20contexts%2C%20we%20reveal%20the%0Atechno-economic%20underpinnings%20that%20threaten%20personal%20autonomy%20by%20disaggregating%0Ahabits%20away%20from%20the%20individual%2C%20into%20an%20AI%20managed%20collection%20of%20behavioural%0Apatterns.%20Our%20current%20predicament%20suggests%20the%20necessity%20of%20a%20paradigm%20shift%0Atowards%20an%20ecology%20of%20attention.%20This%20shift%20aims%20to%20foster%20environments%20that%0Arespect%20and%20preserve%20human%20cognitive%20and%20social%20capacities%2C%20countering%20the%0Aexploitative%20tendencies%20of%20cognitive%20capitalism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520is%2520all%2520they%2520need%253A%2520Cognitive%2520science%2520and%2520the%2520%2528techno%2529political%250A%2520%2520economy%2520of%2520attention%2520in%2520humans%2520and%2520machines%26entry.906535625%3DPablo%2520Gonz%25C3%25A1lez%2520de%2520la%2520Torre%2520and%2520Marta%2520P%25C3%25A9rez-Verdugo%2520and%2520Xabier%2520E.%2520Barandiaran%26entry.1292438233%3D%2520%2520This%2520paper%2520critically%2520analyses%2520the%2520%2522attention%2520economy%2522%2520within%2520the%2520framework%250Aof%2520cognitive%2520science%2520and%2520techno-political%2520economics%252C%2520as%2520applied%2520to%2520both%2520human%250Aand%2520machine%2520interactions.%2520We%2520explore%2520how%2520current%2520business%2520models%252C%2520particularly%250Ain%2520digital%2520platform%2520capitalism%252C%2520harness%2520user%2520engagement%2520by%2520strategically%250Ashaping%2520attentional%2520patterns.%2520These%2520platforms%2520utilize%2520advanced%2520AI%2520and%2520massive%250Adata%2520analytics%2520to%2520enhance%2520user%2520engagement%252C%2520creating%2520a%2520cycle%2520of%2520attention%250Acapture%2520and%2520data%2520extraction.%2520We%2520review%2520contemporary%2520%2528neuro%2529cognitive%2520theories%250Aof%2520attention%2520and%2520platform%2520engagement%2520design%2520techniques%2520and%2520criticize%2520classical%250Acognitivist%2520and%2520behaviourist%2520theories%2520for%2520their%2520inadequacies%2520in%2520addressing%2520the%250Apotential%2520harms%2520of%2520such%2520engagement%2520on%2520user%2520autonomy%2520and%2520wellbeing.%25204E%250Aapproaches%2520to%2520cognitive%2520science%252C%2520instead%252C%2520emphasizing%2520the%2520embodied%252C%2520extended%252C%250Aenactive%252C%2520and%2520ecological%2520aspects%2520of%2520cognition%252C%2520offer%2520us%2520an%2520intrinsic%2520normative%250Astandpoint%2520and%2520a%2520more%2520integrated%2520understanding%2520of%2520how%2520attentional%2520patterns%2520are%250Aactively%2520constituted%2520by%2520adaptive%2520digital%2520environments.%2520By%2520examining%2520the%250Aprecarious%2520nature%2520of%2520habit%2520formation%2520in%2520digital%2520contexts%252C%2520we%2520reveal%2520the%250Atechno-economic%2520underpinnings%2520that%2520threaten%2520personal%2520autonomy%2520by%2520disaggregating%250Ahabits%2520away%2520from%2520the%2520individual%252C%2520into%2520an%2520AI%2520managed%2520collection%2520of%2520behavioural%250Apatterns.%2520Our%2520current%2520predicament%2520suggests%2520the%2520necessity%2520of%2520a%2520paradigm%2520shift%250Atowards%2520an%2520ecology%2520of%2520attention.%2520This%2520shift%2520aims%2520to%2520foster%2520environments%2520that%250Arespect%2520and%2520preserve%2520human%2520cognitive%2520and%2520social%2520capacities%252C%2520countering%2520the%250Aexploitative%2520tendencies%2520of%2520cognitive%2520capitalism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20is%20all%20they%20need%3A%20Cognitive%20science%20and%20the%20%28techno%29political%0A%20%20economy%20of%20attention%20in%20humans%20and%20machines&entry.906535625=Pablo%20Gonz%C3%A1lez%20de%20la%20Torre%20and%20Marta%20P%C3%A9rez-Verdugo%20and%20Xabier%20E.%20Barandiaran&entry.1292438233=%20%20This%20paper%20critically%20analyses%20the%20%22attention%20economy%22%20within%20the%20framework%0Aof%20cognitive%20science%20and%20techno-political%20economics%2C%20as%20applied%20to%20both%20human%0Aand%20machine%20interactions.%20We%20explore%20how%20current%20business%20models%2C%20particularly%0Ain%20digital%20platform%20capitalism%2C%20harness%20user%20engagement%20by%20strategically%0Ashaping%20attentional%20patterns.%20These%20platforms%20utilize%20advanced%20AI%20and%20massive%0Adata%20analytics%20to%20enhance%20user%20engagement%2C%20creating%20a%20cycle%20of%20attention%0Acapture%20and%20data%20extraction.%20We%20review%20contemporary%20%28neuro%29cognitive%20theories%0Aof%20attention%20and%20platform%20engagement%20design%20techniques%20and%20criticize%20classical%0Acognitivist%20and%20behaviourist%20theories%20for%20their%20inadequacies%20in%20addressing%20the%0Apotential%20harms%20of%20such%20engagement%20on%20user%20autonomy%20and%20wellbeing.%204E%0Aapproaches%20to%20cognitive%20science%2C%20instead%2C%20emphasizing%20the%20embodied%2C%20extended%2C%0Aenactive%2C%20and%20ecological%20aspects%20of%20cognition%2C%20offer%20us%20an%20intrinsic%20normative%0Astandpoint%20and%20a%20more%20integrated%20understanding%20of%20how%20attentional%20patterns%20are%0Aactively%20constituted%20by%20adaptive%20digital%20environments.%20By%20examining%20the%0Aprecarious%20nature%20of%20habit%20formation%20in%20digital%20contexts%2C%20we%20reveal%20the%0Atechno-economic%20underpinnings%20that%20threaten%20personal%20autonomy%20by%20disaggregating%0Ahabits%20away%20from%20the%20individual%2C%20into%20an%20AI%20managed%20collection%20of%20behavioural%0Apatterns.%20Our%20current%20predicament%20suggests%20the%20necessity%20of%20a%20paradigm%20shift%0Atowards%20an%20ecology%20of%20attention.%20This%20shift%20aims%20to%20foster%20environments%20that%0Arespect%20and%20preserve%20human%20cognitive%20and%20social%20capacities%2C%20countering%20the%0Aexploitative%20tendencies%20of%20cognitive%20capitalism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06478v1&entry.124074799=Read"},
{"title": "LLM Discussion: Enhancing the Creativity of Large Language Models via\n  Discussion Framework and Role-Play", "author": "Li-Chun Lu and Shou-Jen Chen and Tsung-Min Pai and Chan-Hung Yu and Hung-yi Lee and Shao-Hua Sun", "abstract": "  Large language models (LLMs) have shown exceptional proficiency in natural\nlanguage processing but often fall short of generating creative and original\nresponses to open-ended questions. To enhance LLM creativity, our key insight\nis to emulate the human process of inducing collective creativity through\nengaging discussions with participants from diverse backgrounds and\nperspectives. To this end, we propose LLM Discussion, a three-phase discussion\nframework that facilitates vigorous and diverging idea exchanges and ensures\nconvergence to creative answers. Moreover, we adopt a role-playing technique by\nassigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate\nthe efficacy of the proposed framework with the Alternative Uses Test,\nSimilarities Test, Instances Test, and Scientific Creativity Test through both\nLLM evaluation and human study. Our proposed framework outperforms single-LLM\napproaches and existing multi-LLM frameworks across various creativity metrics.\n", "link": "http://arxiv.org/abs/2405.06373v1", "date": "2024-05-10", "relevancy": 1.3734, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4701}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Discussion%3A%20Enhancing%20the%20Creativity%20of%20Large%20Language%20Models%20via%0A%20%20Discussion%20Framework%20and%20Role-Play&body=Title%3A%20LLM%20Discussion%3A%20Enhancing%20the%20Creativity%20of%20Large%20Language%20Models%20via%0A%20%20Discussion%20Framework%20and%20Role-Play%0AAuthor%3A%20Li-Chun%20Lu%20and%20Shou-Jen%20Chen%20and%20Tsung-Min%20Pai%20and%20Chan-Hung%20Yu%20and%20Hung-yi%20Lee%20and%20Shao-Hua%20Sun%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20exceptional%20proficiency%20in%20natural%0Alanguage%20processing%20but%20often%20fall%20short%20of%20generating%20creative%20and%20original%0Aresponses%20to%20open-ended%20questions.%20To%20enhance%20LLM%20creativity%2C%20our%20key%20insight%0Ais%20to%20emulate%20the%20human%20process%20of%20inducing%20collective%20creativity%20through%0Aengaging%20discussions%20with%20participants%20from%20diverse%20backgrounds%20and%0Aperspectives.%20To%20this%20end%2C%20we%20propose%20LLM%20Discussion%2C%20a%20three-phase%20discussion%0Aframework%20that%20facilitates%20vigorous%20and%20diverging%20idea%20exchanges%20and%20ensures%0Aconvergence%20to%20creative%20answers.%20Moreover%2C%20we%20adopt%20a%20role-playing%20technique%20by%0Aassigning%20distinct%20roles%20to%20LLMs%20to%20combat%20the%20homogeneity%20of%20LLMs.%20We%20evaluate%0Athe%20efficacy%20of%20the%20proposed%20framework%20with%20the%20Alternative%20Uses%20Test%2C%0ASimilarities%20Test%2C%20Instances%20Test%2C%20and%20Scientific%20Creativity%20Test%20through%20both%0ALLM%20evaluation%20and%20human%20study.%20Our%20proposed%20framework%20outperforms%20single-LLM%0Aapproaches%20and%20existing%20multi-LLM%20frameworks%20across%20various%20creativity%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Discussion%253A%2520Enhancing%2520the%2520Creativity%2520of%2520Large%2520Language%2520Models%2520via%250A%2520%2520Discussion%2520Framework%2520and%2520Role-Play%26entry.906535625%3DLi-Chun%2520Lu%2520and%2520Shou-Jen%2520Chen%2520and%2520Tsung-Min%2520Pai%2520and%2520Chan-Hung%2520Yu%2520and%2520Hung-yi%2520Lee%2520and%2520Shao-Hua%2520Sun%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520exceptional%2520proficiency%2520in%2520natural%250Alanguage%2520processing%2520but%2520often%2520fall%2520short%2520of%2520generating%2520creative%2520and%2520original%250Aresponses%2520to%2520open-ended%2520questions.%2520To%2520enhance%2520LLM%2520creativity%252C%2520our%2520key%2520insight%250Ais%2520to%2520emulate%2520the%2520human%2520process%2520of%2520inducing%2520collective%2520creativity%2520through%250Aengaging%2520discussions%2520with%2520participants%2520from%2520diverse%2520backgrounds%2520and%250Aperspectives.%2520To%2520this%2520end%252C%2520we%2520propose%2520LLM%2520Discussion%252C%2520a%2520three-phase%2520discussion%250Aframework%2520that%2520facilitates%2520vigorous%2520and%2520diverging%2520idea%2520exchanges%2520and%2520ensures%250Aconvergence%2520to%2520creative%2520answers.%2520Moreover%252C%2520we%2520adopt%2520a%2520role-playing%2520technique%2520by%250Aassigning%2520distinct%2520roles%2520to%2520LLMs%2520to%2520combat%2520the%2520homogeneity%2520of%2520LLMs.%2520We%2520evaluate%250Athe%2520efficacy%2520of%2520the%2520proposed%2520framework%2520with%2520the%2520Alternative%2520Uses%2520Test%252C%250ASimilarities%2520Test%252C%2520Instances%2520Test%252C%2520and%2520Scientific%2520Creativity%2520Test%2520through%2520both%250ALLM%2520evaluation%2520and%2520human%2520study.%2520Our%2520proposed%2520framework%2520outperforms%2520single-LLM%250Aapproaches%2520and%2520existing%2520multi-LLM%2520frameworks%2520across%2520various%2520creativity%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Discussion%3A%20Enhancing%20the%20Creativity%20of%20Large%20Language%20Models%20via%0A%20%20Discussion%20Framework%20and%20Role-Play&entry.906535625=Li-Chun%20Lu%20and%20Shou-Jen%20Chen%20and%20Tsung-Min%20Pai%20and%20Chan-Hung%20Yu%20and%20Hung-yi%20Lee%20and%20Shao-Hua%20Sun&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20exceptional%20proficiency%20in%20natural%0Alanguage%20processing%20but%20often%20fall%20short%20of%20generating%20creative%20and%20original%0Aresponses%20to%20open-ended%20questions.%20To%20enhance%20LLM%20creativity%2C%20our%20key%20insight%0Ais%20to%20emulate%20the%20human%20process%20of%20inducing%20collective%20creativity%20through%0Aengaging%20discussions%20with%20participants%20from%20diverse%20backgrounds%20and%0Aperspectives.%20To%20this%20end%2C%20we%20propose%20LLM%20Discussion%2C%20a%20three-phase%20discussion%0Aframework%20that%20facilitates%20vigorous%20and%20diverging%20idea%20exchanges%20and%20ensures%0Aconvergence%20to%20creative%20answers.%20Moreover%2C%20we%20adopt%20a%20role-playing%20technique%20by%0Aassigning%20distinct%20roles%20to%20LLMs%20to%20combat%20the%20homogeneity%20of%20LLMs.%20We%20evaluate%0Athe%20efficacy%20of%20the%20proposed%20framework%20with%20the%20Alternative%20Uses%20Test%2C%0ASimilarities%20Test%2C%20Instances%20Test%2C%20and%20Scientific%20Creativity%20Test%20through%20both%0ALLM%20evaluation%20and%20human%20study.%20Our%20proposed%20framework%20outperforms%20single-LLM%0Aapproaches%20and%20existing%20multi-LLM%20frameworks%20across%20various%20creativity%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06373v1&entry.124074799=Read"},
{"title": "Explaining Arguments' Strength: Unveiling the Role of Attacks and\n  Supports (Technical Report)", "author": "Xiang Yin and Potyka Nico and Francesca Toni", "abstract": "  Quantitatively explaining the strength of arguments under gradual semantics\nhas recently received increasing attention. Specifically, several works in the\nliterature provide quantitative explanations by computing the attribution\nscores of arguments. These works disregard the importance of attacks and\nsupports, even though they play an essential role when explaining arguments'\nstrength. In this paper, we propose a novel theory of Relation Attribution\nExplanations (RAEs), adapting Shapley values from game theory to offer\nfine-grained insights into the role of attacks and supports in quantitative\nbipolar argumentation towards obtaining the arguments' strength. We show that\nRAEs satisfy several desirable properties. We also propose a probabilistic\nalgorithm to approximate RAEs efficiently. Finally, we show the application\nvalue of RAEs in fraud detection and large language models case studies.\n", "link": "http://arxiv.org/abs/2404.14304v2", "date": "2024-05-10", "relevancy": 1.3021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4175}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Arguments%27%20Strength%3A%20Unveiling%20the%20Role%20of%20Attacks%20and%0A%20%20Supports%20%28Technical%20Report%29&body=Title%3A%20Explaining%20Arguments%27%20Strength%3A%20Unveiling%20the%20Role%20of%20Attacks%20and%0A%20%20Supports%20%28Technical%20Report%29%0AAuthor%3A%20Xiang%20Yin%20and%20Potyka%20Nico%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20Quantitatively%20explaining%20the%20strength%20of%20arguments%20under%20gradual%20semantics%0Ahas%20recently%20received%20increasing%20attention.%20Specifically%2C%20several%20works%20in%20the%0Aliterature%20provide%20quantitative%20explanations%20by%20computing%20the%20attribution%0Ascores%20of%20arguments.%20These%20works%20disregard%20the%20importance%20of%20attacks%20and%0Asupports%2C%20even%20though%20they%20play%20an%20essential%20role%20when%20explaining%20arguments%27%0Astrength.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20theory%20of%20Relation%20Attribution%0AExplanations%20%28RAEs%29%2C%20adapting%20Shapley%20values%20from%20game%20theory%20to%20offer%0Afine-grained%20insights%20into%20the%20role%20of%20attacks%20and%20supports%20in%20quantitative%0Abipolar%20argumentation%20towards%20obtaining%20the%20arguments%27%20strength.%20We%20show%20that%0ARAEs%20satisfy%20several%20desirable%20properties.%20We%20also%20propose%20a%20probabilistic%0Aalgorithm%20to%20approximate%20RAEs%20efficiently.%20Finally%2C%20we%20show%20the%20application%0Avalue%20of%20RAEs%20in%20fraud%20detection%20and%20large%20language%20models%20case%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Arguments%2527%2520Strength%253A%2520Unveiling%2520the%2520Role%2520of%2520Attacks%2520and%250A%2520%2520Supports%2520%2528Technical%2520Report%2529%26entry.906535625%3DXiang%2520Yin%2520and%2520Potyka%2520Nico%2520and%2520Francesca%2520Toni%26entry.1292438233%3D%2520%2520Quantitatively%2520explaining%2520the%2520strength%2520of%2520arguments%2520under%2520gradual%2520semantics%250Ahas%2520recently%2520received%2520increasing%2520attention.%2520Specifically%252C%2520several%2520works%2520in%2520the%250Aliterature%2520provide%2520quantitative%2520explanations%2520by%2520computing%2520the%2520attribution%250Ascores%2520of%2520arguments.%2520These%2520works%2520disregard%2520the%2520importance%2520of%2520attacks%2520and%250Asupports%252C%2520even%2520though%2520they%2520play%2520an%2520essential%2520role%2520when%2520explaining%2520arguments%2527%250Astrength.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520theory%2520of%2520Relation%2520Attribution%250AExplanations%2520%2528RAEs%2529%252C%2520adapting%2520Shapley%2520values%2520from%2520game%2520theory%2520to%2520offer%250Afine-grained%2520insights%2520into%2520the%2520role%2520of%2520attacks%2520and%2520supports%2520in%2520quantitative%250Abipolar%2520argumentation%2520towards%2520obtaining%2520the%2520arguments%2527%2520strength.%2520We%2520show%2520that%250ARAEs%2520satisfy%2520several%2520desirable%2520properties.%2520We%2520also%2520propose%2520a%2520probabilistic%250Aalgorithm%2520to%2520approximate%2520RAEs%2520efficiently.%2520Finally%252C%2520we%2520show%2520the%2520application%250Avalue%2520of%2520RAEs%2520in%2520fraud%2520detection%2520and%2520large%2520language%2520models%2520case%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Arguments%27%20Strength%3A%20Unveiling%20the%20Role%20of%20Attacks%20and%0A%20%20Supports%20%28Technical%20Report%29&entry.906535625=Xiang%20Yin%20and%20Potyka%20Nico%20and%20Francesca%20Toni&entry.1292438233=%20%20Quantitatively%20explaining%20the%20strength%20of%20arguments%20under%20gradual%20semantics%0Ahas%20recently%20received%20increasing%20attention.%20Specifically%2C%20several%20works%20in%20the%0Aliterature%20provide%20quantitative%20explanations%20by%20computing%20the%20attribution%0Ascores%20of%20arguments.%20These%20works%20disregard%20the%20importance%20of%20attacks%20and%0Asupports%2C%20even%20though%20they%20play%20an%20essential%20role%20when%20explaining%20arguments%27%0Astrength.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20theory%20of%20Relation%20Attribution%0AExplanations%20%28RAEs%29%2C%20adapting%20Shapley%20values%20from%20game%20theory%20to%20offer%0Afine-grained%20insights%20into%20the%20role%20of%20attacks%20and%20supports%20in%20quantitative%0Abipolar%20argumentation%20towards%20obtaining%20the%20arguments%27%20strength.%20We%20show%20that%0ARAEs%20satisfy%20several%20desirable%20properties.%20We%20also%20propose%20a%20probabilistic%0Aalgorithm%20to%20approximate%20RAEs%20efficiently.%20Finally%2C%20we%20show%20the%20application%0Avalue%20of%20RAEs%20in%20fraud%20detection%20and%20large%20language%20models%20case%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14304v2&entry.124074799=Read"},
{"title": "Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides", "author": "Kaikai An and Fangkai Yang and Junting Lu and Liqun Li and Zhixing Ren and Hao Huang and Lu Wang and Pu Zhao and Yu Kang and Hua Ding and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang and Qi Zhang", "abstract": "  Effective incident management is pivotal for the smooth operation of\nenterprises-level cloud services. In order to expedite incident mitigation,\nservice teams compile troubleshooting knowledge into Troubleshooting Guides\n(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are\nenabled to resolve the most frequent and easy incidents, there still exist\ncomplex incidents that require OCEs' intervention. However, TSGs are often\nunstructured and incomplete, which requires manual interpretation by OCEs,\nleading to on-call fatigue and decreased productivity, especially among\nnew-hire OCEs. In this work, we propose Nissist which leverages TSGs and\nincident mitigation histories to provide proactive suggestions, reducing human\nintervention. Leveraging Large Language Models (LLM), Nissist extracts insights\nfrom unstructured TSGs and historical incident mitigation discussions, forming\na comprehensive knowledge base. Its multi-agent system design enhances\nproficiency in precisely discerning user queries, retrieving relevant\ninformation, and delivering systematic plans consecutively. Through our user\ncase and experiment, we demonstrate that Nissist significant reduce Time to\nMitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs\nand improving service reliability. Our demo is available at\nhttps://aka.ms/nissist_demo.\n", "link": "http://arxiv.org/abs/2402.17531v2", "date": "2024-05-10", "relevancy": 1.2841, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4373}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4265}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nissist%3A%20An%20Incident%20Mitigation%20Copilot%20based%20on%20Troubleshooting%20Guides&body=Title%3A%20Nissist%3A%20An%20Incident%20Mitigation%20Copilot%20based%20on%20Troubleshooting%20Guides%0AAuthor%3A%20Kaikai%20An%20and%20Fangkai%20Yang%20and%20Junting%20Lu%20and%20Liqun%20Li%20and%20Zhixing%20Ren%20and%20Hao%20Huang%20and%20Lu%20Wang%20and%20Pu%20Zhao%20and%20Yu%20Kang%20and%20Hua%20Ding%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Effective%20incident%20management%20is%20pivotal%20for%20the%20smooth%20operation%20of%0Aenterprises-level%20cloud%20services.%20In%20order%20to%20expedite%20incident%20mitigation%2C%0Aservice%20teams%20compile%20troubleshooting%20knowledge%20into%20Troubleshooting%20Guides%0A%28TSGs%29%20accessible%20to%20on-call%20engineers%20%28OCEs%29.%20While%20automated%20pipelines%20are%0Aenabled%20to%20resolve%20the%20most%20frequent%20and%20easy%20incidents%2C%20there%20still%20exist%0Acomplex%20incidents%20that%20require%20OCEs%27%20intervention.%20However%2C%20TSGs%20are%20often%0Aunstructured%20and%20incomplete%2C%20which%20requires%20manual%20interpretation%20by%20OCEs%2C%0Aleading%20to%20on-call%20fatigue%20and%20decreased%20productivity%2C%20especially%20among%0Anew-hire%20OCEs.%20In%20this%20work%2C%20we%20propose%20Nissist%20which%20leverages%20TSGs%20and%0Aincident%20mitigation%20histories%20to%20provide%20proactive%20suggestions%2C%20reducing%20human%0Aintervention.%20Leveraging%20Large%20Language%20Models%20%28LLM%29%2C%20Nissist%20extracts%20insights%0Afrom%20unstructured%20TSGs%20and%20historical%20incident%20mitigation%20discussions%2C%20forming%0Aa%20comprehensive%20knowledge%20base.%20Its%20multi-agent%20system%20design%20enhances%0Aproficiency%20in%20precisely%20discerning%20user%20queries%2C%20retrieving%20relevant%0Ainformation%2C%20and%20delivering%20systematic%20plans%20consecutively.%20Through%20our%20user%0Acase%20and%20experiment%2C%20we%20demonstrate%20that%20Nissist%20significant%20reduce%20Time%20to%0AMitigate%20%28TTM%29%20in%20incident%20mitigation%2C%20alleviating%20operational%20burdens%20on%20OCEs%0Aand%20improving%20service%20reliability.%20Our%20demo%20is%20available%20at%0Ahttps%3A//aka.ms/nissist_demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNissist%253A%2520An%2520Incident%2520Mitigation%2520Copilot%2520based%2520on%2520Troubleshooting%2520Guides%26entry.906535625%3DKaikai%2520An%2520and%2520Fangkai%2520Yang%2520and%2520Junting%2520Lu%2520and%2520Liqun%2520Li%2520and%2520Zhixing%2520Ren%2520and%2520Hao%2520Huang%2520and%2520Lu%2520Wang%2520and%2520Pu%2520Zhao%2520and%2520Yu%2520Kang%2520and%2520Hua%2520Ding%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Effective%2520incident%2520management%2520is%2520pivotal%2520for%2520the%2520smooth%2520operation%2520of%250Aenterprises-level%2520cloud%2520services.%2520In%2520order%2520to%2520expedite%2520incident%2520mitigation%252C%250Aservice%2520teams%2520compile%2520troubleshooting%2520knowledge%2520into%2520Troubleshooting%2520Guides%250A%2528TSGs%2529%2520accessible%2520to%2520on-call%2520engineers%2520%2528OCEs%2529.%2520While%2520automated%2520pipelines%2520are%250Aenabled%2520to%2520resolve%2520the%2520most%2520frequent%2520and%2520easy%2520incidents%252C%2520there%2520still%2520exist%250Acomplex%2520incidents%2520that%2520require%2520OCEs%2527%2520intervention.%2520However%252C%2520TSGs%2520are%2520often%250Aunstructured%2520and%2520incomplete%252C%2520which%2520requires%2520manual%2520interpretation%2520by%2520OCEs%252C%250Aleading%2520to%2520on-call%2520fatigue%2520and%2520decreased%2520productivity%252C%2520especially%2520among%250Anew-hire%2520OCEs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Nissist%2520which%2520leverages%2520TSGs%2520and%250Aincident%2520mitigation%2520histories%2520to%2520provide%2520proactive%2520suggestions%252C%2520reducing%2520human%250Aintervention.%2520Leveraging%2520Large%2520Language%2520Models%2520%2528LLM%2529%252C%2520Nissist%2520extracts%2520insights%250Afrom%2520unstructured%2520TSGs%2520and%2520historical%2520incident%2520mitigation%2520discussions%252C%2520forming%250Aa%2520comprehensive%2520knowledge%2520base.%2520Its%2520multi-agent%2520system%2520design%2520enhances%250Aproficiency%2520in%2520precisely%2520discerning%2520user%2520queries%252C%2520retrieving%2520relevant%250Ainformation%252C%2520and%2520delivering%2520systematic%2520plans%2520consecutively.%2520Through%2520our%2520user%250Acase%2520and%2520experiment%252C%2520we%2520demonstrate%2520that%2520Nissist%2520significant%2520reduce%2520Time%2520to%250AMitigate%2520%2528TTM%2529%2520in%2520incident%2520mitigation%252C%2520alleviating%2520operational%2520burdens%2520on%2520OCEs%250Aand%2520improving%2520service%2520reliability.%2520Our%2520demo%2520is%2520available%2520at%250Ahttps%253A//aka.ms/nissist_demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nissist%3A%20An%20Incident%20Mitigation%20Copilot%20based%20on%20Troubleshooting%20Guides&entry.906535625=Kaikai%20An%20and%20Fangkai%20Yang%20and%20Junting%20Lu%20and%20Liqun%20Li%20and%20Zhixing%20Ren%20and%20Hao%20Huang%20and%20Lu%20Wang%20and%20Pu%20Zhao%20and%20Yu%20Kang%20and%20Hua%20Ding%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%20and%20Qi%20Zhang&entry.1292438233=%20%20Effective%20incident%20management%20is%20pivotal%20for%20the%20smooth%20operation%20of%0Aenterprises-level%20cloud%20services.%20In%20order%20to%20expedite%20incident%20mitigation%2C%0Aservice%20teams%20compile%20troubleshooting%20knowledge%20into%20Troubleshooting%20Guides%0A%28TSGs%29%20accessible%20to%20on-call%20engineers%20%28OCEs%29.%20While%20automated%20pipelines%20are%0Aenabled%20to%20resolve%20the%20most%20frequent%20and%20easy%20incidents%2C%20there%20still%20exist%0Acomplex%20incidents%20that%20require%20OCEs%27%20intervention.%20However%2C%20TSGs%20are%20often%0Aunstructured%20and%20incomplete%2C%20which%20requires%20manual%20interpretation%20by%20OCEs%2C%0Aleading%20to%20on-call%20fatigue%20and%20decreased%20productivity%2C%20especially%20among%0Anew-hire%20OCEs.%20In%20this%20work%2C%20we%20propose%20Nissist%20which%20leverages%20TSGs%20and%0Aincident%20mitigation%20histories%20to%20provide%20proactive%20suggestions%2C%20reducing%20human%0Aintervention.%20Leveraging%20Large%20Language%20Models%20%28LLM%29%2C%20Nissist%20extracts%20insights%0Afrom%20unstructured%20TSGs%20and%20historical%20incident%20mitigation%20discussions%2C%20forming%0Aa%20comprehensive%20knowledge%20base.%20Its%20multi-agent%20system%20design%20enhances%0Aproficiency%20in%20precisely%20discerning%20user%20queries%2C%20retrieving%20relevant%0Ainformation%2C%20and%20delivering%20systematic%20plans%20consecutively.%20Through%20our%20user%0Acase%20and%20experiment%2C%20we%20demonstrate%20that%20Nissist%20significant%20reduce%20Time%20to%0AMitigate%20%28TTM%29%20in%20incident%20mitigation%2C%20alleviating%20operational%20burdens%20on%20OCEs%0Aand%20improving%20service%20reliability.%20Our%20demo%20is%20available%20at%0Ahttps%3A//aka.ms/nissist_demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17531v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


